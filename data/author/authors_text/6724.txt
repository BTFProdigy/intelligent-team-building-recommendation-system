Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 9?16
Manchester, August 2008
A Supervised Algorithm for Verb Disambiguation into VerbNet Classes
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
VerbNet (VN) is a major large-scale En-
glish verb lexicon. Mapping verb instances
to their VN classes has been proven use-
ful for several NLP tasks. However, verbs
are polysemous with respect to their VN
classes. We introduce a novel supervised
learning model for mapping verb instances
to VN classes, using rich syntactic features
and class membership constraints. We
evaluate the algorithm in both in-domain
and corpus adaptation scenarios. In both
cases, we use the manually tagged Sem-
link WSJ corpus as training data. For in-
domain (testing on Semlink WSJ data), we
achieve 95.9% accuracy, 35.1% error re-
duction (ER) over a strong baseline. For
adaptation, we test on the GENIA corpus
and achieve 72.4% accuracy with 10.7%
ER. This is the first large-scale experimen-
tation with automatic algorithms for this
task.
1 Introduction
The organization of verbs into classes whose mem-
bers exhibit similar syntactic and semantic behav-
ior has been discussed extensively in the linguistics
literature (see e.g. (Levin and Rappaport Hovav,
2005; Levin, 1993)). Such an organization helps
in avoiding lexicon representation redundancy and
enables generalizations across similar verbs. It
can also be of great practical use, e.g. in com-
pensating NLP statistical models for data sparse-
ness. Indeed, Levin?s seminal work had motivated
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
much research aimed at automatic discovery of
verb classes (see Section 2).
VerbNet (VN) (Kipper et al, 2000; Kipper-
Schuler, 2005) is a large scale, publicly available
domain independent verb lexicon that builds on
Levin classes and extends them with new verbs,
new classes, and additional information such as
semantic roles and selectional restrictions. VN
classes were proven beneficial for Semantic Role
Labeling (SRL) (Swier and Stevenson, 2005), Se-
mantic Parsing (Shi and Mihalcea, 2005) and
building conceptual graphs (Hensman and Dun-
nion, 2004). Levin-inspired classes have been
used in several NLP tasks, such as Machine Trans-
lation (Dorr, 1997) and Document Classification
(Klavans and Kan, 1998).
Many applications that use VN need to map verb
instances onto their VN classes. However, verbs
are polysemous with respect to VN classes. Sem-
link (Loper et al, 2007) is a dataset that maps each
verb instance in the WSJ Penn Treebank to its VN
class. The mapping has been created using a com-
bination of automatic and manual methods. Yi et
al. (2007) have used Semlink to improve SRL.
In this paper we present the first large-scale ex-
perimentation with a supervised machine learning
classification algorithm for disambiguating verb
instances to their VN classes. We use rich syntactic
features extracted from a treebank-style parse tree,
and utilize a learning algorithm capable of impos-
ing class membership constraints, thus taking ad-
vantage of the nature of our task. We use Semlink
as the training set.
We evaluate our algorithm in both in-domain
and corpus adaptation scenarios. In the former,
we test on the WSJ (using Semlink again), ob-
taining 95.9% accuracy with 35.1% error reduc-
tion (ER) over a strong baseline (most frequent
9
class) when using a modern statistical parser. In
the corpus adaptation scenario, we disambiguate
verbs in sentences taken from outside the train-
ing domain. Since the manual annotation of new
corpora is costly, and since VN is designed to be
a domain independent resource, adaptation results
are important to the usability in NLP in practice.
We manually annotated 400 sentences from GE-
NIA (Kim et al, 2003), a medical domain cor-
pus1. Testing on these, we achieved 72.4% ac-
curacy with 10.7% ER. Our adaptation scenario
is complete in the sense that the parser we use
was also trained on a different corpus (WSJ). We
also report experiments done using gold-standard
(manually created) parses.
The most relevant previous works addressing
verb instance class classification are (Lapata and
Brew, 2004; Li and Brew, 2007; Girju et al, 2005).
The former two do not use VerbNet and their ex-
periments were narrower than ours, so we can-
not compare to their results. The latter mapped to
VN, but used a preliminary highly restricted setup
where most instances were monosemous. For
completeness, we compared our method to theirs2,
achieving similar results.
We review related work in Section 2, and dis-
cuss the task in Section 3. Section 4 introduces the
model, Section 5 describes the experimental setup,
and Section 6 presents our results.
2 Related Work
VerbNet. VN is a major electronic English verb
lexicon. It is organized in a hierarchical struc-
ture of classes and sub-classes, each sub-class in-
heriting the full characterization of its super-class.
VN is built on a refinement of the Levin classes,
the intersective Levin classes (Dang et al, 1998),
aimed at achieving more coherent classes both se-
mantically and syntactically. VN was also sub-
stantially extended (Kipper et al, 2006) using the
Levin classes extension proposed in (Korhonen
and Briscoe, 2004). VN today contains 3626 verb
lemmas (forms), organized in 237 main classes
having 4991 verb types (we refer to a lemma with
an ascribed class as a type). Of the 3626 lem-
mas, 912 are polysemous (i.e., appear in more
than a single class). VN?s significant coverage of
the English verb lexicon is demonstrated by the
1Our annotations will be made available to the community.
2Using the same sentences and instances, obtained from
the authors.
75.5% coverage of VN classes over PropBank3
instances (Loper et al, 2007). Each class con-
tains rich semantic information, including seman-
tic roles of the arguments augmented with se-
lectional restrictions, and possible subcategoriza-
tion frames consisting of a syntactic description
and semantic predicates with temporal informa-
tion. VN thematic roles are relatively coarse, vs.
the situation-specific FrameNet role system or the
verb-specific PropBank role system, enabling gen-
eralizations across a wide semantic scope. Swier
and Stevenson (2005) and Yi et al (2007) used VN
for SRL.
Verb type classification. Quite a few works
have addressed the issue of verb type classification
and in particular classification to ?Levin inspired?
classes (e.g., (Schulte im Walde, 2000; Merlo and
Stevenson, 2001)). Such work is not comparable
to ours, as it deals with verb type (sense) rather
than verb token (instance) classification.
Verb token classification. Lapata and Brew
(2004) dealt with classification to Levin classes of
polysemous verbs. They established a prior from
the BNC in an unsupervised manner. They also
showed that this prior helps in the training of a
naive Bayes classifier employed to distinguish be-
tween possible verb classes of a given verb in a
given frame (when the ambiguity is not solved by
knowing the frame alone). Li and Brew (2007) ex-
tended this model by proposing a method to train
the class disambiguator without using hand-tagged
data. While these papers have good results, their
experimental setup was rather narrow and used
only at most 67 polysemous verbs (in 4 frames).
VN includes 912 polysemous verbs, of which 695
appeared in our in-domain experiments.
Girju et al (2005) performed the only previous
work we are aware of that addresses the problem of
token level verb disambiguation into VN classes.
They treated the task as a supervised learning prob-
lem, proposing features based on a POS tagger, a
Chunker and a named entity classifier. In order
to create the data4, they used a mapping between
Propbank rolesets and VN classes, and took the in-
stances in WSJ sections 15-18,20,21 that were an-
notated by Propbank and for which the roleset de-
termines the VN class uniquely. This resulted in
most instances being in fact monosemous. Their
3Propbank (Palmer et al, 2005) is a corpus annotation of
the WSJ sections of the Penn Treebank with semantic roles of
each verbal proposition.
4Semlink was not available then.
10
experiment was conducted in a WSJ in-domain
scenario, and in a much narrower scope than in
this paper. They had 870 (39 polysemous) unique
verb lemmas, compared to 2091 (695 polysemous)
in our in-domain scenario. They did not test their
model in an adaptation scenario. The scope and
difficulty contrast between our setup and theirs are
demonstrated by the large differences in the num-
ber of instances and in the percentage of polyse-
mous instances: 972/12431 (7.8%) in theirs, com-
pared to 49571/84749 (58.5%) in our in-domain
scenario (training+test). We compared our method
to theirs for completeness and achieved similar re-
sults.
Semlink. The Semlink project (Yi et al, 2007;
Loper et al, 2007) aims to create a mapping of
PropBank, FrameNet (Baker et al, 1998), Word-
Net (henceforth WN) and VN to one another, thus
allowing these resources to synergize. In addition,
the project includes the most extensive token map-
ping of verbs to their VN classes available today.
It covers all verbs in the WSJ sections of the Penn
Treebank within VN coverage (out of 113K verb
instances, 97K have lemmas present in VN).
3 Nature of the Task
Polysemy is a major issue in NLP. Verbs are not an
exception, resulting in a single verb form (lemma)
appearing in more than a single class. This pol-
ysemy is also present in the original Levin clas-
sification, where polysemous classes account for
more than 48% of the BNC verb instances (Lapata
and Brew, 2004).
Given a verb instance whose lemma is within
the coverage of VN, given the sentence in which
it appears, given a parse tree of this sentence (see
below), and given the VN resource, our task is to
classify the verb instance to its correct VN class.
There are currently 237 possible classes5. Each
verb has only a few possible classes (no more than
10, but only about 2.5 on the average over the poly-
semous verbs). Depending on the application, the
parse tree for the sentence may be either a gold
standard parse or a parse tree generated by a parser.
We have experimented with both options.
The task can be viewed in two complemen-
tary ways: per-class and per-verb type. The per-
class perspective takes into consideration the small
5We ignore sub-class distinctions. This is justified since in
98.2% of the in-coverage instances in Semlink, knowing the
verb and its class suffices for knowing its exact sub-class.
number of classes relative to the number of types6.
A classifier may gather valuable information for all
members of a certain VN class, without seeing all
of its members in the training data. From this per-
spective the task resembles POS tagging. In both
tasks there are many dozens (or more) of possible
labels, while each word has only a small subset of
possible labels. Different words may receive the
same label.
The per-verb perspective takes into consider-
ation the special properties of every verb type.
Even the best lexicons necessarily ignore certain
idiosyncratic characteristics of the verb when as-
signing it to a certain class. If a verb appears
many times in the corpus, it is possible to estimate
its parameters to a reasonable reliability, and thus
to use its specific distributional properties for dis-
ambiguation. Viewed in this manner, the task re-
sembles a word sense disambiguation (WSD) task:
each verb has a small distinct set of senses (types),
and no two different verbs have the same sense.
The similarity to WSD suggests that our task
might be solved by WN sense disambiguation fol-
lowed by a mapping from WN to VN. However,
good results are not to be expected, due to the
medium quality of today?s WSD algorithms and
because the mapping between WN and VN is both
incomplete and many-to-many7. Even for a perfect
WN WSD algorithm, the resulting WN synset may
not be mapped to VN at all or may be mapped onto
multiple VN classes. We experimented with this
method and obtained results below the MF base-
line we used8.
The above discussion does not rule out the pos-
sibility of obtaining reasonable results through ap-
plying a high quality WSD engine followed by a
WN to VN mapping. However, there are much
fewer VN classes than WN classes per verb. This
may result in the WSD engine learning many dis-
tinctions that are not useful in this context, which
may in turn jeopardize its performance with re-
spect to our task. Moreover, a word sense may
belong to a single verb only while a VN class con-
tains many verbs. Consequently, the performance
6237 classes vs. 4991 types.
7In the WN to VN mapping built into VN, 14.69% of the
covered WN synsets were mapped to more than a single VN
class.
8We used the publicly available SenseLearner 2.0, the VB-
Collocations model. We chose VN classes containing the
lemma in random when a single mapping is not specified. We
obtained 67.74% accuracy on section 00 of the WSJ, which is
less than the MF baseline. See Sections 5 and 7.
11
on a certain lemma may benefit from training in-
stances of other lemmas.
Note that our task is not reducible to VN frame
identification (a non-trivial task given the rich-
ness of the information used to define a frame
in VN). Although the categorizing criterion for
Levin?s classification is the subset of frames the
verb may appear in (equivalently, the diathesis al-
ternations the verbal proposition may perform),
knowing only the frame in which an instance ap-
pears does not suffice, as frames are shared among
classes.
4 The Learning Model
As common in supervised learning models, we en-
code the verb instances into feature vectors and
then apply a learning algorithm to induce a clas-
sifier. We first discuss the feature set and then the
learning algorithm.
Features. Our feature set heavily relies on syn-
tactic annotation. Dorr and Jones (1996) showed
that perfect knowledge of the allowable syntactic
frames for a verb allows 98% accuracy in type as-
signment to Levin classes. This motivates the en-
coding of the syntactic structure of the sentence
as features, since we have no access to all frames,
only to the one appearing in the sentence.
Since some verbs may appear in the same syn-
tactic frame in different VN classes, a model rely-
ing on the syntactic frame alone would not be able
to disambiguate instances of these verbs when ap-
pearing in those frames. Hence our features in-
clude lexical context words. The parse tree en-
ables us to use words that appear in specific syn-
tactic slots rather than in a linear window around
the verb. To this end, we use the head words of
the neighboring constituents. The definition of the
head of a constituent is given in (Collins, 1999).
Our feature set is comprised of two parallel sets
of features. The first contains features extracted
from the parse tree and the verb?s lemma as a stan-
dalone feature. In the second set, each feature is a
conjunction of a feature from the first set with the
verb?s lemma. By doing so we created a general
feature space shared by all verbs, and replications
of it for each and every verb. This feature selection
strategy was chosen in view of the two perspec-
tives on the task (per-class and per-verb) discussed
in Section 3.
Our first set of features encodes the verb?s con-
text as inferred from the sentence?s parse tree (Fig-
First Feature Set
The stemmed head words, POS, parse tree labels,
function tags, and ordinals of the verb?s right k
r
siblings (k
r
is the maximum number of right sib-
lings in the corpus. These are at most 5k
r
differ-
ent features).
The stemmed head words, POS, labels, function
tags and ordinals of the verb?s left k
l
siblings, as
above.
The stemmed head word & POS of the ?second
head word? nodes on the left and right (see text
for precise definition).
All of the above features employed on the sib-
lings of the parent of the verb (only if the verb?s
parent is the head constituent of its grandparent)
The number of right/left siblings of the verb.
The number of right/left siblings of the verb?s
parent.
The parse tree label of the verb?s parent.
The verb?s voice (active or passive).
The verb?s lemma.
Figure 1: The first set of features in our model. All
of them are binary. The final feature set includes
two sets: the set here, and a set obtained by its
conjunction with the verb?s lemma.
ure 1). We attempt to encode both the syntactic
frame, by encoding the tree structure, and the ar-
gument preferences, by encoding the head words
of the arguments and their POS. The restriction on
the verb?s parent being the head constituent of its
grandparent is done in order to focus on the correct
verb in verb series such as ?intend to run?.
The 3rd cell in the table makes use of a ?sec-
ond head word? node, defined as follows. Consider
a left sibling (right siblings are addressed analo-
gously) M of the verb?s node. Take the node H
in the subtree of M where M ?s head appears. H
is a descendent of a node J which is a child of
M . The ?second head word? node is J?s sibling on
the right. For example, in the sentence We went to
school (see Figure 2) the head word of the PP ?to
school? is ?to?, and the ?second head word? node is
?school?. The rationale is that ?school? could be a
useful feature for ?went?, in addition to ?to?, which
is highly polysemous (note that it is also a feature
for ?went?, in the 1st and 2nd cells of the table).
The voice feature was computed using a simple
heuristic based on the verb?s POS tag (past partici-
ple) and presence of auxiliary verbs to its left.
12
SNP
PRP
We
VP
VBD
went
PP
TO
to
NP
NN
school
Figure 2: An example parse tree for the ?second
head word? feature.
The current set of features does not detect verb
particle constructions. We leave this for future re-
search.
Learning Algorithm. Our learning task can be
formulated as follows. Let x
i
denote the feature
vector of an instance i, and let X denote the space
of all such feature vectors. The subset of possi-
ble labels for x
i
is denoted by C
i
, and the correct
label by c
i
? C
i
. We denote the label space by
S. Let T be the training set of instances T = {<
x
1
, C
1
, c
1
>,< x
2
, C
2
, c
2
>, ..., < x
n
, C
n
, c
n
>
} ? (X ? 2
S
? S)
n
, where n is the size of the
training set. Let < x
n+1
, C
n+1
>? (X ? 2
S
) be
a new instance. Our task is to select which of the
labels in C
n+1
is its correct label c
n+1
(x
n+1
does
not have to be a previously observed lemma, but
its lemma must appear in a VN class).
The structure of the task lets us apply a learn-
ing algorithm that is especially appropriate for it.
What we need is an algorithm that allows us to re-
strict the possible labels of each instance, both in
training and in testing. The sequential model algo-
rithm presented by Even-Zohar and Roth (2001)
directly supports this requirement. We use the
SNOW learning architecture for multi-class clas-
sification (Roth, 1998), which contains an imple-
mentation of that algorithm 9.
5 Experimental Setup
We used SemLink VN annotations and parse trees
on sections 02-21 of the WSJ Penn Treebank for
training, and section 00 as a development set, as
is common in the parsing community. We per-
formed two parallel sets of experiments, one us-
ing manually created gold standard parse trees and
one using parse trees created by a state-of-the-art
9Experiments on development data revealed that for verbs
for which almost all of the training instances are mapped to
the same VN class, it is most beneficial to select that class.
Thus, where more than 90% of the training instances of a verb
are mapped to the same class, our algorithm mapped the in-
stances of the verb to that class regardless of the context.
parser (Charniak and Johnson, 2005) (Note that
this parser does not output function tags). The
parser was also trained on sections 02-21 and tuned
on section 0010. Consequently, our adaptation sce-
nario is a full adaptation situation in which both the
parser and the VerbNet training data are not in the
test domain. Note that generative parser adaptation
results are known to be of much lower quality than
in-domain results (Lease and Charniak, 2005). The
quality of the discriminative parser we used did
indeed decrease in our adaptation scenario (Sec-
tion 7).
The training data included 71209 VN in-scope
instances (of them 41753 polysemous) and the de-
velopment 3624 instances (2203 polysemous). An
?in-scope? instance is one that appears in VN and
is tagged with a verb POS. The same trained model
was used in both the in-domain and adaptation sce-
narios, which only differ in their test sets.
In-Domain. Tests were held on sections
01,22,23,24 of WSJ PTB. Test data includes all in-
scope instances for which there is a SemLink anno-
tation, yielding 13540 instances, 7798 (i.e., 57.6%)
of them polysemous.
Adaptation. For the testing we annotated sen-
tences from GENIA (Kim et al, 2003) (version
3.0.2). The GENIA corpus is composed of MED-
LINE abstracts related to transcription factors in
human blood cells. We annotated 400 sentences
from the corpus, each including at least one in-
scope verb instance. We took the first 400 sen-
tences from the corpus that met that criterion11 .
After cleaning some GENIA POS inconsistencies,
this amounts to 690 in-scope instances (380 of
them polysemous). The tagging was done by two
annotators with an inter-annotator agreement rate
of 80.35% and Kappa 67.66%.
Baselines. We used two baselines, random and
most frequent (MF). The random baseline selects
uniformly and independently one of the possible
classes of the verb. The most frequent (MF) base-
line selects the most frequent class of the verb in
the training data for verbs seen while training, and
selects in random for the unseen ones. Conse-
quently, it obtains a perfect score over the monose-
mous verbs. This baseline is a strong one and is
common in disambiguation tasks.
We repeated all of the setup above in two sce-
10For the very few sentences out of coverage for the parser,
we used the MF baseline (see below).
11Discarding the first 120 sentences, which were used to
design the annotator guidelines.
13
narios. In the first (main) scenario, in-scope in-
stances were always mapped to VN classes, while
in the second (?other is possible? (OIP)) scenario,
in-scope instances were allowed to be tagged (dur-
ing training) and classified (during test) as not be-
longing to any existing VN class12. In all cases,
out-of-scope instances (verbs whose lemmas do
not appear in VN) were ignored. For the OIP sce-
nario, we used a different ?other? label for each of
the lemmas, not a single label shared by them all.
6 Results
Table 1 shows our results. In addition to the over-
all results, we also show results for the polysemous
ones alone, since the task is trivial for the monose-
mous ones. The results using gold standard parses
effectively set an upper bound on our model?s per-
formance, while those using statistical parser out-
put demonstrate its current usability.
In-Domain. Results are shown in the WSJ ?
WSJ columns of Table 1. Using gold standard
parses (top), we achieve 96.42% accuracy over-
all. Over the polysemous verbs, the accuracy is
93.68%. This translates to an error reduction over
the MF baseline of 43.35% overall and 43.22% for
the polysemous verbs. In the ?other is possible?
scenario (right), we obtained 36.67% error reduc-
tion. Using a state-of-the-art parser (Charniak and
Johnson, 2005) (bottom), we experienced some
degradation of the results (as expected), but they
remained significantly above baseline. We achieve
95.9% accuracy overall and 92.77% for the polyse-
mous verbs, which translates to about 35.13% and
35.04% error reduction respectively. In the OIP
scenario, we obtained 28.95% error reduction.
The results of the random baseline for the in-
domain scenario are substantially worse than the
MF baseline. On the WSJ the random baseline
scored 66.97% (37.51%) accuracy in the main
(OIP) scenarios.
Adaptation. Here we test our model?s ability
to generalize across domains. Since VN is sup-
posed to be a domain independent resource, we
hope to acquire statistics that are relevant across
domains as well and so to enable us to automati-
cally map verbs in domains of various genres. The
results are shown in the WSJ ? GENIA columns
of Table 1. When using gold standard parses, our
model scored 73.16%. This translates to about
13.17% ER on GENIA. We interestingly experi-
12i.e., including instances tagged by SemLink as ?none?.
enced very little degradation in the results when
moving to parser output, achieving 72.4% accu-
racy which translates to 10.71% error reduction
over the MF baseline. The random baseline on GE-
NIA was again worse than MF, obtaining 66.04%
accuracy as compared to 69.09% of MF (in the OIP
scenario, 39.12% compared to 46.41%).
Run-time performance. Given a parsed cor-
pus, our main model trains and runs in no more
than a few minutes for a training set of ?60K in-
stances and a test set of ?11K instances, using a
Pentium 4 CPU 2.40GHz with 1GB main mem-
ory. The bottleneck in tagging large corpora using
our model is thus most likely the running time of
current parsers.
7 Discussion
In this paper we introduced a new statistical model
for automatically mapping verb instances into
VerbNet classes, and presented the first large-scale
experiments for this task, for both in-domain and
corpus adaptation scenarios.
Using gold standard parse trees, we achieved
96.42% accuracy on WSJ test data, showing
43.35% error reduction over a strong baseline.
For adaptation to the GENIA corpus, we showed
13.1% error reduction over the baseline. A sur-
prising result in the context of adaptation is the lit-
tle influence of using gold standard parses versus
using parser output, especially given the relatively
low performance of today?s parsers in the adapta-
tion task (91.4% F-score for the WSJ in-domain
scenario compared to 81.24% F-score when pars-
ing our GENIA test set). This is an interesting di-
rection for future work.
In addition, we conducted some additional pre-
liminary experiments in order to shed light on
some aspects of the task. The experiments reported
below were conducted on the development data,
given gold standard parse trees.
First, motivated by the close connection be-
tween WSD and our task (see Section 3), we con-
ducted an experiment to test the applicability of
using a WSD engine. In addition to the experi-
ments listed above, we also attempted to encode
the output of a modern WSD engine (the VBCollo-
cations Model of SenseLearner 2.0 (Mihalcea and
Csomai, 2005)), both by encoding the synset (if
exists) of the verb instance as a feature, and by en-
coding each possible mapped class of the WSD
engine output synset as a feature. There are k
14
Main Scenario ?Other is Possible? (OIP) Scenario
WSJ?WSJ WSJ?GENIA WSJ?WSJ WSJ?GENIA
MF Model MF Model MF Model MF Model
Gold Std Total 93.68 96.42 69.09 73.16 88.6 92.78 46.41 52.46
ER 43.35 13.17 36.67 11.29
Poly. 88.87 93.68 48.58 55.35 ? ? ? ?
ER 43.22 13.17 ? ?
Parser Total 93.68 95.9 69.09 72.4 88.6 91.9 46.41 52.46
ER 35.13 10.71 28.95 11.29
Poly. 88.87 92.77 48.58 55.35 ? ? ? ?
ER 35.04 10.72 ? ?
Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline.
Error reduction is computed as MODEL?MF
100?MF
. Results are given for the WSJ and GENIA corpora test
sets. The top table is for a model receiving gold standard parses of the test data. The bottom is for a
model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data. In the main scenario
(left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during
both training and test) to map instances as not belonging to any existing class. For the latter, no results
are displayed for polysemous verbs, since each verb can be mapped both to ?other? and to at least one
class.
features if there are k possible classes13. There
was no improvement over the previous model. A
possible reason for this is the performance of the
WSD engine (e.g. 56.1% precision on the verbs in
Senseval-3 all-words task data). Naturally, more
research is needed to establish better methods of
incorporating WSD information to assist in this
task.
Second, we studied the relative usability of class
information as opposed to verb idiosyncratic infor-
mation in the VN disambiguation task. By mea-
suring the accuracy of our model, first given the
per-class features (the first set of features exclud-
ing the verb?s lemma feature) and second given the
per-verb features (the conjunction of the first set
with the verb?s lemma), we tried to address this
question. We obtained 94.82% accuracy for the
per-class experiment, and 95.51% for the per-verb
experiment, compared to 95.95% when using both
in the in-domain gold standard scenario. The MF
baseline scored 92.45% on this development set.
These results, which are close in the per-class ex-
periment to those of the MF baseline, indicate that
combining both approaches in the construction of
the classifier is justified.
Third, we studied the importance of having a
learning algorithm utilizing the task?s structure
(mapping into a large label space where each in-
13The mapping is many-to-many and partial. To overcome
the first issue, given a WN sense of the verb, we encoded all
possible VN classes that correspond to it. To overcome the
second, we treated a verb in a certain VN class, for which the
mapping to WN was available, as one that can be mapped to
all WN senses of the verb.
stance can be mapped to only a small subspace).
Our choice of the algorithm in (Even-Zohar and
Roth, 2001) was done in light of this requirement.
We conducted an experiment in which we omitted
these per-instance restrictions on the label space,
effectively allowing each verb to take every label
in the label space. We obtained 94.54% accuracy,
which translates to 27.68% error reduction, com-
pared to 95.95% accuracy (46.36% error reduc-
tion) when using the restrictions. These results in-
dicate that although our feature set keeps us sub-
stantially above baseline even without the above
algorithm, using it boosts our results even further.
This result is different from the results obtained
in (Girju et al, 2005), where the results of the un-
constrained (flat) model were significantly below
baseline.
As noted earlier, the field of instance level
verb classification into Levin-inspired classes is far
from being exhaustively explored. We intend to
make our implementation of the model available
to the community, to enable others to engage in
further research on this task.
Acknowledgements. We would like to thank Dan
Roth, Mark Sammons and Ran Luria for their help.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. Proc. of the
36th Meeting of the ACL and the 17th COLING.
Eugene Charniak and Mark Johnson, 2005. Coarse-
15
to-fine n-best parsing and maxent discriminative
reranking. Proc. of the 43rd Meeting of the ACL.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Hoa Trang Dang, Karin Kipper, Martha Palmer and
Joseph Rosenzweig, 1998. Investigating regular
sense extensions based on intersective Levin classes.
Proc. of the 36th Meeting of the ACL and the 17th
COLING.
Bonnie J. Dorr, 1997. Large-Scale Dictionary Con-
struction for Foreign Language Tutoring and Inter-
lingual Machine Translation. Machine Translation,
12:1-55.
Bonnie J. Dorr and Douglas Jones, 1996. Role of Word
Sense Disambiguation in Lexical Acquisition: Pre-
dicting Semantics from Syntactic Cues. Proc. of the
16th COLING.
Yair Even-Zohar and Dan Roth, 2001. A Sequential
Model for Multi-Class Classification. Proc. of the
2001 Conference on Empirical Methods in Natural
Language Processing.
Roxana Girju, Dan Roth and Mark Sammons, 2005.
Token-level Disambiguation of VerbNet classes. The
Interdisciplinary Workshop on Verb Features and
Verb Classes.
Svetlana Hensman and John Dunnion, 2004. Automat-
ically building conceptual graphs using VerbNet and
WordNet. International Symposium on Information
and Communication Technologies (ISICT).
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford U. Press 2003.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
Proc. of the 17th National Conference on Artificial
Intelligence.
Karin Kipper-Schuler, 2005. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph. D. the-
sis, University of Pennsylvania.
Karin Kipper, Anna Korhonen, Neville Ryant and
Martha Palmer, 2006. Extending VerbNet with
Novel Verb Classes. Proc. of the 5th International
Conference on Language Resources and Evaluation.
Judith Klavans and Min-Yen Kan, 1998. Role of verbs
in document analysis. Proc. of the 36th Meeting of
the ACL and the 17th International Conference on
Computational Linguistics.
Anna Korhonen and Ted Briscoe, 2004. Extended
Lexical-Semantic Classification of English Verbs.
Proc. of the 42nd Meeting of the ACL, Workshop on
Computational Lexical Semantics.
Mirella Lapata and Chris Brew, 2004. Verb Class
Disambiguation using Informative Priors. Compu-
tational Linguistics, 30(1):45-73
Matthew Lease and Eugene Charniak, 2005. Towards
a Syntactic Account of Punctuation. Proc. of the 2nd
International Joint Conference on Natural Language
Processing.
Beth Levin, 1993. English Verb Classes And Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Beth Levin and Malka Rappaport Hovav, 2005. Argu-
ment Realization. Cambridge University Press.
Juanguo Li and Chris Brew, 2007. Disambiguating
Levin Verbs Using Untagged Data. Proc. of the
2007 International Conference on Recent Advances
in Natural Language Processing.
Edward Loper, Szu-ting Yi and Martha Palmer, 2007.
Combining Lexical Resources: Mapping Between
PropBank and VerbNet. Proc. of the 7th Inter-
national Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Paola Merlo and Suzanne Stevenson. 2001. Automatic
Verb-Classification Based On Statistical Distribu-
tion Of Argument Structure. Computational Linguis-
tics, 27(3):373?408.
Rada Mihalcea and Andras Csomai 2005. Sense-
Learner: word sense disambiguation for all words
in unrestricted text. Proc. of the 43rd Meeting of the
ACL , Poster Session.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1).
Dan Roth, 1998. Learning to resolve natural language
ambiguities: A unified approach. Proc. of the 15th
National Conference on Artificial Intelligence
Sabine Schulte im Walde, 2000. Clustering verbs se-
mantically according to their alternation behavior.
Proc. of the 18th COLING.
Lei Shi and Rada Mihalcea, 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and WordNet
for robust semantic parsing. Proc. of the Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. Proc. of the 2005 conference on empirical
methods in natural language processing.
Szu-ting Yi, Edward Loper and Martha Palmer, 2007.
Can Semantic Roles Generalize Across Genres?
Proc. of the 2007 conference of the north american
chapter of the association for computational linguis-
tics.
16
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 721?728
Manchester, August 2008
Unsupervised Induction of Labeled Parse Trees
by Clustering with Syntactic Features
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present an algorithm for unsupervised
induction of labeled parse trees. The al-
gorithm has three stages: bracketing, ini-
tial labeling, and label clustering. Brack-
eting is done from raw text using an un-
supervised incremental parser. Initial la-
beling is done using a merging model that
aims at minimizing the grammar descrip-
tion length. Finally, labels are clustered
to a desired number of labels using syn-
tactic features extracted from the initially
labeled trees. The algorithm obtains 59%
labeled f-score on the WSJ10 corpus, as
compared to 35% in previous work, and
substantial error reduction over a random
baseline. We report results for English,
German and Chinese corpora, using two
label mapping methods and two label set
sizes.
1 Introduction
Unsupervised learning of grammar from text
(?grammar induction?) is of great theoretical and
practical importance. It can shed light on language
acquisition by humans and on the general structure
of language, and it can potentially assist NLP ap-
plications that utilize parser output. The problem
has attracted researchers for decades, and interest
has greatly increased recently, in part due to the
availability of huge corpora, computation power,
and new learning algorithms (see Section 2).
A fundamental issue in this research direction is
the representation of the resulting induced gram-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mar. Most recent work (e.g., (Klein and Manning,
2004; Dennis, 2005; Bod, 2006a; Smith and Eis-
ner, 2006; Seginer, 2007)) annotates text sentences
using a hierarchical bracketing (constituents) or a
dependency structure, and thus represents the in-
duced grammar through its behavior in a parsing
task. Solan et al (2005) uses a graph representa-
tion, while (Nakamura, 2006) simply uses a gram-
mar formalism such as PCFG. When the bracket-
ing approach is taken, some algorithms label the
resulting constituents, while most do not.
Each of these approaches can be justified or crit-
icized; a detailed discussion of this issue is be-
yond the scope of this paper. The algorithm pre-
sented here belongs to the first group, annotating
given sentences with labeled bracketing structures.
The main theoretical justification for this approach
is that many linguistic and psycho-linguistic theo-
ries posit some kind of a hierarchical labeled con-
stituent (or constructional) structure, arguing that it
has a measurable psychological (cognitive) reality
(e.g., (Goldberg, 2006)). The main practical argu-
ments in favor of this approach are that it enables
a detailed and large-scale evaluation using anno-
tated corpora, as is done in this paper, and that the
output format is suitable for many applications.
When an algorithm generates labeled structures,
the number of labels is an important issue. From a
theoretical point of view, the algorithm should also
discover the appropriate number of labels. How-
ever, for evaluation and application purposes it is
useful to base the number of labels on a specific
target grammar. In previous work, the number was
set to be equal to that in the target grammar. This
is a reasonable approach that we experiment with
in this paper. In addition, to reduce the possible
arbitrariness in this approach, we also experiment
with the number of prominent labels in the target
721
grammar, determined according to their coverage
of corpus constituents.
Another issue relates to the nature of the in-
put. In most cases (e.g., in the Klein, Smith, Den-
nis and Bod papers above), the input consists of
part-of-speech (POS) sequences, derived from text
corpora by manual or automatic POS tagging. In
some cases (e.g., in the Seginer and Solan papers
above) it can consist of plain text. Again, each
approach has its pros and cons. The algorithm
we present here requires POS tags for its labeling
stages. Parts-of-speech are widely considered to
have a psychological reality (at least in English,
including when they are viewed as low-level con-
structions as in (Croft, 2001)), so this kind of input
is reasonable for theoretical research. Moreover, as
POS induction is of medium quality (Clark, 2003),
using a manually POS tagged corpus enables us to
measure the performance of other induction stages
in a controlled manner. Since supervised POS tag-
ging is of very high quality and very efficient com-
putationally (Brants, 2000), this requirement does
not seriously limit the practical applicability of a
grammar induction algorithm.
Our labeled bracketings induction algorithm
consists of three stages. We first induce unla-
beled bracketing trees using the algorithm given in
(Seginer, 2007)1. We then induce initial labels us-
ing a Bayesian Model Merging (BMM) labeling al-
gorithm (Borensztajn and Zuidema, 2007), which
aims at minimizing the description length of the
input data and the induced grammar. Finally, the
initial labels are clustered to a desired number of
labels using syntactic features extracted from the
initially labeled trees. Previous work on labeled
brackets induction (Section 2) did not differentiate
the unlabeled structure induction phase from the
labeling phase, applying a single phase approach.
To evaluate labeled bracketings, we need a map-
ping between the label symbols of the induced and
target grammars. Previous work used a ?greedy?,
many to one, mapping. We used both the greedy
mapping and a label-to-label (LL) mapping, since
greedy mapping is highly forgiving to structural
problems in the induced labeling. We report results
for two cases: one in which the number of labels in
the induced and target grammars is the same, and
one in which the former is the number of promi-
nent labels in the target grammar. We discuss how
this number can be defined and determined. We
1The algorithm uses raw (not POS tagged) sentences.
experimented with English (WSJ10, Brown10),
German (NEGRA10) and Chinese (CTB10) cor-
pora.
When comparing to previous work that used
manually annotated corpora in its evaluation
(Haghighi and Klein, 2006)2, we obtained 59.5%
labeled f-score on the WSJ10 setup vs. their 35.3%
(Section 5). We also show substantial improve-
ment over a random baseline, and that the cluster-
ing stage of our algorithm improves the results of
the second merging stage.
Section 2 discusses previous work. In Section 3
we detail our algorithm. The experimental setup
and results are presented in Sections 4 and 5.
2 Previous Work
Unsupervised parsing has attracted researchers for
decades (see (Clark, 2001; Klein, 2005) for recent
reviews). Many types of input, syntax formalisms,
search procedures, and success criteria were used.
Among the theoretical and practical motivations to
this problem are the study of human language ac-
quisition (in particular, an empirical study of the
poverty of stimulus hypothesis), preprocessing for
constructing large treebanks (Van Zaanen, 2001),
and improving language models (Chen, 1995).
In recent years efforts have been made to eval-
uate the algorithms on manually annotated cor-
pora such as the WSJ PennTreebank. Recently,
works along this line have for the first time out-
performed the right branching heuristic baseline
for English. These include the constituent?context
model (CCM) (Klein and Manning, 2002), its
extension using a dependency model (Klein and
Manning, 2004), (U)DOP based models (Bod,
2006a; Bod, 2006b; Bod, 2007), an exemplar?
based approach (Dennis, 2005), guiding EM using
contrastive estimation (Smith and Eisner, 2006),
and the incremental parser of (Seginer, 2007). All
of these use as input POS tag sequences, except
of Seginer?s algorithm, which uses plain text. All
of these papers induce unlabeled bracketing or de-
pendencies.
There are other algorithmic approaches to the
problem (e.g., (Adriaans, 1992; Daelemans, 1995;
Van Zaanen, 2001)). None of these had evaluated
labeled bracketing on annotated corpora.
In this paper we focus on the induction of
labeled bracketing. Bayesian Model Merging
2Using, as they did, a greedy mapping with an equal num-
ber of labels in the induced and target grammars.
722
(BMM) (Stolcke, 1994; Stolcke and Omohundro,
1994) is a framework for inducing PCFG contain-
ing both a bracketing and a labeling. The charac-
teristics of this framework (separating prior prob-
ability, data likelihood and heuristic search proce-
dures) can also be found in the grammar induction
models of (Wolf, 1982; Langley and Stromsten,
2000; Petasis et al, 2004; Solan et al, 2005). The
BMM model used here (Borensztajn and Zuidema,
2007) combines features of (Petasis et al, 2004)
and Stolcke?s algorithm, applying the minimum
description length (MDL) principle. We use it here
only for initial labeling of existing bracketings.
The MDL principle was also used in (Grunwald,
1994; de Marcken, 1995; Clark, 2001).
There are only two previous papers we are
aware of that induce labeled bracketing and eval-
uate on corpora annotated with a similar repre-
sentation (Haghighi and Klein, 2006; Borensztajn
and Zuidema, 2007). We utilize and extend the
latter?s labeling algorithm. However, the evalu-
ation done by the latter dealt only with labeling,
using gold-standard (manually annotated) bracket-
ings. Thus, we can directly compare our results
only to (Haghighi and Klein, 2006), where two
models (PCFG ? NONE and PCFG ? CCM) are fully un-
supervised. These models use the inside-outside
and EM algorithms to induce bracketing and label-
ing simultaneously, as opposed to our three step
method3.
3 Algorithm
Our model consists of three stages: bracketing, ini-
tial labeling, and label clustering.
3.1 Induction of Unlabeled Bracketing
In this step, we apply the algorithm of (Seginer,
2007) to induce bracketing from plain text4. We
have chosen that algorithm because it is very fast
(both learning and parsing) and its code is publicly
available. We could have chosen any of the algo-
rithms mentioned above producing a similar output
format.
3.2 Initial Constituent Labeling
Our label clustering stage uses syntactic fea-
tures. To obtain these, we need an initial label-
ing on the bracketings computed in the previous
3Their other models, which were the core of their paper,
are semi-supervised.
4http://www.seggu.net/ccl
stage. To do that we modify the Bayesian Model
Merging (BMM) algorithm of (Borensztajn and
Zuidema, 2007), which induces context-free gram-
mars (bracketing and labeling) from POS tags,
combining features of the models of (Stolcke and
Omohundro, 1994) and (Petasis et al, 2004).
The BMM algorithm (Borensztajn and
Zuidema, 2007) uses an iterative heuristic
greedy search for an optimal PCFG according to
the Bayesian criterion of maximum posterior prob-
ability. Two operators define possible transitions
between grammars: MERGE creates generaliza-
tions by replacing two existing non-terminals
X
1
and X
2
that occur in the same contexts by a
single new non-terminal Y ; CHUNK concatenates
repeating patterns by taking a sequence of two
non-terminals X
1
and X
2
and creating a new
non-terminal Y that expands to X
1
X
2
.
We have used the algorithm to deal only with
labeling. It reads the initial rules of the grammar
from all productions implicit in the bracketed cor-
pus induced in the previous step. Every constituent
(except of the start symbol) is given a unique label.
Since only labeling is required, only MERGE oper-
ations are performed.
The objective function the algorithm tries to op-
timize at each step is the posterior probability cal-
culated according to Bayes? Law:
M
MAP
= argmax
M
P (M|X) = argmax
M
P (X|M) ? P (M)
(1)
where P (X|M) is the likelihood of the data X
given the grammar M and P (M) is the prior prob-
ability of the grammar. This is equivalent to mini-
mizing the function
?log(P (X|M)) ? logP (m) := DDL+ GDL := DL. (2)
Using a Minimal Description Length (MDL)
principle, BMM interprets this function as total de-
scription length (DL): The Grammar Description
Length GDL = ?logP (M) is the space needed
to encode the model, and the Data Description
Length DDL = ?logP (X|M) is the space re-
quired to describe the data given the model. The
rationale for MDL is to prefer smaller grammars
that describe the data well. DDL and GDL are
computed as in (Stolcke, 1994; Stolcke and Omo-
hundro, 1994). In order to reduce the number of
grammars considered at each step, which naively
is quadratic in the number of non-terminals, a
method based on (Petasis et al, 2004) for effi-
ciently predicting DL gain is applied. The process
723
is iterated until no additional merge operation im-
proves the objective function. Full details are given
in (Borensztajn and Zuidema, 2007).
3.3 Label Clustering
Label set size. BMM produces quite a large num-
ber of labels (4944 for WSJ105). In the third step
of our algorithm we reduce that number. We first
discuss the issue of the number of labels in induced
grammars, which is an important issue.
In many situations, it is reasonable to use a num-
ber T identical to the number of labels in a given
target grammar, for example when that grammar
is used for applications or evaluation. This is the
approach in (Haghighi and Klein, 2006) for their
unsupervised models6, and we use it in part of our
evaluation. However, it is also reasonable to argue
that the granularity of syntactic categories (labels)
in the gold standard annotation of the corpora we
experiment with is somewhat arbitrary. For exam-
ple, in the WSJ Penn Treebank noun phrases are
annotated with the symbol NP, but there is no dis-
tinction between subject and object NPs. Incorpo-
rating such a distinction into the WSJ10 grammar
would result in a 27 labels grammar instead of 26.
To examine this issue, consider Figure 1, which
shows the amount of constituent coverage obtained
by a certain number of labels in the four corpora
we use (see Section 4). In all of them, about 95%
of the constituents are covered by 23% ? 37% of
the labels, and the curve rises very sharply until
that 95% value. Motivated by this observation,
given a corpus annotated using a certain hierarchi-
cal labeled grammar, we refer to the set of P labels
that cover at least 95% of the constituents in the
corpus as the grammar?s prominent labels.
The prominent labels are not only the most
frequent in the corpus; each of them substan-
tially contributes to constituent labeling, while the
saliency of other labels is much smaller. It is
thus reasonable to assume that by addressing only
prominent labels, we address a level of granularity
that is uniform and basic (to the annotation scheme
used). As a result, by asking the induced grammar
to produce P labels, we reduce arbitrariness and
enable our testing to focus on our success in iden-
tifying the basic phenomena in the target grammar.
5For completeness, in Section 5 we provide results for this
grammar using greedy mapping evaluation. LL mapping eval-
uation cannot be performed when the numbers of induced and
target labels differ.
6Personal communication with the authors.
0 5 10 15 20 25 30
20
30
40
50
60
70
80
90
100
K most frequent labels
%
 o
f c
on
st
itu
en
ts
 
 
NEGRA10
BROWN10
WSJ10
CTB10
Figure 1: For each k, the fraction of constituents
labeled with the k most frequent labels, for WSJ10
(solid), Brown10 (triangles), NEGRA10 (dashed)
and CTB10 (dotted). In all corpora, more than
95% of the constituents are labeled using less than
10 prominent labels.
As a result, we generated two grammars for each
corpus we experimented with, one having T labels
and the other having P labels.
Clustering. we stop BMM when no improvement
to its objective function is possible, and cluster the
labels to conform to the size constraint. 7
Denote the number of labels in the induced
grammar with M , the set of D most frequent in-
duced labels with A, and the set consisting of the
other induced labels with B (|B| = M ? D). If
M 6> D, there is nothing to do since the con-
straint holds. Otherwise, we map each label in
B to the label in A that exhibits the most simi-
lar syntactic behavior, as follows. We construct
a feature vector representation of each of the la-
bels, using 3M + |K| features, where K is the set
of POS tags in the corpus. The first M features
correspond to parent-child relationships between
each of the induced labels and the represented la-
bel. The i-th feature (i ? [1,M ]) is the number of
times the i-th label is the parent of the represented
label. Similarly, the next M features correspond
to child-parent relationships, the next M features
correspond to sibling relationships and the last |K|
features correspond to the number of times each
POS tag is the leftmost POS tag in a constituent
labeled by the represented label. Note that in order
to compute the values of the first 3M features, we
needed an initial labeling on the induced bracket-
ings; this is the main reason for using the BMM
stage.
For each label b
i
? B, we compute the cosine
7It is possible to force BMM to iterate until a desired num-
ber of induced labels (T or P ) is achieved. However, the in-
duced grammars are of very low quality (see Section 5).
724
metric between its vector bv
i
and that of every a
j
?
A, mapping b
i
to the label a
j
with which it obtains
the highest score:
Map(b
i
) = argmax
j
b
v
i
? a
v
j
|b
v
i
||a
v
j
|
(3)
The cosine metric grows when the same coordi-
nates (features) in both vectors have higher values.
As a result, vectors with high values of the same
features (corresponding to similar syntactic behav-
ior) get high scores.
4 Experimental Setup
We evaluated our algorithm on English, German
and Chinese corpora: the WSJ Penn Treebank,
containing economic English newspaper articles,
the Brown corpus, containing various English gen-
res, the Negra corpus (Brants, 1997) of German
newspaper text, and version 5.0 of the Chinese
Penn Treebank (Xue et al, 2002). In each cor-
pus, we used the sentences of length at most 108,
numbering 7422 (WSJ10), 9117 (Brown10), 7542
(NEGRA10) and 4626 (CTB10).
For each corpus the following T and P values
were used: WSJ10: 26, 8; Brown10: 28, 7; NE-
GRA10: 22, 6; CTB10: 24, 9. Each number pro-
duces a different grammar.
For labeled f-score evaluation, the induced la-
bels should be mapped to the target labels9. We
evaluated with two different mapping schemes.
For each pair (X
i
, Y
j
) of induced and target labels,
let C
X
i
,Y
j
be the number of times they label a con-
stituent having the same span in the same sentence.
Following (Haghighi and Klein, 2006) we applied
a greedy (many to one) mapping where the map-
ping is given by Map(X
i
) = argmax
Y
j
C
X
i
,Y
j
.
This greedy mapping tends to map many induced
labels to the same target label, and is therefore
highly forgiving of large mismatches between the
structures of the induced and target grammars.
Hence, we also applied a label-to-label (LL) map-
ping, computed by reducing this problem to op-
timal assignment in a weighted complete bipar-
tite graph, formally defined as follows. Given a
weighted complete bipartite graph G = (X ?
Y ;X ? Y ) where edge (X
i
, Y
j
) has weight w
ij
,
8Excluding punctuation and null elements, according to
the scheme of (Klein, 2005).
9There are many possible methods for evaluating cluster-
ing quality (Rosenberg and Hirschberg, 2007). For our task,
overall f-score is a very natural one. We will address other
methods in future papers.
find a (one-to-one) matching M from X to Y hav-
ing a maximal weight. In our case, X is the set of
model symbols, Y is the set of T or P most fre-
quent target symbols (depending on the desired la-
bel set size used), and w
ij
:= C
X
i
,Y
j
, computed as
in greedy mapping (the number of times x
i
and y
j
share a constituent). To make the graph complete,
we add zero weight edges between induced and
target labels that do not share any constituent. The
Kuhn-Munkres algorithm (Kuhn, 1955; Munkres,
1957) solves this problem, and we used it to per-
form the LL mapping (see also (Luo, 2005)).
We assessed the overall quality of our algorithm,
the quality of its labeling stage and the quality of
the syntactic clustering (SC) stage. For the over-
all quality of the induced grammar (both brack-
eting and labeling) we compare our results with
(Haghighi and Klein, 2006), using their setup10.
That setup was used for all numbers reported in
this paper. Note that a random baseline would
yield very poor results, so there is nothing to be
gained from comparing to it.
We assessed the quality of the labeling (MDL
and SC) stages alone, using only the correct brack-
etings produced by the first stage of the algorithm.
We compare to a random baseline on these correct
constituents that randomly selects (using a uniform
distribution) a label for each constituent among the
set of labels allowed to the algorithm.
To asses the quality of the third stage (SC)
we compare the f-score performance of our three
stages labeled trees induction algorithm (bracket-
ing, MDL, SC) to an algorithm consisting of the
first two stages only (bracketing and MDL) and
the accuracy of the two stages labeling algorithm
(MDL, SC) to an algorithm where the syntactic
clustering stage is replaced by a simpler method
(MDL, random clustering).
5 Results
We start with comparing our algorithm with
(Haghighi and Klein, 2006), the only previous
work that produces labeled bracketing and was
tested on large manually annotated corpora. Their
relevant models are PCFG ? NONE and PCFG ? CCM11.
10Brackets covering a single word are not counted, multi-
ple labels and the sentence level constituent are counted. Two
sentence level constituents are usually used: one for the root
symbol at the top (which was not counted), and one real sym-
bol (in WSJ10 it is usually, but not always, S), which was
counted. We had verified the setup with the authors.
11They focused on a different, semi-supervised, setting.
725
This Paper PCFG? CCM PCFG ? NONE
WSJ10 59.5 35.3 26.3
Table 1: F-scores of our algorithm and of the unsu-
pervised models in (Haghighi and Klein, 2006) on
WSJ10 (they did not test these models on the other
corpora we experimented with).
The number of labels in their induced grammar
equals the number of labels in the target grammar
(26 for WSJ10), and they had used a greedy map-
ping. Table 1 shows that our algorithm achieves
a superior f-score of 59.5% over their 35.3%.
Haghighi and Klein (2006) did not experiment
with the NEGRA10 and Brown10 corpora, and had
used version 3.0 of CTB10 while we have used the
substantially different version 5.0, so we can only
compare our results on WSJ10.
Table 2 shows the labeled recall, precision and f-
score of our algorithm on the various corpora and
mappings we use. On Brown10, NEGRA10 and
CTB10 (version 5.0) these are the first reported
results for this task. For reference, the table also
shows the unlabeled f-score results of Seginer?s
bracketing algorithm (our first stage)12.
We can see that greedy mapping is indeed more
forgiving than LL mapping, for both T labels and
P labels. WSJ results are generally higher than for
the other corpora, probably because WSJ bracket-
ing results are higher than for the other corpora.
Comparing the left and right columns in each
of the table sections reveals that for greedy map-
ping, mapping to a higher number of labels results
in higher scores than mapping to a lower number.
LL mapping behaves in exactly the opposite way.
The explanation for this is that when we force the
mapping to cover all of the target labels (as done
by LL mapping for T labels), we move probabil-
ity mass from the correct, heavy labels to smaller
ones, thereby magnifying errors.
Table 4 addresses the quality of the whole la-
beling stage (MDL and SC) and of the SC stage.
We report the quality of our labels (top line for
each corpus in the table) the random baseline la-
bels (third line) and the labels of an algorithm
where MDL is performed and the syntactic clus-
tering is replaced by a random clustering (RC) al-
gorithm that, given a label L that is not one of the
T or P most frequent labels, randomly selects one
of the most frequent labels and adds L to its clus-
12The numbers slightly differ from those in Seginer?s paper,
since we use the (Haghighi and Klein, 2006) setup.
Greedy LL
T P T P
WSJ10
MDL,SC 80 67 47 59
MDL,RC 67 61 37 42
Rand. Base. 30 30 5 14
Error Reduction 39%,71% 15%,53% 16%, 44% 29%, 52%
Brown10
MDL,SC 73 61 48 60
MDL,RC 68 59 46 51
Rand. Base. 27 27 4 14
Error Reduction 16%,63% 5%, 47% 4%, 46% 18%, 53%
NEGRA10
MDL,SC 79 72 65 72
MDL,RC 73 69 54 58
Rand. Base. 39 39 5 17
Error Reduction 22%,66% 10%,34% 24%,63% 33%,66%
CTB10
MDL,SC 70 67 44 55
MDL,RC 36 32 40 45
Rand. Base. 29 29 5 12
Error Reduction 53%,58% 51%, 54% 7%,41% 18%,49%
Table 4: Pure labeling results (taking into account
only the correct bracketings produced at stage 1),
compared to the random and (MDL,RC) baselines.
The left number in the Error Reduction lines slots
compares (MDL,SC) to (MDL,RC) and the right
number compares (MDL,SC) to random labeling.
(MDL,SC) algorithm is substantially superior.
ter (second line).13 All three labeling algorithms
used Seginer?s bracketing and results are reported
only for labels of correctly bracketed constituents.
Reported are the algorithm and baselines accuracy
(percentage of correctly labeled constituents after
the mapping has been performed) and the error re-
duction of the algorithm over the baselines (bottom
line). (MDL,SC) substantially outperforms both
the random baseline, demonstrating the power of
the whole labeling stage, and the (MDL,RC) algo-
rithm, demonstrating the power of the SC stage.
We compared our grammars to the grammars in-
duced by the first two stages (bracketing and then
MDL that stops when no DL improvement is pos-
sible) alone. Since the number of labels in these
grammars is much larger than in the target gram-
mar, only the evaluation with the greedy, many to
one, mapping is performed. Using greedy map-
ping, the F-score of these grammars constitutes an
upper bound on the F-score after the subsequent
SC stage. For WSJ10 (4944 labels), NEGRA10
(5557 labels), CTB10 (2298 labels) and Brown10
(3314 labels) F-score values are 64.6, 49.9, 38.7
and 52.5 compared to F-score values of 59.5(50.2),
45.6(42), 36.4(34.7) and 49.4(41.3) after mapping
all induced labels to the T (P ) most frequent la-
bels with SC (Table 2, ?greedy? section). The frac-
13Our algorithm?s numbers can be deduced from Table 2.
Results for all random baselines are averaged over 10 runs.
726
Greedy Mapping LL Mapping Seginer
T labels P labels T labels P labels (unlabeled)
Corpus R P F R P F R P F R P F F
WSJ10 58 61 59.5 48.9 51.5 50.2 34.2 36.1 35.2 42.7 44.9 43.8 74.6
NEGRA10 54.2 39.3 45.6 50 36.2 42 44.7 32.4 37.6 49.5 35.9 41.7 58.1
CTB10 35.1 37.8 36.4 33.4 36 34.7 21.9 23.6 22.7 27.4 29.5 28.4 51.8
Brown10 47.6 51.3 49.4 39.9 43 41.3 31.3 33.7 32.4 38.9 41.9 40.3 67.8
Table 2: Labeled recall, precision and f-score of our algorithm, mapping model labels into target labels
greedily (left) and using LL mapping (right). The number of induced labels was set to be the total
number T of target labels or the number P of prominent labels in the target grammar (WSJ10: 26, 8;
Brown10: 28, 7; NEGRA10: 22, 6; CTB10: 24, 9). Also shown are Seginer?s unlabeled bracketing
results (rightmost column), which constitute an upper bound on the quality of subsequent labeling steps.
WSJ10 Brown10
Label T labels P labels T labels P labels
R P F R P F R P F R P F
S 77.1 77.6 77.3 75.4 67.9 71.5 72.3 60.9 66.1 69.3 63.2 66.1
NP 8.5 79.5 15.4 19.8 61.6 30 10.7 79.3 18.9 15.6 78 26
VP 20.4 67.6 31.3 64.2 36.7 46.7 9.8 72.5 17.3 14.1 59 22.8
PP 40.8 63.5 49.7 8 8.9 8.4 17.4 59.2 26.9 75.5 14.4 24.2
Table 3: Recall, Precision and F-score for constituents labeled with the 4 most frequent labels in the
WSJ10 and Brown10 test sets. LL mapping is used for evaluation.
tion of constituents covered by the T (P ) most fre-
quent labels before mapping with SC is 0.42(0.29),
0.33(0.23), 0.58(0.45) and 0.66(0.42), emphasiz-
ing the effect of SC on the final result.
MDL finds the best merge at each iteration. In-
stead of stopping it when no DL gains are possi-
ble, we can keep merging after the deltas become
worse than the total DL, stopping only when the
desired number of labels (T or P ) is achieved. We
tried this version of a (bracketing and MDL) algo-
rithm and obtained grammars of very low quality.
This further demonstrates the importance of the SC
stage.
Table 3 shows results for the four most frequent
labels of WSJ10 and Brown10 .
6 Conclusion
Unsupervised grammar induction is a central re-
search problem, possessing both theoretical and
practical significance. There is great value in pro-
ducing an output format consistent with and evalu-
ated against formats used in large human annotated
corpora. Most previous work of that kind produces
unlabeled bracketing or dependencies. In this pa-
per we presented an algorithm that induces labeled
bracketing. The labeling stages of the algorithm
use the MDL principle to induce an initial, rela-
tively large, set of labels, which are then clustered
using syntactic features. We discussed the issue of
the desired number of labels, and introduced the
concept of prominent labels, which allows us cov-
erage of the basic and most salient level of a target
grammar. Labels are clearly an important aspect of
grammar induction. Future work will explore their
significance for applications.
Evaluating induced labels is a complex issue.
We applied greedy mapping as in previous work,
and showed that our algorithm significantly out-
performs it. In addition, we introduced LL map-
ping, which overcomes some of the shortcomings
of greedy mapping. There are several other possi-
ble methods for evaluating labeled induced gram-
mars, and we plan to explore them in future work.
We evaluated on large human annotated corpora
of different English domains and three languages,
and showed that our labeling stages, and specif-
ically the SC stage, outperform several baselines
for all corpora and mapping methods.
Acknowledgments. We thank Gideon Borensz-
tajn and Yoav Seginer for their help.
References
Pieter Adriaans, 1992. Learning Language from a
Categorical Perspective. Ph.D. thesis, University of
Amsterdam.
Rens Bod, 2006a. An All-Subtrees Approach to Un-
supervised Parsing. Proc. of the 44th Meeting of the
ACL.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
Proc. of CoNLL X.
727
Rens Bod, 2007. Is the End of Supervised Parsing in
Sight? Proc. of the 45th Meeting of the ACL.
Gideon Borensztajn and Willem Zuidema, 2007.
Bayesian Model Merging for Unsupervised Con-
stituent Labeling and Grammar Induction. Technical
Report, ILLC . http: //staff.science.uva.nl/?gideon/
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Thorsten Brants, 2000. TnT: A Statistical Part-Of-
Speech Tagger. Proc. of the 6th Applied Natural
Language Processing Conference.
Stanley F. Chen, 1995. Bayesian grammar induction
for language modeling. Proc. of the 33th Meeting of
the ACL.
Alexander Clark, 2001. Unsupervised Language Ac-
quisition: Theory and Practice. Ph.D. thesis, Uni-
versity of Sussex.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. Proc. of the 10th Meeting of the European
Chapter of the ACL.
Willliam A. Croft, 2001. Radical Construction Gram-
mar. Cambridge University Press.
Carl G. de Marcken, 1995. Unsupervised Language
Acquisition. Ph.D. thesis, MIT.
Walter Daelemans, 1995. Memory-based lexical ac-
quisition and processing. Lecture Notes In Artificial
Intelligence, 898:85?98.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Adele E. Goldberg, 2006. Constructions at Work. Ox-
ford University Press.
Peter Grunwald, 1994. A minimum description length
approach to grammar inference. Lecture Notes In
Artificial Intelligence, 1004 : 203-216.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
grammar induction. Proc. of the 44th Meeting of the
ACL.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford University Press,
2003.
Dan Klein and Christopher Manning, 2002. A gener-
ative constituent-context model for improved gram-
mar induction. Proc. of the 40th Meeting of the ACL.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. Proc. of the 42nd Meet-
ing of the ACL.
Dan Klein, 2005. The unsupervised learning of natu-
ral language structure. Ph.D. thesis, Stanford Uni-
versity.
Harold W. Kuhn, 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83-97.
Pat Langley and Sean Stromsten, 2000. Learning
context-free grammars with a simplicity bias. Proc.
of the 11th European Conference on Machine Learn-
ing.
Xiaoqiang Luo, 2005. On coreference resolution per-
formance metrics. Proc. of the 2005 Conference on
Empirical Methods in Natural Language Processing.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32?38.
Katsuhiko Nakamura, 2006. Incremental learning of
context free grammars by bridging rule generation
and search for semi-optimum rule sets. Proc. of the
8th ICGI.
Georgios Petasis, Georgios Paliouras and Vangelis
Karkaletsis, 2004. E-grids: Computationally effi-
cient grammatical inference from positive examples.
Grammars, 7:69?110.
Andrew Rosenberg and Julia Hirschberg, 2007.
Entropy-based external cluster evaluation measure.
Proc. of the 2007 Conference on Empirical Methods
in Natural Language Processing.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. Proc. of the 45th Meeting of the ACL.
Noah A. Smith and Jason Eisner, 2006. Annealing
Structural Bias in Multilingual Weighted Grammar
Induction . Proc. of the 44th Meeting of the ACL.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman, 2005. Unsupervised learning of natural
languages. Proceedings of the National Academy of
Sciences, 102 : 11629?11634.
Andreas Stolcke. 1994. Bayesian Learning of Proba-
bilistic Language Models. Ph.D. thesis, University
of of California at Berkeley.
Andreas Stolcke and Stephen M. Omohundro, 1994.
Inducing probabilistic grammars by Bayesian model
merging . Proc. of the 2nd ICGI.
Menno van Zaanen, 2001. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds.
J. Gerard Wolff, 1982. Language acquisition, data
compression and generalization. Language and
Communication, 2(1): 57?89.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer,
2002. Building a large?scale annotated Chinese cor-
pus. Proc. of the 40th Meeting of the ACL.
728
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267?275,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Geo-mining: Discovery of Road and Transport Networks
Using Directional Patterns
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
One of the most desired information types
when planning a trip to some place is
the knowledge of transport, roads and
geographical connectedness of prominent
sites in this place. While some transport
companies or repositories make some of
this information accessible, it is not easy
to find, and the majority of information
about uncommon places can only be found
in web free text such as blogs and fo-
rums. In this paper we present an algo-
rithmic framework which allows an auto-
mated acquisition of map-like information
from the web, based on surface patterns
like ?from X to Y?. Given a set of loca-
tions as initial seeds, we retrieve from the
web an extended set of locations and pro-
duce a map-like network which connects
these locations using transport type edges.
We evaluate our framework in several set-
tings, producing meaningful and precise
connection sets.
1 Introduction
Textual geographical information such as location
descriptions, directions, travel guides and trans-
port tables is extensively used by people. Dis-
covering such information automatically can as-
sist NLP applications such as question answering
(Santos and Cardoso, 2008), and can be useful
for a variety of other applications, including au-
tomatic map annotation.
Some textual geographical information can be
found in web sites of transport companies, tourist
sites and repositories such as Wikipedia. Such
sites usually utilize structured information such
as machine-readable meta-data, tables, schedule
forms or lists, which are relatively convenient
for processing. However, automatic utilization of
such information is limited. Even on these sites,
only a small fraction of the available geographi-
cal information is stored in a well-structured and
freely accessible form. With the growth of the
web, information can be frequently found in ?or-
dinary? web pages such as forums, travelogues or
news. In such sites, information is usually noisy,
unstructured and present in the form of free text.
This type of information can be addressed by
lexical patterns. Patterns were shown to be very
useful in all sorts of lexical acquisition tasks, giv-
ing high precision results at relatively low com-
putational costs (Pantel et al, 2004). Pattern-
driven search engine queries allow to access such
information and gather the required data very effi-
ciently (Davidov et al, 2007).
In this paper we present a framework that given
a few seed locations as a specification of a region,
discovers additional locations (including alternate
location names) and map-like travel paths through
this region labeled by transport type labels.
The type of output produced by our framework
here differs from that in previous pattern-based
studies. Unlike mainstream pattern-based web
mining, it does not target some specific two-slot
relationship and attempts to extract word tuples for
this relationship. Instead, it discovers geographi-
cal networks of transport or access connections.
Such networks are not unstructured sets of word
pairs, but a structured graph with labeled edges.
Our algorithm utilizes variations of the basic
pre-defined pattern ?[Transport] from Location1
to Location2? which allows location names and
connections to be captured starting from the given
seed location set. We acquire search engine snip-
pets and extract contexts where location names co-
appear. Next we construct a location graph and
merge transport edges to identify main transport
group types. Finally, we improve the obtained data
by reducing transitive connections and identifying
key locations.
267
The obtained location data can be used as a
draft for preparation of travel resources and on-
demand travel plans. It can also be used for ques-
tion answering systems and for automated enrich-
ment and verification of existing geographical re-
sources.
We evaluate our framework on three different
regions of different scale and type: Annapurna in
Nepal, the south Israel area and the Cardiff area
in England. In our evaluation we estimated pre-
cision and the amount of discovered locations and
transport edges, and examined the quality of the
obtained map as a whole by visually comparing
the overall connectedness of the graph to an actual
road or transport map.
2 Related Work
In this paper we utilize a pattern-based lexical
acquisition framework for the discovery of geo-
graphical information. Due to the importance of
lexical databases for many NLP tasks, substantial
research has been done on direct or indirect auto-
mated acquisition of concepts (sets of terms shar-
ing a significant aspect of their meaning) and con-
cept relationships in the form of graphs connect-
ing concepts or terms inside concepts into usually
hierarchical or bipartite networks. In the case of
geo-mining, concepts can include sets of alterna-
tive names for some place, or sets of all locations
of the same type (e.g., all countries). Geographical
relationships can include nearness of two locations
and entity-location relationships such as institute-
address, capital-country, tourist site-city etc.
The major differences between relationship ac-
quisition frameworks come from the types and an-
notation requirements of the supplied input and
the basic algorithmic approach used to process
this input. A first major algorithmic approach
is to represent word contexts as vectors in some
space and use distributional measures and auto-
matic clustering in that space. Curran (2002)
and Lin (1998) use syntactic features in the vec-
tor definition. Caraballo (1999) uses conjunction
and appositive annotations in the vector represen-
tation.While efforts have been made for improv-
ing the computational complexity of these meth-
ods (Gorman and Curran, 2006), they remain data
and computation intensive.
The second major algorithmic approach is to
use lexico-syntactic patterns, which have been
shown to produce more accurate results than fea-
ture vectors at a lower computational cost on large
corpora (Pantel et al, 2004). Most related work
deals with discovery of hypernymy (Hearst, 1992;
Pantel and Lin, 2002) and synonymy (Widdows
and Dorow, 2002; Davidov and Rappoport, 2006).
Some studies deal with the discovery of more spe-
cific relation sub-types, including inter-verb re-
lations (Chklovski and Pantel, 2004) and seman-
tic relations between nominals (Davidov and Rap-
poport, 2008). Extensive frameworks were pro-
posed for iterative discovery of pre-specified (e.g.,
(Riloff and Jones, 1999)) and unspecified (e.g.,
(Agichtein and Gravano, 2000)) relation types.
Some concepts and relationships examined by
seed-based discovery methods were of a geo-
graphical nature. For example, (Etzioni et al,
2004) discovered a set of countries and (Davidov
et al, 2007) discovered diverse country relation-
ships, including location relationships between a
country and its capital and a country and its rivers.
As noted in Section 1, the type of output that we
produce here is not an unstructured collection of
word pairs but a labeled network. As such, our
task here is much more complex.
Our study is related to geographical informa-
tion retrieval (GIR) systems. However, our prob-
lem is very far from classic GIR problem settings.
In GIR, the goal is to classify or retrieve possi-
bly multilingual documents in response to queries
in the form ?theme, spatial relationship, location?,
e.g., ?mountains near New York? (Purves et al,
2006). Our goal, in contrast, is not document re-
trieval, but the generation of a structured informa-
tion resource, a labeled location graph.
Spatial relationships used in natural language
tend to be qualitative and descriptive rather than
quantitative. The concept of Naive Geography,
which reflects the way people think and write
about geographical space, is described in (Egen-
hofer and Shariff, 1995). Later in (Egenhofer
and Shariff, 1998) they proposed a way to convert
coordinate-based relationships between spatial en-
tities to natural language using terms as ?crosses?,
?goes through? or ?runs into?. Such terms can be
potentially used in patterns to extract geographi-
cal information from text. In this paper we start
from a different pattern, ?from ... to?, which helps
in discovering transport or connectedness relation-
ships between places, e.g., ?bus from X to Y? and
?road from X to Y?.
The majority of geographical data mining
268
frameworks utilize structured data such as avail-
able gazetteers and Wikipedia metadata. Sev-
eral other studies utilize semi-structured data like
Wikipedia links (Overell and Ruger, 2007) or sub-
structures in web pages, including addresses and
phone numbers (Borges et al, 2007).
The recent Schockaert et al( 2008) framework
for extraction of topological relationships from the
web has some similarities to our study. In both
cases the algorithm produces map-like structures
using the web. However, there are significant dif-
ferences. They utilize relatively structured address
data on web pages and rely on the order of named
entities in address data for extracting containment
relationships. They also use co-appearances in
addresses (e.g., ?R1 / R2? and ?R1 & R2? as in
?Newport & Gabalfa, Cardiff?) to deduce location
boundaries. This allows them to get high precision
data for modern and heavily populated regions like
Cardiff, where the majority of offices have avail-
able well-formatted web pages.
However, in less populated regions (a major tar-
get for tourist information requests), this strategy
could be problematic since a major information
source about these places would be not local web
sites (in which local addresses are likely to be
found) but foreign visitor sites, web forums and
news. We rely on free text available in all types
of web pages, which allows us to capture unstruc-
tured information which contains a significant por-
tion of the web-available geographical knowledge.
Our goals are also different from Schockaert et
al.( 2008), since we focus on obtaining informa-
tion based on paths and transport between loca-
tions, while in their work the goal is to find a net-
work representing nearness of places rather than
their connectivity by means of transport or walk-
ing. Nevertheless, in one of our evaluation settings
we targeted the area of Cardiff as in (Schockaert
et al, 2008). This allowed us to make an indi-
rect comparison of a relevant part of our results
to previous work, achieving state-of-the-art per-
formance.
3 The Algorithm
As input to our algorithm we are given a seed of
a few location names specifiying some geograph-
ical region. In this section we describe the algo-
rithm which, given these names, extracts the la-
beled structure of connections between entities in
the desired region. We first use a predefined pat-
tern for recursive extraction of the first set of enti-
ties. Then we discover additional patterns from
co-appearing location pairs and use them to get
more terms. Next, we label and merge the ob-
tained location pairs. Finally, we construct and
refine the obtained graph.
3.1 Pattern-based discovery with web queries
In order to obtain the first set of location connec-
tions, we use derivatives of the basic pattern ?from
X to Y?. Using Yahoo! BOSS, we have utilized
the API?s ability to search for phrases with wild-
cards. Given a location name L we start the search
with patterns ?from * to L?, ?from * * to L?. These
are Yahoo! BOSS queries where enclosing words
in ?? means searching for an exact phrase and ?*?
means a wildcard for exactly one arbitrary word.
This pattern serves a few goals beyond the dis-
covery of connectedness. Thus putting ?*?s inside
the pattern rather than using ?from L to? allowed
us to avoid arbitrary truncation of multiword ex-
pressions as in ?from Moscow to St. Petersburg?
and reduced the probability of capturing unrelated
sentence parts like ?from Moscow to cover deficit?.
Location names are usually ambiguous and this
type of web queries can lead to a significant
amount of noise or location mix. There are two
types of ambiguity. First, as in ?from Billericay to
Stock....?, stock can be a location or a verb. We
filter most such cases by allowing only capital-
ized location names. Besides, such an ambiguity
is rarely captured by ?from * to L? patterns. The
second type is location ambiguity. Thus ?Moscow?
refers to at least 5 location names including farms
in Africa and Australia and a locality in Ireland. In
order to reduce mixing of locations we use the fol-
lowing simple disambiguation technique. Before
performing ?from...to? queries, we downloaded up
to 100 web pages pointed by each possible pair
from the given seed locations, generating from
a location pair L
1
, L
2
a conjunctive query ?L
1
* L
2
?. Then we extracted the most informative
terms using a simple probabilistic metric:
Rank(w) =
P (w|QueryCorpus)
P (w|GeneralCorpus)
,
comparing word distribution in the downloaded
QueryCorpus to a large general purpose offline
GeneralCorpus
1
. We thus obtained a set of
1
We used the DMOZ corpus (Gabrilovich and
Markovitch, 2005).
269
query-specific disambiguating words. Then we
added to all queries the same most frequent word
(DW) out of the ten words with highest ranks.
Thus for the seed set {Moscow, St. Petersburg},
an example of a query is <?from * to Moscow?
Russia>.
3.2 Iterative location retrieval
We retrieved all search engine snippets for each
of these initial queries
2
. If we succeeded to get
more than 50 snippets, we did not download the
complete documents. In case where only a hand-
ful of snippets were obtained, the algorithm down-
loaded up to 25 documents pointed by these snip-
pets in an attempt to get more pattern instances.
In the majority of tested cases, snippets provide
enough information for our task, and this informa-
tion was not significantly extended by download-
ing the whole documents.
Once we retrieve snippets we identify terms
appearing in the snippets in wildcard slots. For
example, if the query is <?from * to Moscow?
Russia> and we encounter a snippet ?...from
Vladivostok to Moscow...?, we add ?Vladivostok?
to our set of seeds. We then continue the search in
a breadth first search setting, stopping the search
on three conditions: (1) runaway detected ? the
total frequency of newly obtained terms through
some term?s patterns is greater than the total fre-
quency of previously discovered terms+seeds. In
this case we stop exploration through the prob-
lematic term and continue exploration through
other terms
3
; (2) we reached a predefined maxi-
mal depth D
4
; (3) no new terms discovered.
At the end of this stage we get the extended
set of terms using the set of snippets where these
terms co-appear in patterns.
3.3 Enhancement of initial pattern set
In order to get more data, we enhance the pattern
set both by discovery of new useful secondary pat-
terns and by narrowing existing patterns. After ob-
taining the new pattern set we repeat the extraction
stage described in Section 3.2.
2
Yahoo! Boss allows downloading up to a 1000 descrip-
tions, up to 50 in each request. Thus for each seed word, we
have performed a few dozen search requests.
3
Note that the ?problematic? term may be the central term
in the region we focus upon ? if this happen it means that the
seeds do not specify the region well.
4
Depth is a function of the richness of transport links in
the domain. For connected domains (Cardiff, Israel) we used
4, for less connected ones (Nepal) we used 10.
Adding secondary patterns. As in a number of
previous studies, we improve our results discover-
ing additional patterns from the obtained term set.
The algorithm selects a subset of up to 50 discov-
ered (t
1
, t
2
) term pairs appearing in ?from t
1
to t
2
?
patterns and performs the set of additional queries
of the form <?t
1
* t
2
? DW>.
We then extract from the obtained snippets the
patterns of the form ?Prefix t
1
Infix t
2
Postfix?,
where Prefix and Postfix should contain either a
punctuation symbol or 1-3 words. Prefix/Postfix
should also be bounded from left/right by a punc-
tuation or one of the 50 most frequent words in the
language (based on word counts in the offline gen-
eral corpus). Infix should contain 1-3 words with
the possible addition of punctuation symbols
5
.
We examine patterns and select useful ones ac-
cording to the following ranking scheme, based
on how well each pattern captures named entities.
For each discovered pattern we scan the obtained
snippets and offline general corpus for instances
where this pattern connects one of the original
or discovered location terms to some other term.
Let T be the set of all one to three word terms
in the language, T
d
? T the set of discovered
terms, T
c
? T the set of all capitalized terms and
Pat(t
1
, t
2
) indicates one or more co-appearances
of t
1
and t
2
in pattern Pat in the retrieved snippets
or offline general corpus. The rank R of pattern
Pat is defined by:
R(Pat) =
|{Pat|Pat(t
1
, t
2
), t
1
? T
c
, t
2
? T
d
}|
|{Pat|Pat(t
1
, t
2
), t
1
? T, t
2
? T
d
}|
In other words, we rank patterns according to the
percentage of capitalized words connected by this
pattern. We sort patterns by rank and select the
top 20% patterns. Once we have discovered a new
pattern set, we repeat the term extraction in Sec-
tion 3.2. We do this only once and not reiterate
this loop in order to avoid potential runaway prob-
lems. Obtained secondary patterns include dif-
ferent from/to templates ?to X from Y by bus?;
time/distance combinations ?X -N km bus- Y?, ?X
(bus, N min) Y? or patterns in different languages
with English location/transport names.
Narrowing existing patterns. When available
data volume is high, we would like to take advan-
tage of more data by utilizing more specific pattern
5
Search engines do not support punctuation in queries,
hence these symbols were omitted in web requests and con-
sidered only when processing the retrieved snippets.
270
sets. Since Yahoo! allows to obtain only the first
1K snippets, in case we get more than 10K hits,
we extend our queries by adding the most com-
mon term sequences appearing before or after the
pattern. Thus if for the query ?from * to Moscow?
we got more than 10K hits and among the snippets
we see ?... bus from X to Moscow...? we create an
extended pattern ?bus from * to Moscow? and use
the term extraction in Section 3.2 to get additional
terms. Unlike the extraction of secondary patterns,
this narrowing process can be repeated recursively
as long as a query brings more than 10K results.
3.4 Extraction of labeled connections
At the end of the discovery stage we get an ex-
tended set of patterns and a list of search engine
snippets discovered using these patterns. Each
snippet which captures terms t
1
, t
2
in either pri-
mary ?from t
1
to t
2
? or secondary patterns repre-
sents a potential connection between entities.
Using an observed property of the primary pat-
tern, we select as a label a term or set of terms ap-
pearing directly before ?from? and delimited with
some high frequency word or punctuation. For
example, labels for snippets based on ?from...to?
patterns and containing ?the road from...?, ?got a
bus from?, ?a TransSiberian train from...? would
be road, bus and TransSiberian train.
Once we acquire labels for the primary patterns,
we also attempt to find labels in snippets obtained
for secondary patterns discovered as described in
Section 3.3. We first locate some already labeled
pairs in secondary patterns? snippets where we can
see both label and the labeled term pair. Then,
based on the label?s position in this snippet, we
define a label slot position for this type of snip-
pet. Suppose that during the labeling of primary
pattern snippets we assigned the label ?bus? to the
pair (Novgorod, Moscow) and during the pattern
extension stage the algorithm discovered a pattern
P
new
= ?ride to t
2
from t
1
,? with a corresponding
snippet ?... getting bus ride to Moscow from Nov-
gorod...?. Then using the labeled pair our algo-
rithm defines the label slot in such a snippet type:
?getting [label] ride to t
2
from t
1
?. Once a label
slot is defined, all other pairs captured by P
new
can be successfully labeled.
3.5 Merging connection labels
Some labels may denote the same type of con-
nection. Also, large sets of connections can
share the same set of transport types. In this
case it is desired to assign a single label for
a shared set of transports. We do this by a
simple merging technique. Let C
1
, C
2
be sets
of pairs assigned to labels L
1
, L
2
. We merge
two labels if one of the following conditions holds:
(1)|C
1
? C
2
| > 0.75 ?min(|C
1
|, |C
2
|)
(2)|C
1
? C
2
| > 0.45 ?max(|C
1
|, |C
2
|)
Thus, either one group is nearly included in the
other or each group shares nearly half with the
other group. We apply this rule only once and do
not iterate recursively. At this stage we also dis-
miss weakly populated labels, keeping the 10 most
populated labels.
3.6 Processing of connection graph
Now once we have merged and assigned the la-
bels we create a pattern graph for each label and
attempt to clean the graph of noise and unneces-
sary edges. Our graph definition follows (Wid-
dows and Dorow, 2002). In our pattern graph for
label L, nodes represent terms and directed edges
represent co-appearance of two terms in some pat-
tern in snippet labeled by L. We do not add unla-
beled snippets to the graph. Now we use a set of
techniques to reduce noise and unnecessary edges.
3.6.1 Transitivity elimination
One of the main problems with the pattern-based
graph is transitivity of connections. Thus if loca-
tion A is connected to B and B to C, we frequently
acquire a ?shortcut? edge connecting A to C. Such
an edge diminishes our ability to create a clear and
meaningful spatial graph. In order to reduce such
edges we employ the following two strategies.
First, neighboring places frequently form fully
connected subgraphs. We would like to sim-
plify such cliques to reduce the amount of tran-
sitive connections. If three overlapping sets of
nodes {A
1
. . . A
n?2
},{A
2
. . . A
n?1
},{A
3
. . . A
n
}
form three different cliques, then we remove all
edges between A
1
and the nodes in the third clique
and between A
n
and the nodes in the first clique.
Second, in paths obtained by directional pat-
terns, it is common that if there is a path A
1
?
A
2
? ? ? ? A
n
where A
1
and A
n
are some
major ?key? locations
6
, then each of the nodes
A
2
. . . A
n?1
tend to be connected both to A
1
and
6
Such locations will be shown in double circles in the
evaluation.
271
to A
n
while intermediate nodes are usually con-
nected only to their close neighbors. We would
like to eliminate such transitive edges leaving only
the inter-neighbor connections.
We define as key nodes in a graph, nodes whose
degree is more than 1.5 times the average graph
degree. Then we eliminate the transitive con-
nections: if A
1
is a key node and A
1
is con-
nected to each of the nodes A
2
. . . A
n?1
, and
?i ? {2 . . . n ? 1}, A
i
is connected to A
i+1
,
then we remove the connection of A
1
to all nodes
A
3
. . . A
n?1
, leaving A
1
only connected to A
2
.
3.6.2 Clearing noise and merging names
Finally we remove potential noise which acciden-
tally connects remote graph parts. If some edge
discovered through a single pattern instance con-
nects distant (distance>3) parts of the graph we
remove it.
Additionally, we would like to merge common
name alternations and misspellings of places. We
merge two nodes A and B into one node if ei-
ther (1) A, B have exactly the same edges, and
their edge count is greater than 2; or (2) edges
of A are subset of B?s edges and the string edit
distance between A and B is less than a third of
min(StringLength(A), StringLength(B)).
4 Evaluation
Since our problem definition differs significantly
from available related work, it is not possible to
make direct comparisons. We selected three dif-
ferent cases (in Nepal, Israel, Wales) where the
obtained information can be reliably verified, and
applied our framework on these settings. As a de-
velopment set, we used the Russian rail network.
We have estimated the quality of our framework
using several measures and observations. First, we
calculated the precision and quantity of obtained
locations using map information. Then we manu-
ally estimated precision of the proposed edges and
their labels, comparing them with factual infor-
mation obtained from maps
7
, transport companies
and tourist sites. Finally we visually compared a
natural drawing of the obtained graph with a real
map. In addition, while our goals differ, the third
evaluation setting has deliberate significant simi-
larities to (Schockaert et al, 2008), which allows
us to make some comparisons.
7
We recognize that in case of some labels, e.g. ?walk?, the
precision measure is subjective. Nevertheless it provides a
good indication for the quality of our results.
4.1 The Annapurna trek area
One of the most famous sites in Nepal is the Anna-
purna trekking circuit. This is a 14-21 day walk-
ing path which passes many villages. Most of the
tourists going through this path spend weeks in
prior information mining and preparations. How-
ever, even when using the most recent maps and
guides, they discover that available geographical
knowledge is far from being complete and precise.
This trek is a good example of a case when formal
information is lacking while free-text shared expe-
rience in the web is abundant. Our goal was to test
whether the algorithm can discover such knowl-
edge automatically starting from few seed location
names (we used Pokhara, which is one of the cen-
tral cities in the area, and Khudi, a small village).
The quality of results for this task was very good.
While even crude recall estimation is very hard for
this type of task, we have discovered 100% of the
Annapurna trek settlements with population over
1K, all of the flight and bus connections, and about
80% of the walking connections.
On Figure 1 we can compare the real map and
the obtained map
8
. This discovered map includes
a partial map
9
for 4 labels ? flights, trek, bus and
jeep. You can see on the map different lines for
each label. The algorithm discovered 132 enti-
ties, all of them Annapurna-related locations. This
includes correctly recognized typos and alterna-
tive spellings, and the average was 1.2 names per
place. For example for Besisahar and Pokhara
the following spellings were recognized based
both on string distance and spatial collocation:
Besishahar, Bensisahar, BesiSahar, Besi Sahar,
Beshishahar, Beisahar, Phokra, Pohkala, Poka-
hara, Pokhara, Pokhar, Pokra, Pokhura, Pokhra.
We estimated correctness of edges comparing
to existing detailed maps. 95% of the edges were
correctly placed and labeled. Results were good
since this site is well covered and also not very
interconnected ? most of it is connected in a sin-
gle possible way. After the elimination process
described in the previous section, only 6% of the
nodes participate in 3-cliques. Thus, due to the
linearity of the original path, our method success-
8
Graph nodes were manually positioned such that edges
do not intersect. Recall that our goal is to build a network
graph, which is an abstract structure. The 2-D embedding of
the graph shown here is only for illustrative purposes and is
not part of our algorithm.
9
A few dozens of correctly discovered places were omit-
ted to make the picture readable.
272
Figure 1: Real path map of Annapurna circuit
(above) compared to automatically acquired graph
(below). The graph nodes were manually posi-
tioned such that edges do not cross each other.
Dozens of correctly discovered places were omit-
ted for readability. Double circles indicate key
nodes as explained in section 3.6.1
fully avoided the problem of mixing transitively
connected nodes into one large clique.
4.2 The Israeli south
The southern part of Israel (mostly the Negev
desert) is a sparsely populated region containing
a few main roads and a few dozen towns. There
is a limited number of tourists sites in the Negev
and hence little web information is supposed to be
available. Our goal was to see if the algorithm can
successfully detect at least major entities and to
discover their connectedness.
We discovered 56 names of different places, of
them 50 correctly belong to the region, where the
region is defined as south from the Ashquelon-
Jerusalem-Yericho line, the other 6 were Is-
raeli cities/locations outside the region (Tiberias,
Metulla, Ben Gurion, Tel Aviv, Ashdod, Haifa).
In addition we discovered 23 alternative names for
some of the 56 places. We also constructed the
corresponding connectedness graphs.
We tested the usefulness of this data attempting
to find the discovered terms in the NGA GEOnet
Names Server
10
which is considered one of the
most exhaustive geographical resources. We could
find in the database only 60% of the correctly dis-
covered English terms denoting towns, so 40% of
the terms were discovered by us and ignored by
this huge coverage database. We also tested the
quality of edges, and found that 80% of the dis-
covered edges were correctly placed and labeled.
Figure 2 shows a partial graph of the places ob-
tained for the ?road? label.
Figure 2: Partial graph for Israel south settings.
4.3 The Cardiff area
Cardiff is the capital, largest city and most pop-
ulous county in Wales. Our goal was to see
if we can discover basic means of transport and
corresponding locations connected to and inside
Cardiff. This exploration also allowed us to com-
pare some of our results to related studies. We ex-
ecuted our algorithm using as seeds Grangetown,
Cardiff and Barry. Table 1 shows the most utilized
merged labels obtained for most edge-populated
graphs together with graph size and estimated pre-
cision. In case of flights, treks and trains, precision
was estimated using exact data. In other cases we
estimated precision based on reading relevant web
pages. We can see that the majority of connectiv-
ity sets are meaningful and the precision obtained
for most of these sets is high. Figure 3 shows a
partial graph for ?walking?-type labels and Figure
10
http://earth-info.nga.mil/gns/html/
273
Nodes Edges(Prec) Label
88 120(81) walking,walk,cycling,short ride
taxis, Short bus ride,short walk
131 140(95) flights, airlines,# flights a day
12 16(100) foot path, trek, walking # miles
36 51(89) train, railway, rail travel,rail
32 98(65) bus, road, drive,direct bus
Table 1: The merged labels obtained for 5
most edge-populated graphs, including number of
nodes and edges for each label. The estimated pre-
cision according to each label definition is shown
in parentheses.
4 shows such a graph for train labels
11
. Compar-
ing the obtained map with real map data we notice
a definite correlation between actual and induced
relative connection of discovered places.
(Schockaert et al, 2008) used their frame-
work to discover neighborhoods of Cardiff. In
our case, the most appropriate relation which con-
nects neighborhood locations is walking/cycling.
Hence, comparing the results to previous work, we
have examined the results obtained for the ?walk-
ing? label in details. (Schockaert et al, 2008) re-
port discovery of 68 locations, of them 7 are al-
ternate entries, 4 can be considered vernacular or
colloquial, 10 are not considered to be neighbor-
hoods, and 5 are either close to, but not within,
Cardiff, or are areas within Cardiff that are not
recognized neighborhoods. In our set we have dis-
covered 88 neighborhood names, of them 18 are
alternate entries of correct neighborhoods, 4 can
be considered vernacular or colloquial, 3 are not
considered to be neighborhoods, and 15 are areas
outside the Cardiff area.
Considering alternate entries as hits, we got su-
perior precision of 66/88 = 0.75 in comparison to
49/68 = 0.72. It should be noted however that we
found many more alternative names possibly due
to our larger coverage. Also both our framework
and the goal were substantially different.
5 Discussion
In this paper we presented a framework which,
given a small set of seed terms describing a ge-
ographical region, discovers an underlying con-
nectivity and transport graph together with the ex-
traction of common and alternative location names
in this region. Our framework is based on the
11
Spatial position of displayed graph components is arbi-
trary, we only made sure that there are no intersecting edges.
Figure 3: Partial graph of the obtained Cardiff re-
gion for the walk/walking/cycling label.
Figure 4: Partial graph of the obtained Cardiff re-
gion for the railway/train label.
observation that ?from...to?-like patterns can en-
code connectedness in very precise manner. In our
framework, we have combined iterative pattern-
and web-based relationship acquisition with the
discovery of new patterns and refinement of the lo-
cation graph. In our evaluation we showed that our
framework is capable of extracting high quality
non-trivial information from free text given very
restricted input and not relying on any heavy pre-
processing techniques such as parsing or NER.
The success of the proposed framework opens
many challenging directions for its enhancement.
Thus we would like to incorporate in our net-
work patterns which allow traveling times and dis-
tances to be extracted, such as ?N miles from X
to Y?. While in this paper we focus on specific
type of geographical relationships, similar frame-
works can be useful for a wider class of spatial re-
lationships. Automated acquisition of spatial data
can significantly help many NLP tasks, e.g., ques-
tion answering. We would also like to incorpo-
rate some patterns based on (Egenhofer and Shar-
iff, 1998), such as ?crosses?, ?goes through? or
?runs into?, which may allow automated acquisi-
tion of complex spatial relationships. Finally, we
would like to incorporate in our framework mod-
274
ules which may allow recognition of structured
data, like those developed by (Schockaert et al,
2008).
References
Eugene Agichtein, Luis Gravano, 2000. Snowball:
Extracting Relations from Large Plain-text Collec-
tions. ACM DL ?00.
Karla Borges, Alberto Laender, Claudia Medeiros,
Clodoveu Davis, 2007. Discovering Geographic
Locations in Web Pages Using Urban Addresses.
Fourth Workshop on Geographic Information Re-
trieval.
Sharon Caraballo, 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. ACL
?99.
Timothy Chklovski, Patrick Pantel 2004. VerbOcean:
Mining the Web for Fine-grained Semantic Verb Re-
lations. EMNLP ?04.
James R. Curran, Marc Moens, 2002. Improvements
in Automatic Thesaurus Extraction. SIGLEX ?02.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel,
2007. Fully Unsupervised Discovery of Concept-
specific Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008. Classification
of Semantic Relationships Between Nominals Using
Pattern Clusters. ACL ?08.
Max Egenhofer, Rashid Shariff, 1995. Naive Geogra-
phy. Proceedings of COSIT ?95.
Max Egenhofer, Rashid Shariff, 1998. Metric Details
for Natural-Language Spatial Relations. Journal of
the ACM TOIS, 4:295?321, 1998.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, Alexander Yates, 2004.
Web-scale Information Extraction in KnowItAll.
WWW ?04.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture Generation for Text Categorization Using World
Knowledge. IJCAI ?05.
James Gorman, James R. Curran, 2006. Scaling Dis-
tributional Similarity to Large Corpora. COLING-
ACL ?06.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Simon Overell, Stefan Ruger, 2007. Geographic Co-
occurrence as a Tool for GIR. Fourth ACM Work-
shop on Geographical information retrieval.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Ross Purves, Paul Clough, Christopher Jones, Avi
Arampatzis, Benedicte Bucher, Gaihua Fu, Hideo
Joho, Awase Syed, Subodh Vaid, Bisheng Yang,
2007. The Design and Implementation of SPIRIT: a
Spatially Aware Search Engine for Information Re-
trieval on the Internet. International Journal of Ge-
ographical Information Science, 21(7):717-745.
Ellen Riloff, Rosie Jones, 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. AAAI ?99.
Diana Santos, Nuno Cardoso, 2008. GikiP: Evalu-
ating Geographical Answers from Wikipedia. Fifth
Workshop on Geographic Information Retrieval.
Steven Schockaert, Philip Smart, Alia Abdelmoty,
Christopher Jone, 2008. Mining Topological Re-
lations from the Web. International Workshop on
Flexible Database and Information System Technol-
ogy, workshop at DEXA, Turin, pp. 652?656.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
275
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 468?477,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Multi-Word Expression Identification Using Sentence Surface Features
Ram Boukobza
School of Computer Science
Hebrew University of Jerusalem
ram.boukobza@mail.huji.ac.il
Ari Rappoport
School of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Much NLP research on Multi-Word Ex-
pressions (MWEs) focuses on the discov-
ery of new expressions, as opposed to the
identification in texts of known expres-
sions. However, MWE identification is
not trivial because many expressions al-
low variation in form and differ in the
range of variations they allow. We show
that simple rule-based baselines do not
perform identification satisfactorily, and
present a supervised learning method for
identification that uses sentence surface
features based on expressions? canonical
form. To evaluate the method, we have
annotated 3350 sentences from the British
National Corpus, containing potential uses
of 24 verbal MWEs. The method achieves
an F-score of 94.86%, compared with
80.70% for the leading rule-based base-
line. Our method is easily applicable to
any expression type. Experiments in pre-
vious research have been limited to the
compositional/non-compositional distinc-
tion, while we also test on sentences in
which the words comprising the MWE ap-
pear but not as an expression.
1 Introduction
Multi-Word Expressions (MWEs) such as ?pull
strings?, ?make a face? and ?get on one?s nerves?
are very common in language. Such MWEs can
be characterized as being non-compositional: the
meaning of the expression does not transparently
follow from the meaning of the words that com-
prise it. Much of the work on MWEs in NLP has
been in MWE extraction ? the discovery of new
MWEs from a corpus, using statistical and other
methods. Identification of known MWEs in text
has received less attention, but is necessary for
many NLP applications, for example in machine
translation. The current work deals with the MWE
identification task: deciding if a sentence contains
a use of a known expression.
MWE identification is not as simple as may ini-
tially appear, as will be shown by the performance
of two rule-based baselines in our experiments.
One source of difficulty is variations in expres-
sions? usage in text. Although MWEs generally
show less variation than single words, they show
enough that it cannot be ignored. In a study on
V+NP idioms, Riehemann (2001) found that the
idioms? canonical form accounted for 75% of their
appearances in a corpus. Additionally, expressions
differ considerably in the types of variations they
allow, which include passivization, nominalization
and addition of modifying words (Moon, 1998).
A second source of difficulty is that expressions
consisting of very frequent words will often co-
occur in sentences in a non-MWE usage and in
similar but distinct expressions.
MWE identification can be modeled as a two
step process. Given a sentence and a known ex-
pression, step (1) is to decide if the sentence con-
tains a potential use of the expression. This is a
relatively simple step based on the appearance in
the sentence of the words comprising the MWE.
Step (2) is to decide if the potential use is indeed
non-compositional. Consider the following sen-
tences with regard to the expression hit the road,
meaning ?to leave on a journey?:
(a) ?At the time, the road was long and difficult
with few travelers daring to take it.?
(b) ?The headlights of the taxi-van behind us
468
flashed as it hit bumps in the road.?
(c) ?The bullets were hitting the road and I could
see them coming towards me a lot faster than
I was able to reverse.?
(d) ?Lorry trailers which would have been hitting
the road tomorrow now stand idle.?
Sentence (a) does not contain a potential use of
the expression due to the missing component ?hit?.
Each of (b)-(d) does contain a potential use of the
expression. In (b) all of the expression compo-
nents are present, but they do not form an expres-
sion. In (c), the words form an expression, but
with a compositional (literal) meaning. Only (d)
contains a non-compositional use of hit the road.
The task we address in this paper is to identify
whether or not we are in case (d), for a given ex-
pression in a given sentence.
To date, most work in MWE identification has
focused on manually encoding rules that identify
expressions in text. The encodings, usually con-
sisting of regular expressions and syntactic struc-
tures, are intended to contain all the necessary in-
formation for processing the MWE in text. Being
manual, this is time-consuming work and requires
expert knowledge of individual expressions. In
terms of the above model, such encodings handle
both MWE identification steps.
A second approach is to use machine learning
methods to learn an expression?s behavior from a
corpus. Studies taking this approach have focused
on distinguishing between compositional and non-
compositional uses of an expression (cases (c) and
(d) above). As will be detailed in Section 2, exist-
ing methods are tailored to an expression?s type,
and experiment with a single MWE pattern. In ad-
dition, the training and test sets they used did not
contain non-expression uses as in case (b), which
can be quite common in practice.
Our approach is more general. Given a set of
sentences with potential MWE uses, we use sen-
tence surface features to create a Support Vec-
tor Machine (SVM) classifier for each expres-
sion. The classifier is binary and differentiates be-
tween non-compositional uses of the expression
((d) above) on the one hand, and compositional
and non-expression uses ((b) and (c)) on the other.
The experiments and results presented below fo-
cus on verbal MWEs, since verbal MWEs are
quite common in language use and have also been
investigated in related MWE research (e.g., (Cook
et al, 2007)). However, the developed features are
not specific to a particular type of expression.
The supervised method is compared with two
simple rule-based baselines in order to test
whether a simple approach is sufficient. In addi-
tion, the use of surface features is compared with
the use of syntactic features (based on dependency
parse trees of the sentences). Averaged over ex-
pressions in an independent test set, the super-
vised classifiers outperform the rule-based base-
lines, with F-scores of 94.86% (surface features)
and 87.77% (syntactic features), compared with
80.70% for the best baseline.
Section 2 reviews previous work. Section 3 dis-
cusses the features used for the supervised classi-
fier. Section 4 explains the experimental setting.
The results and a discussion are given in sections
5 and 6.
2 Previous Work
2.1 MWE Lexical Encoding
The approach to handling MWEs in early systems
was to employ a list of expressions, each with
a quasi regular expression that encodes morpho-
syntactic variations. One example is Leech et
al. (1994) who used this method for automatic
part-of-speech tagging for the BNC. Another is a
formalism called IDAREX (IDioms And Regular
EXpressions) (Breidt et al, 1996).
More recent research emphasizes the integra-
tion of MWE lexical entries into existing single
word lexicons and grammar systems (Villavicen-
cio et al, 2004; Alegria et al, 2004). There is
also an attempt to take advantage of regularities in
morpho-syntactic properties across MWE groups,
which allows encoding the behavior of the group
instead of individual expressions (Villavicencio et
al., 2004; Gr?egoire, 2007). Fellbaum (1998) dis-
cusses some difficulties in representing idioms,
which are largely figurative in meaning, in Word-
Net. More recent work (Fellbaum et al, 2006) fo-
cuses on German VP idioms.
As already mentioned, one issue with lexi-
cal encoding is that it is done manually, mak-
ing lexicons difficult to create, maintain and ex-
tend. The use of regularities among different types
of MWEs is one way of reducing the amount
of work required. A second issue is that im-
plementations tend to ignore the likelihood and
even the possibility of compositional and other
interpretations of expressions in text, which can
469
be common for some expressions. For exam-
ple, in an MWE identification study, Hashimoto
et al (2006) built an identification system us-
ing hand crafted rules for some 100 Japanese id-
ioms. The results showed near perfect perfor-
mance on expressions without compositional/non-
compositional ambiguity but significantly poorer
performance on expressions with ambiguity.
2.2 MWE Identification by ML
Katz and Giesbrecht (2006) used a supervised
learning method to distinguish between composi-
tional and non-compositional uses of an expres-
sion (in German text) by using contextual infor-
mation in the form of Latent Semantic Analy-
sis (LSA) vectors. LSA vectors of compositional
and non-compositional meaning were built from a
training set of example sentences and then a near-
est neighbor algorithm was applied on the LSA
vector of one tested MWE. The technique was
tested more thoroughly in Cook et al (2007).
Cook et al (2007) devised two unsupervised
methods to distinguish between compositional (lit-
eral) and non-compositional (idiomatic) tokens of
verb-object expressions. The first method is based
on an expression?s canonical form. In a previ-
ous study (Fazly and Stevenson, 2006), the authors
came up with a dozen possible syntactic forms for
verb-object pairs (based on passivization, deter-
miner, and object pluralization) and used a corpus-
based statistical measure to determine the canoni-
cal form(s). The method classifies new tokens as
idiomatic if they use a canonical form, and literal
otherwise.
The second method uses context as well as
form. Co-occurrence vectors representing the id-
iomatic and literal meaning of each expression
were computed based on corpus data. Idiomatic-
meaning vectors were based on examples match-
ing the expressions? canonical form. Literal mean-
ing vectors were based on examples that did not
match the canonical form. New tokens were
classified as literal/idiomatic based on their (co-
occurrence) vector?s cosine similarity to the id-
iomatic and literal vectors.
(Sporleder and Li, 2009) also attempted to dis-
tinguish compositional from non-compositional
uses of expressions in text. Their assumption was
that if an expression is used literally, but not id-
iomatically, its component words will be related
semantically to several words in the surrounding
discourse. For example, when the expression ?play
with fire? is used literally, words such as ?smoke,
?burn?, ?fire department?, and ?alarm? tend to also
be used nearby; when it is used idiomatically, they
aren?t (indeed, other words, e.g., ?danger? or ?risk?
appear nearby but they are not close semantically
to ?play? or to ?fire?). This property was used
to distinguish literal and non-literal instances by
measuring the semantic relatedness of an expres-
sion?s component words to nearby words in the
text. If one or more of the expression?s compo-
nents were sufficiently related to enough nearby
words, forming a ?lexical chain?, the usage was
classified as literal. Otherwise it was idiomatic.
Two classifiers based on lexical chains were de-
vised. These were compared with a supervised
method that trains a classifier for each expression
based on surrounding context. The results showed
that the supervised classifier method did much bet-
ter (90% F-score on literal uses) than the lexical
chain classifier methods (60% F-score).
In the above studies the focus is on the
compositional/non-compositional expression dis-
tinction. The sentence data used contains exam-
ples of either one or the other. In (Sporleder and
Li, 2009) the experimental data included only sen-
tences in which the expressions were in canoni-
cal form (allowing for verb inflection). In (Cook
et al, 2007) a syntactic parser was used to col-
lect sentences containing the MWEs in the active
and passive voice using heuristics. Thus, exam-
ples such as the following (from the BNC) would
not be included in their sample:
1. take a chance: ?While he still had a chance
of being near Maisie, he would take it?.
2. face the consequences: ?. . . she did not have
to face, it appears, the possible serious or
even fatal consequences of her decision?.
3. make a distinction: ?Logically, the distinc-
tion between the two aspects of the theory
can and should be made?.
4. break the ice: ?The ice, if not broken, was
beginning to soften a little?.
5. settle a score: ?Morrissey had another score
to settle?.
This means that their experiments have not in-
cluded all types of sentences that might be encoun-
tered in practice when attempting MWE identifi-
470
cation. Specifically, they would miss many ex-
amples in which the MWE words are present but
are not used as an expression (case (b) in Sec-
tion 1). Moreover, their heuristics are tailored
to the Verb-Direct Object MWE type. Different
heuristics would need to be employed for different
MWE types.
In our approach there is no pre-processing stage
requiring type-specific knowledge. Specifically,
the above examples are used as training sentences
in our experiments.
2.3 MWE Extraction
There exists an extensive body of research on
MWE extraction (see Wermter and Hahn (2004)
for a review), where the only input is a corpus,
and the output is a list of MWEs found in it. Most
methods collect MWE candidates from the corpus,
score them according to some association measure
between their components, and accept candidates
with scores passing some threshold. The focus of
research has been on developing association mea-
sures, including statistical, information-theoretic
and linguistically motivated measures (e.g., Juste-
son and Katz (1995), Wermter and Hahn (2006),
and Deane (2005)).
3 MWE Identification Method
Our method decides if a potential use of a
known expression in a given sentence is non-
compositional. The input to the method, for each
MWE, is a labeled training set of sentences con-
taining one or more potentially non-compositional
uses of the MWE. The output, for each MWE, is a
binary classifier, trained on those sentences. Thus,
we target step (2) of MWE identification, which is
the difficult one.
The learning algorithm used is Support Vector
Machine (SVM), which outputs a binary classifier,
using Sequential Minimal Optimization (Platt,
1998)
1
in the Weka toolkit
2
(Witten and Frank,
2000).
For training, sentences are converted into fea-
ture vectors. Features depend on the assignment
of the lexical components of the expression to spe-
cific tokens in the sentence. In some cases, there
are several tokens in the sentence that match a sin-
gle component in the expression, and this leads to
1
Using the PUK kernel (The Pearson VII function-
based Universal Kernel), with parameters omega=1.0 and
sigma=1.0.
2
Weka version 3.5.6; www.cs.waikato.ac.nz/ ml/ weka/
multiple (potential) assignments. So in the gen-
eral case a sentence is converted to a set of feature
vectors, each corresponding to a single assignment
of the MWE?s lexical components to sentence to-
kens.
Training sentences are labeled positive if they
contain a non-compositional use of the expression
and negative if they do not (i.e., literal and other
uses). If the sentence is positive, at least one of
the assignments is the true assignment (there may
be more than one, e.g., when an expression is used
twice in the same sentence). The vector matching
the true assignment is labeled positive. The others
are labeled negative. If the sentence is negative,
all of the vectors are labeled negative.
As mentioned, the output of the method is a
distinct binary classifier for each MWE. Although
having a single classifier for all expressions would
seem advantageous, the wide variation exhibited
by MWEs (e.g., for some the passive is common,
for other not at all) precludes this option and re-
quires having a separate classifier for each expres-
sion.
3.1 Features
Surface features include order and distance, part-
of-speech and inflection of an expression?s words
in a sentence.
Use of surface features is intuitive and relatively
cheap. In addition, many studies have shown the
importance of order and distance in MWE extrac-
tion in English (two recent examples are (Dias,
2003; Deane, 2005)). Thus, we develop a super-
vised classifier based on surface features.
Many of the surface features make use of an
expression?s Canonical Form (CF), thus the learn-
ing algorithm assumes that it is given such a form.
Formally defining the CF is difficult. Indeed, some
researchers have concluded that some expressions
do not have a CF (Moon, 1998). For our purposes,
CF can be informally defined as the most frequent
form in which the expression appears. In practice,
an approximation of this definition, explained in
Section 4, is used.
3.1.1 Surface Features
1. Word Distance: The number of words be-
tween the leftmost and rightmost MWE to-
kens in the sentence.
2. Ordered Gap List: A list of gaps, measured
in number of words, between each pair of the
471
expression?s tokens in their canonical form
order. For example, if the token locations (in
canonical form order) are 10, 7 and 3, the or-
dered gap list would be (10 ? 7 = 2, 10 ?
3 = 6, 7? 3 = 3).
3. Word Order: A boolean value indicating
whether the expression?s word order in the
sentence matches the canonical form word
order.
4. Word Order Permutation: The permutation
of word order relative to the canonical form.
For example, the permutation (1,0,2) indi-
cates that component words 1 and 0 have
switched order in the sentence.
5. Inflection Ratio: The fraction of words in the
expression that have undergone inflection rel-
ative to the canonical form.
6. Lexical Values: A list of the tokens in the
sentence matching the expression?s compo-
nent words, ordered according to canonical
form. For example, if the expression is ?make
a distinction?, a possible lexical values list
is (made,no,distinction) in the sentence ?No
possible distinction can be made between the
two?.
7. POS Pattern: A boolean value indicating
whether the expression?s use in the sentence
has the same part-of-speech pattern as the
canonical form.
Two combinations of surface features are used
in the experiments below. The first, named R1,
uses all of the above features. The second, R2,
uses only Word Distance, Ordered Gap List and
Word Order Permutation. Using R2 the learner
has only word order and distance information from
which to create a classifier.
3.1.2 Syntactic Features
An expression?s words may appear unrelated in
a sentence, because of distance, order, part-of-
speech and other surface variations. However, the
words will still be closely related syntactically.
Syntactic analysis of the sentence in the form of
a dependency parse tree directly gives the syntac-
tic relationships between the expression?s compo-
nents. Thus, we also develop a classifier based on
syntactic features.
Dependency Parsing. A dependency parse tree
is a directed acyclic graph in which the nodes rep-
resent tokens in the sentence and the edges rep-
resent syntactic dependencies between the words
(e.g., direct-object, prepositional-object, noun-
subject etc.). The Stanford Parser
3
(Marneffe et
al., 2006) was used.
Minimal Sub-Tree. To compute a syntactic fea-
ture, the dependency tree is computed and then the
minimal sub-tree containing the expression?s to-
kens is extracted.
The features are:
1. Sub-Tree Distance Sum: The number of
edges in the minimal sub-tree. A large num-
ber of edges suggests a weaker dependency.
2. Sub-Tree Distance List: A list of the dis-
tances of the MWE component nodes from
the root of their sub-tree.
3. Descendant Relations List: A list of descen-
dant relations between each pair of MWE
component nodes.
A descendant relation between two nodes ex-
ists if there is a directed path from one node
(the ancestor) to the other (the descendant).
Descendant relations are either direct (parent-
child) or indirect. The list consists of the lev-
els of descendant relations between the MWE
component nodes, which can be none, indi-
rect or direct.
4. Descendant Direction List: A list of the di-
rections of the descendant relations between
each pair of MWE component nodes.
If there are descendant relations between a
pair of nodes, the direction of the depen-
dency, indicating which is the modifying and
which the modified node, is important.
5. Sibling Relations List: A list of sibling rela-
tions between each pair of MWE component
nodes.
Two nodes are first degree siblings if they
share the same parent (which usually means
they modify the same word). Two nodes are
second degree siblings if they share a com-
mon ancestor no more than two edges away,
and so on. The list consists of the level of
sibling relations for each pair of component
3
http://www-nlp.stanford.edu/software/lex-parser.shtml
472
nodes, which can be first, second and third
degree.
6. Descendant Type List: A list of the depen-
dency types (e.g., subject, direct object etc.)
between each pair of component nodes. If the
component nodes are not direct descendants
their dependency type is null.
7. Sibling Type List: A list of pairs of depen-
dency types corresponding to the dependen-
cies between a pair of component nodes and
their common parent. If the component nodes
are not first degree siblings, the type is null.
In the experiments reported below, the classifier
using only the syntactic features is denoted by S,
and the one using all surface and all syntactic fea-
tures is denoted by C. We have experimented with
additional feature combinations, with no improve-
ment in results.
4 Experimental Method
Canonical form. As described, an expression?s
canonical form (CF) is used in many of the learn-
ing algorithm?s features. The CF is taken from
Collins COBUILD Advanced Learner?s English
Dictionary (2003) which is also used as our source
for MWEs. COBUILD is an English-English dic-
tionary based on the Bank of English (BOE) cor-
pus (over 520 million words) with approximately
34,000 entries.
Traditional single-word dictionaries are a good
source for expressions because they usually list, as
part of single-word entries, expressions in which
the word is a component. The CF is not explic-
itly given in COBUILD, so an approximation is
the form which appears in the expression?s defini-
tion. This is a reasonable approximation since the
COBUILD authors claim to have selected typical
uses of the expressions in their definitions.
Each CF also has a matching part-of-speech
(POS) pattern, which is a list of the parts-
of-speech of the components in the CF. For
example, ?walking on air? has the pattern
(V erb, Preposition,Noun). COBUILD does
not include part-of-speech information for expres-
sions so this information was determined using the
British National Corpus (BNC) (BNC, 2001), a
(mostly) automatically POS tagged corpus (using
the CLAWS tagger). For each MWE, the POS pat-
terns of all instances of the CF in the corpus were
counted. The most frequent pattern is the expres-
sion?s POS pattern.
The expressions. A set of 17 verbal MWEs, the
development set, was used for development of the
surface and syntactic features described above. All
of the development set MWEs had the POS pattern
(V erb,Determiner,Noun). Another set of 24
verbal MWEs, the training/test set
4
, was then used
to test the method. Because the method is not spe-
cific to the (V erb,Determiner,Noun) pattern,
new POS patterns are included in the training/test
set. The training/test set consists of 8 MWEs
of the POS pattern (V erb,Determiner,Noun),
7 (V erb, Preposition,Noun) MWEs and and 9
(V erb,Noun, Preposition) MWEs. The list of
MWEs was selected randomly from the corre-
sponding POS pattern types. MWEs with a pos-
itive or negative percentage of under 5% in their
data set were discarded
5
. The MWEs, in their
canonical form, are:
Development set:
(V erb,Determiner,Noun) [17]: break the ice,
calls the shots, catch a cold, clear the air, face
the consequences, fits the bill, hit the road, make
a face, make a distinction, makes an impression,
raise the alarm, set an example, sound the alarm,
stay the course, take a chance, take the initiative,
tie the knot.
Training/test set:
(V erb,Determiner,Noun) [8]: changes the
subject, get a grip, get the picture, lead the way,
makes the grade, sets the scene, take a seat, take
the plunge;
(V erb, Preposition,Noun) [7]: fall into place,
goes to extremes, brought to justice, take to heart,
gets on nerves, keep up appearances, comes to
light;
(V erb,Noun, Preposition) [9]: take aim at,
make allowances for, takes advantage of, keep
hands off, lay claim to, take care of, make contact
with, gives rise to, wash hands of.
The sentences. As mentioned, the first step of
MWE identification is to identify if the sentence
contains a potential non-compositional use of the
expression. In order to test our method, which tar-
gets step (2), a set of such sentences (for each ex-
pression) was collected from the BNC corpus and
4
Using 10-fold cross validation.
5
Initially there were 20 MWEs in the development set and
30 (10 per group) in the training/test set.
473
then labeled for use as training/test sentences
6
.
The collection method was intended to allow
a wide range of variations in expression use. In
practice, for each expression sentences contain-
ing all of the expression?s CF components, in any
of their inflections, were collected, but excluding
common auxiliary words. So for example, when
targeting the MWE ?make an impression? we al-
lowed inflections of ?make? and ?impression? and
did not require ?an?, to allow for variations such
as ?make no impression? and ?make some impres-
sion?. For some expressions, sentences were lim-
ited to those with a distance of up to 8 words be-
tween each expression component. Very long sen-
tences (above 80 words) were discarded. The final
set of sentences was then randomly selected.
Given this method, training/test sentences al-
low non-lexical variations: inflection, word or-
der, part-of-speech, syntactic structure and other
non-syntactic transformations. Lexical variations
which involve a change in one of the expression?s
components are not allowed, except for common
auxiliary words.
For the development set an average of 97 (40-
137) sentences were collected per MWE, giving a
total of 1663 sentences, with a micro average of
49% positive labels. For the training/test set there
were 139 (73-150) sentences per MWE on aver-
age, totaling 3350, with a 40% average positive
ratio.
The sentences were manually labeled as posi-
tive if they contained a non-compositional use of
the MWE and negative if they contained a compo-
sitional or non-expression usage. Judgment was
based on a single sentence, without wider context.
Baseline methods. Two baseline methods are
used to test the intuitive notion that simple rule-
based methods are sufficient for MWE identifica-
tion as well as for comparison with the supervised
learning methods.
The first method, CanonicalForm (CF), accepts
a sentence use as a non-compositional MWE use
if and only if the MWE is in canonical form (there
are no intervening words between the MWE com-
ponents, their order matches canonical-form order,
and there is an inflection in at most one component
word).
The second method, DistanceOrder (DO), ac-
6
The PyLucene software package, http://pylucene. os-
afoundation. org/, was used for building an index to the BNC
and for searching.
CF DO R1 R2 S C
Verb-Det-Noun: All (17)
A 73.53 82.27 89.48 90.83 88.58 87.02
P 97.09 89.29 82.71 87.18 83.89 78.54
R 58.81 76.83 92.29 90.35 92.97 97.19
F 67.39 79.68 86.92 88.56 87.78 86.00
Verb-Det-Noun: Best (8)
A 84.51 91.56 95.33 95.48 92.52 93.27
P 95.90 85.70 92.50 95.63 91.12 87.63
R 73.50 89.80 97.25 95.25 95.83 98.50
F 78.63 86.29 94.70 95.36 93.44 92.25
Table 1: Development set: Average performance over all
MWEs and best 8. Supervised classifiers outperform base-
lines. A: Accuracy; P: Positive Precision; R: Positive Recall;
F: F-Score.
cepts a sentence use if and only if the number of
words between the leftmost and rightmost MWE
components is less than or equal to 2 (not count-
ing the middle MWE component), and if the order
matches the canonical form order.
5 Results
The baseline methods (CF and DO) and the super-
vised methods (R1,R2,S,C) were run on the devel-
opment and training/test sets. For the supervised
methods, for each MWE we used 10-fold cross-
validation
7
.
Tables 1 and 2 summarize the results for the de-
velopment and test sets, respectively. For the de-
velopment set, average results over all 17 MWEs
and over the best 8 MWEs (on R1), a group size
comparable to the test set, are shown. For the test
set, results over all 24 MWEs and the three MWE
types tested are shown.
The tables show average overall accuracy and
average precision, recall and F-score on posi-
tive instances, where the averages are taken over
the results of the individual MWEs (i.e., micro-
averaged).
Baselines. Baseline accuracy, (for DO) 82.27%
on the development set and 87.2% on the test set
(over all groups), is probably insufficient for many
NLP applications.
The baselines perform similarly in terms of av-
erage accuracy. CF does this with very high preci-
sion and low recall, while for DO recall improves
at the expense of precision. Looking at individ-
ual MWEs reveals that for expressions which al-
low more variation in terms of intervening words
7
I.e., we ran 10 experiments where in each experiment we
divided the corresponding annotated sentence sets into 90%
training sentences and 10% test sentences, and the results re-
ported are the average of the 10 experiments.
474
CF DO R1 R2 S C
All (24)
A 86.16 87.15 93.50 91.61 89.73 91.50
P 94.16 80.38 93.08 93.16 89.86 89.26
R 68.86 86.88 93.00 89.74 88.94 93.33
F 75.53 80.70 94.86 93.09 87.77 92.80
Verb-Det-Noun (8)
A 89.08 89.08 93.83 93.65 90.07 91.33
P 95.44 84.13 92.88 94.00 91.04 89.25
R 73.30 88.53 97.50 95.50 91.57 97.63
F 80.97 84.91 95.09 94.71 91.21 93.08
Verb-Prep-Noun (7)
A 85.53 91.15 93.64 92.62 88.75 92.10
P 97.13 81.40 96.81 97.20 92.48 94.33
R 64.36 92.67 84.73 82.79 82.71 85.00
F 74.08 86.03 97.81 96.87 83.13 96.65
Verb-Noun-Prep (9)
A 84.06 82.32 93.11 88.99 90.18 91.18
P 90.72 76.26 90.78 89.73 86.78 85.89
R 68.41 80.90 95.44 90.03 91.44 96.00
F 71.82 72.82 92.69 89.14 88.33 89.99
Table 2: Test set: Average performance over all MWEs and
by group. The best supervised classifier outperforms base-
lines in all groups. A: Accuracy; P: Precision; R: Recall; F:
F-Score.
and lexical change, DO outperforms CF. To name
a few, make an impression, raise the alarm, take
a chance and make allowances for. For example,
for take a chance intervening words are quite com-
mon, as in: ?I?m taking a real chance on you.?, or
a change in determiner as in: ?I preferred to take
my chances?. Indeed, CF showed poor precision
only for MWEs with a common literal usage. Two
such MWEs were present in the development set
(break the ice and tie the knot ) and two in the test
set (wash hands of and keep hands off).
Baselines versus supervised classifiers. As
shown in the tables, R1 outperforms the best base-
line in terms of accuracy in both test and devel-
opment. Moreover, the supervised classifiers are
more stable in their accuracy. For the develop-
ment set the standard deviation of accuracy scores
averages 22.58 for CF and DO, and 6.68 for R1,
R2, S, and C. For the test set the baselines av-
erage 9.07 (Verb-Det-Noun), 11.11 (Verb-Prep-
Noun) and 14.26 (Verb-Noun-Prep), and the su-
pervised methods average 4.97 (Verb-Det-Noun),
7.66 (Verb-Prep-Noun) and 7.97. This stability
means that the supervised classifiers are able to
perform well on MWEs with different behavior.
For example, R1 is able to perform well on ex-
pressions where order is strict, as DO does (e.g.,
make a face), while also performing well on those
where order varies (e.g., make a distinction).
Supervised classifiers. R1 and R2, based on
surface features, show similar accuracy values,
with R1 doing somewhat better in the Verb-Prep-
Noun and Verb-Noun-Prep groups. This is due
to the Lexical Values feature, which accounts for
a change in preposition. A change in preposi-
tion (as in ?wash hands of some matter? versus
?wash hands in the sink?) is more significant than
a change in determiner in the Verb-Determiner-
Noun group. This improves precision on negative
instances, which are rejected more precisely based
on the preposition value. Nevertheless, the rela-
tively simple features in R2, essentially order and
distance, perform quite well.
The F-score result for R1, 94.86, is an improve-
ment over the F-score result of the supervised clas-
sifier used in (Sporleder and Li, 2009), 90.15.
Although the sentence data is different (our data
includes sentences with non-expression uses) the
number of sentences used is similar.
S, based on syntactic features, performs worse
than R1/2. It shows better accuracy than the base-
lines in all but the (Verb-Prep-Noun) group and
is also more stable. C, a combination of surface
and syntactic features, performs better than S and
slightly worse than R1/2.
Why do the syntactic features perform worse
than surface features? An analysis of the S clas-
sifier errors reveals two important causes. First,
there is substantial variation in the dependency
tree structures of the non-compositional uses of
the expressions as output by the parser. Thus,
the syntactic feature classifier was more difficult
to learn than the surface feature one, requiring a
larger training set. This is not surprising, given
that many MWEs exhibit an irregular syntactic be-
havior that might even seem strange at times. For
example, in the sentence fragment ?and then he
came to.?, ?came to? is an MWE. A parser might
find it difficult to parse the sentence correctly, ex-
pecting a noun phrase to follow the ?to?.
Second, as described above, the syntactic fea-
tures consist of general syntactic relations ex-
tracted from the parse tree and not type-specific
knowledge. As a result, literal or non-expression
uses of the MWE?s components, which have a
close syntactic relation in a given sentence, appear
as non-compositional uses of the expression to the
classifier.
475
6 Discussion
This study has addressed MWE identification: de-
ciding if a potential use of an expression is a non-
compositional one. Despite its importance in ba-
sic NLP tasks, the problem has been largely over-
looked in NLP research, probably due to it pre-
sumed simplicity. However, as we have shown,
simple methods for MWE identification, such as
our baselines, do not perform consistently well
across MWEs. This study serves to highlight this
point and the need for more sophisticated methods
for MWE identification.
We have shown that using a supervised learning
method employing surface sentence features based
on canonical form, it is possible to improve perfor-
mance significantly. Unlike previous research, our
method is not tailored to specific MWE types, and
we did not ignore non-expression uses in our ex-
periments.
Future research should experiment with non-
verbal MWEs, since our features are not spe-
cific to verbal MWE types. Another direction is
a more sophisticated corpus sampling algorithm.
The current work ignored MWEs which had an un-
balanced training set (usually too few positives).
Methods for gathering enough positive instances
of such MWEs will be useful for testing the meth-
ods proposed here, as well as for general MWE
research.
References
I?nki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza,
Koldo Gojenola and Ruben Urizar. 2004. Repre-
sentation and treatment of multiword expressions in
Basque. ACL ?04 Workshop on Multiword Expres-
sions.
The British National Corpus. 2001. The British
National Corpus, version 2 (BNC World). Dis-
tributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium. URL:
http://www.natcorp.ox.ac.uk/
Elisabeth Breidt, Frederique Segond, and Giuseppen
Valetto. 1996. Local grammars for the description
of multi-word lexemes and their automatic recogni-
tion in texts. COMPLEX ?96. Budapest.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. ACL ?07 Workshop on A
Broader Perspective on Multiword Expressions.
Collins COBUILD. 2003. Collins COBUILD Ad-
vanced Learner?s English Dictionary. Harper-
Collins Publishers, 4th edition.
Paul Deane. 2005. A nonparametric method for ex-
traction of candidate phrasal terms. ACL ?05.
Gael Dias. 2003. Multiword unit hybrid extrac-
tion. ACL ?03 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. EACL ?06.
Christiane Fellbaum, Alexander Geyken, Axel Herold,
Fabian Koerner, and Gerald Neumann. 2006.
Corpus-based studies of German idioms and light
verbs. International Journal of Lexicography,
19(4):349?361.
Christiane Fellbaum. 1998. Towards a representation
of idioms in WordNet. COLING-ACL ?98 Workshop
on the Use of WordNet in Natural Language Pro-
cessing Systems.
Nicole Gr?egoire. 2007. Design and implementation of
a lexicon of Dutch multiword expressions. ACL ?07
Workshop on A Broader Perspective on Multiword
Expressions.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line
between literal and idiomatic meanings. COLING-
ACL ?06, Poster Sessions.
John S. Justeson and Slava M. Katz. 1995. Technical
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1:9?27.
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis.
COLING-ACL ?06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties..
Geoffrey Leech, Roger Garside and Michael Bryant.
1994. CLAWS4: The tagging of the British Na-
tional Corpus. COLING ?94.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
LREC ?06.
Rosamund Moon. 1998. Fixed Expressions and Id-
ioms in English. Oxford: Clarendon Press.
John Platt. 1998. Machines using sequential minimal
optimization. In In B. Schoelkopf and C. Burges and
A. Smola, editors, Advances in Kernel Methods ?
Support Vector Learning.
476
Susanne Z. Riehemann. 2001. A Constructional Ap-
proach to Idioms and Word Formation. Ph.D. Thesis.
Stanford.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. EACL ?09.
Aline Villavicencio, Ann Copestake, Benjamin Wal-
dron, and Fabre Lambeau. 2004. Lexical encoding
of MWE. ACL ?04 Workshop on Multiword Expres-
sions.
Joachim Wermter and Udo Hahn. 2004. Collocation
extraction based on modifiability statistics. COL-
ING ?04.
Joachim Wermter and Udo Hahn. 2006. You can?t beat
frequency (unless you use linguistic knowledge) ?
a qualitative evaluation of association measures for
collocation and term extraction. COLING-ACL ?06.
Ian H. Witten amd Eibe Frank. 2000. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann.
477
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 852?861,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Enhancement of Lexical Concepts Using Cross-lingual Web Mining
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental in linguistics and NLP. Manual
concept compilation is labor intensive, er-
ror prone and subjective. We present a
web-based concept extension algorithm.
Given a set of terms specifying a concept
in some language, we translate them to
a wide range of intermediate languages,
disambiguate the translations using web
counts, and discover additional concept
terms using symmetric patterns. We then
translate the discovered terms back into
the original language, score them, and ex-
tend the original concept by adding back-
translations having high scores. We eval-
uate our method in 3 source languages and
45 intermediate languages, using both hu-
man judgments and WordNet. In all cases,
our cross-lingual algorithm significantly
improves high quality concept extension.
1 Introduction
A concept (or lexical category) is a set of lex-
ical items sharing a significant aspect of their
meanings (e.g., types of food, tool names, etc).
Concepts are fundamental in linguistics and NLP,
in thesauri, dictionaries, and various applications
such as textual entailment and question answering.
Great efforts have been invested in manual
preparation of concept resources such as WordNet
(WN). However, manual preparation is labor in-
tensive, which means it is both costly and slow
to update. Applications needing data on some
very specific domain or on a recent news-related
event may find such resources lacking. In addition,
manual preparation is error-prone and susceptible
to subjective concept membership decisions, fre-
quently resulting in concepts whose terms do not
belong to the same level of granularity
1
. As a re-
sult, there is a need to find methods for automatic
improvement of concept coverage and quality.
The web is a huge up-to-date corpus covering
many domains, so using it for concept extension
has the potential to address the above problems.
The majority of web pages are written in a few
salient languages, hence most of the web-based in-
formation retrieval studies are done on these lan-
guages. However, due to the substantial growth of
the multilingual web
2
, languages in which concept
terms are expressed in the most precise manner
frequently do not match the language where in-
formation is needed. Moreover, representations of
the same concept in different languages may com-
plement each other.
In order to benefit from such cross-lingual in-
formation, concept acquisition systems should be
able to gather concept terms from many available
languages and convert them to the desired lan-
guage. In this paper we present such an algorithm.
Given a set of words specifying a concept in some
source language, we translate them to a range
of intermediate languages and disambiguate the
translations using web counts. Then we discover
additional concept terms using symmetric patterns
and translate the discovered terms back into the
original language. Finally we score the back-
translations using their intermediate languages?
properties, and extend the original concept by
adding back-translations having high scores. The
only language-specific resource required by the al-
gorithm are multilingual dictionaries, and its pro-
cessing times are very modest.
We performed thorough evaluation for 24 con-
cepts in 3 source languages (Hebrew, English and
Russian) and 45 intermediate languages. Concept
definitions were taken from existing WordNet sub-
trees, and the obtained new terms were manually
1
See Section 5.1.1.
2
http://www.internetworldstats.com/stats7.htm
852
scored by human judges. In all cases we have sig-
nificantly extended the original concept set with
high precision. We have also performed a fully
automatic evaluation with 150 concepts, showing
that the algorithm can re-discover WN concepts
with high precision and recall when given only
partial lists as input.
Section 2 discusses related work, Section 3 de-
tails the algorithm, Section 4 describes the evalua-
tion protocol and Section 5 presents our results.
2 Related work
One of the main goals of this paper is the extension
or automated creation of lexical databases such
as WN. Due to the importance of WN for NLP
tasks, substantial research was done on direct or
indirect automated extension of the English WN
(e.g., (Snow et al, 2006)) or WN in other lan-
guages (e.g., (Vintar and Fi?ser, 2008)). The major-
ity of this research was done on extending the tree
structure (finding new synsets (Snow et al, 2006)
or enriching WN with new relationships (Cuadros
and Rigau, 2008)) rather than improving the qual-
ity of existing concept/synset nodes. Other re-
lated studies develop concept acquisition frame-
works for on-demand tasks where concepts are de-
fined by user-provided seeds or patterns (Etzioni et
al., 2005; Davidov et al, 2007), or for fully unsu-
pervised database creation where concepts are dis-
covered from scratch (Banko et al, 2007; Davi-
dov and Rappoport, 2006).
Some papers directly target specific applica-
tions, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an in-
stance of the concept acquisition problem where
the desired concepts contain words that are names
of entities of a particular kind, as done in (Fre-
itag, 2004) using co-clustering and in (Etzioni et
al., 2005) using predefined pattern types.
The two main algorithmic approaches to the
problem are pattern-based concept discovery and
clustering of context feature vectors. The latter
approach represents word contexts as vectors in
some space and uses similarity measures and au-
tomatic clustering in that space (Deerwester et al,
1990). Pereira et al(1993), Curran and Moens
(2002) and Lin (1998) use syntactic features in the
vector definition. Pantel and Lin (2002) improves
on the latter by clustering by committee. Cara-
ballo (1999) uses conjunction and appositive an-
notations in the vector representation. While great
effort has been made for improving the computa-
tional complexity of these methods (Gorman and
Curran, 2006), they still remain data and compu-
tation intensive.
The second major algorithmic approach is to
use lexico-syntactic patterns. Patterns have been
shown to produce more accurate results than fea-
ture vectors, at a lower computational cost on large
corpora (Pantel et al, 2004). In concept acquisi-
tion, pattern-based methods were shown to out-
perform LSA by a large margin (Widdows and
Dorow, 2002). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns in
order to acquire relationships, numerous pattern-
based methods have been proposed for the discov-
ery of concepts from seeds (Pantel et al, 2004;
Davidov et al, 2007; Pasca et al, 2006). Most of
these studies were done for English, while some
show the applicability of their methods to other
languages, including Greek, Czech, Slovene and
French.
Most of these papers attempt to discover con-
cepts from data available in some specific lan-
guage. Recently several studies have proposed to
utilize a second language or several specified lan-
guages in order to extract or extend concepts (Vin-
tar and Fi?ser, 2008; van der Plas and Tiedemann,
2006) or paraphrases (Bosma and Callison-Burch,
2007). However, these methods usually require
the availability of parallel corpora, which limits
their usefulness. Most of these methods utilize
distributional measures, hence they do not possess
the advantages of the pattern-based framework.
Unlike in the majority of recent studies, where
the framework is designed with specific languages
in mind, in our task, in order to take advantage
of information from diverse languages, the algo-
rithm should be able to deal well with a wide va-
riety of possible intermediate languages without
any manual adaptations. Relying solely on mul-
tilingual dictionaries and the web, our algorithm
should be able to discover language-specific pat-
terns and concept terms. While some of the pro-
posed frameworks could potentially be language-
independent, little research has been done to con-
firm this. There are a few obstacles that may
hinder applying common pattern-based methods
to other languages. Many studies utilize parsing
or POS tagging, which frequently depend on the
availability and quality of language-specific tools.
Some studies specify seed patterns in advance, and
853
it is not clear whether translated patterns can work
well on different languages. Also, the absence of
clear word segmentation in some languages (e.g.,
Chinese) can make many methods inapplicable.
A few recently proposed concept acquisition
methods require only a handful of seed words and
no pattern pre-specification (Davidov et al, 2007;
Pasca and Van Durme, 2008). While these studies
avoid some of the obstacles above, it still remains
open whether such methods are indeed language-
independent. In the translation to intermediate lan-
guages part of our framework, we adapt the algo-
rithms in (Davidov and Rappoport, 2006; Davi-
dov et al, 2007) to suit diverse languages (includ-
ing ones without explicit word segmentation). We
also develop a method for efficient automated dis-
ambiguation and translation of terms to and from
any available intermediate language.
Our study is related to cross-language infor-
mation retrieval (CLIR/CLEF) frameworks. Both
deal with information extracted from a set of lan-
guages. However, the majority of CLIR stud-
ies pursue different targets. One of the main
CLIR goals is the retrieval of documents based
on explicit queries, when the document lan-
guage is not the query language (Volk and Buite-
laar, 2002). These frameworks usually develop
language-specific tools and algorithms including
parsers and taggers in order to integrate multilin-
gual queries and documents (Jagarlamudi and Ku-
maran, 2007). Our goal is to develop a language-
independent method using cross-lingual informa-
tion, for the extension and improvement of con-
cepts rather than the retrieval of documents. Be-
sides, unlike in many CLIR frameworks, interme-
diate languages are not specified in advance and
the language of requested data is the same as the
language of request, while available information
may be found in many different intermediate lan-
guages.
3 The Algorithm
Our algorithm is comprised of the following
stages: (1) given a set of words in a source lan-
guage as a specification for some concept, we au-
tomatically translate them to a diverse set of inter-
mediate languages, using multilingual dictionar-
ies; (2) the translations are disambiguated using
web counts; (3) for each language, we retrieve a
set of web snippets where these translations co-
appear and apply a pattern-based concept exten-
sion algorithm for discovering additional terms;
(4) we translate the discovered terms back to the
source language, and disambiguate them; (5) we
score the back-translated terms using data on their
behavior in the intermediate languages, and merge
the sets obtained from different languages into a
single one, retaining terms whose score passes a
certain threshold. Stages 1-3 of the algorithm have
been described in (Davidov and Rappoport, 2009),
where the goal was to translate a concept given in
one language to other languages. The framework
presented here includes the new stages 4-5, and its
goal and evaluation methods are completely dif-
ferent.
3.1 Concept specification and translation
We start from a set of words denoting a concept in
a given source language. Thus we may use words
like (apple, banana, ...) as the definition of the
concept of fruit or (bear, wolf, fox, ...) as the def-
inition of wild animals. In order to reduce noise,
we limit the length (in words) of multiword ex-
pressions considered as terms. To calculate this
limit for a language, we randomly take 100 terms
from the appropriate dictionary and set a limit
as Lim
mwe
= round(avg(length(w))) where
length(w) is the number of words in term w. For
languages like Chinese without inherent word seg-
mentation, length(w) is the number of characters
in w. While for many languages Lim
mwe
= 1,
some languages like Vietnamese usually require
two or more words to express terms.
3.2 Disambiguation of translated terms
One of the problems in utilization of multilingual
information is ambiguity of translation. First, in
order to apply the concept acquisition algorithm,
at least some of the given concept terms must be
automatically translated to each intermediate lan-
guage. In order to avoid reliance on parallel cor-
pora, which do not exist or are extremely small for
most of our language pairs, we use bilingual dic-
tionaries. Such dictionaries usually provide many
translations, one or more for each sense, so this
translation is inherently fuzzy. Second, once we
acquire translated term lists for each intermedi-
ate language, we need to translate them back to
the source language and such back-translations are
also fuzzy. In both cases, we need to select the ap-
propriate translation for each term.
While our desire would be to work with as many
languages as possible, in practice, some or even
854
most of the concept terms may be absent from the
appropriate dictionary. Such concept terms are ig-
nored.
One way to deal with ambiguity is by applying
distributional methods, usually requiring a large
single-language corpus or, more frequently, paral-
lel corpora. However, such corpora are not readily
available for many languages and domains. Ex-
tracting such statistical information on-demand is
also computationally demanding, limiting its us-
ability. Hence, we take a simple but effective
query-based approach. This approach, while be-
ing powerful as we show in the evaluation, only
relies on a few web queries and does not rely on
any language-specific resources or data.
We use the conjecture that terms of the same
concept tend to co-appear more frequently than
ones belonging to different concepts
3
. Thus, we
select a translation of a term co-appearing most
frequently with some translation of a different
term of the same concept. We estimate how well
translations of different terms are connected to
each other. Let C = {C
i
} be the given seed
words for some concept. Let Tr(C
i
, n) be the
n-th available translation of word C
i
and Cnt(s)
denote the web count of string s obtained by a
search engine. We select a translation Tr(C
i
)
according to:
F (w
1
, w
2
) =
Cnt(?w
1
? w
2
?)? Cnt(?w
2
? w
1
?)
Cnt(w
1
)? Cnt(w
2
)
Tr(C
i
) =
argmax
s
i
(
max
s
j
j 6=i
(F (Tr(C
i
, s
i
), T r(C
j
, s
j
)))
)
We utilize the Y ahoo! ?x * y?,?x * * y? wild-
cards that allow to count only co-appearances
where x and y are separated by a single word or
word pair. As a result, we obtain a set of disam-
biguated term translations. This method is used
both in order to translate from the source lan-
guage to each intermediate language and to back-
translate the newly discovered concept terms from
the intermediate to the source language.
The number of queries in this stage depends on
the ambiguity of the concept terms? translations.
In order to decrease the amount of queries, if there
are more than three possible senses we sort them
by frequency
4
and take three senses with medium
frequency. This allows us to skip the most ambigu-
ous and rare senses without any significant effect
on performance. Also, if the number of combina-
3
Our results here support this conjecture.
4
Frequency is estimated by web count for a given word.
tions is still too high (>30), we randomly sample
at most 30 of the possible combinations.
3.3 Pattern-based extension of concept terms
in intermediate languages
We first mine the web for contexts containing
the translations. Then we extract from the re-
trieved snippets contexts where translated terms
co-appear, and detect patterns where they co-
appear symmetrically. Then we use the detected
patterns to discover additional concept terms. In
order to define word boundaries, for each language
we manually specify boundary characters such as
punctuation/space symbols. This data, along with
dictionaries, is the only language-specific data in
our framework.
Web mining for translation contexts. In order
to get language-specific data, we need to restrict
web mining each time to the processed interme-
diate language. This restriction is straightforward
if the alphabet or term translations are language-
specific or if the search API supports restriction to
this language
5
. In case where there are no such
natural restrictions, we attempt to detect and add
to our queries a few language-specific frequent
words. Using our dictionaries, we find 1?3 of the
15 most frequent words in a desired language that
are unique to that language, and we ?and? them
with the queries to ensure proper language selec-
tion. This works well for almost all languages (Es-
peranto being a notable exception).
For each pair A,B of disambiguated term trans-
lations, we construct and execute the following
two queries: {?A * B?, ?B * A?}
6
. When we
have 3 or more terms we also add {A B C D}-like
conjunction queries which include 3-5 words. For
languages with Lim
mwe
> 1, we also construct
queries with several ?*? wildcards between terms.
For each query we collect snippets containing text
fragments of web pages. Such snippets frequently
include the search terms. Since Y ahoo! Boss al-
lows retrieval of up to the 1000 first results (50 in
each query), we collect several thousands snippets.
For most of the intermediate languages, only a few
dozen queries (40 on the average) are required to
obtain sufficient data, and queries can be paral-
lelized. Thus the relevant data can be downloaded
5
Yahoo! allows restriction for 42 languages.
6
These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
855
in seconds. This makes our approach practical for
on-demand retrieval or concept verification tasks.
Meta-patterns. Following (Davidov et al,
2007), we seek symmetric patterns to retrieve
concept terms. We use two meta-pattern types.
First, a Two-Slot pattern type constructed as
follows:
[Prefix] C
1
[Infix] C
2
[Postfix]
C
i
are slots for concept terms. We allow up to
Lim
mwe
space-separated
7
words to be in a sin-
gle slot. Infix may contain punctuation, spaces,
and up to Lim
mwe
? 4 words. Prefix and Post-
fix are limited to contain punctuation characters
and/or Lim
mwe
words.
Terms of the same concept frequently co-appear
in lists. To utilize this, we introduce two additional
List pattern types
8
:
[Prefix] C
1
[Infix] (C
i
[Infix])+ (1)
[Infix] (C
i
[Infix])+ C
n
[Postfix] (2)
Following (Widdows and Dorow, 2002), we define
a pattern graph. Nodes correspond to terms and
patterns to edges. If term pair (w
1
, w
2
) appears
in pattern P , we add nodes N
w
1
, N
w
2
to the graph
and a directed edge E
P
(N
w
1
, N
w
2
) between them.
Symmetric patterns. We consider only sym-
metric patterns. We define a symmetric pat-
tern as a pattern where some concept terms
C
i
, C
j
appear both in left-to-right and right-to-
left order. For example, if we consider the
terms {apple, pineapple} we select a List pattern
?(one C
i
, )+ and C
n
.? if we find both ?one apple,
one pineapple, one guava and orange.? and ?one
watermelon, one pineapple and apple.?. If no such
patterns are found, we turn to a weaker definition,
considering as symmetric those patterns where the
same terms appear in the corpus in at least two dif-
ferent slots. Thus, we select a pattern ?for C
1
and
C
2
? if we see both ?for apple and guava,? and ?for
orange and apple,?.
Retrieving concept terms. We collect terms in
two stages. First, we obtain ?high-quality? core
terms and then we retrieve potentially more noisy
ones. At the first stage we collect all terms
9
that
7
As before, for languages without space-based word sep-
aration Lim
mwe
limits the number of characters instead.
8
(E)+ means one or more instances of E.
9
We do not consider as terms the 50 most frequent words.
are bidirectionally connected to at least two differ-
ent original translations, and call them core con-
cept terms C
core
. We also add the original ones as
core terms. Then we detect the rest of the terms
C
rest
that are connected to the core stronger than
to the remaining words, as follows:
G
in
(c)={w?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
G
out
(c)={w/?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
C
rest
={c||G
in
(c)|>|G
out
(c)|}
For the sake of simplicity, we do not attempt to
discover more patterns/instances iteratively by re-
querying the web. If we have enough data, we use
windowing to improve result quality. If we obtain
more than 400 snippets for some concept, we di-
vide the data into equal parts, each containing up
to 400 snippets. We apply our algorithm indepen-
dently to each part and select only the words that
appear in more than one part.
3.4 Back-translation and disambiguation
At the concept acquisition phase of our framework
we obtained sets of terms for each intermediate
language, each set representing a concept. In or-
der to be useful for the enhancement of the origi-
nal concept, these terms are now back-translated to
the source language. We disambiguate each back-
translated term using the process described in Sec-
tion 3.2. Having sets of back-translated terms for
each intermediate language, our goal is to combine
these into a single set.
3.5 Scoring and merging the back
translations
We do this merging using the following scoring
strategy, assigning for each proposed term t
?
in
concept C the score S(t
?
, C), and selecting terms
with S(t
?
, C) > H where H is a predefined
threshold.
Our scoring is based on the two following con-
siderations. First, we assume that terms extracted
from more languages tend to be less noisy and
language-dependent. Second, we would like to fa-
vor languages with less resources for a given con-
cept, since noise empirically appears to be less
prominent in such languages
10
.
For language L and concept C = {t
1
. . . t
k
}
we get a disambiguated set of translations
{Tr(t
1
, L) . . . T r(t
k
, L)}. We define relative lan-
10
Preliminary experimentation, as well as the evaluation
results presented in this paper, support both of these consid-
erations.
856
guage frequency by
LFreq(L,C) =
?
t
i
?C
(Freq(Tr(t
i
, L)))
?
L
?
,t
i
?C
(Freq(Tr(t
i
, L
?
))
where Freq(Tr(t
i
, L)) is a frequency of term?s t
i
translation to language L estimated by the num-
ber of web hits. Thus languages in which trans-
lated concept terms appear more times will get
higher relative frequency, potentially indicating a
greater concept translation ambiguity. Now, for
each new term t
?
discovered through LNum(t
?
)
different languages L
1
. . . L
LNum(t
?
)
we calculate
a term score
11
S(t
?
, C):
S(t
?
, C) = LNum(t
?
)?
(
1?
?
i
LFreq(L
i
, C)
)
For each discovered term t
?
, S(t
?
, C) ?
[0, LNum(t
?
)], while discovery of t
?
in less fre-
quent languages will cause the score to be closer to
LNum(t
?
). So terms appearing in a greater num-
ber of infrequent languages will get higher scores.
After the calculation of score for each proposed
term, we retain terms whose scores are above the
predefined threshold H . In our experiments we
have used H = 3, usually meaning that acquisi-
tion of a term through 3-4 uncommon intermedi-
ate languages should be enough to accept it. The
same score measure can also be used to filter out
?bad? terms in an already existing concept.
4 Experimental Setup
We describe here the languages, concepts and dic-
tionaries we used in our experiments.
4.1 Languages and concepts
One of the main goals in this research is to take
advantage of concept data in every possible lan-
guage. As intermediate languages, we used 45 lan-
guages including major west European languages
like French or German, Slavic languages like Rus-
sian, Semitic languages as Hebrew and Arabic,
and diverse Asian languages such as Chinese and
Persian. To configure parameters we have used a
set of 10 concepts in Russian as a development set.
These concepts were not used in evaluation.
We examined a wide variety of concepts and for
each of them we used all languages with available
translations. Table 1 shows the resulting top 10
most utilized languages in our experiments.
11
In this expression i runs only on languages with term t
?
hence the summation is not 1.
English Russian Hebrew
German(68%) English(70%) English(66%)
French(60%) German(62%) German(65%)
Italian(60%) French(62%) Italian(61%)
Portuguese(57%) Spanish(58%) French(59%)
Spanish(55%) Italian(56%) Spanish(57%)
Turkish(51%) Portuguese(54%) Portuguese(57%)
Russian(50%) Korean(50%) Korean(48%)
Korean(46%) Turkish(49%) Russian(43%)
Chinese(45%) Chinese(47%) Turkish(43%)
Czech(42%) Polish (44%) Czech(40%)
Table 1: The ten most utilized intermediate languages in
our experiments. In parentheses we show the percentage of
new terms that these languages helped discover.
We have used the English, Hebrew (Ordan and
Winter, 2008) and Russian (Gelfenbeynand et al,
2003) WordNets as sources for concepts and for
the automatic evaluation. Our concept set selec-
tion was based on English WN subtrees. To per-
form comparable experiments with Russian and
Hebrew, we have selected the same subtrees in
the Hebrew and Russian WN. Concept definitions
given to human judges for evaluation were based
on the corresponding WN glosses. For automated
evaluation we selected 150 synsets/subtrees con-
taining at least 10 single word terms (existing in
all three tested languages).
For manual evaluation we used a subset of 24
of these concepts. In this subset we tried to select
generic concepts manually, such that no domain
expert knowledge was required to check their cor-
rectness. Ten of these concepts were identical to
ones used in (Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006), which allowed us to
compare our results to recent work in case of En-
glish. Table 2 shows these 10 concepts along with
the sample terms. While the number of tested con-
cepts is not very large, it provides a good indica-
tion for the quality of our approach.
Concept Sample terms
Musical instruments guitar, flute, piano
Vehicles/transport train, bus, car
Academic subjects physics, chemistry, psychology
Body parts hand, leg, shoulder
Food egg, butter, bread
Clothes pants, skirt, jacket
Tools hammer, screwdriver, wrench
Places park, castle, garden
Crimes murder, theft, fraud
Diseases rubella, measles, jaundice
Table 2: Ten of the selected concepts with sample terms.
857
4.2 Multilingual dictionaries
We developed tools for automatic access to a num-
ber of dictionaries. We used Wikipedia cross-
language links as our main source (> 60%) for
offline translation. These links include translation
of Wikipedia terms into dozens of languages. The
main advantage of using Wikipedia is its wide cov-
erage of concepts and languages. However, one
problem it has is that it frequently encodes too
specific senses and misses common ones (bear is
translated as family Ursidae, missing its common
?wild animal? sense). To overcome these difficul-
ties, we also used Wiktionary and complemented
these offline resources with automated queries to
several (25) online dictionaries. We start with
Wikipedia definitions, then Wiktionary, and then,
if not found, we turn to online dictionaries.
5 Evaluation and Results
Potential applications of our framework include
both the extension of existing lexical databases
and the construction of new databases from a small
set of seeds for each concept. Consequently, in
our evaluation we aim to check both the ability
to extend nearly complete concepts and the abil-
ity to discover most of the concept given a few
seeds. Since in our current framework we extend
a small subset of concepts rather than the whole
database, we could not utilize application-based
evaluation strategies such as performance in WSD
tasks (Cuadros and Rigau, 2008).
5.1 Human judgment evaluation
In order to check how well we can extend existing
concepts, we count and verify the quality of new
concept terms discovered by the algorithm given
complete concepts from WN. Performing an auto-
matic evaluation of such new terms is a challeng-
ing task, since there are no exhaustive term lists
available. Thus, in order to check how well newly
added terms fit the concept definition, we have to
use human judges.
We provided four human subjects with 24 lists
of newly discovered terms, together with original
concept definitions (written as descriptive natural
language sentences) and asked them to rank (1-10,
10 being best) how well each of these terms fits
the given definition. We have instructed judges to
accept common misspellings and reject words that
are too general/narrow for the provided definition.
We mixed the discovered terms with equal
amounts of terms from three control sets: (1) terms
from the original WN concept; (2) randomly se-
lected WN terms; (3) terms obtained by apply-
ing the single-language concept acquisition algo-
rithm described in Section 3.3 in the source lan-
guage. Kappa inter-annotator agreement scores
were above 0.6 for all tests below.
5.1.1 WordNet concept extension
The middle column of Table 3 shows the judge
scores and average amount of added terms for
each source language. In this case the algorithm
was provided with complete term lists as con-
cept definitions, and was requested to extend these
lists. We can see that while the scores for original
WN terms are not perfect (7/10), single-language
and cross-lingual concept extension achieve nearly
the same scores. However, the latter discovers
many more new concept terms without reducing
quality. The difference becomes more substan-
tial for Hebrew, which is a resource-poor source
language, heavily affecting the performance of
single-language concept extension methods.
The low ranks for WN reflect the ambiguity of
definition of some of its classification subtrees.
Thus, for the ?body part? concept defined in Word-
Net as ?any part of an organism such as an or-
gan or extremity? (which is not supposed to re-
quire domain-specific knowledge to identify) low
scores were given (correctly) by judges to generic
terms such as tissue, system, apparatus and pro-
cess (process defined in WN as ?a natural pro-
longation or projection from a part of an organ-
ism?), positioned in WN as direct hyponyms of
body parts. Low scores were also given to very
specific terms like ?saddle? (posterior part of the
back of a domestic fowl) or very ambiguous terms
like ?small? (the slender part of the back).
5.1.2 Seed-based concept extension
The rightmost column of Table 3 shows similar in-
formation to the middle column, but when only
the three most frequent terms from the original
WN concept were given as concept definitions.
We can see that even given three words as seeds,
the cross-lingual framework allows to discover
many new terms. Surprisingly, terms extracted by
the cross-lingual framework achieve significantly
higher scores not only in comparison to the single-
language algorithm but also in comparison to ex-
isting WN terms. Thus while the ?native? WN
concept and single-language concept extension re-
858
sults get a score of 7/10, terms obtained by the
cross-lingual framework obtain an average score
of nearly 9/10.
This suggests that our cross-lingual framework
can lead to better (from a human judgment point
of view) assignment of terms to concepts, even in
comparison to manual annotation.
Input
all terms 3 terms
English
WordNet 7.2 7.2
Random 1.8 1.8
SingleLanguage 7.0(10) 7.8(18)
Crosslingual 6.9(19) 8.8(26)
Russian
WordNet 7.8 7.8
Random 1.9 1.9
SingleLanguage 7.4(10) 8.1(16)
Crosslingual 7.6(21) 9.0(29)
Hebrew
WordNet 7.0 7.0
Random 1.3 1.3
SingleLanguage 6.5(4) 7.5(6)
Crosslingual 6.8(18) 8.9(24)
Table 3: Human judgment scores for concept extension in
three languages (1 . . . 10, 10 is best). The WordNet, Random
and SingleLanguage rows provide corresponding baselines.
Average count of newly added terms are shown in parenthe-
ses. Average original WN concept size in this set was 36 for
English, 32 for Russian and 27 for Hebrew.
5.2 WordNet-based evaluation
While human judgment evaluation provides a
good indication for the quality of our framework,
it has severe limitations. Thus terms in many con-
cepts require domain expertise to be properly la-
beled. We have complemented human judgment
evaluation with automated WN-based evaluation
with a greater (150) number of concepts. For each
of the 150 concepts, we have applied our frame-
work on a subset of the available terms, and esti-
mated precision and recall of the resulting term list
in comparison to the original WN term list. The
evaluation protocol and metrics were very simi-
lar to (Davidov and Rappoport, 2006; Widdows
and Dorow, 2002) which allowed us to do indirect
comparison to previous work.
Table 4 shows precision and recall for this task
comparing single-language concept extension and
the cross-lingual framework. We can see that
in all cases, utilization of the latter greatly im-
proves recall. It also significantly outperforms
the single-language pattern-based method intro-
duced by (Davidov and Rappoport, 2006), which
achieves average precision of 79.3 on a similar set
in English (in comparison to 86.7 in this study).
We can also see a decrease in precision when the
algorithm is provided with 50% of the concept
terms as input and had to discover the remaining
50%. However, careful examination of the results
shows that this decrease is due to discovery of ad-
ditional correct terms not present in WordNet.
Input
50% terms 3 terms
P R F P R F
English
SingleLanguage 89.2 75.9 82.0 80.6 15.2 25.6
CrossLingual 86.5 91.1 88.7 86.7 60.2 71.1
Russian
SingleLanguage 91.3 69.0 78.6 82.1 18.3 29.9
CrossLingual 84.9 86.2 85.5 85.3 62.1 71.9
Hebrew
SingleLanguage 93.8 38.6 54.7 90.2 5.7 10.7
CrossLingual 86.5 82.4 84.4 93.9 55.6 69.8
Table 4: WordNet-based precision (P) and recall (R) for
concept extension.
5.3 Contribution of each language
Each of the 45 languages we used influences the
score of at least 5% of the discovered terms. How-
ever, it is not apparent if all languages are indeed
beneficial or if only a handful of languages can
be used. In order to check this point we have per-
formed partial automated tests as described in Sec-
tion 5.2, removing one language at a time. We also
tried to remove random subsets of 2-3 languages,
comparing them to removal of one of them. We
saw that in each case removal of more languages
caused a consistent (while sometimes minor) de-
crease both in precision and recall metrics. Thus,
each language contributes to the system.
6 Discussion
We proposed a framework which given a set of
terms defining a concept in some language, uti-
lizes multilingual information available on the
web in order to extend this list. This method
allows to take advantage of web data in many
languages, requiring only multilingual dictionar-
ies. Our method was able to discover a substan-
tially greater number of terms than state-of-the-art
single language pattern-based concept extension
methods, while retaining high precision.
We also showed that concepts obtained by this
method tend to be more coherent in compari-
son to corresponding concepts in WN, a man-
ually prepared resource. Due to its relative
language-independence and modest data require-
ments, this framework allows gathering required
859
concept information from the web even if it is scat-
tered among different and relatively uncommon or
resource-poor languages.
References
Mishele Banko, Michael J Cafarella , Stephen Soder-
land, Matt Broadhead, Oren Etzioni, 2007. Open
information extraction from the Web. IJCAI ?07.
Wauter Bosma, Chris Callison-Burch, 2007. Para-
phrase substitution for recognizing textual entail-
ment.. Evaluation of Multilingual and Multimodal
Information Retrieval, Lecture Notes in Computer
Science ?07.
Sharon Caraballo, 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. ACL
?99.
Montse Cuadros, German Rigau, 2008. KnowNet:
Building a large net of knowledge from the Web.
COLING ?08.
James R. Curran, Marc Moens, 2002. Improvements
in automatic thesaurus extraction SIGLEX 02?, 59?
66.
Dmitry Davidov, Ari Rappoport, 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel,
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2009. Translation
and extension of concepts across languages. EACL
?09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. In-
dexing by latent semantic analysis. J. of the Ameri-
can Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in graphs for
lexical acquisition and word sense discrimination.
MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey,
S. Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel Weld, Alexander Yates, 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91134.
Dayne Freitag, 2004. Trained named entity recogni-
tion using distributional clusters. EMNLP ?04.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic
translation of WordNet semantic network to Russian
language (in Russian) International Dialog 2003
Workshop.
J. Gorman, J.R. Curran, 2006. Scaling distributional
similarity to large corpora. COLING-ACL ?06.
Marti Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Jagadeesh Jagarlamudi, A Kumaran, 2007. Cross-
lingual information retrieval system for Indian lan-
guages. Working Notes for the CLEF 2007 Work-
shop.
Dekang Lin, 1998. Automatic retrieval and clustering
of similar words. COLING ?98.
Noam Ordan, Shuly Wintner, 2007. Hebrew Word-
Net: a test case of aligning lexical databases across
languages. International Journal of Translation
19(1):39-58, 2007.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and similarities on
the web: fact extraction in the fast lane. COLING-
ACL ?06.
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
ACL ?08.
Patrick Pantel, Dekang Lin, 2002. Discovering word
senses from text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
John Paolillo, Daniel Pimienta, Daniel Prado, et al,
2005. Measuring linguistic diversity on the Internet.
UNESCO Institute for Statistics Montreal, Canada.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the global WordNet grid. CIL18.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional clustering of English words. ACL ?93.
Ellen Riloff, Rosie Jones, 1999. Learning dictionaries
for information extraction by multi-level bootstrap-
ping. AAAI ?99.
Rion Snow, Daniel Jurafsky, Andrew Ng, 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. COLING-ACL ?06.
Lonneke van der Plas, Jorg Tiedemann, 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. COLING-ACL
?06.
860
Martin Volk, Paul Buitelaar, 2002. A systematic eval-
uation of concept-based cross-language information
retrieval in the medical domain. In: Proc. of 3rd
Dutch-Belgian Information Retrieval Workshop.
?
Spela Vintar, Darja Fi?ser, 2008. Harvesting multi-
word expressions from parallel corpora. LREC ?08.
Dominic Widdows, Beate Dorow, 2002. A graph
model for unsupervised lexical acquisition. COL-
ING ?02.
861
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 175?183,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Translation and Extension of Concepts Across Languages
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a method which, given a few
words defining a concept in some lan-
guage, retrieves, disambiguates and ex-
tends corresponding terms that define a
similar concept in another specified lan-
guage. This can be very useful for
cross-lingual information retrieval and the
preparation of multi-lingual lexical re-
sources. We automatically obtain term
translations from multilingual dictionaries
and disambiguate them using web counts.
We then retrieve web snippets with co-
occurring translations, and discover ad-
ditional concept terms from these snip-
pets. Our term discovery is based on co-
appearance of similar words in symmetric
patterns. We evaluate our method on a set
of language pairs involving 45 languages,
including combinations of very dissimilar
ones such as Russian, Chinese, and He-
brew for various concepts. We assess the
quality of the retrieved sets using both hu-
man judgments and automatically compar-
ing the obtained categories to correspond-
ing English WordNet synsets.
1 Introduction
Numerous NLP tasks utilize lexical databases that
incorporate concepts (or word categories): sets
of terms that share a significant aspect of their
meanings (e.g., terms denoting types of food, tool
names, etc). These sets are useful by themselves
for improvement of thesauri and dictionaries, and
they are also utilized in various applications in-
cluding textual entailment and question answer-
ing. Manual development of lexical databases is
labor intensive, error prone, and susceptible to
arbitrary human decisions. While databases like
WordNet (WN) are invaluable for NLP, for some
applications any offline resource would not be ex-
tensive enough. Frequently, an application re-
quires data on some very specific topic or on very
recent news-related events. In these cases even
huge and ever-growing resources like Wikipedia
may provide insufficient coverage. Hence appli-
cations turn to Web-based on-demand queries to
obtain the desired data.
The majority of web pages are written in En-
glish and a few other salient languages, hence
most of the web-based information retrieval stud-
ies are done on these languages. However, due
to the substantial growth of the multilingual web1,
queries can be performed and the required infor-
mation can be found in less common languages,
while the query language frequently does not
match the language of available information.
Thus, if we are looking for information about
some lexical category where terms are given in
a relatively uncommon language such as Hebrew,
it is likely to find more detailed information and
more category instances in a salient language such
as English. To obtain such information, we need
to discover a word list that represents the desired
category in English. This list can be used, for in-
stance, in subsequent focused search in order to
obtain pages relevant for the given category. Thus
given a few Hebrew words as a description for
some category, it can be useful to obtain a simi-
lar (and probably more extended) set of English
words representing the same category.
In addition, when exploring some lexical cate-
gory in a common language such as English, it is
1http://www.internetworldstats.com/stats7.htm
175
frequently desired to consider available resources
from different countries. Such resources are likely
to be written in languages different from English.
In order to obtain such resources, as before, it
would be beneficial, given a concept definition in
English, to obtain word lists denoting the same
concept in different languages. In both cases a
concept as a set of words should be translated as a
whole from one language to another.
In this paper we present an algorithm that given
a concept defined as a set of words in some source
language discovers and extends a similar set in
some specified target language. Our approach
comprises three main stages. First, given a few
terms, we obtain sets of their translations to the tar-
get language from multilingual dictionaries, and
use web counts to select the appropriate word
senses. Next, we retrieve search engine snippets
with the translated terms and extract symmetric
patterns that connect these terms. Finally, we use
these patterns to extend the translated concept, by
obtaining more terms from the snippets.
We performed thorough evaluation for various
concepts involving 45 languages. The obtained
categories were manually verified with two human
judges and, when appropriate, automatically com-
pared to corresponding English WN synsets. In
all tested cases we discovered dozens of concept
terms with state-of-the-art precision.
Our major contribution is a novel framework for
concept translation across languages. This frame-
work utilizes web queries together with dictio-
naries for translation, disambiguation and exten-
sion of given terms. While our framework relies
on the existence of multilingual dictionaries, we
show that even with basic 1000 word dictionaries
we achieve good performance. Modest time and
data requirements allow the incorporation of our
method in practical applications.
In Section 2 we discuss related work, Section 3
details the algorithm, Section 4 describes the eval-
uation protocol and Section 5 presents our results.
2 Related work
Substantial efforts have been recently made to
manually construct and interconnect WN-like
databases for different languages (Pease et al,
2008; Charoenporn et al, 2007). Some stud-
ies (e.g., (Amasyali, 2005)) use semi-automated
methods based on language-specific heuristics and
dictionaries.
At the same time, much work has been done
on automatic lexical acquisition, and in particu-
lar, on the acquisition of concepts. The two main
algorithmic approaches are pattern-based discov-
ery, and clustering of context feature vectors. The
latter represents word contexts as vectors in some
space and use similarity measures and automatic
clustering in that space (Deerwester et al, 1990).
Pereira (1993), Curran (2002) and Lin (1998) use
syntactic features in the vector definition. (Pantel
and Lin, 2002) improves on the latter by cluster-
ing by committee. Caraballo (1999) uses conjunc-
tion and appositive annotations in the vector rep-
resentation. While a great effort has focused on
improving the computational complexity of these
methods (Gorman and Curran, 2006), they still re-
main data and computation intensive.
The current major algorithmic approach for
concept acquisition is to use lexico-syntactic pat-
terns. Patterns have been shown to produce more
accurate results than feature vectors, at a lower
computational cost on large corpora (Pantel et al,
2004). Since (Hearst, 1992), who used a manu-
ally prepared set of initial lexical patterns in order
to acquire relationships, numerous pattern-based
methods have been proposed for the discovery of
concepts from seeds (Pantel et al, 2004; Davidov
et al, 2007; Pasca et al, 2006). Most of these
studies were done for English, while some show
the applicability of their method to some other
languages including Russian, Greek, Czech and
French.
Many papers directly target specific applica-
tions, and build lexical resources as a side ef-
fect. Named Entity Recognition can be viewed
as an instance of the concept acquisition problem
where the desired categories contain words that
are names of entities of a particular kind, as done
in (Freitag, 2004) using co-clustering and in (Et-
zioni et al, 2005) using predefined pattern types.
Many Information Extraction papers discover re-
lationships between words using syntactic patterns
(Riloff and Jones, 1999).
Unlike in the majority of recent studies where
the acquisition framework is designed with spe-
cific languages in mind, in our task the algorithm
should be able to deal well with a wide variety
of target languages without any significant manual
adaptations. While some of the proposed frame-
works could potentially be language-independent,
little research has been done to confirm it yet.
176
There are a few obstacles that may hinder apply-
ing common pattern-based methods to other lan-
guages. Many studies utilize parsing or POS tag-
ging, which frequently depends on the availabil-
ity and quality of language-specific tools. Most
studies specify seed patterns in advance, and it is
not clear whether translated patterns can work well
on different languages. Also, the absence of clear
word segmentation in some languages (e.g., Chi-
nese) can make many methods inapplicable.
A few recently proposed concept acquisition
methods require only a handful of seed words
(Davidov et al, 2007; Pasca and Van Durme,
2008). While these studies avoid some of the ob-
stacles above, it still remains unconfirmed whether
such methods are indeed language-independent.
In the concept extension part of our algorithm we
adapt our concept acquisition framework (Davi-
dov and Rappoport, 2006; Davidov et al, 2007;
Davidov and Rappoport, 2008a; Davidov and
Rappoport, 2008b) to suit diverse languages, in-
cluding ones without explicit word segmentation.
In our evaluation we confirm the applicability of
the adapted methods to 45 languages.
Our study is related to cross-language infor-
mation retrieval (CLIR/CLEF) frameworks. Both
deal with information extracted from a set of lan-
guages. However, the majority of CLIR stud-
ies pursue different targets. One of the main
CLIR goals is the retrieval of documents based
on explicit queries, when the document lan-
guage is not the query language (Volk and Buite-
laar, 2002). These frameworks usually develop
language-specific tools and algorithms including
parsers, taggers and morphology analyzers in or-
der to integrate multilingual queries and docu-
ments (Jagarlamudi and Kumaran, 2007). Our
goal is to develop and evaluate a language-
independent method for the translation and exten-
sion of lexical categories. While our goals are dif-
ferent from CLIR, CLIR systems can greatly ben-
efit from our framework, since our translated cate-
gories can be directly utilized for subsequent doc-
ument retrieval.
Another field indirectly related to our research
is Machine Translation (MT). Many MT tasks re-
quire automated creation or improvement of dic-
tionaries (Koehn and Knight, 2001). However,
MT mainly deals with translation and disambigua-
tion of words at the sentence or document level,
while we translate whole concepts defined inde-
pendently of contexts. Our primary target is not
translation of given words, but the discovery and
extension of a concept in a target language when
the concept definition is given in some different
source language.
3 Cross-lingual Concept Translation
Framework
Our framework has three main stages: (1) given
a set of words in a source language as definition
for some concept, we automatically translate them
to the target language with multilingual dictionar-
ies, disambiguating translations using web counts;
(2) we retrieve from the web snippets where these
translations co-appear; (3) we apply a pattern-
based concept extension algorithm for discovering
additional terms from the retrieved data.
3.1 Concept words and sense selection
We start from a set of words denoting a category
in a source language. Thus we may use words
like (apple, banana, ...) as the definition of fruits
or (bear, wolf, fox, ...) as the definition of wild
animals2. Each of these words can be ambiguous.
Multilingual dictionaries usually provide many
translations, one or more for each sense. We need
to select the appropriate translation for each term.
In practice, some or even most of the category
terms may be absent in available dictionaries.
In these cases, we attempt to extract ?chain?
translations, i.e., if we cannot find Source?Target
translation, we can still find some indirect
Source?Intermediate1?Intermediate2?Target
paths. Such translations are generally much
more ambiguous, hence we allow up to two
intermediate languages in a chain. We collect all
possible translations at the chains having minimal
length, and skip category terms for whom this
process results in no translations.
Then we use the conjecture that terms of the
same concept tend to co-appear more frequently
than ones belonging to different concepts3. Thus,
2In order to reduce noise, we limit the length (in words)
of multiword expressions considered as terms. To calculate
this limit for a language we randomly take 100 terms from
the appropriate dictionary and set a limit as Limmwe =
round(avg(length(w))) where length(w) is the number of
words in term w. For languages like Chinese without inherent
word segmentation, length(w) is the number of characters in
w. While for many languages Limmwe = 1, some languages
like Vietnamese usually require two words or more to express
terms.
3Our results in this paper support this conjecture.
177
we select a translation of a term co-appearing
most frequently with some translation of a differ-
ent term of the same concept. We estimate how
well translations of different terms are connected
to each other. Let C = {Ci} be the given seed
words for some concept. Let Tr(Ci, n) be the
n-th available translation of word Ci and Cnt(s)
denote the web count of string s obtained by a
search engine. Then we select translation Tr(Ci)
according to:
F (w1, w2) =
Cnt(?w1 ? w2?)? Cnt(?w2 ? w1?)
Cnt(w1)? Cnt(w2)
Tr(Ci) =
argmax
si
(
max
sj
j 6=i
(F (Tr(Ci, si), T r(Cj , sj)))
)
We utilize the Y ahoo! ?x * y? wildcard that al-
lows to count only co-appearances where x and y
are separated by a single word. As a result, we ob-
tain a set of disambiguated term translations. The
number of queries in this stage depends on the am-
biguity of concept terms translation to the target
language. Unlike many existing disambiguation
methods based on statistics obtained from parallel
corpora, we take a rather simplistic query-based
approach. This approach is powerful (as shown
in our evaluation) and only relies on a few web
queries in a language independent manner.
3.2 Web mining for translation contexts
We need to restrict web mining to specific tar-
get languages. This restriction is straightforward
if the alphabet or term translations are language-
specific or if the search API supports restriction to
this language4. In case where there are no such
natural restrictions, we attempt to detect and add
to our queries a few language-specific frequent
words. Using our dictionaries, we find 1?3 of the
15 most frequent words in a desired language that
are unique to that language, and we ?and? them
with the queries to ensure selection of the proper
language. While some languages as Esperanto do
not satisfy any of these requirements, more than
60 languages do.
For each pair A, B of disambiguated term trans-
lations, we construct and execute the following 2
queries: {?A * B?, ?B * A?}5. When we have
3 or more terms we also add {A B C . . .}-like
conjunction queries which include 3?5 terms. For
languages with Limmwe > 1, we also construct
4Yahoo! allows restrictions for 42 languages.
5These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
queries with several ?*? wildcards between terms.
For each query we collect snippets containing text
fragments of web pages. Such snippets frequently
include the search terms. Since Y ahoo! allows re-
trieval of up to the 1000 first results (100 in each
query), we collect several thousands snippets. For
most of the target languages and categories, only a
few dozen queries (20 on the average) are required
to obtain sufficient data. Thus the relevant data
can be downloaded in seconds. This makes our
approach practical for on-demand retrieval tasks.
3.3 Pattern-based extension of concept terms
First we extract from the retrieved snippets con-
texts where translated terms co-appear, and de-
tect patterns where they co-appear symmetrically.
Then we use the detected patterns to discover ad-
ditional concept terms. In order to define word
boundaries, for each target language we manu-
ally specify boundary characters such as punctu-
ation/space symbols. This data, along with dic-
tionaries, is the only language-specific data in our
framework.
3.3.1 Meta-patterns
Following (Davidov et al, 2007) we seek symmet-
ric patterns to retrieve concept terms. We use two
meta-pattern types. First, a Two-Slot pattern type
constructed as follows:
[Prefix] C1 [Infix] C2 [Postfix]
Ci are slots for concept terms. We allow up to
Limmwe space-separated6 words to be in a sin-
gle slot. Infix may contain punctuation, spaces,
and up to Limmwe ? 4 words. Prefix and Post-
fix are limited to contain punctuation characters
and/or Limmwe words.
Terms of the same concept frequently co-appear
in lists. To utilize this, we introduce two additional
List pattern types7:
[Prefix] C1[Infix] (Ci[Infix])+ (1)
[Infix] (Ci[Infix])+ Cn [Postfix] (2)
As in (Widdows and Dorow, 2002; Davidov and
Rappoport, 2006), we define a pattern graph.
Nodes correspond to terms and patterns to edges.
If term pair (w1, w2) appears in pattern P , we add
nodes Nw1 , Nw2 to the graph and a directed edge
EP (Nw1 , Nw2) between them.
6As before, for languages without explicit space-based
word separation Limmwe limits the number of characters in-
stead.
7(X)+ means one or more instances of X .
178
3.3.2 Symmetric patterns
We consider only symmetric patterns. We define
a symmetric pattern as a pattern where some cate-
gory terms Ci, Cj appear both in left-to-right and
right-to-left order. For example, if we consider the
terms {apple, pineapple} we select a List pattern
?(one Ci, )+ and Cn.? if we find both ?one apple,
one pineapple, one guava and orange.? and ?one
watermelon, one pineapple and apple.?. If no such
patterns are found, we turn to a weaker definition,
considering as symmetric those patterns where the
same terms appear in the corpus in at least two dif-
ferent slots. Thus, we select a pattern ?for C1 and
C2? if we see both ?for apple and guava,? and ?for
orange and apple,?.
3.3.3 Retrieving concept terms
We collect terms in two stages. First, we obtain
?high-quality? core terms and then we retrieve po-
tentially more noisy ones. In the first stage we col-
lect all terms8 that are bidirectionally connected to
at least two different original translations, and call
them core concept terms Ccore. We also add the
original ones as core terms. Then we detect the
rest of the terms Crest that appear with more dif-
ferent Ccore terms than with ?out? (non-core) terms
as follows:
Gin(c)={w?Ccore|E(Nw, Nc) ? E(Nc, Nw)}
Gout(c)={w/?Ccore|E(Nw, Nc) ? E(Nc, Nw)}
Crest={c| |Gin(c)|>|Gout(c)| }
where E(Na, Nb) correspond to existence of a
graph edge denoting that translated terms a and b
co-appear in a pattern in this order. Our final term
set is the union of Ccore and Crest.
For the sake of simplicity, unlike in the ma-
jority of current research, we do not attempt to
discover more patterns/instances iteratively by re-
examining the data or re-querying the web. If we
have enough data, we use windowing to improve
result quality. If we obtain more than 400 snip-
pets for some concept, we randomly divide the
data into equal parts, each containing up to 400
snippets. We apply our algorithm independently
to each part and select only the words that appear
in more than one part.
4 Experimental Setup
We describe here the languages, concepts and dic-
tionaries we used in our experiments.
8We do not consider as terms the 50 most frequent words.
4.1 Languages and categories
One of the main goals in this research is to ver-
ify that the proposed basic method can be applied
to different languages unmodified. We examined
a wide variety of languages and concepts. Table
3 shows a list of 45 languages used in our experi-
ments, including west European languages, Slavic
languages, Semitic languages, and diverse Asian
languages.
Our concept set was based on English WN
synsets, while concept definitions for evaluation
were based on WN glosses. For automated evalua-
tion we selected as categories 150 synsets/subtrees
with at least 10 single-word terms in them. For
manual evaluation we used a subset of 24 of these
categories. In this subset we tried to select generic
categories, such that no domain expert knowledge
was required to check their correctness.
Ten of these categories were equal to ones used
in (Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006), which allowed us to indirectly
compare to recent work. Table 1 shows these 10
concepts along with the sample terms. While the
number of tested categories is still modest, it pro-
vides a good indication for the quality of our ap-
proach.
Concept Sample terms
Musical instruments guitar, flute, piano
Vehicles/transport train, bus, car
Academic subjects physics, chemistry, psychology
Body parts hand, leg, shoulder
Food egg, butter, bread
Clothes pants, skirt, jacket
Tools hammer, screwdriver, wrench
Places park, castle, garden
Crimes murder, theft, fraud
Diseases rubella, measles, jaundice
Table 1: 10 of the selected categories with sample terms.
4.2 Multilingual dictionaries
We developed a set of tools for automatic access
to several dictionaries. We used Wikipedia cross-
language links as our main source (60%) for of-
fline translation. These links include translation
of Wikipedia terms into dozens of languages. The
main advantage of using Wikipedia is its wide cov-
erage of concepts and languages. However, one
problem in using it is that it frequently encodes too
specific senses and misses common ones. Thus
bear is translated as family Ursidae missing its
common ?wild animal? sense. To overcome these
179
difficulties, we also used Wiktionary and comple-
mented these offline resources with a few auto-
mated queries to several (20) online dictionaries.
We start with Wikipedia definitions, then if not
found, Wiktionary, and then we turn to online dic-
tionaries.
5 Evaluation and Results
While there are numerous concept acquisition
studies, no framework has been developed so far
to evaluate this type of cross-lingual concept dis-
covery, limiting our ability to perform a meaning-
ful comparison to previous work. Fair estimation
of translated concept quality is a challenging task.
For most languages there are no widely accepted
concept databases. Moreover, the contents of the
same concept may vary across languages. Fortu-
nately, when English is taken as a target language,
the English WN allows an automated evaluation of
concepts. We conducted evaluation in three differ-
ent settings, mostly relying on human judges and
utilizing the English WN where possible.
1. English as source language. We applied our
algorithm on a subset of 24 categories using
each of the 45 languages as a target language.
Evaluation is done by two judges9.
2. English as target language. All other lan-
guages served as source languages. In this
case human subjects manually provided in-
put terms for 150 concept definitions in each
of the target languages using 150 selected
English WN glosses. For each gloss they
were requested to provide at least 2 terms.
Then we ran the algorithm on these term
lists. Since the obtained results were English
words, we performed both manual evaluation
of the 24 categories and automated compari-
son to the original WN data.
3. Language pairs. We created 10 different non-
English language pairs for the 24 concepts.
Concept definitions were the same as in (2)
and manual evaluation followed the same
protocol as in (1).
The absence of exhaustive term lists makes recall
estimation problematic. In all cases we assess the
quality of the discovered lists in terms of precision
(P ) and length of retrieved lists (T ).
9For 19 of the languages, at least one judge was a native
speaker. For other languages at least one of the subjects was
fluent with this language.
5.1 Manual evaluation
Each discovered concept was evaluated by two
judges. All judges were fluent English speakers
and for each target language, at least one was a flu-
ent speaker of this language. They were given one-
line English descriptions of each category and the
full lists obtained by our algorithm for each of the
24 concepts. Table 2 shows the lists obtained by
our algorithm for the category described as Rela-
tives (e.g., grandmother) for several language pairs
including Hebrew?French and Chinese?Czech.
We mixed ?noise? words into each list of terms10.
These words were automatically and randomly ex-
tracted from the same text. Subjects were re-
quired to select all words fitting the provided de-
scription. They were unaware of algorithm details
and desired results. They were instructed to ac-
cept common abbreviations, alternative spellings
or misspellings like yel
?
ow?color and to accept a
term as belonging to a category if at least one
of its senses belongs to it, like orange?color and
orange?fruit. They were asked to reject terms re-
lated or associated but not belonging to the target
category, like tasty/?food, or that are too general,
like animal/?dogs.
The first 4 columns of Table 3 show averaged
results of manual evaluation for 24 categories. In
the first two columns English is used as a source
language and in the next pair of columns English is
used as the target. In addition we display in paren-
theses the amount of terms added during the ex-
tension stage. We can see that for all languages,
average precision (% of correct terms in concept)
is above 80, and frequently above 90, and the aver-
age number of extracted terms is above 30. Inter-
nal concept quality is in line with values observed
on similarly evaluated tasks for recent concept ac-
quisition studies in English. As a baseline, only
3% of the inserted 20-40% noise words were in-
correctly labeled by judges. Due to space limita-
tion we do not show the full per-concept behavior;
all medians for P and T were close to the average.
We can also observe that the majority (> 60%)
of target language terms were obtained during the
extension stage. Thus, even when considering
translation from a rich language such as English
(where given concepts frequently contain dozens
of terms), most of the discovered target language
terms are not discovered through translation but
10To reduce annotator bias, we used a different number of
noise words, adding 20?40% of the original number of words.
180
English?Portuguese:
afilhada,afilhado,amigo,avo?,avo?,bisavo?,bisavo?,
bisneta,bisneto,co?njuge,cunhada,cunhado,companheiro,
descendente,enteado,filha,filho,irma?,irma?o,irma?os,irma?s,
madrasta,madrinha,ma?e,marido,mulher,namorada,
namorado,neta,neto,noivo,padrasto,pai,papai,parente,
prima,primo,sogra,sogro,sobrinha,sobrinho,tia,tio,vizinho
Hebrew?French:
amant,ami,amie,amis,arrie`re-grand-me`re,
arrie`re-grand-pe`re,beau-fre`re,beau-parent,beau-pe`re,bebe,
belle-fille,belle-me`re,belle-soeur,be`be`,compagnon,
concubin,conjoint,cousin,cousine,demi-fre`re,demi-soeur,
e?pouse,e?poux,enfant,enfants,famille,femme,fille,fils,foyer,
fre`re,garcon,grand-me`re,grand-parent,grand-pe`re,
grands-parents,maman,mari,me`re,neveu,nie`ce,oncle,
papa,parent,pe`re,petit-enfant,petit-fils,soeur,tante
English?Spanish:
abuela,abuelo,amante,amiga,amigo,confidente,bisabuelo,
cun?ada,cun?ado,co?nyuge,esposa,esposo,esp??ritu,familia,
familiar,hermana,hermano,hija,hijo,hijos,madre,marido,
mujer,nieta,nieto,nin?o, novia,padre,papa?,primo,sobrina,
sobrino,suegra,suegro,t??a,t??o,tutor, viuda,viudo
Chinese?Czech:
babic?ka,bratr,bra?cha,chlapec,dcera,de?da,de?dec?ek,druh,
kamara?d,kamara?dka,mama,manz?el,manz?elka,matka,
muz?,otec,podnajemnik,pr???telkyne?, sestra,stars???,stry?c,
stry?c?ek, syn,se?gra,tcha?n,tchyne?,teta,vnuk,vnuc?ka,z?ena
Table 2: Sample of results for the Relatives concept. Note
that precision is not 100% (e.g. the Portuguese set includes
?friend? and ?neighbor?).
during the subsequent concept extension. In fact,
brief examination shows that less than half of
source language terms successfully pass transla-
tion and disambiguation stage. However, more
than 80% of terms which were skipped due to lack
of available translations were re-discovered in the
target language during the extension stage, along
with the discovery of new correct terms not exist-
ing in the given source definition.
The first two columns of Table 4 show similar
results for non-English language pairs. We can see
that these results are only slightly inferior to the
ones involving English.
5.2 WordNet based evaluation
We applied our algorithm on 150 concepts with
English used as the target language. Since we
want to consider common misspellings and mor-
phological combinations of correct terms as hits,
we used a basic speller and stemmer to resolve
typos and drop some English endings. The WN
columns in Table 3 display P and T values for
this evaluation. In most cases we obtain > 85%
precision. While these results (P=87,T=17) are
lower than in manual evaluation, the task is much
harder due to the large number (and hence sparse-
ness) of the utilized 150 WN categories and the
incomplete nature of WN data. For the 10 cat-
egories of Table 1 used in previous work, we
have obtained (P=92,T=41) which outperforms
the seed-based concept acquisition of (Widdows
and Dorow, 2002; Davidov and Rappoport, 2006)
(P=90,T=35) on the same concepts. However, it
should be noted that our task setting is substan-
tially different since we utilize more seeds and
they come from languages different from English.
5.3 Effect of dictionary size and source
category size
The first stage in our framework heavily relies on
the existence and quality of dictionaries, whose
coverage may be insufficient. In order to check
the effect of dictionary coverage on our task, we
re-evaluated 10 language pairs using reduced dic-
tionaries containing only the 1000 most frequent
words. The last columns in Table 4 show evalu-
ation results for such reduced dictionaries. Sur-
prisingly, while we see a difference in coverage
and precision, this difference is below 8%, thus
even basic 1000-word dictionaries may be useful
for some applications.
This may suggest that only a few correct trans-
lations are required for successful discovery of
the corresponding category. Hence, even a small
dictionary containing translations of the most fre-
quent terms could be enough. In order to test
this hypothesis, we re-evaluated the 10 language
pairs using full dictionaries while reducing the
initial concept definition to the 3 most frequent
words. The results of this experiment are shown at
columns 3?4 of Table 4. We can see that for most
language pairs, 3 seeds were sufficient to achieve
equally good results, and providing more exten-
sive concept definitions had little effect on perfor-
mance.
5.4 Variance analysis
We obtained high precision. However, we also ob-
served high variance in the number of terms be-
tween different language pairs for the same con-
cept. There are many possible reasons for this out-
come. Below we briefly discuss some of them; de-
tailed analysis of inter-language and inter-concept
variance is a major target for future work.
Web coverage of languages is not uniform (Pao-
lillo et al, 2005); e.g. Georgian has much less
web hits than English. Indeed, we observed a cor-
relation between reported web coverage and the
number of retrieved terms. Concept coverage and
181
English English as target
Language as source
Manual Manual WN
T[xx] P T[xx] P T P
Arabic 29 [12] 90 41 [35] 91 17 87
Armenian 27 [21] 93 40 [32] 92 15 86
Afrikaans 40 [29] 89 51 [28] 86 19 85
Bengali 23 [18] 95 42 [34] 93 18 88
Belorussian 23 [15] 91 43 [30] 93 17 87
Bulgarian 46 [36] 85 58 [33] 87 19 83
Catalan 45 [29] 81 56 [46] 88 21 86
Chinese 47 [34] 87 56 [22] 90 22 89
Croatian 46 [26] 90 57 [35] 92 16 89
Czech 58 [40] 89 65 [39] 94 23 88
Danish 48 [35] 94 59 [38] 97 17 90
Dutch 41 [28] 92 60 [36] 94 20 88
Estonian 35 [21] 96 47 [24] 96 16 90
Finnish 34 [21] 88 47 [29] 90 19 85
French 56 [30] 89 61 [31] 93 17 87
Georgian 22 [15] 95 39 [31] 96 16 90
German 54 [32] 91 62 [34] 92 21 83
Greek 27 [16] 93 44 [30] 95 17 91
Hebrew 38 [28] 93 45 [32] 93 18 92
Hindi 30 [10] 92 46 [28] 93 16 86
Hungarian 43 [27] 90 44 [28] 93 15 87
Italian 45 [26] 89 51 [29] 88 16 81
Icelandic 27 [21] 90 39 [27] 92 15 85
Indonesian 33 [25] 96 49 [25] 95 15 90
Japanese 40 [16] 89 50 [22] 91 20 83
Kazakh 22 [14] 96 43 [36] 97 16 92
Korean 33 [15] 88 46 [29] 89 16 85
Latvian 41 [30] 92 55 [46] 90 19 83
Lithuanian 36 [26] 94 44 [35] 95 16 89
Norwegian 37 [25] 89 46 [29] 93 15 85
Persian 17 [6] 98 40 [29] 96 15 92
Polish 38 [25] 89 55 [36] 92 17 96
Portuguese 55 [34] 87 64 [33] 90 21 85
Romanian 46 [29] 93 56 [25] 96 15 91
Russian 58 [40] 91 65 [35] 92 22 84
Serbian 19 [11] 93 36 [30] 95 17 90
Slovak 32 [20] 89 56 [39] 90 15 87
Slovenian 28 [16] 94 43 [36] 95 18 89
Spanish 53 [37] 90 66 [32] 91 23 85
Swedish 52 [33] 89 62 [39] 93 16 87
Thai 26 [13] 95 41 [34] 97 16 92
Turkish 42 [33] 92 50 [25] 93 16 88
Ukrainian 47 [33] 88 54 [28] 88 16 83
Vietnamese 26 [8] 84 48 [25] 89 15 82
Urdu 27 [14] 84 42 [36] 88 14 82
Average 38 [24] 91 50 [32] 92 17 87
Table 3: Concept translation and extension results. The
first column shows the 45 tested languages. Bold are lan-
guages evaluated with at least one native speaker. P: preci-
sion, T: number of retrieved terms. ?[xx]?: number of terms
added during the concept extension stage. Columns 1-4 show
results for manual evaluation on 24 concepts. Columns 5-6
show automated WN-based evaluation on 150 concepts. For
columns 1-2 the input category is given in English, in other
columns English served as the target language.
content is also different for each language. Thus,
concepts involving fantasy creatures were found
to have little coverage in Arabic and Hindi, and
wide coverage in European languages. For ve-
hicles, Snowmobile was detected in Finnish and
Language pair Regular Reduced Reduced
Source-Target data seed dict.
T[xx] P T P T P
Hebrew-French 43[28] 89 39 90 35 87
Arabic-Hebrew 31[24] 90 25 94 29 82
Chinese-Czech 35[29] 85 33 84 25 75
Hindi-Russian 45[33] 89 45 87 38 84
Danish-Turkish 28[20] 88 24 88 24 80
Russian-Arabic 28[18] 87 19 91 22 86
Hebrew-Russian 45[31] 92 44 89 35 84
Thai-Hebrew 28[25] 90 26 92 23 78
Finnish-Arabic 21[11] 90 14 92 16 84
Greek-Russian 48[36] 89 47 87 35 81
Average 35[26] 89 32 89 28 82
Table 4: Results for non-English pairs. P: precision, T:
number of terms. ?[xx]?: number of terms added in the exten-
sion stage. Columns 1-2 show results for normal experiment
settings, 3-4 show data for experiments where the 3 most fre-
quent terms were used as concept definitions, 5-6 describe
results for experiment with 1000-word dictionaries.
Swedish while Rickshaw appears in Hindi.
Morphology was completely neglected in this
research. To co-appear in a text, terms frequently
have to be in a certain form different from that
shown in dictionaries. Even in English, plurals
like spoons, forks co-appear more than spoon,
fork. Hence dictionaries that include morphol-
ogy may greatly improve the quality of our frame-
work. We have conducted initial experiments with
promising results in this direction, but we do not
report them here due to space limitations.
6 Conclusions
We proposed a framework that when given a set
of terms for a category in some source language
uses dictionaries and the web to retrieve a similar
category in a desired target language. We showed
that the same pattern-based method can success-
fully extend dozens of different concepts for many
languages with high precision. We observed that
even when we have very few ambiguous transla-
tions available, the target language concept can
be discovered in a fast and precise manner with-
out relying on any language-specific preprocess-
ing, databases or parallel corpora. The average
concept total processing time, including all web
requests, was below 2 minutes11. The short run-
ning time and the absence of language-specific re-
quirements allow processing queries within min-
utes and makes it possible to apply our method to
on-demand cross-language concept mining.
11We used a single PC with ADSL internet connection.
182
References
M. Fatih Amasyali, 2005. Automatic Construction of
Turkish WordNet. Signal Processing and Commu-
nications Applications Conference.
Sharon Caraballo, 1999. Automatic Construction of
a Hypernym-Labeled Noun Hierarchy from Text.
ACL ?99.
Thatsanee Charoenporn, Virach Sornlertlamvanich,
Chumpol Mokarat, Hitoshi Isahara, 2008. Semi-
Automatic Compilation of Asian WordNet. Pro-
ceedings of the 14th NLP-2008, University of Tokyo,
Komaba Campus, Japan.
James R. Curran, Marc Moens, 2002. Improvements
in Automatic Thesaurus Extraction. SIGLEX ?02,
59?66.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008a. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08.
Dmitry Davidov, Ari Rappoport, 2008b. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ?08.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey, S.
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, Alexander Yates, 2005.
Unsupervised Named-Entity Extraction from the
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91134.
Dayne Freitag, 2004. Trained Named Entity Recogni-
tion Using Distributional lusters. EMNLP ?04.
James Gorman , James R. Curran, 2006. Scaling Dis-
tributional Similarity to Large Corpora COLING-
ACL ?06.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Jagadeesh Jagarlamudi, A Kumaran, 2007. Cross-
Lingual Information Retrieval System for Indian
Languages Working Notes for the CLEF 2007 Work-
shop.
Philipp Koehn, Kevin Knight, 2001. Knowl-
edge Sources for Word-Level Translation Models.
EMNLP ?01.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Margaret Matlin, 2005. Cognition, 6th edition. John
Wiley & Sons.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
John Paolillo, Daniel Pimienta, Daniel Prado, et al,
2005. Measuring Linguistic Diversity on the In-
ternet. UNESCO Institute for Statistics Montreal,
Canada.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and Similari-
ties on the Web: Fact Extraction in the Fast Lane.
COLING-ACL ?06.
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
Supervised Acquisition of Open-Domain Classes
and Class Attributes from Web Documents and
Query Logs. ACL ?08.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the Global WordNet Grid. CIL18.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
?93.
Ellen Riloff, Rosie Jones, 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. AAAI ?99.
Martin Volk, Paul Buitelaar, 2002. A Systematic Eval-
uation of Concept-Based Cross-Language Informa-
tion Retrieval in the Medical Domain. In: Proc. of
3rd Dutch-Belgian Information Retrieval Workshop.
Leuven.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
183
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 297?304,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Unsupervised Discovery of Word Categories
Using Symmetric Patterns and High Frequency Words
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem 91904, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
www.cs.huji.ac.il/?arir
Abstract
We present a novel approach for discov-
ering word categories, sets of words shar-
ing a significant aspect of their mean-
ing. We utilize meta-patterns of high-
frequency words and content words in or-
der to discover pattern candidates. Sym-
metric patterns are then identified using
graph-based measures, and word cate-
gories are created based on graph clique
sets. Our method is the first pattern-based
method that requires no corpus annota-
tion or manually provided seed patterns
or words. We evaluate our algorithm on
very large corpora in two languages, us-
ing both human judgments and WordNet-
based evaluation. Our fully unsupervised
results are superior to previous work that
used a POS tagged corpus, and computa-
tion time for huge corpora are orders of
magnitude faster than previously reported.
1 Introduction
Lexical resources are crucial in most NLP tasks
and are extensively used by people. Manual com-
pilation of lexical resources is labor intensive, er-
ror prone, and susceptible to arbitrary human deci-
sions. Hence there is a need for automatic author-
ing that would be as unsupervised and language-
independent as possible.
An important type of lexical resource is that
given by grouping words into categories. In gen-
eral, the notion of a category is a fundamental one
in cognitive psychology (Matlin, 2005). A lexi-
cal category is a set of words that share a signif-
icant aspect of their meaning, e.g., sets of words
denoting vehicles, types of food, tool names, etc.
A word can obviously belong to more than a single
category. We will use ?category? instead of ?lexi-
cal category? for brevity1.
Grouping of words into categories is useful in it-
self (e.g., for the construction of thesauri), and can
serve as the starting point in many applications,
such as ontology construction and enhancement,
discovery of verb subcategorization frames, etc.
Our goal in this paper is a fully unsupervised
discovery of categories from large unannotated
text corpora. We aim for categories containing sin-
gle words (multi-word lexical items will be dealt
with in future papers.) Our approach is based on
patterns, and utilizes the following stages:
1. Discovery of a set of pattern candidates that
might be useful for induction of lexical re-
lationships. We do this in a fully unsuper-
vised manner, using meta-patterns comprised
of high frequency words and content words.
2. Identification of pattern candidates that give
rise to symmetric lexical relationships. This
is done using simple measures in a word re-
lationship graph.
3. Usage of a novel graph clique-set alorithm
in order to generate categories from informa-
tion on the co-occurrence of content words in
the symmetric patterns.
We performed a thorough evaluation on two En-
glish corpora (the BNC and a 68GB web corpus)
and on a 33GB Russian corpus, and a sanity-check
test on smaller Danish, Irish and Portuguese cor-
pora. Evaluations were done using both human
1Some people use the term ?concept?. We adhere to the
cognitive psychology terminology, in which ?concept? refers
to the mental representation of a category (Matlin, 2005).
297
judgments and WordNet in a setting quite simi-
lar to that done (for the BNC) in previous work.
Our unsupervised results are superior to previous
work that used a POS tagged corpus, are less lan-
guage dependent, and are very efficient computa-
tionally2 .
Patterns are a common approach in lexical ac-
quisition. Our approach is novel in several as-
pects: (1) we discover patterns in a fully unsu-
pervised manner, as opposed to using a manually
prepared pattern set, pattern seed or words seeds;
(2) our pattern discovery requires no annotation of
the input corpus, as opposed to requiring POS tag-
ging or partial or full parsing; (3) we discover gen-
eral symmetric patterns, as opposed to using a few
hard-coded ones such as ?x and y?; (4) the clique-
set graph algorithm in stage 3 is novel. In addition,
we demonstrated the relatively language indepen-
dent nature of our approach by evaluating on very
large corpora in two languages3 .
Section 2 surveys previous work. Section 3 de-
scribes pattern discovery, and Section 4 describes
the formation of categories. Evaluation is pre-
sented in Section 5, and a discussion in Section 6.
2 Previous Work
Much work has been done on lexical acquisition
of all sorts. The three main distinguishing axes are
(1) the type of corpus annotation and other human
input used; (2) the type of lexical relationship tar-
geted; and (3) the basic algorithmic approach. The
two main approaches are pattern-based discovery
and clustering of context feature vectors.
Many of the papers cited below aim at the con-
struction of hyponym (is-a) hierarchies. Note that
they can also be viewed as algorithms for category
discovery, because a subtree in such a hierarchy
defines a lexical category.
A first major algorithmic approach is to repre-
sent word contexts as vectors in some space and
use similarity measures and automatic clustering
in that space (Curran and Moens, 2002). Pereira
(1993) and Lin (1998) use syntactic features in the
vector definition. (Pantel and Lin, 2002) improves
on the latter by clustering by committee. Cara-
ballo (1999) uses conjunction and appositive an-
notations in the vector representation.
2We did not compare against methods that use richer syn-
tactic information, both because they are supervised and be-
cause they are much more computationally demanding.
3We are not aware of any multilingual evaluation previ-
ously reported on the task.
The only previous works addressing our prob-
lem and not requiring any syntactic annotation are
those that decompose a lexically-defined matrix
(by SVD, PCA etc), e.g. (Schu?tze, 1998; Deer-
wester et al 1990). Such matrix decomposition
is computationally heavy and has not been proven
to scale well when the number of words assigned
to categories grows.
Agglomerative clustering (e.g., (Brown et al
1992; Li, 1996)) can produce hierarchical word
categories from an unannotated corpus. However,
we are not aware of work in this direction that has
been evaluated with good results on lexical cate-
gory acquisition. The technique is also quite de-
manding computationally.
The second main algorithmic approach is to
use lexico-syntactic patterns. Patterns have been
shown to produce more accurate results than fea-
ture vectors, at a lower computational cost on large
corpora (Pantel et al 2004). Hearst (1992) uses a
manually prepared set of initial lexical patterns in
order to discover hierarchical categories, and uti-
lizes those categories in order to automatically dis-
cover additional patterns.
(Berland and Charniak, 1999) use hand crafted
patterns to discover part-of (meronymy) relation-
ships, and (Chklovski and Pantel, 2004) discover
various interesting relations between verbs. Both
use information obtained by parsing. (Pantel et al
2004) reduce the depth of the linguistic data used
but still requires POS tagging.
Many papers directly target specific applica-
tions, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an in-
stance of our problem where the desired categories
contain words that are names of entities of a par-
ticular kind, as done in (Freitag, 2004) using co-
clustering. Many Information Extraction papers
discover relationships between words using syn-
tactic patterns (Riloff and Jones, 1999).
(Widdows and Dorow, 2002; Dorow et al 2005)
discover categories using two hard-coded symmet-
ric patterns, and are thus the closest to us. They
also introduce an elegant graph representation that
we adopted. They report good results. However,
they require POS tagging of the corpus, use only
two hard-coded patterns (?x and y?, ?x or y?), deal
only with nouns, and require non-trivial computa-
tions on the graph.
A third, less common, approach uses set-
theoretic inference, for example (Cimiano et al
298
2005). Again, that paper uses syntactic informa-
tion.
In summary, no previous work has combined
the accuracy, scalability and performance advan-
tages of patterns with the fully unsupervised,
unannotated nature possible with clustering ap-
proaches. This severely limits the applicability
of previous work on the huge corpora available at
present.
3 Discovery of Patterns
Our first step is the discovery of patterns that are
useful for lexical category acquisition. We use two
main stages: discovery of pattern candidates, and
identification of the symmetric patterns among the
candidates.
3.1 Pattern Candidates
An examination of the patterns found useful in
previous work shows that they contain one or more
very frequent word, such as ?and?, ?is?, etc. Our
approach towards unsupervised pattern induction
is to find such words and utilize them.
We define a high frequency word (HFW) as a
word appearing more than TH times per million
words, and a content word (CW) as a word appear-
ing less than TC times per a million words4.
Now define a meta-pattern as any sequence of
HFWs and CWs. In this paper we require that
meta-patterns obey the following constraints: (1)
at most 4 words; (2) exactly two content words; (3)
no two consecutive CWs. The rationale is to see
what can be achieved using relatively short pat-
terns and where the discovered categories contain
single words only. We will relax these constraints
in future papers. Our meta-patterns here are thus
of four types: CHC, CHCH, CHHC, and HCHC.
In order to focus on patterns that are more likely
to provide high quality categories, we removed
patterns that appear in the corpus less than TP
times per million words. Since we can ensure that
the number of HFWs is bounded, the total number
of pattern candidates is bounded as well. Hence,
this stage can be computed in time linear in the
size of the corpus (assuming the corpus has been
already pre-processed to allow direct access to a
word by its index.)
4Considerations for the selection of thresholds are dis-
cussed in Section 5.
3.2 Symmetric Patterns
Many of the pattern candidates discovered in the
previous stage are not usable. In order to find a us-
able subset, we focus on the symmetric patterns.
Our rationale is that two content-bearing words
that appear in a symmetric pattern are likely to
be semantically similar in some sense. This sim-
ple observation turns out to be very powerful, as
shown by our results. We will eventually combine
data from several patterns and from different cor-
pus windows (Section 4.)
For identifying symmetric patterns, we use a
version of the graph representation of (Widdows
and Dorow, 2002). We first define the single-
pattern graph G(P ) as follows. Nodes corre-
spond to content words, and there is a directed arc
A(x, y) from node x to node y iff (1) the words x
and y both appear in an instance of the pattern P
as its two CWs; and (2) x precedes y in P . Denote
by Nodes(G), Arcs(G) the nodes and arcs in a
graph G, respectively.
We now compute three measures on G(P ) and
combine them for all pattern candidates to filter
asymmetric ones. The first measure (M1) counts
the proportion of words that can appear in both
slots of the pattern, out of the total number of
words. The reasoning here is that if a pattern al-
lows a large percentage of words to participate in
both slots, its chances of being a symmetric pat-
tern are greater:
M1 :=
|{x|?yA(x, y) ? ?zA(z, x)}|
|Nodes(G(P ))|
M1 filters well patterns that connect words hav-
ing different parts of speech. However, it may
fail to filter patterns that contain multiple levels
of asymmetric relationships. For example, in the
pattern ?x belongs to y?, we may find a word B
on both sides (?A belongs to B?, ?B belongs to C?)
while the pattern is still asymmetric.
In order to detect symmetric relationships in a
finer manner, for the second and third measures
we define SymG(P ), the symmetric subgraph of
G(P ), containing only the bidirectional arcs and
nodes of G(P ):
SymG(P ) = {{x}, {(x, y)}|A(x, y) ? A(y, x)}
The second and third measures count the pro-
portion of the number of symmetric nodes and
edges in G(P ), respectively:
M2 :=
|Nodes(SymG(P ))|
|Nodes(G(P ))|
299
M3 :=
|Arcs(SymG(P ))|
|Arcs(G(P ))|
All three measures yield values in [0, 1], and
in all three a higher value indicates more symme-
try. M2 and M3 are obviously correlated, but they
capture different aspects of a pattern?s nature: M3
is informative for highly interconnected but small
word categories (e.g., month names), while M2 is
useful for larger categories that are more loosely
connected in the corpus.
We use the three measures as follows. For each
measure, we prepare a sorted list of all candidate
patterns. We remove patterns that are not in the
top ZT (we use 100, see Section 5) in any of the
three lists, and patterns that are in the bottom ZB
in at least one of the lists. The remaining patterns
constitute our final list of symmetric patterns.
We do not rank the final list, since the category
discovery algorithm of the next section does not
need such a ranking. Defining and utilizing such a
ranking is a subject for future work.
A sparse matrix representation of each graph
can be computed in time linear in the size of the in-
put corpus, since (1) the number of patterns |P | is
bounded, (2) vocabulary size |V | (the total number
of graph nodes) is much smaller than corpus size,
and (3) the average node degree is much smaller
than |V | (in practice, with the thresholds used, it
is a small constant.)
4 Discovery of Categories
After the end of the previous stage we have a set
of symmetric patterns. We now use them in order
to discover categories. In this section we describe
the graph clique-set method for generating initial
categories, and category pruning techniques for in-
creased quality.
4.1 The Clique-Set Method
Our approach to category discovery is based on
connectivity structures in the all-pattern word rela-
tionship graph G, resulting from merging all of the
single-pattern graphs into a single unified graph.
The graph G can be built in time O(|V | ? |P | ?
AverageDegree(G(P ))) = O(|V |) (we use V
rather than Nodes(G) for brevity.)
When building G, no special treatment is done
when one pattern is contained within another. For
example, any pattern of the form CHC is contained
in a pattern of the form HCHC (?x and y?, ?both x
and y?.) The shared part yields exactly the same
subgraph. This policy could be changed for a dis-
covery of finer relationships.
The main observation on G is that words that
are highly interconnected are good candidates to
form a category. This is the same general obser-
vation exploited by (Widdows and Dorow, 2002),
who try to find graph regions that are more con-
nected internally than externally.
We use a different algorithm. We find all strong
n-cliques (subgraphs containing n nodes that are
all bidirectionally interconnected.) A clique Q de-
fines a category that contains the nodes in Q plus
all of the nodes that are (1) at least unidirectionally
connected to all nodes in Q, and (2) bidirectionally
connected to at least one node in Q.
In practice we use 2-cliques. The strongly con-
nected cliques are the bidirectional arcs in G and
their nodes. For each such arc A, a category is gen-
erated that contains the nodes of all triangles that
contain A and at least one additional bidirectional
arc. For example, suppose the corpus contains the
text fragments ?book and newspaper?, ?newspaper
and book?, ?book and note?, ?note and book? and
?note and newspaper?. In this case the three words
are assigned to a category.
Note that a pair of nodes connected by a sym-
metric arc can appear in more than a single cate-
gory. For example, suppose a graph G containing
five nodes and seven arcs that define exactly three
strongly connected triangles, ABC,ABD,ACE.
The arc (A,B) yields a category {A,B,C,D},
and the arc (A,C) yields a category {A,C,B,E}.
Nodes A and C appear in both categories. Cate-
gory merging is described below.
This stage requires an O(1) computation for
each bidirectional arc of each node, so its com-
plexity is O(|V | ? AverageDegree(G)) =
O(|V |).
4.2 Enhancing Category Quality: Category
Merging and Corpus Windowing
In order to cover as many words as possible, we
use the smallest clique, a single symmetric arc.
This creates redundant categories. We enhance the
quality of the categories by merging them and by
windowing on the corpus.
We use two simple merge heuristics. First,
if two categories are identical we treat them as
one. Second, given two categories Q,R, we merge
them iff there?s more than a 50% overlap between
them: (|Q ? R| > |Q|/2) ? (|Q ? R| > |R|/2).
300
This could be added to the clique-set stage, but the
phrasing above is simpler to explain and imple-
ment.
In order to increase category quality and re-
move categories that are too context-specific, we
use a simple corpus windowing technique. In-
stead of running the algorithm of this section on
the whole corpus, we divide the corpus into win-
dows of equal size (see Section 5 for size deter-
mination) and perform the category discovery al-
gorithm of this section on each window indepen-
dently. Merging is also performed in each win-
dow separately. We now have a set of categories
for each window. For the final set, we select only
those categories that appear in at least two of the
windows. This technique reduces noise at the po-
tential cost of lowering coverage. However, the
numbers of categories discovered and words they
contain is still very large (see Section 5), so win-
dowing achieves higher precision without hurting
coverage in practice.
The complexity of the merge stage is O(|V |)
times the average number of categories per word
times the average number of words per category.
The latter two are small in practice, so complexity
amounts to O(|V |).
5 Evaluation
Lexical acquisition algorithms are notoriously
hard to evaluate. We have attempted to be as
thorough as possible, using several languages and
both automatic and human evaluation. In the auto-
matic part, we followed as closely as possible the
methodology and data used in previous work, so
that meaningful comparisons could be made.
5.1 Languages and Corpora
We performed in-depth evaluation on two lan-
guages, English and Russian, using three cor-
pora, two for English and one for Russian. The
first English corpus is the BNC, containing about
100M words. The second English corpus, Dmoz
(Gabrilovich and Markovitch, 2005), is a web cor-
pus obtained by crawling and cleaning the URLs
in the Open Directory Project (dmoz.org), result-
ing in 68GB containing about 8.2G words from
50M web pages.
The Russian corpus was assembled from many
web sites and carefully filtered for duplicates, to
yield 33GB and 4G words. It is a varied corpus
comprising literature, technical texts, news, news-
groups, etc.
As a preliminary sanity-check test we also ap-
plied our method to smaller corpora in Danish,
Irish and Portuguese, and noted some substantial
similarities in the discovered patterns. For exam-
ple, in all 5 languages the pattern corresponding to
?x and y? was among the 50 selected.
5.2 Thresholds, Statistics and Examples
The thresholds TH , TC , TP , ZT , ZB , were deter-
mined by memory size considerations: we com-
puted thresholds that would give us the maximal
number of words, while enabling the pattern ac-
cess table to reside in main memory. The resulting
numbers are 100, 50, 20, 100, 100.
Corpus window size was determined by starting
from a very small window size, defining at ran-
dom a single window of that size, running the al-
gorithm, and iterating this process with increased
window sizes until reaching a desired vocabulary
category participation percentage (i.e., x% of the
different words in the corpus assigned into cate-
gories. We used 5%.) This process has only a
negligible effect on running times, because each
iteration is run only on a single window, not on
the whole corpus.
The table below gives some statistics. V is the
total number of different words in the corpus. W
is the number of words belonging to at least one
of our categories. C is the number of categories
(after merging and windowing.) AS is the aver-
age category size. Running times are in minutes
on a 2.53Ghz Pentium 4 XP machine with 1GB
memory. Note how small they are, when com-
pared to (Pantel et al 2004), which took 4 days
for a smaller corpus using the same CPU.
V W C AS Time
Dmoz 16M 330K 142K 12.8 93m
BNC 337K 25K 9.6K 10.2 6.8m
Russian 10M 235K 115K 11.6 60m
Among the patterns discovered are the ubiqui-
tous ?x and y?, ?x or y? and many patterns con-
taining them. Additional patterns include ?from x
to y?, ?x and/or y? (punctuation is treated here as
white space), ?x and a y?, and ?neither x nor y?.
We discover categories of different parts of
speech. Among the noun ones, there are many
whose precision is 100%: 37 countries, 18 lan-
guages, 51 chemical elements, 62 animals, 28
types of meat, 19 fruits, 32 university names, etc.
A nice verb category example is {dive, snorkel,
swim, float, surf, sail, canoe, kayak, paddle, tube,
drift}. A nice adjective example is {amazing,
301
awesome, fascinating, inspiring, inspirational, ex-
citing, fantastic, breathtaking, gorgeous.}
5.3 Human Judgment Evaluation
The purpose of the human evaluation was dual: to
assess the quality of the discovered categories in
terms of precision, and to compare with those ob-
tained by a baseline clustering algorithm.
For the baseline, we implemented k-means as
follows. We have removed stopwords from the
corpus, and then used as features the words which
appear before or after the target word. In the calcu-
lation of feature values and inter-vector distances,
and in the removal of less informative features, we
have strictly followed (Pantel and Lin, 2002). We
ran the algorithm 10 times using k = 500 with
randomly selected centroids, producing 5000 clus-
ters. We then merged the resulting clusters us-
ing the same 50% overlap criterion as in our algo-
rithm. The result included 3090, 2116, and 3206
clusters for Dmoz, BNC and Russian respectively.
We used 8 subjects for evaluation of the English
categories and 15 subjects for evaluation of the
Russian ones. In order to assess the subjects? re-
liability, we also included random categories (see
below.)
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were generated by random selec-
tion of content words from the same window in
the corpus.
In Part II, subjects were given the full categories
of the triplets that were graded as 1 or 2 in Part I
(that is, the full ?good? categories in terms of shar-
ing of meaning.) They were asked to grade the
categories from 1 (worst) to 10 (best) according to
how much the full category had met the expecta-
tions they had when seeing only the triplet.
Results are given in Table 1. The first line gives
the average percentage of triplets that were given
scores of 1 or 2 (that is, ?significant shared mean-
ing?.) The 2nd line gives the average score of
a triplet (1 is best.) In these lines scores of 5
were not counted. The 3rd line gives the average
score given to a full category (10 is best.) Inter-
evaluator Kappa between scores 1,2 and 3,4 was
0.56, 0.67 and 0.72 for Dmoz, BNC and Russian
respectively.
Our algorithm clearly outperforms k-means,
which outperforms random. We believe that the
Russian results are better because the percentage
of native speakers among our subjects for Russian
was larger than that for English.
5.4 WordNet-Based Evaluation
The major guideline in this part of the evalua-
tion was to compare our results with previous
work having a similar goal (Widdows and Dorow,
2002). We have followed their methodology as
best as we could, using the same WordNet (WN)
categories and the same corpus (BNC) in addition
to the Dmoz and Russian corpora5 .
The evaluation method is as follows. We took
the exact 10 WN subsets referred to as ?subjects?
in (Widdows and Dorow, 2002), and removed all
multi-word items. We now selected at random 10
pairs of words from each subject. For each pair,
we found the largest of our discovered categories
containing it (if there isn?t one, we pick another
pair. This is valid because our Recall is obviously
not even close to 100%, so if we did not pick an-
other pair we would seriously harm the validity of
the evaluation.) The various morphological forms
of the same word were treated as one during the
evaluation.
The only difference from the (Widdows and
Dorow, 2002) experiment is the usage of pairs
rather than single words. We did this in order to
disambiguate our categories. This was not needed
in (Widdows and Dorow, 2002) because they had
directly accessed the word graph, which may be
an advantage in some applications.
The Russian evaluation posed a bit of a prob-
lem because the Russian WordNet is not readily
available and its coverage is rather small. Fortu-
nately, the subject list is such that WordNet words
5(Widdows and Dorow, 2002) also reports results for an
LSA-based clustering algorithm that are vastly inferior to the
pattern-based ones.
302
Dmoz BNC Russian
us k-means random us k-means random us k-means random
avg ?shared meaning? (%) 80.53 18.25 1.43 86.87 8.52 0.00 95.00 18.96 7.33
avg triplet score (1-4) 1.74 3.34 3.88 1.56 3.61 3.94 1.34 3.32 3.76
avg category score (1-10) 9.27 4.00 1.8 9.31 4.50 0.00 8.50 4.66 3.32
Table 1: Results of evaluation by human judgment of three data sets (ours, that obtained by k-means, and
random categories) on the three corpora. See text for detailed explanations.
could be translated unambiguously to Russian and
words in our discovered categories could be trans-
lated unambiguously into English. This was the
methodology taken.
For each found category C containing N words,
we computed the following (see Table 2): (1) Pre-
cision: the number of words present in both C and
WN divided by N ; (2) Precision*: the number of
correct words divided by N . Correct words are ei-
ther words that appear in the WN subtree, or words
whose entry in the American Heritage Dictionary
or the Britannica directly defines them as belong-
ing to the given class (e.g., ?keyboard? is defined
as ?a piano?; ?mitt? is defined by ?a type of glove?.)
This was done in order to overcome the relative
poorness of WordNet; (3) Recall: the number of
words present in both C and WN divided by the
number of (single) words in WN; (4) The num-
ber of correctly discovered words (New) that are
not in WN. The Table also shows the number of
WN words (:WN), in order to get a feeling by how
much WN could be improved here. For each sub-
ject, we show the average over the 10 randomly
selected pairs.
Table 2 also shows the average of each measure
over the subjects, and the two precision measures
when computed on the total set of WN words. The
(uncorrected) precision is the only metric given in
(Widdows and Dorow, 2002), who reported 82%
(for the BNC.) Our method gives 90.47% for this
metric on the same corpus.
5.5 Summary
Our human-evaluated and WordNet-based results
are better than the baseline and previous work re-
spectively. Both are also of good standalone qual-
ity. Clearly, evaluation methodology for lexical
acquisition tasks should be improved, which is an
interesting research direction in itself.
Examining our categories at random, we found
a nice example that shows how difficult it is to
evaluate the task and how useful automatic cate-
gory discovery can be, as opposed to manual def-
inition. Consider the following category, discov-
ered in the Dmoz corpus: {nightcrawlers, chicken,
shrimp, liver, leeches}. We did not know why
these words were grouped together; if asked in an
evaluation, we would give the category a very low
score. However, after some web search, we found
that this is a ?fish bait? category, especially suitable
for catfish.
6 Discussion
We have presented a novel method for pattern-
based discovery of lexical semantic categories.
It is the first pattern-based lexical acquisition
method that is fully unsupervised, requiring no
corpus annotation or manually provided patterns
or words. Pattern candidates are discovered us-
ing meta-patterns of high frequency and content
words, and symmetric patterns are discovered us-
ing simple graph-theoretic measures. Categories
are generated using a novel graph clique-set alo-
rithm. The only other fully unsupervised lexical
category acquisition approach is based on decom-
position of a matrix defined by context feature vec-
tors, and it has not been shown to scale well yet.
Our algorithm was evaluated using both human
judgment and automatic comparisons with Word-
Net, and results were superior to previous work
(although it used a POS tagged corpus) and more
efficient computationally. Our algorithm is also
easy to implement.
Computational efficiency and specifically lack
of annotation are important criteria, because they
allow usage of huge corpora, which are presently
becoming available and growing in size.
There are many directions to pursue in the fu-
ture: (1) support multi-word lexical items; (2) in-
crease category quality by improved merge algo-
rithms; (3) discover various relationships (e.g., hy-
ponymy) between the discovered categories; (4)
discover finer inter-word relationships, such as
verb selection preferences; (5) study various prop-
erties of discovered patterns in a detailed manner;
and (6) adapt the algorithm to morphologically
rich languages.
303
Subject Prec. Prec.* Rec. New:WN
Dmoz
instruments 79.25 89.34 34.54 7.2:163
vehicles 80.17 86.84 18.35 6.3:407
academic 78.78 89.32 30.83 15.5:396
body parts 73.85 79.29 5.95 9.1:1491
foodstuff 83.94 90.51 28.41 26.3:1209
clothes 83.41 89.43 10.65 4.5:539
tools 83.99 89.91 21.69 4.3:219
places 76.96 84.45 25.82 6.3:232
crimes 76.32 86.99 31.86 4.7:102
diseases 81.33 88.99 19.58 6.8:332
set avg 79.80 87.51 22.77 9.1:509
all words 79.32 86.94
BNC
instruments 92.68 95.43 9.51 0.6:163
vehicles 94.16 95.23 3.81 0.2:407
academic 93.45 96.10 12.02 0.6:396
body parts 96.38 97.60 0.97 0.3:1491
foodstuff 93.76 94.36 3.60 0.6:1209
cloths 93.49 94.90 4.04 0.3:539
tools 96.84 97.24 6.67 0.1:219
places 87.88 97.25 6.42 1.5:232
crimes 83.79 91.99 19.61 2.6:102
diseases 95.16 97.14 5.54 0.5:332
set avg 92.76 95.72 7.22 0.73:509
all words 90.47 93.80
Russian
instruments 82.46 89.09 25.28 3.4:163
vehicles 83.16 89.58 16.31 5.1:407
academic 87.27 92.92 15.71 4.9:396
body parts 81.42 89.68 3.94 8.3:1491
foodstuff 80.34 89.23 13.41 24.3:1209
clothes 82.47 87.75 15.94 5.1:539
tools 79.69 86.98 21.14 3.7:219
places 82.25 90.20 33.66 8.5:232
crimes 84.77 93.26 34.22 3.3:102
diseases 80.11 87.70 20.69 7.7:332
set avg 82.39 89.64 20.03 7.43:509
all words 80.67 89.17
Table 2: WordNet evaluation. Note the BNC ?all
words? precision of 90.47%. This metric was re-
ported to be 82% in (Widdows and Dorow, 2002).
It should be noted that our algorithm can be
viewed as one for automatic discovery of word
senses, because it allows a word to participate in
more than a single category. When merged prop-
erly, the different categories containing a word can
be viewed as the set of its senses. We are planning
an evaluation according to this measure after im-
proving the merge stage.
References
Matthew Berland and Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Peter Brown, Vincent Della Pietra, Peter deSouza,
Jenifer Lai, Robert Mercer, 1992. Class-based n-
gram models for natural language. Comp. Linguis-
tics, 18(4):468?479.
Sharon Caraballo, 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. ACL
?99.
Timothy Chklovski, Patrick Pantel, 2004. VerbOcean:
mining the web for fine-grained semantic verb rela-
tions. EMNLP ?04.
Philipp Cimiano, Andreas Hotho, Steffen Staab, 2005.
Learning concept hierarchies from text corpora us-
ing formal concept analysis. J. of Artificial Intelli-
gence Research, 24:305?339.
James Curran, Marc Moens, 2002. Improvements in
automatic thesaurus extraction. ACL Workshop on
Unsupervised Lexical Acquisition, 2002.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. Index-
ing by latent semantic analysis. J. of the American
Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in graphs for
lexical acquisition and word sense discrimination.
MEANING ?05.
Dayne Freitag, 2004. Trained named entity recognition
using distributional clusters. EMNLP ?04.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture generation for text categorization using world
knowledge. IJCAI ?05.
Marti Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Hang Li, Naoki Abe, 1996. Clustering words with the
MDL principle. COLING ?96.
Dekang Lin, 1998. Automatic retrieval and clustering
of similar words. COLING ?98.
Margaret Matlin, 2005. Cognition, 6th edition. John
Wiley & Sons.
Patrick Pantel, Dekang Lin, 2002. Discovering word
senses from text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional clustering of English words. ACL ?93.
Ellen Riloff, Rosie Jones, 1999. Learning dictionaries
for information extraction by multi-level bootstrap-
ping. AAAI ?99.
Hinrich Schu?tze, 1998. Automatic word sense discrim-
ination. Comp. Linguistics, 24(1):97?123.
Dominic Widdows, Beate Dorow, 2002. A graph model
for unsupervised Lexical acquisition. COLING ?02.
304
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232?239,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Fully Unsupervised Discovery of Concept-Specific Relationships
by Web Mining
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem 91904, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
www.cs.huji.ac.il/?arir
Moshe Koppel
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
koppel@cs.biu.ac.il
Abstract
We present a web mining method for discov-
ering and enhancing relationships in which a
specified concept (word class) participates.
We discover a whole range of relationships
focused on the given concept, rather than
generic known relationships as in most pre-
vious work. Our method is based on cluster-
ing patterns that contain concept words and
other words related to them. We evaluate the
method on three different rich concepts and
find that in each case the method generates a
broad variety of relationships with good pre-
cision.
1 Introduction
The huge amount of information available on the
web has led to a flurry of research on methods for
automatic creation of structured information from
large unstructured text corpora. The challenge is to
create as much information as possible while pro-
viding as little input as possible.
A lot of this research is based on the initial insight
(Hearst, 1992) that certain lexical patterns (?X is a
country?) can be exploited to automatically gener-
ate hyponyms of a specified word. Subsequent work
(to be discussed in detail below) extended this initial
idea along two dimensions.
One objective was to require as small a user-
provided initial seed as possible. Thus, it was ob-
served that given one or more such lexical patterns,
a corpus could be used to generate examples of hy-
ponyms that could then, in turn, be exploited to gen-
erate more lexical patterns. The larger and more reli-
able sets of patterns thus generated resulted in larger
and more precise sets of hyponyms and vice versa.
The initial step of the resulting alternating bootstrap
process ? the user-provided input ? could just as well
consist of examples of hyponyms as of lexical pat-
terns.
A second objective was to extend the information
that could be learned from the process beyond hy-
ponyms of a given word. Thus, the approach was
extended to finding lexical patterns that could pro-
duce synonyms and other standard lexical relations.
These relations comprise all those words that stand
in some known binary relation with a specified word.
In this paper, we introduce a novel extension of
this problem: given a particular concept (initially
represented by two seed words), discover relations
in which it participates, without specifying their
types in advance. We will generate a concept class
and a variety of natural binary relations involving
that class.
An advantage of our method is that it is particu-
larly suitable for web mining, even given the restric-
tions on query amounts that exist in some of today?s
leading search engines.
The outline of the paper is as follows. In the next
section we will define more precisely the problem
we intend to solve. In section 3, we will consider re-
lated work. In section 4 we will provide an overview
of our solution and in section 5 we will consider the
details of the method. In section 6 we will illustrate
and evaluate the results obtained by our method. Fi-
nally, in section 7 we will offer some conclusions
and considerations for further work.
232
2 Problem Definition
In several studies (e.g., Widdows and Dorow, 2002;
Pantel et al 2004; Davidov and Rappoport, 2006)
it has been shown that relatively unsupervised and
language-independent methods could be used to
generate many thousands of sets of words whose
semantics is similar in some sense. Although ex-
amination of any such set invariably makes it clear
why these words have been grouped together into
a single concept, it is important to emphasize that
the method itself provides no explicit concept defi-
nition; in some sense, the implied class is in the eye
of the beholder. Nevertheless, both human judgment
and comparison with standard lists indicate that the
generated sets correspond to concepts with high pre-
cision.
We wish now to build on that result in the fol-
lowing way. Given a large corpus (such as the web)
and two or more examples of some concept X , au-
tomatically generate examples of one or more rela-
tions R ? X ? Y , where Y is some concept and R
is some binary relationship between elements of X
and elements of Y .
We can think of the relations we wish to gener-
ate as bipartite graphs. Unlike most earlier work,
the bipartite graphs we wish to generate might be
one-to-one (for example, countries and their capi-
tals), many-to-one (for example, countries and the
regions they are in) or many-to-many (for example,
countries and the products they manufacture). For a
given class X , we would like to generate not one but
possibly many different such relations.
The only input we require, aside from a corpus,
is a small set of examples of some class. However,
since such sets can be generated in entirely unsuper-
vised fashion, our challenge is effectively to gener-
ate relations directly from a corpus given no addi-
tional information of any kind. The key point is that
we do not in any manner specify in advance what
types of relations we wish to find.
3 Related Work
As far as we know, no previous work has directly
addressed the discovery of generic binary relations
in an unrestricted domain without (at least implic-
itly) pre-specifying relationship types. Most related
work deals with discovery of hypernymy (Hearst,
1992; Pantel et al 2004), synonymy (Roark and
Charniak, 1998; Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006) and meronymy (Berland
and Charniak, 1999).
In addition to these basic types, several stud-
ies deal with the discovery and labeling of more
specific relation sub-types, including inter-verb re-
lations (Chklovski and Pantel, 2004) and noun-
compound relationships (Moldovan et al 2004).
Studying relationships between tagged named en-
tities, (Hasegawa et al 2004; Hassan et al 2006)
proposed unsupervised clustering methods that as-
sign given (or semi-automatically extracted) sets of
pairs into several clusters, where each cluster corre-
sponds to one of a known relationship type. These
studies, however, focused on the classification of
pairs that were either given or extracted using some
supervision, rather than on discovery and definition
of which relationships are actually in the corpus.
Several papers report on methods for using the
web to discover instances of binary relations. How-
ever, each of these assumes that the relations them-
selves are known in advance (implicitly or explic-
itly) so that the method can be provided with seed
patterns (Agichtein and Gravano, 2000; Pantel et al
2004), pattern-based rules (Etzioni et al 2004), rela-
tion keywords (Sekine, 2006), or word pairs exem-
plifying relation instances (Pasca et al 2006; Alfon-
seca et al 2006; Rosenfeld and Feldman, 2006).
In some recent work (Strube and Ponzetto, 2006),
it has been shown that related pairs can be gener-
ated without pre-specifying the nature of the rela-
tion sought. However, this work does not focus on
differentiating among different relations, so that the
generated relations might conflate a number of dis-
tinct ones.
It should be noted that some of these papers utilize
language and domain-dependent preprocessing in-
cluding syntactic parsing (Suchanek et al 2006) and
named entity tagging (Hasegawa et al 2004), while
others take advantage of handcrafted databases such
as WordNet (Moldovan et al 2004; Costello et al
2006) and Wikipedia (Strube and Ponzetto, 2006).
Finally, (Turney, 2006) provided a pattern dis-
tance measure which allows a fully unsupervised
measurement of relational similarity between two
pairs of words; however, relationship types were not
discovered explicitly.
233
4 Outline of the Method
We will use two concept words contained in a con-
cept class C to generate a collection of distinct re-
lations in which C participates. In this section we
offer a brief overview of our method.
Step 1: Use a seed consisting of two (or more) ex-
ample words to automatically obtain other examples
that belong to the same class. Call these concept
words. (For instance, if our example words were
France and Angola, we would generate more coun-
try names.)
Step 2: For each concept word, collect instances
of contexts in which the word appears together with
one other content word. Call this other word a tar-
get word for that concept word. (For example, for
France we might find ?Paris is the capital of France?.
Paris would be a target word for France.)
Step 3: For each concept word, group the contexts
in which it appears according to the target word that
appears in the context. (Thus ?X is the capital of Y ?
would likely be grouped with ?Y ?s capital is X?.)
Step 4: Identify similar context groups that ap-
pear across many different concept words. Merge
these into a single concept-word-independent clus-
ter. (The group including the two contexts above
would appear, with some variation, for other coun-
tries as well, and all these would be merged into
a single cluster representing the relation capital-
of(X,Y).)
Step 5: For each cluster, output the relation con-
sisting of all <concept word, target word> pairs that
appear together in a context included in the cluster.
(The cluster considered above would result in a set
of pairs consisting of a country and its capital. Other
clusters generated by the same seed might include
countries and their languages, countries and the re-
gions in which they are located, and so forth.)
5 Details of the Method
In this section we consider the details of each of
the above-enumerated steps. It should be noted
that each step can be performed using standard web
searches; no special pre-processed corpus is re-
quired.
5.1 Generalizing the seed
The first step is to take the seed, which might con-
sist of as few as two concept words, and generate
many (ideally, all, when the concept is a closed set
of words) members of the class to which they be-
long. We do this as follows, essentially implement-
ing a simplified version of the method of Davidov
and Rappoport (2006). For any pair of seed words
Si and Sj , search the corpus for word patterns of the
form SiHSj , where H is a high-frequency word in
the corpus (we used the 100 most frequent words
in the corpus). Of these, we keep all those pat-
terns, which we call symmetric patterns, for which
SjHSi is also found in the corpus. Repeat this pro-
cess to find symmetric patterns with any of the struc-
tures HSHS, SHSH or SHHS. It was shown in
(Davidov and Rappoport, 2006) that pairs of words
that often appear together in such symmetric pat-
terns tend to belong to the same class (that is, they
share some notable aspect of their semantics). Other
words in the class can thus be generated by search-
ing a sub-corpus of documents including at least two
concept words for those words X that appear in a
sufficient number of instances of both the patterns
SiHX and XHSi, where Si is a word in the class.
The same can be done for the other three pattern
structures. The process can be bootstrapped as more
words are added to the class.
Note that our method differs from that of Davidov
and Rappoport (2006) in that here we provide an ini-
tial seed pair, representing our target concept, while
there the goal is grouping of as many words as pos-
sible into concept classes. The focus of our paper is
on relations involving a specific concept.
5.2 Collecting contexts
For each concept word S, we search the corpus for
distinct contexts in which S appears. (For our pur-
poses, a context is a window with exactly five words
or punctuation marks before or after the concept
word; we choose 10,000 of these, if available.) We
call the aggregate text found in all these context win-
dows the S-corpus.
From among these contexts, we choose all pat-
terns of the form H1SH2XH3 or H1XH2SH3,
where:
234
? X is a word that appears with frequency below
f1 in the S-corpus and that has sufficiently high
pointwise mutual information with S. We use
these two criteria to ensure that X is a content
word and that it is related to S. The lower the
threshold f1, the less noise we allow in, though
possibly at the expense of recall. We used f1 =
1, 000 occurrences per million words.
? H2 is a string of words each of which occurs
with frequency above f2 in the S-corpus. We
want H2 to consist mainly of words common
in the context of S in order to restrict patterns
to those that are somewhat generic. Thus, in
the context of countries we would like to retain
words like capital while eliminating more spe-
cific words that are unlikely to express generic
patterns. We used f2 = 100 occurrences per
million words (there is room here for automatic
optimization, of course).
? H1 and H3 are either punctuation or words that
occur with frequency above f3 in the S-corpus.
This is mainly to ensure that X and S aren?t
fragments of multi-word expressions. We used
f3 = 100 occurrences per million words.
? We call these patterns, S-patterns and we call
X the target of the S-pattern. The idea is that S
and X very likely stand in some fixed relation
to each other where that relation is captured by
the S-pattern.
5.3 Grouping S-patterns
If S is in fact related to X in some way, there might
be a number of S-patterns that capture this relation-
ship. For each X , we group all the S-patterns that
have X as a target. (Note that two S-patterns with
two different targets might be otherwise identical,
so that essentially the same pattern might appear in
two different groups.) We now merge groups with
large (more than 2/3) overlap. We call the resulting
groups, S-groups.
5.4 Identifying pattern clusters
If the S-patterns in a given S-group actually capture
some relationship between S and the target, then
one would expect that similar groups would appear
for a multiplicity of concept words S. Suppose that
we have S-groups for three different concept words
S such that the pairwise overlap among the three
groups is more than 2/3 (where for this purpose two
patterns are deemed identical if they differ only at S
and X). Then the set of patterns that appear in two or
three of these S-groups is called a cluster core. We
now group all patterns in other S-groups that have an
overlap of more than 2/3 with the cluster core into a
candidate pattern pool P . The set of all patterns in
P that appear in at least two S-groups (among those
that formed P ) pattern cluster. A pattern cluster that
has patterns instantiated by at least half of the con-
cept words is said to represent a relation.
5.5 Refining relations
A relation consists of pairs (S, X) where S is a con-
cept word and X is the target of some S-pattern in a
given pattern cluster. Note that for a given S, there
might be one or many values of X satisfying the re-
lation. As a final refinement, for each given S, we
rank all such X according to pointwise mutual in-
formation with S and retain only the highest 2/3. If
most values of S have only a single corresponding X
satisfying the relation and the rest have none, we try
to automatically fill in the missing values by search-
ing the corpus for relevant S-patterns for the missing
values of S. (In our case the corpus is the web, so
we perform additional clarifying queries.)
Finally, we delete all relations in which all con-
cept words are related to most target words and all
relations in which the concept words and the target
words are identical. Such relations can certainly be
of interest (see Section 7), but are not our focus in
this paper.
5.6 Notes on required Web resources
In our implementation we use the Google search
engine. Google restricts individual users to 1,000
queries per day and 1,000 pages per query. In each
stage we conducted queries iteratively, each time
downloading all 1,000 documents for the query.
In the first stage our goal was to discover sym-
metric relationships from the web and consequently
discover additional concept words. For queries in
this stage of our algorithm we invoked two require-
ments.
First, the query should contain at least two con-
cept words. This proved very effective in reduc-
235
ing ambiguity. Thus of 1,000 documents for the
query bass, 760 deal with music, while if we add to
the query a second word from the intended concept
(e.g., barracuda), then none of the 1,000 documents
deal with music and the vast majority deal with fish,
as intended.
Second, we avoid doing overlapping queries. To
do this we used Google?s ability to exclude from
search results those pages containing a given term
(in our case, one of the concept words).
We performed up to 300 different queries for in-
dividual concepts in the first stage of our algorithm.
In the second stage, we used web queries to as-
semble S-corpora. On average, about 1/3 of the con-
cept words initially lacked sufficient data and we
performed up to twenty additional queries for each
rare concept word to fill its corpus.
In the last stage, when clusters are constructed,
we used web queries for filling missing pairs of one-
to-one or several-to-several relationships. The to-
tal number of filling queries for a specific concept
was below 1,000, and we needed only the first re-
sults of these queries. Empirically, it took between
0.5 to 6 day limits (i.e., 500?6,000 queries) to ex-
tract relationships for a concept, depending on its
size (the number of documents used for each query
was at most 100). Obviously this strategy can be
improved by focused crawling from primary Google
hits, which can drastically reduce the required num-
ber of queries.
6 Evaluation
In this section we wish to consider the variety of re-
lations that can be generated by our method from a
given seed and to measure the quality of these rela-
tions in terms of their precision and recall.
With regard to precision, two claims are being
made. One is that the generated relations correspond
to identifiable relations. The other claim is that to
the extent that a generated relation can be reason-
ably identified, the generated pairs do indeed belong
to the identified relation. (There is a small degree of
circularity in this characterization but this is proba-
bly the best we can hope for.)
As a practical matter, it is extremely difficult to
measure precision and recall for relations that have
not been pre-determined in any way. For each gen-
erated relation, authoritative resources must be mar-
shaled as a gold standard. For purposes of evalu-
ation, we ran our algorithm on three representative
domains ? countries, fish species and star constel-
lations ? and tracked down gold standard resources
(encyclopedias, academic texts, informative web-
sites, etc) for the bulk of the relations generated in
each domain.
This choice of domains allowed us to explore
different aspects of algorithmic behavior. Country
and constellation domains are both well defined and
closed domains. However they are substantially dif-
ferent.
Country names is a relatively large domain which
has very low lexical ambiguity, and a large number
of potentially useful relations. The main challenge
in this domain was to capture it well.
Constellation names, in contrast, are a relatively
small but highly ambiguous domain. They are used
in proper names, mythology, names of entertainment
facilities etc. Our evaluation examined how well the
algorithm can deal with such ambiguity.
The fish domain contains a very high number of
members. Unlike countries, it is a semi-open non-
homogenous domain with a very large number of
subclasses and groups. Also, unlike countries, it
does not contain many proper nouns, which are em-
pirically generally easier to identify in patterns. So
the main challenge in this domain is to extract un-
blurred relationships and not to diverge from the do-
main during the concept acquisition phase.
We do not show here all-to-all relationships such
as fish parts (common to all or almost all fish), be-
cause we focus on relationships that separate be-
tween members of the concept class, which are
harder to acquire and evaluate.
6.1 Countries
Our seed consisted of two country names. The in-
tended result for the first stage of the algorithm
was a list of countries. There are 193 countries in
the world (www.countrywatch.com) some of which
have multiple names so that the total number of
commonly used country names is 243. Of these,
223 names (comprising 180 countries) are charac-
ter strings with no white space. Since we consider
only single word names, these 223 are the names we
hope to capture in this stage.
236
Using the seed words France and Angola, we
obtained 202 country names (comprising 167 dis-
tinct countries) as well as 32 other names (consisting
mostly of names of other geopolitical entities). Us-
ing the list of 223 single word countries as our gold
standard, this gives precision of 0.90 and recall of
0.86. (Ten other seed pairs gave results ranging in
precision: 0.86-0.93 and recall: 0.79-0.90.)
The second part of the algorithm generated a set
of 31 binary relations. Of these, 25 were clearly
identifiable relations many of which are shown in
Table 1. Note that for three of these there are stan-
dard exhaustive lists against which we could mea-
sure both precision and recall; for the others shown,
sources were available for measuring precision but
no exhaustive list was available from which to mea-
sure recall, so we measured coverage (the number
of countries for which at least one target concept is
found as related).
Another eleven meaningful relations were gener-
ated for which we did not compute precision num-
bers. These include celebrity-from, animal-of, lake-
in, borders-on and enemy-of. (The set of relations
generated by other seed pairs differed only slightly
from those shown here for France and Angola.)
6.2 Fish species
In our second experiment, our seed consisted of two
fish species, barracuda and bluefish. There are 770
species listed in WordNet of which 447 names are
character strings with no white space. The first stage
of the algorithm returned 305 of the species listed
in Wordnet, another 37 species not listed in Word-
net, as well as 48 other names (consisting mostly
of other sea creatures). The second part of the al-
gorithm generated a set of 15 binary relations all of
which are meaningful. Those for which we could
find some gold standard are listed in Table 2.
Other relations generated include served-with,
bait-for, food-type, spot-type, and gill-type.
6.3 Constellations
Our seed consisted of two constellation names,
Orion and Cassiopeia. There are 88 standard
constellations (www.astro.wisc.edu) some of which
have multiple names so that the total number of com-
monly used constellations is 98. Of these, 87 names
(77 constellations) are strings with no white space.
Relationship Prec. Rec/Cov
Sample pattern
(Sample pair)
capital-of 0.92 R=0.79
in (x), capital of (y),
(Luanda, Angola)
language-spoken-in 0.92 R=0.60
to (x) or other (y) speaking
(Spain, Spanish)
in-region 0.73 R=0.71
throughout (x), from (y) to
(America, Canada)
city-in 0.82 C=0.95
west (x) ? forecast for (y).
(England, London)
river-in 0.92 C=0.68
central (x), on the (y) river
(China, Haine)
mountain-range-in 0.77 C=0.69
the (x) mountains in (y) ,
(Chella, Angola)
sub-region-of 0.81 C=0.81
the (y) region of (x),
(Veneto, Italy)
industry-of 0.70 C=0.90
the (x) industry in (y) ,
(Oil, Russia)
island-in 0.98 C=0.55
, (x) island , (y) ,
(Bathurst, Canada)
president-of 0.86 C=0.51
president (x) of (y) has
(Bush, USA)
political-position-in 0.81 C=0.75
former (x) of (y) face
(President, Ecuador)
political-party-of 0.91 C=0.53
the (x) party of (y) ,
(Labour, England)
festival-of 0.90 C=0.78
the (x) festival, (y) ,
(Tanabata, Japan)
religious-denomination-of 0.80 C=0.62
the (x) church in (y) ,
(Christian, Rome)
Table 1: Results on seed { France, Angola }.
237
Relationship Prec. Cov
Sample pattern
(Sample pair)
region-found-in 0.83 0.80
best (x) fishing in (y) .
(Walleye, Canada)
sea-found-in 0.82 0.64
of (x) catches in the (y) sea
(Shark, Adriatic)
lake-found-in 0.79 0.51
lake (y) is famous for (x) ,
(Marion, Catfish)
habitat-of 0.78 0.92
, (x) and other (y) fish
(Menhaden, Saltwater)
also-called 0.91 0.58
. (y) , also called (x) ,
(Lemonfish, Ling)
eats 0.90 0.85
the (x) eats the (y) and
(Perch, Minnow)
color-of 0.95 0.85
the (x) was (y) color
(Shark, Gray)
used-for-food 0.80 0.53
catch (x) ? best for (y) or
(Bluefish, Sashimi)
in-family 0.95 0.60
the (x) family , includes (y) ,
(Salmonid, Trout)
Table 2: Results on seed { barracud, bluefish }.
The first stage of the algorithm returned 81 constel-
lation names (77 distinct constellations) as well as
38 other names (consisting mostly of names of indi-
vidual stars). Using the list of 87 single word con-
stellation names as our gold standard, this gives pre-
cision of 0.68 and recall of 0.93.
The second part of the algorithm generated a set
of ten binary relations. Of these, one concerned
travel and entertainment (constellations are quite
popular as names of hotels and lounges) and another
three were not interesting. Apparently, the require-
ment that half the constellations appear in a relation
limited the number of viable relations since many
constellations are quite obscure. The six interesting
relations are shown in Table 3 along with precision
and coverage.
7 Discussion
In this paper we have addressed a novel type of prob-
lem: given a specific concept, discover in fully un-
supervised fashion, a range of relations in which it
participates. This can be extremely useful for study-
ing and researching a particular concept or field of
study.
As others have shown as well, two concept words
can be sufficient to generate almost the entire class
to which the words belong when the class is well-
defined. With the method presented in this paper,
using no further user-provided information, we can,
for a given concept, automatically generate a diverse
collection of binary relations on this concept. These
relations need not be pre-specified in any way. Re-
sults on the three domains we considered indicate
that, taken as an aggregate, the relations that are gen-
erated for a given domain paint a rather clear picture
of the range of information pertinent to that domain.
Moreover, all this was done using standard search
engine methods on the web. No language-dependent
tools were used (not even stemming); in fact, we re-
produced many of our results using Google in Rus-
sian.
The method depends on a number of numerical
parameters that control the subtle tradeoff between
quantity and quality of generated relations. There is
certainly much room for tuning of these parameters.
The concept and target words used in this paper
are single words. Extending this to multiple-word
expressions would substantially contribute to the ap-
plicability of our results.
In this research we effectively disregard many re-
lationships of an all-to-all nature. However, such
relationships can often be very useful for ontology
construction, since in many cases they introduce
strong connections between two different concepts.
Thus, for fish we discovered that one of the all-to-
all relationships captures a precise set of fish body
parts, and another captures swimming verbs. Such
relations introduce strong and distinct connections
between the concept of fish and the concepts of fish-
body-parts and swimming. Such connections may
be extremely useful for ontology construction.
238
Relationship Prec. Cov
Sample pattern
(Sample pair)
nearby-constellation 0.87 0.70
constellation (x), near (y),
(Auriga, Taurus)
star-in 0.82 0.76
star (x) in (y) is
(Antares , Scorpius)
shape-of 0.90 0.55
, (x) is depicted as (y).
(Lacerta, Lizard)
abbreviated-as 0.93 0.90
. (x) abbr (y),
(Hidra, Hya)
cluster-types-in 0.92 1.00
famous (x) cluster in (y),
(Praesepe, Cancer)
location 0.82 0.70
, (x) is a (y) constellation
(Draco, Circumpolar)
Table 3: Results on seed { Orion, Cassiopeia }.
References
Agichtein, E., Gravano, L., 2000. Snowball: Extracting
relations from large plain-text collections. Proceedings
of the 5th ACM International Conference on Digital
Libraries.
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ?99.
Chklovski T., Pantel P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
?04.
Costello, F., Veale, T., Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds, COLING-ACL ?06.
Davidov, D., Rappoport, A., 2006. Efficient unsupervised
discovery of word categories using symmetric patterns
and high frequency words. COLING-ACL ?06.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., Yates, A., 2004.
Methods for domain-independent information extrac-
tion from the web: an experimental comparison. AAAI
?04.
Hasegawa, T., Sekine, S., Grishman, R., 2004. Discover-
ing relations among named entities from large corpora.
ACL ?04.
Hassan, H., Hassan, A., Emam, O., 2006. unsupervised
information extraction approach using graph mutual
reinforcement. EMNLP ?06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. Workshop on Comput. Lexical
Semantics at HLT-NAACL ?04.
Pantel, P., Ravichandran, D., Hovy, E., 2004. Towards
terascale knowledge acquisition. COLING ?04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006.
Names and similarities on the web: fact extraction in
the fast lane. COLING-ACL ?06.
Roark, B., Charniak, E., 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lex-
icon construction. ACL ?98.
Rosenfeld B., Feldman, R.: URES : an unsupervised
web relation extraction system. Proceedings, ACL ?06
Poster Sessions.
Sekine, S., 2006 On-demand information extraction.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Suchanek F. M., G. Ifrim, G. Weikum. 2006. LEILA:
learning to extract information by linguistic analysis.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Widdows, D., Dorow, B., 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ?02.
239
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408?415,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
An Ensemble Method for Selection of High Quality Parses
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
While the average performance of statisti-
cal parsers gradually improves, they still at-
tach to many sentences annotations of rather
low quality. The number of such sentences
grows when the training and test data are
taken from different domains, which is the
case for major web applications such as in-
formation retrieval and question answering.
In this paper we present a Sample Ensem-
ble Parse Assessment (SEPA) algorithm for
detecting parse quality. We use a function
of the agreement among several copies of
a parser, each of which trained on a differ-
ent sample from the training data, to assess
parse quality. We experimented with both
generative and reranking parsers (Collins,
Charniak and Johnson respectively). We
show superior results over several baselines,
both when the training and test data are from
the same domain and when they are from
different domains. For a test setting used by
previous work, we show an error reduction
of 31% as opposed to their 20%.
1 Introduction
Many algorithms for major NLP applications such
as information extraction (IE) and question answer-
ing (QA) utilize the output of statistical parsers
(see (Yates et al, 2006)). While the average per-
formance of statistical parsers gradually improves,
the quality of many of the parses they produce is
too low for applications. When the training and test
data are taken from different domains (the parser
adaptation scenario) the ratio of such low quality
parses becomes even higher. Figure 1 demonstrates
these phenomena for two leading models, Collins
(1999) model 2, a generative model, and Charniak
and Johnson (2005), a reranking model. The parser
adaptation scenario is the rule rather than the excep-
tion for QA and IE systems, because these usually
operate over the highly variable Web, making it very
difficult to create a representative corpus for manual
annotation. Medium quality parses may seriously
harm the performance of such systems.
In this paper we address the problem of assess-
ing parse quality, using a Sample Ensemble Parse
Assessment (SEPA) algorithm. We use the level of
agreement among several copies of a parser, each of
which trained on a different sample from the training
data, to predict the quality of a parse. The algorithm
does not assume uniformity of training and test data,
and is thus suitable to web-based applications such
as QA and IE.
Generative statistical parsers compute a probabil-
ity p(a, s) for each sentence annotation, so the im-
mediate technique that comes to mind for assess-
ing parse quality is to simply use p(a, s). Another
seemingly trivial method is to assume that shorter
sentences would be parsed better than longer ones.
However, these techniques produce results that are
far from optimal. In Section 5 we show the superi-
ority of our method over these and other baselines.
Surprisingly, as far as we know there is only one
previous work explicitly addressing this problem
(Yates et al, 2006). Their WOODWARD algorithm
filters out high quality parses by performing seman-
408
80 85 90 95 1000.2
0.4
0.6
0.8
1
F score
Fr
ac
tio
n 
of
 p
ar
se
s
 
 Collins, ID
Collins, Adap.
Charniak, ID
Charniak,Adap.
Figure 1: F-score vs. the fraction of parses whose
f-score is at least that f-score. For the in-domain
scenario, the parsers are tested on sec 23 of the WSJ
Penn Treebank. For the parser adaptation scenario,
they are tested on the Brown test section. In both
cases they are trained on sections 2-21 of WSJ.
tic analysis. The present paper provides a detailed
comparison between the two algorithms, showing
both that SEPA produces superior results and that
it operates under less restrictive conditions.
We experiment with both the generative parsing
model number 2 of Collins (1999) and the reranking
parser of Charniak and Johnson (2005), both when
the training and test data belong to the same domain
(the in-domain scenario) and in the parser adapta-
tion scenario. In all four cases, we show substantial
improvement over the baselines. The present paper
is the first to use a reranking parser and the first to
address the adaptation scenario for this problem.
Section 2 discusses relevant previous work, Sec-
tion 3 describes the SEPA algorithm, Sections 4 and
5 present the experimental setup and results, and
Section 6 discusses certain aspects of these results
and compares SEPA to WOODWARD.
2 Related Work
The only previous work we are aware of that explic-
itly addressed the problem of detecting high quality
parses in the output of statistical parsers is (Yates et
al., 2006). Based on the observation that incorrect
parses often result in implausible semantic interpre-
tations of sentences, they designed the WOODWARD
filtering system. It first maps the parse produced by
the parser to a logic-based representation (relational
conjunction (RC)) and then employs four methods
for semantically analyzing whether a conjunct in the
RC is likely to be reasonable. The filters use seman-
tic information obtained from the Web. Measuring
errors using filter f-score (see Section 3) and using
the Collins generative model, WOODWARD reduces
errors by 67% on a set of TREC questions and by
20% on a set of a 100 WSJ sentences. Section 5
provides a detailed comparison with our algorithm.
Reranking algorithms (Koo and Collins, 2005;
Charniak and Johnson, 2005) search the list of best
parses output by a generative parser to find a parse of
higher quality than the parse selected by the genera-
tive parser. Thus, these algorithms in effect assess
parse quality using syntactic and lexical features.
The SEPA algorithm does not use such features, and
is successful in detecting high quality parses even
when working on the output of a reranker. Rerank-
ing and SEPA are thus relatively independent.
Bagging (Breiman, 1996) uses an ensemble of in-
stances of a model, each trained on a sample of the
training data1. Bagging was suggested in order to
enhance classifiers; the classification outcome was
determined using a majority vote among the mod-
els. In NLP, bagging was used for active learning
for text classification (Argamon-Engelson and Da-
gan, 1999; McCallum and Nigam, 1998). Specif-
ically in parsing, (Henderson and Brill, 2000) ap-
plied a constituent level voting scheme to an en-
semble of bagged models to increase parser perfor-
mance, and (Becker and Osborne, 2005) suggested
an active learning technique in which the agreement
among an ensemble of bagged parsers is used to pre-
dict examples valuable for human annotation. They
reported experiments with small training sets only
(up to 5,000 sentences), and their agreement func-
tion is very different from ours. Both works experi-
mented with generative parsing models only.
Ngai and Yarowsky (2000) used an ensemble
based on bagging and partitioning for active learning
for base NP chunking. They select top items with-
out any graded assessment, and their f-complement
function, which slightly resembles our MF (see the
next section), is applied to the output of a classifier,
while our function is applied to structured output.
A survey of several papers dealing with mapping
1Each sample is created by sampling, with replacement, L
examples from the training pool, where L is the size of the train-
ing pool. Conversely, each of our samples is smaller than the
training set, and is created by sampling without replacement.
See Section 3 (?regarding S?) for a discussion of this issue.
409
predictors in classifiers? output to posterior proba-
bilities is given in (Caruana and Niculescu-Mizil,
2006). As far as we know, the application of a sam-
ple based parser ensemble for assessing parse qual-
ity is novel.
Many IE and QA systems rely on the output of
parsers (Kwok et al, 2001; Attardi et al, 2001;
Moldovan et al, 2003). The latter tries to address
incorrect parses using complex relaxation methods.
Knowing the quality of a parse could greatly im-
prove the performance of such systems.
3 The Sample Ensemble Parse Assessment
(SEPA) Algorithm
In this section we detail our parse assessment algo-
rithm. Its input consists of a parsing algorithm A, an
annotated training set TR, and an unannotated test
set TE. The output provides, for each test sentence,
the parse generated for it by A when trained on the
full training set, and a grade assessing the parse?s
quality, on a continuous scale between 0 to 100. Ap-
plications are then free to select a sentence subset
that suits their needs using our grades, e.g. by keep-
ing only high-quality parses, or by removing low-
quality parses and keeping the rest. The algorithm
has the following stages:
1. Choose N random samples of size S from the
training set TR. Each sample is selected with-
out replacement.
2. Train N copies of the parsing algorithm A,
each with one of the samples.
3. Parse the test set with each of the N models.
4. For each test sentence, compute the value of an
agreement function F between the models.
5. Sort the test set according to F ?s value.
The algorithm uses the level of agreement among
several copies of a parser, each trained on a different
sample from the training data, to predict the qual-
ity of a parse. The higher the agreement, the higher
the quality of the parse. Our approach assumes that
if the parameters of the model are well designed to
annotate a sentence with a high quality parse, then
it is likely that the model will output the same (or
a highly similar) parse even if the training data is
somewhat changed. In other words, we rely on the
stability of the parameters of statistical parsers. Al-
though this is not always the case, our results con-
firm that strong correlation between agreement and
parse quality does exist.
We explored several agreement functions. The
one that showed the best results is Mean F-score
(MF)2, defined as follows. Denote the models by
m1 . . .mN , and the parse provided by mi for sen-
tence s as mi(s). We randomly choose a model ml,
and compute
MF (s) = 1N ? 1
?
i?[1...N ],i6=l
fscore(mi, ml) (1)
We use two measures to evaluate the quality of
SEPA grades. Both measures are defined using a
threshold parameter T , addressing only sentences
whose SEPA grades are not smaller than T . We refer
to these sentences as T-sentences.
The first measure is the average f-score of the
parses of T-sentences. Note that we compute the
f-score of each of the selected sentences and then
average the results. This stands in contrast to the
way f-score is ordinarily calculated, by computing
the labeled precision and recall of the constituents
in the whole set and using these as the arguments of
the f-score equation. The ordinary f-score is com-
puted that way mostly in order to overcome the fact
that sentences differ in length. However, for appli-
cations such as IE and QA, which work at the single
sentence level and which might reach erroneous de-
cision due to an inaccurate parse, normalizing over
sentence lengths is less of a factor. For this reason,
in this paper we present detailed graphs for the aver-
age f-score. For completeness, Table 4 also provides
some of the results using the ordinary f-score.
The second measure is a generalization of the fil-
ter f-score measure suggested by Yates et al (2006).
They define filter precision as the ratio of correctly
parsed sentences in the filtered set (the set the algo-
rithm choose) to total sentences in the filtered set and
filter recall as the ratio of correctly parsed sentences
in the filtered set to correctly parsed sentences in the
2Recall that sentence f-score is defined as: f = 2?P?RP+R ,
where P and R are the labeled precision and recall of the con-
stituents in the sentence relative to another parse.
410
whole set of sentences parsed by the parser (unfil-
tered set or test set). Correctly parsed sentences are
sentences whose parse got f-score of 100%.
Since requiring a 100% may be too restrictive, we
generalize this measure to filter f-score with param-
eter k. In our measure, the filter recall and precision
are calculated with regard to sentences that get an
f-score of k or more, rather than to correctly parsed
sentences. Filtered f-score is thus a special case of
our filtered f-score, with parameter 100.
We now discuss the effect of the number of mod-
els N and the sample size S. The discussion is based
on experiments (using development data, see Sec-
tion 4) in which all the parameters are fixed except
for the parameter in question, using our development
sections.
Regarding N (see Figure 2): As the number of
models increases, the number of T-sentences se-
lected by SEPA decreases and their quality im-
proves, in terms of both average f-score and filter
f-score (with k = 100). The fact that more mod-
els trained on different samples of the training data
agree on the syntactic annotation of a sentence im-
plies that this syntactic pattern is less sensitive to
perturbations in the training data. The number of
such sentences is small and it is likely the parser will
correctly annotate them. The smaller T-set size leads
to a decrease in filter recall, while the better quality
leads to an increase in filter precision. Since the in-
crease in filter precision is sharper than the decrease
in filter recall, filter f-score increases with the num-
ber of models N .
Regarding S3: As the sample size increases, the
number of T-sentences increases, and their qual-
ity degrades in terms of average f-score but im-
proves in terms of filter f-score (again, with param-
eter k = 100). The overlap among smaller sam-
ples is small and the data they supply is sparse. If
several models trained on such samples attach to a
sentence the same parse, this syntactic pattern must
be very prominent in the training data. The num-
ber of such sentences is small and it is likely that
the parser will correctly annotate them. Therefore
smaller sample size leads to smaller T-sets with high
average f-score. As the sample size increases, the T-
set becomes larger but the average f-score of a parse
3Graphs are not shown due to lack of space.
5 10 15 20
90
91
92
93
94
A
ve
ra
ge
 f 
sc
or
e
Number of models ? N
54
56
58
60
62
F
ilt
e
r 
f 
sc
o
re
, 
k 
=
 1
0
0
0 5 10 15 20
65
70
75
80
85
90
F
ilt
e
r 
re
ca
ll,
 k
 =
 1
0
0
35
40
45
50
55
60
F
ilt
e
r 
pr
ec
is
io
n,
 k
 =
 1
00
Number of models ? N
Figure 2: The effect of the number of models N on
SEPA (Collins? model). The scenario is in-domain,
sample size S = 33, 000 and T = 100. We see:
average f-score of T-sentences (left, solid curve and
left y-axis), filter f-score with k = 100 (left, dashed
curve and right y-axis), filter recall with k = 100
(right, solid curve and left y-axis), and filter preci-
sion with k = 100 (right, dashed curve and right
y-axis).
decreases. The larger T-set size leads to increase in
filter recall, while the lower average quality leads
to decrease in filter precision. Since the increase in
filter recall is sharper than the decrease in filter pre-
cision, the result is that filter f-score increases with
the sample size S.
This discussion demonstrates the importance of
using both average f-score and filter f-score, since
the two measures reflect characteristics of the se-
lected sample that are not necessarily highly (or pos-
itively) correlated.
4 Experimental Setup
We performed experiments with two parsing mod-
els, the Collins (1999) generative model number
2 and the Charniak and Johnson (2005) reranking
model. For the first we used a reimplementation
(?). We performed experiments with each model
in two scenarios, in-domain and parser adaptation.
In both experiments the training data are sections
02-21 of the WSJ PennTreebank (about 40K sen-
tences). In the in-domain experiment the test data
is section 23 (2416 sentences) of WSJ and in the
parser adaptation scenario the test data is Brown test
section (2424 sentences). Development sections are
WSJ section 00 for the in-domain scenario (1981
sentences) and Brown development section for the
adaptation scenario (2424 sentences). Following
411
(Gildea, 2001), the Brown test and development sec-
tions consist of 10% of Brown sentences (the 9th and
10th of each 10 consecutive sentences in the devel-
opment and test sections respectively).
We performed experiments with many configu-
rations of the parameters N (number of models),
S (sample size) and F (agreement function). Due
to space limitations we describe only experiments
where the values of the parameters N, S and F are
fixed (F is MF , N and S are given in Section 5)
and the threshold parameter T is changed.
5 Results
We first explore the quality of the selected set in
terms of average f-score. In Section 3 we reported
that the quality of a selected T-set of parses increases
as the number of models N increases and sample
size S decreases. We therefore show the results for
relatively high N (20) and relatively low S (13,000,
which is about a third of the training set). Denote
the cardinality of the set selected by SEPA by n (it
is actually a function of T but we omit the T in order
to simplify notations).
We use several baseline models. The first, confi-
dence baseline (CB), contains the n sentences hav-
ing the highest parser assigned probability (when
trained on the whole training set). The second, min-
imum length (ML), contains the n shortest sentences
in the test set. Since many times it is easier to parse
short sentences, a trivial way to increase the aver-
age f-score measure of a set is simply to select short
sentences. The third, following (Yates et al, 2006),
is maximum recall (MR). MR simply predicts that all
test set sentences should be contained in the selected
T-set. The output set of this model gets filter recall of
1 for any k value, but its precision is lower. The MR
baseline is not relevant to the average f-score mea-
sure, because it selects all of the sentences in a set,
which leads to the same average as a random selec-
tion (see below). In order to minimize visual clutter,
for the filter f-score measure we use the maximum
recall (MR) baseline rather than the minimum length
(ML) baseline, since the former outperforms the lat-
ter. Thus, ML is only shown for the average f-score
measure. We have also experimented with a random
baseline model (containing n randomly selected test
sentences), whose results are the worst and which is
shown for reference.
Readers of this section may get confused between
the agreement threshold parameter T and the param-
eter k of the filter f-score measure. Please note: as to
T , SEPA sorts the test set by the values of the agree-
ment function. One can then select only sentences
whose agreement score is at least T . T ?s values are
on a continuous scale from 0 to 100. As to k, the fil-
ter f-score measure gives a grade. This grade com-
bines three values: (1) the number of sentences in
the set (selected by an algorithm) whose f-score rel-
ative to the gold standard parse is at least k, (2) the
size of the selected set, and (3) the total number of
sentences with such a parse in the whole test set. We
did not introduce separate notations for these values.
Figure 3 (top) shows average f-score results where
SEPA is applied to Collins? generative model in the
in-domain (left) and adaptation (middle) scenarios.
SEPA outperforms the baselines for all values of the
agreement threshold parameter T . Furthermore, as
T increases, not only does the SEPA set quality in-
crease, but the quality differences between this set
and the baseline sets increases as well. The graphs
on the right show the number of sentences in the sets
selected by SEPA for each T value. As expected,
this number decreases as T increases.
Figure 3 (bottom) shows the same pattern of re-
sults for the Charniak reranking parser in the in-
domain (left) and adaptation (middle) scenarios. We
see that the effects of the reranker and SEPA are rel-
atively independent. Even after some of the errors of
the generative model were corrected by the reranker
by selecting parses of higher quality among the 50-
best, SEPA can detect parses of high quality from
the set of parsed sentences.
To explore the quality of the selected set in terms
of filter f-score, we recall that the quality of a se-
lected set of parses increases as both the number of
models N and the sample size S increase, and with
T . Therefore, for k = 85 . . . 100 we show the value
of filter f-score with parameter k when the parame-
ters configuration is a relatively high N (20), rela-
tively high S (33,000, which are about 80% of the
training set), and the highest T (100).
Figure 4 (top) shows filter f-score results for
Collins? generative model in the in-domain (left)
and adaptation (middle) scenarios. As these graphs
show, SEPA outperforms CB and random for all val-
412
ues of the filter f-score parameter k, and outper-
forms the MR baseline where the value of k is 95 or
more. Although for small k values MR gets a higher
f-score than SEPA, the filter precision of SEPA is
much higher (right, shown for adaptation. The in-
domain pattern is similar and not shown). This stems
from the definition of the MR baseline, which sim-
ply predicts any sentence to be in the selected set.
Furthermore, since the selected set is meant to be
the input for systems that require high quality parses,
what matters most is that SEPA outperforms the MR
baseline at the high k ranges.
Figure 4 (bottom) shows the same pattern of re-
sults for the Charniak reranking parser in the in-
domain (left) and adaptation (middle) scenarios. As
for the average f-score measure, it demonstrates that
the effects of the reranker and SEPA algorithm are
relatively independent.
Tables 1 and 2 show the error reduction achieved
by SEPA for the filter f-score measure with param-
eters k = 95, 97, 100 (Table 1) and for the aver-
age f-score measure with several SEPA agreement
threshold (T ) values (Table 2) . The error reductions
achieved by SEPA for both measures are substantial.
Table 3 compares SEPA and WOODWARD on the
exact same test set used by (Yates et al, 2006)
(taken from WSJ sec 23). SEPA achieves error re-
duction of 31% over the MR baseline on this set,
compared to only 20% achieved by WOODWARD.
Not shown in the table, in terms of ordinary f-score
WOODWARD achieves error reduction of 37% while
SEPA achieves 43%. These numbers were the only
ones reported in (Yates et al, 2006).
For completeness of reference, Table 4 shows the
superiority of SEPA over CB in terms of the usual f-
score measure used by the parsing community (num-
bers are counted for constituents first). Results for
other baselines are even more impressive. The con-
figuration is similar to that of Figure 3.
6 Discussion
In this paper we introduced SEPA, a novel algorithm
for assessing parse quality in the output of a statis-
tical parser. SEPA is the first algorithm shown to
be successful when a reranking parser is considered,
even though such models use a reranker to detect
and fix some of the errors made by the base gener-
Filter f-score
In-domain Adaptation
k value 95 97 100 95 97 100
Coll. MR 3.5 20.1 29.2 22.8 29.8 33.6
Coll. CB 11.6 11.7 3.4 14.2 9.9 7.4
Char. MR 1.35 13.6 23.44 21.9 30 32.5
Char. CB 21.9 16.8 11.9 25 20.2 16.2
Table 1: Error reduction in the filter f-score mea-
sure obtained by SEPA with Collins? (top two lines)
and Charniak?s (bottom two lines) model, in the
two scenarios (in-domain and adaptation), vs. the
maximum recall (MR lines 1 and 3) and confi-
dence (CB, lines 2 and 4) baselines, using N =
20, T = 100 and S = 33, 000. Shown are pa-
rameter values k = 95, 97, 100. Error reduction
numbers were computed by 100?(fscoreSEPA?
fscorebaseline)/(1? fscorebaseline).
Average f-score
In-domain Adaptation
T 95 97 100 95 97 100
Coll. ML 32.6 37.2 60.8 46.8 52.7 70.7
Coll. CB 26.5 31.4 53.9 46.9 53.6 70
Char. ML 25.1 33.2 58.5 46.9 58.4 77.1
Char. CB 20.4 30 52 44.4 55.5 73.5
Table 2: Error reduction in the average f-score mea-
sure obtained by SEPA with Collins (top two lines)
and Charniak (bottom two lines) model, in the two
scenarios (in-domain and adaptation), vs. the min-
imum length (ML lines 1 and 3) and confidence
(CB, lines 2 and 4) baselines, using N = 20 and
S = 13, 000. Shown are agreement threhsold pa-
rameter values T = 95, 97, 100. Error reduction
numbers were computed by 100?(fscoreSEPA?
fscorebaseline)/(1? fscorebaseline).
SEPA WOODWARD CB
ER 31% 20% -31%
Table 3: Error reduction compared to the MR base-
line, measured by filter f-score with parameter 100.
The data is the WSJ sec 23 test set usd by (Yates
et al, 2006). All three methods use Collins? model.
SEPA uses N = 20, S = 33, 000, T = 100.
ative model. WOODWARD, the only previously sug-
gested algorithm for this problem, was tested with
Collins? generative model only. Furthermore, this is
the first time that an algorithm for this problem suc-
ceeds in a domain adaptation scenario, regardless of
413
85 90 95 10088
90
92
94
96
98
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 10080
85
90
95
100
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 SEPA
CB
ML
Rand.
85 90 95 1000
500
1000
1500
2000
2500
Agreement threshold
N
um
be
r o
f s
en
te
nc
es
 
 
In domain
Adaptation
85 90 95 10092
93
94
95
96
97
98
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 10085
90
95
100
Agreement threshold
Av
er
ag
e 
fs
co
re
 
 
SEPA
CB
ML
Rand.
85 90 95 100500
1000
1500
2000
2500
Agreement threshold
N
um
be
r o
f s
en
te
nc
es
 
 
In domain
Adaptation
Figure 3: Agreement threshold T vs. average f-score (left and middle) and number of sentences in the se-
lected set (right), for SEPA with Collins? generative model (top) and the Charniak reranking model (bottom).
SEPA parameters are S = 13, 000, N = 20. In both rows, SEPA results for the in-domain (left) and adap-
tation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines. The
graphs on the right show the number of sentences in the selected set for both scenarios.
85 90 95 1000.3
0.4
0.5
0.6
0.7
0.8
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand. 85 90 95 100
0.4
0.5
0.6
0.7
0.8
0.9
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.2
0.4
0.6
0.8
1
K
Fi
lte
r p
re
cis
io
n 
wi
th
 p
ar
am
et
er
 k
 
 SEPA
CB
MR
Rand.
85 90 95 1000.4
0.5
0.6
0.7
0.8
0.9
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.4
0.5
0.6
0.7
0.8
0.9
1
K
Fi
lte
r f
sc
or
e 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA
CB
MR
Rand.
85 90 95 1000.2
0.4
0.6
0.8
1
K
Fi
lte
r p
re
cis
io
n 
wi
th
 p
ar
am
et
er
 k
 
 
SEPA CB MR Rand.
Figure 4: Parameter k vs. filter f-score (left and middle) and filter precision (right) with that parameter, for
SEPA with Collins? generative model (top) and the Charniak reranking model (bottom). SEPA parameters
are S = 33, 000, N = 20, T = 100. In both rows, results for the in-domain (left) and adaptation (middle)
scenarios. In two leftmost graphs, the performance of the algorithm is compared to the confidence baseline
(CB) and maximum recall (MR). The graphs on the right compare the filter precision of SEPA with that of
the MR and CB baselines.
414
the parsing model. In the Web environment this is
the common situation.
The WSJ and Brown experiments performed with
SEPA are much broader than those performed with
WOODWARD, considering all sentences of WSJ sec
23 and Brown test section rather than a subset
of carefully selected sentences from WSJ sec 23.
However, we did not perform a TREC experiment,
as (Yates et al, 2006) did. Our WSJ and Brown
results outperformed several baselines. Moreover,
WSJ (or Brown) sentences that contain conjunctions
were avoided in the experiments of (Yates et al,
2006). We have verified that our algorithm shows
substantial error reduction over the baselines for this
type of sentences (in the ranges 13 ? 46% for the
filter f-score with k = 100, and 30 ? 60% for the
average f-score).
As Table 3 shows, on a WSJ sec 23 test set similar
to that used by (Yates et al, 2006), SEPA achieves
31% error reduction compared to 20% of WOOD-
WARD.
WOODWARD works under several assumptions.
Specifically, it requires a corpus whose content over-
laps at least in part with the content of the parsed
sentences. This corpus is used to extract semanti-
cally related statistics for its filters. Furthermore, the
filters of this algorithm (except of the QA filter) are
focused on verb and preposition relations. Thus, it
is more natural for it to deal with mistakes contained
in such relations. This is reflected in the WSJ based
test set on which it is tested. SEPA does not make
any of these assumptions. It does not use any exter-
nal information source and is shown to select high
quality parses from diverse sets.
In-domain Adaptation
F ER F ER
SEPA Collins 97.09 44.36% 95.38 66.38%
CB Collins 94.77 ? 86.3 ?
SEPA Char-
niak
97.21 35.69% 96.3 54.66%
CB Charniak 95.6 ? 91.84 ?
Table 4: SEPA error reduction vs. the CB base-
line in the in-domain and adaptation scenarios, us-
ing the traditional f-score of the parsing literature.
N = 20, S = 13, 000, T = 100.
For future work, integrating SEPA into the rerank-
ing process seems a promising direction for enhanc-
ing overall parser performance.
Acknowledgement. We would like to thank Dan
Roth for his constructive comments on this paper.
References
Shlomo Argamon-Engelson and Ido Dagan, 1996.
committee-based sample selection for probabilistic
classifiers. Journal of Artificial Intelligence Research,
11:335?360.
Giuseppe Attardi, Antonio Cisternino, Francesco
Formica, Maria Simi and Alessandro Tommasi, 2001.
PiQASso: Pisa question answering system. TREC
?01.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Daniel Bikel, 2004. Code developed at University of
Pennsylvania. http://www.cis.upenn.edu.bikel.
Leo Breiman, 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Rich Caruana and Alexandru Niculescu-Mizil, 2006.
An empirical comparison of supervised learning algo-
rithms. ICML ?06.
Eugene Charniak and Mark Johnson, 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. ACL ?05.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Daniel Gildea, 2001. Corpus variation and parser perfor-
mance. EMNLP ?01.
John C. Henderson and Eric Brill, 2000. Bagging and
boosting a treebank parser. NAACL ?00.
Terry Koo and Michael Collins, 2005. Hidden-variable
models for discriminative reranking. EMNLP ?05.
Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.
Scaling question answering to the web. WWW ?01.
Andrew McCallum and Kamal Nigam, 1998. Employing
EM and pool-based active learning for text classifica-
tion. ICML ?98.
Dan Moldovan, Christine Clark, Sanda Harabagiu and
Steve Maiorano, 2003. Cogex: A logic prover for
question answering. HLT-NAACL ?03.
Grace Ngai and David Yarowsky, 2000. Rule writing or
annotation: cost-efficient resource usage for base noun
phrase chunking. ACL ?00.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting parser errors using web-based
semantic filters. EMNLP ?06.
415
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616?623,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Self-Training for Enhancement and Domain Adaptation of
Statistical Parsers Trained on Small Datasets
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Creating large amounts of annotated data to
train statistical PCFG parsers is expensive,
and the performance of such parsers declines
when training and test data are taken from
different domains. In this paper we use self-
training in order to improve the quality of
a parser and to adapt it to a different do-
main, using only small amounts of manually
annotated seed data. We report significant
improvement both when the seed and test
data are in the same domain and in the out-
of-domain adaptation scenario. In particu-
lar, we achieve 50% reduction in annotation
cost for the in-domain case, yielding an im-
provement of 66% over previous work, and a
20-33% reduction for the domain adaptation
case. This is the first time that self-training
with small labeled datasets is applied suc-
cessfully to these tasks. We were also able
to formulate a characterization of when self-
training is valuable.
1 Introduction
State of the art statistical parsers (Collins, 1999;
Charniak, 2000; Koo and Collins, 2005; Charniak
and Johnson, 2005) are trained on manually anno-
tated treebanks that are highly expensive to create.
Furthermore, the performance of these parsers de-
creases as the distance between the genres of their
training and test data increases. Therefore, enhanc-
ing the performance of parsers when trained on
small manually annotated datasets is of great impor-
tance, both when the seed and test data are taken
from the same domain (the in-domain scenario) and
when they are taken from different domains (the out-
of-domain or parser adaptation scenario). Since the
problem is the expense in manual annotation, we de-
fine ?small? to be 100-2,000 sentences, which are the
sizes of sentence sets that can be manually annotated
by constituent structure in a few hours1.
Self-training is a method for using unannotated
data when training supervised models. The model is
first trained using manually annotated (?seed?) data,
then the model is used to automatically annotate a
pool of unannotated (?self-training?) data, and then
the manually and automatically annotated datasets
are combined to create the training data for the fi-
nal model. Self-training of parsers trained on small
datasets is of enormous potential practical impor-
tance, due to the huge amounts of unannotated data
that are becoming available today and to the high
cost of manual annotation.
In this paper we use self-training to enhance the
performance of a generative statistical PCFG parser
(Collins, 1999) for both the in-domain and the parser
adaptation scenarios, using only small amounts of
manually annotated data. We perform four experi-
ments, examining all combinations of in-domain and
out-of-domain seed and self-training data.
Our results show that self-training is of substantial
benefit for the problem. In particular, we present:
? 50% reduction in annotation cost when the seed
and test data are taken from the same domain,
which is 66% higher than any previous result
with small manually annotated datasets.
1We note in passing that quantitative research on the cost of
annotation using various annotation schemes is clearly lacking.
616
? The first time that self-training improves a gen-
erative parser when the seed and test data are
from the same domain.
? 20-33% reduction in annotation cost when the
seed and test data are from different domains.
? The first time that self-training succeeds in
adapting a generative parser between domains
using a small manually annotated dataset.
? The first formulation (related to the number of
unknown words in a sentence) of when self-
training is valuable.
Section 2 discusses previous work, and Section 3
compares in-depth our protocol to a previous one.
Sections 4 and 5 present the experimental setup and
our results, and Section 6 analyzes the results in an
attempt to shed light on the phenomenon of self-
training.
2 Related Work
Self-training might seem a strange idea: why should
a parser trained on its own output learn anything
new? Indeed, (Clark et al, 2003) applied self-
training to POS-tagging with poor results, and
(Charniak, 1997) applied it to a generative statisti-
cal PCFG parser trained on a large seed set (40K
sentences), without any gain in performance.
Recently, (McClosky et al, 2006a; McClosky et
al., 2006b) have successfully applied self-training to
various parser adaptation scenarios using the rerank-
ing parser of (Charniak and Johnson, 2005). A
reranking parser (see also (Koo and Collins, 2005))
is a layered model: the base layer is a generative sta-
tistical PCFG parser that creates a ranked list of k
parses (say, 50), and the second layer is a reranker
that reorders these parses using more detailed fea-
tures. McClosky et al(2006a) use sections 2-21 of
the WSJ PennTreebank as seed data and between
50K to 2,500K unlabeled NANC corpus sentences
as self-training data. They train the PCFG parser and
the reranker with the manually annotated WSJ data,
and parse the NANC data with the 50-best PCFG
parser. Then they proceed in two directions. In
the first, they reorder the 50-best parse list with the
reranker to create a new 1-best list. In the second,
they leave the 1-best list produced by the genera-
tive PCFG parser untouched. Then they combine the
1-best list (each direction has its own list) with the
WSJ training set, to retrain the PCFG parser. The
final PCFG model and the reranker (trained only on
annotated WSJ material) are then used to parse the
test section (23) of WSJ.
There are two major differences between these pa-
pers and the current one, stemming from their usage
of a reranker and of large seed data. First, when
their 1-best list of the base PCFG parser was used
as self training data for the PCFG parser (the sec-
ond direction), the performance of the base parser
did not improve. It had improved only when the 1-
best list of the reranker was used. In this paper we
show how the 1-best list of a base (generative) PCFG
parser can be used as a self-training material for the
base parser itself and enhance its performance, with-
out using any reranker. This reveals a noteworthy
characteristic of generative PCFG models and offers
a potential direction for parser improvement, since
the quality of a parser-reranker combination criti-
cally depends on that of the base parser.
Second, these papers did not explore self-training
when the seed is small, a scenario whose importance
has been discussed above. In general, PCFG mod-
els trained on small datasets are less likely to parse
the self-training data correctly. For example, the f-
score of WSJ data parsed by the base PCFG parser
of (Charniak and Johnson, 2005) when trained on
the training sections of WSJ is between 89% to
90%, while the f-score of WSJ data parsed with the
Collins? model that we use, and a small seed, is be-
tween 40% and 80%. As a result, the good results of
(McClosky et al 2006a; 2006b) with large seed sets
do not immediately imply success with small seed
sets. Demonstration of such success is a contribu-
tion of the present paper.
Bacchiani et al(2006) explored the scenario of
out-of-domain seed data (the Brown training set
containing about 20K sentences) and in-domain
self-training data (between 4K to 200K sentences
from the WSJ) and showed an improvement over
the baseline of training the parser with the seed data
only. However, they did not explore the case of small
seed datasets (the effort in manually annotating 20K
is substantial) and their work addresses only one of
our scenarios (OI, see below).
617
A work closely related to ours is (Steedman et
al., 2003a), which applied co-training (Blum and
Mitchell, 1998) and self-training to Collins? pars-
ing model using a small seed dataset (500 sentences
for both methods and 1,000 sentences for co-training
only). The seed, self-training and test datasets they
used are similar to those we use in our II experi-
ment (see below), but the self-training protocols are
different. They first train the parser with the seed
sentences sampled from WSJ sections 2-21. Then,
iteratively, 30 sentences are sampled from these sec-
tions, parsed by the parser, and the 20 best sentences
(in terms of parser confidence defined as probability
of top parse) are selected and combined with the pre-
viously annotated data to retrain the parser. The co-
training protocol is similar except that each parser
is trained with the 20 best sentences of the other
parser. Self-training did not improve parser perfor-
mance on the WSJ test section (23). Steedman et
al (2003b) followed a similar co-training protocol
except that the selection function (three functions
were explored) considered the differences between
the confidence scores of the two parsers. In this pa-
per we show a self-training protocol that achieves
better results than all of these methods (Table 2).
The next section discusses possible explanations for
the difference in results. Steedman et al(2003b) and
Hwa et al (2003) also used several versions of cor-
rected co-training which are not comparable to ours
and other suggested methods because their evalua-
tion requires different measures (e.g. reviewed and
corrected constituents are separately counted).
As far as we know, (Becker and Osborne, 2005)
is the only additional work that tries to improve a
generative PCFG parsers using small seed data. The
techniques used are based on active learning (Cohn
et al, 1994). The authors test two novel methods,
along with the tree entropy (TE) method of (Hwa,
2004). The seed, the unannotated and the test sets,
as well as the parser used in that work, are similar
to those we use in our II experiment. Our results are
superior, as shown in Table 3.
3 Self-Training Protocols
There are many possible ways to do self-training.
A main goal of this paper is to identify a self-
training protocol most suitable for enhancement and
domain adaptation of statistical parsers trained on
small datasets. No previous work has succeeded in
identifying such a protocol for this task. In this sec-
tion we try to understand why.
In the protocol we apply, the self-training set con-
tains several thousand sentences A parser trained
with a small seed set parses the self-training set, and
then the whole automatically annotated self-training
set is combined with the manually annotated seed
set to retrain the parser. This protocol and that of
Steedman et al(2003a) were applied to the problem,
with the same seed, self-training and test sets. As
we show below (see Section 4 and Section 5), while
Steedman?s protocol does not improve over the base-
line of using only the seed data, our protocol does.
There are four differences between the protocols.
First, Steedman et als seed set consists of consecu-
tive WSJ sentences, while we select them randomly.
In the next section we show that this difference is
immaterial. Second, Steedman et als protocol looks
for sentences of high quality parse, while our pro-
tocol prefers to use many sentences without check-
ing their parse quality. Third, their protocol is itera-
tive while ours uses a single step. Fourth, our self-
training set is orders of magnitude larger than theirs.
To examine the parse quality issue, we performed
their experiment using their setting but selecting the
high quality parse sentences using their f-score rel-
ative to the gold standard annotation from secs 2-
21 rather than a quality estimate. No improvement
over the baseline was achieved even with this or-
acle. Thus the problem with their protocol does
not lie with the parse quality assessment function;
no other function would produce results better than
the oracle. To examine the iteration issue, we per-
formed their experiment in a single step, selecting at
once the oracle-best 2,000 among 3,000 sentences2,
which produced only a mediocre improvement. We
thus conclude that the size of the self-training set is a
major factor responsible for the difference between
the protocols.
4 Experimental Setup
We used a reimplementation of Collins? parsing
model 2 (Bikel, 2004). We performed four experi-
ments, II, IO, OI, and OO, two with in-domain seed
2Corresponding to a 100 iterations of 30 sentences each.
618
(II, IO) and two with out-of-domain seed (OI, OO),
examining in-domain self-training (II, OI) and out-
of-domain self-training (IO, OO). Note that being
?in? or ?out? of domain is determined by the test data.
Each experiment contained 19 runs. In each run a
different seed size was used, from 100 sentences on-
wards, in steps of 100. For statistical significance,
we repeated each experiment five times, in each rep-
etition randomly sampling different manually anno-
tated sentences to form the seed dataset3.
The seed data were taken from WSJ sections 2-
21. For II and IO, the test data is WSJ section 23
(2416 sentences) and the self-training data are either
WSJ sections 2-21 (in II, excluding the seed sen-
tences) or the Brown training section (in IO). For
OI and OO, the test data is the Brown test section
(2424 sentences), and the self-training data is either
the Brown training section (in OI) or WSJ sections
2-21 (in OO). We removed the manual annotations
from the self-training sections before using them.
For the Brown corpus, we based our division
on (Bacchiani et al, 2006; McClosky et al, 2006b).
The test and training sections consist of sentences
from all of the genres that form the corpus. The
training division consists of 90% (9 of each 10 con-
secutive sentences) of the data, and the test section
are the remaining 10% (We did not use any held out
data). Parsing performance is measured by f-score,
f = 2?P?RP+R , where P, R are labeled precision and
recall.
To further demonstrate our results for parser adap-
tation, we also performed the OI experiment where
seed data is taken from WSJ sections 2-21 and both
self-training and test data are taken from the Switch-
board corpus. The distance between the domains of
these corpora is much greater than the distance be-
tween the domains of WSJ and Brown. The Brown
and Switchboard corpora were divided to sections in
the same way.
We have also performed all four experiments with
the seed data taken from the Brown training section.
3 (Steedman et al, 2003a) used the first 500 sentences of
WSJ training section as seed data. For direct comparison, we
performed our protocol in the II scenario using the first 500 or
1000 sentences of WSJ training section as seed data and got
similar results to those reported below for our protocol with ran-
dom selection. We also applied the protocol of Steedman et al
to scenario II with 500 randomly selected sentences, getting no
improvement over the random baseline.
The results were very similar and will not be detailed
here due to space constraints.
5 Results
5.1 In-domain seed data
In these two experiments we show that when the
seed and test data are taken from the same domain, a
very significant enhancement of parser performance
can be achieved, whether the self-training material
is in-domain (II) or out-of-domain (IO). Figure 1
shows the improvement in parser f-score when self-
training data is used, compared to when it is not
used. Table 1 shows the reduction in manually an-
notated seed data needed to achieve certain f-score
levels. The enhancement in performance is very im-
pressive in the in-domain self-training data scenario
? a reduction of 50% in the number of manually an-
notated sentences needed for achieving 75 and 80 f-
score values. A significant improvement is achieved
in the out-of-domain self-training scenario as well.
Table 2 compares our results with self-training
and co-training results reported by (Steedman et al
20003a; 2003b). As stated earlier, the experimental
setup of these works is similar to ours, but the self-
training protocols are different. For self-training,
our II improves an absolute 3.74% over their 74.3%
result, which constitutes a 14.5% reduction in error
(from 25.7%).
The table shows that for both seed sizes our
self training protocol outperforms both the self-
training and co-training protocols of (Steedman et
al, 20003a; 2003b). Results are not included in the
table only if they are not reported in the relevant pa-
per. The self-training protocol of (Steedman et al,
2003a) does not actually improve over the baseline
of using only the seed data. Section 3 discussed a
possible explanation to the difference in results.
In Table 3 we compare our results to the results of
the methods tested in (Becker and Osborne, 2005)
(including TE)4. To do that, we compare the reduc-
tion in manually annotated data needed to achieve
an f-score value of 80 on WSJ section 23 achieved
by each method. We chose this measure since it is
4The measure is constituents and not sentences because this
is how results are reported in (Becker and Osborne, 2005).
However, the same reduction is obtained when sentences are
counted, because the number of constituents is averaged when
taking many sentences.
619
f-score 75 80
Seed data only 600(0%) 1400(0%)
II 300(50%) 700(50%)
IO 500(17%) 1200(14.5%)
Table 1: Number of in-domain seed sentences
needed for achieving certain f-scores. Reductions
compared to no self-training (line 1) are given in
parentheses.
Seed
size
our
II
our
IO
Steedman
ST
Steedman
CT
Steedman
CT
2003a 2003b
500
sent.
78.04 75.81 74.3 76.9 ?-
1,000
sent.
81.43 79.49 ?- 79 81.2
Table 2: F-scores of our in-domain-seed self-
training vs. self-training (ST) and co-training (CT)
of (Steedman et al 20003a; 2003b).
the only explicitly reported number in that work. As
the table shows, our method is superior: our reduc-
tion of 50% constitutes an improvement of 66% over
their best reduction of 30.6%.
When applying self-training to a parser trained
with a small dataset we expect the coverage of the
parser to increase, since the combined training set
should contain items that the seed dataset does not.
On the other hand, since the accuracy of annota-
tion of such a parser is poor (see the no self-training
curve in Figure 1) the combined training set surely
includes inaccurate labels that might harm parser
performance. Figure 2 (left) shows the increase in
coverage achieved for in-domain and out-of-domain
self-training data. The improvements induced by
both methods are similar. This is quite surpris-
ing given that the Brown sections we used as self-
training data contain science, fiction, humor, ro-
mance, mystery and adventure texts while the test
section in these experiments, WSJ section 23, con-
tains only news articles.
Figure 2 also compares recall (middle) and preci-
sion (right) for the different methods. For II there
is a significant improvement in both precision and
recall even though many more sentences are parsed.
For IO, there is a large gain in recall and a much
smaller loss in precision, yielding a substantial im-
provement in f-score (Figure 1).
F -
score
This
work - II
Becker
unparsed
Becker en-
tropy/unparsed
Hwa
TE
80 50% 29.4% 30.6% -5.7%
Table 3: Reduction of the number of manually anno-
tated constituents needed for achieving f score value
of 80 on section 23 of the WSJ. In all cases the seed
and additional sentences selected to train the parser
are taken from sections 02-21 of WSJ.
5.2 Out-of-domain seed data
In these two experiments we show that self-training
is valuable for adapting parsers from one domain to
another. Figure 3 compares out-of-domain seed data
used with in-domain (OI) or out-of-domain (OO)
self-training data against the baseline of training
only with the out-of-domain seed data.
The left graph shows a significant improvement
in f-score. In the middle and right graphs we exam-
ine the quality of the parses produced by the model
by plotting recall and precision vs. seed size. Re-
garding precision, the difference between the three
conditions is small relative to the f-score difference
shown in the left graph. The improvement in the
recall measure is much greater than the precision
differences, and this is reflected in the f-score re-
sult. The gain in coverage achieved by both meth-
ods, which is not shown in the figure, is similar to
that reported for the in-domain seed experiments.
The left graph along with the increase in coverage
show the power of self-training in parser adaptation
when small seed datasets are used: not only do OO
and OI parse many more sentences than the baseline,
but their f-score values are consistently better.
To see how much manually annotated data can
be saved by using out-of-domain seed, we train the
parsing model with manually annotated data from
the Brown training section, as described in Sec-
tion 4. We assume that given a fixed number of
training sentences the best performance of the parser
without self-training will occur when these sen-
tences are selected from the domain of the test sec-
tion, the Brown corpus. We compare the amounts of
manually annotated data needed to achieve certain f-
score levels in this condition with the corresponding
amounts of data needed by OI and OO. The results
are summarized in Table 4. We compare to two base-
lines using in- and out-of-domain seed data without
620
0 200 400 600 800 100040
50
60
70
80
90
number of manually annotated sentences
f s
co
re
 
 
no self training
wsj self?training
brown self?training
1000 1200 1400 1600 1800 200078
79
80
81
82
83
84
number of manually annotated sentences
f s
co
re
 
 
no self?training
wsj self?training
brown self?training
Figure 1: Number of seed sentences vs. f-score, for the two in-domain seed experiments: II (triangles) and
IO (squares), and for the no self-training baseline. Self-training provides a substantial improvement.
0 500 1000 1500 20001000
1500
2000
2500
number of manually annotated sentences
n
u
m
be
r o
f c
ov
er
ed
 s
en
te
nc
es
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200020
40
60
80
100
number of manually annotated sentences
re
c
a
ll
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200065
70
75
80
85
number of manually annotated sentences
pr
ec
is
io
n
 
 
no self?training
wsj self?training
brown self?training
Figure 2: Number of seed sentences vs. coverage (left), recall (middle) and precision (right) for the two
in-domain seed experiments: II (triangles) and IO (squares), and for the no self-training baseline.
any self-training. The second line (ID) serves as a
reference to compute how much manual annotation
of the test domain was saved, and the first line (OD)
serves as a reference to show by how much self-
training improves the out-of-domain baseline. The
table stops at an f-score of 74 because that is the
best that the baselines can do.
A significant reduction in annotation cost over the
ID baseline is achieved where the seed size is be-
tween 100 and 1200. Improvement over the OD
baseline is for the whole range of seed sizes. Both
OO and OI achieve 20-33% reduction in manual an-
notation compared to the ID baseline and enhance
the performance of the parser by as much as 42.9%.
The only previous work that adapts a parser
trained on a small dataset between domains is that
of (Steedman et al, 2003a), which used co-training
(no self-training results were reported there or else-
where). In order to compare with that work, we per-
formed OI with seed taken from the Brown corpus
and self-training and test taken from WSJ, which
is the setup they use, obtaining a similar improve-
ment to that reported there. However, co-training is
a more complex method that requires an additional
parser (LTAG in their case).
To further substantiate our results for the parser
adaptation scenario, we used an additional corpus,
Switchboard. Figure 4 shows the results of an OI
experiment with WSJ seed and Switchboard self-
training and test data. Although the domains of these
two corpora are very different (more so than WSJ
and Brown), self-training provides a substantial im-
provement.
We have also performed all four experiments with
Brown and WSJ trading places. The results obtained
were very similar to those reported here, and will not
be detailed due to lack of space.
6 Analysis
In this section we try to better understand the ben-
efit in using self-training with small seed datasets.
We formulate the following criterion: the number of
words in a test sentence that do not appear in the
seed data (?unknown words?) is a strong indicator
621
0 500 1000 1500 200030
40
50
60
70
80
number of manually annotated sentences
f s
co
re
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200020
30
40
50
60
70
80
number of manually annotated sentences
re
c
a
ll
 
 
no self?training
wsj self?training
brown self?training
0 500 1000 1500 200072
74
76
78
80
82
number of manually annotated sentences
pr
ec
is
io
n
 
 
no self?training
wsj self?training
brown self?training
Figure 3: Number of seed sentences vs. f-score (left), recall (middle) and precision (right), for the two
out-of-domain seed data experiments: OO (triangles) and OI (squares), and for the no self-training baseline.
f-sc. 66 68 70 72 74
OD 600 800 1, 000 1, 400 ?
ID 600 700 800 1, 000 1, 200
OO 400 500 600 800 1100
33, 33 28.6, 37.5 33, 40 20, 42.9 8, ?
OI 400 500 600 800 1, 300
33, 33 28.6, 37.5 33, 40 20, 42.9 ?8, ?
Table 4: Number of manually annotated seed sen-
tences needed for achieving certain f-score values.
The first two lines show the out-of-domain and in-
domain seed baselines. The reductions compared to
the baselines is given as ID, OD.
0 500 1000 1500 200010
20
30
40
50
number of manually annotated sentences
f s
co
re
 
 
switchboard self?training
no self?training
Figure 4: Number of seed sentences vs. f-score,
for the OI experiment using WSJ seed data and
SwitchBoard self-training and test data. In spite of
the strong dissimilarity between the domains, self-
training provides a substantial improvement.
to whether it is worthwhile to use small seed self-
training. Figure 5 shows the number of unknown
words in a sentence vs. the probability that the self-
training model will parse a sentence no worse (up-
per curve) or better (lower curve) than the baseline
model.
The upper curve shows that regardless of the
0 10 20 30 40 500
0.2
0.4
0.6
0.8
1
number of unknown words
pr
ob
ab
ili
ty
 
 
ST > baseline
ST >= baseline
Figure 5: For sentences having the same number of
unknown words, we show the probability that the
self-training model parses a sentence from the set
no worse (upper curve) or better (lower curve) than
the baseline model.
number of unknown words in the sentence, there is
more than 50% chance that the self-training model
will not harm the result. This probability decreases
from almost 1 for a very small number of unknown
words to about 0.55 for 50 unknown words. The
lower curve shows that when the number of un-
known words increases, the probability that the
self-training model will do better than the baseline
model increases from almost 0 (for a very small
number of unknown words) to about 0.55. Hence,
the number of unknown words is an indication for
the potential benefit (value on the lower curve)
and risk (1 minus the value on the upper curve) in
using the self-training model compared to using the
baseline model. Unknown words were not identified
in (McClosky et al, 2006a) as a useful predictor for
the benefit of self-training.
622
We also identified a length effect similar to that
studied by (McClosky et al, 2006a) for self-training
(using a reranker and large seed, as detailed in Sec-
tion 2). Due to space limitations we do not discuss
it here.
7 Discussion
Self-training is usually not considered to be a valu-
able technique in improving the performance of gen-
erative statistical parsers, especially when the man-
ually annotated seed sentence dataset is small. In-
deed, in the II scenario, (Steedman et al, 2003a;
McClosky et al, 2006a; Charniak, 1997) reported
no improvement of the base parser for small (500
sentences, in the first paper) and large (40K sen-
tences, in the last two papers) seed datasets respec-
tively. In the II, OO, and OI scenarios, (McClosky et
al, 2006a; 2006b) succeeded in improving the parser
performance only when a reranker was used to re-
order the 50-best list of the generative parser, with a
seed size of 40K sentences. Bacchiani et al(2006)
improved the parser performance in the OI scenario
but their seed size was large (about 20K sentences).
In this paper we have shown that self-training
can enhance the performance of generative parsers,
without a reranker, in four in- and out-of-domain
scenarios using a small seed dataset. For the II, IO
and OO scenarios, we are the first to show improve-
ment by self-training for generative parsers. We
achieved a 50% (20-33%) reduction in annotation
cost for the in-domain (out-of-domain) seed data
scenarios. Previous work with small seed datasets
considered only the II and OI scenarios. Our results
for the former are better than any previous method,
and our results for the latter (which are the first
reported self-training results) are similar to previ-
ous results for co-training, a more complex method.
We demonstrated our results using three corpora of
varying degrees of domain difference.
A direction for future research is combining
self-training data from various domains to enhance
parser adaptation.
Acknowledgement. We would like to thank Dan
Roth for his constructive comments on this paper.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat, 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Daniel Bikel, 2004. Code developed at University of
Pennsylvania. http://www.cis.upenn.edu.bikel.
Avrim Blum and Tom M. Mitchell, 1998. Combining la-
beled and unlabeled data with co-training. COLT ?98.
Eugene Charniak, 1997. Statistical parsing with a
context-free grammar and word statistics. AAAI ?97.
Eugene Charniak, 2000. A maximum-entropy-inspired
parser. ANLP ?00.
Eugene Charniak and Mark Johnson, 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. ACL ?05.
Stephen Clark, James Curran, and Miles Osborne,
2003. Bootstrapping pos taggers using unlabelled
data. CoNLL ?03.
David A. Cohn, Les Atlas, and Richard E. Ladner, 1994.
Improving generalization with active learning. Ma-
chine Learning, 15(2):201?221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Rebecca Hwa, Miles Osborne, Anoop Sarkar and Mark
Steedman, 2003. Corrected co-training for statistical
parsers. In ICML ?03, Workshop on the Continuum
from Labeled to Unlabeled Data in Machine Learning
and Data Mining.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Terry Koo and Michael Collins, 2005. Hidden-variable
models for discriminative reranking. EMNLP ?05.
David McClosky, Eugene Charniak, and Mark John-
son, 2006a. Effective self-training for parsing. HLT-
NAACL ?06.
David McClosky, Eugene Charniak, and Mark Johnson,
2006b. Reranking and self-training for parser adapta-
tion. ACL-COLING ?06.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca
Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim, 2003a. Bootstrap-
ping statistical parsers from small datasets. EACL ?03.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles
Osborne, Anoop Sarkar, Julia Hockenmaier, Paul
Ruhlen,Steven Baker, and Jeremiah Crim, 2003b. Ex-
ample selection for bootstrapping statistical parsers.
NAACL ?03.
623
Proceedings of ACL-08: HLT, pages 227?235,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Classification of Semantic Relationships between Nominals
Using Pattern Clusters
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
There are many possible different semantic re-
lationships between nominals. Classification
of such relationships is an important and dif-
ficult task (for example, the well known noun
compound classification task is a special case
of this problem). We propose a novel pat-
tern clusters method for nominal relationship
(NR) classification. Pattern clusters are dis-
covered in a large corpus independently of
any particular training set, in an unsupervised
manner. Each of the extracted clusters cor-
responds to some unspecified semantic rela-
tionship. The pattern clusters are then used
to construct features for training and classifi-
cation of specific inter-nominal relationships.
Our NR classification evaluation strictly fol-
lows the ACL SemEval-07 Task 4 datasets and
protocol, obtaining an f-score of 70.6, as op-
posed to 64.8 of the best previous work that
did not use the manually provided WordNet
sense disambiguation tags.
1 Introduction
Automatic extraction and classification of seman-
tic relationships is a major field of activity, of both
practical and theoretical interest. A prominent type
of semantic relationships is that holding between
nominals1. For example, in noun compounds many
different semantic relationships are encoded by the
same simple form (Girju et al, 2005): ?dog food? de-
notes food consumed by dogs, while ?summer morn-
1Our use of the term ?nominal? follows (Girju et al, 2007),
and includes simple nouns, noun compounds and multiword ex-
pressions serving as nouns.
ing? denotes a morning that happens in the summer.
These two relationships are completely different se-
mantically but are similar syntactically, and distin-
guishing between them could be essential for NLP
applications such as question answering and ma-
chine translation.
Relation classification usually relies on a train-
ing set in the form of tagged data. To improve re-
sults, some systems utilize additional manually con-
structed semantic resources such as WordNet (WN)
(Beamer et al, 2007). However, in many domains
and languages such resources are not available. Fur-
thermore, usage of such resources frequently re-
quires disambiguation and connection of the data to
the resource (word sense disambiguation in the case
of WordNet). Manual disambiguation is unfeasible
in many practical tasks, and an automatic one may
introduce errors and greatly degrade performance. It
thus makes sense to try to minimize the usage of
such resources, and utilize only corpus contexts in
which the relevant words appear.
A leading method for utilizing context informa-
tion for classification and extraction of relationships
is that of patterns (Hearst, 1992; Pantel and Pen-
nacchiotti, 2006). The standard classification pro-
cess is to find in an auxiliary corpus a set of patterns
in which a given training word pair co-appears, and
use pattern-word pair co-appearance statistics as fea-
tures for machine learning algorithms.
In this paper we introduce a novel approach,
based on utilizing pattern clusters that are prepared
separately and independently of the training set. We
do not utilize any manually constructed resource or
any manual tagging of training data beyond the cor-
227
rect classification, thus making our method applica-
ble to fully automated tasks and less domain and lan-
guage dependent. Moreover, our pattern clustering
algorithm is fully unsupervised.
Our method is based on the observation that while
each lexical pattern can be highly ambiguous, sev-
eral patterns in conjunction can reliably define and
represent a lexical relationship. Accordingly, we
construct pattern clusters from a large generic cor-
pus, each such cluster potentially representing some
important generic relationship. This step is done
without accessing any training data, anticipating that
most meaningful relationships, including those in a
given classification problem, will be represented by
some of the discovered clusters. We then use the
training set to label some of the clusters, and the la-
beled clusters to assign classes to tested items. One
of the advantages of our method is that it can be used
not only for classification, but also for further anal-
ysis and retrieval of the observed relationships2.
The semantic relationships between the compo-
nents of noun compounds and between nominals in
general are not easy to categorize rigorously. Sev-
eral different relationship hierarchies have been pro-
posed (Nastase and Szpakowicz, 2003; Moldovan et
al., 2004). Some classes, like Container-Contained,
Time-Event and Product-Producer, appear in sev-
eral classification schemes, while classes like Tool-
Object are more vaguely defined and are subdivided
differently. Recently, SemEval-07 Task 4 (Girju et
al., 2007) proposed a benchmark dataset that in-
cludes a subset of 7 widely accepted nominal rela-
tionship (NR) classes, allowing consistent evalua-
tion of different NR classification algorithms. In the
SemEval event, 14 research teams evaluated their al-
gorithms using this benchmark. Some of the teams
have used the manually annotated WN labels pro-
vided with the dataset, and some have not.
We evaluated our algorithm on SemEval-07 Task
4 data, showing superior results over participating
algorithms that did not utilize WordNet disambigua-
tion tags. We also show how pattern clusters can be
used for a completely unsupervised classification of
2In (Davidov and Rappoport, 2008) we focus on the pat-
tern cluster resource type itself, presenting an evaluation of its
intrinsic quality based on SAT tests. In the present paper we
focus on showing how the resource can be used to improve a
known NLP task.
the test set. Since in this case no training data is
used, this allows the automated discovery of a po-
tentially unbiased classification scheme.
Section 2 discusses related work, Section 3 out-
lines the pattern clustering algorithm, Section 4 de-
tails three classification methods, and Sections 5 and
6 describe the evaluation protocol and results.
2 Related Work
Numerous methods have been devised for classifica-
tion of semantic relationships, among which those
holding between nominals constitute a prominent
category. Major differences between these methods
include available resources, degree of preprocessing,
features used, classification algorithm and the nature
of training/test data.
2.1 Available Resources
Many relation classification algorithms utilize
WordNet. Among the 15 systems presented by
the 14 SemEval teams, some utilized the manually
provided WordNet tags for the dataset pairs (e.g.,
(Beamer et al, 2007)). In all cases, usage of WN
tags improves the results significantly. Some other
systems that avoided using the labels used WN as
a supporting resource for their algorithms (Costello,
2007; Nakov and Hearst, 2007; Kim and Baldwin,
2007). Only three avoided WN altogether (Hen-
drickx et al, 2007; Bedmar et al, 2007; Aramaki
et al, 2006).
Other resources used for relationship discovery
include Wikipedia (Strube and Ponzetto, 2006), the-
sauri or synonym sets (Turney, 2005) and domain-
specific semantic hierarchies like MeSH (Rosario
and Hearst, 2001).
While usage of these resources is beneficial in
many cases, high quality word sense annotation is
not easily available. Besides, lexical resources are
not available for many languages, and their coverage
is limited even for English when applied to some re-
stricted domains. In this paper we do not use any
manually annotated resources apart from the classi-
fication training set.
2.2 Degree of Preprocessing
Many relationship classification methods utilize
some language-dependent preprocessing, like deep
or shallow parsing, part of speech tagging and
228
named entity annotation (Pantel et al, 2004). While
the obtained features were shown to improve classi-
fication performance, they tend to be language de-
pendent and error-prone when working on unusual
text domains and are also highly computationally in-
tensive when processing large corpora. To make our
approach as language independent and efficient as
possible, we avoided using any such preprocessing
techniques.
2.3 Classification Features
A wide variety of features are used by different
algorithms, ranging from simple bag-of-words fre-
quencies to WordNet-based features (Moldovan et
al., 2004). Several studies utilize syntactic features.
Many other works manually develop a set of heuris-
tic features devised with some specific relationship
in mind, like a WordNet-based meronymy feature
(Bedmar et al, 2007) or size-of feature (Aramaki
et al, 2006). However, the most prominent feature
type is based on lexico-syntactic patterns in which
the related words co-appear.
Since (Hearst, 1992), numerous works have used
patterns for discovery and identification of instances
of semantic relationships (e.g., (Girju et al, 2006;
Snow et al, 2006; Banko et al 2007)). Rosenfeld
and Feldman (2007) discover relationship instances
by clustering entities appearing in similar contexts.
Strategies were developed for discovery of multi-
ple patterns for some specified lexical relationship
(Pantel and Pennacchiotti, 2006) and for unsuper-
vised pattern ranking (Turney, 2006). Davidov et
al. (2007) use pattern clusters to define general rela-
tionships, but these are specific to a given concept.
No study so far has proposed a method to define, dis-
cover and represent general relationships present in
an arbitrary corpus.
In (Davidov and Rappoport, 2008) we present
an approach to extract pattern clusters from an un-
tagged corpus. Each such cluster represents some
unspecified lexical relationship. In this paper, we
use these pattern clusters as the (only) source of ma-
chine learning features for a nominal relationship
classification problem. Unlike the majority of cur-
rent studies, we avoid using any other features that
require some language-specific information or are
devised for specific relationship types.
2.4 Classification Algorithm
Various learning algorithms have been used for re-
lation classification. Common choices include vari-
ations of SVM (Girju et al, 2004; Nastase et al,
2006), decision trees and memory-based learners.
Freely available tools like Weka (Witten and Frank,
1999) allow easy experimentation with common
learning algorithms (Hendrickx et al, 2007). In this
paper we did not focus on a single ML algorithm,
letting algorithm selection be automatically based
on cross-validation results on the training set, as in
(Hendrickx et al, 2007) but using more algorithms
and allowing a more flexible parameter choice.
2.5 Training Data
As stated above, several categorization schemes for
nominals have been proposed. Nastase and Sz-
pakowicz (2003) proposed a two-level hierarchy
with 5 (30) classes at the top (bottom) levels3. This
hierarchy and a corresponding dataset were used in
(Turney, 2005; Turney, 2006) and (Nastase et al,
2006) for evaluation of their algorithms. Moldovan
et al (2004) proposed a different scheme with 35
classes. The most recent dataset has been developed
for SemEval 07 Task 4 (Girju et al, 2007). This
manually annotated dataset includes a representative
rather than exhaustive list of 7 important nominal
relationships. We have used this dataset, strictly fol-
lowing the evaluation protocol. This made it possi-
ble to meaningfully compare our method to state-of-
the-art methods for relation classification.
3 Pattern Clustering Algorithm
Our pattern clustering algorithm is designed for the
unsupervised definition and discovery of generic se-
mantic relationships. The algorithm first discovers
and clusters patterns in which a single (?hook?) word
participates, and then merges the resulting clusters
to form the final structure. In (Davidov and Rap-
poport, 2008) we describe the algorithm at length,
discuss its behavior and parameters in detail, and
evaluate its intrinsic quality. To assist readers of
the present paper, in this section we provide an
overview. Examples of some resulting pattern clus-
ters are given in Section 6. We refer to a pattern
3Actually, there were 50 relationships at the bottom level,
but valid nominal instances were found only for 30.
229
contained in our clusters (a pattern type) as a ?pat-
tern? and to an occurrence of a pattern in the corpus
(a pattern token) as a ?pattern instance?.
The algorithm does not rely on any data from the
classification training set, hence we do not need to
repeat its execution for different classification prob-
lems. To calibrate its parameters, we ran it a few
times with varied parameters settings, producing
several different configurations of pattern clusters
with different degrees of noise, coverage and granu-
larity. We then chose the best configuration for our
task automatically without re-running pattern clus-
tering for each specific problem (see Section 5.3).
3.1 Hook Words and Hook Corpora
As a first step, we randomly sample a set of hook
words, which will be used in order to discover re-
lationships that generally occur in the corpus. To
avoid selection of ambiguous words or typos, we do
not select words with frequency higher than a pa-
rameter FC and lower than a threshold FB . We also
limit the total number N of hook words. For each
hook word, we now create a hook corpus, the set of
the contexts in which the word appears. Each con-
text is a window containing W words or punctuation
characters before and after the hook word.
3.2 Pattern Specification
To specify patterns, following (Davidov and Rap-
poport, 2006) we classify words into high-
frequency words (HFWs) and content words (CWs).
A word whose frequency is more (less) than FH
(FC) is considered to be a HFW (CW). Our patterns
have the general form
[Prefix] CW1 [Infix] CW2 [Postfix]
where Prefix, Infix and Postfix contain only HFWs.
We require Prefix and Postfix to be a single HFW,
while Infix can contain any number of HFWs (limit-
ing pattern length by window size). This form may
include patterns like ?such X as Y and?. At this stage,
the pattern slots can contain only single words; how-
ever, when using the final pattern clusters for nomi-
nal relationship classification, slots can contain mul-
tiword nominals.
3.3 Discovery of Target Words
For each of the hook corpora, we now extract all
pattern instances where one CW slot contains the
hook word and the other CW slot contains some
other (?target?) word. To avoid the selection of com-
mon words as target words, and to avoid targets ap-
pearing in pattern instances that are relatively fixed
multiword expressions, we sort all target words in
a given hook corpus by pointwise mutual informa-
tion between hook and target, and drop patterns ob-
tained from pattern instances containing the lowest
and highest L percent of target words.
3.4 Pattern Clustering
We now have for each hook corpus a set of patterns,
together with the target words used for their extrac-
tion, and we want to cluster pattern types. First,
we group in clusters all patterns extracted using the
same target word. Second, we merge clusters that
share more than S percent of their patterns. Some
patterns can appear in more than a single cluster.
Finally, we merge pattern clusters from different
hook corpora, to avoid clusters specific to a single
hook word. During merging, we define and utilize
core patterns and unconfirmed patterns, which are
weighed differently during cluster labeling (see Sec-
tion 4.2). We merge clusters from different hook
corpora using the following algorithm:
1. Remove all patterns originating from a single hook
corpus only.
2. Mark all patterns of all present clusters as uncon-
firmed.
3. While there exists some cluster C1 from corpus DX
containing only unconfirmed patterns:
(a) Select a cluster with a minimal number of pat-
terns.
(b) For each corpus D different from DX :
i. Scan D for clusters C2 that share at least
S percent of their patterns, and all of their
core patterns, with C1.
ii. Add all patterns of C2 to C1, setting all
shared patterns as core and all others as
unconfirmed.
iii. Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-
move C1.
4. If several clusters have the same set of core patterns
merge them according to rules (i,ii).
At the end of this stage, we have a set of pattern
clusters where for each cluster there are two subsets,
core patterns and unconfirmed patterns.
230
4 Relationship Classification
Up to this stage we did not access the training set in
any way and we did not use the fact that the target re-
lations are those holding between nominals. Hence,
only a small part of the acquired pattern clusters may
be relevant for a given NR classification task, while
other clusters can represent completely different re-
lationships (e.g., between verbs). We now use the
acquired clusters to learn a model for the given la-
beled training set and to use this model for classifi-
cation of the test set. First we describe how we deal
with data sparseness. Then we propose a HITS mea-
sure used for cluster labeling, and finally we present
three different classification methods that utilize pat-
tern clusters.
4.1 Enrichment of Provided Data
Our classification algorithm is based on contexts
of given nominal pairs. Co-appearance of nomi-
nal pairs can be very rare (in fact, some word pairs
in the Task 4 set co-appear only once in Yahoo
web search). Hence we need more contexts where
the given nominals or nominals similar to them co-
appear. This step does not require the training la-
bels (the correct classifications), so we do it for both
training and test pairs. We do it in two stages: ex-
tracting similar nominals, and obtaining more con-
texts.
4.1.1 Extracting more words
For each nominal pair (w1, w2) in a given sentence
S, we use a method similar to (Davidov and Rap-
poport, 2006) to extract words that have a shared
meaning with w1 or w2. We discover such words
by scanning our corpora and querying the web for
symmetric patterns (obtained automatically from the
corpus as in (Davidov and Rappoport, 2006)) that
contain w1 or w2. To avoid getting instances of
w1,2 with a different meaning, we also require that
the second word will appear in the same text para-
graph or the same web page. For example, if we are
given a pair <loans, students> and we see a sen-
tence ?... loans and scholarships for students and
professionals ...?, we use the symmetric pattern ?X
and Y? to add the word scholarships to the group of
loans and to add the word professionals to the group
of students. We do not take words from the sen-
tence ?In European soccer there are transfers and
loans...? since its context does not contain the word
students. In cases where there are only several or
zero instances where the two nominals co-appear,
we dismiss the latter rule and scan for each nominal
separately. Note that ?loans? can also be a verb, so
usage of a part-of-speech tagger might reduce noise.
If the number of instances for a desired nom-
inal is very low, our algorithm trims the first
words in these nominal and repeats the search (e.g.,
<simulation study, voluminous results> becomes
<study, results>). This step is the only one specific
to English, using the nature of English noun com-
pounds. Our desire in this case is to keep the head
words.
4.1.2 Extracting more contexts using the new
words
To find more instances where nominals similar to
w1 and w2 co-appear in HFW patterns, we construct
web queries using combinations of each nominal?s
group and extract patterns from the search result
snapshots (the two line summary provided by search
engines for each search result).
4.2 The HITS Measure
To use clusters for classification we define a HITS
measure similar to that of (Davidov et al, 2007), re-
flecting the affinity of a given nominal pair to a given
cluster. We use the pattern clusters from Section 3
and the additional data collected during the enrich-
ment phase to estimate a HITS value for each cluster
and each pair in the training and test sets. For a given
nominal pair (w1, w2) and cluster C with n core pat-
terns Pcore and m unconfirmed patterns Punconf ,
HITS(C, (w1, w2)) =
|{p; (w1, w2) appears in p ? Pcore}| /n+
?? |{p; (w1, w2) appears in p ? Punconf}| /m.
In this formula, ?appears in? means that the nomi-
nal pair appears in instances of this pattern extracted
from the original corpus or retrieved from the web
at the previous stage. Thus if some pair appears in
most of the patterns of some cluster it receives a high
HITS value for this cluster. ? (0..1) is a parameter
that lets us modify the relative weight of core and
unconfirmed patterns.
231
4.3 Classification Using Pattern Clusters
We present three ways to use pattern clusters for re-
lationship classification.
4.3.1 Classification by cluster labeling
One way to train a classifier in our case is to attach
a single relationship label to each cluster during the
training phase, and to assign each unlabeled pair to
some labeled cluster during the test phase. We use
the following normalized HITS measure to label the
involved pattern clusters. Denote by ki the number
of training pairs in class i in training set T . Then
Label(C) = argmaxi
?
p?T,Label(p)=i
hits(C, p)/ki
Clusters where the above sum is zero remain un-
labeled. In the test phase we assign to each test pair
p the label of the labeled cluster C that received the
highest HITS(C, p) value. If there are several clus-
ters with a highest HITS value, then the algorithm se-
lects a ?clarifying? set of patterns ? patterns that are
different in these best clusters. Then it constructs
clarifying web queries that contain the test nomi-
nal pair inside the clarifying patterns. The effect is
to increment the HITS value of the cluster contain-
ing a clarifying pattern if an appropriate pattern in-
stance (including the target nominals) was found on
the web. We start with the most frequent clarifying
pattern and perform additional queries until no clar-
ifying patterns are left or until some labeled cluster
obtains a highest HITS value. If no patterns are left
but there are still several winning clusters, we assign
to the pair the label of the cluster with the largest
number of pattern instances in the corpus.
One advantage of this method is that we get as
a by-product a set of labeled pattern clusters. Ex-
amination of this set can help to distinguish and an-
alyze (by means of patterns) which different rela-
tionships actually exist for each class in the train-
ing set. Furthermore, labeled pattern clusters can be
used for web queries to obtain additional examples
of the same relationship.
4.3.2 Classification by cluster HITS values as
features
In this method we treat the HITS measure for a clus-
ter as a feature for a machine learning classification
algorithm. To do this, we construct feature vectors
from each training pair, where each feature is the
HITS measure corresponding to a single pattern clus-
ter. We prepare test vectors similarly. Once we have
feature vectors, we can use a variety of classifiers
(we used those in Weka) to construct a model and to
evaluate it on the test set.
4.3.3 Unsupervised clustering
If we are not given any training set, it is still possi-
ble to separate between different relationship types
by grouping the feature vectors of Section 4.3.2 into
clusters. This can be done by applying k-means or
another clustering algorithm to the feature vectors
described above. This makes the whole approach
completely unsupervised. However, it does not pro-
vide any inherent labeling, making an evaluation dif-
ficult.
5 Experimental Setup
The main problem in a fair evaluation of NR classifi-
cation is that there is no widely accepted list of pos-
sible relationships between nominals. In our eval-
uation we have selected the setup and data from
SemEval-07 Task 4 (Girju et al, 2007). Selecting
this type of dataset alowed us to compare to 6 sub-
mitted state-of-art systems that evaluated on exactly
the same data and to 9 other systems that utilize
additional information (WN labels). We have ap-
plied our three different classification methods on
the given data set.
5.1 SemEval-07 Task 4 Overview
Task 4 (Girju et al, 2007) involves classification of
relationships between simple nominals other than
named entities. Seven distinct relationships were
chosen: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. For each relationship, the
provided dataset consists of 140 training and 70 test
examples. Examples were binary tagged as belong-
ing/not belonging to the tested relationship. The vast
majority of negative examples were near-misses, ac-
quired from the web using the same lexico-syntactic
patterns as the positives. Examples appear as sen-
tences with the nominal pair tagged. Nouns in this
pair were manually labeled with their correspond-
ing WordNet 3 labels and the web queries used to
232
obtain the sentences. The 15 submitted systems
were assigned into 4 categories according to whether
they use the WordNet and Query tags (some systems
were assigned to more than a single category, since
they reported experiments in several settings). In our
evaluation we do not utilize WordNet or Query tags,
hence we compare ourselves with the corresponding
group (A), containing 6 systems.
5.2 Corpus and Web Access
Our algorithm uses two corpora. We estimate fre-
quencies and perform primary search on a local web
corpus containing about 68GB untagged plain text.
This corpus was extracted from the web starting
from open directory links, comprising English web
pages with varied topics and styles (Gabrilovich and
Markovitch, 2005). To enrich the set of given word
pairs and patterns as described in Section 4.1 and
to perform clarifying queries, we utilize the Yahoo
API for web queries. For each query, if the desired
words/patterns were found in a page link?s snapshot,
we do not use the link, otherwise we download the
page from the retrieved link and then extract the re-
quired data. If only several links were found for a
given word pair we perform local crawling to depth
3 in an attempt to discover more instances.
5.3 Parameters and Learning Algorithm
Our algorithm utilizes several parameters. Instead
of calibrating them manually, we only provided
a desired range for each, and the final parameter
values were obtained during selection of the best-
performing setup using 10-fold cross-validation on
the training set. For each parameter we have esti-
mated its desired range using the (Nastase and Sz-
pakowicz, 2003) set as a development set. Note that
this set uses an entirely different relationship classi-
fication scheme. We ran the pattern clustering phase
on 128 different sets of parameters, obtaining 128
different clustering schemes with varied granularity,
noise and coverage.
The parameter ranges obtained are: FC (meta-
pattern content word frequency and upper bound for
hook word selection): 100?5000 words per million
(wpm); FH (meta-pattern HFW): 10 ? 100 wpm;
FB (low word count for hook word filtering): 1?50
wpm; N (number of hook words): 100 ? 1000; W
(window size): 5 or window = sentence; L (tar-
get word mutual information filter): 1/3 ? 1/5; S
(cluster overlap filter for cluster merging): 2/3; ?
(core vs. unconfirmed weight for HITS estimation):
0.1 ? 0.01; S (commonality for cluster merging):
2/3. As designed, each parameter indeed influences
a certain effect. Naturally, the parameters are not
mutually independent. Selecting the best configu-
ration in the cross-validation phase makes the algo-
rithm flexible and less dependent on hard-coded pa-
rameter values.
Selection of learning algorithm and its algorithm-
specific parameters were done as follows. For each
of the 7 classification tasks (one per relationship
type), for each of the 128 pattern clustering schemes,
we prepared a list of most of the compatible al-
gorithms available in Weka, and we automatically
selected the model (a parameter set and an algo-
rithm) which gave the best 10-fold cross-validation
results. The winning algorithms were LWL (Atke-
son et al, 1997), SMO (Platt, 1999), and K* (Cleary
and Trigg, 1995) (there were 7 tasks, and different
algorithms could be selected for each task). We then
used the obtained model to classify the testing set.
This allowed us to avoid fixing parameters that are
best for a specific dataset but not for others. Since
each dataset has only 140 examples, the computa-
tion time of each learning algorithm is negligible.
6 Results
The pattern clustering phase results in 90 to 3000
distinct pattern clusters, depending on the parameter
setup. Manual sampling of these clusters indeed re-
veals that many clusters contain patterns specific to
some apparent lexical relationship. For example, we
have discovered such clusters as: {?buy Y accessory
for X!?, ?shipping Y for X?, ?Y is available for X?, ?Y
are available for X?, ?Y are available for X systems?,
?Y for X? } and {?best X for Y?, ?X types for Y?, ?Y
with X?, ?X is required for Y?, ?X as required for Y?,
?X for Y?}. Note that some patterns (?Y for X?) can
appear in many clusters.
We applied the three classification methods de-
scribed in Section 4.3 to Task 4 data. For super-
vised classification we strictly followed the SemEval
datasets and rules. For unsupervised classification
we did not use any training data. Using the k-means
algorithm, we obtained two nearly equal unlabeled
233
Method P R F Acc
Unsupervised clustering (4.3.3) 64.5 61.3 62.0 64.5
Cluster Labeling (4.3.1) 65.1 69.0 67.2 68.5
HITS Features (4.3.2) 69.1 70.6 70.6 70.1
Best Task 4 (no WordNet) 66.1 66.7 64.8 66.0
Best Task 4 (with WordNet) 79.7 69.8 72.4 76.3
Table 1: Our SemEval-07 Task 4 results.
Relation Type F Acc C
Cause-Effect 69.7 71.4 2
Instrument-Agency 76.5 74.2 1
Product-Producer 76.4 83.8 1
Origin-Entity 65.4 62.6 4
Theme-Tool 59.4 58.7 6
Part-Whole 74.3 70.9 1
Content-Container 72.6 69.2 2
Table 2: By-relation Task 4 HITS-based results. C is the
number of clusters with positive labels.
clusters containing test samples. For evaluation we
assigned a negative/positive label to these two clus-
ters according to the best alignment with true labels.
Table 1 shows our results, along with the best Task
4 result not using WordNet labels (Costello, 2007).
For reference, the best results overall (Beamer et al,
2007) are also shown. The table shows precision (P)
recall (R), F-score (F), and Accuracy (Acc) (percent-
age of correctly classified examples).
We can see that while our algorithm is not as good
as the best method that utilizes WordNet tags, results
are superior to all participants who did not use these
tags. We can also see that the unsupervised method
results are above the random baseline (50%). In fact,
our results (f-score 62.0, accuracy 64.5) are better
than the averaged results (58.0, 61.1) of the group
that did not utilize WN tags.
Table 2 shows the HITS-based classification re-
sults (F-score and Accuracy) and the number of pos-
itively labeled clusters (C) for each relation. As ob-
served by participants of Task 4, we can see that dif-
ferent sets vary greatly in difficulty. However, we
also obtain a nice insight as to why this happens ?
relations like Theme-Tool seem very ambiguous and
are mapped to several clusters, while relations like
Product-Producer seem to be well-defined by the ob-
tained pattern clusters.
The SemEval dataset does not explicitly mark
items whose correct classification requires analysis
of the context of the whole sentence in which they
appear. Since our algorithm does not utilize test sen-
tence contextual information, we do not expect it to
show exceptional performance on such items. This
is a good topic for future research.
Since the SemEval dataset is of a very spe-
cific nature, we have also applied our classification
framework to the (Nastase and Szpakowicz, 2003)
dataset, which contains 600 pairs labeled with 5
main relationship types. We have used the exact
evaluation procedure described in (Turney, 2006),
achieving a class f-score average of 60.1, as opposed
to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al,
2006). This shows that our method produces supe-
rior results for rather differing datasets.
7 Conclusion
Relationship classification is known to improve
many practical tasks, e.g., textual entailment (Tatu
and Moldovan, 2005). We have presented a novel
framework for relationship classification, based on
pattern clusters prepared as a standalone resource in-
dependently of the training set.
Our method outperforms current state-of-the-art
algorithms that do not utilize WordNet tags on Task
4 of SemEval-07. In practical situations, it would
not be feasible to provide a large amount of such
sense disambiguation tags manually. Our method
also shows competitive performance compared to
the majority of task participants that do utilize WN
tags. Our method can produce labeled pattern clus-
ters, which can be potentially useful for automatic
discovery of additional instances for a given rela-
tionship. We intend to pursue this promising direc-
tion in future work.
Acknowledgement. We would like to thank
the anonymous reviewers, whose comments have
greatly improved the quality of this paper.
References
Aramaki, E., Imai, T., Miyo, K., and Ohe, K., 2007.
UTH: semantic relation classification using physical
sizes. ACL SemEval ?07 Workshop.
Atkeson, C., Moore, A., and Schaal, S., 1997. Lo-
cally weighted learning. Artificial Intelligence Review,
11(1?5): 75?113.
234
Banko, M., Cafarella, M. J., Soderland, S., Broadhead,
M., and Etzioni, O., 2007. Open information extrac-
tion from the Web. IJCAI ?07.
Beamer, B., Bhat, S., Chee, B., Fister, A., Rozovskaya
A., and Girju, R., 2007. UIUC: A knowledge-rich ap-
proach to identifying semantic relations between nom-
inals. ACL SemEval ?07 Workshop.
Bedmar, I. S., Samy, D., and Martinez, J. L., 2007.
UC3M: Classification of semantic relations between
nominals using sequential minimal optimization. ACL
SemEval ?07 Workshop.
Cleary, J. G. , Trigg, L. E., 1995. K*: An instance-based
learner using and entropic distance measure. ICML
?95.
Costello, F. J., 2007. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently in a
database of noun-noun compounds. ACL SemEval ?07
Workshop.
Davidov, D., Rappoport, A., 2006. Efficient unsuper-
vised discovery of word categories using symmetric
patterns and high frequency words. COLING-ACL ?06
Davidov D., Rappoport A. and Koppel M., 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. ACL ?07.
Davidov, D., Rappoport, A., 2008. Unsupervised discov-
ery of generic relationships using pattern clusters and
its evaluation by automatically generated SAT analogy
questions. ACL ?08.
Gabrilovich, E., Markovitch, S., 2005. Feature gener-
ation for text categorization using world knowledge.
IJCAI ?05.
Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,
O., and Moldovan, D., 2004. Support vector ma-
chines applied to the classification of semantic rela-
tions in nominalized noun phrases. HLT/NAACL ?04
Workshop on Computational Lexical Semantics.
Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19(4):479-496.
Girju, R., Badulescu, A., and Moldovan, D., 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakowicz,
S., Turney, P., and Yuret, D., 2007. Task 04: Classi-
fication of semantic relations between nominal at Se-
mEval 2007. 4th Intl. Workshop on Semantic Evalua-
tions (SemEval ?07), in ACL ?07.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92
Hendrickx, I., Morante, R., Sporleder, C., and van den
Bosch, A., 2007. Machine learning of semantic rela-
tions with shallow features and almost no data. ACL
SemEval ?07 Workshop.
Kim, S.N., Baldwin, T., 2007. MELB-KB: Nominal
classification as noun compound interpretation. ACL
SemEval ?07 Workshop.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., and
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. HLT-NAACL ?04 Workshop on
Computational Lexical Semantics.
Nakov, P., and Hearst, M., 2007. UCB: System descrip-
tion for SemEval Task #4. ACL SemEval ?07 Work-
shop.
Nastase, V., Szpakowicz, S., 2003. Exploring noun-
modifier semantic relations. In Fifth Intl. Workshop
on Computational Semantics (IWCS-5).
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-
pakowicz, S., 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proceedings of the 21st National Conference
on Artificial Intelligence, Boston, MA.
Pantel, P., Ravichandran, D., and Hovy, E., 2004. To-
wards terascale knowledge acquisition. COLING ?04.
Pantel, P., Pennacchiotti, M., 2006. Espresso: leveraging
generic patterns for automatically harvesting semantic
relations. COLING-ACL ?06.
Platt, J., 1999. Fast training of support vector machines
using sequential minimal optimization. In Scholkopf,
Burges, and Smola, Advances in Kernel Methods ?
Support Vector Learning, pp. 185?208. MIT Press.
Rosario, B., Hearst, M., 2001. Classifying the semantic
relations in noun compounds. EMNLP ?01.
Rosenfeld, B., Feldman, R., 2007. Clustering for unsu-
pervised relation identification. CIKM ?07.
Snow, R., Jurafsky, D., Ng, A.Y., 2006. Seman-
tic taxonomy induction from heterogeneous evidence.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Tatu, M., Moldovan, D., 2005. A semantic approach to
recognizing textual entailment. HLT/EMNLP ?05.
Turney, P., 2005. Measuring semantic similarity by la-
tent relational analysis. IJCAI ?05.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Witten, H., Frank, E., 1999. Data Mining: Practical Ma-
chine Learning Tools and Techniques with Java Imple-
mentations. Morgan Kaufman, San Francisco, CA.
235
Proceedings of ACL-08: HLT, pages 692?700,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Discovery of Generic Relationships Using Pattern Clusters
and its Evaluation by Automatically Generated SAT Analogy Questions
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a novel framework for the dis-
covery and representation of general semantic
relationships that hold between lexical items.
We propose that each such relationship can be
identified with a cluster of patterns that cap-
tures this relationship. We give a fully unsu-
pervised algorithm for pattern cluster discov-
ery, which searches, clusters and merges high-
frequency words-based patterns around ran-
domly selected hook words. Pattern clusters
can be used to extract instances of the corre-
sponding relationships. To assess the quality
of discovered relationships, we use the pattern
clusters to automatically generate SAT anal-
ogy questions. We also compare to a set of
known relationships, achieving very good re-
sults in both methods. The evaluation (done
in both English and Russian) substantiates the
premise that our pattern clusters indeed reflect
relationships perceived by humans.
1 Introduction
Semantic resources can be very useful in many NLP
tasks. Manual construction of such resources is la-
bor intensive and susceptible to arbitrary human de-
cisions. In addition, manually constructed semantic
databases are not easily portable across text domains
or languages. Hence, there is a need for developing
semantic acquisition algorithms that are as unsuper-
vised and language independent as possible.
A fundamental type of semantic resource is that
of concepts (represented by sets of lexical items)
and their inter-relationships. While there is rel-
atively good agreement as to what concepts are
and which concepts should exist in a lexical re-
source, identifying types of important lexical rela-
tionships is a rather difficult task. Most established
resources (e.g., WordNet) represent only the main
and widely accepted relationships such as hyper-
nymy and meronymy. However, there are many
other useful relationships between concepts, such as
noun-modifier and inter-verb relationships. Identi-
fying and representing these explicitly can greatly
assist various tasks and applications. There are al-
ready applications that utilize such knowledge (e.g.,
(Tatu and Moldovan, 2005) for textual entailment).
One of the leading methods in semantics acqui-
sition is based on patterns (see e.g., (Hearst, 1992;
Pantel and Pennacchiotti, 2006)). The standard pro-
cess for pattern-based relation extraction is to start
with hand-selected patterns or word pairs express-
ing a particular relationship, and iteratively scan
the corpus for co-appearances of word pairs in pat-
terns and for patterns that contain known word pairs.
This methodology is semi-supervised, requiring pre-
specification of the desired relationship or hand-
coding initial seed words or patterns. The method
is quite successful, and examining its results in de-
tail shows that concept relationships are often being
manifested by several different patterns.
In this paper, unlike the majority of studies that
use patterns in order to find instances of given rela-
tionships, we use sets of patterns as the definitions
of lexical relationships. We introduce pattern clus-
ters, a novel framework in which each cluster cor-
responds to a relationship that can hold between the
lexical items that fill its patterns? slots. We present
a fully unsupervised algorithm to compute pat-
692
tern clusters, not requiring any, even implicit, pre-
specification of relationship types or word/pattern
seeds. Our algorithm does not utilize preprocess-
ing such as POS tagging and parsing. Some patterns
may be present in several clusters, thus indirectly ad-
dressing pattern ambiguity.
The algorithm is comprised of the following
stages. First, we randomly select hook words and
create a context corpus (hook corpus) for each hook
word. Second, we define a meta-pattern using high
frequency words and punctuation. Third, in each
hook corpus, we use the meta-pattern to discover
concrete patterns and target words co-appearing
with the hook word. Fourth, we cluster the patterns
in each corpus according to co-appearance of the tar-
get words. Finally, we merge clusters from different
hook corpora to produce the final structure. We also
propose a way to label each cluster by word pairs
that represent it best.
Since we are dealing with relationships that are
unspecified in advance, assessing the quality of the
resulting pattern clusters is non-trivial. Our evalu-
ation uses two methods: SAT tests, and compari-
son to known relationships. We used instances of
the discovered relationships to automatically gener-
ate analogy SAT tests in two languages, English and
Russian1. Human subjects answered these and real
SAT tests. English grades were 80% for our test and
71% for the real test (83% and 79% for Russian),
showing that our relationship definitions indeed re-
flect human notions of relationship similarity. In ad-
dition, we show that among our pattern clusters there
are clusters that cover major known noun-compound
and verb-verb relationships.
In the present paper we focus on the pattern clus-
ter resource itself and how to evaluate its intrinsic
quality. In (Davidov and Rappoport, 2008) we show
how to use the resource for a known task of a to-
tally different nature, classification of relationships
between nominals (based on annotated data), obtain-
ing superior results over previous work.
Section 2 discusses related work, and Section 3
presents the pattern clustering and labeling algo-
rithm. Section 4 describes the corpora we used and
the algorithm?s parameters in detail. Sections 5 and
1Turney and Littman (2005) automatically answers SAT
tests, while our focus is on generating them.
6 present SAT and comparison evaluation results.
2 Related Work
Extraction of relation information from text is a
large sub-field in NLP. Major differences between
pattern approaches include the relationship types
sought (including domain restrictions), the degrees
of supervision and required preprocessing, and eval-
uation method.
2.1 Relationship Types
There is a large body of related work that deals with
discovery of basic relationship types represented in
useful resources such as WordNet, including hyper-
nymy (Hearst, 1992; Pantel et al, 2004; Snow
et al, 2006), synonymy (Davidov and Rappoport,
2006; Widdows and Dorow, 2002) and meronymy
(Berland and Charniak, 1999; Girju et al, 2006).
Since named entities are very important in NLP,
many studies define and discover relations between
named entities (Hasegawa et al, 2004; Hassan et
al., 2006). Work was also done on relations be-
tween verbs (Chklovski and Pantel, 2004). There
is growing research on relations between nominals
(Moldovan et al, 2004; Girju et al, 2007).
2.2 Degree of Supervision and Preprocessing
While numerous studies attempt to discover one or
more pre-specified relationship types, very little pre-
vious work has directly attempted the discovery of
which main types of generic relationships actually
exist in an unrestricted domain. Turney (2006) pro-
vided a pattern distance measure that allows a fully
unsupervised measurement of relational similarity
between two pairs of words; such a measure could
in principle be used by a clustering algorithm in or-
der to deduce relationship types, but this was not
discussed. Unlike (Turney, 2006), we do not per-
form any pattern ranking. Instead we produce (pos-
sibly overlapping) hard clusters, where each pattern
cluster represents a relationship discovered in the
domain. Banko et al (2007) and Rosenfeld and
Feldman (2007) find relationship instances where
the relationships are not specified in advance. They
aim to find relationship instances rather than iden-
tify generic semantic relationships. Thus, their rep-
resentation is very different from ours. In addition,
(Banko et al, 2007) utilize supervised tools such
693
as a POS tagger and a shallow parser. Davidov et
al. (2007) proposed a method for unsupervised dis-
covery of concept-specific relations. That work, like
ours, relies on pattern clusters. However, it requires
initial word seeds and targets the discovery of rela-
tionships specific for some given concept, while we
attempt to discover and define generic relationships
that exist in the entire domain.
Studying relationships between tagged named en-
tities, (Hasegawa et al, 2004; Hassan et al, 2006)
proposed unsupervised clustering methods that as-
sign given sets of pairs into several clusters, where
each cluster corresponds to one of a known set of re-
lationship types. Their classification setting is thus
very different from our unsupervised discovery one.
Several recent papers discovered relations on the
web using seed patterns (Pantel et al, 2004), rules
(Etzioni et al, 2004), and word pairs (Pasca et al,
2006; Alfonseca et al, 2006). The latter used the
notion of hook which we also use in this paper.
Several studies utilize some preprocessing, includ-
ing parsing (Hasegawa et al, 2004; Hassan et al,
2006) and usage of syntactic (Suchanek et al, 2006)
and morphological (Pantel et al, 2004) informa-
tion in patterns. Several algorithms use manually-
prepared resources, including WordNet (Moldovan
et al, 2004; Costello et al, 2006) and Wikipedia
(Strube and Ponzetto, 2006). In this paper, we
do not utilize any language-specific preprocessing
or any other resources, which makes our algorithm
relatively easily portable between languages, as we
demonstrate in our bilingual evaluation.
2.3 Evaluation Method
Evaluation for hypernymy and synonymy usually
uses WordNet (Lin and Pantel, 2002; Widdows and
Dorow, 2002; Davidov and Rappoport, 2006). For
more specific lexical relationships like relationships
between verbs (Chklovski and Pantel, 2004), nom-
inals (Girju et al, 2004; Girju et al, 2007) or
meronymy subtypes (Berland and Charniak, 1999)
there is still little agreement which important rela-
tionships should be defined. Thus, there are more
than a dozen different type hierarchies and tasks pro-
posed for noun compounds (and nominals in gen-
eral), including (Nastase and Szpakowicz, 2003;
Girju et al, 2005; Girju et al, 2007).
There are thus two possible ways for a fair eval-
uation. A study can develop its own relationship
definitions and dataset, like (Nastase and Szpakow-
icz, 2003), thus introducing a possible bias; or it
can accept the definition and dataset prepared by
another work, like (Turney, 2006). However, this
makes it impossible to work on new relationship
types. Hence, when exploring very specific relation-
ship types or very generic, but not widely accepted,
types (like verb strength), many researchers resort
to manual human-based evaluation (Chklovski and
Pantel, 2004). In our case, where relationship types
are not specified in advance, creating an unbiased
benchmark is very problematic, so we rely on hu-
man subjects for relationship evaluation.
3 Pattern Clustering Algorithm
Our algorithm first discovers and clusters patterns in
which a single (?hook?) word participates, and then
merges the resulting clusters to form the final struc-
ture. In this section we detail the algorithm. The
algorithm utilizes several parameters, whose selec-
tion is detailed in Section 4. We refer to a pattern
contained in our clusters (a pattern type) as a ?pat-
tern? and to an occurrence of a pattern in the corpus
(a pattern token) as a ?pattern instance?.
3.1 Hook Words and Hook Corpora
As a first step, we randomly select a set of hook
words. Hook words were used in e.g. (Alfonseca
et al, 2006) for extracting general relations starting
from given seed word pairs. Unlike most previous
work, our hook words are not provided in advance
but selected randomly; the goal in those papers is
to discover relationships between given word pairs,
while we use hook words in order to discover rela-
tionships that generally occur in the corpus.
Only patterns in which a hook word actually par-
ticipates will eventually be discovered. Hence, in
principle we should select as many hook words as
possible. However, words whose frequency is very
high are usually ambiguous and are likely to produce
patterns that are too noisy, so we do not select words
with frequency higher than a parameter FC . In ad-
dition, we do not select words whose frequency is
below a threshold FB , to avoid selection of typos
and other noise that frequently appear on the web.
We also limit the total number N of hook words.
694
Our algorithm merges clusters originating from dif-
ferent hook words. Using too many hook words in-
creases the chance that some of them belong to a
noisy part in the corpus and thus lowers the quality
of our resulting clusters.
For each hook word, we now create a hook cor-
pus, the set of the contexts in which the word ap-
pears. Each context is a window containing W
words or punctuation characters before and after the
hook word. We avoid extracting text from clearly
unformatted sentences and our contexts do not cross
paragraph boundaries.
The size of each hook corpus is much smaller than
that of the whole corpus, easily fitting into main
memory; the corpus of a hook word occurring h
times in the corpus contains at most 2hW words.
Since most operations are done on each hook corpus
separately, computation is very efficient.
Note that such context corpora can in principle be
extracted by focused querying on the web, making
the system dynamically scalable. It is also possi-
ble to restrict selection of hook words to a specific
domain or word type, if we want to discover only
a desired subset of existing relationships. Thus we
could sample hook words from nouns, verbs, proper
names, or names of chemical compounds if we are
only interested in discovering relationships between
these. Selecting hook words randomly allows us to
avoid using any language-specific data at this step.
3.2 Pattern Specification
In order to reduce noise and to make the computa-
tion more efficient, we did not consider all contexts
of a hook word as pattern candidates, only contexts
that are instances of a specified meta-pattern type.
Following (Davidov and Rappoport, 2006), we clas-
sified words into high-frequency words (HFWs) and
content words (CWs). A word whose frequency is
more (less) than FH (FC) is considered to be a HFW
(CW). Unlike (Davidov and Rappoport, 2006), we
consider all punctuation characters as HFWs. Our
patterns have the general form
[Prefix] CW1 [Infix] CW2 [Postfix]
where Prefix, Infix and Postfix contain only HFWs.
To reduce the chance of catching CWi?s that are
parts of a multiword expression, we require Prefix
and Postfix to have at least one word (HFW), while
Infix is allowed to contain any number of HFWs (but
recall that the total length of a pattern is limited by
window size). A pattern example is ?such X as Y
and?. During this stage we only allow single words
to be in CW slots2.
3.3 Discovery of Target Words
For each of the hook corpora, we now extract all
pattern instances where one CW slot contains the
hook word and the other CW slot contains some
other (?target?) word. To avoid the selection of com-
mon words as target words, and to avoid targets ap-
pearing in pattern instances that are relatively fixed
multiword expressions, we sort all target words in
a given hook corpus by pointwise mutual informa-
tion between hook and target, and drop patterns ob-
tained from pattern instances containing the lowest
and highest L percent of target words.
3.4 Local Pattern Clustering
We now have for each hook corpus a set of patterns.
All of the corresponding pattern instances share the
hook word, and some of them also share a target
word. We cluster patterns in a two-stage process.
First, we group in clusters all patterns whose in-
stances share the same target word, and ignore the
rest. For each target word we have a single pattern
cluster. Second, we merge clusters that share more
than S percent of their patterns. A pattern can ap-
pear in more than a single cluster. Note that clusters
contain pattern types, obtained through examining
pattern instances.
3.5 Global Cluster Merging
The purpose of this stage is to create clusters of pat-
terns that express generic relationships rather than
ones specific to a single hook word. In addition,
the technique used in this stage reduces noise. For
each created cluster we will define core patterns and
unconfirmed patterns, which are weighed differently
during cluster labeling (see Section 3.6). We merge
clusters from different hook corpora using the fol-
lowing algorithm:
1. Remove all patterns originating from a single hook
corpus.
2While for pattern clusters creation we use only single words
as CWs, later during evaluation we allow multiword expressions
in CW slots of previously acquired patterns.
695
2. Mark all patterns of all present clusters as uncon-
firmed.
3. While there exists some cluster C1 from corpus DX
containing only unconfirmed patterns:
(a) Select a cluster with a minimal number of pat-
terns.
(b) For each corpus D different from DX :
i. Scan D for clusters C2 that share at least
S percent of their patterns, and all of their
core patterns, with C1.
ii. Add all patterns of C2 to C1, setting all
shared patterns as core and all others as
unconfirmed.
iii. Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-
move C1.
4. If several clusters have the same set of core patterns
merge them according to rules (i,ii).
We start from the smallest clusters because we ex-
pect these to be more precise; the best patterns for
semantic acquisition are those that belong to small
clusters, and appear in many different clusters. At
the end of this algorithm, we have a set of pattern
clusters where for each cluster there are two subsets,
core patterns and unconfirmed patterns.
3.6 Labeling of Pattern Clusters
To label pattern clusters we define a HITS measure
that reflects the affinity of a given word pair to a
given cluster. For a given word pair (w1, w2) and
cluster C with n core patterns Pcore and m uncon-
firmed patterns Punconf ,
Hits(C, (w1, w2)) =
|{p; (w1, w2) appears in p ? Pcore}| /n+
?? |{p; (w1, w2) appears in p ? Punconf}| /m.
In this formula, ?appears in? means that the word
pair appears in instances of this pattern extracted
from the original corpus or retrieved from the web
during evaluation (see Section 5.2). Thus if some
pair appears in most of patterns of some cluster it
receives a high HITS value for this cluster. The top
5 pairs for each cluster are selected as its labels.
? ? (0..1) is a parameter that lets us modify the
relative weight of core and unconfirmed patterns.
4 Corpora and Parameters
In this section we describe our experimental setup,
and discuss in detail the effect of each of the algo-
rithms? parameters.
4.1 Languages and Corpora
The evaluation was done using corpora in English
and Russian. The English corpus (Gabrilovich and
Markovitch, 2005) was obtained through crawling
the URLs in the Open Directory Project (dmoz.org).
It contains about 8.2G words and its size is about
68GB of untagged plain text. The Russian corpus
was collected over the web, comprising a variety of
domains, including news, web pages, forums, nov-
els and scientific papers. It contains 7.5G words of
size 55GB untagged plain text. Aside from remov-
ing noise and sentence duplicates, we did not apply
any text preprocessing or tagging.
4.2 Parameters
Our algorithm uses the following parameters: FC ,
FH , FB , W , N , L, S and ?. We used part of the
Russian corpus as a development set for determin-
ing the parameters. On our development set we have
tested various parameter settings. A detailed analy-
sis of the involved parameters is beyond the scope
of this paper; below we briefly discuss the observed
qualitative effects of parameter selection. Naturally,
the parameters are not mutually independent.
FC (upper bound for content word frequency in
patterns) influences which words are considered as
hook and target words. More ambiguous words gen-
erally have higher frequency. Since content words
determine the joining of patterns into clusters, the
more ambiguous a word is, the noisier the result-
ing clusters. Thus, higher values of FC allow more
ambiguous words, increasing cluster recall but also
increasing cluster noise, while lower ones increase
cluster precision at the expense of recall.
FH (lower bound for HFW frequency in patterns)
influences the specificity of patterns. Higher val-
ues restrict our patterns to be based upon the few
most common HFWs (like ?the?, ?of?, ?and?) and
thus yield patterns that are very generic. Lowering
the values, we obtain increasing amounts of pattern
clusters for more specific relationships. The value
we use for FH is lower than that used for FC , in or-
der to allow as HFWs function words of relatively
low frequency (e.g., ?through?), while allowing as
content words some frequent words that participate
in meaningful relationships (e.g., ?game?). However,
this way we may also introduce more noise.
696
FB (lower bound for hook words) filters hook
words that do not appear enough times in the cor-
pus. We have found that this parameter is essential
for removing typos and other words that do not qual-
ify as hook words.
N (number of hook words) influences relation-
ship coverage. With higher N values we discover
more relationships roughly of the same specificity
level, but computation becomes less efficient and
more noise is introduced.
W (window size) determines the length of the dis-
covered patterns. Lower values are more efficient
computationally, but values that are too low result in
drastic decrease in coverage. Higher values would
be more useful when we allow our algorithm to sup-
port multiword expressions as hooks and targets.
L (target word mutual information filter) helps in
avoiding using as targets common words that are
unrelated to hooks, while still catching as targets
frequent words that are related. Low L values de-
crease pattern precision, allowing patterns like ?give
X please Y more?, where X is the hook (e.g., ?Alex?)
and Y the target (e.g., ?some?). High values increase
pattern precision at the expense of recall.
S (minimal overlap for cluster merging) is a clus-
ters merge filter. Higher values cause more strict
merging, producing smaller but more precise clus-
ters, while lower values start introducing noise. In
extreme cases, low values can start a chain reaction
of total merging.
? (core vs. unconfirmed weight for HITS labeling)
allows lower quality patterns to complement higher
quality ones during labeling. Higher values increase
label noise, while lower ones effectively ignore un-
confirmed patterns during labeling.
In our experiments we have used the following
values (again, determined using a development set)
for these parameters: FC : 1, 000 words per mil-
lion (wpm); FH : 100 wpm; FB: 1.2 wpm; N : 500
words; W : 5 words; L: 30%; S: 2/3; ?: 0.1.
5 SAT-based Evaluation
As discussed in Section 2, the evaluation of semantic
relationship structures is non-trivial. The goal of our
evaluation was to assess whether pattern clusters in-
deed represent meaningful, precise and different re-
lationships. There are two complementary perspec-
tives that a pattern clusters quality assessment needs
to address. The first is the quality (precision/recall)
of individual pattern clusters: does each pattern clus-
ter capture lexical item pairs of the same semantic
relationship? does it recognize many pairs of the
same semantic relationship? The second is the qual-
ity of the cluster set as whole: does the pattern clus-
ters set alow identification of important known se-
mantic relationships? do several pattern clusters de-
scribe the same relationship?
Manually examining the resulting pattern clus-
ters, we saw that the majority of sampled clusters in-
deed clearly express an interesting specific relation-
ship. Examples include familiar hypernymy clusters
such as3 {?such X as Y?, ?X such as Y?, ?Y and other
X?,} with label (pets, dogs), and much more specific
clusters like { ?buy Y accessory for X!?, ?shipping Y
for X?, ?Y is available for X?, ?Y are available for X?,
?Y are available for X systems?, ?Y for X? }, labeled
by (phone, charger). Some clusters contain overlap-
ping patterns, like ?Y for X?, but represent different
relationships when examined as a whole.
We addressed the evaluation questions above us-
ing a SAT-like analogy test automatically generated
from word pairs captured by our clusters (see below
in this section). In addition, we tested coverage and
overlap of pattern clusters with a set of 35 known re-
lationships, and we compared our patterns to those
found useful by other algorithms (the next section).
Quantitatively, the final number of clusters is 508
(470) for English (Russian), and the average cluster
size is 5.5 (6.1) pattern types. 55% of the clusters
had no overlap with other clusters.
5.1 SAT Analogy Choice Test
Our main evaluation method, which is also a use-
ful application by itself, uses our pattern clusters to
automatically generate SAT analogy questions. The
questions were answered by human subjects.
We randomly selected 15 clusters. This allowed
us to assess the precision of the whole cluster set as
well as of the internal coherence of separate clus-
ters (see below). For each cluster, we constructed
a SAT analogy question in the following manner.
The header of the question is a word pair that is one
of the label pairs of the cluster. The five multiple
3For readability, we omit punctuations in Prefix and Postfix.
697
choice items include: (1) another label of the clus-
ter (the ?correct? answer); (2) three labels of other
clusters among the 15; and (3) a pair constructed by
randomly selecting words from those making up the
various cluster labels.
In our sample there were no word pairs assigned
as labels to more than one cluster4. As a baseline for
comparison, we have mixed these questions with 15
real SAT questions taken from English and Russian
SAT analogy tests. In addition, we have also asked
our subjects to write down one example pair of the
same relationship for each question in the test.
As an example, from one of the 15 clusters we
have randomly selected the label (glass, water). The
correct answer selected from the same cluster was
(schoolbag, book). The three pairs randomly se-
lected from the other 14 clusters were (war, death),
(request, license) and (mouse, cat). The pair ran-
domly selected from a cluster not among the 15 clus-
ters was (milk, drink). Among the subjects? propos-
als for this question were (closet, clothes) and (wal-
let, money).
We computed accuracy of SAT answers, and the
correlation between answers for our questions and
the real ones (Table 1). Three things are demon-
strated about our system when humans are capable
of selecting the correct answer. First, our clusters
are internally coherent in the sense of expressing a
certain relationship, because people identified that
the pairs in the question header and in the correct
answer exhibit the same relationship. Second, our
clusters distinguish between different relationships,
because the three pairs not expressing the same rela-
tionship as the header were not selected by the evalu-
ators. Third, our cluster labeling algorithm produces
results that are usable by people.
The test was performed in both English and Rus-
sian, with 10 (6) subjects for English (Russian).
The subjects (biology and CS students) were not in-
volved with the research, did not see the clusters,
and did not receive any special training as prepara-
tion. Inter-subject agreement and Kappa were 0.82,
0.72 (0.9, 0.78) for English (Russian). As reported
in (Turney, 2005), an average high-school SAT
grade is 57. Table 1 shows the final English and Rus-
4But note that a pair can certainly obtain a positive HITS
value for several clusters.
Our method Real SAT Correlation
English 80% 71% 0.85
Russian 83% 79% 0.88
Table 1: Pattern cluster evaluation using automatically
generated SAT analogy choice questions.
sian grade average for ours and real SAT questions.
We can see that for both languages, around 80%
of the choices were correct (the random choice base-
line is 20%). Our subjects are university students,
so results higher than 57 are expected, as we can
see from real SAT performance. The difference
in grades between the two languages might be at-
tributed to the presence of relatively hard and un-
common words. It also may result from the Russian
test being easier because there is less verb-noun am-
biguity in Russian.
We have observed a high correlation between true
grades and ours, suggesting that our automatically
generated test reflects the ability to recognize analo-
gies and can be potentially used for automated gen-
eration of SAT-like tests.
The results show that our pattern clusters indeed
mirror a human notion of relationship similarity and
represent meaningful relationships. They also show
that as intended, different clusters describe different
relationships.
5.2 Analogy Invention Test
To assess recall of separate pattern clusters, we have
asked subjects to provide (if possible) an additional
pair for each SAT question. On each such pair
we have automatically extracted a set of pattern in-
stances that capture this pair by using automated
web queries. Then we calculated the HITS value for
each of the selected pairs and assigned them to clus-
ters with highest HITS value. The numbers of pairs
provided were 81 for English and 43 for Russian.
We have estimated precision for this task as
macro-average of percentage of correctly assigned
pairs, obtaining 87% for English and 82% for Rus-
sian (the random baseline of this 15-class classifi-
cation task is 6.7%). It should be noted however
that the human-provided additional relationship ex-
amples in this test are not random so it may intro-
duce bias. Nevertheless, these results confirm that
our pattern clusters are able to recognize new in-
698
30 Noun Compound Relationships
Avg. num Overlap
of clusters
Russian 1.8 0.046
English 1.7 0.059
5 Verb Verb Relationships
Russian 1.4 0.01
English 1.2 0
Table 2: Patterns clusters discovery of known relation-
ships.
stances of relationships of the same type.
6 Evaluation Using Known Information
We also evaluated our pattern clusters using relevant
information reported in related work.
6.1 Discovery of Known Relationships
To estimate recall of our pattern cluster set, we
attempted to estimate whether (at least) a subset
of known relationships have corresponding pattern
clusters. As a testing subset, we have used 35 re-
lationships for both English and Russian. 30 rela-
tions are noun compound relationships as proposed
in the (Nastase and Szpakowicz, 2003) classifica-
tion scheme, and 5 relations are verb-verb relations
proposed by (Chklovski and Pantel, 2004). We
have manually created sets of 5 unambiguous sam-
ple pairs for each of these 35 relationships. For each
such pair we have assigned the pattern cluster with
best HITS value.
The middle column of Table 2 shows the average
number of clusters per relationship. Ideally, if for
each relationship all 5 pairs are assigned to the same
cluster, the average would be 1. In the worst case,
when each pair is assigned to a different cluster, the
average would be 5. We can see that most of the
pairs indeed fall into one or two clusters, success-
fully recognizing that similarly related pairs belong
to the same cluster. The column on the right shows
the overlap between different clusters, measured as
the average number of shared pairs in two randomly
selected clusters. The baseline in this case is essen-
tially 5, since there are more than 400 clusters for 5
word pairs. We see a very low overlap between as-
signed clusters, which shows that these clusters in-
deed separate well between defined relations.
6.2 Discovery of Known Pattern Sets
We compared our clusters to lists of patterns re-
ported as useful by previous papers. These lists
included patterns expressing hypernymy (Hearst,
1992; Pantel et al, 2004), meronymy (Berland and
Charniak, 1999; Girju et al, 2006), synonymy
(Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006), and verb strength + verb happens-
before (Chklovski and Pantel, 2004). In all cases,
we discovered clusters containing all of the reported
patterns (including their refinements with domain-
specific prefix or postfix) and not containing patterns
of competing relationships.
7 Conclusion
We have proposed a novel way to define and identify
generic lexical relationships as clusters of patterns.
Each such cluster is set of patterns that can be used
to identify, classify or capture new instances of some
unspecified semantic relationship. We showed how
such pattern clusters can be obtained automatically
from text corpora without any seeds and without re-
lying on manually created databases or language-
specific text preprocessing. In an evaluation based
on an automatically created analogy SAT test we
showed on two languages that pairs produced by our
clusters indeed strongly reflect human notions of re-
lation similarity. We also showed that the obtained
pattern clusters can be used to recognize new ex-
amples of the same relationships. In an additional
test where we assign labeled pairs to pattern clus-
ters, we showed that they provide good coverage for
known noun-noun and verb-verb relationships for
both tested languages.
While our algorithm shows good performance,
there is still room for improvement. It utilizes a set
of constants that affect precision, recall and the gran-
ularity of the extracted cluster set. It would be ben-
eficial to obtain such parameters automatically and
to create a multilevel relationship hierarchy instead
of a flat one, thus combining different granularity
levels. In this study we applied our algorithm to a
generic domain, while the same method can be used
for more restricted domains, potentially discovering
useful domain-specific relationships.
699
References
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
COLING-ACL ?06 Ontology Learning & Population
Workshop.
Banko, M., Cafarella, M. J. , Soderland, S., Broadhead,
M., and Etzioni, O., 2007. Open information extrac-
tion from the Web. IJCAI ?07.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ?99.
Chklovski, T., Pantel, P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
?04.
Costello, F., Veale, T. Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds. COLING-ACL ?06.
Davidov, D., Rappoport, A., 2006. Efficient unsuper-
vised discovery of word categories using symmetric
patterns and high frequency words. COLING-ACL
?06.
Davidov, D., Rappoport, A. and Koppel, M., 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. ACL ?07.
Davidov, D., Rappoport, A., 2008. Classification of re-
lationships between nominals using pattern clusters.
ACL ?08.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., and Yates, A.,
2004. Methods for domain-independent information
extraction from the web: An experimental compari-
son. AAAI 04
Gabrilovich, E., Markovitch, S., 2005. Feature gener-
ation for text categorization using world knowledge.
IJCAI 2005.
Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,
O., and Moldovan, D., 2004. Support vector machines
applied to the classification of semantic relations in
nominalized noun phrases. HLT/NAACL Workshop on
Computational Lexical Semantics.
Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19(4):479-496.
Girju, R., Badulescu, A., and Moldovan, D., 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakow-
icz, S., Turney, P., and Yuret, D., 2007. Task 04:
Classification of semantic relations between nominal
at SemEval 2007. ACL ?07 SemEval Workshop.
Hasegawa, T., Sekine, S., and Grishman, R., 2004. Dis-
covering relations among named entities from large
corpora. ACL ?04.
Hassan, H., Hassan, A. and Emam, O., 2006. Unsu-
pervised information extraction approach using graph
mutual reinforcement. EMNLP ?06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92
Lin, D., Pantel, P., 2002. Concept discovery from text.
COLING 02.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,Girju,
R., 2004. Models for the semantic classification of
noun phrases. HLT-NAACL ?04 Workshop on Compu-
tational Lexical Semantics.
Nastase, V., Szpakowicz, S., 2003. Exploring noun mod-
ifier semantic relations. IWCS-5.
Pantel, P., Pennacchiotti, M., 2006. Espresso: leveraging
generic patterns for automatically harvesting semantic
relations. COLING-ACL 2006.
Pantel, P., Ravichandran, D. and Hovy, E.H., 2004. To-
wards terascale knowledge acquisition. COLING ?04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A.,
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. COLING-ACL ?06.
Rosenfeld, B., Feldman, R., 2007. Clustering for unsu-
pervised relation identification. CIKM ?07.
Snow, R., Jurafsky, D., Ng, A.Y., 2006. Seman-
tic taxonomy induction from heterogeneous evidence.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Suchanek, F., Ifrim, G., and Weikum, G., 2006. LEILA:
learning to extract information by linguistic analysis.
COLING-ACL ?06 Ontology Learning & Population
Workshop.
Tatu, M., Moldovan, D., 2005. A semantic approach to
recognizing textual entailment. HLT/EMNLP 2005.
Turney, P., 2005. Measuring semantic similarity by la-
tent relational analysis. IJCAI ?05.
Turney, P., Littman, M., 2005. Corpus-based learn-
ing of analogies and semantic selations. Machine
Learning(60):1?3:251?278.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Widdows, D., Dorow, B., 2002. A graph model for un-
supervised lexical acquisition. COLING ?02.
700
Proceedings of ACL-08: HLT, pages 861?869,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multi-Task Active Learning for Linguistic Annotations
Roi Reichart1? Katrin Tomanek2? Udo Hahn2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem, Israel
{roiri|arir}@cs.huji.ac.il
2Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We extend the classical single-task active
learning (AL) approach. In the multi-task ac-
tive learning (MTAL) paradigm, we select ex-
amples for several annotation tasks rather than
for a single one as usually done in the con-
text of AL. We introduce two MTAL meta-
protocols, alternating selection and rank com-
bination, and propose a method to implement
them in practice. We experiment with a two-
task annotation scenario that includes named
entity and syntactic parse tree annotations on
three different corpora. MTAL outperforms
random selection and a stronger baseline, one-
sided example selection, in which one task is
pursued using AL and the selected examples
are provided also to the other task.
1 Introduction
Supervised machine learning methods have success-
fully been applied to many NLP tasks in the last few
decades. These techniques have demonstrated their
superiority over both hand-crafted rules and unsu-
pervised learning approaches. However, they re-
quire large amounts of labeled training data for every
level of linguistic processing (e.g., POS tags, parse
trees, or named entities). When, when domains
and text genres change (e.g., moving from common-
sense newspapers to scientific biology journal arti-
cles), extensive retraining on newly supplied train-
ing material is often required, since different do-
mains may use different syntactic structures as well
as different semantic classes (entities and relations).
? Both authors contributed equally to this work.
Consequently, with an increasing coverage of a
wide variety of domains in human language tech-
nology (HLT) systems, we can expect a growing
need for manual annotations to support many kinds
of application-specific training data.
Creating annotated data is extremely labor-
intensive. The Active Learning (AL) paradigm
(Cohn et al, 1996) offers a promising solution to
deal with this bottleneck, by allowing the learning
algorithm to control the selection of examples to
be manually annotated such that the human label-
ing effort be minimized. AL has been successfully
applied already for a wide range of NLP tasks, in-
cluding POS tagging (Engelson and Dagan, 1996),
chunking (Ngai and Yarowsky, 2000), statistical
parsing (Hwa, 2004), and named entity recognition
(Tomanek et al, 2007).
However, AL is designed in such a way that it se-
lects examples for manual annotation with respect to
a single learning algorithm or classifier. Under this
AL annotation policy, one has to perform a separate
annotation cycle for each classifier to be trained. In
the following, we will refer to the annotations sup-
plied for a classifier as the annotations for a single
annotation task.
Modern HLT systems often utilize annotations re-
sulting from different tasks. For example, a machine
translation system might use features extracted from
parse trees and named entity annotations. For such
an application, we obviously need the different an-
notations to reside in the same text corpus. It is not
clear how to apply the single-task AL approach here,
since a training example that is beneficial for one
task might not be so for others. We could annotate
861
the same corpus independently by the two tasks and
merge the resulting annotations, but that (as we show
in this paper) would possibly yield sub-optimal us-
age of human annotation efforts.
There are two reasons why multi-task AL, and
by this, a combined corpus annotated for various
tasks, could be of immediate benefit. First, annota-
tors working on similar annotation tasks (e.g., con-
sidering named entities and relations between them),
might exploit annotation data from one subtask for
the benefit of the other. If for each subtask a sepa-
rate corpus is sampled by means of AL, annotators
will definitely lack synergy effects and, therefore,
annotation will be more laborious and is likely to
suffer in terms of quality and accuracy. Second, for
dissimilar annotation tasks ? take, e.g., a compre-
hensive HLT pipeline incorporating morphological,
syntactic and semantic data ? a classifier might re-
quire features as input which constitute the output
of another preceding classifier. As a consequence,
training such a classifier which takes into account
several annotation tasks will best be performed on
a rich corpus annotated with respect to all input-
relevant tasks. Both kinds of annotation tasks, simi-
lar and dissimilar ones, constitute examples of what
we refer to as multi-task annotation problems.
Indeed, there have been efforts in creating re-
sources annotated with respect to various annotation
tasks though each of them was carried out indepen-
dently of the other. In the general language UPenn
annotation efforts for the WSJ sections of the Penn
Treebank (Marcus et al, 1993), sentences are anno-
tated with POS tags, parse trees, as well as discourse
annotation from the Penn Discourse Treebank (Milt-
sakaki et al, 2008), while verbs and verb arguments
are annotated with Propbank rolesets (Palmer et al,
2005). In the biomedical GENIA corpus (Ohta et
al., 2002), scientific text is annotated with POS tags,
parse trees, and named entities.
In this paper, we introduce multi-task active
learning (MTAL), an active learning paradigm for
multiple annotation tasks. We propose a new AL
framework where the examples to be annotated are
selected so that they are as informative as possible
for a set of classifiers instead of a single classifier
only. This enables the creation of a single combined
corpus annotated with respect to various annotation
tasks, while preserving the advantages of AL with
respect to the minimization of annotation efforts.
In a proof-of-concept scenario, we focus on two
highly dissimilar tasks, syntactic parsing and named
entity recognition, study the effects of multi-task AL
under rather extreme conditions. We propose two
MTAL meta-protocols and a method to implement
them for these tasks. We run experiments on three
corpora for domains and genres that are very differ-
ent (WSJ: newspapers, Brown: mixed genres, and
GENIA: biomedical abstracts). Our protocols out-
perform two baselines (random and a stronger one-
sided selection baseline).
In Section 2 we introduce our MTAL framework
and present two MTAL protocols. In Section 3 we
discuss the evaluation of these protocols. Section
4 describes the experimental setup, and results are
presented in Section 5. We discuss related work in
Section 6. Finally, we point to open research issues
for this new approach in Section 7.
2 A Framework for Multi-Task AL
In this section we introduce a sample selection
framework that aims at reducing the human anno-
tation effort in a multiple annotation scenario.
2.1 Task Definition
To measure the efficiency of selection methods, we
define the training quality TQ of annotated mate-
rial S as the performance p yielded with a reference
learner X trained on that material: TQ(X, S) = p.
A selection method can be considered better than an-
other one if a higher TQ is yielded with the same
amount of examples being annotated.
Our framework is an extension of the Active
Learning (AL) framework (Cohn et al, 1996)). The
original AL framework is based on querying in an it-
erative manner those examples to be manually anno-
tated that are most useful for the learner at hand. The
TQ of an annotated corpus selected by means of AL
is much higher than random selection. This AL ap-
proach can be considered as single-task AL because
it focuses on a single learner for which the examples
are to be selected. In a multiple annotation scenario,
however, there are several annotation tasks to be ac-
complished at once and for each task typically a sep-
arate statistical model will then be trained. Thus, the
goal of multi-task AL is to query those examples for
862
human annotation that are most informative for all
learners involved.
2.2 One-Sided Selection vs. Multi-Task AL
The naive approach to select examples in a multiple
annotation scenario would be to perform a single-
task AL selection, i.e., the examples to be annotated
are selected with respect to one of the learners only.1
In a multiple annotation scenario we call such an ap-
proach one-sided selection. It is an intrinsic selec-
tion for the reference learner, and an extrinsic selec-
tion for all the other learners also trained on the an-
notated material. Obviously, a corpus compiled with
the help of one-sided selection will have a good TQ
for that learner for which the intrinsic selection has
taken place. For all the other learners, however, we
have no guarantee that their TQ will not be inferior
than the TQ of a random selection process.
In scenarios where the different annotation tasks
are highly dissimilar we can expect extrinsic selec-
tion to be rather poor. This intuition is demonstrated
by experiments we conducted for named entity (NE)
and parse annotation tasks2 (Figure 1). In this sce-
nario, extrinsic selection for the NE annotation task
means that examples where selected with respect
to the parsing task. Extrinsic selection performed
about the same as random selection for the NE task,
while for the parsing task extrinsic selection per-
formed markedly worse. This shows that examples
that were very informative for the NE learner were
not that informative for the parse learner.
2.3 Protocols for Multi-Task AL
Obviously, we can expect one-sided selection to per-
form better for the reference learner (the one for
which an intrinsic selection took place) than multi-
task AL selection, because the latter would be a
compromise for all learners involved in the multi-
ple annotation scenario. However, the goal of multi-
task AL is to minimize the annotation effort over all
annotation tasks and not just the effort for a single
annotation task.
For a multi-task AL protocol to be valuable in a
specific multiple annotation scenario, the TQ for all
considered learners should be
1Of course, all selected examples would be annotated w.r.t.
all annotation tasks.
2See Section 4 for our experimental setup.
1. better than the TQ of random selection,
2. and better than the TQ of any extrinsic selec-
tion.
In the following, we introduce two protocols for
multi-task AL. Multi-task AL protocols can be con-
sidered meta-protocols because they basically spec-
ify how task-specific, single-task AL approaches can
be combined into one selection decision. By this,
the protocols are independent of the underlying task-
specific AL approaches.
2.3.1 Alternating Selection
The alternating selection protocol alternates one-
sided AL selection. In sj consecutive AL iterations,
the selection is performed as one-sided selection
with respect to learning algorithm Xj . After that,
another learning algorithm is considered for selec-
tion for sk consecutive iterations and so on. Depend-
ing on the specific scenario, this enables to weight
the different annotation tasks by allowing them to
guide the selection in more or less AL iterations.
This protocol is a straight-forward compromise be-
tween the different single-task selection approaches.
In this paper we experiment with the special case
of si = 1, where in every AL iteration the selection
leadership is changed. More sophisticated calibra-
tion of the parameters si is beyond the scope of this
paper and will be dealt with in future work.
2.3.2 Rank Combination
The rank combination protocol is more directly
based on the idea to combine single-task AL selec-
tion decisions. In each AL iteration, the usefulness
score sXj (e) of each unlabeled example e from the
pool of examples is calculated with respect to each
learner Xj and then translated into a rank rXj (e)
where higher usefulness means lower rank number
(examples with identical scores get the same rank
number). Then, for each example, we sum the rank
numbers of each annotation task to get the overall
rank r(e) = ?nj=1 rXj (e). All examples are sorted
by this combined rank and b examples with lowest
rank numbers are selected for manual annotation.3
3As the number of ranks might differ between the single an-
notation tasks, we normalize them to the coarsest scale. Then
we can sum up the ranks as explained above.
863
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
random selection
extrinsic selection (PARSE?AL)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
random selection
extrinsic selection (NE?AL)
Figure 1: Learning curves for random and extrinsic selection on both tasks: named entity annotation (left) and syntactic
parse annotation (right), using the WSJ corpus scenario
This protocol favors examples which are good for
all learning algorithms. Examples that are highly in-
formative for one task but rather uninformative for
another task will not be selected.
3 Evaluation of Multi-Task AL
The notion of training quality (TQ) can be used to
quantify the effectiveness of a protocol, and by this,
annotation costs in a single-task AL scenario. To ac-
tually quantify the overall training quality in a multi-
ple annotation scenario one would have to sum over
all the single task?s TQs. Of course, depending on
the specific annotation task, one would not want to
quantify the number of examples being annotated
but different task-specific units of annotation. While
for entity annotations one does typically count the
number of tokens being annotated, in the parsing
scenario the number of constituents being annotated
is a generally accepted measure. As, however, the
actual time needed for the annotation of one exam-
ple usually differs for different annotation tasks, nor-
malizing exchange rates have to be specified which
can then be used as weighting factors. In this paper,
we do not define such weighting factors4, and leave
this challenging question to be discussed in the con-
text of psycholinguistic research.
We could quantify the overall efficiency score E
of a MTAL protocol P by
E(P ) =
n
?
j=1
?j ? TQ(Xj , uj)
where uj denotes the individual annotation task?s
4Such weighting factors not only depend on the annotation
level or task but also on the domain, and especially on the cog-
nitive load of the annotation task.
number of units being annotated (e.g., constituents
for parsing) and the task-specific weights are defined
by ?j . Given weights are properly defined, such a
score can be applied to directly compare different
protocols and quantify their differences.
In practice, such task-specific weights might also
be considered in the MTAL protocols. In the alter-
nating selection protocol, the numbers of consecu-
tive iterations si each single task protocol can be
tuned according to the ? parameters. As for the
rank combination protocol, the weights can be con-
sidered when calculating the overall rank: r(e) =
?n
j=1 ?j ? rXj (e) where the parameters ?1 . . . ?n re-
flect the values of ?1 . . . ?n (though they need not
necessarily be the same).
In our experiments, we assumed the same weight
for all annotation schemata, thus simply setting si =
1, ?i = 1. This was done for the sake of a clear
framework presentation. Finding proper weights for
the single tasks and tuning the protocols accordingly
is a subject for further research.
4 Experiments
4.1 Scenario and Task-Specific Selection
Protocols
The tasks in our scenario comprise one semantic
task (annotation with named entities (NE)) and one
syntactic task (annotation with PCFG parse trees).
The tasks are highly dissimilar, thus increasing the
potential value of MTAL. Both tasks are subject to
intensive research by the NLP community.
The MTAL protocols proposed are meta-
protocols that combine the selection decisions of
the underlying, task-specific AL protocols. In
our scenario, the task-specific AL protocols are
864
committee-based (Freund et al, 1997) selection
protocols. In committee-based AL, a committee
consists of k classifiers of the same type trained
on different subsets of the training data.5 Each
committee member then makes its predictions on
the unlabeled examples, and those examples on
which the committee members disagree most are
considered most informative for learning and are
thus selected for manual annotation. In our scenario
the example grain-size is the sentence level.
For the NE task, we apply the AL approach of
Tomanek et al (2007). The committee consists of
k1 = 3 classifiers and the vote entropy (VE) (Engel-
son and Dagan, 1996) is employed as disagreement
metric. It is calculated on the token-level as
V Etok(t) = ?
1
log k
c
?
i=0
V (li, t)
k log
V (li, t)
k (1)
where V (li,t)k is the ratio of k classifiers where the
label li is assigned to a token t. The sentence level
vote entropy V Esent is then the average over all to-
kens tj of sentence s.
For the parsing task, the disagreement score is
based on a committee of k2 = 10 instances of Dan
Bikel?s reimplementation of Collins? parser (Bickel,
2005; Collins, 1999). For each sentence in the un-
labeled pool, the agreement between the committee
members was calculated using the function reported
by Reichart and Rappoport (2007):
AF (s) = 1N
?
i,l?[1...N ],i6=l
fscore(mi, ml) (2)
Where mi and ml are the committee members and
N = k2?(k2?1)2 is the number of pairs of different
committee members. This function calculates the
agreement between the members of each pair by cal-
culating their relative f-score and then averages the
pairs? scores. The disagreement of the committee on
a sentence is simply 1 ? AF (s).
4.2 Experimental settings
For the NE task we employed the classifier described
by Tomanek et al (2007): The NE tagger is based on
Conditional Random Fields (Lafferty et al, 2001)
5We randomly sampled L = 34 of the training data to create
each committee member.
and has a rich feature set including orthographical,
lexical, morphological, POS, and contextual fea-
tures. For parsing, Dan Bikel?s reimplementation of
Collins? parser is employed, using gold POS tags.
In each AL iteration we select 100 sentences for
manual annotation.6 We start with a randomly cho-
sen seed set of 200 sentences. Within a corpus we
used the same seed set in all selection scenarios. We
compare the following five selection scenarios: Ran-
dom selection (RS), which serves as our baseline;
one-sided AL selection for both tasks (called NE-AL
and PARSE-AL); and multi-task AL selection with
the alternating selection protocol (alter-MTAL) and
the rank combination protocol (ranks-MTAL).
We performed our experiments on three dif-
ferent corpora, namely one from the newspaper
genre (WSJ), a mixed-genre corpus (Brown), and a
biomedical corpus (Bio). Our simulation corpora
contain both entity annotations and (constituent)
parse annotations. For each corpus we have a pool
set (from which we select the examples for annota-
tion) and an evaluation set (used for generating the
learning curves). The WSJ corpus is based on the
WSJ part of the PENN TREEBANK (Marcus et al,
1993); we used the first 10,000 sentences of section
2-21 as the pool set, and section 00 as evaluation set
(1,921 sentences). The Brown corpus is also based
on the respective part of the PENN TREEBANK. We
created a sample consisting of 8 of any 10 consec-
utive sentences in the corpus. This was done as
Brown contains text from various English text gen-
res, and we did that to create a representative sample
of the corpus domains. We finally selected the first
10,000 sentences from this sample as pool set. Every
9th from every 10 consecutive sentences package
went into the evaluation set which consists of 2,424
sentences. For both WSJ and Brown only parse an-
notations though no entity annotations were avail-
able. Thus, we enriched both corpora with entity
annotations (three entities: person, location, and or-
ganization) by means of a tagger trained on the En-
glish data set of the CoNLL-2003 shared task (Tjong
Kim Sang and De Meulder, 2003).7 The Bio corpus
6Manual annotation is simulated by just unveiling the anno-
tations already contained in our corpora.
7We employed a tagger similar to the one presented by Set-
tles (2004). Our tagger has a performance of ? 84% f-score on
the CoNLL-2003 data; inspection of the predicted entities on
865
is based on the parsed section of the GENIA corpus
(Ohta et al, 2002). We performed the same divi-
sions as for Brown, resulting in 2,213 sentences in
our pool set and 276 sentences for the evaluation set.
This part of the GENIA corpus comes with entity an-
notations. We have collapsed the entity classes an-
notated in GENIA (cell line, cell type, DNA, RNA,
protein) into a single, biological entity class.
5 Results
In this section we present and discuss our results
when applying the five selection strategies (RS, NE-
AL, PARSE-AL, alter-MTAL, and ranks-MTAL) to
our scenario on the three corpora. We refrain from
calculating the overall efficiency score (Section 3)
here due to the lack of generally accepted weights
for the considered annotation tasks. However, we
require from a good selection protocol to exceed the
performance of random selection and extrinsic se-
lection. In addition, recall from Section 3 that we
set the alternate selection and rank combination pa-
rameters to si = 1, ?i = 1, respectively to reflect a
tradeoff between the annotation efforts of both tasks.
Figures 2 and 3 depict the learning curves for
the NE tagger and the parser on WSJ and Brown,
respectively. Each figure shows the five selection
strategies. As expected, on both corpora and both
tasks intrinsic selection performs best, i.e., for the
NE tagger NE-AL and for the parser PARSE-AL.
Further, random selection and extrinsic selection
perform worst. Most importantly, both MTAL pro-
tocols clearly outperform extrinsic and random se-
lection in all our experiments. This is in contrast
to NE-AL which performs worse than random se-
lection for all corpora when used as extrinsic selec-
tion, and for PARSE-AL that outperforms the ran-
dom baseline only for Brown when used as extrin-
sic selection. That is, the MTAL protocols suggest a
tradeoff between the annotation efforts of the differ-
ent tasks, here.
On WSJ, both for the NE and the parse annotation
tasks, the performance of the MTAL protocols is
very similar, though ranks-MTAL performs slightly
better. For the parser task, up to 30,000 constituents
MTAL performs almost as good as does PARSE-
AL. This is different for the NE task where NE-AL
WSJ and Brown revealed a good tagging performance.
clearly outperforms MTAL. On Brown, in general
we see the same results, with some minor differ-
ences. On the NE task, extrinsic selection (PARSE-
AL) performs better than random selection, but it is
still much worse than intrinsic AL or MTAL. Here,
ranks-MTAL significantly outperforms alter-MTAL
and almost performs as good as intrinsic selection.
For the parser task, we see that extrinsic and ran-
dom selection are equally bad. Both MTAL proto-
cols perform equally well, again being quite similar
to the intrinsic selection. On the BIO corpus8 we ob-
served the same tendencies as in the other two cor-
pora, i.e., MTAL clearly outperforms extrinsic and
random selection and supplies a better tradeoff be-
tween annotation efforts of the task at hand than one-
sided selection.
Overall, we can say that in all scenarios MTAL
performs much better than random selection and ex-
trinsic selection, and in most cases the performance
of MTAL (especially but not exclusively, ranks-
MTAL) is even close to intrinsic selection. This is
promising evidence that MTAL selection can be a
better choice than one-sided selection in multiple an-
notation scenarios. Thus, considering all annotation
tasks in the selection process (even if the selection
protocol is as simple as the alternating selection pro-
tocol) is better than selecting only with respect to
one task. Further, it should be noted that overall the
more sophisticated rank combination protocol does
not perform much better than the simpler alternating
selection protocol in all scenarios.
Finally, Figure 4 shows the disagreement curves
for the two tasks on the WSJ corpus. As has already
been discussed by Tomanek and Hahn (2008), dis-
agreement curves can be used as a stopping crite-
rion and to monitor the progress of AL-driven an-
notation. This is especially valuable when no anno-
tated validation set is available (which is needed for
plotting learning curves). We can see that the dis-
agreement curves significantly flatten approximately
at the same time as the learning curves do. In the
context of MTAL, disagreement curves might not
only be interesting as a stopping criterion but rather
as a switching criterion, i.e., to identify when MTAL
could be turned into one-sided selection. This would
be the case if in an MTAL scenario, the disagree-
8The plots for the Bio are omitted due to space restrictions.
866
10000 20000 30000 40000 50000
0.
65
0.
70
0.
75
0.
80
0.
85
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000
0.
55
0.
60
0.
65
0.
70
0.
75
0.
80
tokens
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 2: Learning curves for NE task on WSJ (left) and Brown (right)
10000 20000 30000 40000
0.
76
0.
78
0.
80
0.
82
0.
84
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
5000 10000 15000 20000 25000 30000 35000
0.
65
0.
70
0.
75
0.
80
constituents
f?
sc
or
e
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 3: Learning curves for parse task on WSJ (left) and Brown (right)
ment curve of one task has a slope of (close to) zero.
Future work will focus on issues related to this.
6 Related Work
There is a large body of work on single-task AL ap-
proaches for many NLP tasks where the focus is
mainly on better, task-specific selection protocols
and methods to quantify the usefulness score in dif-
ferent scenarios. As to the tasks involved in our
scenario, several papers address AL for NER (Shen
et al, 2004; Hachey et al, 2005; Tomanek et al,
2007) and syntactic parsing (Tang et al, 2001; Hwa,
2004; Baldridge and Osborne, 2004; Becker and Os-
borne, 2005). Further, there is some work on ques-
tions arising when AL is to be used in real-life anno-
tation scenarios, including impaired inter-annotator
agreement, stopping criteria for AL-driven annota-
tion, and issues of reusability (Baldridge and Os-
borne, 2004; Hachey et al, 2005; Zhu and Hovy,
2007; Tomanek et al, 2007).
Multi-task AL is methodologically related to ap-
proaches of decision combination, especially in the
context of classifier combination (Ho et al, 1994)
and ensemble methods (Breiman, 1996). Those ap-
proaches focus on the combination of classifiers in
order to improve the classification error rate for one
specific classification task. In contrast, the focus of
multi-task AL is on strategies to select training ma-
terial for multi classifier systems where all classifiers
cover different classification tasks.
7 Discussion
Our treatment of MTAL within the context of the
orthogonal two-task scenario leads to further inter-
esting research questions. First, future investiga-
tions will have to focus on the question whether
the positive results observed in our orthogonal (i.e.,
highly dissimilar) two-task scenario will also hold
for a more realistic (and maybe more complex) mul-
tiple annotation scenario where tasks are more sim-
ilar and more than two annotation tasks might be
involved. Furthermore, several forms of interde-
pendencies may arise between the single annotation
tasks. As a first example, consider the (functional)
interdependencies (i.e., task similarity) in higher-
level semantic NLP tasks of relation or event recog-
nition. In such a scenario, several tasks including
entity annotations and relation/event annotations, as
well as syntactic parse data, have to be incorporated
at the same time. Another type of (data flow) inter-
867
10000 20000 30000 40000
0.
01
0
0.
01
4
0.
01
8
tokens
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
10000 20000 30000 40000
5
10
15
20
25
30
35
40
constituents
di
sa
gr
ee
m
en
t
RS
NE?AL
PARSE?AL
alter?MTAL
ranks?MTAL
Figure 4: Disagreement curves for NE task (left) and parse task (right) on WSJ
dependency occurs in a second scenario where ma-
terial for several classifiers that are data-dependent
on each other ? one takes the output of another clas-
sifier as input features ? has to be efficiently anno-
tated. Whether the proposed protocols are beneficial
in the context of such highly interdependent tasks is
an open issue. Even more challenging is the idea
to provide methodologies helping to predict in an
arbitrary application scenario whether the choice of
MTAL is truly advantageous.
Another open question is how to measure and
quantify the overall annotation costs in multiple an-
notation scenarios. Exchange rates are inherently
tied to the specific task and domain. In practice, one
might just want to measure the time needed for the
annotations. However, in a simulation scenario, a
common metric is necessary to compare the perfor-
mance of different selection strategies with respect
to the overall annotation costs. This requires stud-
ies on how to quantify, with a comparable cost func-
tion, the efforts needed for the annotation of a textual
unit of choice (e.g., tokens, sentences) with respect
to different annotation tasks.
Finally, the question of reusability of the anno-
tated material is an important issue. Reusability in
the context of AL means to which degree corpora
assembled with the help of any AL technique can be
(re)used as a general resource, i.e., whether they are
well suited for the training of classifiers other than
the ones used during the selection process.This is
especially interesting as the details of the classifiers
that should be trained in a later stage are typically
not known at the resource building time. Thus, we
want to select samples valuable to a family of clas-
sifiers using the various annotation layers. This, of
course, is only possible if data annotated with the
help of AL is reusable by modified though similar
classifiers (e.g., with respect to the features being
used) ? compared to the classifiers employed for the
selection procedure.
The issue of reusability has already been raised
but not yet conclusively answered in the context of
single-task AL (see Section 6). Evidence was found
that reusability up to a certain, though not well-
specified, level is possible. Of course, reusability
has to be analyzed separately in the context of var-
ious MTAL scenarios. We feel that these scenarios
might both be more challenging and more relevant
to the reusability issue than the single-task AL sce-
nario, since resources annotated with multiple lay-
ers can be used to the design of a larger number of a
(possibly more complex) learning algorithms.
8 Conclusions
We proposed an extension to the single-task AL ap-
proach such that it can be used to select examples for
annotation with respect to several annotation tasks.
To the best of our knowledge this is the first paper on
this issue, with a focus on NLP tasks. We outlined
a problem definition and described a framework for
multi-task AL. We presented and tested two proto-
cols for multi-task AL. Our results are promising as
they give evidence that in a multiple annotation sce-
nario, multi-task AL outperforms naive one-sided
and random selection.
Acknowledgments
The work of the second author was funded by the
German Ministry of Education and Research within
the STEMNET project (01DS001A-C), while the
work of the third author was funded by the EC
within the BOOTSTREP project (FP6-028099).
868
References
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
EMNLP?04, pages 9?16.
Markus Becker and Miles Osborne. 2005. A two-stage
method for active learning of statistical grammars. In
Proceedings of IJCAI?05, pages 991?996.
Daniel M. Bickel. 2005. Code developed at the Univer-
sity of Pennsylvania, http://www.cis.upenn.
edu/
?
dbikel/software.html.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Sean Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In Proceedings of ACL?96, pages 319?326.
Yoav Freund, Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query
by committee algorithm. Machine Learning, 28(2-
3):133?168.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of CoNLL?05, pages
144?151.
Tin Kam Ho, Jonathan J. Hull, and Sargur N. Srihari.
1994. Decision combination in multiple classifier sys-
tems. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 16(1):66?75.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of ICML?01, pages 282?289.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Eleni Miltsakaki, Livio Robaldo, Alan Lee, and Ar-
avind K. Joshi. 2008. Sense annotation in the penn
discourse treebank. In Proceedings of CICLing?08,
pages 275?286.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of ACL?00,
pages 117?125.
Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim. 2002.
The GENIA corpus: An annotated research abstract
corpus in molecular biology domain. In Proceedings
of HLT?02, pages 82?86.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Roi Reichart and Ari Rappoport. 2007. An ensemble
method for selection of high quality parses. In Pro-
ceedings of ACL?07, pages 408?415, June.
Burr Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of JNLPBA?04, pages 107?110.
Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and
Chew Lim Tan. 2004. Multi-criteria-based active
learning for named entity recognition. In Proceedings
of ACL?04, pages 589?596.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2001. Ac-
tive learning for statistical natural language parsing. In
Proceedings of ACL?02, pages 120?127.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CONLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL?03, pages 142?147.
Katrin Tomanek and Udo Hahn. 2008. Approximating
learning curves for active-learning-driven annotation.
In Proceedings of LREC?08.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains corpus reusabil-
ity of annotated data. In Proceedings of EMNLP-
CoNLL?07, pages 486?495.
Jingbo Zhu and Eduard Hovy. 2007. Active learning for
word sense disambiguation with methods for address-
ing the class imbalance problem. In Proceedings of
EMNLP-CoNLL?07, pages 783?790.
869
Proceedings of ACL-08: HLT, pages 1030?1038,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extraction of Entailed Semantic Relations Through
Syntax-based Comma Resolution
Vivek Srikumar 1 Roi Reichart2 Mark Sammons1 Ari Rappoport2 Dan Roth1
1University of Illinois at Urbana-Champaign
{vsrikum2|mssammon|danr}@uiuc.edu
2Institute of Computer Science, Hebrew University of Jerusalem
{roiri|arir}@cs.huji.ac.il
Abstract
This paper studies textual inference by inves-
tigating comma structures, which are highly
frequent elements whose major role in the ex-
traction of semantic relations has not been
hitherto recognized. We introduce the prob-
lem of comma resolution, defined as under-
standing the role of commas and extracting the
relations they imply. We show the importance
of the problem using examples from Tex-
tual Entailment tasks, and present A Sentence
Transformation Rule Learner (ASTRL), a ma-
chine learning algorithm that uses a syntac-
tic analysis of the sentence to learn sentence
transformation rules that can then be used to
extract relations. We have manually annotated
a corpus identifying comma structures and re-
lations they entail and experimented with both
gold standard parses and parses created by a
leading statistical parser, obtaining F-scores of
80.2% and 70.4% respectively.
1 Introduction
Recognizing relations expressed in text sentences is
a major topic in NLP, fundamental in applications
such as Textual Entailment (or Inference), Question
Answering and Text Mining. In this paper we ad-
dress this issue from a novel perspective, that of un-
derstanding the role of the commas in a sentence,
which we argue is a key component in sentence
comprehension. Consider for example the following
three sentences:
1. Authorities have arrested John Smith, a retired
police officer.
2. Authorities have arrested John Smith, his friend
and his brother.
3. Authorities have arrested John Smith, a retired
police officer announced this morning.
Sentence (1) states that John Smith is a retired
police officer. The comma and surrounding sen-
tence structure represent the relation ?IsA?. In (2),
the comma and surrounding structure signifies a list,
so the sentence states that three people were ar-
rested: (i) John Smith, (ii) his friend, and (iii) his
brother. In (3), a retired police officer announced
that John Smith has been arrested. Here, the comma
and surrounding sentence structure indicate clause
boundaries.
In all three sentences, the comma and the sur-
rounding sentence structure signify relations essen-
tial to comprehending the meaning of the sentence,
in a way that is not easily captured using lexical-
or even shallow parse-level information. As a hu-
man reader, we understand them easily, but auto-
mated systems for Information Retrieval, Question
Answering, and Textual Entailment are likely to en-
counter problems when comparing structures like
these, which are lexically similar, but whose mean-
ings are so different.
In this paper we present an algorithm for comma
resolution, a task that we define to consist of (1) dis-
ambiguating comma type and (2) determining the
relations entailed from the sentence given the com-
mas? interpretation. Specifically, in (1) we assign
each comma to one of five possible types, and in
(2) we generate a set of natural language sentences
that express the relations, if any, signified by each
comma structure. The algorithm uses information
extracted from parse trees. This work, in addition to
having immediate significance for natural language
processing systems that use semantic content, has
potential applications in improving a range of auto-
1030
mated analysis by decomposing complex sentences
into a set of simpler sentences that capture the same
meaning. Although there are many other widely-
used structures that express relations in a similar
way, commas are one of the most commonly used
symbols1. By addressing comma resolution, we of-
fer a promising first step toward resolving relations
in sentences.
To evaluate the algorithm, we have developed an-
notation guidelines, and manually annotated sen-
tences from the WSJ PennTreebank corpus. We
present a range of experiments showing the good
performance of the system, using gold-standard and
parser-generated parse trees.
In Section 2 we motivate comma resolution
through Textual Entailment examples. Section 3 de-
scribes related work. Sections 4 and 5 present our
corpus annotation and learning algorithm. Results
are given in Section 6.
2 Motivating Comma Resolution Through
Textual Entailment
Comma resolution involves not only comma dis-
ambiguation but also inference of the arguments
(and argument boundaries) of the relationship repre-
sented by the comma structure, and the relationships
holding between these arguments and the sentence
as a whole. To our knowledge, this is the first pa-
per that deals with this problem, so in this section
we motivate it in depth by showing its importance
to the semantic inference task of Textual Entailment
(TE) (Dagan et al, 2006), which is increasingly rec-
ognized as a crucial direction for improving a range
of NLP tasks such as information extraction, ques-
tion answering and summarization.
TE is the task of deciding whether the meaning
of a text T (usually a short snippet) can be inferred
from the meaning of another text S. If this is the
case, we say that S entails T . For example2, we say
that sentence (1) entails sentence (2):
1. S: Parviz Davudi was representing Iran at a
meeting of the Shanghai Co-operation Orga-
nization (SCO), the fledgling association that
1For example, the WSJ corpus has 49K sentences, among
which 32K with one comma or more, 17K with two or more,
and 7K with three or more.
2The examples of this section are variations of pairs taken
from the Pascal RTE3 (Dagan et al, 2006) dataset.
binds two former Soviet republics of central
Asia, Russia and China to fight terrorism.
2. T: SCO is the fledgling association that binds
several countries.
To see that (1) entails (2), one must understand
that the first comma structure in sentence (1) is an
apposition structure, and does not indicate the begin-
ning of a list. The second comma marks a boundary
between entities in a list. To make the correct infer-
ence one must determine that the second comma is a
list separator, not an apposition marker. Misclassify-
ing the second comma in (1) as an apposition leads
to the conclusion that (1) entails (3):
3. T: Russia and China are two former Soviet re-
publics of central Asia .
Note that even to an educated native speaker of
English, sentence 1 may be initially confusing; dur-
ing the first reading, one might interpret the first
comma as indicating a list, and that ?the Shanghai
Co-operation Organization? and ?the fledgling asso-
ciation that binds...? are two separate entities that are
meeting, rather than two representations of the same
entity.
From these examples we draw the following con-
clusions: 1. Comma resolution is essential in com-
prehending natural language text. 2. Explicitly rep-
resenting relations derived from comma structures
can assist a wide range of NLP tasks; this can be
done by directly augmenting the lexical-level rep-
resentation, e.g., by bringing surface forms of two
text fragments with the same meaning closer to-
gether. 3. Comma structures might be highly am-
biguous, nested and overlapping, and consequently
their interpretation is a difficult task. The argument
boundaries of the corresponding extracted relations
are also not easy to detect.
The output of our system could be used to aug-
ment sentences with an explicit representation of en-
tailed relations that hold in them. In Textual Entail-
ment systems this can increase the likelihood of cor-
rect identification of entailed sentences, and in other
NLP systems it can help understanding the shallow
lexical/syntactic content of a sentence. A similar ap-
proach has been taken in (Bar-Haim et al, 2007; de
Salvo Braz et al, 2005), which augment the source
sentence with entailed relations.
1031
3 Related Work
Since we focus on extracting the relations repre-
sented by commas, there are two main strands of
research with similar goals: 1) systems that directly
analyze commas, whether labeling them with syn-
tactic information or correcting inappropriate use in
text; and 2) systems that extract relations from text,
typically by trying to identify paraphrases.
The significance of interpreting the role of com-
mas in sentences has already been identified by (van
Delden and Gomez, 2002; Bayraktar et al, 1998)
and others. A review of the first line of research is
given in (Say and Akman, 1997).
In (Bayraktar et al, 1998) the WSJ PennTreebank
corpus (Marcus et al, 1993) is analyzed and a very
detailed list of syntactic patterns that correspond to
different roles of commas is created. However, they
do not study the extraction of entailed relations as
a function of the comma?s interpretation. Further-
more, the syntactic patterns they identify are unlexi-
calized and would not support the level of semantic
relations that we show in this paper. Finally, theirs
is a manual process completely dependent on syn-
tactic patterns. While our comma resolution system
uses syntactic parse information as its main source
of features, the approach we have developed focuses
on the entailed relations, and does not limit imple-
mentations to using only syntactic information.
The most directly comparable prior work is that
of (van Delden and Gomez, 2002), who use fi-
nite state automata and a greedy algorithm to learn
comma syntactic roles. However, their approach dif-
fers from ours in a number of critical ways. First,
their comma annotation scheme does not identify
arguments of predicates, and therefore cannot be
used to extract complete relations. Second, for each
comma type they identify, a new Finite State Au-
tomaton must be hand-encoded; the learning com-
ponent of their work simply constrains which FSAs
that accept a given, comma containing, text span
may co-occur. Third, their corpus is preprocessed by
hand to identify specialized phrase types needed by
their FSAs; once our system has been trained, it can
be applied directly to raw text. Fourth, they exclude
from their analysis and evaluation any comma they
deem to have been incorrectly used in the source
text. We include all commas that are present in the
text in our annotation and evaluation.
There is a large body of NLP literature on punctu-
ation. Most of it, however, is concerned with aiding
syntactic analysis of sentences and with developing
comma checkers, much based on (Nunberg, 1990).
Pattern-based relation extraction methods (e.g.,
(Davidov and Rappoport, 2008; Davidov et al,
2007; Banko et al, 2007; Pasca et al, 2006; Sekine,
2006)) could in theory be used to extract relations
represented by commas. However, the types of
patterns used in web-scale lexical approaches cur-
rently constrain discovered patterns to relatively
short spans of text, so will most likely fail on
structures whose arguments cover large spans (for
example, appositional clauses containing relative
clauses). Relation extraction approaches such as
(Roth and Yih, 2004; Roth and Yih, 2007; Hirano
et al, 2007; Culotta and Sorenson, 2004; Zelenko et
al., 2003) focus on relations between Named Enti-
ties; such approaches miss the more general apposi-
tion and list relations we recognize in this work, as
the arguments in these relations are not confined to
Named Entities.
Paraphrase Acquisition work such as that by (Lin
and Pantel, 2001; Pantel and Pennacchiotti, 2006;
Szpektor et al, 2004) is not constrained to named
entities, and by using dependency trees, avoids the
locality problems of lexical methods. However,
these approaches have so far achieved limited accu-
racy, and are therefore hard to use to augment exist-
ing NLP systems.
4 Corpus Annotation
For our corpus, we selected 1,000 sentences con-
taining at least one comma from the Penn Treebank
(Marcus et al, 1993) WSJ section 00, and manu-
ally annotated them with comma information3. This
annotated corpus served as both training and test
datasets (using cross-validation).
By studying a number of sentences from WSJ (not
among the 1,000 selected), we identified four signif-
icant types of relations expressed through commas:
SUBSTITUTE, ATTRIBUTE, LOCATION, and LIST.
Each of these types can in principle be expressed us-
ing more than a single comma. We define the notion
3The guidelines and annotations are available at http://
L2R.cs.uiuc.edu/
?
cogcomp/data.php.
1032
of a comma structure as a set of one or more commas
that all relate to the same relation in the sentence.
SUBSTITUTE indicates an IS-A relation. An ex-
ample is ?John Smith, a Renaissance artist, was fa-
mous?. By removing the relation expressed by the
commas, we can derive three sentences: ?John Smith
is a Renaissance artist?, ?John Smith was famous?,
and ?a Renaissance artist was famous?. Note that in
theory, the third relation will not be valid: one exam-
ple is ?The brothers, all honest men, testified at the
trial?, which does not entail ?all honest men testified
at the trial?. However, we encountered no examples
of this kind in the corpus, and leave this refinement
to future work.
ATTRIBUTE indicates a relation where one argu-
ment describes an attribute of the other. For ex-
ample, from ?John, who loved chocolate, ate with
gusto?, we can derive ?John loved chocolate? and
?John ate with gusto?.
LOCATION indicates a LOCATED-IN relation. For
example, from ?Chicago, Illinois saw some heavy
snow today? we can derive ?Chicago is located in
Illinois? and ?Chicago saw some heavy snow today?.
LIST indicates that some predicate or property
is applied to multiple entities. In our annotation,
the list does not generate explicit relations; instead,
the boundaries of the units comprising the list are
marked so that they can be treated as a single unit,
and are considered to be related by the single rela-
tion ?GROUP?. For example, the derivation of ?John,
James and Kelly all left last week? is written as
?[John, James, and Kelly] [all left last week]?.
Any commas not fitting one of the descriptions
above are designated as OTHER. This does not in-
dicate that the comma signifies no relations, only
that it does not signify a relation of interest in this
work (future work will address relations currently
subsumed by this category). Analysis of 120 OTHER
commas show that approximately half signify clause
boundaries, which may occur when sentence con-
stituents are reordered for emphasis, but may also
encode implicit temporal, conditional, and other re-
lation types (for example, ?Opening the drawer, he
found the gun.?). The remainder comprises mainly
coordination structures (for example, ?Although he
won, he was sad?) and discourse markers indicating
inter-sentence relations (such as ?However, he soon
cheered up.?). While we plan to develop an anno-
Rel. Type Avg. Agreement # of Commas # of Rel.s
SUBSTITUTE 0.808 243 729
ATTRIBUTE 0.687 193 386
LOCATION 0.929 71 140
LIST 0.803 230 230
OTHER 0.949 909 0
Combined 0.869 1646 1485
Table 1: Average inter-annotator agreement for identify-
ing relations.
tation scheme for such relations, this is beyond the
scope of the present work.
Four annotators annotated the same 10% of the
WSJ sentences in order to evaluate inter-annotator
agreement. The remaining sentences were divided
among the four annotators. The resulting corpus was
checked by two judges and the annotation corrected
where appropriate; if the two judges disagreed, a
third judge was consulted and consensus reached.
Our annotators were asked to identify comma struc-
tures, and for each structure to write its relation type,
its arguments, and all possible simplified version(s)
of the original sentence in which the relation implied
by the comma has been removed. Arguments must
be contiguous units of the sentence and will be re-
ferred to as chunks hereafter. Agreement statistics
and the number of commas and relations of each
type are shown in Table 4. The Accuracy closely ap-
proximates Kappa score in this case, since the base-
line probability of chance agreement is close to zero.
5 A Sentence Tranformation Rule Learner
(ASTRL)
In this section, we describe a new machine learning
system that learns Sentence Transformation Rules
(STRs) for comma resolution. We first define the
hypothesis space (i.e., STRs) and two operations ?
substitution and introduction. We then define the
feature space, motivating the use of Syntactic Parse
annotation to learn STRs. Finally, we describe the
ASTRL algorithm.
5.1 Sentence Transformation Rules
A Sentence Transformation Rule (STR) takes a
parse tree as input and generates new sentences. We
formalize an STR as the pair l ? r, where l is a
tree fragment that can consist of non-terminals, POS
tags and lexical items. r is a set {ri}, each ele-
ment of which is a template that consists of the non-
1033
terminals of l and, possibly, some new tokens. This
template is used to generate a new sentence, called a
relation.
The process of applying an STR l ? r to a parse
tree T of a sentence s begins with finding a match for
l in T . A match is said to be found if l is a subtree
of T . If matched, the non-terminals of each ri are
instantiated with the terminals that they cover in T .
Instantiation is followed by generation of the output
relations in one of two ways: introduction or sub-
stitution, which is specified by the corresponding ri.
If an ri is marked as an introductory one, then the
relation is the terminal sequence obtained by replac-
ing the non-terminals in ri with their instantiations.
For substitution, firstly, the non-terminals of the ri
are replaced by their instantiations. The instantiated
ri replaces all the terminals in s that are covered by
the l-match. The notions of introduction and substi-
tution were motivated by ideas introduced in (Bar-
Haim et al, 2007).
Figure 1 shows an example of an STR and Figure
2 shows the application of this STR to a sentence. In
the first relation, NP1 and NP2 are instantiated with
the corresponding terminals in the parse tree. In the
second and third relations, the terminals of NP1 and
NP2 replace the terminals covered by NPp.
LHS: NPp
NP1 , NP2 ,
RHS:
1. NP1 be NP2 (introduction)
2. NP1 (substitution)
3. NP2 (substitution)
Figure 1: Example of a Sentence Transformation Rule. If
the LHS matches a part of a given parse tree, then the
RHS will generate three relations.
5.2 The Feature Space
In Section 2, we discussed the example where there
could be an ambiguity between a list and an apposi-
tion structure in the fragment two former Soviet re-
publics, Russia and China. In addition, simple sur-
face examination of the sentence could also identify
the noun phrases ?Shanghai Co-operation Organi-
zation (SCO)?, ?the fledgling association that binds
S
NPp
NP1
John Smith
, NP2
a renaissance
artist
,
V P
was
famous
RELATIONS:
1 [John Smith]/NP1 be [a renaissance artist]/NP2
2 [John Smith] /NP1 [was famous]
3 [a renaissance artist]/NP2 [was famous]
Figure 2: Example of application of the STR in Figure 1.
In the first relation, an introduction, we use the verb ?be?,
without dealing with its inflections. NP1 and NP2 are
both substitutions, each replacing NPp to generate the
last two relations.
two former Soviet Republics?, ?Russia? and ?China?
as the four members of a list. To resolve such ambi-
guities, we need a nested representation of the sen-
tence. This motivates the use of syntactic parse trees
as a logical choice of feature space. (Note, however,
that semantic and pragmatic ambiguities might still
remain.)
5.3 Algorithm Overview
In our corpus annotation, the relations and their ar-
gument boundaries (chunks) are explicitly marked.
For each training example, our learning algorithm
first finds the smallest valid STR ? the STR with the
smallest LHS in terms of depth. Then it refines the
LHS by specializing it using statistics taken from
the entire data set.
5.4 Generating the Smallest Valid STR
To transform an example into the smallest valid
STR, we utilize the augmented parse tree of the
sentence. For each chunk in the sentence, we find
the lowest node in the parse tree that covers the
chunk and does not cover other chunks (even par-
tially). It may, however, cover words that do not
belong to any chunk. We refer to such a node as
a chunk root. We then find the lowest node that cov-
ers all the chunk roots, referring to it as the pat-
tern root. The initial LHS consists of the sub-
tree of the parse tree rooted at the pattern root and
whose leaf nodes are all either chunk roots or nodes
that do not belong to any chunk. All the nodes are
labeled with the corresponding labels in the aug-
1034
mented parse tree. For example, if we consider the
parse tree and relations shown in Figure 2, then do-
ing the above procedure gives us the initial LHS
as S (NPp(NP1, NP2, ) V P ). The three relations
gives us the RHS with three elements ?NP1 be
NP2?, ?NP1 V P ? and ?NP1 V P ?, all three being
introduction.
This initial LHS need not be the smallest one that
explains the example. So, we proceed by finding the
lowest node in the initial LHS such that the sub-
tree of the LHS at that node can form a new STR
that covers the example using both introduction and
substitution. In our example, the initial LHS has a
subtree, NPp(NP1, NP2, ) that can cover all the re-
lations with the RHS consisting of ?NP1 be NP2?,
NP1 and NP2. The first RHS is an introduction,
while the second and the third are both substitutions.
Since no subtree of this LHS can generate all three
relations even with substitution, this is the required
STR. The final step ensures that we have the small-
est valid STR at this stage.
5.5 Statistical Refinement
The STR generated using the procedure outlined
above explains the relations generated by a single
example. In addition to covering the relations gen-
erated by the example, we wish to ensure that it does
not cover erroneous relations by matching any of the
other comma types in the annotated data.
Algorithm 1 ASTRL: A Sentence Transformation
Rule Learning.
1: for all t: Comma type do
2: Initialize STRList[t] = ?
3: p = Set of annotated examples of type t
4: n = Annotated examples of all other types
5: for all x ? p do
6: r = Smallest Valid STR that covers x
7: Get fringe of r.LHS using the parse tree
8: S = Score(r,p,n)
9: Sprev = ??
10: while S 6= Sprev do
11: if adding some fringe node to r.LHS causes a signifi-
cant change in score then
12: Set r = New rule that includes that fringe node
13: Sprev = S
14: S = Score(r,p,n)
15: Recompute new fringe nodes
16: end if
17: end while
18: Add r to STRList[t]
19: Remove all examples from p that are covered by r
20: end for
21: end for
For this purpose, we specialize the LHS so that it
covers as few examples from the other comma types
as possible, while covering as many examples from
the current comma type as possible. Given the most
general STR, we generate a set of additional, more
detailed, candidate rules. Each of these is obtained
from the original rule by adding a single node to
the tree pattern in the rule?s LHS, and updating the
rule?s RHS accordingly. We then score each of the
candidates (including the original rule). If there is
a clear winner, we continue with it using the same
procedure (i.e., specialize it). If there isn?t a clear
winner, we stop and use the current winner. After
finishing with a rule (line 18), we remove from the
set of positive examples of its comma type all exam-
ples that are covered by it (line 19).
To generate the additional candidate rules that we
add, we define the fringe of a rule as the siblings
and children of the nodes in its LHS in the original
parse tree. Each fringe node defines an additional
candidate rule, whose LHS is obtained by adding
the fringe node to the rule?s LHS tree. We refer to
the set of these candidate rules, plus the original one,
as the rule?s fringe rules. We define the score of an
STR as
Score(Rule,p,n) = Rp|p| ?
Rn
|n|
where p and n are the set of positive and negative
examples for this comma type, and Rp and Rn are
the number of positive and negative examples that
are covered by the STR. For each example, all exam-
ples annotated with the same comma type are pos-
itive while all examples of all other comma types
are negative. The score is used to select the win-
ner among the fringe rules. The complete algorithm
we have used is listed in Algorithm 1. For conve-
nience, the algorithm?s main loop is given in terms
of comma types, although this is not strictly nec-
essary. The stopping criterion in line 11 checks
whether any fringe rule has a significantly better
score than the rule it was derived from, and exits the
specialization loop if there is none.
Since we start with the smallest STR, we only
need to add nodes to it to refine it and never have
to delete any nodes from the tree. Also note that the
algorithm is essentially a greedy algorithm that per-
forms a single pass over the examples; other, more
1035
complex, search strategies could also be used.
6 Evaluation
6.1 Experimental Setup
To evaluate ASTRL, we used the WSJ derived cor-
pus. We experimented with three scenarios; in two
of them we trained using the gold standard trees
and then tested on gold standard parse trees (Gold-
Gold), and text annotated using a state-of-the-art sta-
tistical parser (Charniak and Johnson, 2005) (Gold-
Charniak), respectively. In the third, we trained and
tested on the Charniak Parser (Charniak-Charniak).
In gold standard parse trees the syntactic cate-
gories are annotated with functional tags. Since cur-
rent statistical parsers do not annotate sentences with
such tags, we augment the syntactic trees with the
output of a Named Entity tagger. For the Named
Entity information, we used a publicly available NE
Recognizer capable of recognizing a range of cat-
egories including Person, Location and Organiza-
tion. On the CoNLL-03 shared task, its f-score is
about 90%4. We evaluate our system from different
points of view, as described below. For all the eval-
uation methods, we performed five-fold cross vali-
dation and report the average precision, recall and
f-scores.
6.2 Relation Extraction Performance
Firstly, we present the evaluation of the performance
of ASTRL from the point of view of relation ex-
traction. After learning the STRs for the different
comma types using the gold standard parses, we
generated relations by applying the STRs on the test
set once. Table 2 shows the precision, recall and
f-score of the relations, without accounting for the
comma type of the STR that was used to generate
them. This metric, called the Relation metric in fur-
ther discussion, is the most relevant one from the
point of view of the TE task. Since a list does not
generate any relations in our annotation scheme, we
use the commas to identify the list elements. Treat-
ing each list in a sentence as a single relation, we
score the list with the fraction of its correctly identi-
fied elements.
In addition to the Gold-Gold and Gold-Charniak
4A web demo of the NER is at http://L2R.cs.uiuc.
edu/
?
cogcomp/demos.php.
settings described above, for this metric, we also
present the results of the Charniak-Charniak setting,
where both the train and test sets were annotated
with the output of the Charniak parser. The improve-
ment in recall in this setting over the Gold-Charniak
case indicates that the parser makes systematic er-
rors with respect to the phenomena considered.
Setting P R F
Gold-Gold 86.1 75.4 80.2
Gold-Charniak 77.3 60.1 68.1
Charniak-Charniak 77.2 64.8 70.4
Table 2: ASTRL performance (precision, recall and f-
score) for relation extraction. The comma types were
used only to learn the rules. During evaluation, only the
relations were scored.
6.3 Comma Resolution Performance
We present a detailed analysis of the performance of
the algorithm for comma resolution. Since this paper
is the first one that deals with the task, we could not
compare our results to previous work. Also, there
is no clear baseline to use. We tried a variant of
the most frequent baseline common in other disam-
biguation tasks, in which we labeled all commas as
OTHER (the most frequent type) except when there
are list indicators like and, or and but in adjacent
chunks (which are obtained using a shallow parser),
in which case the commas are labeled LIST. This
gives an average precision 0.85 and an average recall
of 0.36 for identifying the comma type. However,
this baseline does not help in identifying relations.
We use the following approach to evaluate the
comma type resolution and relation extraction per-
formance ? a relation extracted by the system is con-
sidered correct only if both the relation and the type
of the comma structure that generated it are correctly
identified. We call this metric the Relation-Type
metric. Another way of measuring the performance
of comma resolution is to measure the correctness of
the relations per comma type. In both cases, lists are
scored as in the Relation metric. The performance of
our system with respect to these two metrics are pre-
sented in Table 3. In this table, we also compare the
performance of the STRs learned by ASTRL with
the smallest valid STRs without further specializa-
tion (i.e., using just the procedure outlined in Sec-
tion 5.4).
1036
Type Gold-Gold Setting Gold-Charniak Setting
Relation-Type metric
Smallest Valid STRs ASTRL Smallest Valid STRs ASTRL
P R F P R F P R F P R F
Total 66.2 76.1 70.7 81.8 73.9 77.6 61.0 58.4 59.5 72.2 59.5 65.1
Relations Metric, Per Comma Type
ATTRIBUTE 40.4 68.2 50.4 70.6 59.4 64.1 35.5 39.7 36.2 56.6 37.7 44.9
SUBSTITUTE 80.0 84.3 81.9 87.9 84.8 86.1 75.8 72.9 74.3 78.0 76.1 76.9
LIST 70.9 58.1 63.5 76.2 57.8 65.5 58.7 53.4 55.6 65.2 53.3 58.5
LOCATION 93.8 86.4 89.1 93.8 86.4 89.1 70.3 37.2 47.2 70.3 37.2 47.2
Table 3: Performance of STRs learned by ASTRL and the smallest valid STRs in identifying comma types and
generating relations.
There is an important difference between the Re-
lation metric (Table 2) and the Relation-type met-
ric (top part of Table 3) that depends on the seman-
tic interpretation of the comma types. For example,
consider the sentence ?John Smith, 59, went home.?
If the system labels the commas in this as both AT-
TRIBUTE and SUBSTITUTE, then, both will gener-
ate the relation ?John Smith is 59.? According to
the Relation metric, there is no difference between
them. However, there is a semantic difference be-
tween the two sentences ? the ATTRIBUTE relation
says that being 59 is an attribute of John Smith while
the SUBSTITUTE relation says that John Smith is the
number 59. This difference is accounted for by the
Relation-Type metric.
From this standpoint, we can see that the special-
ization step performed in the full ASTRL algorithm
greatly helps in disambiguating between the AT-
TRIBUTE and SUBSTITUTE types and consequently,
the Relation-Type metric shows an error reduction
of 23.5% and 13.8% in the Gold-Gold and Gold-
Charniak settings respectively. In the Gold-Gold
scenario the performance of ASTRL is much better
than in the Gold-Charniak scenario. This reflects the
non-perfect performance of the parser in annotating
these sentences (parser F-score of 90%).
Another key evaluation question is the per-
formance of the method in identification of the
OTHER category. A comma is judged to be as
OTHER if no STR in the system applies to it.
The performance of ASTRL in this aspect is pre-
sented in Table 4. The categorization of this cate-
gory is important if we wish to further classify the
OTHER commas into finer categories.
Setting P R F
Gold-Gold 78.9 92.8 85.2
Gold-Charniak 72.5 92.2 81.2
Table 4: ASTRL performance (precision, recall and f-
score) for OTHER identification.
7 Conclusions
We defined the task of comma resolution, and devel-
oped a novel machine learning algorithm that learns
Sentence Transformation Rules to perform this task.
We experimented with both gold standard and parser
annotated sentences, and established a performance
level that seems good for a task of this complexity,
and which will provide a useful measure of future
systems developed for this task. When given au-
tomatically parsed sentences, performance degrades
but is still much higher than random, in both sce-
narios. We designed a comma annotation scheme,
where each comma unit is assigned one of four types
and an inference rule mapping the patterns of the
unit with the entailed relations. We created anno-
tated datasets which will be made available over the
web to facilitate further research.
Future work will investigate four main directions:
(i) studying the effects of inclusion of our approach
on the performance of Textual Entailment systems;
(ii) using features other than those derivable from
syntactic parse and named entity annotation of the
input sentence; (iii) recognizing a wider range of im-
plicit relations, represented by commas and in other
ways; (iv) adaptation to other domains.
Acknowledgement
The UIUC authors were supported by NSF grant
ITR IIS-0428472, DARPA funding under the Boot-
strap Learning Program and a grant from Boeing.
1037
References
M. Banko, M. Cafarella, M. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Proc. of IJCAI, pages 2670?2676.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In Proc. of AAAI, pages 871?876.
M. Bayraktar, B. Say, and V. Akman. 1998. An analysis
of english punctuation: The special case of comma.
International Journal of Corpus Linguistics, 3(1):33?
57.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
of the Annual Meeting of the ACL, pages 173?180.
A. Culotta and J. Sorenson. 2004. Dependency tree ker-
nels for relation extraction. In Proc. of the Annual
Meeting of the ACL, pages 423?429.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge., volume 3944. Springer-Verlag, Berlin.
D. Davidov and A. Rappoport. 2008. Unsupervised dis-
covery of generic relationships using pattern clusters
and its evaluation by automatically generated sat anal-
ogy questions. In Proc. of the Annual Meeting of the
ACL.
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the Annual Meeting
of the ACL, pages 232?239.
R. de Salvo Braz, R. Girju, V. Punyakanok, D. Roth, and
M. Sammons. 2005. An inference model for seman-
tic entailment in natural language. In Proc. of AAAI,
pages 1678?1679.
T. Hirano, Y. Matsuo, and G. Kikui. 2007. Detecting
semantic relations between named entities in text using
contextual features. In Proc. of the Annual Meeting of
the ACL, pages 157?160.
D. Lin and P. Pantel. 2001. DIRT: discovery of inference
rules from text. In Proc. of ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining 2001,
pages 323?328.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
G. Nunberg. 1990. CSLI Lecture Notes 18: The Lin-
guistics of Punctuation. CSLI Publications, Stanford,
CA.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lever-
aging generic patterns for automatically harvesting se-
mantic relations. In Proc. of the Annual Meeting of the
ACL, pages 113?120.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Proc. of the Annual Meeting of
the ACL, pages 809?816.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Hwee Tou Ng and Ellen Riloff, editors, Proc. of the
Annual Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 1?8. Association for
Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
B. Say and V. Akman. 1997. Current approaches to
punctuation in computational linguistics. Computers
and the Humanities, 30(6):457?469.
S. Sekine. 2006. On-demand information extraction. In
Proc. of the Annual Meeting of the ACL, pages 731?
738.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based of entailment relations. In Proc. of
EMNLP, pages 49?56.
S. van Delden and F. Gomez. 2002. Combining finite
state automata and a greedy learning algorithm to de-
termine the syntactic roles of commas. In Proc. of IC-
TAI, pages 293?300.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. Journal of Machine
Learning Research, 3:1083?1106.
1038
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 28?36,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Argument Identification for Semantic Role Labeling
Omri Abend1 Roi Reichart2 Ari Rappoport1
1Institute of Computer Science , 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
The task of Semantic Role Labeling
(SRL) is often divided into two sub-tasks:
verb argument identification, and argu-
ment classification. Current SRL algo-
rithms show lower results on the identifi-
cation sub-task. Moreover, most SRL al-
gorithms are supervised, relying on large
amounts of manually created data. In
this paper we present an unsupervised al-
gorithm for identifying verb arguments,
where the only type of annotation required
is POS tagging. The algorithm makes use
of a fully unsupervised syntactic parser,
using its output in order to detect clauses
and gather candidate argument colloca-
tion statistics. We evaluate our algorithm
on PropBank10, achieving a precision of
56%, as opposed to 47% of a strong base-
line. We also obtain an 8% increase in
precision for a Spanish corpus. This is
the first paper that tackles unsupervised
verb argument identification without using
manually encoded rules or extensive lexi-
cal or syntactic resources.
1 Introduction
Semantic Role Labeling (SRL) is a major NLP
task, providing a shallow sentence-level semantic
analysis. SRL aims at identifying the relations be-
tween the predicates (usually, verbs) in the sen-
tence and their associated arguments.
The SRL task is often viewed as consisting of
two parts: argument identification (ARGID) and ar-
gument classification. The former aims at identi-
fying the arguments of a given predicate present
in the sentence, while the latter determines the
type of relation that holds between the identi-
fied arguments and their corresponding predicates.
The division into two sub-tasks is justified by
the fact that they are best addressed using differ-
ent feature sets (Pradhan et al, 2005). Perfor-
mance in the ARGID stage is a serious bottleneck
for general SRL performance, since only about
81% of the arguments are identified, while about
95% of the identified arguments are labeled cor-
rectly (Ma`rquez et al, 2008).
SRL is a complex task, which is reflected by the
algorithms used to address it. A standard SRL al-
gorithm requires thousands to dozens of thousands
sentences annotated with POS tags, syntactic an-
notation and SRL annotation. Current algorithms
show impressive results but only for languages and
domains where plenty of annotated data is avail-
able, e.g., English newspaper texts (see Section 2).
Results are markedly lower when testing is on a
domain wider than the training one, even in En-
glish (see the WSJ-Brown results in (Pradhan et
al., 2008)).
Only a small number of works that do not re-
quire manually labeled SRL training data have
been done (Swier and Stevenson, 2004; Swier and
Stevenson, 2005; Grenager and Manning, 2006).
These papers have replaced this data with the
VerbNet (Kipper et al, 2000) lexical resource or
a set of manually written rules and supervised
parsers.
A potential answer to the SRL training data bot-
tleneck are unsupervised SRL models that require
little to no manual effort for their training. Their
output can be used either by itself, or as training
material for modern supervised SRL algorithms.
In this paper we present an algorithm for unsu-
pervised argument identification. The only type of
annotation required by our algorithm is POS tag-
28
ging, which needs relatively little manual effort.
The algorithm consists of two stages. As pre-
processing, we use a fully unsupervised parser to
parse each sentence. Initially, the set of possi-
ble arguments for a given verb consists of all the
constituents in the parse tree that do not contain
that predicate. The first stage of the algorithm
attempts to detect the minimal clause in the sen-
tence that contains the predicate in question. Us-
ing this information, it further reduces the possible
arguments only to those contained in the minimal
clause, and further prunes them according to their
position in the parse tree. In the second stage we
use pointwise mutual information to estimate the
collocation strength between the arguments and
the predicate, and use it to filter out instances of
weakly collocating predicate argument pairs.
We use two measures to evaluate the perfor-
mance of our algorithm, precision and F-score.
Precision reflects the algorithm?s applicability for
creating training data to be used by supervised
SRL models, while the standard SRL F-score mea-
sures the model?s performance when used by it-
self. The first stage of our algorithm is shown to
outperform a strong baseline both in terms of F-
score and of precision. The second stage is shown
to increase precision while maintaining a reason-
able recall.
We evaluated our model on sections 2-21 of
Propbank. As is customary in unsupervised pars-
ing work (e.g. (Seginer, 2007)), we bounded sen-
tence length by 10 (excluding punctuation). Our
first stage obtained a precision of 52.8%, which is
more than 6% improvement over the baseline. Our
second stage improved precision to nearly 56%, a
9.3% improvement over the baseline. In addition,
we carried out experiments on Spanish (on sen-
tences of length bounded by 15, excluding punctu-
ation), achieving an increase of over 7.5% in pre-
cision over the baseline. Our algorithm increases
F?score as well, showing an 1.8% improvement
over the baseline in English and a 2.2% improve-
ment in Spanish.
Section 2 reviews related work. In Section 3 we
detail our algorithm. Sections 4 and 5 describe the
experimental setup and results.
2 Related Work
The advance of machine learning based ap-
proaches in this field owes to the usage of large
scale annotated corpora. English is the most stud-
ied language, using the FrameNet (FN) (Baker et
al., 1998) and PropBank (PB) (Palmer et al, 2005)
resources. PB is a corpus well suited for evalu-
ation, since it annotates every non-auxiliary verb
in a real corpus (the WSJ sections of the Penn
Treebank). PB is a standard corpus for SRL eval-
uation and was used in the CoNLL SRL shared
tasks of 2004 (Carreras and Ma`rquez, 2004) and
2005 (Carreras and Ma`rquez, 2005).
Most work on SRL has been supervised, requir-
ing dozens of thousands of SRL annotated train-
ing sentences. In addition, most models assume
that a syntactic representation of the sentence is
given, commonly in the form of a parse tree, a de-
pendency structure or a shallow parse. Obtaining
these is quite costly in terms of required human
annotation.
The first work to tackle SRL as an indepen-
dent task is (Gildea and Jurafsky, 2002), which
presented a supervised model trained and evalu-
ated on FrameNet. The CoNLL shared tasks of
2004 and 2005 were devoted to SRL, and stud-
ied the influence of different syntactic annotations
and domain changes on SRL results. Computa-
tional Linguistics has recently published a special
issue on the task (Ma`rquez et al, 2008), which
presents state-of-the-art results and surveys the lat-
est achievements and challenges in the field.
Most approaches to the task use a multi-level
approach, separating the task to an ARGID and an
argument classification sub-tasks. They then use
the unlabeled argument structure (without the se-
mantic roles) as training data for the ARGID stage
and the entire data (perhaps with other features)
for the classification stage. Better performance
is achieved on the classification, where state-
of-the-art supervised approaches achieve about
81% F-score on the in-domain identification task,
of which about 95% are later labeled correctly
(Ma`rquez et al, 2008).
There have been several exceptions to the stan-
dard architecture described in the last paragraph.
One suggestion poses the problem of SRL as a se-
quential tagging of words, training an SVM clas-
sifier to determine for each word whether it is in-
side, outside or in the beginning of an argument
(Hacioglu and Ward, 2003). Other works have in-
tegrated argument classification and identification
into one step (Collobert and Weston, 2007), while
others went further and combined the former two
along with parsing into a single model (Musillo
29
and Merlo, 2006).
Work on less supervised methods has been
scarce. Swier and Stevenson (2004) and Swier
and Stevenson (2005) presented the first model
that does not use an SRL annotated corpus. How-
ever, they utilize the extensive verb lexicon Verb-
Net, which lists the possible argument structures
allowable for each verb, and supervised syntac-
tic tools. Using VerbNet alng with the output of
a rule-based chunker (in 2004) and a supervised
syntactic parser (in 2005), they spot instances in
the corpus that are very similar to the syntactic
patterns listed in VerbNet. They then use these as
seed for a bootstrapping algorithm, which conse-
quently identifies the verb arguments in the corpus
and assigns their semantic roles.
Another less supervised work is that
of (Grenager and Manning, 2006), which presents
a Bayesian network model for the argument
structure of a sentence. They use EM to learn
the model?s parameters from unannotated data,
and use this model to tag a test corpus. However,
ARGID was not the task of that work, which dealt
solely with argument classification. ARGID was
performed by manually-created rules, requiring a
supervised or manual syntactic annotation of the
corpus to be annotated.
The three works above are relevant but incom-
parable to our work, due to the extensive amount
of supervision (namely, VerbNet and a rule-based
or supervised syntactic system) they used, both in
detecting the syntactic structure and in detecting
the arguments.
Work has been carried out in a few other lan-
guages besides English. Chinese has been studied
in (Xue, 2008). Experiments on Catalan and Span-
ish were done in SemEval 2007 (Ma`rquez et al,
2007) with two participating systems. Attempts
to compile corpora for German (Burdchardt et al,
2006) and Arabic (Diab et al, 2008) are also un-
derway. The small number of languages for which
extensive SRL annotated data exists reflects the
considerable human effort required for such en-
deavors.
Some SRL works have tried to use unannotated
data to improve the performance of a base su-
pervised model. Methods used include bootstrap-
ping approaches (Gildea and Jurafsky, 2002; Kate
and Mooney, 2007), where large unannotated cor-
pora were tagged with SRL annotation, later to
be used to retrain the SRL model. Another ap-
proach used similarity measures either between
verbs (Gordon and Swanson, 2007) or between
nouns (Gildea and Jurafsky, 2002) to overcome
lexical sparsity. These measures were estimated
using statistics gathered from corpora augmenting
the model?s training data, and were then utilized
to generalize across similar verbs or similar argu-
ments.
Attempts to substitute full constituency pars-
ing by other sources of syntactic information have
been carried out in the SRL community. Sugges-
tions include posing SRL as a sequence labeling
problem (Ma`rquez et al, 2005) or as an edge tag-
ging problem in a dependency representation (Ha-
cioglu, 2004). Punyakanok et al (2008) provide
a detailed comparison between the impact of us-
ing shallow vs. full constituency syntactic infor-
mation in an English SRL system. Their results
clearly demonstrate the advantage of using full an-
notation.
The identification of arguments has also been
carried out in the context of automatic subcatego-
rization frame acquisition. Notable examples in-
clude (Manning, 1993; Briscoe and Carroll, 1997;
Korhonen, 2002) who all used statistical hypothe-
sis testing to filter a parser?s output for arguments,
with the goal of compiling verb subcategorization
lexicons. However, these works differ from ours
as they attempt to characterize the behavior of a
verb type, by collecting statistics from various in-
stances of that verb, and not to determine which
are the arguments of specific verb instances.
The algorithm presented in this paper performs
unsupervised clause detection as an intermedi-
ate step towards argument identification. Super-
vised clause detection was also tackled as a sepa-
rate task, notably in the CoNLL 2001 shared task
(Tjong Kim Sang and De`jean, 2001). Clause in-
formation has been applied to accelerating a syn-
tactic parser (Glaysher and Moldovan, 2006).
3 Algorithm
In this section we describe our algorithm. It con-
sists of two stages, each of which reduces the set
of argument candidates, which a-priori contains all
consecutive sequences of words that do not con-
tain the predicate in question.
3.1 Algorithm overview
As pre-processing, we use an unsupervised parser
that generates an unlabeled parse tree for each sen-
30
tence (Seginer, 2007). This parser is unique in that
it is able to induce a bracketing (unlabeled pars-
ing) from raw text (without even using POS tags)
achieving state-of-the-art results. Since our algo-
rithm uses millions to tens of millions sentences,
we must use very fast tools. The parser?s high
speed (thousands of words per second) enables us
to process these large amounts of data.
The only type of supervised annotation we
use is POS tagging. We use the taggers MX-
POST (Ratnaparkhi, 1996) for English and Tree-
Tagger (Schmid, 1994) for Spanish, to obtain POS
tags for our model.
The first stage of our algorithm uses linguisti-
cally motivated considerations to reduce the set of
possible arguments. It does so by confining the set
of argument candidates only to those constituents
which obey the following two restrictions. First,
they should be contained in the minimal clause
containing the predicate. Second, they should be
k-th degree cousins of the predicate in the parse
tree. We propose a novel algorithm for clause de-
tection and use its output to determine which of
the constituents obey these two restrictions.
The second stage of the algorithm uses point-
wise mutual information to rule out constituents
that appear to be weakly collocating with the pred-
icate in question. Since a predicate greatly re-
stricts the type of arguments with which it may
appear (this is often referred to as ?selectional re-
strictions?), we expect it to have certain character-
istic arguments with which it is likely to collocate.
3.2 Clause detection stage
The main idea behind this stage is the observation
that most of the arguments of a predicate are con-
tained within the minimal clause that contains the
predicate. We tested this on our development data
? section 24 of the WSJ PTB, where we saw that
86% of the arguments that are also constituents
(in the gold standard parse) were indeed contained
in that minimal clause (as defined by the tree la-
bel types in the gold standard parse that denote
a clause, e.g., S, SBAR). Since we are not pro-
vided with clause annotation (or any label), we at-
tempted to detect them in an unsupervised manner.
Our algorithm attempts to find sub-trees within the
parse tree, whose structure resembles the structure
of a full sentence. This approximates the notion of
a clause.
L
L
DT
The
NNS
materials
L
L
IN
in
L
DT
each
NN
set
L
VBP
reach
L
L
IN
about
CD
90
NNS
students
L
L L
L L
VBP L
L
VBP L
Figure 1: An example of an unlabeled POS tagged
parse tree. The middle tree is the ST of ?reach?
with the root as the encoded ancestor. The bot-
tom one is the ST with its parent as the encoded
ancestor.
Statistics gathering. In order to detect which
of the verb?s ancestors is the minimal clause, we
score each of the ancestors and select the one that
maximizes the score. We represent each ancestor
using its Spinal Tree (ST ). The ST of a given
verb?s ancestor is obtained by replacing all the
constituents that do not contain the verb by a leaf
having a label. This effectively encodes all the k-
th degree cousins of the verb (for every k). The
leaf labels are either the word?s POS in case the
constituent is a leaf, or the generic label ?L? de-
noting a non-leaf. See Figure 1 for an example.
In this stage we collect statistics of the occur-
rences of ST s in a large corpus. For every ST in
the corpus, we count the number of times it oc-
curs in a form we consider to be a clause (positive
examples), and the number of times it appears in
other forms (negative examples).
Positive examples are divided into two main
types. First, when the ST encodes the root an-
cestor (as in the middle tree of Figure 1); second,
when the ancestor complies to a clause lexico-
syntactic pattern. In many languages there is a
small set of lexico-syntactic patterns that mark a
clause, e.g. the English ?that?, the German ?dass?
and the Spanish ?que?. The patterns which were
used in our experiments are shown in Figure 2.
For each verb instance, we traverse over its an-
31
English
TO + VB. The constituent starts with ?to? followed by
a verb in infinitive form.
WP. The constituent is preceded by a Wh-pronoun.
That. The constituent is preceded by a ?that? marked
by an ?IN? POS tag indicating that it is a subordinating
conjunction.
Spanish
CQUE. The constituent is preceded by a word with the
POS ?CQUE? which denotes the word ?que? as a con-
junction.
INT. The constituent is preceded by a word with the
POS ?INT? which denotes an interrogative pronoun.
CSUB. The constituent is preceded by a word with one
of the POSs ?CSUBF?, ?CSUBI? or ?CSUBX?, which
denote a subordinating conjunction.
Figure 2: The set of lexico-syntactic patterns that
mark clauses which were used by our model.
cestors from top to bottom. For each of them we
update the following counters: sentence(ST ) for
the root ancestor?s ST , patterni(ST ) for the ones
complying to the i-th lexico-syntactic pattern and
negative(ST ) for the other ancestors1.
Clause detection. At test time, when detecting
the minimal clause of a verb instance, we use
the statistics collected in the previous stage. De-
note the ancestors of the verb with A1 . . . Am.
For each of them, we calculate clause(STAj )
and total(STAj ). clause(STAj ) is the sum
of sentence(STAj ) and patterni(STAj ) if this
ancestor complies to the i-th pattern (if there
is no such pattern, clause(STAj ) is equal to
sentence(STAj )). total(STAj ) is the sum of
clause(STAj ) and negative(STAj ).
The selected ancestor is given by:
(1) Amax = argmaxAj
clause(STAj )
total(STAj )
An ST whose total(ST ) is less than a small
threshold2 is not considered a candidate to be the
minimal clause, since its statistics may be un-
reliable. In case of a tie, we choose the low-
est constituent that obtained the maximal score.
1If while traversing the tree, we encounter an ancestor
whose first word is preceded by a coordinating conjunction
(marked by the POS tag ?CC?), we refrain from performing
any additional counter updates. Structures containing coor-
dinating conjunctions tend not to obey our lexico-syntactic
rules.
2We used 4 per million sentences, derived from develop-
ment data.
If there is only one verb in the sentence3 or if
clause(STAj ) = 0 for every 1 ? j ? m, we
choose the top level constituent by default to be
the minimal clause containing the verb. Other-
wise, the minimal clause is defined to be the yield
of the selected ancestor.
Argument identification. For each predicate in
the corpus, its argument candidates are now de-
fined to be the constituents contained in the min-
imal clause containing the predicate. However,
these constituents may be (and are) nested within
each other, violating a major restriction on SRL
arguments. Hence we now prune our set, by keep-
ing only the siblings of all of the verb?s ancestors,
as is common in supervised SRL (Xue and Palmer,
2004).
3.3 Using collocations
We use the following observation to filter out some
superfluous argument candidates: since the argu-
ments of a predicate many times bear a semantic
connection with that predicate, they consequently
tend to collocate with it.
We collect collocation statistics from a large
corpus, which we annotate with parse trees and
POS tags. We mark arguments using the argu-
ment detection algorithm described in the previous
two sections, and extract all (predicate, argument)
pairs appearing in the corpus. Recall that for each
sentence, the arguments are a subset of the con-
stituents in the parse tree.
We use two representations of an argument: one
is the POS tag sequence of the terminals contained
in the argument, the other is its head word4. The
predicate is represented as the conjunction of its
lemma with its POS tag.
Denote the number of times a predicate x
appeared with an argument y by nxy. Denote
the total number of (predicate, argument) pairs
by N . Using these notations, we define the
following quantities: nx = ?ynxy, ny = ?xnxy,
p(x) = nxN , p(y) =
ny
N and p(x, y) =
nxy
N . The
pointwise mutual information of x and y is then
given by:
3In this case, every argument in the sentence must be re-
lated to that verb.
4Since we do not have syntactic labels, we use an approx-
imate notion. For English we use the Bikel parser default
head word rules (Bikel, 2004). For Spanish, we use the left-
most word.
32
(2) PMI(x, y) = log p(x,y)p(x)?p(y) = log
nxy
(nx?ny)/N
PMI effectively measures the ratio between
the number of times x and y appeared together and
the number of times they were expected to appear,
had they been independent.
At test time, when an (x, y) pair is observed, we
check if PMI(x, y), computed on the large cor-
pus, is lower than a threshold ? for either of x?s
representations. If this holds, for at least one rep-
resentation, we prune all instances of that (x, y)
pair. The parameter ? may be selected differently
for each of the argument representations.
In order to avoid using unreliable statistics,
we apply this for a given pair only if nx?nyN >
r, for some parameter r. That is, we consider
PMI(x, y) to be reliable, only if the denomina-
tor in equation (2) is sufficiently large.
4 Experimental Setup
Corpora. We used the PropBank corpus for de-
velopment and for evaluation on English. Section
24 was used for the development of our model,
and sections 2 to 21 were used as our test data.
The free parameters of the collocation extraction
phase were tuned on the development data. Fol-
lowing the unsupervised parsing literature, multi-
ple brackets and brackets covering a single word
are omitted. We exclude punctuation according
to the scheme of (Klein, 2005). As is customary
in unsupervised parsing (e.g. (Seginer, 2007)), we
bounded the lengths of the sentences in the cor-
pus to be at most 10 (excluding punctuation). This
results in 207 sentences in the development data,
containing a total of 132 different verbs and 173
verb instances (of the non-auxiliary verbs in the
SRL task, see ?evaluation? below) having 403 ar-
guments. The test data has 6007 sentences con-
taining 1008 different verbs and 5130 verb in-
stances (as above) having 12436 arguments.
Our algorithm requires large amounts of data
to gather argument structure and collocation pat-
terns. For the statistics gathering phase of the
clause detection algorithm, we used 4.5M sen-
tences of the NANC (Graff, 1995) corpus, bound-
ing their length in the same manner. In order
to extract collocations, we used 2M sentences
from the British National Corpus (Burnard, 2000)
and about 29M sentences from the Dmoz cor-
pus (Gabrilovich and Markovitch, 2005). Dmoz
is a web corpus obtained by crawling and clean-
ing the URLs in the Open Directory Project
(dmoz.org). All of the above corpora were parsed
using Seginer?s parser and POS-tagged by MX-
POST (Ratnaparkhi, 1996).
For our experiments on Spanish, we used 3.3M
sentences of length at most 15 (excluding punctua-
tion) extracted from the Spanish Wikipedia. Here
we chose to bound the length by 15 due to the
smaller size of the available test corpus. The
same data was used both for the first and the sec-
ond stages. Our development and test data were
taken from the training data released for the Se-
mEval 2007 task on semantic annotation of Span-
ish (Ma`rquez et al, 2007). This data consisted
of 1048 sentences of length up to 15, from which
200 were randomly selected as our development
data and 848 as our test data. The development
data included 313 verb instances while the test
data included 1279. All corpora were parsed us-
ing the Seginer parser and tagged by the ?Tree-
Tagger? (Schmid, 1994).
Baselines. Since this is the first paper, to our
knowledge, which addresses the problem of unsu-
pervised argument identification, we do not have
any previous results to compare to. We instead
compare to a baseline which marks all k-th degree
cousins of the predicate (for every k) as arguments
(this is the second pruning we use in the clause
detection stage). We name this baseline the ALL
COUSINS baseline. We note that a random base-
line would score very poorly since any sequence of
terminals which does not contain the predicate is
a possible candidate. Therefore, beating this ran-
dom baseline is trivial.
Evaluation. Evaluation is carried out using
standard SRL evaluation software5. The algorithm
is provided with a list of predicates, whose argu-
ments it needs to annotate. For the task addressed
in this paper, non-consecutive parts of arguments
are treated as full arguments. A match is consid-
ered each time an argument in the gold standard
data matches a marked argument in our model?s
output. An unmatched argument is an argument
which appears in the gold standard data, and fails
to appear in our model?s output, and an exces-
sive argument is an argument which appears in
our model?s output but does not appear in the gold
standard. Precision and recall are defined accord-
ingly. We report an F-score as well (the harmonic
mean of precision and recall). We do not attempt
5http://www.lsi.upc.edu/?srlconll/soft.html#software.
33
to identify multi-word verbs, and therefore do not
report the model?s performance in identifying verb
boundaries.
Since our model detects clauses as an interme-
diate product, we provide a separate evaluation
of this task for the English corpus. We show re-
sults on our development data. We use the stan-
dard parsing F-score evaluation measure. As a
gold standard in this evaluation, we mark for each
of the verbs in our development data the minimal
clause containing it. A minimal clause is the low-
est ancestor of the verb in the parse tree that has
a syntactic label of a clause according to the gold
standard parse of the PTB. A verb is any terminal
marked by one of the POS tags of type verb ac-
cording to the gold standard POS tags of the PTB.
5 Results
Our results are shown in Table 1. The left section
presents results on English and the right section
presents results on Spanish. The top line lists re-
sults of the clause detection stage alone. The next
two lines list results of the full algorithm (clause
detection + collocations) in two different settings
of the collocation stage. The bottom line presents
the performance of the ALL COUSINS baseline.
In the ?Collocation Maximum Precision? set-
ting the parameters of the collocation stage (? and
r) were generally tuned such that maximal preci-
sion is achieved while preserving a minimal recall
level (40% for English, 20% for Spanish on the de-
velopment data). In the ?Collocation Maximum F-
score? the collocation parameters were generally
tuned such that the maximum possible F-score for
the collocation algorithm is achieved.
The best or close to best F-score is achieved
when using the clause detection algorithm alone
(59.14% for English, 23.34% for Spanish). Note
that for both English and Spanish F-score im-
provements are achieved via a precision improve-
ment that is more significant than the recall degra-
dation. F-score maximization would be the aim of
a system that uses the output of our unsupervised
ARGID by itself.
The ?Collocation Maximum Precision?
achieves the best precision level (55.97% for
English, 21.8% for Spanish) but at the expense
of the largest recall loss. Still, it maintains a
reasonable level of recall. The ?Collocation
Maximum F-score? is an example of a model that
provides a precision improvement (over both the
baseline and the clause detection stage) with a
relatively small recall degradation. In the Spanish
experiments its F-score (23.87%) is even a bit
higher than that of the clause detection stage
(23.34%).
The full two?stage algorithm (clause detection
+ collocations) should thus be used when we in-
tend to use the model?s output as training data for
supervised SRL engines or supervised ARGID al-
gorithms.
In our algorithm, the initial set of potential ar-
guments consists of constituents in the Seginer
parser?s parse tree. Consequently the fraction
of arguments that are also constituents (81.87%
for English and 51.83% for Spanish) poses an
upper bound on our algorithm?s recall. Note
that the recall of the ALL COUSINS baseline is
74.27% (45.75%) for English (Spanish). This
score emphasizes the baseline?s strength, and jus-
tifies the restriction that the arguments should be
k-th cousins of the predicate. The difference be-
tween these bounds for the two languages provides
a partial explanation for the corresponding gap in
the algorithm?s performance.
Figure 3 shows the precision of the collocation
model (on development data) as a function of the
amount of data it was given. We can see that
the algorithm reaches saturation at about 5M sen-
tences. It achieves this precision while maintain-
ing a reasonable recall (an average recall of 43.1%
after saturation). The parameters of the colloca-
tion model were separately tuned for each corpus
size, and the graph displays the maximum which
was obtained for each of the corpus sizes.
To better understand our model?s performance,
we performed experiments on the English cor-
pus to test how well its first stage detects clauses.
Clause detection is used by our algorithm as a step
towards argument identification, but it can be of
potential benefit for other purposes as well (see
Section 2). The results are 23.88% recall and 40%
precision. As in the ARGID task, a random se-
lection of arguments would have yielded an ex-
tremely poor result.
6 Conclusion
In this work we presented the first algorithm for ar-
gument identification that uses neither supervised
syntactic annotation nor SRL tagged data. We
have experimented on two languages: English and
Spanish. The straightforward adaptability of un-
34
English (Test Data) Spanish (Test Data)
Precision Recall F1 Precision Recall F1
Clause Detection 52.84 67.14 59.14 18.00 33.19 23.34
Collocation Maximum F?score 54.11 63.53 58.44 20.22 29.13 23.87
Collocation Maximum Precision 55.97 40.02 46.67 21.80 18.47 20.00
ALL COUSINS baseline 46.71 74.27 57.35 14.16 45.75 21.62
Table 1: Precision, Recall and F1 score for the different stages of our algorithm. Results are given for English (PTB, sentences
length bounded by 10, left part of the table) and Spanish (SemEval 2007 Spanish SRL task, right part of the table). The results
of the collocation (second) stage are given in two configurations, Collocation Maximum F-score and Collocation Maximum
Precision (see text). The upper bounds on Recall, obtained by taking all arguments output by our unsupervised parser, are
81.87% for English and 51.83% for Spanish.
0 2 4 6 8 10
42
44
46
48
50
52
Number of Sentences (Millions)
Pr
ec
isi
on
 
 
Second Stage
First Stage
Baseline
Figure 3: The performance of the second stage on English
(squares) vs. corpus size. The precision of the baseline (trian-
gles) and of the first stage (circles) is displayed for reference.
The graph indicates the maximum precision obtained for each
corpus size. The graph reaches saturation at about 5M sen-
tences. The average recall of the sampled points from there
on is 43.1%. Experiments were performed on the English
development data.
supervised models to different languages is one
of their most appealing characteristics. The re-
cent availability of unsupervised syntactic parsers
has offered an opportunity to conduct research on
SRL, without reliance on supervised syntactic an-
notation. This work is the first to address the ap-
plication of unsupervised parses to an SRL related
task.
Our model displayed an increase in precision of
9% in English and 8% in Spanish over a strong
baseline. Precision is of particular interest in this
context, as instances tagged by high quality an-
notation could be later used as training data for
supervised SRL algorithms. In terms of F?score,
our model showed an increase of 1.8% in English
and of 2.2% in Spanish over the baseline.
Although the quality of unsupervised parses is
currently low (compared to that of supervised ap-
proaches), using great amounts of data in identi-
fying recurring structures may reduce noise and
in addition address sparsity. The techniques pre-
sented in this paper are based on this observation,
using around 35M sentences in total for English
and 3.3M sentences for Spanish.
As this is the first work which addressed un-
supervised ARGID, many questions remain to be
explored. Interesting issues to address include as-
sessing the utility of the proposed methods when
supervised parses are given, comparing our model
to systems with no access to unsupervised parses
and conducting evaluation using more relaxed
measures.
Unsupervised methods for syntactic tasks have
matured substantially in the last few years. No-
table examples are (Clark, 2003) for unsupervised
POS tagging and (Smith and Eisner, 2006) for un-
supervised dependency parsing. Adapting our al-
gorithm to use the output of these models, either to
reduce the little supervision our algorithm requires
(POS tagging) or to provide complementary syn-
tactic information, is an interesting challenge for
future work.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. ACL-
COLING ?98.
Daniel M. Bikel, 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Ted Briscoe, John Carroll, 1997. Automatic Extraction
of Subcategorization from Corpora. Applied NLP
1997.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad and Manfred Pinkal, 2006
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. LREC ?06.
Lou Burnard, 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University.
Xavier Carreras and Llu?`s Ma`rquez, 2004. Intro-
duction to the CoNLL?2004 Shared Task: Semantic
Role Labeling. CoNLL ?04.
35
Xavier Carreras and Llu?`s Ma`rquez, 2005. Intro-
duction to the CoNLL?2005 Shared Task: Semantic
Role Labeling. CoNLL ?05.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Ronan Collobert and Jason Weston, 2007. Fast Se-
mantic Extraction Using a Novel Neural Network
Architecture. ACL ?07.
Mona Diab, Aous Mansouri, Martha Palmer, Olga
Babko-Malaya, Wajdi Zaghouani, Ann Bies and
Mohammed Maamouri, 2008. A pilot Arabic Prop-
Bank. LREC ?08.
Evgeniy Gabrilovich and Shaul Markovitch, 2005.
Feature Generation for Text Categorization using
World Knowledge. IJCAI ?05.
Daniel Gildea and Daniel Jurafsky, 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Elliot Glaysher and Dan Moldovan, 2006. Speed-
ing Up Full Syntactic Parsing by Leveraging Partial
Parsing Decisions. COLING/ACL ?06 poster ses-
sion.
Andrew Gordon and Reid Swanson, 2007. Generaliz-
ing Semantic Role Annotations across Syntactically
Similar Verbs. ACL ?07.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Trond Grenager and Christopher D. Manning, 2006.
Unsupervised Discovery of a Statistical Verb Lexi-
con. EMNLP ?06.
Kadri Hacioglu, 2004. Semantic Role Labeling using
Dependency Trees. COLING ?04.
Kadri Hacioglu and Wayne Ward, 2003. Target Word
Detection and Semantic Role Chunking using Sup-
port Vector Machines. HLT-NAACL ?03.
Rohit J. Kate and Raymond J. Mooney, 2007. Semi-
Supervised Learning for Semantic Parsing using
Support Vector Machines. HLT?NAACL ?07.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Dan Klein, 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Anna Korhonen, 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Christopher D. Manning, 1993. Automatic Acquisition
of a Large Subcategorization Dictionary. ACL ?93.
Llu?`s Ma`rquez, Xavier Carreras, Kenneth C. Lit-
tkowski and Suzanne Stevenson, 2008. Semantic
Role Labeling: An introdution to the Special Issue.
Computational Linguistics, 34(2):145?159
Llu?`s Ma`rquez, Jesus Gime`nez Pere Comas and Neus
Catala`, 2005. Semantic Role Labeling as Sequential
Tagging. CoNLL ?05.
Llu?`s Ma`rquez, Lluis Villarejo, M. A. Mart?` and Mar-
iona Taule`, 2007. SemEval?2007 Task 09: Multi-
level Semantic Annotation of Catalan and Spanish.
The 4th international workshop on Semantic Evalu-
ations (SemEval ?07).
Gabriele Musillo and Paula Merlo, 2006. Accurate
Parsing of the proposition bank. HLT-NAACL ?06.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky,
2005. Support Vector Learning for Semantic Argu-
ment Classification. Machine Learning, 60(1):11?
39.
Sameer Pradhan, Wayne Ward, James H. Martin, 2008.
Towards Robust Semantic Role Labeling. Computa-
tional Linguistics, 34(2):289?310.
Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?96.
Helmut Schmid, 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees International Confer-
ence on New Methods in Language Processing.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing
Structural Bias in Multilingual Weighted Grammar
Induction. ACL ?06.
Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
pervised Semantic Role Labeling. EMNLP ?04.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. EMNLP ?05.
Erik F. Tjong Kim Sang and Herve? De?jean, 2001. In-
troduction to the CoNLL-2001 Shared Task: Clause
Identification. CoNLL ?01.
Nianwen Xue and Martha Palmer, 2004. Calibrating
Features for Semantic Role Labeling. EMNLP ?04.
Nianwen Xue, 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2):225?255.
36
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 45?52,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Second Language Acquisition Model Using  
Example Generalization and Concept Categories 
 
Ari Rappoport Vera Sheinman 
Institute of Computer Science Institute of Computer Science 
The Hebrew University The Hebrew University 
Jerusalem, Israel Jerusalem, Israel 
arir@cs.huji.ac.il vera46@cl.cs.titech.ac.jp 
 
Abstract 
We present a computational model of ac-
quiring a second language from example 
sentences. Our learning algorithms build a 
construction grammar language model, 
and generalize using form-based patterns 
and the learner?s conceptual system. We 
use a unique professional language learn-
ing corpus, and show that substantial reli-
able learning can be achieved even though 
the corpus is very small. The model is ap-
plied to assisting the authoring of Japa-
nese language learning corpora. 
1 Introduction 
Second Language Acquisition (SLA) is a central 
topic in many of the fields of activity related to 
human languages. SLA is studied in cognitive sci-
ence and theoretical linguistics in order to gain a 
better understanding of our general cognitive abili-
ties and of first language acquisition (FLA)1. Gov-
ernments, enterprises and individuals invest 
heavily in foreign language learning due to busi-
ness, cultural, and leisure time considerations. SLA 
is thus vital for both theory and practice and should 
be seriously examined in computational linguistics 
(CL), especially when considering the close rela-
tionship to FLA and the growing attention devoted 
to the latter by the CL community. 
In this paper we present a computational model 
of SLA. As far as we could determine, this is the 
first model that simulates the learning process 
 
1 Note that the F stands here for ?First?, not ?Foreign?.  
computationally. Learning is done from examples, 
with no reliance on explicit rules. The model is 
unique in the usage of a conceptual system by the 
learning algorithms. We use a unique professional 
language learning corpus, showing effective learn-
ing from a very small number of examples. We 
evaluate the model by applying it to assisting the 
authoring of Japanese language learning corpora.   
We focus here on basic linguistic aspects of 
SLA, leaving other aspects to future papers. In par-
ticular, we assume that the learner possesses per-
fect memory and is capable of invoking the 
provided learning algorithms without errors.  
In sections 2 and 3 we provide relevant back-
ground and discuss previous work. Our input, 
learner and language models are presented in sec-
tion 4, and the learning algorithms in section 5. 
Section 6 discusses the authoring application. 
2 Background 
We use the term ?second language acquisition? to 
refer to any situation in which adults learn a new 
language2. A major concept in SLA theory 
[Gass01, Mitchell03] is that of interlanguage:
when learning a new language (L2), at any given 
point in time the learner has a valid partial L2 lan-
guage system that differs from his/her native lan-
guage(s) (L1) and from the L2. The SLA process is 
that of progressive enhancement and refinement of 
interlanguage. The main trigger for interlanguage 
modification is when the learner notices a gap be-
tween interlanguage and L2 forms. In order for this 
to happen, the learner must be provided with com-
 
2 Some SLA texts distinguish between ?second? and ?foreign? 
and between ?acquisition? and ?learning?. We will not make 
those distinctions here.   
45
prehensible input. Our model directly supports all 
of these notions.  
A central, debated issue in language acquisition 
is whether FLA mechanisms [Clark03] are avail-
able in SLA. What is clear is that SL learners al-
ready possess a mature conceptual system and are 
capable of explicit symbolic reasoning and abstrac-
tion. In addition, the amount of input and time 
available for FLA are usually orders of magnitude 
larger than those for SLA. 
The general linguistic framework that we utilize 
in this paper is that of Construction Grammar 
(CG) [Goldberg95, Croft01], in which the building 
blocks of language are words, phrases and phrase 
templates that carry meanings. [Tomasello03] pre-
sents a CG theory of FLA in which children learn 
whole constructions as ?islands? that are gradually 
generalized and merged. Our SLA model is quite 
similar to this process. 
In language education, current classroom meth-
ods use a combination of formal rules and commu-
nicative situations. Radically different is the 
Pimsleur method [Pimsleur05], an audio-based 
self-study method in which rules and explanations 
are kept to a minimum and most learning occurs by 
letting the learner infer L2 constructs from transla-
tions of contextual L1 sentences. Substantial anec-
dotal evidence (as manifested by learner comments 
and our own experience) suggests that the method 
is highly effective. We have used a Pimsleur cor-
pus in our experiments. One of the goals of our 
model is to assist the authoring of such corpora. 
3 Previous Work 
There is almost no previous CL work explicitly 
addressing SLA. The only one of which we are 
aware is [Maritxalar97], which represents interlan-
guage levels using manually defined symbolic 
rules. No language model (in the CL sense) or 
automatic learning are provided.   
Many aspects of SLA are similar to first lan-
guage acquisition. Unsupervised grammar induc-
tion from corpora is a growing CL research area 
([Clark01, Klein05] and references there), mostly 
using statistical learning of model parameters or 
pattern identification by distributional criteria. The 
resulting models are not easily presentable to hu-
mans, and do not utilize semantics.  
[Edelman04] presents an elegant FLA system in 
which constructions and word categories are iden-
tified iteratively using a graph. [Chang04] presents 
an FLA system that truly supports construction 
grammar and is unique in its incorporation of gen-
eral cognitive concepts and embodied semantics.  
SLA is related to machine translation (MT), 
since learning how to translate is a kind of acquisi-
tion of the L2. Most relevant to us here is modern 
example-based machine translation (EBMT) [So-
mers01, Carl03], due to its explicit computation of 
translation templates and to the naturalness of 
learning from a small number of examples 
[Brown00, Cicekli01]. 
The Computer Assisted Language Learning 
(CALL) literature [Levy97, Chapelle01] is rich in 
project descriptions, and there are several commer-
cial CALL software applications. In general, 
CALL applications focus on teacher, environment, 
memory and automatization aspects, and are thus 
complementary to the goals that we address here. 
4 Input, Learner and Language Knowl-
edge Models  
Our ultimate goal is a comprehensive computa-
tional model of SLA that covers all aspects of the 
phenomenon. The present paper is a first step in 
that direction. Our goals here are to:  
 
 Explore what can be learned from exam-
ple-based, small, beginner-level input 
corpora tailored for SLA; 
 Model a learner having a mature concep-
tual system;
 Use an L2 language knowledge model 
that supports sentence enumeration; 
 Identify cognitively plausible and effective 
SL learning algorithms;
 Apply the model in assisting the author-
ing of corpora tailored for SLA.  
In this section we present the first three compo-
nents; the learning algorithms and the application 
are presented in the next two sections. 
4.1 Input Model 
The input potentially available for SL learners is of 
high variability, consisting of meta-linguistic rules, 
usage examples isolated for learning purposes, us-
age examples partially or fully understood in con-
text, dictionary-like word definitions, free-form 
explanations, and more.  
46
One of our major goals is to explore the rela-
tionship between first and second language acqui-
sition. Methodologically, it therefore makes sense 
to first study input that is the most similar linguis-
tically to that available during FLA, usage exam-
ples. As noted in section 2, a fundamental property 
of SLA is that learners are capable of mature un-
derstanding. Input in our model will thus consist of 
an ordered set of comprehensible usage exam-
ples, where an example is a pair of L1, L2 sen-
tences such that the former is a translation of the 
latter in a certain understood context.  
We focus here on modeling beginner-level pro-
ficiency, which is qualitatively different from na-
tive-like fluency [Gass01] and should be studied 
before the latter. 
We are interested in relatively small input cor-
pora (thousands of examples at most), because this 
is an essential part of SLA modeling. In addition, it 
is of great importance, in both theoretical and 
computational linguistics, to explore the limits of 
what can be learned from meager input.  
One of the main goals of SLA modeling is to 
discover which input is most effective for SLA, 
because a substantial part of learners? input can be 
controlled, while their time capacity is small. We 
thus allow our input to be optimized for SLA, by 
containing examples that are sub-parts of other 
examples and whose sole purpose is to facilitate 
learning those (our corpus is also optimized in the 
sense of covering simpler constructs and words 
first, but this issue is orthogonal to our model). We 
utilize two types of such sub-examples. First, we 
require that new words are always presented first 
on their own. This is easy to achieve in controlled 
teaching, and is actually very frequent in FLA as 
well [Clark03]. In the present paper we will as-
sume that this completely solves the task of seg-
menting a sentence into words, which is reasonable 
for a beginner level corpus where the total number 
of words is relatively small. Word boundaries are 
thus explicitly and consistently marked.  
Second, the sub-example mechanism is also use-
ful when learning a construction. For example, if 
the L2 sentence is ?the boy went to school? (where 
the L2 here is English), it could help learning algo-
rithms if it were preceded by ?to school? or ?the 
boy?. Hence we do not require examples to be 
complete sentences.  
In this paper we do not deal with phonetics or 
writing systems, assuming L2 speech has been 
consistently transcribed using a quasi-phonetic 
writing system. Learning L2 phonemes is certainly 
an important task in SLA, but most linguistic and 
cognitive theories view it as separable from the rest 
of language acquisition [Fromkin02, Medin05].  
The input corpus we have used is a transcribed 
Pimsleur Japanese course, which fits the input 
specification above. 
4.2 Learner Model 
A major aspect of SLA is that learners already pos-
sess a mature conceptual system (CS), influenced 
by their life experience (including languages they 
know). Our learning algorithms utilize a CS model. 
We opted for being conservative: the model is only 
allowed to contain concepts that are clearly pos-
sessed by the learner before learning starts. Con-
cepts that are particular to the L2 (e.g., ?noun 
gender? for English speakers learning Spanish) are 
not allowed. Examples for concept classes include 
fruits, colors, human-made objects, physical activi-
ties and emotions, as well as meta-linguistic con-
cepts such as pronouns and prepositions. A single 
concept is simply represented by a prototypical 
English word denoting it (e.g., ?child?, ?school?). A 
concept class is represented by the concepts it con-
tains and is conveniently named using an English 
word or phrase (e.g., ?types of people?, ?buildings?, 
?language names?).  
Our learners can explicitly reason about concept 
inter-relationships. Is-a relationships between 
classes are represented when they are beyond any 
doubt (e.g., ?buildings? and ?people? are both 
?physical things?).  
A basic conceptual system is assumed to exist 
before the SLA process starts. When the input is 
controlled and small, as in our case, it is both 
methodologically valid and practical to prepare the 
CS manually. CS design is discussed in detail in 
section 6.  
In the model described in the present  paper we 
do not automatically modify the CS during the 
learning process; CS evolution will be addressed in 
future models.  
As stated in section 1, in this paper we focus on 
linguistic SLA aspects and do not address issues 
such as human errors, motivation and attention. 
We thus assume that our learner possesses perfect 
memory and can invoke our learning algorithms 
without any mistakes.   
47
4.3 Language Knowledge Model 
We require our model to support a basic capability 
of a grammar: enumeration of language sentences 
(parsing will be reported in other papers). In addi-
tion, we provide a degree of certainty for each. The 
model?s quality is evaluated by its applicability for 
learning corpora authoring assistance (section 6).   
The representation is based on construction 
grammar (CG), explicitly storing a set of construc-
tions and their inter-relationships. CG is ideally 
suited for SLA interlanguage because it enables the 
representation of partial knowledge: every lan-
guage form, from concrete words and sentences to 
the most abstract constructs, counts as a construc-
tion. The generative capacity of language is ob-
tained by allowing constructions to replace 
arguments. For example, (child), (the child goes to 
school), (<x> goes to school), (<x> <v> to school) 
and (X goes Z) are all constructions, where <x>, 
<v> denote word classes and X, Z denote other 
constructions.  
SL learners can make explicit judgments as to 
their level of confidence in the grammaticality of 
utterances. To model this, our learning algorithms 
assign a degree of certainty (DOC) to each con-
struction and to the possibility of it being an argu-
ment of another construction. The certainty of a 
sentence is a function (e.g., sum or maximum) of 
the DOCs present in its derivation path. 
Our representation is equivalent to a graph 
whose nodes are constructions and whose directed, 
labeled arcs denote the possibility of a node filling 
a particular argument of another node. When the 
graph is a-cyclic the resulting language contains a 
finite number of concrete sentences, easily com-
puted by graph traversal. This is similar to [Edel-
man04]; we differ in our partial support for 
semantics through a conceptual system (section 5) 
and in the notion of a degree of certainty.   
5 Learning Algorithms 
Our general SLA scheme is that of incremental 
learning ? examples are given one by one, each 
causing an update to the model. A major goal of 
our model is to identify effective, cognitively plau-
sible learning algorithms. In this section we present 
a concrete set of such algorithms. 
Structured categorization is a major driving 
force in perception and other cognitive processes 
[Medin05]. Our learners are thus driven by the de-
sire to form useful generalizations over the input. 
A generalization of two or more examples is possi-
ble when there is sufficient similarity of form and 
meaning between them. Hence, the basic ingredi-
ent of our learning algorithms is identifying such 
similarities. 
To identify concrete effective learning algo-
rithms, we have followed our own inference proc-
esses when learning a foreign language from an 
example-based corpus (section 6). The set of algo-
rithms described below are the result of this study.  
The basic form similarity algorithm is Single 
Word Difference (SWD). When two examples 
share all but a single word, a construction is 
formed in which that word is replaced by an argu-
ment class containing those words. For example, 
given ?eigo ga wakari mas? and ?nihongo ga wakari 
mas?, the construction (<eigo, nihongo> ga wakari 
mas) (?I understand English/Japanese?), containing 
one argument class, is created. In itself, SWD only 
compresses the input, so its degree of certainty is 
maximal. It does not create new sentences, but it 
organizes knowledge in a form suitable for gener-
alization.  
The basic meaning-based similarity algorithm is 
Extension by Conceptual Categories (ECC). For 
an argument class W in a construction C, ECC at-
tempts to find the smallest concept category U? 
that contains W?, the set of concepts corresponding 
to the words in W. If no such U? exists, C is re-
moved from the model. If U? was found, W is re-
placed by U, which contains the L2 words 
corresponding to the concepts in U?. When the re-
placement occurs, it is possible that not all such 
words have already been taught; when a new word 
is taught, we add it to all such classes U (easily 
implemented using the new word?s translation, 
which is given when it is introduced.) 
In the above example, the words in W are ?eigo? 
and ?nihongo?, with corresponding concepts ?Eng-
lish? and ?Japanese?. Both are contained in W?, the 
?language names? category, so in this case U? 
equals W?. The language names category contains 
concepts for many other language names, includ-
ing Korean, so it suffices to teach our learner the 
Japanese word for Korean (?kankokugo?) at some 
point in the future in order to update the construc-
tion to be (<eigo, nihongo, kankokugo> ga wakari 
mas). This creates a new sentence ?kankokugo ga 
wakari mas? meaning ?I understand Korean?. An 
48
example in which U? does not equal W? is given in 
Table 1 by ?child? and ?car?.  
L2 words might be ambiguous ? several con-
cepts might correspond to a single word. Because 
example semantics are not explicitly represented, 
our system has no way of knowing which concept 
is the correct one for a given construction, so it 
considers all possibilities. For example, the Japa-
nese ?ni? means both ?two? and ?at/in?, so when 
attempting to generalize a construction in which 
?ni? appears in an argument class, ECC would con-
sider both the ?numbers? and ?prepositions? con-
cepts.  
The degree of certainty assigned to the new con-
struction by ECC is a function of the quality of the 
match between W and U?. The more abstract is U, 
the lower the certainty. 
The main form-based induction algorithm is 
Shared Prefix, Generated Suffix (SPGS). Given 
an example ?x y? (x, y are word sequences), if there 
exist (1) an example of the form ?x z?, (2) an ex-
ample ?x?, and (3) a construction K that derives ?z? 
or ?y?, we create the construction (x K) having a 
degree of certainty lower than that of K. A Shared 
Suffix version can be defined similarly. Require-
ment (2) ensures that the cut after the prefix will 
not be arbitrary, and assumes that the lesson author 
presents constituents as partial examples before-
hand (as indeed is the case in our corpus).  
SPGS utilizes the learner?s current generative 
capacity. Assume input ?watashi wa biru o nomi 
mas? (?I drink beer?), previous inputs ?watashi wa 
america jin des? (?I am American?), ?watashi wa? 
(?as to me...?) and an existing construction K = 
(<biru, wain> o nomi mas). SPGS would create the 
construction (watashi wa K), yielding the new sen-
tence ?watashi wa wain o nomi mas? (?I drink 
wine?). 
To enable faster learning of more abstract con-
structions, we use generalized versions of SWD 
and SPGS, which allow the differing or shared 
elements to be a construction rather than a word or 
a word sequence.  
The combined learning algorithm is: given a 
new example, iteratively invoke each of the above 
algorithms at the given order until nothing new can 
be learned. Our system is thus a kind of inductive 
programming system (see [Thompson99] for a sys-
tem using inductive logic programming for seman-
tic parsing).  
Note that the above algorithms treat words as 
atomic units, so they can only learn morphological 
rules if boundaries between morphemes are 
marked in the corpus. They are thus more useful 
for languages such as Japanese than, say, for Ro-
mance or Semitic languages. 
Our algorithms have been motivated by general 
cognitive considerations. It is possible to refine 
them even further, e.g. by assigning a higher cer-
tainty when the focus element is a prefix or a suf-
fix, which are more conspicuous cognitively. 
6 Results and Application to Authoring of 
Learning Corpora 
We have experimented with our model using the 
Pimsleur Japanese I (for English speakers) course, 
which comprises 30 half-hour lessons, 1823 differ-
ent examples, and about 350 words. We developed 
a simple set of tools to assist transcription, using an 
arbitrary, consistent Latin script transliteration 
based on how the Japanese phonemes are pre-
sented in the course, which differs at places from 
common transliterations (e.g., we use ?mas?, not 
?masu?). Word boundaries were marked during 
transliteration, as justified in section 4.  
Example sentences from the corpus are ?nani o 
shi mas kaa ? / what are you going to do??, ?wa-
tashi ta chi wa koko ni i mas / we are here?, ?kyo 
wa kaeri masen / today I am not going back?, 
?demo hitori de kaeri mas / but I am going to return 
alone?, etc. Sentences are relatively short and ap-
propriate for a beginner level learner.  
Evaluating the quality of induced language 
models is notoriously difficult. Current FLA prac-
tice favors comparison of predicted parses with 
ones in human annotated corpora. We have fo-
cused on another basic task of a grammar, sentence 
enumeration, with the goal of showing that our 
model is useful for a real application, assistance for 
authoring of learning corpora. 
The algorithm has learned 113 constructions 
from the 1823 examples, generating 525 new sen-
tences. These numbers do not include construc-
tions that are subsumed by more abstract ones 
(generating a superset of their sentences) or those 
involving number words, which would distort the 
count upwards. The number of potential new sen-
tences is much higher: these numbers are based 
only on the 350 words present, organized in a 
rather flat CS. The constructions contain many 
49
placeholders for concepts whose words would be 
taught in the future, which could increase the num-
ber exponentially.  
In terms of precision, 514 of the 525 sentences 
were judged (by humans) to be syntactically cor-
rect (53 of those were problematic semantically). 
Regarding recall, it is very difficult to assess for-
mally. Our subjective impression is that the learned 
constructions do cover most of what a reasonable 
person would learn from the examples, but this is 
not highly informative ? as indicated, the algo-
rithms were discovered by following our own in-
herence processes. In any case, our algorithms 
have been deliberately designed to be conservative 
to ensure precision, which we consider more im-
portant than recall for our model and application. 
There is no available standard benchmark to 
serve as a baseline, so we used a simpler version of 
our own system as a baseline. We modified ECC to 
not remove C in case of failure of concept match 
(see ECC?s definition in section 5). The number of 
constructions generated after seeing 1300 exam-
ples is 3,954 (yielding 35,429 sentences), almost 
all of which are incorrect.  
The applicative scenario we have in mind is the 
following. The corpus author initially specifies the 
desired target vocabulary and the desired syntacti-
cal constructs, by writing examples (the easiest 
interface for humans). Vocabulary is selected ac-
cording to linguistic or subject  (e.g., tourism, 
sports) considerations. The examples are fed one 
by one into the model (see Table 1). For a single 
word example, its corresponding concepts are first 
manually added to the CS. 
The system now lists the constructions learned. 
For a beginner level and the highest degree of cer-
tainty, the sentences licensed by the model can be 
easily grasped just by looking at the constructions. 
The fact that our model?s representations can be 
easily communicated to people is also an advan-
tage from an SLA theory point of view, where ?fo-
cus on form? is a major topic [Gass01]. For 
advanced levels or lower certainties, viewing the 
sentences themselves (or a sample, when their 
number gets too large) might be necessary.  
The author can now check the learned items for 
errors. There are two basic error types, errors 
stemming from model deficiencies and errors that 
human learners would make too. As an example of 
the former, wrong generalizations may result from 
discrepancies between the modeled conceptual sys-
tem and that of a real person. In this case the au-
thor fixes the modeled CS. Discovering errors of 
the second kind is exactly the point where the 
model is useful. To address those, the author usu-
ally introduces new full or partial examples that 
would enable the learner to induce correct syntax. 
In extreme cases there is no other practical choice 
but to provide explicit linguistic explanations in 
order to clarify examples that are very far from the 
learner?s current knowledge. For example, English 
speakers might be confused by the variability of 
the Japanese counting system, so it might be useful 
to insert an explanation of the sort ?X is usually 
used when counting long and thin objects, but be 
aware that there are exceptions?. In the scenario of 
Table 1, the author might eventually notice that the 
learner is not aware that when speaking of some-
body else?s child a more polite reference is in or-
der, which can be fixed by giving examples 
followed by an explanation. The DOC can be used 
to draw the author?s attention to potential prob-
lems.  
Preparation of the CS is a sensitive issue in our 
model, because it is done manually while it is not 
clear at all what kind of CS people have (WordNet 
is sometimes criticized for being arbitrary, too fine, 
and omitting concepts). We were highly conserva-
tive in that only concepts that are clearly part of the 
conceptual system of English speakers before any 
exposure to Japanese were included. Our task is 
made easier by the fact that it is guided by words 
actually appearing in the corpus, whose number is 
not large, so that it took only about one hour to 
produce a reasonable CS. Example categories are 
names (for languages, places and people), places 
(park, station, toilet, hotel, restaurant, shop, etc), 
people (person, friend, wife, husband, girl, boy), 
food, drink, feelings towards something (like, 
need, want), self motion activities (arrive, come, 
return), judgments of size, numbers, etc. We also 
included language-related categories such as pro-
nouns and prepositions. 
7 Discussion 
We have presented a computational model of sec-
ond language acquisition. SLA is a central subject 
in linguistics theory and practice, and our main 
contribution is in addressing it in computational 
linguistics. The model?s learning algorithms are 
unique in their usage of a conceptual system, and 
50
its generative capacity is unique in its support for 
degrees of certainty. The model was tested on a 
unique corpus. 
The dominant trend in CL in the last years has 
been the usage of ever growing corpora. We have 
shown that meaningful learning can be achieved 
from a small corpus when the corpus has been pre-
pared by a ?good teacher?. Automatic identification 
(and ordering) of corpora subsets from which 
learning is effective should be a fruitful research 
direction for CL. 
We have shown that using a simple conceptual 
system can greatly assist language learning algo-
rithms. Previous FLA algorithms have in effect 
computed a CS simultaneously with the syntax; 
decoupling the two stages could be a promising 
direction for FLA.  
The model presented here is the first computa-
tional SLA model and obviously needs to be ex-
tended to address more SLA phenomena. It is clear 
that the powerful notion of certainty is only used in 
a rudimentary manner. Future research should also 
address constraints (e.g. for morphology and agree-
ment), recursion, explicit semantics (e.g. parsing 
into a semantic representation), word segmenta-
tion, statistics (e.g. collocations), and induction of 
new concept categories that result from the learned 
language itself (e.g. the Japanese counting system). 
An especially important SLA issue is L1 trans-
fer, which refers to the effect that the L1 has on the 
learning process. In this paper the only usage of the 
L1 part of the examples was for accessing a con-
ceptual system. Using the L1 sentences (and the 
existing conceptual system) to address transfer is 
an interesting direction for research, in addition to 
using the L1 sentences for modeling sentence se-
mantics.  
Many additional important SLA issues will be 
addressed in future research, including memory, 
errors, attention, noticing, explicit learning, and 
motivation. We also plan additional applications, 
such as automatic lesson generation. 
 
Acknowledgement. We would like to thank Dan 
Melamed for his comments on a related document.  
References  
Brown Ralf, 2000, Automated Generalization of Trans-
lation Examples, COLING ?00. 
Carl Michael, Way Andy, (eds), 2003, Recent Advances 
in Example Based Machine Translation, Kluwer. 
Chang Nancy, Gurevich Olya, 2004. Context-Driven 
Construction Learning. Proceedings, Cognitive Sci-
ence ?04.  
Chapelle Carol, 2001. Computer Applications in SLA. 
Cambridge University Press. .  
Cicekli Ilyas, Gu?venir Altay, 2001, Learning Transla-
tion Templates from Bilingual Translational Exam-
ples. Applied Intelligence 15:57-76, 2001.  
Clark Alexander, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. PhD thesis, University of 
Sussex. 
Clark Eve Vivienne, 2003. First Language Acquisition. 
Cambridge University Press.   
Croft, William, 2001. Radical Construction Grammar. 
Oxford University Press.   
Edelman Shimon, Solan Zach, Horn David, Ruppin 
Eytan, 2004. Bridging Computational, Formal and 
Psycholinguistic Approaches to Language. Proceed-
ings, Cognitive Science ?04.  
Fromkin Victoria, Rodman Robert, Hyams Nina, 2002. 
An Introduction to Language, 7th ed. Harcourt. 
Gass Susan M, Selinker Larry, 2001. Second Language 
Acquisition: an Introductory Course. 2nd ed. LEA 
Publishing.  
Goldberg Adele, 1995. Constructions: a Construction 
Grammar Approach to Argument Structure. Chicago 
University Press. 
Klein Dan, 2005. The Unsupervised Learning of Natural 
Language Structure. PhD Thesis, Stanford.  
Levy Michael, 1997. Computer-Assisted Language 
Learning. Cambridge University Press.  
Maritxalar Montse, Diaz de Ilarraza Arantza, Oronoz 
Maite, 1997. From Psycholinguistic Modelling of In-
terlanguage in SLA to a Computational Model. 
CoNLL ?97.  
Medin Douglas, Ross Brian, Markman Arthur, 2005. 
Cognitive Psychology, 4th ed. John Wiley & Sons.   
Mitchell Rosamond, Myles Florence, 2003. Second 
Language Learning Theories. 2nd ed. Arnold Publica-
tion. 
Pimsleur 2005. www.simonsays.com, under ?foreign 
language instruction?.  
Somers Harold, 2001. Example-based Machine Transla-
tion. Machine Translation 14:113-158. 
Thompson Cynthia, Califf Mary Elaine, Mooney Ray-
mond, 1999. Active Learning for Natural Language 
Parsing and Information Extraction. ICML ?99.  
Tomasello Michael, 2003. Constructing a Language: a 
Usage Based Theory of Language Acquisition. Har-
vard University Press. 
51
Construction  DOC Source  Comment 
1 anata / you 0 example  
2 watashi / I  0 example  
3 anata no / your 0 example  
4 watashi no / my 0 example  
5 (<anata,watashi> no ) 0 SWD(3,4) The first words of 3 and 4 are different, the 
rest is identical. 
6 (W no), where W is <anata, 
watashi, Japanese word for 
?we?> 
-1 ECC(5)  The concept category W?={I, you, we} was 
found in the CS. We know how to say ?I? and 
?you?, but not ?we?.   
7 watashi ta chi / we  0 example  
8 (W no), where W is  
<anata, watashi, watashi ta 
chi> 
-2 ECC(6,7) We were taught how to say ?we?, and an 
empty slot for it was found in 6.  
Now we can generate a new sentence: ?wa-
tashi ta chi no?, whose meaning (?our?) is 
inferred from the meaning of construction 6. 
9 chiisai / small  0 example  
10 kuruma / car 0 example  
11 chiisai kuruma / a small car 0 example  
12 watashi ta chi no kuruma / our 
car 
0 example  
13 ((W no) kuruma) -3 SSGP (12, 
11, 10, 8) 
Shared Suffix Generated Prefix: 
(0) new example 12 = ?y x? (x: kuruma)  
(1) existing example 11 = ?z x?  
(2) existing example 10 = ?x?  
(3) construction K (#8) deriving ?y?  
learns the new construction (K x) 
Now we can generate a new sentence: ?wa-
tashi no kuruma?, meaning ?my car?.  
14 kodomo / child 0 example  
... ... 0 examples Skipping a few examples... 
20 ((W no) kodomo) -3 ... This construction was learned using the 
skipped examples.  
21 ((W no) <kuruma, kodomo>) -3 SWD (13, 
20) 
Note that the shared element is a construction 
this time, not a sub-sentence.  
22 ((W no) P), where P is the set 
of Japanese words for physi-
cal things (animate or inani-
mate)  
-4 ECC (21) The smallest category that contains the con-
cepts ?car? and ?child? is P?=PhysicalThings.  
Now we can generate many new sen-
tences, meaning ?my X? where X is any 
Japanese word we will learn in the future 
denoting a physical thing.  
Table 1: A learning scenario. For simplicity, the degree of certainty here is computed by adding that of the algorithm 
type to that of the most uncertain construction used. Note that the notation used was designed for succinct presen-
tation and is not the optimal one for authors of learning corpora (for example, it is probably easier to visualize the 
sentences generated by construction #22 if it were shown as ((<watashi, anata, watashi ta chi> no) <kuruma, 
kodomo>).) 
52
Induction of Cross-Language Affix and 
Letter Sequence Correspondence 
 
Ari Rappoport 
Institute of Computer Science 
The Hebrew University 
www.cs.huji.ac.il/~arir 
 
Tsahi Levent-Levi 
Institute of Computer Science 
The Hebrew University 
Abstract 
We introduce the problem of explicit 
modeling of form relationships between 
words in different languages, focusing 
here on languages having an alphabetic 
writing system and affixal morphology. 
We present an algorithm that learns the 
cross-language correspondence between 
affixes and letter sequences. The algo-
rithm does not assume prior knowledge 
of affixes in any of the languages, using 
only a simple single letter correspon-
dence as seed. Results are given for the 
English-Spanish language pair. 
1 Introduction 
Studying various relationships between lan-
guages is a central task in computational linguis-
tics, with many application areas. In this paper 
we introduce the problem of induction of form 
relationships between words in different lan-
guages. More specifically, we focus on lan-
guages having an alphabetic writing system and 
affixal morphology, and we construct a model 
for the cross-language correspondence between 
letter sequences and between affixes. Since the 
writing system is alphabetic, letter sequences are 
highly informative regarding sound sequences as 
well.  
Concretely, the model is designed to answer 
the following question: what are the affixes and 
letter sequences in one language that correspond 
frequently to similar entities in another lan-
guage? Such a model has obvious applications to 
the construction of learning materials in language 
education and to statistical machine translation.  
The input to our algorithm consists of word 
pairs from two languages, a sizeable fraction of 
which is assumed to be related graphemically 
and affixally. The algorithm has three main 
stages. First, an alignment between the word 
pairs is computed by an EM algorithm that uses 
an edit distance metric based on an increasingly 
refined individual letter correspondence cost 
function. Second, affix pair candidates are dis-
covered and ranked, based on a language inde-
pendent abstract model of affixal morphology. 
Third, letter sequences that correspond produc-
tively in the two languages are discovered and 
ranked by EM iterations that use a cost function 
based on the discovered affixes and on compati-
bility of alignments.  
The affix learning part of the algorithm is to-
tally unsupervised, in that we do not assume 
knowledge of affixes in any of the single lan-
guages involved. The letter sequence learning 
part utilizes a simple initial correspondence be-
tween individual letters, and the rest of its opera-
tion is unsupervised.  
We believe that this is the first paper that ex-
plicitly addresses cross-language morphology, 
and the first that presents a comprehensive inter-
language word form correspondence model that 
combines morphology and letter sequences.  
Section 2 motivates the problem and defines it 
in detail. In Section 3 we discuss relevant previ-
ous work. The algorithm is presented in Section 
4, and results for English-Spanish in Section 5. 
2 Problem Motivation and Definition 
We would like to discover characteristics of 
word form correspondence between languages. 
In this section we discuss what exactly this 
means and why it is useful.  
17
Word form. Word forms have at least three dif-
ferent aspects: sound, writing system, and inter-
nal structure, corresponding to the linguistics 
fields of phonology, orthography and morphol-
ogy. When the writing system is phonetically 
based, the written form of a word is highly in-
formative of how the word sounds. Individual 
writing units are referred to as graphemes.  
Morphology studies the internal structure of 
words when viewed as comprised of semantics 
carrying components. Morphological units can 
be classified into two general classes, stems (or 
roots) and bound morphemes, which combine to 
create words using various kinds of operators. 
The linear affixing operator combines stems and 
bound morphemes (affixes) using linear ordering 
with possible fusion effects, usually at the seams.  
Word form correspondence. In this paper we 
study cross-language word form correspondence. 
We should first ask why there should be any re-
lationship at all between word forms in different 
languages. There are at least two factors that cre-
ate such relationships. First, languages may share 
a common ancestor. Second, languages may bor-
row words, writing systems and even morpho-
logical operators from each other. Note that us-
age of proper names can be viewed as a kind of 
borrowing. In both cases form relationships are 
accompanied by semantic relatedness. Words 
that possess a degree of similarity of form and 
meaning are usually termed cognates.
Our goal in examining word forms in different 
languages is to identify correspondence phenom-
ena that could be useful for certain applications. 
These would usually be correspondence similari-
ties that are common to many word pairs.  
Problem statement for the present paper. For 
reasons of paper length, we focus here on lan-
guages having the following two characteristics. 
First, we assume an alphabetic writing system. 
This implies that grapheme correspondences will 
be highly informative of sound correspondences 
as well. From now on we will use the term ?let-
ter? instead of ?grapheme?. Second, we assume 
linear affixal morphology (prefixing and suffix-
ing), which is an extremely frequent morpho-
logical operator in many languages.   
We address the two fundamental word form 
entities in languages that obey those assump-
tions: affixes and letter sequences. Our goal is to 
discover frequent cross-language pairs of those 
entities and quantify the correspondence. Pairing 
of letter sequences is expected to be mostly due 
to regular sound transformations and spelling 
conventions. Pairing of affixes could be due to 
morphological principles ? predictable relation-
ships between the affixing operators (their form 
and meaning) ? or, again, due to sound transfor-
mations and spelling.  
The input to the algorithm consists of a set of 
ordered pairs of words, one from each language. 
We do not assume that all input word pairs ex-
hibit the correspondence relationships of interest, 
but obviously the quality of results will depend 
on the fraction of the pair set that does exhibit 
them. A particular word may participate in more 
than a single pair. As explained above, the rela-
tionships of interest to us in this paper usually 
imply semantic affinity between the words; 
hence, a suitable pair set can be generated by 
selecting word pairs that are possible translations 
of each other. Practical ways to obtain such pairs 
are using a bilingual dictionary or a word aligned 
parallel corpus. We had used the former, which 
implies that we addressed only derivational, not 
inflectional, morphology. Using a dictionary pro-
vides a kind of semantic supervision that allows 
us to focus on the desired form relationships.  
We also assume that the algorithm is provided 
with a prototypical individual letter mapping as 
seed. Such a mapping is trivial to obtain in virtu-
ally all practical situations, either because both 
languages utilize the same alphabet or by using a 
manually prepared, coarse alphabet mapping 
(e.g., anybody even shallowly familiar with Cy-
rillic or Semitic scripts can prepare such a map-
ping in just a few minutes.) 
We do not assume knowledge of affixes in any 
of the languages. Our algorithm is thus fully un-
supervised in terms of morphology and very 
weakly seeded in term of orthography.  
Motivating applications. There are two main 
applications that motivate our research. In sec-
ond language education, a major challenge for 
adult learners is the high memory load due to the 
huge number of lexical items in a language. Item 
memorization is known to be greatly assisted by 
tying items with existing knowledge (Matlin02). 
When learning a second language lexicon, it is 
beneficial to consciously note similarities be-
tween new and known words. Discovering and 
explaining such similarities automatically would 
help teachers in preparing reliable study materi-
als, and learners in remembering words.  
Recognition of familiar components also helps 
learners when encountering previously unseen 
words. For example, suppose an English speaker 
who learns Spanish and sees the word ?parcial-
18
mente?. A word form correspondence model 
would tell her that ?mente? is an affix strongly 
corresponding to the English ?ly?, and that the 
letter pair ?ci? often corresponds to the English 
?ti?. The model thus enables guessing or recalling 
the English word ?partially?.  
Our model could also warn the learner of cog-
nates that are possibly false, by recognizing simi-
lar words that are not paired in the dictionary.  
A second application area is machine transla-
tion. Both cognate identification (Kondrak et al
03) and morphological information in one of the 
languages (Niessen00) have been proven useful 
in statistical machine translation. 
3 Previous Work  
Cross-language models for phonology and or-
thography have been developed for back-
transliteration in cross-lingual information re-
trieval (CLIR), mostly from Japanese and Chi-
nese to English. (Knight98) uses a series of 
weighted finite state transducers, each focusing 
on a particular mapping. (Lin02) uses minimal 
edit distance with a ?confusion matrix? that mod-
els phonetic similarity. (Li04, Bilac04) general-
ize using the sequence alignment algorithm pre-
sented in (Brill00) for spelling correction. (Bi-
lac04) explicitly separates the phonemic and gra-
phemic models. None of that work addresses 
morphology and in all of it grapheme and pho-
neme correspondence is only a transient tool 
which is not studied on its own.  (Mueller05) 
explicitly models phonological similarities be-
tween related languages, but does not address 
morphology and orthography.  
Cognate identification has been studied in 
computational historical linguistics. (Coving-
ton96, Kondrak03a) use a fixed, manually de-
termined single entity mapping. (Kondrak03b) 
generalizes to letter sequences based on the algo-
rithm in (Melamed97). The results are good for 
the historical linguistics application. However, 
morphology is not addressed, and the sequence 
correspondence model is less powerful than that 
employed in the back-transliteration and spelling 
correction literature. In addition, all effects that 
occur at word endings, including suffixes, are 
completely ignored. (Mackay05) presents good 
results for cognate identification using a word 
similarity measure based on pair hidden Markov 
models. Again, morphology was not modeled 
explicitly.    
A nice application for cross-language mor-
phology is (Schulz04), which acquires a Spanish 
medical lexicon from a Portuguese seed lexicon 
using a manually prepared table of 842 Spanish 
affixes.  
Unsupervised learning of affixal morphology 
in a single language is a heavily researched prob-
lem. (Medina00) studies several methods, includ-
ing the squares method we use in Section 4. 
(Goldsmith01) presents an impressive system 
that searches for ?signatures?, which can be 
viewed as generalized squares. (Creutz04) pre-
sents a very general method that excels at dealing 
with highly inflected languages. (Wicen-
towsky04) deals with inflectional and irregular 
morphology by using semantic similarity be-
tween stem and stem+affix, also addressing 
stem-affix fusion effects. None of these papers 
deals with cross-language morphology. 
4 The Algorithm 
Overview. Letter sequences and affixes are dif-
ferent entities exhibiting different correspon-
dence phenomena, hence are addressed at sepa-
rate stages. The result of addressing one will as-
sist us in addressing the other.  
The fundamental tool that we use to discover 
correspondence effects is alignment of the two 
words in a pair. Stage 1 of the algorithm creates 
an alignment using the given coarse individual 
letter mapping, which is simultaneously im-
proved to a much more accurate one.  
Stage 2 discovers affix pairs using a general 
language independent affixal morphology model.  
In stage 3 we utilize the improved individual 
letter relation from stage 1 and the affix pairs 
discovered in stage 2 to create a general letter 
sequence mapping, again using word alignments. 
In the following we describe in detail each of 
these stages. 
Initial alignment. The main goal of stage 1 is to 
align the letters of each word pair. This is done 
by a standard minimal edit distance algorithm, 
efficiently implemented using dynamic pro-
gramming (Gusfield97, Ristad98). We use the 
standard edit distance operations of replace, in-
sert and delete. The letter mapping given as input 
defines a cost matrix where replacement of cor-
responding letters has a low (0) cost and of all 
others a high (1) cost. The cost of insert and de-
lete is arbitrarily set to be the same as that of re-
placing non-identical letters. We use a hash table 
rather than a matrix, to prepare for later stages of 
the algorithm.   
When the correspondence between the lan-
guages is very high, this initial alignment can 
19
already provide acceptable results for the next 
stage. However, in order to increase the accuracy 
of the alignment we now refine the letter cost 
matrix by employing an EM algorithm that itera-
tively updates the cost matrix using the current 
alignment and computes an improved alignment 
based on the updated cost matrix (Brill00, Lin02, 
Li04, Bilac04). The cost of mapping a letter K to 
a letter L is updated to be proportional to the 
count of this mapping in all of the current align-
ments divided by the total number of mappings 
of the letter K.  
Affix pairs. The computed letter alignment as-
sists us in addressing affixes. Recall that we pos-
sess no knowledge of affixes; hence, we need to 
discover not only pairing of affixes, but the par-
ticipating affixes as well. Our algorithm discov-
ers affixes and their pairing simultaneously. It is 
inspired by the squares algorithm for affix learn-
ing in a single language (Medina00)1.
The squares method assumes that affixes gen-
erally combine with very many stems, and that 
stems are generally utilized more than once. 
These assumptions are due to a functional view 
of affixal morphology as a process whose goal is 
to create a large number of word forms using 
fewer parameters. A stem that combines with an 
affix is quite likely to also appear alone, so the 
empty affix is allowed. 
We first review the method as it is used in a 
single language. Given a word W=AB (where A 
and B are non-empty letter sequences), our task 
is to measure how likely it is for B to be a suffix 
(prefix learning is similar.) We refer to AB as a 
segmentation of W, using a hyphen to show 
segmentations of concrete words. Define a 
square to be four words (including W) of the 
forms W=AB, U=AD, V=CB, and Y=CD (one of 
the letter sequences C, D is allowed to be 
empty.)  
Such a square might attest that B, D are suf-
fixes and that A, C are stems. However, we must 
be careful: it might also attest that B, D are stems 
and A, C are prefixes. A square attests for a seg-
mentation, not for a particular labeling of its 
components.  
As an example, if W is ?talking?, a possible 
square is {talk-ing, hold-ing, talk-s, hold-s} 
where A=talk, B=ing, C=hold, and D=s. Another 
possible square is {talk-ing, danc-ing, talk-ed, 
danc-ed}, where A=talk, B=ing, C=danc, and 
D=ed. This demonstrates a drawback of the 
 
1 (Medina00) attributes the algorithm to Joseph Greenberg. 
method, namely its sensitivity to spelling; C with 
the empty suffix is written ?dance?, not ?danc?. 
The four words {talking, dancing, talk, dance} 
do not form a square. 
We now count the number of squares in which 
B appears. If this number is relatively large 
(which needs to be precisely defined), we have a 
strong evidence that B is a suffix or a stem. We 
can distinguish between these two cases using 
the number of witnesses ? actual words in which 
B appears.   
We generalize the squares method to the dis-
covery of cross-language affix pairs, as follows. 
We now use W to denote not a single word but a 
word pair W1:W2. B does not denote a suffix 
candidate but a suffix pair candidate, B1:B2, and 
similarly for D. A and C denote stem pair candi-
dates A1:A2 and C1:C2, respectively.  
We now define a key concept. Given a word 
pair W=W1:W2 aligned under an alignment T, 
two segmentations W1=A1B1 and W2=A2B2 
are said to be compatible if no alignment line of 
T connects a subset of A1 to a subset of B2 or a 
subset of A2 to a subset of B1. This definition is 
also applicable to alignments between letter se-
quences. 
We now impose our key requirement: for all 
of the words involved in the cross-lingual square, 
their segmentations into two parts must be com-
patible under the alignment computed at stage 1. 
For example, consider the English-Spanish 
word pair affirmation : afirmacion. The segmen-
tation affirma-tion : afirma-cion is attested by the 
square 
 A1B1 : A2B2 = affirma-tion : afirma-cion 
 A1D1 : A2D2 = affirma-tively :  
afirma-tivamente 
 C1B1 : C2B2 = coopera-tion : coopera-cion 
 C1D1 : C2D2 = coopera-tively :  
coopera-tivamente 
assuming that the appropriate parts are aligned. 
Note that ?tively? is comprised of two smaller 
affixes, but the squares method legitimately con-
siders it an affix by itself. Note also that since all 
of A1, A2, C1 and C2 end with the same letter, 
that letter can be moved to the beginning of B1, 
B2, D1, D2 to produce a different square (affirm-
ation : afirm-acion, etc.) from the same four 
word pairs.  
 Since we have no initial reason to favor a par-
ticular affix candidate over another, and since the 
total computational cost is not prohibitive, we 
20
now simply count the number of attesting 
squares for all possible compatible segmenta-
tions of all word pairs, and sort the list according 
to the number of witnesses. To reduce noise, we 
remove affix candidates for which the absolute 
number of witnesses or squares is small (e.g.,  
ten.) 
Letter sequences. The third and last stage of the 
algorithm discovers letter sequences that corre-
spond frequently. This is again done by an edit 
distance algorithm, generalizing that of stage 1 
so that sequences longer than a single letter can 
be replaced, inserted or deleted. In order to re-
duce noise, prior to that we remove word pairs 
whose stems are very different. Those are identi-
fied by comparing their edit distance costs, 
which should hence be normalized according to 
length (of the longer stem in a pair.) Note that 
accuracy is increased by considering only stems: 
affix pairs might be very different, thus might 
increase edit distance cost even when the stems 
do exhibit good sequence pairing effects.  
When generalizing the edit distance algorithm, 
we need to specify which letter sequences will be 
considered, because it does not make sense to 
consider all possible mappings of all subsets to 
all possible subsets ? the number of different 
such pairs will be too large to show any mean-
ingful statistics.  
The letter sequences considered were obtained 
by ?fattening? the lines in alignments yielding 
minimal edit distances, using an EM algorithm as 
done in (Brill00, Bilac04, Li04). The details of 
the algorithm can be found in these papers. The 
most important step, line fattening, is done as 
follows. We examine all alignment lines, each 
connecting two letter sequences (initially, of 
length 1.) We unite those sequences with adja-
cent sequences in all ways that are compatible 
with the alignment, and add the new sequences 
to the cost function to be used in the next EM 
iteration.  
If we kept letter sequence pairs that are not 
frequent in the cost function, they would distort 
the counts of more frequent letter sequences with 
which they partially overlap. We thus need to 
retain only some of the sequence pairs discov-
ered. We have experimented with several ways 
to do that, all yielding quite similar results. For 
the results presented in this paper, we used the 
idea that sequences that clearly map to specific 
sequences are more important to our model than 
sequences that ?fuzzily? map to many sequences. 
To quantify this approach, for each language-1 
sequence we sorted the corresponding language-
2 sequences according to count, and removed 
pairs in which the language-2 item was responsi-
ble for only a small percentage of the total (we 
used a threshold of 0.05). We further removed 
sequence pairs whose absolute counts are low.  
Discussion. We deal with affixes before se-
quences because, as we have seen, identification 
of affixes helps us in identifying sequences, 
while the opposite order actually hurts us: se-
quences sometimes contain letters from both 
stem and affix, which invalidates squares that are 
otherwise valid.  
It may be asked why the squares stage is 
needed at all ? perhaps affixes would be discov-
ered anyway as sequences in stage 3. Our as-
sumption was that affixes are best discovered 
using properties resulting from their very nature. 
We have experimented with the option of remov-
ing stage 2 and discovering affixes as letter se-
quences in stage 3, and verified that it gives 
markedly lower quality results. Even the very 
frequent pair -ly:-mente was not signaled out, 
because its count was lowered by those of the 
pairs -ly:-ente, -ly:nte, -y:-te, etc. 
5 Results 
We have run the algorithm on several language 
pairs using affixal morphology and the Latin al-
phabet: English vs. Spanish, Portuguese and Ital-
ian, and Spanish vs. Portuguese. All of them are 
related both historically and through borrowing 
(obviously at varying degrees), so we expect 
relatively many correspondence phenomena. 
Testing results for one of these pairs, English ? 
Spanish, are presented in this section.   
The input word pair set was created from a bi-
lingual dictionary (Freelang04) by taking all 
translations of single English words to single 
Spanish words, generating about 13,000 word 
pairs.   
Individual letter mapping. The cost matrix af-
ter EM convergence (25 iterations) exhibits the 
following phenomena (e:s (c) denotes that the 
final cost of replacing the English letter e by the 
Spanish letter s is c): (1) English letters mostly 
map to identical Spanish letters, apart from let-
ters that Spanish does not make use of like k and 
w; (2) some English vowels map frequently to 
some Spanish vowels: y maps almost exclusively 
to i (0.01), e:a (0.47) is highly productive, e:o 
(0.98), i:e (0.97), e:o (0.98); (3) some English 
consonants map to different Spanish ones: t:c 
21
(0.89) (due to an affix, -tion:-cion); m:n (0.44) is 
highly frequent; b:v(0.80); x:j (0.78), x:s(0.94); 
w always maps to v; j:y (0.11); (4) h usually dis-
appears, h:NULL (0.13); and (5) inserted Span-
ish letters include the vowels o, e, a and i, at that 
order, where o overwhelms the others. The Eng-
lish o maps exclusively to the Spanish o and not 
to other vowels. 
Affixes. Table 1 shows some of the conspicuous 
affix pairs discovered by the algorithm. We show 
both the number of witnesses and of squares. 
The table shows many interesting correspon-
dence phenomena. However, discussing those at 
depth from a linguistic point of view is out of the 
scope of this paper. Some notes: (1) some of the 
most frequent affix pairs are not that close ortho-
graphically: -ity:-idad, -ness:- (nouns), -ate:-ar 
(verbs), -ly-:-mente (adverbs), -al:-o (adjectives), 
so will not necessarily be found using ordinary 
edit distance methods; (2) some affixes are 
ranked high both with and without a letter that 
they favor when attaching to a stem: -ation:-
acion, -ate:-ar; (3) some English suffixes map 
strongly to several Spanish ones: -er:-o, -er:-
ador. 
Recall that the table cannot include inflec-
tional affixes, since our input was taken from a 
bilingual dictionary, not from a text corpus.  
Letter sequences. Table 2 shows some nice pair-
ings, stemming from all three expected phenom-
ena: st-:est- (due to phonology), ph:f, th:t, ll:l 
(due to orthography), and tion:cion, tia:cia (due 
to morphology: affixes located in the middle of 
words.) 
Such affix and letter sequence pairing results 
can clearly be useful for English speakers learn-
ing Spanish (and vice versa), for remembering 
words by associating them to known ones, for 
avoidance of spelling mistakes, and for analyzing 
previously unseen words.   
Evaluation. An unsupervised learning model can 
be evaluated on the strength of the phenomena 
that it discovers, on its predictive power for un-
seen data, or by comparing its data analysis re-
sults with results obtained using other means. We 
have performed all three evaluations.  
For evaluating the discovered phenomena, a 
repository of known phenomena is needed. The 
only such repository of which we are aware are 
language learning texts. Unfortunately, the phe-
nomena these present are limited to the few most 
conspicuous pairs (e.g., -ly:-mente, -ity:-idad, 
ph:f), all of which are easily discovered by our 
model. The next best thing are studies that pre-
sent data of a single language. We took the affix 
information given in a recent, highly detailed, 
corpus based English grammar (Biber99), and 
compared it manually to ours. Of the 35 most 
productive affixes, our model finds 27. Careful 
study of the word pair list showed that the re-
maining 8 (-ment, -ship, -age, -ful, -less, -en, dis-
, mis-) indeed do not map to Spanish ones fre-
quently. Note that some of those are indeed ex-
tremely frequent inside English yet do not corre-
spond significantly with any Spanish affix. 
As a second test, we took a comprehensive 
English-Spanish dictionary (Collins), selected 10 
pages at random (out of 680), studied them, and 
listed the prominent word form phenomena (85). 
All but one (the verbal suffix in seduce:seducir) 
were found by our model.  
The numbers reported above for the two tests 
are recall numbers. To evaluate affix precision, 
we have manually graded the top 100 affix pairs 
(as sorted at the end of stage 2 of the algorithm.) 
8 of those were clearly not affixes; however, 3 of 
the 8 (-t:-te, ?t:-to, -ve:-vo) were important pho-
nological phenomena that should indeed appear 
in our final model. Of the remaining 92, 15 were 
valid but ?duplicates? in the sense of being sub-
strings of other affixes (e.g., -ly:-mente, -ly:-
emente.) In the next 50 pairs, only 6 were clearly 
not affixes. Note that by their very definition, we 
should not expect the number of frequent deriva-
tional affixes to be very large, so there is not 
much point in looking further down the list. 
Nonetheless, inspection of the rest of the list re-
veals that it is not dominated by noise but by du-
plicates, with many specialized, less frequent 
affixes (e.g., -graphy:-grafia) being discovered.  
Regarding letter sequences, precision was very 
high: of the 38 different pairs discovered, only 
one (hr:r) was not regular, and there were 11 du-
plicates. Recall was impressive, but harder to 
verify due to the lack of standards. We found 
only one (not very frequent) pair that was not 
discovered (-sp:-esp).  
To evaluate the model on its data analysis ca-
pability, we took out 100 word pairs at random, 
trained the model without them, analyzed them 
using the final cost function, and compared with 
prominent phenomena noted manually (again, we 
had to grade manually due to the lack of a gold 
standard.) The model identified those prominent 
phenomena (including a total lack thereof) in 91 
of the pairs. Notable failures included the pairs 
superscribe : sobrescribir and coded : codificado, 
where none of the prefixes and suffixes were 
22
identified. Some successful examples are listed 
below (affixes are denoted by [], sequences by 
<>, and insert by _: or :_):  
installation : instalacion. <ll:l>, [ation:acion] 
volution : circonvolucion.  _:c, _:i, _:r, _:c, _:o, _:n,  
[tion:cion] 
intelligibility : inteligibilidad. [in:in], <ll:l>, 
[ity:idad] 
sapper : zapador. <s:z>, <pp:p>, [er:ador] 
harpist : arpista. <h:_>, [ist:ista] 
pathologist : patologo.  <th:t>, [ist:o] 
elongate : prolongar.  [te:r] 
industrialize: industrializar. [in:in], <ial>, [e:ar] 
demographic : demografico. <ph:f>, [ic:ico] 
gynecological :ginecologico. <yn:in>, [ical:ico] 
peeled : pelado. [ed:ado] 
The third and final evaluation method is to 
compare the model?s results with results obtained 
using other means. We are not aware of any data 
bank in which cross-language affix or letter se-
quence correspondences are explicitly tagged, so 
we had used a relatively simple algorithm as a 
baseline: We invoked the squares method for 
each language independently, ending up with 
affix candidates. For every word pair E:S, if E 
contains an affix candidate C and S contains an 
affix candidate D, we increment the count of the 
candidate affix pair C:D. Finally, we sort the 
candidates according to their count.  
Baseline recall is obviously as good as in our 
algorithm (it produces a superset), but precision 
is so bad so as to render the baseline method use-
less: out of the first 100, only 19 were affixes, 
the rest being made up of noise and badly seg-
mented ?duplicates?.  
In summary, the results are good, but gold 
standards are needed for a more consistent 
evaluation of different cross-language word form 
algorithms. Results for the other language pairs 
were overall good as well. 
6 Discussion 
We have introduced the problem of cross-
language modeling of word forms, presented an 
algorithm for addressing affixal morphology and 
letter sequences, and described good results on 
English-Spanish dictionary word pairs.  
Natural directions for future work on the 
model include: (1) test the algorithm on more 
language pairs, including languages utilizing 
non-Latin alphabets; (2) modify the input model 
to assume that single language affixes are 
known; (3) address additional morphological 
operators, such as templatic morphology; (4) ad-
dress phonology directly instead of indirectly; (5) 
use pairs acquired from a parallel corpus rather 
than a dictionary, to address inflectional mor-
phology and to see how the algorithm performs 
with more noisy data; (6) extend the algorithm to 
other types of writing systems; (7) examine more 
sophisticated affix discovery algorithms, such as 
(Goldsmith01); and (8) improve the evaluation 
methodology.  
There are many possible applications of the 
model: (1) for statistical machine translation; (2) 
for computational historical linguistics; (3) for 
CLIR back-transliteration; (4) for constructing 
learning materials and word memorization meth-
ods in second language education; and (5) for 
improving word form learning algorithms inside 
a single language.  
The length and diversity of the lists above pro-
vide an indication of the benefit and importance 
of cross-language word form modeling in com-
putational linguistics and its application areas.  
References  
Biber Douglas, 1999. Longman Grammar of Spoken 
and Written English. (Pages 320, 399, 530, 539. )  
Bilac Slaven, Tanaka Hozumi, 2004. A Hybrid Back-
Transliteration System for Japanese. COLING 
2004.  
Brill Eric, Moore Robert, 2000. An Improved Error 
Model for Noisy Channel Spelling Correction. 
ACL 2000. 
Covington Michael A, 1996. An Algorithm to Align 
Words for Historical Comparison. Comput. Ling., 
22(4):481?496.  
Creutz Mathias, Lugas Krista, 2004. Induction of a 
Simple Morphology for Highly-Inflecting Lan-
guages. ACL 2004 Workshop on Comput. Phonol-
ogy and Morphology. 
Freelang, 2004.  http: // www.freelang.net / dictionary 
/ spanish.html.  
Goldsmith John. 2001. Unsupervised Learning of the 
Morphology of a Natural Language. Comput. Ling. 
153-189 (also see an unpublished 2004 document 
at 
http://humanities.uchicago.edu/faculty/goldsmith.)  
Gusfield, Dan, 1997. Algorithms on Strings, Trees, 
and Sequences. Cambridge University Press.  
Knight Kevin, Graehl Jonathan, 1998. Machine 
Transliteration. Comput. Ling. 24(4):599?612. 
Kondrak Grzegorz, 2003a. Phonetic Alignment and 
Similarity. Comput. & the Humanities 37:273?
291.  
Kondrak Grzegorz, 2003b. Identifying Complex 
Sound Correspondences in Bilingual Wordlists.. 
Comput. Ling. & Intelligent Text Processing (CI-
CLing 2003). 
Kondrak Grzegorz, Marcu Daniel, Knight Kevin, 
2003. Cognates Can Improve Statistical Transla-
23
tion Models.  Human Language Technology (HLT) 
2003.  
Li Haizhou et al 2004. A Joint Source-Channel 
Model for Machine Transliteration. ACL 2004.  
Lin Wei-Hao, Chen Hsin-Hsi, 2002. Backward Ma-
chine Transliteration by Learning Phonetic Similar-
ity. CoNLL 2002. 
Mackay Wesley, Kondrak Grzegorz, 2005. Comput-
ing Word Similarity and Identifying Cognates with 
Pair Hidden Markov Models. CoNLL 2005.   
Matlin Margaret W., 2002. Cognition, 6th ed. John 
Wiley & Sons.  
Medina Urrea Alfonso, 2000. Automatic Discovery of 
Affixes by Means of a Corpus: A Catalog of Span-
ish Affixes. J. of Quantitative Linguistics 7(3):97 ? 
114.  
Melamed Dan, 1997. Automatic Discovery of Non-
Compositional Compounds in Parallel Data. 
EMNLP 1997. 
Mueller Karin, 2005. Revealing Phonological Simi-
larities between Related Languages from Auto-
matically Generated Parallel Corpora. ACL ?05  
Workshop on Building and Using Parallel Texts.  
Niessen Sonja, Ney Hermann, 2000. Improving SMT 
Quality with Morph-syntactic analysis. COLING 
2000. 
Ristad Eric Sven, Yianilos Peter, 1998. Learning 
String Edit Distance. IEEE PAMI, 20(5):522?
532.    
Schulz Stefan, et al2004. Cognate Mapping. COL-
ING 2004.  
Wicentowsky Richard, 2004. Multilingual Noise-
Robust Supervised Morphological Analysis using 
the WordFrame Model. ACL 2004 Workshop on 
Comput. Phonology and Morphology. 
 
Eng. Span. Wit. Squ. Example 
-tion - 623 309 reformation:reforma 
-e -ar 461 1182 convene:convocar 
-tion -cion 434 3770 vibration:vibracion 
co- co- 363 95 coexistence:coexistencia 
-ness - 352 128 persuasiveness:persuasiva 
-ation -acion 333 4854 formulation:formulacion 
in- In- 332 1294 inapt:inepto 
re- Re- 312 194 recreative:recreativo 
-ed -ado 289 102 abridged:abreviado 
-ic -ico 274 3192 strategic:estrategico 
-ly -mente 269 207 aggressively:agresivamente
-y -ia 251 2086 agronomy:agronomia 
-ble -ble 238 153 incredible:increible 
-al -al 233 440 genital:genital 
-ity -idad 208 687 stability:estabilidad 
-te -r 206 3603 tabulate:tabular 
-er -o 203 166 biographer:biografo 
-al -o 186 2728 practical:practico 
de- de- 174 68 deformation:deformacion 
-ate -ar 170 3593 manipulate:manipular 
-ous -o 154 59 analogous:analogo 
con- con- 153 53 conceivable:concebible 
-ism -ismo 147 2173 tourism:turismo 
un- In- 134 164 undistinguishable:indistinto
-er -ador 134 95 progammer:programador 
-nt -nte 120 514 tolerant:tolerante 
-ical -ico 111 3185 lyrical:lirico 
-ist -ista 111 1691 tourist:turista 
-ize -izar 90 974 privatize:privatizar 
-ce -cia 87 445 belligerence:beligerancia 
-tive -tivo 70 249 superlative:superlativo 
Table 1: Some affix pairs discovered. 
Eng. Span. Example 
ph f aphoristic:aforistico 
th t lithography:litografia 
ll l collaboration:colaboracion 
tion cion unconditional:incodicional 
st- est- stylist:estilista 
tia cia unnegotiable:innegociable 
Table 2: Some letter sequence pairs discovered. 
 
24
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9?16,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
Using Classifier Features for Studying the Effect of Native Language on the
Choice of Written Second Language Words
Oren Tsur
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
oren@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
www.cs.huji.ac.il/?arir
Abstract
We apply machine learning techniques to
study language transfer, a major topic in
the theory of Second Language Acquisition
(SLA). Using an SVM for the problem of
native language classification, we show that
a careful analysis of the effects of various
features can lead to scientific insights. In
particular, we demonstrate that character bi-
grams alone allow classification levels of
about 66% for a 5-class task, even when con-
tent and function word differences are ac-
counted for. This may show that native lan-
guage has a strong effect on the word choice
of people writing in a second language.
1 Introduction
While advances in NLP achieve improved results for
NLP applications such as machine translation, ques-
tion answering and document summarization, there
are other fields of research that can benefit from the
methods used by the NLP community. Second Lan-
guage Acquisition (SLA), a major area in Applied
Linguistics and Cognitive Science, is one such field.
In this paper we demonstrate how modern machine
learning tools can contribute to SLA theory. In par-
ticular, we address the major SLA topic of language
transfer, the effect of native language on second lan-
guage learners. Using an SVM for the computa-
tional problem of native language classification, we
study in detail the effects of various SVM features.
Surprisingly, character bi-grams alone lead to a clas-
sification accuracy of about 66% in a 5-class task,
even when accounting for differences in content and
function words.
This result leads us to form a novel hypothesis on
the role of language transfer in SLA: that the choice
of words people make when writing in a second lan-
guage is strongly influenced by the phonology of
their native language.
As far as we know, this is the first time that such
a hypothesis has beed formulated. Moreover, this is
the first statistical learning-supported hypothesis in
language transfer. Our results should be further sub-
stantiated by additional psycholinguistic and com-
putational experiments; nonetheless, we provide a
strong starting point.
The next section provides some essential back-
ground. In Section 3 we describe our experimen-
tal setup and feature selection, and in Section 4 we
detail an array of variations of experiments for rul-
ing out some possible types of bias that might have
affected the results. In Section 5 we discuss our hy-
pothesis in the context of psycho-linguistic theory.
We conclude with directions for future research.
2 Background
Our hypothesis is tested within an algorithm ad-
dressing the practical problem of determining the
native language of an anonymous writer writing in a
foreign language. The problem is applicable to dif-
ferent fields, such as language instructing, tailored
error correction, security applications and psycho-
linguistic research.
As background, we start from the somewhat re-
lated problem of authorship attribution. The au-
thorship attribution problem was addressed by lin-
9
guists and other literary experts trying to pinpoint
an anonymous author, such as that of The Federalist
Papers (Holmes and Forsyth, 1995). Traditionally,
authorship experts analyzed topics, stylistic idiosyn-
crasies and personal information about the possible
candidates in order to determine an author.
While authorship is usually addressed with deep
human inspection of the texts in question, it has al-
ready been shown that automatic text analysis based
on various stylistic features can identify the gender
of an anonymous author with accuracy above 80%
(Argamon et al 2003). Various papers (Diedrich et
al, 2003; Koppel and Schler, 2003; Koppel et al
2005a; Stamatatos et al 2004) report relative suc-
cess in machine based authorship attribution tasks
for small sets of known candidates.
Native language detection is a harder problem
than the authorship attribution problem, since we
wish to characterize the writing style of a set of
writers rather than the unique style of a single
person. There are several works presenting non-
native speech recognition and dialect analysis sys-
tems (Bouselmi et al 2005; Bouselmi et al 2006;
Hansen et al 2004). However, all those works are
based on acoustic signals, not on written texts.
Koppel et al(2005a) report an accuracy of 80% in
the task of determining a writer?s native language.
To the best of our knowledge, this is the only pub-
lished work on automated classification of an au-
thor?s native language (along with another version
of the paper by the same authors (Koppel et al
2005b)). Koppel et alused an SVM (Scho?lkopf and
Smola, 2002) and a combination of features in their
system (such as errors analysis and POS-error co-
occurrences, as described in section 2.2), but sur-
prisingly, it appears that a very naive set of features
achieves a relatively high accuracy. The charac-
ter bi-gram frequencies feature performs rather well,
and definitely outperforms the intuitive contribution
of frequent bigrams in this type of task.
3 Experimental Setting
3.1 The Corpus
The corpus that served for all of the experiments
described in this paper is the International Corpus
of Learner English (ICLE) (Granger et al 2002),
which was also the one used by Koppel et al(2005a;
2005b). The corpus was compiled for the purpose of
studying the English writing of non-native speakers.
All contributors to the corpus are advanced English
students and are roughly the same age. The corpus is
combined from a number of sub-corpora, each con-
taining one native language. The corpus was assem-
bled in ten years of international collaboration be-
tween a number of universities and it contains more
than 2 million words of writing by students from 19
different native language backgrounds. We followed
Koppel et al(2005a) and worked on 5 sub-corpora,
each containing 238 randomly selected essays by na-
tive speakers of the following languages: Bulgarian,
Czech, French, Russian and Spanish. Each of the
texts in the corpus was written by a different author
and is of length between 500 to 1,000 words. Each
of the sub corpora contains about 180,000 (unique)
types, for a total of 886,677 tokens.
Essays in the corpus are of two types: argumen-
tative essays and literature examination papers. De-
scriptive, narrative or technical subjects were not in-
cluded in the corpus. The literature examination es-
says were restricted to no more than 25% of each
sub-corpus. Each contributor was requested to fill a
learner profile that was used to fine-proof the corpus
as needed.
In order to verify our results we used another con-
trol corpus containing the Dutch and Italian sub-
corpora contained in the ICLE instead of the Bul-
garian and French ones.
3.2 Document Representation
In the original experiment by Koppel et al(2005a)
each document was represented by a numerical vec-
tor of 1,035 dimensions. Each vector entry rep-
resented the frequency (relative to the document?s
length) of a given feature. The features were of 4
types:
? 400 function words
? 200 most frequent letter n-grams
? 250 rare POS bi-gram
? 185 error types
While the first three types of attributes are relatively
straightforward, the fourth is more complex. It rep-
resents clusters of families of spelling errors as well
as co-occurrences of errors and POS tags. Document
10
representation is described in detail in (Koppel et al
2005a; Koppel et al 2005b).
A multi-class SVM (Witten and Frank, 2005) was
employed for learning and evaluating the classifica-
tion model. The experiment was run in a 10-fold
cross validation manner in order to test the effec-
tiveness of the model.
3.3 Previous Results
Koppel et al(2005a) report that when all features
types were used in tandem, an accuracy of 80.2%
was achieved. In the discussion section they an-
alyze the frequency of a few function words, er-
ror types, the co-occurrences of POS tags and er-
rors, and the co-occurrences of POS tags and certain
function words that seem to have significance in the
support vectors learnt by the SVM.
The goal of their research was to obtain the best
classification, therefore the results obtained by us-
ing only bi-grams of characters were not particularly
noted, although, surprisingly, representing each doc-
ument by only using the relative frequency of the
top 200 characters bi-grams achieves an accuracy of
about 66%. We believe that this surprising fact ex-
poses some fundamental phenomenon of human lan-
guage behavior. In the next section we describe a set
of experiments designed to isolate the causes of this
phenomenon.
4 Experimental Variations and Results
Intuitively, we do not expect the most frequent char-
acter n-grams to serve as good native language pre-
dictors, expecting that these will only reflect the
most frequent English words (and characters se-
quences). Accordingly, without language transfer
effects, a naive baseline classifier based on an n-
gram model is expected to achieve about 20% ac-
curacy in a 5 native languages classification task.
However, using classification based on the relative
frequency of top 200 bi-grams achieves about 66%1
in all experiments, substantially higher than the ran-
dom baseline. These results are so surprising that
they suggest that the characters bi-grams classifi-
cation masks some other bias or noise in the cor-
pus, or, conversely, that it mirrors other simple-to-
1Koppel et aldid not report these results explicitly. How-
ever, they can be roughly estimated from their graph.
Figure 1: Classification accuracy of the different
variations of document representation. b-g: bi-
grams, f-w: function words, c-w: content words.
explain phenomena such as shallow language trans-
fer through the use of function words, or content
bias. The following sub-sections describe different
variations of the experiment, ruling out the effect of
these different types of bias.
4.1 Unigram Baseline
We first implemented a naive baseline classifier. We
represented each document by the normalized fre-
quencies of the (de-capitalized) letters it contains2.
These frequencies are simply a unigram model of
the sub-corpora. Using the multi-class SVM (Wit-
ten and Frank, 2005) we obtained 46.78% accu-
racy. This accuracy is more than twice the ran-
dom baseline accuracy. This result is in accordance
with our bi-grams results. Our discussion focuses on
bi-grams rather than unigrams because the former?s
results are much higher and because bi-grams are
much closer to the phonology of the language (for
alphabetic scripts, of course).
4.2 Bi-grams Based Classification
Choosing the 200 most frequent character bi-grams
in the corpus, we used a vector of the same dimen-
sion. Each vector entry contained the normalized
frequency of one of the bi-grams. Using a multi-
class SVM in a 10-fold cross validation manner we
2White spaces were considered a letter. However, sequences
of white spaces and tabs were collapsed to a single white space.
All the experiments that make use of character frequencies were
performed twice, including and excluding punctuation marks.
Results for both experiments are similar, therefore all the num-
bers reported in this paper are based on letters and punctuation
marks.
11
Bulg. Czech French Russian Spanish
dr 170 183 n/a 195 n/a
am 117 135 142 140 152
m 121 120 133 119 139
iv 104 138 144 148 148
y 161 181 196 183 166
la 122 123 122 142 105
Table 1: Some of the separating bi-grams found in
the feature selection process. ? ? indicates a white
space. The numbers are the frequency ranking of
the bi-grams in each sub-corpus (e.g., there are 103
bi-grams more frequent than ?iv? in the Bulgarian
corpus). n/a indicates that this bi-gram is not one of
the 200 most frequent bi-grams of the sub-corpus.
achieved 65.60% accuracy with standard deviation
of 3.99.
The bi-grams features in the 200 dimensional vec-
tor are the 200 most frequent bi-grams in the whole
corpus, regardless of their frequency in each sub-
corpus. We note that the effect of misspelled words
on the 200 most frequent bi-grams is negligible.
A more sophisticated feature selection could re-
duce the dimension of the representation vector
without detracting from the results. Careful fea-
ture selection can also give a better intuition regard-
ing the support vectors. We performed feature se-
lection in the following manner: we chose the top
200 bi-grams of each sub-corpus, getting 245 unique
bi-grams in total. We then chose all the bi-grams
that were ranked significantly higher or significantly
lower in one language than in at least one other
language, assuming that those bi-grams have strong
separating power. With the threshold of significance
set to 20 we obtained 84 separating bi-grams. Table
1 shows some of the separating bi-grams thus found.
For example, ?la? is a good separator between Rus-
sian and Spanish (its rank in the Spanish corpus is
much higher than that in the Russian corpus), but
not between other pairs.
Using only those 84 bigrams we obtained clas-
sification accuracy of 61.38%, a drop of only 4%
compared to the results achieved with the 200 di-
mensional vectors. These results show that increas-
ing the dimension of the representation vector using
additional bi-grams contribute a marginal improve-
ment while it does not introduce substantial noise.
4.3 Using Tri-gram Frequencies as Features
Repeating the same experiment with the top 200 tri-
grams, we obtained an accuracy of 59.67%, which
is 40% higher than the expected baseline and 15%
higher than the uni-grams baseline. These results
show that the texts in our corpus can be classified
by only using naive n-gram models, while the op-
timal n of the n-gram is a different question that
might be addressed in a different work (and might
be language-dependent).
4.4 Function Words Based Classification
Function words are words that have a little lexical
meaning but instead serve to express grammatical
relations within a sentence or specify the attitude of
the speaker (function words should not be confused
with stopwords, although the lists of most frequent
function words and the stopword list share a large
subset). We used the same list of 460 function words
used by Koppel et al(2005a). A partial list includes:
{a, afterward, although, because, cannot, do, enter,
eventually, fifteenth, hither, hath, hence, lastly, oc-
casionally, presumable, said, seldom, undoubtedly,
was}.
In this variation of the experiment, we represented
each document only by the relative frequencies of
the function words it contained. Using the same
experimental setup as before, we achieved an ac-
curacy of 66.7%. These results are less surprising
than the results obtained by the character n-grams
vectors, since we do expect native speakers of a cer-
tain language to use, misuse or ignore certain func-
tion words as a result from language transfer mech-
anisms (Odlin, 1989). For example, it is well known
that native speakers of Russian tend to omit English
articles.
4.5 Function Words Bias
The previous results suggest that the n-gram based
classification is simply the result of the different
uses of function words by speakers of different na-
tive languages. In order to rule out the effect of the
function words on the bi-gram-based classification,
we removed all function words from the corpus, re-
calculated the bi-gram frequencies and ran the ex-
periment once again, this time achieving an accuracy
of 62.92% in the 10-fold cross validation test.
12
These results, obtained on the function words-free
corpus, clearly show that n-gram based classification
is not a mere artifact masking the use of function
words.
4.6 Content Bias
Bi-gram frequencies could also reflect content bias
rather than language use. By content bias we mean
that the subject matter of the documents in the dif-
ferent sub-corpora could exhibit internal sub-corpus
uniformity and external sub-corpus disparity. In or-
der to rule this out, we employed a variation on the
Term Frequency ? Inverted Document Frequency
(tf-idf ) content analysis metric.
The tf-idf measure is a statistical measure that is
used in information retrieval tasks to evaluate how
important a word/term is to a document in a collec-
tion or corpus (Salton and Buckley, 1988). Given a
collection of documents D, the tf-idf weight of term
t in a document d ? D is computed as follows:
tfidft = ft,d ? log
|D|
ft,D
where ft,d is the frequency of term t in document
d, and ft,D is the number of documents in which t
appears. Therefore, the weight of term t ? d is max-
imal if it is a common term in d while the number of
documents it appears in is relatively low.
We used the tf-idf weights in the information re-
trieval sense in order to discover the dominant con-
tent words of each sub-corpus. We treated each sub-
corpus (set of documents by writers who share a
native language) as a single document and calcu-
lated the tf-idf of each word. In order to determine
whether there is a content bias or not, we set a domi-
nance threshold, and removed all words such that the
difference between their tf-idf score in two different
sub-corpora is higher than the dominance threshold.
Given a threshold t, the dominance Dw,t, of a token
w is given by:
Dw,t = maxi,j |tfidfw,i ? tfidfw,j |
where tfidfw,k is the tf-idf score of token w in
sub-corpus k. Changing the threshold in 0.0005 in-
tervals, we removed from 1 to 340 unique content
words (between 1,545 and 84,725 word tokens in to-
tal). However, the classification accuracy was essen-
tially the same (see Figure 2), with a slight drop of
Word Bulg. Czech Fr. Rus. Spa.
europe 0 0.3 2.7 0.2 0.2
european 0 0.3 3 0.1 0.5
imagination 4.3 2 0.8 1 0.8
television 0 3.6 1.9 3.1 0.3
women 0.4 1.7 1.2 5.5 2.6
Table 2: The tf-idf score of some of the most domi-
nant words, multiplied by 1,000 for easier reading.
Subcorpus content function unique
words words stems
Bulgarian 1543 94685 11325
Czech 2784 110782 12834
French 2059 67016 9474
Russian 2730 112410 12338
Spanish 2985 108052 12627
Total 12101 492945 36474
Table 3: Numbers of dominant content words (with
a threshold of 0.0025) and function words that were
removed from each sub-corpus. The unique stems
column indicates the number of unique stems (types)
that remained after removal of c-w and f-w.
only 2% after removing 51 content words (by using
a threshold of 0.0015).
We calculated the tf-idf weights after stop-words
removal and stemming (using a Porter stemmer
(Porter, 1980)), trying to pinpoint dominant stems.
The results were similar to the word?s tf-idf and no
significantly dominant stem was found in either of
the sub-corpora.
A drop of only 3% in accuracy was noticed after
removing both dominant content words and function
words. These results show that if a content bias ex-
ists in the corpus it has only a minor effect on the
SVM classification, and that the n-grams based clas-
Figure 2: Classification accuracy as a function of the
threshold (removed content words).
13
Thresh. 0.004 0.003 0.0025 0.0015 0.0012 c-w 9 c-w 15 c-w 51 c-w 113 c-w
Bulg. 77 908 1543 3955 7426
Czech 306 1829 2784 5139 8588
French 665 1829 2059 3603 6205
Russian 781 1886 2730 6302 9918
Spanish 389 1418 2985 6548 10521
Total 2218 7970 12101 25547 42658
Table 4: Number of occurrences of content words
that were removed from each sub-corpus for some
of the thresholds. The numbers in the top row indi-
cate the threshold and the number of unique content
words that were found with this threshold.
sification is not an artifact of a content bias.
We ran the same experiment five more times, each
time on 4 sub-corpora instead of 5, removing one
(different) language each time. The results in all 5
4-class experiments were essentially the same, and
similar to those of the 5 language task (beyond the
fact that the random baseline for the former is 25%
rather than 20%).
4.7 Suffix Bias
Bias might also be attributed to the use of suf-
fixes. There are numerous types of English suf-
fixes, which, roughly speaking, may be categorized
as derivational or inflectional. It is reasonable to ex-
pect that just like a use of function words, use or mis-
use of certain suffixes might occur due to language
transfer. Frequent use of a certain suffix or avoid-
ance of the use of a certain suffix may influence the
bi-grams statistics and thus the bi-grams classifica-
tion may be only an artifact of the suffixes usage.
Checking the use of the 50 most productive suf-
fixes taken from a standard list (e.g. ing, ed, less,
able, most, en) we have found that only a small num-
ber of suffixes are not equally used by speakers of all
5 languages. Most notable are the differences in the
use of ing between native French speakers and na-
tive Czech speakers and the differences of use of less
between Bulgarian and Spanish speakers (Table 5).
However, no real bias can be attributed to the use of
any of the suffixes because their relative aggregate
effect on the values in the support vector entries is
very small.
Suffix Bulg. Czech French Russian Spanish
ing 872 719 932 903 759
less 47 36 39 45 32
Table 5: Counts of two of the suffixes whose fre-
quency of use differs the most between sub-corpora.
4.8 Control Corpus
Finally, we have also ran the experiment on a differ-
ent corpus replacing the French and the Spanish sub-
corpora by the Dutch and Italian ones, introducing a
new Roman language and a new Germanic language
to the corpus. We obtained 64.66% accuracy, essen-
tially the same as in the original 5-language setting.
The corpus was compiled from works of advanced
English students of the same level who write essays
of approximately the same length, on a set of ran-
domly and roughly equally distributed topics. We
expected that these students will use roughly the
same n-grams distribution. However, the results de-
scribed above suggest that there exists some mecha-
nism that influences the authors? choice of words. In
the next section we present a computational psycho-
linguistic framework that might explain our results.
5 Statistical Learning and Language
Transfer in SLA
5.1 Statistical Learning by Infants
Psychologists, linguists, and cognitive science re-
searchers try to understand the process of language
learning by infants. Many models for language
learning and cognitive language modeling were sug-
gested (Clark, 2003).
Infants learn their first language by a combina-
tion of speech streams, vocal cues and body ges-
tures. Infants as young as 8 months old have a
limited grasp of their native tongue as they react
to familiar words. In that age they already under-
stand the meaning of single words, they learn to spot
these words in a speech stream, and very soon they
learn to combine different words into new sentential
units. Parental speech stream analysis shows that it
is impossible to separate between words by identi-
fying sequences of silence between words (Saffran,
2001). Recent studies of infant language learning
are in favor of the statistical framework (Saffran,
2001; Saffran et al 1996). Saffran (2002) exam-
14
ined 8 month-old to one year-old infants who were
stimulated by speech sequences. The infants showed
a significant discrimination between word and non-
word stimuli. In a different experimental setup in-
fants showed a significant discrimination between
frequent syllable n-grams and non frequent sylla-
ble n-grams, heard as part of a gibberish speech se-
quence generated by a computer according to var-
ious statistical language models. In a third experi-
mental setup infants showed a significant discrimi-
nation in favor of English-like gibberish speech se-
quences upon non-English-like gibberish speech se-
quences. These findings along with the established
finding (Jusczyk, 1997) that infants prefer the sound
of their native tongue suggest that humans learn ba-
sic language units in a statistical manner and that
they store some statistical parameters pertaining to
these units. We should note that some researchers
doubt these conclusions (Yang, 2004).
5.2 Language Transfer in SLA
The role of the first language in second language ac-
quisition is under a continuous debate (Ellis, 1999).
Language Transfer between L1 and L2 is the pro-
cess in which a language learner of L2 whose na-
tive language is L1, is influenced by L1 when using
L2 (actually, when building his/her inter-language).
This influence might appear helpful when L2 is rel-
atively close to L1, but it interferes with the learn-
ing process due to over- and under-generalization or
other problems. Although there is clear evidence
that language learners use constructs of their first
language when learning a foreign language (James,
1980; Odlin, 1989), it is not clear that the majority
of learner errors can be attributed to the L1 transfer
(Ellis, 1999).
5.3 Sound Transfer Hypothesis
For alphabetic scripts, character bi-grams reflect ba-
sic sounds and sound sequences of the language3.
We have shown that native language strongly corre-
lates with character bi-grams when people write in
English as a second language. After ruling out usage
of function words, content bias, and morphology-
related influences, the most plausible explanation is
3Note that for English, they do not directly correspond to
phonemes or syllables. Nonetheless, they do reflect English
phonology to some extent.
that these are language transfer effects related to L1
sounds.
We hypothesize that there are language transfer
effects related to L1 sounds and manifested by the
words that people choose to use when writing in a
second language. (We say ?writing? because we have
only experimented with written texts; a more gen-
eral hypothesis covering speaking and writing can
be formulated as well.)
Furthermore, since the acquisition and represen-
tation of phonology is strongly influenced by statis-
tical considerations (Section 5.1), we speculate that
the general language transfer phenomenon might be
related to frequency. This does not directly follow
from our findings, of course, but is an exciting direc-
tion to investigate, and it is in accordance with the
growing body of work on the effects of frequency
on language learning and the emergence of syntax
(Ellis, 2002; Bybee, 2006).
We note that there is one obvious and well-known
lexical transfer effect: the usage of cognates (words
that have similar form (sound) and meaning in two
different languages). However, the languages we
used in our experiments contain radically differing
amounts of cognates of English words (just consider
French vs. Bulgarian, for example), while the clas-
sification results were about the same for all 5 lan-
guages. Hence, cognates might play a role, but they
do not constitute a single major explaining factor for
our findings.
We note that the hypothesis put forward in the
present paper is the first that attributes a language
transfer phenomenon to a cognitive representation
(phonology) whose statistical nature has been seri-
ously substantiated.
6 Conclusion
In this paper we have demonstrated how modern ma-
chine learning can aid other fields, here the impor-
tant field of Second Language Acquisition (SLA).
Our analysis of the features useful for a multi-class
SVM in the task of native language classification has
resulted in the formulation of a hypothesis of poten-
tial significance in the theory of language transfer
in SLA. We hypothesize language transfer effects at
the level of basic sounds and short sound sequences,
manifested by the words that people choose when
15
writing in a second language. In other words, we
hypothesize that use of L2 words is strongly influ-
enced by L1 sounds and sound patterns.
As noted above, further experiments (psycholog-
ical and computational) must be conducted for vali-
dating our hypothesis. In particular, construction of
a wide-scale learners? corpus with tight control over
content bias is essential for reaching stronger con-
clusions.
Additional future work should address sound se-
quences vs. the orthographic sequences that were
used in this work. If our hypothesis is correct, then
using spoken language corpora should produce even
stronger results, since (1) writing systems rarely
show a 1-1 correspondence with how words are at
the phonological level; and (2) writing allows more
conscious thinking that speaking, thus potentially re-
duces transfer effects. Our eventual goal is creating
a unified model of statistical transfer mechanisms.
References
Argamon S., Koppel M. and Shimoni A. 2003. Gender,
Genre, and Writing Style in Formal Written Texts. Text
23(3).
Bouselmi G., Fohr D., Illina, I., and Haton J.P.
2005. Fully Automated Non-Native Speech Recog-
nition Using Confusion-Based Acoustic Model. Eu-
rospeech/Interspeech ?05.
Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006.
Fully Automated Non-Native Speech Recognition Us-
ing Confusion-Based Acoustic Model Integration and
Graphemic Constraints. IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
2006.
Bybee J. 2006. Frequency of Use and the Organization
of Language. Oxford University Press.
Clark, E. 2003. First Language Acquisition. Cambridge
University Press.
Diederich J., Kindermann J., Leopold E. and Paass G.
2004. Authorship Attribution with Support Vector Ma-
chines. Applied Intelligence, 109?123.
Ellis N. 2002. Frequency Effects in Language Pro-
cessing. Studies in Second Language Acquisition,
24(2):143?188.
Ellis R. 1999. Understanding Second Language Acqui-
sition. Oxford University Press.
Granger S., Dagneaux E. and Meunier F. 2002. Inter-
national Corpus of Learner English. Presses universi-
taires de Louvain.
Hansen J. H., Yapanel U., Huang, R. and Ikeno A. 2004.
Dialect Analysis and Modeling for Automatic Classi-
fication. Interspeech-2004/ICSLP-2004: International
Conference Spoken Language Processing. Jeju Island,
South Korea.
Holmes D. and Forsyth R. 1995. The Federalist Revis-
ited: New Directions in Authorship Attribution. Liter-
ary and Linguistic Computing, pp. 111?127.
James C. E. 1980. Contrastive Analysis. New York:
Longman.
Jusczyk P. W. 1997. The Discovery of Spoken Language.
MIT Press.
Koppel M. and Schler J. 2003. Exploiting Stylistic Id-
iosyncrasies for Authorship Attribution. In Proceed-
ings of IJCAI ?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis. Acapulco,
Mexico.
Koppel M., Schler J. and Zigdon K. 2005(a). Determin-
ing an Author?s Native Language by Mining a Text for
Errors. Proceedings of KDD ?05. Chicago IL.
Koppel M., Schler J. and Zigdon K. 2005(b). Auto-
matically Determining an Anonymous Author?s Native
Language. In Intelligence and Security Informatics
(pp. 209?217). Berlin / Heidelberg: Springer.
Odlin T. 1989. Language Transfer: Cross-Linguistic In-
fluence in Language Learning. Cambridge University
Press.
Porter F. M. 1980. An Algorithm for Suffix Stripping.
Program, 14(3):130?137.
Saffran J. R. 2001. Words in a Sea of Sounds: The Output
of Statistical Learning. Cognition, 81, 149?169.
Saffran J. R. 2002. Constraints on Statistical Language
Learning. Journal of Memory and Language, 47, 172?
196.
Saffran J. R., Aslin R. N. and Newport E. N. 1996. Sta-
tistical Learning by 8-month Old Infants. Science, is-
sue 5294, 1926?1928.
Salton G. and Buckley C. 1988. Term Weighing Ap-
proaches in Automatic Text Retrieval. Information
Processing and Management, 24(5):513?523.
Scho?lkopf B,. Smola A 2002. Learning with Kernels.
MIT Press.
Stamatatos E,. Fakotakis N. and Kokkinakis G. 2004.
Computer-Based Authorship Attribution Without Lex-
ical Measures. Computers and the Humanities, 193?
214.
Witten I. H. and Frank E. 2005. Data Mining: Practical
Machine Learning Tools and Techniques. San Fran-
cisco: Morgan Kaufmann.
Yang C. 2004. Universal Grammar, Statistics, or Both?.
Trends in Cognitive Science 8(10):451?456, 2004.
16
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 36?44,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Unsupervised Concept Discovery In Hebrew Using Simple Unsupervised
Word Prefix Segmentation for Hebrew and Arabic
Elad Dinur1 Dmitry Davidov2
1Institute of Computer Science
2ICNC
The Hebrew University of Jerusalem
Ari Rappoport1
Abstract
Fully unsupervised pattern-based methods
for discovery of word categories have been
proven to be useful in several languages.
The majority of these methods rely on the
existence of function words as separate
text units. However, in morphology-rich
languages, in particular Semitic languages
such as Hebrew and Arabic, the equiva-
lents of such function words are usually
written as morphemes attached as prefixes
to other words. As a result, they are missed
by word-based pattern discovery methods,
causing many useful patterns to be unde-
tected and a drastic deterioration in per-
formance. To enable high quality lexical
category acquisition, we propose a sim-
ple unsupervised word segmentation algo-
rithm that separates these morphemes. We
study the performance of the algorithm for
Hebrew and Arabic, and show that it in-
deed improves a state-of-art unsupervised
concept acquisition algorithm in Hebrew.
1 Introduction
In many NLP tasks, we wish to extract informa-
tion or perform processing on text using minimal
knowledge on the input natural language. Towards
this goal, we sometimes find it useful to divide the
set of words in natural language to function words
and content words, a division that applies in the
vast majority of languages. Function words (or
grammatical words, e.g., a, an, the, in, of, etc) are
words that have little or highly ambiguous lexi-
cal meaning, and serve to express grammatical or
semantic relationships with the other words in a
sentence.
In some morphologically-rich languages, im-
portant function words are not written as space-
separated units but as morphemes attached as pre-
fixes to other words. This fact can cause prob-
lems when statistically analyzing text in these lan-
guages, for two main reasons: (1) the vocabulary
of the language grows, as our lexical knowledge
comes solely from a corpus (words appear with
and without the function morphemes); (2) infor-
mation derived from the presence of these mor-
phemes in the sentence is usually lost.
In this paper we address the important task of
a fully unsupervised acquisition of Hebrew lexical
categories (or concepts ? words sharing a signifi-
cant aspect of their meaning). We are not aware of
any previous work on this task for Hebrew. Due
to the problem above, the performance of many
acquisition algorithms deteriorates unacceptably.
This happens, for example, in the (Davidov and
Rappoport, 2006) algorithm that utilizes automati-
cally detected function words as the main building
block for pattern construction.
In order to overcome this problem, one should
separate such prefixes from the compound words
(words consisting of function morphemes attached
to content words) in the input corpus. When
we consider some particular word, there are fre-
quently many options to split it to smaller strings.
Fortunately, the set of function words is small and
closed, and the set of grammatical sequences of
function prefixes is also small. Hence we assume
it does not cost us much to know in advance what
are the possible sequences for a specific language.
Even when considering the small number of
possible function words, the task of separating
them is not simple, as some words may be ambigu-
ous. When reading a word that starts with a prefix
known to be a function morpheme, the word may
36
be a compound word, or it may be a meaningful
word by itself. For example, the word ?hsws? in
Hebrew1 can be interpreted as ?hsws? (hesitation),
or ?h sws? (the horse). The segmentation of the
word is context dependent ? the same string may
be segmented differently in different contexts.
One way of doing such word prefix segmenta-
tion is to perform a complete morphological dis-
ambiguation of the sentence. The disambigua-
tion algorithm finds for each word its morpho-
logical attributes (POS tag, gender, etc.), and de-
cides whether a word is a compound word or a
word without prefixes. A disambiguation algo-
rithm generally relies on a language-specific mor-
phological analyzer. It may also require a large
manually tagged corpus, construction of which for
some particular language or domain requires sub-
stantial human labor. We avoid the utilization of
such costly and language-specific disambiguation
algorithms and manually annotated data.
In this paper we present a novel method to sep-
arate function word prefixes, and evaluate it us-
ing manually labeled gold standards in Hebrew
and Arabic. We incorporate the method into a
pattern-based Hebrew concept acquisition frame-
work and show that it greatly improves state-of-art
results for unsupervised lexical category acquisi-
tion. This improvement allows the pattern-based
unsupervised framework to use one-tenth of the
Hebrew data in order to reach a similar level of
results.
Section 2 discusses related work, and Section 3
reviews the word categories discovery algorithm.
Section 4 presents the word prefix segmentation
algorithm. Results are given in Section 5.
2 Related Work
In this paper we develop an unsupervised frame-
work for segmentation of the function words for
languages where context is important for correct
segmentation. Our main target language is He-
brew, and we experimented with Arabic as well.
As far as we know, there is no work on unsu-
pervised segmentation of words in Hebrew which
does not utilize language-specific tools such as
morphological analyzers.
Lee et al (2003) addressed supervised word
segmentation in Arabic and have some aspects
similar to our approach. As in their study, we
1Transcription is according to (Ornan, 2005), except for
Shin which is denoted by ?$?.
also have a pre-supplied list of possible prefix
sequences and assume a trigram model in order
to find the most probable morpheme sequence.
Both studies evaluate performance on a segmented
text, and not just on words in the lexicon. How-
ever, their algorithm, while achieving good per-
formance (97% accuracy), relies on a training set
? a manually segmented corpus of about 110,000
words, while our unsupervised framework does
not require any annotation and is thus easier to im-
plement and to apply to different domains and lan-
guages.
Snyder and Barzilay (2008) study the task of un-
supervised morphological segmentation of multi-
ple languages. Their algorithm automatically in-
duces a segmentation and morpheme alignment of
short parallel phrases from a multilingual corpus.
Their corpus (The Hebrew Bible and translations)
contains parallel phrases in English, Arabic, He-
brew and Aramaic. They obtain 63.87 F-Score
for Hebrew words segmentation (prefix and suf-
fix), where recall and precision is calculated based
on all possible segmentation points.
Another type of segmentation algorithms in-
volves utilization of language-specific morpholog-
ical analyzers for complete morphological disam-
biguation. In Hebrew each word usually has more
than one possible POS (along with other attributes,
such as gender, number, etc.). Assuming we have
a morphological analyzer (producing the set of
possible analyses for a given word), we can try to
discover the correct segmentation of each word.
Levinger et al (1995) developed a method for
disambiguation of the results provided by a mor-
phological analyzer for Hebrew. Adler and El-
hadad (2006) proposed an unsupervised algorithm
for word segmentation. They estimate an initial
language model (using (Levinger et al, 1995))
and improve this model with EM. Direct compar-
ison to their work is problematic, however, since
we avoid utilization of a language-specific mor-
phology/POS analyzer. There are also studies of
this type that utilize labeled data (Bar-Haim et al,
2005), where the language model is learned from
the training data.
Extensive research has been done on word seg-
mentation, where, unlike in our study, the segmen-
tation is evaluated for every word, regardless of its
context. Creutz (2003) presents an algorithm for
unsupervised segmentation under these assump-
tions. He proposes a probabilistic model which
37
utilizes the distributions of morpheme length and
frequency to estimate the quality of the induced
morphemes. Dasgupta and Ng (2007) improves
over (Creutz, 2003) by suggesting a simpler ap-
proach. They segment a prefix using the word
frequency with and without a prefix. Other re-
cent studies that follow the context-independent
setup include (Creutz and Lagus, 2005; Keshava
and Pitler, 2005; Demberg, 2007). They test
their methods on English, Finnish and Turkish.
All of these studies, however, assume context-
independency of segmentation, disregarding the
ambiguity that may come from context. This
makes it problematic to apply the proposed meth-
ods to context-dependent morphology types as in
Hebrew and Arabic.
The guiding goal in the present paper is the con-
cept acquisition problem. Concept acquisition of
different kinds has been studied extensively. The
two main classification axes for this task are the
type of human input and annotation, and the basic
algorithmic approach used. The two main algo-
rithmic approaches are clustering of context fea-
ture vectors and pattern-based discovery.
The first approach is to map each word to a fea-
ture vector and cluster these vectors. Example of
such algorithms are (Pereira et al, 1993) and (Lin,
1998) that use syntactic features in the vector def-
inition. Pantel and Lin (2002) improves on the lat-
ter by clustering by committee.
Recently, there is a growing interest in the sec-
ond main algorithmic approach, usage of lexico-
syntactic patterns. Patterns have been shown to
produce more accurate results than feature vectors,
at a lower computational cost on large corpora
(Pantel et al, 2004). Thus (Dorow et al, 2005)
discover categories using two basic pre-specified
patterns (?x and y?, ?x or y?).
Some recent studies have proposed frameworks
that attempt to avoid any implicit or explicit pre-
specification of patterns. Davidov and Rappoport
(2006) proposed a method that detects function
words by their high frequency, and utilizes these
words for the discovery of symmetric patterns.
Their method is based on two assumptions: (1)
some function words in the language symmetri-
cally connect words belonging to the same cat-
egory; (2) such function words can be detected
as the most frequent words in language. While
these assumptions are reasonable for many lan-
guages, for some morphologically rich languages
the second assumption may fail. This is due to
the fact that some languages like Hebrew and Ara-
bic may express relationships not by isolated func-
tion words but by morphemes attached in writing
to other words.
As an example, consider the English word
?and?, which was shown to be very useful in con-
cept acquisition (Dorow et al, 2005). In Hebrew
this word is usually expressed as the morpheme
?w? attached to the second word in a conjunc-
tion (?... wsws? ? ?... and horse?). Patterns dis-
covered by such automatic pattern discovery al-
gorithms are based on isolated words, and hence
fail to capture ?and?-based relationships that are
very useful for detection of words belonging to the
same concept. Davidov and Rappoport (2006) re-
ports very good results for English and Russian.
However, no previous work applies a fully unsu-
pervised concept acquisition for Hebrew.
In our study we combine their concept ac-
quisition framework with a simple unsupervised
word segmentation technique. Our evaluation con-
firms the weakness of word-based frameworks for
morphology-rich languages such as Hebrew, and
shows that utilizing the proposed word segmen-
tation can overcome this weakness while keeping
the concept acquisition approach fully unsuper-
vised.
3 Unsupervised Discovery of Word
Categories
In this study we use word segmentation to improve
the (Davidov and Rappoport, 2006) method for
discovery of word categories, sets of words shar-
ing a significant aspect of their meaning. An ex-
ample for such a discovered category is the set of
verbs {dive, snorkel, swim, float, surf, sail, drift,
...}. Below we briefly describe this category ac-
quisition algorithm.
The algorithm consists of three stages as fol-
lows. First, it discovers a set of pattern candidates,
which are defined by a combination of high fre-
quency words (denoted by H) and slots for low
frequency (content) words (denoted by C). An ex-
ample for such a pattern candidate is ?x belongs to
y?, where ?x? and ?y? stand for content word slots.
The patterns are found according to a predefined
set of possible meta-patterns. The meta-patterns
are language-independent2 and consist of up to 4
2They do not include any specific words, only a relative
order of high/low frequency words, and hence can be used on
38
words in total, from which two are (non-adjacent)
content words. Four meta-patterns are used: CHC,
CHCH, CHHC, HCHC.
Second, those patterns which give rise to sym-
metric lexical relationships are identified. The
meaning of phrases constructed from those pat-
terns is (almost) invariant to the order of the con-
tent words contained in them. An example for
such a pattern is ?x and y?. In order to iden-
tify such useful patterns, for each pattern we build
a graph following (Widdows and Dorow, 2002).
The graph is constructed from a node for each con-
tent word, and a directed arc from the node ?x? to
?y? if the corresponding content words appear in
the pattern such that ?x? precedes ?y?. Then we
calculate several symmetry measures on the graph
structure and select the patterns with best values
for these measures.
The third stage is the generation of categories.
We extract tightly connected sets of words from
the unified graph which combines all graphs of se-
lected patterns. Such sets of words define the de-
sired categories.
The patterns which include the ?x and y? sub-
string are among the most useful patterns for gen-
eration of categories (they were used in (Dorow et
al., 2005) and discovered in all 5 languages tested
in (Davidov and Rappoport, 2006)). However, in
Hebrew such patterns can not be found in the same
way, since the function word ?and? is the prefix ?w?
and not a standalone high frequency word.
Another popular set of patterns are ones includ-
ing ?x or y?. Such patterns can be identified in
Hebrew, as ?or? in Hebrew is a separate word.
However, even in this case, the content word rep-
resented by ?x? or ?y? may appear with a pre-
fix. This damages the construction of the pattern
graph, since two different nodes may be created
instead of one ? one for a regular content word,
the other for the same word with a prefix. Conse-
quently, it is reasonable to assume that segmenting
the corpus in advance should improve the results
of discovery of word categories.
4 Word Segmentation Algorithm
We assume we know the small and closed set of
grammatical function word prefix sequences in the
language3. Our input is a sentence, and our ob-
any languages with explicit word segmentation.
3Unlike development of labeled training data, handcraft-
ing such a closed set is straightforward for many languages
and does not requires any significant time/human labor
jective is to return the correct segmentation of the
sentence. A sentence L is a sequence of words
{w1, w2, ..., wn}. A segmentation Si of L is a se-
quence of morphemes {m1, m2, ..., mk} and l(Si)
is the number of morphemes in the sequence. Note
that l(Si) may be different for each segmentation.
The best segmentation S will be calculated by:
P (Si) = p(m1)p(m2|m1)
l(Si)
?
i=3
p(mi|mi?1mi?2)
S = arg max
Si
P (Si)
Calculation of joint probabilities requires a tri-
gram model of the language. Below we describe
the construction of the trigram model and then we
detail the algorithm for efficient calculation of S.
4.1 Construction of trigram model
Creating the trigram language model is done in
two stages: (1) we segment a corpus automati-
cally, and (2) we learn a trigram language model
from the segmented corpus.
4.1.1 Initial corpus segmentation
For initial corpus segmentation, we define a sta-
tistical measure for the segmentation of individual
words. Let wx be a word, such that w is the pre-
fix of the word composed of a sequence of func-
tion word prefixes and x is a string of letters. Let
f(x) be the frequency of the word x in the cor-
pus. Denote by al the average length of the strings
(with prefixes) in the language. This can be eas-
ily estimated from the corpus ? every string that
appears in the corpus is counted once. l(x) is the
number of characters in the word x. We utilize
two parameters G, H , where G < H (we used
G = 2.5, H = 3.5) and define the following func-
tions :
factor(x) =
{
al?G?l(x)
al?H l(x) < al ? G
0 otherwise
Rank(wx) =
f(wx)
f(wx) + f(x)
+ factor(x)
Note that the expression f(wx)f(wx)+f(x) is a number
in (0, 1], inversely correlated with the frequency of
the prefixed word. Thus higher Rank(wx) values
indicate that the word is less likely to be composed
of the prefix w followed by the word x.
39
The expression al?G?l(x)al?H is a number in (0, 1],
therefore factor(x) ? [0, 1]. H is G ? 1 in order
to keep the expression smaller than 1. The term
factor(x) is greater as x is shorter. The factor
is meant to express the fact that short words are
less likely to have a prefix. We have examined
this in Hebrew ? as there are no words of length
1, two letter words have no prefix. We have ana-
lyzed 102 randomly chosen three letter words, and
found that only 19 of them were prefixed words.
We have analyzed 100 randomly chosen four let-
ter words, and found that 40 of them were pre-
fixed words. The result was about the same for
five letter words. In order to decide whether a
word needs to be separated, we define a thresh-
old T ? [0, 1]. We allow word separation only
when Rank(wx) is lower than T . When there
are more than two possible sequences of function
word prefixes (?mhsws?,?m hsws?, ?mh sws?),
we choose the segmentation with the lower rank.
4.1.2 Learning the trigram model
The learning of the language model is based on
counts of the corpus, assigning a special symbol,
?u/k? (unknown) for all words that do not appear
in the corpus. As estimated by (Lee et al, 2003),
we set the probability of ?u/k? to be 1E ? 9. The
value of the symbol ?u/k? was observed to be sig-
nificant. We found that the value proposed by (Lee
et al, 2003) for Arabic gives good results also for
Hebrew.
4.2 Dynamic programming approach for
word segmentation
The naive method to find S is to iterate over
all possible segmentations of the sentence. This
method may fail to handle long sentences, as
the number of segmentations grows exponentially
with the length of the sentence. To overcome this
problem, we use dynamic programming.
Each morpheme has an index i to its place in a
segmentation sequence. Iteratively, for index i, for
every morpheme which appears in some segmen-
tation in index i, we calculate the best segmen-
tation of the sequence m1 . . .mi. Two problems
arise here: (1) we need to calculate which mor-
phemes may appear in a given index; (2) we need
to constrain the calculation, such that only valid
segmentations would be considered.
To calculate which morphemes can appear in a
given index we define the object Morpheme. It
contains the morpheme (string), the index of a
word in the sentence the morpheme belongs to,
reference to the preceding Morpheme in the same
word, and indication whether it is the last mor-
pheme in the word. For each index of the sen-
tence segmentation, we create a list of Morphemes
(index-list).
For each word wi, and for segmentation
m1i , .., m
k
i , we create Morphemes M1i , .., Mki . We
traverse sequentially the words in the sentence,
and for each segmentation we add the sequence of
Morphemes to all possible index-lists. The index-
list for the first Morpheme M1i is the combination
of successors of all the index-lists that contain a
Morpheme Mki?1. The constraints are enforced
easily ? if a Morpheme M ji is the first in a word,
the preceding Morpheme in the sequence must be
the last Morpheme of the previous word. Oth-
erwise, the preceding Morpheme must be M j?1i ,
which is referenced by M ji .
4.3 Limitations
While our model handles the majority of cases, it
does not fully comply with a linguistic analysis of
Hebrew, as there are a few minor exceptions. We
assumed that there is no ambiguity in the function
word prefixes. This is not entirely correct, as in
Hebrew we have two different kinds of exceptions
for this rule. For example, the prefix ?k$? (when),
can also be interpreted as the prefix ?k? (as) fol-
lowed by the prefix ?$? (that). As the second in-
terpretation is rare, we always assumed it is the
prefix ?k$?. This rule was applied wherever an
ambiguity exists. However, we did not treat this
problem as it is very rare, and in the development
set and test set it did not appear even once.
A harder problem is encountered when process-
ing the word ?bbyt?. Two interpretations could
be considered here: ?b byt? (?in a house?), and
?b h byt? (?in the house?). Whether this actu-
ally poses a problem or not depends on the ap-
plication. We assume that the correct segmenta-
tion here is ?b byt?. Without any additional lin-
guistic knowledge (for example, diacritical vowel
symbols should suffice in Hebrew), solving these
problems requires some prior discriminative data.
5 Evaluation and Results
We evaluate our algorithm in two stages. First we
test the quality of our unsupervised word segmen-
tation framework on Hebrew and Arabic, compar-
ing our segmentation results to a manually anno-
40
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.70 0.844 0.798 0.820 0.875 0.811 0.851 0.830 0.881
0.73 0.841 0.828 0.834 0.883 0.808 0.866 0.836 0.884
0.76 0.837 0.846 0.841 0.886 0.806 0.882 0.842 0.887
0.79 0.834 0.870 0.851 0.893 0.803 0.897 0.847 0.890
0.82 0.826 0.881 0.852 0.892 0.795 0.904 0.846 0.888
0.85 0.820 0.893 0.854 0.892 0.787 0.911 0.844 0.886
0.88 0.811 0.904 0.855 0.891 0.778 0.917 0.841 0.882
Table 1: Ranks vs. Threshold T for Hebrew.
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.91 0.940 0.771 0.846 0.892 0.903 0.803 0.850 0.891
0.93 0.930 0.797 0.858 0.898 0.903 0.840 0.870 0.904
0.95 0.931 0.810 0.866 0.904 0.902 0.856 0.878 0.909
0.97 0.927 0.823 0.872 0.906 0.896 0.869 0.882 0.911
0.99 0.925 0.848 0.872 0.915 0.878 0.896 0.886 0.913
1.00 0.923 0.852 0.886 0.915 0.841 0.896 0.867 0.895
Table 2: Ranks vs. Threshold T for Arabic.
Algorithm P R F A
Rank seg. 0.834 0.870 0.851 0.893
Baseline 0.561 0.491 0.523 0.69
Morfessor 0.630 0.689 0.658 0.814
Table 3: Segmentation results comparison.
tated gold standard. Then we incorporate word
segmentation into a concept acquisition frame-
work and compare the performance of this frame-
work with and without word segmentation.
5.1 Corpora and annotation
For our experiments in Hebrew we used a 19MB
Hebrew corpus obtained from the ?Mila? Knowl-
edge Center for Processing Hebrew4. The cor-
pus consists of 143,689 different words, and a
total of 1,512,737 word tokens. A sample text
of size about 24,000 words was taken from the
corpus, manually segmented by human annotators
and used as a gold standard in our segmentation
evaluation. In order to estimate the quality of our
algorithm for Arabic, we used a 7MB Arabic news
items corpus, and a similarly manually annotated
test text of 4715 words. The Arabic corpus is too
small for meaningful category discovery, so we
used it only in the segmentation evaluation.
5.2 Evaluation of segmentation framework
In order to estimate the performance of word seg-
mentation as a standalone algorithm we applied
our algorithm on the Hebrew and Arabic corpora,
4http://mila.cs.technion.ac.il.
using different parameter settings. We first cal-
culated the word frequencies, then applied initial
segmentation as described in Section 4. Then we
used SRILM (Stolcke, 2002) to learn the trigram
model from the segmented corpus. We utilized
Good-Turing discounting with Katz backoff, and
we gave words that were not in the training set the
constant probability 1E ? 9. Finally we utilized
the obtained trigram model to select sentence seg-
mentations. To test the influence of the factor(x)
component of the Rank value, we repeated our
experiment with and without usage of this com-
ponent. We also ran our algorithm with a set of
different threshold T values in order to study the
influence of this parameter.
Tables 1 and 2 show the obtained results for He-
brew and Arabic respectively. Precision is the ra-
tio of correct prefixes to the total number of de-
tected prefixes in the text. Recall is the ratio of pre-
fixes that were split correctly to the total number
of prefixes. Accuracy is the number of correctly
segmented words divided by the total number of
words.
As can be seen from the results, the best F-score
with and without usage of the factor(x) compo-
nent are about the same, but usage of this compo-
nent gives higher precision for the same F-score.
From comparison of Arabic and Hebrew perfor-
mance we can also see that segmentation decisions
for the task in Arabic are likely to be easier, since
the accuracy for T=1 is very high. It means that,
unlike in Hebrew (where the best results were ob-
tained for T=0.79), a word which starts with a pre-
41
Method us k-means random
avg ?shared meaning?(%) 85 24.61 10
avg triplet score(1-4) 1.57 2.32 3.71
avg category score(1-10) 9.35 6.62 3.5
Table 4: Human evaluation results.
abuse, robbery, murder, assault, extortion
good, cheap, beautiful, comfortable
son, daughter, brother, parent
when, how, where
essential, important, central, urgent
Table 5: A sample from the lexical categories dis-
covered in Hebrew (translated to English).
fix should generally be segmented.
We also compared our best results to the base-
line and to previous work. The baseline draws a
segmentation uniformly for each word, from the
possible segmentations of the word. In an at-
tempt to partially reproduce (Creutz and Lagus,
2005) on our data, we also compared our results
to the results obtained from Morfessor Categories-
MAP, version 0.9.1 (Described in (Creutz and La-
gus, 2005)). The Morfessor Categories-MAP al-
gorithm gets a list of words and their frequen-
cies, and returns the segmentation for every word.
Since Morfessor may segment words with prefixes
which do not exist in our predefined list of valid
prefixes, we did not segment the words that had
illegal prefixes as segmented by Morfessor.
Results for this comparison are shown in Table
3. Our method significantly outperforms both the
baseline and Morfessor-based segmentation. We
have also tried to improve the language model by
a self training scheme on the same corpus but we
observed only a slight improvement, giving 0.848
Precision and 0.872 Recall.
5.3 Discovery of word categories
We divide the evaluation of the word categories
discovery into two parts. The first is evaluating
the improvement in the quantity of found lexical
categories. The second is evaluating the quality
of these categories. We have applied the algo-
rithm to a Hebrew corpus of size 130MB5, which
is sufficient for a proof of concept. We compared
the output of the categories discovery on two dif-
ferent settings, with function word separation and
without such separation. In both settings we omit-
5Again obtained from the ?Mila? Knowledge Center for
Processing Hebrew.
N A J
With Separation 148 4.1 1
No Separation 36 2.9 0
Table 6: Lexical categories discovery results com-
parison. N: number of categories. A: average cat-
egory size. J: ?junk? words.
ted all punctuation symbols. In both runs of the
algorithm we used the same parameters. Eight
symmetric patterns were automatically chosen for
each run. Two of the patterns that were chosen
by the algorithm in the unseparated case were also
chosen in the separated case.
5.3.1 Manual estimation of category quality
Evaluating category quality is challenging since
no exhaustive lists or gold standards are widely
accepted even in English, certainly so in resource-
poor languages such as Hebrew. Hence we follow
the human judgment evaluation scheme presented
in (Davidov and Rappoport, 2006), for the cate-
gories obtained from the segmented corpus.
We compared three methods of word categories
discovery. The first is random sampling of words
into categories. The second is k-means, where
each word is mapped to a vector, and similarity is
calculated as described in (Pantel and Lin, 2002).
We applied k-means to the set of vectors, with sim-
ilarity as a distance function. If a vector had low
similarity with all means, we leave it unattached.
Therefore some clusters contained only one vec-
tor. Running the algorithm 10 times, with different
initial means each time, produced 60 clusters with
three or more words. An interesting phenomenon
we observed is that this method produces very nice
clusters of named entities. The last method is the
one in (Davidov and Rappoport, 2006).
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
42
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were selected at random from con-
tent words in the same document.
In Part II, subjects were given the full categories
represented by the triplets that were graded as 1 or
2 in Part I (the full ?good? categories in terms of
sharing of meaning). Subjects were asked to grade
the categories from 1 (worst) to 10 (best) accord-
ing to how much the full category had met the ex-
pectations they had when seeing only the triplet.
Nine people participated in the evaluation. A
summary of the results is given in Table 4.
The categories obtained from the unsegmented
corpus are too few and too small for a significant
evaluation. Therefore we applied the evaluation
scheme only for the segmented corpus.
The results from the segmented corpus contain
some interesting categories, with a 100% preci-
sion, like colors, Arab leaders, family members
and cities. An interesting category is {Arabic, En-
glish, Russian, French, German, Yiddish, Polish,
Math}. A sample of some other interesting cate-
gories can be seen in Table 5.
5.3.2 Segmentation effect on category
discovery
In Table 6, we find that there is a major improve-
ment in the number of acquired categories, and an
interesting improvement in the average category
size. One might expect that as a consequence of
an incorrect segmentation of a word, ?junk? words
may appear in the discovered categories. As can
be seen, only one ?junk? word was categorized.
Throughout this paper we have assumed that
function word properties of languages such as He-
brew and Arabic decrease performance of whole-
word pattern-based concept acquisition methods.
To check this assumption, we have applied the
concept acquisition algorithm on several web-
based corpora of several languages, while choos-
ing corpora size to be exactly equal to the size of
the Hebrew corpus (130Mb) and utilizing exactly
the same parameters. We did not perform quality
evaluation6, but measured the number of concepts
and concept size. Indeed the number of categories
was (190, 170, 159, 162, 150, 29) for Russian, En-
glish, Spanish, French, Turkish and Arabic respec-
tively, clearly inferior for Arabic in comparison to
these European and Slavic languages. A similar
6Brief manual examination suggests no significant drops
in concept quality.
tendency was observed for average concept size.
At the same time prefix separation does help to ex-
tract 148 concepts for Hebrew, making it nearly in-
line with other languages. In contrast, our prelim-
inary experiments on English and Russian suggest
that the effect of applying similar morphological
segmentation on these languages in insignificant.
In order to test whether more data can substi-
tute segmentation even for Hebrew, we have ob-
tained by means of crawling and web queries a
larger (while potentially much more noisy) web-
based 2GB Hebrew corpus which is based on fo-
rum and news contents. Our goal was to estimate
which unsegmented corpus size (if any) can bring
similar performance (in terms of concept number,
size and quality). We gradually increased corpus
size and applied the concept acquisition algorithm
on this corpus. Finally, we have obtained similar,
nearly matching, results to our 130MB corpus for
a 1.2GB Hebrew subcorpus of the 2GB Hebrew
corpus. The results remain stable for 4 different
1.2GB subsets taken from the same 2GB corpus.
This suggests that while segmentation can be sub-
stituted with more data, it may take roughly x10
more data for Hebrew to obtain the same results
without segmentation as with it.
6 Summary
We presented a simple method for separating func-
tion word prefixes from words. The method re-
quires very little language-specific knowledge (the
prefixes), and it can be applied to any morpholog-
ically rich language. We showed that this segmen-
tation dramatically improves lexical acquisition in
Hebrew, where nearly ?10 data is required to ob-
tain the same number of concepts without segmen-
tation.
While in this paper we evaluated our framework
on the discovery of concepts, we have recently
proposed fully unsupervised frameworks for the
discovery of different relationship types (Davidov
et al, 2007; Davidov and Rappoport, 2008a; Davi-
dov and Rappoport, 2008b). Many of these meth-
ods are mostly based on function words, and may
greatly benefit from the proposed segmentation
framework.
References
Meni Adler, Michael Elhadad, 2006. An Unsupervised
Morpheme-Based HMM for Hebrew Morphological
Disambiguation. ACL ?06.
43
Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005.
Choosing an Optimal Architecture for Segmentation
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages ?05.
Mathias Creutz, 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. ACL ?03.
Mathias Creutz and Krista Lagus, 2005. Unsuper-
vised Morpheme Segmentation and Morphology In-
duction from Text Corpora Using Morfessor 1.0.
In Computer and Information Science, Report A81,
Helsinki University of Technology.
Sajib Dasgupta and Vincent Ng, 2007. High Perfor-
mance, Language-Independent Morphological Seg-
mentation. NAACL/HLT ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008a. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ?08.
Dmitry Davidov, Ari Rappoport, 2008b. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08.
Vera Demberg, 2007. A Language-Independent Un-
supervised Model for Morphological Segmentation.
ACL ?07.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ?05.
Samarth Keshava, Emily Pitler, 2006. A Simpler, Intu-
itive Approach to Morpheme Induction. In Proceed-
ings of 2nd Pascal Challenges Workshop, Venice,
Italy.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, Hany Hassan, 2003. Language Model
Based Arabic Word Segmentation. ACL ?03.
Moshe Levinger, Uzzi Ornan, Alon Itai, 1995. Learn-
ing Morpho-Lexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Comput.
Linguistics, 21:383:404.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Uzzi Ornan, 2005. Hebrew in Latin Script. Lesonenu,
LXIV:137:151 (in Hebrew).
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
?93.
Benjamin Snyder, Regina Bazilay, 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. ACL/HLT ?08.
Andreas Stolcke, 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. ICSLP, pages 901-904,
Denver, Colorado.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
44
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 3?11,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Sample Selection for Statistical Parsers: Cognitively Driven
Algorithms and Evaluation Measures
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Creating large amounts of manually annotated
training data for statistical parsers imposes
heavy cognitive load on the human annota-
tor and is thus costly and error prone. It
is hence of high importance to decrease the
human efforts involved in creating training
data without harming parser performance. For
constituency parsers, these efforts are tradi-
tionally evaluated using the total number of
constituents (TC) measure, assuming uniform
cost for each annotated item. In this paper, we
introduce novel measures that quantify aspects
of the cognitive efforts of the human annota-
tor that are not reflected by the TC measure,
and show that they are well established in the
psycholinguistic literature. We present a novel
parameter based sample selection approach
for creating good samples in terms of these
measures. We describe methods for global op-
timisation of lexical parameters of the sam-
ple based on a novel optimisation problem, the
constrained multiset multicover problem, and
for cluster-based sampling according to syn-
tactic parameters. Our methods outperform
previously suggested methods in terms of the
new measures, while maintaining similar TC
performance.
1 Introduction
State of the art statistical parsers require large
amounts of manually annotated data to achieve good
performance. Creating such data imposes heavy
cognitive load on the human annotator and is thus
costly and error prone. Statistical parsers are ma-
jor components in NLP applications such as QA
(Kwok et al, 2001), MT (Marcu et al, 2006) and
SRL (Toutanova et al, 2005). These often oper-
ate over the highly variable Web, which consists of
texts written in many languages and genres. Since
the performance of parsers markedly degrades when
training and test data come from different domains
(Lease and Charniak, 2005), large amounts of train-
ing data from each domain are required for using
them effectively. Thus, decreasing the human efforts
involved in creating training data for parsers without
harming their performance is of high importance.
In this paper we address this problem through
sample selection: given a parsing algorithm and a
large pool of unannotated sentences S, select a sub-
set S1 ? S for human annotation such that the hu-
man efforts in annotating S1 are minimized while
the parser performance when trained with this sam-
ple is maximized.
Previous works addressing training sample size
vs. parser performance for constituency parsers
(Section 2) evaluated training sample size using the
total number of constituents (TC). Sentences differ
in length and therefore in annotation efforts, and it
has been argued (see, e.g, (Hwa, 2004)) that TC re-
flects the number of decisions the human annotator
makes when syntactically annotating the sample, as-
suming uniform cost for each decision.
In this paper we posit that important aspects of
the efforts involved in annotating a sample are not
reflected by the TC measure. Since annotators ana-
lyze sentences rather than a bag of constituents, sen-
tence structure has a major impact on their cognitive
efforts. Sizeable psycholinguistic literature points
to the connection between nested structures in the
syntactic structure of a sentence and its annotation
efforts. This has motivated us to introduce (Sec-
tion 3) three sample size measures, the total and av-
3
erage number of nested structures of degree k in the
sample, and the average number of constituents per
sentence in the sample.
Active learning algorithms for sample selection
focus on sentences that are difficult for the parsing
algorithm when trained with the available training
data (Section 2). In Section 5 we show that active
learning samples contain a high number of complex
structures, much higher than their number in a ran-
domly selected sample that achieves the same parser
performance level. To avoid that, we introduce (Sec-
tion 4) a novel parameter based sample selection
(PBS) approach which aims to select a sample that
enables good estimation of the model parameters,
without focusing on difficult sentences. In Section 5
we show that the methods derived from our approach
select substantially fewer complex structures than
active learning methods and the random baseline.
We propose two different methods. In cluster
based sampling (CBS), we aim to select a sample
in which the distribution of the model parameters is
similar to their distribution in the whole unlabelled
pool. To do that we build a vector representation for
each sentence in the unlabelled pool reflecting the
distribution of the model parameters in this sentence,
and use a clustering algorithm to divide these vectors
into clusters. In the second method we use the fact
that a sample containing many examples of a certain
parameter yields better estimation of this parameter.
If this parameter is crucial for model performance
and the selection process does not harm the distri-
bution of other parameters, then the selected sam-
ple is of high quality. To select such a sample we
introduce a reduction between this selection prob-
lem and a variant of the NP-hard multiset-multicover
problem (Hochbaum, 1997). We call this problem
the constrained multiset multicover (CMM) problem,
and present an algorithm to approximate it.
We experiment (Section 5) with the WSJ Pen-
nTreebank (Marcus et al, 1994) and Collins? gen-
erative parser (Collins, 1999), as in previous work.
We show that PBS algorithms achieve good results
in terms of both the traditional TC measure (signifi-
cantly better than the random selection baseline and
similar to the results of the state of the art tree en-
tropy (TE) method of (Hwa, 2004)) and our novel
cognitively driven measures (where PBS algorithms
significantly outperform both TE and the random
baseline). We thus argue that PBS provides a way to
select a sample that imposes reduced cognitive load
on the human annotator.
2 Related Work
Previous work on sample selection for statistical
parsers applied active learning (AL) (Cohn and Lad-
ner, 1994) to corpora of various languages and syn-
tactic annotation schemes and to parsers of different
performance levels. In order to be able to compare
our results to previous work targeting high parser
performance, we selected the corpus and parser
used by the method reporting the best results (Hwa,
2004), WSJ and Collins? parser.
Hwa (2004) used uncertainty sampling with the
tree entropy (TE) selection function1 to select train-
ing samples for the Collins parser. In each it-
eration, each of the unlabelled pool sentences is
parsed by the parsing model, which outputs a list
of trees ranked by their probabilities. The scored
list is treated as a random variable and the sentences
whose variable has the highest entropy are selected
for human annotation. Sample size was measured
in TC and ranged from 100K to 700K WSJ con-
stituents. The initial size of the unlabelled pool was
800K constituents (the 40K sentences of sections 2-
21 of WSJ). A detailed comparison between the re-
sults of TE and our methods is given in Section 5.
The following works addressed the task of sam-
ple selection for statistical parsers, but in signifi-
cantly different experimental setups. Becker and
Osborne (2005) addressed lower performance lev-
els of the Collins parser. Their uncertainty sam-
pling protocol combined bagging with the TE func-
tion, achieving a 32% TC reduction for reaching a
parser f-score level of 85.5%. The target sample size
set contained a much smaller number of sentences
(?5K) than ours. Baldridge and Osborne (2004) ad-
dressed HPSG parse selection using a feature based
log-linear parser, the Redwoods corpus and commit-
tee based active learning, obtaining 80% reduction
in annotation cost. Their annotation cost measure
was related to the number of possible parses of the
sentence. Tang et al (2002) addressed a shallow
parser trained on a semantically annotated corpus.
1Hwa explored several functions in the experimental setup
used in the present work, and TE gave the best results.
4
They used an uncertainty sampling protocol, where
in each iteration the sentences of the unlabelled pool
are clustered using a distance measure defined on
parse trees to a predefined number of clusters. The
most uncertain sentences are selected from the clus-
ters, the training taking into account the densities of
the clusters. They reduced the number of training
sentences required for their parser to achieve its best
performance from 1300 to 400.
The importance of cognitively driven measures of
sentences? syntactic complexity has been recognized
by Roark et al (2007) who demonstrated their utility
for mild cognitive impairment diagnosis. Zhu et al
(2008) used a clustering algorithm for sampling the
initial labeled set in an AL algorithm for word sense
disambiguation and text classification. In contrast to
our CBS method, they proceeded with iterative un-
certainty AL selection. Melville et al (2005) used
parameter-based sample selection for a classifier in
a classic active learning setting, for a task very dif-
ferent from ours.
Sample selection has been applied to many NLP
applications. Examples include base noun phrase
chunking (Ngai, 2000), named entity recognition
(Tomanek et al, 2007) and multi?task annotation
(Reichart et al, 2008).
3 Cognitively Driven Evaluation Measures
While the resources, capabilities and constraints of
the human parser have been the subject of extensive
research, different theories predict different aspects
of its observed performance. We focus on struc-
tures that are widely agreed to impose a high cog-
nitive load on the human annotator and on theories
considering the cognitive resources required in pars-
ing a complete sentence. Based on these, we derive
measures for the cognitive load on the human parser
when syntactically annotating a set of sentences.
Nested structures. A nested structure is a parse
tree node representing a constituent created while
another constituent is still being processed (?open?).
The degree K of a nested structure is the number of
such open constituents. In this paper, we enumer-
ate the constituents in a top-down left-right order,
and thus when a constituent is created, only its an-
cestors are processed2. A constituent is processed
2A good review on node enumeration of the human parser
in given in (Abney and Johnson, 1991).
S
NP1
JJ
Last
NN
week
NP2
NNP
IBM
VP
VBD
bought
NP3
NNP
Lotus
Figure 1: An example parse tree.
until the processing of its children is completed. For
example, in Figure 1, when the constituent NP3 is
created, it starts a nested structure of degree 2, since
two levels of its ancestors (VP, S) are still processed.
Its parent (VP) starts a nested structure of degree 1.
The difficulty of deeply nested structures for the
human parser is well established in the psycholin-
guistics literature. We review here some of the vari-
ous explanations of this phenomenon; for a compre-
hensive review see (Gibson, 1998).
According to the classical stack overflow theory
(Chomsky and Miller, 1963) and its extension, the
incomplete syntactic/thematic dependencies theory
(Gibson, 1991), the human parser should track the
open structures in its short term memory. When the
number of these structures is too large or when the
structures are nested too deeply, the short term mem-
ory fails to hold them and the sentence becomes un-
interpretable.
According to the perspective shifts theory
(MacWhinney, 1982), processing deeply nested
structures requires multiple shifts of the annotator
perspective and is thus more difficult than process-
ing shallow structures. The difficulty of deeply
nested structured has been demonstrated for many
languages (Gibson, 1998).
We thus propose the total number of nested struc-
tures of degree K in a sample (TNSK) as a measure
of the cognitive efforts that its annotation requires.
The higher K is, the more demanding the structure.
Sentence level resources. In the psycholinguis-
tic literature of sentence processing there are many
theories describing the cognitive resources required
during a complete sentence processing. These re-
sources might be allocated during the processing of
a certain word and are needed long after its con-
stituent is closed. We briefly discuss two lines of
theory, focusing on their predictions that sentences
consisting of a large number of structures (e.g., con-
5
stituents or nested structures) require more cognitive
resources for longer periods.
Levelt (2001) suggested a layered model of the
mental lexicon organization, arguing that when one
hears or reads a sentence s/he activates word forms
(lexemes) that in turn activate lemma information.
The lemma information contains information about
syntactic properties of the word (e.g., whether it is
a noun or a verb) and about the possible sentence
structures that can be generated given that word. The
process of reading words and retrieving their lemma
information is incremental and the lemma informa-
tion for a given word is used until its syntactic struc-
ture is completed. The information about a word in-
clude all syntactic predictions, obligatory (e.g., the
prediction of a noun following a determiner) and op-
tional (e.g., optional arguments of a verb, modifier
relationships). This information might be relevant
long after the constituents containing the word are
closed, sometimes till the end of the sentence.
Another line of research focuses on working
memory, emphasizing the activation decay princi-
ple. It stresses that words and structures perceived
during sentence processing are forgotten over time.
As the distance between two related structures in a
sentence grows, it is more demanding to reactivate
one when seeing the other. Indeed, supported by
a variety of observations, many of the theories of
the human parser (see (Lewis et al, 2006) for a sur-
vey) predict that processing items towards the end of
longer sentences should be harder, since they most
often have to be integrated with items further back.
Thus, sentences with a large number of structures
impose a special cognitive load on the annotator.
We thus propose to use the number of structures
(constituents or nested structures) in a sentence as a
measure of its difficulty for human annotation. The
measures we use for a sample (a sentence set) are the
average number of constituents (AC) and the aver-
age number of nested structures of degree k (ANSK)
per sentence in the set. Higher AC or ANSK values
of a set imply higher annotation requirements3.
Pschycolinguistics research makes finer observa-
3The correlation between the number of constituents and
sentence length is very strong (e.g., correlation coefficient of
0.93 in WSJ section 0). We could use the number of words, but
we prefer the number of structures since the latter better reflects
the arguments made in the literature.
tions about the human parser than those described
here. A complete survey of that literature is beyond
the scope of this paper. We consider the proposed
measures a good approximation of some of the hu-
man parser characteristics.
4 Parameter Based Sampling (PBS)
Our approach is to sample the unannotated pool with
respect to the distribution of the model parameters
in its sentences. In this paper, in order to compare to
previous works, we apply our methods to the Collins
generative parser (Collins, 1999). For any sentence
s and parse tree t it assigns a probability p(s, t),
and finds the tree for which this probability is maxi-
mized. To do that, it writes p(t, s) as a product of the
probabilities of the constituents in t and decomposes
the latter using the chain rule. In simplified notation,
it uses:
p(t, s) =
?
P (S1 ? S2 . . . Sn) =
?
P (S1)?. . .?P (Sn|?(S1 . . . Sn))
(1)
We refer to the conditional probabilities as the model
parameters.
Cluster Based Sampling (CBS). We describe
here a method for sampling subsets that leads to a
parameter estimation that is similar to the parame-
ter estimation we would get if annotating the whole
unannotated set.
To do that, we randomly select M sentences from
the unlabelled pool N , manually annotate them,
train the parser with these sentences and parse the
rest of the unlabelled pool (G = N ? M ). Using
this annotation we build a syntactic vector repre-
sentation for each sentence in G. We then cluster
these sentences and sample the clusters with respect
to their weights to preserve the distribution of the
syntactic features. The selected sentences are man-
ually annotated and combined with the group of M
sentences to train the final parser. The size of this
combined sample is measured when the annotation
efforts are evaluated.
Denote the left hand side nonterminal of a con-
stituent by P and the unlexicalized head of the con-
stituent by H . The domain of P is the set of non-
terminals (excluding POS tags) and the domain of H
is the set of nonterminals and POS tags of WSJ. In
all the parameters of the Collins parser P and H are
conditioned upon. We thus use (P,H) pairs as the
6
features in the vector representation of each sentence
in G. The i-th coordinate is given by the equation:
?
c?t(s)
?
i
Fi(Q(c) == i) ? L(c) (2)
Where c are the constituents of the sentence parse
t(s), Q is a function that returns the (P,H) pair
of the constituent c, Fi is a predicate that returns 1
iff it is given pair number i as an argument and 0
otherwise, and L is the number of modifying non-
terminals in the constituent plus 1 (for the head),
counting the number of parameters that condition
on (P,H). Following equation (2), the ith coordi-
nate of the vector representation of a sentence in G
contains the number of parameters that will be cal-
culated conditioned on the ith (P,H) pair.
We use the k-means clustering algorithm, with the
L2 norm as a distance metric (MacKay, 2002), to di-
vide vectors into clusters. Clusters created by this
algorithm contain adjacent vectors in a Euclidean
space. Clusters represent sentences with similar fea-
tures values. To initialize k-means, we sample the
initial centers values from a uniform distribution
over the data points.
We do not decide on the number of clusters in ad-
vance but try to find inherent structure in the data.
Several methods for estimating the ?correct? num-
ber of clusters are known (Milligan and Cooper,
1985). We used a statistical heuristic called the
elbow test. We define the ?within cluster disper-
sion? Wk as follows. Suppose that the data is di-
vided into k clusters C1 . . . Ck with |Cj | points in
the jth cluster. Let Dt = ?i,j?Ct di,j where
di,j is the squared Euclidean distance, then Wk :=?k
t=1
1
2|Ct|Dt. Wk tends to decrease monotonically
as k increases. In many cases, from some k this de-
crease flattens markedly. The heuristic is that the
location of such an ?elbow? indicates the appropriate
number of clusters. In our experiments, an obvious
elbow occurred for 15 clusters.
ki sentences are randomly sampled from each
cluster, ki = D |Ci|?
j |Cj |
, where D is the number
of sentences to be sampled from G. That way we
ensure that in the final sample each cluster is repre-
sented according to its size.
CMM Sampling. All of the parameters in the
Collins parser are conditioned on the constituent?s
head word. Since word statistics are sparse, sam-
pling from clusters created according to a lexical
vector representation of the sentences does not seem
promising4.
Another way to create a sample from which the
parser can extract robust head word statistics is to
select a sample containing many examples of each
word. More formally, we denote the words that oc-
cur in the unlabelled pool at least t times by t-words,
where t is a parameter of the algorithm. We want to
select a sample containing at least t examples of as
many t-words as possible.
To select such a sample we introduce a novel op-
timisation problem. Our problem is a variant of the
multiset multicover (MM) problem, which we call
the constrained multiset multicover (CMM) prob-
lem. The setting of the MM problem is as fol-
lows (Hochbaum, 1997): Given a set I of m ele-
ments to be covered each bi times, a collection of
multisets Sj ? I , j ? J = {1, . . . , n} (a multiset is
a set in which members? multiplicity may be greater
than 1), and weights wj , find a subcollection C of
multisets that covers each i ? I at least bi times, and
such that
?
j?C wj is minimized.
CMM differs from MM in that in CMM the sum
of the weights (representing the desired number of
sentences to annotate) is bounded, while the num-
ber of covered elements (representing the t-words)
should be maximized. In our case, I is the set of
words that occur at least t times in the unlabelled
pool, bi = t,?i ? I , the multisets are the sentences
in that pool and wj = 1,?j ? J .
Multiset multicover is NP-hard. However, there is
a good greedy approximation algorithm for it. De-
fine a(sj , i) = min(R(sj , i), di), where di is the
difference between bi and the number of instances
of item i that are present in our current sample, and
R(sj , i) is the multiplicity of the i-th element in the
multiset sj . Define A(sj) to be the multiset contain-
ing exactly a(sj , i) copies of any element i if sj is
not already in the set cover and the empty set if it
is. The greedy algorithm repeatedly adds a set mini-
mizing wj|A(sj)| . This algorithm provenly achieves an
approximation ratio between ln(m) and ln(m) + 1.
In our case all weights are 1, so the algorithm would
4We explored CBS with several lexical features schemes and
got only marginal improvement over random selection.
7
simply add the sentence that maximizes A(sj) to the
set cover.
The problem in directly applying the algorithm to
our case is that it does not take into account the de-
sired sample size. We devised a variant of the algo-
rithm where we use a binary tree to ?push? upwards
the number of t-words in the whole batch of unan-
notated sentences that occurs at least t times in the
selected one. Below is a detailed description. D de-
notes the desired number of items to sample.
The algorithm has two steps. First, we iter-
atively sample (without replacement) D multisets
(sentences) from a uniform distribution over the
multisets. In each iteration we calculate for the se-
lected multiset its ?contribution? ? the number of
items that cross the threshold of t occurrences with
this multiset minus the number of items that cross
the t threshold without this multiset (i.e. the contri-
bution of the first multiset is the number of t-words
occurring more than t times in it). For each multiset
we build a node with a key that holds its contribu-
tion, and insert these nodes in a binary tree. Inser-
tion is done such that all downward paths are sorted
in decreasing order of key values.
Second, we iteratively sample (from a uniform
distribution, without replacement) the rest of the
multisets pool. For each multiset we perform two
steps. First, we prepare a node with a key as de-
scribed above. We then randomly choose Z leaves5
in the binary tree (if the number of leaves is smaller
than Z all of the leaves are chosen). For each leaf we
find the place of the new node in the path from the
root to the leaf (paths are sorted in decreasing order
of key values). We insert the new node to the high-
est such place found (if the new key is not smaller
than the existing paths), add its multiset to the set of
selected multisets, and remove the multiset that cor-
responds to the leaf of this path from the batch and
the leaf itself from the binary tree. We finally choose
the multisets that correspond to the highest D nodes
in the tree.
An empirical demonstration of the quality of ap-
proximation that the algorithm provides is given in
Figure 2. We ran our algorithm with the threshold
parameter set to t ? [2, 14] and counted the num-
5We tried Z values from 10 to 100 in steps of 10 and ob-
served very similar results. We report results for Z = 100.
0 100 200 300 400 500 600 700 800
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
number of training constituents (thousands)
n
u
m
be
r  
of
  t
?w
or
ds
t=2
t=8
t=11
t=14
t=5
random
Figure 2: Number of t-words for t = 5 in samples selected
by CMM runs with different values of the threshold pa-
rameter t and in a randomly selected sample. CMM with
t = 5 is significantly higher. All the lines except for the
line for t = 5 are unified. For clarity, we do not show all t
values: their curves are also similar to the t 6= 5 lines.
Method 86% 86.5% 87% 87.5% 88%
TE 16.9% 27.1% 26.9% 14.8% 15.8%
(152K) (183K) (258K) (414K) (563 K)
CBS 19.6% 16.8% 19% 21.1% 9%
(147K) (210K) (286K) (382K) (610K)
CMM 9% 10.4% 8.9% 10.3% 14%
(167K) (226K) (312K) (433K) (574K)
Table 1: Reduction in annotation cost in TC terms com-
pared to the random baseline for tree entropy (TE), syn-
tactic clustering (CBS) and CMM. The compared samples
are the smallest samples selected by each of the methods
that achieve certain f-score levels. Reduction is calcu-
lated by: 100 ? 100 ? (TCmethod/TCrandom).
ber of words occurring at least 5 times in the se-
lected sample. We followed the same experimen-
tal protocol as in Section 5. The graph shows that
the number of words occurring at least 5 times in a
sample selected by our algorithm when t = 5 is sig-
nificantly higher (by about a 1000) than the number
of such words in a randomly selected sample and in
samples selected by our algorithm with other t pa-
rameter values. We got the same pattern of results
when counting words occurring at least t times for
the other values of the t parameter ? only the run of
the algorithm with the corresponding t value created
a sample with significantly higher number of words
not below threshold. The other runs and random se-
lection resulted in samples containing significantly
lower number of words not below threshold.
In Section 5 we show that the parser performance
when it is trained with a sample selected by CMM
is significantly better than when it is trained with a
randomly selected sample. Improvement is similar
across the t parameter values.
8
86% 87% 88%
Method TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK TNSK TNSK ANSK ANSK
(1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22) (1-6) (7-22)
TE 34.9% 3.6% - 8.9% - 61.3% 42.2% 14.4% - 9.9% - 62.7% 25% 8.1% - 6.3% - 30%
CBS 21.3% 18.6% - 0.5% - 3.5% 19.6% 24.2% - 0.3% - 1.8% 8.9% 8.6 % 0% - 0.3%
CMM 10.18% 8.87% -0.82% -3.39% 11% 16.22% -0.34% -1.8% 14.65% 14.11% -0.02% - 0.08%
Table 2: Annotation cost reduction in TNSK and ANSK compared to the random baseline for tree entropy (TE), syntactic
clustering (CBS) and CMM. The compared samples are the smallest selected by each of the methods that achieve certain
f-score levels. Each column represents the reduction in total or average number of structures of degree 1?6 or 7?22.
Reduction for each measure is calculated by: 100? 100? (measuremethod/measurerandom). Negative reduction
is an addition. Samples with a higher reduction in a certain measure are better in terms of that measure.
0 5 10 15 20 25?1
0
1
2
3
x 104
K
T
N
S
K
 (K
)
 
 
CMM(t=8) ? TE
CBS ? TE
0 line
0 5 10 15 20 251
1.1
1.2
1.3
1.4
1.5
1.6
1.7
K
A
N
S
K
 m
e
th
o
d
/A
N
S
K
 r
a
n
d
o
m
 
 
TE
CMM,t=8
CBS
86  86.5 87  87.5 88  18
20
22
24
26
28
F score
A
v
e
ra
g
e
 n
u
m
b
e
r 
o
f 
c
o
n
s
ti
tu
e
n
ts
 
 
TE
CMM,  t = 8
CBS
0 1 2 3 3.5
x 104
1  
1.25
1.5
Number  of  sentences
A
C
 m
e
th
o
d
/A
C
 r
a
n
d
o
m
 
 
TE
CMM, t=8
CBS
Figure 3: Left to right: First: The difference between the number of nested structures of degree K of CMM and TE and
of CBS and TE. The curves are unified. The 0 curve is given for reference. Samples selected by CMM and CBS have
more nested structures of degrees 1?6 and less nested structures of degrees 7?22. Results are presented for the smallest
samples required for achieving f-score of 88. Similar patterns are observed for other f-score values. Second: Average
number of nested structures of degree K as a function of K for the smallest sample required for achieving f-score of
88. Results for each of the methods are normalized by the average number of nested structures of degree K in the
smallest randomly selected sample required for achieving f-score of 88. The sentences in CMM and CBS samples are
not more complex than sentences in a randomly selected sample. In TE samples sentences are more complex. Third:
Average number of constituents (AC) for the smallest sample of each of the methods that is required for achieving a
given f-score. CMM and CBS samples contain sentences with a smaller number of constituents. Fourth: AC values for
the samples created by the methods (normalized by AC values of a randomly selected sample). The sentences in TE
samples, but not in CMM and CBS samples, are more complex than sentences in a randomly selected sample.
5 Results
Experimental setup. We used Bikel?s reimplemen-
tation of Collins? parsing model 2 (Bikel, 2004).
Sections 02-21 and 23 of the WSJ were stripped
from their annotation. Sections 2-21 (39832 sen-
tences, about 800K constituents) were used for train-
ing, Section 23 (2416 sentences) for testing. No
development set was used. We used the gold stan-
dard POS tags in two cases: in the test section (23)
in all experiments, and in Sections 02-21 in the
CBS method when these sections are to be parsed
in the process of vector creation. In active learn-
ing methods the unlabelled pool is parsed in each
iteration and thus should be tagged with POS tags.
Hwa (2004) (to whom we compare our results) used
the gold standard POS tags for the same sections
in her work6. We implemented a random baseline
6Personal communication with Hwa. Collins? parser uses an
where sentences are uniformly selected from the un-
labelled pool for annotation. For reliability we re-
peated each experiment with the algorithms and the
random baseline 10 times, each time with different
random selections (M sentences for creating syntac-
tic tagging and k-means initialization for CBS, sen-
tence order in CMM), and averaged the results.
Each experiment contained 38 runs. In each run
a different desired sample size was selected, from
1700 onwards, in steps of 1000. Parsing perfor-
mance is measured in terms of f-score
Results. We compare the performance of our
CBS and CMM algorithms to the TE method (Hwa,
2004)7, which is the only sample selection work ad-
input POS tag only if it cannot tag its word using the statistics
learned from the training set.
7Hwa has kindly sent us the samples selected by her TE. We
evaluated these samples with TC and the new measures. The TC
of the minimal sample she sent us needed for achieving f-score
9
dressing our experimental setup. Unless otherwise
stated, we report the reduction in annotation cost:
100? 100? (measuremethod/measurerandom).
CMM results are very similar for t ? {2, 3, . . . , 14},
and presented for t = 8.
Table 1 presents reduction in annotation cost in
TC terms. CBS achieves greater reduction for f =
86, 87.5, TE for f = 86.5, 87, 88. For f = 88, TE
and CMM performance are almost similar. Examin-
ing the f-score vs. TC sample size over the whole
constituents range (not shown due to space con-
straints) reveals that CBS, CMM and TE outperform
random selection over the whole range. CBS and
TE performance are quite similar with TE being bet-
ter in the ranges of 170?300K and 520?650K con-
stituents (42% of the 620K constituents compared)
and CBS being better in the ranges of 130?170K and
300?520K constituents (44% of the range). CMM
performance is worse than CBS and TE until 540K
constituents. From 650K constituents on, where
the parser achieves its best performance, the perfor-
mance of CMM and TE methods are similar, outper-
forming CBS.
Table 2 shows the annotation cost reduction in
ANSK and TNSK terms. TE achieves remarkable
reduction in the total number of relatively shallow
structures (TNSK K = 1?6). Our methods, in con-
trast, achieve remarkable reduction in the number of
deep structures (TNSK K = 7?22)8. This is true for
all f-score values. Moreover, the average number of
nested structures per sentence, for every degree K
(ANSK for every K) in TE sentences is much higher
than in sentences of a randomly selected sample. In
samples selected by our methods, the ANSK values
are very close to the ANSK values of randomly se-
lected samples. Thus, sentences in TE samples are
much more complex than in CBS and CMM samples.
The two leftmost graphs in Figure 3 demonstrate
(for the minimal samples required for f-score of 88)
that these reductions hold for each K value (ANSK)
and for each K ? [7, 22] (TNSK) not just on the av-
of 88 is different from the number reported in (Hwa, 2004). We
compare our TC results with the TC result in the sample sent us
by Hwa.
8We present results where the border between shallow and
deep structures is set to be Kborder = 6. For every Kborder ?
{7, 8, . . . , 22} TNSK reductions with CBS and CMM are much
more impressive than with TE for structures whose degree is
K ? [Kborder, 22].
erage over these K values. We observed similar re-
sults for other f-score values.
The two rightmost graphs of Figure 3 demon-
strates AC results. The left of them shows that for
every f-score value, the AC measure of the minimal
TE sample required to achieve that f-score is higher
than the AC value of PBS samples (which are very
similar to the AC values of randomly selected sam-
ples). The right graph demonstrates that for every
sample size, the AC value of TE samples is higher
than that of PBS samples.
All AL based previous work (including TE) is it-
erative. In each iteration thousands of sentences
are parsed, while PBS algorithms perform a single
iteration. Consequently, PBS computational com-
plexity is dramatically lower. Empirically, using a
Pentium 4 2.4GHz machine, CMM requires about an
hour and CBS about 16.5 hours, while the TE parsing
steps alone take 662 hours (27.58 days).
6 Discussion and Future Work
We introduced novel evaluation measures: AC,
TNSK and ANSK for the task of sample selection
for statistical parsers. Based on the psycholinguis-
tic literature we argue that these measures reflect as-
pects of the cognitive efforts of the human annota-
tor that are not reflected by the traditional TC mea-
sure. We introduced the parameter based sample se-
lection (PBS) approach and its CMM and CBS algo-
rithms that do not deliberately select difficult sen-
tences. Therefore, our intuition was that they should
select a sample that leads to an accurate parameter
estimation but does not contain a high number of
complex structures. We demonstrated that CMM and
CBS achieve results that are similar to the state of the
art TE method in TC terms and outperform it when
the cognitively driven measures are considered.
The measures we suggest do not provide a full
and accurate description of human annotator efforts.
In future work we intend to extend and refine our
measures and to revise our algorithms accordingly.
We also intend to design stopping criteria for the
PBS methods. These are criteria that decide when
the selected sample suffices for the parser best per-
formance and further annotation is not needed.
10
References
Steven Abney and Mark Johnson, 1991. Memory re-
quirements and local ambiguities of parsing strategies.
Psycholinguistic Research, 20(3):233?250.
Daniel M. Biken, 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Jason Baldridge and Miles Osborne, 2004. Active learn-
ing and the total cost of annotation. EMNLP ?04.
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI 05.
Markus Becker, 2008. Active learning ? an explicit treat-
ment of unreliable parameters. Ph.D. thesis, The Uni-
versity of Edinburgh.
Noam Chomsky and George A. Miller, 1963. Fini-
tary models of language users. In R. Duncan Luce,
Robert R. Bush, and Eugene Galanter, editors, Hand-
book of Mathematical Psychology, volume II. John
Wiley, New York, 419?491.
David Cohn, Les Atlas and Richard E. Ladner, 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?221.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Edward Gibson, 1991. A computational theory of hu-
man linguistic processing: memory limitations and
processing breakdowns. Ph.D. thesis, Carnegie Mel-
lon University, Pittsburg, PA.
Edward Gibson, 1998. Linguistic complexity: locality
of syntactic dependencies. Cognition, 68:1?76.
Dorit Hochbaum (ed), 1997. Approximation algorithms
for NP-hard problems. PWS Publishing, Boston.
Rebecca Hwa, 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Cody Kwok, Oren Etzioni, Daniel S. Weld, 2001. Scal-
ing question answering to the Web. WWW ?01.
Matthew Lease and Eugene Charniak, 2005. Parsing
biomedical literature. IJCNLP ?05.
Willem J.M. Levelt, 2001. Spoken word production: A
theory of lexical access. PNAS, 98(23):13464?13471.
Richard L. Lewis, Shravan Vasishth and Julie Van Dyke,
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ence, October:447-454.
David MacKay, 2002. Information theory, inference and
learning algorithms. Cambridge University Press.
Brian MacWhinney, 1982. The competition model. In
B. MacWhinney, editor, Mechanisms of language ac-
quisition. Hillsdale, NJ: Lawrence Erlbaum, 249?308.
Daniel Marcu, Wei Wang, Abdessamabad Echihabi, and
Kevin Knight, 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
EMNLP ?06.
P. Melville, M. Saar-Tsechansky, F. Provost and R.J.
Mooney, 2005. An expected utility approach to ac-
tive feature-value acquisition. 5th IEEE Intl. Conf. on
Data Mining ?05.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz, 1994. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics, 19(2):313?330.
G.W. Milligan and M.C Cooper, 1985. An examination
of procedures for determining the number of clusters
in a data set. Psychometrika, 58(2):159?157.
Grace Ngai and David Yarowski, 2000. Rule writing
or annotation: cost?efficient resource usage for base
noun phrase chunking. ACL ?00.
Roi Reichart, Katrin Tomanek, Udo Hahn and Ari Rap-
poport, 2008. Multi-task active learning for linguistic
annotations. ACL ?08.
Brian Roark, Margaret Mitchell and Kristy Hollingshead,
2007. Syntactic complexity measures for detecting
mild cognitive impairment. BioNLP workshop, ACL
?07.
Min Tang, Xiaoqiang Luo, and Salim Roukos, 2002. Ac-
tive learning for statistical natural language parsing.
ACL ?02.
Katrin Tomanek, Joachim Wermtre, and Udo Hahn,
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. EMNLP ?07.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning, 2005. Joint learning improves semantic role
labeling. ACL ?05.
Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Benjamin
K. Tsou, 2008. Active learning with sampling by un-
certainty and density for word sense disambiguation
and text classification. COLING ?08.
11
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 48?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Superior and Efficient Fully Unsupervised Pattern-based Concept
Acquisition Using an Unsupervised Parser
Dmitry Davidov1 Roi Reichart1 Ari Rappoport2
1ICNC , 2Institute of Computer Science
Hebrew University of Jerusalem
{dmitry@alice.nc|roiri@cs|arir@cs}.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental for linguistics and NLP. Unsuper-
vised concept acquisition algorithms have
been shown to produce good results, and are
preferable over manual preparation of con-
cept resources, which is labor intensive, er-
ror prone and somewhat arbitrary. Some ex-
isting concept mining methods utilize super-
vised language-specific modules such as POS
taggers and computationally intensive parsers.
In this paper we present an efficient fully
unsupervised concept acquisition algorithm
that uses syntactic information obtained from
a fully unsupervised parser. Our algorithm
incorporates the bracketings induced by the
parser into the meta-patterns used by a sym-
metric patterns and graph-based concept dis-
covery algorithm. We evaluate our algorithm
on very large corpora in English and Russian,
using both human judgments and WordNet-
based evaluation. Using similar settings as
the leading fully unsupervised previous work,
we show a significant improvement in con-
cept quality and in the extraction of multiword
expressions. Our method is the first to use
fully unsupervised parsing for unsupervised
concept discovery, and requires no language-
specific tools or pattern/word seeds.
1 Introduction
Comprehensive lexical resources for many domains
and languages are essential for most NLP applica-
tions. One of the most utilized types of such re-
sources is a repository of concepts: sets of lexical
items sharing a significant aspect of their meanings
(e.g., types of food, tool names, etc).
While handcrafted concept databases (e.g., Word-
Net) are extensively used in NLP, manual compila-
tion of such databases is labor intensive, error prone,
and somewhat arbitrary. Hence, for many languages
and domains great efforts have been made for au-
tomated construction of such databases from avail-
able corpora. While language-specific and domain-
specific studies show significant success in develop-
ment of concept discovery frameworks, the majority
of domains and languages remain untreated. Hence
there is a need for a framework that performs well
for many diverse settings and is as unsupervised and
language-independent as possible.
Numerous methods have been proposed for seed-
based concept extraction where a set of concept pat-
terns (or rules), or a small set of seed words for each
concept, is provided as input to the concept acqui-
sition system. However, even simple definitions for
concepts are not always available.
To avoid requiring this type of input, a number of
distributional and pattern-based methods have been
proposed for fully unsupervised seed-less acquisi-
tion of concepts from text. Pattern-based algorithms
were shown to obtain high quality results while be-
ing highly efficient in comparison to distributional
methods. Such fully unsupervised methods do not
incorporate any language-specific parsers or taggers,
so can be successfully applied to diverse languages.
However, unsupervised pattern-based methods
suffer from several weaknesses. Thus they are fre-
quently restricted to single-word terms and are un-
able to discover multiword expressions in efficient
and precise manner. They also usually ignore poten-
tially useful part-of-speech and other syntactic in-
formation. In order to address these weaknesses,
several studies utilize language-specific parsing or
48
tagging systems in concept acquisition. Unfortu-
nately, while improving results, this heavily affects
the language- and domain- independence of such
frameworks, and severely impacts efficiency since
even shallow parsing is computationally demanding.
In this paper we present a method to utilize the in-
formation induced by unsupervised parsers in an un-
supervised pattern-based concept discovery frame-
work. With the recent development of fast fully un-
supervised parsers, it is now possible to add parser-
based information to lexical patterns while keep-
ing the language-independence of the whole frame-
work and still avoiding heavy computational costs.
Specifically, we incorporate the bracketings induced
by the parser into the meta-patterns used by a sym-
metric patterns and graph-based unsupervised con-
cept discovery algorithm.
We performed a thorough evaluation on two En-
glish corpora (the BNC and a 68GB web corpus)
and on a 33GB Russian corpus. Evaluations were
done using both human judgments and WordNet, in
similar settings as that of the leading unsupervised
previous work. Our results show that utilization of
unsupervised parser both improves the assignment
of single-word terms to concepts and allows high-
precision discovery and assignment of of multiword
expressions to concepts.
2 Previous Work
Much work has been done on lexical acquisition of
all sorts and the acquisition of concepts in particu-
lar. Concept acquisition methods differ in the type of
corpus annotation and other human input used, and
in their basic algorithmic approach. Some methods
directly aim at concept acquisition, while the direct
goal in some is the construction of hyponym (?is-a?)
hierarchies. A subtree in such a hierarchy can be
viewed as defining a concept.
A major algorithmic approach is to represent
word contexts as vectors in some space and use dis-
tributional measures and clustering in that space.
Pereira (1993), Curran (2002) and Lin (1998) use
syntactic features in the vector definition. (Pantel
and Lin, 2002) improves on the latter by clustering
by committee. Caraballo (1999) uses conjunction
and appositive annotations in the vector representa-
tion. Several studies avoid requiring any syntactic
annotation. Some methods are based on decompo-
sition of a lexically-defined matrix (by SVD, PCA
etc), e.g. (Schu?tze, 1998; Deerwester et al, 1990).
While great effort has been made for improv-
ing the computational complexity of distributional
methods (Gorman and Curran, 2006), they still re-
main highly computationally intensive in compari-
son to pattern approaches (see below), and most of
them do not scale well for very large datasets.
The second main approach is to use lexico-
syntactic patterns. Patterns have been shown to pro-
duce more accurate results than feature vectors, at
a lower computational cost on large corpora (Pan-
tel et al, 2004). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns, nu-
merous pattern-based methods have been proposed
for the discovery of concepts from seeds. Other
studies develop concept acquisition for on-demand
tasks where concepts are defined by user-provided
seeds. Many of these studies utilize information ob-
tained by language-specific parsing and named en-
tity recognition tools (Dorow et al, 2005). Pantel et
al. (2004) reduce the depth of linguistic data used,
but their method requires POS tagging.
TextRunner (Banko et al, 2007) utilizes a set
of pattern-based seed-less strategies in order to ex-
tract relational tuples from text. However, this sys-
tem contains many language-specific modules, in-
cluding the utilization of a parser in one of the pro-
cessing stages. Thus the majority of the existing
pattern-based concept acquisition systems rely on
pattern/word seeds or supervised language-specific
tools, some of which are very inefficient.
Davidov and Rappoport (2006) developed a
framework which discovers concepts based on high
frequency words and symmetry-based pattern graph
properties. This framework allows a fully unsuper-
vised seed-less discovery of concepts without rely-
ing on language-specific tools. However, it com-
pletely ignores potentially useful syntactic or mor-
phological information.
For example, the pattern ?X and his Y? is useful
for acquiring the concept of family member types,
as in ?his siblings and his parents?. Without syn-
tactic information, it can capture noise, as in ?... in
ireland) and his wife)? (parentheses denote syntac-
tic constituent boundaries). As another example, the
useful symmetric pattern ?either X or Y? can appear
in both good examples (?choose either Chihuahua
49
or Collie.?) and bad ones (?either Collie or Aus-
tralian Bulldog?). In the latter case, the algorithm
both captures noise (?Australlian? is now consid-
ered as a candidate for the ?dog type? concept), and
misses the discovery of a valid multiword candidate
(?Australlian Bulldog?). While symmetry-based fil-
tering greatly reduces such noise, the basic problem
remains. As a result, incorporating at least some
parsing information in a language-independent and
efficient manner could be beneficial.
Unsupervised parsing has been explored for sev-
eral decades (see (Clark, 2001; Klein, 2005) for re-
cent reviews). Recently, unsupervised parsers have
for the first time outperformed the right branch-
ing heuristic baseline for English. These include
CCM (Klein and Manning, 2002), the DMV and
DMV+CCM models (Klein and Manning, 2004),
(U)DOP based models (Bod, 2006a; Bod, 2006b;
Bod, 2007), an exemplar based approach (Den-
nis, 2005), guiding EM using contrastive estimation
(Smith and Eisner, 2006), and the incremental parser
of Seginer (2007) which we use here. These works
learn an unlabeled syntactic structure, dependency
or constituency. In this work we use constituency
trees as our syntactic representation.
Another important factor in concept acquisition
is the source of textual data used. To take advan-
tage of the rapidly expanding web, many of the pro-
posed frameworks utilize web queries rather than
local corpora (Etzioni et al, 2005; Davidov et al,
2007; Pasca and Van Durme, 2008; Davidov and
Rappoport, 2009). While these methods have a defi-
nite practical advantage of dealing with the most re-
cent and comprehensive data, web-based evaluation
has some methodological drawbacks such as limited
repeatability (Kilgarriff, 2007). In this study we ap-
ply our framework on offline corpora in settings sim-
ilar to that of previous work, in order to be able to
make proper comparisons.
3 Efficient Unsupervised Parsing
Our method utilizes the information induced by un-
supervised parsers. Specifically, we make use of the
bracketings induced by Seginer?s parser1 (Seginer,
2007). This parser has advantages in three major as-
1The parser is freely available at
http://staff.science.uva.nl/?yseginer/ccl
pects relevant to this paper.
First, it achieves state of the art unsupervised
parsing performance: its F-score2 is 75.9% for sen-
tences of up to 10 words from the PennTreebank
Wall Street Journal corpus (WSJ) (Marcus, 1993),
and 59% for sentences of the same length from the
German NEGRA (Brants, 1997) corpus. These cor-
pora consists of newspaper texts.
Second, to obtain good results, manually created
POS tags are used as input in all the unsupervised
parsers mentioned above except of Seginer?s, which
uses raw sentences as input. (Headden et al, 2008)
have shown that the performance of algorithms that
require POS tags substantially decreases when using
POS tags induced by unsupervised POS taggers in-
stead of manually created ones. Seginer?s incremen-
tal parser is therefore the only fully unsupervised
parser providing high quality parses.
Third, Seginer?s parser is extremely fast. During
its initial stage, the parser builds a lexicon. Our Pen-
tium 2.8GHB machines with 4GHB RAM can store
in memory the lexicon created by up to 0.2M sen-
tences. We thus divided our corpora to batches of
0.2M sentences and parsed each of them separately.
Note that in this setup parsing quality might be even
better than the quality reported in (Seginer, 2007),
since in the setup reported in that paper the parser
was applied to a few thousand sentences only. On
average, the parsing time of a single batch was 5
minutes (run time did not significantly differ across
batches and corpora).
Parser description. The parser utilizes the novel
common-cover link representation for syntactic
structure. This representation resembles depen-
dency structure but unlike the latter, it can be trans-
lated into a constituency tree, which is the syntactic
representation we use in this work.
The parsing algorithm creates the common-cover
links structure of a sentence in an incremental man-
ner. This means that the parser reads the words of
a sentence one after the other and, as each word is
read, it is only allowed to add links that have one of
their ends at that words (and update existing ones).
Words which have not yet been read are not avail-
2F = 2?R?PR+P , where R and P are the recall and precision of
the parsers? bracketing compared to manually created bracket-
ing of the same text. This is the accepted measure for parsing
performance (see (Klein, 2005)).
50
able to the parser at this stage. This restriction is
inspired by psycholinguistics research which sug-
gests that humans process language incrementally.
This results in a significant restriction of the parser?s
search space, which is the reason it is so fast.
During its initial stage the parser builds a lexicon
containing, for each word, statistics helping the deci-
sion of whether to link that word to other words. The
lexicon is updated as any new sentence is read. Lex-
icon updating is also done in an incremental manner
so this stage is also very fast.
4 Unsupervised Pattern Discovery
In the first stage of our algorithm, we run the unsu-
pervised parser on the corpus in order to produce a
bracketing structure for each sentence. In the sec-
ond stage, described here, we use these bracketings
in order to discover, in a fully unsupervised manner,
patterns that could be useful for concept mining.
Our algorithm is based on the concept acquisition
method of (Davidov and Rappoport, 2006). We dis-
cover patterns that connect terms belonging to the
same concept in two main stages: discovery of pat-
tern candidates, and identification of the symmetric
patterns among the candidates.
Pattern candidates. A major idea of (Davidov
and Rappoport, 2006) is that a few dozen high fre-
quency words (HFW) such as ?and? and ?is? con-
nect other, less frequent content terms into relation-
ships. They define meta-patterns, which are short
sequences of H?s and C?s, where H is a slot for
a HFW and C is a slot for a content word (later
to become a word belonging to a discovered con-
cept). Their method was shown to produce good
results. However, the fact that it does not consider
any syntactic information causes problems. Specif-
ically, it does not consider the constituent structure
of the sentence. Meta-patterns that cross constituent
boundaries are likely to generate noise ? two content
words (C?s) in a meta-pattern that belong to differ-
ent constituents are likely to belong to different con-
cepts as well. In addition, meta-patterns that do not
occupy a full constituent are likely to ?cut? multi-
word expressions (MWEs) into two parts, one part
that gets treated as a valid C word and one part that
is completely ignored.
The main idea in the present paper is to use the
bracketings induced by unsupervised parsers in or-
der to avoid the problems above. We utilize brack-
eting boundaries in our meta-patterns in addition
to HFW and C slots. In other words, their origi-
nal meta-patterns are totally lexical, while ours are
lexico-syntactic meta-patterns. We preserve the at-
tractive properties of meta-patterns, because both
HFWs and bracketings can be found or computed in
a language independent manner and very efficiently.
Concretely, we define a HFW as a word appearing
more than TH times per million words, and a C as
a word or multiword expression containing up to 4
words, appearing less than TC times per million.
We require that our patterns include two slots for
C?s, separated by at least a single HFW or bracket.
We allow separation by a single bracket because the
lowest level in the induced bracketing structure usu-
ally corresponds to lexical items, while higher levels
correspond to actual syntactic constituents.
In order to avoid truncation of multiword expres-
sions, we also require the meta pattern to start and
end by a HFW or bracket. Thus our meta-patterns
match the following regular expression:
{H|B}? C1 {H|B}+ C2 {H|B}?
where ?*? means zero or more times, and ?+? means
one or more time and B can be ?(?,?)? brackets pro-
duced by the parser (in these patterns we do not
need to guarantee that brackets match properly). Ex-
amples of such patterns include ?((C1)in C2))?,
?(C1)(such(as(((C2)?, and ?(C1)and(C2)?3. We
dismiss rare patterns that appear less than TP times
per million words.
Symmetric patterns. Many of the pattern candi-
dates discovered in the previous stage are not usable.
In order to find a usable subset, we focus on the sym-
metric patterns. We define a symmetric pattern as a
pattern in which the same pair of terms (C words)
is likely to appear in both left-to-right and right-to-
left orders. In order to identify symmetric patterns,
for each pattern we define a pattern graph G(P ), as
proposed by (Widdows and Dorow, 2002). If term
pair (C1, C2) appears in pattern P in some context,
3This paper does not use any punctuation since the parser
is provided with sentences having all non-alphabetic characters
removed. We assume word separation. C1,2 can be a word or a
multiword expression.
51
we add nodes c1, c2 to the graph and a directed edge
EP (c1, c2) between them. In order to select sym-
metric patterns, we create such a pattern graph for
every discovered pattern, and create a symmetric
subgraph SymG(P) in which we take only bidirec-
tional edges from G(P ). Then we compute three
measures for each pattern candidate as proposed by
(Davidov and Rappoport, 2006):
M1(P ) := |{c1|?c2EP (c1, c2) ? ?c3EP (c3, c1)}||Nodes(G(P ))|
M2(P ) := |Nodes(SymG(P ))||Nodes(G(P ))|
M3(P ) := |Edges(SymG(P ))||Edges(G(P ))|
For each measure, we prepare a sorted list of all can-
didate patterns. We remove patterns that are not in
the top ZT (we use 100, see Section 6) in any of the
three lists, and patterns that are in the bottom ZB in
at least one of the lists.
5 Concept Discovery
At the end of the previous stage we have a set of
symmetric patterns. We now use them in order to
discover concepts. The concept discovery algorithm
is essentially the same as used by (Davidov and Rap-
poport, 2006) and has some similarity with the one
used by (Widdows and Dorow, 2002). In this section
we outline the algorithm.
The clique-set method. The utilized approach to
concept discovery is based on connectivity struc-
tures in the all-pattern term relationship graph G,
resulting from merging all of the single-pattern
graphs for symmetric patterns selected in the previ-
ous stage. The main observation regarding G is that
highly interconnected words are good candidates to
form a concept. We find all strong n-cliques (sub-
graphs containing n nodes that are all interconnected
in both directions). A clique Q defines a concept that
contains all of the nodes in Q plus all of the nodes
that are (1) at least unidirectionally connected to all
nodes in Q, and (2) bidirectionally connected to at
least one node in Q. Using this definition, we create
a concept for each such clique.
Note that a single term can be assigned to several
concepts. Thus a clique based on a connection of the
word ?Sun? to ?Microsoft? can lead to a concept of
computer companies, while the connection of ?Sun?
to ?Earth? can lead to a concept of celestial bodies.
Reducing noise: merging and windowing. Since
any given term can participate in many cliques, the
algorithm creates overlapping categories, some of
which redundant. In addition, due to the nature of
language and the imperfection of the corpus some
noise is obviously to be expected. We enhance the
quality of the obtained concepts by merging them
and by windowing on the corpus. We merge two
concepts Q,R, iff there is more than a 50% overlap
between them: (|Q?R| > |Q|/2) ? (|Q?R| >
|R|/2). In order to increase concept quality and re-
move concepts that are too context-specific, we use
a simple corpus windowing technique. Instead of
running the algorithm of this section on the whole
corpus, we divide the corpus into windows of equal
size and perform the concept discovery algorithm of
this section (without pattern discovery) on each win-
dow independently. We now have a set of concepts
for each window. For the final set, we select only
those concepts that appear in at least two of the win-
dows. This technique reduces noise at the potential
cost of lowering coverage.
A decrease in the number of windows should pro-
duce more noisy results, while discovering more
concepts and terms. In the next section we show that
while windowing is clearly required for a large cor-
pus, incorporation of parser data increases the qual-
ity of the extracted corpus to the point where win-
dowing can be significantly reduced.
6 Results
In order to estimate the quality of concepts and to
compare it to previous work, we have performed
both automatic and human evaluation. Our basic
comparison was to (Davidov and Rappoport, 2006)
(we have obtained their data and utilized their al-
gorithm), where we can estimate if incorporation of
parser data can solve some fundamental weaknesses
of their framework. In the following description, we
call their algorithm P and our parser-based frame-
work P+. We have also performed an indirect com-
parison to (Widdows and Dorow, 2002).
While there is a significant number of other re-
lated studies4 on concept acquisition (see Section 2),
4Most are supervised and/or use language-specific tools.
52
direct or even indirect comparison to these works is
problematic due to difference in corpora, problem
definitions and evaluation strategies. Below we de-
scribe the corpora and parameters used in our evalu-
ation and then show and discuss WordNet-based and
Human evaluation settings and results.
Corpora. We performed in-depth evaluation in
two languages, English and Russian, using three
corpora, two for English and one for Russian.
The first English corpus is the BNC, containing
about 100M words. The second English corpus,
DMOZ(Gabrilovich and Markovitch, 2005), is a
web corpus obtained by crawling URLs in the Open
Directory Project (dmoz.org), resulting in 68GB
containing about 8.2G words from 50M web pages.
The Russian corpus (Davidov and Rappoport, 2006)
was assembled from web-based Russian reposito-
ries, to yield 33GB and 4G words. All of these cor-
pora were also used by (Davidov and Rappoport,
2006) and BNC was used in similar settings by
(Widdows and Dorow, 2002).
Algorithm parameters. The thresholds
TH , TC , TP , ZT , ZB , were determined mostly
by practical memory size considerations: we com-
puted thresholds that would give us the maximal
number of terms, while enabling the pattern access
table to reside in main memory. The resulting
numbers are 100, 50, 20, 100, 100. Corpus window
size was determined by starting from a small
window size, extracting at random a single window,
running the algorithm, and iterating this process
with increased ?2 window sizes until reaching a
desired vocabulary concept participation percentage
(before windowing) (i.e., x% of the different words
in the corpus participate in terms assigned into
concepts. We used 5%.). We also ran the algorithm
without windowing in order to check how well the
provided parsing information can help reduce noise.
Among the patterns discovered are the ubiquitous
ones containing ?and?,?or?, e.g. ?((X) or (a Y))?,
and additional ones such as ?from (X) to (Y)?.
Influence of parsing data on number of discov-
ered concepts. Table 1 compares the concept ac-
quisition framework with (P+) and without (P) uti-
lization of parsing data.
We can see that the amount of different words
V W C AS
P P+ P P+ P P+
DMOZ 16 330 504 142 130 12.8 16.0
BNC 0.3 25 42 9.6 8.9 10.2 15.6
Russ. 10 235 406 115 96 11.6 15.1
Table 1: Results for concept discovery with (P+) and
without (P) utilization of parsing data. V is the total num-
ber (millions) of different words in the corpus. W is the
number (thousands) of words belonging to at least one of
the terms for one of the concepts. C is the number (thou-
sands) of concepts (after merging and windowing). AS
is the average(words) category size.
covered by discovered concepts raises nearly 1.5-
fold when we utilize patterns based on parsing data
in comparison to pure HFW patterns used in previ-
ous work. We can also see nearly the same increase
in average concept size. At the same time we ob-
serve about 15% reduction in the total number of
discovered concepts.
There are two opposite factors in P+ which may
influence the number of concepts, their size and cov-
erage in comparison to P. On one hand, utilization of
more restricted patterns that include parsing infor-
mation leads to a reduced number of concept term
instances being discovered. Thus, the P+ pattern ?(X
(or (a Y))? will recognize ?(TV (or (a movie))? in-
stance and will miss ?(lunch) or (a snack))?, while
the P pattern ?X or a Y? will capture both. This leads
to a decrease in the number of discovered concepts.
On the other hand, P+ patterns, unlike P ones, al-
low the extraction of multiword expressions5, and
indeed more than third of the discovered terms us-
ing P+ were MWEs. Utilization of MWEs not only
allows to cover a greater amount of different words,
but also increases the number of discovered concepts
since new concepts can be found using cliques of
newly discovered MWEs. From the results, we can
see that for a given concept size and word coverage,
the ability to discover MWEs overcomes the disad-
vantage of ignoring potentially useful concepts.
Human judgment evaluation. Our human judge-
ment evaluation closely followed the protocol (Davi-
dov and Rappoport, 2006).
We used 4 subjects for evaluation of the English
5While P method can potentially be used to extract MWEs,
preliminary experimentation shows that without significant
modification, quality of MWEs obtained by P is very low in
comparison to P+
53
concepts and 4 subjects for Russian ones. In order
to assess subjects? reliability, we also included ran-
dom concepts (see below). The goal of the exper-
iment was to examine the differences between the
P+ and P concept acquisition frameworks. Subjects
were given 50 triplets of words and were asked to
rank them using the following scale: (1) the words
definitely share a significant part of their meaning;
(2) the words have a shared meaning but only in
some context; (3) the words have a shared mean-
ing only under a very unusual context/situation; (4)
the words do not share any meaning; (5) I am not
familiar enough with some/all of the words.
The 50 triplets were obtained as follows. We have
randomly selected 40 concept pairs (C+,C): C+ in
P+ and C in P using five following restrictions: (1)
concepts should contain at least 10 words; (2) for
a selected pair, C+ should share at least half of its
single-word terms with C, and C should share at
least half of its words with C+; (3) C+ should con-
tain at least 3 MWEs; (4) C should contain at least 3
words not appearing in C+; (5) C+ should contain at
least 3 single-word terms not appearing in C.
These restrictions allow to select concept pairs
such that C+ is similar to C while they still carry
enough differences which can be examined. We se-
lected the triplets as following: for pairs (C+, C) ten
triplets include terms appearing in both C+ and C
(Both column in Table 2), ten triplets include single-
word terms appearing in C+ but not C (P+ single
column), ten triplets include single-word terms ap-
pearing in C but not C+ (P column), ten triplets in-
clude MWEs appearing in C+ (P+ mwe column) and
ten triplets include random terms obtained from P+
concepts (Rand column).
P+ P Both Rand
mwe single
% shared
meaning
DMOZ 85 88 68 81 6
BNC 85 90 61 88 0
Russ. 89 95 70 93 11
triplet
score (1-4)
DMOZ 1.7 1.4 2.5 1.7 3.8
BNC 1.6 1.3 2.1 1.5 4.0
Russ. 1.5 1.1 2.0 1.3 3.7
Table 2: Results of evaluation by human judgment of
three data sets. P+ single/mwe: single-word/MWE terms
existing only in P+ concept; P: single-word terms existing
only in P concept; Both: terms existing in both concepts;
Rand: random terms. See text for detailed explanations.
The first part of Table 2 gives the average per-
centage of triplets that were given scores of 1 or 2
(that is, ?significant shared meaning?). The second
part gives the average score of a triplet (1 is best).
In these lines scores of 5 were not counted. Inter-
evaluator Kappa between scores are 0.68/0.75/0.76
for DMOZ, BNC and Russian respectively. We can
see that terms selected by P and skipped by P+
receive low scores, at the same time even single-
word terms selected by P+ and skipped by P show
very high scores. This shows that using parser data,
the proposed framework can successfully avoid se-
lection of erroneous terms, while discovering high-
quality terms missed by P. We can also see that P+
performance on MWEs, while being slightly infe-
rior to the one for single-word terms, still achieves
results comparable to those of single-word terms.
Thus our algorithm can greatly improve the re-
sults not only by discovering of MWEs but also by
improving the set of single word concept terms.
WordNet-based evaluation. The major guideline
in this part of the evaluation was to compare our re-
sults with previous work (Davidov and Rappoport,
2006; Widdows and Dorow, 2002) without the pos-
sible bias of human evaluation. We have followed
their methodology as best as we could, using the
same WordNet (WN) categories and the same cor-
pora. This also allows indirect comparison to several
other studies, thus (Widdows and Dorow, 2002) re-
ports results for an LSA-based clustering algorithm
that are vastly inferior to the pattern-based ones.
The evaluation method is as follows. We took
the exact 10 WN subsets referred to as ?subjects? in
(Widdows and Dorow, 2002), and removed all multi-
word items. We then selected at random 10 pairs of
words from each subject. For each pair, we found
the largest of our discovered concepts containing it.
The various morphological forms or clear typos of
the same word were treated as one in the evaluation.
We have improved the evaluation framework for
Russian by using the Russian WordNet (Gelfenbey-
nand et al, 2003) instead of back-translations as
done in (Davidov and Rappoport, 2006). Prelim-
inary examination shows that this has no apparent
effect on the results.
For each found concept C containing N words,
we computed the following: (1) Precision: the num-
54
ber of words present in both C and WN divided by
N ; (2) Precision*: the number of correct words di-
vided by N . Correct words are either words that
appear in the WN subtree, or words whose entry in
the American Heritage Dictionary or the Britannica
directly defines them as belonging to the given class
(e.g., ?murder? is defined as ?a crime?). This was
done in order to overcome the relative poorness of
WN; (3) Recall: the number of words present in
both C and WN divided by the number of words
in WN; (4) The percentage of correctly discovered
words (according to Precision*) that are not in WN.
Table 3 compares the macro-average of these 10
categories to corresponding related work. We do not
Prec. Prec.* Rec. %New
DMOZ
P 79.8 86.5 22.7 2.5
P+ 79.5 91.3 28.6 3.7
BNC
P 92.76 95.72 7.22 0.4
P+ 93.0 96.1 14.6 1.7
Widdows 82.0 - - -
Russian
P 82.39 89.64 20.03 2.1
P+ 83.5 92.6 29.6 4.0
Table 3: WordNet evaluation in comparison to P (Davi-
dov and Rappoport, 2006) and to Widdows(Widdows and
Dorow, 2002). Columns show average precision, preci-
sion* (as defined in text), recall, and % of new words
added to corresponding WN subtree.
observe apparent rise in precision when comparing
P+ and P, but we can see significant improvement
in both recall and precision* for all of three cor-
pora. In combination with human judgement results,
this suggests that the P+ framework successfully dis-
covers more correct terms not present in WN. This
causes precision to remain constant while precision*
improves significantly. Rise in recall also shows that
the P+ framework can discover significantly more
correct terms from the same data.
Windowing requirement. As discussed in Sec-
tion 5, windowing is required for successful noise
reduction. However, due to the increase in pattern
quality with parser data, it is likely that less noise
will be captured by the discovered patterns. Hence,
windowing could be relaxed allowing to obtain more
data with sufficiently high precision.
In order to test this issue we applied our algo-
rithms on the DMOZ corpus with 3 different win-
dowing settings: (1) choosing window size as de-
scribed above; (2) using ?4 larger window; (3)
avoiding windowing altogether. Each time we ran-
domly sampled a set of 100 concepts and tagged (by
the authors) noisy ones. A concept is considered to
be noisy if it has at least 3 words unrelated to each
other. Table 4 shows results of this test.
Reg. Window ?4 Window No windowing
P 4 18 33
P+ 4 5 21
Table 4: Percentage of noisy concepts as a function of
windowing.
We can see that while windowing is still essential
even with available parser data, using this data we
can significantly reduce windowing requirements,
allowing us to discover more concepts from the
same data.
Timing requirements are modest, considering we
parsed such large amounts of data. BNC pars-
ing took 45 minutes, and the total single-machine
processing time for the 68Gb DMOZ corpus was
4 days6. In comparison, a state-of-art supervised
parser (Charniak and Johnson, 2005) would process
the same amount of data in 1.3 years7.
7 Discussion
We have presented a framework which utilizes an
efficient fully unsupervised parser for unsupervised
pattern-based discovery of concepts. We showed
that utilization of unsupervised parser in pattern ac-
quisition not only allows successful extraction of
MWEs but also improves the quality of obtained
concepts, avoiding noise and adding new terms
missed by the parse-less approach. At the same time,
the framework remains fully unsupervised, allowing
its straightforward application to different languages
as supported by our bilingual evaluation.
This research presents one more step towards the
merging of fully unsupervised techniques for lex-
ical acquisition, allowing to extract semantic data
without strong assumptions on domain or language.
While we have aimed for concept acquisition, the
proposed framework can be also useful for extrac-
tion of different types of lexical relationships, both
among concepts and between concept terms.
6In fact, we used a PC cluster, and all 3 corpora were parsed
in 15 hours.
7Considering the reported parsing rate of 10 sentences per
second
55
References
Mishele Banko, Michael J Cafarella , Stephen Soderland,
Matt Broadhead, Oren Etzioni, 2007. Open Informa-
tion Extraction from the Web. IJCAI ?07.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ?06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Rens Bod, 2007. Is the End of Supervised Parsing in
Sight? ACL ?07.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Sharon Caraballo, 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. ACL
?99.
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. ACL ?05.
Alexander Clark, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. Ph.D. thesis, University
of Sussex.
James R. Curran, Marc Moens, 2002. Improvements in
Automatic Thesaurus Extraction SIGLEX 02?, 59?66.
Dmitry Davidov, Ari Rappoport, 2006. Efficient Un-
supervised Discovery of Word Categories using Sym-
metric Patterns and High Frequency Words. COLING-
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2009. Translation and
Extension of Concepts Across Languages. EACL ?09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. Index-
ing by Latent Semantic Analysis. J. of the American
Society for Info. Science, 41(6):391?407.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in Graphs for
Lexical Acquisition and Word Sense Discrimination.
MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey, S. Kok,
Ana-Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel Weld, Alexander Yates, 2005. Unsupervised
Named-entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91134.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture Generation for Text Categorization Using World
Knowledge. IJCAI ?05.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic Trans-
lation of WordNet Semantic Network to Russian Lan-
guage (in Russian) International Dialog 2003 Work-
shop.
James Gorman, James R. Curran, 2006. Scaling Distri-
butional Similarity to Large Corpora. COLING-ACL
?06.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech tagging for Grammar Induction. COLING ?08.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Adam Kilgarriff, 2007. Googleology is Bad Science.
Computational Linguistics ?08, Vol.33 No. 1,pp147-
151. .
Dan Klein and Christopher Manning, 2002. A genera-
tive constituent-context model for improved grammar
induction. Proc. of the 40th Meeting of the ACL.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. ACL ?04.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Hang Li, Naoki Abe, 1996. Clustering Words with the
MDL Principle. COLING ?96.
Dekang Lin, 1998. Automatic Retrieval and Clustering
of Similar Words. COLING ?98.
Marcus Mitchell P., Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised Acquisition of Open-domain Classes and
Class Attributes from Web Documents and Query
Logs. ACL 08.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993. Dis-
tributional Clustering of English Words. ACL ?93.
Hinrich Schu?tze, 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics , 24(1):97?123.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion . ACL ?06.
Dominic Widdows, Beate Dorow, 2002. A Graph Model
for Unsupervised Lexical Acquisition. COLING ?02.
56
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 156?164,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Selection of High Quality Parses Created By a Fully
Unsupervised Parser
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
The average results obtained by unsupervised
statistical parsers have greatly improved in the
last few years, but on many specific sentences
they are of rather low quality. The output of
such parsers is becoming valuable for vari-
ous applications, and it is radically less expen-
sive to create than manually annotated training
data. Hence, automatic selection of high qual-
ity parses created by unsupervised parsers is
an important problem.
In this paper we present PUPA, a POS-based
Unsupervised Parse Assessment algorithm.
The algorithm assesses the quality of a parse
tree using POS sequence statistics collected
from a batch of parsed sentences. We eval-
uate the algorithm by using an unsupervised
POS tagger and an unsupervised parser, se-
lecting high quality parsed sentences from En-
glish (WSJ) and German (NEGRA) corpora.
We show that PUPA outperforms the leading
previous parse assessment algorithm for su-
pervised parsers, as well as a strong unsuper-
vised baseline. Consequently, PUPA allows
obtaining high quality parses without any hu-
man involvement.
1 Introduction
In unsupervised parsing an algorithm should un-
cover the syntactic structure of an input sentence
without using any manually created structural train-
ing data. The last decade has seen significant
progress in this field of research (Klein and Man-
ning, 2002; Klein and Manning, 2004; Bod, 2006a;
Bod, 2006b; Smith and Eisner, 2006; Seginer,
2007).
Many NLP systems use the output of supervised
parsers (e.g., (Kwok et al, 2001) for QA, (Moldovan
et al, 2003) for IE, (Punyakanok et al, 2008) for
SRL, (Srikumar et al, 2008) for Textual Inference
and (Avramidis and Koehn, 2008) for MT). To
achieve good performance, these parsers should be
trained on large amounts of manually created train-
ing data from a domain similar to that of the sen-
tences they parse (Lease and Charniak, 2005; Mc-
Closky and Charniak, 2008). In the highly variable
Web, where many of these systems are used, it is
very difficult to create a representative corpus for
manual annotation. The high cost of manual annota-
tion of training data for supervised parsers imposes
a significant burden on their usage.
A possible answer to this problem can be pro-
vided by high quality parses produced by unsuper-
vised parsers that require little to no manual efforts
for their training. These parses can be used either
as input for applications, or as training material for
modern supervised parsers whose output will in turn
be used by applications.
Although unsupervised parser results improve,
the quality of many of the parses they produce is still
too low for such goals. For example, the Seginer
(2007) parser achieves an F-score of 75.9% on the
WSJ10 corpus and 59% on the NEGRA10 corpus,
but the percentage of individual sentences with an
F-score of 100% is 21.5% for WSJ10 and 11% for
NEGRA10. When requirements are relaxed, only
asking for an F-score higher than 85%, percentage
is still low, 42% for WSJ10 and 15% for NEGRA10.
In this paper we address the task of a fully un-
supervised assessment of high quality parses cre-
156
ated by an unsupervised parser. The assessment
should be unsupervised in order to avoid the prob-
lems mentioned above with manually trained super-
vised parsers. Assessing the quality of a learning al-
gorithm?s output and selecting high quality instances
has been addressed for supervised algorithms (Caru-
ana and Niculescu-Mizil, 2006) and specifically for
supervised parsers (Yates et al, 2006; Reichart and
Rappoport, 2007; Kawahara and Uchimoto, 2008;
Ravi et al, 2008). Moreover, it has been shown
to be valuable for supervised parser adaptation be-
tween domains (Sagae and Tsujii, 2007; Kawahara
and Uchimoto, 2008; Chen et al, 2008). However,
as far as we know the present paper is the first to
address the task of unsupervised assessment of the
quality of parses created by unsupervised parsers.
Our POS-based Unsupervised Parse Assessment
(PUPA) algorithm uses statistics about POS tag se-
quences in a batch of parsed sentences1. The con-
stituents in the batch are represented using the POS
sequences of their yield and of the yields of neigh-
boring constituents. Constituents whose representa-
tion is frequent in the output of the parser are con-
sidered to be of a high quality. A score for each
range of constituent length is calculated, reflecting
the robustness of statistics used for the creation of
the constituents of that length. The final sentence
score is a weighted average of the scores calculated
for each constituent length. The score thus integrates
the quality of short and long constituents into one
score reflecting the quality of the whole parse tree.
PUPA provides a quality score for every sentence
in a parsed sentences set. An NLP application can
then decide if to use a parse or not, according to
its own definition of a high quality parse. For ex-
ample, it can select every sentence whose score is
above some threshold, or the k top scored sentences.
The selection strategy is application dependent and
is beyond the scope of this paper.
The unsupervised parser we use is the Seginer
(2007) incremental parser2, which achieves state-of-
1The algorithm can be used with supervised POS taggers
and parsers, but we focus here on the fully unsupervised sce-
nario, which is novel and more useful. For completeness of
analysis, we experimented with PUPA using a supervised POS
tagger (see Section 5). Using PUPA with supervised parsers is
left for future work.
2www.seggu.net/ccl.
the-art results without using manually created POS
tags. The POS tags we use are induced by the un-
supervised tagger of (Clark, 2003)3. Since both tag-
ger and parser do not require any manual annotation,
PUPA identifies high quality parses without any hu-
man involvement.
The incremental parser of (Seginer, 2007) does
not give any prediction of its output quality, and
extracting such a prediction from its internal data
structures is not straightforward. Such a predic-
tion can be given by supervised parsers in terms
of the parse likelihood, but this was shown to be
of medium quality (Reichart and Rappoport, 2007).
While the algorithms of Yates et al (2006), Kawa-
hara and Uchimoto (2008) and Ravi et al (2008) are
supervised (Section 3), the ensemble based SEPA al-
gorithm (Reichart and Rappoport, 2007) can be ap-
plied to unsupervised parsers in a way that preserves
the unsupervised nature of the selection task.
To compare between two algorithms, we use each
of them to assess the quality of the sentences in En-
glish and German corpora (WSJ and NEGRA)4. We
show that for every sentence length (up to 20) the
quality of the top scored k sentences according to
PUPA is higher than the quality of SEPA?s list (for
every k). As in (Reichart and Rappoport, 2007), the
quality of a set selected from the parser?s output is
evaluated using two measures: constituent F-score5
and average sentence F-score.
Section 2 describes the PUPA algorithm, Sec-
tion 3 discusses previous work, and Sections 4 and
5 present the evaluation setup and results.
2 The POS-based Unsupervised Parse
Assessment (PUPA) Algorithm
In this section we detail our parse assessment algo-
rithm. Its input consists of a set I of parsed sen-
tences, which in our evaluation scenario are pro-
duced by an unsupervised parser. The algorithm
assigns each parsed sentence a score reflecting its
quality.
3www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html,
the neyessenmorph model.
4This is in contrast to algorithms for selection from the re-
sults of supervised constituency parsers, which were evaluated
only for English (Yates et al, 2006; Reichart and Rappoport,
2007; Ravi et al, 2008).
5This is the traditional parsing F-score.
157
The algorithm has three steps. First, the words in
I are POS tagged (in our case, using the fully unsu-
pervised POS induction algorithm of Clark (2003)).
Second, POS statistics about the constituents in I
are collected. Finally, a quality score is calculated
for each parsed sentence in I using the POS statis-
tics. In the following we detail the last two steps.
Collecting POS statistics. In its second step, the
algorithm collects statistics about the constituents in
the input set I . Recall that the yield of a constituent
is the set of words covered by it. The PUPA con-
stituent representation (PCR) consists of three fea-
tures: (1) the ordered POS tag sequence of the con-
stituent?s yield, (2) the constituents? right context,
and (3) the constituents? left context.
We define context to be the leftmost and rightmost
POS tags in the yield of the neighbor of the con-
stituent (if there is only one POS tag in the neigh-
bor?s yield, this POS tag is the context). For the
right and left contexts we consider the right and left
neighbors respectively. A constituent C1 is the right
neighbor of a constituent C2 if C1 is the highest level
constituent such that the first word in the yield of C1
comes immediately after the last word in the yield of
C2. A constituent C1 is the left neighbor of a con-
stituent C2 if C1 is the highest level constituent such
that the first word in the yield of C2 comes immedi-
ately after the last word in the yield of C1.
Figure 1 shows an example, an unlabeled tree for
the sentence ?I will give you the ball?. The tree has
6 constituents (C0-C5). C3 and C4 have both right
and left neighbors. For C3, the POS sequence of its
yield is POS2, POS3 , the left neighbor is C1 and thus
the left context is POS1, and the right neighbor is C4
and thus the right context is POS4. Note that the
left and right neighbors of C3 have only one POS
tag in their yield and therefore this POS tag is the
context. For C4 the yield is POS4, the left neighbor
is C3 (and thus the left context is POS2,POS3), and
the right neighbor is C5 (and thus the right context
is POS5,POS6). C1, whose yield is POS1, has only
a right neighbor, C2, and thus its right context is
POS2,POS6 and its left context is NULL. C2 and C5
(whose yields are POS2, POS3, POS4, POS5, POS6 for
C2 and POS5, POS6 for C5) have only a left neigh-
bor. For C2, this is C1 (and the context is POS1)
while for C5 this is C4 (with the context POS4).
0
1
POS1
I
2
3
POS2
will
POS3
give
4
POS4
you
5
POS5
the
POS6
ball
Figure 1: An example parse tree for contexts and neigh-
bors (see text).
The right context of both constituents is NULL. As
all sentence level constituents, C0 has no neighbors,
and thus both its left and right contexts are NULL.
We have also explored other representations of
left and right contexts based on the POS tags of their
yields. In these, we represented the left/right neigh-
bor using only the leftmost/rightmost POS tags of
its yield or other subsets of the yield?s POS tags.
These variations produced lower quality results than
the main variant above in our experiments, which
were for English and German. Exploring the suit-
ability of our representation for other languages is
left for future research.
Score computation. The third and last step of the
algorithm is a second pass over I for computing a
quality score for each parse tree.
Short constituents tend to be more frequent than
long ones. In order not to distort our score due to
parsing errors in short constituents, PUPA computes
the grade using a division into lengths, in three steps.
First, constituents are assigned to bins according to
their length, each bin containing the constituents of
a certain range of lengths. Denote this range by
W (for width), and the number of bins by N(W ).
For example, in our experiments the longest possible
constituent is of length 20, so we can take W = 5,
resulting in N(W ) = 4: bin 1 for constituents of
length 1-5, bin 2 for constituents of length 6-10, and
so on for bins 3, 4.
The score of bini is given by
(1) BinScore(bini) =?t=Xt=2 (X ? t + 2) ? |C
i
t |
|Ci|
Where X is the maximal number of occurrences
of constituents in the bin that we consider as impor-
tant for the score (see below for its selection), |Cit |
is the number of constituents in bin i occurring at
158
least t times in the batch of parsed sentences, and
|Ci| is the number of constituents in bin i. In words,
the score is a weighted average: the fraction of the
constituents in the bin occuring at least 2 times (with
weight X), plus the fraction of the constituents in the
bin occuring at least 3 times (with weight X ? 1),
etc, until the fraction of the constituents in the bin
occuring at least X times (with weight 2).
A score for the division into N bins is given by
(2) Score(N(W )) =
?N(W )
i=1 BinScore(bini)
Z?M
Where Z is the maximum bin score (according to
(1)) and M is the number of bins containing at least
one constituent. If, for example, N(W ) = 4 and
there is no constituent whose length is between 11
and 15 then bin number 3 is empty. If every other
bin contains at least one constituent, M = 3.
To get a final score for the parse tree of sentence
S that is independent of a specific bin division, we
sum the scores of the various bin division:
(3) PupaScore(S) =
?W=Y
W=1 Score(N(W ))
Y
where Y is the length of S (which is also its max-
imum bin width). PupaScore thus takes values in
the [0, 1] range.
In equation (1), if, for example, X = 20 then
the weight of the fraction of the bin?s constituents
occurring at least 2 times is 20 while the weight of
the fraction of the constituents occurring at least 10
times is 12 and of the fraction of constituents occur-
ring at least 20 times is 2. We consider the number
of times a constituent appears in a batch to be an in-
dication of its correctness. The difference between 3
and 2 occurrences is therefore more indicative than
the difference between 20 and 19 occurrences. More
generally, the more times a constituent occurs, the
less indicative any additional appearance is.
In equation (2) we give all bins the same weight.
Short constituents are more frequent and are gener-
ally more likely to be correct. However, the cor-
rectness of long constituents is an indication that the
parser has a correct interpretation of the tree struc-
ture and that it is likely to create a high quality tree.
The usage of equal bin weights was done to balance
the tendency of parse trees to have more short con-
stituents.
Parameters. PUPA has two parameters: X , the
maximal number of occurrences considered in equa-
tion (1), and P , the number of POS tags induced by
the unsupervised POS tagger. In the following we
present the unsupervised technique we used to tune
these parameters.
Figure 2 shows nc(t), the number of constituents
appearing at least t times in WSJ20 (left) and NE-
GRA20 (right). For both corpora, the pattern is
shown when using 5 POS tags (P = 5, solid line)
and 50 POS tags (P = 50, dashed line). The distri-
bution obeys Zipf?s law: many constituents appear a
small number of times while a few constituents ap-
pear a large number of times. We denote the t value
where the slope changes from steep to moderate by
telbow. Practically, we approximate the ?real? elbow
value and define telbow to be the smallest t for which
nc(t + 1) ? nc(t) = 1. When P = 5, telbow is 32
for WSJ and 19 for NEGRA. When P = 50, telbow is
15 for WSJ and 9 for NEGRA.
The number of constituents appearing more than
telbow times is considerably smaller than the number
of constituents appearing telbow times or less. There-
fore, the fact that a constituent appears telbow + S
times (for a positive integer S) is not a better indica-
tion of its quality than the fact that it appears telbow
times. We thus select X to be telbow.
The graphs also demonstrate that for both cor-
pora, telbow for P = 50 is smaller than telbow for
P = 5. Generally, telbow is a monotonically decreas-
ing function of P . Lower telbow values imply that
PUPA would be less distinctive between constituents
quality (see equation (1); recall that X = telbow).
We thus want to select the P value that maximizes
telbow. We therefore minimize P . telbow values for
P ? {3, . . . , 10} are very similar. Indeed, PUPA
achieves its best performance for P ? {3, . . . , 10}
and it is insensitive to the selection of P in this
range. In Section 5 we report results with P = 5.
3 Related Work
Unsupervised parsing has been explored for several
decades (see (Klein, 2005) for a recent review). Re-
cently, unsupervised parsing algorithms have for the
first time outperformed the right branching heuristic
baseline for English. These include CCM (Klein and
Manning, 2002), the DMV and DMV+CCM models
(Klein and Manning, 2004), (U)DOP based mod-
159
0 50 100
0
5000
10000
15000
t
# o
f co
nst
itue
nts
 ap
pea
ring
 at 
lea
st t
 tim
es
 
 
0 50 100
0
1000
2000
3000
4000
5000
6000
7000
8000
t
 
 
P = 5
P = 50
P = 5
P = 50
Figure 2: Number of constituents appearing at least t
times (nc(t)) as a function of t. Shown are WSJ (left)
and NEGRA (right), where constituents are represented
according to PUPA?s PCR with 5 POS tags (P = 5, solid
line) or 50 POS tags (P = 50, dashed line).
els (Bod, 2006a; Bod, 2006b), an exemplar based
approach (Dennis, 2005), guiding EM using con-
trastive estimation (Smith and Eisner, 2006), and the
incremental parser of Seginer (2007) that we use in
this work. To obtain good results, manually created
POS tags are used as input in all of these algorithms
except Seginer?s, which uses plain text.
Quality assessment of a learning algorithm?s out-
put and selection of high quality instances have been
addressed for supervised algorithms (see (Caruana
and Niculescu-Mizil, 2006) for a survey) and specif-
ically for supervised constituency parsers (Yates et
al., 2006; Reichart and Rappoport, 2007; Ravi et al,
2008). For dependency parsing in a corpus adapta-
tion scenario, (Kawahara and Uchimoto, 2008) built
a binary classifier that classifies each parse in the
parser?s output as reliable or not. To do that, they
selected 2500 sentences from the parser?s output,
compared them to their manually created gold stan-
dard, and used accurate (inaccurate) parses as posi-
tive (negative) examples for the classifier. Their ap-
proach is supervised and the features used by the
classifier are dependency motivated .
As far as we know, the present paper is the first to
address the task of selecting high quality parses from
the output of unsupervised parsers. The algorithms
of Yates et al (2006), Kawahara and Uchimoto
(2008) and Ravi et al (2008) are supervised, per-
forming semantic analysis of the parse tree and gold
standard-based calssification, respectively. How-
ever, the SEPA algorithm of Reichart and Rappoport
(2007), an algorithm for supervised constituency
parsers, can be applied to unsupervised parsers in
a way that preserves the unsupervised nature of the
selection task. In Section 5 we provide a detailed
comparison between PUPA and SEPA showing the
first to be superior. Below is a brief description of
the SEPA algorithm.
The input of the SEPA algorithm consists of a
parsing algorithm A, a training set, and a test set
(which in the unsupervised case might be the same
set). The algorithm provides, for each of the test
set?s parses generated by A when trained on the full
training set, a grade assessing the parse quality, on
a continuous scale between 0 to 100. The qual-
ity grade is calculated in the following way: N ran-
dom samples of size S are sampled from the train-
ing data and used for training the parsing algorithm
A. In that way N committee members are created.
Then, each of the test sentences is parsed by each of
the N committee members and an agreement score
ranging from 0 to 100 between the committee mem-
bers is calculated. All unsupervised parsers men-
tioned above (including the Seginer parser), have a
training phase where parameter values are estimated
from unlabeled data. SEPA can thus be applied to the
unsupervised case.
Automatic selection of high quality parses has
been shown to improve parser adaptation. Sagae and
Tsujii (2007) and Kawahara and Uchimoto (2008)
applied a self-training protocol to a parser adaptation
scenario but used only high quality parses to retrain
the parser. In the first work, high quality parses were
selected using an ensemble method, while in the sec-
ond a binary classifier was used (see above). The
first system achieved the highest score in the CoNLL
2007 shared task on domain adaptation of depen-
dency parsers, and the second system improved over
the basic self-training protocol. Chen et al (2008)
parsed target domain sentences and used short de-
pendencies information, which is often accurate, to
adapt a dependency parser to the Chinese language.
Automatic quality assessment has been exten-
sively explored for machine translation (Ueffing and
Ney, 2007) and speech recognition (Koo et al,
2001). Other NLP tasks where it has been explored
include semi-supervised relation extraction (Rosen-
feld and Feldman, 2007), IE (Culotta and McCal-
lum, 2004), QA (Chu-Carroll et al, 2003), and dia-
log systems (Lin and Weng, 2008).
The idea of representing a constituent by its yield
160
and (a different definition of) context is used by the
CCM unsupervised parsing model (Klein and Man-
ning, 2002). As far as we know the current work is
the first to use unsupervised POS tags for the selec-
tion of high quality parses.
4 Evaluation Setup
We experiment with sentences of up to 20 words
from the English WSJ Penn Treebank (WSJ20,
25236 sentences, 225126 constituents) and the Ger-
man NEGRA corpus (Brants, 1997) (NEGRA20,
15610 sentences, 108540 constiteunts), both con-
taining newspaper texts.
The unsupervised parsers of the kind addressed
in this paper output unlabeled parse trees. To eval-
uate the quality of a single parse tree with respect
to another, we use the unlabeled F-score (UF =
2?UR?UP
UR+UP ), where UR and UP are unlabeled recall
and unlabeled precision respectively.
Following the unsupervised parsing literature,
multiple brackets and brackets covering a single
word are not counted, but the sentence level bracket
is. We exclude punctuation and null elements ac-
cording to the scheme of (Klein, 2005).
The performance of unsupervised parsers
markedly degrades as sentence length increases.
For example, the Average sentence F?score for WSJ
sentences of length 10 is 71.4% compared to 58.5
for sentences of length 20 (the numbers for NEGRA
are 48.2% and 36.9%). We therefore evaluate PUPA
(and the baseline) for sentences of a given length.
We do this for every sentence of length 2-20 in
WSJ20 and NEGRA20.
For every sentence length L, we use PUPA and the
baseline algorithm (SEPA) to give a quality score to
each of the sentences of that length in the experi-
mental corpus. We then compare the quality of the
top k parsed sentences according to each algorithm.
We do this for every k from 1 to the number of sen-
tences of length L.
Following Reichart and Rappoport (2007), we use
two measures to evaluate the quality of a set of
parses: the constituent F-score (the traditional F-
score used in the parsing literature), and the average
F-score of the parses in the set. In the first mea-
sure we treat the whole set as a bag of constituents.
Each constituent is marked as correct (if it appears
in the gold standard parses of the set) or erroneous
(if it does not). Then, recall, precision and F-score
are calculated over these constituents. In the sec-
ond measure, the constituent F-score of each of the
parses in the set is computed, and then results are
averaged.
There are applications that use individual con-
stituents from the output of a parser while others
need the whole parse tree. For example, if the se-
lected set is used for training supervised parsers such
as the Collins parser (Collins, 1999), which collects
constituent statistics, the constituent F-score of the
selected set is the important measure. In applica-
tions such as the syntax based machine translation
model of (Yamada and Knight, 2001), a low qual-
ity tree might lead to errorenous translation of the
sentence. For such applications the average F-score
is more indicative. These measures thus represent
complementary aspects of a set quality and we con-
sider both of them.
The parser we use is the incremental parser of
(Seginer, 2007), POS tags are induced using the un-
supervised POS tagger of ((Clark, 2003), neyessen-
morph model). In each experiment, the tagger was
trained with the raw sentences of the experiment cor-
pus, and then the corpus words were POS tagged.
The output of the unsupervised POS tagger de-
pends on a random initialization. We ran the tagger
5 times, each time with a different random initializa-
tion, and then ran PUPA with its output. The results
we report for PUPA are the average over these 5 runs.
Random selection results (given for reference) were
also averages over 5 samples.
PUPA ?s parameter estimation is completely unsu-
pervised (see Section 2). No development data was
used to tune its parameters.
A 200 sentences development set from each cor-
pus was used for calibrating the parameters of the
SEPA algorithm. Based on the analysis of SEPA per-
formance with different assignments of its param-
eters given by Reichart and Rappoport (2007) (see
Section 3), we ran the SEPA algorithm with sam-
ple size (SEPA parameter S) of 30% and 80%, and
with 2 ? 10 committee members (N )6. The optimal
parameters were N = 10,S = 80 for WSJ20, and
6We tried higherN values but observed no improvements in
SEPA?s performance.
161
0 200 400 60070
75
80
85
90
95
100
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(a) WSJ, length 5
0 500 1000
60
65
70
75
80
85
90
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(b) WSJ, length 10
0 500 1000 1500
50
55
60
65
70
75
80
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(c) WSJ, length 15
0 500 1000 1500 2000
48
50
52
54
56
58
60
62
64
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(d) WSJ, length 20
0 200 400 600 800
50
55
60
65
70
75
80
Number of Sentneces
A
ve
ra
ge
 F
 S
co
re
(e) NEGRA, length5
0 200 400 600 800 1000
40
45
50
55
60
65
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(f) NEGRA, length 10
0 200 400 600 800
30
35
40
45
50
55
60
Number of Sentences
A
ve
ra
ge
 F
 S
co
re
(g) NEGRA, length 15
0 200 400 600 800
50
55
60
65
70
75
80
Number of Sentneces
A
ve
ra
ge
 F
 S
co
re
(h) NEGRA, length 20
0 500 1000 1500 2000
70
75
80
85
90
95
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(i) WSJ, length 5
0 2000 4000 6000 8000 10000
60
65
70
75
80
85
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(j) WSJ, length 10
0 0.5 1 1.5 2
x 104
55
60
65
70
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(k) WSJ, length 15
0 0.5 1 1.5 2 2.5
x 104
54
56
58
60
62
64
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(l) WSJ, length 20
0 500 1000 1500 2000 2500 3000
62
63
64
65
66
67
68
69
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(m) NEGRA, length5
0 1000 2000 3000 4000 5000
44
46
48
50
52
54
56
58
Number of Constituents
C
on
st
itu
en
t F
 S
co
re
(n) NEGRA, length 10
0 2000 4000 6000 8000
40
42
44
46
48
50
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(o) NEGRA, length 15
0 500 1000 1500 2000 2500 3000
62
63
64
65
66
67
68
69
Number of Constituents
C
on
st
itu
en
ts
 F
 S
co
re
(p) NEGRA, length 20
Figure 3: In all graphs: PUPA: solid line. SEPA: line with triangles. MC: line with circles. Random selection is
presented for reference as a dotted line. Top two rows: Average F-score for PUPA, SEPA and MC for sentences from
WSJ (top row) and NEGRA (bottom row). Bottom two rows: Constituents F-score for PUPA, SEPA and MC for
sentences from WSJ (top row) and NEGRA (bottom row). Results are presented for sentence lengths of 5,10,15 and
20 (patterns for other sentence lengths between 2 and 20 are very similar). PUPA is superior in all cases. The graphs
for PUPA and SEPA show a downward trend because parsed sentences were sorted according to score, which correlates
positively with F-score (unlike MC). The graphs converge because on the extreme right all test sentences were selected.
N = 10, S = 30 for NEGRA20.
We also compare PUPA to a baseline selecting the
sentences with the lowest number of constituents.
Since the number of constituents is an indication of
the complexity of the syntactic structure of a sen-
tence, it is reasonable to assume that selecting the
sentences with the lowest number of constituents is
a good selection strategy. We denote this baseline by
MC (for minimum constituents).
The incremental parser does not give any predic-
tion of its output quality as supervised generative
parsers do. We are thus not able to compare to such
a score.
5 Results
Figure 3 shows Average F-score and Constituents F-
score results for PUPA SEPA and MC, for sentences
162
of lengths 5,10,15 and 20 in WSJ20 and NEGRA20.
The top two rows are for Average F-score (top row:
WSJ, bottom row: NEGRA), while the bottom two
rows are for Constituents F-score (top row: WSJ,
bottom row: NEGRA).
PUPA and SEPA are both better than random selec-
tion for both corpora for every sentence length. The
MC baseline is better than random selection only for
NEGRA (in which case it outperforms SEPA). For
WSJ, however, random selection is a better strategy
than MC.
It is clear from the graphs that PUPA outperforms
SEPA and MC in all experimental conditions. We
observed very similar patterns in all other sentence
lengths in WSJ20 and NEGRA20 for both Average
F-score and Constituent F-score. In other words, for
every sentence length in both corpora, PUPA outper-
forms SEPA and MC in terms of both measures. we
present our results per sentence length to deprive the
possibility that PUPA is useful only for short sen-
tences or that it prefers sentences whose syntactic
structure is not complex (i.e. with a small number of
constituents, like MC).
Table 1 shows that the same pattern of results
holds when evaluating on the whole corpus (WSJ20
or NEGRA20) without any sentence length restric-
tion.
Note that while PUPA is a fully unsupervised al-
gorithm, SEPA requires a few hundreds of sentences
for its parameters tuning.
The main result of this paper is for sentences
whose length is up to 20 words (note that most un-
supervised parser literature reports numbers for sen-
tences up to length 10). We have also ran the exper-
iments for the remaining length range, 20-40. For
NEGRA, PUPA is superior over MC up to length 36,
and both are much better than SEPA. For WSJ, PUPA
and SEPA both outperform MC, but SEPA is a bit bet-
ter than PUPA. When evaluating on the whole corpus
(i.e. without sentence length restriction, like in Ta-
ble 1) PUPA is superior over both SEPA and MC for
WSJ40 and NEGRA40.
For completeness of analysis we also experi-
mented in the condition where PUPA uses gold stan-
dard POS tags as input. The number of these tags is
35 for WSJ and 57 for NEGRA. Interestingly, PUPA
achieves in this condition the same performance as
when using the same number of POS tags induced
by an unsupervised POS tagger. Since PUPA?s per-
formance for a smaller number of POS tags is better
(see our parameter tuning discussion above), the bot-
tom line is that PUPA pefers using induced POS tags
over gold POS tags.
5% 10% 20% 30% 40% 50%
WSJ20
PUPA 82.75 79.34 75.77 73.46 71.68 70.3
SEPA 78.68 75.7 72.64 70.72 69.54 68.58
MC 76.75 74.6 72.1 70.35 68.97 67.77
NEGRA20
PUPA 70.66 67.06 61.89 58.75 56.6 54.73
SEPA 66.19 62.75 59.41 57.16 55.23 53.7
MC 69.41 65.79 60.87 58.08 55.9 54.36
Table 1: Average F?score for the top k% of constituents
selected from WSJ20 (up) and NEGRA20 (down). No sen-
tence length restriction is imposed. Results presented for
PUPA , SEPA and MC. Average F?score of random se-
lection is 66.55 (WSJ20) and 47.05 (NEGRA20). PUPA is
superior over all methods.
6 Conclusions
We introduced PUPA, an algorithm for unsupervised
parse assessment that utilizes POS sequence statis-
tics. PUPA is a fully unsupervised algorithm whose
parameters can be tuned in an unsupervised man-
ner. Experimenting with the Seginer unsupervised
parser and Clark?s unsupervised POS tagger on En-
glish and German corpora, PUPA was shown to out-
perform both the leading parse assessment algorithm
for supervised parsers (SEPA, even when its param-
eters are tuned on manually annotated development
data) and a strong baseline (MC).
Using PUPA, we extracted high quality parses
from the output of a parser which requires raw text
as input, using POS tags induced by an unsupervised
tagger. PUPA thus provides a way of obtaining high
quality parses without any human involvement.
For future work, we intend to use parses selected
by PUPA from the output of unsupervised parsers
as training data for supervised parsers, and in NLP
applications that use parse trees. A challenge for
the first direction is the fact that state of the art su-
pervised parsers require labeled parse trees, while
modern unsupervised parsers create unlabeled trees.
Combining PUPA with algorithms for labeled parse
trees induction (Haghighi and Klein, 2006; Reichart
and Rappoport, 2008) is a one direction to overcome
this challenge. We also intend to use PUPA to assess
the quality of parses created by supervised parsers.
163
References
Eleftherios Avramidis and Philipp Koehn, 2008. En-
riching Morphologically Poor Languages for Statisti-
cal Machine Translation. ACL ?08.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ?06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Rich Caruana and Alexandru Niculescu-Mizil, 2006. An
Empirical Comparison of Supervised Learning Algo-
rithms. ICML ?06.
Jennifer Chu-Carroll, Krzysztof Czuba, John Prager and
Abraham Ittycheriah, 2003. In Question Answering,
Two Heads Are Better Than One. HLT-NAACL ?03.
Wenliang Chen, Youzheng Wu and Hitoshi Isahara,
2008. Learning Reliable Information for Dependency
Parsing Adaptation. Coling ?08.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech Induc-
tion. EACL ?03.
Michael Collins, 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Aron Culotta and Andrew McCallum, 2004. Confidence
Estimation for Information Extraction. HLT-NAACL
?04.
Simon Dennis, 2005. An Exemplar-based Approach to
Unsupervised Parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Grammar Induction. ACL ?06.
Daisuke Kawahara and Kiyotaka Uchimoto 2008.
Learning Reliability of Parses for Domain Adaptation
of Dependency Parsing. IJCNLP ?08.
Dan Klein and Christopher Manning, 2002. A Gener-
ative Constituent-Context Model for Improved Gram-
mar Induction. ACL ?02.
Dan Klein and Christopher Manning, 2004. Corpus-
based Induction of Syntactic Structure: Models of De-
pendency and Constituency. ACL ?04.
Dan Klein, 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford University.
Myoung?Wan Koo, Chin-Hui Lee and Biing?Hwang
Juang 2001. Speech Recognition and Utterance Ver-
ification Based on a Generalized Confidence Score.
IEEE Transactions on Speech and Audio Processing,
9(8):821?832.
Cody Kwok, Oren Etzioni and Daniel S. Weld, 2001.
Scaling Question Answering to the Web. WWW ?01.
Matthew Lease and Eugene Charniak, 2005. Towards a
Syntactic Account of Punctuation. IJCNLP ?05.
Feng Lin and Fuliang Weng, 2008. Computing Confi-
dence Scores for All Sub Parse Trees. ACL ?08, short
paper.
David McClosky and Eugene Charniak, 2008. Self-
Training for Biomedical Parsing. ACL ?08, short pa-
per.
Dan Moldovan, Christine Clark, Sanda Harabagiu and
Steve Maiorano, 2003. Cogex: A Logic Prover for
Question Answering. HLT-NAACL ?03.
Vasin Punyakanok and Dan Roth and Wen-tau Yih, 2008.
The Importance of Syntactic Parsing and Inference in
Semantic Role Labeling. Computational Linguistics,
34(2):257-287.
Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Au-
tomatic Prediction of Parser Accuracy. EMNLP ?08.
Roi Reichart and Ari Rappoport, 2007. An Ensemble
Method for Selection of High Quality Parses. ACL
?07.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Benjamin Rosenfeld and Ronen Feldman, 2007. Us-
ing Corpus Statistics on Entities to Improve Semi?
Supervised Relation Extraction From The WEB. ACL
?07.
Kenji Sagae and Junichi Tsujii, 2007. Dependency
Parsing and Domain Adaptation with LR Models and
Parser Ensemble. EMNLP-CoNLL ?07.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Vivek Srikumar, Roi Reichart, Mark Sammons, Ari Rap-
poport and Dan Roth, 2008. Extraction of Entailed
Semantic Relations Through Syntax-based Comma
Resolution. ACL ?08.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion. ACL ?06.
Nicola Ueffing and Hermann Ney, 2007. Word-
Level Confidence Estimation for Machine Translation.
Computational Linguistics, 33(1):9?40.
Kenji Yamada and Kevin Knight, 2001. A Syntax-Based
Statistical Translation Model. ACL ?01.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting Parser Errors Using Web-
based Semantic Filters . EMNLP ?06.
164
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 165?173,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The NVI Clustering Evaluation Measure
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Clustering is crucial for many NLP tasks and
applications. However, evaluating the results
of a clustering algorithm is hard. In this paper
we focus on the evaluation setting in which a
gold standard solution is available. We discuss
two existing information theory based mea-
sures, V and VI, and show that they are both
hard to use when comparing the performance
of different algorithms and different datasets.
The V measure favors solutions having a large
number of clusters, while the range of scores
given by VI depends on the size of the dataset.
We present a new measure, NVI, which nor-
malizes VI to address the latter problem. We
demonstrate the superiority of NVI in a large
experiment involving an important NLP appli-
cation, grammar induction, using real corpus
data in English, German and Chinese.
1 Introduction
Clustering is a major technique in machine learn-
ing and its application areas. It lies at the heart
of unsupervised learning, which has great potential
advantages over supervised learning. This is es-
pecially true for NLP, due to the high efforts and
costs incurred by the human annotations required for
training supervised algorithms. Recent NLP prob-
lems addressed by clustering include POS induction
(Clark, 2003; Goldwater and Griffiths, 2007), word
sense disambiguation (Shin and Choi, 2004), seman-
tic role labeling (Baldewein et al, 2004), pitch ac-
cent type disambiguation (Levow, 2006) and gram-
mar induction (Klein, 2005).
Evaluation of clustering results is a challenging
task. In this paper we address the external measures
setting, where a correct assignment of elements to
classes is available and is used for evaluating the
quality of another assignment of the elements into
clusters. Many NLP works have used external clus-
tering evaluation measures (see Section 2).
Recently, two measures have been proposed that
avoid many of the weaknesses of previous measures
and exhibit several attractive properties (see Sec-
tions 2 and 3): the VI measure (Meila, 2007) and
the V measure (Rosenberg and Hirschberg, 2007).
However, each of these has a serious drawback. The
possible values of VI lie in [0, 2log N ], where N is
the size of the clustered dataset. Hence it has lim-
ited use when comparing performance on different
datasets. V measure values lie in [0, 1] regardless of
the dataset, but the measure strongly favors a cluster-
ing having many small clusters. In addition, V does
not have many of the attractive properties of VI.
This paper has two contributions. First, we pro-
pose the NVI measure, a normalization of VI which
guarantees that the score of clusterings that VI con-
siders good lies in [0,1], regardless of dataset size.
Most of VI?s attractive properties are retained by
NVI.
Second, we compare the behavior of V, VI and
NVI in various situations to the desired behavior and
to each other. In particular, we show that V gives
high scores to clusterings with a large number of
clusters even when they are of low quality. We
demonstrate this both in a synthetic example (Sec-
tion 5) and in the evaluation (in three languages) of
a difficult NLP problem, labeled parse tree induc-
165
tion (Section 6). We show that in both cases, NVI
constitutes a better clustering evaluation measure.
2 Previous Evaluation Measures
A large number of clustering quality measures have
been proposed. Here we briefly survey the three
main types, mapping based measures, counting pairs
measures and information theory based measures.
We first review some terminology (Meila, 2007;
Rosenberg and Hirschberg, 2007). In a homoge-
neous clustering, every cluster contains only ele-
ments from a single class. In a complete cluster-
ing, all elements of each class are assigned to the
same cluster. The perfect solution is the fully ho-
mogeneous and complete clustering. We will illus-
trate the behavior of some measures using three ex-
treme cases: the single cluster case, in which all
data elements are put in the same single cluster; the
singletons case, in which each data element is put
in a cluster of its own; and the no knowledge case,
in which the class distribution within each cluster
is identical to the class distribution in the entire
dataset. If the single cluster solution is not the per-
fect one, the no knowledge solution is the worst pos-
sible solution. Throughout the paper, the number of
data elements to be clustered is denoted by N.
Mapping based measures are based on a post-
processing step in which each cluster is mapped to a
class. Among these are: L (Larsen, 1999), D (Van
Dongen, 2000), misclassification index (MI) (Zeng
et al, 2002), H (Meila, 2001), clustering F-measure
(Fung et al, 2003) and micro-averaged precision
and recall (Dhillon et al, 2003). As noted in (Rosen-
berg and Hirschberg, 2007), these measures evalu-
ate not only the quality of the proposed clustering
but also of the mapping scheme. Different mapping
schemes can lead to different quality scores for the
same clustering. Moreover, even when the mapping
scheme is fixed, it can lead to not evaluating the en-
tire membership of a cluster and not evaluating every
cluster (Meila, 2007).
Counting pairs measures are based on a com-
binatorial approach which examines the number of
pairs of data elements that are clustered similarly in
the reference and proposed clustering. Among these
are Rand Index (Rand, 1971), Adjusted Rand In-
dex (Hubert and Arabie, 1985), ? statistic (Hubert
and Schultz, 1976), Jaccard (Milligan et al, 1983),
Fowlkes-Mallows (Fowlkes and Mallows, 1983) and
Mirkin (Mirkin, 1996).
Meila (2007) described a number of problems
with such measures. The most acute one is that their
values are unbounded, making it hard to interpret
their results. The problem can be solved by transfor-
mations adjusting their values to lie in [0, 1], but the
adjusted measures suffer from severe distributional
problems, again limiting their usability in practice.
Information-theoretic (IT) based measures are
those addressed in this work. The measures in this
family suffer neither from the problems associated
with mappings, since they evaluate the entire mem-
bership of each cluster and not just a mapped por-
tion, nor from the distributional problems of the
counting pairs measures.
Zhao and Karypis (2001) define Purity and En-
tropy as follows:
Purity = ?kr=1 1Nmaxi(nir)
Entropy = ?kr=1 nrN (? 1logq
?q
i=1
nir
nr log(n
i
r
nr ))
where q is the number of classes, k the number of
clusters, nr cluster r?s size, and nir is the number of
elements in class i assigned to cluster r.
Both measures are good measures for homogene-
ity (Purity increases and Entropy decreases when
homogeneity increases). However, they do not eval-
uate completeness at all. The singletons solution is
thus considered optimal even if in fact it is of very
low quality.
Dom (2001) proposed the Q measure, the sum of
a homogeneity term H(C|K) and a model cost term
calculated using a coding theory argument:
Q(C,K) = H(C|K) + 1N
?|k|
k=1 log
(h(k)+|C|?1
|C|?1
)
where C are the correct classes, K are the induced
clusters and h(k) is the number of elements in clus-
ter k. Dom also presented a normalized version of
the Q measure (called Q2) whose range is (0, 1] and
gives higher scores to clusterings that are preferable.
As noted by (Rosenberg and Hirschberg, 2007), the
Q measure does not explicitly address the complete-
ness of the suggested clustering. Due to the cost
term, if two clusterings have the same H(C|K)
value, the model prefers the one with the lower num-
ber of clusters, but the trade-off between homogene-
ity and completeness is not explicitly addressed.
In the next section we describe the V and VI mea-
166
sures, which are IT measures that explicitly assess
both the homogeneity and completeness of the clus-
tering solution.
BCubed (Bagga and Baldwin, 1998) is an attrac-
tive measure that addresses both completeness and
homogeneity. It does not explicitly use IT concepts
and avoids mapping. In this paper we focus on V
and VI; a detailed comparison with BCubed is out of
our scope here and will be done in future work.
Several recent NLP papers used clustering tech-
niques and evaluation measures. Examples include
(Finkel and Manning, 2008), using VI, Rand in-
dex and clustering F-score for evaluating corefer-
ence resolution; (Headden et al, 2008), using VI, V,
greedy 1-to-1 and many-to-1 mapping for evaluating
unsupervised POS induction; (Walker and Ringger,
2008), using clustering F-score, the adjusted Rand
index, V, VI and Q2 for document clustering; and
(Reichart and Rappoport, 2008), using greedy 1-to-
1 and many-to-1 mappings for evaluating labeled
parse tree induction.
Schulte im Walde (2003) used clustering to in-
duce semantic verb classes and extensively dis-
cussed non-IT based clustering evaluation measures.
Pfitzner et al (2008) presented a comparison of clus-
tering evaluation measures (IT based and others).
While their analysis is extensive, their experiments
were confined to artificial data. In this work, we
experiment with a complex NLP application using
large real datasets.
3 The V and VI Measures
The V (Rosenberg and Hirschberg, 2007) and VI
(Meila, 2007) measures are IT based measures. In
this section we give a detailed description of these
measures and analyze their properties.
Notations. The partition of the N data elements
into classes is denoted by C = {c1, . . . , c|C|}.
The clustering solution is denoted by K =
{k1, . . . , k|K|}. A = {aij} is a |C| ? |K| contin-
gency matrix such that aij is the number of data ele-
ments that are members of class ci and are assigned
by the algorithm to cluster kj .
As other IT measures, V and VI assume that the
elements in the dataset are taken from a known dis-
tribution (both assume the uniform distribution), and
thus the classes and clusters can be treated as ran-
dom variables. When assuming the uniform distri-
bution, the probability of an event (a class or a clus-
ter) is its relative size, so p(c) = ?|K|k=1 ackN and
p(k) = ?|C|c=1 ackN . Under this assumption we can
talk about the entropies H(C) and H(K) and the
conditional entropies H(C|K) and H(K|C):
H(C) = ??|C|c=1
P|K|
k=1 ack
N log
P|K|
k=1 ack
N
H(K) = ??|K|k=1
P|C|
c=1 ack
N log
P|C|
c=1 ack
N
H(C|K) = ??|K|k=1
?|C|
c=1
ack
N log
ack
P|C|
c=1 ack
H(K|C) = ??|K|k=1
?|C|
c=1
ack
N log
ack
P|K|
k=1 ack
In Section 2 we defined the concepts of homo-
geneity and completeness. In order to satisfy the ho-
mogeneity criterion, each cluster must be contained
in a certain class. This results in the minimization
of the conditional entropy of the classes given the
clusters, H(C|K) = 0. In the least homogeneous
solution, the conditional entropy is maximized, and
H(C|K) = H(C). Similarly, in order to satisfy the
completeness criterion, each class must be contained
in a certain cluster, which results in the minimiza-
tion of the conditional entropy of the clusters given
the classes, H(K|C) = 0. In the least complete
solution, the conditional entropy is maximized, and
H(K|C) = H(K).
The VI measure. Variation of information (VI) is
defined as follows:
V I(C,K) = H(C|K) + H(K|C).
In the least homogeneous (complete) clustering, the
values of H(C|K) (H(K|C)) are maximal. As
a clustering solution becomes more homogeneous
(complete), the values of H(C|K) (H(K|C)) de-
crease to zero. Consequently, lower VI values im-
ply better clustering solutions. In the perfect so-
lution, both H(C|K) = 0 and H(K|C) = 0 and
thus V I = 0. For the least homogeneous and com-
plete clustering solution, where knowing the cluster
tells nothing about the class and vise versa, V I =
H(C) + H(K).
As a result, the range of values that VI takes is
dataset dependent, and the numbers themselves tell
167
us nothing about the quality of the clustering solu-
tion (apart from a score of 0, which is given to the
best possible solution).
A bound for VI values is a function of the maxi-
mum number of clusters in C or K, denoted by k?.
This is obtained when each cluster contains a sin-
gle element, and k? = N . Thus, V I ? [0, 2logN ].
Consequently, the range of VI values is dataset de-
pendent and unbounded when datasets change. This
means that it is hard to use VI to compare the perfor-
mance of a clustering algorithm across datasets.
An apparent simple solution to this problem
would be to normalize VI by 2logk? or 2logN , so
that its values would lie in [0, 1]. We discuss this at
the end of the next section.
VI has two useful properties. First, it satis-
fies the metric axioms, that is: V I(C,K) ?
0, V I(C,K) = V I(K,C), V I(C1, C2) +
V I(C2, C3) ? V I(C1, C3). This gives an intuitive
understanding of the relation between VI values.
Second, it is convexly additive. This
means that if K is obtained from C by
splitting Cj into clusters K1j , . . . ,Kmj ,
H?(Kj) = ??mi=1 P (Kij |Cj)logP (Kij |Cj),
then V I(C,K) = P (Cj)H?(Kj). This property
guarantees that all changes to VI are local; the
impact of splitting or merging clusters is limited
only to those clusters involved, and its size is
relative to the size of these clusters.
The V measure. The V measure uses homogeneity
(h) and completeness (c) terms as follows:
h =
{1 H(C) = 0
1? H(C|K)H(C) H(C) 6= 0
c =
{1 H(K) = 0
1? H(K|C)H(K) H(K) 6= 0
V = 2hch + c
In the least homogeneous clustering, H(C|K) is
maximal, at H(C|K) = H(C). In this case h
reaches its minimum value, which is 0. As homo-
geneity increases H(C|K) values decrease. For the
most homogeneous clustering, H(C|K) = 0 and
h = 1. The same considerations hold for c, which
ranges between 0 (for the least complete clustering)
and 1 (for a complete clustering). Since V is de-
fined to be the harmonic mean of h and c, V values
lie in [0, 1]. Consequently, it can be used to com-
pare the performance of clustering algorithms across
datasets. Higher V values imply better clusterings.
Unlike VI, V does not satisfy the metric axioms
and is not convexly additive. The range of values it
can get does not depend on dataset size.
Extreme cases for the two measures. In the
single cluster solution H(C|K) = H(C) and
H(K|C) = 0, and thus V = 0 (the worst possi-
ble score) and V I = H(C). If there is indeed only
a single class, then V I = 0, the best possible score,
which is the correct behavior. VI behaves better than
V here.
The singletons solution is a fully homogeneous
clustering in which H(C|K) = 0. The score of each
measure depends on the completeness of the solu-
tion. The completeness of a singletons clustering in-
creases with the number of classes. In the extreme
case where every element is assigned to a unique
class (|C| = |K| = N ) singletons is also complete,
H(K|C) = 0, and V (C,K) = 1, V I(C,K) = 0.
Both measures exhibit the correct behavior.
If there are classes that contain many elements,
singletons is far from being complete and should be
treated as a low quality solution. Again, in the sin-
gletons solution V I = H(K|C). Suppose that the
number of clusters is fixed. When the number of
classes increases, this value decreases, which is what
we want. When the number of classes decreases, the
score increases, which is again the correct behav-
ior. In Section 5 we show that this desired behavior
shown by VI is not shown by V.
Both measures treat the no knowledge solution as
the worst one possible: V = 0, and V I = H(C) +
H(K).
4 Normalized Variation of Information
In this section we define NVI, a normalization of
VI. NVI is N -independent and its values for clus-
terings considered as good by VI lie in [0, 1]. Hence,
NVI can be used to compare clustering performance
across datasets. We show that NVI keeps the convex
additivity property of VI but not its metric axioms.
168
Definition. We define NVI to be:
NV I(C,K) =
{H(C|K)+H(K|C)
H(C) H(C) 6= 0
H(K) H(C) = 0
We define NVI to be H(K) when H(C) = 0 to sat-
isfy the requirements that NVI values decrease as C
and K become more similar and that NVI would be
0 when they are identical1.
Range and extreme cases. Like VI, NVI decreases
as the clustering becomes more complete and more
homogeneous. For the perfect solution, NV I = 0.
In both the single cluster and the no knowledge so-
lutions, H(C|K) = H(C). Thus, in the former case
NV I = 1, and in the latter NV I = 1 + H(K)HC ? 1.
For the singletons clustering case, NV I =
H(K|C)
H(C) . Suppose that the number of clusters is
fixed. When the number of classes increases, the
numerator decreases and the denominator increases,
and hence the score decreases. In other words, as the
real solution gets closer to the singletons solution,
the score decreases, which is the correct behavior.
When the number of classes decreases, the score in-
creases, which is again the correct behavior.
For any pair of clusterings K1 and K2,
V I(C,K1) > V I(C,K2) iff NV I(C,K1) >
V I(C,K2). This implies that only clustering solu-
tions whose VI scores are better (i.e., numerically
lower) than the score of the single cluster solution
will be scored lower than 1 by NVI.
Note that NVI is meant to be used when there is
a ?correct? reference solution. In this case H(C) is
constant, so the property above holds. In this sense,
VI is more general, allowing us to compare any three
clustering solutions even when we do not have a cor-
rect reference one.
To summarize:
1. All clusterings considered by VI to be of high
quality (i.e., better than the single cluster solu-
tion) are scored by NVI in the range of [0, 1].
2. All clusterings considered by VI to be of lower
quality than the single cluster solution are
scored higher than 1 by NVI.
1H(C) = 0 iff C consists of a single class, and therefore
H(C) = H(K) = 0 iff C (K) consists of a single class (clus-
ter).
3. The ordering of scores between solutions given
by VI is preserved by NVI.
4. The behavior of NVI on the extreme cases is the
desired one.
Useful properties. In Section 3 we saw that VI has
two useful properties, satisfying the metric axioms
and being convexly additive. NVI is not symmetric
since the term in its denominator is H(C), the en-
tropy of the correct class assignment. Thus, it does
not satisfy the metric axioms. Being convexly addi-
tive, however, is preserved. In the class splitting sce-
nario (see convex additivity definition in Section 3)
it holds that NV I(C,K) = P (Cj)H?(Kj)H(C) . That is,
like for VI, the impact of splitting or merging a clus-
ter on NVI is limited only to those clusters involved,
and its size is relative to the size of these clusters.
Meila (2007) derived various interesting properties
of VI from the convex additivity property. These
properties generally hold for NVI as well.
H(K) normalization. Normalizing by H(C)
takes into consideration the complexity of the cor-
rect clustering. Another normalization option would
be to normalize by H(K), which represents the in-
duced clustering complexity. This normalization
does not guarantee that the scores of the ?good? clus-
terings lie in a data-independent range.
Let us define NVIK(C,K) to be V I(C,K)H(K) if
H(K) > 0 and H(C) if H(K) = 0. Recall that
in order for NVIK to be 0 iff C and K are identi-
cal, we must require that NV IK = H(C) when
H(K) = 0. In the no knowledge case, NV IK =
H(C)+H(K)
H(K) = H(C)H(K) + 1 > 1. In the single cluster
solution, however, NV IK = H(C) (since in this
case H(K) = 0) which ranges in [0, logN ]. This is
a serious drawback of NVIK. In Section 6 we empir-
ically show an additional drawback of NVIK.
logN normalization. Another possible normal-
ization of VI is by 2logN (or 2logk?), which is an
upper bound on VI values. However, this results in
the values of the measure being dependent on dataset
size, so results on datasets with different sizes again
cannot be compared. For example, take any C and
K and split each element into two. All entropy val-
ues, and the quality of the solution, are preserved,
but the scores given to the two K?s (before and after
169
7 1 1 1 0 0 0 0 0 0
0 7 1 1 1 0 0 0 0 0
0 0 7 1 1 1 0 0 0 0
0 0 0 7 1 1 1 0 0 0
0 0 0 0 7 1 1 1 0 0
0 0 0 0 0 7 1 1 1 0
0 0 0 0 0 0 7 1 1 1
1 0 0 0 0 0 0 7 1 1
1 1 0 0 0 0 0 0 7 1
1 1 1 0 0 0 0 0 0 7
V VI NVI NVIK
Singletons 0.667 2.303 1 0.5
Solution R 0.587 1.88 0.81 0.81
Table 1: The clustering matrix of solution R (top), and
the scores given to it and to the singletons solution by the
four measures (bottom). Although solution R is superior,
the score given by V to the singletons solution is much
higher. NVI exhibits the most preferable behavior (recall
that higher V values are better, as opposed to the other
three measures).
the split) by such a normalized VI would be differ-
ent. Since H(C) is preserved, the scores given by
NVI to the two K?s are identical.
5 Problematic V Behavior Example
In this section we provide a synthetic example that
demonstrates an undesireable behavior of V (and
NVIK) not manifested by VI and NVI. Specifically,
V favors solutions with a large number of clusters,
giving them higher scores than to solutions that are
evidently superior. In addition, the score given to the
singletons solution is high in absolute terms.
To present the example, we use the matrix repre-
sentation A of a clustering solution defined in Sec-
tion 3. The entries in row i sum to the number of
elements in class i, while those in column j sum to
the number of elements in cluster j.
Suppose that we have 100 elements assigned to 10
classes such that there are 10 elements in each class.
We consider two clustering solutions: the singletons
solution, and solution R whose matrix is shown in
Table 1 (top). Like the real solution, solution R also
has 10 clusters each having 10 elements. Solution
R is not very far from the correct solution, since
each cluster has 7 elements of the same class, and
the three other elements in a cluster are taken from
a different class each and can be viewed as ?noise?.
Solution R is thus much better than the singletons
solution. In order not to rely on our own opinion,
we have performed a simple human judgment ex-
periment with 30 subjects (university graduates in
different fields), all of whom preferred solution R2.
The scores given by V, VI, NVI and NVIK to the
two solutions are shown in Table 1 (bottom). V
scores solution R as being worse than the single-
tons solution, and gives the latter a number that?s
relatively high in absolute terms (0.667). VI ex-
hibits qualitatively correct behavior, but the num-
bers it uses are hard to interpret since they are N-
dependent. NVI scores solution R as being better
than singletons, and its score is less than 1, indicat-
ing that it might be a good solution.
6 Grammar Induction Experiment
In this section we analyse the behavior of V, VI,
NVI and NVIK using a highly non-trivial NLP ap-
plication with large real datasets, the unsupervised
labeled parse tree induction (LTI) algorithm of (Re-
ichart and Rappoport, 2008). We focus on the label-
ing that the algorithm finds for parsing constituents,
which is a clustering of constituents.
Summary of result. We show that V gives about
the same score to a labeling that uses thousands of
labels and to labelings in which the number of la-
bels (dozens) is identical or smaller than the number
of labels in the reference evaluation set (an anno-
tated corpus). Contrary to V, both NVI and VI give
much better scores to the solutions having a smaller
number of labels.
It could be argued that the total number of ?real?
labels in the data is indeed large (e.g., because every
verb exhibits its own syntactic patterns) and that a
small number of labels is just an arbitrary decision of
the corpus annotators. However, most linguistic the-
ories agree that there is a prototypical level of gen-
eralization that uses concepts such as Noun Phrase
and Verb Phrase, a level which consists of at most
dozens of labels and is strongly manifested by real
language data. Under these accepted assumptions,
the scoring behavior of V is unreasonable.
2We must rely on people?s expectations, since the whole
point in this area is that clustering quality cannot be formalized
in an objective, application-independent way.
170
MDL+SC (T labels) MDL+SC (P labels) MDL labels
Corpus L = 1 < 10 < 102 ? 102 L = 1 < 10 < 102 ? 102 L = 1 < 10 < 102 ? 102
WSJ10 26 0 0 3 23 8 0 0 0 8 2916 2282 2774 2864 52
NEGRA10 22 0 2 12 10 6 0 0 1 5 1202 902 1114 1191 11
CTB10 24 1 4 11 13 9 1 2 4 5 1050 816 993 1044 6
Table 2: The number of elements (constituents) covered by the clusters (labels) produced by the MDL+SC (T or P
labels) and MDL clusterings. L is the total number of labels. Shown are the number of clusters having one element,
less than 10 elements, less than 100 elements, and more than 100 elements. It is evident that MDL induces a sparse
clustering with many clusters that annotate very few constituents.
V VI NVI NVIK
Corpus MDL T P MDL T P MDL T P MDL T P
WSJ10 0.4 0.44 0.41 3.83 2.32 1.9 2.21 1.34 1.1 0.81 0.86 1.2
NEGRA10 0.47 0.5 0.5 2.56 1.8 1.4 1.51 1.1 0.83 0.76 0.96 1.1
CTB10 0.42 0.42 0.45 3 2.22 1.85 1.72 1.26 1.1 0.87 1.1 1.25
Table 3: V, VI, NVI and NVIK values for MDL and MDL+SC with T or P labels. V gives the three clusterings
very similar scores. NVIK prefers MDL labeling. NVI and VI both show the expected qualitative behavior, favoring
MDL+SC clustering with P labels. The most preferable scores are those of NVI, whose numbers are also the easiest
to interpret.
The experiment. The LTI algorithm has three
stages: bracketing, initial labeling, and label clus-
tering. Bracketing is done from raw text using
the unsupervised incremental parser of (Seginer,
2007). Initial labeling is done using the BMM model
(Borensztajn and Zuidema, 2007), which aims at
minimizing the grammar description length (MDL).
Finally, labels are clustered to a desired number of
labels using the k-means algorithm with syntactic
features extracted from the initially labeled trees.
We refer to this stage as MDL+SC (for ?syntactic
clustering?). Using a mapping-based evaluation with
two different mapping functions, the LTI algorithm
was shown to outperform previous work on unsu-
pervised labeled parse tree induction.
The MDL clustering step induces several thou-
sand labels for corpora of several tens of thousands
of constituents. The role of the SC step is to gen-
eralize these labels using syntactic features. There
are two versions of the SC step. In one, the num-
ber of clusters is identical to the number of labels
in the gold standard annotation of the experimental
corpus. This set of labels is called T (for target)
labels. In the other SC version, the number of la-
bels is the minimum number of labels required to
annotate more than 95% of the constituents in the
gold standard annotation of the corpus. This set of
labels is called P (for prominent) labels. Since con-
stituent labels follow the Zipfian distribution, P is
much smaller than T .
In this paper we run the LTI algorithm and evalu-
ate its labeling quality using V, VI, NVI and NVIK.
We compare the quality of the clustering induced by
the first clustering step alone (the MDL clustering)
to the quality of the clustering induced by the full
algorithm (i.e., first applying MDL and then clus-
tering its output using the SC algorithm for T or P
labels)3.
We follow the experimental setup in (Reichart
and Rappoport, 2008), running the algorithm on En-
glish, German and Chinese corpora: the WSJ Penn
Treebank (English), the Negra corpus (Brants, 1997)
(German), and version 5.0 of the Chinese Penn Tree-
bank (Xue et al, 2002). In each corpus, we used
the sentences of length at most 10,4 numbering 7422
(WSJ10), 7542 (NEGRA10) and 4626 (CTB10).
The characteristics of the induced clusterings are
shown in Table 25. The table demonstrates the
fact that MDL labeling, while perhaps capturing the
3Note that our evaluation here has nothing to do with the
evaluation done in (Reichart and Rappoport, 2008), which pro-
vided a comparison of the full grammar induction results be-
tween different algorithms, using mapping-based measures. We
evaluate the labeling stages alone.
4Excluding punctuation and null elements, according to the
scheme of (Klein, 2005).
5The number of MDL labels in the table differs from their
numbers, since we report the number of unique MDL labels
used for annotating correct constituents in the parser?s output,
while they report the number of unique labels used for annotat-
ing all constituents in the parser?s output.
171
salient level of generalization of the data in its lead-
ing clusters, is extremely noisy. For WSJ10, for ex-
ample, 2282 of the 2916 unique labels annotate only
one constituent, and 2774 labels label less than 10
constituents. These 2774 labels annotate 14.4% of
compared constituents, and the 2864 labels that an-
notate less than 100 constituents each, cover 30.7%
of the compared constituents (these percentages are
not shown in the table). In other words, MDL is not
a solution in which almost all of the mass is concen-
trated in the few leading clusters; its tail occupies a
large percentage of its mass.
MDL patterns for NEGRA10 and CTB10 are very
similar. For MDL+SC with T or P labels, most
of the induced labels annotate 100 constituents or
more. We thus expect MDL+SC to provide better
clustering than MDL; a good clustering evaluation
measure should reflect this expectation.
Table 3 shows V, VI, NVI and NVIK scores for
MDL and MDL+SC (with T or P labels). For all
three corpora, V values are almost identical for the
MDL and the MDL+SC schemes. This is in con-
trast to VI and NVI values that strongly prefer the
MDL+SC clusterings, fitting our expectations (re-
call that for these measures, the lower the score, the
better the clustering). Moreover, VI and NVI pre-
fer MDL+SC with P labels, which again accords
with our expectations, since P labels were defined
as those that are more salient in the data (see above).
The patterns of NVI and VI are identical, since
NV I = V IH(C) and H(C) is independent of the
induced clustering. However, the numbers given
by NVI are easier to interpret than those given by
VI. The latter are basically meaningless, convey-
ing nothing about clustering quality. The former are
quite close to 1, telling us that clustering quality is
not that good but not horrible either. This makes
sense, because the overall quality of the labeling in-
duction algorithm is indeed not that high: using one-
to-one mapping (the more forgiving mapping), the
accuracy of the labels induced by MDL+SC is only
45?72% (Reichart and Rappoport, 2008).
NVIK, the normalization of VI with H(K), is
worse even than V. This measure (which also gives
lower scores to better clusterings) prefers the MDL
over MDL+SC labels. This is a further justification
of our decision to define NVI by normalizing VI by
H(C) rather than by H(K).
Corpus H(C) H(K)
MDL T P
WSJ10 1.73 4.72 2.7 1.58
NEGRA10 1.69 3.36 1.87 1.29
CTB10 1.76 3.45 2.1 1.48
Table 4: Class (H(C)) and cluster (H(K)) entropy for
MDL and MDL+SC with T or P labels. H(C) is cluster
independent. H(K) increases with the number of clus-
ters.
Table 4 shows the H(C) and H(K) values in the
experiment. While H(C) is independent of the in-
duced clustering and is thus constant for a given
annotated corpus, H(K) monotonically increases
with the number of induced clusters. Since both
NVIK and the completeness term of V are normalized
by H(K), these measures prefer clusterings with a
large number of clusters even when many of these
clusters provide useless information.
7 Conclusion
Unsupervised clustering evaluation is important for
various NLP tasks and applications. Recently, the
importance of the completeness and homogeneity as
evaluation criteria for such clusterings has been rec-
ognized. In this paper we addressed the two mea-
sures that address these criteria: VI (Meila, 2007)
and V (Rosenberg and Hirschberg, 2007).
While VI has many useful properties, the range of
values it can take is dataset dependent, which makes
it unsuitable for comparing clusterings of different
datasets. This imposes a serious restriction on the
measure usage. We presented NVI, a normalized ver-
sion of VI, which does not have this restriction and
still retains some of its useful properties.
Using experiments with both synthetic data and
a complex NLP application, we showed that the V
measure prefers clusterings with many clusters even
when these are clearly of low quality. VI and NVI do
not exhibit such behavior, and the numbers given by
NVI are easier to interpret than those given by VI.
In future work we intend to explore more of the
properties of NVI and use it in other real NLP appli-
cations.
References
Amit Bagga and Breck Baldwin, 1998. Entity-based
cross-document coreferencing using the vector space
172
model. ACL 98.
Ulrike Baldewein, Katrin Erk, Sebastian Pado, and Detlef
Prescher 2004. Semantic role labeling with similarity
based generalization using EM?based clustering. Sen-
seval ?04.
Thorsten Brants, 1997. The NEGRA export format.
CLAUS Report, Saarland University.
Gideon Borensztajn and Willem Zuidema, 2007.
Bayesian model merging for unsupervised constituent
labeling and grammar induction. Technical Report,
ILLC. http: //staff.science.uva.nl/?gideon/
Alexander Clark, 2003. Combining distributional and
morphological information for part of speech induc-
tion. EACL ?03.
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. Infor-
mation theoretic co-clustering. KDD ?03.
Byron E. Dom, 2001. An information-theoretic external
cluster validity measure . Journal of American statis-
tical Association,78:553?569.
Jenny Rose Finkel and Christopher D. Manning, 2008.
Enforcing transitivity in coreference resolution. ACL
?08.
E.B Fowlkes and C.L. Mallows, 1983. A method for
comparing two hierarchical clusterings. Journal of
American statistical Association,78:553?569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester, 2003.
Hierarchical document clustering using frequent item-
sets. SIAM International Conference on Data Mining
?03.
Sharon Goldwater and Thomas L. Griffiths, 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. ACL ?07.
William P. Headden, David McClosky, and Eugene Char-
niak, 2008. Evaluating unsupervised part-of-speech
tagging for grammar induction. COLING ?08.
L. Hubert and P. Arabie, 1985. Comparing partitions.
Journal of Classification, 2:193?218.
L. Hubert and J. Schultz, 1976. Quadratic assignment
as a general data analysis strategy. British Journal
of Mathematical and Statistical Psychology, 29:190?
241.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and effec-
tive text mining using linear-time document clustering.
KDD ?99.
Gina-Anne Levow, 2006. Unsupervised and semi-
supervised learning of tone and pitch accent. HLT-
NAACL ?06.
Marina Meila and David Heckerman, 2001. An exper-
imental comparison of model-based clustering meth-
ods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing clustering ? an infor-
mation based distance. Journal of Multivariate Analy-
sis, 98:873?895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
effect of cluster size, dimensionality and the number
of clusters on recovery of true cluster structure. IEEE
transactions on Pattern Analysis and Machine Intelli-
gence, 5:40?47.
Boris G. Mirkin, 1996. Mathematical classification and
clustering. Kluwer Academic Press.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and evaluation
of similarity measures for pairs of clusterings. Knowl-
edge and Information Systems: An International Jour-
nal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statstical Association, 66(336):846?850.
Roi Reichart and Ari Rappoport, 2008. Unsupervised in-
duction of labeled parse trees by clustering with syn-
tactic features. COLING ?08.
Andrew Rosenberg and Julia Hirschberg, 2007. V?
Measure: a conditional entropy?based external cluster
evaluation measure. EMNLP ?07.
Sabine Schulte im Walde, 2003. Experiments on the
automatic induction of German semantic verb classes.
Ph.D. thesis, Universitat Stuttgart.
Yoav Seginer, 2007. Fast unsupervised incremental pars-
ing. ACL 07.
Sa-Im Shin and Key-Sun Choi, 2004. Automatic word
sense clustering using collocation for sense adaptation.
The Second Global WordNet Conference.
Stijn van Dongen, 2000. Performance criteria for graph
clustering and markov cluster experiments. Technical
report CWI, Amsterdam
Daniel D. Walker and Eric K. Ringger, 2008. Model-
based document clustering with a collapsed Gibbs
sampler. KDD ?08.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer, 2002.
Building a large?scale annotated Chinese corpus. ACL
?02.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An adaptive meta-clustering
approach: combining the information from different
clustering results. IEEE Computer Society Bioinfor-
matics Conference (CSB ?02) .
Ying Zhao and George Karypis, 2001. Criterion func-
tions for document clustering: experiments and analy-
sis. Technical Report TR 01-40, Department of Com-
puter Science, University of Minnesota.
173
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 241?249,
Beijing, August 2010
Automated Translation of Semantic Relationships
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a method for translating se-
mantic relationships between languages
where relationships are defined as pattern
clusters. Given a pattern set which rep-
resents a semantic relationship, we use
the web to extract sample term pairs of
this relationship. We automatically trans-
late the obtained term pairs using multi-
lingual dictionaries and disambiguate the
translated pairs using web counts. Finally
we discover the set of most relevant tar-
get language patterns for the given rela-
tionship. The obtained pattern set can be
utilized for extraction of new relationship
examples for the target language.
We evaluate our method on 11 diverse tar-
get languages. To assess the quality of
the discovered relationships, we use an au-
tomatically generated cross-lingual SAT
analogy test, WordNet relationships, and
concept-specific relationships, achieving
high precision. The proposed framework
allows fully automated cross-lingual rela-
tionship mining and construction of mul-
tilingual pattern dictionaries without rely-
ing on parallel corpora.
1 Introduction
Acquiring and understanding semantic relation-
ships is crucial for many NLP applications. In
many cases, we would like to know if a given
term pair participates in a specified semantic re-
lationship or if two different term pairs encode
the same (possibly unspecified) type of relation-
ship. Beyond the well-known major relationship
types such as hyponymy (is-a) and meronymy
(part-of), there is a huge number of other rela-
tionships between objects and concepts. Exam-
ples include general relations such as larger-than,
contained-in, liked-by and domain specific ones
such as country-language, product-manufacturer,
product-seller, drug-disease etc.
The vast majority of NLP research is done in
a few languages for which extensive corpora (in-
cluding the web) are available. As a result, most
relationship retrieval studies and lexical database
compilation efforts target only a few languages.
However, due to the substantial growth of the mul-
tilingual web1 and a growing demand for NLP
application coverage for less common languages,
there is a need for relationship data in many less
studied languages.
In this paper we address the task of translating
relationships between languages, which has two
obvious benefits. First, it can directly help appli-
cations such as machine translation, cross-lingual
information retrieval, cross-lingual web mining
and the construction and enrichment of seman-
tic databases. Second, it can assist applications
in a single language, especially when compensat-
ing for a relative scarcity of resources in that lan-
guage. We focus on relations between two enti-
ties, which are the most common type.
When discussing the translation of relation-
ships, it is important to define how these are rep-
resented and in what way the task differs from
MT. While there is wide agreement on the def-
inition and representation of major relationship
types such as hypernymy and (to a lesser extent)
meronymy, there is no single accepted method (or
1http://www.internetworldstats.com/stats7.htm
241
resources) for other less common relationships.
Among the methods that have been proposed for
specifying lexical relationships are natural lan-
guage description and rules (Girju et al, 2007),
distributional means (Turney, 2005), sample term
pairs (Pasca et al 2006), relationship instances
(Banko et al, 2007) and pattern clusters (Davi-
dov and Rappoport, 2008a).
In this paper we utilize the last definition. Fol-
lowing (Davidov and Rappoport, 2008a) each se-
mantic relationship can be defined and repre-
sented by a set of lexical patterns such that the
represented relation holds between entities filling
the patterns? slots. We focus on pattern clusters re-
lationship definition due to several reasons. First,
as opposed to natural language descriptions, pat-
tern clusters are formal. Second, as opposed to
the other methods above, pattern clusters provide
a ?generative? model for the represented relation-
ship ? it is possible to obtain from them relation-
ship instances and term pairs, as we indeed uti-
lize in this paper. Third, pattern clusters can be
mined in a fully unsupervised manner, or in a
focused manner when the relationship desired is
known. Finally, pattern methods have proven to
be highly efficient and effective for lexical acqui-
sition tasks (Pantel et al 2004; Davidov and Rap-
poport, 2006).
The proposed framework comprises the follow-
ing stages. First, given a set of patterns defining a
relationship in a source language, we obtain from
the web a set of corresponding term pairs. Next,
for each of the terms in the obtained term pairs,
we retrieve sets of their translations to the target
language using available multilingual dictionar-
ies. Now that we have a set of translations for
each term in each pair, we retrieve search engine
snippets with the translated term pairs. We then
select appropriate word senses using web counts,
and extract a set of patterns which connect these
disambiguated terms. As a result we get a set
of relation-specific target language patterns, ef-
fectively obtaining the desired relationship defi-
nition. We can optionally use the retrieved pattern
sets to obtain term pairs of target language rela-
tionships from the web.
We performed a thorough evaluation for var-
ious relationships involving 11 languages. We
tested our framework on major relationships like
meronymy, specific relationships like country-
capital and unspecified unsupervisedly discovered
English relationships. The obtained relationships
were manually verified by human judges using
cross-lingual SAT analogy questions, and a few
specific factual relationships were evaluated using
a gold standard.
Our main contribution is a novel framework
for automated relationship translation across lan-
guages, where relationships are defined as pattern
clusters or as term pairs. This framework allows
fully automated cross-lingual relationship mining
and construction of multilingual pattern dictionar-
ies without relying on parallel corpora.
In Section 2 we discuss related work. Section 3
details the algorithm. Section 4 describes the eval-
uation, and Section 5 concludes.
2 Related work
Recently, with the development of practical appli-
cations which utilize WN-like databases in dozens
of languages, great effort has been made to manu-
ally construct and interconnect such databases for
different languages (Pease et al 2008; Charoen-
porn et al, 2007). Some studies (e.g., (Amasyali,
2005)) use semi-automated methods based on
language-specific heuristics and dictionaries.
At the same time, much work has been done
on automated lexical acquisition for a single lan-
guage, and in particular, on the web-based ac-
quisition of various types of semantic relation-
ships. There is a substantial amount of related
studies which deal with the discovery of vari-
ous relationship types represented in useful re-
sources such as WordNet, including hypernymy
(Pantel et al 2004; Snow et al, 2006), synonymy
(Davidov and Rappoport, 2006; Widdows and
Dorow, 2002) and meronymy (Berland and Char-
niak, 1999; Girju et al 2006). Since named
entities are very important in NLP, many studies
define and discover relations between named en-
tities (Hassan et al, 2006). Work was also done
on relations between verbs (Chklovski and Pan-
tel, 2004). There is growing research on relations
between nominals (Girju et al, 2007).
While the majority of studies focus on extract-
ing pre-specified semantic relationships, several
242
recent studies were done on the automated discov-
ery of unspecified relationship types. Thus Tur-
ney (2006) provided a pattern distance measure
that allows a fully unsupervised measurement of
relational similarity between two pairs of words
on the same language. Banko et al (2007) and
Rosenfeld and Feldman (2007) find relationship
instances where the relationships are not speci-
fied in advance. (Davidov and Rappoport, 2008a)
introduced the idea that salient semantic relation-
ships can be defined as pattern clusters, confirm-
ing it with SAT analogy test. As explained above,
we use this definition in the present study. We
also use pattern clusters given by (Davidov and
Rappoport, 2008a) as input in our evaluation.
Most of the relationship acquisition studies
were done in a single language. Those that ex-
periment in several languages usually treat each
language separately, while we extract a relation-
ship definition for one language using the pro-
vided definition for the other language.
Our study is related to cross-language infor-
mation retrieval (CLIR) frameworks. Both deal
with multilingual information extracted from the
Web. However, the majority of CLIR stud-
ies pursue different targets. Thus, one of the
main CLIR goals is the retrieval of documents
based on explicit queries, when the document
language is not the query language (Volk and
Buitelaar, 2002). These frameworks usually de-
velop language-specific tools and algorithms in-
cluding parsers, taggers and morphology analyz-
ers in order to integrate multilingual queries and
documents (Jagarlamudi and Kumaran, 2007).
Our goal is to develop and evaluate a language-
independent algorithm for the cross-lingual trans-
lation of relationship-defining structures. While
our targets are different from those of CLIR, CLIR
systems can greatly benefit from our framework,
since we can translate the relationships in CLIR
queries and subsequently check if the same rela-
tionships are present in the retrieved documents.
Another field indirectly related to our research
is Machine translation (MT). Many MT tasks re-
quire automated creation or improvement of dic-
tionaries (Koehn and Knight, 2001). However,
MT mainly deals with translation and disambigua-
tion of words at the sentence or document level,
while we translate relationship structures as a set
of patterns, defined independently of contexts.
We also perform pattern-set to pattern-set trans-
lation rather than the pattern-to-pattern or pair-to-
pair translation commonly explored in MT stud-
ies. This makes it difficult to perform meaning-
ful comparison to existing MT frameworks. How-
ever, the MT studies benefit from the proposed
framework by enhancement and verification of
translated relationship instances.
In (Davidov and Rappoport, 2009), we pro-
posed a framework for automated cross-lingual
concept mining. We incorporate several princi-
ples from this study including concept extension
and disambiguation of query language (See Sec-
tion 3.3). However our goals here are different
since we target cross-lingual acquisition of rela-
tionship structures rather then concept term lists.
3 Relationship Translation Framework
Our framework has the following stages: (1) given
a set of patterns in a source language defining
some lexical relationship, we use the web to ob-
tain source language term pairs participating in
this relationship; (2) we automatically translate
the obtained terms in each pair to the target lan-
guage using available multilingual dictionaries;
(3) we retrieve web snippets where these transla-
tions co-appear, disambiguating translations with
web counts and extracting the corresponding pat-
terns. As an optional final stage, the translated
pattern cluster can be used to extract and extend
a set of target language term pairs. Now we de-
scribe each of these stages in detail.
3.1 Acquisition of representative term pairs
We are provided with a pattern cluster, a set of pat-
terns representing a specific lexical relationship in
some language. The goal of the first stage is to
discover the most representative term pairs for this
cluster and language from the web. If the relation-
ship is already specified by a representative set of
term pairs, we skip this stage and continue to the
next stage. Note that the method described be-
low can also be used at the final stage to obtain
representative target language term pairs once we
obtain a target language pattern cluster.
The input lexical patterns are surface patterns
243
which include several fixed words or punctuation
symbols and two slots for content words, e.g. ?the
[X] of the [Y],?. Given a cluster of patterns defin-
ing a semantic relationship, we would like to ob-
tain from the web the most representative and fre-
quent examples of the represented relationship.
In order to do that we construct search engine
queries2 from the given patterns using wildcard
symbols to represent pattern slots. For example,
given a pattern ?the [X] of the [Y],? we construct
queries such as ?the * of the?; ?the * * of the?3.
We collect all the retrieved search engine snip-
pets and extract the appropriate term pairs found
in these snippets.
Now we would like to select the most useful of
the extracted pairs. Since the obtained pairs are
only useful if we can translate them into the tar-
get language, we dismiss all pairs in which one or
both terms have no translations to the target lan-
guage in our dictionaries (see Section 3.2). Since
each particular pattern can be ambiguous, we also
dismiss pairs which were found for only a single
pattern in the given cluster.
For the remaining term pairs we would like
to estimate their specificity for the given pattern
cluster. For each pattern, we retrieve and use two
web hit counts: Fterms(p, T1, T2), a hit count for
co-appearance of the pair in a way similar to that
in the pattern, and Fall(p, T1, T2), the hit count
of the full pattern instance.
For example, if for the pattern p=?the * of
the? we obtain a term pair (CEO, company), then
Fall(p)=Hits(?the CEO of the company?) and
Fterms(CEO, company)= Hits(?CEO * * com-
pany?). Given a pattern cluster C with patterns
{p1 . . . pn} ? C, we estimate the specificity of
a term pair (T1, T2) using the following simple
probabilistic metric, giving to all patterns in the
cluster an equal weight:
Spec(T1, T2) = 1n
?
pi?C
Fall(pi, T1, T2)
Fterms(pi, T1, T2)
We select the top 15 pairs with the highest speci-
ficity and use them in the next stage.
2We use Yahoo! Boss.
3Since the search engine API doesn?t allow punctuation,
we omit the punctuation in queries, but require a proper
punctuation when processing the obtained snippet data.
3.2 Translation of the term pairs
After the previous stage we have a good represen-
tative set of term pairs for the desired source lan-
guage relationship. Now we would like to trans-
late the words in these pairs to the target language.
In order to do that we use an extensive set of
1067 multilingual dictionaries developed for Star-
Dict4, including Wikipedia cross-language links
and Wiktionary. For each term we obtain a set
of its translations to the target language. If we
get more than five different translations, we select
the five having the highest number of dictionaries
where this translation appears.
As discussed in Section 3.1, we dismissed
terms for which no translation was found in any of
the available dictionaries, so each term in each of
the obtained pairs has at least a single translation
to the target language. However, in many cases the
available translations represent the wrong word
sense, since both the source terms and their trans-
lations can be ambiguous. Thus at this stage many
of the obtained term translations are irrelevant for
the given relationship and require disambiguation.
3.3 Web mining for translation contexts
For this stage, we need to restrict web mining
to specific target languages. This restriction is
straightforward if the alphabet or term translations
are language-specific or if the search API supports
restriction to this language. In case where there is
no such natural restrictions, we attempt to detect
and add to our queries a few language-specific fre-
quent words. Following (Davidov and Rappoport,
2009), we use our dictionaries to find 1?3 of the
15 most frequent words in a desired language5 that
are unique to that language and ?and? them with
the queries to ensure proper language selection.
This allows applying our algorithm to more than
60 diverse languages. The only data required for
each language is at least a partial coverage of the
obtained term pairs by some available dictionary.
Given a term pair (T1, T2) we obtain a set
of translations (T1?i?1...n, T2?j?1...m). For each
combination T1?i, T2?j of the obtained term trans-
lations, we construct and execute the following
4http://stardict.sourceforge.net/
5We estimated the word frequencies from text available
in the corresponding multilingual dictionaries.
244
four queries: {?T1?i ? T2?j?, ?T2?j ? T1?i?,
?T1?i ? ? T2?j?, ?T2?j ? ? T1?i?}6. Since
Y ahoo!Boss allows retrieval of up to the 1000
first results, we can collect up to four thousand
snippets for each combination. However, the ma-
jority of these combinations return no snippets at
all, effectively generating an average of a dozen
snippets per query.
3.4 Pattern extraction
Now for each pair of term translations we would
like to extract from the snippets all surface pat-
terns which connect the terms in this pair. We use
the basic two-slot meta-pattern type:
[Prefix] X [Infix] Y [Postfix]
X and Y should be the translated terms, Infix may
contain punctuation, spaces, and up to four words
(or up to eight symbols in languages without
space-separated words like Chinese). Prefix and
Postfix are limited to contain one or zero punctu-
ation characters and/or up to two words. We do
not allow empty Infix, Prefix of Postfix. If there
are several possible combinations of Prefix and
Postfix we generate a pattern set for all possible
combinations (e.g., if we retrieve a snippet . . . ?,
consider using [plexiglass] for [kitchen].?. . . , we
create patterns ?using X for Y.?, ?consider using
X for Y.? and ?, consider using X for Y.?).
Now we would like to find the patterns repre-
senting the relationship in the target language. We
do this in two stages. First we would like to detect
the most common patterns for the given relation-
ship. Let Sk be the union set of all patterns ob-
tained for all combinations of the extracted trans-
lations for a specific source language term pair
k ? 1 . . .K. Let Salience(p) = 1K |{k|p ? Sk}|
be the portion of source language term pairs which
lead to detection of the target language pattern
p. We compute salience for each pattern, and
select a subset of salient patterns, defined to be
those whose Salience exceeds a predefined thresh-
old (we used 1/3). If one salient pattern is a sub-
string of another salient pattern, we only select the
longer one.
6These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
In our salience estimation we mix data from
all combinations of translations including incor-
rect senses and wrong translations of ambiguous
terms. Now we would like to select a single cor-
rect target language pair for each source language
pair in order to find more refined relationship rep-
resenting patterns. For each source language term
pair, we select the target language translated pair
which captured the highest number of salient pat-
terns. In case there are several pairs with the same
number of salient patterns, we select a pair with
the greatest web hit count. We drop term pairs
with zero salient patterns.
Finally we would like to enhance the obtained
set of salient patterns with more precise and rep-
resentative relationship-specific patterns. Since
we disambiguated the translated pairs, target lan-
guage patterns captured by the remaining term
pairs should be more trusted. We compare the
target language pattern sets obtained for differ-
ent remaining term pairs, and collect all patterns
that were captured by at least three different term
pairs. As before, if one pattern is a substring of
another we retain only the longer one. As a result
we get a comprehensive target language pattern
cluster for the desired relationship.
3.5 Retrieval of target language term pairs
As an optional final stage, we can utilize the re-
trieved target language pattern clusters in order to
discover target language term pairs for the desired
relationship. We do this by utilizing the strategy
described in Section 3.1 on the obtained target
language pattern clusters. We do not dismiss ob-
tained terms having no available dictionary trans-
lations, and we do not limit our search to the 15
terms with highest specificity. Instead we either
select N term pairs with top specificity (where N
is provided by user as in our evaluation), or we
select all term pairs with specificity above some
threshold.
4 Evaluation
In order to test the quality of the translated pat-
tern clusters and the corresponding translated term
pairs, we need to check both flexibility and cor-
rectness. Flexibility measures how well the re-
trieval works well across languages and for many
245
types of semantic relationships. To do that, we
tested our framework on both generic and specific
relationships for 11 languages. Correctness ver-
ifies that the retrieved set of target language pat-
terns and the corresponding term pairs represent
the same semantic relationship as the given set
of source language term pairs or patterns. To do
that, we used both manual cross-lingual analogy-
based correctness evaluation and evaluation based
of factual data.
4.1 Languages and relationships
One of the main goals in this research was to pro-
vide a fully automated and flexible framework,
which requires minimal modifications when ap-
plied to different languages and relationships.
We examined an extensive set of target lan-
guages using English as a source language. Ta-
ble 1 shows 11 languages used in our experiments.
We included west European languages, Slavic lan-
guages like Russian, Semitic languages like He-
brew, and Asian languages such as Chinese. We
developed a set of tools for automatic off-line ac-
cess to an extensive set of 1067 multilingual dic-
tionaries created for the StarDict platform. These
dictionaries include recent dumps of Wikipedia
cross-language links and Wiktionary data.
In our experiments we used three sets of rela-
tionships: (1) Generic: 15 unsupervisedly dis-
covered English pattern clusters representing var-
ious generic relationships. (2) H-M-C: The
three most studied relationships: hypernymy,
meronymy and co-hyponymy.(3) Specific: Three
factual relationships: country-capital, country-
language and dog breed-origin. Below we de-
scribe the evaluation of each of these sets in de-
tail. Note that our framework allows two ways of
specifying a source language relationship ? a pat-
tern cluster and a set of term pairs.
4.2 Evaluation of generic pattern clusters
In our Generic evaluation setting, we utilized as
input a random sample of 15 automatically dis-
covered relationship definitions. We started from
a set of 508 English pattern clusters, unsuper-
visedly discovered using the method of (Davidov
and Rappoport, 2008a). Each of these clusters
is assumed to represent a distinct semantic rela-
tionship. We randomly selected 15 pattern clus-
ters from this set and executed our framework on
these clusters to obtain the corresponding target
language pattern clusters for each of the 11 tested
languages. An example of a partial set of patterns
in a cluster is: ?this [X] was kept in [Y],?;?the X that he
kept in [Y],?;?the [X] in the [Y] and?;?the [Y] containing
the [X]?. . . .
We then used the term pair selection algorithm
described in Section 3.1 to select the most spe-
cific term pair for each of the 15 source language
clusters and 10 pairs for each of the corresponding
translated target language clusters. Thus for each
of the 15 pattern clusters and for each of the 11
languages we produced a single source language
term pair and up to 10 corresponding target lan-
guage term pairs.
In order to check the correctness of transla-
tion of an unspecified semantic relationship we
need to compare source and target language rela-
tionships. Comparison of relationships is a chal-
lenging task, since there are no relationship re-
sources for most relationship types even in a sin-
gle language, and certainly so for their trans-
lations across languages. Thus various studies
define and split generic relationships differently
even when describing relatively restricted rela-
tionship domains (e.g., relationships holding be-
tween parts of noun phrases (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004)). In order
to compare generic relationships we used a man-
ual cross-lingual SAT-like analogy human judg-
ment evaluation7. This allowed us to assess the
quality of the translated pattern clusters, in a sim-
ilar way as (Davidov and Rappoport, 2008a) did
for testing clusters in a single language.
For each of the 15 clusters we constructed a
cross-lingual analogy question in the following
manner. The header of the question was a term
pair obtained for the source language pattern clus-
ter. The six multiple choice items included: (1)
one of the 10 discovered translated term pairs of
the same cluster (the ?correct? answer)8; (2) three
7Using Amazon?s Mechanical Turk.
8We avoid selection of the target language pairs which
were obtained through direct translation of the source lan-
guage pair given at the header of the question. This is crucial
so that subjects will not judge correctness of translation but
correctness of the relationship.
246
of the translated pairs of the other clusters among
the 15; (3) a pair constructed by randomly select-
ing terms from different translated clusters; (4) the
6th option states that either the given options in-
clude broken words or incorrect language, or none
of the presented pairs even remotely exemplifies
the relationship in question. An example question
for English-Italian:
The English pair: (kennel, dog); (1) ?correct? pair: (ac-
quario, pesce ); (2)-(4) ?wrong? pairs: (topo, orecchio),
(mela, rossa), (occhio, grande); (5) ?random?: (scodella,
scatola); (6) Pairs comprise non-Italian/broken words or no
pair exemplifies the relationship
In order to check the English proficiency of the
subjects we added 5 ?easy? monolingual English
SAT analogy questions. We also added a single
hand-crafted cross-lingual question of an obvious
analogy case, making a total of 16 cross-lingual
questions. Subjects who failed more than one of
the easy English SAT questions or failed the obvi-
ous cross-lingual question were rejected from the
evaluation. Finally we have three subjects for each
of the tested languages. We also asked the sub-
jects to assign a confidence score from 0 (worst)
to 10 (best) to express how well the selected term
pair represents the source language relationship in
question.
Language P % 6th Scorec Scorew
Chinese 71 9 9.1 1.8
Czech 73 9 8.3 2.0
French 80 10 8.4 1.9
German 68 9 8.3 1.5
Greek 72 11 8.7 2.0
Hebrew 69 11 9.0 2.5
Hindi 62 12 7.4 1.9
Italian 70 10 8.5 1.5
Russian 75 8 9.0 1.6
Turkish 61 13 9.1 2.0
Ukrainian 73 11 9.3 2.3
Average 70 10 9.1 1.9
Table 1: Averaged results for manual evaluation of 15 pat-
tern clusters. P: precision (% of correct answers); % 6th: per-
centage of 6th selection; Scorec: averaged confidence score
for correct selections; Scorew: confidence score for wrong
selections.
We computed accuracy and agreement for the
given answers (Table 1). We can see that for all
languages above 61% of the choices were cor-
rect (comparing to 75% reported by (Davidov
and Rappoport, 2008a) for a similar monolingual
analogy test for the same set of pattern clusters).
While the results are obviously lower than the cor-
responding single-language test, they are signifi-
cantly above the random baseline of 20%9. Also
note that as reported in (Turney, 2006), an aver-
age single-language highschool SAT grade is 57,
which is lower than the scores obtained for our
cross-lingual test. We can also see that for the cor-
rectly selected pairs the confidence score was very
high, while the score for wrongly selected pairs
was significantly lower.
4.3 Evaluation of the H-M-C relationships
In order to test how well our algorithm performs
on the most common and useful relationships, hy-
pernymy, meronymy and co-hyponymy, we au-
tomatically sampled from WordNet a set of 10
source language term pairs for each of these re-
lationships and applied our framework to extract
up to 100 target language term pairs for each of
the three relationships as done above.
For each of the tested languages we presented
to three human subjects for each language a short
English definition of hypernymy, meronymy and
co-hyponymy, along with the corresponding ran-
domly selected 10 of 100 extracted pairs, and
asked them to rank how well (0 (worst) to 10
(best)) each pair represents the described relation-
ship. In order to reduce possible bias, we mixed in
each set 3 randomly selected term pairs obtained
for the other two relationships. Table 2 shows the
average scores for this task.
Language Hypernymy Meronymy Co-hyponymy Random
Chinese 8.0 7.1 8.1 1.9
Czech 8.4 7.0 8.5 2.3
French 8.1 7.5 8.4 1.8
German 8.4 7.1 8.6 2.4
Greek 8.7 7.5 8.6 1.8
Hebrew 8.6 7.9 8.3 1.6
Hindi 7.5 7.1 7.8 2.2
Italian 7.9 7.8 8.2 1.5
Russian 8.6 8.1 8.9 1.7
Turkish 8.3 7.2 8.6 1.7
Ukrainian 8.2 7.7 8.2 1.7
Average 8.3 7.5 8.4 1.9
Table 2: Averaged results for hypernymy, meronymy and
co-hyponymy translations. The three first columns show av-
erage scores for hypernymy, meronymy and co-hyponymy
relationships. The last column shows scores for the random
baseline.
We can see that our algorithm successfully de-
tects the common relationships, achieving high
scores. Also the results indicate that the patterns
9A reasonable random baseline omits the 6th option.
247
are sufficiently precise to extract at least 100 of
the instances for the given salient relationships.
4.4 Evaluation of the specific relationships
To check how well our algorithm performs on
some specific relationships, we examined its per-
formance on three specific relationships explored
in previous studies. We provided it with 10 source
language (English) term pair examples for each
of the (country, capital), (country, language) and
(dog breed, origin) relationships. For each of
these relationships we have factual information
for every tested target language available through
Wikipedia list articles. This allows us to perform
an unbiased automated evaluation of the quality of
the obtained target language data.
We applied our framework on these examples
and generated 30 target language pairs with high-
est specificity for each of these relationships and
languages. We compared the retrieved pairs to the
factual data. Table 3 shows the precision of the
results obtained for these patterns.
Language Capital Language Dog breed
Chinese 0.87 0.83 0.8
Czech 0.93 0.83 0.77
French 0.97 0.9 0.87
German 0.93 0.9 0.83
Greek 0.87 0.83 0.77
Hebrew 0.83 0.8 0.8
Hindi 0.83 0.8 0.77
Italian 0.93 0.87 0.83
Russian 0.97 0.9 0.87
Turkish 0.87 0.83 0.83
Ukrainian 0.93 0.87 0.8
Average 0.9 0.85 0.81
Table 3: Precision for three specific relationship
types: (country, capital), (country, language) and (dog
breed,origin).
The precision observed for this task is compara-
ble to precision obtained for Country-Capital and
Country-Language in a previous single-language
acquisition study (Davidov et al, 2007)10. The
high precision observed for this task indicates that
the obtained translated patterns are sufficiently
good as a seed for pattern-based mining of spe-
cific relationships.
10It should be noted however that unlike previous work,
we only examine the first 30 pairs and we do not use addi-
tional disambiguating words as input.
5 Conclusion
We proposed a framework which given a set of
patterns defining a semantic relationship in a spe-
cific source language uses multilingual dictionar-
ies and the web to discover a corresponding pat-
tern cluster for a target language. In the evaluation
we confirmed the applicability of our method for
different languages and relationships.
The obtained set of target language pattern clus-
ters can be used for acquisition of relationship in-
stances as shown in our evaluation. An interest-
ing direction for future work is to use the discov-
ered target language pattern clusters in NLP tasks
like textual entailment which require distinguish-
ing between semantic relationships.
Applying our framework to the set of unsuper-
visedly discovered relationships allows a fully au-
tomated construction of a relationship dictionary,
where pattern clusters in one language correspond
to patten clusters in many other languages. Un-
like the majority of existing machine translation
systems, construction of this dictionary does not
require parallel corpora. Such a dictionary can be
useful for machine translation, cross-lingual tex-
tual entailment and query translation, to name just
a few applications. In the future we plan to create
a multilingual pattern cluster dictionary which in-
terconnects pattern clusters from many languages
and allows cross-lingual definition of lexical rela-
tionships.
References
Amasyali Fatih, 2005. Automatic Construction of
Turkish Wordnet. Signal Processing and Commu-
nications Applications Conference.
Mishele Banko, Michael Cafarella , Stephen Soder-
land, Matt Broadhead, Oren Etzioni, 2007. Open
information extraction from the Web. IJCAI ?07.
Matthew Berland, Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Thatsanee Charoenporn, Virach Sornlertlamvanich,
Chumpol Mokarat, and Hitoshi Isahara, 2008.
Semi-automatic Compilation of Asian WordNet.
Proceedings of the 14th NLP-2008, University of
Tokyo, Komaba Campus, Japan.
Timothy Chklovski, Patrick Pantel, 2004. VerbOcean:
248
mining the web for fine-grained semantic verb rela-
tions. EMNLP ?04.
Dmitry Davidov, Ari Rappoport, 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport and Moshe Koppel,
2007. Fully Unsupervised Discovery of Concept-
Specific Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport. 2008a. Unsuper-
vised Discovery of Generic Relationships Using
Pattern Clusters and its Evaluation by Automatically
Generated SAT Analogy Questions. ACL ?08.
Dmitry Davidov and Ari Rappoport, 2008b. Classifi-
cation of relationships between nominals using pat-
tern clusters. ACL ?08.
Dmitry Davidov and Ari Rappoport, 2009. Transla-
tion and Extension of Concepts Across Languages.
EACL ?09.
Roxana Girju, Adriana Badulescu, and Dan Moldovan,
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1).
Roxana Girju, Marthy Hearst, Preslav Nakov, Vivi
Nastase, Stan Szpakowicz, Peter Turney and Yuret,
D., 2007. Task 04: Classification of semantic re-
lations between nominal at SemEval 2007. 4th Intl.
Workshop on Semantic Evaluations (SemEval ?07),
in ACL ?07.
Hany Hassan, Ahmed Hassan and Ossama Emam,
2006. Unsupervised information extraction ap-
proach using graph mutual reinforcement. EMNLP
?06.
Jagadeesh Jagarlamudi, A Kumaran, 2007 Cross-
Lingual Information Retrieval System for Indian
Languages Working Notes for the CLEF 2007
Workshop.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. EMNLP
?01.
Dan Moldovan, Adriana Badulescu, Marta Tatu,
Daniel Antohe, and Roxana Girju, 2004. Mod-
els for the semantic classification of noun phrases.
HLT-NAACL ?04 Workshop on Computational Lexi-
cal Semantics.
Vivi Nastase, Stan Szpakowicz, 2003. Exploring
noun-modifier semantic relations. In Fifth Intl.
Workshop on Computational Semantics (IWCS-5).
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and similari-
ties on the web: fact extraction in the fast lane.
COLING-ACL 06.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the Global WordNet Grid. CIL18.
Benjamin Rosenfeld , Ronen Feldman, 2007. Cluster-
ing for unsupervised relation identification. CIKM
?07.
Rion Snow, Daniel Jurafsky, Andrew Ng, 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. COLING-ACL ?06.
Peter Turney, 2005. Measuring semantic similarity by
latent relational analysis, IJCAI ?05.
Peter Turney, 2006. Expressing implicit semantic re-
lations without supervision. COLING-ACL ?06.
Martin Volk, Paul Buitelaar, 2002 A Systematic Eval-
uation of Concept-Based Cross-Language Informa-
tion Retrieval in the Medical Domain. In: Proc. of
3rd Dutch-Belgian Information Retrieval Workshop.
Leuven.
Dominic Widdows, Beate Dorow, 2002. A graph
model for unsupervised Lexical acquisition. COL-
ING ?02.
249
Coling 2010: Poster Volume, pages 241?249,
Beijing, August 2010
Enhanced Sentiment Learning Using Twitter Hashtags and Smileys
Dmitry Davidov? 1 Oren Tsur? 2
1ICNC / 2Institute of Computer Science
The Hebrew University
{oren,arir}@cs.huji.ac.il
Ari Rappoport 2
Abstract
Automated identification of diverse sen-
timent types can be beneficial for many
NLP systems such as review summariza-
tion and public media analysis. In some of
these systems there is an option of assign-
ing a sentiment value to a single sentence
or a very short text.
In this paper we propose a supervised
sentiment classification framework which
is based on data from Twitter, a popu-
lar microblogging service. By utilizing
50 Twitter tags and 15 smileys as sen-
timent labels, this framework avoids the
need for labor intensive manual annota-
tion, allowing identification and classifi-
cation of diverse sentiment types of short
texts. We evaluate the contribution of dif-
ferent feature types for sentiment classifi-
cation and show that our framework suc-
cessfully identifies sentiment types of un-
tagged sentences. The quality of the senti-
ment identification was also confirmed by
human judges. We also explore dependen-
cies and overlap between different sen-
timent types represented by smileys and
Twitter hashtags.
1 Introduction
A huge amount of social media including news,
forums, product reviews and blogs contain nu-
merous sentiment-based sentences. Sentiment is
defined as ?a personal belief or judgment that
?* Both authors equally contributed to this paper.
is not founded on proof or certainty?1. Senti-
ment expressions may describe the mood of the
writer (happy/sad/bored/grateful/...) or the opin-
ion of the writer towards some specific entity (X
is great/I hate X, etc.).
Automated identification of diverse sentiment
types can be beneficial for many NLP sys-
tems such as review summarization systems, dia-
logue systems and public media analysis systems.
Sometimes it is directly requested by the user to
obtain articles or sentences with a certain senti-
ment value (e.g Give me all positive reviews of
product X/ Show me articles which explain why
movie X is boring). In some other cases obtaining
sentiment value can greatly enhance information
extraction tasks like review summarization. While
the majority of existing sentiment extraction sys-
tems focus on polarity identification (e.g., positive
vs. negative reviews) or extraction of a handful of
pre-specified mood labels, there are many useful
and relatively unexplored sentiment types.
Sentiment extraction systems usually require
an extensive set of manually supplied sentiment
words or a handcrafted sentiment-specific dataset.
With the recent popularity of article tagging, some
social media types like blogs allow users to add
sentiment tags to articles. This allows to use blogs
as a large user-labeled dataset for sentiment learn-
ing and identification. However, the set of senti-
ment tags in most blog platforms is somewhat re-
stricted. Moreover, the assigned tag applies to the
whole blog post while a finer grained sentiment
extraction is needed (McDonald et al, 2007).
With the recent popularity of the Twitter micro-
blogging service, a huge amount of frequently
1WordNet 2.1 definitions.
241
self-standing short textual sentences (tweets) be-
came openly available for the research commu-
nity. Many of these tweets contain a wide vari-
ety of user-defined hashtags. Some of these tags
are sentiment tags which assign one or more senti-
ment values to a tweet. In this paper we propose a
way to utilize such tagged Twitter data for classi-
fication of a wide variety of sentiment types from
text.
We utilize 50 Twitter tags and 15 smileys as
sentiment labels which allow us to build a clas-
sifier for dozens of sentiment types for short tex-
tual sentences. In our study we use four different
feature types (punctuation, words, n-grams and
patterns) for sentiment classification and evaluate
the contribution of each feature type for this task.
We show that our framework successfully identi-
fies sentiment types of the untagged tweets. We
confirm the quality of our algorithm using human
judges.
We also explore the dependencies and overlap
between different sentiment types represented by
smileys and Twitter tags.
Section 2 describes related work. Section 3
details classification features and the algorithm,
while Section 4 describes the dataset and labels.
Automated and manual evaluation protocols and
results are presented in Section 5, followed by a
short discussion.
2 Related work
Sentiment analysis tasks typically combine two
different tasks: (1) Identifying sentiment expres-
sions, and (2) determining the polarity (sometimes
called valence) of the expressed sentiment. These
tasks are closely related as the purpose of most
works is to determine whether a sentence bears a
positive or a negative (implicit or explicit) opinion
about the target of the sentiment.
Several works (Wiebe, 2000; Turney, 2002;
Riloff, 2003; Whitelaw et al, 2005) use lexical re-
sources and decide whether a sentence expresses
a sentiment by the presence of lexical items (sen-
timent words). Others combine additional feature
types for this decision (Yu and Hatzivassiloglou,
2003; Kim and Hovy, 2004; Wilson et al, 2005;
Bloom et al, 2007; McDonald et al, 2007; Titov
and McDonald, 2008a; Melville et al, 2009).
It was suggested that sentiment words may have
different senses (Esuli and Sebastiani, 2006; An-
dreevskaia and Bergler, 2006; Wiebe and Mihal-
cea, 2006), thus word sense disambiguation can
improve sentiment analysis systems (Akkaya et
al., 2009). All works mentioned above identify
evaluative sentiment expressions and their polar-
ity.
Another line of works aims at identifying a
broader range of sentiment classes expressing var-
ious emotions such as happiness, sadness, bore-
dom, fear, and gratitude, regardless (or in addi-
tion to) positive or negative evaluations. Mihalcea
and Liu (2006) derive lists of words and phrases
with happiness factor from a corpus of blog posts,
where each post is annotated by the blogger with
a mood label. Balog et al (2006) use the mood
annotation of blog posts coupled with news data
in order to discover the events that drive the dom-
inant moods expressed in blogs. Mishne (2005)
used an ontology of over 100 moods assigned
to blog posts to classify blog texts according to
moods. While (Mishne, 2005) classifies a blog en-
try (post), (Mihalcea and Liu, 2006) assign a hap-
piness factor to specific words and expressions.
Mishne used a much broader range of moods.
Strapparava and Mihalcea (2008) classify blog
posts and news headlines to six sentiment cate-
gories.
While most of the works on sentiment analy-
sis focus on full text, some works address senti-
ment analysis in the phrasal and sentence level,
see (Yu and Hatzivassiloglou, 2003; Wilson et al,
2005; McDonald et al, 2007; Titov and McDon-
ald, 2008a; Titov and McDonald, 2008b; Wilson
et al, 2009; Tsur et al, 2010) among others.
Only a few studies analyze the sentiment and
polarity of tweets targeted at major brands. Jansen
et al (2009) used a commercial sentiment ana-
lyzer as well as a manually labeled corpus. Davi-
dov et al (2010) analyze the use of the #sarcasm
hashtag and its contribution to automatic recogni-
tion of sarcastic tweets. To the best of our knowl-
edge, there are no works employing Twitter hash-
tags to learn a wide range of emotions and the re-
lations between the different emotions.
242
3 Sentiment classification framework
Below we propose a set of classification features
and present the algorithm for sentiment classifica-
tion.
3.1 Classification features
We utilize four basic feature types for sentiment
classification: single word features, n-gram fea-
tures, pattern features and punctuation features.
For the classification, all feature types are com-
bined into a single feature vector.
3.1.1 Word-based and n-gram-based features
Each word appearing in a sentence serves as a
binary feature with weight equal to the inverted
count of this word in the Twitter corpus. We also
took each consecutive word sequence containing
2?5 words as a binary n-gram feature using a sim-
ilar weighting strategy. Thus n-gram features al-
ways have a higher weight than features of their
component words, and rare words have a higher
weight than common words. Words or n-grams
appearing in less than 0.5% of the training set sen-
tences do not constitute a feature. ASCII smileys
and other punctuation sequences containing two
or more consecutive punctuation symbols were
used as single-word features. Word features also
include the substituted meta-words for URLs, ref-
erences and hashtags (see Subsection 4.1).
3.1.2 Pattern-based features
Our main feature type is based on surface pat-
terns. For automated extraction of patterns, we
followed the pattern definitions given in (Davidov
and Rappoport, 2006). We classified words into
high-frequency words (HFWs) and content words
(CWs). A word whose corpus frequency is more
(less) than FH (FC) is considered to be a HFW
(CW).We estimate word frequency from the train-
ing set rather than from an external corpus. Unlike
(Davidov and Rappoport, 2006), we consider all
single punctuation characters or consecutive se-
quences of punctuation characters as HFWs. We
also consider URL, REF, and HASHTAG tags as
HFWs for pattern extraction. We define a pattern
as an ordered sequence of high frequency words
and slots for content words. Following (Davidov
and Rappoport, 2008), the FH and FC thresholds
were set to 1000 words per million (upper bound
for FC) and 100 words per million (lower bound
for FH )2.
The patterns allow 2?6 HFWs and 1?5 slots for
CWs. To avoid collection of patterns which cap-
ture only a part of a meaningful multiword ex-
pression, we require patterns to start and to end
with a HFW. Thus a minimal pattern is of the
form [HFW] [CW slot] [HFW]. For each sentence
it is possible to generate dozens of different pat-
terns that may overlap. As with words and n-gram
features, we do not treat as features any patterns
which appear in less than 0.5% of the training set
sentences.
Since each feature vector is based on a single
sentence (tweet), we would like to allow approx-
imate pattern matching for enhancement of learn-
ing flexibility. The value of a pattern feature is
estimated according the one of the following four
scenarios3:
?
????????????????????????
????????????????????????
1
count(p) : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
?
count(p) : Sparse match ? same as exact match
but additional non-matching words can
be inserted between pattern components.
??n
N?count(p) : Incomplete match ? only n > 1 of N
pattern components appear in
the sentence, while some non-matching
words can be inserted in-between.
At least one of the appearing components
should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
? = ? = 0.1 in all experiments.
This pattern based framework was proven effi-
cient for sarcasm detection in (Tsur et al, 2010;
2Note that the FH and FC bounds allow overlap between
some HFWs and CWs. See (Davidov and Rappoport, 2008)
for a short discussion.
3As with word and n-gram features, the maximal feature
weight of a pattern p is defined as the inverse count of a pat-
tern in the complete Twitter corpus.
243
Davidov et al, 2010).
3.1.3 Efficiency of feature selection
Since we avoid selection of textual features
which have a training set frequency below 0.5%,
we perform feature selection incrementally, on
each stage using the frequencies of the features
obtained during the previous stages. Thus first
we estimate the frequencies of single words in
the training set, then we only consider creation
of n-grams from single words with sufficient fre-
quency, finally we only consider patterns com-
posed from sufficiently frequent words and n-
grams.
3.1.4 Punctuation-based features
In addition to pattern-based features we used
the following generic features: (1) Sentence
length in words, (2) Number of ?!? characters in
the sentence, (3) Number of ??? characters in the
sentence, (4) Number of quotes in the sentence,
and (5) Number of capitalized/all capitals words
in the sentence. All these features were normal-
ized by dividing them by the (maximal observed
value times averaged maximal value of the other
feature groups), thus the maximal weight of each
of these features is equal to the averaged weight
of a single pattern/word/n-gram feature.
3.2 Classification algorithm
In order to assign a sentiment label to new exam-
ples in the test set we use a k-nearest neighbors
(kNN)-like strategy. We construct a feature vec-
tor for each example in the training and the test
set. We would like to assign a sentiment class to
each example in the test set. For each feature vec-
tor V in the test set, we compute the Euclidean
distance to each of the matching vectors in the
training set, where matching vectors are defined as
ones which share at least one pattern/n-gram/word
feature with v.
Let ti, i = 1 . . . k be the k vectors with low-
est Euclidean distance to v4 with assigned labels
Li, i = 1 . . . k. We calculate the mean distance
d(ti, v) for this set of vectors and drop from the set
up to five outliers for which the distance was more
then twice the mean distance. The label assigned
4We used k = 10 for all experiments.
to v is the label of the majority of the remaining
vectors.
If a similar number of remaining vectors have
different labels, we assigned to the test vector the
most frequent of these labels according to their
frequency in the dataset. If there are no matching
vectors found for v, we assigned the default ?no
sentiment? label since there is significantly more
non-sentiment sentences than sentiment sentences
in Twitter.
4 Twitter dataset and sentiment tags
In our experiments we used an extensive Twit-
ter data collection as training and testing sets. In
our training sets we utilize sentiment hashtags and
smileys as classification labels. Below we de-
scribe this dataset in detail.
4.1 Twitter dataset
We have used a Twitter dataset generously pro-
vided to us by Brendan O?Connor. This dataset
includes over 475 million tweets comprising
roughly 15% of all public, non-?low quality?
tweets created from May 2009 to Jan 2010.
Tweets are short sentences limited to 140 UTF-
8 characters. All non-English tweets and tweets
which contain less than 5 proper English words5
were removed from the dataset.
Apart of simple text, tweets may contain URL
addresses, references to other Twitter users (ap-
pear as @<user>) or a content tags (also called
hashtags) assigned by the tweeter (#<tag>)
which we use as labels for our supervised clas-
sification framework.
Two examples of typical tweets are: ?#ipad
#sucks and 6,510 people agree. See more on Ipad
sucks page: http://j.mp/4OiYyg??, and ?Pay no
mind to those who talk behind ur back, it sim-
ply means that u?re 2 steps ahead. #ihatequotes?.
Note that in the first example the hashtagged
words are a grammatical part of the sentence (it
becomes meaningless without them) while #ihate-
qoutes of the second example is a mere sentiment
label and not part of the sentence. Also note that
hashtags can be composed of multiple words (with
no spaces).
5Identification of proper English words was based on an
available WN-based English dictionary
244
Category # of tags % agreement
Strong sentiment 52 87
Likely sentiment 70 66
Context-dependent 110 61
Focused 45 75
No sentiment 3564 99
Table 1: Annotation results (2 judges) for the 3852 most
frequent tweeter tags. The second column displays the av-
erage number of tags, and the last column shows % of tags
annotated similarly by two judges.
During preprocessing, we have replaced URL
links, hashtags and references by URL/REF/TAG
meta-words. This substitution obviously had
some effect on the pattern recognition phase (see
Section 3.1.2), however, our algorithm is robust
enough to overcome this distortion.
4.2 Hashtag-based sentiment labels
The Twitter dataset contains above 2.5 million dif-
ferent user-defined hashtags. Many tweets include
more than a single tag and 3852 ?frequent? tags
appear in more than 1000 different tweets. Two
human judges manually annotated these frequent
tags into five different categories: 1 ? strong sen-
timent (e.g #sucks in the example above), 2 ?
most likely sentiment (e.g., #notcute), 3 ? context-
dependent sentiment (e.g., #shoutsout), 4 ? fo-
cused sentiment (e.g., #tmobilesucks where the
target of the sentiment is part of the hashtag), and
5 ? no sentiment (e.g. #obama). Table 1 shows
annotation results and the percentage of similarly
assigned values for each category.
We selected 50 hashtags annotated ?1? or ?2?
by both judges. For each of these tags we automat-
ically sampled 1000 tweets resulting in 50000 la-
beled tweets. We avoided sampling tweets which
include more than one of the sampled hashtags.
As a no-sentiment dataset we randomly sampled
10000 tweets with no hashtags/smileys from the
whole dataset assuming that such a random sam-
ple is unlikely to contain a significant amount of
sentiment sentences.
4.3 Smiley-based sentiment labels
While there exist many ?official? lists of possible
ASCII smileys, most of these smileys are infre-
quent or not commonly accepted and used as sen-
timent indicators by online communities. We used
the Amazon Mechanical Turk (AMT) service in
order to obtain a list of the most commonly used
and unambiguous ASCII smileys. We asked each
of ten AMT human subjects to provide at least 6
commonly used ASCII mood-indicating smileys
together with one or more single-word descrip-
tions of the smiley-related mood state. From the
obtained list of smileys we selected a subset of 15
smileys which were (1) provided by at least three
human subjects, (2) described by at least two hu-
man subject using the same single-word descrip-
tion, and (3) appear at least 1000 times in our
Twitter dataset. We then sampled 1000 tweets for
each of these smileys, using these smileys as sen-
timent tags in the sentiment classification frame-
work described in the previous section.
5 Evaluation and Results
The purpose of our evaluation was to learn how
well our framework can identify and distinguish
between sentiment types defined by tags or smi-
leys and to test if our framework can be success-
fully used to identify sentiment types in new un-
tagged sentences.
5.1 Evaluation using cross-validation
In the first experiment we evaluated the consis-
tency and quality of sentiment classification us-
ing cross-validation over the training set. Fully
automated evaluation allowed us to test the per-
formance of our algorithm under several dif-
ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-,
Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/?
stands for utilization/omission of the following
feature types: Pn:punctuation, W:Word, M:n-
grams (M stands for ?multi?), Pt:patterns. FULL
stands for utilization of all feature types.
In this experimental setting, the training set was
divided to 10 parts and a 10-fold cross validation
test is executed. Each time, we use 9 parts as the
labeled training data for feature selection and con-
struction of labeled vectors and the remaining part
is used as a test set. The process was repeated ten
times. To avoid utilization of labels as strong fea-
tures in the test set, we removed all instances of
involved label hashtags/smileys from the tweets
used as the test set.
245
Setup Smileys Hashtags
random 0.06 0.02
Pn+W-M-Pt- 0.16 0.06
Pn+W+M-Pt- 0.25 0.15
Pn+W+M+Pt- 0.29 0.18
Pn-W-M-Pt+ 0.5 0.26
FULL 0.64 0.31
Table 2: Multi-class classification results for smileys and
hashtags. The table shows averaged harmonic f-score for 10-
fold cross validation. 51 (16) sentiment classes were used for
hashtags (smileys).
Multi-class classification. Under multi-class
classification we attempt to assign a single label
(51 labels in case of hashtags and 16 labels in case
of smileys) to each of vectors in the test set. Note
that the random baseline for this task is 0.02 (0.06)
for hashtags (smileys). Table 2 shows the perfor-
mance of our framework for these tasks.
Results are significantly above the random
baseline and definitely nontrivial considering the
equal class sizes in the test set. While still rel-
atively low (0.31 for hashtags and 0.64 for smi-
leys), we observe much better performance for
smileys which is expected due to the lower num-
ber of sentiment types.
The relatively low performance of hashtags can
be explained by ambiguity of the hashtags and
some overlap of sentiments. Examination of clas-
sified sentences reveals that many of them can
be reasonably assigned to more than one of the
available hashtags or smileys. Thus a tweet ?I?m
reading stuff that I DON?T understand again! ha-
haha...wth am I doing? may reasonably match
tags #sarcasm, #damn, #haha, #lol, #humor, #an-
gry etc. Close examination of the incorrectly
classified examples also reveals that substantial
amount of tweets utilize hashtags to explicitly in-
dicate the specific hashtagged sentiment, in these
cases that no sentiment value could be perceived
by readers unless indicated explicitly, e.g. ?De
Blob game review posted on our blog. #fun?.
Obviously, our framework fails to process such
cases and captures noise since no sentiment data
is present in the processed text labeled with a spe-
cific sentiment label.
Binary classification. In the binary classifica-
tion experiments, we classified a sentence as ei-
ther appropriate for a particular tag or as not bear-
Hashtags Avg #hate #jealous #cute #outrageous
Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53
Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6
Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64
Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69
FULL 0.8 0.83 0.76 0.71 0.78
Smileys Avg :) ; ) X( : d
Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65
Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69
Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69
Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72
FULL 0.86 0.87 0.9 0.74 0.81
Table 3: Binary classification results for smileys and hash-
tags. Avg column shows averaged harmonic f-score for 10-
fold cross validation over all 50(15) sentiment hashtags (smi-
leys).
ing any sentiment6. For each of the 50 (15) labels
for hashtags (smileys) we have performed a bi-
nary classification when providing as training/test
sets only positive examples of the specific senti-
ment label together with non-sentiment examples.
Table 3 shows averaged results for this case and
specific results for selected tags. We can see that
our framework successfully identifies diverse sen-
timent types. Obviously the results are much bet-
ter than those of multi-class classification, and the
observed > 0.8 precision confirms the usefulness
of the proposed framework for sentiment classifi-
cation of a variety of different sentiment types.
We can see that even for binary classification
settings, classification of smiley-labeled sentences
is a substantially easier task compared to classifi-
cation of hashtag-labeled tweets. Comparing the
contributed performance of different feature types
we can see that punctuation, word and pattern fea-
tures, each provide a substantial boost for classi-
fication quality while we observe only a marginal
boost when adding n-grams as classification fea-
tures. We can also see that pattern features con-
tribute the performance more than all other fea-
tures together.
5.2 Evaluation with human judges
In the second set of experiments we evaluated our
framework on a test set of unseen and untagged
tweets (thus tweets that were not part of the train-
6Note that this is a useful application in itself, as a filter
that extracts sentiment sentences from a corpus for further
focused study/processing.
246
ing data), comparing its output to tags assigned by
human judges. We applied our framework with
its FULL setting, learning the sentiment tags from
the training set for hashtags and smileys (sepa-
rately) and executed the framework on the reduced
Tweeter dataset (without untagged data) allowing
it to identify at least five sentences for each senti-
ment class.
In order to make the evaluation harsher, we re-
moved all tweets containing at least one of the
relevant classification hashtags (or smileys). For
each of the resulting 250 sentences for hashtags,
and 75 sentences for smileys we generated an ?as-
signment task?. Each task presents a human judge
with a sentence and a list of ten possible hash-
tags. One tag from this list was provided by our
algorithm, 8 other tags were sampled from the re-
maining 49 (14) available sentiment tags, and the
tenth tag is from the list of frequent non-sentiment
tags (e.g. travel or obama). The human judge was
requested to select the 0-2 most appropriate tags
from the list. Allowing assignment of multiple
tags conforms to the observation that even short
sentences may express several different sentiment
types and to the observation that some of the se-
lected sentiment tags might express similar senti-
ment types.
We used the Amazon Mechanical Turk service
to present the tasks to English-speaking subjects.
Each subject was given 50 tasks for Twitter hash-
tags or 25 questions for smileys. To ensure the
quality of assignments, we added to each test five
manually selected, clearly sentiment bearing, as-
signment tasks from the tagged Twitter sentences
used in the training set. Each set was presented to
four subjects. If a human subject failed to provide
the intended ?correct? answer to at least two of
the control set questions we reject him/her from
the calculation. In our evaluation the algorithm
is considered to be correct if one of the tags se-
lected by a human judge was also selected by the
algorithm. Table 4 shows results for human judge-
ment classification. The agreement score for this
task was ? = 0.41 (we consider agreement when
at least one of two selected items are shared).
Table 4 shows that the majority of tags selected
by humans matched those selected by the algo-
rithm. Precision of smiley tags is substantially
Setup % Correct % No sentiment Control
Smileys 84% 6% 92%
Hashtags 77% 10% 90%
Table 4: Results of human evaluation. The second col-
umn indicates percentage of sentences where judges find no
appropriate tags from the list. The third column shows per-
formance on the control set.
Hashtags #happy #sad #crazy # bored
#sad 0.67 - - -
#crazy 0.67 0.25 - -
#bored 0.05 0.42 0.35 -
#fun 1.21 0.06 1.17 0.43
Smileys :) ; ) : ( X(
; ) 3.35 - - -
: ( 3.12 0.53 - -
X( 1.74 0.47 2.18 -
: S 1.74 0.42 1.4 0.15
Table 5: Percentage of co-appearance of tags in tweeter
corpus.
higher than of hashtag labels, due to the lesser
number of possible smileys and the lesser ambi-
guity of smileys in comparison to hashtags.
5.3 Exploration of feature dependencies
Our algorithm assigns a single sentiment type
for each tweet. However, as discussed above,
some sentiment types overlap (e.g., #awesome and
#amazing). Many sentences may express several
types of sentiment (e.g., #fun and #scary in ?Oh
My God http://goo.gl/fb/K2N5z #entertainment
#fun #pictures #photography #scary #teaparty?).
We would like to estimate such inter-sentiment
dependencies and overlap automatically from the
labeled data. We use two different methods for
overlap estimation: tag co-occurrence and feature
overlap.
5.3.1 Tag co-occurrence
Many tweets contain more than a single hash-
tag or a single smiley type. As mentioned, we ex-
clude such tweets from the training set to reduce
ambiguity. However such tag co-appearances can
be used for sentiment overlap estimation. We cal-
culated the relative co-occurrence frequencies of
some hashtags and smileys. Table 5 shows some
of the observed co-appearance ratios. As expected
some of the observed tags frequently co-appear
with other similar tags.
247
Hashtags #happy #sad #crazy # bored
#sad 12.8 - - -
#crazy 14.2 3.5 - -
#bored 2.4 11.1 2.1 -
#fun 19.6 2.1 15 4.4
Smileys :) ; ) : ( X(
; ) 35.9 - - -
: ( 31.9 10.5 - -
X( 8.1 10.2 36 -
: S 10.5 12.6 21.6 6.1
Table 6: Percentage of shared features in feature vectors
for different tags.
Interestingly, it appears that a relatively high
ratio of co-appearance of tags is with opposite
meanings (e.g., ?#ilove eating but #ihate feeling
fat lol? or ?happy days of training going to end
in a few days #sad #happy?). This is possibly due
to frequently expressed contrast sentiment types
in the same sentence ? a fascinating phenomena
reflecting the great complexity of the human emo-
tional state (and expression).
5.3.2 Feature overlap
In our framework we have created a set of fea-
ture vectors for each of the Twitter sentiment tags.
Comparison of shared features in feature vector
sets allows us to estimate dependencies between
different sentiment types even when direct tag co-
occurrence data is very sparse. A feature is con-
sidered to be shared between two different senti-
ment labels if for both sentiment labels there is
at least a single example in the training set which
has a positive value of this feature. In order to au-
tomatically analyze such dependencies we calcu-
late the percentage of sharedWord/n-gram/Pattern
features between different sentiment labels. Table
6 shows the observed feature overlap values for
selected sentiment tags.
We observe the trend of results obtained by
comparison of shared feature vectors is similar to
those obtained by means of label co-occurrence,
although the numbers of the shared features are
higher. These results, demonstrating the pattern-
based similarity of conflicting, sometimes contra-
dicting, emotions are interesting from a psycho-
logical and cognitive perspective.
6 Conclusion
We presented a framework which allows an au-
tomatic identification and classification of various
sentiment types in short text fragments which is
based on Twitter data. Our framework is a su-
pervised classification one which utilizes Twitter
hashtags and smileys as training labels. The sub-
stantial coverage and size of the processed Twit-
ter data allowed us to identify dozens of senti-
ment types without any labor-intensive manually
labeled training sets or pre-provided sentiment-
specific features or sentiment words.
We evaluated diverse feature types for senti-
ment extraction including punctuation, patterns,
words and n-grams, confirming that each fea-
ture type contributes to the sentiment classifica-
tion framework. We also proposed two different
methods which allow an automatic identification
of sentiment type overlap and inter-dependencies.
In the future these methods can be used for au-
tomated clustering of sentiment types and senti-
ment dependency rules. While hashtag labels are
specific to Twitter data, the obtained feature vec-
tors are not heavily Twitter-specific and in the fu-
ture we would like to explore the applicability of
Twitter data for sentiment multi-class identifica-
tion and classification in other domains.
References
Akkaya, Cem, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
EMNLP.
Andreevskaia, A. and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL.
Balog, Krisztian, Gilad Mishne, and Maarten de Ri-
jke. 2006. Why are they excited? identifying and
explaining spikes in blog mood levels. In EACL.
Bloom, Kenneth, Navendu Garg, and Shlomo Arga-
mon. 2007. Extracting appraisal expressions. In
HLT/NAACL.
Davidov, D. and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
248
Davidov, D. and A. Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pat-
tern clusters and its evaluation by automatically gen-
erated sat analogy questions. In ACL.
Davidov, D., O. Tsur, and A. Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In CoNLL.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jansen, B.J., M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Infor-
mation Science and Technology.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In ACL.
Melville, Prem, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by
combining lexical knowledge with text classifica-
tion. In KDD. ACM.
Mihalcea, Rada and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In In AAAI
2006 Symposium on Computational Approaches to
Analysing Weblogs. AAAI Press.
Mishne, Gilad. 2005. Experiments with mood clas-
sification in blog posts. In Proceedings of the 1st
Workshop on Stylistic Analysis Of Text.
Riloff, Ellen. 2003. Learning extraction patterns for
subjective expressions. In EMNLP.
Strapparava, Carlo and Rada Mihalcea. 2008. Learn-
ing to identify emotions in text. In SAC.
Titov, Ivan and Ryan McDonald. 2008a. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL/HLT, June.
Titov, Ivan and Ryan McDonald. 2008b. Modeling
online reviews with multi-grain topic models. In
WWW, pages 111?120, New York, NY, USA. ACM.
Tsur, Oren, Dmitry Davidov, and Ari Rappoport.
2010. Icwsm ? a great catchy name: Semi-
supervised recognition of sarcastic sentences in
product reviews. In AAAI-ICWSM.
Turney, Peter D. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL ?02, volume 40.
Whitelaw, Casey, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In CIKM.
Wiebe, Janyce and Rada Mihalcea. 2006. Word sense
and subjectivity. In COLING/ACL, Sydney, AUS.
Wiebe, Janyce M. 2000. Learning subjective adjec-
tives from corpora. In AAAI.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In EMNLP.
249
Coling 2010: Poster Volume, pages 1274?1282,
Beijing, August 2010
A Multi-Domain Web-Based Algorithm for
POS Tagging of Unknown Words
Shulamit Umansky-Pesin
Institute of computer science
The Hebrew University
pesin@cs.huji.ac.il
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
We present a web-based algorithm for the
task of POS tagging of unknown words
(words appearing only a small number
of times in the training data of a super-
vised POS tagger). When a sentence s
containing an unknown word u is to be
tagged by a trained POS tagger, our algo-
rithm collects from the web contexts that
are partially similar to the context of u in
s, which are then used to compute new
tag assignment probabilities for u. Our
algorithm enables fast multi-domain un-
known word tagging, since, unlike pre-
vious work, it does not require a corpus
from the new domain. We integrate our
algorithm into the MXPOST POS tagger
(Ratnaparkhi, 1996) and experiment with
three languages (English, German and
Chinese) in seven in-domain and domain
adaptation scenarios. Our algorithm pro-
vides an error reduction of up to 15.63%
(English), 18.09% (German) and 13.57%
(Chinese) over the original tagger.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
NLP task that has attracted much research in the
last decades. While supervised POS taggers have
achieved high accuracy (e.g., (Toutanova et al,
2003) report a 97.24% accuracy in the WSJ Penn
Treebank), tagger performance on words appear-
ing a small number of times in their training
corpus (unknown words) is substantially lower.
This effect is especially pronounced in the do-
main adaptation scenario, where the training and
test corpora are from different domains. For ex-
ample, when training the MXPOST POS tagger
(Ratnaparkhi, 1996) on sections 2-21 of the WSJ
Penn Treebank it achieves 97.04% overall accu-
racy when tested on WSJ section 24, and 88.81%
overall accuracy when tested on the BNC cor-
pus, which contains texts from various genres.
For unknown words (test corpus words appearing
8 times or less in the training corpus), accuracy
drops to 89.45% and 70.25% respectively.
In this paper we propose an unknown word POS
tagging algorithm based on web queries. When a
new sentence s containing an unknown word u is
to be tagged by a trained POS tagger, our algo-
rithm collects from the web contexts that are par-
tially similar to the context of u in s. The collected
contexts are used to compute new tag assignment
probabilities for u.
Our algorithm is particularly suitable for multi-
domain tagging, since it requires no information
about the domain from which the sentence to be
tagged is drawn. It does not need domain specific
corpora or external dictionaries, and it requires
no preprocessing step. The information required
for tagging an unknown word is very quickly col-
lected from the web.
This behavior is unlike previous works for the
task (e.g (Blitzer et al, 2006)), which require a
time consuming preprocessing step and a corpus
collected from the target domain. When the target
domain is heterogeneous (as is the web itself), a
corpus representing it is very hard to assemble. To
the best of our knowledge, ours is the first paper to
provide such an on-the-fly unknown word tagging
algorithm.
To demonstrate the power of our algorithm as a
1274
fast multi-domain learner, we experiment in three
languages (English, German and Chinese) and
several domains. We implemented the MXPOST
tagger and integrated it with our algorithm. We
show error reduction in unknown word tagging
of up to 15.63% (English), 18.09% (German) and
13.57% (Chinese) over MXPOST. The run time
overhead is less than 0.5 seconds per an unknown
word in the English and German experiments, and
less than a second per unknown word in the Chi-
nese experiments.
Section 2 reviews previous work on unknown
word Tagging. Section 3 describes our web-query
based algorithm. Section 4 and Section 5 describe
experimental setup and results.
2 Previous Work
Most supervised POS tagging works address the
issue of unknown words. While the general meth-
ods of POS tagging vary from study to study
? Maximum Entropy (Ratnaparkhi, 1996), con-
ditional random fields (Lafferty et al, 2001),
perceptron (Collins, 2002), Bidirectional Depen-
dency Network (Toutanova et al, 2003) ? the
treatment of unknown words is more homoge-
neous and is generally based on additional fea-
tures used in the tagging of the unknown word.
Brants (2000) used only suffix features. Rat-
naparkhi (1996) used orthographical data such as
suffixes, prefixes, capital first letters and hyphens,
combined with a local context of the word. In this
paper we show that we improve upon this method.
Toutanova and Manning (2000), Toutanova et al
(2003), Lafferty et al (2001) and Vadas and Cur-
ran (2005) used additional language-specific mor-
phological or syntactic features. Huihsin et al
(2005) combined orthographical and morpholog-
ical features with external dictionaries. Naka-
gawa and Matsumoto (2006) used global and local
information by considering interactions between
POS tags of unknown words with the same lexical
form.
Unknown word tagging has also been explored
in the context of domain adaptation of POS tag-
gers. In this context two directions were explored:
a supervised method that requires a manually an-
notated corpus from the target domain (Daume III,
2007), and a semi-supervised method that uses an
unlabeled corpus from the target domain (Blitzer
et al, 2006).
Both methods require the preparation of a cor-
pus of target domain sentences and re-training
the learning algorithm. Blitzer et al (2006) used
100K unlabeled sentences from the WSJ (source)
domain as well as 200K unlabeled sentences from
the biological (target) domain. Daume III (2007)
used an 11K words labeled corpus from the target
domain.
There are two serious problems with these ap-
proaches. First, it is not always realistically pos-
sible to prepare a corpus representing the target
domain, for example when that domain is the web
(e.g., when the POS tagger serves an application
working on web text). Second, preparing a cor-
pus is time consuming, especially when it needs
to be manually annotated. Our algorithm requires
no corpus from the target data domain, no prepro-
cessing step, and it doesn?t even need to know the
identity of the target domain. Consequently, the
problem we address here is more difficult (and ar-
guably more useful) than that addressed in previ-
ous work1.
The domain adaptation techniques above have
not been applied to languages other than English,
while our algorithm is shown to perform well in
seven scenarios in three languages.
Qiu et al (2008) explored Chinese unknown
word POS tagging using internal component and
contextual features. Their work is not directly
comparable to ours since they did not test a do-
main adaptation scenario, and used substantially
different corpora and evaluation measures in their
experiments.
Numerous works utilized web resources for
NLP tasks. Most of them collected corpora us-
ing data mining techniques and used them off-
line. For example, Keller et al, (2002) and Keller
and Lapata (2003) described a method to obtain
frequencies for unseen adjective-noun, noun-noun
and verb-object bigrams from the web by query-
1We did follow their experimental procedure as much as
we could. Like (Blitzer et al, 2006), we compare our algo-
rithm to the performance of the MXPOST tagger trained on
sections 2-21 of WSJ. Like both papers, we experimented
in domain adaptation from WSJ to a biological domain. We
used the freely available Genia corpus, while they used data
from the Penn BioIE project (PennBioIE, 2005).
1275
ing a Web engine.
On-line usage of web queries is less frequent
and was used mainly in semantic acquisition ap-
plications: the discovery of semantic verb rela-
tions (Chklovski and Pantel, 2004), the acquisi-
tion of entailment relations (Szpektor et al, 2004),
and the discovery of concept-specific relation-
ships (Davidov et al, 2007). Chen et al (2007)
used web queries to suggest spelling corrections.
Our work is related to self-training (McClosky
et al, 2006a; Reichart and Rappoport, 2007) as
the algorithm used its own tagging of the sen-
tences collected from the web in order to produce
a better final tagging. Unlike most self-training
works, our algorithm is not re-trained using the
collected data but utilizes it at test time. More-
over, unlike in these works, in this work the data
is collected from the web and is used only dur-
ing unknown words tagging. Interestingly, previ-
ous works did not succeed in improving POS tag-
ging performance using self-training (Clark et al,
2003).
3 The Algorithm
Our algorithm utilizes the correlation between the
POS of a word and the contexts in which the word
appears. When tackling an unknown word, the al-
gorithm searches the web to find contexts similar
to the one in which the word appears in the sen-
tence. A new tag assignment is then computed for
the unknown word based on the extracted contexts
as well as the original ones.
We start with a description of the web-based
context searching algorithm. We then describe
how we combine the context information col-
lected by our algorithm with the statistics of the
MXPOST tagger. While in this paper we imple-
mented this tagger and used it in our experiments,
the context information collected by our web-
query based algorithm can be integrated into any
POS tagger.
3.1 Web-Query Based Context Collection
An unknown word usually appears in a given sen-
tence with other words on its left and on its right.
We use three types of contexts. The first includes
all of these neighboring words, the second in-
cludes the words on the left, and the third includes
the words on the right.
For each context type we define a web query us-
ing two common features supported by the major
search engines: wild-card search, expressed using
the ?*? character, and exact sentence search, ex-
pressed by quoted characters. The retrieved sen-
tences contain the parts enclosed in quotes in the
exact same place they appear in the query, while
an asterisk can be replaced by any single word.
For a word u we execute the following three
queries for each of its test contexts:
1. Replacement: "u?2u?1 ?u+1u+2". This re-
trieves words that appear in the same context
as u.
2. Left-side: "? ? u u+1 u+2". This retrieves
alternative left-side contexts for the word u
and its original right-side context.
3. Right-side: query "u?2 u?1 u ? ?". This
retrieves alternative right-side contexts for u
and its original left-side context.
Query Type Query Matches (Counts)
Replacement "irradiation and * heat (15)
treatment of" chemical (7)
the (6)
radiation (1)
pressure (1)
Left-side "* * H2O2 by an (9)
treatment of" indicated that (5)
enhanced by (4)
familiar with (3)
observed after (3)
Right-side "irradiation and in comparison (3)
H2O2 * *" on Fe (1)
treatment by (1)
cause an (1)
does not (1)
Table 1: Top 5 matches of each query type for the word
?H2O2? in the GENIA sentence: ?UV irradiation and H2O2
treatment of T lymphocytes induce protein tyrosine phospho-
rylation and Ca2+ signals similar to those observed following
biological stimulation.?. For each query the matched words
(matches) are ranked by the number of times they occur in
the query results (counts).
An example is given in Table 1, presenting the
top 5 matches of every query type for the word
?H2O2?, which does not appear in the English
WSJ corpus, in a sentence taken from the English
Genia corpus. Since matching words can appear
1276
multiple times in the results, the algorithm main-
tains for each match a counter denoting the num-
ber of times it appeared in the results, and sorts
the results according to this number.
Seeing the table, readers might think of the fol-
lowing algorithm: take the leading match in the
Replacement query, and tag the unknown word us-
ing its most frequent tag (assuming it is a known
word). We have experimented with this method,
and it turned out that its results are worse than
those given by MXPOST, which we use as a base-
line.
The web queries are executed by Yahoo!
BOSS2, and the resulting XML containing up to a
1000 results (a limit set by BOSS) is processed for
matches. A list of matches is extracted from the
abstract and title nodes of the web results along
with counts of the number of times they appear.
The matches are filtered to include only known
words (words that appear in the training data of
the POS tagger more than a threshold) and to ex-
clude the original word or context.
Our algorithm uses a positive integer parameter
Nweb: only the Nweb top-scoring unique results
of each query type are used for tagging. If a left-
side or right-side query returns less than Nweb re-
sults, the algorithm performs a ?reduced? query:
"? ? u u+1" for left-side and "u?1 u ? ?" for the
right side. These queries should produce more re-
sults than the original ones due to the reduced con-
text. If these reduced queries do not produce Nweb
results, the web query algorithm is not used to as-
sist the tagger for the unknown word u at hand.
If a replacement query does not produce at least
Nweb unique results, only the left-side and right-
side queries are used.
For Chinese queries, search engines do their
own word segmentation so the semantics of the
?*? operator is supposedly the same as for English
and German. However, the answer returned by
the search engine does not provide this segmen-
tation. To obtain the words filling the ?*? slots in
our queries, we take all possible segmentations in
which the two words appears in our training data.
The queries we use in our algorithm are not the
only possible ones. For example, a possible query
2http://developer.yahoo.com/search/boss/
we do not use for the word u is "??u?1uu+1u+2".
The aforementioned set of queries gave the best
results in our English, German and Chinese de-
velopment data and is therefore the one we used.
3.2 Final Tagging
The MXPOST Tagger. We integrated our algo-
rithm into the maximum entropy tagger of (Rat-
naparkhi, 1996). The tagger uses a set h of con-
texts (?history?) for each word wi (the index i is
used to allow an easy notation of the previous and
next words, whose lexemes and POS tags are used
as features). For each such word, the tagger com-
putes the following conditional probability for the
tag tr:
p(tr|h) =
p(h, tr)?
t?r?T p(h, t
?r)
(1)
where T is the tag set, and the denominator is sim-
ply p(h). The joint probability of a history h and
a tag t is defined by:
p(h, t) = Z
k?
j=1
?fj(h,t)j (2)
where ?1, . . . , ?k are the model parameters,
f1, . . . , fk are the model?s binary features (indica-
tor functions), and Z is a normalization term for
ensuring that p(h, t) is a probability.
In the training phase the algorithm performs
maximum likelihood estimation for the ? param-
eters. These parameters are then used when the
model tags a new sentence (the test phase). For
words that appear 5 times or less in the training
data, the tagger extracts special features based on
the morphological properties of the word.
Combining Models. In general, we use the
same equation as MXPOST to compute joint prob-
abilities, and our training phase is identical to its
training phase. What we change are two things.
First, we add new contexts to the ?history? of a
word when it is considered as unknown (so Equa-
tion (2) is computed using different histories).
Second, we use a different equation for comput-
ing the conditional probability (below).
When the algorithm encounters an unknown
word wi in the context h during tagging, it per-
forms the web queries defined in Section 3.1. For
1277
each of the Nweb top resulting matches for each
query, {h?n|n ? [1, Nweb]}, the algorithm creates
its corresponding history representation hn. Con-
verting h?n to hn is required since in MXPOST a
history consists of an ordered set of words to-
gether with their POS tags, while h?n is an ordered
set of words without POS tags. Consequently, we
define hn to consist of the same ordered set of
words as h?n, and we tag each word using its most
frequent POS tag in the training corpus. If wi?1 or
wi?2 are unknown words, we do not tag them, let-
ting MXPOST use its back-off technique for such a
case (which is simply to compute the features that
it can and ignore those it cannot).
For each possible tag t ? T , its final assign-
ment probability to wi is computed as an average
between its probability given the various contexts:
p?(tr|h) =
porg(tr|h) +
?QNweb
n=1 pn(tr|hn)
QNweb + 1
(3)
where Q is the number of query types used (1, 2
or 3, see Section 3.1).
During inference, we use the two search space
constraints applied by the original MXPOST. First,
we apply a beam search procedure that consid-
ers the 10 most probable different tag sequences
of the tagged sentence at any point in the tagging
process. Second, known words are constrained to
be annotated only by tags with which they appear
in the training corpus.
4 Experimental Setup
Languages and Datasets. We experimented
with three languages, English, German and Chi-
nese, in various combinations of training and test-
ing domains (see Table 2). For English we used
the Penn Treebank WSJ corpus (WSJ) (Marcus
et al, 1993) from the economics newspapers do-
main, the GENIA corpus version 3.02p (GENIA)
(Kim et al, 2003) from the biological domain
and the British National Corpus version 3 (BNC)
(Burnard, 2000) consisting of various genres. For
German we used two different corpora from the
newspapers domain: NEGRA (Brants, 1997) and
TIGER (Brants et al, 2002). For Chinese we
used the Penn Chinese Treebank corpus version
5.0 (CTB) (Xue et al, 2002).
All corpora except of WSJ were split using
random sampling. For the NEGRA and TIGER
corpora we used the Stuttgart-Tuebingen Tagset
(STTS).
According to the annotation policy of the GE-
NIA corpus, only the names of journals, authors,
research institutes, and initials of patients are an-
notated by the ?NNP? (Proper Name) tag. Other
proper names such as general people names, tech-
nical terms (e.g. ?Epstein-Barr virus?) genes, pro-
teins, etc. are tagged by other noun tags (?NN? or
?NNS?). This is in contrast to the WSJ corpus, in
which every proper name is tagged by the ?NNP?
tag. We therefore omitted cases where ?NNP?
is replaced by another noun tag from the accu-
racy computation of the GENIA domain adapta-
tion scenario (see analysis in (Lease and Charniak,
2005)).
In all experimental setups except of WSJ-BNC
the training and test corpora are tagged with the
same POS tag set. In order to evaluate the WSJ-
BNC setup, we converted the BNC tagset to the
Penn Treebank tagset using the comparison table
provided in (Manning and Schuetze, 1999) (pages
141?142).
Baseline. As a baseline we implemented the
MXPOST tagger. An executable code for MXPOST
written by its author is available on the internet,
but we needed to re-implement it in order to in-
tegrate our technique. We made sure that our
implementation does not degrade results by run-
ning it on our WSJ scenario (see Table 2), which
is very close to the scenario reported in (Ratna-
parkhi, 1996). The accuracy of our implementa-
tion is 97.04%, a bit better than the numbers re-
ported in (Ratnaparkhi, 1996) for a WSJ scenario
using different sections.
Parameter Tuning. We ran experiments with
three values of the unknown word threshold T : 0
(only words that do not appear in the training data
are considered unknown), 5 and 8. That is, the al-
gorithm performs the web context queries and uti-
lizes the tag probabilities of equation 3 for words
that appear up to 0 ,5 or 8 times in the training
data.
Our algorithm has one free parameter Nweb, the
number of query results for each context type used
1278
Language Expe. name Training Development Test
English WSJ sections 2-21 (WSJ) section 22 (WSJ) section 23 (WSJ)
(2.4%,6.7%,8.4%)
English WSJ-BNC sections 2-21 (WSJ) 2000 BNC sentence 2000 BNC sentences
(8.4%,14.9%,17%)
English WSJ-GENIA WSJ sections 2-21 2000 GENIA sentences 2000 GENIA sentences
(22.7%,30.65%,32.9%)
German NEGRA 15689 NEGRA sentences 1746 NEGRA sentences 2096 NEGRA sentences
(11.1%,24.7%,28.7%)
German NEGRA-TIGER 15689 NEGRA sentences 2000 TIGER sentences 2000 TIGER sentences
(16%,27.3%,30.6%)
German TIGER-NEGRA 15689 TIGER sentences 1746 NEGRA sentences 2096 NEGRA sentence
(16.2%,27.9%,31.6%)
Chinese CTB 14903 CTB sentences 1924 CTB sentences 1945 CTB senteces
(7.4%,15.7%,18.1%)
Table 2: Details of the experimental setups. In the ?Test? column the numbers in parentheses are the
fraction of the test corpus words that are considered unknown, when the unknown word threshold is set
to 0, 5 and 8 respectively.
T = 0 T = 5 T = 8
WSJ WSJ- WSJ- WSJ WSJ- WSJ- WSJ WSJ- WSJ-
BNC GENIA BNC GENIA BNC GENIA
Baseline 83.56 61.22 80.05 88.79 68.71 80.12 89.45 70.25 80.8
Unlimited (-) 84.85 63.51 82.50 89.86 71.12 82.51 90.47 72.77 83.16
Top 5 (-) 84.25 64.24 82.75 89.73 71.21 82.78 90.36 72.74 83.46
Top 10 (-) 84.42 64.10 83.17 89.70 71.36 83.00 90.29 72.87 83.70
Top 10 (+) 84.67 64.47 82.60 89.83 72.12 82.54 90.29 73.53 83.22
best imp. 1.19 3.25 3.12 1.07 3.41 2.88 1.02 3.28 2.9
7.23% 8.38% 15.63% 9.54% 10.89% 14.48% 9.66% 11.02% 15.1%
T = 0 T = 5 T = 8
NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER- NEGRA NEGRA- TIGER-
TIGER NEGRA TIGER NEGRA TIGER NEGRA
Baseline 90.26 85.71 87.18 91.06 87.88 87.86 91.45 88.22 88.18
Unlimited (-) 91.22 86.60 89.49 91.66 88.22 89.84 92.25 89.08 90.23
Top 5 (-) 91.41 86.68 89.32 91.95 89.01 89.72 92.38 89.33 90.26
Top 10 (-) 91.06 86.83 89.50 91.25 88.36 89.84 92.33 89.38 90.26
Top 10 (+) 90.58 86.86 89.43 91.25 88.36 89.84 91.53 88.35 89.71
best imp. 1.15 1.15 2.32 0.89 1.13 1.98 0.93 1.16 2.08
11.8% 8.04% 18.09% 9.95% 9.32% 16.3% 10.87% 9.84% 17.59%
CTB
T = 0 T = 5 T = 8
Baseline 74.99 78.03 79.81
Unlimited (-) 77.01 80.46 81.94
Top 5 (-) 77.58 80.75 82.19
Top 10 (-) 77.43 80.68 82.45
Top 10 (+) 77.43 80.68 82.35
best imp. 2.59 2.72 2.74
10.35% 12.28% 13.57%
Table 3: Accuracy of unknown word tagging in the English (top table), German (middle table) and Chi-
nese (bottom table) experiments. Results are presented for three values of the unknown word threshold
parameter T : 0, 5 and 8. For all setups our models improves over the MXPOST baseline of (Ratnaparkhi,
1996). The bottom line of each table (?best imp.?) presents the improvement (top number) and error
reduction (bottom number) of the best performing model over the baseline. The best improvement is in
domain adaptation scenarios.
1279
in the probability computation of equation 3. For
each setup (Table 2) we ran several combinations
of query types and values of Nweb. We report re-
sults for the four leading combinations:
? Nweb = 5, left-side and right-side queries
(Top 5 (-)).
? Nweb = 10, left-side and right-side queries
(Top 10 (-)).
? Nweb = 10, replacement, left-side and right-
side queries (Top 10 (+)).
? Nweb = Unlimited (in practice, this means
1000, the maximum number of results pro-
vided by Yahoo! Boss), left-side and right-
side queries (Unlimited (-) ).
The order of the models with respect to their
performance was identical for the development
and test data. That is, the best parameter/queries
combination for each scenario can be selected us-
ing the development data. We experimented with
other parameter/queries combinations and addi-
tional query types but got worse results.
5 Results
The results of the experiments are shown in Ta-
ble 3. Our algorithm improves the accuracy of the
MXPOST tagger for all three languages and for all
values of the unknown word parameter.
Our experimental scenarios consist of three in-
domain setups in which the model is trained and
tested on the same corpus (the WSJ, NEGRA
and CTB experiments), and four domain adap-
tation setups: WSJ-GENIA, WSJ-BNC, TIGER-
NEGRA and NEGRA-TIGER.
Table 3 shows that our model is relatively
more effective in the domain adaptation scenar-
ios. While in the in-domain setups the error reduc-
tion values are 7.23% ? 9.66% (English), 9.95% ?
11.8% (German) and 10.35% ? 13.57% (Chinese),
in the domain adaptation scenarios they are 8.38%
? 11.02% (WSJ-BNC), 14.48% ? 15.63% (WSJ-
GENIA), 8.04% ? 9.84% (NEGRA-TIGER) and
16.3% ? 18.09% (TIGER-NEGRA).
Run Time. As opposed to previous approaches
to unknown word tagging (Blitzer et al, 2006;
Daume III, 2007), our algorithm does not contain
a step in which the base tagger is re-trained with a
corpus collected from the target domain. Instead,
when an unknown word is tackled at test time, a
set of web queries is run. This is an advantage for
flexible multi-domain POS tagging because pre-
processing times are minimized, but might cause
an issue of overhead per test word.
To show that the run time overhead created by
our algorithm is small, we measured its time per-
formance (using an Intel Xeon 3.06GHz, 3GB
RAM computer). The average time it took the best
configuration of our algorithm to process an un-
known word and the resulting total addition to the
run time of the base tagger are given in Table 4.
The average time added to an unknown word tag-
ging is less than half a second for English, even
less for German, and less than a second for Chi-
nese. This is acceptable for interactive applica-
tions that need to examine a given sentence with-
out being provided with any knowledge about its
domain.
Error Analysis. In what follows we try to ana-
lyze the cases in which our algorithm is most ef-
fective and the cases where further work is still
required. Due to space limitations we focus only
on the (Top 10 (+), T = 5) parameters setting,
and report the patterns for one English setup. The
corresponding patterns of the other parameter set-
tings, languages and setups are similar.
We report the errors of the base tagger that our
algorithm most usually fixes and the errors that
our algorithm fails to fix. We describe the base
tagger errors of the type ?POS tag ?a? is replaced
with POS tag ?b? (denoted by: a -> b)? using
the following data: (1) total number of unknown
words whose correct tag is ?a? that were assigned
?b? by the base tagger; (2) the percentage of un-
known words whose correct tag is ?a? that were
assigned ?b? by the base tagger; (3) the percentage
of unknown words whose correct tag is ?a? that
were assigned ?b? by our algorithm; (4) the per-
centage of mistakes of type (1) that were corrected
by our algorithm.
In the English WSJ-BNC setup, the base tagger
mistakes that our algorithm handles well (accord-
ing to the percentage of corrected mistakes) are:
(1) NNS -> VBZ (23, 3.73%, 0.8%, 65.2%); (2)
CD -> JJ (19 ,13.2% ,9.7% ,37.5%); (3) NN ->
1280
WSJ WSJ- WSJ- NEGRA NEGRA- TIGER- CTB
BNC GENIA TIGER NEGRA
Total addition 00:28:26 00:31:53 1:37:32 00:57:03 00:19:10 00:36:54 2:29:13
Avg. time per word 0.42 0.32 0.33 0.36 0.11 0.21 0.95
Table 4: The processing time added by the web based algorithm to the base tagger. For each setup results are presented for
the best performing model and for the unknown word threshold of 8. Results for the other models and threshold parameters
are very similar. The top line presents the total time added in the tagging of the full test data (hours:minutes:seconds). The
bottom line presents the average processing time of an unknown word by the web based algorithm (in seconds).
JJ (97, 6.17%, 5.3%, 27.8%); (4) JJ -> NN (69,
9.73%, 7.76%, 33.3%). The errors that were not
handled well by our algorithm are: (1) IN -> JJ
(70, 46.36% , 41%, 8.57%); (2) VBP -> NN (25,
19.5%, 21.9% , 0%).
In this setup, ?CD? is a cardinal number, ?IN? is
a preposition, ?JJ? is an adjective, ?NN? is a noun
(singular or mass), ?NNS? is a plural noun, ?VBP?
is a verb in non-third person singular present tense
and ?VBZ? is a verb in third person, singular
present tense.
We can see that no single factor is responsible
for the improvement over the baseline. Rather,
it is due to correcting many errors of different
types. The same general behavior is exhibited in
the other setups for all languages.
Multiple Unknown Words. Our method is ca-
pable of handling sentences containing several un-
known words. Query results in which ?*? is re-
placed by an unknown word are filtered. For
queries in which an unknown word appears as part
of the query (when it is one of the two right or left
non-?*? words), we let MXPOST invoke its own
unknown word heuristics if needed3.
In fact, the relative improvement of our algo-
rithm over the baseline is better for adjacent un-
known words than for single words. For ex-
ample, consider a sequence of consecutive un-
known words as correctly tagged if all of its
words are assigned their correct tag. In the
WSJ-GENIA scenario (Top 10 (+), T = 5), the
error reduction for sequences of length 1 (un-
known words surrounded by known words, 8767
sequences) is 8.26%, while for 2-words (2620
sequences) and 3-words (614 sequences) it is
11.26% and 19.11% respectively. Similarly, for
TIGER-NEGRA (same parameters setting) the er-
3They are needed only if the word is on the left of the
word to be tagged.
ror reduction is 6.85%, 8.07% and 18.18% for se-
quences of length 1 (4819) ,2 (1126) and 3 (223)
respectively.
6 Conclusions and Future Work
We presented a web-based algorithm for POS tag-
ging of unknown words. When an unknown word
is tackled at test time, our algorithm collects web
contexts of this word that are then used to improve
the tag probability computations of the POS tag-
ger.
In our experiments we used our algorithm to en-
hance the unknown word tagging quality of the
MXPOST tagger (Ratnaparkhi, 1996), a leading
state-of-the-art tagger, which we implemented for
this purpose. We showed significant improvement
(error reduction of up to 18.09%) for three lan-
guages (English, German and Chinese) in seven
experimental setups. Our algorithm is especially
effective in domain-adaptation scenarios where
the training and test data are from different do-
mains.
Our algorithm is fast (requires less than a sec-
ond for processing an unknown word) and can
handle test sentences coming from any desired un-
known domain without the costs involved in col-
lecting domain-specific corpora and retraining the
tagger. These properties makes it particularly ap-
propriate for applications that work on the web,
which is highly heterogeneous.
In future work we intend to integrate our al-
gorithm with additional POS taggers, experiment
with additional corpora and domains, and improve
our context extraction mechanism so that our al-
gorithm will be able to fix more error types.
References
Blitzer, John, Ryan McDonald, and Fernando Pereira,
2006. Domain adaptation with structural correspon-
1281
dence learning. EMNLP ?06.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius and George Smith, 2002. The TIGER
Treebank. Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Brants, Thorsten, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Brants, Thorsten, 2000. Tnt: a statistical part-of-
speech tagger. In The Sixth Conference on Applied
Natural Language Processing.
Burnard, Lou, 2000. The British National Corpus
User Reference Guide. Technical Report, Oxford
University.
Chen, Qing, Mu Li, and Ming Zhou. 2007. Improving
query spelling correction using web search results.
In EMNLP-CoNLL ?07.
Chklovski, Timothy and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained semantic
verb relations. EMNLP ?04.
Clark, Stephen, James Curran and Miles Osborne.
2003. Bootstrapping POS-taggers using unlabeled
data. CoNLL ?03.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. EMNLP ?02.
Daume III, Hal. 2007. Frustratingly easy domain
adaptation. ACL ?07.
Davidov, Dmitry, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Huihsin, Tseng, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. The Fourth SIGHAN Workshop on Chinese
Language Processing.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and
Jun?ichi Tsujii, 2003. GENIA corpus ? a seman-
tically annotated corpus for bio-textmining. Bioin-
formatics, 19:i180?i182, Oxford University Press,
2003.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. The Eighteenth International Con-
ference on Machine Learning.
Keller, Frank, Mirella Lapata, and Olga Ourioupina.
2002. Using the Web to Overcome Data Sparseness.
EMNLP ?02.
Keller, Frank, Mirella Lapata. 2003. . Computational
Linguistics, 29(3):459?484.
Lease, Matthew and Eugene Charniak. 2005. Pars-
ing Biomedical Literature. Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing..
Manning Chris and Hinrich Schuetze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press..
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
McClosky, David, Eugene Charniak, and Mark John-
son, 2006a. Effective self-training for parsing.
HLT-NAACL ?06.
Nakagawa, Tetsuji and Yuji Matsumoto. 2006. Guess-
ing parts-of-speech of unknown words using global
information. ACL-COLING ?06 .
PennBioIE. 2005. Mining the Bibliome Project..
http://bioie.ldc.upenn.edu.
Qiu, Likun, Changjian Hu and Kai Zhao. 2008. A
method for automatic POS guessing of Chinese un-
known words. COLING ?08.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. EMNLP ?96.
Reichart, Roi and Ari Rappoport. 2007. Self-Training
for Enhancement and Domain Adaptation of Statis-
tical Parsers Trained on Small Datasets. ACL ?07.
Reynolds, Sheila M. and Jeff A. Bilmes. 2005. Part-
of-speech tagging using virtual evidence and nega-
tive training. EMNLP ?06.
Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. EMNLP ?04.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. EMNLP
?00.
Toutanova, Kristina, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
NAACL ?03.
Vadas, David and James R. Curran. 2005. Tagging un-
known words with raw text features. Australasian
Language Technology Workshop 2005.
Nianwen Xue, Fu-Dong Chiou and Martha Palmer,
2002. Building a large?scale annotated Chinese
corpus. ACL ?02.
1282
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1612?1623, Dublin, Ireland, August 23-29 2014.
Minimally Supervised Classification to Semantic Categories using
Automatically Acquired Symmetric Patterns
Roy Schwartz
1
Roi Reichart
2
1
Institute of Computer Science, The Hebrew University
{roys02|arir}@cs.huji.ac.il
2
Technion IIT
roiri@ie.technion.ac.il
Ari Rappoport
1
Abstract
Classifying nouns into semantic categories (e.g., animals, food) is an important line of research
in both cognitive science and natural language processing. We present a minimally supervised
model for noun classification, which uses symmetric patterns (e.g., ?X and Y?) and an iterative
variant of the k-Nearest Neighbors algorithm. Unlike most previous works, we do not use a
predefined set of symmetric patterns, but extract them automatically from plain text, in an unsu-
pervised manner. We experiment with four semantic categories and show that symmetric patterns
constitute much better classification features compared to leading word embedding methods. We
further demonstrate that our simple k-Nearest Neighbors algorithm outperforms two state-of-
the-art label propagation alternatives for this task. In experiments, our model obtains 82%-94%
accuracy using as few as four labeled examples per category, emphasizing the effectiveness of
simple search and representation techniques for this task.
1 Introduction
The role of language is to express meaning. In the field of NLP, there has been an increasingly grow-
ing number of approaches that deal with semantics. Among these are vector space models (Turney and
Pantel, 2010; Baroni and Lenci, 2010), lexical acquisition (Hearst, 1992; Dorow et al., 2005; Davidov
and Rappoport, 2006), universal cognitive conceptual annotation (Abend and Rappoport, 2013) and au-
tomatic induction of feature representations (Collobert et al., 2011). In this paper, we utilize extremely
weak supervision to classify words into fundamental cognitive semantic categories.
There are several types of semantic categories expressed by languages, e.g., objects, actions, and
properties. We follow human development, acquiring coarse-grained categories and distinctions before
detailed ones (Mandler, 2004). Specifically, we focus on the major class of concrete ?things? (Langacker,
2008, Ch. 4), roughly corresponding to nouns ? the main participants in linguistic clauses ? that are
universally present in the semantics of virtually all languages (Dixon, 2005).
Most works on noun classification to semantic categories require large amounts of human annotation
to build training corpora for supervised algorithms (Bowman and Chopra, 2012; Moore et al., 2013) or
rely on language-specific resources such as WordNet (Evans and Or?asan, 2000; Or?asan and Evans, 2007).
Such heavy supervision is labor intensive and makes these models domain and language dependent.
Our reasoning is that weak supervision is highly valuable for semantic categorization, as it can com-
pensate for the lack of input from the senses in text corpora. Our model therefore performs semantic
category classification using only a small number of labeled seed words per category. The experiments
we conduct show that such weak supervision is sufficient to construct a high quality classifier.
A key component of our model is the application of symmetric patterns. We define patterns to be
sequences of words and wildcards (e.g., ?X is a dog?, ?both X and Y?, etc.). Accordingly, symmet-
ric patterns are patterns that contain exactly two wildcards, where both wildcards are interchangeable.
Examples of symmetric patterns include ?X and Y?, ?X as well as Y? and ?neither X nor Y?.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/.
1612
Works that apply symmetric patterns in their model generally require expert knowledge in the form of a
pre-compiled set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we extract
symmetric patterns in an unsupervised manner using the (Davidov and Rappoport, 2006) algorithm. This
algorithm automatically extracts a set of symmetric patterns from plain text using simple statistics about
high and low frequency word co-occurrences. The unsupervised nature of our approach makes it domain
and language independent.
Our model addresses semantic classification in a transductive setup. It takes advantage of word sim-
ilarity scores that are computed based on symmetric pattern features, and propagates information from
concepts with known classes to the rest of the concepts. For this aim we apply an iterative variant of the
k-Nearest Neighbors algorithm (denoted with I-k-NN) to a graph in which vertices correspond to nouns
and word pairs are connected with edges based on their participation in symmetric patterns.
We experiment with a subset of 450 nouns from the CSLB dataset (Devereux et al., 2013), which were
annotated with semantic categories by thirty human subjects. From the set of semantic categories in this
dataset, we select categories that are both frequent and have a high inter-annotator agreement (Section 2).
This results in a set of four semantic categories ? animacy, edibility, is a tool and is worn.
Our experiments show that our model performs very well even when only a small number of labeled
seed words are available. For example, on the task of binary classification with respect to a single
category, when using as few as four labeled seed words, classification accuracy reaches 82%-94%.
Furthermore, our model outperforms several strong baselines for this task. First, we compare our
model against a model that uses a deep neural network word embedding baseline (Collobert et al., 2011)
instead of our symmetric pattern based features, and applies the exact same I-k-NN algorithm. In recent
years, deep networks word embeddings obtained state-of-the-art results in several NLP tasks (Collobert
and Weston, 2008; Socher et al., 2013). However, in our task, features based on simple, intuitive and
easy to compute symmetric patterns, lead to substantially better performance (average improvement of
0.15 F1 points). Second, our model outperforms two baseline models that utilize the same symmetric
pattern classification features as in our model, but replace our simple I-k-NN algorithm with two leading
label propagation alternatives (the normalized graph cut (N-Cut) algorithm (Yu and Shi, 2003) and the
Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)). The average improvement over
these two baselines is 0.21 and 0.03 F1 points .
The rest of the paper is organized as follows. Section 2 describes our semantic classification task
and, particularly, the semantic classes that we aim to learn. Section 3 presents our method for automatic
symmetric patterns acquisition. Sections 4, 5 and 6 describe our model, experimental setup and results,
respectively. Related work is finally surveyed in Section 7.
2 Task Definition
The task we tackle in this paper is the classification of nouns into semantic categories. This section
defines the categories we address and the dataset we use.
Semantic Categorization of Concrete Nouns. We focus on concrete ?things? (Langacker, 2008),
which correspond to noun categories. Nouns are interesting because they are the most basic lexical
semantic categories. Specifically, children acquire nouns before any other category (Clark, 2009). More-
over, noun categories are generally not subjective. For example, it is hard to argue that a dog is not
an animal, or that an apple is inedible, in most reasonable contexts. The context independent nature of
nouns makes them appropriate for a type level classification task, such as the one we tackle. In order to
provide a better description of the categories we aim to predict, we now turn to discuss the CSLB dataset,
with which we experiment.
Dataset. We experiment with the CSLB property norms dataset (Devereux et al., 2013). In order to
prepare this data set, thirty human subjects were presented with 638 concrete nouns and were asked to
write the categories associated with each concept. Table 1 presents the top five categories for the nouns
apple and horse.
1613
Noun Categories
Apple is a fruit, does grow on trees, is green, is red, has pips seeds
Horse is ridden, is an animal, has four legs, has legs, has hooves
Table 1: Five most frequent semantic categories for the words apple and horse in the CSLB dataset.
Category Selection. The CSLB dataset consists of a total of 2725 semantic categories. We apply
a selection mechanism that provides us with a dataset in which (1) only noun categories (things) are
included; and (2) only semantic categories that are prominent across humans are considered. For this,
we apply the following filtering stages. First, since the vast majority of annotated categories are rare (for
example, 1691 categories are assigned to a single noun only), we set a minimum threshold of 35 nouns
per category (5% of the nouns). After removing highly infrequent categories, 28 are left. We then apply
an inter-annotator agreement criterion: for each semantic category c, we compute the average number
of human annotators that associated this category with a given noun, across the nouns annotated with c.
We select the category c only if the value of this statistic is higher than 10 subjects (1/3 of the subjects),
which results in a semantic category set of size 18. Finally, we discard categories, such as color and size,
that do not correspond to things. We are left with four noun semantic categories: animacy (animals),
edibility (food items), is a tool (tools), and is worn (clothes).
Interestingly, the resulting semantic categories can also be justified from a cognitive perspective. There
is a large body of work indicating that our categories relate to brain organization principles. For example,
Just et al. (2010) showed that food products and tools arouse different brain activation patterns. More-
over, a number of works showed that both animate objects and tools are represented in specific brain re-
gions. These works used neuroimaging methods such as functional magnetic resonance imaging (fMRI)
(Naselaris et al., 2012), electroencephalography (EEG) (Chan et al., 2011) and magnetoencephalogra-
phy (MEG) (Sudre et al., 2012). See (Martin, 2007) for a detailed survey. This parallel evidence to the
prominence of our categories provides substance for intriguing future research.
3 Symmetric Patterns
Patterns. In this work, patterns are combinations of words and wildcards, which provide a structural
phrase representation. Examples of patterns include ?X and Y?, ?X such as Y?, ?X is a country?, etc.
Patterns can be used to extract various relations between words. For example, patterns such as ?X of a
Y? (?basement of a building?) can be useful for detecting the meronymy (part-of) relation (Berland and
Charniak, 1999). Symmetric patterns (e.g., ?X and Y?, ?France and Holland?), which we use in this
paper, can be used to detect semantic similarity between words (Widdows and Dorow, 2002).
Symmetric Patterns. Symmetric patterns are patterns that contain exactly two wildcards, and where
these wildcards are interchangeable. Examples of symmetric patterns include ?X and Y?, ?X or Y? and
?X as well as Y?. Previous works have shown that word pairs that participate in symmetric patterns bare
strong semantic resemblance, and consequently, that these patterns can be used to cluster words into
semantic categories, where a high precision, but low coverage (recall) solution is good enough (Dorow
et al., 2005; Davidov and Rappoport, 2006). A key observation of this paper is that symmetric patterns
can be also used for semantic classification, where recall is as important as precision.
Flexible Patterns. It has been shown in previous work (Davidov and Rappoport, 2006; Turney, 2008;
Tsur et al., 2010; Schwartz et al., 2013) that patterns can be extracted from plain text in a fully unsu-
pervised manner. The key idea that makes this procedure possible is the concept of ?flexible patterns?,
which are composed of high frequency words (HFW) and content words (CW). Every word in the lan-
guage is defined as either HFW or CW, based on the number of times this word appears in a large corpus.
This clustering procedure is applied by traversing a large corpus, and marking words that appear with
corpus frequency higher than a predefined threshold t
1
as HFWs, and words with corpus frequency lower
than t
2
as CWs.
1
1
We follow (Davidov and Rappoport, 2006) and set t
1
= 10
?5
, t
2
= 10
?3
. Note that some words are marked both as HFW
and as CW. See (Davidov and Rappoport, 2008) for discussion.
1614
The resulting clusters have a desired property: HFWs are comprised mostly of function words (prepo-
sitions, determiners, etc.) while CWs are comprised mostly of content words (nouns, verbs, adjectives
and adverbs). This coarse grained clustering is useful for pattern extraction from plain text, since lan-
guage patterns tend to use fixed function words, while content words change from one instance of the
pattern to another (Davidov and Rappoport, 2006).
Flexible patterns are extracted by traversing a large corpus and, based on the clustering of words to
CWs and HFWs, extracting all pattern instances. An extracted pattern instance consists of CW wildcards
and the actual words replacing the HFWs in the pattern type. Consider the sentence ?The boy is happy
and joyful?. Replacing the content words with the CW wildcard results in ?The CW is CW and CW?.
From this intermediate representation, we extract word sequences of a given length constraint and denote
them as flexible patterns.
2
The flexible patterns of length 5 extracted from this sentence are ?The CW is
CW and? and ?CW is CW and CW?. The reader is referred to (Davidov and Rappoport, 2006) for more
details.
Automatically Extracted Symmetric Patterns. Most models that incorporate symmetric patterns use
a predefined set of patterns (Widdows and Dorow, 2002; Kozareva et al., 2008). In this work, we apply
an automatic, completely unsupervised procedure for symmetric pattern extraction. This procedure,
described in Algorithm 1, is adopted from (Davidov and Rappoport, 2006).
The procedure first extracts flexible patterns that contain exactly two CW wildcards. It then selects
those flexible patterns in which both CWs are interchangeable. That is, it selects a pattern p if every
word pair CW
1
, CW
2
that participates in p indicates with high probability that the word pair C
2
, C
1
also participates in p. For example, for the symmetric pattern ?CW and CW?, both ?cats and dogs?
and ?dogs and cats? are semantically plausible expressions, and are therefore likely to appear in a large
corpus. On the other hand, the flexible pattern ?CW such as CW? is asymmetric, as exemplified in
expressions like ?countries such as France?, where replacing the CWs does not result in a semantically
plausible expression (# ?France such as countries?). The selection process is done by computing the
proportion of CW
1
, CW
2
pairs that participate in p for which CW
2
, CW
1
also participates in p. Patterns
for which this proportion exceed a certain threshold are selected.
We apply Algorithm 1 on the google books 5-gram corpus (Michel et al., 2011)
3
and extract 20 sym-
metric patterns. Some of the more interesting symmetric patterns extracted using this algorithm include
?CW and the CW?, ?from CW to CW?, ?CW rather than CW? and ?CW versus CW?. In the next section
we present our approach to semantic classification, which makes use of automatically acquired symmet-
ric patterns for word similarity computations.
4 Model
In this section we present our model for binary word classification according to a single semantic category
in a minimally-supervised, transductive setup. Given a set of words, we label a small number of words
with their correct label according to the category at hand (+1 for words that belong to the category, -1
for words that do not belong to it). Our model is based on an undirected weighted graph, in which
vertices correspond to words, and edges correspond to relations between words. Our goal is to classify
the unlabeled words (vertices) in the graph through a label propagation process. We now turn to describe
our model in detail.
Graph Construction. We construct our graph such that an edge is added between two words (vertices)
if both words participate in a symmetric pattern. The edge generation process is performed as follows.
We first apply our symmetric pattern extraction procedure (Algorithm 1), and denote the set of selected
symmetric patterns with P . We then traverse a large corpus
4
and extract all word pairs that participate
in any pattern p ? P . We denote the number of occurrences of a word pair (w
1
, w
2
) in such patterns
with f
w
1
,w
2
. Finally, we select all word pairs (w
1
, w
2
) for which min(f
w
1
,w
2
, f
w
2
,w
1
) > ?. Each such
2
We set the maximal flexible pattern length to be 5.
3
https://books.google.com/ngrams
4
We use google books 5-grams (Michel et al., 2011).
1615
Algorithm 1 Symmetric pattern extraction
1: procedure EXTRACT SYMMETRIC PATTERNS(C,W )
2: . C is a large corpus, W is a lexicon
3: . Traverse C and extract all flexible patterns of length 3-5 that appear in C and contain exactly two content words
4: P ? extract flexible patterns(C,W )
5: for p ? P do
6: if p appears in <10
?6
of the sentences in C then
7: Discard p and continue
8: end if
9: G
p
? a directed graph s.t. V (G
p
)?W ,E(G
p
)?{(w
1
, w
2
)?W
2
:w
1
,w
2
participate in at least one instance of p}
10: . An undirected graph based on the bidirectional edges of the G
p
11: symG
p
? an undirected graph: {(w
1
), (w
1
,w
2
) : (w
1
,w
2
) ? E(G
p
) ? (w
2
, w
1
) ? E(G
p
)}
12: . Two measures of symmetry
13: M
1
?
|
V (symG
p
)
|
|
V (G
p
)
|
,M
2
?
|
E(symG
p
)
|
|
E(G
p
)
|
14: . Symmetric pattern candidates are those with high M
1
and M
2
values
15: if min (M
1
,M
2
) < 0.05 then
16: Discard p
17: end if
18: end for
19: for p ? P do
20: . E.g., ?CW and CW? is contained in ?both CW and CW?
21: if ?p
?
? P s.t. p
?
is contained in p then
22: Discard p
23: end if
24: end for
25: return The top 20 members of P with the highest M
1
value
26: end procedure
pair is connected with an edge e
w
1
,w
2
in the graph, where the edge weight (denoted with w
w
1
,w
2
) is the
geometric mean between f
w
1
,w
2
and f
w
2
,w
1
.
Label Propagation. Given a small number of annotated words (vertices), our goal is to propagate the
information these words convey to other words in the graph. To do so, we apply an iterative variant of the
k-Nearest Neighbors algorithm (I-k-NN). This iterative variant is required due to graph sparsity; when
starting with a small set of labeled vertices, most unlabeled vertices do not have any labeled neighbor, and
thus running the standard k-NN algorithm would result in classifying a very small number of vertices.
Our approach is to run iterations of the k-NN algorithm, and thus propagate information to additional
vertices at each iteration. At each k-NN step, the algorithm selects words that have at least one labeled
neighbor. From this set, only the words that have the highest ratio of neighbors with the same label are
selected, and are assigned with this label.
Consider a simple example. Say we have three candidate vertices a, b and c, where a has one neighbor
with label +1 (ratio(a) = 1/1 = 1.0), b has two neighbors with label -1 (ratio(b) = 2/2 = 1.0) and
c has three neighbors with label +1 and one neighbor with label -1 (ratio(c) = max(3, 1)/4 = 3/4).
Then, a and b are selected and are assigned with +1 and ?1, respectively.
Seed Expansion. In minimally supervised setups like ours, the model is initialized with a small set of
labeled seed examples. A natural approach in such settings is to apply a seed expansion step, in order to
obtain a larger set of labeled seeds. Our method uses the same graph construction procedure described
above, but uses a larger edge generation threshold ? >> ?.
5
We then apply an iterative procedure that
labels a vertex v with a label l if either (a) v is directly connected to ? of the vertices labeled with l or (b)
v is connected to ?
l
of the neighbors of vertices labeled with l.
6
This procedure is run iteratively until no
more vertices meet any of the criteria (a) or (b).
5
Using a larger threshold results in a sparser graph. Nevertheless, each edge in this graph is more likely to represent a real
semantic relation.
6
? and ?
l
are hyperparameters tuned on our development set (see Section 5.2).
1616
5 Experimental Setup
5.1 Baselines
We compare our model to two types of baselines. The first (Classification Features Baselines) utilizes
the I-k-NN algorithm, along with a different set of classification features. The second (Label Propa-
gation Baselines) utilizes the same classification features as we do, but replaces I-k-NN with a more
sophisticated label propagation algorithm.
5.1.1 Classification Features Baselines
In this set of baselines, we use different methods for building our graph. Concretely, instead of adding
edges for pairs of words that appear in the same symmetric pattern, we use word similarity measures
based on different feature sets as described below. The process of building the graph using the baseline
word similarity measures is described in Section 5.2.
SENNA. Deep neural networks have gained recognition as leading feature extraction methods for word
representation (Collobert and Weston, 2008; Socher et al., 2013). We use SENNA,
7
a deep network based
word embedding method, which has been used to produce state-of-the-art results in several NLP tasks,
including POS tagging, chunking, NER, parsing and SRL (Collobert et al., 2011). We use the cosine
similarity between two word embeddings as a word similarity measure.
Brown. This baseline is derived from the clustering induced by the Brown algorithm (Brown et al.,
1992).
8
This clustering, in which words share a cluster if they tend to appear in the same lexical con-
text, has shown useful for several NLP tasks, including POS tagging (Clark, 2000), NER (Miller et al.,
2004) and dependency parsing (Koo et al., 2008). We use it in order to control for the possibility that a
simple contextual preference similarity correlates with similarity in semantic categorization better than
symmetric pattern features.
The Brown algorithm builds a binary tree, where words are located at leaf nodes. We use the graph
distance between two words u, v (i.e., the shortest path length between u, v in the tree) as a word simi-
larity measure for building our graph.
5.1.2 Label Propagation Baselines
In this type of baselines, we replace I-k-NN with a different label propagation algorithm, while still using
the symmetric pattern features for word similarity computations.
N-Cut. This baseline applies the normalized graph cut algorithm (Yu and Shi, 2003)
9
for label propa-
gation. Given a graphG = (V,E) and two sets of verticesA,B ? V , this algorithm defines links(A,B)
to be the sum of edge weights between A and B. The objective of the algorithm is to find the clusters
A, V \ A that minimize
links(A,V \A)
links(A,V )
. The algorithm of (Yu and Shi, 2003) is particularly efficient for
this problem as it avoids eigenvector computations which may become computationally prohibitive for
large graphs (for more details, see their paper). In order to encode information about our labeled seed
words, we hard-code a large negative value (-100000) to the weights of edges between seed words with
different labels (positive and negative).
MAD. The Modified Adsorption (MAD) algorithm (Talukdar and Crammer, 2009)
10
is an extension
of the Adsorption algorithm (Baluja et al., 2008). MAD is a stochastic graph-based label propagation
algorithm which has shown to have a number of attractive theoretical properties and demonstrated good
experimental results.
7
The word embeddings were downloaded from http://ml.nec-labs.com/senna/
8
We use the clusters induced by (Koo et al., 2008), who applied the Brown algorithm implementation of (Liang,
2005) to the BLLIP corpus (Charniak et al., 2000). http://www.people.csail.mit.edu/maestro/papers/
bllip-clusters.gz
9
http://www.cis.upenn.edu/
?
jshi/software/Ncut_9.zip
10
http://github.com/parthatalukdar/junto
1617
5.2 Experiments
Graph Construction. We experiment with the CSLB dataset (Devereux et al., 2013), consisting of 638
nouns, annotated with their semantic categories by thirty human subjects. We first omit all nouns that
are annotated as having more than one sense, and use the remaining 603 nouns to build our graph. From
these nouns, 146 nouns are annotated as animate, 115 as edible, 50 as wearable and 35 as tools.
11
We
then discard nouns that have less than two neighbors, which results in a final set of 450 nouns (vertices).
The graphs used in the classification features baselines are different than those used by the models that
use our symmetric pattern classification features, since the features define the graph structure (Section 4).
In order to provide a meaningful comparison, we build graphs with the same number of vertices for each
of these baselines. We do so by selecting the n edges with the highest weight, together with the set of
vertices connected by these edges, such that the resulting graph has 450 vertices. Working with these
sets of vertices is the optimal setting for these baselines, as the resulting graphs are the ones with the
highest possible edge weights for graphs with 450 vertices.
12
Parameter Tuning. In order to avoid adding additional labeled examples for the sake of parameter
tuning, we set the hyperparameter values to the ones for which each model performs best on an auxiliary
semantic classification task. Concretely, we experiment with a fifth semantic category (audibility),
13
which is not part of our evaluation setting, for parameter tuning. Note that this results in our model
having the same hyperparamter values for all four classification tasks.
In order to ensure that the models assign all participating words with labels, we set ?=3, where ? is
the minimal number of times a word pair should appear in the same symmetric pattern in order to have
an edge in our graph (See Section 4). In our seed expansion procedure, where we search for seeds whose
label is predicted with high confidence, only word pairs that appear at least ?=50 times in the same
symmetric pattern are assigned an edge in the graph. We set the seed expansion procedure parameters to
be ? = 0.6, ?
+1
= 0.5, ?
?1
= 0.2.
Evaluation. For each classification task, we run experiments with 4, 10, 20 and 40 labeled seed words.
In each setting, half of the labeled seed words are assigned a positive label and the other half are assigned
a negative label. For each semantic category and labeled seed set size, we repeat our experiment 1000
times, each of which with a different set of randomly selected labeled seed examples, and report the
average results. We report both accuracy (number of correct labels divided by number of vertices in
the graph) and F1 score, which is the harmonic mean of p (the average precision across labels) and r
(average recall across labels).
These two measures represent complementary aspects of our results. On the one hand, accuracy is
the most natural classification performance measure. On the other hand, the number of positive labels is
substantially smaller than the number of negative labels,
14
and thus this measure can be manipulated: a
dummy model that always assigns the negative label gets a high accuracy. The F1 score controls against
such models by assigning them low scores.
6 Results
Our experiments are designed to explore two main questions: (a) the value of symmetric patterns as
semantic classification features, compared to state-of-the-art word clustering and embedding methods;
and (b) the required complexity of an algorithm that can propagate information about semantic simi-
larity. Particularly, we test the value of our simple I-k-NN algorithm compared to more sophisticated
alternatives.
A Minimally Supervised Setting. Our first set of experiments is in a minimally supervised setting
where only two positive and two negative examples are available for each binary classification task. This
11
Some words are classified as belonging to more than one category (e.g., ?chicken? is both animate and edible).
12
The resulting graphs are actually denser than the symmetric patterns-based graph: 14K and 9K edges for the Brown and
SENNA graphs, respectively, compared to < 5K edges in the symmetric patterns graph.
13
We used four labeled seed words in these experiments.
14
Only 6-25% of the nouns have a positive label.
1618
Animacy Edibility is worn is a tool
SP SENNA Brown SP SENNA Brown SP SENNA Brown SP SENNA Brown
Acc.
MAD 80.4% 77.7% 12.0% 75.0% 56.5% 14.8% 82.7% 66.8% 14.7% 73.3% 67.7% 12.2%
N-Cut 71.4% 60.4% 51.2% 75.5% 59.4% 50.9% 83.3% 71.5% 51.4% 82.7% 77.1% 52.0%
I-k-NN 85.1% 76.0% 55.5% 82.2% 56.8% 68.0% 94.1% 70.9% 66.7% 82.0% 75.7% 65.0%
F1
MAD 0.77 0.76 0.18 0.69 0.55 0.24 0.71 0.56 0.22 0.58 0.47 0.17
N-Cut 0.49 0.45 0.46 0.51 0.44 0.45 0.61 0.56 0.41 0.56 0.50 0.38
I-k-NN 0.78 0.70 0.48 0.71 0.53 0.62 0.86 0.59 0.55 0.64 0.52 0.51
Table 2: Accuracy and F1 score comparison between our model and the baselines. The columns cor-
respond to the type of classification features used by the model: SP ? symmetric patterns, SENNA ?
word embeddings extracted using deep networks (Collobert et al., 2011), Brown ? Brown word clus-
tering (Brown et al., 1992). The rows correspond to the algorithms applied by the model: N-Cut ? the
normalized graph cut algorithm (Yu and Shi, 2003), MAD ? the modified adsorption algorithm (Talukdar
and Crammer, 2009), I-k-NN ? our iterative k-NN algorithm. Our model (I-k-NN + SP) is superior in all
cases, except for the accuracy of the ?is a tool? semantic category, where it is second only to N-Cut+SP.
5 10 15 200.55
0.6
0.65
0.7
0.75
0.8
0.85
#training_samples
F1 Sco
re
 
 
SENNA (MAD)Brown (I?k?NN)Symmetric Patterns (I?k?NN)
(a) Classification Features Comparison
5 10 15 200.45
0.50.55
0.60.65
0.70.75
0.80.85
#training_samples
F1 Sco
re
 
 
N?Cut (Symmetric Patterns)MAD (Symmetric Patterns)I?k?NN (Symmetric Patterns)
(b) Algorithm Comparison
5 10 15 200.5
0.550.6
0.650.7
0.750.8
0.85
#training_samples
F1 Sco
re
 
 
I?k?NN, SENNAMAD, SENNAMAD, Symmetric PatternsI?k?NN, Symmetric Patterns
(c) Top four Best Models
Figure 1: (a) Comparison of the different classification features. The figure shows the F1 scores of the
best model that uses each of the feature sets (the label propagation algorithm used in each model appears
in parentheses). (b) Comparison of the different label propagation algorithms. The figure shows the F1
scores of the best model that uses each of the algorithms (the classification feature sets used in each model
appears in parentheses. It is always symmetric patterns). (c) The four best overall models (algorithm +
classification feature set). The figures show that the symmetric pattern feature set is superior to the other
feature sets, and that I-k-NN is superior or comparable to the other label propagation algorithms.
setup enables us to explore the performance of our model when the amounts of labeled training data is
taken to the possible minimum.
Table 2 presents our results. With respect to objective (a), the table clearly demonstrates that symmetric
patterns lead to much better results compared to the alternatives. Particularly, for all four semantic
categories, and across both evaluation measures, it is a model that utilizes symmetric pattern classification
features that achieves the best results. The average difference between the best model that uses symmetric
patterns and the best model that does not is 12.5% accuracy and 0.13 F1 points. The dominance of
symmetric pattern classification features is further demonstrated by the fact that a model that uses these
features always performs better than a model that uses the same algorithm but different features.
With respect to objective (b) the table shows that I-k-NN provides a large improvement in seven out
of eight (category ? evaluation measure) settings. The average difference between the best model that
utilizes I-k-NN and the best model that applies a different algorithm is 5.4% accuracy and 0.06 F1 points.
Analysis of Labeled Seed Set Size. In order to get a wider perspective on our model, we repeated our
experiments with various sizes of the labeled seed set: 5,10 and 20 positive and negative labeled examples
per semantic category. For brevity, only the F1 score results of the edibility category are presented. The
trends observed on the other semantic categories (as well as when using the accuracy measure) are very
similar.
Figure 1a compares the different classification features. For each feature f , results of the best per-
forming model that uses f are shown. The results reveal that symmetric patterns clearly outperform the
other features. The average differences between the best symmetric patterns-based model and the best
1619
models that use the other features are 0.15 (SENNA) and 0.16 (Brown) F1 points.
Figure 1b compares the different label propagation algorithms. For each algorithm a, results for the
best performing model that uses a are presented. The results reveal that the I-k-NN algorithm outper-
forms both algorithms by 0.03 (MAD) and 0.21 (N-Cut) F1 points. The results also show that for all
algorithms, the best performing model uses symmetric patterns classification features, which further
demonstrates the dominance of these features.
Finally, Figure 1c presents the four top performing models (algorithm + classification feature). In
accordance with the other findings presented in this section, the top two models, which outperform the
other models by a large margin, apply symmetric pattern classification features.
Seed Expansion Effect. Our model uses a seed expansion procedure in order to expand a small set of
labeled seed words to a larger set (see Section 4). In order to assess the quality of this procedure we
compute, for each semantic category, the average size of the expanded set and the accuracy of the new
seeds (i.e., the proportion of new seeds that are labeled correctly). Results show that the initial set is
increased from four seeds (two positive + two negative) to 48-52, and that the accuracy of the new seeds
is as high as 88-99%. Our experiments also show that this procedure provides a substantial performance
boost to our I-k-NN algorithm, which obtains a 7.2% accuracy and 0.05 F1 points improvement (averaged
over the four semantic categories) when applied with the expanded set of labeled seed words compared
to the original set of size four.
7 Related Work
Classification into Semantic Categories. Several works tackled the task of semantic classification,
mostly focusing on animacy, concreteness and countability. The vast majority of these works are either
supervised (Hatzivassiloglou and McKeown, 1997; Baldwin and Bond, 2003; Peng and Araki, 2005;
?vrelid, 2005; Nagata et al., 2006; Xing et al., 2010; Kwong, 2011; Bowman and Chopra, 2012) or
make use of external, language-specific resources such as WordNet (Or?asan and Evans, 2001; Or?asan
and Evans, 2007; Moore et al., 2013). Our work, in contrast, is minimally supervised, requiring only a
small set of labeled seed words.
Ji and Lin (2009) classified words into the gender and animacy categories, based on their occurrences
in instances of hand-crafted patterns such as ?X who Y? and ?X and his Y?. While their model uses
patterns that are tailored to the animacy and gender categories, our model uses automatically induced
patterns and is thus applicable to a range of semantic categories.
Finally, Turney et al. (2011) built a label propagation model that utilizes LSA (Landauer and Dumais,
1997) based classification features. They used their model to classify nouns into the concrete/abstract
category using 40 labeled seed words . Unlike our model, which requires only a small set of labeled seeds,
their algorithm is actually heavily supervised, requiring thousands of labeled examples for selecting the
seed set of labeled words that are used for propagation. Our model, on the other hand, does not require
any seed selection procedure, and utilizes a randomly selected set of labeled seed words.
Lexical Acquisition. Another line of work focused on the acquisition of semantic categories. In this
setup, a model aims to find a core seed of words belonging to a given category, sacrificing recall for
precision. Our model tackles a different task, namely the classification of words according to a given
category where both recall and precision are to be optimized.
Lexical acquisition models are either supervised (Snow et al., 2006), unsupervised, making use of
symmetric patterns (Davidov and Rappoport, 2006), or lightly supervised, requiring expert, language
specific knowledge for compiling a set of hand-crafted patterns (Widdows and Dorow, 2002; Kozareva et
al., 2008; Wang and Cohen, 2009). Other models require syntactic annotation derived from a supervised
parser to extract coordination phrases (Riloff and Shepherd, 1997; Dorow et al., 2005). Our model
automatically induces symmetric patterns, obtaining high quality results without relying on any type of
language specific knowledge or annotation. Moreover, some of the works mentioned above (Riloff and
Shepherd, 1997; Widdows and Dorow, 2002; Kozareva et al., 2008) also require manually selected label
1620
seeds to achieve good performance; in contrast, our work performs very well with a randomly selected
set of labeled seed words.
8 Conclusion
We presented a minimally supervised model for noun classification into coarse grained semantic cate-
gories. Our model obtains 82%-94% accuracy on four semantic categories even when using only four
labeled seed words per category. We showed that our modeling decisions ? using symmetric patterns as
classification features and a simple iterative k-NN algorithm for label propagation ? lead to a substantial
performance gain compared to state-of-the-art, more sophisticated, alternatives. Our results demonstrate
the applicability of minimally supervised methods for semantic classification tasks. Future work will
include modifying our model to support other, more fine-grained types of semantic categories, includ-
ing adjectival categories (properties). We also plan to work on token-level word classification, and thus
support multi-sense words, as well as demonstrate the power of unsupervised patterns acquisition for
multilingual setups.
Acknowledgments
This research was funded (in part) by the Harry and Sylvia Hoffman leadership and responsibility pro-
gram (for the first author), the Google Faculty research award (for the second author), the Intel Collab-
orative Research Institute for Computational Intelligence (ICRI-CI) and the Israel Ministry of Science
and Technology Center of Knowledge in Machine Learning and Artificial Intelligence (Grant number
3-9243).
References
O. Abend and A. Rappoport. 2013. Universal conceptual cognitive annotation (UCCA). In Proc. of ACL.
T. Baldwin and F. Bond. 2003. A plethora of methods for learning English countability. In Proc. of EMNLP.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random walks through the view graph. In Proc. of WWW, pages
895?904. ACM.
M. Baroni and A. Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Com-
putational Linguistics, 36(4):673?721.
M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Proc. of ACL.
S. R. Bowman and H. Chopra. 2012. Automatic Animacy Classification. In Proc. of NAACL-HLT Student
Research Workshop.
P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4):467?479.
A. M. Chan, J. M. Baker, E. Eskandar, D. Schomer, I. Ulbert, K. Marinkovic, S. S. Cash, and E. Halgren. 2011.
First-pass selectivity for semantic categories in human anteroventral temporal lobe. The Journal of Neuro-
science, 31(49):18119?18129.
E. Charniak, D. Blaheta, N. Ge, K. Hall, J. Hale, and M. Johnson. 2000. BLLIP 198789 WSJ Corpus Release 1,
LDC No. LDC2000T43. Linguistic Data Consortium.
A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of CoNLL.
E. V. Clark. 2009. First language acquisition. Cambridge University Press.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks
with multitask learning. In Proc. of ICML.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing
(almost) from scratch. JMLR, 12:2493?2537.
1621
D. Davidov and A. Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric pat-
terns and high frequency words. In Proc. of ACL-Coling.
D. Davidov and A. Rappoport. 2008. Unsupervised discovery of generic relationships using pattern clusters and
its evaluation by automatically generated SAT analogy questions. In Proc. of ACL-HLT.
B. J. Devereux, L. K. Tyler, J. Geertzen, and B. Randall. 2013. The centre for speech, language and the brain
(CSLB) concept property norms. Behavior research methods, pages 1?9.
R. M. Dixon. 2005. A semantic approach to English grammar. Oxford University Press.
B. Dorow, D. Widdows, K. Ling, J. P. Eckmann, D. Sergi, and E. Moses. 2005. Using Curvature and Markov
Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.
R. Evans and C. Or?asan. 2000. Improving anaphora resolution by identifying animate entities in texts. In Proc. of
DAARC.
V. Hatzivassiloglou and K. R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proc. of ACL.
M. A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of Coling ? Volume 2.
H. Ji and D. Lin. 2009. Gender and Animacy Knowledge Discovery from Web-Scale N-Grams for Unsupervised
Person Mention Detection. In Proc. of PACLIC.
M. A. Just, V. L. Cherkassky, S. Aryal, and T. M. Mitchell. 2010. A neurosemantic theory of concrete noun
representation based on the underlying brain codes. PloS one, 5(1):e8622.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of ACL-HLT.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage
graphs. In Proc. of ACL-HLT.
O. Y. Kwong. 2011. Measuring concept concreteness from the lexicographic perspective. In Proc. of PACLIC.
T. K. Landauer and S. T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.
R. W. Langacker. 2008. Cognitive grammar: A basic introduction. Oxford University Press.
P. Liang. 2005. Semi-supervised learning for natural language. Master?s thesis, Massachusetts Institute of Tech-
nology.
J. M. Mandler. 2004. The foundations of mind: Origins of conceptual thought. Oxford University Press New
York.
A. Martin. 2007. The representation of object concepts in the brain. Annual Review of Psychology, 58:25?45.
J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, D. Hoiberg, D. Clancy, P. Norvig, J. Or-
want, et al. 2011. Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176?
182.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In
Proc. of NAACL.
J. L. Moore, C. J. Burges, E. Renshaw, and W.-t. Yih. 2013. Animacy Detection with Voting Models. In Proc. of
EMNLP.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. Reinforcing English countability prediction with one
countability per discourse property. Proc. of ACL-Coling.
T. Naselaris, D. E. Stansbury, and J. L. Gallant. 2012. Cortical representation of animate and inanimate objects in
complex natural scenes. Journal of Physiology-Paris, 106(5):239?249.
C. Or?asan and R. Evans. 2001. Learning to identify animate references. In Proc. of the Workshop on Computa-
tional Natural Language.
C. Or?asan and R. Evans. 2007. NP Animacy Identification for Anaphora Resolution. JAIR, 29:79?103.
1622
L. ?vrelid. 2005. Animacy classification based on morphosyntactic corpus frequencies: some experiments with
Norwegian nouns. In Proc. of the Workshop on Exploring Syntactically Annotated Corpora, pages 1?11.
J. Peng and K. Araki. 2005. Detecting the countability of english compound nouns using web-based models. In
Proc. of IJCNLP.
E. Riloff and J. Shepherd. 1997. A corpus-based approach for building semantic lexicons. In Proc. of EMNLP.
R. Schwartz, O. Tsur, A. Rappoport, and M. Koppel. 2013. Authorship Attribution of Micro-Messages. In Proc.
of EMNLP.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proc.
of ACL-Coling.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank. In Proc. of EMNLP.
G. Sudre, D. Pomerleau, M. Palatucci, L. Wehbe, A. Fyshe, R. Salmelin, and T. Mitchell. 2012. Tracking neural
coding of perceptual and semantic features of concrete nouns. NeuroImage, 62(1):451?463.
P. P. Talukdar and K. Crammer. 2009. New regularized algorithms for transductive learning. In ECML-PKDD,
pages 442?457. Springer.
O. Tsur, D. Davidov, and A. Rappoport. 2010. ICWSM ? a great catchy name: Semi-supervised recognition of
sarcastic sentences in online product reviews. In Proc. of ICWSM.
P. D. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of
artificial intelligence research, 37(1):141?188.
P. Turney, Y. Neuman, D. Assaf, and Y. Cohen. 2011. Literal and metaphorical sense identification through
concrete and abstract context. In Proc. of EMNLP.
P. D. Turney. 2008. The latent relation mapping engine: Algorithm and experiments. Journal of Artificial Intelli-
gence Research, 33:615?655.
R. C. Wang and W. W. Cohen. 2009. Automatic set instance extraction using the web. In Proc. of ACL-IJCNLP.
D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of Coling.
X. Xing, Y. Zhang, and M. Han. 2010. Query difficulty prediction for contextual image retrieval. In Advances in
Information Retrieval, pages 581?585. Springer.
S. X. Yu and J. Shi. 2003. Multiclass spectral clustering. In Proc. of ICCV.
1623
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 325?334,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Tense Sense Disambiguation: a New Syntactic Polysemy Task
Roi Reichart
ICNC
Hebrew University of Jerusalem
roiri@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Polysemy is a major characteristic of natu-
ral languages. Like words, syntactic forms
can have several meanings. Understanding the
correct meaning of a syntactic form is of great
importance to many NLP applications. In this
paper we address an important type of syn-
tactic polysemy ? the multiple possible senses
of tense syntactic forms. We make our dis-
cussion concrete by introducing the task of
Tense Sense Disambiguation (TSD): given a
concrete tense syntactic form present in a sen-
tence, select its appropriate sense among a
set of possible senses. Using English gram-
mar textbooks, we compiled a syntactic sense
dictionary comprising common tense syntac-
tic forms and semantic senses for each. We an-
notated thousands of BNC sentences using the
defined senses. We describe a supervised TSD
algorithm trained on these annotations, which
outperforms a strong baseline for the task.
1 Introduction
The function of syntax is to combine words to ex-
press meanings, using syntactic devices such as
word order, auxiliary words, and morphology (Gold-
berg, 1995). Virtually all natural language devices
used for expressing meanings (e.g., words) exhibit
polysemy. Like words, concrete syntactic forms (the
sentence words generated by specific syntactic de-
vices) can have several meanings. Consider the fol-
lowing sentences:
(a) They are playing chess in the park.
(b) They are playing chess next Tuesday.
Both contain the concrete syntactic form ?are play-
ing?, generated by the abstract syntactic form usu-
ally known as ?present progressive? (am/is/are + V-
ing). In (a), the meaning is ?something happening
now?, while in (b) it is ?a plan to do something in the
future?. Note that the polysemy is of the syntactic
form as a unit, not of individual words. In particu-
lar, the verb ?play? is used in the same sense in both
cases.
In this paper we address a prominent type of syn-
tactic form polysemy: the multiple possible senses
that tense syntactic forms can have. Disambiguat-
ing the polysemy of tense forms is of theoretical
and practical importance (Section 2). To make our
discussion concrete, we introduce the task of Tense
Sense Disambiguation (TSD): given a concrete tense
syntactic form in a sentence, select its correct sense
among a given set of possible senses (Section 3).
The disambiguation of polysemy is a fundamental
problem in NLP. For example, Word Sense Disam-
biguation (WSD) continues to attract a large number
of researchers (Agirre and Edmonds, 2006). TSD
has the same structure as WSD, with different dis-
ambiguated entities.
For experimenting with the TSD task, we com-
piled an English syntactic sense dictionary based
on a thorough study of three major English gram-
mar projects (Section 4). We selected 3000 sen-
tences from the British National Corpus containing
4702 concrete syntactic forms, and annotated each
of these by its sense (Section 5).We developed a su-
pervised learning TSD algorithm that uses various
feature types and takes advantage of the task struc-
ture (Section 6). Our algorithm substantially outper-
325
forms the ?most frequent sense? baseline (Section 7).
TSD is fundamental to sentence understanding
and thus to NLP applications such as textual infer-
ence, question answering and information retrieval.
To the best of our knowledge, this is the first paper to
address this task. In Section 8 we discuss research
directions relevant to TSD placing the new task in
the context of the previous research of syntactic am-
biguity resolution.
2 TSD Motivation
In this work we follow linguistics theories that posit
that tense does not directly reflect conceptual time as
one might think. Dinsmore (1991) and Cutrer (1994)
explain that the same tense may end up indicating
very different objective time relations relative to the
sentence production time.
Fauconnier (2007) exemplifies such phenomena.
In the following sentences, the present tense corre-
sponds to the future time: (1) The boat leaves next
week. (2) When he comes tomorrow, I will tell him
about the party. (3) If I see him next week, I will ask
him to call you.
In contrast, the following present tense sentences
talk about events that happened in the past: (1) I am
walking down the street one day when suddenly this
guy walks up to me. (2) He catches the ball. He
runs. He makes a touchdown. (morning-after sports
report).
Another set of examples is related to the past
tense. In the following sentences it corresponds to
a present time: (1) Do you have a minute? I wanted
to ask you a question. (2) I wish I lived closer to my
family now. In contrast, in the following two sen-
tences, it corresponds to a future time: (1) If I had
the time next week, I would go to your party. (2) I
cannot go to the concert tonight. You will have to
tell me how it was.
Fauconnier explains these phenomena by a model
for the grammar of tense. According to this model,
the grammar specifies partial constraints on time and
fact/prediction status that hold locally between men-
tal spaces within a discourse configuration. We may
obtain actual information about time by combining
this with other available pragmatic information. Ac-
cordingly, the same tense may end up indicating
very different objective time relations relative to the
speech event.
TSD fits well with modern linguistics theories.
For example, in the construction grammar frame-
work (Goldberg, 1995), the ?construction? is the ba-
sic unit, comprised of a form and a meaning. Words,
multiword expressions, and syntactic forms are all
valid constructions. It is thus very natural to address
the sense disambiguation problem for all of these. In
this paper we focus on tense constructions.
For many NLP applications, it is very important
to disambiguate the tense forms of the sentence.
Among these applications are: (1) machine transla-
tion, as the actual time described by one tense form
in the source language may be described by a dif-
ferent tense form in the target language; (2) under-
standing the order of events in a text; (3) textual en-
tailment, when the optional entailed sentences refer
to the time and/or order of events of the source sen-
tence. Many more examples also exist.
3 The TSD Task
In this section we formally define the TSD task, dis-
cuss its nature vs. WSD, and describe various con-
crete task variants.
Task definition. First, some essential terminol-
ogy. The function of syntax is to combine lexi-
cal items (words, multiword expressions) to express
meanings. This function is achieved through syntac-
tic devices. The most common devices in English
are word order, morphology, and the usage of auxil-
iary words. An Abstract Syntactic Form (ASF) is a
particular set of devices that can be used to express a
set of meanings. A Concrete Syntactic Form (CSF)
is a concrete set of words generated by an ASF for
expressing a certain meaning in an utterance1. A
CSF is ambiguous if its generating ASF has more
than one meaning, which is the usual case. In this
case we also say that the ASF is ambiguous.
Here are a few examples. The ?present progres-
sive? ASF has the form ?am/is/are V-ing?2, which
employs all three main devices. It is ambiguous,
1In some linguistic theories, the central notion is the con-
struction, which combines an ASF (referred to as the form of
the construction) with a single meaning (Goldberg, 1995).
2Note that strictly speaking, these are three different ASFs.
We refer to this ASF family by a single name because they have
the same set of meanings and because it is standard to treat them
as a single ASF.
326
as shown in Section 1. The ?present simple? ASF
has the form ?V(+s)?3, and is ambiguous as well: in
the sentence ?My Brother arrives this evening?, the
CSF ?arrives? conveys the meaning of ?a future event
arranged for a definite time?, while in the sentence
?The sun rises in the East? the meaning is that of a
repeated event.
TSD vs. WSD. The TSD task is to disambiguate
the semantic sense of a tense syntactic form. TSD
is clearly different from WSD. This is obvious when
the CSF comprises two words that are not a multi-
word expression, and is usually also the case when it
comprises a single word. Consider the ?My Brother
arrives this evening? example above. While the verb
?arrive? has two main senses: ?reach a place?, and
?begin?, as in ?Summer has arrived?, in that example
we focused on the disambiguation of the tense sense
of the ?arrives? construction.
Concrete task variants. Unlike with words, the
presence of a particular CSF in a sentence is not
trivially recognizable. Consequently, there are three
versions of the TSD task: (1) we are given the sen-
tence, a marked subset of its words comprising a
CSF, and the ASF that has generated these words;
(2) we are given the sentence and a marked subset
of its words comprising a CSF, without knowing the
generating ASF; (3) we are given only the sentence
and we need to find the contained CSFs and their
ASFs. In all cases, we need to disambiguate the
sense of the ASFs. We feel that the natural granu-
larity of the task is captured by version (2). How-
ever, since the ASF can usually be identified using
relatively simple features, we also report results for
version (1). The main difficulty in all versions is
identifying the appropriate sense, as is the case with
WSD.
4 The Syntactic Sense Dictionary
A prerequisite to any concrete experimentation with
the TSD task is a syntactic sense dictionary. Based
on a thorough examination of three major English
grammar projects, we compiled a set of 18 com-
mon English tense ASFs and their possible senses.
The projects are (1) the Cambridge University Press
3Again, these are two ASFs, one adding an ?s? and one using
the verb as is.
English Grammar In Use series, comprising three
books (essential, intermediate and advanced) (Mur-
phy, 2007; Murphy, 1994; Hewings, 2005); (2)
the English grammar texts resulting from the sem-
inal corpus-based Cobuild project (elementary, ad-
vanced) (Willis and Wright, 2003; Willis, 2004); (3)
the Longman Grammar of Spoken and Written En-
glish (Biber et al, 1999).
As in any sense dictionary, in many cases it is hard
to draw the line between senses. In order to be able
to explore the computational limits of the task, we
have adopted a policy of fine sense granularity. For
example, senses 1 and 3 of the ?present simple? ASF
in Table 1 can be argued to be quite similar to each
other, having a very fine semantic distinction. A spe-
cific application may choose to collapse some senses
into one.
We used the conventional ASF names, which
should not be confused with their meanings (e.g., the
?present simple? ASF can be used to refer to future,
not present, events, as in Table 1, sense 4).
The ASF set thus obtained is: real conditionals,
hypothetical conditionals, wishes, reported speech,
present simple, present progressive, present perfect,
present perfect progressive, past simple, past pro-
gressive, past perfect, past perfect progressive, ?be
+ going + to + infinitive?, future progressive, future
perfect, future perfect progressive, ?would? tense
forms, and ?be + to + infinitive?. Note that the first
four ASFs are not direct tense forms; we include
them because they involve tensed sub-sentences
whose disambiguation is necessary for disambigua-
tion of the whole ASF. The total number of possible
senses for these 18 ASFs is 103.
Table 1 shows the complete senses set for the
?present simple? and ?be + to + infinitive? ASFs, plus
an example sentence for each sense. Space limita-
tions prevent us from listing all form senses here;
we will make the listing available online.
5 Corpus Creation and Annotation
We selected 3000 sentences from the British Na-
tional Corpus (BNC) (Burnard, 2000), containing
4702 CSFs (1.56 per sentence). These sentences
with their CSFs were sense annotated. To select
the 3000 sentences, we randomly sampled sentences
from the various written and spoken sections of the
327
Present Simple
1 Things that are always true
It gets cold in the winter.
2 Regular and repeated actions and habits
My parents often eat meat.
3 General facts
Mr. Brown is a teacher.
4 A future event arranged for a definite time
The next train arrives at 11:30.
5 Plans, expectations and hopes
We hope to see you soon.
6 Ordering someone to do something
Take your hands out of your pockets!
7 Something happening now, with verbs that are
not used in the present progressive in this sense
I do not deny the allegation.
8 Events happening now (informal;
common in books, scripts, radio etc.)
She goes up to this man and looks into his eyes.
9 Past actions
I was sitting in the park reading a newspaper
when all of a sudden this dog jumps at me.
10 Newspaper headlines, for recent events
Quake hits central Iran.
11 When describing the content of a book
Thompson gives an exhaustive list in chapter six.
?be + to + infinitive?
1 Events that are likely to happen in the near future
Police officers are to visit every home in the area.
2 Official arrangements, formal instructions & or-
ders
You are not to leave without my permission.
3 In an if-clause to say that something must
happen before something else can happen
If the human race is to survive, we must look at
environmental problems now.
Table 1: The full set of senses of the ?present simple?
and ?be + to + infinitive? abstract syntactic forms (ASFs),
with an example for each.
corpus, giving each section an equal weight. To
guarantee ample representation of ASFs, we man-
ually defined auxiliary words typical of each ASF
(e.g., ?does?, ?been? etc), and sampled hundreds of
sentences for each set of these auxiliary words. To
make sure that our definition of auxiliary words does
not skew the sampling process, and to obtain ASFs
that do not have clear auxiliary words, we have also
added 1000 random sentences. The number of CSF
instances obtained for each ASF ranges from 100
(future perfect) to over 850 (present simple). All
senses are represented; the number of senses repre-
sented by at least 15 CSFs is 77 (out of 103, average
number of CSFs per sense is 45.65).
We implemented an interactive application that
displays a sentence and asks an annotator to (1) mark
words that participate in the CSFs contained in the
sentence; (2) specify the ASF(s) of these CSFs; and
(3) select the appropriate ASF sense from the set
of possible senses. Annotators could also indicate
?none of these senses?, which they did for 2.6% (122
out of 4702) of the CSFs.
Annotation was done by two annotators (univer-
sity students). To evaluate inter-annotator agree-
ment, a set of 210 sentences (7% of the corpus),
containing at least 10 examples of each ASF, was
tagged by both annotators. The CSF+ASF identifi-
cation inter-annotator agreement was 98.7%, and the
inter-annotator agreement for the senses was 84.2%.
We will make the annotated corpus and annotation
guidelines available online.
6 Learning Algorithm
In this section we describe our learning model for
the TSD task. First, note that the syntactic sense is
not easy to deduce from readily computable anno-
tations such as the sentence?s POS tagging, depen-
dency structure, or parse tree (see Section 8). Hence,
a learning algorithm is definitely needed.
As common in supervised learning, we encode the
CSFs into feature vectors and then apply a learning
algorithm to induce a classifier. We first discuss the
feature set and then the algorithm.
Features. We utilize three sets of features: basic
features, lexical features, and a set of features based
on part-of-speech (POS) tags (Table 2). The ?aux-
iliary words? referred to in the table are the manu-
ally specified words for each ASF that have assisted
us in sampling the corpus (see Section 5). ?Content
words? are the non-auxiliary words appearing in the
CSF4. Content words are usually verbs, since we fo-
cus here on tense-related ASFs. The position and
distance of a form are based on its leftmost word
(auxiliary or content).
The personal pronouns used in the position fea-
tures are: I, you, he, she, it, they, and we. For
4Usually, there is a single content word. However, there may
be more than one, e.g. for phrasal verbs.
328
simplicity, we considered every word starting with
a capital letter that is not the first word in the sen-
tence to be a name.
Each ?Conditional? CSF contains two tense CSFs.
The one that is not the CSF currently encoded by the
features is referred to as its ?mate?.
For the time lexical features we used 16 words
(e.g., recently, often, now). For the reported speech
lexical features we used 14 words (e.g., said, replied,
wrote5). The words were obtained from the gram-
mar texts and our corpus development set.
The POS tagset used by the POS-based features is
that of the WSJ PennTreebank (see Section 7). The
possible verb tags in this tagset are: VB for the base
form, VBD for past tense, VBN for past participle,
VBG for a present participle or gerund (-ing), VBP
for present tense that is not 3rd person singular, and
VBZ for present simple 3rd person singular.
Conjunctions and prepositions are addressed
through the POS tags CC and IN. Using the PRP
tag to detect pronouns or lexical lists for conjunc-
tions and prepositions yielded no significant change
in the results.
In Section 7 we explore the impact each of the
feature sets has on the performance of the algorithm.
Our results indicate that the basic features have the
strongest impact, the POS-based features enhance
the performance in specific cases and the lexical fea-
tures only marginally affect the final results.
Algorithm. Denote by xi the feature vector of a
CSF instance i, by Ci the set of possible labels for
xi, and by ci ? Ci the correct label. The training
set is {(xj , Cj , cj)}nj=1. Let (xn+1, Cn+1) be a test
CSF. As noted in Section 3, there are two versions
of the task, one in which Ci includes the totality of
sense labels, and one in which it includes only the la-
bels associated with a particular ASF. In both cases,
the task is to select which of the labels in Cn+1 is its
correct label cn+1.
Owing to the task structure, it is preferable to
use an algorithm that allows us to restrict the pos-
sible labels of each CSF. For both task versions, this
would help in computing better probabilities during
the training stage, since we know the ASF type of
training CSFs. For the task version in which the ASF
5These are all in a past form due to the semantics of the
reported speech form.
Basic Features
Form words. Auxiliary and content words of the CSF.
Form type. The type, if it is known during test time.
Other forms. The auxiliary and content words (and
type, if known) of the other CSFs present in the sen-
tence.
Position. The position of the CSF in the sentence, its
distance from the end of the sentence, whether it is in
the first (last) three words in the sentence, its distance
from the closest personal pronoun or name.
Wish. Is there a CSF of type ?wish? before the en-
coded form, the number of CSFs between that ?wish?
form and the encoded CSF (if there are several such
?wish? forms, we take the closest one to the encoded
form).
Conditional. Does the word ?if? appear before the en-
coded form, is the ?if? the first word in the sentence,
the number of CSFs between the ?if? and the encoded
form, the auxiliary and content words (and type, if
known) of the mate form, is there a comma between
the encoded form and its mate form, does the word
?then? appear between the encoded form and its mate
form.
Punctuation. The type of end of sentence marker, dis-
tance of the encoded form from the closest predeces-
sor (successor) comma.
Lexical Features
Time. Time words appearing in the sentence, if any.
Reported speech. Reported speech words appearing
in the sentence, if any.
Be. Does the encoded form contain the verb ?be?.
Features Based on POS Tags
Form.The POS of the verb in the encoded form.
Other forms. The POS of the verb in the other CSFs
in the sentence.
POS tags. The POS tags of the two words to the left
(right) of the encoded form.
Conjunction POS. Is there a Conjunction (CC) be-
tween the encoded form and its closest predecessor
(successor) form, the distance from that conjunction.
Preposition POS. Is there a Preposition (IN) between
the encoded form and its closest predecessor (succes-
sor) form, the distance from that preposition.
Table 2: Basic features (top), lexical features (middle)
and POS tags-based features (bottom) used by the TSD
classifier.
type is known at test time, this would also help dur-
ing the test stage.
For the version in which ASF type is known at test
time, we experimented in two scenarios. In the first,
329
we take the ASF type at test time from the manual
annotation and provide it to the algorithm. In the
second, instead of the manual annotation, we imple-
mented a simple rule-based classifier for selecting
ASF types. The classifier decides what is the type of
an ASF according to the POS tag of its verb and to
its auxiliary words (given in the annotation). For ex-
ample, if we see the auxiliary phrase ?had been? and
the verb POS is not VBG, then the ASF is ?past per-
fect simple?. This classifier?s accuracy on our devel-
opment (test) data is 94.1 (91.6)%. In this scenario,
when given a test CSF, Xn+1, its set of possible la-
bels Cn+1 is defined by the classifier output. In the
features in which ASF type is used (see table 2), it is
taken from the classifier output in this case.
The sequential model algorithm presented by
Even-Zohar and Roth (2001) directly supports this
label restriction requirement 6. We use the SNOW
learning architecture for multi-class classification
(Roth, 1998), which contains an implementation of
that algorithm. The SNOW system allows us not
to define restrictions if so desired. It also lets us
choose the learning algorithm used when it builds
its classifier network. The algorithm can be Percep-
tron (MacKay, 2002), Winnow (Littlestone, 1988)
or Naive Bayes (MacKay, 2002)7. In Section 7 we
analyze the effect that these decisions have on our
results.
Classifier Selection. Investigating the best config-
uration of the SNOW system with development data,
we found that Naive Bayes gave the best or close
to best result in all experimental conditions. We
therefore report our results when this algorithm is
used. Naive Bayes is particularly useful when rela-
tively small amounts of training CSF instances are
available (Zhang, 2004), and achieves good results
when compared to other classifiers for the WSD task
(Mooney, 1996), which might explain our results.
Fine tuning of Winnow parameters also leads to high
performance (sometimes the best), but most other
parameter configurations lead to disappointing re-
6Note that the name of the learning algorithm is derived
from the fact that it utilizes classifiers to sequentially restrict
the number of competing classes while maintaining with high
probability the presence of the true outcome. The classification
task it performs is not sequential in nature.
7Or a combination of these algorithms, which we did not
explore in this paper.
sults. For the Perceptron, most parameter config-
urations lead to good results (much better than the
baseline), but these were a few percent worse than
the best Winnow or Naive Bayes results.
7 Experimental Results
Experimental setup. We divided the 3000 anno-
tated sentences (containing 4702 CSFs) to three
datasets: training data (2100 sentences, 3183
forms), development data (300 sentences, 498
forms) and test data (600 sentences, 1021 forms).
We used the development data to design the features
for our learning model and to tune the parameters
of the SNOW sequential model. In addition we used
this data to design the rules of the ASF type classifier
(which is not statistical and does not have a training
phase).
For the POS features, we induced POS tags using
the MXPOST POS tagger (Ratnaparkhi, 1996). The
tagger was trained on sections 2-21 of the WSJ Pen-
nTreebank (Marcus et al, 1993) annotated with gold
standard POS tags. We used a publicly available im-
plementation of the sequential SNOW model8.
We experimented in three conditions. In the first
(TypeUnknown), the ASF type is not known at test
time. In the last two, it is known at test time.
These two conditions differ in whether the type is
taken from the gold standard annotation of the test
sentences (TypeKnown), or from the output of the
simple rule-based classifier (TypeClassifier, see Sec-
tion 6). For both conditions, the results reported be-
low are when both ASF type features and possible
labels sets are provided during training by the man-
ual annotation. This is true also for the training of
the MFS baseline (see below)9.
We report an algorithm?s quality using accuracy,
that is, the number of test CSFs that were correctly
resolved by the algorithm divided by the total num-
ber of test CSFs.
Baseline. We compared the performance of our al-
gorithm to the ?most frequent sense? (MFS) base-
8http://l2r.cs.uiuc.edu/?cogcomp/asoftware.php?
skey=SNOW
9For the TypeClassifier condition, we also experimented us-
ing an ML technique that sometimes reduces noise, where train-
ing is done using the classifier types. We obtained very similar
results to those reported.
330
TypeUnknown TypeClassifier TypeKnown
Our algorithm 49.7% 58.8% 62%
MFS baseline 13.5% 42.9% 46.7%
Table 3: Performance of our algorithm and of the MFS
baseline where at test time ASF type is known (right),
unknown (left) or given by a simple rule-based classifier
(middle). Our algorithm is superior in all three condi-
tions.
Constrained Model Unconstrained Classifier
All Base+Lexical All Base+Lexical
features features features features
Type 57.9% 57.7% 53% 50.1%
features
No type 57.2% 55.4% 48% 42.6%
features
Table 4: Impact of POS features. When the constrained
model is used (left section), POS features have no effect
on the results when ASF type information is encoded.
When an unconstrained classifier is used, POS features
affect the results both when ASF type features are used
and when they are not (see discussion in the text).
line. This baseline is common in semantic disam-
biguation tasks and is known to be quite strong. In
the condition where the ASF type is not known at
test time, MFS gives each form in the test set the
sense that was the overall most frequent in the train-
ing set. That is, in this case the baseline gives all
test set CSFs the same sense. When the ASF type
is known at test time, MFS gives each test CSF the
most frequent sense of that ASF type in the training
set. That is, in this case all CSFs having the same
ASF type get the same sense, and forms of different
types are guaranteed to get different senses.
Recall that the condition where ASF type is
known at test time is further divided to two condi-
tions. In the TypeKnown condition, MFS selects the
most frequent sense of the manually created ASF
type, while in the TypeClassifier condition it selects
the most frequent sense of the type decided by the
rule-based classifier. In this condition, if the classi-
fier makes a mistake, MFS will necessarily make a
mistake as well.
Note that a random baseline which selects a sense
for every test CSF from a uniform distribution over
the possible senses (103 in our case) would score
very poorly.
Results. Table 3 shows our results. Results are
shown where ASF type is not known at test time
(left), when it is decided at test time by a rule-based
classifier (middle) and when it is known at test time
(right). Our algorithm outperforms the MFS base-
line in all three conditions. As expected, both our al-
gorithm and the MFS baseline perform better when
ASF type information is available at test time (Type-
Classifier and TypeKnown conditions), and improve
as this data becomes more accurate (the TypeKnown
condition)10.
Analyzing the per-type performance of our algo-
rithm reveals that it outperforms the MFS baseline
for each and every ASF type. For example, in the
TypeKnown condition, the accuracy gain of our al-
gorithm over the baseline11 varies from 4% for the
?present perfect? to 30.6% and 29.1% for the ?past
perfect? and ?present simple? ASFs.
Below we analyze the roles of the different com-
ponents of our learning algorithm in performing the
TSD task. Since this is the first exploration of the
task, it is important to understand what properties
are essential for achieving good performance. The
analysis is done by experimenting with development
data, and focuses on the TypeKnown and TypeUn-
known conditions. Patterns for the TypeClassifier
condition are very similar to the patterns for the
TypeKnown condition.
The Possible Senses Constraint. We use the
learning model of Even-Zohar and Roth (2001),
which allows us to constrain the possible senses
an input vector can get to the senses of its ASF
type. We ran our model without this constraint dur-
ing both training and test time (recall that for the
above results, this constraint was always active dur-
ing training). In this case, the only difference be-
tween the TypeKnown and the TypeUnknown con-
ditions is whether ASF type features are encoded at
test time. In the TypeKnown condition, the accu-
racy of the algorithm drops from 57.9% (when us-
ing training and test time constraints and ASF type
features) to 53% (when using only ASF type fea-
tures but no constraints). In the TypeUnknown con-
dition, accuracy drops from 57.24% (when using
training time constraints) to 48.03% (when neither
constraints nor ASF type features are used). Note
10Recall that the performance of the rule-based ASF type
classifier on test data is not 100% but 91.6% (Section 6).
11accuracy(algorithm)? accuracy(MFS).
331
that the difference between the constrained model
and the unconstrained model is quite large.
The MFS baseline achieves on development data
42.9% and 13.2% in the TypeKnown and TypeUn-
known conditions respectively12. Thus, the algo-
rithm outperforms the baseline both when the con-
strained model is used and when an unconstrained
multi-class classifier is used.
Note also that when constraints on the possible
labels are available at training time, test time con-
straints and ASF type features (whose inclusion is
the difference between the TypeKnown and Type-
Unknown) have a minor effect on the results (57.9%
for TypeKnown compared to 57.24% for TypeUn-
known). However, when training time constraints
on the possible labels are not available at training
time, ASF type features alone do have a significant
effect on the result (53% for TypeKnown compared
to 48.03% for TypeUnknown).
POS Features. We next explore the impact of the
POS features on the results. These features encode
the inflection of the verbs in the CSF, as well as the
POS tags of the two words to the left and right of the
CSF.
Verb forms provide some partial information cor-
responding to the ASF type features encoded at the
TypeKnown scenario. Table 4 shows that when both
label constraints and ASF type features are used,
POS features have almost no impact on the final re-
sults. When the constrained model is used but ASF
type features are not encoded, POS features have an
effect on the results. We conclude that when using
the constrained model, POS features are important
mainly for ASF type information. When the uncon-
strained classifier is used, POS features have an ef-
fect on performance whether ASF type features are
encoded or not. In the last case the impact of POS
features is larger. In other words, when using an un-
constrained classifier, POS features give more than
ASF type information to to the model.
Lexical Features. To explore the impact of the
lexical features, we removed the following features:
time words, reported speech words and ?be? indi-
cation features. We saw no impact on model per-
formance when using the constrained model, and a
12Note that these numbers are for development data only.
0.5% decrease when using the unconstrained classi-
fier. That is, our model does not require these lexical
features, which is somewhat counter-intuitive. Lex-
ical statistics may turn out to be helpful when using
a much larger training set.
Conditional and Wish Features. The condition-
als and ?wish? features have a more substantial im-
pact on the results, as they have a role in defining the
overall syntactic structure of the sentence. Discard-
ing these features leads to 4% and 1.4% degradation
in model accuracy when using the constrained and
unconstrained models respectively.
8 Relevant Previous Work
As far as we know, this is the first paper to address
the TSD task. In this section we describe related
research directions and compare them with TSD.
A relevant task to TSD is WSD (Section 1 and
Section 3). Many algorithmic approaches and tech-
niques have been applied to supervised WSD (for
reviews see (Agirre and Edmonds, 2006; Mihalcea
and Pedersen, 2005; Navigli, 2009)). Among these
are various classifiers, ensemble methods combin-
ing several supervised classifiers, bootstrapping and
semi-supervised learning methods, using the Web
as a corpus and knowledge-based methods relying
mainly on machine readable dictionaries. Specif-
ically related to this paper are works that exploit
syntax (Martinez et al, 2002; Tanaka et al, 2007)
and ensemble methods (e.g. (Brody et al, 2006))
to WSD. The references above also describe some
unsupervised word sense induction algorithms.
Our TSD algorithm uses the SNOW algorithm,
which is a sparse network of classifiers (Section 6).
Thus, it most resembles the ensemble approach to
WSD. That approach has achieved very good results
in several WSD shared tasks (Pedersen, 2000; Flo-
rian and Yarowsky, 2002).
Since temporal reasoning is a direct applica-
tion of TSD, research on this direction is relevant.
Such research goes back to (Passonneau, 1988),
which introduced the PUNDIT temporal reasoning
system. For each tensed clause, PUNDIT first de-
cides whether it refers to an actual time (as in ?We
flew TWA to Boston?) or not (as in ?Tourists flew
TWA to Boston?, or ?John always flew his own plane
to Boston?). The temporal structure of actual time
332
clauses is then further analyzed. PUNDIT?s classi-
fication is much simpler than in the TSD task, ad-
dressing only actual vs. non-actual time. PUNDIT?s
algorithmic approach is that of a Prolog rule based
system, compared to our statistical learning corpus-
based approach. We are not aware of further re-
search that followed their sense disambiguation di-
rection.
Current temporal reasoning research focuses on
temporal ordering of events (e.g., (Lapata, 2006;
Chambers and Jurafsky, 2008)), for which an ac-
cepted atomic task is the identification of the tem-
poral relation between two expressions (see e.g., the
TempEval task in SemEval ?07 (Verhagen et al,
2007)). This direction is very different from TSD,
which deals with the semantics of individual con-
crete tense syntactic forms. In this sense, TSD is an
even more atomic task for temporal reasoning.
A potential application of TSD is machine trans-
lation where it can assist in translating tense and as-
pect. Indeed several papers have explored tense and
aspect in the MT context. Dorr (1992) explored the
integration of tense and aspect information with lex-
ical semantics for machine translation. Schiehlen
(2000) analyzed the effect tense understanding has
on MT. Ye and Zhang (2005) explored tense tagging
in a cross-lingual context. Ye et al, (2006) extracted
features for tense translation between Chinese and
English. Murata et al, (2007) compared the perfor-
mance of several MT systems in translating tense
and aspect and found that various ML techniques
perform better on the task.
Another related field is ?deep? parsing, where a
sentence is annotated with a structure containing in-
formation that might be relevant for semantic inter-
pretation (e.g. (Hajic, 1998; Baldwin et al, 2007)).
TSD senses, however, are not explicitly represented
in these grammatical structures, and we are not
aware of any work that utilized them to do some-
thing close to TSD. This is a good subject for future
research.
9 Conclusion and Future Work
In this paper we introduced the Tense Sense Disam-
biguation (TSD) task, defined as selecting the cor-
rect sense of a concrete tense syntactic form in a sen-
tence among the senses of abstract syntactic forms
in a syntactic sense dictionary. Unlike in other se-
mantic disambiguation tasks, the sense to be disam-
biguated is not lexical but of a syntactic structure.
We prepared a syntactic sense dictionary, annotated
a corpus by it, and developed a supervised classifier
for sense disambiguation that outperformed a strong
baseline.
An obvious direction for future work is to expand
the annotated corpus and improve the algorithm by
experimenting with additional features. For exam-
ple, we saw that seeing the full paragraph containing
a sentence helps human annotators decide on the ap-
propriate sense which implies that using larger con-
texts may improve the algorithm.
TSD can be a very useful operation for various
high-level applications, for example textual infer-
ence, question answering, and information retrieval,
in the same way that textual entailment (Dagan et
al., 2006) was designed to be. In fact, TSD can assist
textual entailment as well, since the sense of a tense
form may provide substantial information about the
relations entailed from the sentence. Using TSD
in such applications is a major direction for future
work.
References
Eneko Agirre and Philip Edmonds (Eds). 2006. Word
Sense Disambiguation: Algorithms and Applications.
Springer Verlag.
Timothy Baldwin, Mark Dras, Julia Hockenmaier, Tracy
Holloway King, and Gertjan van Noord. 2007. The
Impact of Deep Linguistic Processing on Parsing
Technology. IWPT ?07.
Douglas Biber, Stig Johansson, Geoffrey Leech, Susan
Conard, Edward Finegan. 1999. Longman Grammar
of Spoken and Written English. Longman.
Samuel Brody, Roberto Navigli and Mirella Lapata.
2006. Ensemble Methods for Unsupervised WSD.
ACL-COLING ?06.
Lou Burnard. 2000. The British National Corpus User
Reference Guide. Technical Report, Oxford Univer-
sity.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
Combining Implicit Constraints Improves Temporal
Ordering. EMNLP ?08.
Michelle Cutrer. 1994. Time and Tense in Narratives and
in Everyday Language. PhD dissertation, University
of California at San Diego.
333
Ido Dagan, Oren Glickman and Bernardo Magnini. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge. Lecture Notes in Computer Science 2006,
3944:177-190.
John Dinsmore. 1991. Partitioned representations. Dor-
drecht, Netherlands: Kluwer.
Bonnie Dorr. 1992. A Two-Level Knowledge Repre-
sentation for Machine Translation: Lexical Semantics
and Tense/Aspect. In James Pustejovsky and Sabine
Bergler, editors, Lexical Semantics and Knowledge
Representation.
Yair Even-Zohar and Dan Roth. 2001. A Sequential
Model for Multi-Class Classification. EMNLP ?01.
Gilles Fauconnier. 2007. Mental Spaces. in Dirk Geer-
aerts and Hubert Cuyckens, editors, The Oxford Hand-
book of Cognitive Linguistics.
Radu Florian and David Yarowsky. 2002. Modeling
Consensus: Classifier Combination for Word Sense
Disambiguation. EMNLP ?02.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar Approach to Argument Structure. Uni-
versity of Chicago Press.
Jan Hajic. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. Issues of
Valency and Meaning, 106?132.
Martin Hewings. 2005. Advanced Grammar in Use, Sec-
ond Edition. Cambridge University University.
Mirella Lapata and Alex Lascarides. 2006. Learning
Sentence-internal Temporal Relations. Journal of Ar-
tificial Intelligence Research, 27:85?117.
Nick Littlestone. 1988. Learning Quickly When Irrele-
vant Attributes Abound: A New Linear-threshold Al-
gorithm. Machine Learning, 285?318.
David MacKay. 2002. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mitchell P. Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Martinez, Eneko Agirre, Lluis Marquez. 2002.
Syntactic Features for High Precision Word Sense Dis-
ambiguation. COLING ?02.
Rada Mihalcea and Ted Pedersen. 2005. Advances in
Word Sense Disambiguation. Tutorial in ACL ?05.
Raymond J. Mooney. 1996. Comparative Experiments
on Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. EMNLP ?96.
Masaki Murata, Qing Ma, Kiyotaka Uchimoto, Toshiyuki
Kanamaru and Hitoshi Isahara. 2007. Japanese-to-
English translations of Tense, Aspect, and Modality
Using Machine-Learning Methods and Comparison
with Cachine-Translation Systems on Market. LREC
?07.
Raymond Murphy. 1994. English Grammar In Use, Sec-
ond Edition. Cambridge University Press.
Raymond Murphy. 2007. Essential Grammar In Use,
Third Edition. Cambridge University Press.
Roberto Navigli. 2009. Word Sense Disambiguation: a
Survey. ACM Computing Surveys, 41(2) 1?69.
Rebecca J. Passonneau. 1988. A Computational Model
of Semantics of Tenses and Aspect. Computational
Linguistics, 14(2):44?60.
Ted Pedersen. 2000. A Simple Approach to Building En-
sembles of Naive Bayesian Classifiers for Word Sense
Disambiguation. NAACL ?00.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?06.
Dan Roth. 1998. Learning to Resolve Natural Language
Ambiguities: A Unified Approach. AAAI ?98.
Michael Schiehlen. 2000. Granularity Effects in Tense
Translation. COLING ?00.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval Temporal
Relation Identification. ACL ?07.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae
Fujita and Chikara Hashimoto. 2007. Word Sense
Disambiguation Incorporating Lexical and Structural
Semantic Information. EMNLP-CoNLL ?07.
Dave Willis and Jon Wright. 2003. Collins Cobuild El-
ementary English Grammar, Second Edition. Harper-
Collins Publishers.
Dave Willis. 2004. Collins Cobuild Intermediate English
Grammar, Second Edition. HarperCollins Publishers.
Yang Ye, Victoria Li Fossum and Steven Abney. 2006.
Latent Features in Automatic Tense Translation be-
tween Chinese and English. SIGHAN ?06.
Yang Ye and Zhu Zhang. 2005. Tense Tagging for Verbs
in Cross-Lingual Context: A Case Study. IJCNLP ?05.
Harry Zhang. 2004. The Optimality of Naive Bayes.
FLAIRS ?04.
334
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 684?693,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Improved Fully Unsupervised Parsing with Zoomed Learning
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
We introduce a novel training algorithm
for unsupervised grammar induction, called
Zoomed Learning. Given a training set T and
a test set S, the goal of our algorithm is to
identify subset pairs Ti, Si of T and S such
that when the unsupervised parser is trained
on a training subset Ti its results on its paired
test subset Si are better than when it is trained
on the entire training set T . A successful ap-
plication of zoomed learning improves overall
performance on the full test set S.
We study our algorithm?s effect on the leading
algorithm for the task of fully unsupervised
parsing (Seginer, 2007) in three different En-
glish domains, WSJ, BROWN and GENIA, and
show that it improves the parser F-score by up
to 4.47%.
1 Introduction
Grammar induction is the task of learning grammati-
cal structure from plain text without human supervi-
sion. The task is of great importance both for the
understanding of human language acquisition and
since its output can be used by NLP applications,
avoiding the costly and error prone creation of man-
ually annotated corpora. Many recent works have
addressed the task (e.g. (Klein and Manning, 2004;
Seginer, 2007; Cohen and Smith, 2009; Headden et
al., 2009)) and its importance has increased due to
the recent availability of huge corpora.
A basic challenge to this research direction is
how to utilize training data in the best possible
way. Klein and Manning (2004) report results for
their dependency model with valence (DMV) for
unsupervised dependency parsing when it is trained
and tested on the same corpus (both when sentence
length restriction is imposed, such as for WSJ10,
and when it is not, such as for the entire WSJ). To-
day?s best unsupervised dependency parsers, which
are rooted in this model, train on short sentences
only: both Headen et al, (2009) and Cohen and
Smith (2009) train on WSJ10 even when the test set
includes longer sentences.
Recently, Spitkovsky et al, (2010) demonstrated
that training the DMV model on sentences of up to
15 words length yields better results on the entire
section 23 of WSJ (with no sentence length restric-
tion) than training with the entire WSJ corpus.
In contrast to these dependency models, the
Seginer constituency parser achieves its best perfor-
mance when trained on the entire WSJ corpus ei-
ther if sentence length restriction is imposed on the
test corpus or not. The sentence length restriction
training protocol of (Spitkovsky et al, 2010), harms
this parser. When the parser is trained with the
entire WSJ corpus its F-score performance on the
WSJ10, WSJ20 and the entire WSJ corpora are 76,
64.8 and 56.7 respectively. When training is done
with WSJ10 (WSJ20) performance degrades to 60
(72.2), 37.4 (61.9) and 29.7 (48) respectively.
In this paper we introduce the Zoomed Learn-
ing (ZL) technique for unsupervised parser training:
given a training set T and a test set S, it identifies
subset pairs Ti, Si of T and S such that when the
unsupervised parser is trained on a training subset
Ti its results on its paired test subset Si are better
than when it is trained on the entire training set T . A
684
successful application of zoomed learning improves
performance on the full test set S.
We describe ZL algorithms of increasing sophis-
tication. In the simplest algorithm the subsets are
randomly selected while in the more sophisticated
versions subset selection is done using a fully unsu-
pervised measure of constituency parse tree quality.
We apply ZL to the Seginer parser, the best al-
gorithm for fully unsupervised constituency parsing.
The input is a plain text corpus without any annota-
tion, not even POS tagging1, and the output is an
unlabeled bracketing for each sentence.
We experiment in three different English do-
mains: WSJ (economic newspaper), GENIA (biolog-
ical articles) and BROWN (heterogeneous domains),
and show that ZL improves the parser F-score by as
much as 4.47%.
2 Related Work
Unsupervised parsing has attracted researchers for
over a quarter of a century (see (Clark, 2001; Klein,
2005) for reviews). In recent years efforts have been
made to evaluate the algorithms on manually anno-
tated corpora such as the WSJ PennTreebank. Re-
cent works on unlabeled bracketing or dependencies
induction include (Klein and Manning, 2002; Klein
and Manning, 2004; Dennis, 2005; Bod, 2006a;
Bod, 2006b; Bod, 2007; Smith and Eisner, 2006;
Seginer, 2007; Cohen et al, 2008; Cohen and Smith,
2009; Headden et al, 2009). Most of the works
above use POS tag sequences, created either manu-
ally or by a supervised algorithm, as input. The only
exception is Seginer?s parser, which induces brack-
eting from plain text.
Our confidence-based ZL algorithms use the
PUPA unsupervised parsing quality score (Reichart
and Rappoport, 2009b). As far as we know, PUPA is
the only unsupervised quality assessment algorithm
for syntactic parsers that has been proposed. Com-
bining PUPA with Seginer?s parser thus preserves the
fully unsupervised nature of the task.
Quality assessment of a learning algorithm?s out-
put has been addressed for supervised algorithms
1For clarity of exposition, we still refer to this corpus as our
training corpus. In the algorithms presented in this paper, the
test set is included in the training set which is a common prac-
tice in unsupervised parsing.
(see (Caruana and Niculescu-Mizil, 2006) for a sur-
vey) and specifically for supervised syntactic pars-
ing (Yates et al, 2006; Reichart and Rappoport,
2007; Ravi et al, 2008; Kawahara and Uchimoto,
2008). All these algorithms are based on manually
annotated data and thus do not preserve the unsuper-
vised nature of the task addressed in this paper.
We experiment with the Seginer parser for two
reasons. First, this is the best algorithm for the task
of fully unsupervised parsing which motivates us to
improve its performance. Second, this is the only
publicly available unsupervised parser that induces
constituency trees. The PUPA score we use in our
confidence-based algorithms is applicable for con-
stituency trees only. When additional constituency
parsers will be made available, we will test ZL with
them as well. Interestingly, the results reported for
other constituency models (the CCM model (Klein
and Manning, 2002) and the U-DOP model (Bod,
2006a; Bod, 2006b)) are reported when the parser is
trained on its test corpus even if the sentences is that
corpus are of bounded length (e.g. WSJ10). This
raises the question if using more training data (e.g.
the entire WSJ) wisely can enhance these models.
Recently, Spitkovsky et al, (2010) proposed three
approaches for improvement of unsupervised gram-
mar induction by considering the complexity of the
training data. The approaches have been applied
to the DMV unsupervised dependency parser (Klein
and Manning, 2004) and improved its performance.
One of these approaches is to train the model with
sentences whose length is up to 15 words. As noted
above, such a training protocol fails to improve the
performance of the Seginer parser.
The other approaches in that paper, bootstrapping
via iterated learning of increasingly longer sentences
and a combination of the bootstrapping and the short
sentences approaches, are not directly applicable to
the Seginer parser since its training method cannot
be trivially bootstrapped with parses created in for-
mer steps (Seginer, 2007).
Related machine learning methods. ZL is re-
lated to ensemble methods. Both ZL and such meth-
ods produce multiple learners, each of them trained
on a different subset of the training data, and decide
which learner to use for a particular test instance.
Bagging (Breiman, 1996) and boosting (Freund and
Schapire, 1996), where the experts utilize the same
685
learning algorithm and differ in the sample of the
training data they use for its training, were applied
to supervised parsing (Henderson and Brill, 2000;
Becker and Osborne, 2005). In Section 3 we discuss
the connection of ZL to boosting.
Owing to the fact that ZL produces different
learners, it is natural to use it in conjunction with
an ensemble method, which is what we do in this
paper with our EZL model (Section 3).
ZL is also related to active learning (AL) (Cohn
and Ladner, 1994). AL also uses training subset se-
lection, with the goal of obtaining a faster learning
curve for an algorithm. AL is done in supervised
settings, usually in order to minimize human anno-
tation costs. AL algorithms providing faster learning
than random subset selection for parsing have been
proposed (Reichart and Rappoport, 2009a; Hwa,
2004). However, we are not aware of AL applica-
tions in which the overall performance on the test
set has been improved. In addition, our application
here is to an unsupervised problem.
Algorithms that utilize unsupervised clustering
for class decomposition in order to improve classi-
fiers? performance (e.g. (Vilalta and Rish, 2003)) are
related to ZL. In such methods, examples that be-
long to the same class are clustered, and the induced
clusters are considered as separate classes. These
methods, however, have been applied only to super-
vised classification in contrast to our work that ad-
dresses unsupervised structured learning. Moreover,
after class decomposition a classifier is trained with
the entire training data while the subsets identified
by a ZL algorithm are parsed by a parser trained only
with the sentences they contain.
3 Zoomed Learning Algorithms
Zoomed Learning proposes that performance on a
particular test instance might improve if training is
done on a proper subset of the training set. The
ZL view is clearly applicable when the training data
is comprised of subsets originating from different
sources having different natures. If the test data is
also similarly composed, performance on any partic-
ular test instance might improve if training is done
on a training subset coming from the same source.
However, even when the training and test data are
from the same source, a ZL algorithm may capture
fine differences between subsets.
The ZL idea is therefore related to the notions of
in-domain and out-of-domain (domain adaptation).
In the former, the training and test data are assumed
to originate from the same domain. In the latter, the
test data comes from a different domain, and there-
fore has different statistics from the training data.
Indeed, the performance of NLP algorithms in do-
main adaptation scenarios is markedly lower than in
in-domain ones (McClosky et al, 2006).
ZL takes this observation to the extreme, assum-
ing that a similar situation might exist even in in-
domain scenarios. After all, a ?domain? is only a
coarse qualification of the nature of a data set. In
NLP, a domain is usually specified as the genre of
the text involved (e.g., ?newspapers?). However,
there are additional axes that might influence the
statistics obtained from training data, e.g., the syn-
tactic nature of sentences.
This section presents our ZL algorithms. We start
with the simplest possible ZL algorithm where the
subsets are randomly selected. We then describe ZL
algorithms based on quality-based parse selection.
We first detail a basic version and then an extended
version consisting of another level of parse selec-
tion. Finally, we briefly discuss the PUPA quality
measure that we use to evaluate the quality of a parse
tree.
In all versions of the algorithm the input consists
of a set T of N training sentences, a set S ? T of
test sentences, and an integer number NH ? N .
Zoomed Learning with Random Selection
(RZL). The simplest ZL algorithm randomly assigns
each of the training sentences to one of n sets (n = 2
in this paper). More explicitly, the set number is
drawn from a uniform distribution on {1, 2, . . . n}.
Each set is then parsed by a parser that is trained
only with the sentences contained in that set.
The intuition behind this algorithm is that differ-
ent sets of sentences are likely to manifest differ-
ent syntactic patterns. Consequently, the best way to
learn the syntactic patterns of any given set of sen-
tences might be to train the parser on the sentences
contained in the set.
While simple, in Section 5 it is shown to improve
the performance of the Seginer parser.
The Basic Quality-Based Algorithm (BZL). The
idea of the basic ZL algorithm is that sentences for
686
which the parser provides low quality parses man-
ifest different syntactic patterns than the sentences
for which the parser provides high quality parses.
The main challenge is therefore to estimate the qual-
ity of the produced parses without supervision.
The algorithm has three stages. In the first, we
create the fully-trained model by training the parser
using all of the N sentences of T . We then parse
these N sentences using the fully-trained model.
In the second, we compute a parse confidence
score for each of the N sentences, based on the N
parses produced in the first stage. We divide the
training sentences to two subsets: a high quality sub-
set H consisting of the top scored NH sentences,
and a lower quality subset L consisting of the other
NL = N ? NH sentences.
As is common practice for this problem (Klein
and Manning, 2004; Seginer, 2007), the test set is
contained in the training set. This methodology is
a valid one because the training set is unannotated.
Our test set is thus naturally divided into two sub-
sets, a high quality subset HT consisting of the test
set sentences contained in H and a lower quality
subset LT consisting of the test set sentences con-
tained in L.
In the third stage, each of the test subsets is parsed
by a model trained only on its corresponding train-
ing subset. This stage is motivated by our assump-
tion that the high and low quality subsets manifest
dissimilar syntactic patterns, and consequently the
statistics of the parser?s parameters suitable for one
subset differ from those suitable for another.
We compute the confidence score in the second
stage using the unsupervised PUPA algorithm (Re-
ichart and Rappoport, 2009b). POS tags for it are
induced using the fully unsupervised algorithm of
Clark (2003). The parser we experiment with is the
incremental parser of Seginer (2007), whose input
consists of raw sentences and does not include any
kind of supervised POS tags (created either manu-
ally or by a supervised algorithm). Consequently,
our algorithm is fully unsupervised. The only pa-
rameter it has is NH but ZL improves parser perfor-
mance for most NH values.
BZL is related to boosting. In boosting after train-
ing one member of the ensemble, examples are re-
weighted such that examples that are classified cor-
rectly are down-weighted. BZL does something sim-
ilar: it uses PUPA to estimate which sentences are
given high quality parse trees, and down-weights ex-
amples with high (low) PUPA score to 0 when train-
ing the L-trained (H-trained) model. However, in
boosting the entire test set is annotated by the same
learning model, while ZL parses each test subset
with a model trained on its corresponding training
subset.
The Extended Quality-Based Algorithm (EZL).
The basic algorithm produces an ensemble of two
parsing experts: the one trained on H and the one
trained on L. It uses the ensemble to parse the test
set by applying the H-trained expert to HT and the
L-trained expert to LT . Naturally, there are other
ways to utilize the ensemble to parse the test set. In
addition, even if parse trees generated by the experts
are better with high probability than those of the
fully trained parser, they are not guaranteed to be so.
The fully trained parser is therefore also a valuable
member in the ensemble. Consequently, we intro-
duce an extended zoomed learning algorithm (EZL).
The extended version is implemented as a final
fourth stage of the previously described basic algo-
rithm. In this stage, the two test subsets are parsed
by the fully trained parsing model, in addition to be-
ing parsed by the zooming parsing models. We now
have two parses for each test sentence s: PZ(s), the
parse created by a parser trained with the sentences
contained in its corresponding training subset, and
PF (s), created by the fully trained parser.
For each of the two parses of each test sentence,
a confidence score is computed by PUPA. As will
be reviewed below, PUPA uses a set of parsed sen-
tences to compute the statistics on which its scores
are based. Therefore, there are two sources for a dif-
ference between the scores of the two parse trees of a
given test sentence: the difference between the trees
themselves, and the difference between the parses of
the other sentences in the set.
The PUPA score for PZ(s) is computed using the
parses created for the sentences contained in the test
subset of s by a parser trained with the correspond-
ing training subset. The PUPA score for PF (s) is
computed using the parses created for the entire test
set by the fully trained parser.
The algorithm now outputs a final parse by select-
ing for each sentence the parse tree having the higher
PUPA score.
687
The PUPA Confidence Score. In the second and
fourth stages of the confidence-based algorithms, an
unsupervised confidence score is computed for each
of the induced parse trees. The confidence score
algorithm we use is the POS-based Unsupervised
Parse Assessment (PUPA) algorithm (Reichart and
Rappoport, 2009b). We provide here a brief descrip-
tion of this algorithm.
The input to PUPA is a set I of parsed sentences,
and its output consists of a confidence score in [0, 1]
assigned to each sentence in I .
The PUPA algorithm collects statistics of the syn-
tactic structures (parse tree constituents) contained
in the set I of parsed sentences. The constituent rep-
resentation is based on the POS tags of the words in
the yield of the constituent and of the words in the
yields of neighboring constituents. We follow Re-
ichart and Rappoport (2009b) and induce the POS
tags using the fully unsupervised POS induction al-
gorithm of Clark (2003).
The algorithm then goes over each individual tree
in the set I and scores it according to the collected
statistics The PUPA algorithm is guided by the idea
that syntactic structures that are frequently created
by the parser are more likely to be correct than struc-
tures the parser produces less frequently. Therefore,
constituents that are more frequent in the set I re-
ceive higher scores after proper regularization is ap-
plied to prevent potential biases. The tree score is a
combination of the scores of its constituents.
Full details of the PUPA algorithm are given in
(Reichart and Rappoport, 2009b). The resulting
score was shown to be strongly correlated with the
extrinsic quality of the parse tree, defined to be its F-
score similarity to the manually created (gold stan-
dard) parse tree of the sentence.
4 Experimental Setup
We experimented with three English corpora: the
WSJ Penn Treebank (Marcus et al, 1993) consist-
ing of economic newspaper texts, the BROWN cor-
pus (Francis and Kucera, 1979) consisting of texts
of various English genres (e.g. fiction, humor, ro-
mance, mystery and adventure) and the GENIA cor-
pus (Kim et al, 2003) consisting of abstracts of sci-
entific articles from the biological domain. All cor-
pora were stripped of all annotation (bracketing and
POS tags).
For all corpora we report the parser perfor-
mance on the entire corpus (WSJ: 49206 sentences,
BROWN: 24243 sentences, GENIA: 4661 sentences).
For WSJ we also provide an analysis of the per-
formance of the parser when applied to sentences
of bounded length. These sub-corpora are WSJ10
(7422 sentences), WSJ20 (25522 sentences) and
WSJ40 (47513 sentences) where WSJY denotes
the subset of WSJ containing sentences of length at
most Y (excluding punctuation).
Seginer?s parser achieves its best reported results
when trained on the full WSJ corpus. Consequently,
for all corpora, we compare the performance of the
parser when trained with the ZL algorithms to its
performance when trained with the full corpus.
The POS tags required as input by the PUPA al-
gorithm are induced by the fully unsupervised POS
induction algorithm of Clark (2003)2. Reichart and
Rappoport (2009b) demonstrated an unsupervised
technique for the estimation of the number of in-
duced POS tags with which the correlation between
PUPA?s score and the parse F-score is maximized.
When exploring an experimental setup identical to
our WSJ setup, they set the number of induced tags
to be 5. We therefore induced 5 POS tags for each
corpus, using all its sentences as input for Clark?s al-
gorithm. Our implementation of the PUPA algorithm
will be made available on line.
For each corpus we performed K experiments
with each of the three ZL algorithms, where K
equals to the number of sentences in the corpus di-
vided by 1000 (rounded upwards). In each experi-
ment the size of the high quality H and lower quality
L training subsets is different. H consists of the NH
top ranked sentences according to PUPA (or NH ran-
domly selected sentences for RZL), with NH chang-
ing from 1000 upwards in steps of 1000. L consists
of the rest of the sentences in the training corpus
(WSJ). The results reported for RZL are averaged
over 10 runs.
We report the parser performance on the test cor-
pus for each training protocol. Following the un-
supervised parsing literature multiple brackets and
brackets covering a single word are not counted, but
the sentence level bracket is. We exclude punctua-
2www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
688
WSJ10, F(Full) = 76 WSJ20, F(Full) = 64.82 WSJ40, F(Full) = 57.54 WSJ, F(Full) = 56.7
NH 25% 50% 75% 25% 50% 75% 25% 50% 75% 25% 50% 75%
EZL 76.38 76.80 76.14 65.75 66.14 65.66 58.32 58.75 58.56 57.47 57.90 57.73
+0.38 +0.80 +0.14 +0.93 +1.30 +0.82 +0.78 +1.21 +1.02 +0.77 +1.20 + 1.13
BZL 75.07 75.78 75.02 65.08 65.74 64.79 58.13 58.70 58.21 57.30 57.88 57.66
-0.93 -0.22 -0.98 +0.26 +0.92 -0.03 +0.59 +1.16 +0.67 +0.60 +1.18 +1.06
RZL 75.41 75.00 75.32 64.43 64.66 65.32 57.27 57.63 58.39 56.44 56.84 57.59
-0.59 -1.00 -0.68 -0.39 -0.16 +0.50 -0.27 +0.09 +0.85 -0.26 +0.14 +0.89
WSJ10 WSJ20 WSJ40 WSJ
|LT | 10% 20% 30% 10% 20% 30% 10% 20% 30% 10% 20% 30%
EZL 1.32 0.95 0.61 2.98 3.13 1.76 2.60 2.80 2.62 2.44 2.40 2.50
BZL 0.37 0.80 0.53 2.38 3.12 1.23 2.34 3.20 3.35 2.28 2.50 3.23
RZL -2.10 -1.88 -1.20 -0.91 -0.50 0.72 0.30 0.35 1.50 0.34 0.50 1.60
Table 1: Performance of the EZL, BZL and RZL algorithms in the WSJ experiments (results for BROWN and GENIA
are shown in Table 2). Results are presented for four test corpora WSJ10, WSJ20, WSJ40 and the entire WSJ. Top
table: Results for various values of NH (the number of sentences in the high quality training subset). Evaluation
is performed for all sentences in the test corpora. For each algorithm, the top line is its F-score performance and
the bottom line is the difference from the F-score of the fully-trained Seginer parser (denoted by F(Full)). The EZL
algorithm is superior. Bottom table: Results for various lower quality test subsets. Presented are the differences from
the F-score of the fully-trained Seginer parser. The test subsets selected by different algorithms for a specific NH
value are not necessarily identical and for the sub-corpora they are not necessarily of identical size. Reported are the
improvements for the LT ?s of smallest size which is over 10%, 20%, and 30% of the test corpus (the top table reports
results for the entire test set, which is why we can report F-scores there). The LT set size is denoted with |LT |.
tion and null elements as in (Klein, 2005). To evalu-
ate the quality of a parse tree with respect to its gold
standard, the unlabeled parsing F-score is used.
5 Results
Entire Corpus Results. We start by discussing
the effect of ZL on the performance of the Seginer
parser when no length restriction is imposed on the
test corpus sentences (WSJ, BROWN and GENIA).
Table 1 (top, right section, for WSJ), Figure 1 (top
line, right graph, for WSJ), and Table 2 (the left sec-
tion of each table, top table for BROWN and bot-
tom table for GENIA) present the difference between
the F-score performance of the Seginer parser when
trained with the ZL algorithms and the parser?s per-
formance when trained with the entire corpus.
For all test corpora and sizes of the high quality
training subset (NH ), zoomed learning improves the
parser performance. ZL improves the parser perfor-
mance by 1.13% (WSJ), 1.46% (BROWN, the number
does not appear in the table) and 4.47% (GENIA).
For WSJ, the most substantial improvement is pro-
vided by EZL, while for BROWN and GENIA the best
results for some NH values are achieved by BZL and
for others by EZL (and for GENIA with small NH
values even by RZL).
Note, that for all three corpora zoomed learning
with random selection (RZL) improves the parser
performance on the entire test corpus, although to a
lesser extent than confidence-based ZL. This is true
for almost all NH values, including those that do not
appear in the tables. See Figure 1 (top line, right-
most graph) for WSJ.
We follow the unsupervised parsing literature and
provide performance analysis for WSJ sentences of
bounded length (WSJ10, WSJ20 and WSJ40). To
prevent clutter, for BROWN and GENIA we report
only entire corpus results.
Table 1 (top, left three sections) and Figure 1
(top line, three leftmost graphs) present results for
WSJ10, WSJ20 and WSJ40.
The result patterns for the sub-corpora are similar
to those reported for the entire WSJ corpus. EZL and
BZL both improve over the fully-trained parser, and
689
BROWN ENTIRE CORPUS (F = 57.19) LT HT
NH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%
EZL 0.55 0.69 0.64 0.66 0.65 0.82 1.15 1.31 -1.44 -0.03 0.04 0.30
BZL 1.11 0.80 0.02 -0.10 1.42 1.20 0.76 0.51 -4.80 -1.30 -0.79 -0.37
RZL 0.257 0.755 0.49 0.24 0.23 0.75 0.60 0.53 0.44 0.76 0.42 0.12
GENIA ENTIRE CORPUS (F = 42.71) LT HT
NH 10% 30% 50% 70% 10% 30% 50% 70% 10% 30% 50% 70%
EZL 0.01 0.83 1.10 1.66 -0.01 0.76 0.80 3.37 0.34 1.00 1.40 1.55
BZL -0.46 1.40 2.74 4.47 -0.54 0.40 0.96 4.09 0.42 4.29 4.60 5.49
RZL 0.61 1.70 2.09 1.99 0.28 2.08 3.30 3.86 3.04 3.22 2.80 1.85
Table 2: Results for the BROWN (top table) and GENIA (bottom table) corpora. Results are presented for the entire
corpus (left column section), the low quality test subset (middle column section, LT ) and the high quality test subset
(right column section, HT ) of each corpus, as a function of the high quality training set size (NH). Since the tables
present entire corpus results, the training and test subsets are identical.
the improvement of the former is more substantial.
Baselines. A key principle of ZL is the selection
of subsets that are better parsed by a parser trained
only with the sentences they contain than with a
parser trained with the entire training corpus. To
verify the importance of this principle we consid-
ered two alternative training protocols.
In the first, the entire test corpus is parsed with
a parser that was trained with a subset of randomly
selected sentences from the training set. We run this
protocol for all three corpora (and for the WSJ sub-
corpora) with various training set sizes and obtained
substantial degradation in the parser performance.
The performance monotonically increases with the
training set size and reached its maximum when the
entire corpus is used. We conclude that using less
training material harms the parser performance if a
test subset is not carefully selected.
The second protocol is the ?less is more proto-
col of Spitkovsky et al, (2010) in which we parsed
each test corpus using a parser that was trained with
all training sentences of a bounded length. Unlike
in their paper, in which this protocol improves the
perofrmance of the DMV unsupervised dependency
parser (Klein and Manning, 2004), for the Seginer
parser the protocol harms the results. When pars-
ing the entire WSJ with a WSJ10-trained parser or
with a WSJ20-trained parser, the F-score results are
59.99% and 72.22% compared to 76.00% of the
fully-trained parser. For GENIA the numbers are
15.61 and 35.87 compared to 42.71 and for BROWN
they are 36.05 and 50.02 compared to 57.19 3.
It is also interesting that sentence length is gen-
erally not a good subset selection criterion for ZL.
When parsing WSJ10 with a WSJ10-trained parser,
F-score results are 59.29 while the F-score of the
fully-trained parser on this corpus is 76.00. The
same phenomenon is observed with WSJ20 (F-score
of 61.90 with WSJ20 training and of 64.82 with
the entire WSJ training), and for the BROWN corpus
(65.01 vs. 69.43 for BROWN10 and 61.90 vs 62.92
for BROWN20). For GENIA, however, while parsing
GENIA10 with a GENIA10-trained parser harms the
performance (45.28 vs. 60.23), parsing GENIA20
with a GENIA20-trained parser enhances the perfor-
mance (53.23 vs. 50.00).
These results emphasize the power of random se-
lection for ZL as random selection does provide a
good selection criterion.
LT vs. HT. In what follows we analyze the ZL
algorithms aiming to characterize their strengths and
weaknesses.
Table 1 (bottom), the middle and right sections
of Table 2 (both tables) and Figure 1 (second and
third lines) present the performance of the ZL algo-
rithms on the lower quality and higher quality test
subsets (LT and HT ). The results patterns for WSJ
and BROWN are different than those of GENIA.
For WSJ (and its sub-corpora) and BROWN,
3We repeated this protocol multiple times for each corpus,
training the parser with sentences of length 5 to 45 in steps of 5.
In all cases we observed performance degradation compared to
the fully-trained parser.
690
1 2 3 4
x 104
?2
?1
0
1
WSJ10, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?0.5
0
0.5
1
1.5
WSJ20, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?0.5
0
0.5
1
1.5
WSJ40, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?0.5
0
0.5
1
1.5
WSJ, Whole Corpus
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?3
?2
?1
0
1
WSJ10, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ20, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ40, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?4
?2
0
2
4
WSJ, Low Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?10
?8
?6
?4
?2
0
WSJ10, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
1 2 3 4
x 104
?6
?4
?2
0 
1
WSJ20, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?6
?4
?2
0 
1
WSJ40, High Quality set
X
F 
Sc
or
e 
D
iff
er
en
ce
0 1 2 3 4 5
x 104
?6
?4
?2
0 
1
X
F 
Sc
or
e 
D
iff
er
en
ce
WSJ, High Quality set
1 2 3 4
x 104
?5
0
5
10
WSJ, Low Quality set
X
F 
Sc
or
e D
iffe
re
nc
e
0 1 2 3 4 5
x 104
?30
?20
?10
0
WSJ, High Quality set
X
F 
Sc
or
e D
iffe
re
nc
e
0 1 2 3 4 5
x 104
?1
0
1
2
3
WSJ, Whole Corpus
X
F 
Sc
or
e D
iffe
re
nc
e
Figure 1: WSJ results. Top Three Lines: Difference in F-score performance of the Seginer parser between training
with ZL and training with the entire WSJ corpus. Results are presented for the entire corpus (top line), the lower
quality test subset (LT , middle line) and the higher quality test subset (HT , bottom line) as a function of the size of
the high quality training subset X = NH , measured in sentences. The curve with triangles is for the extended zoomed
learning algorithm (EZL), the solid curve is for the basic zoomed learning algorithm (BZL) and the dashed curve is
for zoomed learning with random selection (RZL). Bottom line: Comparison between the performance of the Seginer
parser with the EZL algorithm (curves with triangles) and when subset selection is performed using the oracle F-score
of the trees (solid curves). F-score differences from the performance of the fully trained parser are presented for the
WSJ test corpus as a function of NH , the high quality training subset size. Oracle selection is superior for the lower
quality subset but inferior for the high quality subset.
confidence-based ZL (BZL and EZL) provides a sub-
stantial improvement for LT . For WSJ, F-score im-
provement is up to 1.32% (WSJ10), 3.13% (WSJ20),
3.35% (WSJ40) and 3.23% (the entire WSJ). For
BROWN the improvement is up to 1.42%.
For HT , confidence-based ZL is less effective
when these corpora are considered. As indicated
in the third line of Figure 1, for WSJ and its sub-
corpora, EZL leads to a small improvement on HT ,
while BZL generally leads to a performance degra-
dation on this test subset. For BROWN (the right sec-
tion of Table 2 (top)), confidence-based ZL gener-
ally leads to a performance degradation on HT .
For GENIA, EZL and BZL improve the parser per-
formance on both LT and HT for most NH values.
Understanding this difference is a subject for future
research. Our initial hypothesis is that due to the
relative small size of the GENIA corpus (4661 sen-
tences compared 24243 and 49206 sentences of WSJ
and BROWN respectively), there is more room for
improvement in the parser performance on this cor-
pus, and consequently ZL improves on both sets.
Oracle Analysis. Confidence-based ZL is based
on the idea that sentences for which the fully-trained
691
parser provides parses of similar quality manifest
similar syntactic patterns. Consequently, the parser
performance on a set of such sentences can be im-
proved if it is trained only with the sentences con-
tained in the set. An oracle experiment, where se-
lection is based on the F-score computed using the
gold standard tree instead of on the PUPA score, can
shed light on the validity of this idea.
Figure 1 (bottom line) compares the performance
of EZL with that of the oracle-based zoomed learn-
ing algorithm when the test corpus is the entire WSJ.
For the low quality test subset, oracle selection is
dramatically better than confidence-based selection.
For the high quality test subset the opposite pattern
holds, that is, EZL is superior. These differences lead
to the entire corpus pattern where EZL is superior for
most NH values.
Oracle-based and confidence-based zoomed
learning demonstrate the same trend: they improve
over the baseline for LT much more than for HT .
For HT , oracle-based ZL even harms results and
so does BZL, which does not benefit from the
averaging effect of EZL. The magnitude of the
effect of oracle-based zoomed learning is much
stronger. These results support our idea that training
the parser on a set selected by a well-designed
confidence test leads to improvement of the parser
performance for the selected sentences when the
fully-trained parser produces parses of mediocre
quality for them.
Integration of the experimental results for zoomed
learning with the three selection methods: random,
confidence-based and oracle-based leads to an im-
portant conclusion that should guide future research.
The more accurate the confidence score used by the
zoomed learning algorithm, the more substantial is
the performance improvement for the low quality
test subset, at the cost of more substantial degrada-
tion in the performance on the high quality subset
(but recall the different GENIA pattern which should
be further explored).
EZL Variants. For confidence-based ZL we ex-
plored two methods for utilizing the ensemble mem-
bers for generating a final parse tree for each of the
test sentences. In BZL, the L-trained parser and the
H-trained parser generate parse trees for LT and
HT sentences respectively. In EZL, for each sen-
tence the final parse is selected between the parse
created by a parser trained with the sentences con-
tained in its corresponding training subset, and the
parse created by the fully trained parser.
There are other ways to use the ensemble mem-
bers. While for all corpora it is beneficial to use
the L-trained parser for the low quality test subset
(LT ), the results for WSJ and BROWN imply that it
might be better to use the fully-trained parser or the
EZL algorithm to parse the high quality test subset
(HT ). We have experimented with these methods
and got only a minor improvement over the results
reported here (improvement is more substantial for
BROWN than for WSJ but does not exceed 0.5% for
both). This can also be inferred from the relative
minor performance degradation of BZL and EZL on
HT .
We also explored a ZL scenario in which the en-
tire test set is parsed either by the H-trained parser
or by the L-trained parser. These protocols result in
substantial degradation in parser performance (com-
pared to the fully-trained parser) since the perfor-
mance of the H-trained parser on LT and the per-
formance of the L-trained parser on HT are poor.
6 Conclusions
We introduced zoomed learning ? a training algo-
rithm for unsupervised parsers. We applied three
variants of ZL to the best fully unsupervised pars-
ing algorithm (Seginer, 2007) and show an improve-
ment of up to 4.47% in three English domains: WSJ,
BROWN and GENIA.
Future research should focus on the development
of more accurate estimators of parser output qual-
ity, and experimentation with different corpora, lan-
guages and parsers.
Developing a quality assessment algorithm for de-
pendency trees will allow us to apply confidence-
based ZL to unsupervised dependency parsing. Par-
ticularly, it will enable us to explore the combina-
tion of the methods proposed in (Spitkovsky et al,
2010) with ZL for the DMV model and to integrate
the PUPA score into their bootstrapping algorithm.
Another direction is to apply ZL to other NLP
tasks and ML areas, supervised and unsupervised.
692
References
Markus Becker and Miles Osborne, 2005. A two-stage
method for active learning of statistical grammars. IJ-
CAI ?05.
Rens Bod, 2006a. An all-subtrees approach to unsuper-
vised parsing. ACL-COLING ?06.
Rens Bod, 2006b. Unsupervised parsing with U-DOP.
CoNLL ?06.
Rens Bod, 2007. Is the end of supervised parsing in
sight? ACL ?07.
Leo Breiman, 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Rich Caruana and Alexandru Niculescu-Mizil, 2006.
An empirical comparison of supervised learning algo-
rithms. ICML ?06.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Alexander Clark, 2003. Combining distributional and
morphological information for part of speech induc-
tion. EACL ?03.
Shay Cohen, Kevin Gimpel and Noah Smith, 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. NIPS ?08.
Shay Cohen and Noah Smith, 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. NAACL ?09.
David Cohn, Les Atlas and Richard Ladner. 1994. Im-
proving generalization with active learning. Machine
Learning, 15(2):201?221.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. CogSci ?05.
W. N. Francis and H. Kucera 1979. Manual of infor-
mation to accompany a standard corpus of present-day
edited American English, for use with digital com-
puters. Department of Linguistics, Brown University
Press, Providence, RI.
Yoav Freund and Robert E. Schapire, 1996. Experiments
with a new boosting algorithm. ICML ?96.
William Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. NAACL
?09.
John Henderson and Eric Brill, 2000. Bagging and
boosting a treebank parser. NAACL ?00.
Rebecca Hwa. 2004. Sample selection for statistical
parsing. Computational Linguistics, 30(3):253?276.
Daisuke Kawahara and Kiyotaka Uchimoto 2008.
Learning reliability of parses for domain adaptation of
dependency parsing. IJCNLP ?08.
Dan Klein and Christopher Manning, 2002. A genera-
tive constituent-context model for improved grammar
induction. ACL ?02.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. ACL ?04.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi and Jun?ichi
Tsujii, 2003. GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics, (sup-
plement: 11th ISMB) 19:i180?i182, Oxford Univer-
sity Press, 2003.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson,
2006. Reranking and self-training for parser adapta-
tion. ACL-COLING ?06.
Sujith Ravi, Kevin Knight and Radu Soricut, 2008. Au-
tomatic prediction of parser accuracy. EMNLP ?08.
Roi Reichart and Ari Rappoport, 2007. An ensemble
method for selection of high quality parses. ACL ?07.
Roi Reichart and Ari Rappoport, 2009a. Sample se-
lection for statistical parsers: cognitively driven algo-
rithms and evaluation measures. CoNLL ?09.
Roi Reichart and Ari Rappoport, 2009b. Automatic se-
lection of high quality parses created by a fully unsu-
pervised parser. CoNLL ?09.
Yoav Seginer, 2007. Fast unsupervised incremental pars-
ing. ACL ?07.
Noah Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
ACL-COLING ?06.
Valentin Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky, 2010. From baby steps to leapfrog: how ?less is
more? in unsupervised dependency parsing. NAACL
?10.
Ricardo Vilalta and Irina Rish, 2003. A decomposition
of classes via clustering to explain and improve naive
bayes. ECML ?03.
Alexander Yates, Stefan Schoenmackers and Oren Et-
zioni, 2006. Detecting parser errors using web-based
semantic filters . EMNLP ?06.
693
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1880?1891,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Authorship Attribution of Micro-Messages
Roy Schwartz Oren Tsur Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
{roys02|oren|arir}@cs.huji.ac.il
Moshe Koppel
Department of Computer Science
Bar Ilan University
koppel@macs.biu.ac.il
Abstract
Work on authorship attribution has tradition-
ally focused on long texts. In this work, we
tackle the question of whether the author of
a very short text can be successfully iden-
tified. We use Twitter as an experimental
testbed. We introduce the concept of an au-
thor?s unique ?signature?, and show that such
signatures are typical of many authors when
writing very short texts. We also present a new
authorship attribution feature (?flexible pat-
terns?) and demonstrate a significant improve-
ment over our baselines. Our results show that
the author of a single tweet can be identified
with good accuracy in an array of flavors of
the authorship attribution task.
1 Introduction
Research in authorship attribution has developed
substantially over the last decade (Stamatatos,
2009). The vast majority of such research has been
dedicated towards finding the author of long texts,
ranging from single passages to book chapters. In
recent years, the growing popularity of social me-
dia has created special interest, both theoretical and
computational, in short texts. This has led to many
recent authorship attribution projects that experi-
mented with web data such as emails (Abbasi and
Chen, 2008), web forum messages (Solorio et al,
2011) and blogs (Koppel et al, 2011b). This paper
addresses the question to what extent the authors of
very short texts can be identified. To answer this
question, we experiment with Twitter tweets.
Twitter messages (tweets) are limited to 140 char-
acters. This restriction imposes major difficulties on
authorship attribution systems, since authorship at-
tribution methods that work well on long texts are
often not as useful when applied to short texts (Bur-
rows, 2002; Sanderson and Guenter, 2006).
Nonetheless, tweets are relatively self-contained
and have smaller sentence length variance com-
pared to excerpts from longer texts (see Section 3).
These characteristics make Twitter data appealing as
a testbed when focusing on short texts. Moreover,
an authorship attribution system of tweets may have
various applications. Specifically, a range of cyber-
crimes can be addressed using such a system, includ-
ing identity fraud and phishing.
In this paper, we introduce the concept of k-
signatures. We denote the k-signatures of an author
a as the features that appear in at least k% of a?s
training samples, while not appearing in the training
set of any other author. When k is large, such signa-
tures capture a unique style used by a. An analysis
of our training set reveals that unique k-signatures
are typical of many authors. Moreover, a substantial
portion of the tweets in our training set contain at
least one such signature. These findings suggest that
a single tweet, although short and sparse, often con-
tains sufficient information for identifying its author.
Our results show that this is indeed the case.
We train an SVM classifier with a set of features
that include character n-grams and word n-grams.
We use a rigorous experimental setup, with varying
number of authors (values between 50-1,000) and
various sizes of the training set, ranging from 50 to
1,000 tweets per author. In all our experiments, a
single tweet is used as test document. We also use
a setting in which the system is allowed to respond
don?t know in cases of uncertainty. Applying this
option results in higher precision, at the expense of
1880
lower recall.
Our results show that the author of a tweet can be
successfully identified. For example, when using a
dataset of as many as 1,000 authors with 200 train-
ing tweets per author, we are able to obtain 30.3%
accuracy (as opposed to a random baseline of only
0.1%). Using a dataset of 50 authors with as few
as 50 training tweets per author, we obtain 50.7%
accuracy. Using a dataset of 50 authors with 1,000
training tweets per author, our results reach as high
as 71.2% in the standard classification setting, and
exceed 91% accuracy with 60% recall in the don?t
know setting.
We also apply a new set of features, never previ-
ously used for this task ? flexible patterns. Flexi-
ble patterns essentially capture the context in which
function words are used. The effectiveness of func-
tion words as authorship attribution features (Koppel
et al, 2009) suggests using flexible pattern features.
The fact that flexible patterns are learned from plain
text in a fully unsupervised manner makes them
domain and language independent. We demon-
strate that using flexible patterns gives significant
improvement over our baseline system. Further-
more, using flexible patterns, our system obtains a
6.1% improvement over current state-of-the-art re-
sults in authorship attribution on Twitter.
To summarize, the contribution of this paper is
threefold.
? We provide the most extensive research to date
on authorship attribution of micro-messages,
and show that authors of very short texts can
be successfully identified.
? We introduce the concept of an author?s unique
k-signature, and demonstrate that such signa-
tures are used by many authors in their writing
of micro-messages.
? We present a new feature for authorship attri-
bution ? flexible patterns ? and show its sig-
nificant added value over other methods. Us-
ing this feature, our system obtains a 6.1% im-
provement over the current state-of-the-art.
The rest of the paper is organized as follows. Sec-
tions 2 and 3 describe our methods and our experi-
mental testbed (Twitter). Section 4 presents the con-
cept of k-signatures. Sections 5 and 6 present our
experiments and results. Flexible patterns are pre-
sented in Section 7 and related work is presented in
Section 8.
2 Methodology
In the following we briefly describe the main fea-
tures employed by our system. The features below
are binary features.
Character n-grams. Character n-gram features
are especially useful for authorship attribution on
micro-messages since they are relatively tolerant
to typos and non-standard use of punctuation (Sta-
matatos, 2009). These are common in the non-
formal style generally applied in social media ser-
vices. Consider the example of misspelling ?Brit-
ney? as ?Brittney?. The misspelled name shares the
4-grams ?Brit? and ?tney? with the correct name. As
a result, these features provide information about the
author?s style (or at least her topic of interest), which
is not available through lexical features.
Following standard practice, we use 4-grams
(Sanderson and Guenter, 2006; Layton et al, 2010;
Koppel et al, 2011b). White spaces are considered
characters (i.e., a character n-gram may be com-
posed of letters from two different words). A sin-
gle white-space is appended to the beginning and
the end of each tweet. For efficiency, we consider
only character n-gram features that appear at least
tcng times in the training set of at least one author
(see Section 5).
Word n-grams. We hypothesize that word n-gram
features would be useful for authorship attribution
on micro-messages. We assume that under a strict
length restriction, many authors would prefer using
short, repeating phrases (word n-grams).
In our experiments, we consider 2 ? n ? 5.1
We regard sequences of punctuation marks as words.
Two special words are added to each tweet to indi-
cate the beginning and the end of the tweet. For effi-
ciency, we consider only word n-gram features that
appear at least twng times in the training set of at
least one author (see Section 5).
Model. We use libsvm?s Matlab implementation
of a multi-class SVM classifier with a linear kernel
1We skip unigrams as they are generally captured by the
character n-gram features.
1881
(Chang and Lin, 2011). We use ten-fold cross vali-
dation on the training set to select the best regular-
ization factor between 0.5 and 0.005.2
3 Experimental Testbed
Our main research question in this paper is to deter-
mine the extent to which authors of very short texts
can be identified. A major issue in working with
short texts is selecting the right dataset. One ap-
proach is breaking longer texts into shorter chunks
(Sanderson and Guenter, 2006). We take a differ-
ent approach and experiment with micro-messages
(specifically, tweets).
Tweets have several properties making them an
ideal testbed for authorship attribution of short texts.
First, tweets are posted as single units and do not
necessarily refer to each other. As a result, they tend
to be self contained. Second, tweets have more stan-
dardized length distribution compared to other types
of web data. We compared the mean and standard
deviation of sentence length in our Twitter dataset
and in a corpus of English web data (Ferraresi et al,
2008).3 We found that (a) tweets are shorter than
standard web data (14.2 words compared to 20.9),
and (b) the standard deviation of the length of tweets
is much smaller (6.4 vs. 21.4).
Pre-Processing. We use a Twitter corpus that in-
cludes approximately 5 ? 108 tweets.4 All non-
English tweets and tweets that contain fewer than
3 words are removed from the dataset. We also re-
move tweets marked as retweets (using the RT sign,
a standard Twitter symbol to indicate that this tweet
was written by a different user). As some users
retweet without using the RT sign, we also remove
tweets that are an exact copy of an existing tweet
posted in the previous seven days.
Apart from plain text, some tweets contain ref-
erences to other Twitter users (in the format of
@<user>). Since using reference information
makes this task substantially easier (Layton et al,
2010), we replace each user reference with the spe-
cial meta tag REF. For sparsity reasons, we also re-
place web addresses with the meta tag URL, num-
2In practice, 0.05 or 0.1 are selected in almost all cases.
3http://wacky.sslmit.unibo.it
4These comprise ?15% of all public tweets created from
May 2009 to March 2010.
0 5 10 15 20 25 30 35 40 45 >50
0
10
20
30
40
50
60
70
80
90
Number of k?signatures per user
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 1: Number of users with at least x k-signatures
(100 authors, 180 training tweets per author).
bers with the meta tag NUM, time of day with the
meta tag TIME and dates with the meta tag DATE.
4 k-Signatures
In this section, we show that many authors adopt
a unique style when writing micro-messages. This
style can be detected by a strong classification algo-
rithm (such as SVM), and be sufficient to correctly
identify the author of a single tweet.
We define the concept of the k-signature of an au-
thor a to be a feature that appears in at least k% of
a?s training set, while not appearing in the training
set of any other user. Such signatures can be useful
for identifying future (unlabeled) tweets written by
a.
To validate our hypothesis, we use a dataset of
100 authors with 180 tweets per author. We com-
pute the number of k-signatures used by each of
the authors in our dataset. Figure 1 shows our re-
sults for a range of k values (2%, 5%, 10%, 20%
and 50%). Results demonstrate that 81 users use
at least one 2%-signature, 43 users use at least one
5%-signature, and 17 users use at least one 10%-
signature. These results indicate that a large portion
of the users adopt a unique signature (or set of sig-
natures) when writing short texts. Table 1 provides
examples of 10%-signatures.
1882
Signature Type 10%-signature Examples
Character n-grams
? ? ??
REF oh ok ? ? Glad you found it!
Hope everyone is having a good afternoon ? ?
REF Smirnoff lol keeping the goose in the freezer ? ?
?yew ?
gurl yew serving me tea nooch
REF about wen yew and ronnie see each other
REF lol so yew goin to check out tini?s tonight huh???
Word n-grams
.. lal
REF aww those are cool where u get those.. how do ppl react.. lal
Ludas album is gone be hott.. lal
Dayum refs don?t get injury timeouts.. lal.. get him off the field..
smoochies , e3
I?m just back after takin? a very long, icy cold
shower........Shivering smoochies,E3 http://bit.ly/4CzzP9
A blue stout or two would be nice as well, Purr!Blue smooth
smoochies,E3 http://bit.ly/75D4fO
That is sooooooooooooooooooo unfair!Double smoochies,E3
http://bit.ly/07sXRGX
Table 1: Examples of 10%-signatures.
Results also show that seven users use one or
more 20%-signatures, and five users even use one
or more 50%-signatures. Looking carefully at these
users, we find that they write very structured mes-
sages, and are probably bots, such as news feeds,
bidding systems, etc. Table 2 provides examples of
tweets posted by such users.5
Another interesting question is how many tweets
contain at least one k-signature. Figure 2 shows
for each user the number of tweets in her training
set for which at least one k-signature is found. Re-
sults demonstrate that a total of 18.6% of the train-
ing tweets contain at least one 2%-signature, 10.3%
the training tweets contain at least one 5%-signature
and 6.5% of the training tweets contain at least one
10%-signature. These findings validate our assump-
tion that many users use k-signatures in short texts.
These findings also have direct implications on
authorship attribution of micro-messages, since k-
signatures are reliable classification features. As
a result, texts written by authors that tend to use
k-signatures are likely to be easily identified by a
reasonable classification algorithm. Consequently,
k-signatures provide a possible explanation for the
high quality results presented in this paper.
In the broader context, the presence (and contri-
5Our k-signature method can actually be useful for automat-
ically identifying such users. We defer this to future work.
0 20 40 60 80 100 120 140 160 180
0
10
20
30
40
50
60
70
80
90
Number of Tweets with at least one k?Signature
N
um
be
r o
f U
se
rs
 
 
k = 2%
k = 5%
k = 10%
k = 20%
k = 50%
Figure 2: Number of users with at least x training tweets
that contain at least one k-signature (100 authors, 180
training tweets per author).
bution) of k-signatures is in line with the hypothesis
proposed by (Davidov et al, 2010a): while still us-
ing an informal and unstructured (grammatical) lan-
guage, authors tend to use typical and unique struc-
tures in order to allow a short message to stand alone
without a clear conversational context.
1883
User 20%-signature Examples
1 I?m listening to :
I?m listening to: Sigur R?s ? Intro:
http://www.last.fm/music/Sigur+R%C3%B3s http://bit.ly/3XJHyb
I?m listening to: Tina Arena ? In Command:
http://www.last.fm/music/Tina+Arena http://bit.ly/7q9E25
I?m listening to: Midnight Oil ? Under the Overpass:
http://www.last.fm/music/Midnight+Oil http://bit.ly/7IH4cg
2 news now ( str )
#Hotel News Now(STR) 5 things to know: 27 May 2009: From the desks of
the HotelNewsNow.com editor... http://bit.ly/aZTZOq #Tourism #Lodging
#Hotel News Now(STR) Five sales renegotiating tactics: As bookings rep-
resentatives press to reneg... http://bit.ly/bHPn2L
#Hotel News Now(STR) Risk of hotel recession retreats: The Hotel Indus-
try?s Pulse Index increases... http://bit.ly/a8EKrm #Tourism #Lodging
3
( NUM bids )
end date :
NEW PINK NINTENDO DS LITE CONSOLE WITH 21 GIFTS +
CASE: &#163;66.50 (13 Bids) End Date: Tuesday Dec-08-2009 17:..
http://bit.ly/7uPt6V
Microsoft Xbox 360 Game System - Console Only - Working: US $51.99
(25 Bids) End Date: Saturday Dec-12-2009 13:.. http://bit.ly/8VgdTv
Microsoft Sony Playstation 3 (80 GB) Console 6 Months Old:
&#163;190.00 (25 Bids) End Date: Sunday Dec-13-2009 21:21:39 G..
http://bit.ly/7kwtDS
Table 2: Examples of tweets published by very structured users, suspected to be bots, along with one of their 20%-
signatures.
5 Experiments
We report of three different experimental configu-
rations. In the experiments described below, each
dataset is divided into training and test sets using
ten-fold cross validation. On the test phase, each
document contains a single tweet.
Experimenting with varying Training Set Sizes.
In order to test the affect of the training set size,
we experiment with an increasingly larger number
of tweets per author. Experimenting with a range of
training set sizes serves two purposes: (a) to check
whether the author of a tweet can be identified us-
ing a very small number of (short) training samples,
and (b) check howmuch our system can benefit from
training on a larger corpus.
In our experiments we only consider users who
posted between 1,000?2,000 tweets6 (a total of
6This range is selected since on one hand we want at least
1,000 tweets per author for our experiments, and on the other
hand we noticed that users with a larger number of tweets in
corpus tend to be spammers or bots that are very easy to identify,
so we limit this number to 2,000.
10,183 users), and randomly select 1,000 tweets per
user. From these users, we select 10 groups of 50
users each.7 We perform a set of classification ex-
periments, selecting for each author an increasingly
larger subset of her 1,000 tweets as training set. Sub-
set sizes are (50, 100, 200, 500, 1,000). Thresh-
old values for our features in each setting (see Sec-
tion 2) are (2, 2, 4, 10, 20) for tcng and (2, 2, 2, 3, 5)
for twng, respectively.
Experimenting with varying Numbers of Au-
thors. In a second set of experiments, we use an
increasingly larger number of authors (values be-
tween 100-1,000), in order to check whether the au-
thor of a very short text can be identified in a ?needle
in a haystack? type of setting.
Due to complexity issues, we only experiment
with 200 tweets per author as training set. We se-
lect groups of size 100, 200, 500 and 1,000 users
(one group per size). We use the same threshold val-
ues as the 200 tweets per author setting previously
described (tcng = 4, twng = 2).
7An eleventh group is selected as development set.
1884
0 100 200 300 400 500 600 700 800 900 1000
45
50
55
60
65
70
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 3: Authorship attribution accuracy for 50 authors
with various training set sizes. The values are averaged
over 10 groups. The random baseline is 2%.
Recall-Precision Tradeoff. Another aspect of our
research question is the level of certainty our system
has when suggesting an author for a given tweet.
In cases of uncertainty, many real life applications
would prefer not to get any response instead of get-
ting a response with low certainty. Moreover, in real
life applications we are often not even sure that the
real author is part of our training set. Consequently,
we allow our system to respond ?don?t know? in
cases of low confidence (Koppel et al, 2006; Kop-
pel et al, 2011b). This allows our system to obtain
higher precision, at the expense of lower recall.
To implement this feature, we use SVM?s proba-
bility estimates, as implemented in libsvm. These
estimates give a score to each potential author.
These scores reflect the probability that this author
is the correct author, as decided by the prediction
model. The selected author is always the one with
the highest probability estimate.
As selection criterion, we use a set of increasingly
larger thresholds (0.05-0.9) for the probability of the
selected author. This means that we do not select test
samples for which the selected author has a proba-
bility estimate value lower than the threshold.
0 100 200 300 400 500 600 700 800 900 1000
25
30
35
40
45
50
55
60
Number of Candidate Authors
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams + Word N?grams
Char. N?grams
Figure 4: Authorship attribution accuracy with varying
number of candidate authors, using 200 training tweets
per author. The random baselines for 509, 100, 200, 500
and 1,000 authors are 2%, 1%, 0.5%, 0.2% and 0.1%,
respectively.
6 Basic Results
Experimenting with varying Training Set Sizes.
Figure 3 shows results for our experiments with
50 authors and various training set sizes. Results
demonstrate that authors of very short texts can be
successfully identified, even with as few as 50 tweets
per author (49.5%). When given more training sam-
ples, authors are identified much more accurately
(up to 69.7%). Results also show that, according to
our hypothesis, word n-gram features substantially
improve over character n-grams features only (3%
averaged improvement over all settings).
Experimenting with varying Numbers of Au-
thors. Figure 4 shows our results for various num-
bers of authors, using 200 tweets per author as train-
ing set. Results demonstrate that authors of an
unknown tweet can be identified to a large extent
even when there are as many as 1,000 candidate au-
thors (30.3%, as opposed to a random baseline of
only 0.1%). Results further validate that word n-
gram features substantially improve over character
9Results for 50 authors with 200 tweets per author are taken
from Figure 3.
1885
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
1,000 tweets/author
500 tweets/author
200 tweets/author
100 tweets/author
50 tweets/author
Figure 5: Recall-precision curves for 50 authors with
varying training set sizes.
n-grams features (2.6% averaged improvement).
Recall-Precision Tradeoff. Figure 5 shows the
recall-precision curves for our experiments with 50
authors and varying training set sizes. Results
demonstrate that we are able to obtain very high pre-
cision (over 90%) while still maintaining a relatively
high recall (from ?35% recall for 50 tweets per au-
thor up to> 60% recall for 1,000 tweets per author).
Figure 6 shows the recall-precision curves for our
experiments with varying number of authors. Re-
sults demonstrate that even in the 1,000 authors set-
ting, we are able to obtain high precision values
(90% and 70%) with reasonable recall values (18%
and ?30%, respectively).
7 Flexible Patterns
In previous sections we provided strong evidence
that authors of micro-messages can be successfully
identified using standard methods. In this section we
present a new feature, never previously used for this
task ? flexible patterns. We show that flexible pat-
terns can be used to improve classification results.
Flexible patterns are a generalization of word n-
grams, in the sense that they capture potentially un-
seen word n-grams. As a result, flexible patterns
can pick up fine-grained differences between au-
thors? styles. Unlike other types of pattern features,
0 10 20 30 40 50 60 70 80 90 100
30
40
50
60
70
80
90
100
Recall (%)
Pr
ec
is
io
n 
(%
)
 
 
50 authors
100 authors
200 authors
500 authors
1,000 authors
Figure 6: Recall-precision curves for varying number of
authors.
flexible patterns are computed automatically from
plain text. As such, they can be applied to various
tasks, independently of domain and language. We
describe them in detail.
Word Frequency. Flexible patterns are composed
of high frequency words (HFW) and content words
(CW). Every word in the corpus is defined as either
HFW or CW. This clustering is performed by count-
ing the number of times each word appears in the
corpus of size s. A word that appears more than
10?4?s times in a corpus is considered HFW. A
word that appears less than 10?3?s times in a cor-
pus is considered CW. Some words may serve both
as HFWs and CWs (see Davidov and Rappoport
(2008b) for discussion).
Structure of a Flexible Pattern. Flexible patterns
start and end with an HFW. A sequence of zero or
more CWs separates consecutive HFWs. At least
one CW must appear in every pattern.10 For effi-
ciency, at most six HFWs (and as a result, five CW
sequences) may appear in a flexible pattern. Exam-
ples of flexible patterns include
1. ?theHFW CW ofHFW theHFW?
10Omitting this treats word n-grams as flexible patterns.
1886
Flexible Pattern Features. Flexible patterns can
serve as binary classification features; a tweet
matches a given flexible pattern if it contains the
flexible pattern sequence. For example, (1) is
matched by (2).
2. ?Go to theHFW houseCW ofHFW theHFW rising sun?
Partial Flexible Patterns. A flexible pattern may
appear in a given tweet with additional words not
originally found in the flexible pattern, and/or with
only a subset of the HFWs (Davidov et al, 2010a).
For example, (3) is a partial match of (1), since the
word ?great? is not part of the original flexible pat-
tern. Similarly, (4) is another partial match of (1),
since (a) the word ?good? is not part of the original
flexible pattern and (b) the second occurrence of the
word ?the? does not appear in (4) (missing word is
marked by ).
3. ?TheHFW greatHFW kingCW ofHFW theHFW ring?
4. ?TheHFW goodHFW kingCW ofHFW Spain?
We use such cases as features with lower weight,
proportional to the number of found HFWs in the
tweet (w = 0.5?nfoundnexpected ). For example, (1) receives a
weight of 1 (complete match) against (2). Against
(3), it receives a weight of 0.5 (= 0.5?33 , partial
match with no missing HFWs). Against (4) it re-
ceives a weight of 1/3 (= 0.5?23 , partial match with
only 2/3 HFWs found).
Experimenting with Flexible Pattern Features.
We repeat our experiments with varying training set
sizes (see Section 5) with two more systems: one
that uses character n-grams and flexible pattern fea-
tures, and another that uses character n-grams, word
n-grams and flexible patterns. High frequency word
counts are computed separately for each author us-
ing her training set. We only consider flexible pat-
tern features that appear at least tfp times in the
training set of at least one author. Values of tfp for
training set sizes (50, 100, 200, 500, 1,000) are (2,
3, 7, 7, 8), respectively.
Results. Figure 7 shows our results. Results
demonstrate that flexible pattern features have an
added value over both character n-grams alone (av-
eraged 2.9% improvement) and over character n-
grams and word n-grams together (averaged 1.5%
0 100 200 300 400 500 600 700 800 900 1000
35
40
45
50
55
60
65
70
75
Training Set Size
Ac
cu
ra
cy
 (%
)
 
 
Char. N?grams, Word N?grams &
Flex. Patt. Feats.
Char. N?grams + Flex. Patt. Feats.
Char. N?grams + Word N?grams
Char. N?grams
SCAP
Naive Bayes
Figure 7: Authorship attribution accuracy for 50 authors
with various training set sizes and various feature sets.
The values are averaged over 10 groups. The random
baseline is 2%.
Comparison to previous work: SCAP ? SCAP algo-
rithm results, as reported by (Layton et al, 2010), Naive
Bayes ? Naive Bayes algorithm results, as reported by
(Boutwell, 2011).
improvement). We perform t-tests on each of our
training set sizes to check whether the latter im-
provement is significant. Results demonstrate that
it is highly significant in all settings, with p-values
smaller than values between 10?3 (for 50 tweets per
author) and 10?8 (1,000 tweets per author).
Comparison to Previous Works. Figure 7 also
shows results for the only two works that experi-
mented in some of the settings we experimented in:
Layton et al (2010) and Boutwell (2011) (see Sec-
tion 8). Our system substantially outperforms these
two systems, by margins of 5.9% to 19%. These
margins are explained by the choice of algorithm
(SVM and not SCAP/naive Bayes) and our set of
features (character n-grams + word n-grams + flex-
ible patterns compared to character n-grams only).
In order to rule out the possibility that these mar-
gins stem from using different datasets, we tested
our system on the dataset used in (Layton et al,
2010). Our system obtains even higher results on
this dataset than on our datasets (61.6%, a total im-
1887
provement of 6.1% over (Layton et al, 2010)).
Discussion. To illustrate the additional contribu-
tion of flexible patterns over word n-grams, consider
the following tweets, written by the same author.
5. ?. . . theHFW wayCW IHFW treatedCW herHFW?
6. ?. . . half of theHFW thingsCW IHFW have seen?
7. ?. . . theHFW friendsCW IHFW have had for years?
8. ?. . . in theHFW neighborhoodCW IHFW grew up in?
Consider a case where (5) is part of the test set,
while (6-8) appear in the training set. As (5) shares
no sequence of words with (6-8), no word n-gram
feature is able to identify the author?s style in (5).
However, this style can be successfully identified us-
ing the flexible pattern (9), shared by (5-8).
9. theHFW CW IHFW
This demonstrates the added value flexible pattern
features have over word n-gram features.
8 Related Work
Authorship attribution dates back to the end of 19th
century, when (Mendenhall, 1887) applied sentence
length and word length features to plays of Shake-
speare. Ever since, many methods have been devel-
oped for this task. For recent surveys, see (Koppel
et al, 2009; Stamatatos, 2009; Juola, 2012).
Authorship attribution methods can be generally
divided into two categories (Stamatatos, 2009). In
similarity-based methods, an anonymous text is at-
tributed to some author whose writing style is most
similar (by some distance metric). In machine learn-
ing methods, which we follow in this paper, anony-
mous texts are classified, using machine learning al-
gorithms, into different categories (in this case, dif-
ferent authors).
Machine learning papers differ from each other by
the features and machine learning algorithm. Exam-
ples of features include HFWs (Mosteller and Wal-
lace, 1964; Argamon et al, 2007), character n-gram
(Kjell, 1994; Hoorn et al, 1999; Stamatatos, 2008),
word n-grams (Peng et al, 2004), part-of-speech
n-grams (Koppel and Schler, 2003; Koppel et al,
2005) and vocabulary richness (Abbasi and Chen,
2005).
The various machine learning algorithms used in-
clude naive Bayes (Mosteller and Wallace, 1964;
Kjell, 1994), neural networks (Matthews and Mer-
riam, 1993; Kjell, 1994), K-nearest neighbors (Kjell
et al, 1995; Hoorn et al, 1999) and SVM (De Vel et
al., 2001; Diederich et al, 2003; Koppel and Schler,
2003).
Traditionally, authorship attribution systems have
mainly been evaluated against long texts such as
theater plays (Mendenhall, 1887), essays (Yule,
1939; Mosteller and Wallace, 1964), biblical books
(Mealand, 1995; Koppel et al, 2011a) and book
chapters (Argamon et al, 2007; Koppel et al, 2007).
In recent year, many works focused on web data
such as emails (De Vel et al, 2001; Koppel and
Schler, 2003; Abbasi and Chen, 2008), web forum
messages (Abbasi and Chen, 2005; Solorio et al,
2011), blogs (Koppel et al, 2006; Koppel et al,
2011b) and chat messages (Abbasi and Chen, 2008).
Some works focused on SMS messages (Mohan et
al., 2010; Ishihara, 2011).
Authorship Attribution on Twitter. The perfor-
mance of authorship attribution systems on short
texts is affected by several factors (Stamatatos,
2009). These factors include the number of candi-
date authors, the training set size and the size of the
test document.
Very few authorship attribution works experi-
mented with Twitter. Unlike our work, all used a
single group of authors (group sizes varied between
3-50). Layton et al (2010) used the SCAP method-
ology (Frantzeskou et al, 2007) with character n-
gram features. They experimented with 50 authors
and compared different numbers of tweets per au-
thor (values between 20-200). Surprisingly, they
showed that their system does not improve when
given more training tweets. In our work, we no-
ticed a different trend, and showed that more data
can be extremely valuable for authorship attribution
systems on micro-messages (see Section 6). Silva
et al (2011) trained an SVM classifier with various
features (e.g., punctuation and vocabulary features)
on a small dataset of three authors only, with vary-
ing training set size. Although their work used a
set of Twitter-specific features that we do not explic-
itly use, our features implicitly cover a large portion
of their features (such as punctuation and emoticon
1888
features, which are largely covered by character n-
grams).
Boutwell (2011) used a naive Bayes classifier
with character n-gram features. She experimented
with 50 authors and two training size values (120
and 230). She also provided a set of experiments that
studied the effect of joining several tweets into a sin-
gle document. Mikros and Perifanos (2013) trained
an SVM classifier with character n-gram and word
n-grams. They experimented with 10 authors of
Greek text, and also joined several tweets into a sin-
gle document. Joining several tweets into a longer
document is appealing since it can lead to substantial
improvement of the classification results, as demon-
strated by the works above. However, this approach
requires the test data to contain several tweets that
are known a-priori to be written by the same author.
This assumption is not always realistic. In our paper,
we intentionally focus on a single tweet as document
size.
Flexible Patterns. Patterns were introduced by
(Hearst, 1992), who used hand crafted patterns
to discover hyponyms. Hard coded patterns
were used for many tasks, such as discovering
meronymy (Berland and Charniak, 1999), noun cat-
egories (Widdows and Dorow, 2002), verb relations
(Chklovski and Pantel, 2004) and semantic class
learning (Kozareva et al, 2008).
Patterns were first extracted in a fully unsuper-
vised manner (?flexible patterns?) by (Davidov and
Rappoport, 2006), who used flexible patterns in or-
der to establish noun categories, and (Bicic?i and
Yuret, 2006) who used them for analogy question
answering. Ever since, flexible patterns were used
as features for various tasks such as extraction of
semantic relationships (Davidov et al, 2007; Tur-
ney, 2008b; Bollegala et al, 2009), detection of
synonyms (Turney, 2008a), disambiguation of nom-
inal compound relations (Davidov and Rappoport,
2008a), sentiment analysis (Davidov et al, 2010b)
and detection of sarcasm (Tsur et al, 2010).
9 Conclusion
The main goal of this paper is to measure to what
extent authors of micro-messages can be identified.
We have shown that authors of very short texts
can be successfully identified in an array of au-
thorship attribution settings reported for long doc-
uments. This is the first work on micro-messages
to address some of these settings. We introduced
the concept of k-signature. Using this concept, we
proposed an interpretation of our results. Last, we
presented the first authorship attribution system that
uses flexible patterns, and demonstrated that using
these features significantly improves over other sys-
tems. Our system obtains 6.1% improvement over
the current state-of-the-art.
Acknowledgments
We would like to thank Elad Eban and Susan Good-
man for their helpful advice, as well as Robert Lay-
ton for providing us with his dataset. This research
was funded (in part) by the Harry and Sylvia Hoff-
man leadership and responsibility program (for the
first author) and the Intel Collaborative Research In-
stitute for Computational Intelligence (ICRI-CI).
References
Ahmed Abbasi and Hsinchun Chen. 2005. Applying au-
thorship analysis to extremist-group web forum mes-
sages. IEEE Intelligent Systems, 20:67?75.
Ahmed Abbasi and Hsinchun Chen. 2008. Writeprints:
A stylometric approach to identity-level identification
and similarity detection in cyberspace. ACM Transac-
tions on Information Systems, 26(2):7:1?7:29.
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional lex-
ical features: Research articles. J. Am. Soc. Inf. Sci.
Technol., 58(6):802?822.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proc. of ACL, pages
57?64, College Park, Maryland, USA.
Ergun Bicic?i and Deniz Yuret. 2006. Clustering word
pairs to answer analogy questions. In Proc. of TAINN,
pages 1?8.
Danushka T. Bollegala, Yutaka Matsuo, and Mitsuru
Ishizuka. 2009. Measuring the similarity between
implicit semantic relations from the web. In Proc. of
WWW, New York, New York, USA. ACM Press.
Sarah R. Boutwell. 2011. Authorship Attribution of
Short Messages Using Multimodal Features. Master?s
thesis, Naval Postgraduate School.
John Burrows. 2002. ?Delta?: a Measure of Stylistic
Difference and a Guide to Likely Authorship. Literary
and Linguistic Computing, 17(3):267?287.
1889
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Dekang Lin and Dekai Wu, editors, Proc.
of EMNLP, pages 33?40, Barcelona, Spain.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proc.
of ACL-Coling, pages 297?304, Sydney, Australia.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of semantic relationships between nominals us-
ing pattern clusters. In Proceedings of ACL-08: HLT,
pages 227?235, Columbus, Ohio, June. Association
for Computational Linguistics.
Dmitry Davidov and Ari Rappoport. 2008b. Unsuper-
vised discovery of generic relationships using pattern
clusters and its evaluation by automatically generated
SAT analogy questions. In Proc. of ACL-HLT, pages
692?700, Columbus, Ohio.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of ACL,
pages 232?239, Prague, Czech Republic.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010a.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proc. of CoNLL, pages 107?
116, Uppsala, Sweden.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010b.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proc. of Coling, pages 241?249, Bei-
jing, China.
Olivier De Vel, Alison Anderson, Malcolm Corney, and
George Mohay. 2001. Mining e-mail content for au-
thor identification forensics. ACM Sigmod Record,
30(4):55?64.
JoachimDiederich, Jo?rg Kindermann, Edda Leopold, and
Gerhard Paass. 2003. Authorship attribution with
support vector machines. Applied intelligence, 19(1-
2):109?123.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukwac, a very large web-derived corpus of english. In
Proc. of the 4th Web as Corpus Workshop, WAC-4.
Georgia Frantzeskou, Efstathios Stamatatos, Stefanos
Gritzalis, and Carole E Chaski. 2007. Identifying au-
thorship by byte-level n-grams: The source code au-
thor profile (scap) method. Int Journal of Digital Evi-
dence, 6(1):1?18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of Coling
? Volume 2, pages 539?545, Stroudsburg, PA, USA.
Johan F Hoorn, Stefan L Frank, Wojtek Kowalczyk, and
Floor van der Ham. 1999. Neural network identifi-
cation of poets using letter sequences. Literary and
Linguistic Computing, 14(3):311?338.
Shunichi Ishihara. 2011. A forensic authorship clas-
sification in sms messages: A likelihood ratio based
approach using n-gram. In Proc. of the Australasian
Language Technology Association Workshop 2011,
pages 47?56, Canberra, Australia.
Patrick Juola. 2012. Large-scale experiments in author-
ship attribution. English Studies, 93(3):275?283.
Bradley Kjell, W Addison Woods, and Ophir Frieder.
1995. Information retrieval using letter tuples with
neural network and nearest neighbor classifiers. In
IEEE International Conference on Systems, Man and
Cybernetics, volume 2, pages 1222?1226. IEEE.
Bradley Kjell. 1994. Authorship determination using let-
ter pair frequency features with neural network classi-
fiers. Literary and Linguistic Computing, 9(2):119?
124.
Moshe Koppel and Jonathan Schler. 2003. Exploiting
stylistic idiosyncrasies for authorship attribution. In
Proc. of IJCAI?03 Workshop on Computational Ap-
proaches to Style Analysis and Synthesis, volume 69,
page 72.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, KDD ?05, pages 624?628, New York,
NY, USA.
Moshe Koppel, Jonathan Schler, Shlomo Argamon, and
EranMesseri. 2006. Authorship attribution with thou-
sands of candidate authors. In SIGIR, pages 659?660.
Moshe Koppel, Jonathan Schler, and Elisheva Bonchek-
Dokow. 2007. Measuring differentiability: Unmask-
ing pseudonymous authors. JMLR, 8:1261?1276.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2009. Computational methods in authorship attribu-
tion. J. Am. Soc. Inf. Sci. Technol., 60(1):9?26.
Moshe Koppel, Navot Akiva, Idan Dershowitz, and
Nachum Dershowitz. 2011a. Unsupervised decom-
position of a document into authorial components. In
Proc. of ACL-HLT, pages 1356?1364, Portland, Ore-
gon, USA.
Moshe Koppel, Jonathan Schler, and Shlomo Argamon.
2011b. Authorship attribution in the wild. Language
Resources and Evaluation, 45(1):83?94.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
1890
pattern linkage graphs. In Proc. of ACL-HLT, pages
1048?1056, Columbus, Ohio.
Robert Layton, PaulWatters, and Richard Dazeley. 2010.
Authorship attribution for twitter in 140 characters or
less. In Proc. of the 2010 Second Cybercrime and
Trustworthy Computing Workshop, CTC ?10, pages 1?
8, Washington, DC, USA. IEEE Computer Society.
Robert AJ Matthews and Thomas VN Merriam. 1993.
Neural computation in stylometry i: An application to
the works of shakespeare and fletcher. Literary and
Linguistic Computing, 8(4):203?209.
DL Mealand. 1995. Correspondence analysis of luke.
Literary and linguistic computing, 10(3):171?182.
Thomas Corwin Mendenhall. 1887. The characteristic
curves of composition. Science, ns-9(214S):237?246.
George K Mikros and Kostas Perifanos. 2013. Author-
ship attribution in greek tweets using authors multi-
level n-gram profiles. In 2013 AAAI Spring Sympo-
sium Series.
Ashwin Mohan, Ibrahim M Baggili, and Marcus K
Rogers. 2010. Authorship attribution of sms mes-
sages using an n-grams approach. Technical report,
CERIAS Tech Report 2011.
Frederick Mosteller and David Lee Wallace. 1964.
Inference and disputed authorship: The Federalist.
Addison-Wesley.
Fuchun Peng, Dale Schuurmans, and Shaojun Wang.
2004. Augmenting naive bayes classifiers with sta-
tistical language models. Information Retrieval, 7(3-
4):317?345.
Conrad Sanderson and Simon Guenter. 2006. Short text
authorship attribution via sequence kernels, markov
chains and author unmasking: An investigation. In
Proc. of EMNLP, pages 482?491, Sydney, Australia.
Rui Sousa Silva, Gustavo Laboreiro, Lu??s Sarmento, Tim
Grant, Euge?nio Oliveira, and Belinda Maia. 2011.
?twazn me!!! ;(? automatic authorship analysis of
micro-blogging messages. In Proc. of the 16th inter-
national conference on Natural language processing
and information systems, NLDB?11, pages 161?168,
Berlin, Heidelberg. Springer-Verlag.
Thamar Solorio, Sangita Pillay, Sindhu Raghavan, and
Manuel Montes-Gomez. 2011. Modality specific
meta features for authorship attribution in web forum
posts. In Proc. of IJCNLP, pages 156?164, Chiang
Mai, Thailand, November.
Efstathios Stamatatos. 2008. Author identification: Us-
ing text sampling to handle the class imbalance prob-
lem. Inf. Process. Manage., 44(2):790?799.
Efstathios Stamatatos. 2009. A survey of modern au-
thorship attribution methods. Journal of the Ameri-
can Society for Information Science and Technology,
60(3):538?556.
Oren Tsur, Dmitry Davidov, and Ari Rappoport. 2010.
Icwsm?a great catchy name: Semi-supervised recog-
nition of sarcastic sentences in online product reviews.
In Proc. of ICWSM.
Peter Turney. 2008a. A uniform approach to analogies,
synonyms, antonyms, and associations. In Proc. of
Coling, pages 905?912,Manchester, UK, August. Col-
ing 2008 Organizing Committee.
Peter D. Turney. 2008b. The latent relation mapping en-
gine: Algorithm and experiments. Journal of Artificial
Intelligence Research, 33:615?655.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Proc.
of Coling, pages 1?7, Stroudsburg, PA, USA.
George Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: with application
to two cases of disputed authorship. Biometrika, 30(3-
4):363?390.
1891
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 98?107,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bilingual Lexicon Generation Using Non-Aligned Signatures
Daphna Shezaf
Institute of Computer Science
Hebrew University of Jerusalem
daphna.shezaf@mail.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Bilingual lexicons are fundamental re-
sources. Modern automated lexicon gen-
eration methods usually require parallel
corpora, which are not available for most
language pairs. Lexicons can be gener-
ated using non-parallel corpora or a pivot
language, but such lexicons are noisy.
We present an algorithm for generating
a high quality lexicon from a noisy one,
which only requires an independent cor-
pus for each language. Our algorithm in-
troduces non-aligned signatures (NAS), a
cross-lingual word context similarity score
that avoids the over-constrained and inef-
ficient nature of alignment-based methods.
We use NAS to eliminate incorrect transla-
tions from the generated lexicon. We eval-
uate our method by improving the quality
of noisy Spanish-Hebrew lexicons gener-
ated from two pivot English lexicons. Our
algorithm substantially outperforms other
lexicon generation methods.
1 Introduction
Bilingual lexicons are useful for both end users
and computerized language processing tasks.
They provide, for each source language word or
phrase, a set of translations in the target language,
and thus they are a basic component of dictio-
naries, which also include syntactic information,
sense division, usage examples, semantic fields,
usage guidelines, etc.
Traditionally, when bilingual lexicons are not
compiled manually, they are extracted from par-
allel corpora. However, for most language pairs
parallel bilingual corpora either do not exist or are
at best small and unrepresentative of the general
language.
Bilingual lexicons can be generated using non-
parallel corpora or pivot language lexicons (see
Section 2). However, such lexicons are noisy. In
this paper we present a method for generating a
high quality lexicon given such a noisy one. Our
evaluation focuses on the pivot language case.
Pivot language approaches deal with the
scarcity of bilingual data for most language pairs
by relying on the availability of bilingual data for
each of the languages in question with a third,
pivot, language. In practice, this third language
is often English.
A naive method for pivot-based lexicon genera-
tion goes as follows. For each source headword1,
take its translations to the pivot language using the
source-to-pivot lexicon, then for each such transla-
tion take its translations to the target language us-
ing the pivot-to-target lexicon. This method yields
highly noisy (?divergent?) lexicons, because lexi-
cons are generally intransitive. This intransitivity
stems from polysemy in the pivot language that
does not exist in the source language. For ex-
ample, take French-English-Spanish. The English
word spring is the translation of the French word
printemps, but only in the season of year sense.
Further translating spring into Spanish yields both
the correct translation primavera and an incorrect
one, resorte (the elastic object).
To cope with the issue of divergence due to lex-
ical intransitivity, we present an algorithm for as-
sessing the correctness of candidate translations.
The algorithm is quite simple to understand and
to implement and is computationally efficient. In
spite of its simplicity, we are not aware of previous
work applying it to our problem.
The algorithm utilizes two monolingual cor-
pora, comparable in their domain but otherwise
unrelated, in the source and target languages. It
does not need a pivot language corpus. The al-
gorithm comprises two stages: signature genera-
1In this paper we focus on single word head entries.
Multi-word expressions form a major topic in NLP and their
handling is deferred to future work.
98
tion and signature ranking. The signature of word
w is the set of words that co-occur with w most
strongly. While co-occurrence scores are used
to compute signatures, signatures, unlike context
vectors, do not contain the score values. For
each given source headword we compute its sig-
nature and the signatures of all of its candidate
translations. We present the non-aligned signa-
tures (NAS) similarity score for signature and use
it to rank these translations. NAS is based on the
number of headword signature words that may be
translated using the input noisy lexicon into words
in the signature of a candidate translation.
We evaluate our algorithm by generating a
bilingual lexicon for Hebrew and Spanish using
pivot Hebrew-English and English-Spanish lexi-
cons compiled by a professional publishing house.
We show that the algorithm outperforms exist-
ing algorithms for handling divergence induced by
lexical intransitivity.
2 Previous Work
2.1 Parallel Corpora
Parallel corpora are often used to infer word-
oriented machine-readable bilingual lexicons. The
texts are aligned to each other, at chunk- and/or
word-level. Alignment is generally evaluated by
consistency (source words should be translated to
a small number of target words over the entire cor-
pus) and minimal shifting (in each occurrence, the
source should be aligned to a translation nearby).
For a review of such methods see (Lopez, 2008).
The limited availability of parallel corpora of suffi-
cient size for most language pairs restricts the use-
fulness of these methods.
2.2 Pivot Language Without Corpora
2.2.1 Inverse Consultation
Tanaka and Umemura (1994) generated a bilin-
gual lexicon using a pivot language. They ap-
proached lexical intransitivity divergence using
Inverse Consultation (IC). IC examines the inter-
section of two pivot language sets: the set of pivot
translations of a source-language word w, and the
set of pivot translations of each target-language
word that is a candidate for being a translation
to w. IC generally requires that the intersection
set contains at least two words, which are syn-
onyms. For example, the intersection of the En-
glish translations of French printemps and Spanish
resorte contains only a single word, spring. The
intersection for a correct translation pair printemps
and primavera may include two synonym words,
spring and springtime. Variations of this method
were proposed by (Kaji and Aizono, 1996; Bond
et al, 2001; Paik et al, 2004; Ahn and Frampton,
2006).
One weakness of IC is that it relies on pivot lan-
guage synonyms to identify correct translations.
In the above example, if the relatively rare spring-
time had not existed or was missing from the input
lexicons, IC would not have been able to discern
that primavera is a correct translation. This may
result in low recall.
2.2.2 Multiple Pivot Languages
Mausam et al (2009) used many input bilingual
lexicons to create bilingual lexicons for new lan-
guage pairs. They represent the multiple input
lexicons in a single undirected graph, with words
from all the lexicons as nodes. The input lexi-
cons translation pairs define the edges in the graph.
New translation pairs are inferred based on cycles
in the graph, that is, the existence of multiple paths
between two words in different languages.
In a sense, this is a generalization of the pivot
language idea, where multiple pivots are used. In
the example above, if both English and German
are used as pivots, printemps and primavera would
be accepted as correct because they are linked by
both English spring and German Fruehling, while
printemps and resorte are not linked by any Ger-
man pivot. This multiple-pivot idea is similar to
Inverse Consultation in that multiple pivots are re-
quired, but using multiple pivot languages frees it
from the dependency on rich input lexicons that
contain a variety of synonyms. This is replaced,
however, with the problem of coming up with mul-
tiple suitable input lexicons.
2.2.3 Micro-Structure of Dictionary Entries
Dictionaries published by a single publishing
house tend to partition the semantic fields of head-
words in the same way. Thus the first translation
of some English headword in the English-Spanish
and in the English-Hebrew dictionaries would cor-
respond to the same sense of the headword, and
would therefore constitute translations of each
other. The applicability of this method is lim-
ited by the availability of machine-readable dic-
tionaries produced by the same publishing house.
Not surprisingly, this method has been proposed
by lexicographers working in such companies (Sk-
99
oumalova, 2001).
2.3 Cross-lingual Co-occurrences in Lexicon
Construction
Rapp (1999) and Fung (1998) discussed seman-
tic similarity estimation using cross-lingual con-
text vector alignment. Both works rely on a
pre-existing large (16-20K entries), correct, one-
to-one lexicon between the source and target
languages, which is used to align context vec-
tors between languages. The context vector
data was extracted from comparable (monolingual
but domain-related) corpora. Koehn and Knight
(2002) were able to do without the initial large lex-
icon by limiting themselves to related languages
that share a writing system, and using identically-
spelled words as context words. Garera et al
(2009) and Pekar et al (2006) suggested different
methods for improving the context vectors data in
each language before aligning them. Garera et al
(2009) replaced the traditional window-based co-
occurrence counting with dependency-tree based
counting, while Pekar et al (2006) predicted miss-
ing co-occurrence values based on similar words
in the same language. In the latter work, the one-
to-one lexicon assumption was not made: when
a context word had multiple equivalents, it was
mapped into all of them, with the original prob-
ability equally distributed between them.
Pivot Language. Using cross-lingual co-
occurrences to improve a lexicon generated using
a pivot language was suggested by Tanaka and
Iwasaki (1996). Schafer and Yarowsky (2002)
created lexicons between English and a target
local language (e.g. Gujarati) using a related
language (e.g. Hindi) as pivot. An English pivot
lexicon was used in conjunction with pivot-target
cognates. Cross-lingual co-occurrences were used
to remove errors, together with other cues such as
edit distance and Inverse Document Frequencies
(IDF) scores. It appears that this work assumed a
single alignment was possible from English to the
target language.
Kaji et al (2008) used a pivot English lexicon
to generate initial Japanese-Chinese and Chinese-
Japanese lexicons, then used co-occurrences in-
formation, aligned using the initial lexicon, to
identify correct translations. Unlike other works,
which require alignments of pairs (i.e., two co-
occurring words in one language translatable into
two co-occurring words in the other), this method
relies on alignments of 3-word cliques in each
language, every pair of which frequently co-
occurring. This is a relatively rare occurrence,
which may explain the low recall rates of their re-
sults.
3 Algorithm
Our algorithm transforms a noisy lexicon into a
high quality one. As explained above, in this paper
we focus on noisy lexicons generated using pivot
language lexicons. Other methods for obtaining
an initial noisy lexicon could be used as well; their
evaluation is deferred to future work.
In the setting evaluated in this paper, we first
generate an initial noisy lexicon iLex possibly
containing many translation candidates for each
source headword. iLex is computed from two
pivot-language lexicons, and is the only place in
which the algorithm utilizes the pivot language.
Afterwards, for each source headword, we com-
pute its signature and the signatures of each of its
translation candidates. Signature computation uti-
lizes a monolingual corpus to discover the words
that are most strongly related to the word. We now
rank the candidates according to the non-aligned
signatures (NAS) similarity score, which assesses
the similarity between each candidate?s signature
and that of the headword. For each headword,
we select the t translations with the highest NAS
scores as correct translations.
3.1 Input Resources
The resources required by our algorithm as evalu-
ated in this paper are: (a) two bilingual lexicons,
one from the source to the pivot language and the
other from the pivot to the target language. In
principle, these two pivot lexicons can be noisy,
although in our evaluation we use manually com-
piled lexicons; (b) two monolingual corpora, one
for each of the source and target languages. We
have tested the method with corpora of compa-
rable domains, but not covering the same well-
defined subjects (the corpora contain news from
different countries and over non-identical time pe-
riods).
3.2 Initial Lexicon Construction
We create an initial lexicon from the source to the
target language using the pivot language: we look
up each source language word s in the source-
pivot lexicon, and obtain the set Ps of its pivot
100
translations. We then look up each of the mem-
bers of Ps in the pivot-target lexicon, and obtain
a set Ts of candidate target translations. iLex is
therefore a mapping from the set of source head-
words to the set of candidate target translations.
Note that it is possible that not all target lexicon
words appear as translation candidates. To create
a target to source lexicon, we repeat the process
with the directions reversed.
3.3 Signatures
The signature of a word w in a language is the
set of N words most strongly related to w. There
are various possible ways to formalize this notion.
We use a common and simple one, the words hav-
ing the highest tendency to co-occur with w in a
corpus. We count co-occurrences using a sliding
fixed-length window of size k. We compute, for
each pair of words, their Pointwise Mutual Infor-
mation (PMI), that is:
PMI(w1, w2) = log
Pr(w1, w2)
Pr(w1)Pr(w2)
where Pr(w1, w2) is the co-occurrence count, and
Pr(wi) is the total number of appearance of wi
in the corpus (Church and Hanks, 1990). We de-
fine the signature G(w)N,k of w to be the set of N
words with the highest PMI with w.
Note that a word?s signature includes words in
the same language. Therefore, two signatures of
words in different languages cannot be directly
compared; we compare them using a lexicon L as
explained below.
Signature is a function of w parameterized by
N and k. We discuss the selection of these param-
eters in section 4.1.5.
3.4 Non-aligned Signatures (NAS) Similarity
Scoring
The core strength of our method lies in the way
in which we evaluate similarity between words in
the source and target languages. For a lexicon L,
a source word s and a target word t, NASL(s, t)
is defined as the number of words in the signature
G(s)N,k of s that may be translated, using L, to
words in the signature G(t)N,k of t, normalized by
dividing it by N. Formally,
NASL(s, t) =
|{w?G(s)|L(w)?G(t)6=?}|
N
Where L(x) is the set of candidate translations
of x under the lexicon L. Since we use a single
Language Sites Tokens
Hebrew haartz.co.il, ynet.co.il,
nrg.co.il
510M
Spanish elpais.com,
elmundo.com, abc.es
560M
Table 1: Hebrew corpus data.
lexicon, iLex, throughout this work, we usually
omit the L subscript when referring to NAS.
4 Lexicon Generation Experiments
We tested our algorithm by generating bilingual
lexicons for Hebrew and Spanish, using English
as a pivot language. We chose a language pair for
which basically no parallel corpora exist2, and that
do not share ancestry or writing system in a way
that can provide cues for alignment.
We conducted the test twice: once creating
a Hebrew-Spanish lexicon, and once creating a
Spanish-Hebrew one.
4.1 Experimental Setup
4.1.1 Corpora
The Hebrew and Spanish corpora were extracted
from Israeli and Spanish newspaper websites re-
spectively (see table 1 for details). Crawling a
small number of sites allowed us to use special-
tailored software to extract the textual data from
the web pages, thus improving the quality of the
extracted texts. Our two corpora are comparable
in their domains, news and news commentary.
No kind of preprocessing was used for the Span-
ish corpus. For Hebrew, closed-class words that
are attached to the succeeding word (e.g., ?the?,
?and?, ?in?) were segmented using a simple un-
supervised method (Dinur et al, 2009). This
method compares the corpus frequencies of the
non-prefixed form x and the prefixed form wx. If x
is frequent enough, it is assumed to be the correct
form, and all the occurrences of wx are segmented
into two tokens, w x. This method was chosen for
being simple and effective. However, the segmen-
tation it produces is not perfect. It is context insen-
sitive, segmenting all appearances of a token in the
same way, while many wx forms are actually am-
biguous. Even unambiguous token segmentations
may fail when the non-segmented form is very fre-
quent in the domain.
2Old testament corpora are for biblical Hebrew, which is
very different from modern Hebrew.
101
Lexicon # headwords BF
Eng-Spa 55057 2.4
Spa-Eng 44349 2.9
Eng-Heb 48857 2.5
Heb-Eng 33439 3.7
Spa-Heb 34077 12.6
Heb-Spa 27591 14.8
Table 2: Number of words in lexicons, and branch-
ing factors (BF).
Hebrew orthography presents additional diffi-
culties: there are relatively many homographs, and
spelling is not quite standardized. These consid-
erations lead us to believe that our choice of lan-
guage pair is more challenging than, for example,
a pair of European languages.
4.1.2 Lexicons
The source of the Hebrew-English lexicon was the
Babylon on-line dictionary3. For Spanish-English,
we used the union of Babylon with the Oxford
English-Spanish lexicon. Since the corpus was
segmented to words using spaces, lexicon entries
containing spaces were discarded.
Lexicon directionality was ignored. All trans-
lation pairs extracted for Hebrew-Spanish via En-
glish, were also reversed and added to the Spanish-
Hebrew lexicon, and vice-versa. Therefore, every
L1-L2 lexicon we mention is identical to the cor-
responding L2-L1 lexicon in the set of translation
pairs it contains. Our lexicon is thus the ?noisi-
est? that can be generated using a pivot language
and two source-pivot-target lexicons, but it also
provides the most complete candidate set possible.
Ignoring directionality is also in accordance with
the reversibility principle of the lexicographic lit-
erature (Tomaszczyk, 1998).
Table 2 details the sizes and branching factors
(BF) (the average number of translations for head-
word) of the input lexicons, as well as those of the
generated initial noisy lexicon.
4.1.3 Baseline
The performance of our method was compared to
three baselines: Inverse Consultation (IC), average
cosine distance, and average city block distance.
The first is a completely different algorithm, and
the last two are a version of our algorithm in which
3www.babylon.com.
the NAS score is replaced by other scores.
IC (see section 2.2.1) is a corpus-less method.
It ranks t1, t2, ..., the candidate translations of a
source word s, by the size of the intersections of
the sets of pivot translations of ti and s. Note that
IC ranking is a partial order, as the intersection
size may be the same for many candidate transla-
tions. IC is a baseline for our algorithm as a whole.
Cosine and city block distances are widely
used methods for calculating distances of vectors
within the same vector space. They are defined
here as4
Cosine(v, u) = 1?
?
viui??
vi
?
ui
CityBlock(v, u) = ?
?
i
|vi ? ui|
In the case of context vectors, the vector in-
dices, or keys, are words, and their values are co-
occurrence based scores. We used the words in
our signatures as context vector keys, and PMI
scores as values. In this way, the two scores are
?plugged? into our method and serve as baselines
for our NAS similarity score.
Since the context vectors are in different lan-
guages, we had to translate, or align, the baseline
context vectors for the source and target words.
Our initial lexicon is a many-to-many relation, so
multiple alignments were possible; in fact, the
number of possible alignments tends to be very
large5. We therefore generated M random possible
alignments, and used the average distance metric
across these alignments.
4.1.4 Test Sets and Gold Standard
Following other works (e.g. (Rapp, 1999)), and to
simplify the experimental setup, we focused in our
experiments on nouns.
A p-q frequency range in a corpus is the set of
tokens in the places between p and q in the list of
corpus tokens, sorted by frequency from high to
low. Two types of test sets were used. The first
(R1) includes all the singular, correctly segmented
(in Hebrew) nouns among the 500 words in the
1001-1500 frequency range. The 1000 highest-
frequency tokens were discarded, as a large num-
ber of these are utilized as auxiliary syntactic
4We modified the standard cosine and city block metrics
so that for all measures higher values would be better.
5This is another advantage of our NAS score.
102
R1 R2
Precision Recall Precision Recall
NAS 82.1% 100% 56% 100%
Cosine 60.7% 100% 28% 100%
City block 56.3% 100% 32% 100%
IC 55.2% 85.7% 52% 88%
Table 3: Hebrew-Spanish lexicon generation:
highest-ranking translation.
words. This yielded a test set of 112 Hebrew
nouns and 169 Spanish nouns. The second (R2),
contains 25 words for each of the two languages,
obtained by randomly selecting 5 singular cor-
rectly segmented nouns from each of the 5 fre-
quency ranges 1-1000 to 4001-5000.
For each of the test words, the correct transla-
tions were extracted from a modern professional
concise printed Hebrew-Spanish-Hebrew dictio-
nary (Prolog, 2003). This dictionary almost al-
ways provides a single Spanish translation for He-
brew headwords. Spanish headwords had 1.98 He-
brew translations on the average. In both cases
this is a small number of correct translation com-
paring to what we might expect with other evalu-
ation methods; therefore this evaluation amounts
to a relatively high standard of correctness. Our
score comparison experiments (section 5) extend
the evaluation beyond this gold standard.
4.1.5 Parameters
The following parameter values were used. The
window size for co-occurrence counting, k, was 4.
This value was chosen in a small pre-test. Signa-
ture size N was 200 (see Section 6.1). The number
of alignments M for the baseline scores was 100.
The number of translations selected for each head-
word, t, was set to 1 for ease of testing, but see
further notes under results.
4.2 Results
Tables 3 and 4 summarize the results of the
Hebrew-Spanish and Spanish-Hebrew lexicon
generation respectively, for both the R1 and R2
test sets.
In the three co-occurrence based methods, NAS
similarity, cosine distance and and city block dis-
tance, the highest ranking translation was selected.
Recall is always 100% as a translation from the
candidate set is always selected, and all of this set
is valid. Precision is computed as the number of
R1 R2
Precision Recall Precision Recall
NAS 87.6% 100% 80% 100%
Cosine 68% 100% 44% 100%
City block 69.8% 100% 36% 100%
IC 76.4% 100% 48% 92%
Table 4: Spanish-Hebrew Lexicon Generation:
highest-ranking translation.
test words whose selected translation was one of
the translations in the gold standard.
IC translations ranking is a partial order, as usu-
ally many translations are scored equally. When
all translations have the same score, IC is effec-
tively undecided. We calculate recall as the per-
centage of cases in which there was more than one
score rank. A result was counted as precise if any
of the highest-ranking translations was in the gold-
standard, even if other translations were equally
ranked, creating a bias in favor of IC.
In both of the Hebrew-Spanish and the Spanish-
Hebrew cases, our method significantly outper-
formed all baselines in generating a precise lexi-
con on the highest-ranking translations.
All methods performed better in R1 than in
R2, which included also lower-frequency words,
and this was more noticeable with the corpus-
based methods (Hebrew-Spanish) than with IC.
This suggests, not surprisingly, that the perfor-
mance of corpus-based methods is related to the
amount of information in the corpus.
That the results for the Spanish-Hebrew lexi-
con are higher may arise from the difference in the
gold standard. As mentioned, Hebrew words only
had one ?correct? Spanish translation, while Span-
ish had 1.98 correct translations on the average.
If we had used a more comprehensive resource to
test against, the precision of the method would be
higher than shown here.
In translation pairs generation, the results be-
yond the top-ranking pair are also of importance.
Tables 5 and 6 present the accuracy of the first
three translation suggestions, for the three co-
occurrence based scores, calculated for the R1 test
set. IC results are not included, as they are incom-
parable to those of the other methods: IC tends to
score many candidate translations identically, and
in practice, the three highest-scoring sets of trans-
lation candidates contained on average 77% of all
103
1st 2nd 3rd total
NAS 82.1% 6.3% 1.8% 90.2%
Cosine 60.7% 9.8% 2.7% 73.2%
City block 56.3% 4.5% 10.7% 71.4%
Table 5: Hebrew-Spanish lexicon generation: ac-
curacy of 3 best translations for the R1 condition.
The table shows how many of the 2nd and 3rd
translations are correct. Note that NAS is always
a better solution, even though its numbers for 2nd
and 3rd are smaller, because its accumulative per-
centage, shown in the last column, is higher.
1st 2nd 3rd total
NAS 87.6% 77.5% 16% 163.9%
Cosine 68% 66.3% 10.1% 144.4%
City block 69.8% 64.5% 7.7% 142%
Table 6: Spanish-Hebrew lexicon generation: ac-
curacy of 3 best translations for the R1 condition.
The total exceeds 100% because Spanish words
had more than one correct translation. See also
the caption of Table 5.
the candidates, thus necessarily yielding mostly
incorrect translations. Recall was omitted from the
tables as it is always 100%.
For all methods, many of the correct translations
that do not rank first, rank as second or third. For
both languages, NAS ranks highest for total ac-
curacy of the three translations, with considerable
advantage.
5 Score Comparison Experiments
Lexicon generation, as defined in our experiment,
is a relatively high standard for cross-linguistic se-
mantic distance evaluation. This is especially cor-
Heb-Spa Spa-Heb
SCE1 SCE2 SCE1 SCE2
NAS 93.8% 76.2% 94.1% 83.7%
Cosine 74.1% 57.1% 70.7% 63.2%
City block 74.1% 68.3% 78,1% 75.2%
Table 7: Precision of score comparison experi-
ments. The percentage of cases in which each
of the scoring methods was able to successfully
distinguish the correct (SCE1) or possible correct
(SCE2) translation from the random translation.
rect since our gold standard gives only a small set
of translations. The set of possible translations in
iLex tends to include, besides the ?correct? transla-
tion of the gold standard, other translations that are
suitable in certain contexts or are semantically re-
lated. For example, for one Hebrew word, kvuza,
the gold standard translation was grupo (group),
while our method chose equipo (team), which was
at least as plausible given the amount of sports
news in the corpus.
Thus to better compare the capability of NAS to
distinguish correct and incorrect translations with
that of other scores, we performed two more ex-
periments. In the first score comparison experi-
ment (SCE1), we used the two R1 test sets, He-
brew and Spanish, from the lexicon generation test
(section 4.1.4). For each word in the test set, we
used our method to select between one of two
translations: a correct translation, from the gold
standard, and a random translation, chosen ran-
domly among all the nouns similar in frequency
to the correct translation.
The second score comparison experiment
(SCE2) was designed to test the score with a more
extensive test set. For each of the two languages,
we randomly selected 1000 nouns, and used our
method to select between a possibly correct trans-
lation, chosen randomly among the translations
suggested in iLex, and a random translation, cho-
sen randomly among nouns similar in frequency
to the possibly correct translation. This test, while
using a more extensive test set, is less accurate
because it is not guaranteed that any of the input
translations is correct.
In both SCE1 and SCE2, cosine and city block
distance were used as baselines. Inverse Consul-
tation is irrelevant here because it can only score
translation pairs that appear in iLex.
Table 7 presents the results of the two score
comparison experiments, each of them for each of
the translation directions. Recall is by definition
100% and is omitted.
Again, NAS performs better than the baselines
in all cases. With all scores, precision values in
SCE1 are higher than in the lexicon generation
experiment. This is consistent with the expecta-
tion that selection between a correct and a ran-
dom, probably incorrect, translation is easier than
selecting among the translations in iLex. The pre-
cision in SCE2 is lower than that in SCE1. This
may be a result of both translations in SCE2 being
104
Figure 1: NAS values (not algorithm precision) for
various N sizes. NAS is not sensitive to the value
of N (see text).
in some cases incorrect. Yet this may also reflect a
weakness of all three scores with lower-frequency
words, which are represented in the 1000-word
samples but not in the ones used in SCE1.
6 NAS Score Properties
6.1 Signature Size
NAS values are in the range [0, 1]. The values de-
pend on N, the size of the signature used. With an
extremely small N, NAS values would usually be
0, and would tend to be noisy, due to accidental
inclusion of high-frequency or highly ambiguous
words in the signature. As N approaches the size
of the lexicon used for alignment, NAS values ap-
proach 1 for all word pairs.
This suggests that choosing a suitable value of
N is critical for effectively using NAS. Yet an em-
pirical test has shown that NAS may be useful for
a wide range of N values: we computed NAS val-
ues for the correct and random translations used
in the Hebrew-Spanish SCE1 experiment (section
5), using N values between 50 and 2000.
Figure 1 shows the average score values (note
that these are not precision values) for the correct
and random translations across that N range. The
scores for the correct translations are consistently
higher than those of the random translations, even
while there is a discernible decline in the differ-
ence between them. In fact, the precision of the se-
lection between the correct and random translation
is persistent throughout the range. This suggests
that while extreme N values should be avoided, the
selection of N is not a major issue.
6.2 Dependency on Alignment Lexicon
NASL values depend on L, the lexicon in use.
Clearly again, in the extremes, an almost empty
lexicon or a lexicon containing every possible pair
of words (a Cartesian product), this score would
not be useful. In the first case, it would yield 0
for every pair, and in the second, 1. However as
our experiments show, it performed well with real-
world examples of a noisy lexicon, with branching
factors of 12.6 and 14.8 (see table 2).
6.3 Lemmatization
Lemmatization is the process of extracting the
lemmas of words in the corpus. Our experiments
show that good results can be achieved without
lemmatization, at least for nouns in the pair of lan-
guages tested (aside from the simple prefix seg-
mentation we used for Hebrew, see section 4.1.1).
For other language pairs lemmatization may be
needed. In general, correct lemmatization should
improve results, since the signatures would con-
sist of more meaningful information. If automatic
lemmatization introduces noise, it may reduce the
results? quality.
6.4 Alternative Models for Relatedness
Cosine and city block, as well as other related dis-
tance metrics, rely on context vectors. The context
vector of a word w collects words and maps them
to some score of their ?relatedness? to w; in this
case, we used PMI. NAS, in contrast, relies on the
signature, the set of N words most related to w.
That is, it requires a Boolean relatedness indica-
tion, rather than a numeric relatedness score. We
used PMI to generate this Boolean indication, and
naturally, other similar measures could be used as
well. More significantly, it may be possible to use
it with corpus-less sources of ?relatedness?, such
as WordNet or search result snippets.
7 Conclusion
We presented a method to create a high quality
bilingual lexicon given a noisy one. We focused
on the case in which the noisy lexicon is created
using two pivot language lexicons. Our algorithm
uses two unrelated monolingual corpora. At the
heart of our method is the non-aligned signatures
(NAS) context similarity score, used for remov-
ing incorrect translations using cross-lingual co-
occurrences.
105
Words in one language tend to have multiple
translations in another. The common method for
context similarity scoring utilizes some algebraic
distance between context vectors, and requires a
single alignment of context vectors in one lan-
guage into the other. Finding a single correct
alignment is unrealistic even when a perfectly cor-
rect lexicon is available. For example, alignment
forces us to choose one correct translation for each
context word, while in practice a few possible
terms may be used interchangeably in the other
language. In our task, moreover, the lexicon used
for alignment was automatically generated from
pivot language lexicons and was expected to con-
tain errors.
NAS does not depend on finding a single correct
alignment. While it measures how well the sets of
words that tend to co-occur with these two words
align to each other, its strength may lie in bypass-
ing the question of which word in one language
should be aligned to a certain context word in the
other language. Therefore, unlike other scoring
methods, it is not effected by incorrect alignments.
We have shown that NAS outperforms the more
traditional distance metrics, which we adapted to
the many-to-many scenario by amortizing across
multiple alignments. Our results confirm that
alignment is problematic in using co-occurrence
methods across languages, at least in our settings.
NAS constitutes a way to avoid this problem.
While the purpose of this work was to discern
correct translations from incorrect one, it is worth
noting that our method actually ranks translation
correctness. This is a stronger property, which
may render it useful in a wider range of scenarios.
In fact, NAS can be viewed as a general mea-
sure for word similarity between languages. It
would be interesting to further investigate this ob-
servation with other sources of lexicons (e.g., ob-
tained from parallel or comparable corpora) and
for other tasks, such as cross-lingual word sense
disambiguation and information retrieval.
References
Kisuh Ahn and Matthew Frampton. 2006. Automatic
generation of translation dictionaries using interme-
diary languages. In EACL 2006 Workshop on Cross-
Language Knowledge Induction.
Francis Bond, Ruhaida Binti Sulong, Takefumi Ya-
mazaki, and Kentaro Ogura. 2001. Design and con-
struction of a machine-tractable japanese-malay dic-
tionary. In MT Summit VIII: Machine Translation in
the Information Age, Proceedings, pages 53?58.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16:22?29.
Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
Unsupervised concept discovery in hebrew using
simple unsupervised word prefix segmentation for
hebrew and arabic. In EACL 2009 Workshop on
Computational Approaches to Semitic Languages.
Pascale Fung. 1998. A statistical view on bilin-
gual lexicon extraction:from parallel corpora to non-
parallel corpora. In The Third Conference of the As-
sociation for Machine Translation in the Americas.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexi-
con induction from monolingual corpora via depen-
dency contexts and part-of-speech equivalences. In
CoNLL.
Hiroyuki Kaji and Toshiko Aizono. 1996. Extracting
word correspondences from bilingual corpora based
on word co-occurrence information. In COLING.
Hiroyuki Kaji, Shin?ichi Tamamura, and Dashtseren
Erdenebat. 2008. Automatic construction of a
japanese-chinese dictionary via english. In LREC.
Philipp Koehn and Kevin Knight. 2002. Learn-
ing a translation lexicon from monolingual corpora.
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition.
Adam Lopez. 2008. Statistical machine translation.
ACM Computing Surveys, 40(3):1?49.
Mausam, Stephen Soderland, Oren Etzioni, Daniel S.
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and 4th International Joint Con-
ference on Natural Language Processing.
Kyonghee Paik, Satoshi Shirai, and Hiromi Nakaiwa.
2004. Automatic construction of a transfer dictio-
nary considering directionality. In COLING, Multi-
lingual Linguistic Resources Workshop.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20:247 ? 266.
Prolog. 2003. Practical Bilingual Dictionary:
Spanish-Hebew/Hebrew-Spanish. Israel.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In ACL.
106
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Hana Skoumalova. 2001. Bridge dictionaries as
bridges between languages. International Journal
of Corpus Linguistics, 6:95?105.
Kumiko Tanaka and Hideya Iwasaki. 1996. Extraction
of lexical translations from non-aligned corpora. In
Conference on Computational linguistics.
Kumiko Tanaka and Kyoji Umemura. 1994. Construc-
tion of a bilingual dictionary intermediated by a third
language. In Conference on Computational Linguis-
tics.
Jerzy Tomaszczyk. 1998. The bilingual dictionary un-
der review. In ZuriLEX?86.
107
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 226?236,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Fully Unsupervised Core-Adjunct Argument Classification
Omri Abend?
Institute of Computer Science
The Hebrew University
omria01@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
arir@cs.huji.ac.il
Abstract
The core-adjunct argument distinction is a
basic one in the theory of argument struc-
ture. The task of distinguishing between
the two has strong relations to various ba-
sic NLP tasks such as syntactic parsing,
semantic role labeling and subcategoriza-
tion acquisition. This paper presents a
novel unsupervised algorithm for the task
that uses no supervised models, utilizing
instead state-of-the-art syntactic induction
algorithms. This is the first work to tackle
this task in a fully unsupervised scenario.
1 Introduction
The distinction between core arguments (hence-
forth, cores) and adjuncts is included in most the-
ories on argument structure (Dowty, 2000). The
distinction can be viewed syntactically, as one
between obligatory and optional arguments, or
semantically, as one between arguments whose
meanings are predicate dependent and indepen-
dent. The latter (cores) are those whose function in
the described event is to a large extent determined
by the predicate, and are obligatory. Adjuncts are
optional arguments which, like adverbs, modify
the meaning of the described event in a predictable
or predicate-independent manner.
Consider the following examples:
1. The surgeon operated [on his colleague].
2. Ron will drop by [after lunch].
3. Yuri played football [in the park].
The marked argument is a core in 1 and an ad-
junct in 2 and 3. Adjuncts form an independent
semantic unit and their semantic role can often be
inferred independently of the predicate (e.g., [af-
ter lunch] is usually a temporal modifier). Core
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
roles are more predicate-specific, e.g., [on his col-
league] has a different meaning with the verbs ?op-
erate? and ?count?.
Sometimes the same argument plays a different
role in different sentences. In (3), [in the park]
places a well-defined situation (Yuri playing foot-
ball) in a certain location. However, in ?The troops
are based [in the park]?, the same argument is
obligatory, since being based requires a place to
be based in.
Distinguishing between the two argument types
has been discussed extensively in various formu-
lations in the NLP literature, notably in PP attach-
ment, semantic role labeling (SRL) and subcatego-
rization acquisition. However, no work has tack-
led it yet in a fully unsupervised scenario. Unsu-
pervised models reduce reliance on the costly and
error prone manual multi-layer annotation (POS
tagging, parsing, core-adjunct tagging) commonly
used for this task. They also allow to examine the
nature of the distinction and to what extent it is
accounted for in real data in a theory-independent
manner.
In this paper we present a fully unsupervised al-
gorithm for core-adjunct classification. We utilize
leading fully unsupervised grammar induction and
POS induction algorithms. We focus on preposi-
tional arguments, since non-prepositional ones are
generally cores. The algorithm uses three mea-
sures based on different characterizations of the
core-adjunct distinction, and combines them us-
ing an ensemble method followed by self-training.
The measures used are based on selectional prefer-
ence, predicate-slot collocation and argument-slot
collocation.
We evaluate against PropBank (Palmer et al,
2005), obtaining roughly 70% accuracy when
evaluated on the prepositional arguments and
more than 80% for the entire argument set. These
results are substantially better than those obtained
by a non-trivial baseline.
226
Section 2 discusses the core-adjunct distinction.
Section 3 describes the algorithm. Sections 4 and
5 present our experimental setup and results.
2 Core-Adjunct in Previous Work
PropBank. PropBank (PB) (Palmer et al, 2005)
is a widely used corpus, providing SRL annotation
for the entire WSJ Penn Treebank. Its core labels
are predicate specific, while adjunct (or modifiers
under their terminology) labels are shared across
predicates. The adjuncts are subcategorized into
several classes, the most frequent of which are
locative, temporal and manner1.
The organization of PropBank is based on
the notion of diathesis alternations, which are
(roughly) defined to be alternations between two
subcategorization frames that preserve meaning or
change it systematically. The frames in which
each verb appears were collected and sets of al-
ternating frames were defined. Each such set was
assumed to have a unique set of roles, named ?role-
set?. These roles include all roles appearing in any
of the frames, except of those defined as adjuncts.
Adjuncts are defined to be optional arguments
appearing with a wide variety of verbs and frames.
They can be viewed as fixed points with respect to
alternations, i.e., as arguments that do not change
their place or slot when the frame undergoes an
alternation. This follows the notions of optionality
and compositionality that define adjuncts.
Detecting diathesis alternations automatically
is difficult (McCarthy, 2001), requiring an initial
acquisition of a subcategorization lexicon. This
alone is a challenging task tackled in the past us-
ing supervised parsers (see below).
FrameNet. FrameNet (FN) (Baker et al, 1998)
is a large-scale lexicon based on frame semantics.
It takes a different approach from PB to semantic
roles. Like PB, it distinguishes between core and
non-core arguments, but it does so for each and
every frame separately. It does not commit that a
semantic role is consistently tagged as a core or
a non-core across frames. For example, the se-
mantic role ?path? is considered core in the ?Self
Motion? frame, but as non-core in the ?Placing?
frame. Another difference is that FN does not al-
low any type of non-core argument to attach to
a given frame. For instance, while the ?Getting?
1PropBank annotates modals and negation words as mod-
ifiers. Since these are not arguments in the common usage of
the term, we exclude them from the discussion in this paper.
frame allows a ?Duration? non-core argument, the
?Active Perception? frame does not.
PB and FN tend to agree in clear (prototypical)
cases, but to differ in others. For instance, both
schemes would tag ?Yuri played football [in the
park]? as an adjunct and ?The commander placed
a guard [in the park]? as a core. However, in ?He
walked [into his office]?, the marked argument is
tagged as a directional adjunct in PB but as a ?Di-
rection? core in FN.
Under both schemes, non-cores are usually con-
fined to a few specific semantic domains, no-
tably time, place and manner, in contrast to cores
that are not restricted in their scope of applica-
bility. This approach is quite common, e.g., the
COBUILD English grammar (Willis, 2004) cate-
gorizes adjuncts to be of manner, aspect, opinion,
place, time, frequency, duration, degree, extent,
emphasis, focus and probability.
Semantic Role Labeling. Work in SRL does
not tackle the core-adjunct task separately but as
part of general argument classification. Super-
vised approaches obtain an almost perfect score
in distinguishing between the two in an in-domain
scenario. For instance, the confusion matrix in
(Toutanova et al, 2008) indicates that their model
scores 99.5% accuracy on this task. However,
adaptation results are lower, with the best two
models in the CoNLL 2005 shared task (Carreras
and Ma`rquez, 2005) achieving 95.3% (Pradhan et
al., 2008) and 95.6% (Punyakanok et al, 2008) ac-
curacy in an adaptation between the relatively sim-
ilar corpora WSJ and Brown.
Despite the high performance in supervised sce-
narios, tackling the task in an unsupervised man-
ner is not easy. The success of supervised methods
stems from the fact that the predicate-slot com-
bination (slot is represented in this paper by its
preposition) strongly determines whether a given
argument is an adjunct or a core (see Section 3.4).
Supervised models are provided with an anno-
tated corpus from which they can easily learn the
mapping between predicate-slot pairs and their
core/adjunct label. However, induction of the
mapping in an unsupervised manner must be based
on inherent core-adjunct properties. In addition,
supervised models utilize supervised parsers and
POS taggers, while the current state-of-the-art in
unsupervised parsing and POS tagging is consid-
erably worse than their supervised counterparts.
This challenge has some resemblance to un-
227
supervised detection of multiword expressions
(MWEs). An important MWE sub-class is that
of phrasal verbs, which are also characterized by
verb-preposition pairs (Li et al, 2003; Sporleder
and Li, 2009) (see also (Boukobza and Rappoport,
2009)). Both tasks aim to determine semantic
compositionality, which is a highly challenging
task.
Few works addressed unsupervised SRL-related
tasks. The setup of (Grenager and Manning,
2006), who presented a Bayesian Network model
for argument classification, is perhaps closest to
ours. Their work relied on a supervised parser
and a rule-based argument identification (both dur-
ing training and testing). Swier and Stevenson
(2004, 2005), while addressing an unsupervised
SRL task, greatly differ from us as their algorithm
uses the VerbNet (Kipper et al, 2000) verb lex-
icon, in addition to supervised parses. Finally,
Abend et al (2009) tackled the argument identi-
fication task alone and did not perform argument
classification of any sort.
PP attachment. PP attachment is the task of de-
termining whether a prepositional phrase which
immediately follows a noun phrase attaches to the
latter or to the preceding verb. This task?s relation
to the core-adjunct distinction was addressed in
several works. For instance, the results of (Hindle
and Rooth, 1993) indicate that their PP attachment
system works better for cores than for adjuncts.
Merlo and Esteve Ferrer (2006) suggest a sys-
tem that jointly tackles the PP attachment and the
core-adjunct distinction tasks. Unlike in this work,
their classifier requires extensive supervision in-
cluding WordNet, language-specific features and
a supervised parser. Their features are generally
motivated by common linguistic considerations.
Features found adaptable to a completely unsuper-
vised scenario are used in this work as well.
Syntactic Parsing. The core-adjunct distinction
is included in many syntactic annotation schemes.
Although the Penn Treebank does not explicitly
annotate adjuncts and cores, a few works sug-
gested mapping its annotation (including func-
tion tags) to core-adjunct labels. Such a mapping
was presented in (Collins, 1999). In his Model
2, Collins modifies his parser to provide a core-
adjunct prediction, thereby improving its perfor-
mance.
The Combinatory Categorial Grammar (CCG)
formulation models the core-adjunct distinction
explicitly. Therefore, any CCG parser can be used
as a core-adjunct classifier (Hockenmaier, 2003).
Subcategorization Acquisition. This task spec-
ifies for each predicate the number, type and order
of obligatory arguments. Determining the allow-
able subcategorization frames for a given predi-
cate necessarily involves separating its cores from
its allowable adjuncts (which are not framed). No-
table works in the field include (Briscoe and Car-
roll, 1997; Sarkar and Zeman, 2000; Korhonen,
2002). All these works used a parsed corpus in
order to collect, for each predicate, a set of hy-
pothesized subcategorization frames, to be filtered
by hypothesis testing methods.
This line of work differs from ours in a few
aspects. First, all works use manual or super-
vised syntactic annotations, usually including a
POS tagger. Second, the common approach to the
task focuses on syntax and tries to identify the en-
tire frame, rather than to tag each argument sep-
arately. Finally, most works address the task at
the verb type level, trying to detect the allowable
frames for each type. Consequently, the common
evaluation focuses on the quality of the allowable
frames acquired for each verb type, and not on the
classification of specific arguments in a given cor-
pus. Such a token level evaluation was conducted
in a few works (Briscoe and Carroll, 1997; Sarkar
and Zeman, 2000), but often with a small num-
ber of verbs or a small number of frames. A dis-
cussion of the differences between type and token
level evaluation can be found in (Reichart et al,
2010).
The core-adjunct distinction task was tackled in
the context of child language acquisition. Villav-
icencio (2002) developed a classifier based on
preposition selection and frequency information
for modeling the distinction for locative preposi-
tional phrases. Her approach is not entirely corpus
based, as it assumes the input sentences are given
in a basic logical form.
The study of prepositions is a vibrant research
area in NLP. A special issue of Computational Lin-
guistics, which includes an extensive survey of re-
lated work, was recently devoted to the field (Bald-
win et al, 2009).
228
3 Algorithm
We are given a (predicate, argument) pair in a test
sentence, and we need to determine whether the
argument is a core or an adjunct. Test arguments
are assumed to be correctly bracketed. We are al-
lowed to utilize a training corpus of raw text.
3.1 Overview
Our algorithm utilizes statistics based on the
(predicate, slot, argument head) (PSH) joint dis-
tribution (a slot is represented by its preposition).
To estimate this joint distribution, PSH samples
are extracted from the training corpus using unsu-
pervised POS taggers (Clark, 2003; Abend et al,
2010) and an unsupervised parser (Seginer, 2007).
As current performance of unsupervised parsers
for long sentences is low, we use only short sen-
tences (up to 10 words, excluding punctuation).
The length of test sentences is not bounded. Our
results will show that the training data accounts
well for the argument realization phenomena in
the test set, despite the length bound on its sen-
tences. The sample extraction process is detailed
in Section 3.2.
Our approach makes use of both aspects of the
distinction ? obligatoriness and compositionality.
We define three measures, one quantifying the
obligatoriness of the slot, another quantifying the
selectional preference of the verb to the argument
and a third that quantifies the association between
the head word and the slot irrespective of the pred-
icate (Section 3.3).
The measures? predictions are expected to coin-
cide in clear cases, but may be less successful in
others. Therefore, an ensemble-based method is
used to combine the three measures into a single
classifier. This results in a high accuracy classifier
with relatively low coverage. A self-training step
is now performed to increase coverage with only a
minor deterioration in accuracy (Section 3.4).
We focus on prepositional arguments. Non-
prepositional arguments in English tend to be
cores (e.g., in more than 85% of the cases in
PB sections 2?21), while prepositional arguments
tend to be equally divided between cores and ad-
juncts. The difficulty of the task thus lies in the
classification of prepositional arguments.
3.2 Data Collection
The statistical measures used by our classifier
are based on the (predicate, slot, argument head)
(PSH) joint distribution. This section details the
process of extracting samples from this joint dis-
tribution given a raw text corpus.
We start by parsing the corpus using the Seginer
parser (Seginer, 2007). This parser is unique in its
ability to induce a bracketing (unlabeled parsing)
from raw text (without even using POS tags) with
strong results. Its high speed (thousands of words
per second) allows us to use millions of sentences,
a prohibitive number for other parsers.
We continue by tagging the corpus using
Clark?s unsupervised POS tagger (Clark, 2003)
and the unsupervised Prototype Tagger (Abend et
al., 2010)2. The classes corresponding to preposi-
tions and to verbs are manually selected from the
induced clusters3. A preposition is defined to be
any word which is the first word of an argument
and belongs to a prepositions cluster. A verb is
any word belonging to a verb cluster. This manual
selection requires only a minute, since the number
of classes is very small (34 in our experiments).
In addition, knowing what is considered a prepo-
sition is part of the task definition itself.
Argument identification is hard even for super-
vised models and is considerably more so for un-
supervised ones (Abend et al, 2009). We there-
fore confine ourselves to sentences of length not
greater than 10 (excluding punctuation) which
contain a single verb. A sequence of words will
be marked as an argument of the verb if it is a con-
stituent that does not contain the verb (according
to the unsupervised parse tree), whose parent is
an ancestor of the verb. This follows the pruning
heuristic of (Xue and Palmer, 2004) often used by
SRL algorithms.
The corpus is now tagged using an unsupervised
POS tagger. Since the sentences in question are
short, we consider every word which does not be-
long to a closed class cluster as a head word (an
argument can have several head words). A closed
class is a class of function words with relatively
few word types, each of which is very frequent.
Typical examples include determiners, preposi-
tions and conjunctions. A class which is not closed
is open. In this paper, we define closed classes to
be clusters in which the ratio between the number
of word tokens and the number of word types ex-
2Clark?s tagger was replaced by the Prototype Tagger
where the latter gave a significant improvement. See Sec-
tion 4.
3We also explore a scenario in which they are identified
by a supervised tagger. See Section 4.
229
ceeds a threshold T 4.
Using these annotation layers, we traverse the
corpus and extract every (predicate, slot, argument
head) triplet. In case an argument has several head
words, each of them is considered as an inde-
pendent sample. We denote the number of times
that a triplet occurred in the training corpus by
N(p, s, h).
3.3 Collocation Measures
In this section we present the three types of mea-
sures used by the algorithm and the rationale be-
hind each of them. These measures are all based
on the PSH joint distribution.
Given a (predicate, prepositional argument) pair
from the test set, we first tag and parse the argu-
ment using the unsupervised tools above5. Each
word in the argument is now represented by its
word form (without lemmatization), its unsuper-
vised POS tag and its depth in the parse tree of the
argument. The last two will be used to determine
which are the head words of the argument (see be-
low). The head words themselves, once chosen,
are represented by the lemma. We now compute
the following measures.
Selectional Preference (SP). Since the seman-
tics of cores is more predicate dependent than the
semantics of adjuncts, we expect arguments for
which the predicate has a strong preference (in a
specific slot) to be cores.
Selectional preference induction is a well-
established task in NLP. It aims to quantify the
likelihood that a certain argument appears in a
certain slot of a predicate. Several methods have
been suggested (Resnik, 1996; Li and Abe, 1998;
Schulte im Walde et al, 2008).
We use the paradigm of (Erk, 2007). For a given
predicate slot pair (p, s), we define its preference
to the argument head h to be:
SP (p, s, h) =
?
h??Heads
Pr(h?|p, s) ? sim(h, h?)
Pr(h|p, s) = N(p, s, h)?h?N(p, s, h?)
sim(h, h?) is a similarity measure between argu-
ment heads. Heads is the set of all head words.
4We use sections 2?21 of the PTB WSJ for these counts,
containing 0.95M words. Our T was set to 50.
5Note that while current unsupervised parsers have low
performance on long sentences, arguments, even in long sen-
tences, are usually still short enough for them to operate well.
Their average length in the test set is 5.1 words.
This is a natural extension of the naive (and sparse)
maximum likelihood estimator Pr(h|p, s), which
is obtained by taking sim(h, h?) to be 1 if h = h?
and 0 otherwise.
The similarity measure we use is based on the
slot distributions of the arguments. That is, two
arguments are considered similar if they tend to
appear in the same slots. Each head word h is as-
signed a vector where each coordinate corresponds
to a slot s. The value of the coordinate is the num-
ber of times h appeared in s, i.e. ?p?N(p?, s, h)
(p? is summed over all predicates). The similarity
measure between two head words is then defined
as the cosine measure of their vectors.
Since arguments in the test set can be quite long,
not every open class word in the argument is taken
to be a head word. Instead, only those appearing in
the top level (depth = 1) of the argument under its
unsupervised parse tree are taken. In case there are
no such open class words, we take those appearing
in depth 2. The selectional preference of the whole
argument is then defined to be the arithmetic mean
of this measure over all of its head words. If the ar-
gument has no head words under this definition or
if none of the head words appeared in the training
corpus, the selectional preference is undefined.
Predicate-Slot Collocation. Since cores are
obligatory, when a predicate persistently appears
with an argument in a certain slot, the arguments
in this slot tends to be cores. This notion can be
captured by the (predicate, slot) joint distribu-
tion. We use the Pointwise Mutual Information
measure (PMI) to capture the slot and the predi-
cate?s collocation tendency. Let p be a predicate
and s a slot, then:
PS(p, s) = PMI(p, s) = log Pr(p, s)
Pr(s) ? Pr(p) =
= log N(p, s)?p?,s?N(p
?, s?)
?s?N(p, s?)?p?N(p?, s)
Since there is only a meager number of possi-
ble slots (that is, of prepositions), estimating the
(predicate, slot) distribution can be made by the
maximum likelihood estimator with manageable
sparsity.
In order not to bias the counts towards predi-
cates which tend to take more arguments, we de-
fine here N(p, s) to be the number of times the
(p, s) pair occurred in the training corpus, irre-
spective of the number of head words the argu-
ment had (and not e.g., ?hN(p, s, h)). Argu-
230
ments with no prepositions are included in these
counts as well (with s = NULL), so not to bias
against predicates which tend to have less non-
prepositional arguments.
Argument-Slot Collocation. Adjuncts tend to
belong to one of a few specific semantic domains
(see Section 2). Therefore, if an argument tends to
appear in a certain slot in many of its instances, it
is an indication that this argument tends to have a
consistent semantic flavor in most of its instances.
In this case, the argument and the preposition can
be viewed as forming a unit on their own, indepen-
dent of the predicate with which they appear. We
therefore expect such arguments to be adjuncts.
We formalize this notion using the following
measure. Let p, s, h be a predicate, a slot and a
head word respectively. We then use6:
AS(s, h) = 1?Pr(s|h) = 1? ?p?N(p
?, s, h)
?p?,s?N(p?, s?, h)
We select the head words of the argument as
we did with the selectional preference measure.
Again, the AS of the whole argument is defined
to be the arithmetic mean of the measure over all
of its head words.
Thresholding. In order to turn these measures
into classifiers, we set a threshold below which ar-
guments are marked as adjuncts and above which
as cores. In order to avoid tuning a parameter for
each of the measures, we set the threshold as the
median value of this measure in the test set. That
is, we find the threshold which tags half of the ar-
guments as cores and half as adjuncts. This relies
on the prior knowledge that prepositional argu-
ments are roughly equally divided between cores
and adjuncts7.
3.4 Combination Model
The algorithm proceeds to integrate the predic-
tions of the weak classifiers into a single classi-
fier. We use an ensemble method (Breiman, 1996).
Each of the classifiers may either classify an argu-
ment as an adjunct, classify it as a core, or ab-
stain. In order to obtain a high accuracy classifier,
to be used for self-training below, the ensemble
classifier only tags arguments for which none of
6The conditional probability is subtracted from 1 so that
higher values correspond to cores, as with the other measures.
7In case the test data is small, we can use the median value
on the training data instead.
the classifiers abstained, i.e., when sufficient infor-
mation was available to make all three predictions.
The prediction is determined by the majority vote.
The ensemble classifier has high precision but
low coverage. In order to increase its coverage, a
self-training step is performed. We observe that a
predicate and a slot generally determine whether
the argument is a core or an adjunct. For instance,
in our development data, a classifier which assigns
all arguments that share a predicate and a slot their
most common label, yields 94.3% accuracy on the
pairs appearing at least 5 times. This property of
the core-adjunct distinction greatly simplifies the
task for supervised algorithms (see Section 2).
We therefore apply the following procedure: (1)
tag the training data with the ensemble classifier;
(2) for each test sample x, if more than a ratio of ?
of the training samples sharing the same predicate
and slot with x are labeled as cores, tag x as core.
Otherwise, tag x as adjunct.
Test samples which do not share a predicate and
a slot with any training sample are considered out
of coverage. The parameter ? is chosen so half
of the arguments are tagged as cores and half as
adjuncts. In our experiments ? was about 0.25.
4 Experimental Setup
Experiments were conducted in two scenarios. In
the ?SID? (supervised identification of prepositions
and verbs) scenario, a gold standard list of prepo-
sitions was provided. The list was generated by
taking every word tagged by the preposition tag
(?IN?) in at least one of its instances under the
gold standard annotation of the WSJ sections 2?
21. Verbs were identified using MXPOST (Ratna-
parkhi, 1996). Words tagged with any of the verb
tags, except of the auxiliary verbs (?have?, ?be? and
?do?) were considered predicates. This scenario
decouples the accuracy of the algorithm from the
quality of the unsupervised POS tagging.
In the ?Fully Unsupervised? scenario, preposi-
tions and verbs were identified using Clark?s tag-
ger (Clark, 2003). It was asked to produce a tag-
ging into 34 classes. The classes corresponding
to prepositions and to verbs were manually identi-
fied. Prepositions in the test set were detected with
84.2% precision and 91.6% recall.
The prediction of whether a word belongs to an
open class or a closed was based on the output of
the Prototype tagger (Abend et al, 2010). The
Prototype tagger provided significantly more ac-
231
curate predictions in this context than Clark?s.
The 39832 sentences of PropBank?s sections 2?
21 were used as a test set without bounding their
lengths8. Cores were defined to be any argument
bearing the labels ?A0? ? ?A5?, ?C-A0? ? ?C-A5?
or ?R-A0? ? ?R-A5?. Adjuncts were defined to
be arguments bearing the labels ?AM?, ?C-AM? or
?R-AM?. Modals (?AM-MOD?) and negation mod-
ifiers (?AM-NEG?) were omitted since they do not
represent adjuncts.
The test set includes 213473 arguments, 45939
(21.5%) are prepositional. Of the latter, 22442
(48.9%) are cores and 23497 (51.1%) are adjuncts.
The non-prepositional arguments include 145767
(87%) cores and 21767 (13%) adjuncts. The aver-
age number of words per argument is 5.1.
The NANC (Graff, 1995) corpus was used as a
training set. Only sentences of length not greater
than 10 excluding punctuation were used (see Sec-
tion 3.2), totaling 4955181 sentences. 7673878
(5635810) arguments were identified in the ?SID?
(?Fully Unsupervised?) scenario. The average
number of words per argument is 1.6 (1.7).
Since this is the first work to tackle this task
using neither manual nor supervised syntactic an-
notation, there is no previous work to compare
to. However, we do compare against a non-trivial
baseline, which closely follows the rationale of
cores as obligatory arguments.
Our Window Baseline tags a corpus using MX-
POST and computes, for each predicate and
preposition, the ratio between the number of times
that the preposition appeared in a window of W
words after the verb and the total number of
times that the verb appeared. If this number ex-
ceeds a certain threshold ?, all arguments hav-
ing that predicate and preposition are tagged as
cores. Otherwise, they are tagged as adjuncts. We
used 18.7M sentences from NANC of unbounded
length for this baseline. W and ? were fine-tuned
against the test set9.
We also report results for partial versions of
the algorithm, starting with the three measures
used (selectional preference, predicate-slot col-
location and argument-slot collocation). Results
for the ensemble classifier (prior to the bootstrap-
ping stage) are presented in two variants: one
8The first 15K arguments were used for the algorithm?s
development and therefore excluded from the evaluation.
9Their optimal value was found to be W=2, ?=0.03. The
low optimal value of ? is an indication of the noisiness of this
technique.
in which the ensemble is used to tag arguments
for which all three measures give a prediction
(the ?Ensemble(Intersection)? classifier) and one
in which the ensemble tags all arguments for
which at least one classifier gives a prediction (the
?Ensemble(Union)? classifier). For the latter, a tie
is broken in favor of the core label. The ?Ensem-
ble(Union)? classifier is not a part of our model
and is evaluated only as a reference.
In order to provide a broader perspective on the
task, we compare the measures in the basis of our
algorithm to simplified or alternative measures.
We experiment with the following measures:
1. Simple SP ? a selectional preference measure
defined to be Pr(head|slot, predicate).
2. Vast Corpus SP ? similar to ?Simple SP?
but with a much larger corpus. It uses roughly
100M arguments which were extracted from the
web-crawling based corpus of (Gabrilovich and
Markovitch, 2005) and the British National Cor-
pus (Burnard, 2000).
3. Thesaurus SP ? a selectional preference mea-
sure which follows the paradigm of (Erk, 2007)
(Section 3.3) and defines the similarity between
two heads to be the Jaccard affinity between their
two entries in Lin?s automatically compiled the-
saurus (Lin, 1998)10.
4. Pr(slot|predicate) ? an alternative to the used
predicate-slot collocation measure.
5. PMI(slot, head) ? an alternative to the used
argument-slot collocation measure.
6. Head Dependence ? the entropy of the pred-
icate distribution given the slot and the head (fol-
lowing (Merlo and Esteve Ferrer, 2006)):
HD(s, h) = ??pPr(p|s, h) ? log(Pr(p|s, h))
Low entropy implies a core.
For each of the scenarios and the algorithms,
we report accuracy, coverage and effective accu-
racy. Effective accuracy is defined to be the ac-
curacy obtained when all out of coverage argu-
ments are tagged as adjuncts. This procedure al-
ways yields a classifier with 100% coverage and
therefore provides an even ground for comparing
the algorithms? performance.
We see accuracy as important on its own right
since increasing coverage is often straightforward
given easily obtainable larger training corpora.
10Since we aim for a minimally supervised scenario,
we used the proximity-based version of his thesaurus
which does not require parsing as pre-processing.
http://webdocs.cs.ualberta.ca/?lindek/Downloads/sims.lsp.gz
232
Collocation Measures Ensemble + Cov.
Sel. Preference Pred-Slot Arg-Slot Ensemble(I) Ensemble(U) E(I) + ST
SID Scenario Accuracy 65.6 64.5 72.4 74.1 68.7 70.6
Coverage 35.6 77.8 44.7 33.2 88.1 74.2
Eff. Acc. 56.7 64.8 58.8 58.8 67.8 68.4
Fully Unsupervised Accuracy 62.6 61.1 69.4 70.6 64.8 68.8
Scenario Coverage 24.8 59.0 38.7 22.8 74.2 56.9
Eff. Acc. 52.6 57.5 55.8 53.8 61.0 61.4
Table 1: Results for the various models. Accuracy, coverage and effective accuracy are presented in percents. Effective
accuracy is defined to be the accuracy resulting from labeling each out of coverage argument with an adjunct label. The
rows represent the following models (left to right): selectional preference, predicate-slot collocation, argument-slot collocation,
?Ensemble(Intersection)?, ?Ensemble(Union)? and the ?Ensemble(Intersection)? followed by self-training (see Section 3.4). ?En-
semble(Intersection)? obtains the highest accuracy. The ensemble + self-training obtains the highest effective accuracy.
Selectional Preference Measures Pred-Slot Measures Arg-Slot Measures
SP? S. SP V.C. SP Lin SP PS? Pr(s|p) Window AS? PMI(s, h) HD
Acc. 65.6 41.6 44.8 49.9 64.5 58.9 64.1 72.4 67.5 67.4
Cov. 35.6 36.9 45.3 36.7 77.8 77.8 92.6 44.7 44.7 44.7
Eff. Acc. 56.7 48.2 47.7 51.3 64.8 60.5 65.0 58.8 56.6 56.6
Table 2: Comparison of the measures used by our model to alternative measures in the ?SID? scenario. Results are in percents.
The sections of the table are (from left to right): selectional preference measures, predicate-slot measures, argument-slot mea-
sures and head dependence. The measures are (left to right): SP?, Simple SP, Vast Corpus SP, Lin SP, PS?, Pr(slot|predicate),
Window Baseline, AS?, PMI(slot, head) and Head Dependence. The measures marked with ? are the ones used by our model.
See Section 4.
Another reason is that a high accuracy classifier
may provide training data to be used by subse-
quent supervised algorithms.
For completeness, we also provide results for
the entire set of arguments. The great majority of
non-prepositional arguments are cores (87% in the
test set). We therefore tag all non-prepositional as
cores and tag prepositional arguments using our
model. In order to minimize supervision, we dis-
tinguish between the prepositional and the non-
prepositional arguments using Clark?s tagger.
Finally, we experiment on a scenario where
even argument identification on the test set is
not provided, but performed by the algorithm of
(Abend et al, 2009), which uses neither syntactic
nor SRL annotation but does utilize a supervised
POS tagger. We therefore run it in the ?SID? sce-
nario. We apply it to the sentences of length at
most 10 contained in sections 2?21 of PB (11586
arguments in 6007 sentences). Non-prepositional
arguments are invariably tagged as cores and out
of coverage prepositional arguments as adjuncts.
We report labeled and unlabeled recall, preci-
sion and F-scores for this experiment. An un-
labeled match is defined to be an argument that
agrees in its boundaries with a gold standard ar-
gument and a labeled match requires in addition
that the arguments agree in their core/adjunct la-
bel. We also report labeling accuracy which is the
ratio between the number of labeled matches and
the number of unlabeled matches11.
5 Results
Table 1 presents the results of our main experi-
ments. In both scenarios, the most accurate of the
three basic classifiers was the argument-slot col-
location classifier. This is an indication that the
collocation between the argument and the prepo-
sition is more indicative of the core/adjunct label
than the obligatoriness of the slot (as expressed by
the predicate-slot collocation).
Indeed, we can find examples where adjuncts,
although optional, appear very often with a certain
verb. An example is ?meet?, which often takes a
temporal adjunct, as in ?Let?s meet [in July]?. This
is a semantic property of ?meet?, whose syntactic
expression is not obligatory.
All measures suffered from a comparable dete-
rioration of accuracy when moving from the ?SID?
to the ?Fully Unsupervised? scenario. The dete-
rioration in coverage, however, was considerably
lower for the argument-slot collocation.
The ?Ensemble(Intersection)? model in both
cases is more accurate than each of the basic clas-
sifiers alone. This is to be expected as it combines
the predictions of all three. The self-training step
significantly increases the ensemble model?s cov-
11Note that the reported unlabeled scores are slightly lower
than those reported in the 2009 paper, due to the exclusion of
the modals and negation modifiers.
233
Precision Recall F-score lAcc.
Unlabeled 50.7 66.3 57.5 ?
Labeled 42.4 55.4 48.0 83.6
Table 3: Unlabeled and labeled scores for the experi-
ments using the unsupervised argument identification system
of (Abend et al, 2009). Precision, recall, F-score and label-
ing accuracy are given in percents.
erage (with some loss in accuracy), thus obtaining
the highest effective accuracy. It is also more accu-
rate than the simpler classifier ?Ensemble(Union)?
(although the latter?s coverage is higher).
Table 2 presents results for the comparison to
simpler or alternative measures. Results indicate
that the three measures used by our algorithm
(leftmost column in each section) obtain superior
results. The only case in which performance is
comparable is the window baseline compared to
the Pred-Slot measure. However, the baseline?s
score was obtained by using a much larger corpus
and a careful hand-tuning of the parameters12.
The poor performance of Simple SP can be as-
cribed to sparsity. This is demonstrated by the
median value of 0, which this measure obtained
on the test set. Accuracy is only somewhat better
with a much larger corpus (Vast Corpus SP). The
Thesaurus SP most probably failed due to insuffi-
cient coverage, despite its applicability in a similar
supervised task (Zapirain et al, 2009).
The Head Dependence measure achieves a rel-
atively high accuracy of 67.4%. We therefore at-
tempted to incorporate it into our model, but failed
to achieve a significant improvement to the overall
result. We expect a further study of the relations
between the measures will suggest better ways of
combining their predictions.
The obtained effective accuracy for the entire
set of arguments, where the prepositional argu-
ments are automatically identified, was 81.6%.
Table 3 presents results of our experiments with
the unsupervised argument identification model
of (Abend et al, 2009). The unlabeled scores
reflect performance on argument identification
alone, while the labeled scores reflect the joint per-
formance of both the 2009 and our algorithms.
These results, albeit low, are potentially benefi-
cial for unsupervised subcategorization acquisi-
tion. The accuracy of our model on the entire
set (prepositional argument subset) of correctly
identified arguments was 83.6% (71.7%). This is
12We tried about 150 parameter pairs for the baseline. The
average of the five best effective accuracies was 64.3%.
somewhat higher than the score on the entire test
set (?SID? scenario), which was 83.0% (68.4%),
probably due to the bounded length of the test sen-
tences in this case.
6 Conclusion
We presented a fully unsupervised algorithm for
the classification of arguments into cores and ad-
juncts. Since most non-prepositional arguments
are cores, we focused on prepositional arguments,
which are roughly equally divided between cores
and adjuncts. The algorithm computes three sta-
tistical measures and utilizes ensemble-based and
self-training methods to combine their predictions.
The algorithm applies state-of-the-art unsuper-
vised parser and POS tagger to collect statistics
from a large raw text corpus. It obtains an accu-
racy of roughly 70%. We also show that (some-
what surprisingly) an argument-slot collocation
measure gives more accurate predictions than a
predicate-slot collocation measure on this task.
We speculate the reason is that the head word dis-
ambiguates the preposition and that this disam-
biguation generally determines whether a preposi-
tional argument is a core or an adjunct (somewhat
independently of the predicate). This calls for
a future study into the semantics of prepositions
and their relation to the core-adjunct distinction.
In this context two recent projects, The Preposi-
tion Project (Litkowski and Hargraves, 2005) and
PrepNet (Saint-Dizier, 2006), which attempt to
characterize and categorize the complex syntactic
and semantic behavior of prepositions, may be of
relevance.
It is our hope that this work will provide a better
understanding of core-adjunct phenomena. Cur-
rent supervised SRL models tend to perform worse
on adjuncts than on cores (Pradhan et al, 2008;
Toutanova et al, 2008). We believe a better under-
standing of the differences between cores and ad-
juncts may contribute to the development of better
SRL techniques, in both its supervised and unsu-
pervised variants.
References
Omri Abend, Roi Reichart and Ari Rappoport, 2009.
Unsupervised Argument Identification for Semantic
Role Labeling. ACL ?09.
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ?10.
234
Collin F. Baker, Charles J. Fillmore and John B. Lowe,
1998. The Berkeley FrameNet Project. ACL-
COLING ?98.
Timothy Baldwin, Valia Kordoni and Aline Villavicen-
cio, 2009. Prepositions in Applications: A Sur-
vey and Introduction to the Special Issue. Computa-
tional Linguistics, 35(2):119?147.
Ram Boukobza and Ari Rappoport, 2009. Multi-
Word Expression Identification Using Sentence Sur-
face Features. EMNLP ?09.
Leo Breiman, 1996. Bagging Predictors. Machine
Learning, 24(2):123?140.
Ted Briscoe and John Carroll, 1997. Automatic Ex-
traction of Subcategorization from Corpora. Ap-
plied NLP ?97.
Lou Burnard, 2000. User Reference Guide for the
British National Corpus. Technical report, Oxford
University.
Xavier Carreras and Llu?`s Ma`rquez, 2005. Intro-
duction to the CoNLL?2005 Shared Task: Semantic
Role Labeling. CoNLL ?05.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Michael Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
David Dowty, 2000. The Dual Analysis of Adjuncts
and Complements in Categorial Grammar. Modify-
ing Adjuncts, ed. Lang, Maienborn and Fabricius?
Hansen, de Gruyter, 2003.
Katrin Erk, 2007. A Simple, Similarity-based Model
for Selectional Preferences. ACL ?07.
Evgeniy Gabrilovich and Shaul Markovitch, 2005.
Feature Generation for Text Categorization using
World Knowledge. IJCAI ?05.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Trond Grenager and Christopher D. Manning, 2006.
Unsupervised Discovery of a Statistical Verb Lexi-
con. EMNLP ?06.
Donald Hindle and Mats Rooth, 1993. Structural Am-
biguity and Lexical Relations. Computational Lin-
guistics, 19(1):103?120.
Julia Hockenmaier, 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Anna Korhonen, 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Hang Li and Naoki Abe, 1998. Generalizing Case
Frames using a Thesaurus and the MDL Principle.
Computational Linguistics, 24(2):217?244.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang and
Rohini Srihari, 2003. An Expert Lexicon Approach
to Identifying English Phrasal Verbs. ACL ?03.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING?ACL ?98.
Ken Litkowski and Orin Hargraves, 2005. The Prepo-
sition Project. ACL-SIGSEM Workshop on ?The
Linguistic Dimensions of Prepositions and Their
Use in Computational Linguistic Formalisms and
Applications?.
Diana McCarthy, 2001. Lexical Acquisition at the
Syntax-Semantics Interface: Diathesis Alternations,
Subcategorization Frames and Selectional Prefer-
ences. Ph.D. thesis, University of Sussex.
Paula Merlo and Eva Esteve Ferrer, 2006. The No-
tion of Argument in Prepositional Phrase Attach-
ment. Computational Linguistics, 32(3):341?377.
Martha Palmer, Daniel Gildea and Paul Kingsbury,
2005. The Proposition Bank: A Corpus Annotated
with Semantic Roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Wayne Ward and James H. Martin,
2008. Towards Robust Semantic Role Labeling.
Computational Linguistics, 34(2):289?310.
Vasin Punyakanok, Dan Roth and Wen-tau Yih, 2008.
The Importance of Syntactic Parsing and Inference
in Semantic Role Labeling. Computational Linguis-
tics, 34(2):257?287.
Adwait Ratnaparkhi, 1996. Maximum Entropy Part-
Of-Speech Tagger. EMNLP ?96.
Roi Reichart, Omri Abend and Ari Rappoport, 2010.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ?10.
Philip Resnik, 1996. Selectional constraints: An
information-theoretic model and its computational
realization. Cognition, 61:127?159.
Patrick Saint-Dizier, 2006. PrepNet: A Multilingual
Lexical Description of Prepositions. LREC ?06.
Anoop Sarkar and Daniel Zeman, 2000. Automatic
Extraction of Subcategorization Frames for Czech.
COLING ?00.
Sabine Schulte im Walde, Christian Hying, Christian
Scheible and Helmut Schmid, 2008. Combining
EM Training and the MDL Principle for an Auto-
matic Verb Classification Incorporating Selectional
Preferences. ACL ?08.
235
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Caroline Sporleder and Linlin Li, 2009. Unsupervised
Recognition of Literal and Non-Literal Use of Id-
iomatic Expressions. EACL ?09.
Robert S. Swier and Suzanne Stevenson, 2004. Unsu-
pervised Semantic Role Labeling. EMNLP ?04.
Robert S. Swier and Suzanne Stevenson, 2005. Ex-
ploiting a Verb Lexicon in Automatic Semantic Role
Labelling. EMNLP ?05.
Kristina Toutanova, Aria Haghighi and Christopher D.
Manning, 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2):161?191.
Aline Villavicencio, 2002. Learning to Distinguish PP
Arguments from Adjuncts. CoNLL ?02.
Dave Willis, 2004. Collins Cobuild Intermedia En-
glish Grammar, Second Edition. HarperCollins Pub-
lishers.
Nianwen Xue and Martha Palmer, 2004. Calibrating
Features for Semantic Role Labeling. EMNLP ?04.
Ben?at Zapirain, Eneko Agirre and Llu??s Ma`rquez,
2009. Generalizing over Lexical Features: Selec-
tional Preferences for Semantic Role Classification.
ACL ?09, short paper.
236
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Improved Unsupervised POS Induction through Prototype Discovery
Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science, 2ICNC
Hebrew University of Jerusalem
{omria01|roiri|arir}@cs.huji.ac.il
Abstract
We present a novel fully unsupervised al-
gorithm for POS induction from plain text,
motivated by the cognitive notion of proto-
types. The algorithm first identifies land-
mark clusters of words, serving as the
cores of the induced POS categories. The
rest of the words are subsequently mapped
to these clusters. We utilize morpho-
logical and distributional representations
computed in a fully unsupervised manner.
We evaluate our algorithm on English and
German, achieving the best reported re-
sults for this task.
1 Introduction
Part-of-speech (POS) tagging is a fundamental
NLP task, used by a wide variety of applications.
However, there is no single standard POS tag-
ging scheme, even for English. Schemes vary
significantly across corpora and even more so
across languages, creating difficulties in using
POS tags across domains and for multi-lingual
systems (Jiang et al, 2009). Automatic induction
of POS tags from plain text can greatly alleviate
this problem, as well as eliminate the efforts in-
curred by manual annotations. It is also a problem
of great theoretical interest. Consequently, POS
induction is a vibrant research area (see Section 2).
In this paper we present an algorithm based
on the theory of prototypes (Taylor, 2003), which
posits that some members in cognitive categories
are more central than others. These practically de-
fine the category, while the membership of other
elements is based on their association with the
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
central members. Our algorithm first clusters
words based on a fine morphological representa-
tion. It then clusters the most frequent words,
defining landmark clusters which constitute the
cores of the categories. Finally, it maps the rest
of the words to these categories. The last two
stages utilize a distributional representation that
has been shown to be effective for unsupervised
parsing (Seginer, 2007).
We evaluated the algorithm in both English and
German, using four different mapping-based and
information theoretic clustering evaluation mea-
sures. The results obtained are generally better
than all existing POS induction algorithms.
Section 2 reviews related work. Sections 3 and
4 detail the algorithm. Sections 5, 6 and 7 describe
the evaluation, experimental setup and results.
2 Related Work
Unsupervised and semi-supervised POS tagging
have been tackled using a variety of methods.
Schu?tze (1995) applied latent semantic analysis.
The best reported results (when taking into ac-
count all evaluation measures, see Section 5) are
given by (Clark, 2003), which combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al, 1992). Clark?s tagger is very sen-
sitive to its initialization. Reichart et al (2010b)
propose a method to identify the high quality runs
of this algorithm. In this paper, we show that
our algorithm outperforms not only Clark?s mean
performance, but often its best among 100 runs.
Most research views the task as a sequential la-
beling problem, using HMMs (Merialdo, 1994;
Banko and Moore, 2004; Wang and Schuurmans,
2005) and discriminative models (Smith and Eis-
ner, 2005; Haghighi and Klein, 2006). Several
1298
techniques were proposed to improve the HMM
model. A Bayesian approach was employed by
(Goldwater and Griffiths, 2007; Johnson, 2007;
Gao and Johnson, 2008). Van Gael et al (2009)
used the infinite HMM with non-parametric pri-
ors. Grac?a et al (2009) biased the model to induce
a small number of possible tags for each word.
The idea of utilizing seeds and expanding them
to less reliable data has been used in several pa-
pers. Haghighi and Klein (2006) use POS ?pro-
totypes? that are manually provided and tailored
to a particular POS tag set of a corpus. Fre-
itag (2004) and Biemann (2006) induce an ini-
tial clustering and use it to train an HMM model.
Dasgupta and Ng (2007) generate morphological
clusters and use them to bootstrap a distributional
model. Goldberg et al (2008) use linguistic con-
siderations for choosing a good starting point for
the EM algorithm. Zhao and Marcus (2009) ex-
pand a partial dictionary and use it to learn dis-
ambiguation rules. Their evaluation is only at the
type level and only for half of the words. Ravi
and Knight (2009) use a dictionary and an MDL-
inspired modification to the EM algorithm.
Many of these works use a dictionary provid-
ing allowable tags for each or some of the words.
While this scenario might reduce human annota-
tion efforts, it does not induce a tagging scheme
but remains tied to an existing one. It is further
criticized in (Goldwater and Griffiths, 2007).
Morphological representation. Many POS in-
duction models utilize morphology to some ex-
tent. Some use simplistic representations of termi-
nal letter sequences (e.g., (Smith and Eisner, 2005;
Haghighi and Klein, 2006)). Clark (2003) models
the entire letter sequence as an HMM and uses it
to define a morphological prior. Dasgupta and Ng
(2007) use the output of the Morfessor segmenta-
tion algorithm for their morphological representa-
tion. Morfessor (Creutz and Lagus, 2005), which
we use here as well, is an unsupervised algorithm
that segments words and classifies each segment
as being a stem or an affix. It has been tested on
several languages with strong results.
Our work has several unique aspects. First,
our clustering method discovers prototypes in a
fully unsupervised manner, mapping the rest of
the words according to their association with the
prototypes. Second, we use a distributional repre-
sentation which has been shown to be effective for
unsupervised parsing (Seginer, 2007). Third, we
use a morphological representation based on sig-
natures, which are sets of affixes that represent a
family of words sharing an inflectional or deriva-
tional morphology (Goldsmith, 2001).
3 Distributional Algorithm
Our algorithm is given a plain text corpus and op-
tionally a desired number of clusters k. Its output
is a partitioning of words into clusters. The al-
gorithm utilizes two representations, distributional
and morphological. Although eventually the latter
is used before the former, for clarity of presenta-
tion we begin by detailing the base distributional
algorithm. In the next section we describe the mor-
phological representation and its integration into
the base algorithm.
Overview. The algorithm consists of two main
stages: landmark clusters discovery, and word
mapping. For the former, we first compute a dis-
tributional representation for each word. We then
cluster the coordinates corresponding to high fre-
quency words. Finally, we define landmark clus-
ters. In the word mapping stage we map each word
to the most similar landmark cluster.
The rationale behind using only the high fre-
quency words in the first stage is twofold. First,
prototypical members of a category are frequent
(Taylor, 2003), and therefore we can expect the
salient POS tags to be represented in this small
subset. Second, higher frequency implies more re-
liable statistics. Since this stage determines the
cores of all resulting clusters, it should be as accu-
rate as possible.
Distributional representation. We use a sim-
plified form of the elegant representation of lexi-
cal entries used by the Seginer unsupervised parser
(Seginer, 2007). Since a POS tag reflects the
grammatical role of the word and since this rep-
resentation is effective to parsing, we were moti-
vated to apply it to the present task.
Let W be the set of word types in the corpus.
The right context entry of a word x ? W is a pair
of mappings r intx : W ? [0, 1] and r adjx :
W ? [0, 1]. For each w ? W , r adjx(w) is an
adjacency score of w to x, reflecting w?s tendency
to appear on the right hand side of x.
For each w ? W , r intx(w) is an interchange-
ability score of x with w, reflecting the tendency
of w to appear to the left of words that tend to ap-
pear to the right of x. This can be viewed as a
1299
similarity measure between words with respect to
their right context. The higher the scores the more
the words tend to be adjacent/interchangeable.
Left context parameters l intx and l adjx are
defined analogously.
There are important subtleties in these defini-
tions. First, for two words x,w ? W , r adjx(w)
is generally different from l adjw(x). For exam-
ple, if w is a high frequency word and x is a low
frequency word, it is likely that w appears many
times to the right of x, yielding a high r adjx(w),
but that x appears only a few times to the left of w
yielding a low l adjw(x). Second, from the defi-
nition of r intx(w) and r intw(x), it is clear that
they need not be equal.
These functions are computed incrementally by
a bootstrapping process. We initialize all map-
pings to be identically 0. We iterate over the words
in the training corpus. For every word instance x,
we take the word immediately to its right y and
update x?s right context using y?s left context:
?w ? W : r intx(w) +=
l adjy(w)
N(y)
?w ? W : r adjx(w) +=
{
1 w = y
l inty(w)
N(y) w 6= y
The division by N(y) (the number of times y
appears in the corpus before the update) is done in
order not to give a disproportional weight to high
frequency words. Also, r intx(w) and r adjx(w)
might become larger than 1. We therefore nor-
malize them after all updates are performed by the
number of occurrences of x in the corpus.
We update l intx and l adjx analogously using
the word z immediately to the left of x. The up-
dates of the left and right functions are done in
parallel.
We define the distributional representation of a
word type x to be a 4|W | + 2 dimensional vector
vx. Each word w yields four coordinates, one for
each direction (left/right) and one for each map-
ping type (int/adj). Two additional coordinates
represent the frequency in which the word appears
to the left and to the right of a stopping punc-
tuation. Of the 4|W | coordinates corresponding
to words, we allow only 2n to be non-zero: the
n top scoring among the right side coordinates
(those of r intx and r adjx), and the n top scoring
among the left side coordinates (those of l intx
and l adjx). We used n = 50.
The distance between two words is defined to
be one minus the cosine of the angle between their
representation vectors.
Coordinate clustering. Each of our landmark
clusters will correspond to a set of high frequency
words (HFWs). The number of HFWs is much
larger than the number of expected POS tags.
Hence we should cluster HFWs. Our algorithm
does that by unifying some of the non-zero coordi-
nates corresponding to HFWs in the distributional
representation defined above.
We extract the words that appear more than N
times per million1 and apply the following proce-
dure I times (5 in our experiments).
We run average link clustering with a threshold
? (AVGLINK?, (Jain et al, 1999)) on these words,
in each iteration initializing every HFW to have
its own cluster. AVGLINK? means running the av-
erage link algorithm until the two closest clusters
have a distance larger than ?. We then use the in-
duced clustering to update the distributional rep-
resentation, by collapsing all coordinates corre-
sponding to words appearing in the same cluster
into a single coordinate whose value is the sum
of the collapsed coordinates? values. In order to
produce a conservative (fine) clustering, we used a
relatively low ? value of 0.25.
Note that the AVGLINK? initialization in each
of the I iterations assigns each HFW to a sepa-
rate cluster. The iterations differ in the distribu-
tional representation of the HFWs, resulting from
the previous iterations.
In our English experiments, this process re-
duced the dimension of the HFWs set (the num-
ber of coordinates that are non-zero in at least one
of the HFWs) from 14365 to 10722. The aver-
age number of non-zero coordinates per word de-
creased from 102 to 55.
Since all eventual POS categories correspond to
clusters produced at this stage, to reduce noise we
delete clusters of less than five elements.
Landmark detection. We define landmark clus-
ters using the clustering obtained in the final iter-
ation of the coordinate clustering stage. However,
the number of clusters might be greater than the
desired number k, which is an optional parame-
ter of the algorithm. In this case we select a sub-
set of k clusters that best covers the HFW space.
We use the following heuristic. We start from the
most frequent cluster, and greedily select the clus-
1We used N = 100, yielding 1242 words for English and
613 words for German.
1300
ter farthest from the clusters already selected. The
distance between two clusters is defined to be the
average distance between their members. A clus-
ter?s distance from a set of clusters is defined to
be its minimal distance from the clusters in the
set. The final set of clusters {L1, ..., Lk} and their
members are referred to as landmark clusters and
prototypes, respectively.
Mapping all words. Each word w ? W is as-
signed the cluster Li that contains its nearest pro-
totype:
d(w,Li) = minx?Li{1 ? cos(vw, vx)}
Map(w) = argminLi{d(w,Li)}
Words that appear less than 5 times are consid-
ered as unknown words. We consider two schemes
for handling unknown words. One randomly maps
each such word to a cluster, using a probabil-
ity proportional to the number of unique known
words already assigned to that cluster. However,
when the number k of landmark clusters is rela-
tively large, it is beneficial to assign all unknown
words to a separate new cluster (after running the
algorithm with k? 1). In our experiments, we use
the first option when k is below some threshold
(we used 15), otherwise we use the second.
4 Morphological Model
The morphological model generates another word
clustering, based on the notion of a signature.
This clustering is integrated with the distributional
model as described below.
4.1 Morphological Representation
We use the Morfessor (Creutz and Lagus, 2005)
word segmentation algorithm. First, all words in
the corpus are segmented. Then, for each stem,
the set of all affixes with which it appears (its sig-
nature, (Goldsmith, 2001)) is collected. The mor-
phological representation of a word type is then
defined to be its stem?s signature in conjunction
with its specific affixes2 (See Figure 1).
We now collect all words having the same rep-
resentation. For instance, if the words joined and
painted are found to have the same signature, they
would share the same cluster since both have the
affix ? ed?. The word joins does not share the same
cluster with them since it has a different affix, ? s?.
This results in coarse-grained clusters exclusively
defined according to morphology.
2A word may contain more than a single affix.
Types join joins joined joining
Stem join join join join
Affixes ? s ed ing
Signature {?, ed, s, ing}
Figure 1: An example for a morphological representation,
defined to be the conjunction of its affix(es) with the stem?s
signature.
In addition, we incorporate capitalization infor-
mation into the model, by constraining all words
that appear capitalized in more than half of their
instances to belong to a separate cluster, regard-
less of their morphological representation. The
motivation for doing so is practical: capitalization
is used in many languages to mark grammatical
categories. For instance, in English capitalization
marks the category of proper names and in Ger-
man it marks the noun category . We report En-
glish results both with and without this modifica-
tion.
Words that contain non-alphanumeric charac-
ters are represented as the sequence of the non-
alphanumeric characters they include, e.g., ?vis-a`-
vis? is represented as (?-?, ?-?). We do not as-
sign a morphological representation to words in-
cluding more than one stem (like weatherman), to
words that have a null affix (i.e., where the word
is identical to its stem) and to words whose stem
is not shared by any other word (signature of size
1). Words that were not assigned a morphologi-
cal representation are included as singletons in the
morphological clustering.
4.2 Distributional-Morphological Algorithm
We detail the modifications made to our base
distributional algorithm given the morphological
clustering defined above.
Coordinate clustering and landmarks. We
constrain AVGLINK? to begin by forming links be-
tween words appearing in the same morphologi-
cal cluster. Only when the distance between the
two closest clusters gets above ? we remove this
constraint and proceed as before. This is equiv-
alent to performing AVGLINK? separately within
each morphological cluster and then using the re-
sult as an initial condition for an AVGLINK? coor-
dinate clustering. The modified algorithm in this
stage is otherwise identical to the distributional al-
gorithm.
Word mapping. In this stage words that are not
prototypes are mapped to one of the landmark
1301
clusters. A reasonable strategy would be to map
all words sharing a morphological cluster as a sin-
gle unit. However, these clusters are too coarse-
grained. We therefore begin by partitioning the
morphological clusters into sub-clusters according
to their distributional behavior. We do so by apply-
ing AVGLINK? (the same as AVGLINK? but with a
different parameter) to each morphological clus-
ter. Since our goal is cluster refinement, we use a
? that is considerably higher than ? (0.9).
We then find the closest prototype to each such
sub-cluster (averaging the distance across all of
the latter?s members) and map it as a single unit
to the cluster containing that prototype.
5 Clustering Evaluation
We evaluate the clustering produced by our algo-
rithm using an external quality measure: we take
a corpus tagged by gold standard tags, tag it using
the induced tags, and compare the two taggings.
There is no single accepted measure quantifying
the similarity between two taggings. In order to
be as thorough as possible, we report results using
four known measures, two mapping-based mea-
sures and two information theoretic ones.
Mapping-based measures. The induced clus-
ters have arbitrary names. We define two map-
ping schemes between them and the gold clus-
ters. After the induced clusters are mapped, we
can compute a derived accuracy. The Many-to-1
measure finds the mapping between the gold stan-
dard clusters and the induced clusters which max-
imizes accuracy, allowing several induced clusters
to be mapped to the same gold standard cluster.
The 1-to-1 measure finds the mapping between
the induced and gold standard clusters which max-
imizes accuracy such that no two induced clus-
ters are mapped to the same gold cluster. Com-
puting this mapping is equivalent to finding the
maximal weighted matching in a bipartite graph,
whose weights are given by the intersection sizes
between matched classes/clusters. As in (Reichart
and Rappoport, 2008), we use the Kuhn-Munkres
algorithm (Kuhn, 1955; Munkres, 1957) to solve
this problem.
Information theoretic measures. These are
based on the observation that a good clustering re-
duces the uncertainty of the gold tag given the in-
duced cluster, and vice-versa. Several such mea-
sures exist; we use V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009),
VI?s (Meila, 2007) normalized version.
6 Experimental Setup
Since a goal of unsupervised POS tagging is in-
ducing an annotation scheme, comparison to an
existing scheme is problematic. To address this
problem we compare to three different schemes
in two languages. In addition, the two English
schemes we compare with were designed to tag
corpora contained in our training set, and have
been widely and successfully used with these cor-
pora by a large number of applications.
Our algorithm was run with the exact same pa-
rameters on both languages: N = 100 (high fre-
quency threshold), n = 50 (the parameter that
determines the effective number of coordinates),
? = 0.25 (cluster separation during landmark
cluster generation), ? = 0.9 (cluster separation
during refinement of morphological clusters).
The algorithm we compare with in most detail
is (Clark, 2003), which reports the best current
results for this problem (see Section 7). Since
Clark?s algorithm is sensitive to its initialization,
we ran it a 100 times and report its average and
standard deviation in each of the four measures.
In addition, we report the percentile in which our
result falls with respect to these 100 runs.
Punctuation marks are very frequent in corpora
and are easy to cluster. As a result, including them
in the evaluation greatly inflates the scores. For
this reason we do not assign a cluster to punctua-
tion marks and we report results using this policy,
which we recommend for future work. However,
to be able to directly compare with previous work,
we also report results for the full POS tag set.
We do so by assigning a singleton cluster to each
punctuation mark (in addition to the k required
clusters). This simple heuristic yields very high
performance on punctuation, scoring (when all
other words are assumed perfect tagging) 99.6%
(99.1%) 1-to-1 accuracy when evaluated against
the English fine (coarse) POS tag sets, and 97.2%
when evaluated against the German POS tag set.
For English, we trained our model on the
39832 sentences which constitute sections 2-21 of
the PTB-WSJ and on the 500K sentences from
the NYT section of the NANC newswire corpus
(Graff, 1995). We report results on the WSJ part
of our data, which includes 950028 words tokens
in 44389 types. Of the tokens, 832629 (87.6%)
1302
English Fine k=13 Coarse k=13 Fine k=34
Prototype Clark Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? % Tagger ? ? %
Many?to?1 61.0 55.1 1.6 100 70.0 66.9 2.1 94 71.6 69.8 1.5 90
55.5 48.8 1.8 100 66.1 62.6 2.3 94 67.5 65.5 1.7 90
1?to?1 60.0 52.2 1.9 100 58.1 49.4 2.9 100 63.5 54.5 1.6 100
54.9 46.0 2.2 100 53.7 43.8 3.3 100 58.8 48.5 1.8 100
NVI 0.652 0.773 0.027 100 0.841 0.972 0.036 100 0.663 0.725 0.018 100
0.795 0.943 0.033 100 1.052 1.221 0.046 100 0.809 0.885 0.022 100
V 0.636 0.581 0.015 100 0.590 0.543 0.018 100 0.677 0.659 0.008 100
0.542 0.478 0.019 100 0.484 0.429 0.023 100 0.608 0.588 0.010 98
German k=17 k=26
Prototype Clark Prototype Clark
Tagger ? ? % Tagger ? ? %
Many?to-1 64.6 64.7 1.2 41 68.2 67.8 1.0 60
58.9 59.1 1.4 40 63.2 62.8 1.2 60
1?to?1 53.7 52.0 1.8 77 56.0 52.0 2.1 99
48.0 46.0 2.3 78 50.7 45.9 2.6 99
NVI 0.667 0.675 0.019 66 0.640 0.682 0.019 100
0.819 0.829 0.025 66 0.785 0.839 0.025 100
V 0.646 0.645 0.010 50 0.675 0.657 0.008 100
0.552 0.553 0.013 48 0.596 0.574 0.010 100
Table 1: Top: English. Bottom: German. Results are reported for our model (Prototype Tagger), Clark?s average score (?),
Clark?s standard deviation (?) and the fraction of Clark?s results that scored worse than our model (%). For the mapping based
measures, results are accuracy percentage. For V ? [0, 1], higher is better. For high quality output, NV I ? [0, 1] as well, and
lower is better. In each entry, the top number indicates the score when including punctuation and the bottom number the score
when excluding it. In English, our results are always better than Clark?s. In German, they are almost always better.
are not punctuation. The percentage of unknown
words (those appearing less than five times) is
1.6%. There are 45 clusters in this annotation
scheme, 34 of which are not punctuation.
We ran each algorithm both with k=13 and
k=34 (the number of desired clusters). We com-
pare the output to two annotation schemes: the fine
grained PTB WSJ scheme, and the coarse grained
tags defined in (Smith and Eisner, 2005). The
output of the k=13 run is evaluated both against
the coarse POS tag annotation (the ?Coarse k=13?
scenario) and against the full PTB-WSJ annotation
scheme (the ?Fine k=13? scenario). The k=34 run
is evaluated against the full PTB-WSJ annotation
scheme (the ?Fine k=34? scenario).
The POS cluster frequency distribution tends to
be skewed: each of the 13 most frequent clusters
in the PTB-WSJ cover more than 2.5% of the to-
kens (excluding punctuation) and together 86.3%
of them. We therefore chose k=13, since it is both
the number of coarse POS tags (excluding punctu-
ation) as well as the number of frequent POS tags
in the PTB-WSJ annotation scheme. We chose
k=34 in order to evaluate against the full 34 tags
PTB-WSJ annotation scheme (excluding punctua-
tion) using the same number of clusters.
For German, we trained our model on the 20296
sentences of the NEGRA corpus (Brants, 1997)
and on the first 450K sentences of the DeWAC
corpus (Baroni et al, 2009). DeWAC is a cor-
pus extracted by web crawling and is therefore
out of domain. We report results on the NEGRA
part, which includes 346320 word tokens of 49402
types. Of the tokens, 289268 (83.5%) are not
punctuation. The percentage of unknown words
(those appearing less than five times) is 8.1%.
There are 62 clusters in this annotation scheme,
51 of which are not punctuation.
We ran the algorithms with k=17 and k=26.
k=26 was chosen since it is the number of clus-
ters that cover each more than 0.5% of the NE-
GRA tokens, and in total cover 96% of the (non-
punctuation) tokens. In order to test our algo-
rithm in another scenario, we conducted experi-
ments with k=17 as well, which covers 89.9% of
the tokens. All outputs are compared against NE-
GRA?s gold standard scheme.
We do not report results for k=51 (where the
number of gold clusters is the same as the number
of induced clusters), since our algorithm produced
only 42 clusters in the landmark detection stage.
We could of course have modified the parame-
ters to allow our algorithm to produce 51 clusters.
However, we wanted to use the exact same param-
eters as those used for the English experiments to
minimize the issue of parameter tuning.
In addition to the comparisons described above,
we present results of experiments (in the ?Fine
1303
B B+M B+C F(I=1) F
M-to-1 53.3 54.8 58.2 57.3 61.0
1-to-1 50.2 51.7 55.1 54.8 60.0
NVI 0.782 0.720 0.710 0.742 0.652
V 0.569 0.598 0.615 0.597 0.636
Table 2: A comparison of partial versions of the model in
the ?Fine k=13? WSJ scenario. M-to-1 and 1-to-1 results are
reported in accuracy percentage. Lower NVI is better. B is the
strictly distributional algorithm, B+M adds the morphologi-
cal model, B+C adds capitalization to B, F(I=1) consists of
all components, where only one iteration of coordinate clus-
tering is performed, and F is the full model.
M-to-1 1-to-1 V VI
Prototype 71.6 63.5 0.677 2.00
Clark 69.8 54.5 0.659 2.18
HK ? 41.3 ? ?
J 43?62 37?47 ? 4.23?5.74
GG ? ? ? 2.8
GJ ? 40?49.9 ? 4.03?4.47
VG ? ? 0.54-0.59 2.5?2.9
GGTP-45 65.4 44.5 ? ?
GGTP-17 70.2 49.5 ? ?
Table 4: Comparison of our algorithms with the recent fully
unsupervised POS taggers for which results are reported. The
models differ in the annotation scheme, the corpus size and
the number of induced clusters (k) that they used. HK:
(Haghighi and Klein, 2006), 193K tokens, fine tags, k=45.
GG: (Goldwater and Griffiths, 2007), 24K tokens, coarse
tags, k=17. J : (Johnson, 2007), 1.17M tokens, fine tags,
k=25?50. GJ: (Gao and Johnson, 2008), 1.17M tokens, fine
tags, k=50. VG: (Van Gael et al, 2009), 1.17M tokens, fine
tags, k=47?192. GGTP-45: (Grac?a et al, 2009), 1.17M to-
kens, fine tags, k=45. GGTP-17: (Grac?a et al, 2009), 1.17M
tokens, coarse tags, k=17. Lower VI values indicate better
clustering. VI is computed using e as the base of the loga-
rithm. Our algorithm gives the best results.
k=13? scenario) that quantify the contribution of
each component of the algorithm. We ran the base
distributional algorithm, a variant which uses only
capitalization information (i.e., has only one non-
singleton morphological class, that of words ap-
pearing capitalized in most of their instances) and
a variant which uses no capitalization information,
defining the morphological clusters according to
the morphological representation alone.
7 Results
Table 1 presents results for the English and Ger-
man experiments. For English, our algorithm ob-
tains better results than Clark?s in all measures and
scenarios. It is without exception better than the
average score of Clark?s and in most cases better
than the maximal Clark score obtained in 100 runs.
A significant difference between our algorithm
and Clark?s is that the latter, like most algorithms
which addressed the task, induces the clustering
0 5 10 15 20 25 30 35 40 45
0
0.2
0.4
0.6
0.8
1
 
 
Gold Standard
Induced
Figure 2: POS class frequency distribution for our model
and the gold standard, in the ?Fine k=34? scenario. The dis-
tributions are similar.
by maximizing a non-convex function. These
functions have many local maxima and the specific
solution to which algorithms that maximize them
converge strongly depends on their (random) ini-
tialization. Therefore, their output?s quality often
significantly diverges from the average. This issue
is discussed in depth in (Reichart et al, 2010b).
Our algorithm is deterministic3.
For German, in the k=26 scenario our algorithm
outperforms Clark?s, often outperforming even its
maximum in 100 runs. In the k=17 scenario, our
algorithm obtains a higher score than Clark with
probability 0.4 to 0.78, depending on the measure
and scenario. Clark?s average score is slightly bet-
ter in the Many-to-1 measure, while our algorithm
performs somewhat better than Clark?s average in
the 1-to-1 and NVI measures.
The DeWAC corpus from which we extracted
statistics for the German experiments is out of do-
main with respect to NEGRA. The correspond-
ing corpus in English, NANC, is a newswire cor-
pus and therefore clearly in-domain with respect
to WSJ. This is reflected by the percentage of un-
known words, which was much higher in German
than in English (8.1% and 1.6%), lowering results.
Table 2 shows the effect of each of our algo-
rithm?s components. Each component provides
an improvement over the base distributional algo-
rithm. The full coordinate clustering stage (sev-
eral iterations, F) considerably improves the score
over a single iteration (F(I=1)). Capitalization in-
formation increases the score more than the mor-
phological information, which might stem from
the granularity of the POS tag set with respect to
names. This analysis is supported by similar ex-
periments we made in the ?Coarse k=13? scenario
(not shown in tables here). There, the decrease in
performance was only of 1%?2% in the mapping
3The fluctuations inflicted on our algorithm by the random
mapping of unknown words are of less than 0.1% .
1304
Excluding Punctuation Including Punctuation Perfect Punctuation
M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V M-to-1 1-to-1 NVI V
Van Gael 59.1 48.4 0.999 0.530 62.3 51.3 0.861 0.591 64.0 54.6 0.820 0.610
Prototype 67.5 58.8 0.809 0.608 71.6 63.5 0.663 0.677 71.6 63.9 0.659 0.679
Table 3: Comparison between the iHMM: PY-fixed model (Van Gael et al, 2009) and ours with various punctuation assign-
ment schemes. Left section: punctuation tokens are excluded. Middle section: punctuation tokens are included. Right section:
perfect assignment of punctuation is assumed.
based measures and 3.5% in the V measure.
Finally, Table 4 presents reported results for all
recent algorithms we are aware of that tackled the
task of unsupervised POS induction from plain
text. Results for our algorithm?s and Clark?s are
reported for the ?Fine, k=34? scenario. The set-
tings of the various experiments vary in terms of
the exact annotation scheme used (coarse or fine
grained) and the size of the test set. However, the
score differences are sufficiently large to justify
the claim that our algorithm is currently the best
performing algorithm on the PTB-WSJ corpus for
POS induction from plain text4.
Since previous works provided results only for
the scenario in which punctuation is included, the
reported results are not directly comparable. In
order to quantify the effect various punctuation
schemes have on the results, we evaluated the
?iHMM: PY-fixed? model (Van Gael et al, 2009)
and ours when punctuation is excluded, included
or perfectly tagged5. The results (Table 3) indi-
cate that most probably even after an appropriate
correction for punctuation, our model remains the
best performing one.
8 Discussion
In this work we presented a novel unsupervised al-
gorithm for POS induction from plain text. The al-
gorithm first generates relatively accurate clusters
of high frequency words, which are subsequently
used to bootstrap the entire clustering. The dis-
tributional and morphological representations that
we use are novel for this task.
We experimented on two languages with map-
ping and information theoretic clustering evalua-
tion measures. Our algorithm obtains the best re-
ported results on the English PTB-WSJ corpus. In
addition, our results are almost always better than
Clark?s on the German NEGRA corpus.
4Grac?a et al (2009) report very good results for 17 tags in
the M-1 measure. However, their 1-1 results are quite poor,
and results for the common IT measures were not reported.
Their results for 45 tags are considerably lower.
5We thank the authors for sending us their data.
We have also performed a manual error anal-
ysis, which showed that our algorithm performs
much better on closed classes than on open
classes. In order to asses this quantitatively, let
us define a random variable for each of the gold
clusters, which receives a value corresponding to
each induced cluster with probability proportional
to their intersection size. For each gold cluster,
we compute the entropy of this variable. In ad-
dition, we greedily map each induced cluster to a
gold cluster and compute the ratio between their
intersection size and the size of the gold cluster
(mapping accuracy).
We experimented in the ?Fine k=34? scenario.
The clusters that obtained the best scores were
(brackets indicate mapping accuracy and entropy
for each of these clusters) coordinating conjunc-
tions (95%, 0.32), prepositions (94%, 0.32), de-
terminers (94%, 0.44) and modals (93%, 0.45).
These are all closed classes.
The classes on which our algorithm performed
worst consist of open classes, mostly verb types:
past tense verbs (47%, 2.2), past participle verbs
(44%, 2.32) and the morphologically unmarked
non-3rd person singular present verbs (32%, 2.86).
Another class with low performance is the proper
nouns (37%, 2.9). The errors there are mostly
of three types: confusions between common and
proper nouns (sometimes due to ambiguity), un-
known words which were put in the unknown
words cluster, and abbreviations which were given
a separate class by our algorithm. Finally, the al-
gorithm?s performance on the heterogeneous ad-
verbs class (19%, 3.73) is the lowest.
Clark?s algorithm exhibits6 a similar pattern
with respect to open and closed classes. While
his algorithm performs considerably better on ad-
verbs (15% mapping accuracy difference and 0.71
entropy difference), our algorithm scores consid-
erably better on prepositions (17%, 0.77), su-
perlative adjectives (38%, 1.37) and plural proper
names (45%, 1.26).
6Using average mapping accuracy and entropy over the
100 runs.
1305
Naturally, this analysis might reflect the arbi-
trary nature of a manually design POS tag set
rather than deficiencies in automatic POS induc-
tion algorithms. In future work we intend to ana-
lyze the output of such algorithms in order to im-
prove POS tag sets.
Our algorithm and Clark?s are monosemous
(i.e., they assign each word exactly one tag), while
most other algorithms are polysemous. In order to
assess the performance loss caused by the monose-
mous nature of our algorithm, we took the M-1
greedy mapping computed for the entire dataset
and used it to compute accuracy over the monose-
mous and polysemous words separately. Results
are reported for the English ?Fine k=34? scenario
(without punctuation). We define a word to be
monosemous if more than 95% of its tokens are
assigned the same gold standard tag. For English,
there are approximately 255K polysemous tokens
and 578K monosemous ones. As expected, our
algorithm is much more accurate on the monose-
mous tokens, achieving 76.6% accuracy, com-
pared to 47.1% on the polysemous tokens.
The evaluation in this paper is done at the token
level. Type level evaluation, reflecting the algo-
rithm?s ability to detect the set of possible POS
tags for each word type, is important as well. It
could be expected that a monosemous algorithm
such as ours would perform poorly in a type level
evaluation. In (Reichart et al, 2010a) we discuss
type level evaluation at depth and propose type
level evaluation measures applicable to the POS
induction problem. In that paper we compare the
performance of our Prototype Tagger with lead-
ing unsupervised POS tagging algorithms (Clark,
2003; Goldwater and Griffiths, 2007; Gao and
Johnson, 2008; Van Gael et al, 2009). Our al-
gorithm obtained the best results in 4 of the 6
measures in a margin of 4?6%, and was second
best in the other two measures. Our results were
better than Clark?s (the only other monosemous
algorithm evaluated there) on all measures in a
margin of 5?21%. The fact that our monose-
mous algorithm was better than good polysemous
algorithms in a type level evaluation can be ex-
plained by the prototypical nature of the POS phe-
nomenon (a longer discussion is given in (Reichart
et al, 2010a)). However, the quality upper bound
for monosemous algorithms is obviously much
lower than that for polysemous algorithms, and
we expect polysemous algorithms to outperform
monosemous algorithms in the future in both type
level and token level evaluations.
The skewed (Zipfian) distribution of POS class
frequencies in corpora is a problem for many POS
induction algorithms, which by default tend to in-
duce a clustering having a balanced distribution.
Explicit modifications to these algorithms were in-
troduced in order to bias their model to produce
such a distribution (see (Clark, 2003; Johnson,
2007; Reichart et al, 2010b)). An appealing prop-
erty of our model is its ability to induce a skewed
distribution without being explicitly tuned to do
so, as seen in Figure 2.
Acknowledgements. We would like to thank
Yoav Seginer for his help with his parser.
References
Michele Banko and Robert C. Moore, 2004. Part of
Speech Tagging in Context. COLING ?04.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi and
Eros Zanchetta, 2009. The WaCky Wide Web: A
Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ?06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18(4):467?479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Mathias Creutz and Krista Lagus, 2005. Inducing the
Morphological Lexicon of a Natural Language from
Unannotated Text. AKRR ?05.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ?04.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ?08.
1306
John Goldsmith, 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
David Graff, 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT?NAACL ?06.
Anil K. Jain, Narasimha M. Murty and Patrick J. Flynn,
1999. Data Clustering: A Review. ACM Computing
Surveys 31(3):264?323.
Wenbin Jiang, Liang Huang and Qun Liu, 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. ACL ?09.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ?07.
Harold W. Kuhn, 1955. The Hungarian method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873?895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155?172.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32?38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
?09.
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Roi Reichart, Omri Abend and Ari Rappoport, 2010a.
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study. CoNLL ?10.
Roi Reichart, Raanan Fattal and Ari Rappoport, 2010b.
Improved Unsupervised POS Induction Using In-
trinsic Clustering Quality and a Zipfian Constraint.
CoNLL ?10.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ?07.
Hinrich Schu?tze, 1995. Distributional part-of-speech
tagging. EACL ?95.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ?05.
John R. Taylor, 2003. Linguistic Categorization: Pro-
totypes in Linguistic Theory, Third Edition. Oxford
University Press.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP?KE ?05.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ?09.
1307
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extraction and Approximation of Numerical Attributes from the Web
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
arir@cs.huji.ac.il
Abstract
We present a novel framework for auto-
mated extraction and approximation of nu-
merical object attributes such as height
and weight from the Web. Given an
object-attribute pair, we discover and ana-
lyze attribute information for a set of com-
parable objects in order to infer the desired
value. This allows us to approximate the
desired numerical values even when no ex-
act values can be found in the text.
Our framework makes use of relation
defining patterns and WordNet similarity
information. First, we obtain from the
Web and WordNet a list of terms similar to
the given object. Then we retrieve attribute
values for each term in this list, and infor-
mation that allows us to compare different
objects in the list and to infer the attribute
value range. Finally, we combine the re-
trieved data for all terms from the list to
select or approximate the requested value.
We evaluate our method using automated
question answering, WordNet enrichment,
and comparison with answers given in
Wikipedia and by leading search engines.
In all of these, our framework provides a
significant improvement.
1 Introduction
Information on various numerical properties of
physical objects, such as length, width and weight
is fundamental in question answering frameworks
and for answering search engine queries. While
in some cases manual annotation of objects with
numerical properties is possible, it is a hard and
labor intensive task, and is impractical for dealing
with the vast amount of objects of interest. Hence,
there is a need for automated semantic acquisition
algorithms targeting such properties.
In addition to answering direct questions, the
ability to make a crude comparison or estimation
of object attributes is important as well. For ex-
ample, it allows to disambiguate relationships be-
tween objects such as X part-of Y or X inside Y.
Thus, a coarse approximation of the height of a
house and a window is sufficient to decide that
in the ?house window? nominal compound, ?win-
dow? is very likely to be a part of house and not
vice versa. Such relationship information can, in
turn, help summarization, machine translation or
textual entailment tasks.
Due to the importance of relationship and at-
tribute acquisition in NLP, numerous methods
were proposed for extraction of various lexical re-
lationships and attributes from text. Some of these
methods can be successfully used for extracting
numerical attributes. However, numerical attribute
extraction is substantially different in two aspects,
verification and approximation.
First, unlike most general lexical attributes, nu-
merical attribute values are comparable. It usually
makes no sense to compare the names of two ac-
tors, but it is meaningful to compare their ages.
The ability to compare values of different objects
allows to improve attribute extraction precision by
verifying consistency with attributes of other sim-
ilar objects. For example, suppose that for Toy-
ota Corolla width we found two different values,
1.695m and 27cm. The second value can be either
an extraction error or a length of a toy car. Ex-
tracting and looking at width values for different
car brands and for ?cars? in general we find:
? Boundaries: Maximal car width is 2.195m,
minimal is 88cm.
? Average: Estimated avg. car width is 1.7m.
? Direct/indirect comparisons: Toyota Corolla
is wider than Toyota Corona.
? Distribution: Car width is distributed nor-
mally around the average.
1308
Usage of all this knowledge allows us to select the
correct value of 1.695m and reject other values.
Thus we can increase the precision of value ex-
traction by finding and analyzing an entire group
of comparable objects.
Second, while it is usually meaningless and im-
possible to approximate general lexical attribute
values like an actor?s name, numerical attributes
can be estimated even if they are not explicitly
mentioned in the text.
In general, attribute extraction frameworks usu-
ally attempt to discover a single correct value (e.g.,
capital city of a country) or a set of distinct correct
values (e.g., actors of a movie). So there is es-
sentially nothing to do when there is no explicit
information present in the text for a given object
and an attribute. In contrast, in numerical attribute
extraction it is possible to provide an approxima-
tion even when no explicit information is present
in the text, by using values of comparable objects
for which information is provided.
In this paper we present a pattern-based frame-
work that takes advantage of the properties of sim-
ilar objects to improve extraction precision and
allow approximation of requested numerical ob-
ject properties. Our framework comprises three
main stages. First, given an object name we uti-
lize WordNet and pattern-based extraction to find
a list of similar objects and their category labels.
Second, we utilize a predefined set of lexical pat-
terns in order to extract attribute values of these
objects and available comparison/boundary infor-
mation. Finally, we analyze the obtained informa-
tion and select or approximate the attribute value
for the given (object, attribute) pair.
We performed a thorough evaluation using three
different applications: Question Answering (QA),
WordNet (WN) enrichment, and comparison with
Wikipedia and answers provided by leading search
engines. QA evaluation was based on a designed
dataset of 1250 questions on size, height, width,
weight, and depth, for which we created a gold
standard and compared against it automatically1.
For WN enrichment evaluation, our framework
discovered size and weight values for 300 WN
physical objects, and the quality of results was
evaluated by human judges. For interactive search,
we compared our results to information obtained
through Wikipedia, Google and Wolfram Alpha.
1This dataset is available in the authors? websites for the
research community.
Utilization of information about comparable ob-
jects provided a significant boost to numerical at-
tribute extraction quality, and allowed a meaning-
ful approximation of missing attribute values.
Section 2 discusses related work, Section 3 de-
tails the algorithmic framework, Section 4 de-
scribes the experimental setup, and Section 5
presents our results.
2 Related work
Numerous methods have been developed for ex-
traction of diverse semantic relationships from
text. While several studies propose relationship
identification methods using distributional analy-
sis of feature vectors (Turney, 2005), the major-
ity of the proposed open-domain relations extrac-
tion frameworks utilize lexical patterns connect-
ing a pair of related terms. (Hearst, 1992) man-
ually designed lexico-syntactic patterns for ex-
tracting hypernymy relations. (Berland and Char-
niak, 1999; Girju et al 2006) proposed a set of
patterns for meronymy relations. Davidov and
Rappoport (2008a) used pattern clusters to disam-
biguate nominal compound relations. Extensive
frameworks were proposed for iterative discov-
ery of any pre-specified (e.g., (Riloff and Jones,
1999; Chklovski and Pantel, 2004)) and unspec-
ified (e.g., (Banko et al, 2007; Rosenfeld and
Feldman, 2007; Davidov and Rappoport, 2008b))
relation types.
The majority of the above methods utilize the
following basic strategy. Given (or discovering
automatically) a set of patterns or relationship-
representing term pairs, these methods mine the
web for these patterns and pairs, iteratively obtain-
ing more instances. The proposed strategies gen-
erally include some weighting/frequency/context-
based algorithms (e.g. (Pantel and Pennacchiotti,
2006)) to reduce noise. Some of the methods are
suitable for retrieval of numerical attributes. How-
ever, most of them do not exploit the numerical
nature of the attribute data.
Our research is related to a sub-domain of ques-
tion answering (Prager, 2006), since one of the
applications of our framework is answering ques-
tions on numerical values. The majority of the
proposed QA frameworks rely on pattern-based
relationship acquisition (Ravichandran and Hovy,
2009). However, most QA studies focus on dif-
ferent types of problems than our paper, including
question classification, paraphrasing, etc.
1309
Several recent studies directly target the acqui-
sition of numerical attributes from the Web and
attempt to deal with ambiguity and noise of the
retrieved attribute values. (Aramaki et al, 2007)
utilize a small set of patterns to extract physical
object sizes and use the averages of the obtained
values for a noun compound classification task.
(Banerjee et al 2009) developed a method for
dealing with quantity consensus queries (QCQs)
where there is uncertainty about the answer quan-
tity (e.g. ?driving time from Paris to Nice?). They
utilize a textual snippet feature and snippet quan-
tity in order to select and rank intervals of the
requested values. This approach is particularly
useful when it is possible to obtain a substantial
amount of a desired attribute values for the re-
quested query. (Moriceau, 2006) proposed a rule-
based system which analyzes the variation of the
extracted numerical attribute values using infor-
mation in the textual context of these values.
A significant body of recent research deals with
extraction of various data from web tables and
lists (e.g., (Cafarella et al, 2008; Crestan and
Pantel, 2010)). While in the current research we
do not utilize this type of information, incorpo-
ration of the numerical data extracted from semi-
structured web pages can be extremely beneficial
for our framework.
All of the above numerical attribute extraction
systems utilize only direct information available
in the discovered object-attribute co-occurrences
and their contexts. However, as we show, indirect
information available for comparable objects can
contribute significantly to the selection of the ob-
tained values. Using such indirect information is
particularly important when only a modest amount
of values can be obtained for the desired object.
Also, since the above studies utilize only explic-
itly available information they were unable to ap-
proximate object values in cases where no explicit
information was found.
3 The Attribute Mining Framework
Our algorithm is given an object and an attribute.
In the WN enrichment scenario, it is also given
the object?s synset. The algorithm comprises three
main stages: (1) mining for similar objects and
determination of a class label; (2) mining for at-
tribute values and comparison statements; (3) pro-
cessing the results.
3.1 Similar objects and class label
To verify and estimate attribute values for the
given object we utilize similar objects (co-
hyponyms) and the object?s class label (hyper-
nym). In the WN enrichment scenario we can eas-
ily obtain these, since we get the object?s synset as
input. However, in Question Answering (QA) sce-
narios we do not have such information. To obtain
it we employ a strategy which uses WordNet alng
with pattern-based web mining.
Our web mining part follows common pattern-
based retrieval practice (Davidov et al, 2007). We
utilize Yahoo! Boss API to perform search engine
queries. For an object name Obj we query the
Web using a small set of pre-defined co-hyponymy
patterns like ?as * and/or [Obj]?2. In the WN en-
richment scenario, we can add the WN class la-
bel to each query in order to restrict results to the
desired word sense. In the QA scenario, if we
are given the full question and not just the (ob-
ject, attribute) pair we can add terms appearing in
the question and having a strong PMI with the ob-
ject (this can be estimated using any fixed corpus).
However, this is not essential.
We then extract new terms from the retrieved
web snippets and use these terms iteratively to re-
trieve more terms from the Web. For example,
when searching for an object ?Toyota?, we execute
a search engine query [ ?as * and Toyota?] and
we might retrieve a text snippet containing ?. . . as
Honda and Toyota . . . ?. We then extract from this
snippet the additional word ?Honda? and use it for
iterative retrieval of additional similar terms. We
attempt to avoid runaway feedback loop by requir-
ing each newly detected term to co-appear with the
original term in at least a single co-hyponymy pat-
tern.
WN class labels are used later for the retrieval
of boundary values, and here for expansion of the
similar object set. In the WN enrichment scenario,
we already have the class label of the object. In the
QA scenario, we automatically find class labels as
follows. We compute for each WN subtree a cov-
erage value, the number of retrieved terms found
in the subtree divided by the number of subtree
terms, and select the subtree having the highest
coverage. In all scenarios, we add all terms found
in this subtree to the retrieved term list. If no WN
subtree with significant (> 0.1) coverage is found,
2
?*? means a search engine wildcard. Square brackets
indicate filled slots and are not part of the query.
1310
we retrieve a set of category labels from the Web
using hypernymy detection patterns like ?* such
as [Obj]? (Hearst, 1992). If several label candi-
dates were found, we select the most frequent.
Note that we perform this stage only once for
each object and do not need to repeat it for differ-
ent attribute types.
3.2 Querying for values, bounds and
comparison data
Now we would like to extract the attribute values
for the given object and its similar objects. We
will also extract bounds and comparison informa-
tion in order to verify the extracted values and to
approximate the missing ones.
To allow us to extract attribute-specific informa-
tion, we provided the system with a seed set of ex-
traction patterns for each attribute type. There are
three kinds of patterns: value extraction, bounds
and comparison patterns. We used up to 10 pat-
terns of each kind. These patterns are the only
attribute-specific resource in our framework.
Value extraction. The first pattern group,
Pvalues, allows extraction of the attribute values
from the Web. All seed patterns of this group
contain a measurement unit name, attribute name,
and some additional anchoring words, e.g., ?Obj
is * [height unit] tall? or ?Obj width is * [width
unit]?. As in Section 3.1, we execute search en-
gine queries and collect a set of numerical val-
ues for each pattern. We extend this group it-
eratively from the given seed as commonly done
in pattern-based acquisition methods. To do this
we re-query the Web with the obtained (object, at-
tribute value, attribute name) triplets (e.g., ?[Toy-
ota width 1.695m]?). We then extract new pat-
terns from the retrieved search engine snippets and
re-query the Web with the new patterns to obtain
more attribute values.
We provided the framework with unit names
and with an appropriate conversion table which
allows to convert between different measurement
systems and scales. The provided names include
common abbreviations like cm/centimeter. All
value acquisition patterns include unit names, so
we know the units of each extracted value. At the
end of the value extraction stage, we convert all
values to a single unit format for comparison.
Boundary extraction. The second group,
Pboundary, consists of boundary-detection patterns
like ?the widest [label] is * [width unit]?. These
patterns incorporate the class labels discovered in
the previous stage. They allow us to find maximal
and minimal values for the object category defined
by labels. If we get several lower bounds and
several upper bounds, we select the highest upper
bound and the lowest lower bound.
Extraction of comparison information. The
third group, Pcompare, consists of comparison pat-
terns. They allow to compare objects directly
even when no attribute values are mentioned. This
group includes attribute equality patterns such as
?[Object1] has the same width as [Object2]?, and
attribute inequality ones such as ?[Object1] is
wider than [Object2]?. We execute search queries
for each of these patterns, and extract a set of or-
dered term pairs, keeping track of the relationships
encoded by the pairs.
We use these pairs to build a directed graph
(Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006) in which nodes are objects (not nec-
essarily with assigned values) and edges corre-
spond to extracted co-appearances of objects in-
side the comparison patterns. The directions of
edges are determined by the comparison sign. If
two objects co-appear inside an equality pattern
we put a bidirectional edge between them.
3.3 Processing the collected data
As a result of the information collection stage, for
each object and attribute type we get:
? A set of attribute values for the requested ob-
ject.
? A set of objects similar or comparable to
the requested object, some of them annotated
with one or many attribute values.
? Upper and lowed bounds on attribute values
for the given object category.
? A comparison graph connecting some of the
retrieved objects by comparison edges.
Obviously, some of these components may be
missing or noisy. Now we combine these informa-
tion sources to select a single attribute value for
the requested object or to approximate this value.
First we apply bounds, removing out-of-range val-
ues, then we use comparisons to remove inconsis-
tent comparisons. Finally we examine the remain-
ing values and the comparison graph.
Processing bounds. First we verify that indeed
most (? 50%) of the retrieved values fit the re-
trieved bounds. If the lower and/or upper bound
1311
contradicts more than half of the data, we reject
the bound. Otherwise we remove all values which
do not satisfy one or both of the accepted bounds.
If no bounds are found or if we disable the bound
retrieval (see Section 4.1), we assign the maximal
and minimal observed values as bounds.
Since our goal is to obtain a value for the single
requested object, if at the end of this stage we re-
main with a single value, no further processing is
needed. However, if we obtain a set of values or
no values at all, we have to utilize comparison data
to select one of the retrieved values or to approx-
imate the value in case we do not have an exact
answer.
Processing comparisons. First we simplify the
comparison graph. We drop all graph components
that are not connected (when viewing the graph as
undirected) to the desired object.
Now we refine the graph. Note that each graph
node may have a single value, many assigned val-
ues, or no assigned values. We define assigned
nodes as nodes that have at least one value. For
each directed edge E(A ? B), if both A and
B are assigned nodes, we check if Avg(A) ?
Avg(B)3. If the average values violate the equa-
tion, we gradually remove up to half of the highest
values for A and up to half of the lowest values
for B till the equation is satisfied. If this cannot
be done, we drop the edge. We repeat this process
until every edge that connects two assigned nodes
satisfies the inequality.
Selecting an exact attribute value. The goal
now is to select an attribute value for the given
object. During the first stage it is possible that
we directly extract from the text a set of values
for the requested object. The bounds processing
step rejects some of these values, and the com-
parisons step may reject some more. If we still
have several values remaining, we choose the most
frequent value based on the number of web snip-
pets retrieved during the value acquisition stage.
If there are several values with the same frequency
we select the median of these values.
Approximating the attribute value. In the case
when we do not have any values remaining after
the bounds processing step, the object node will
remain unassigned after construction of the com-
parison graph, and we would like to estimate its
value. Here we present an algorithm which allows
3Avg. is of values of an object, without similar objects.
us to set the values of all unassigned nodes, includ-
ing the node of the requested object.
In the algorithm below we treat all node groups
connected by bidirectional (equality) edges as a
same-value group, i.e., if a value is assigned to one
node in the group, the same value is immediately
assigned to the rest of the nodes in the same group.
We start with some preprocessing. We create
dummy lower and upper bound nodes L and U
with corresponding upper/lower bound values ob-
tained during the previous stage. These dummy
nodes will be used when we encounter a graph
which ends with one or more nodes with no avail-
able numerical information. We then connect
them to the graph as follows: (1) if A has no in-
coming edges, we add an edge L ? A; (2) if A
has no outgoing edges, we add an edge A ? U .
We define a legal unassigned path as a di-
rected path A0 ? A1 ? . . . ? An ? An+1
where A0 and An+1 are assigned satisfying
Avg(A0) ? Avg(An+1) and A1 . . . An are
unassigned. We would like to use dummy bound
nodes only in cases when no other information is
available. Hence we consider paths L ? . . . ? U
connecting both bounds are illegal. First we
assign values for all unassigned nodes that belong
to a single legal unassigned path, using a simple
linear combination:
V al(Ai)i?(1...n) =
n + 1? i
n + 1 Avg(A0) +
i
n + 1Avg(An+1)
Then, for all unassigned nodes that belong to
multiple legal unassigned paths, we compute node
value as above for each path separately and assign
to the node the average of the computed values.
Finally we assign the average of all extracted
values within bounds to all the remaining unas-
signed nodes. Note that if we have no compari-
son information and no value information for the
requested object, the requested object will receive
the average of the extracted values of the whole set
of the retrieved comparable objects and the com-
parison step will be essentially empty.
4 Experimental Setup
We performed automated question answering
(QA) evaluation, human-based WN enrichment
evaluation, and human-based comparison of our
results to data available through Wikipedia and to
the top results of leading search engines.
1312
4.1 Experimental conditions
In order to test the main system components, we
ran our framework under five different conditions:
? FULL: All system components were used.
? DIRECT: Only direct pattern-based acqui-
sition of attribute values (Section 3.2, value
extraction) for the given object was used, as
done in most general-purpose attribute acqui-
sition systems. If several values were ex-
tracted, the most common value was used as
an answer.
? NOCB: No boundary and no comparison
data were collected and processed (Pcompare
and Pbounds were empty). We only collected
and processed a set of values for the similar
objects.
? NOB: As in FULL but no boundary data was
collected and processed (Pbounds was empty).
? NOC: As in FULL but no comparison data
was collected and processed (Pcompare was
empty).
4.2 Automated QA Evaluation
We created two QA datasets, Web and TREC
based.
Web-based QA dataset. We created QA
datasets for size, height, width, weight, and depth
attributes. For each attribute we extracted from
the Web 250 questions in the following way.
First, we collected several thousand questions,
querying for the following patterns: ?How
long/tall/wide/heavy/deep/high is?,?What is the
size/width/height/depth/weight of?. Then we
manually filtered out non-questions and heavily
context-specific questions, e.g., ?what is the width
of the triangle?. Next, we retained only a single
question for each entity by removing duplicates.
For each of the extracted questions we manu-
ally assigned a gold standard answer using trusted
resources including books and reliable Web data.
For some questions, the exact answer is the only
possible one (e.g., the height of a person), while
for others it is only the center of a distribution
(e.g., the weight of a coffee cup). Questions
with no trusted and exact answers were eliminated.
From the remaining questions we randomly se-
lected 250 questions for each attribute.
TREC-based QA dataset. As a small comple-
mentary dataset we used relevant questions from
the TREC Question Answering Track 1999-2007.
From 4355 questions found in this set we collected
55 (17 size, 2 weight, 3 width, 3 depth and 30
height) questions.
Examples. Some example questions from our
datasets are (correct answers are in parentheses):
How tall is Michelle Obama? (180cm); How tall
is the tallest penguin? (122cm); What is the height
of a tennis net? (92cm); What is the depth of the
Nile river? (1000cm = 10 meters); How heavy
is a cup of coffee? (360gr); How heavy is a gi-
raffe? (1360000gr = 1360kg); What is the width
of a DNA molecule? (2e-7cm); What is the width
of a cow? (65cm).
Evaluation protocol. Evaluation against the
datasets was done automatically. For each ques-
tion and each condition our framework returned
a numerical value marked as either an exact an-
swer or as an approximation. In cases where no
data was found for an approximation (no similar
objects with values were found), our framework
returned no answer.
We computed precision4, comparing results to
the gold standard. Approximate answers are con-
sidered to be correct if the approximation is within
10% of the gold standard value. While a choice of
10% may be too strict for some applications and
too generous for others, it still allows to estimate
the quality of our framework.
4.3 WN enrichment evaluation
We manually selected 300 WN entities from about
1000 randomly selected objects below the object
tree in WN, by filtering out entities that clearly
do not possess any of the addressed numerical at-
tributes.
Evaluation was done using human subjects. It
is difficult to do an automated evaluation, since
the nature of the data is different from that of the
QA dataset. Most of the questions asked over the
Web target named entities like specific car brands,
places and actors. There is usually little or no vari-
ability in attribute values of such objects, and the
major source of extraction errors is name ambigu-
ity of the requested objects.
WordNet physical objects, in contrast, are much
less specific and their attributes such as size and
4Due to the nature of the task recall/f-score measures are
redundant here
1313
weight rarely have a single correct value, but usu-
ally possess an acceptable numerical range. For
example, the majority of the selected objects like
?apple? are too general to assign an exact size.
Also, it is unclear how to define acceptable val-
ues and an approximation range. Crudeness of
desired approximation depends both on potential
applications and on object type. Some objects
show much greater variability in size (and hence a
greater range of acceptable approximations) than
others. This property of the dataset makes it diffi-
cult to provide a meaningful gold standard for the
evaluation. Hence in order to estimate the quality
of our results we turn to an evaluation based on
human judges.
In this evaluation we use only approximate re-
trieved values, keeping out the small amount of
returned exact values5.
We have mixed (Object, Attribute name, At-
tribute value) triplets obtained through each of the
conditions, and asked human subjects to assign
these to one of the following categories:
? The attribute value is reasonable for the given
object.
? The value is a very crude approximation of
the given object attribute.
? The value is incorrect or clearly misleading.
? The object is not familiar enough to me so I
cannot answer the question.
Each evaluator was provided with a random sam-
ple of 40 triplets. In addition we mixed in 5 manu-
ally created clearly correct triplets and 5 clearly in-
correct ones. We used five subjects, and the agree-
ment (inter-annotator Kappa) on shared evaluated
triplets was 0.72.
4.4 Comparisons to search engine output
Recently there has been a significant improvement
both in the quality of search engine results and in
the creation of manual well-organized and anno-
tated databases such as Wikipedia.
Google and Yahoo! queries frequently provide
attribute values in the top snippets or in search
result web pages. Many Wikipedia articles in-
clude infoboxes with well-organized attribute val-
ues. Recently, the Wolfram Alpha computational
knowledge engine presented the computation of
attribute values from a given query text.
5So our results are in fact higher than shown.
Hence it is important to test how well our frame-
work can complement the manual extraction of at-
tributes from resources such as Wikipedia and top
Google snippets. In order to test this, we randomly
selected 100 object-attribute pairs from our Web
QA and WordNet datasets and used human sub-
jects to test the following:
1. Go1: Querying Google for [object-name
attribute-name] gives in some of the first
three snippets a correct value or a good ap-
proximation value6 for this pair.
2. Go2: Querying Google for [object-name
attribute-name] and following the first three
links gives a correct value or a good approxi-
mation value.
3. Wi: There is a Wikipedia page for the given
object and it contains an appropriate attribute
value or an approximation in an infobox.
4. Wf: A Wolfram Alpha query for [object-
name attribute-name] retrieves a correct
value or a good approximation value
5 Results
5.1 QA results
We applied our framework to the above QA
datasets. Table 1 shows the precision and the per-
centage of approximations and exact answers.
Looking at %Exact+%Approx, we can see that
for all datasets only 1-9% of the questions re-
main unanswered, while correct exact answers
are found for 65%/87% of the questions for
Web/TREC (% Exact and Prec(Exact) in the ta-
ble). Thus approximation allows us to answer 13-
24% of the requested values which are either sim-
ply missing from the retrieved text or cannot be de-
tected using the current pattern-based framework.
Comparing performance of FULL to DIRECT, we
see that our framework not only allows an approx-
imation when no exact answer can be found, but
also significantly increases the precision of exact
answers using the comparison and the boundary
information. It is also apparent that both bound-
ary and comparison features are needed to achieve
good performance and that using both of them
achieves substantially better results than each of
them separately.
6As defined in the human subject questionnaire.
1314
FULL DIRECT NOCB NOB NOC
Web QA
Size
%Exact 80 82 82 82 80
Prec(Exact) 76 40 40 54 65
%Approx 16 - 14 14 16
Prec(Appr) 64 - 34 53 46
Height
%Exact 79 84 84 84 79
Prec(Exact) 86 56 56 69 70
%Approx 16 - 11 11 16
Prec(Appr) 72 - 25 65 53
Width
%Exact 74 76 76 76 74
Prec(Exact) 86 45 45 60 72
%Approx 17 - 15 15 17
Prec(Appr) 75 - 26 63 55
Weight
%Exact 71 73 73 73 71
Prec(Exact) 82 57 57 64 70
Prec(Appr) 24 - 22 22 24
%Approx 61 - 39 51 46
Depth
%Exact 82 82 82 82 82
Prec(Exact) 89 60 60 71 78
%Approx 19 - 19 19 19
Prec(Appr) 92 - 58 76 63
Total average
%Exact 77 79 79 79 77
Prec(Exact) 84 52 52 64 71
%Approx 18 - 16 16 19
Prec(Appr) 72 - 36 62 53
TREC QA
%Exact 87 90 90 90 87
Prec(Exact) 100 62 62 84 76
%Approx 13 - 9 9 13
Prec(Appr) 57 - 20 40 57
Table 1: Precision and amount of exact and approximate
answers for QA datasets.
Comparing results for different question types
we can see substantial performance differences be-
tween the attribute types. Thus depth shows much
better overall results than width. This is likely due
to a lesser difficulty of depth questions or to a more
exact nature of available depth information com-
pared to width or size.
5.2 WN enrichment
As shown in Table 2, for the majority of examined
WN objects, the algorithm returned an approxi-
mate value, and only for 13-15% of the objects (vs.
70-80% in QA data) the algorithm could retrieve
exact answers.
Note that the common pattern-based acquisition
framework, presented as the DIRECT condition,
could only extract attribute values for 15% of the
objects since it does not allow approximations and
FULL DIRECT NOCB NOB NOC
Size
%Exact 15.3 18.0 18.0 18.0 15.3
%Approx 80.3 - 38.2 20.0 23.6
Weight
%Exact 11.8 12.5 12.5 12.5 11.8
%Approx 71.7 - 38.2 20.0 23.6
Table 2: Percentage of exact and approximate values for the
WordNet enrichment dataset.
FULL NOCB NOB NOC
Size
%Correct 73 21 49 28
%Crude 15 54 31 49
%Incorrect 8 21 16 19
Weight
%Correct 64 24 46 38
%Crude 24 45 30 41
%Incorrect 6 25 18 15
Table 3: Human evaluation of approximations for the WN
enrichment dataset (the percentages are averaged over the hu-
man subjects).
may only extract values from the text where they
explicitly appear.
Table 3 shows human evaluation results. We
see that the majority of approximate values were
clearly accepted by human subjects, and only 6-
8% were found to be incorrect. We also observe
that both boundary and comparison data signifi-
cantly improve the approximation results. Note
that DIRECT is missing from this table since no
approximations are possible in this condition.
Some examples for WN objects and approx-
imate values discovered by the algorithm are:
Sandfish, 15gr; skull, 1100gr; pilot, 80.25kg. The
latter value is amusing due to the high variabil-
ity of the value. However, even this value is valu-
able, as a sanity check measure for automated in-
ference systems and for various NLP tasks (e.g.,
?pilot jacket? likely refers to a jacket used by pi-
lots and not vice versa).
5.3 Comparison with search engines and
Wikipedia
Table 4 shows results for the above datasets in
comparison to the proportion of correct results and
the approximations returned by our framework un-
der the FULL condition (correct exact values and
approximations are taken together).
We can see that our framework, due to its ap-
proximation capability, currently shows signifi-
cantly greater coverage than manual extraction of
data from Wikipedia infoboxes or from the first
1315
FULL Go1 Go2 Wi Wf
Web QA 83 32 40 15 21
WordNet 87 24 27 18 5
Table 4: Comparison of our attribute extraction framework
to manual extraction using Wikipedia and search engines.
search engine results.
6 Conclusion
We presented a novel framework which allows
an automated extraction and approximation of nu-
merical attributes from the Web, even when no ex-
plicit attribute values can be found in the text for
the given object. Our framework retrieves simi-
larity, boundary and comparison information for
objects similar to the desired object, and com-
bines this information to approximate the desired
attribute.
While in this study we explored only several
specific numerical attributes like size and weight,
our framework can be easily augmented to work
with any other consistent and comparable attribute
type. The only change required for incorpora-
tion of a new attribute type is the development of
attribute-specific Pboundary , Pvalues, and Pcompare
pattern groups; the rest of the system remains un-
changed.
In our evaluation we showed that our frame-
work achieves good results and significantly out-
performs the baseline commonly used for general
lexical attribute retrieval7.
While there is a growing justification to rely
on extensive manually created resources such as
Wikipedia, we have shown that in our case auto-
mated numerical attribute acquisition could be a
preferable option and provides excellent coverage
in comparison to handcrafted resources or man-
ual examination of the leading search engine re-
sults. Hence a promising direction would be to
use our approach in combination with Wikipedia
data and with additional manually created attribute
rich sources such as Web tables, to achieve the best
possible performance and coverage.
We would also like to explore the incorpora-
tion of approximate discovered numerical attribute
data into existing NLP tasks such as noun com-
pound classification and textual entailment.
7It should be noted, however, that in our DIRECT base-
line we used a basic pattern-based retrieval strategy; more
sophisticated strategies for value selection might bring better
results.
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo and Kazuhiko
Ohe. 2007 UTH: SVM-based Semantic Relation
Classification using Physical Sizes. Proceedings
of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007).
Somnath Banerjee, Soumen Chakrabarti and Ganesh
Ramakrishnan. 2009. Learning to Rank for Quan-
tity Consensus Queries. SIGIR ?09.
Michele Banko, Michael J Cafarella , Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. IJCAI
?07.
Matthew Berland, Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Michael Cafarella, Alon Halevy, Yang Zhang, Daisy
Zhe Wang and Eugene Wu. 2008. WebTables: Ex-
ploring the Power of Tables on the Web. VLDB ?08.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: mining the Web for fine-grained semantic verb
relations. EMNLP ?04.
Eric Crestan and Patrick Pantel. 2010. Web-Scale
Knowledge Extraction from Semi-Structured Ta-
bles. WWW ?10.
Dmitry Davidov and Ari Rappoport. 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL-Coling ?06.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. ACL ?08.
Dmitry Davidov and Ari Rappoport. 2008b. Unsu-
pervised Discovery of Generic Relationships Using
Pattern Clusters and its Evaluation by Automatically
Generated SAT Analogy Questions. ACL ?08.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1).
Marty Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Veronique Moriceau, 2006. Numerical Data Integra-
tion for Cooperative Question-Answering. EACL -
KRAQ06 ?06.
John Prager, 2006. Open-domain question-answering.
In Foundations and Trends in Information Re-
trieval,vol. 1, pp 91-231.
1316
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. COLING-ACL
?06.
Deepak Ravichandran and Eduard Hovy. 2002 Learn-
ing Surface Text Patterns for a Question Answering
System. ACL ?02.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. AAAI ?99.
Benjamin Rosenfeld and Ronen Feldman. 2007.
Clustering for unsupervised relation identification.
CIKM ?07.
Peter Turney, 2005. Measuring semantic similarity by
latent relational analysis, IJCAI ?05.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised Lexical acquisition. COL-
ING ?02.
1317
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 663?672,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Neutralizing Linguistically Problematic Annotations
in Unsupervised Dependency Parsing Evaluation
Roy Schwartz1 Omri Abend1? Roi Reichart2 Ari Rappoport1
1Institute of Computer Science
Hebrew University of Jerusalem
{roys02|omria01|arir}@cs.huji.ac.il
2Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
roiri@csail.mit.edu
Abstract
Dependency parsing is a central NLP task. In
this paper we show that the common eval-
uation for unsupervised dependency parsing
is highly sensitive to problematic annotations.
We show that for three leading unsupervised
parsers (Klein and Manning, 2004; Cohen and
Smith, 2009; Spitkovsky et al, 2010a), a small
set of parameters can be found whose mod-
ification yields a significant improvement in
standard evaluation measures. These param-
eters correspond to local cases where no lin-
guistic consensus exists as to the proper gold
annotation. Therefore, the standard evaluation
does not provide a true indication of algorithm
quality. We present a new measure, Neutral
Edge Direction (NED), and show that it greatly
reduces this undesired phenomenon.
1 Introduction
Unsupervised induction of dependency parsers is a
major NLP task that attracts a substantial amount
of research (Klein and Manning, 2004; Cohen et
al., 2008; Headden et al, 2009; Spitkovsky et al,
2010a; Gillenwater et al, 2010; Berg-Kirkpatrick
et al, 2010; Blunsom and Cohn, 2010, inter alia).
Parser quality is usually evaluated by comparing its
output to a gold standard whose annotations are lin-
guistically motivated. However, there are cases in
which there is no linguistic consensus as to what the
correct annotation is (Ku?bler et al, 2009). Examples
include which verb is the head in a verb group struc-
ture (e.g., ?can? or ?eat? in ?can eat?), and which
? Omri Abend is grateful to the Azrieli Foundation for the
award of an Azrieli Fellowship.
noun is the head in a sequence of proper nouns (e.g.,
?John? or ?Doe? in ?John Doe?). We refer to such
annotations as (linguistically) problematic. For such
cases, evaluation measures should not punish the al-
gorithm for deviating from the gold standard.
In this paper we show that the evaluation mea-
sures reported in current works are highly sensitive
to the annotation in problematic cases, and propose
a simple new measure that greatly neutralizes the
problem.
We start from the following observation: for three
leading algorithms (Klein and Manning, 2004; Co-
hen and Smith, 2009; Spitkovsky et al, 2010a), a
small set (at most 18 out of a few thousands) of pa-
rameters can be found whose modification dramati-
cally improves the standard evaluation measures (the
attachment score measure by 9.3-15.1%, and the
undirected measure by a smaller but still significant
1.3-7.7%). The phenomenon is implementation in-
dependent, occurring with several algorithms based
on a fundamental probabilistic dependency model1.
We show that these parameter changes can be
mapped to edge direction changes in local structures
in the dependency graph, and that these correspond
to problematic annotations. Thus, the standard eval-
uation measures do not reflect the true quality of the
evaluated algorithm.
We explain why the standard undirected evalua-
tion measure is in fact sensitive to such edge direc-
1It is also language-independent; we have produced it in five
different languages: English, Czech, Japanese, Portuguese, and
Turkish. Due to space considerations, in this paper we focus
on English, because it is the most studied language for this task
and the most practically useful one at present.
663
tion changes, and present a new evaluation measure,
Neutral Edge Direction (NED), which greatly allevi-
ates the problem by ignoring the edge direction in lo-
cal structures. Using NED, manual modifications of
model parameters always yields small performance
differences. Moreover, NED sometimes punishes
such manual parameter tweaking by yielding worse
results. We explain this behavior using an exper-
iment revealing that NED always prefers the struc-
tures that are more consistent with the modeling as-
sumptions lying in the basis of the algorithm. When
manual parameter modification is done against this
preference, the NED results decrease.
The contributions of this paper are as follows.
First, we show the impact of a small number of an-
notation decisions on the performance of unsuper-
vised dependency parsers. Second, we observe that
often these decisions are linguistically controversial
and therefore this impact is misleading. This reveals
a problem in the common evaluation of unsuper-
vised dependency parsing. This is further demon-
strated by noting that recent papers evaluate the task
using three gold standards which differ in such deci-
sions and which yield substantially different results.
Third, we present the NED measure, which is agnos-
tic to errors arising from choosing the non-gold di-
rection in such cases.
Section 2 reviews related work. Section 3 de-
scribes the performed parameter modifications. Sec-
tion 4 discusses the linguistic controversies in anno-
tating problematic dependency structures. Section 5
presents NED. Section 6 describes experiments with
it. A discussion is given in Section 7.
2 Related Work
Grammar induction received considerable attention
over the years (see (Clark, 2001; Klein, 2005) for
reviews). For unsupervised dependency parsing, the
Dependency Model with Valence (DMV) (Klein and
Manning, 2004) was the first to beat the simple
right-branching baseline. A technical description of
DMV is given at the end of this section.
The great majority of recent works, including
those experimented with in this paper, are elabora-
tions of DMV. Smith and Eisner (2005) improved
the DMV results by generalizing the function maxi-
mized by DMV?s EM training algorithm. Smith and
Eisner (2006) used a structural locality bias, experi-
menting on five languages. Cohen et al (2008) ex-
tended DMV by using a variational EM training al-
gorithm and adding logistic normal priors. Cohen
and Smith (2009, 2010) further extended it by us-
ing a shared logistic normal prior which provided a
new way to encode the knowledge that some POS
tags are more similar than others. A bilingual joint
learning further improved their performance.
Headden et al (2009) obtained the best reported
results on WSJ10 by using a lexical extension of
DMV. Gillenwater et al (2010) used posterior reg-
ularization to bias the training towards a small num-
ber of parent-child combinations. Berg-Kirkpatrick
et al (2010) added new features to the M step of the
DMV EM procedure. Berg-Kirkpatrick and Klein
(2010) used a phylogenetic tree to model parame-
ter drift between different languages. Spitkovsky
et al (2010a) explored several training protocols
for DMV. Spitkovsky et al (2010c) showed the
benefits of Viterbi (?hard?) EM to DMV training.
Spitkovsky et al (2010b) presented a novel lightly-
supervised approach that used hyper-text mark-up
annotation of web-pages to train DMV.
A few non-DMV-based works were recently pre-
sented. Daume? III (2009) used shift-reduce tech-
niques. Blunsom and Cohn (2010) used tree sub-
stitution grammar to achieve best results on WSJ?.
Druck et al (2009) took a semi-supervised ap-
proach, using a set of rules such as ?A noun is usu-
ally the parent of a determiner which is to its left?,
experimenting on several languages. Naseem et al
(2010) further extended this idea by using a single
set of rules which globally applies to six different
languages. The latter used a model similar to DMV.
The controversial nature of some dependency
structures was discussed in (Nivre, 2006; Ku?bler
et al, 2009). Klein (2005) discussed controversial
constituency structures and the evaluation problems
stemming from them, stressing the importance of a
consistent standard of evaluation.
A few works explored the effects of annotation
conventions on parsing performance. Nilsson et
al. (2006) transformed the dependency annotations
of coordinations and verb groups in the Prague
TreeBank. They trained the supervised MaltParser
(Nivre et al, 2006) on the transformed data, parsed
the test data and re-transformed the resulting parse,
664
w3 w2 w1
(a)
w3 w2 w1
(b)
Figure 1: A dependency structure on the words
w1, w2, w3 before (Figure 1(a)) and after (Figure 1(b))
an edge-flip of w2?w1.
thus improving performance. Klein and Manning
(2004) observed that a large portion of their errors is
caused by predicting the wrong direction of the edge
between a noun and its determiner. Ku?bler (2005)
compared two different conversion schemes in Ger-
man supervised constituency parsing and found one
to have positive influence on parsing quality.
Dependency Model with Valence (DMV). DMV
(Klein and Manning, 2004) defines a probabilistic
grammar for unlabeled dependency structures. It is
defined as follows: the root of the sentence is first
generated, and then each head recursively generates
its right and left dependents. The parameters of the
model are of two types: PSTOP and PATTACH .
PSTOP (dir, h, adj) determines the probability to
stop generating arguments, and is conditioned on 3
arguments: the head h, the direction dir ((L)eft
or (R)ight) and adjacency adj (whether the head
already has dependents ((Y )es) in direction dir or
not ((N)o)). PATTACH(arg|h, dir) determines the
probability to generate arg as head h?s dependent in
direction dir.
3 Significant Effects of Edge Flipping
In this section we present recurring error patterns
in some of the leading unsupervised dependency
parsers. These patterns are all local, confined to a
sequence of up to three words (but mainly of just
two consecutive words). They can often be mended
by changing the directions of a few types of edges.
The modified parameters described in this section
were handpicked to improve performance: we ex-
amined the local parser errors occurring the largest
number of times, and found the corresponding pa-
rameters. Note that this is a valid methodology,
since our goal is not to design a new algorithm but
to demonstrate that modifying a small set of param-
eters can yield a major performance boost and even-
tually discover problems with evaluation methods or
algorithms.
I
PRP
want
VBP
to
TO
eat
VB
.
ROOT
Figure 2: A parse of the sentence ?I want to eat?, before
(straight line) and after (dashed line) an edge-flip of the
edge ?to???eat?.
We start with a few definitions. Consider Fig-
ure 1(a) that shows a dependency structure on the
words w1, w2, w3. Edge flipping (henceforth, edge-
flip) the edge w2?w1 is the following modification
of a parse tree: (1) setting w2?s parent as w1 (instead
of the other way around), and (2) setting w1?s par-
ent as w3 (instead of the edge w3?w2). Figure 1(b)
shows the dependency structure after the edge-flip.
Note that (1) imposes setting a new parent to w2,
as otherwise it would have had no parent. Setting
this parent to be w3 is the minimal modification of
the original parse, since it does not change the at-
tachment of the structure [w2, w1] to the rest of the
sentence, but only the direction of the internal edge.
Figure 2 presents a parse of the sentence ?I want
to eat?, before and after an edge-flip of the edge
?to???eat?.
Since unsupervised dependency parsers are gen-
erally structure prediction models, the predictions
of the parse edges are not independent. Therefore,
there is no single parameter which completely con-
trols the edge direction, and hence there is no direct
way to perform an edge-flip by parameter modifica-
tion. However, setting extreme values for the param-
eters controlling the direction of a certain edge type
creates a strong preference towards one of the direc-
tions, and effectively determines the edge direction.
This procedure is henceforth termed parameter-flip.
We show that by performing a few parameter-
flips, a substantial improvement in the attachment
score can be obtained. Results are reported for three
algorithms.
Parameter Changes. All the works experimented
with in this paper are not lexical and use sequences
of POS tags as their input. In addition, they all use
the DMV parameter set (PSTOP and PATTACH) for
parsing. We will henceforth refer to this set, condi-
tioned on POS tags, as the model parameter set.
We show how an edge in the dependency graph
is encoded using the DMV parameters. Say the
665
model prefers setting ?to? (POS tag: TO) as a de-
pendent of the infinitive verb (POS tag: V B) to its
right (e.g., ?to eat?). This is reflected by a high
value of PATTACH(TO|V B,L), a low value of
PATTACH(V B|TO,R), since ?to? tends to be a left
dependent of the verb and not the other way around,
and a low value of PSTOP (V B,L,N), as the verb
usually has at least one left argument (i.e., ?to?).
A parameter-flip of w1?w2 is hence performed
by setting PATTACH(w2|w1, R) to a very low
value and PATTACH(w1|w2, L) to a very high
value. When the modifications to PATTACH
are insufficient to modify the edge direction,
PSTOP (w2, L,N) is set to a very low value and
PSTOP (w1, R,N) to a very high value2.
Table 1 describes the changes made for the three
algorithms. The ?+? signs in the table correspond to
edges in which the algorithm disagreed with the gold
standard, and were thus modified. Similarly, the ???
signs in the table correspond to edges in which the
algorithm agreed with the gold standard, and were
thus not modified. The number of modified param-
eters does not exceed 18 (out of a few thousands).
The Freq. column in the table shows the percent-
age of the tokens in sections 2-21 of PTB WSJ that
participate in each structure. Equivalently, the per-
centage of edges in the corpus which are of either
of the types appearing in the Orig. Edge column.
As the table shows, the modified structures cover a
significant portion of the tokens. Indeed, 42.9% of
the tokens in the corpus participate in at least one of
them3.
Experimenting with Edge Flipping. We experi-
mented with three DMV-based algorithms: a repli-
cation of (Klein and Manning, 2004), as appears in
(Cohen et al, 2008) (henceforth, km04), Cohen and
Smith (2009) (henceforth, cs09), and Spitkovsky et
al. (2010a) (henceforth, saj10a). Decoding is done
using the Viterbi algorithm4. For each of these algo-
rithms we present the performance gain when com-
pared to the original parameters.
The training set is sections 2-21 of the Wall Street
2Note that this yields unnormalized models. Again, this is
justified since the resulting model is only used as a basis for
discussion and is not a fully fledged algorithm.
3Some tokens participate in more than one structure.
4http://www.cs.cmu.edu/?scohen/parser.html.
Structure Freq. Orig. Edge km04 cs09 saj10a
Coordination
(?John & Mary?) 2.9% CC?NNP ? + ?
Prepositional
Phrase (?in
the house?)
32.7%
DT?NN + + +
DT?NNP ? + +
DT?NNS ? ? +
IN?DT + + ?
IN?NN + + ?
IN?NNP + ? ?
IN?NNS ? + ?
PRP$?NN ? ? +
Modal Verb
(?can eat?) 2.4% MD?V B ? + ?
Infinitive Verb
(?to eat?) 4.5% TO?V B ? + +
Proper Name
Sequence
(?John Doe?)
18.5% NNP?NNP + ? ?
Table 1: Parameter changes for the three algorithms. The
Freq. column shows what percentage of the tokens in sec-
tions 2-21 of PTB WSJ participate in each structure. The
Orig. column indicates the original edge. The modified
edge is of the opposite direction. The other columns show
the different algorithms: km04: basic DMV model (repli-
cation of (Klein and Manning, 2004)); cs09; (Cohen and
Smith, 2009); saj10a: (Spitkovsky et al, 2010a).
Journal Penn TreeBank (Marcus et al, 1993). Test-
ing is done on section 23. The constituency annota-
tion was converted to dependencies using the rules
of (Yamada and Matsumoto, 2003)5.
Following standard practice, we present the at-
tachment score (i.e., percentage of words that have a
correct head) of each algorithm, with both the origi-
nal parameters and the modified ones. We present
results both on all sentences and on sentences of
length ? 10, excluding punctuation.
Table 2 shows results for all algorithms6. The
performance difference between the original and the
modified parameter set is considerable for all data
sets, where differences exceed 9.3%, and go up to
15.1%. These are enormous differences from the
perspective of current algorithm evaluation results.
4 Linguistically Problematic Annotations
In this section, we discuss the controversial nature
of the annotation in the modified structures (Ku?bler
5http://www.jaist.ac.jp/?h-yamada/
6Results are slightly worse than the ones published in the
original papers due to the different decoding algorithms (cs09
use MBR while we used Viterbi) and a different conversion pro-
cedure (saj10a used (Collins, 1999) and not (Yamada and Mat-
sumoto, 2003)) ; see Section 5.
666
Algo. ? 10 ? ?
Orig. Mod. ? Orig. Mod. ?
km04 45.8 59.8 14 34.6 43.9 9.3
cs09 60.9 72.9 12 39.9 54.6 14.7
saj10a 54.7 69.8 15.1 41.6 54.3 12.7
Table 2: Results of the original (Orig. columns), the
modified (Mod. columns) parameter sets and their dif-
ference (? columns) for the three algorithms.
et al, 2009). We remind the reader that structures
for which no linguistic consensus exists as to their
correct annotation are referred to as (linguistically)
problematic.
We begin by showing that all the structures mod-
ified are indeed linguistically problematic. We then
note that these controversies are reflected in the eval-
uation of this task, resulting in three, significantly
different, gold standards currently in use.
Coordination Structures are composed of two
proper nouns, separated by a conjunctor (e.g., ?John
and Mary?). It is not clear which token should be the
head of this structure, if any (Nilsson et al, 2006).
Prepositional Phrases (e.g., ?in the house? or ?in
Rome?), where every word is a reasonable candidate
to head this structure. For example, in the annotation
scheme used by (Collins, 1999) the preposition is the
head, in the scheme used by (Johansson and Nugues,
2007) the noun is the head, while TUT annotation,
presented in (Bosco and Lombardo, 2004), takes the
determiner to be the noun?s head.
Verb Groups are composed of a verb and an aux-
iliary or a modal verb (e.g., ?can eat?). Some
schemes choose the modal as the head (Collins,
1999), others choose the verb (Rambow et al, 2002).
Infinitive Verbs (e.g., ?to eat?) are also in contro-
versy, as in (Yamada and Matsumoto, 2003) the verb
is the head while in (Collins, 1999; Bosco and Lom-
bardo, 2004) the ?to? token is the head.
Sequences of Proper Nouns (e.g., ?John Doe?)
are also subject to debate, as PTB?s scheme takes the
last proper noun as the head, and BIO?s scheme de-
fines a more complex scheme (Dredze et al, 2007).
Evaluation Inconsistency Across Papers. A fact
that may not be recognized by some readers is that
comparing the results of unsupervised dependency
parsers across different papers is not directly pos-
sible, since different papers use different gold stan-
dard annotations even when they are all derived from
the Penn Treebank constituency annotation. This
happens because they use different rules for con-
verting constituency annotation to dependency an-
notation. A probable explanation for this fact is that
people have tried to correct linguistically problem-
atic annotations in different ways, which is why we
note this issue here7.
There are three different annotation schemes
in current use: (1) Collins head rules (Collins,
1999), used in e.g., (Berg-Kirkpatrick et al, 2010;
Spitkovsky et al, 2010a); (2) Conversion rules of
(Yamada and Matsumoto, 2003), used in e.g., (Co-
hen and Smith, 2009; Gillenwater et al, 2010); (3)
Conversion rules of (Johansson and Nugues, 2007)
used, e.g., in the CoNLL shared task 2007 (Nivre et
al., 2007) and in (Blunsom and Cohn, 2010).
The differences between the schemes are substan-
tial. For instance, 14.4% of section 23 is tagged dif-
ferently by (1) and (2)8.
5 The Neutral Edge Direction (NED)
Measure
As shown in the previous sections, the annotation
of problematic edges can substantially affect perfor-
mance. This was briefly discussed in (Klein and
Manning, 2004), which used undirected evaluation
as a measure which is less sensitive to alternative
annotations. Undirected accuracy was commonly
used since to assess the performance of unsuper-
vised parsers (e.g., (Smith and Eisner, 2006; Head-
den et al, 2008; Spitkovsky et al, 2010a)) but also
of supervised ones (Wang et al, 2005; Wang et al,
2006). In this section we discuss why this measure
is in fact not indifferent to edge-flips and propose a
new measure, Neutral Edge Direction (NED).
7Indeed, half a dozen flags in the LTH Constituent-to-
Dependency Conversion Tool (Johansson and Nugues, 2007)
are used to control the conversion in problematic cases.
8In our experiments we used the scheme of (Yamada and
Matsumoto, 2003), see Section 3. The significant effects of
edge flipping were observed with the other two schemes as well.
667
w1
w2
w3
(a)
w1
w3
w2
(b)
w4
w3
w2
(c)
Figure 3: A dependency structure on the words
w1, w2, w3 before (Figure 3(a)) and after (Figure 3(b)) an
edge-flip of w2?w3, and when the direction of the edge
between w2 and w3 is switched and the new parent of w3
is set to be some other word, w4 (Figure 3(c)).
Undirected Evaluation. The measure is defined
as follows: traverse over the tokens and mark a cor-
rect attachment if the token?s induced parent is either
(1) its gold parent or (2) its gold child. The score is
the ratio of correct attachments and the number of
tokens.
We show that this measure does not ignore edge-
flips. Consider Figure 3 that shows a depen-
dency structure on the words w1, w2, w3 before (Fig-
ure 3(a)) and after (Figure 3(b)) an edge-flip of
w2?w3. Assume that 3(a) is the gold standard and
that 3(b) is the induced parse. Consider w2. Its
induced parent (w3) is its gold child, and thus undi-
rected evaluation does not consider it an error. On
the other hand, w3 is assigned w2?s gold parent, w1.
This is considered an error, since w1 is neither w3?s
gold parent (as it is w2), nor its gold child9. There-
fore, one of the two tokens involved in the edge-flip
is penalized by the measure.
Recall the example ?I want to eat? and the edge-
flip of the edge ?to???eat? (Figure 2). As ?to??s
parent in the induced graph (?want?) is neither its
gold parent nor its gold child, the undirected evalu-
ation measure marks it as an error. This is an exam-
ple where an edge-flip in a problematic edge, which
should not be considered an error, was in fact con-
sidered an error by undirected evaluation.
Neutral Edge Direction (NED). The NED measure
is a simple extension of the undirected evaluation
measure10. Unlike undirected evaluation, NED ig-
nores all errors directly resulting from an edge-flip.
9Otherwise, the gold parse would have contained a
w1?w2?w3?w1 cycle.
10An implementation of NED is available at
http://www.cs.huji.ac.il/?roys02/software/ned.html
NED is defined as follows: traverse over the to-
kens and mark a correct attachment if the token?s in-
duced parent is either (1) its gold parent (2) its gold
child or (3) its gold grandparent. The score is the ra-
tio of correct attachments and the number of tokens.
NED, by its definition, ignores edge-flips. Con-
sider again Figure 3, where we assume that 3(a) is
the gold standard and that 3(b) is the induced parse.
Much like undirected evaluation, NED will mark the
attachment of w2 as correct, since its induced parent
is its gold child. However, unlike undirected evalua-
tion, w3?s induced attachment will also be marked as
correct, as its induced parent is its gold grandparent.
Now consider another induced parse in which the
direction of the edge between w2 and w3 is switched
and the w3?s parent is set to be some other word,
w4 (Figure 3(c)). This should be marked as an er-
ror, even if the direction of the edge between w2 and
w3 is controversial, since the structure [w2, w3] is no
longer a dependent of w1. It is indeed a NED error.
Note that undirected evaluation gives the parses in
Figure 3(b) and Figure 3(c) the same score, while if
the structure [w2, w3] is problematic, there is a major
difference in their correctness.
Discussion. Problematic structures are ubiquitous,
with more than 40% of the tokens in PTB WSJ
appearing in at least one of them (see Section 3).
Therefore, even a substantial difference in the at-
tachment between two parsers is not necessarily in-
dicative of a true quality difference. However, an at-
tachment score difference that persists under NED is
an indication of a true quality difference, since gen-
erally problematic structures are local (i.e., obtained
by an edge-flip) and NED ignores such errors.
Reporting NED alone is insufficient, as obviously
the edge direction does matter in some cases. For
example, in adjective?noun structures (e.g., ?big
house?), the correct edge direction is widely agreed
upon (?big???house?) (Ku?bler et al, 2009), and
thus choosing the wrong direction should be con-
sidered an error. Therefore, we suggest evaluating
using both NED and attachment score in order to get
a full picture of the parser?s performance.
A possible criticism on NED is that it is only in-
different to alternative annotations in structures of
size 2 (e.g., ?to eat?) and does not necessarily handle
larger problematic structures, such as coordinations
668
ROOT
John
and Mary
(a)
ROOT
John
and
Mary
(b)
ROOT
in
house
the
(c)
ROOT
in
the
house
(d)
ROOT
house
in
the
(e)
Figure 4: Alternative parses of ?John and Mary? and ?in
the house?. Figure 4(a) follows (Collins, 1999), Fig-
ure 4(b) follows (Johansson and Nugues, 2007). Fig-
ure 4(c) follows (Collins, 1999; Yamada and Matsumoto,
2003). Figure 4(d) and Figure 4(e) show induced parses
made by (km04,saj10a) and cs09, respectively.
(see Section 4). For example, Figure 4(a) and Fig-
ure 4(b) present two alternative annotations of the
sentence ?John and Mary?. Assume the parse in Fig-
ure 4(a) is the gold parse and that in Figure 4(b) is
the induced parse. The word ?Mary? is a NED error,
since its induced parent (?and?) is neither its gold
child nor its gold grandparent. Thus, NED does not
accept all possible annotations of structures of size
3. On the other hand, using a method which accepts
all possible annotations of structures of size 3 seems
too permissive. A better solution may be to modify
the gold standard annotation, so to explicitly anno-
tate problematic structures as such. We defer this
line of research to future work.
NED is therefore an evaluation measure which is
indifferent to edge-flips, and is consequently less
sensitive to alternative annotations. We now show
that NED is indifferent to the differences between the
structures originally learned by the algorithms men-
tioned in Section 3 and the gold standard annotation
in all the problematic cases we consider.
Most of the modifications made are edge-flips,
and are therefore ignored by NED. The exceptions
are coordinations and prepositional phrases which
are structures of size 3. In the former, the alter-
native annotations differ only in a single edge-flip
(i.e., CC?NNP ), and are thus not NED errors. Re-
garding prepositional phrases, Figure 4(c) presents
the gold standard of ?in the house?, Figure 4(d) the
parse induced by km04 and saj10a and Figure 4(e)
the parse induced by cs09. As the reader can verify,
both induced parses receive a perfect NED score.
In order to further demonstrate NED?s insensitiv-
ity to alternative annotations, we took two of the
three common gold standard annotations (see Sec-
tion 4) and evaluated them one against the other. We
considered section 23 of WSJ following the scheme
of (Yamada and Matsumoto, 2003) as the gold stan-
dard and of (Collins, 1999) as the evaluated set. Re-
sults show that the attachment score is only 85.6%,
the undirected accuracy is improved to 90.3%, while
the NED score is 95.3%. This shows that NED is sig-
nificantly less sensitive to the differences between
the different annotation schemes, compared to the
other evaluation measures.
6 Experimenting with NED
In this section we show that NED indeed reduces
the performance difference between the original and
the modified parameter sets, thus providing empiri-
cal evidence for its validity. For brevity, we present
results only for the entire WSJ corpus. Results on
WSJ10 are similar. The datasets and decoding algo-
rithms are the same as those used in Section 3.
Table 3 shows the score differences between the
parameter sets using attachment score, undirected
evaluation and NED. A substantial difference per-
sists under undirected evaluation: a gap of 7.7% in
cs09, of 3.5% in saj10a and of 1.3% in km04.
The differences are further reduced using NED.
This is consistent with our discussion in Section 5,
and shows that undirected evaluation only ignores
some of the errors inflicted by edge-flips.
For cs09, the difference is substantially reduced,
but a 4.2% performance gap remains. For km04 and
saj10a, the original parameters outperform the new
ones by 3.6% and 1% respectively.
We can see that even when ignoring edge-flips,
some difference remains, albeit not necessarily in
the favor of the modified models. This is because
we did not directly perform edge-flips, but rather
parameter-flips. The difference is thus a result of
second-order effects stemming from the parameter-
flips. In the next section, we explain why the remain-
ing difference is positive for some algorithms (cs09)
and negative for others (km04, saj10a).
For completeness, Table 4 shows a comparison of
some of the current state-of-the-art algorithms, using
attachment score, undirected evaluation and NED.
The training and test sets are those used in Section 3.
The table shows that the relative orderings of the al-
gorithms under NED is different than under the other
669
Algo. Mod. ? Orig.Attach. Undir. NED
km04 9.3 (43.9?34.6) 1.3 (54.2?52.9) ?3.6 (63?66.6)
cs09 14.7 (54.6?39.9) 7.7 (56.9?49.2) 4.2 (66.8?62.6)
saj10a 12.7 (54.3?41.6) 3.5 (59.4?55.9) ?1 (66.8?67.8)
Table 3: Differences between the modified and original
parameter sets when evaluated using attachment score
(Attach.), undirected evaluation (Undir.), and NED.
measures. This is an indication that NED provides a
different perspective on algorithm quality11 .
Algo. Att10 Att? Un10 Un? NED10 NED?
bbdk10 66.1 49.6 70.1 56.0 75.5 61.8
bc10 67.2 53.6 73 61.7 81.6 70.2
cs09 61.5 42 66.9 50.4 81.5 62.9
gggtp10 57.1 45 62.5 53.2 80.4 65.1
km04 45.8 34.6 60.3 52.9 78.4 66.6
saj10a 54.7 41.6 66.5 55.9 78.9 67.8
saj10c 63.8 46.1 72.6 58.8 84.2 70.8
saj10b? 67.9 48.2 74.0 57.7 86.0 70.7
Table 4: A comparison of recent works, using Att (at-
tachment score) Un (undirected evaluation) and NED, on
sentences of length ? 10 (excluding punctuation) and
on all sentences. The gold standard is obtained using
the rules of (Yamada and Matsumoto, 2003). bbdk10:
(Berg-Kirkpatrick et al, 2010), bc10: (Blunsom and
Cohn, 2010), cs09: (Cohen and Smith, 2009), gggtp10:
(Gillenwater et al, 2010), km04: A replication of (Klein
and Manning, 2004), saj10a: (Spitkovsky et al, 2010a),
saj10c: (Spitkovsky et al, 2010c), saj10b?: A lightly-
supervised algorithm (Spitkovsky et al, 2010b).
7 Discussion
In this paper we explored two ways of dealing with
cases in which there is no clear theoretical justifi-
cation to prefer one dependency structure over an-
other. Our experiments suggest that it is crucial to
deal with such structures if we would like to have
a proper evaluation of unsupervised parsing algo-
rithms against a gold standard.
The first way was to modify the parameters of the
parsing algorithms so that in cases where such prob-
lematic decisions are to be made they follow the gold
standard annotation. Indeed, this modification leads
to a substantial improvement in the attachment score
of the algorithms.
11Results may be different than the ones published in the
original papers due to the different conversion procedures used
in each work. See Section 4 for discussion.
The second way was to change the evaluation.
The NED measure we proposed does not punish for
differences between gold and induced structures in
the problematic cases. Indeed, in Section 6 (Table 3)
we show that the differences between the original
and modified models are much smaller when eval-
uating with NED compared to when evaluating with
the traditional attachment score.
As Table 3 reveals, however, even when evaluat-
ing with NED, there is still some difference between
the original and the modified model, for each of the
algorithms we consider. Moreover, for two of the al-
gorithms (km04 and saj10a) NED prefers the original
model while for one (cs09) it prefers the modified
version. In this section we explain these patterns and
show that they are both consistent and predictable.
Our hypothesis, for which we provide empirical
justification, is that in cases where there is no theo-
retically preferred annotation, NED prefers the struc-
tures that are more learnable by DMV. That is, NED
gives higher scores to the annotations that better fit
the assumptions and modeling decisions of DMV,
the model that lies in the basis of the parsing algo-
rithms.
To support our hypothesis we perform an experi-
ment requiring two preparatory steps for each algo-
rithm. First, we construct a supervised version of
the algorithm. This supervised version consists of
the same statistical model as the original unsuper-
vised algorithm, but the parameters are estimated to
maximize the likelihood of a syntactically annotated
training corpus, rather than of a plain text corpus.
Second, we construct two corpora for the algo-
rithm, both consist of the same text and differ only
in their syntactic annotation. The first is annotated
with the gold standard annotation. The second is
similarly annotated except in the linguistically prob-
lematic structures. We replace these structures with
the ones that would have been created with the un-
supervised version of the algorithm (see Table 1 for
the relevant structures for each algorithm)12. Each
12In cases the structures are comprised of a single edge, the
second corpus is obtained from the gold standard by an edge-
flip. The only exceptions are the cases of the prepositional
phrases. Their gold standard and the learned structures for each
of the algorithms are shown in Figure 4. In this case, the sec-
ond corpus is obtained from the gold standard by replacing each
prepositional phrase in the gold standard with the corresponding
670
corpus is divided into a training and a test set.
We then train the supervised version of the algo-
rithms on each of the training sets. We parse the test
data twice, once with each of the resulting models.
We evaluate both parsed corpora against the corpus
annotation from which they originated.
The training set of each corpus consists of sec-
tions 2?21 of WSJ20 (i.e., WSJ sentences of length
?20, excluding punctuation)13 and the test set is sec-
tion 23 of WSJ?. Evaluation is performed using
both NED and attachment score. The patterns we
observed are very similar for both. For brevity, we
report only attachment score results.
km04 cs09 saj10a
Orig. Gold Orig. Gold Orig. Gold
NED,
Unsup. 66.6 63 62.6 66.8 67.8 66.8
Sup. 71.3 69.9 63.3 69.9 71.8 69.9
Table 5: The first line shows the NED results from
Section 6, when using the original parameters (Orig.
columns) and the modified parameters (Gold columns).
The second line shows the results of the supervised ver-
sions of the algorithms using the corpus which agrees
with the unsupervised model in the problematic cases
(Orig.) and the gold standard (Gold).
The results of our experiment are presented in Ta-
ble 5 along with a comparison to the NED scores
from Section 6. The table clearly demonstrates that a
set of parameters (original or modified) is preferred
by NED in the unsupervised experiments reported in
Section 6 (top line) if and only if the structures pro-
duced by this set are better learned by the supervised
version of the algorithm (bottom line).
This observation supports our hypothesis that in
cases where there is no theoretical preference for
one structure over the other, NED (unlike the other
measures) prefers the structures that are more con-
sistent with the modeling assumptions lying in the
basis of the algorithm. We consider this to be a de-
sired property of a measure since a more consistent
model should be preferred where no theoretical pref-
erence exists.
learned structure.
13In using WSJ20, we follow (Spitkovsky et al, 2010a),
which showed that training the DMV on sentences of bounded
length yields a higher score than using the entire corpus. We
use it as we aim to use an optimal setting.
8 Conclusion
In this paper we showed that the standard evalua-
tion of unsupervised dependency parsers is highly
sensitive to problematic annotations. We modified a
small set of parameters that controls the annotation
in such problematic cases in three leading parsers.
This resulted in a major performance boost, which
is unindicative of a true difference in quality.
We presented Neutral Edge Direction (NED), a
measure that is less sensitive to the annotation of
local structures. As the problematic structures are
generally local, NED is less sensitive to their alterna-
tive annotations. In the future, we suggest reporting
NED along with the current measures.
Acknowledgements. We would like to thank Shay
Cohen for his assistance with his implementation of
the DMV parser and Taylor Berg-Kirkpatrick, Phil
Blunsom and Jennifer Gillenwater for providing us
with their data sets. We would also like to thank
Valentin I. Spitkovsky for his comments and for pro-
viding us with his data sets.
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero and Dan Klein, 2010. Painless unsu-
pervised learning with features. In Proc. of NAACL.
Taylor Berg-Kirkpatrick and Dan Klein, 2010. Phyloge-
netic Grammar Induction. In Proc. of ACL.
Cristina Bosco and Vincenzo Lombardo, 2004. Depen-
dency and relational structure in treebank annotation.
In Proc. of the Workshop on Recent Advances in De-
pendency Grammar at COLING?04.
Phil Blunsom and Trevor Cohn, 2010. Unsupervised
Induction of Tree Substitution Grammars for Depen-
dency Parsing. In Proc. of EMNLP.
Shay B. Cohen, Kevin Gimpel and Noah A. Smith, 2008.
Logistic Normal Priors for Unsupervised Probabilistic
Grammar Induction. In Proc. of NIPS.
Shay B. Cohen and Noah A. Smith, 2009. Shared Logis-
tic Normal Distributions for Soft Parameter Tying. In
Proc. of HLT-NAACL.
Michael J. Collins, 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania, Philadelphia.
Alexander Clark, 2001. Unsupervised language acquisi-
tion: theory and practice. Ph.D. thesis, University of
Sussex.
Hal Daume? III, 2009. Unsupervised search-based struc-
tured prediction. In Proc. of ICML.
671
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuz-
man Ganchev, Joa?o V. Grac?a and Fernando Pereira,
2007. Frustratingly Hard Domain Adaptation for De-
pendency Parsing. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
Gregory Druck, Gideon Mann and Andrew McCal-
lum, 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In
Proc. of ACL.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o V. Grac?a,
Ben Taskar and Fernando Preira, 2010. Sparsity in
dependency grammar induction. In Proc. of ACL.
William P. Headden III, David McClosky, and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. In Proc. of
COLING.
William P. Headden III, Mark Johnson and David Mc-
Closky, 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Proc.
of HLT-NAACL.
Richard Johansson and Pierre Nugues, 2007. Ex-
tended Constituent-to-Dependency Conversion for En-
glish. In Proc. of NODALIDA.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proc. of ACL.
Sandra Ku?bler, 2005. How Do Treebank Annotation
Schemes Influence Parsing Results? Or How Not to
Compare Apples And Oranges. In Proc. of RANLP.
Sandra Ku?bler, R. McDonald and Joakim Nivre, 2009.
Dependency Parsing. Morgan And Claypool Publish-
ers.
Mitchell Marcus, Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn treebank. Computational
Linguistics 19:313-330.
Tahira Naseem, Harr Chen, Regina Barzilay and Mark
Johnson, 2010. Using universal linguistic knowledge
to guide grammar induction. In Proc. of EMNLP.
Joakim Nivre, 2006. Inductive Dependency Parsing.
Springer.
Joakim Nivre, Johan Hall and Jens Nilsson, 2006. Malt-
Parser: A data-driven parser-generator for depen-
dency parsing. In Proc. of LREC-2006.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel and Deniz Yuret,
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task, EMNLP-
CoNLL, 2007.
Jens Nilsson, Joakim Nivre and Johan Hall, 2006. Graph
transformations in data-driven dependency parsing.
In Proc. of ACL.
Owen Rambow, Cassandre Creswell, Rachel Szekely,
Harriet Tauber and Marilyn Walker, 2002. A depen-
dency treebank for English. In Proc. of LREC.
Noah A. Smith and Jason Eisner, 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proc. of IJCAI.
Noah A. Smith and Jason Eisner, 2006. Annealing struc-
tural bias in multilingual weighted grammar induc-
tion. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010a. From Baby Steps to Leapfrog: How ?Less
is More? in Unsupervised Dependency Parsing. In
Proc. of NAACL-HLT.
Valentin I. Spitkovsky, Hiyan Alshawi and Daniel Juraf-
sky, 2010b. Profiting from Mark-Up: Hyper-Text An-
notations for Guided Parsing. In Proc. of ACL.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky
and Christopher D. Manning, 2010c. Viterbi training
improves unsupervised dependency parsing. In Proc.
of CoNLL.
Qin Iris Wang, Dale Schuurmans and Dekang Lin, 2005.
Strictly Lexical Dependency Parsing. In IWPT.
Qin Iris Wang, Colin Cherry, Dan Lizotte and Dale Schu-
urmans, 2006. Improved Large Margin Dependency
Parsing via Local Constraints and Laplacian Regular-
ization. In Proc. of CoNLL.
Hiroyasu Yamada and Yuji Matsumoto, 2003. Statistical
dependency analysis with support vector machines. In
Proc. of the International Workshop on Parsing Tech-
nologies.
672
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228?238,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Conceptual Cognitive Annotation (UCCA)
Omri Abend?
Institute of Computer Science
The Hebrew University
omria01@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
arir@cs.huji.ac.il
Abstract
Syntactic structures, by their nature, re-
flect first and foremost the formal con-
structions used for expressing meanings.
This renders them sensitive to formal vari-
ation both within and across languages,
and limits their value to semantic ap-
plications. We present UCCA, a novel
multi-layered framework for semantic rep-
resentation that aims to accommodate the
semantic distinctions expressed through
linguistic utterances. We demonstrate
UCCA?s portability across domains and
languages, and its relative insensitivity
to meaning-preserving syntactic variation.
We also show that UCCA can be ef-
fectively and quickly learned by annota-
tors with no linguistic background, and
describe the compilation of a UCCA-
annotated corpus.
1 Introduction
Syntactic structures are mainly committed to rep-
resenting the formal patterns of a language, and
only indirectly reflect semantic distinctions. For
instance, while virtually all syntactic annotation
schemes are sensitive to the structural difference
between (a) ?John took a shower? and (b) ?John
showered?, they seldom distinguish between (a)
and the markedly different (c) ?John took my
book?. In fact, the annotations of (a) and (c) are
identical under the most widely-used schemes for
English, the Penn Treebank (PTB) (Marcus et al,
1993) and CoNLL-style dependencies (Surdeanu
et al, 2008) (see Figure 1).
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
Underscoring the semantic similarity between
(a) and (b) can assist semantic applications. One
example is machine translation to target languages
that do not express this structural distinction (e.g.,
both (a) and (b) would be translated to the same
German sentence ?John duschte?). Question An-
swering applications can also benefit from dis-
tinguishing between (a) and (c), as this knowl-
edge would help them recognize ?my book? as a
much more plausible answer than ?a shower? to
the question ?what did John take??.
This paper presents a novel approach to gram-
matical representation that annotates semantic dis-
tinctions and aims to abstract away from specific
syntactic constructions. We call our approach Uni-
versal Conceptual Cognitive Annotation (UCCA).
The word ?cognitive? refers to the type of cate-
gories UCCA uses and its theoretical underpin-
nings, and ?conceptual? stands in contrast to ?syn-
tactic?. The word ?universal? refers to UCCA?s
capability to accommodate a highly rich set of se-
mantic distinctions, and its aim to ultimately pro-
vide all the necessary semantic information for
learning grammar. In order to accommodate this
rich set of distinctions, UCCA is built as a multi-
layered structure, which allows for its open-ended
extension. This paper focuses on the foundational
layer of UCCA, a coarse-grained layer that rep-
resents some of the most important relations ex-
pressed through linguistic utterances, including ar-
gument structure of verbs, nouns and adjectives,
and the inter-relations between them (Section 2).
UCCA is supported by extensive typologi-
cal cross-linguistic evidence and accords with
the leading Cognitive Linguistics theories. We
build primarily on Basic Linguistic Theory (BLT)
(Dixon, 2005; 2010a; 2010b; 2012), a typological
approach to grammar successfully used for the de-
228
scription of a wide variety of languages. BLT uses
semantic similarity as its main criterion for cate-
gorizing constructions both within and across lan-
guages. UCCA takes a similar approach, thereby
creating a set of distinctions that is motivated
cross-linguistically. We demonstrate UCCA?s rel-
ative insensitivity to paraphrasing and to cross-
linguistic variation in Section 4.
UCCA is exceptional in (1) being a semantic
scheme that abstracts away from specific syntactic
forms and is not defined relative to a specific do-
main or language, (2) providing a coarse-grained
representation which allows for open-ended ex-
tension, and (3) using cognitively-motivated cat-
egories. An extensive comparison of UCCA to ex-
isting approaches to syntactic and semantic repre-
sentation, focusing on the major resources avail-
able for English, is found in Section 5.
This paper also describes the compilation of a
UCCA-annotated corpus. We provide a quanti-
tative assessment of the annotation quality. Our
results show a quick learning curve and no sub-
stantial difference in the performance of annota-
tors with and without background in linguistics.
This is an advantage of UCCA over its syntactic
counterparts that usually need annotators with ex-
tensive background in linguistics (see Section 3).
We note that UCCA?s approach that advocates
automatic learning of syntax from semantic super-
vision stands in contrast to the traditional view of
generative grammar (Clark and Lappin, 2010).
2 The UCCA Scheme
2.1 The Formalism
UCCA uses directed acyclic graphs (DAGs) to
represent its semantic structures. The atomic
meaning-bearing units are placed at the leaves of
the DAG and are called terminals. In the founda-
tional layer, terminals are words and multi-word
chunks, although this definition can be extended
to include arbitrary morphemes.
The nodes of the graph are called units. A unit
may be either (i) a terminal or (ii) several ele-
ments jointly viewed as a single entity according
to some semantic or cognitive consideration. In
many cases, a non-terminal unit is comprised of a
single relation and the units it applies to (its argu-
ments), although in some cases it may also contain
secondary relations. Hierarchy is formed by using
units as arguments or relations in other units.
Categories are annotated over the graph?s edges,
and represent the descendant unit?s role in forming
the semantics of the parent unit. Therefore, the in-
ternal structure of a unit is represented by its out-
bound edges and their categories, while the roles
a unit plays in the relations it participates in are
represented by its inbound edges.
We note that UCCA?s structures reflect a single
interpretation of the text. Several discretely dif-
ferent interpretations (e.g., high vs. low PP at-
tachments) may therefore yield several different
UCCA annotations.
UCCA is a multi-layered formalism, where
each layer specifies the relations it encodes. The
question of which relations will be annotated
(equivalently, which units will be formed) is de-
termined by the layer in question. For example,
consider ?John kicked his ball?, and assume our
current layer encodes the relations expressed by
?kicked? and by ?his?. In that case, the unit ?his?
has a single argument1 (?ball?), while ?kicked?
has two (?John? and ?his ball?). Therefore, the
units of the sentence are the terminals (which are
always units), ?his ball? and ?John kicked his
ball?. The latter two are units by virtue of express-
ing a relation along with its arguments. See Fig-
ure 2(a) for a graph representation of this example.
For a brief comparison of the UCCA formalism
with other dependency annotations see Section 5.
2.2 The UCCA Foundational Layer
The foundational layer is designed to cover the
entire text, so that each word participates in at
least one unit. It focuses on argument structures
of verbal, nominal and adjectival predicates and
the inter-relations between them. Argument struc-
ture phenomena are considered basic by many ap-
proaches to semantic and grammatical representa-
tion, and have a high applicative value, as demon-
strated by their extensive use in NLP.
The foundational layer views the text as a col-
lection of Scenes. A Scene can describe some
movement or action, or a temporally persistent
state. It generally has a temporal and a spatial di-
mension, which can be specific to a particular time
and place, but can also describe a schematized
event which refers to many events by highlight-
ing a common meaning component. For example,
the Scene ?John loves bananas? is a schematized
event, which refers to John?s disposition towards
bananas without making any temporal or spatial
1The anaphoric aspects of ?his? are not considered part of
the current layer (see Section 2.3).
229
John took a shower -ROOT-
ROOT
SBJ
OBJ
NMOD
(a)
John showered -ROOT-
ROOTSBJ
(b)
John took my book -ROOT-
ROOT
SBJ
OBJ
NMOD
(c)
Figure 1: CoNLL-style dependency annotations. Note that (a) and (c), which have different semantics but superficially similar
syntax, have the same annotation.
Abb. Category Short Definition
Scene Elements
P Process The main relation of a Scene that evolves in time (usually an action or movement).
S State The main relation of a Scene that does not evolve in time.
A Participant A participant in a Scene in a broad sense (including locations, abstract entities and Scenes serving
as arguments).
D Adverbial A secondary relation in a Scene (including temporal relations).
Elements of Non-Scene Units
C Center Necessary for the conceptualization of the parent unit.
E Elaborator A non-Scene relation which applies to a single Center.
N Connector A non-Scene relation which applies to two or more Centers, highlighting a common feature.
R Relator All other types of non-Scene relations. Two varieties: (1) Rs that relate a C to some super-ordinate
relation, and (2) Rs that relate two Cs pertaining to different aspects of the parent unit.
Inter-Scene Relations
H Parallel
Scene
A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive).
L Linker A relation between two or more Hs (e.g., ?when?, ?if?, ?in order to?).
G Ground A relation between the speech event and the uttered Scene (e.g., ?surprisingly?, ?in my opinion?).
Other
F Function Does not introduce a relation or participant. Required by the structural pattern it appears in.
Table 1: The complete set of categories in UCCA?s foundational layer.
specifications. The definition of a Scene is moti-
vated cross-linguistically and is similar to the se-
mantic aspect of the definition of a ?clause? in Ba-
sic Linguistic Theory2.
Table 1 provides a concise description of the
categories used by the foundational layer3. We
turn to a brief description of them.
Simple Scenes. Every Scene contains one main
relation, which is the anchor of the Scene, the most
important relation it describes (similar to frame-
evoking lexical units in FrameNet (Baker et al,
1998)). We distinguish between static Scenes, that
describe a temporally persistent state, and proces-
sual Scenes that describe a temporally evolving
event, usually a movement or an action. The main
relation receives the category State (S) in static and
Process (P) in processual Scenes. We note that
the S-P distinction is introduced here mostly for
practical purposes, and that both categories can be
viewed as sub-categories of the more abstract cat-
egory Main Relation.
A Scene contains one or more Participants (A).
2As UCCA annotates categories on its edges, Scene nodes
bear no special indication. They can be identified by examin-
ing the labels on their outgoing edges (see below).
3Repeated here with minor changes from (Abend and
Rappoport, 2013), which focuses on the categories them-
selves.
This category subsumes concrete and abstract par-
ticipants as well as embedded Scenes (see be-
low). Scenes may also contain secondary rela-
tions, which are marked as Adverbials (D).
The above categories are indifferent to the syn-
tactic category of the Scene-evoking unit, be it a
verb, a noun, an adjective or a preposition. For in-
stance, in the Scene ?The book is in the garden?,
?is in? is the S, while ?the book? and ?the garden?
are As. In ?Tomatoes are red?, the main static re-
lation is ?are red?, while ?Tomatoes? is an A.
The foundational layer designates a separate set
of categories to units that do not evoke a Scene.
Centers (C) are the sub-units of a non-Scene unit
that are necessary for the unit to be conceptualized
and determine its semantic type. There can be one
or more Cs in a non-Scene unit4.
Other sub-units of non-Scene units are catego-
rized into three types. First, units that apply to a
single C are annotated as Elaborators (E). For in-
stance, ?big? in ?big dogs? is an E, while ?dogs? is
a C. We also mark determiners as Es in this coarse-
grained layer5. Second, relations that relate two or
4By allowing several Cs we avoid the difficulties incurred
by the common single head assumption. In some cases the
Cs are inferred from context and can be implicit.
5Several Es that apply to a single C are often placed in
230
more Cs, highlighting a common feature or role
(usually coordination), are called Connectors (N).
See an example in Figure 2(b).
Relators (R) cover all other types of relations
between two or more Cs. Rs appear in two main
varieties. In one, Rs relate a single entity to a
super-ordinate relation. For instance, in ?I heard
noise in the kitchen?, ?in? relates ?the kitchen?
to the Scene it is situated in. In the other, Rs re-
late two units pertaining to different aspects of the
same entity. For instance, in ?bottom of the sea?,
?of? relates ?bottom? and ?the sea?, two units that
refer to different aspects of the same entity.
Some units do not introduce a new relation or
entity into the Scene, and are only part of the for-
mal pattern in which they are situated. Such units
are marked as Functions (F). For example, in the
sentence ?it is customary for John to come late?,
the ?it? does not refer to any specific entity or re-
lation and is therefore an F.
Two example annotations of simple Scenes are
given in Figure 2(a) and Figure 2(b).
More complex cases. UCCA allows units to
participate in more than one relation. This is a nat-
ural requirement given the wealth of distinctions
UCCA is designed to accommodate. Already in
the foundational layer of UCCA, the need arises
for multiple parents. For instance, in ?John asked
Mary to join him?, ?Mary? is a Participant of both
the ?asking? and the ?joining? Scenes.
In some cases, an entity or relation is prominent
in the interpretation of the Scene, but is not men-
tioned explicitly anywhere in the text. We mark
such entities as Implicit Units. Implicit units are
identical to terminals, except that they do not cor-
respond to a stretch of text. For example, ?playing
games is fun? has an implicit A which corresponds
to the people playing the game.
UCCA annotates inter-Scene relations (linkage)
and, following Basic Linguistic Theory, distin-
guishes between three major types of linkage.
First, a Scene can be an A in another Scene. For
instance, in ?John said he must leave?, ?he must
leave? is an A inside the Scene evoked by ?said?.
Second, a Scene may be an E of an entity in an-
other Scene. For instance, in ?the film we saw yes-
terday was wonderful?, ?film we saw yesterday? is
a Scene that serves as an E of ?film?, which is both
an A in the Scene and the Center of an A in the
a flat structure. In general, the coarse-grained foundational
layer does not try to resolve fine scope issues.
John
A
kicked
P
his
E
ball
C
A
(a)
John
C
and
N
Mary
C
A
bought
P
a
E
sofa
C
A
together
D
(b)
the film
A
we
A
saw
P
yesterday
D
E
A
was
F
wonderful
C
S
E C
(c)
Figure 2: Examples of UCCA annotation graphs.
Scene evoked by ?wonderful? (see Figure 2(c)).
A third type of linkage covers all other cases,
e.g., temporal, causal and conditional inter-Scene
relations. The linked Scenes in such cases are
marked as Parallel Scenes (H). The units speci-
fying the relation between Hs are marked as Link-
ers (L)6. As with other relations in UCCA, Linkers
and the Scenes they link are bound by a unit.
Unlike common practice in grammatical anno-
tation, linkage relations in UCCA can cross sen-
tence boundaries, as can relations represented in
other layers (e.g., coreference). UCCA therefore
annotates texts comprised of several paragraphs
and not individual sentences (see Section 3).
Example sentences. Following are complete
annotations of two abbreviated example sentences
from our corpus (see Section 3).
?Golf became a passion for his oldest daughter:
she took daily lessons and became very good,
reaching the Connecticut Golf Championship.?
This sentence contains four Scenes, evoked by
?became a passion?, ?took daily lessons?, ?be-
came very good? and ?reaching?. The individual
Scenes are annotated as follows:
1. ?GolfA [becameE aE passionC]P [forR hisE
oldestE daughterC]A?
6It is equally plausible to include Linkers for the other two
linkage types. This is not included in the current layer.
231
2. ?sheA [tookF [dailyE lessonsC]C]P ?
3. ?sheA ... [becameE [veryE goodC]C]S?
4. ?sheA ... reachingP [theE ConnecticutE
GolfE ChampionshipC ]A?
There is only one explicit Linker in this sen-
tence (?and?), which links Scenes (2) and (3).
None of the Scenes is an A or an E in the other, and
they are therefore all marked as Parallel Scenes.
We also note that in the case of the light verb
construction ?took lessons? and the copula clauses
?became good? and ?became a passion?, the verb
is not the Center of the main relation, but rather
the following noun or adjective. We also note that
the unit ?she? is an A in Scenes (2), (3) and (4).
We turn to our second example:
?Cukor encouraged the studio to
accept her demands.?
This sentence contains three Scenes, evoked by
?encouraged?, ?accept? and ?demands?:
1. CukorA encouragedP [theE studioC]A [toR
[accept her demands]C ]A
2. [the studio]A ... acceptP [her demands]A
3. herA demandsP IMPA
Scenes (2) and (3) act as Participants in Scenes
(1) and (2) respectively. In Scene (2), there is
an implicit Participant which corresponds to what-
ever was demanded. Note that ?her demands? is a
Scene, despite being a noun phrase.
2.3 UCCA?s Multi-layered Structure
Additional layers may refine existing relations or
otherwise annotate a complementary set of dis-
tinctions. For instance, a refinement layer can
categorize linkage relations according to their se-
mantic types (e.g., temporal, purposive, causal) or
provide tense distinctions for verbs. Another im-
mediate extension to UCCA?s foundational layer
can be the annotation of coreference relations. Re-
call the example ?John kicked his ball?. A coref-
erence layer would annotate a relation between
?John? and ?his? by introducing a new node whose
descendants are these two units. The fact that
this node represents a coreference relation would
be represented by a label on the edge connecting
them to the coreference node.
There are three common ways to extend an an-
notation graph. First, by adding a relation that re-
lates previously established units. This is done by
introducing a new node whose descendants are the
related units. Second, by adding an intermediate
Passage #
1 2 3 4 5 6
# Sents. 8 20 23 14 13 15
# Tokens 259 360 343 322 316 393
ITA 67.3 74.1 71.2 73.5 77.8 81.1
Vs. Gold 72.4 76.7 75.5 75.7 79.5 84.2
Correction 93.7
Table 2: The upper part of the table presents the number of
sentences and the number of tokens in the first passages used
for the annotator training. The middle part presents the av-
erage F-scores obtained by the annotators throughout these
passages. The first row presents the average F-score when
comparing the annotations of the different annotators among
themselves. The second row presents the average F-score
when comparing them to a ?gold standard?. The bottom row
shows the average F-score between an annotated passage of
a trained annotator and its manual correction by an expert. It
is higher due to conforming analyses (see text). All F-scores
are in percents.
unit between a parent unit and some of its sub-
units. For instance, consider ?he replied foolishly?
and ?he foolishly replied?. A layer focusing on
Adverbial scope may refine the flat Scene structure
assigned by the foundational layer, expressing the
scope of ?foolishly? over the relation ?replied? in
the first case, and over the entire Scene in the sec-
ond. Third, by adding sub-units to a terminal. For
instance, consider ?gave up?, an expression which
the foundational layer considers atomic. A layer
that annotates tense can break the expression into
?gave? and ?up?, in order to annotate ?gave? as the
tense-bearing unit.
Although a more complete discussion of the for-
malism is beyond the scope of this paper, we note
that the formalism is designed to allow different
annotation layers to be defined and annotated in-
dependently of one another, in order to facilitate
UCCA?s construction through a community effort.
3 A UCCA-Annotated Corpus
The annotated text is mostly based on English
Wikipedia articles for celebrities. We have chosen
this genre as it is an inclusive and diverse domain,
which is still accessible to annotators from varied
backgrounds.
For the annotation process, we designed and im-
plemented a web application tailored for UCCA?s
annotation. A sample of the corpus containing
roughly 5K tokens, as well as the annotation ap-
plication can be found in our website7.
UCCA?s annotations are not confined to a sin-
gle sentence. The annotation is therefore carried
out in passages of 300-400 tokens. After its an-
7www.cs.huji.ac.il/?omria01
232
notation, a passage is manually corrected before
being inserted into the repository.
The section of the corpus annotated thus far
contains 56890 tokens in 148 annotated passages
(average length of 385 tokens). Each passage con-
tains 450 units on average and 42.2 Scenes. Each
Scene contains an average of 2 Participants and 0.3
Adverbials. 15% of the Scenes are static (contain
an S as the main relation) and the rest are dynamic
(containing a P). The average number of tokens in
a Scene (excluding punctuation) is 10.7. 18.3%
of the Scenes are Participants in another Scene,
11.4% are Elaborator Scenes and the remaining
are Parallel Scenes. A passage contains an aver-
age of 11.2 Linkers.
Inter-annotator agreement. We employ 4 an-
notators with varying levels of background in lin-
guistics. Two of the annotators have no back-
ground in linguistics, one took an introductory
course and one holds a Bachelor?s degree in lin-
guistics. The training process of the annotators
lasted 30?40 hours, which includes the time re-
quired for them to get acquainted with the web
application. As this was the first large-scale trial
with the UCCA scheme, some modifications to the
scheme were made during the annotator?s training.
We therefore expect the training process to be even
faster in later distributions.
There is no standard evaluation measure for
comparing two grammatical annotations in the
form of labeled DAGs. We therefore converted
UCCA to constituency trees8 and, following stan-
dard practice, computed the number of brackets in
both trees that match in both span and label. We
derive an F-score from these counts.
Table 2 presents the inter-annotator agreement
in the training phase. The four annotators were
given the same passage in each of these cases. In
addition, a ?gold standard? was annotated by the
authors of this paper. The table presents the av-
erage F-score between the annotators, as well as
the average F-score when comparing to the gold
standard. Results show that although it repre-
sents complex hierarchical structures, the UCCA
scheme is learned quickly and effectively.
We also examined the influence of prior linguis-
tic background on the results. In the first passage
there was a substantial advantage to the annotators
8In cases a unit had multiple parents, we discarded all but
one of its incoming edges. This resulted in discarding 1.9%
of the edges. We applied a simple normalization procedure to
the resulting trees.
who had prior training in linguistics. The obtained
F-scores when comparing to a gold standard, or-
dered decreasingly according to the annotator?s
acquaintance with linguistics, were 78%, 74.4%,
69.5% and 67.8%. However, this performance gap
quickly vanished. Indeed, the obtained F-scores,
again compared to a gold standard and averaged
over the next five training passages, were (by the
same order) 78.6%, 77.3%, 79.2% and 78%.
This is an advantage of UCCA over other syn-
tactic annotation schemes that normally require
highly proficient annotators. For instance, both
the PTB and the Prague Dependency Treebank
(Bo?hmova? et al, 2003) employed annotators with
extensive linguistic background. Similar findings
to ours were reported in the PropBank project,
which successfully employed annotators with var-
ious levels of linguistic background. We view
this as a major advantage of semantic annotation
schemes over their syntactic counterparts, espe-
cially given the huge amount of manual labor re-
quired for large syntactic annotation projects.
The UCCA interface allows for multiple non-
contradictory (?conforming?) analyses of a stretch
of text. It assumes that in some cases there is
more than one acceptable option, each highlight-
ing a different aspect of meaning of the analyzed
utterance (see below). This makes the computa-
tion of inter-annotator agreement fairly difficult.
It also suggests that the above evaluation is exces-
sively strict, as it does not take into account such
conforming analyses. To address this issue, we
conducted another experiment where an expert an-
notator corrected the produced annotations. Com-
paring the corrected versions to the originals, we
found that F-scores are typically in the range of
90%?95%. An average taken over a sample of
passages annotated by all four annotators yielded
an F-score of 93.7%.
It is difficult to compare the above results to the
inter-annotator agreement of other projects for two
reasons. First, many existing schemes are based
on other annotation schemes or heavily rely on
automatic tools for providing partial annotations.
Second, some of the most prominent annotation
projects do not provide reliable inter-annotator
agreement scores (Artstein and Poesio, 2008).
A recent work that did report inter-annotator
agreement in terms of bracketing F-score is an an-
notation project of the PTB?s noun phrases with
more elaborate syntactic structure (Vadas and Cur-
233
ran, 2011). They report an agreement of 88.3% in
a scenario where their two annotators worked sep-
arately. Note that this task is much more limited
in scope than UCCA (annotates noun phrases in-
stead of complete passages in UCCA; uses 2 cat-
egories instead of 12 in UCCA). Nevertheless, the
obtained inter-annotator agreement is comparable.
Disagreement examples. Here we discuss two
major types of disagreements that recurred in the
training process. The first is the distinction be-
tween Elaborators and Centers. In most cases this
distinction is straightforward, particularly where
one sub-unit determines the semantic type of the
parent unit, while its siblings add more informa-
tion to it (e.g., ?truckE companyC? is a type of a
company and not of a truck). Some structures do
not nicely fall into this pattern. One such case is
with apposition. In the example ?the Fox drama
Glory days?, both ?the Fox drama? and ?Glory
days? are reasonable candidates for being a Cen-
ter, which results in disagreements.
Another case is the distinction between Scenes
and non-Scene relations. Consider the example
?[John?s portrayal of the character] has been de-
scribed as ...?. The sentence obviously contains
two scenes, one in which John portrays a charac-
ter and another where someone describes John?s
doings. Its internal structure is therefore ?John?sA
portrayalP [of the character]A?. However, the
syntactic structure of this unit leads annotators at
times into analyzing the subject as a non-Scene re-
lation whose C is ?portrayal?.
Static relations tend to be more ambiguous be-
tween a Scene and a non-Scene interpretation.
Consider ?Jane Smith (ne?e Ross)?. It is not at all
clear whether ?ne?e Ross? should be annotated as a
Scene or not. Even if we do assume it is a Scene,
it is not clear whether the Scene it evokes is her
Scene of birth, which is dynamic, or a static Scene
which can be paraphrased as ?originally named
Ross?. This leads to several conforming analyses,
each expressing a somewhat different conceptual-
ization of the Scene. This central notion will be
more elaborately addressed in future work.
We note that all of these disagreements can be
easily resolved by introducing an additional layer
focusing on the construction in question.
4 UCCA?s Benefits to Semantic Tasks
UCCA?s relative insensitivity to syntactic forms
has potential benefits for a wide variety of seman-
tic tasks. This section briefly demonstrates these
benefits through a number of examples.
Recall the example ?John took a shower? (Sec-
tion 1). UCCA annotates the sentence as a sin-
gle Scene, with a single Participant and a proces-
sual main relation: ?JohnA [tookF [aE showerC]C
]P ?. The paraphrase ?John showered? is anno-
tated similarly: ?JohnA showeredP ?. The struc-
ture is also preserved under translation to other
languages, such as German (?JohnA duschteP ?,
where ?duschte? is a verb), or Portuguese ?JohnA
[tomouF banhoC]P ? (literally, John took shower).
In all of these cases, UCCA annotates the example
as a Scene with an A and a P, whose Center is a
word expressing the notion of showering.
Another example is the sentence ?John does
not have any money?. The foundational layer
of UCCA annotates negation units as Ds, which
yields the annotation ?JohnA [doesF ]S- notD
[haveC]-S [anyE moneyC]A? (where ?does ...
have? is a discontiguous unit)9. This sentence can
be paraphrased as ?JohnA hasP noD moneyA?.
UCCA reflects the similarity of these two sen-
tences, as it annotates both cases as a single Scene
which has two Participants and a negation. A syn-
tactic scheme would normally annotate ?no? in the
second sentence as a modifier of ?money?, and
?not? as a negation of ?have?.
The value of UCCA?s annotation can again be
seen in translation to languages that have only one
of these forms. For instance, the German transla-
tion of this sentence, ?JohnA hatS keinD GeldA?,
is a literal translation of ?John has no money?. The
Hebrew translation of this sentence is ?eyn le john
kesef? (literally, ?there-is-no to John money?).
The main relation here is therefore ?eyn? (there-
is-no) which will be annotated as S. This yields
the annotation ?eynS [leR JohnC]A kesefA?.
The UCCA annotation in all of these cases is
composed of two Participants and a State. In En-
glish and German, the negative polarity unit is rep-
resented as a D. The negative polarity of the He-
brew ?eyn? is represented in a more detailed layer.
As a third example, consider the two sentences
?There are children playing in the park? and ?Chil-
dren are playing in the park?. The two sentences
have a similar meaning but substantially different
syntactic structures. The first contains two clauses,
an existential main clause (headed by ?there are?)
9The foundational layer places ?not? in the Scene level to
avoid resolving fine scope issues (see Section 2)
234
and a subordinate clause (?playing in the park?).
The second contains a simple clause headed by
?playing?. While the parse trees of these sentences
are very different, their UCCA annotation in the
foundational layer differ only in terms of Function
units: ?ChildrenA [areF playingC]P [inR theE
parkC]A? and ?ThereF areF childrenA [playing]P
[inR theE parkC]A?10.
Aside from machine translation, a great vari-
ety of semantic tasks can benefit from a scheme
that is relatively insensitive to syntactic variation.
Examples include text simplification (e.g., for sec-
ond language teaching) (Siddharthan, 2006), para-
phrase detection (Dolan et al, 2004), summariza-
tion (Knight and Marcu, 2000), and question an-
swering (Wang et al, 2007).
5 Related Work
In this section we compare UCCA to some of the
major approaches to grammatical representation in
NLP. We focus on English, which is the most stud-
ied language and the focus of this paper.
Syntactic annotation schemes come in many
forms, from lexical categories such as POS tags
to intricate hierarchical structures. Some for-
malisms focus particularly on syntactic distinc-
tions, while others model the syntax-semantics in-
terface as well (Kaplan and Bresnan, 1981; Pollard
and Sag, 1994; Joshi and Schabes, 1997; Steed-
man, 2001; Sag, 2010, inter alia). UCCA diverges
from these approaches in aiming to abstract away
from specific syntactic forms and to only represent
semantic distinctions. Put differently, UCCA ad-
vocates an approach that treats syntax as a hidden
layer when learning the mapping between form
and meaning, while existing syntactic approaches
aim to model it manually and explicitly.
UCCA does not build on any other annotation
layers and therefore implicitly assumes that se-
mantic annotation can be learned directly. Recent
work suggests that indeed structured prediction
methods have reached sufficient maturity to allow
direct learning of semantic distinctions. Examples
include Naradowsky et al (2012) for semantic role
labeling and Kwiatkowski et al (2010) for seman-
tic parsing to logical forms. While structured pre-
diction for the task of predicting tree structures
is already well established (e.g., (Suzuki et al,
10The two sentences are somewhat different in terms of
their information structure (Van Valin Jr., 2005), which is rep-
resented in a more detailed UCCA layer.
2009)), recent work has also successfully tackled
the task of predicting semantic structures in the
form of DAGs (Jones et al, 2012).
The most prominent annotation scheme in NLP
for English syntax is the Penn Treebank. Many
syntactic schemes are built or derived from it. An
increasingly popular alternative to the PTB are
dependency structures, which are usually repre-
sented as trees whose nodes are the words of the
sentence (Ivanova et al, 2012). Such represen-
tations are limited due to their inability to natu-
rally represent constructions that have more than
one head, or in which the identity of the head
is not clear. They also face difficulties in repre-
senting units that participate in multiple relations.
UCCA proposes a different formalism that ad-
dresses these problems by introducing a new node
for every relation (cf. (Sangati and Mazza, 2009)).
Several annotated corpora offer a joint syntac-
tic and semantic representation. Examples in-
clude the Groningen Meaning bank (Basile et al,
2012), Treebank Semantics (Butler and Yoshi-
moto, 2012) and the Lingo Redwoods treebank
(Oepen et al, 2004). UCCA diverges from these
projects in aiming to abstract away from syntac-
tic variation, and is therefore less coupled with a
specific syntactic theory.
A different strand of work addresses the con-
struction of an interlingual representation, often
with a motivation of applying it to machine trans-
lation. Examples include the UNL project (Uchida
and Zhu, 2001), the IAMTC project (Dorr et al,
2010) and the AMR project (Banarescu et al,
2012). These projects share with UCCA their
emphasis on cross-linguistically valid annotations,
but diverge from UCCA in three important re-
spects. First, UCCA emphasizes the notion of
a multi-layer structure where the basic layers are
maximally coarse-grained, in contrast to the above
works that use far more elaborate representations.
Second, from a theoretical point of view, UCCA
differs from these works in aiming to represent
conceptual semantics, building on works in Cog-
nitive Linguistics (e.g., (Langacker, 2008)). Third,
unlike interlingua that generally define abstract
representations that may correspond to several dif-
ferent texts, UCCA incorporates the text into its
structure, thereby facilitating learning.
Semantic role labeling (SRL) schemes bear
similarity to the foundational layer, due to their
focus on argument structure. The leading SRL ap-
235
proaches are PropBank (Palmer et al, 2005) and
NomBank (Meyers et al, 2004) on the one hand,
and FrameNet (Baker et al, 1998) on the other. At
this point, all these schemes provide a more fine-
grained set of categories than UCCA.
PropBank and NomBank are built on top of the
PTB annotation, and provide for each verb (Prop-
Bank) and noun (NomBank), a delineation of their
arguments and their categorization into semantic
roles. Their structures therefore follow the syn-
tax of English quite closely. UCCA is generally
less tailored to the syntax of English (e.g., see sec-
ondary verbs (Dixon, 2005)).
Furthermore, PropBank and NomBank do not
annotate the internal structure of their arguments.
Indeed, the construction of the commonly used se-
mantic dependencies derived from these schemes
(Surdeanu et al, 2008) required a set of syntactic
head percolation rules to be used. These rules are
somewhat arbitrary (Schwartz et al, 2011), do not
support multiple heads, and often reflect syntac-
tic rather than semantic considerations (e.g., ?mil-
lions? is the head of ?millions of dollars?, while
?dollars? is the head of ?five million dollars?).
Another difference is that PropBank and Nom-
Bank each annotate only a subset of predicates,
while UCCA is more inclusive. This difference
is most apparent in cases where a single complex
predicate contains both nominal and verbal com-
ponents (e.g., ?limit access?, ?take a shower?). In
addition, neither PropBank nor Nomabnk address
copula clauses, despite their frequency. Finally,
unlike PropBank and NomBank, UCCA?s founda-
tional layer annotates linkage relations.
In order to quantify the similarity between
UCCA and PropBank, we annotated 30 sentences
from the PropBank corpus with their UCCA anno-
tations and converted the outcome to PropBank-
style annotations11. We obtained an unlabeled
F-score of 89.4% when comparing to PropBank,
which indicates that PropBank-style annotations
are generally derivable from UCCA?s. The dis-
agreement between the schemes reflects both an-
notation conventions and principle differences,
some of which were discussed above.
The FrameNet project (Baker et al, 1998)
11The experiment was conducted on the first 30 sentences
of section 02. The identity of the predicates was determined
according to the PropBank annotation. We applied a simple
conversion procedure that uses half a dozen rules that are not
conditioned on any lexical item. We used a strict evaluation
that requires an exact match in the argument?s boundaries.
proposes a comprehensive approach to semantic
roles. It defines a lexical database of Frames, each
containing a set of possible frame elements and
their semantic roles. It bears similarity to UCCA
both in its use of Frames, which are a context-
independent abstraction of UCCA?s Scenes, and
in its emphasis on semantic rather than distribu-
tional considerations. However, despite these sim-
ilarities, FrameNet focuses on constructing a lex-
ical resource covering specific cases of interest,
and does not provide a fully annotated corpus of
naturally occurring text. UCCA?s foundational
layer can be seen as a complementary effort to
FrameNet, as it focuses on high-coverage, coarse-
grained annotation, while FrameNet is more fine-
grained at the expense of coverage.
6 Conclusion
This paper presented Universal Conceptual Cog-
nitive Annotation (UCCA), a novel framework
for semantic representation. We described the
foundational layer of UCCA and the compilation
of a UCCA-annotated corpus. We demonstrated
UCCA?s relative insensitivity to paraphrases and
cross-linguistic syntactic variation. We also dis-
cussed UCCA?s accessibility to annotators with no
background in linguistics, which can alleviate the
almost prohibitive annotation costs of large syn-
tactic annotation projects.
UCCA?s representation is guided by conceptual
notions and has its roots in the Cognitive Linguis-
tics tradition and specifically in Cognitive Gram-
mar (Langacker, 2008). These theories represent
the meaning of an utterance according to the men-
tal representations it evokes and not according to
its reference in the world. Future work will ex-
plore options to further reduce manual annotation,
possibly by combining texts with visual inputs
during training.
We are currently attempting to construct a
parser for UCCA and to apply it to several seman-
tic tasks, notably English-French machine trans-
lation. Future work will also discuss UCCA?s
portability across domains. We intend to show
that UCCA, which is less sensitive to the idiosyn-
crasies of a specific domain, can be easily adapted
to highly dynamic domains such as social media.
Acknowledgements. We would like to thank
Tomer Eshet for partnering in the development of
the web application and to Amit Beka for his help
with UCCA?s software and development set.
236
References
Omri Abend and Ari Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
IWCS ?13, pages 1?12.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley Framenet project. In ACL-
COLING ?98, pages 86?90.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and
Nathan Schneider. 2012. Abstract mean-
ing representation (AMR) 1.0 specification.
http://www.isi.edu/natural-language/people/amr-
guidelines-10-31-12.pdf.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In LREC ?12, pages 3196?3200.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank.
Treebanks, pages 103?127.
Alistair Butler and Kei Yoshimoto. 2012. Banking
meaning representations from treebanks. Linguistic
Issues in Language Technology, 7(1).
Alexander Clark and Shalom Lappin. 2010. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Robert M. W. Dixon. 2005. A Semantic Approach To
English Grammar. Oxford University Press.
Robert M. W. Dixon. 2010a. Basic Linguistic Theory:
Methodology, volume 1. Oxford University Press.
Robert M. W. Dixon. 2010b. Basic Linguistic Theory:
Grammatical Topics, volume 2. Oxford University
Press.
Robert M. W. Dixon. 2012. Basic Linguistic The-
ory: Further Grammatical Topics, volume 3. Ox-
ford University Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
COLING ?04, pages 350?356.
Bonnie Dorr, Rebecca Passonneau, David Farwell, Re-
becca Green, Nizar Habash, Stephen Helmreich, Ed-
ward Hovy, Lori Levin, Keith Miller, Teruko Mi-
tamura, Owen Rambow, and Advaith Siddharthan.
2010. Interlingual annotation of parallel text cor-
pora: A new framework for annotation and evalu-
ation. Natural Language Engineering, 16(3):197?
243.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and
Dan Flickinger. 2012. Who did what to whom?:
A contrastive study of syntacto-semantic dependen-
cies. In LAW ?12, pages 2?11.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING ?12,
pages 1359?1376.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. Handbook Of Formal Lan-
guages, 3:69?123.
Ronald M. Kaplan and Joan Bresnan. 1981. Lexical-
Functional Grammar: A Formal System For Gram-
matical Representation. Massachusetts Institute Of
Technology, Center For Cognitive Science.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization ? step one: Sentence compres-
sion. In AAAI ?00, pages 703?710.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In EMNLP ?10, pages 1223?1233.
R.W. Langacker. 2008. Cognitive Grammar: A Basic
Introduction. Oxford University Press, USA.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn Treebank. Com-
putational Linguistics, 19(2):313?330.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating noun argu-
ment structure for Nombank. In LREC ?04, pages
803?806.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In EMNLP ?12, pages
810?820.
Stephan Oepen, Dan Flickinger, Kristina Toutanova,
and Christopher D Manning. 2004. Lingo red-
woods. Research on Language and Computation,
2(4):575?596.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):145?159.
Carl Pollard and Ivan A. Sag. 1994. Head-driven
Phrase Structure Grammar. University Of Chicago
Press.
Ivan A Sag. 2010. Sign-based construction gram-
mar: An informal synopsis. Sign-based Construc-
tion Grammar. CSLI Publications, Stanford, pages
39?170.
237
Federico Sangati and Chiara Mazza. 2009. An En-
glish dependency treebank a` la Tesnie`re. In TLT ?09,
pages 173?184.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In ACL-HLT ?11, pages 663?
672.
Advaith Siddharthan. 2006. Syntactic simplification
and text cohesion. Research on Language & Com-
putation, 4(1):77?109.
Mark Steedman. 2001. The Syntactic Process. MIT
Press.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ?08,
pages 159?177.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In EMNLP ?09, pages 551?560.
Hiroshi Uchida and Meiying Zhu. 2001. The uni-
versal networking language beyond machine trans-
lation. In International Symposium on Language in
Cyberspace, pages 26?27.
David Vadas and James R Curran. 2011. Parsing noun
phrases in the Penn Treebank. Computational Lin-
guistics, 37(4):753?809.
Robert D. Van Valin Jr. 2005. Exploring The Syntax-
semantics Interface. Cambridge University Press.
Mengqiu Wang, Noah A. Smith, and Teruko Mita-
mura. 2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In EMNLP-CoNLL
?07, pages 22?32.
238
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 57?66,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Unsupervised POS Induction Using Intrinsic Clustering Quality
and a Zipfian Constraint
Roi Reichart
ICNC
The Hebrew University
roiri@cs.huji.ac.il
Raanan Fattal
Institute of computer science
The Hebrew University
raananf@cs.huji.ac.il
Ari Rappoport
Institute of computer science
The Hebrew University
arir@cs.huji.ac.il
Abstract
Modern unsupervised POS taggers usually
apply an optimization procedure to a non-
convex function, and tend to converge to
local maxima that are sensitive to start-
ing conditions. The quality of the tag-
ging induced by such algorithms is thus
highly variable, and researchers report av-
erage results over several random initial-
izations. Consequently, applications are
not guaranteed to use an induced tagging
of the quality reported for the algorithm.
In this paper we address this issue using
an unsupervised test for intrinsic cluster-
ing quality. We run a base tagger with
different random initializations, and select
the best tagging using the quality test. As
a base tagger, we modify a leading un-
supervised POS tagger (Clark, 2003) to
constrain the distributions of word types
across clusters to be Zipfian, allowing us
to utilize a perplexity-based quality test.
We show that the correlation between our
quality test and gold standard-based tag-
ging quality measures is high. Our re-
sults are better in most evaluation mea-
sures than all results reported in the liter-
ature for this task, and are always better
than the Clark average results.
1 Introduction
Unsupervised part-of-speech (POS) induction is
of major theoretical and practical importance. It
counters the arbitrary nature of manually designed
tag sets, and avoids manual corpus annotation
costs. The task enjoys considerable current inter-
est in the research community (see Section 3).
Most unsupervised POS tagging algorithms ap-
ply an optimization procedure to a non-convex
function, and tend to converge to local maxima
that strongly depend on the algorithm?s (usually
random) initialization. The quality of the tag-
gings produced by different initializations varies
substantially. Figure 1 demonstrates this phe-
nomenon for a leading POS induction algorithm
(Clark, 2003). The absolute variability of the in-
duced tagging quality is 10-15%, which is around
20% of the mean. Strong variability has also been
reported by other authors (Section 3).
The common practice in the literature is to re-
port mean results over several random initializa-
tions of the algorithm (e.g. (Clark, 2003; Smith
and Eisner, 2005; Goldwater and Griffiths, 2007;
Johnson, 2007)). This means that applications us-
ing the induced tagging are not guaranteed to use
a tagging of the reported quality.
In this paper we address this issue using an
unsupervised test for intrinsic clustering quality.
We present a quality-based algorithmic family Q.
Each of its concrete member algorithms Q(B) runs
a base tagger B with different random initializa-
tions, and selects the best tagging according the
quality test. If the test is highly positively corre-
lated with external tagging quality measures (e.g.,
those based on gold standard tagging), Q(B) will
produce better results than B with high probability.
We experiment with two base taggers, Clark?s
original tagger (CT) and Zipf Constrained Clark
(ZCC). ZCC is a novel algorithm of interest in its
own right, which is especially suitable as a base
tagger in the family Q. ZCC is a modification of
Clark?s algorithm in which the distribution of the
number of word types in a cluster (cluster type
size) is constrained to be Zipfian. This property
holds for natural languages, hence we can expect
a higher correlation between ZCC and an accepted
unsupervised quality measure, perplexity.
We show that for both base taggers, the corre-
lation between our unsupervised quality test and
gold standard based tagging quality measures is
high. For the English WSJ corpus, the Q(ZCC)
57
0.45 0.5 0.550
20
40
V
0.7 0.8 0.90
10
20
NVI
0.4 0.5 0.60
20
40
Many to 1
0.4 0.50
20
40
1 to 1
Figure 1: Distribution of the quality of the tag-
gings produced in 100 runs of the Clark POS in-
duction algorithm (with different random initial-
izations) for sections 2-21 of the WSJ corpus. All
graphs are 10-bin histograms presenting the num-
ber of runs (y-axis) with the corresponding qual-
ity (x-axis). Quality is evaluated with 4 clustering
evaluation measures: V, NVI, greedy m-1 map-
ping and greedy 1-1 mapping. The quality of the
induced tagging varies considerably.
algorithm gives better results than CT with proba-
bility 82-100% (depending on the external quality
measure used). Q(CT) is shown to be better than
the original CT algorithm as well. Our results are
better in most evaluation measures than all previ-
ous results reported in the literature for this task,
and are always better than Clark?s average results.
Section 2 describes the ZCC algorithm and our
quality measure. Section 3 discusses previous
work. Section 4 presents the experimental setup
and Section 5 reports our results.
2 The Q(ZCC) Algorithm
Given an N word corpus M consisting of plain
text, with word types W = {w1, . . . , wm}, the
unsupervised POS induction task is to find a class
membership function g from W into a set of class
labels {c1, . . . , cn}. In the version tackled in this
paper, the number of classes n is an input of the al-
gorithm. The membership function g can be used
to tag a corpus if it is deterministic (as the func-
tion learned in this work) or if a rule for selecting
a single tag for every word is provided.
Most modern unsupervised POS taggers pro-
duce taggings of variable quality that strongly de-
pend on their initialization. Our approach towards
generating a single high quality tagging is to use a
family of algorithms Q. Each member Q(B) of Q
utilizes a base tagger B, which is run using several
random initializations. The final output is selected
according to an unsupervised quality test. We fo-
cus here on Clark?s tagger (Clark, 2003) (CT),
probably the leading POS induction algorithm (see
Table 3).
We start with a description of the original CT.
We then detail ZCC, a modification of CT that
constrains the clustering space by adding a Zipf-
based constraint. Our perplexity-based unsuper-
vised tagging quality test is discussed next. Fi-
nally, we provide an unsupervised technique for
selecting the parameter of the Zipfian constraint.
2.1 The Original Clark Tagger (CT)
The tagger?s statistical model combines dis-
tributional and morphological information with
the likelihood function of the Brown algorithm
(Brown et al, 1992; Ney et al, 1994; Martin et
al., 1998). In the Brown algorithm a class assign-
ment function g is selected such that the class bi-
gram likelihood of the corpus, p(M |g), is max-
imized. Morphological and distributional infor-
mation is introduced to the Clark model through
a prior p(g). The prior prefers morphologically
uniform clusters and skewed cluster sizes.
The probability function the algorithm tries to
maximize is:
(1) p(M, g) = p(M |g) ? p(g)
(2) p(M |g) = ?i=Ni=2 p(g(wi)|g(wi?1))
(3) p(g) = ?nj=1 ?j
?
g(w)=j qj(w)
Where qj(wi) is the probability of assigning
wi ? W by cluster cj according to the morpho-
logical model and ?j is the coefficient of cluster
j, which equals to the number of word types as-
signed to that cluster divided by the total number
of word types in the vocabulary W . The objective
of the algorithm is formally specified by:
g? = argmaxgp(M, g)
To find the cluster assignment g? an iterative
algorithm is applied. As initialization, the words
in W are randomly assigned to clusters (clusters
are thus of similar sizes). Then, for each word
(words are ordered by their frequency in the cor-
pus) the algorithm computes the effect that mov-
ing it from its current cluster to each of the other
clusters would have on the probability function.
The word is moved to the cluster having the high-
est positive effect (if there is no such cluster, the
word is not moved). The last step is performed it-
eratively until no improvement to the probability
function is possible through a single operation.
58
The probability function has many local max-
ima and the one to which the algorithm conver-
gences strongly depends on the initial assignment
of words to clusters. The quality of the clusters in-
duced in different runs of the algorithm is highly
variable (Figure 1).
2.2 The Cluster Type Size Zipf Constraint
The motivation behind using a Zipfian constraint is
the following observation: when a certain statistic
is known to affect the quality of the induced clus-
tering and it is not explicitly manipulated by the al-
gorithm, strong fluctuations in its values are likely
to imply that there are uncontrolled fluctuations in
the quality of the induced clusterings. Thus, in-
troducing a constraint that we believe holds in real
data increases the correlation between clustering
quality and a well accepted unsupervised quality
measure (perplexity).
Our ZCC algorithm searches for a class assign-
ment function g that maximizes the probability
function (1) under a constraint on the clustering
space, namely constraining the cluster type size
distribution induced by g to be Zipfian. This con-
straint holds in many languages (Mitzenmacher,
2004) and is demonstrated in Figure 3 for the En-
glish corpus with which we experiment in this pa-
per.
Zipf?s law predicts that the fraction of elements
in class k is given by:
f(k; s;n) = 1/k
s
?n
i=1(1/is)
where s is a parameter of the distribution and n the
number of clusters.
Denote the cluster type size distribution derived
from the algorithm?s cluster assignment function g
by T (g). The objective of the algorithm is
g?? = argmaxgp(M, g) s.t. T (g) ? Zipf(s)
To impose the Zipfian distribution on the in-
duced clusters size, we make two modifications to
the original CT algorithm. First, at initialization,
words are randomly assigned to clusters in a way
that cluster sizes are distributed according to the
Zipfian distribution (with a parameter s). Specifi-
cally, we randomly select words to be assigned to
the first cluster until the fraction of word types in
the cluster equals to the prediction given by Zipf?s
law. We then randomly assign words to the second
cluster and so on.
Second, we change the basic operation of the al-
gorithm from moving a word to a cluster to swap-
ping two words between two different clusters.
For each word wi (again, words are ordered by
their frequency in the corpus as in CT), the algo-
rithm computes the effect on the probability func-
tion of moving it from its current cluster ccurr to
each of the other clusters. We denote the cluster
showing the best effect by cbest. Then, we search
the words of cbest for the word wj whose transition
to ccurr has the best effect on the probability func-
tion. If the sum of the effects of moving wi from
ccurr to cbest and moving wj from cbest to ccurr
is positive, the swapping is performed. If swap-
ping is not performed, we repeat the process for
wi, this time searching for cbest among all other
clusters except of former cbest candidates1.
2.3 Unsupervised Identification of High
Quality Runs
Perplexity is a standard measure for language
model evaluation. A language model defines the
transition probabilities for every word wi given the
words that precede it. The perplexity of a language
model for a given corpus having N words is de-
fined to be
N
?
?
?
?
N
?
i=1
1
p(wi|w1 . . . wi?1)
An important property of perplexity that makes
it attractive as a measure for language model per-
formance is that in some sense the best model for
any corpus has the lowest perplexity for that cor-
pus (Goodman, 2001). Thus, the lower the per-
plexity of the language model, the better it is.
Clark (2003) proposed a perplexity based test
for the quality of his POS induction algorithm. In
that test, a bigram class-based language model is
trained on a training corpus (using the tagging of
the unsupervised tagger) and applied to another
test corpus. In such a model the transition prob-
ability from a word wj to a word wi is given
by p(C(wi)|C(wj)) where C(wk) is the class as-
signed by the POS induction algorithm to wk. In
the training phase the bigram transition probabili-
ties are computed using the training corpus, and in
1To make the algorithm more time efficient, for each word
wi we perform only three iterations of the searching for cbest,
and for each cbest candidate we compute for at most 500
words the effect on the probability function of the removal
to ccurr .
59
2 4 6 8 10880
882
884
886
888
890
K
A
ve
ra
g
e
 P
e
rp
le
xi
ty
2 4 6 8 100.4
0.5
0.6
0.7
0.8
0.9
K
R
a
n
k 
C
o
rr
e
la
tio
n
Figure 2: Left: average perplexity vs. the param-
eter K (tightness of the entropy outliers filter; see
text for a full explanation). Right: Spearman?s
rank correlation between perplexity and an exter-
nal (many-to-one) quality of the clustering as a
function of K. The three curves are for ZCC,
using different exponents (triangles: 0.9, circles:
1.3, solid: 1.1). A model whose quality improves
(decreased perplexity) with K (left) demonstrates
better correlation between perplexity and external
quality (right). In all three graphs the x axis is in
units of 5K (e.g., a graph x value of 2 means that
10 clusterings were removed from the top of the
list and 10 from its bottom).
the test phase the perplexity of the learned model
is evaluated on the test corpus. Better POS induc-
tion algorithms yield lower perplexity language
models. However, Clark did not study the correla-
tion between the perplexity measure and the gold
standard tagging.
In this paper, we use Clark?s perplexity based
test as the unsupervised quality test used by the
family Q. To provide a high quality prediction, this
test should highly correlate with external cluster-
ing quality. To the best of our knowledge, such a
correlation has not been explored so far.
2.4 Unsupervised Parameter Selection
The base ZCC algorithm has one input parame-
ter, the exponent s of the Zipfian distribution. Vir-
tually all unsupervised algorithms utilize param-
eters whose values affect their results. While it
is methodologically valid to simply determine a
value based on reasonable considerations or a de-
velopment set, to keep the fully unsupervised na-
ture of our work we now present a method for
identifying the best parameter assignment. The
method also casts some additional interesting light
on the nature of the problem.
Like cluster type size, the distribution of cluster
instance size in natural languages is also Zipfian
(see Figure 3). A naive application of this con-
straint into the ZCC algorithm would be to allow
swapping words between clusters only if they an-
notate the same number of word instances in the
corpus. However, this constraint, either by itself
or in combination with the cluster type size con-
straint, is too restrictive.
We utilize it for parameter selection as follows.
Recall that our family of algorithms Q(B) runs a
base tagger B several times. Each specific run
yields a clustering Ci. The final result is selected
from the set of clusterings C = {Ci}. We do
not explicitly address the number of instances con-
tained in a cluster, but we can prune from C those
clusterings for which this distribution is very dif-
ferent. Again, imposing a constraint that is known
to hold reduces quality fluctuations between dif-
ferent runs.
To measure the similarity between the cluster
instance size distribution of two clusterings in-
duced by two runs of the algorithm, we treat the
clusters induced by a given run as samples from
a random variable. The events of this variable are
the induced clusters and the probability assigned
to each event is equal to the number of word in-
stances contained in the corresponding cluster, di-
vided by the total number of word instances in the
tagged corpus. The entropy of this random vari-
able is used as a statistic for the word instance
distribution. Clusterings having similar cluster in-
stance size distributions also have similar values
of this statistic.
We apply an entropy outliers filter to the set of
clusterings C. In this filter, we sort the members
of C (these are clusterings obtained in different
runs of the base tagger) according to their clus-
ter instance size entropy, and prune K runs from
the beginning and K runs from the end of the list.
The perplexity-based quality test described above
is applied only to members of C that were not
pruned in this step.
Figure 2 (left) shows the average perplexity of
a set of clusterings as a function of the parame-
ter K of the entropy-based filter. Results are pre-
sented for 100 runs of ZCC2 with three different
exponent values (0.9, 1.1, 1.3). These assignments
yield considerably different Zipfian distributions.
While all three models have similar average per-
plexity over all 100 runs, only the solid line (cor-
responding to an exponent value of 1.1) consis-
2See Section 4 for the experimental setup.
60
tently decreases (improves) with K. The circled
line (corresponding to an exponent value of 1.3)
monotonically decreases with K until a certain K
value, while the line with triangles (correspond-
ing to an exponent value of 0.9) remains relatively
constant.
Figure 2 (right) shows that models for which
the entropy-based filter improves perplexity more
drastically, exhibit better correlation between per-
plexity and external clustering quality3.
Our unsupervised parameter selection method is
thus based on finding a value which exhibits a con-
sistent decrease in perplexity as a function of K,
the number of clusterings pruned from the begin-
ning and end of the entropy-sorted list. In the rest
of this paper we show results where the exponent
value is 1.1.
3 Previous Work
Unsupervised POS induction/tagging is a fruitful
area of research. A major direction is Hidden
Markov Models (HMM) (Merialdo, 1994; Banko
and Moore, 2004; Wang and Schuurmans, 2005).
Several recent works have tried to improve this
model using Bayesian estimation (Goldwater and
Griffiths, 2007; Johnson, 2007; Gao and Johnson,
2008), sophisticated initialization (Goldberg et al,
2008), induction of an initial clustering used to
train an HMM (Freitag, 2004; Biemann, 2006),
infinite HMM models (Van Gael et al, 2009), in-
tegration of integer linear programming into the
parameter estimation process (Ravi and Knight,
2009), and biasing the model such that the num-
ber of possible tags that each word can get is small
(Grac?a et al, 2009).
The Bayesian works integrated into the model
information about the distribution of words to POS
tags. For example, Johnson (2007) integrated to
the EM-HMM model a prior that prefers cluster-
ings where the distributions of hidden states to
words is skewed.
Other approaches include transformation based
learning (Brill, 1995), contrastive estimation for
conditional random fields (Smith and Eisner,
2005), Markov random fields (Haghighi and
Klein, 2006), a multilingual approach (Snyder et
al., 2008; Snyder et al, 2008) and expanding a
3The figure is for greedy many-to-one mapping and
Spearman?s rank correlation coefficient, explained in further
Sections. Other external measures and rank correlation scores
demonstrate the same pattern.
partial dictionary and use it to learn disambigua-
tion rules (Zhao and Marcus, 2009).
These works, except (Haghighi and Klein,
2006; Johnson, 2007; Gao and Johnson, 2008)
and one experiment in (Goldwater and Griffiths,
2007), used a dictionary listing the allowable tags
for each word in the text. This dictionary is usu-
ally extracted from the manual tagging of the text,
contradicting the unsupervised nature of the task.
Clearly, the availability of such a dictionary is not
always a reasonable assumption (see e.g. (Gold-
water and Griffiths, 2007)).
In a different algorithmic direction, (Schuetze,
1995) applied latent semantic analysis with SVD
based dimensionality reduction, and (Schuetze,
1995; Clark, 2003; Dasgupta and NG, 2007) used
distributional and morphological statistics to find
meaningful word types clusters. Clark (2003) is
the only such work to have evaluated its algorithm
as a POS tagger for large corpora, like we do in
this paper.
A Zipfian constraint was utilized in (Goldwater
and et al, 2006) for language modeling and mor-
phological disambiguation.
The problem of convergence to local maxima
has been discussed in (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Goldwater and Grif-
fiths, 2007; Johnson, 2007; Gao and Johnson,
2008) with a detailed demonstration in (Johnson,
2007). All these authors (except Smith and Eisner
(2005), see below), however, reported average re-
sults over several runs and did not try to identify
the runs that produce high quality tagging.
Smith and Eisner (2005) initialized with all
weights equal to zero (uninformed, deterministic
initialization) and performed unsupervised model
selection across smoothing parameters by evaluat-
ing the training criterion on unseen, unlabeled de-
velopment data. In this paper we show that for the
tagger of (Clark, 2003) such a method provides
mediocre results (Table 2) even when the train-
ing criterion (likelihood or data probability for this
tagger) is evaluated on the test set. Moreover, we
show that our algorithm outperforms existing POS
taggers for most evaluation measures (Table 3).
Identifying good solutions among many runs of
a randomly-initialized algorithm is a well known
problem. We discuss here the work of (Smith and
Eisner, 2004) that addressed the problem in the un-
supervised POS tagging context. In this work, de-
terministic annealing (Rose et al, 1990) was ap-
61
plied to an HMM model for unsupervised POS
tagging with a dictionary. This method is not sen-
sitive to its initialization, and while it is not the-
oretically guaranteed to converge to a better so-
lution than the traditional EM-HMM, it was ex-
perimentally shown to achieve better results. The
problem has, of course, been addressed in other
contexts as well (see, e.g., (Wang et al, 2002)).
4 Experimental Setup and Evaluation
Setup. We used the English WSJ PennTreebank
corpus in our experiments. We induced POS tags
for sections 2-21 (43K word types, 950K word in-
stances of which 832K (87.6%) are not punctua-
tion marks), using Q(ZCC), Q(CT), and CT. For
the unsupervised quality test, we trained the bi-
gram class-based language model on sections 2-21
with the induced clusters, and computed its per-
plexity on section 23.
In Q(ZCC) and Q(CT), the base taggers were
run a 100 times each, using different random ini-
tializations. In each run we induce 13 clusters,
since this is the number of unique POS tags re-
quired to cover 98% of the word types in WSJ
(Figure 3)4. Some previous work (e.g., (Smith and
Eisner, 2005)) also induced 13 non-punctuation
tags.
We compare the results of our algorithm to
those of the original Clark algorithm5. The in-
duced clusters are evaluated against two POS tag
sets: one is the full set of WSJ POS tags, and the
other consists of the non-punctuation tags of the
first set.
Punctuation marks constitute a sizeable volume
of corpus tokens and are easy to cluster correctly.
Hence, evaluting against the full tag set that in-
cludes punctuation artificially increases the qual-
ity of the reported results, which is why we report
results for the non-punctuation tag set. However,
to be able to directly compare with previous work,
we also report results for the full WSJ POS tag
set. We do so by assigning a singleton cluster to
each punctuation mark (in addition to the 13 clus-
ters). This simple heuristic yields very high per-
formance on punctuation, scoring (when all other
terminals are assumed perfect tagging) 99.6% in
1-to-1 accuracy.
4Some words can get more than one POS tag. In the fig-
ure, for these words we increased the counters of all their
possible tags.
5Downloaded from www.cs.rhul.ac.uk/home/alexc/
RHUL/Downloads.html.
In addition to comparing the different algo-
rithms, we compare the correlation between our
tagging quality test and external clustering quality
for both the original CT algorithm and our ZCC
algorithm.
Clustering Quality Evaluation. The induced
POS tags have arbitrary names. To evaluate them
against a manually annotated corpus, a proper
correspondence with the gold standard POS tags
should be established. Many evaluation measures
for unsupervised clustering against gold standard
exist. Here we use measures from two well ac-
cepted families: mapping based and information
theoretic (IT) based. For a recent discussion on
this subject see (Reichart and Rappoport, 2009).
The mapping based measures are accuracy with
greedy many-to-1 (M-1) and with greedy 1-to-1
(1-1) mappings of the induced to the gold labels.
In the former mapping, two induced clusters can
be mapped to the same gold standard cluster, while
in the latter mapping each and every induced clus-
ter is assigned a unique gold cluster.
After each induced label is mapped to a gold
label, tagging accuracy is computed. Accuracy is
defined to be the number of correctly tagged words
in the corpus divided by the total number of words
in the corpus.
The IT based measures we use are V (Rosen-
berg and Hirschberg, 2007) and NVI (Reichart and
Rappoport, 2009). The latter is a normalization of
the VI measure (Meila, 2007). VI and NVI induce
the same order over clusterings but NVI values for
good clusterings lie in [0, 1]. For V, the higher
the score, the better the clustering. For NVI lower
scores imply improved clustering quality. We use
e as the base of the logarithm.
Evaluation of the Quality Test. To mea-
sure the correlation between the score produced
by the tagging quality test and the external qual-
ity of a tagging, we use two well accepted mea-
sures: Spearman?s rank correlation coefficient
and Kendall Tau (Kendall and Dickinson, 1990).
These measure the correlation between two sorted
lists. For the computation of these measures, we
rank the clusterings once according to the identifi-
cation criterion and once according to the external
quality measure.
The measures are given by the equations:
(6) kendall ? tau = 2(nc?nd)r(r?1)
(7) Spearsman = 1 ? 6
?r
i=1 d
2
i
r(r2?1)
62
0 10 20 30 400
0.2
0.4
0.6
0.8
1
Number of POS Tags
F
ra
ct
io
n
 o
f 
It
e
m
s
Figure 3: The fraction of word types (solid curve)
and word instances (dashed curve) labeled with
the k (X axis) most frequent POS tags (in types
and tokens respectively) in sections 2-21 of the
WSJ corpus.
where r is the number of runs (100 in our case),
nc and nd are the numbers of concordant and dis-
cordant pairs respectively6 and di is the absolute
value of the difference between the ranks of item
i.
The two measures have the properties that a
perfect agreement between rankings results in a
score of 1, a perfect disagreement results in a score
of ?1, completely independent rankings have the
value of 0 on the average, the range of values is
between ?1 and 1, and increasing values imply
increasing agreement between the rankings. For a
discussion see (Lapata, 2006).
5 Results
Table 1 presents the results of the Q(ZCC) and
Q(CT) algorithms, which are both better than
those of the original Clark tagger CT. The Q al-
gorithms provide a tagging that is better than that
produced by CT in 82-100% (Q(ZCC)) and 75-
100% (Q(CT)) of the cases.
The Q(ZCC) algorithm is superior when eval-
uated with the mapping based measures. The
Q(CT) algorithm is superior when evaluated with
the IT measures.
Table 3 presents reported results for all recent
algorithms we are aware of that tackled the task
of unsupervised POS induction from plain text 7.
The settings of the various experiments vary in
terms of the exact gold annotation scheme used
for evaluation (the full WSJ set was used by all
authors except Goldwater and Griffiths (2007) and
6A pair r, t in two lists X and Y is concordant if
sign(Xt ? Xr) = sign(Yt ? Yr), where Xr is the index
of r in the list X .
7VG and GG used 2 as the base of the logarithm in IT
measures, which affects VI. We converted the VI numbers
reported in their papers to base e.
the GGTP-17 model which used the set of 17
coarse grained tags proposed by (Smith and Eis-
ner, 2005)) and the size of the test set. The num-
bers reported for the algorithms of other works are
the average performance over multiple runs, since
no method for identification of high quality tag-
gings was used.
The results of our algorithms are superior, ex-
cept for the M-1 performance of some of the mod-
els of (Johnson, 2007) and of the GGTP-17 and
GGTP-45 models of (Grac?a et al, 2009). Note
that the models of (Johnson, 2007) and the GGTP-
45 model induce 40-50 clusters compared to our
34 (13 non-punctuation plus the additional 21 sin-
gleton punctuation tags). Increasing the number
of clusters is known to improve the M-1 mea-
sure (Reichart and Rappoport, 2009). GGTP-17
gives the best M-1 results, but its 1-1 results are
much worse than those of Q(ZCC), Q(CT), and
CT, and the information theoretic measures V and
NVI were not reported for it.
Recall that the Q algorithms tag punctuation
marks according to the scheme which assigns each
of them a unique cluster (Section 4), while previ-
ous work does not distinguish punctuation marks
from other tokens. To quantify the effect vari-
ous punctuation schemes have on the results re-
ported in Table 3, we evaluated the ?iHMM: PY-
fixed? model (Van Gael et al, 2009) and the Q al-
gorithms when punctuation is excluded and when
both PY-fixed and Q algorithms use the punctua-
tion scheme described in Section 4.
For the PY-fixed, which induces 91 clusters,
results are (punctuation is excluded, heuristic is
used): V(0.530, 0.608), NVI (0.999, 0.823), 1-1
(0.484, 0.543), M-1 (0.591, 0.639). The results
for the Q algorithms are given in Table 1 (top
line: excluding punctuation, bottom line: using
the heuristic). The Q algorithms are better for the
V, NVI and 1-1 measures. For M-1 evaluation,
PY-fixed, which induces substantially more clus-
ters (91 compared to our 34) is better.
In what follows, we provide an analysis of the
components of our algorithms. To explore the
quality of our tagging component, ZCC, table 4
compares the mean, mode and standard deviation
of a 100 runs of ZCC with 100 runs of the original
CT algorithm8. The performance of the tagging
8In mode calculation we treat the 100 runs as samples of
a continuous random variable. We divide the results range
to 10 bins of the same size. The mode is the center of the
bin having the largest number of runs. If there is more than
63
Alg. V NVI 1-1 M-1
Q(ZCC)
no punct. 0.538 (85, 2.6) 0.849 (82, 3.2) 0.521 (100, 4.3) 0.533 (84, 1.7)
with punct. 0.637 (85, 1.8) 0.678 (82, 2.6) 0.58 (100, 3) 0.591 (84, 1.18)
Q(CT)
no punct. 0.545 (92, 3.3) 0.837 (88, 4.4) 0.492 (99,1.4) 0.526 (75, 1)
with punct. 0.644 (92, 2.5) 0.662 (88, 4.2) 0.555 (99, 0.5) 0.585 (75, 0.58)
Table 1: Quality of the tagging produced by Q(ZCC) and Q(CT). The top (bottom) line for each algorithm
presents the results when punctuation is not included (is included) in the evaluation (Section 4). The left
number in the parentheses is the fraction of Clark?s (CT) results that scored worse than our models (%
from 100 runs). The right number in the parentheses is 100 times the difference between the score of our
model and the mean score of 100 runs of Clark?s (CT). Q(ZCC) is better than Q(CT) in the mappings
measures, while Q(CT) is better in the IT measures. Both are better than the original Clark tagger CT.
Data Probability Likelihood Perplexity
V m-to-1 V m-to-1 V m-to-1
Alg. SRC KT SRC KT SRC KT SRC KT SRC KT SRC KT
CT 0.2 0.143 0.071 0.045 0.338 0.23 0.22 0.148 0.568 0.397 0.476 0.33
ZCC 0.134 0.094 0.118 0.078 0.517 0.352 0.453 0.321 0.82 0.62 0.659 0.484
Table 2: Correlation of unsupervised quality measures (columns) with clustering quality of two base
taggers (CT and ZCC, rows). Correlation is measured by Spearman (SRC) and Kendall Tau (KT) rank
correlation coefficients. The quality measures are data probability (left part), likelihood (middle side)
and perplexity (right part), and correlation is between these and two of the external evaluation measures,
m-to-1 mapping and V (results for the other two clustering evaluation measures, 1-1 mapping and NVI,
are very similar). Results for the perplexity quality test used by family Q are superior; data probability
and likelihood provide only a mediocre indication for the quality of induced clustering. Note that the
correlation values are much higher for ZCC than for CT.
components are quite similar, with a small advan-
tage to CT in mean and to ZCC in mode.
Our quality test is based on the perplexity of a
class bigram language model trained with the in-
duced tagging. To emphasize its strength we com-
pare it to two natural quality tests: the likelihood
and value of the probability function to which the
tagging algorithm converges (equations (2) and (1)
in Section 2.1). The results are shown in Table
2 First, we see that our perplexity quality test is
much better correlated with the quality of the tag-
ging induced by both ZCC and CT. Second, the
correlation is indeed much higher for ZCC than
for CT.
The power of Q(ZCC) lies in the combination
between the perplexity-based quality test and the
tagging component ZCC. The performance of the
tagging component ZCC does not provide a def-
inite improvement over the original Clark tagger.
ZCC compromises mean tagging results for an im-
proved correlation between Q?s quality measure
one such bin, we average their centers. We use this technique
since it is rare to see two different runs of either algorithm
with the exact same quality.
and gold standard-based tagging evaluation.
6 Conclusion
In this paper we addressed unsupervised POS tag-
ging as a task where the quality of a single tag-
ging is to be reported, rather than the average per-
formance of a tagging algorithm over many runs.
We introduced a family of algorithms Q(B) based
on an unsupervised test for tagging quality that is
used to select a high quality tagging from the out-
put of multiple runs of a POS tagger B.
We introduced the ZCC tagger which modifies
the original Clark tagger by constraining the clus-
tering space using a cluster type size Zipfian con-
straint, conforming with a known property of nat-
ural languages.
We showed that the tagging produced by our
Q(ZCC) algorithm is better than that of the Clark
algorithm with a probability of 82-100%, depend-
ing on the measure used. Moreover, our tagging
outperforms in most evaluation measures the re-
sults reported in all recent works that addressed
the task.
In future work, we intend to try to improve
64
Alg. V VI M-1 1-1
Q(ZCC) 0.637 2.06 0.591 0.58
Q(CT) 0.644 2.01 0.585 0.555
CT 0.619 2.14 0.576 0.543
HK ? ? ? 0.413
J ? 4.23 -
5.74
0.43 -
0.62
0.37 ?
0.47
GG ? 2.8 ? ?
G-J ? 4.03 ?
4.47
? 0.4 ?
0.499
VG 0.54 -
0.59
2.49 ?
2.91
? ?
GGTP-45 ? ? 0.654 0.445
GGTP-17 ? ? 0.702 0.495
Table 3: Comparison of our algorithms with the
recent fully unsupervised POS taggers for which
results are reported. HK: (Haghighi and Klein,
2006), trained and evaluated with a corpus of
193K tokens and 45 induced tags. GG: (Goldwa-
ter and Griffiths, 2007), trained and evaluated with
a corpus of 24K tokens and 17 induced tags. J :
(Johnson, 2007) inducing 25-50 tags (the results
that are higher than Q in the M-1 measure are for
40-50 tags). GJ: (Gao and Johnson, 2008), induc-
ing 50 tags. VG: (Van Gael et al, 2009), inducing
47-192 tags. GGTP-45: (Grac?a et al, 2009), in-
ducing 45 tags. GGTP-17: (Grac?a et al, 2009),
inducing 17 tags. All five were trained and evalu-
ated with the full WSJ PTB (1.17M words). Lower
VI values indicates better clustering.
Statistic V NVI M-1 1-1
CT
Mean 0.512 0.881 0.516 0.478
Mode 0.502 0.886 0.514 0.465
Std 0.022 0.035 0.018 0.028
ZCC
Mean 0.503 0.908 0.512 0.478
Mode 0.509 0.907 0.518 0.47
Std 0.021 0.036 0.018 0.0295
Table 4: Average performance of ZCC compared
with CT (results presented without punctuation).
Presented are mean, mode (see text for its calcu-
lation), and standard deviation (std). CT mean re-
sults are slightly better, and both algorithms have
about the same standard deviation. ZCC sacrifices
a small amount of mean quality for a good corre-
lation with our quality test, which allows Q(ZCC)
to be much better than the mean of CT and most
of its runs.
our quality measure, experiment with additional
languages, and apply the ?family of algorithms?
paradigm to additional relevant NLP tasks.
References
Michele Banko and Robert C. Moore, 2003. Part of
Speech Tagging in Context. COLING ?04.
Chris Biemann, 2006. Unsupervised Part-of-
Speech Tagging Employing Efficient Graph Cluster-
ing. COLING-ACL ?06 Student Research Work-
shop.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Eric Brill, 1995. Unsupervised Learning if Disam-
biguation Rules for Part of Speech Tagging. 3rd
Workshop on Very Large Corpora.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de
Souze, Jenifer C. Lai and Robert Mercer, 1992.
Class-Based N-Gram Models of Natural Language.
Computational Linguistics, 18:467-479.
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP ?07.
Steven Finch, Nick Chater and Martin Redington,
1995. Acquiring syntactic information from distri-
butional statistics. Connectionist models of memory
and language. UCL Press, London.
Dayne Freitag, 2004. Toward Unsupervised Whole-
Corpus Tagging. COLING ?04.
Jianfeng Gao and Mark Johnson, 2008. A compari-
son of Bayesian estimators for unsupervised Hidden
Markov Model POS taggers. EMNLP ?08.
Yoav Goldberg, Meni Adler and Michael Elhadad,
2008. EM Can Find Pretty Good HMM POS-
Taggers (When Given a Good Start). ACL ?08
Sharon Goldwater, Tom Griffiths, and Mark Johnson,
2006. Interpolating between types and tokens by es-
timating power-law generators. NIPS ?06.
Sharon Goldwater and Tom Griffiths, 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. ACL ?07.
Joshua Goodman, 2001. A Bit of Progress in Lan-
guage Modeling, Extended Version. Microsoft Re-
search Technical Report MSR-TR-2001-72.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
65
Maurice Kendall and Jean Dickinson, 1990. Rank
Correlation methods. Oxford University Press, New
York.
Aria Haghighi and Dan Klein, 2006. Prototype-driven
Learning for Sequence Labeling. HLT-NAACL ?06.
Mark Johnson, 2007. Why Doesnt EM Find Good
HMM POS-Taggers? EMNLP-CoNLL ?07.
Harold W. Kuhn, 1955. The Hungarian method for
the assignment problem. Naval Research Logistics
Quarterly, 2:83-97.
Mirella Lapata, 2006. Automatic Evaluation of In-
formation Ordering: Kendall?s Tau. Computational
Linguistics, 4:471-484.
Sven Martin, Jorg Liermann, and Hermann Ney, 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19-37.
Marina Meila, 2007. Comparing Clustering - an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
Bernard Merialdo, 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155-172.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions. Internet Mathematics, 1(2):226-251.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Hermann Ney, Ute Essen, and Reinhard Kneser,
1994. On structuring probabilistic dependencies in
stochastic language modelling. Computer Speech
and Language, 8:1-38.
Sujith Ravi and Kevin Knight, 2009. Minimized Mod-
els for Unsupervised Part-of-Speech Tagging. ACL
?09.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox,
1990. Statistical Mechanics and Phase Transitions
in Clustering. Physical Review Letters, 65(8):945-
948.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-Based External
Cluster Evaluation Measure. EMNLP ?07.
Hinrich Schuetze, 1995. Distributional part-of-speech
tagging. EACL ?95.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ?04.
Noah A. Smith and Jason Eisner, 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. ACL ?05.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2009. Adding More Lan-
guages Improves Unsupervised Multilingual Part-
of-Speech Tagging: A Bayesian Non-Parametric
Approach. NAACL ?09.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay, 2008. Unsupervised Multi-
lingual Learning for POS Tagging. EMNLP ?08.
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Qin Iris Wang and Dale Schuurmans, 2005. Im-
proved Estimation for Unsupervised Part-of-Speech
Tagging. IEEE NLP-KE ?05.
Shaojun Wang, Dale Schuurmans and Yunxin Zhao,
2002. The Latent Maximum Entropy Principle.
ISIT ?02.
Qiuye Zhao and Mitch Marcus, 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. EMNLP ?09.
66
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Type Level Clustering Evaluation: New Measures
and a POS Induction Case Study
Roi Reichart1? Omri Abend2?? Ari Rappoport2
1ICNC 2Institute of Computer Science
Hebrew University of Jerusalem
{roiri|omria01|arir}@cs.huji.ac.il
Abstract
Clustering is a central technique in NLP.
Consequently, clustering evaluation is of
great importance. Many clustering algo-
rithms are evaluated by their success in
tagging corpus tokens. In this paper we
discuss type level evaluation, which re-
flects class membership only and is inde-
pendent of the token statistics of a partic-
ular reference corpus. Type level evalua-
tion casts light on the merits of algorithms,
and for some applications is a more natural
measure of the algorithm?s quality.
We propose new type level evaluation
measures that, contrary to existing mea-
sures, are applicable when items are pol-
ysemous, the common case in NLP. We
demonstrate the benefits of our measures
using a detailed case study, POS induc-
tion. We experiment with seven leading
algorithms, obtaining useful insights and
showing that token and type level mea-
sures can weakly or even negatively corre-
late, which underscores the fact that these
two approaches reveal different aspects of
clustering quality.
1 Introduction
Clustering is a central machine learning technique.
In NLP, clustering has been used for virtually ev-
ery semi- and unsupervised task, including POS
tagging (Clark, 2003), labeled parse tree induction
(Reichart and Rappoport, 2008), verb-type clas-
sification (Schulte im Walde, 2006), lexical ac-
quisition (Davidov and Rappoport, 2006; Davi-
dov and Rappoport, 2008), multilingual document
?
* Both authors equally contributed to this paper.
? Omri Abend is grateful to the Azrieli Foundation for
the award of an Azrieli Fellowship.
clustering (Montavlo et al, 2006), coreference res-
olution (Nicolae and Nicolae, 2006) and named
entity recognition (Elsner et al, 2009). Conse-
quently, the methodology of clustering evaluation
is of great importance. In this paper we focus
on external clustering evaluation, i.e., evaluation
against manually annotated gold standards, which
exist for almost all such NLP tasks. External eval-
uation is the dominant form of clustering evalu-
ation in NLP, although other methods have been
proposed (see e.g. (Frank et al, 2009)).
In this paper we discuss type level evaluation,
which evaluates the set membership structure cre-
ated by the clustering, independently of the token
statistics of the gold standard corpus. Many clus-
tering algorithms are evaluated by their success
in tagging corpus tokens (Clark, 2003; Nicolae
and Nicolae, 2006; Goldwater and Griffiths, 2007;
Gao and Johnson, 2008; Elsner et al, 2009). How-
ever, in many cases a type level evaluation is the
natural one. This is the case, for example, when
a POS induction algorithm is used to compute a
tag dictionary (the set of tags that each word can
take), or when a lexical acquisition algorithm is
used for constructing a lexicon containing the set
of frames that a verb can participate in, or when a
sense induction algorithm computes the set of pos-
sible senses of each word. In addition, even when
the goal is corpus tagging, a type level evaluation
is highly valuable, since it may cast light on the
relative or absolute merits of different algorithms
(as we show in this paper).
Clustering evaluation has been extensively in-
vestigated (Section 3). However, the discussion
centers around the monosemous case, where each
item belongs to exactly one cluster, although pol-
ysemy is the common case in NLP.
The contribution of the present paper is as fol-
lows. First, we discuss the issue of type level eval-
uation and explain why even in the monosemous
case a token level evaluation presents a skewed
77
picture (Section 2). Second, we show for the
common polysemous case why adapting existing
information-theoretic measures to type level eval-
uation is not natural (Section 3). Third, we pro-
pose new mapping-based measures and algorithms
to compute them (Section 4). Finally, we perform
a detailed case study with part-of-speech (POS)
induction (Section 5). We compare seven lead-
ing algorithms, showing that token and type level
measures can weakly or even negatively correlate.
This shows that type level evaluation indeed re-
veals aspects of a clustering solution that are not
revealed by the common tagging-based evaluation.
Clustering is a vast research area. As far as we
know, this is the first NLP paper to propose type
level measures for the polysemous case.
2 Type Level Clustering Evaluation
This section motivates why both type and token
level external evaluations should be done, even in
the monosemous case.
Clustering algorithms compute a set of induced
clusters (a clustering). Some algorithms directly
compute a clustering, while some others produce
a tagging of corpus tokens from which a clustering
can be easily derived. A clustering is monosemous
if each item is allowed to belong to a single cluster
only, and polysemous otherwise. An external eval-
uation is one which is based on a comparison of an
algorithm?s result to a gold standard. In this paper
we focus solely on external evaluation, which is
the most common evaluation approach in NLP.
Token and type level evaluations reflect differ-
ent aspects of a clustering. External token level
evaluation assesses clustering quality according to
the clustering?s accuracy on a given manually an-
notated corpus. This is certainly a useful evalua-
tion measure, e.g. when the purpose of the cluster-
ing algorithm is to annotate a corpus to serve as
input to another application.
External type level evaluation views the com-
puted clustering as a set membership structure and
evalutes it independently of the token statistics in
the gold standard corpus. There are two main
cases in which this is useful. First, a type level
evaluation can be the natural one in light of the
problem itself. For example, if the purpose of
the clustering algorithm is to automatically build
a lexicon (e.g., VerbNet (Kipper et al, 2000)),
then the lexicon structure itself should be evalu-
ated. Second, it may be valuable to decouple cor-
pus statistics from the induced clustering when the
latter is to be used for annotating corpora that ex-
hibit different statistics. In other words, if we eval-
uate an algorithm that will be invoked on a diverse
set of corpora having different token statistics, a
type level evaluation might provide a better picture
(or at least a complementary one) on the quality of
the clustering algorithm.
To motivate type level evaluation, consider POS
induction, which exemplifies both cases above.
Clearly, a word form may belong to several parts
of speech (e.g., ?contrast? is both a noun and a
verb, ?fast? is both an adjective and an adverb,
?that? can be a determiner, conjunction and adverb,
etc.). As an evaluation of a POS induction algo-
rithm, it is natural to evaluate the lexicon it gener-
ates, even if the main goal is to annotate a corpus.
The lexicon lists the possible POS tags for each
word, and thus its evaluation is a polysemous type
level one.
Even if we ignore polysemy, type level evalua-
tion is useful for a POS induction algorithm used
to tag a corpus. There are POS classes whose
members are very frequent, e.g., determiners and
prepositions. Here, a very small number of word
types usually accounts for a large portion of corpus
tokens. For example, in the WSJ Penn Treebank
(Marcus et al, 1993), there are 43,740 word types
and over 1M word tokens. Of the types, 88 are
tagged as prepositions. These types account for
only 0.2% of the types, but for as many as 11.9%
of the tokens. An algorithm which is accurate only
on prepositions would do much better in a token
level evaluation than in a type level one.
This phenomenon is not restricted to preposi-
tions or English. In the WSJ corpus, determiners
account for 0.05% of the types but for 9.8% of the
tokens. In the German NEGRA corpus (Brants,
1997), the article class (both definite and indefi-
nite) accounts for 0.04% of the word types and for
12.5% of the word tokens, and the coordinating
conjunctions class accounts for 0.05% of the word
types but for 3% of the tokens.
The type and token behavior differences result
from the Zipfian distribution of word tokens to
word types (Mitzenmacher, 2004). Since the word
frequency distribution is Zipfian, any clustering al-
gorithm that is accurate only on a small number of
frequent words (not necessarily members of a par-
ticular class) would perform well in a token level
evaluation but not in a type one. For example,
78
the most frequent 100 word types (regardless of
POS class) in WSJ (NEGRA) account for 43.9%
(41.3%) of the tokens in the corpus. These words
appear in 32 out of the 34 non-punctuation POS
classes in WSJ and in 38 out of the 51 classes in
NEGRA.
Other natural language entities also demonstrate
Zipfian distribution of tokens to types. For exam-
ple, the distribution of syntactic categories in parse
tree constituents is Zipfian, as shown in (Reichart
and Rappoport, 2008) for English, German and
Chinese corpora. Thus, the distinction between to-
ken and type level evaluation is important also for
grammar induction algorithms.
It may be argued that a token level evaluation
is sufficient since it already reflects type informa-
tion. In this paper we demonstrate that this is not
the case, by showing that they correlate weakly or
even negatively in an important NLP task.
3 Existing Clustering Evaluation
Measures
Clustering evaluation is challenging. Many mea-
sures have been proposed in the past decades
(Pfitzner et al, 2008). In this section, we briefly
survey the three main types: mapping based,
counting pairs, and information theoretic mea-
sures, and motivate our decision to focus on the
first in this paper.
Mapping based measures are based on a post-
processing step in which each induced cluster is
mapped to a gold class (or vice versa). The stan-
dard mappings are greedy many-to-one (M-1) and
greedy one-to-one (1-1). Several measures which
rely on these mappings were proposed. The most
common and perhaps the simplest one is accu-
racy, which computes the fraction of items cor-
rectly clustered under the mapping. Other mea-
sures include: L (Larsen, 1999), D (Van Dongen,
2000), misclassification index (MI) (Zeng et al,
2002), H (Meila and Heckerman, 2001), clustering
F-measure (Fung et al, 2003) and micro-averaged
precision and recall (Dhillon et al, 2003). In Sec-
tion 4 we show why existing mapping-based mea-
sures cannot be applied to the polysemous type
case and present new mapping-based measures for
this case.
Counting pairs measures are based on a com-
binatorial approach which examines the number
of data element pairs that are clustered similarly
in the reference and proposed clustering. Among
these are Rand Index (Rand, 1971), Adjusted Rand
Index (Hubert and Arabie, 1985), ? statistic (Hu-
bert and Schultz, 1976), Jaccard (Milligan et al,
1983), Fowlkes-Mallows (Fowlkes and Mallows,
1983) and Mirkin (Mirkin, 1996). Schulte im
Walde (2006) used such a measure for type level
evaluation of monosemous verb type clustering.
Meila (2007) described a few problems with
such measures. A serious one is that their values
are unbounded, making it hard to interpret their
results. This can be solved by adjusting their val-
ues to lie in [0, 1], but even adjusted measures suf-
fer from severe distributional problems, limiting
their usability in practice. We thus do not address
counting pairs measures in this paper.
Information-theoretic (IT) measures. IT
measures assume that the items in the dataset are
taken from a known distribution (usually the uni-
form distribution), and thus the gold and induced
clusters can be treated as random variables. These
measures utilize a co-occurrence matrix I between
the gold and induced clusters. We denote the in-
duced clustering by K and the gold clustering by
C. Iij contains the number of items in the in-
tersection of the i-th gold class and the j-th in-
duced cluster. When assuming the uniform dis-
tribution, the probability of an event (a gold class
c or an induced cluster k) is its relative size, so
p(c) = ?|K|k=1
Ick
N and p(k) =
?|C|
c=1
Ick
N (N is the
total number of clustered items).
Under this assumption we define the entropies
and the conditional entropies:
H(C) = ? P|C|c=1
P|K|
k=1 Ick
N log
P|K|
k=1 Ick
N
H(C|K) = ? P|K|k=1
P|C|
c=1
Ick
N log
Ick
P|C|
c=1 Ick
H(K) and H(K|C) are defined similarly.
In Section 5 we use two IT measures for token
level evaluation, V (Rosenberg and Hirschberg,
2007) and NVI (Reichart and Rappoport, 2009)
(a normalized version of VI (Meila, 2007)). The
appealing properties of these measures have been
extensively discussed in these references; see also
(Pfitzner et al, 2008). V and NVI are defined as
follows:
h =
(
1 H(C) = 0
1 ? H(C|K)H(C) H(C) 6= 0
c =
(
1 H(K) = 0
1 ? H(K|C)H(K) H(K) 6= 0
V = 2hc
h + c
79
NV I(C, K) =
(
H(C|K)+H(K|C)
H(C) H(C) 6= 0
H(K) H(C) = 0
In the monosemous case (type or token), the ap-
plication of the measures described in this section
to type level evaluation is straightforward. In the
polysemous case, however, they suffer from seri-
ous shortcomings.
Consider a case in which each item is assigned
exactly r gold clusters and each gold cluster has
the exact same number of items (i.e., each has a
size of l?r|C| , where l is the number of items). Now,
consider an induced clustering where there are |C|
induced clusters (|K| = |C|) and each item is as-
signed to all induced clusters. The co-occurrence
matrix in this case should have identical values in
all its entries. Even if we allow the weight each
item contributes to the matrix to depend on its gold
and induced entry sizes, the situation will remain
the same. This is because all items have the exact
same entry size and both gold and induced cluster-
ings have uniform cluster sizes.
In this case, the random variables defined by the
induced and gold clustering assignments are in-
dependent (this easily follows from the definition
of independent events, since the joint probability
is the multiplication of the marginals). Hence,
H(K|C) = H(K) and H(C|K) = H(C), and
both V and NVI obtain their worst possible val-
ues1. However, the score should surely depend on
r (the size of each word?s gold entry). Specifi-
cally, when r = |C| we get that the induced and
gold clusterings are identical. This case should not
get the worst score, and it should definitely score
higher than the case in which r = 1, where K is
dramatically different from C.
The problem can in theory be solved by pro-
viding the number of clusters per item as an input
to the algorithm. However, in NLP this is unre-
alistic (even if the total number of clusters can be
provided) and the number should be determined
by the algorithm. We therefore do not consider
IT-based measures in this paper, deferring them to
future work.
4 Mapping Based Measures for
Polysemous Type Evaluation
In this section we present new type level evalu-
ation measures for the polysemous case. As we
1V values are in [0, 1], 0 being the worst. NVI obtains its
highest and worst possible value, 1 + log(|K|)H(C) .
show below, these measures do not suffer from the
problems discussed for IT measures in Section 3.
All measures are mapping-based: first, a map-
ping between the induced and gold clusters is per-
formed, and then a measure E is computed. As
is common in the clustering evaluation literature
(Section 3), we use M-1 and 1-1 greedy mappings,
defined to be those that maximize the correspond-
ing measure E.
Let C = {c1, ..., cn} be the set of gold classes
and K = {k1, ..., km} be the set of induced clus-
ters. Denote the number of words types by l. Let
Ai ? C, Bi ? K, i = 1...l be the set of gold
classes and set of induced clusters for each word.
The polysemous nature of task is reflected by the
fact that Ai and Bi are subsets, rather than mem-
bers, of C and K respectively.
Our measures address quality from two persec-
tives, that of the individual items clustered (Sec-
tion 4.1) and that of the clusters (Section 4.2).
Item-based measures especially suit evaluation of
clustering quality for the purpose of lexicon induc-
tion, and have no counterpart in the monosemous
case. Cluster-based measures are a direct general-
ization of existing mapping based measures to the
polysemous case.
The difficulty in designing item-based and
cluster-based measures is that the number of clus-
ters assigned to each item is determined by the
clustering algorithm. Below we show how to over-
come this.
4.1 Item-Based Evaluation
For a given mapping h : K ? C, denote
h(Bi) = {h(x) : x ? Bi}. A fundamental quan-
tity for item-based evaluation is the number of cor-
rect clusters for each item (word type) under this
mapping, denoted by IMi (IM stands for ?item
match?):
IMi = |Ai ? h(Bi)|
The total item match IM is defined to be:
IM = ?li=1 IMi =
?l
i=1 |Ai ? h(Bi)|
In the monosemous case, IM is normalized by
the number of items, yielding an accuracy score.
Applying a similar definition in the polysemous
case, normalizing instead by the total number of
gold clusters assigned to the items, can be easily
manipulated. Even a clustering which has the cor-
rect number of induced clusters (equal to the num-
ber of gold classes) but which assigns each item to
80
all induced clusters, receives a perfect score under
both greedy M-1 and 1-1 mappings. This holds for
any induced clustering for which ?i, Ai ? h(Bi).
Note that using a mapping from C to K (or a
combination of both directions) would exhibit the
same problem.
To overcome the problem, we use the harmonic
average of two normalized terms (F-score). We
use two average variants, micro and macro. Macro
average computes the total number of matches
over all words and normalizes in the end. Recall
(R), Precision (P) and their harmonic average (F-
score) are accordingly defined:
R = IMPl
i=1 |Ai|
P = IMPl
i=1 |h(Bi)|
MacroI = 2RP
R + P =
= 2IM
Pl
i=1 |Ai| +
Pl
i=1 |h(Bi)|
= F (h) ?
l
X
i=1
IMi
F (h) is a constant depending on h. As all items
are equally weighted, those with larger gold and
induced entries have more impact on the measure.
The micro average, aiming to give all items an
equal status, first computes an F-score for each
item and then averages over them. Hence, each
item contributes at most 1 to the measure. This
MicroI measure is given by:
Ri = IMi|Ai| Pi =
IMi
|h(Bi)| Fi =
2RiPi
Ri+Pi =
2IMi
|Ai|+|h(Bi)|
MicroI = 1
l
l
X
i=1
Fi =
1
l
l
X
i=1
2IMi
|Ai| + |h(Bi)|
=
= 1
l
l
X
i=1
wi(h) ? IMi
Where wi(h) is a weight depending on h but
also on i.
For both measures, the maximum score is 1. It
is obtained if and only if Ai = h(Bi) for every i.
In 1-1 mapping, when the number of induced
clusters is larger than the number of gold clus-
ters, some of the induced clusters are not mapped.
To preserve the nature of 1-1 mapping that pun-
ishes for excessive clusters2, we define |h(Bi)| to
be equal to |Bi| even for these unmapped clusters.
Recall that any induced clustering in which
?i, Ai ? h(Bi) gets the best score under a greedy
mapping with the accuracy measure. In MacroI
and MicroI the obtained recalls are perfect, but the
precision terms reflect deviation from the correct
solution.
2And to allow us to compute it accurately, see below.
In the example in Section 3 showing an unrea-
sonable behavior of IT-based measures, the score
depends on r for both MacroI and MicroI. With
our new measures, recall is always 1, but precision
is rn . This is true both for 1-1 and M-1 mappings.
Hence, the new measures show reasonable behav-
ior in this example for all r values.
MicroI was used in (Dasgupta and Ng, 2007)
with a manually compiled mapping. Their map-
ping was not based on a well-defined scheme but
on a heuristic. Moreover, providing a manual
mapping might be impractical when the number of
clusters is large, and can be inaccurate, especially
when the clustering is not of very high quality.
In the following we discuss how to compute the
1-1 and M-1 greedy mappings for each measure.
1-1 Mapping. We compute h by finding the
maximal weighted matching in a bipartite graph.
In this graph one side represents the induced clus-
ters, the other represents the gold classes and
the matchings correspond to 1-1 mappings. The
problem can be efficiently solved by the Kuhn-
Munkres algorithm (Kuhn, 1955; Munkres, 1957).
To be able to use this technique, edge weights
must not depend upon h. In 1-1 mapping,
|h(Bi)| = |Bi|, and therefore F (h) = F and
wi(h) = wi. That is, both quantities are inde-
pendent of h3. For MacroI, the weight on the edge
between the s-th gold class and the j-th induced
cluster is: W (esj) =
?l
i=1 F ? Is?AiIj?Bi . For
MicroI it is: W (esj) =
?l
i=1 wi ? Is?AiIj?Bi .
Is?Ai is 1 if s ? Ai and 0 otherwise.
M-1 Mapping. There are two problems in ap-
plying the bipartite graph technique to finding an
M-1 mapping. First, under such mapping wi(h)
and F (h) do depend on h. The problem may
be solved by selecting some constant weighting
scheme. However, a more serious problem also
arises.
Consider a case in which an item x has a gold
entry {C1} and an induced entry {K1, K2}. Say
the chosen mapping mapped both K1 and K2 to
C1. By summing over the graph?s edges selected
by the mapping, we add weight (F (h) for MacroI
and wi(h) for MicroI) both to the edge between
K1 and C1 and to the edge between K2 and C1.
However, the item?s IMi is only 1. This prohibits
3Consequently, the increase in MacroI and MicroI follow-
ing an increase of 1 in an item?s gold/induced intersection size
(IMi) is independent of h.
81
the use of the bipartite graph method for the M-1
case.
Since we are not aware of any exact method for
solving this problem, we use a hill-climbing al-
gorithm. We start with a random mapping and a
random order on the induced clusters. Then we
iterate over the induced clusters and map each of
them to the gold class which maximizes the mea-
sure given that the rest of the mapping remains
constant. We repeat the process until no improve-
ment to the measure can be obtained by changing
the assignment of a single induced cluster. Since
the score depends on the initial random mapping
and random order, we repeat this process several
times and choose the maximum between the ob-
tained scores.
4.2 Cluster-Based Evaluation
The cluster-based evaluation measures we propose
are a direct generalization of existing monose-
mous mapping based measures to the polysemous
type case.
For a given mapping h : K ? C, we define h? :
Kh ? C. Kh is defined to be a clustering which
is obtained by performing set union between every
two clusters in K that are mapped to the same gold
cluster. The resulting h? is always 1-1. We denote
|Kh| = mh.
Our motivation for using h? in the definition of
the measures instead of h is to stay as close as
possible to accuracy, the most common mapping-
based measure in the monosemous case. M-1
(monosemous) accuracy does not punish for split-
ing classes. For instance, in a case where there is
a gold cluster ci and two induced clusters k1 and
k2 such that ci = k1 ? k2, the M-1 accuracy is the
same as in the case where there is one cluster k1
such that ci = k1. M-1 accuracy, despite its in-
difference to splitting, was shown to reflect better
than 1-1 accuracy the clustering?s applicability for
subsequent applications (at least in some contexts)
(Headden III et al, 2008).
Recall that in item-based evaluation, IMi mea-
sures the intersection between the induced and
gold entries of each item. Therefore, the set union
operation is not needed for that case, since when
an item appears in two induced clusters that are
mapped to the same gold cluster, its IMi is in-
creased only by 1.
A fundamental quantity for cluster-based eval-
uation is the intersection between each induced
cluster and the gold class to which it is mapped.
We denote this value by CMj (CM stands for
?cluster match?):
CMj = |kj ? h?(kj)|
The total intersection (CM ) is accordingly de-
fined to be:
CM = ?mhj=1 CMj =
?mh
j=1 |kj ? h?(kj)|
As with the item-based evaluation (Section 4.1),
using CM or a derived accuracy as a measure is
problematic. A clustering that assigns n induced
classes to each word (n is the number of gold
classes) will get the highest possible score under
every greedy mapping (1-1 or M-1), as will any
clustering in which ?i, Ai ? h(Bi).
As in the item-based evaluation, a possible so-
lution is based on defining recall, precision and F-
score measures, computed either in the micro or in
the macro level. The macro cluster-based measure
turns out to be identical to the macro item-based
measure MacroI4.
The following derivation shows the equivalence
for the 1-1 case. The M-1 case is similar. We note
that h = h? in the 1-1 case and we therefore ex-
change them in the definition of CM . It is enough
to show that CM = IM , since the denominator is
the same in both cases:
CM = Pmj=1 |kj ? h(kj)| =
= Pmj=1
Pl
i=1 Ii?kj Ii?h(kj) =
= Pli=1
Pm
j=1 Ii?kj Ii?h(kj) =
= Pli=1 |Ai ? h(Bi)| = IM
The micro cluster-based measures are defined:
Rj = CMj|h?(kj)| Pj =
CMj
|kj | Fj =
2RjPj
Rj+Pj
The micro cluster measure MicroC is obtained
by taking a weighted average over the Fj?s:
MicroC = ?k?Kh
|k|
N? Fk
Where N? = ?z?Kh |z| is the number of clus-
tered items after performing the set union and
including repetitions. If, in the 1-1 case where
m > n, an induced cluster is not mapped, we de-
fine Fk = 0. A definition of the measure using
a reverse mapping (i.e., from C to K) would have
used a weighted average with weights proportional
to the gold classes? sizes.
4Hence, we have six type level measures: MacroI (which
is equal to MacroC), MicroI, and MicroC, each of which in
two versions, M-1 and 1-1.
82
The definition of h? causes a similar computa-
tional difficulty as in the M-1 item-based mea-
sures. Consequently, we apply a hill climbing
algorithm similar to the one described in Sec-
tion 4.1.
The 1-1 mapping is computed using the same
bipartite graph method described in Section 4.1.
The graph?s vertices correspond to gold and in-
duced clusters and an edge?s weight is the F-score
between the class and cluster corresponding to its
vertices times the cluster?s weight (|k|/N?).
5 Evaluation of POS Induction Models
As a detailed case study for the ideas presented
in this paper, we apply the various measures for
the POS induction task, using seven leading POS
induction algorithms.
5.1 Experimental Setup
POS Induction Algorithms. We experimented
with the following models: ARR10 (Abend et al,
2010), Clark03 (Clark, 2003), GG07 (Goldwa-
ter and Griffiths, 2007), GJ08 (Gao and Johnson,
2008), and GVG09 (Van Gael et al, 2009) (three
models). Additional recent good results for vari-
ous variants of the POS induction problem are de-
scribed in e.g., (Smith and Eisner, 2004; Grac?a et
al., 2009).
Clark03 and ARR10 are monosemous algo-
rithms, allowing a single cluster for each word
type. The other algorithms are polysemous. They
perform sequence labeling where each token is
tagged in its context, and different tokens (in-
stances) of the same type (word form) may receive
different tags.
Data Set. All models were tested on sections
2-21 of the PTB-WSJ, which consists of 39832
sentences, 950028 tokens and 39546 unique types.
Of the tokens, 832629 (87.6%) are not punctuation
marks.
Evaluation Measures. Type level evaluation
used the measures MacroI (which is equal to
MacroC), MicroI and MicroC both with greedy
1-1 and M-1 mappings as described in Section 4.
The type level gold (induced) entry is defined to
be the set of all gold (induced) clusters with which
it appears.
For the token level evaluation, six measures are
used (see Section 3): accuracy with M-1 and 1-1
mappings, NVI, V, H(C|K) and H(K|C), using e
as the logarithm?s base. We use the full WSJ POS
tags set excluding punctuation5.
Punctuation. Punctuation marks occupy a
large volume of the corpus tokens (12.4% in our
experimental corpus), and are easy to cluster.
Clustering punctuation marks thus greatly inflates
token level results. To study the relationship be-
tween type and token level evaluations in a fo-
cused manner, we excluded punctuation from the
evaluation (they are still used during training, so
algorithms that rely on them are not harmed).
Number of Induced Clusters. The number
of gold POS tags in WSJ is 45, of which 11 are
punctuation marks. Therefore, for the ARR10 and
Clark03 models, 34 clusters were induced. For
GJ08 we received the output with 45 clusters. The
iHMM models of GVG09 determine the number
of clusters automatically (resulting in 47, 91 and
192 clusters, see below). For GG07, our com-
puting resources did not enable us to induce 45
clusters and we therefore used 176. Our focus in
this paper is to study the type vs. token distinction
rather than to provide a full scope comparison be-
tween algorithms, for which more clustering sizes
would need to be examined.
Configurations. We ran the ARR10 tagger
with the configuration detailed in (Abend et al,
2010). For Clark03, we ran his neyessenmorph
model7 10 times (using an unknown words thresh-
old of 5) and report the average score for each
measure. The models of GVG09 were run in the
three configurations reported in their paper: one
with a Dirichlet process prior and fixed parame-
ters, another with a Pittman-Yore prior with fixed
parameters, and a third with a Dirichlet process
prior with parameters learnt from the data. All five
models were run in an optimal configuration.
We obtained the code of Goldwater and Grif-
fiths? BHMM model and ran it for 10K iterations
with an annealing technique for parameter estima-
tion. That was the best parameter estimation tech-
nique available to us. This is the first time that this
model is evaluated on such a large experimental
corpus, and it performed well under these condi-
tions.
The output of the model of GJ08 was sent to
us by the authors. The model was run on sec-
5We use all WSJ tokens in the training stage, but omit
punctuation marks during evaluation.
6The 17 most frequent tags cover 94% of the word in-
stances and more than 99% of the word types in the WSJ
gold standard tagging.
7www.cs.rhul.ac.uk/home/alexc/RHUL/Downloads.html
83
tions 2-21 of the WSJ-PTB using significantly
inferior computing resources compared to those
used for producing the results reported in their
paper. While this model cannot be compared to
the aforementioned six models due to the subopti-
mal configuration, we evaluate its output using our
measures to get a broader variety of experimental
results8.
5.2 Results and Discussion
Table 1 presents the scores of the compared mod-
els under all evaluation measures (six token level,
six type level). What is important here to note
are the differences between type and token level
evaluations for the algorithms. We are mainly
interested in two things: (1) seeing how relative
rankings change in the two evaluation types, thus
showing that the two types are not highly corre-
lated and are both useful; and (2) insights gained
by using a type level evaluation in addition to the
usual token level one.
Note that the table should not be used to deduce
which algorithm is the ?best? for the task, even ac-
cording to a single evaluation type. This is be-
cause, as explained above, the algorithms do not
induce the same number of clusters and this affects
their results.
Results indicate that type level evaluation re-
veals aspects of the clustering quality that are not
expressed in the token level. For the Clark03
model the disparity is most apparent. While in
the token level it performs very well (better than
the polysemous algorithms for the 1-1, V and NVI
token level measures), in the type level it is the
second worst in the item-based 1-1 scores and the
worst in the M-1 scores.
Here we have a clear demonstration of the value
of type level evaluation. The Clark03 algorithm
is assessed as excellent using token level evalua-
tion (second only to ARR10 in M-1, 1-1, V and
NVI), and only a type level one shows its rela-
tively poor type performance. Although readers
may think that this is natural due to the algorithm?s
monosemous nature, this is not the case, since the
monosemous ARR10 generally ranked first in the
type level measures (more on this below).
The disparity is also observed for polysemous
algorithms. The GG07 model?s token level scores
are mediocre, while in the type level MicroC 1-1
8We would like to thank all authors for sending us the
data.
measure this model is the best and in the type level
MicroI and MacroI 1-1 measures it is the second
best.
Monosemous vs. polysemous algorithms. The
table shows that the ARR10 model achieves the
best results in most type and token level evalua-
tion measures. The fact that this monosemous al-
gorithm outperforms the polysemous ones, even
in a type level evaluation, may seem strange at
first sight but can be explained as follows. Pol-
ysemous tokens account for almost 60% of the
corpus (565K out of 950K), so we could expect
that a monosemous algorithm should do badly in
a token-level evaluation. However, for most of the
polysemous tokens the polysemy is only weakly
present in the corpus9, so it is hard to detect even
for polysemous algorithms. Regarding types, pol-
ysemous types constitute only 16.6% of the cor-
pus types, so a monosemous algorithm which is
quite good in assigning types to clusters has a good
chance of beating polysemous algorithms in a type
level evaluation.
Hence, monosemous POS induction algorithms
are not at such a great disadvantage relative to pol-
ysemous ones. This observation, which was fully
motivated by our type level case study, might be
used to guide future work on POS induction, and
it thus serves as another demonstration for the util-
ity of type level evaluation.
Hill climbing algorithm. For the type level
measures with greedy M-1 mapping, we used the
hill-climbing algorithm described in Section 4.
Recall that the mapping to which our algorithm
converges depends on its random initialization.
We therefore ran the algorithm with 10 differ-
ent random initializations and report the obtained
maximum for MacroI, MicroI and MicroC in Ta-
ble 1. The different initializations caused very lit-
tle fluctuation: not more than 1% in the 9 (7) best
runs for the item-based (MicroC) measures. We
take this result as an indication that the obtained
maximum is a good approximation of the global
maximum.
We tried to improve the algorithm by selecting
an intelligent initialization heuristic. We used the
M-1 mapping obtained by mapping each induced
cluster to the gold class with which it has the high-
9Only about 27% of the tokens are instances of words that
are polysemous but not weakly polysemous (we call a word
weakly polysemous if more than 95% of its instances (tokens)
are tagged by the same tag).
84
Token Level Evaluation Type Level Evaluation
MacroI MicroI MicroC
M-1 1-1 NVI V H(C|K) H(K|C) M-1 1-1 M-1 1-1 M-1 1-1
ARR10 0.675 0.588 0.809 0.608 1.041 1.22 0.579 0.444 0.596 0.455 0.624 0.403
Clark03 0.65 0.484 0.887 0.586 1.04 1.441 0.396 0.301 0.384 0.288 0.463 0.347
GG07 0.5 0.415 0.989 0.479 1.523 1.241 0.497 0.405 0.461 0.398 0.563 0.445
GVG09(1) 0.51 0.444 1.033 0.477 1.471 1.409 0.513 0.354 0.436 0.352 0.486 0.33
GVG09(2) 0.591 0.484 0.998 0.529 1.221 1.564 0.637 0.369 0.52 0.373 0.548 0.32
GVG09(3) 0.668 0.368 1.132 0.534 0.978 2.18 0.736 0.280 0.558 0.276 0.565 0.199
GJ08* 0.605 0.383 1.09 0.506 1.231 1.818 0.467 0.298 0.446 0.311 0.561 0.291
Table 1: Token level (left columns) and type level (right columns) results for seven POS induction
algorithms (rows) (see text for details). Token and type level performance are weakly correlated and
complement each other as evaluation measures. ARR10, a monosemous algorithm, yields the best results
in most measures. (GJ08* results are different from those reported in the original paper because it was
run with weaker computing resources than those used there.)
est weight edge in the bipartite graph. Recall from
Section 4.1 that this is a reasonable approximation
of the greedy M-1 mapping. Again, we ran it for
the three type level measures for 10 times with a
random update order on the induced clusters. This
had only a minor effect on the final scores.
Number of clusters. Previous work (Reichart
and Rappoport, 2009) demonstrated that in data
sets where a relatively small fraction of the gold
classes covers most of the items, it is reasonable
to choose this number to be the number of induced
clusters. In our experimental data set, this number
(the ?prominent cluster number?) is around 17 (see
Section 5.1). Up to this number, increasing the
number of clusters is likely to have a positive ef-
fect on token level M-1, 1-1, H(C|K), and H(K|C)
scores. Inducing a larger number of clusters, how-
ever, is likely to positively affect M-1 and H(C|K)
but to have a negative effect on 1-1 and H(K|C).
This tendency is reflected in Table 1. For the
GG07 model the number of induced clusters, 17,
approximates the number of prominent clusters
and is lower than the number of induced clus-
ters of the other models. This is reflected by
its low token level M-1 and H(C|K) performance
and its high quality H(K|C) and NVI token level
scores. The GVG (1)-(3) models induced 47, 91
and 192 clusters respectively. This might explain
the high token level M-1 and H(C|K) performance
of GVG(3), as well as its high M-1 type level
performance, compared to its mediocre scores in
other measures.
The item based measures. The table indicates
that there is no substantial difference between the
two item based type level scores with 1-1 map-
ping. The definitions of MacroI and MicroI imply
that if |Ai|+ |h(Bi)| (which equals |Ai|+ |Bi| un-
der a 1-1 mapping) is constant for all word types,
then a clustering will score equally on both 1-1
type measures. Indeed, in our experimental cor-
pus 83.4% of the word types have one POS tag,
12.5% have 2, 3.1% have 3 and only 1% of the
words have more. Therefore, |Ai| is roughly con-
stant. The ARR10 and Clark03 models assign a
word type to a single cluster. For the other models,
the number of clusters per word type is generally
similar to that of the gold standard. Consequently,
|Bi| is roughly constant as well, which explains
the similar behavior of the two measures.
Note that for other clustering tasks |Ai| may not
necessarily be constant, so the MacroI and MicroI
scores are not likely to be as similar under the 1-1
mapping.
6 Summary
We discussed type level evaluation for polysemous
clustering, presented new mapping-based evalu-
ation measures, and applied them to the evalua-
tion of POS induction algorithms, demonstrating
that type level measures provide value beyond the
common token level ones.
We hope that type level evaluation in general
and the proposed measures in particular will be
used in the future for evaluating clustering perfor-
mance in NLP tasks.
References
Omri Abend, Roi Reichart and Ari Rappoport, 2010.
Improved Unsupervised POS Induction through Pro-
totype Discovery. ACL ?10.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
85
Alexander Clark, 2003. Combining Distributional and
Morphological Information for Part of Speech In-
duction. EACL ?03.
Sajib Dasgupta and Vincent Ng, 2007. Unsu-
pervised Part-of-Speech Acquisition for Resource-
Scarce Languages. EMNLP-CoNLL ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport. 2008. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08
I. S. Dhillon, S. Mallela, and D. S. Modha, 2003. In-
formation Theoretic Co-clustering. KDD ?03
Micha Elsner, Eugene Charniak, and Mark Johnson,
2009. Structured Generative Models for Unsuper-
vised Named-Entity Clustering. NAACL ?09.
Stella Frank, Sharon Goldwater, and Frank Keller,
2009. Evaluating Models of Syntactic Category
Acquisition without Using a Gold Standard. Proc.
31st Annual Conf. of the Cognitive Science Society,
2576?2581.
E.B Fowlkes and C.L. Mallows, 1983. A Method for
Comparing Two Hierarchical Clusterings. Journal
of American statistical Association,78:553-569.
Benjamin C. M. Fung, Ke Wang, and Martin Ester,
2003. Hierarchical Document Clustering using Fre-
quent Itemsets. SIAM International Conference on
Data Mining ?03.
Jianfeng Gao and Mark Johnson, 2008. A Compar-
ison of Bayesian Estimators for Unsupervised Hid-
den Markov Model POS Taggers. EMNLP ?08.
Sharon Goldwater and Tom Griffiths, 2007. Fully
Bayesian Approach to Unsupervised Part-of-Speech
Tagging. ACL ?07.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar and Fre-
nando Pereira, 2009. Posterior vs. Parameter Spar-
sity in Latent Variable Models. NIPS ?09.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech Tagging for Grammar Induction. COLING
?08.
L. Hubert and J. Schultz, 1976. Quadratic Assignment
as a General Data Analysis Strategy. British Journal
of Mathematical and Statistical Psychology, 29:190-
241.
L. Hubert and P. Arabie, 1985. Comparing Partitions.
Journal of Classification, 2:193-218.
Maurice Kandall and Jean Dickinson, 1990. Rank
Correlation Methods. Oxford University Press, New
York.
Karin Kipper, Hoa Trang Dang and Martha Palmer,
2000. Class-Based Construction of a Verb Lexicon.
AAAI ?00.
Harold W. Kuhn, 1955. The Hungarian Method for
the Assignment Problem. Naval Research Logistics
Quarterly, 2:83-97.
Bjornar Larsen and Chinatsu Aone, 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. KDD ?99.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313-330.
Marina Meila and David Heckerman, 2001. An Ex-
perimental Comparison of Model-based Clustering
Methods. Machine Learning, 42(1/2):9-29.
Marina Meila, 2007. Comparing Clustering ? an In-
formation Based Distance. Journal of Multivariate
Analysis, 98:873-895.
C.W Milligan, S.C Soon and L.M Sokol, 1983. The
Effect of Cluster Size, Dimensionality and the Num-
ber of Clusters on Recovery of True Cluster Struc-
ture. IEEE transactions on Pattern Analysis and
Machine Intelligence, 5:40-47.
Boris G. Mirkin, 1996. Mathematical Classification
and Clustering. Kluwer Academic Press.
Michael Mitzenmacher , 2004. A Brief History of
Generative Models for Power Law and Lognormal
Distributions . Internet Mathematics, 1(2):226-251.
Soto Montalvo, Raquel Martnez, Arantza Casillas, and
Vctor Fresno, 2006. Multilingual Document Clus-
tering: an Heuristic Approach Based on Cognate
Named Entities. ACL ?06.
James Munkres, 1957. Algorithms for the Assignment
and Transportation Problems. Journal of the SIAM,
5(1):32-38.
Cristina Nicolae and Gabriel Nicolae, 2006. BEST-
CUT: A Graph Algorithm for Coreference Resolu-
tion. EMNLP ?06.
Darius M. Pfitzner, Richard E. Leibbrandt and David
M.W Powers, 2008. Characterization and Evalua-
tion of Similarity Measures for Pairs of Clusterings.
Knowledge and Information Systems: An Interna-
tional Journal, DOI 10.1007/s10115-008-0150-6.
William Rand, 1971. Objective Criteria for the Evalu-
ation of Clustering Methods. Journal of the Ameri-
can Statstical Association, 66(336):846-850.
86
Roi Reichart and Ari Rappoport, 2008. Unsupervised
Induction of Labeled Parse Trees by Clustering with
Syntactic Features. COLING ?08.
Roi Reichart and Ari Rappoport, 2009. The NVI Clus-
tering Evaluation Measure. CoNLL ?09.
Andrew Rosenberg and Julia Hirschberg, 2007. V-
Measure: A Conditional Entropy-based External
Cluster Evaluation Measure. EMNLP ?07.
Sabine Schulte im Walde, 2006. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Computational Linguistics, 32(2):159-194.
Noah A. Smith and Jason Eisner, 2004. Annealing
Techniques for Unsupervised Statistical Language
Learning. ACL ?04.
Stijn Van Dongen, 2000. Performance Criteria for
Graph Clustering and Markov Cluster Experiments.
Technical report CWI, Amsterdam
Jurgen Van Gael, Andreas Vlachos and Zoubin Ghahra-
mani, 2009. The Infinite HMM for Unsupervised
POS Tagging. EMNLP ?09.
Yujing Zeng, Jianshan Tang, Javier Garcia-Frias, and
Guang R. Gao, 2002. An Adaptive Meta-clustering
Approach: Combining the Information from Differ-
ent Clustering Results . CSB 00:276
87
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107?116,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem, Israel
dmitry@alice.nc.huji.ac.il
Oren Tsur
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
oren@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
arir@cs.huji.ac.il
Abstract
Sarcasm is a form of speech act in which
the speakers convey their message in an
implicit way. The inherently ambiguous
nature of sarcasm sometimes makes it hard
even for humans to decide whether an ut-
terance is sarcastic or not. Recognition of
sarcasm can benefit many sentiment analy-
sis NLP applications, such as review sum-
marization, dialogue systems and review
ranking systems.
In this paper we experiment with semi-
supervised sarcasm identification on two
very different data sets: a collection of
5.9 million tweets collected from Twit-
ter, and a collection of 66000 product re-
views from Amazon. Using the Mechani-
cal Turk we created a gold standard sam-
ple in which each sentence was tagged by
3 annotators, obtaining F-scores of 0.78 on
the product reviews dataset and 0.83 on
the Twitter dataset. We discuss the dif-
ferences between the datasets and how the
algorithm uses them (e.g., for the Amazon
dataset the algorithm makes use of struc-
tured information). We also discuss the
utility of Twitter #sarcasm hashtags for the
task.
1 Introduction
Sarcasm (also known as verbal irony) is a sophis-
ticated form of speech act in which the speakers
convey their message in an implicit way. One in-
herent characteristic of the sarcastic speech act is
that it is sometimes hard to recognize. The dif-
ficulty in recognition of sarcasm causes misun-
derstanding in everyday communication and poses
problems to many NLP systems such as online
review summarization systems, dialogue systems
or brand monitoring systems due to the failure of
state of the art sentiment analysis systems to detect
sarcastic comments. In this paper we experiment
with a semi-supervised framework for automatic
identification of sarcastic sentences.
One definition for sarcasm is: the activity of
saying or writing the opposite of what you mean,
or of speaking in a way intended to make someone
else feel stupid or show them that you are angry
(Macmillan English Dictionary (2007)). Using the
former definition, sarcastic utterances appear in
many forms (Brown, 1980; Gibbs and O?Brien,
1991). It is best to present a number of examples
which show different facets of the phenomenon,
followed by a brief review of different aspects of
the sarcastic use. The sentences are all taken from
our experimental data sets:
1. ?thank you Janet Jackson for yet another
year of Super Bowl classic rock!? (Twitter)
2. ?He?s with his other woman: XBox 360. It?s
4:30 fool. Sure I can sleep through the gun-
fire? (Twitter)
3. ?Wow GPRS data speeds are blazing fast.?
(Twitter)
4. ?[I] Love The Cover? (book, amazon)
5. ?Defective by design? (music player, ama-
zon)
Example (1) refers to the supposedly lame mu-
sic performance in super bowl 2010 and attributes
it to the aftermath of the scandalous performance
of Janet Jackson in the previous year. Note that the
previous year is not mentioned and the reader has
to guess the context (use universal knowledge).
The words yet and another might hint at sarcasm.
107
Example (2) is composed of three short sentences,
each of them sarcastic on its own. However, com-
bining them in one tweet brings the sarcasm to
its extreme. Example (3) is a factual statement
without explicit opinion. However, having a fast
connection is a positive thing. A possible sar-
casm emerges from the over exaggeration (?wow?,
?blazing-fast?).
Example (4) from Amazon, might be a genuine
compliment if it appears in the body of the review.
However, recalling the expression ?don?t judge a
book by its cover?, choosing it as the title of the
review reveals its sarcastic nature. Although the
negative sentiment is very explicit in the iPod re-
view (5), the sarcastic effect emerges from the pun
that assumes the knowledge that the design is one
of the most celebrated features of Apple?s prod-
ucts. (None of the above reasoning was directly
introduced to our algorithm.)
Modeling the underlying patterns of sarcastic
utterances is interesting from the psychological
and cognitive perspectives and can benefit var-
ious NLP systems such as review summariza-
tion (Popescu and Etzioni, 2005; Pang and Lee,
2004; Wiebe et al, 2004; Hu and Liu, 2004) and
dialogue systems. Following the ?brilliant-but-
cruel? hypothesis (Danescu-Niculescu-Mizil et al,
2009), it can help improve ranking and recommen-
dation systems (Tsur and Rappoport, 2009). All
systems currently fail to correctly classify the sen-
timent of sarcastic sentences.
In this paper we utilize the semi-supervised sar-
casm identification algorithm (SASI) of (Tsur et
al., 2010). The algorithm employs two modules:
semi supervised pattern acquisition for identify-
ing sarcastic patterns that serve as features for a
classifier, and a classification stage that classifies
each sentence to a sarcastic class. We experiment
with two radically different datasets: 5.9 million
tweets collected from Twitter, and 66000 Amazon
product reviews. Although for the Amazon dataset
the algorithm utilizes structured information, re-
sults for the Twitter dataset are higher. We discuss
the possible reasons for this, and also the utility
of Twitter #sarcasm hashtags for the task. Our al-
gorithm performed well in both domains, substan-
tially outperforming a strong baseline based on se-
mantic gap and user annotations. To further test its
robustness we also trained the algorithm in a cross
domain manner, achieving good results.
2 Data
The datasets we used are interesting in their own
right for many applications. In addition, our algo-
rithm utilizes some aspects that are unique to these
datasets. Hence, before describing the algorithm,
we describe the datasets in detail.
Twitter Dataset. Since Twitter is a relatively
new service, a somewhat lengthy description of
the medium and the data is appropriate.
Twitter is a very popular microblogging service.
It allows users to publish and read short messages
called tweets (also used as a verb: to tweet: the act
of publishing on Twitter). The tweet length is re-
stricted to 140 characters. A user who publishes a
tweet is referred to as a tweeter and the readers are
casual readers or followers if they are registered to
get al tweets by this tweeter.
Apart from simple text, tweets may contain ref-
erences to url addresses, references to other Twit-
ter users (these appear as @<user>) or a con-
tent tag (called hashtags) assigned by the tweeter
(#<tag>). An example of a tweet is: ?listen-
ing to Andrew Ridgley by Black Box Recorder on
@Grooveshark: http://tinysong.com/cO6i #good-
music?, where ?grooveshark? is a Twitter user
name and #goodmusic is a tag that allows to
search for tweets with the same tag. Though fre-
quently used, these types of meta tags are optional.
In order to ignore specific references we substi-
tuted such occurrences with special tags: [LINK],
[USER] and [HASHTAG] thus we have ?listen-
ing to Andrew Ridgley by Black Box Recorder on
[USER]: [LINK] [HASHTAG]?. It is important
to mention that hashtags are not formal and each
tweeter can define and use new tags as s/he likes.
The number of special tags in a tweet is only
subject to the 140 characters constraint. There is
no specific grammar that enforces the location of
special tags within a tweet.
The informal nature of the medium and the 140
characters length constraint encourages massive
use of slang, shortened lingo, ascii emoticons and
other tokens absent from formal lexicons.
These characteristics make Twitter a fascinat-
ing domain for NLP applications, although posing
great challenges due to the length constraint, the
complete freedom of style and the out of discourse
nature of tweets.
We used 5.9 million unique tweets in our
dataset: the average number of words is 14.2
108
words per tweet, 18.7% contain a url, 35.3% con-
tain reference to another tweeter and 6.9% contain
at least one hashtag1.
The #sarcasm hashtag One of the hashtags
used by Twitter users is dedicated to indicate sar-
castic tweets. An example of the use of the tag
is: ?I guess you should expect a WONDERFUL
video tomorrow. #sarcasm?. The sarcastic hashtag
is added by the tweeter. This hashtag is used in-
frequently as most users are not aware of it, hence,
the majority of sarcastic tweets are not explicitly
tagged by the tweeters. We use tagged tweets as
a secondary gold standard. We discuss the use of
this tag in Section 5.
Amazon dataset. We used the same dataset
used by (Tsur et al, 2010), containing 66000 re-
views for 120 products from Amazon.com. The
corpus contained reviews for books from differ-
ent genres and various electronic products. Ama-
zon reviews are much longer than tweets (some
reach 2000 words, average length is 953 charac-
ters), they are more structured and grammatical
(good reviews are very structured) and they come
in a known context of a specific product. Reviews
are semi-structured as besides the body of the re-
view they all have the following fields: writer,
date, star rating (the overall satisfaction of the re-
view writer) and a one line summary.
Reviews refer to a specific product and rarely
address each other. Each review sentence is, there-
fore, part of a context ? the specific product, the
star rating, the summary and other sentences in
that review. In that sense, sentences in the Ama-
zon dataset differ radically from the contextless
tweets. It is worth mentioning that the majority
of reviews are on the very positive side (star rating
average of 4.2 stars).
3 Classification Algorithm
Our algorithm is semi-supervised. The input is
a relatively small seed of labeled sentences. The
seed is annotated in a discrete range of 1 . . . 5
where 5 indicates a clearly sarcastic sentence and
1 indicates a clear absence of sarcasm. A 1 . . . 5
scale was used in order to allow some subjectiv-
ity and since some instances of sarcasm are more
explicit than others.
1The Twitter data was generously provided to us by Bren-
dan O?Connor.
Given the labeled sentences, we extracted a set
of features to be used in feature vectors. Two basic
feature types are utilized: syntactic and pattern-
based features. We constructed feature vectors for
each of the labeled examples in the training set and
used them to build a classifier model and assign
scores to unlabeled examples. We next provide a
description of the algorithmic framework of (Tsur
et al, 2010).
Data preprocessing A sarcastic utterance usu-
ally has a target. In the Amazon dataset these
targets can be exploited by a computational al-
gorithm, since each review targets a product, its
manufacturer or one of its features, and these are
explicitly represented or easily recognized. The
Twitter dataset is totally unstructured and lacks
textual context, so we did not attempt to identify
targets.
Our algorithmic methodology is based on
patterns. We could use patterns that include
the targets identified in the Amazon dataset.
However, in order to use less specific patterns,
we automatically replace each appearance
of a product, author, company, book name
(Amazon) and user, url and hashtag (Twitter)
with the corresponding generalized meta tags
?[PRODUCT]?,?[COMPANY]?,?[TITLE]? and
?[AUTHOR]? tags2 and ?[USER]?,?[LINK]? and
?[HASHTAG]?. We also removed all HTML tags
and special symbols from the review text.
Pattern extraction Our main feature type is
based on surface patterns. In order to extract such
patterns automatically, we followed the algorithm
given in (Davidov and Rappoport, 2006). We clas-
sified words into high-frequency words (HFWs)
and content words (CWs). A word whose cor-
pus frequency is more (less) than FH (FC) is con-
sidered to be a HFW (CW). Unlike in (Davidov
and Rappoport, 2006), we consider all punctuation
characters as HFWs. We also consider [product],
[company], [title], [author] tags as HFWs for pat-
tern extraction. We define a pattern as an ordered
sequence of high frequency words and slots for
content words. The FH and FC thresholds were
set to 1000 words per million (upper bound for
FC) and 100 words per million (lower bound for
FH )3.
2Appropriate names are provided with each review so this
replacement can be done automatically.
3Note that FH and FC set bounds that allow overlap be-
tween some HFWs and CWs.
109
The patterns allow 2-6 HFWs and 1-6 slots for
CWs. For each sentence it is possible to gener-
ate dozens of patterns that may overlap. For ex-
ample, given a sentence ?Garmin apparently does
not care much about product quality or customer
support?, we have generated several patterns in-
cluding ?[COMPANY] CW does not CW much?,
?does not CW much about CW CW or?, ?not CW
much? and ?about CW CW or CW CW.?. Note
that ?[COMPANY]? and ?.? are treated as high
frequency words.
Pattern selection The pattern extraction stage
provides us with hundreds of patterns. However,
some of them are either too general or too specific.
In order to reduce the feature space, we have used
two criteria to select useful patterns.
First, we removed all patterns which appear
only in sentences originating from a single prod-
uct/book (Amazon). Such patterns are usually
product-specific. Next we removed all patterns
which appear in the seed both in some example la-
beled 5 (clearly sarcastic) and in some other exam-
ple labeled 1 (obviously not sarcastic). This filters
out frequent generic and uninformative patterns.
Pattern selection was performed only on the Ama-
zon dataset as it exploits review?s meta data.
Pattern matching Once patterns are selected,
we have used each pattern to construct a single en-
try in the feature vectors. For each sentence we
calculated a feature value for each pattern as fol-
lows:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
? : Sparse match ? same as exact match
but additional non-matching words can be
inserted between pattern components.
? ? n/N : Incomplete match ? only n > 1 of N pattern
components appear in the sentence,
while some non-matching words can
be inserted in-between. At least one of the
appearing components should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
?\? 0.05 0.1 0.2
0.05 0.48 0.45 0.39
0.1 0.50 0.51 0.40
0.2 0.40 0.42 0.33
Table 1: Results (F-Score for ?no enrichment? mode) of
cross validation with various values for ? and ? on Twit-
ter+Amazon data
? = ? = 0.1 in all experiments. Table 1 demon-
strates the results obtained with different values
for ? and ?.
Thus, for the sentence ?Garmin apparently does
not care much about product quality or customer
support?, the value for ?[company] CW does not?
would be 1 (exact match); for ?[company] CW
not? would be 0.1 (sparse match due to insertion
of ?does?); and for ?[company] CW CW does not?
would be 0.1 ? 4/5 = 0.08 (incomplete match
since the second CW is missing).
Punctuation-based features In addition to
pattern-based features we used the following
generic features: (1) Sentence length in words,
(2) Number of ?!? characters in the sentence, (3)
Number of ??? characters in the sentence, (4)
Number of quotes in the sentence, and (5) Num-
ber of capitalized/all capitals words in the sen-
tence. All these features were normalized by di-
viding them by the (maximal observed value ? av-
eraged maximal value of the other feature groups),
thus the maximal weight of each of these fea-
tures is equal to the averaged weight of a single
pattern/word/n-gram feature.
Data enrichment Since we start with only a
small annotated seed for training (particularly, the
number of clearly sarcastic sentences in the seed is
modest) and since annotation is noisy and expen-
sive, we would like to find more training examples
without requiring additional annotation effort.
To achieve this, we posited that sarcastic sen-
tences frequently co-appear in texts with other sar-
castic sentences (i.e. example (2) in Section 1).
We performed an automated web search using the
Yahoo! BOSS API4, where for each sentence s in
the training set (seed), we composed a search en-
gine query qs containing this sentence5. We col-
lected up to 50 search engine snippets for each
example and added the sentences found in these
snippets to the training set. The label (level of sar-
4http://developer.yahoo.com/search/boss.
5If the sentence contained more than 6 words, only the
first 6 words were included in the search engine query.
110
casm) Label(sq) of a newly extracted sentence sq
is similar to the label Label(s) of the seed sen-
tence s that was used for the query that acquired it.
The seed sentences together with newly acquired
sentences constitute the (enriched) training set.
Data enrichment was performed only for the
Amazon dataset where we have a manually tagged
seed and the sentence structure is closer to stan-
dard English grammar. We refer the reader to
(Tsur et al, 2010) for more details about the en-
richment process and for a short discussion about
the usefulness of web-based data enrichment in the
scope of sarcasm recognition.
Classification In order to assign a score to new
examples in the test set we use a k-nearest neigh-
bors (kNN)-like strategy. We construct feature
vectors for each example in the training and test
sets. We would like to calculate the score for each
example in the test set. For each feature vector v in
the test set, we compute the Euclidean distance to
each of the matching vectors in the extended train-
ing set, where matching vectors share at least one
pattern feature with v.
Let ti, i = 1..k be the k vectors with lowest
Euclidean distance to v6. Then v is classified with
a label l as follows:
Count(l) = Fraction of training vectors with label l
Label(v) =
[
1
k
?
i
Count(Label(ti)) ? Label(ti)
?
j Count(label(tj))
]
Thus the score is a weighted average of the k clos-
est training set vectors. If there are less than k
matching vectors for the given example then fewer
vectors are used in the computation. If there are
no matching vectors found for v, we assigned the
default value Label(v) = 1, since sarcastic sen-
tences are fewer in number than non-sarcastic ones
(this is a ?most common tag? strategy).
4 Evaluation Setup
Seed and extended training sets (Amazon). As
described in the previous section, SASI is semi su-
pervised, hence requires a small seed of annotated
data. We used the same seed of 80 positive (sar-
castic) examples and 505 negative examples de-
scribed at (Tsur et al, 2010).
After automatically expanding the training set,
our training data now contains 471 positive exam-
ples and 5020 negative examples. These ratios are
6We used k = 5 for all experiments.
to be expected, since non-sarcastic sentences out-
number sarcastic ones, definitely when most on-
line reviews are positive (Liu et al, 2007). This
generally positive tendency is also reflected in our
data ? the average number of stars is 4.12.
Seed training set with #sarcasm (Twitter). We
used a sample of 1500 tweets marked with the
#sarcasm hashtag as a positive set that represents
sarcasm styles special to Twitter. However, this set
is very noisy (see discussion in Section 5).
Seed training set (cross domain). Results ob-
tained by training on the 1500 #sarcasm hash-
tagged tweets were not promising. Examination of
the #sarcasm tagged tweets shows that the annota-
tion is biased and noisy as we discuss in length
in Section 5. A better annotated set was needed
in order to properly train the algorithm. Sarcas-
tic tweets are sparse and hard to find and annotate
manually. In order to overcome sparsity we used
the positive seed annotated on the Amazon dataset.
The training set was completed by manually se-
lected negative example from the Twitter dataset.
Note that in this setting our training set is thus of
mixed domains.
4.1 Star-sentiment baseline
Many studies on sarcasm suggest that sarcasm
emerges from the gap between the expected utter-
ance and the actual utterance (see echoic mention,
allusion and pretense theories in Related Work
Section( 6)). We implemented a baseline designed
to capture the notion of sarcasm as reflected by
these models, trying to meet the definition ?saying
the opposite of what you mean in a way intended
to make someone else feel stupid or show you are
angry?.
We exploit the meta-data provided by Amazon,
namely the star rating each reviewer is obliged
to provide, in order to identify unhappy review-
ers. From this set of negative reviews, our base-
line classifies as sarcastic those sentences that ex-
hibit strong positive sentiment. The list of positive
sentiment words is predefined and captures words
typically found in reviews (for example, ?great?,
?excellent?, ?best?, ?top?, ?exciting?, etc).
4.2 Evaluation procedure
We used two experimental frameworks to test
SASI?s accuracy. In the first experiment we eval-
uated the pattern acquisition process, how consis-
tent it is and to what extent it contributes to correct
111
classification. We did that by 5-fold cross valida-
tion over the seed data.
In the second experiment we evaluated SASI on
a test set of unseen sentences, comparing its out-
put to a gold standard annotated by a large number
of human annotators (using the Mechanical Turk).
This way we verify that there is no over-fitting and
that the algorithm is not biased by the notion of
sarcasm of a single seed annotator.
5-fold cross validation (Amazon). In this ex-
perimental setting, the seed data was divided to 5
parts and a 5-fold cross validation test is executed.
Each time, we use 4 parts of the seed as the train-
ing data and only this part is used for the feature
selection and data enrichment. This 5-fold pro-
cess was repeated ten times. This procedure was
repeated with different sets of optional features.
We used 5-fold cross validation and not the
standard 10-fold since the number of seed exam-
ples (especially positive) is relatively small hence
10-fold is too sensitive to the broad range of possi-
ble sarcastic patterns (see the examples in Section
1).
Classifying new sentences (Amazon & Twitter).
Evaluation of sarcasm is a hard task due to the
elusive nature of sarcasm, as discussed in Sec-
tion 1. In order to evaluate the quality of our al-
gorithm, we used SASI to classify all sentences
in both corpora (besides the small seed that was
pre-annotated and was used for the evaluation in
the 5-fold cross validation experiment). Since it
is impossible to created a gold standard classifica-
tion of each and every sentence in the corpus, we
created a small test set by sampling 90 sentences
which were classified as sarcastic (labels 3-5) and
90 sentences classified as not sarcastic (labels 1,2).
The sampling was performed on the whole corpus
leaving out only the seed data.
Again, the meta data available in the Amazon
dataset alows us a stricter evaluation. In order
to make the evaluation harder for our algorithm
and more relevant, we introduced two constraints
to the sampling process: i) we sampled only sen-
tences containing a named-entity or a reference to
a named entity. This constraint was introduced in
order to keep the evaluation set relevant, since sen-
tences that refer to the named entity (the target of
the review) are more likely to contain an explicit
or implicit sentiment. ii) we restricted the non-
sarcastic sentences to belong to negative reviews
(1-3 stars) so that all sentences in the evaluation
set are drawn from the same population, increas-
ing the chances they convey various levels of di-
rect or indirect negative sentiment7.
Experimenting with the Twitter dataset, we sim-
ply classified each tweet into one of 5 classes
(class 1: not sarcastic, class 5: clearly sarcastic)
according to the label given by the algorithm. Just
like the evaluation of the algorithm on the Amazon
dataset, we created a small evaluation set by sam-
pling 90 sentences which were classified as sarcas-
tic (labels 3-5) and 90 sentences classified as not
sarcastic (labels 1,2).
Procedure Each evaluation set was randomly
divided to 5 batches. Each batch contained 36 sen-
tences from the evaluation set and 4 anchor sen-
tences: two with sarcasm and two sheer neutral.
The anchor sentences were not part of the test set
and were the same in all five batches. The purpose
of the anchor sentences is to control the evaluation
procedure and verify that annotation is reasonable.
We ignored the anchor sentences when assessing
the algorithm?s accuracy.
We used Amazon?s Mechanical Turk8 service
in order to create a gold standard for the evalua-
tion. We employed 15 annotators for each eval-
uation set. We used a relatively large number of
annotators in order to overcome the possible bias
induced by subjectivity (Muecke, 1982). Each an-
notator was asked to assess the level of sarcasm of
each sentence of a set of 40 sentences on a scale of
1-5. In total, each sentence was annotated by three
different annotators.
Inter Annotator Agreement. To simplify the
assessment of inter-annotator agreement, the scal-
ing was reduced to a binary classification where 1
and 2 were marked as non-sarcastic and 3-5 as sar-
castic (recall that 3 indicates a hint of sarcasm and
5 indicates ?clearly sarcastic?). We checked the
Fleiss? ? statistic to measure agreement between
multiple annotators. The inter-annotator agree-
ment statistic was ? = 0.34 on the Amazon dataset
and ? = 0.41 on the Twitter dataset.
These agreement statistics indicates a fair
agreement. Given the fuzzy nature of the task at
7Note that the second constraint makes the problem less
easy. If taken from all reviews, many of the sentences would
be positive sentences which are clearly non-sarcastic. Doing
this would bias selection to positive vs. negative samples in-
stead of sarcastic-nonsarcastic samples.
8https://www.mturk.com/mturk/welcome
112
Prec. Recall Accuracy F-score
punctuation 0.256 0.312 0.821 0.281
patterns 0.743 0.788 0.943 0.765
pat+punct 0.868 0.763 0.945 0.812
enrich punct 0.4 0.390 0.832 0.395
enrich pat 0.762 0.777 0.937 0.769
all: SASI 0.912 0.756 0.947 0.827
Table 2: 5-fold cross validation results on the Amazon gold
standard using various feature types. punctuation: punctua-
tion mark;, patterns: patterns; enrich: after data enrichment;
enrich punct: data enrichment based on punctuation only; en-
rich pat: data enrichment based on patterns only; SASI: all
features combined.
hand, this ? value is certainly satisfactory. We at-
tribute the better agreement on the twitter data to
the fact that in twitter each sentence (tweet) is con-
text free, hence the sentiment in the sentence is ex-
pressed in a way that can be perceived more easily.
Sentences from product reviews come as part of a
full review, hence the the sarcasm sometimes re-
lies on other sentences in the review. In our evalu-
ation scheme, our annotators were presented with
individual sentences, making the agreement lower
for those sentences taken out of their original con-
text. The agreement on the control set (anchor sen-
tences) had ? = 0.53.
Using Twitter #sarcasm hashtag. In addition to
the gold standard annotated using the Mechanical
Turk, we collected 1500 tweets that were tagged
#sarcastic by their tweeters. We call this sample
the hash-gold standard. It was used to further eval-
uate recall. This set (along with the negative sam-
ple) was used for a 5-fold cross validation in the
same manner describe for Amazon.
5 Results and discussion
5-fold cross validation (Amazon). Results are
analyzed and discussed in detail in (Tsur et al,
2010), however, we summarize it here (Table 2)
in order to facilitate comparison with the results
obtained on the Twitter dataset. SASI, including
all components, exhibits the best overall perfor-
mances with 91.2% precision and with F-Score
of 0.827. Interestingly, although data enrichment
brings SASI to the best performance in both preci-
sion and F-score, patterns+punctuations achieves
almost comparable results.
Newly introduced sentences (Amazon). In the
second experiment we evaluated SASI based on a
gold standard annotation created by 15 annotators.
Table 3 presents the results of our algorithm as
well as results of the heuristic baseline that makes
Prec. Recall FalsePos FalseNeg F Score
Star-sent. 0.5 0.16 0.05 0.44 0.242
SASI (AM) 0.766 0.813 0.11 0.12 0.788
SASI (TW) 0.794 0.863 0.094 0.15 0.827
Table 3: Evaluation on the Amazon (AM) and the Twitter
(TW) evaluation sets obtained by averaging on 3 human an-
notations per sentence. TW results were obtained with cross-
domain training.
Prec. Recall Accuracy F-score
punctuation 0.259 0.26 0.788 0.259
patterns 0.765 0.326 0.889 0.457
enrich punct 0.18 0.316 0.76 0.236
enrich pat 0.685 0.356 0.885 0.47
all no enrich 0.798 0.37 0.906 0.505
all SASI: 0.727 0.436 0.896 0.545
Table 4: 5-fold cross validation results on the Twitter hash-
gold standard using various feature types. punctuation: punc-
tuation marks; patterns: patterns; enrich: after data enrich-
ment; enrich punct: data enrichment based on punctuation
only; enrich pat: data enrichment based on patterns only;
SASI: all features combined.
use of meta-data, designed to capture the gap be-
tween an explicit negative sentiment (reflected by
the review?s star rating) and explicit positive senti-
ment words used in the review. Precision of SASI
is 0.766, a significant improvement over the base-
line with precision of 0.5.
The F-score shows more impressive improve-
ment as the baseline shows decent precision but a
very limited recall since it is incapable of recog-
nizing subtle sarcastic sentences. These results fit
the works of (Brown, 1980; Gibbs and O?Brien,
1991) claiming many sarcastic utterances do not
conform to the popular definition of ?saying or
writing the opposite of what you mean?. Table 3
also presents the false positive and false negative
ratios. The low false negative ratio of the baseline
confirms that while recognizing a common type
of sarcasm, the naive definition of sarcasm cannot
capture many other types sarcasm.
Newly introduced sentences (Twitter). Results
on the Twitter dataset are even better than those
obtained on the Amazon dataset, with accuracy of
0.947 (see Table 3 for precision and recall).
Tweets are less structured and are context free,
hence one would expect SASI to perform poorly
on tweets. Moreover, the positive part of the seed
is taken from the Amazon corpus hence might
seem tailored to sarcasm type targeted at prod-
ucts and part of a harsh review. On top of that,
the positive seed introduces some patterns with
tags that never occur in the Twitter test set ([prod-
uct/company/title/author]).
113
Our explanation of the excellent results is three-
fold: i) SASI?s robustness is achieved by the sparse
match (?) and incomplete match (?) that toler-
ate imperfect pattern matching and enable the use
of variations of the patterns in the learned feature
vector. ? and ? allow the introduction of patterns
with components that are absent from the posi-
tive seed, and can perform even with patterns that
contain special tags that are not part of the test
set. ii) SASI learns a model which spans a feature
space with more than 300 dimensions. Only part
of the patterns consist of meta tags that are spe-
cial to product reviews, the rest are strong enough
to capture the structure of general sarcastic sen-
tences and not product-specific sarcastic sentences
only. iii) Finally, in many cases, it might be that
the contextless nature of Twitter forces tweeters to
express sarcasm in a way that is easy to understand
from individual sentence. Amazon sentences co-
appear with other sentences (in the same review)
thus the sarcastic meaning emerges from the con-
text. Our evaluation scheme presents the annota-
tors with single sentences therefore Amazon sen-
tences might be harder to agree on.
hash gold standard (Twitter). In order to fur-
ther test out algorithm we built a model consist-
ing of the positive sample of the Amazon training,
the #sarcasm hash-tagged tweets and a sample of
non sarcastic tweets as the negative training set.
We evaluated it in a 5-fold cross validation man-
ner (only against the hash-gold standard). While
precision is still high with 0.727, recall drops to
0.436 and the F-Score is 0.545.
Looking at the hash-gold standard set, we ob-
served three main uses for the #sarcasm hashtag.
Differences between the various uses can explain
the relatively low recall. i) The tag is used as a
search anchor. Tweeters add the hashtag to tweets
in order to make them retrievable when searching
for the tag. ii) The tag is often abused and added
to non sarcastic tweets, typically to clarify that a
previous tweet should have been read sarcastically,
e.g.: ?@wrightfan05 it was #Sarcasm ?. iii) The
tag serves as a sarcasm marker in cases of a very
subtle sarcasm where the lack of context, the 140
length constraint and the sentence structure make
it impossible to get the sarcasm without the ex-
plicit marker. Typical examples are: ?#sarcasm
not at all.? or ?can?t wait to get home tonite #sar-
casm.?, which cannot be decided sarcastic without
the full context or the #sarcasm marker.
These three observations suggest that the hash-
gold standard is noisy (containing non-sarcastic
tweets) and is biased toward the hardest (insepa-
rable) forms of sarcasm where even humans get
it wrong without an explicit indication. Given
the noise and the bias, the recall is not as bad as
the raw numbers suggest and is actually in synch
with the results obtained on the Mechanical Turk
human-annotated gold standard. Table 4 presents
detailed results and the contribution of each type
of feature to the classification.
We note that the relative sparseness of sarcas-
tic utterances in everyday communication as well
as in these two datasets make it hard to accurately
estimate the recall value over these huge unanno-
tated data sets. Our experiment, however, indi-
cates that we achieve reasonable recall rates.
Punctuation Surprisingly, punctuation marks
serve as the weakest predictors, in contrast to Tep-
permann et al (2006). An exception is three con-
secutive dots, which when combine with other fea-
tures constitute a strong predictor. Interestingly
though, while in the cross validation experiments
SASI performance varies greatly (due to the prob-
lematic use of the #sarcasm hashtag, described
previously), performance based only on punctua-
tion are similar (Table 2 and Table 4).
Tsur et al (2010) presents some additional ex-
amples for the contribution of each type of feature
and their combinations.
6 Related Work
While the use of irony and sarcasm is well stud-
ied from its linguistic and psychologic aspects
(Muecke, 1982; Stingfellow, 1994; Gibbs and Col-
ston, 2007), automatic recognition of sarcasm is a
novel task, addressed only by few works. In the
context of opinion mining, sarcasm is mentioned
briefly as a hard nut that is yet to be cracked, see
comprehensive overview by (Pang and Lee, 2008).
Tepperman et al (2006) identify sarcasm in
spoken dialogue systems, their work is restricted
to sarcastic utterances that contain the expres-
sion ?yeah-right? and it depends heavily on cues
in the spoken dialogue such as laughter, pauses
within the speech stream, the gender (recognized
by voice) of the speaker and prosodic features.
Burfoot and Baldwin (2009) use SVM to deter-
mine whether newswire articles are true or satir-
ical. They introduce the notion of validity which
models absurdity via a measure somewhat close to
114
PMI. Validity is relatively lower when a sentence
includes a made-up entity or when a sentence con-
tains unusual combinations of named entities such
as, for example, those in the satirical article be-
ginning ?Missing Brazilian balloonist Padre spot-
ted straddling Pink Floyd flying pig?. We note
that while sarcasm can be based on exaggeration
or unusual collocations, this model covers only a
limited subset of the sarcastic utterances.
Tsur et al (2010) propose a semi supervised
framework for recognition of sarcasm. The pro-
posed algorithm utilizes some features specific to
(Amazon) product reviews. This paper continues
this line, proposing SASI a robust algorithm that
successfully captures sarcastic sentences in other,
radically different, domains such as twitter.
Utsumi (1996; 2000) introduces the implicit dis-
play theory, a cognitive computational framework
that models the ironic environment. The complex
axiomatic system depends heavily on complex for-
malism representing world knowledge. While
comprehensive, it is currently impractical to im-
plement on a large scale or for an open domain.
Mihalcea and Strapparava (2005) and Mihalcea
and Pulman (2007) present a system that identi-
fies humorous one-liners. They classify sentences
using naive Bayes and SVM. They conclude that
the most frequently observed semantic features are
negative polarity and human-centeredness. These
features are also observed in some sarcastic utter-
ances.
Some philosophical, psychological and linguis-
tic theories of irony and sarcasm are worth refer-
encing as a theoretical framework: the constraints
satisfaction theory (Utsumi, 1996; Katz, 2005),
the role playing theory (Clark and Gerrig, 1984),
the echoic mention framework (Wilson and Sper-
ber, 1992) and the pretence framework (Gibbs,
1986). These are all based on violation of the max-
ims proposed by Grice (1975).
7 Conclusion
We used SASI, the first robust algorithm for recog-
nition of sarcasm, to experiment with a novel
Twitter dataset and compare performance with an
Amazon product reviews dataset. Evaluating in
various ways and with different parameters con-
figurations, we achieved high precision, recall and
F-Score on both datasets even for cross-domain
training and with no need for domain adaptation.
In the future we will test the contribution of
sarcasm recognition for review ranking and sum-
marization systems and for brand monitoring sys-
tems.
References
R. L. Brown. 1980. The pragmatics of verbal irony.
In R. W. Shuy and A. Snukal, editors, Language use
and the uses of language, pages 111?127. George-
town University Press.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 161?164, Suntec, Singapore, August.
Association for Computational Linguistics.
H. Clark and R. Gerrig. 1984. On the pretence the-
ory of irony. Journal of Experimental Psychology:
General, 113:121?126.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study on
amazon.com helpfulness votes. Jun.
D. Davidov and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
Macmillan English Dictionary. 2007. Macmillan En-
glish Dictionary. Macmillan Education, 2 edition.
Raymond W Gibbs and Herbert L. Colston, editors.
2007. Irony in Language and Thought. Routledge
(Taylor and Francis), New York.
R. W. Gibbs and J. E. O?Brien. 1991. Psychological
aspects of irony understanding. Journal of Pragmat-
ics, 16:523?530.
R. Gibbs. 1986. On the psycholinguistics of sar-
casm. Journal of Experimental Psychology: Gen-
eral, 105:3?15.
H. P. Grice. 1975. Logic and conversation. In Peter
Cole and Jerry L. Morgan, editors, Syntax and se-
mantics, volume 3. New York: Academic Press.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 168?177, New York, NY, USA. ACM.
A. Katz. 2005. Discourse and social-cultural factors
in understanding non literal language. In Colston H.
and Katz A., editors, Figurative language compre-
hension: Social and cultural influences, pages 183?
208. Lawrence Erlbaum Associates.
115
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang,
and Ming Zhou. 2007. Low-quality product re-
view detection in opinion summarization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 334?342.
Rada Mihalcea and Stephen G. Pulman. 2007. Char-
acterizing humour: An exploration of features in hu-
morous texts. In CICLing, pages 337?347.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. pages 531?538, Vancouver, Canada.
D.C. Muecke. 1982. Irony and the ironic. Methuen,
London, New York.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 339?346,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Frank Jr. Stingfellow. 1994. The Meaning of Irony.
State University of NY, New York.
J. Tepperman, D. Traum, and S. Narayanan. 2006.
Yeah right: Sarcasm recognition for spoken dialogue
systems. In InterSpeech ICSLP, Pittsburgh, PA.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most help-
ful book reviews. In International AAAI Conference
on Weblogs and Social Media.
Oren Tsur, Dmitry Davidiv, and Ari Rappoport. 2010.
Icwsm ? a great catchy name: Semi-supervised
recognition of sarcastic sentences in product re-
views. In International AAAI Conference on We-
blogs and Social Media.
Akira Utsumi. 1996. A unified theory of irony and
its computational formalization. In COLING, pages
962?967.
Akira Utsumi. 2000. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics,
32(12):1777?1806.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277? 308, January.
D. Wilson and D. Sperber. 1992. On verbal irony. Lin-
gua, 87:53?76.
116
