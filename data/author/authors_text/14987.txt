Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1406?1415,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Statistical NLG Framework for Aggregated Planning and Realization
Ravi Kondadadi?, Blake Howald and Frank Schilder
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We present a hybrid natural language gen-
eration (NLG) system that consolidates
macro and micro planning and surface re-
alization tasks into one statistical learn-
ing process. Our novel approach is based
on deriving a template bank automatically
from a corpus of texts from a target do-
main. First, we identify domain specific
entity tags and Discourse Representation
Structures on a per sentence basis. Each
sentence is then organized into semanti-
cally similar groups (representing a do-
main specific concept) by k-means cluster-
ing. After this semi-automatic processing
(human review of cluster assignments), a
number of corpus?level statistics are com-
piled and used as features by a ranking
SVM to develop model weights from a
training corpus. At generation time, a set
of input data, the collection of semanti-
cally organized templates, and the model
weights are used to select optimal tem-
plates. Our system is evaluated with au-
tomatic, non?expert crowdsourced and ex-
pert evaluation metrics. We also introduce
a novel automatic metric ? syntactic vari-
ability ? that represents linguistic variation
as a measure of unique template sequences
across a collection of automatically gener-
ated documents. The metrics for generated
weather and biography texts fall within ac-
ceptable ranges. In sum, we argue that our
statistical approach to NLG reduces the
need for complicated knowledge-based ar-
chitectures and readily adapts to different
domains with reduced development time.
?*Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
1 Introduction
NLG is the process of generating natural-sounding
text from non-linguistic inputs. A typical NLG
system contains three main components: (1) Doc-
ument (Macro) Planning - deciding what content
should be realized in the output and how it should
be structured; (2) Sentence (Micro) planning -
generating a detailed sentence specification and
selecting appropriate referring expressions; and
(3) Surface Realization - generating the final text
after applying morphological modifications based
on syntactic rules (see e.g., Bateman and Zock
(2003), Reiter and Dale (2000) and McKeown
(1985)). However, document planning is arguably
one of the most crucial components of an NLG
system and is responsible for making the texts ex-
press the desired communicative goal in a coher-
ent structure. If the document planning stage fails,
the communicative goal of the generated text will
not be met even if the other two stages are perfect.
While most traditional systems simplify develop-
ment by using a pipelined approach where (1-3)
are executed in a sequence, this can result in er-
rors at one stage propagating to successive stages
(see e.g., Robin and McKeown (1996)). We pro-
pose a hybrid framework that combines (1-3) by
converting data to text in one single process.
Most NLG systems fall into two broad
categories: knowledge-based and statistical.
Knowledge-based systems heavily depend on hav-
ing domain expertise to come up with hand-
crafted rules at each stage of a pipeline. Although
knowledge-based systems can produce high qual-
ity text, they are (1) very expensive to build, in-
volving a lot of discussion with the end users of the
system for the document planning stage alone; (2)
have limited linguistic coverage, as it is time con-
suming to capture linguistic variation; and (3) one
has to start from scratch for each new domain be-
cause the developed components cannot be reused.
1406
Statistical systems, on the other hand, are fairly
inexpensive, more adaptable and rely on having
historical data for the given domain. Coverage is
likely to be high if more historical data is avail-
able. The main disadvantage with statistical sys-
tems is that they are more prone to errors and the
output text may not be coherent as there are less
constraints on the generated text.
Our framework is a hybrid of statistical and
template-based systems. Many knowledge-based
systems use templates to generate text. A tem-
plate structure contains ?gaps? that are filled to
generate the output. The idea is to create a lot
of templates from the historical data and select
the right template based on some constraints. To
the best of our knowledge, this is the first hy-
brid statistical-template-based system that com-
bines all three stages of NLG. Experiments with
different variants of our system (for biography and
weather subject matter domains) demonstrate that
our system generates reasonable texts.
Also, in addition to the standard metrics used
to evaluate NLG systems (e.g., BLEU, NIST, etc.),
we present a unique text evaluation metric called
syntactic variability to measure the linguistic vari-
ation of generated texts. This metric applies to the
document collection level and is based on com-
puting the number of unique template sequences
among all the generated texts. A higher number
indicates the texts are more variable and natural-
sounding whereas a lower number shows they are
more redundant. We argue that this metric is use-
ful for evaluating template-based systems and for
any type of text generation for domains where lin-
guistic variability is favored (e.g., the user is ex-
pected to go through more than one document in
the same session).
The main contributions of this paper are (1) A
statistical NLG system that combines document
and sentence planning and surface realization into
one single process; and (2) A new metric ? syntac-
tic variability ? is proposed to measure the syntac-
tic and morphological variability of the generated
texts. We believe this is the first work to propose
an automatic metric to measure linguistic variabil-
ity of generated texts in NLG.
Section 2 provides an overview of related work
on NLG. We present our main system in Section 3.
The system is evaluated and discussed in Section
4. Finally, we conclude in Section 5 and point out
future directions of research.
2 Background
Typically, knowledge-based NLG systems are im-
plemented by rules and, as mentioned above, have
a pipelined architecture for the document and
sentence planning stages and surface realization
(Hovy, 1993; Moore and Paris, 1993). However,
document planning is arguably the most impor-
tant task (Sripada et al, 2001). It follows that ap-
proaches to document planning are rule-based as
well and, concomitantly, are usually domain spe-
cific. For example, Bouayad-Agha, et al (2011)
proposed document planning based on an ontol-
ogy knowledge base to generate football sum-
maries. For rule?based systems, rules exist for
selecting content to grammatical choices to post-
processing (e.g., pronoun generation). These rules
are often tailored to a given system, with input
from multiple experts; consequently, there is a
high associated development cost (e.g., 12 person
months for the SUMTIME-METEO system (Belz,
2007)).
Statistical approaches can reduce extensive de-
velopment time by relying on corpus data to
?learn? rules for one or more components of an
NLG system (Langkilde and Knight, 1998). For
example, Duboue and McKeown (2003) proposed
a statistical approach to extract content selection
rules for biography descriptions. Further, statisti-
cal approaches should be more adaptable to differ-
ent domains than their rule-based equivalents (An-
geli et al, 2012). For example, Barzilay and Lap-
ata (2005) formulated content selection as a clas-
sification task to produce football summaries and
Kelly et al (2009) extended Barzilay and Lapata?s
approach for generating match reports for cricket.
The present work builds on Howald et al
(2013) where, in a given corpus, a combination of
domain specific named entity tagging and cluster-
ing sentences (based on semantic predicates) were
used to generate templates. However, while the
system consolidated both sentence planning and
surface realization with this approach (described
in more detail in Section 3), the document plan
was given via the input data and sequencing infor-
mation was present in training documents. For the
present research, we introduce a similar method
that leverages the distributions of document?level
features in the training corpus to incorporate a
statistical document planning component. Con-
sequently, we are able to create a streamlined
statistical NLG architecture that balances natural
1407
human?like variability with appropriate and accu-
rate information.
3 Methodology
In order to generate text for a given domain our
system runs input data through a statistical ranking
model to select a sequence of templates that best
fit the input data (E). In order to build the rank-
ing model, our system takes historical data (cor-
pus) for the domain through four components: (A)
preprocessing; (B) ?conceptual unit? creation; (C)
collecting statistics; and (D) ranking model build-
ing (summarized in Figure 1). In this section, we
describe each component in detail.
Figure 1: System Architecture.
3.1 Preprocessing
The first component processes the given corpus to
extract templates. We assume that each document
in the corpus is classified to a specific domain.
Preprocessing involves uncovering the underlying
semantic structure of the corpus and using this as
a foundation for template creation (Lu et al, 2009;
Lu and Ng, 2011; Konstas and Lapata, 2012).
We first split each document in the corpus into
sentences and create a shallow Discourse Repre-
sentation Structure (following Discourse Repre-
sentation Theory (Kamp and Reyle, 1993)) of each
sentence. The DRS consists of semantic predi-
cates and named entity tags. We use Boxer se-
mantic analyzer (Bos, 2008) to extract semantic
predicates such as EVENT or DATE. In parallel,
domain specific named entity tags are identified
and, in conjunction with the semantic predicates,
are used to create templates. We developed the
named-entity tagger for the weather domain our-
selves. To tag entities in the biography domain,
we used OpenCalais (www.opencalais.com). For
example, in the biography in (1), the conceptual
meaning (semantic predicates and domain-specific
entities) of sentences (a-b) are represented in (c-d).
The corresponding templates are showing in (e-f).
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing Di-
rector of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | TITLE | PERSON | COMPANY | DATE
d. HOLDS | DEGREE | SUBJECT | INSTITUTION| EVENT
Templates
e. [person] has been serving as [title] of the [company]
since [date].
f. [person] holds a [degree] in [subject] from [institution]
and a [degree] from [institution].
The outputs of the preprocessing stage are the tem-
plate bank and predicate information for each tem-
plate in the corpus.1
3.2 Creating Conceptual Units
The next step is to create conceptual units for the
corpus by clustering templates. This is a semi-
automatic process where we use the predicate in-
formation for each template to compute similar-
ity between templates. We use k-means clustering
with k (equivalent to the number of semantic con-
cepts in the domain) set to an arbitrarily high value
(100) to over-generate (using the WEKA toolkit
(Witten and Frank, 2005)). This allows for easier
manual verification of the generated clusters and
we merge them if necessary. We assign a unique
identifier called a CuId (Conceptual Unit Identi-
fier) to each cluster, which represents a ?concep-
tual unit?. We associate each template in the cor-
pus to a corresponding CuId. For example, in (2),
using the templates in (1e-f), the identified named
entities are assigned to a clustered CuId (2a-b).
(2) Conceptual Units
a. {CuId : 000} ? [person] has been serving as [title] of the
[company] since [date].
b. {CuId : 001} ? [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of conceptual
units with corresponding template collections (see
Howald et al (2013) for a further explanation of
Sections 3.1-3.2).
1A similar approach to the clustering of semantic content
is found in Duboue and McKeown (2003), where text with
stopwords removed were used as semantic input. Boxer pro-
vides a similar representation with the addition of domain
general tags. However, to contrast our work from Duboue
and McKeown, which focused on content selection, we are
focused on learning templates from the semantic representa-
tions for the complete generation system (covering content
selection, aggregation, sentence and document planning).
1408
3.3 Collecting Corpus Statistics
After identifying the different conceptual units and
the template bank, we collect a number of statistics
from the corpus:
? Frequency distribution of templates overall and per po-
sition
? Frequency distribution of CuIds overall and per posi-
tion
? Average number of entity tags by CuId as well as the
entity distribution by CuId
? Average number of entity tags by position as well as
the entity distribution by position
? Average number of words per CuId.
? Average number of words per CuId and position com-
bination.
? Average number of words per position
? Frequency distribution of the main verbs by position
? Frequency distribution of CuId sequences (bigrams and
trigrams only) overall and per position
? Frequency distribution of template sequences (bigrams
and trigrams only) overall and per position
? Frequency distribution of entity tag sequences overall
and per position
? The average, minimum, maximum number of CuIds
across all documents
As discussed in the next section, these statistics
are turned into features used for building a ranking
model in the next component.
3.4 Building a ranking model
The core component of our system is a statistical
model that ranks a set of templates for a given
position (sentence 1, sentence 2, ..., sentence n)
based on the input data. The input data in our
tasks was extracted from a training document; this
serves as a temporary surrogate to a database. The
task is to learn the ranks of all the templates from
all CuIds at each position.
To generate the training data, we first filter the
templates that have named entity tags not specified
in the input data. This will make sure the gener-
ated text does not have any unfilled entity tags. We
then rank templates according to the Levenshtein
edit distance (Levenshtein, 1966) from the tem-
plate corresponding to the current sentence in the
training document (using the top 10 ranked tem-
plates in training for ease of processing effort). We
experimented with other ranking schemes such as
entity-based similarity (similarity between entity
sequences in the templates) and a combination of
edit-distance based and entity-based similarities.
We obtained better results with edit distance. For
each template, we generate the following features
to build the ranking model. Most of the features
are based on the corpus statistics mentioned above.
? CuId given position: This is a binary feature where
the current CuId is either the same as the most frequent
CuId for the position (1) or not (0).
? Overlap of named entities: Number of common enti-
ties between current CuId and most likely CuId for the
position
? Prior template: Probability of the sequence of tem-
plates selected at the previous position and the current
template (iterated for the last three positions).
? Prior CuId: Probability of the sequence of the CuId
selected at the previous position and the current CuId
(iterated for the last three positions).
? Difference in number of words: Absolute difference
between number of words for current template and av-
erage number of words for the CuId
? Difference in number of words given position: Ab-
solute difference between number of words for cur-
rent template and average number of words for CuId
at given position
? Percentage of unused data: This feature represents
the portion of the unused input so far.
? Difference in number of named entities: Absolute
difference between the number of named entities in the
current template and the average number of named en-
tities for the current position
? Most frequent verb for the position: Binary valued
feature where the main verb of the template belongs to
the most frequent verb group given the position is either
the same (1) or not (0).
? Average number of words used: Ratio of number of
words in the generated text so far to the average number
of words.
? Average number of entities: Ratio of number of
named entities in the generated text so far to the av-
erage number of named entities.
? Most likely CuId given position and previous CuId:
Binary feature indicating if the current CuId is most
likely given the position and the previous CuId.
? Similarity between the most likely template in CuId
and current template: Edit distance between the cur-
rent template and the most likely template for the cur-
rent CuId.
? Similarity between the most likely template in CuId
given position and current template: Edit distance
between the current template and the most likely tem-
plate for the current CuId at the current position.
We used a linear kernel for a ranking SVM
(Joachims, 2002) (cost set to total queries) to learn
the weights associated with each feature for the
different domains.
3.5 Generation
At generation time, our system has a set of in-
put data, a semantically organized template bank
(collection of templates organized by CuId) and a
model from training on the documents for a given
domain. We first filter out those templates that
contain a named entity tag not present in the in-
put data. Then, we compute a score for each of the
remaining templates from the feature values and
the feature weights from the model. The template
with the highest overall score is selected and filled
with matching entity tags from the input data and
1409
appended to the generated text.
Before generating the next sentence, we track
those entities used in the initial sentence gener-
ation and decide to either remove those entities
from the input data or keep the entity for one or
more additional sentence generations. For exam-
ple, in the biography discourses, the name of the
person may occur only once in the input data, but
it may be useful for creating good texts to have
that person?s name available for subsequent gen-
erations. To illustrate in (3), if we remove James
Smithton from the input data after the initial gen-
eration, an irrelevant sentence (d) is generated as
the input data will only have one company after
the removal of James Smithton and the model will
only select a template with one company. If we
keep James Smithton, then the generations in (a-b)
are more cohesive.
(3) Use more than once
a. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
b. Previously, Mr. Smithton was CFO of the Keyes
Development Group.
Use once and remove
c. Mr. James Smithton was appointed CFO at Fordway
Internation in April.
d. Keyes Development Group is a venture capital firm.
Deciding on what type of entities and how to
remove them is different for each domain. For ex-
ample, some entities are very unique to a text and
should not be made available for subsequent gen-
erations as doing so would lead to unwanted re-
dundancies (e.g., mentioning the name of current
company in a biography discourse more than once
as in (3)) and some entities are general and should
be retained. Our system possesses the ability to
monitor the data usage from historical data and we
can set parameters (based on the distribution of en-
tities) on the usage to ensure coherent generations
for a given domain.
Once the input data has been modified (i.e., an
entity have been removed, replaced or retained),
it serves as the new input data for the next sen-
tence generation. This process repeats until reach-
ing the minimum number of sentences for the do-
main (determined from the training corpus statis-
tic) and then continues until all of the remaining
input data is consumed (and not to exceed the pre-
determined maximum number of sentences, also
determined from the training corpus statistic).
4 Evaluation and Discussion
In this section, we first discuss the corpus data
used to train and generate texts. Then, the re-
sults of both automatic and human evaluations of
our system?s generations against the original and
baseline texts are considered as a means of de-
termining performance. For all experiments re-
ported in this section, the baseline system selects
the most frequent conceptual unit at the given po-
sition, chooses the most likely template for the
conceptual unit, and fills the template with input
data. The above process is repeated until the num-
ber of sentences is less than or equal to the average
number of sentences for the given domain.
4.1 Data
We ran our system on two different domains: cor-
porate officer and director biographies and off-
shore oil rig weather reports from the SUMTIME-
METEO corpus ((Reiter et al, 2005)). The biogra-
phy domain includes 1150 texts ranging from 3-17
sentences and the weather domain includes 1045
weather reports ranging from 1-6 sentences.2 We
used a training-test(generation) split of 70/30.
(4) provides generation comparisons for the
system ( DocSys), baseline ( DocBase) and orig-
inal ( DocOrig) randomly selected text snippets
from each domain. The variability of the gener-
ated texts ranges from a close similarity to slightly
shorter - not an uncommon (Belz and Reiter,
2006), but not necessarily detrimental, observation
for NLG systems (van Deemter et al, 2005).
(4) Weather DocOrig
a. Another weak cold front will move ne to Cornwall by later
Friday.
Weather DocSys
b. Another weak cold front will move ne to Cornwall during
Friday.
Weather DocBase
c. Another weak cold front from ne through the Cornwall will
remain slow moving.
Bio DocOrig
d. He previously served as Director of Sales Planning and
Manager of Loan Center.
Bio DocSys
e. He previously served as Director of Sales in Loan Center
of the Company.
Bio DocBase
2The SUMTIME-METEO project is a common bench
mark in NLG. However, we provide no comparison between
our system and SUMTIME-METEO as our system utilized the
generated forecasts from SUMTIME-METEO?s system as the
historical data. We cannot compare with other statistical gen-
eration systems like (Belz, 2007) as they only focussed on the
part of the forecasts the predicts wind characteristics whereas
our system generates the complete forecasts.
1410
f. He previously served as Director of Sales of the Company.
The DocSys and DocBase generations are
largely grammatical and coherent on the surface
with some variance, but there are graded semantic
variations (e.g., Director of Sales Planning vs. Di-
rector of Sales (4g-h) and move ne to Cornwall vs.
from ne through the Cornwall). Both automatic
and human evaluations are required in NLG to de-
termine the impact of these variances on the under-
standability of the texts in general (non-experts)
and as they are representative of particular subject
matter domains (experts). The following sections
discuss the evaluation results.
4.2 Automatic Metrics
We used BLEU?4 (Papineni et al, 2002), METEOR
(v.1.3) (Denkowski and Lavie, 2011) to evaluate
the texts at document level. Both BLEU?4 and
METEOR originate from machine translation re-
search. BLEU?4 measures the degree of 4-gram
overlap between documents. METEOR uses a un-
igram weighted f?score less a penalty based on
chunking dissimilarity. These metrics only eval-
uate the text on a document level but fail to iden-
tify ?syntactic repetitiveness? across documents in
a document collection. This is an important char-
acteristic of a document collection to avoid banal-
ity. To address this issue, we propose a new auto-
matic metric called syntactic variability. In order
to compute this metric, each document should be
represented as a sequence of templates by associ-
ating each sentence in the document with a tem-
plate in the template bank. Syntactic variability is
defined as the percentage of unique template se-
quences across all generated documents. It ranges
between 0 and 1. A higher value indicates that
more documents in the collection are linguistically
different from each other and a value closer to zero
shows that most of documents have the similar
language despite different input data.3
As indicated in Figure 2, the BLEU-4 scores are
low for all DocSys and DocBase generations (as
compared to DocOrig) for each domain. How-
ever, the METEOR scores, while low overall (rang-
ing from .201-.437) are noticeably increased over
BLEU-4 (which ranges from .199-.320).
Given the nature of each metric, the results in-
dicate that the generated and baseline texts have
3Of course, syntactic and semantic repetitiveness could be
captured by syntactic variability, but only if this is the nature
of the underlying historical data - financial texts tend to be
fairly repetitive.
Figure 2: Automatic Evaluations.
very different surface realizations compared to the
originals (low BLEU-4), but are still capturing the
content of the originals (higher METEOR). Both
BLEU?4 and METEOR measure the similarity of
the generated text to the original text, but fail to
penalize repetitiveness across texts, which is ad-
dressed by the syntactic variability metric. There
is no statistically significant difference between
DocSys and DocBase generations for METEOR
and BLEU?4.4 However, there is a statistically
significant difference in the syntactic variability
metric for both domains (weather - ?2=137.16,
d.f.=1, p<.0001; biography - ?2=96.641, d.f.=1,
p<.0001) - the variability of the DocSys gener-
ations is greater than the DocBase generations,
which shows that texts generated by our system
are more variable than the baseline texts.
The use of automatic metrics is a common eval-
uation method in NLG, but they must be recon-
ciled against non?expert and expert level evalua-
tions.
4.3 Non-Expert Human Evaluations
Two sets of crowdsourced human evaluation tasks
(run on CrowdFlower) were constructed to com-
pare against the automatic metrics: (1) an under-
standability evaluation of the entire text on a three-
point scale: Fluent = no grammatical or infor-
mative barriers; Understandable = some gram-
matical or informative barriers; Disfluent = sig-
nificant grammatical or informative barriers; and
(2) a sentence?level preference between sentence
pairs (e.g., ?Do you prefer Sentence A (from Do-
cOrig) or the corresponding Sentence B (from
DocBase/DocSys)?).
4BLEU?4: weather - ?2=1.418, d.f.=1, p=.230; biography
- ?2=0.311, d.f.=1, p=.354. METEOR: weather - ?2=1.016,
d.f.=1, p=.313; biography - ?2=0.851, d.f.=1, p=.354.
1411
Over 100 native English speakers contributed,
each one restricted to providing no more than
50 responses and only after they successfully an-
swered 4 ?gold data? questions correctly. We also
omitted those evaluators with a disproportionately
high response rate. No other data was collected on
the contributors (although geographic data (coun-
try, region, city) and IP addresses were available).
For the sentence?level preference task, the pair or-
derings were randomized to prevent click bias.
For the text?understandability task, 40 docu-
ments were chosen at random from the DocOrig
test set alng with the corresponding 40 Doc-
Sys and DocBase generations (240 documents to-
tal/120 for each domain). 8 judgments per doc-
ument were solicited from the crowd (1920 to-
tal judgments, 69.51 average agreement) and are
summarized in Figures 3 and 4 (biography and
weather respectively).
If the system is performing well and the rank-
ing model is actually contributing to increased
performance, the accepted trend should be that
the DocOrig texts are more fluent and preferred
compared to both the DocSys and DocBase sys-
tems. However, the differences between DocOrig
and DocSys will not be significant, the differences
between DocOrig and DocBase and DocSys and
DocBase will be significantly different.
Figure 3: Biography Text Evaluations.
Focusing on fluency ratings, it is expected that
the DocOrig generations will have the highest flu-
ency (as they are human generated). Further, if the
DocSys is performing well, it is expected that the
fluency rating will be less than the DocOrig and
higher than DocBase. Figure 3, which shows the
biography text evaluations, demonstrates this ac-
ceptable distribution of performances.
For the weather discourses, as evident from
Figure 4, the acceptable trend holds between the
DocSys and DocBase generations, and the Doc-
Sys generation fluency is actually slightly higher
than DocOrig. This is possibly because the Do-
cOrig texts are from a particular subject matter -
weather forecasts for offshore oil rigs in the U.K.
- which may be difficult for people in general to
understand. Nonetheless, the demonstrated trend
is favorable to our system.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=.347,
d.f.=1, p=.555; DocOrig vs. DocBase - ?2=.090,
d.f.=1, p=.764; DocSys vs. DocBase - ?2=.790,
d.f.=1, p=.373). While this is a good result for
comparing DocOrig and DocSys generations, it is
not for the other pairs. However, numerically, the
trend is in the right direction despite not being
able to demonstrate significance. For biography,
the trend fits nicely both numerically and in terms
of statistical significance (DocOrig vs. DocSys -
?2=5.094, d.f.=1, p=.024; DocOrig vs. DocBase -
?2=35.171, d.f.=1, p<.0001; DocSys vs. DocBase
- ?2=14.000, d.f.=1, p<.0001).
Figure 4: Weather Text Evaluations.
For the sentence preference task, equivalent
sentences across the 120 documents were chosen
at random (80 sentences from biography and 74
sentences from weather). 8 judgments per com-
parison were solicited from the crowd (3758 to-
tal judgments, 75.87 average agreement) and are
summarized in Figures 5 and 6 (biography and
weather, respectively).
Similar to the text?understandability task, an
acceptable performance pattern should include the
DocOrig texts being preferred to both DocSys and
DocBase generations and the DocSys generation
preferred to the DocBase. The closer the Doc-
Sys generation is to the DocOrig, the better Doc-
Sys is performing. The biography domain illus-
1412
Figure 5: Biography Sentence Evaluations.
trates this scenario (Figure 5) where the results are
similar to the text-understandability experiments.
In contrast, for weather domain, sentences from
DocBase system were preferred to our system?s
(Figure 6). We looked at the cases where the
preferences were in favor of DocBase. It appears
that because of high syntactic variability, our sys-
tem can produce quite complex sentences where as
the non-experts seem to prefer shorter and simpler
sentences because of the complexity of the text.
In terms of significance, there are no statisti-
cally significant differences between the systems
for weather (DocOrig vs. DocSys - ?2=6.48,
d.f.=1, p=.011; DocOrig vs. DocBase - ?2=.720,
d.f.=1, p=.396; DocSys vs. DocBase - ?2=.720,
d.f.=1, p=.396). The trend is different compared to
the fluency metric above in that the DocBase sys-
tem is outperforming the DocOrig generations to
an almost statistically significant difference - the
remaining comparisons follow the trend. We be-
lieve that this is for similar reasons stated above
- i.e., the generation may be a more digestible
version of a technical document. More problem-
atic is the results of the biography evaluations.
Here there is a statistically significant difference
between the DocSys and DocOrig and no sta-
tistically significant difference between the Doc-
Sys and DocBase generations (DocOrig vs. Doc-
Sys - ?2=76.880, d.f.=1, p<.0001; DocOrig vs.
DocBase - ?2=38.720, d.f.=1, p<.0001; DocSys
vs. DocBase - ?2=.720, d.f.=1, p=.396). Again,
this distribution of preferences is numerically sim-
ilar to the trend we would like to see, but the sta-
tistical significance indicates that there is some
ground to make up. Expert evaluations are po-
tentially informative for identifying specific short-
comings and how best to address them.
Figure 6: Weather Sentence Evaluations.
4.4 Expert Human Evaluations
We performed expert evaluations for the biogra-
phy domain only as we do not have access to
weather experts. The four biography reviewers are
journalists who write short biographies for news
archives.
For the biography domain, evaluations of the
texts were largely similar to the evaluations of
the non-expert crowd (76.22 average agreement
for the sentence?preference task and 72.95 for the
text?understandability task). For example, the dis-
fluent ratings were highest for the DocBase gen-
erations and lowest for the DocOrig generations.
Also, the fluent ratings were highest for the Do-
cOrig generations, and while the combined flu-
ent and understandable are higher for DocSys as
compared to DocBase, the DocBase generations
had a 10% higher fluent score (58.22%) as com-
pared to the DocSys fluent score (47.97%). Based
on notes from the reviewers, the succinctness of
the the DocBase generations are preferred in some
ways as they are in keeping with certain editorial
standards. This is further reflected in the sentence
preferences being 70% in favor of the DocBase
generations as compared to the DocSys genera-
tions (all other sentence comparisons were consis-
tent with the non-expert crowd).
These expert evaluations provide much needed
clarity to the NLG process. Overall, our system
is generating clearly acceptable texts. Further,
there are enough parameters inherent in the system
to tune to different domain expectations. This is
an encouraging result considering that no experts
were involved in the development of the system -
a key contrast to many other existing (especially
rule-based) NLG systems.
1413
5 Conclusions and Future Work
We have presented a hybrid (template-based and
statistical), single?staged NLG system that gen-
erates natural sounding texts and is domain?
adaptable. Our experiments with both ex-
perts and non?experts demonstrate that the
system-generated texts are comparable to human?
authored texts. The development time to adapt
our system to new domains is small compared to
other NLG systems; around a week to adapt the
system to weather and biography domains. Most
of the development time was spent on creating the
domain-specific entity taggers for the weather do-
main. The development time would be reduced to
hours if the historical data for a domain is readily
available with the corresponding input data.
The main limitation of our system is that it re-
quires significant historical data. Our system does
consolidate many traditional components (macro-
and micro-planning, lexical choice and aggrega-
tion),5 but the system cannot be applied to the do-
mains with no historical data. The quality and the
linguistic variability of the generated text is di-
rectly proportional to the amount of historical data
available.
We also presented a new automatic metric to
evaluate generated texts at document collection
level to identify boilerplate texts. This metric
computes ?syntactic repetitiveness? by counting
the number of unique template sequences across
the given document collection.
Future work will focus on extending our frame-
work by adding additional features to the model
that could improve the quality of the generated
text. For example, most NLG pipelines have a
separate component responsible for referring ex-
pression generation (Krahmer and van Deemter,
2012). While we address the associated concern
of data consumption in Section 3.5, we currently
do not have any features that would handle refer-
ring expression generation. We believe that this
is possible by identifying referring expressions in
templates and adding features to the model to give
higher scores to the templates having relevant re-
ferring expressions. We also would like to inves-
tigate using all the top-scored templates instead
of the highest-scoring template. This would help
achieve better syntactic-variability scores by pro-
ducing more natural-sounding texts.
5Lexical choice and aggregation are ?handled? insofar as
their existence in the historical data.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragment, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2011. Content selection from an ontology-
based knowledge base for the generation of foot-
ball summaries. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), pages 72?81.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
1414
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical acquisition of content selection rules for
natural language generation. In Proceedings of the
2003 Conference on Empirical Methods for Natural
Language Processing (EMNLP 2003), pages 2003?
2007.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in
statistical nlg. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Colin Kelly, Ann Copestake, and Nikiforos Karama-
nis. 2009. Investigating content selection for lan-
guage generation using machine learning. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG), pages 130?137.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Emiel Krahmer and Kees van Deemter. 2012. Com-
putational generation of referring expression: A sur-
vey. Computational Linguistics, 38(1):173?218.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-
ural language generation with tree conditional ran-
dom fields. In Proceedings of the 2009 Conference
on Empirical Methods for Natural Language Pro-
cessing (EMNLP 2009), pages 400?409.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Johanna D. Moore and Cecile L. Paris. 1993. Planning
text for advisory dialogues: Capturing intentional
and rhetorical information. Computational Linguis-
tics, 19(4):651?694.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Exmpiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and
Jin Yu. 2001. A two-stage model for content
determination. In Proceedings of the 8th Euro-
pean Workshop on Natural Language Generation
(ENLG), pages 1?8.
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
1415
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 44?48,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Use of Granularity in Rhetorical Relation Prediction
Blake Stephen Howald and Martha Abramson
Ultralingua, Inc.
1313 SE Fifth Street, Suite 108
Minneapolis, MN 55414
{howald, abramson}@ultralingua.com
Abstract
We present the results of several machine
learning tasks designed to predict rhetori-
cal relations that hold between clauses in
discourse. We demonstrate that organizing
rhetorical relations into different granularity
categories (based on relative degree of detail)
increases average prediction accuracy from
58% to 70%. Accuracy further increases to
80% with the inclusion of clause types. These
results, which are competitive with existing
systems, hold across several modes of written
discourse and suggest that features of informa-
tion structure are an important consideration
in the machine learnability of discourse.
1 Introduction
The rhetorical relations that hold between clauses
in discourse index temporal and event information
and contribute to a discourse?s pragmatic coherence
(Hobbs, 1985). For example, in (1) the NARRATION
relation holds between (1a) and (1b) as (1b) tempo-
rally follows (1a) at event time.
(1) a. Pascale closed the toy chest.
b. She walked to the gate.
c. The gate was locked securely.
d. So she couldn?t get into the kitchen.
The ELABORATION relation, describing the sur-
rounding state of affairs, holds between (1b) and
(1c). (1c) is temporally inclusive (subordinated)
with (1b) and there is no temporal progression at
event time. The RESULT relation holds between (1b-
c) and (1d). (1d) follows (1b) and its subordinated
ELABORATION relation (1c) at event time.
Additional pragmatic information is encoded in
these relations in terms of granularity. Granularity
refers to the relative increases or decreases in the
level of described detail. For example, moving from
(1b) to (1c), we learn more information about the
gate via the ELABORATION relation. Also, moving
from (1b-c) to (1d) there is a consolidation of infor-
mation associated with the RESULT relation.
Through several supervised machine learning
tasks, we investigate the degree to which granularity
(as well as additional elements of discourse struc-
ture (e.g. tense, aspect, event)) serves as a viable
organization and predictor of rhetorical relations in
a range of written discourses. This paper is orga-
nized as follows. Section 2 reviews prior research
on rhetorical relations, discourse structure, granular-
ity and prediction. Section 3 discusses the analyzed
data, the selection and annotation of features, and
the construction of several machine learning tasks.
Section 4 provides the results which are then dis-
cussed in Section 5.
2 Background
Rhetorical relation prediction has received consid-
erable attention and has been shown to be useful
for text summarization (Marcu, 1998). Prediction
tasks rely on a number of features (discourse con-
nectives, part of speech, etc.) (Marcu and Echihabi,
2002; Lapata and Lascarides, 2004). A wide range
of accuracies are also reported - 33.96% (Marcu and
Echihabi, 2002) to 70.70% (Lapata and Lascarides,
2004) for all rhetorical relations and, for individ-
ual relations, CONTRAST (43.64%) and CONTINU-
ATION (83.35%) (Sporleder and Lascarides, 2005).
44
We seek to predict the inventory of rhetorical
relations defined in Segmented Discourse Repre-
sentation Theory (?SDRT?) (Asher and Lascarides,
2003). In addition to the relations illustrated in
(1), we consider: BACKGROUND: It was Christ-
mas. Pascale got a new toy.; EXPLANATION: The
aardvark was dirty. It fell into a puddle.; CONSE-
QUENCE: If the aardvark fell in the puddle, then it
got dirty.; ALTERNATION: Pascale got an aardvark
or a stuffed bunny.; and CONTINUATION: Pascale
got an aardvark. Grimsby got a rawhide.
Discourses were selected based on Smith (2003)
who defines five primary discourse modes by: (1)
the situations (events and states) they describe; (2)
the overarching temporality (tense, aspect); and (3)
the type of text progression (temporal - text and
event time progression are similar; atemporal - text
and event time progression are not similar). These
contrastive elements inform the features selected
for the machine learning tasks discussed in Section
3.2. The five modes, narratives, reports (news ar-
ticles), description (recipes), information (scientific
essays), and argument (editorials) were selected to
ensure a balanced range of theoretically supported
discourse types.
2.1 Granularity of Information
Granularity in discourse refers to the relative degree
of detail. The higher the level of detail, the more
informative the discourse is. We assume that there
will be some pragmatic constraints on the informa-
tiveness of a discourse (e.g., consistent with Grice?s
(1975) Maxim of Quantity). For our purposes, we
rely specifically on granularity as defined in Mulkar-
Mehta et al (2011) (?MM?) who characterize gran-
ularity in terms of entities and events.
To illustrate, consider (2) where the rhetorical
structure indicates that (2b) is an ELABORATION of
(2a), the NARRATION relation holds between (2b)
and (2c) and (2c) and (2d), and the RESULT relation
between (2d) and (2e).
(2) a. The Pittsburgh Steelers needed to win.
b. Batch took the first snap.
c. Then he threw the ball into the endzone.
d. Ward caught the ball.
e. A touchdown was scored.
Entities and events can stand in part-whole and
causality relationships with entities and events in
subsequent clauses. A positive granularity shift in-
dicates movement from whole to part (more detail)
- e.g., Batch (2b) is a part of the whole Pittsburgh
Steelers (2a). A negative granularity shift indicates
movement from part to whole (less detail), or if
one event causes a subsequent event (if an event is
caused by a subsequent event, this is a positive shift)
- e.g., Ward?s catching of the ball (2d) caused the
scoring of the touchdown (2e). Maintained granular-
ities (not considered by MM) are illustrated in (2b-c)
and (2c-d). Clauses (2b) through (2d) are temporally
linked events, but there is no part-whole shift in, nor
a causal relationship between, the entities or events;
the granularity remains the same.
We maintain that there is a close relationship be-
tween rhetorical relations and granularity. Con-
sequently, rhetorical relations can be organized as
follows: positive: BACKGROUND, ELABORATION,
EXPLANATION; negative: CONSEQUENCE, RE-
SULT; and maintained: ALTERNATION, CONTINU-
ATION, NARRATION. The machine learning tasks
discussed in the remainder of the paper consider this
information in the prediction of rhetorical relations.
3 Data and Methods
Five written discourses of similar sentence length
were selected from each mode for 25 total dis-
courses. The discourses were segmented by inde-
pendent or dependent (subordinate) clauses, if the
clauses contained discourse markers (but, however),
and if the clauses were embedded in the sentence
provided in the orginal written discourse (e.g., John,
who is the director of NASA, gave a speech on Fri-
day). The total number of clauses is 1090, averaging
43.6 clauses per discourse (?=7.2).
3.1 Feature Annotation
For prediction, we use a feature set distilled from
Smith?s classification of discourses: TENSE and
ASPECT; EVENT (from the TimeML annotation
scheme (Pustejovksy, et al, 2005), Aspectual, Oc-
curence, States, etc.); SEQUENCE information as
the clause position normalized to the unit interval;
and discourse MODE. We also include CLAUSE
type - independent (IC) or dependent clauses (DC)
with the inclusion of a discourse marker (M) or not,
45
Table 1: Distribution of Relations by Granularity Type.
Relation Number (Avg.)
Positive 515 (47%)
BACKGROUND 315 (61%)
ELABORATION 161 (31%)
EXPLANATION 39 (7%)
Negative 59 (5%)
CONSEQUENCE 16 (26%)
RESULT 43 (71%)
Maintenance 490 (44%)
ALTERNATION 76 (14%)
CONTINUATION 30 (6%)
NARRATION 384 (78%)
embedded (EM) or not - and GRANULARITY shift
categories which are an organization of the SDRT
rhetorical relations (Asher and Lascarides, 2003),
summarized in Table 1.
All 25 discourses were annotated by one of the au-
thors using only a reference sheet. The other author
independently coded 80% of the data (20 discourses,
four from each mode). Average agreement and Co-
hen?s Kappa (Cohen, 1960) statistics were computed
and are within acceptable ranges: TENSE (99.65
/ .9945), ASPECT (99.30 / .9937), SDRT (77.42 /
.6850), and EVENT (75.88 / .6362).
These results are consistent with previously re-
ported annotations for rhetorical relations (Sporleder
and Lascarides, 2005; Howald and Katz, 2011),
event verbs and durations, tense and aspect (Puscasu
and Mititelu, 2008; Wiebe et al, 1997). Positive,
negative and maintained granularities were not an-
notated, but MM report a Kappa between .8500 and
1. The distribution of these granularities, based on
the organization of the annotated rhetorical relations
is presented in Table 1.
3.2 Machine Learning
Three supervised machine learning tasks were con-
structed to predict SDRT relations. The first task
(Uncollapsed) created a 8-way classifier to predict
the SDRT relations based on the feature set, omit-
ting the GRANULARITY feature. The second task
(Collapsed) created a 3-way classifier to predict
the GRANULARITY categories (the SDRT feature
was omitted). The third task (Combined) included
Table 2: Relation Prediction - Combined Modes.
Feature J48 K* NB MCB
Uncollapsed 58.99 55.41 56.69 35
Collapsed 69.90 70.18 69.81 41
Combined 78.62 71.92 80.00 35 (70)
the GRANULARITY feature back into the Uncol-
lapsed 8-way classifier. We utilized the WEKA
toolkit (Witten and Frank, 2005) and treated each
clause as a vector of information (SDRT, EVENT,
TENSE, ASPECT, SEQUENCE, CLAUSE, MODE,
GRANULARITY), illustrated in (3)1:
(3) a. The Pittsburgh Steelers needed to win.
START, State, Pa., N, .200, IC, NA, start
b. Batch took the first snap.
ELAB., Occ., Pa., N, .400, IC, NA, pos.
c. Then he threw the ball into the endzone.
NAR., Asp., Pa., N, .600, IC-M, NA, main.
d. Ward caught the ball.
NAR., Occ., Pa., N, .800, IC, NA, main.
e. A touchdown was scored.
RESULT, Occ., Pa., Perf., 1.00, IC, NA, neg.
We report results from the Na??ve Bayes (NB), J48
(C4.5 decision tree (Quinlan, 1993)) and K* (Cleary
and Trigg, 1995) classifiers, run at 10-fold cross-
validation.
4 Results
Table 2 indicates that the best average accuracy for
the Uncollapsed task is 58.99 (J48). The accu-
racy increases to 70.18 (K*) for the Collapsed task.
The accuracy increases further to 80.00 (NB) for the
Combined task. All accuracies are statistically sig-
nificant over majority class baselines (?MCB?): Un-
collapsed (MCB = 35) - ?2 = 15.11, d.f. = 0, p ?
.001; Collapsed (MCB = 41) - ?2 = 20.51, d.f. =
0, p ? .001; and Combined (treating the best Col-
lapsed accuracy as the new baseline (MCB = 70)) -
?2 = 1.43, d.f. = 0, p ? .001.
As shown in Table 3, based on the NB 8-way
Combined classifier, the prediction accuracies of
1Note that what is being predicted is the rhetorical relation,
or associated granularity, with the second clause in a clause pair.
Tasks were performed where clause information was paired, but
this did not translate into improved accuracies.
46
Table 3: Individual Relation Prediction Accuracies (%).
Relation A I D N R T
NAR. 73 55 100 100 94 96
RES. 75 88 85 100 100 93
BACK. 93 92 96 87 94 92
ELAB. 57 41 69 21 48 69
CONSEQ. 20 0 0 0 0 37
ALTER. 50 42 0 0 43 27
CONTIN. 8 0 0 0 0 23
EXPLAN. 0 20 0 9 0 2
Total 68 72 92 74 74 80
the individual modes are no more than 12 percent-
age points off of the average (80.00). Accura-
cies range from 68% A(rgument) (?=-12) to 92%
D(escription) (?=+12) with N(arrative), R(eport),
and I(nformation) being closest to average (?=-6-
8). For individual relation predictions, NARRATION,
RESULT and BACKGROUND have the highest total
accuracies followed by ELABORATION and CON-
TRAST. Performing less well is CONSEQUENCE,
ALTERNATION and CONTINUATION with EXPLA-
NATION performing the worst. All accuracies are
statistically significant above baseline (?2 = 341.89,
d.f. = 7, p ? .001).
5 Discussion and Conclusion
Using the Collapsed performance as a baseline for
the Combined classifier, we discuss the features
contributing to the 10 percentage point increase as
well as the optimal (minimal) set of features for pre-
diction. The best accuracies for the Combined ex-
periment only require CLAUSE and GRANULAR-
ITY information; achieving 79.08% (NB - 44 above
MCB, f-score=.750). Both CLAUSE and GRANU-
LARITY are necessary. Relying only on CLAUSE
achieves a 48.25% accuracy (J48) and relying only
on GRANULARITY achieves 70.36% for all clas-
sifiers, but this higher accuracy is an artifact of the
organization as evidenced by the f-score (.585).
The relationship between CLAUSE and the
rhetorical relations is straightforward. For example,
the CONSEQUENCE relation is often an ?intersenten-
tial? relation (if the aardvark fell in the puddle, then
it got dirty), each of the 16 CONSEQUENCE relations
are embedded. Similarly, 93% of all ELABORATION
relations, which are temporally subordinating, are
embedded. Clause types appear to be a viable source
of co-varying information in rhetorical relation pre-
diction in the tasks under discussion.
The aspects of syntactic-semantic form and prag-
matic function in the relationship between granular-
ity and rhetorical relations is of central interest in
this investigation. Asher and Lascarides represent
discourses hierarchically through coordination and
subordination of information which corresponds to
changes in granularity. However, while the notion
of granularity enters into the motivation and formu-
lation of the SDRT inventory, it is not developed fur-
ther. These results potentailly allow us to say some-
thing deeper about the structural organization of dis-
course as it relates to granularity.
In particualr, while there is some probabilistic
leverage in collapsing categories, it is not the case
that arbitrary categorizations will perform similarly.
This observation holds true even for theoretically
informed categorizations. For example, organizing
the SDRT inventory into coordinated and subordi-
nated relations yields lower performance on relation
prediction. Coordinated and subordinated can be
predicted with 80% accuracy, but the prediction of
the individual relations given the category performs
only at 70%. Since the granularity-based organiza-
tion presented here performs better, we suggest that
the pragmatic function of the relation is more sys-
tematic than the syntactic-semantic form of the rela-
tion.
Future research will focus on more data, differ-
ent machine learning techniques (e.g. unsupervised
learning) and automatization. Where clause, tense,
aspect and event are readily automatable, rhetorical
relations and granularity are less so. Automatically
extracting such information from an annotated cor-
pus such as the Penn Discourse Tree Bank is cer-
tainly feasible. However, the distribution of genres
in this corpus is somewhat limited (i.e., predomi-
nately news text (Webber, 2009)) and calls into ques-
tion the generalizeability of results to other modes of
discourse. Overall, we have demonstrated that the
inclusion of a granularity-based organization in the
machine learning prediction of rhetorical relations
increases performance by 37%, which is roughly
14% above previous reported results for a broader
range of discourses and relations.
47
Acknowledgments
Thank you to Jeff Ondich and Ultralingua for facil-
itating this research and to four anonymous *SEM
reviewers for insightful and constructive comments.
References
Nicholas Asher and Alex Lascarides. 2003. Logics
of Conversation. Cambridge University Press, Cam-
bridge, UK.
John G. Cleary and Leonard E. Trigg 1995. K*: An
Instance-based Learner Using an Entropic Distance
Measure. In Proceedings of the 12 International Con-
ference on Machine Learning, 108?113.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
H. Paul Grice. 1975. Logic and Conversation. In Syntax
and Semantics, Vol. 3, Speech Acts, 43?85. Academic
Press, New York.
Jerry R. Hobbs. 1985. On The Coherence and Structure
of Discourse. CSLI Technical Report, CSLI-85-37.
Blake Stephen Howald and Graham Katz. 2011. The
Exploitation of Spatial Information in Narrative Dis-
course. In Proceedings of the Ninth International
Workshop on Computational Semantics, 175?184.
Mirella Lapata and Alex Lascarides. 2004. Inferring
Sentence Internal Temporal Relations. In Proceedings
of the North American Association of Computational
Linguistics (NAACL-04) 2004, 153?160.
Daniel Marcu. 1998. Improving Summarization
Through Rhetorical Parsing Tuning. In Proceedings of
The 6th Workshop on Very Large Corpora, 206?215.
Daniel Marcu and Abdessamad Echihabi. 2002. An Un-
supervised Approach to Recognizing Discourse Rela-
tions. In Proceedings of the Association of Computa-
tional Linguistics (ACL-02) 2002, 368?375.
Rutu Mulkar-Mehta, Jerry R. Hobbs and Eduard Hovy.
2011. Granulairty in Natural Language Discourse.
In Proceedings of the Ninth International Conference
on Computational Semantics (IWCS 2011) 2011, 195?
204.
Georgiana Puscasu and Verginica Mititelu. 2008. Anno-
tation of WordNet Verbs with TimeML Event Classes.
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC08)
James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser
Saur, Robert Gaizauskas, Andrea Setzer, and Graham
Katz. 2005. TimeML: Robust Specification of Event
and Temporal Expressions in Text. In Proceedings of
the Fith International Conference on Computational
Semantics (IWCS 2005)
Ross Quinlan. 1993 C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann, San Francisco, CA.
Carlota Smith. 2003. Modes of Discourse: The Local
Structure of Texts. Cambridge University Press, Cam-
bridge, UK.
Caroline Sporleder and Alex Lascarides. 2005. Exploit-
ing Linguistic Cues to Classify Rhetorical Relations.
In Proceedings of Recent Advances in Natural Lan-
guage Processing (RANLP-05), 532?539.
Caroline Sporleder and Alex Lascarides. 2008. Using
Automatically Labelled Examples to Classify Rhetori-
cal Relations: An Assessment. Natural Language En-
gineering, 14:369?416.
Janyce Wiebe, Thomas O?Hara, Thorsten O?hrstro?m-
Sandgren and Kenneth McKeever. 1997. An Em-
pirical Approach to Temporal Reference Resolution.
In Proceedings of the 2nd Conference on Empirical
Methods in Natural Language Processing (EMNLP-
97), 174?186.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.) Morgan Kaufmann, San Fran-
cisco, CA.
Bonnie Webber 2009. Genre Distictions for Discourse
in the Penn TreeBank. In Proceedings of the 47th ACL
Conference, 674?682.
48
The Exploitation of Spatial Information in Narrative Discourse
Blake Stephen Howald
Georgetown University
bsh25@georgetown.edu
E. Graham Katz
Georgetown University
egk7@georgetown.edu
Abstract
We present the results of several machine learning tasks that exploit explicit spatial language
to classify rhetorical relations and the spatial information of narrative events. Three corpora are
annotated with figure and ground (granularity) relationships, mereotopologically classified verbs
and prepositions, and frames of reference. For rhetorical relations, Na??ve Bayesian models achieve
84.90% and 57.87% accuracy in classifying NARRATION and BACKGROUND / ELABORATION re-
lations respectively (16% and 23% above baseline). For the spatial information of narrative events,
K* models achieve 55.68% average accuracy (12% above baseline) for all spatial information types.
This result is boosted to 71.85% (28% above baseline) when inertial spatial reference and text se-
quence information are considered. Overall, spatial information is shown to be central to narrative
discourse structure and prediction tasks.
1 Introduction
Clauses in discourse are related to one another in a number of semantic and pragmatic ways. Some of the
most prominent are temporal relations that hold among the times of events and states described (Partee,
1984; Pustejovsky et al, 2003) and the rhetorical relations that hold between a pair of clauses (Mann
and Thompson, 1987; Asher and Lascarides, 2003). For example, (1) illustrates the NARRATION relation
which obtains between (1a-b) and between (1b-c).
(1) a. Klose was sitting with his teammates.
b. He walked to the sidelines.
c. Then he entered the game.
Because of the temporal properties of NARRATION (Asher and Lascarides 2003, p. 462), the event
described in (1a) is taken to precede that described in (1b) and (1b)?s event to precede (1c)?s. As Asher
and Lascarides show, there is a close tie between the rhetorical structure of a discourse and its temporal
structure. In (2), for example, the fact that the clauses are related by ELABORATION entails that the
temporal relation between (2a) and (2b) is inclusion.
(2) a. Klose scored a goal.
b. He headed the ball into the upper corner.
We observe that the spatial relations among the locations of the events described in these discourses
are also highly determined by the rhetorical relations between the clauses used to describe them. In
the NARRATION-related discourse (1), there is a spatial progression: Klose is located relative to his
teammates (1a), he then moves from the bench to the sidelines (1b), and then he moves from the sidelines
into the game (1c). In the ELABORATION-related discourse (2), there is no such progression.
In this paper, we investigate the degree to which the spatial structure of discourse and its rhetorical
structure are co-determined. Using supervised machine learning techniques (Witten and Frank, 2002),
we evaluate two hypotheses: (a) spatial information encoded in adjacent clauses is highly predictive of
the rhetorical relations that hold between them and (b) spatial information is highly predictable based on
associated spatial information within narrative event clauses. To do this, we build a corpus of narrative
texts which are annotated both for spatial information (figure and ground (granularity) relationships,
175
mereotopologically classified verbs and prepositions, and frames of reference) and rhetorical relations (a
binary NARRATION vs. ELABORATION/BACKGROUND distinction discussed in Section 3.2). This corpus
is then used to train two types of classifiers - one type that classifies the rhetorical relations holding
between clauses on the basis of spatial information, and another type that classifies spatial relationships
within clauses where the NARRATION relation holds. The results support both hypotheses and indicate
the centrality of spatial information to narrative discourse structure and associated classification tasks.
2 Background and Related Research
2.1 Rhetorical Relations
Rhetorical relations describe the role that one clause plays with respect to another in a text and contributes
to a text?s coherence (Hobbs, 1985). As such, these relations are pragmatic features of a text. In NLP
generally, classifying rhetorical relations has been an important area of research (Marcu, 2000; Sporleder
and Lascarides, 2005) and has been shown to be useful for tasks such as text summarization (Marcu,
1998). The inventory of rhetorical relations in Segmented Discourse Representation Theory (SDRT)
(Asher and Lascarides, 2003) is widely used in these applications. This inventory includes the following
relations, illustrated by example: NARRATION: Klose got up. He entered the game. ELABORATION:
Klose pushed the Serbian midfielder. He knew him from school. BACKGROUND: Klose entered the game.
The pitch was very wet. EXPLANATION: Klose received a red card. He pushed the Serbian midfielder.
CONSEQUENCE: If Klose received a red card, then he pushed the Serbian midfielder. RESULT: Klose
pushed the Serbian midfielder. He received a red card. ALTERNATION: Klose received a red card or he
received a yellow card. CONTINUATION: Klose received a red card. Ronaldo received a yellow card.
In previous work, rhetorical relations have been predicted based on a range of features including
discourse connectives, relation location, clause length, part-of-speech, content and function words, and
syntactic features (Marcu and Echihabi, 2002; Lapata and Lascarides, 2004). These systems have a wide
range of average accuracies for all relations sought to be predicted - e.g. 33.96% (Marcu and Echihabi,
2002) to 70.70% (Lapata and Lascarides, 2004) - and individual relations - e.g. RESULT - 16.21% and
EXPLANATION - 75.39% (Marcu and Echihabi, 2002) and CONTRAST - 43.64% and CONTINUATION -
83.35% (Sporleder and Lascarides, 2005). Our focus is on the NARRATION, BACKGROUND and ELAB-
ORATION relations, which account for over 90% of the discourses in our corpus.
2.2 Spatial Language and Discourse
Spatial language has been discussed in a number of NLP contexts. For example, linking natural language
with physical locations via semantic mark-up (e.g. SpatialML (MITRE, 2009)); spatial description and
wayfinding tasks (e.g. Anderson et al, 1991); and dialogue systems (e.g. Coventry et al, 2009), just
to name a very few. Perspectives on spatial language are similarly varied in terms of their focus and
theoretical background (e.g. cognitive, semantic and syntactic); however, common threads do emerge.
First, all physical spatial references are reducible to figure and ground relationships (Talmy, 2000). In
English, these are triggered by a deictic verb or adverb (e.g. went, here) (3a); a spatial preposition (e.g.
in, at) (3b); a particle verb (e.g. put on, got out) (3c); or a motion verb (e.g. drive, follow) (3d).
(3) a. [Ronaldo]figure is [here]ground.
b. [Ronaldo]figure is in [the park]ground.
c. [Ronaldo]figure rolled over [?]ground.
d. [Ronaldo]figure ran to [the park]ground.
Second, figure and ground relationships qualitatively vary by the type of verb and preposition cre-
ating the relationship. These differences can be modeled in mereotopology, which defines spatial re-
lationships in terms of regions and connections (e.g. RCC-8 (Randell et al, 1992)). We follow Asher
and Sablayrolles (1995) who classify prepositions based on the position (Position - at, Initial Direction
- from, Medial Position - through, Final Position - to) and contact (Inner - in, Contact - against, Outer
176
- along, and Outer-Most - beyond) of two regions (figure and ground). For verbs, Muller (2002) pro-
poses six mereotopological classes: Reach, Leave, Internal, External, Hit, and Cross. Pustejovsky and
Moszkowicz (2008) mapped Muller?s classes to FrameNet and VerbNet and propose ten general classes
of motion (Move, Move-External, Move-Internal, Leave, Reach, Detach, Hit, Follow, Deviate, Stay).
Third, figure and ground relationships vary by the perspective used to describe the relationship.
For this discussion, perspective takes two forms, granularity of spatial description (following Montello
(1993)) and frames of reference (following Levinson (1996)). Granularity refers to the level of detail
in a given spatial description. Montello (1993, p. 315) indicates four spatial granularities based on the
cognitive organization of spatial knowledge (summarized in (4)).
(4) a. Ronaldo jumped on the ball.
b. Ronaldo is in the corner.
c. Ronaldo is running around the field.
d. Ronaldo is in Cape Town.
(4a) is a Figural granularity which describes space smaller than the human body. (4b) is a Vista gran-
ularity which describes space from a single point of view. (4c) is an Environmental granularity which
describes space larger than the body with multiple (scanning) point(s) of view. (4d) is a Geographic
granularity which describes space even larger than the body and is learned by symbolic representation.
Frames of reference provide different ways of describing the same spatial relationships. For example,
given a static scene of Ronaldo sitting on a bench next to his coach, each utterance in (5) would be an
accurate spatial description.
(5) a. Deictic: Ronaldo is there.
b. Contiguity: Ronaldo is on the bench.
c. Named Location: Ronaldo is at the sideline.
d. Relative: Ronaldo is in front of me.
e. Intrinsic: Ronaldo is behind his coach.
f. Absolute: Ronaldo is north of his coach.
(5a-c) are non-coordinated as they relate just the figure and ground. Coordinated information, relating
the figure to an additional entity within the ground, occurs in (5d-f). Frames of reference apply to both
static and dynamic relationships (Levinson, 1996, p. 360).
In terms of attending to spatial information in discourse, Herman (2001) argues that spatial informa-
tion patterns in narrative discourse carve out spatially defined domains that group narrative actions. In
particular, the emergence and change in different types of spatial reference to physical location (discourse
cues) create maps of the narrative actions. These discourse cues include figure, ground and path (motion)
relationships (3); frames of reference (5); and deictic shifts - here vs. there. Herman?s demonstration is
based on ghost story narratives that are rich in spatial reference.
Howald (2010) showed in a corpus of serial killer first person narratives, also rich in spatial reference,
that these spatial narrative domains, in the form of abstract Pre-Crime, Crime and Post-Crime events,
were predicted to a 90% accuracy from three spatial features (figure, ground, and spatial verb) and
discourse sequence. Overall, research by Herman (2001) and Howald (2010) demonstrates some level
of dependency between spatial information and discourse structure. The present research addresses the
specific question of whether there is a systematic relationship between spatial information and temporal
information via rhetorical relations and the spatial architecture of narrative events.
3 Data and Annotation
3.1 Data
Three corpora of narrative discourse were annotated with rhetorical and spatial information. These cor-
pora were then used to train and test machine learning systems. Summarized in Table 1, the three dif-
ferent narrative corpora selected for analysis were: (1) narratives from serial criminals (CRI) - oral and
177
written confession statements and guilty pleas; (2) American National Corpus Charlotte Narrative and
Conversation Collection (Ide and Suderman, 2007) (ANC) - oral narratives in conversations collected in
a sociolinguistic interview format; and (3) The Degree Confluence Project (DEG) - this project, which
seeks to map all possible latitude-longitude intersections on Earth, requires that participants who visit
these intersections provide written narratives of the visit for inclusion on the project?s website.
Table 1: Relation and Spatial Clause Distribution
Corpus ANC (n=20) DEG (n=20) CRI (n=20) Total (N=60)
Total Clauses 588 611 1,710 2,909
Spatial Clauses 260 354 932 1,546
Average 44.21 57.93 54.50 53.14
Total Rhetorical 568 591 1,690 2,848
Spatial Rhetorical 259 345 929 1,533
Average 45.59 58.37 55.00 53.82
20 narratives from each corpus were selected. There was a total of 2,909 (independent) clauses with
1,546 of those clauses containing spatial information - spatial clauses (53.14% on average). There was a
total of 2,848 relations with 1,533 of those relations where both clauses contained spatial information -
spatial rhetorical (53.82% on average).
3.2 Spatial Information and Rhetorical Relation Annotation
We developed a coding scheme for spatial information that consolidates the insights on spatial langauge
discussed in Section 2.2.
? FIGURE is an indication of grammatical person or a non-person entity (1 = I, my; 2 = you, your;
3 = he, she, it, his, her; 4 = we, our; 5 = you, your; 6 = they, their; NP = the purse, a bench, three
cars);
? VERB is one of the four mereotopological classes - a consolidation of Pustejovsky and Moszkow-
icz?s (2008) ten classifications (State = was, stay, was sitting; Move = run, go, jump; Outside =
follow, pass, track; Hit = attach, detach, strike);
? PREPOSITION is one of four mereotopological classes based on Asher and Sablayrolles (1995)
(Positional = in, on; Initial = from ; Medial = through; Final = to);
? GROUND is one of four granularities (Figural, Environmental, Vista, Geographic) (see (4)
above);
? FRAME is one of six frames of reference (Deictic, Contiguity, Named Location, Relative, In-
trinsic, Absolute) (see (5) above).
The three corpora were annotated by one of the authors. Annotation occurred one narrative at a
time and any information from that narrative could be used to resolve rhetorical relations and spatial
information. A reference sheet including several examples of each coding element was available to
the annotator. The annotation happened in two phases. First, each pair of clauses was annotated with
an SDRT relation. Second, each clause that contained a physical figure and ground relationship was
identified. The figure, ground, preposition and verb were annotated with a Figure, Verb, Preposition,
Ground, and Frame. We illustrate with (6) where the NARRATION relation obtains between (6a-b).
(6) a. Kaka kicked the ball into the goal.
b. Then he ran to the left side of the bench.
178
The spatial annotation of (6a) is: FIGURE = NP, the ball; VERB = Hit (H), kicked; PREPOSITION =
Final (F), into; GROUND = Environmental (E), the goal; and FRAME = Contiguity (C). The spatial
annotation of (6b) is: FIGURE = 3, he; VERB = Move (M), ran; PREPOSITION = Final (F), to the left
side of; GROUND = Environmental (E), the bench; and FRAME = Intrinsic (INT). The distribution of
spatial rhetorical relations is summarized in Table 2.
Table 2: Spatial Rhetorical Relation Distribution per Corpus
Relation ANC DEG CRI Total
NARRATION 133 124 654 911
BACKGROUND 74 87 238 399
ELABORATION 34 63 17 114
CONTINUATION 14 27 10 51
RESULT 3 22 0 25
EXPLANATION 0 16 1 17
ALTERNATION 0 0 9 9
CONSEQUENCE 1 6 0 7
Total 259 345 929 1,533
An additional individual was queried for inter-rater reliability against the author annotation. The rater
was given roughly one-third of the data (10 narratives (4 ANC, 4 DEG, 2 CRI) accounting for 510 spatial
clause pairs), the same example sheet used by the author, and as much time as needed to complete the
task. Average agreement and Cohen?s kappa statistics (Cohen, 1960) were computed between the inter-
rater and the author for the spatial annotations and NARRATION, BACKGROUND, and ELABORATION
codings. Individually, BACKGROUND and ELABORATION have low interannotator agreement (? = 32.92
and 54.20 respectively), but these two relations were often confused (26% of BACKGROUND relations
coded as ELABORATION and 12% of ELABORATION relations coded as BACKGROUND). As illustrated
in (7-8), both BACKGROUND and ELABORATION add information to the surrounding state of affairs.
(7) a. Klose entered the game.
b. The pitch was very wet.
(8) a. Klose pushed the Serbian midfielder.
b. He knew him from school.
As evidenced by the annotation confusions, the difference between these relations is difficult to distin-
guish and the distinction made by Asher and Lascarides (2003) is subtle - BACKGROUND?s temporal
consequence is one of overlap and ELABORATION, a subordinating relation, is one of part-of. However
collapsing these relations resulted in a fairly reliably distinguished category. Average agreement and
kappa statistics are summarized in Table 3.
Table 3: Agreement and Kappa Statistics for Relation and Spatial Codings
Coding Agreement (%) Kappa (?)
All Rhetorical Relations 71.97 60.27
NARRATION 86.32 74.36
BACKGROUND / ELABORATION 73.40 62.20
Figure 94.91 89.92
Verb 90.90 81.80
Preposition 78.35 56.70
Granularity 87.87 75.74
Frame 69.38 38.76
179
For rhetorical relations, the average agreement and kappa statistic are consistent with previously re-
ported performances (e.g. Agreement = 71.25 / ? = 61.00 (Sporleder and Lascarides, 2005)). We have
not been able to find previously reported performance accuracies for NARRATION, ELABORATION and
BACKGROUND relations specifically. However, ? statistics from 60.00 to 75.00 and above are considered
acceptable (e.g. Landis and Koch, 1977). For the spatial codings, the average agreements are relatively
high with Preposition and Frame falling lowest. There is no basis for direct comparison of these num-
bers to other research as the coding scheme is novel.
4 Machine Learning Experiments
We constructed two machine learning tasks to exploit the annotated spatial information to determine what
contributions the information is making to narrative structure. The first task evaluates the prediction of
NARRATION and BACKGROUND/ ELABORATION relations based on pairs of spatial clauses. The second
task evaluates the prediction of spatial information types, based on the other spatial information types in
that clause, in individual clauses where the NARRATION relation holds.
4.1 Rhetorical Relation Prediction
4.1.1 Methods and Results
Task 1 builds a 2-way classifier for the NARRATION and BACKGROUND/ ELABORATION relations.
Clause pairs were coded as vectors (n = 1,424) - for example, the vector for (6) is NP3, HM, FF,
EE, CINT. These vectors were used to train and test (10-fold cross-validation) a number of classifiers.
The Na??ve Bayes classifier performed the best. Results are reported in Table 4.
Table 4: Na??ve Bayes Classification Accuracy and F-Measures for Task 1
NARRATION Accuracy (% / baseline) Precision Recall F-Score
ANC 63.29 / 58 .676 .633 .654
DEG 75.71 / 61 .803 .757 .779
CRI 90.12 / 73 .822 .901 .860
TOTAL 84.90 / 68 .808 .841 .824
BACK/ ELAB Accuracy (% / baseline) Precision Recall F-Score
ANC 57.89 / 41 .532 .579 .555
DEG 70.11 / 38 .642 .701 .670
CRI 45.63 / 26 .624 .456 .527
TOTAL 57.87 / 35 .622 .567 .593
For all corpora combined, the majority class (?baseline?) for NARRATION is 68% and 26% for BACK-
GROUND / ELABORATION; the classifier performs 16% and 22% above baseline respectively. The differ-
ence between the NARRATION and BACKGROUND / ELABORATION relations and baselines is statistically
significant for each corpus and all corpora combined - ANC: ?2 = 25.64, d.f. = 1, p ? .001; DEG: ?2 =
33.86, d.f. = 1, p ? .001; CRI: ?2 = 22.69, d.f. = 1, p ? .001; and TOTAL:?2 = 34.09, d.f. = 1, p ? .001.
4.1.2 Discussion
Again, we have not been able to find reported results for a direct comparison of NARRATION and BACK-
GROUND/ ELABORATION. However, the 84.90% and 57.87% (at 16% and 22% over baseline) perfor-
mance of our Na??ve Bayesian model is consistent with results reported in similar tasks. For example,
Marcu and Echihabi (2002) report an average accuracy of 33.96% (5-way classifier) and 49.70% (6-way
classifier) based on training with very large data sets. Sporleder and Lascarides (2005) report a 57.55%
average accuracy, based on training with large data sets, which is 20% over Marcu and Echihabi?s 5-way
180
classifier and almost 40% over a random 20% baseline. Lapata and Lascarides (2004) report an average
accuracy of 70.70% for inferring temporal relations based on training.
We ran an additional set of experiments to determine the relative contribution of spatial features to
predict NARRATION and BACKGROUND / ELABORATION relations. As shown in Table 5, Figure and
Verb outperform Ground, Preposition and Frame in accuracy. Figure performs at a 71% average
accuracy (85% for NARRATION and 40% for BACKGROUND/ ELABORATION) and Verb performs at a
74% average accuracy (84% for NARRATION and 54% for BACKGROUND/ ELABORATION). Figure and
Verb appear to be most discriminating. Note that we are not suggesting that subject and verb generally
are similarly discriminatory - Figure and Verb in this task are overtly spatial. Despite the performance
of Figure and Verb, different subsets of spatial information worked better (we ran all permutations of
spatial features - the top five are listed in Table 5). However, the difference in performance is negligible.
For example, the best subset of Figure, Verb and Ground (85% and 58%) only performed 1% above
NARRATION and BACKGROUND/ ELABORATION prediction based on all five features combined.
Table 5: Single and Combined Spatial Feature Performance
Feature NARRATION BACK/ ELAB Features NARRATION BACK/ ELAB
Figure (F) 85.58 40.33 FVG 85.24 58.33
Verb (V) 84.59 54.97 VGP 84.34 58.33
Prepostion (P) 97.34 1.00 FVGR 86.33 56.45
Ground (G) 97.33 1.00 FV 86.56 56.90
Frame (R) 98.02 2.00 VG 85.37 57.33
These results tell us several things about the relationship between spatial information and rhetorical
structure as it applies to narrative discourse. First, spatial information predicts rhetorical structure as
good as non-spatial types of linguistic information reported in other investigations and with many fewer
features. For example, Sporleder and Lascarides (2005) rely on 72 different features falling into nine
classes whereas we rely on 14 features in five classes. This suggests that spatial information is not only
central to rhetorical stucture, like temporal components, but central to the task of prediction. Second,
while the type of spatial information that predicts rhetorical structure is based on the primary figure and
ground relationship, it is the qualitative semantic variations within these elements that is providing the
discrimination. It is the organization of spatial relationships - (Verb and Preposition) and the perspective
provided by the narrator (Figure, Ground and Frame) combined - rather than any individual elements.
4.2 Spatial Information Prediction
4.2.1 Methods and Results
Task 2 is a series of five experiments. Each experiment builds a classifier for each type of spatial infor-
mation: a 6-way classifier for Frame; a 5-way classifier for Figure (Figure types 2 and 5 did not occur
in our corpus); and 4-way classifiers for Ground, Preposition and Verb. Single clauses that contribute
to the NARRATION relation were coded as vectors (n = 911) - for example, the single vectors for (6a)
and (6b) are NP, H, F, E, C and 3, M, F, E, INT. These vectors were used to train and test (10-fold
cross-validation) a number of classifiers to predict one of the five spatial features given the remaining
four. The K* classifier performed the best. Results are reported in Table 6. For all corpora combined, the
K* classifier performs above baseline for all spatial information (Figure = 9%, Verb = 17%, Preposition
= 9%, Ground = 19%, Frame = 8%) (?2 = 20.95, d.f. = 4, p ? .001).
4.2.2 Discussion
Even though the accuracies of predicting spatial information are significantly above baseline, we sought
ways to boost performance by considering implicit spatial information. For those clauses without explicit
spatial information, we extended the annotation of the previous clause?s coding based on the inertia of
181
Table 6: K* Classification Accuracy and F-Measures for Task 2
Spatial Information Accuracy (% / baseline) Precision Recall F-Score
Figure 47.97 / 38 .464 .480 .428
Verb 67.32 / 50 .635 .673 .640
Preposition 53.69 / 46 .492 .537 .499
Ground 53.59 / 34 .530 .536 .519
Frame 55.67 / 47 .507 .557 .511
narrative texts. Rapaport, et al (1994) discuss the temporal inertia of narrative texts - time moves forward
through narrative events. In the absence of updating, information is maintained. We suggest that inertia
applies to spatial information as well. For example, given the clauses - John entered the room. He sat
down. - we make the assumption that John sat down in the room that he entered. We illustrate with (9).
(9) a. Kaka kicked the ball into the goal.
NP, H, F, E, C, .33
b. The goaltender yelled in frustration.
NP, H, F, E, C, .66
c. Then Kaka ran to the left side of the bench.
3, M, F, E, INT, 1
No explicit spatial information exists in (9b). We took the coding from the explicit spatial information
in (9a) and maintained it for (9b). New explicit spatial information occurs in (9c) and the coding is
updated. Further, we included explicit sequence information as a measure of a given clause?s proportional
position within the text (.33, .66 and 1). In the absence of overt temporal specification (occuring in only
10% of the clauses in our corpus), the sequence information, a textual feature, parallels the temporal
progression (and inertia) of narrative events. This added 560 additional vectors (n = 1,471). The K*
classifier still performed the best. The results are summarized in Table 7.
Table 7: K* Classification Accuracy and F-Measures for Task 2 Boosted Vectors
SPATIAL INERTIA Accuracy (% / baseline) Precision Recall F-Score
Figure 51.73 / 41 .509 .517 .473
Verb 70.22 / 48 .673 .700 .679
Preposition 57.30 / 47 .571 .573 .540
Ground 62.61 / 35 .636 .626 .611
Frame 59.82 / 44 .574 .598 .564
SPATIAL INERTIA + SEQUENCE Accuracy (% / baseline) Precision Recall F-Score
Figure 70.56 / 41 .702 .706 .699
Verb 79.33 / 48 .789 .793 .790
Preposition 67.91 / 47 .676 .679 .674
Ground 72.39 / 35 .721 .724 .721
Frame 69.06 / 44 .678 .691 .681
Inclusion of the spatial inertia values improves performance of the K* classifier in all cases (?2 =
40.59, d.f. = 4, p ? .001). Inclusion of sequence information improves performance even further (?2
= 102.36, d.f. = 4, p ? .001). Note that, despite the increase in performance, sequencing information
alone does not do as well, indicating that spatial information still plays a discriminatory role. Using
sequence information alone as a baseline (Figure = 47%, Verb = 52%, Preposition = 47%, Ground =
44%, Frame = 48%;), the normalized performance values above sequence baseline become Figure =
23%, Verb = 27%, Preposition = 28%, Ground = 20%, and Frame = 21%.
The ability to predict spatial features appears to be dependent both on a patterned distribution of
182
the per-clause spatial information (increased by spatial inertia) and on the textual feature of sequence
(temporal inertia). This seems to hold despite the specific subject matter or spatial characteristics of a
given narrative. Considering the complete spatiotemporal picture for narrative clauses yields the best
prediction results and suggests that the spatial information structure of narrative discourse represents
some type of organization akin to what Herman (2001) and Howald (2010) have evaluated in spatially-
rich narratives. Based on the tasks presented here, this organization appears to be fundamental and
relative to formal temporally-informed discourse structure.
5 Conclusion
Exploration of the spatial dimension in narrative discourse provides interesting and robust possibilities
for computational discourse analysis. We have described two machine learning tasks which exploit
spatial linguistic features. In addition to improving on existing prediction systems, both tasks empirically
demonstrate that, when available, certain types of spatial information are predictors of the rhetorical
structure of narrative discourse and the spatial information of narrative event sequences. Based on these
results, we indicate that spatial structure is related to temporal structure in narrative discourse.
The coding scheme proposed here models complex and interrelated properties of spatial relationships
and perspectives and should be generalizeable to other non-narrative discourses. Future research will fo-
cus on different discourse corpora to determine how spatial information is related to rhetorical structure.
Additional future research will also focus on automation of the annotation process. The ambiguity of
spatial language makes automatic extraction of spatial features infeasible at the current state of the art.
Fortunately, average agreement and kappa statistics for coding of the spatial information and rhetorical
relations are within acceptable ranges. The annotated spatial features are semantically deep and useful
for not only computational discourse systems, but tasks that involve the semantic modeling of spatial
relations and spatial reasoning.
Acknowledgments
Thank you to David Herman and James Pustejovsky for productive comments and discussion and to
Jerry Hobbs for suggesting the Degree Confluence Project as a source of spatially rich narratives. Thank
you also to four anonymous reviewers for very helpful insights.
References
[1] Anne Anderson, Miles Bader, Ellen Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen
Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, Catherine Sotillo, Henry Thompson, and Regina
Weinert. 1991. The HCRC Map Task Corpus. Language and Speech, 34:351?366.
[2] Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press,
Cambridge, UK.
[3] Nicholas Asher and Pierre Sablayrolles. 1995. A Typology and Discourse Semantics for Motion
Verbs and Spatial PPs in French. Journal of Semantics, 12(2):163?209.
[4] Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
[5] Kenny Coventry, Thora Tenbrink, and John Bateman. 2009. Spatial Language and Dialogue. Oxford
University Press, Oxford, UK.
[6] David Herman. 2001. Spatial Reference in Narrative Domains. Text, 21(4):515?541.
[7] Jerry R. Hobbs. 1985. On The Coherence and Structure of Discourse. CSLI Technical Report, 85-37.
183
[8] Blake Howald. 2010. Linguistic Spatial Classifications of Event Domains in Narratives of Crime.
Journal of Spatial Information Science, 1.75?93.
[9] Nancy Ide and Keith Suderman. 2007. The Open American National Corpus (OANC), available at
http://www.AmericanNationalCorpus.org/OANC.
[10] Richard Landis and Gary Koch. 1977. The Measurement of Observer Agreement for Categorical
Data. Biometrics, 33(1):159?174.
[11] Mirella Lapata and Alex Lascarides. 2004. Inferring sentence internal temporal relations. In Pro-
ceedings of NAACL-04, 153?160.
[12] Stephen C. Levinson. 1996. Language and Space. Annual Review of Anthropology, 25(1):353?382.
[13] William Mann and Sandra Thompson. 1987. Rhetorical Structure Theory: A Framework for The
Analysis of Texts. International Pragmatics Association Papers in Pragmatics, 1:79?105.
[14] Daniel Marcu. 1998. Improving Summarization Through Rhetorical Parsing Tuning. In The 6th
Workshop on Very Large Corpora, 206?215.
[15] Daniel Marcu. 2000. The Rhetorical Parsing of Unrestricted Texts: A Surface-Based Approach.
Computational Linguistics, 26(3):395?448.
[16] Daniel Marcu and Abdessamad Echihabi. 2002. An Unsupervised Approach to Recognizing Dis-
course Relations. In Proceedings of ACL-02, 368?375.
[17] MITRE. 2009. SpatialML: Annotation Scheme for Marking Spatial Expressions in Natural Lan-
guage, Version 3.0. April 3, 2009.
[18] Daniel R. Montello. 1993. Scale and Multiple Psychologies of Space. In A. Frank and I. Campari
(eds.), Spatial Information Theory: A Theoretical Basis for GIS (LNCS 716), 312?321. Springer-Verlag,
Berlin.
[19] Philippe Muller. 2002. Topological Spatio-temporal Reasoning and Representation. Computational
Intelligence, 18(3):420?450.
[20] Barbara Partee. 1984. Nominal and Temporal Anaphora. Linguistics and Philosophy, 7(3):243?286.
[21] James Pustejovsky and Jessica Moszkowicz. 2008. Integrating motion predicate classes with spatial
and temporal annotations. COLING 2008:95?98.
[22] James Pustejovsky, Jose? Castan?o, Robert Ingria, Roser Saur, Robert Gaizauskas, Andrea Setzer,
and Graham Katz. 2003. TimeML: Robust Specification of Event and Temporal Expressions in Text. In
Proceedings of the IWCS-5, Fifth International Workshop on Computational Semantics.
[23] David Randell, Zhan Cui, and Anthony Cohn. 1992. A Spatial Logic Based on Regions and
Connection. Proceedings of KR92, 394?398. Los Altos, CA: Morgan Kaufmann.
[24] William Rapaport, Erwin Segal, Stuart Shapiro, David Zubin, Gail Bruder, Judith Duchan, Michael
Almeida, Joyce Daniels, Mary Galbraith, Janyce Wiebe and Albert Yuhan. 1994. Deictic Centers and
the Cognitive Structure of Narrative Comprehension. Technical Report No. 89-01. Buffalo, NY: SUNY
Buffalo Department of Computer Science.
[25] Caroline Sporleder and Alex Lascarides. 2005. Exploiting Linguistic Cues to Classify Rhetorical
Relations. Proceedings of Recent Advances in Natural Language Processing (RANLP-05), 532?539.
[26] Leonard Talmy. 2000. Toward a Cognitive Semantics, Volume 2. The MIT Press, Cambridge, MA.
[27] Ian Witten and Eibe Frank. 2002. Data Mining Practical Machine Learning Tools and Techniques
with Java Implementation. Morgan Kaufmann.
184
Proceedings of the 14th European Workshop on Natural Language Generation, pages 178?182,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
GenNext: A Consolidated Domain Adaptable NLG System
Frank Schilder, Blake Howald and Ravi Kondadadi?
Thomson Reuters, Research & Development
610 Opperman Drive, Eagan, MN 55123
firstname.lastname@thomsonreuters.com
Abstract
We introduce GenNext, an NLG system
designed specifically to adapt quickly and
easily to different domains. Given a do-
main corpus of historical texts, GenNext
allows the user to generate a template bank
organized by semantic concept via derived
discourse representation structures in con-
junction with general and domain-specific
entity tags. Based on various features
collected from the training corpus, the
system statistically learns template rep-
resentations and document structure and
produces well?formed texts (as evaluated
by crowdsourced and expert evaluations).
In addition to domain adaptation, Gen-
Next?s hybrid approach significantly re-
duces complexity as compared to tradi-
tional NLG systems by relying on tem-
plates (consolidating micro-planning and
surface realization) and minimizing the
need for domain experts. In this descrip-
tion, we provide details of GenNext?s the-
oretical perspective, architecture and eval-
uations of output.
1 Introduction
NLG systems are typically tailored to very spe-
cific domains and tasks such as text summaries
from neonatal intensive care units (SUMTIME-
NEONATE (Portet et al, 2007)) or offshore oil
rig weather reports (SUMTIME-METEO (Reiter et
al., 2005)) and require significant investments in
development resources (e.g. people, time, etc.).
For example, for SUMTIME-METEO, 12 person
months were required for two of the system com-
ponents alone (Belz, 2007). Given the subject
matter of such systems, the investment is perfectly
?Ravi Kondadadi is now affiliated with Nuance Commu-
nications, Inc.
reasonable. However, if the domains to be gener-
ated are comparatively more general, such as fi-
nancial reports or biographies, then the scaling of
development costs becomes a concern in NLG.
NLG in the editorial process for companies and
institutions where content can vary must be do-
main adaptable. Spending a year or more of devel-
opment time to produce high quality market sum-
maries, for example, is not a viable solution if it is
necessary to start from scratch to produce other re-
ports. GenNext, a hybrid system that statistically
learns document and sentence template represen-
tations from existing historical data, is developed
to be consolidated and domain adaptable. In par-
ticular, GenNext reduces complexity by avoiding
the necessity of having a separate document plan-
ner, surface realizer, etc., and extensive expert in-
volvement at the outset of system development.
Section 2 describes the theoretical background,
architecture and implementation of GenNext. Sec-
tion 3 discusses the results of a non?expert and ex-
pert crowdsourced sentence preference evaluation
task. Section 4 concludes with several future ex-
periments for system improvement.
2 Architecture of GenNext
In general, NLG systems follow a prototypical ar-
chitecture where some input data from a given do-
main is sent to a ?document planner? which de-
cides content and structuring to create a document
plan. That document plan serves as an input to
a ?micro planner? where the content is converted
into a syntactic expression (with associated con-
siderations of aggregation and referring expres-
sion generation) and a text specification is created.
The text specification then goes through the final
stage of ?surface realization? where everything is
put together into an output text (McKeown, 1985;
Reiter and Dale, 2000; Bateman and Zock, 2003).
In contrast, the architecture of GenNext (sum-
marized in Figure 1) is driven by a domain-specific
178
Figure 1: GenNext System Architecture.
corpus text. There is often a structured database
underlying the domains of corpus text, the fields
of which are used for domain specific entity tag-
ging (in addition to domain general entity tagging
[e.g. DATE, LOCATION, etc.]). An overview of
the different stages, which are a combination of
statistical (e.g., Langkilde and Knight (1998)) and
template?based (e.g., van Deemter, et al (2005))
approaches, follows in (A-E).1
A: Semantic Representation - We take a do-
main specific training corpus and reduce each
sentence to a Discourse Representation Structure
(DRS) - formal semantic representations of sen-
tences (and texts) from Discourse Representation
Theory (Kamp and Reyle, 1993; Basile and Bos,
2011). Each DRS is a combination of domain gen-
eral named entities, predicates (content words) and
relational elements (function words). In parallel,
domain specific named entity tags are identified
and are used to create templates that syntactically
represent some conceptual meaning; for example,
the short biography in (1):
(1) Sentence
a. Mr. Mitsutaka Kambe has been serving as Managing
Director of the 77 Bank, Ltd. since June 27, 2008.
b. He holds a Bachelor?s in finance from USC and a MBA
from UCLA.
Conceptual Meaning
c. SERVING | MANAGING | DIRECTOR | PERSON | ...
d. HOLDS | BACHELOR | FINANCE | MBA | HOLD | ...
Once the semantic representations are created,
they are organized and identified by semantic con-
cept (?CuId?) (described in (B)). Our assumption
is that each cluster equates with a CuId repre-
sented by each individual sentence in the cluster
and is contrastive with other CuIds (for similar ap-
1For more detail see Howald, et al (2013) - semantic
clustering and micro-planning and Kondadadi, et al (2013) -
document planning.
proaches, see Barzilay and Lapata (2005), Angeli,
et al (2010) and Lu and Ng (2011)).
B: Creating Conceptual Units - To create the
CuIds (a semi-automatic process), we cluster the
sentences using k-means clustering with k set ar-
bitrarily high to over-generate (Witten and Frank,
2005). This facilitates manual verification of the
generated clusters to merge (rather than split) them
if necessary. We assign a unique CuId to each
cluster and associate each template in the corpus to
a corresponding CuId. For example, in (2), using
the sentences in (1a-b), the identified named en-
tities are assigned to a clustered CuId (2a-b) and
then each sentence in the training corpus is re-
duced to a template (2c-d).
(2) Content Mapping
a. {CuId : 000} ? Information: person: Mr. Mitsutaka
Kambe; title: Managing Director; company: 77 Bank,
Ltd.; date: June 27, 2008
b. {CuId : 001} ? Information: person: he; degree:
Bachelor?s, MBA; subject: finance; institution: USC;
UCLA
Templates
c. {CuId : 000}: [person] has been serving as [title] of the
[company] since [date].
d. {CuId : 001}: [person] holds a [degree] in [subject]
from [institution] and a [degree] from [institution].
At this stage, we will have a set of CuIds with cor-
responding template collections which represent
the entire ?micro-planning? aspect of our system.
C: Collecting Statistics - For the ?document plan-
ning? stage, we collect a number of statistics for
each domain, for example:
? Frequency distribution of CuIds by position
? Frequency distribution of templates by position
? Frequency distribution of entity sequence
? Average number of entities by CuId and position
These statistics, in addition to entity tags and tem-
plates, are used in building different features used
by the ranking model (D).
D: Building a Ranking Model - The core compo-
nent of our system is a statistical model that ranks
a set of templates for a given position (e.g. sen-
tence 1, sentence 2, ..., sentence n) based on the
input data (see also Konstas and Lapata (2012).
The learning task is to find the rank for all the tem-
plates from all CuIds at each position. To gener-
ate the training data, we first exclude the templates
that have named entities not specified in the input
data (ensuring completeness). We then rank tem-
plates according to the edit distance (Levenshtein,
179
1966) from the template corresponding to the cur-
rent sentence in the training document. For each
template, we build a ranking model with features,
for example:
? Prior template and CuId
? Difference in number of words given position
? Most likely CuId given position and previous CuId
? Template 1-3grams given position and CuId
We use a linear kernel for a ranking SVM
(Joachims, 2002) to learn the weights associated
with each feature. Each domain has its own model
that is used when generating texts (E).
E: Generation: At generation time, our system
has a set of input data, a semantically organized
template bank and a model from training on a
given domain of texts. For each sentence, we first
exclude those templates that contain a named en-
tity not present in the input data. Then we cal-
culate the feature values times the model weight
for each of the remaining templates. The tem-
plate with the highest score is selected, filled
with matching entities from the input data and ap-
pended to the generated text. Example generations
for each domain are included in (3).
(3) Financial
a. First quarter profit per share for Brown-Forman
Corporation expected to be $0.91 per share by analysts.
b. Brown-Forman Corporation July first quarter profits will
be below that previously estimated by Wall Street with
a range between $0.89 and $0.93 per share and a projected
mean per share of $0.91 per share.
c. The consensus recommendation is Hold.
Biography
d. Mr. Satomi Mitsuzaki has been serving as Managing
Director of Mizuho Bank since June 27, 2008.
e. He was previously Director of Regional Compliance of
Kyoto Branch.
f. He is a former Managing Executive Officer and Chief
Executive Officer of new Industrial Finance Business
Group in Mitsubishi Corporation.
Weather
g. Complex low from southern Norway will drift slowly NNE
to the Lofoten Islands by early tomorrow.
h. A ridge will persist to the west of British Isles for Saturday
with a series of weak fronts moving east across
the North Sea.
i. A front will move ENE across the northern North Sea
Saturday.
3 Evaluation and Discussion
We have tested GenNext on three domains: Corpo-
rate Officer and Director Biographies (1150 texts
ranging from 3-10 period ended sentences), Fi-
nancial Texts (Mutual Fund Performances [162
texts, 2-4 sentences] and Broker Recommenda-
tions [905 texts, 8-20 sentences]), and Offshore
Oil Rig Weather Reports (1054 texts, 2-6 sen-
tences) from SUMTIME-METEO (Reiter et al,
2005). The total number of templates for the finan-
cial domain is 1379 distributed across 38 different
semantic concepts; 2836 templates across 19 con-
cepts for biography; and 2749 templates across 9
concepts for weather texts.
We have conducted several evaluation experi-
ments comparing two versions of GenNext, one
applying the ranking model (rank) and one with
random selection of templates (non-rank) (both
systems use the same template bank, CuId as-
signment and filtering) and the original texts from
which the data was extracted (original).
We used a combination of automatic (e.g.
BLEU?4 (Papineni et al, 2002), METEOR
(Denkowski and Lavie, 2011)) and human metrics
(using crowdsourcing) to evaluate the output (see
generally, Belz and Reiter (2006). However, in the
interest of space, we will restrict the discussion to
a human judgment task on output preferences. We
found this evaluation task to be most informative
for system improvement. The task asks an evalu-
ator to provide a binary preference determination
(100 sentence pairs/domain): ?Do you prefer Sen-
tence A (from original) or the corresponding Sen-
tence B (from rank or non-rank)?. This task was
performed for each domain.2 We also engaged 3
experts from the financial and 4 from the biogra-
phy domains to perform the same preference task
(average agreement was 76.22) as well as provide
targeted feedback.
For the preference results, summarized in Fig-
ure 2, we would like to see no statistically signifi-
cant difference between GenNext-rank and orig-
inal, but statistically significant differences be-
tween GenNext-rank and GenNext-non-rank, and
original and GenNext-non-rank. If this is the case,
then GenNext-rank is producing texts similar to
the original texts, and is providing an observ-
able improvement over not including the model at
all (GenNext-non-rank). This is exactly what we
see for all domains.3 However, in general, there
2Over 100 native English speakers contributed, each one
restricted to providing no more than 50 responses and only
after they successfully answered 4 initial gold data questions
correctly and continued to answer periodic gold data ques-
tions. The pair orderings were randomized to prevent click
bias. 8 judgments per sentence pair was collected (2400 judg-
ments) and average agreement was 75.87.
3Original vs. GenNext-rank : financial - ?2=.29, p?.59;
biography - ?2=3.01, p?.047; weather - ?2=.95, p?.32.
Original vs. GenNext-non-rank : financial - ?2=16.71,
p?.0001; biography - ?2=45.43, p?.0001; weather -
180
Figure 2: Cross-Domain Non-Expert Preference Evaluations.
is a greater difference between the original and
GenNext-rank biographies compared to the finan-
cial and weather texts. We take it as a goal to ap-
proach, as close as possible, the preferences for
the original texts.
The original financial documents were machine
generated from a different existing system. As
such, it is not surprising to see similarity in perfor-
mance compared to GenNext-rank and potentially
explains why preferences for the originals is some-
what low (assuming a higher preference rating for
well-formed human texts). Further, the original
weather documents are highly technical and not
easily understood by the lay person, so, again, it is
not surprising to see similar performance. Biogra-
phies were human generated and easy to under-
stand for the average reader. Here, both GenNext-
rank and GenNext-non-rank have some ground to
make up. Insights from domain experts are poten-
tially helpful in this regard.
Expert evaluations provided similar results and
agreements compared to the non?expert crowd.
Most beneficial about the expert evaluations was
the discussion of integrating certain editorial stan-
dards into the system. For example, shorter texts
were preferred to longer texts in the financial do-
main, but not the biographies. Consequently, we
could adjust weights to favor shorter templates.
Also, in biographies, sentences with subordinated
elaborations were not preferred because these con-
tained subjective comments (e.g. a leader in in-
dustry, a well respected individual, etc.). Here,
?2=24.27, p?.0001. GenNext-rank vs. GenNext-non-rank
: financial - ?2=12.81, p?.0003; biography - ?2=25.19,
p?.0001; weather - ?2=16.19, p?.0001.
we could manually curate or could automatically
detect templates with subordinated clauses and re-
move them. These types of comments are useful
to adjust the system accordingly to end user ex-
pectations.
4 Conclusion and Future Work
We have presented our system GenNext which is
domain adaptable, given adequate historical data,
and has a significantly reduced complexity com-
pared to other NLG systems (see generally, Robin
and McKeown (1996)). To the latter point, devel-
opment time for semantically processing the cor-
pus, applying domain general and specific tags,
and building a model is accomplished in days and
weeks as opposed to months and years.
Future experimentation will focus on being able
to automatically extract templates for different do-
mains to create preset banks of templates in the
absence of adequate historical data. We are also
looking into different ways to increase the vari-
ability of output texts from selecting templates
within a range of top scores (rather than just the
highest score) to providing additional generated
information from input data analytics.
Acknowledgments
This research is made possible by Thomson
Reuters Global Resources (TRGR) with particu-
lar thanks to Peter Pircher, Jaclyn Sprtel and Ben
Hachey for significant support. Thank you also
to Khalid Al-Kofahi for encouragement, Leszek
Michalak and Andrew Lipstein for expert evalua-
tions and three anonymous reviewers for construc-
tive feedback.
181
References
Gabor Angeli, Percy Liang, and Dan Klein. 2012. A
simple domain-independent probabilistic approach
to generation. In Proceedings of the 2010 Confer-
ence on Empirical Methods for Natural Language
Processing (EMNLP 2010), pages 502?512.
Regina Barzilay and Mirella Lapata. 2005. Collective
content selection for concept-to-text generation. In
Proceedings of the 2005 Conference on Empirical
Methods for Natural Language Processing (EMNLP
2005), pages 331?338.
Valerio Basile and Johan Bos. 2011. Towards generat-
ing text from discourse representation structures. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation (ENLG), pages 145?150.
John Bateman and Michael Zock. 2003. Natural
language generation. In R. Mitkov, editor, Oxford
Handbook of Computational Linguistics, Research
in Computational Semantics, pages 284?304. Ox-
ford University Press, Oxford.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of NLG systems. In
Proceedings of the European Association for Com-
putational Linguistics (EACL?06), pages 313?320.
Anja Belz. 2007. Probabilistic generation of weather
forecast texts. In Proceedings of Human Language
Technologies 2007: The Annual Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-HLT?07), pages
164?171.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the EMNLP 2011 Workshop on Statisti-
cal Machine Translation, pages 85?91.
Blake Howald, Ravi Kondadadi, and Frank Schilder.
2013. Domain adaptable semantic clustering in sta-
tistical NLG. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013), pages 143?154. Association for Com-
putational Linguistics, March.
Thorsten Joachims. 2002. Learning to Classify Text
Using Support Vector Machines. Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical NLG framework for aggregated
planning and realization. In Proceedings of the An-
nual Conference for the Association of Computa-
tional Linguistics (ACL 2013). Association for Com-
putational Linguistics.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics (ACL?98),
pages 704?710.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. So-
viet Physics Doklady, 10:707?710.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceed-
ings of the 2011 Conference on Empirical Methods
for Natural Language Processing (EMNLP 2011),
pages 1611?1622.
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Kishore Papineni, Slim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), pages 311?318.
Franois Portet, Ehud Reiter, Jim Hunter, and Somaya-
julu Sripada. 2007. Automatic generation of tex-
tual summaries from neonatal intensive care data. In
In Proccedings of the 11th Conference on Artificial
Intelligence in Medicine (AIME 07). LNCS, pages
227?236.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, and Jin
Yu. 2005. Choosing words in computer-generated
weather forecasts. Artificial Intelligence, 167:137?
169.
Jacques Robin and Kathy McKeown. 1996. Empiri-
cally designing and evaluating a new revision-based
model for summary generation. Artificial Intelli-
gence, 85(1-2).
Kees van Deemter, Marie?t Theune, and Emiel Krahmer.
2005. Real vs. template-based natural language gen-
eration: a false opposition? Computational Linguis-
tics, 31(1):15?24.
Ian Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Techniques with Java Imple-
mentation (2nd Ed.). Morgan Kaufmann, San Fran-
cisco, CA.
182
