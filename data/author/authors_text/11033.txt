Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 25?30,
Columbus, June 2008. c?2008 Association for Computational Linguistics
A Re-examination on Features in Regression Based Approach to Auto-
matic MT Evaluation 
 
Shuqi Sun, Yin Chen and Jufeng Li 
School of Computer Science and Technology 
Harbin Institute of Technology, Harbin, China 
{sqsun, chenyin, jfli}@mtlab.hit.edu.cn 
 
Abstract 
Machine learning methods have been exten-
sively employed in developing MT evaluation 
metrics and several studies show that it can 
help to achieve a better correlation with hu-
man assessments. Adopting the regression 
SVM framework, this paper discusses the lin-
guistic motivated feature formulation strategy. 
We argue that ?blind? combination of avail-
able features does not yield a general metrics 
with high correlation rate with human assess-
ments. Instead, certain simple intuitive fea-
tures serve better in establishing the 
regression SVM evaluation model. With six 
features selected, we show evidences to sup-
port our view through a few experiments in 
this paper. 
1 Introduction 
The automatic evaluation of machine translation 
(MT) system has become a hot research issue in 
MT circle. Compared with the huge amount of 
manpower cost and time cost of human evaluation, 
the automatic evaluations have lower cost and re-
usability. Although the automatic evaluation met-
rics have succeeded in the system level, there are 
still on-going investigations to get reference trans-
lation better (Russo-Lassner et al, 2005) or to deal 
with sub-document level evaluation (Kulesza et al, 
2004; Leusch et al 2006). 
N-grams? co-occurrence based metrics such as 
BLEU and NIST can reach a fairly good correla-
tion with human judgments, but due to their con-
sideration for the capability of generalization 
across multiple languages, they discard the inher-
ent linguistic knowledge of the sentence evaluated. 
Actually, for a certain target language, one could 
exploit this knowledge to help us developing a 
more ?human-like? metric. Gim?nez and M?rquez 
(2007) showed that compared with metrics limited 
in lexical dimension, metrics integrating deep lin-
guistic information will be more reliable. 
The introduction of machine learning methods 
aimed at the improvement of MT evaluation met-
rics? precision is a recent trend. Corston-Oliver et 
al. (2001) treated the evaluation of MT outputs as 
classification problem between human translation 
and machine translation. Kulesza et al (2004) pro-
posed a SVM classifier based on confidence score, 
which takes the distance between feature vector 
and the decision surface as the measure of the MT 
system?s output. Joshua S. Albrecht et al (2007) 
adopted regression SVM to improve the evaluation 
metric. 
In the rest of this paper, we will first discuss 
some pitfalls of the n-gram based metrics such as 
BLEU and NIST, together with the intuition that 
factors from the linguist knowledge can be used to 
evaluate MT system?s outputs. Then, we will pro-
pose a MT evaluation metric based on SVM re-
gression using information from various linguistic 
levels (lexical level, phrase level, syntax level and 
sentence-level) as features. Finally, from empirical 
studies, we will show that this metric, with less 
simple linguistic motivated features, will result in a 
better correlation with human judgments than pre-
vious regression-based methods. 
2 N-gram Based vs Linguistic Motivated 
Metrics 
N-gram co-occurrence based metrics is the main 
trend of MT evaluation. The basic idea is to com-
pute the similarity between MT system output and 
25
several human reference translations through the 
co-occurrence of n-grams. BLEU (Papineni et al, 
2002) is one of the most popular automatic evalua-
tion metrics currently used. Although with a good 
correlation with human judgment, it still has some 
defects: 
? BLEU considers precision regardless of recall. 
To avoid a low recall, BLEU introduces a brevity 
penalty factor, but this is only an approximation.  
? Though BLEU makes use of high order n-
grams to assess the fluency of a sentence, it does 
not exploit information from inherent structures of 
a sentence. 
? BLEU is a ?perfect matching only? metric. 
This is a serious problem. Although it can be alle-
viated by adding more human reference transla-
tions, there may be still a number of informative 
words that will be labeled as ?unmatched?. 
? BLEU lacks models determining each n-
gram?s own contribution to the meaning of the sen-
tence. Correct translations of the headwords which 
express should be attached more importance to 
than that of accessory words e.g. 
? While computing geometric average of preci-
sions from unigram to n-gram, if a certain preci-
sion is zero, the whole score will be zero.  
In the evaluation task of a MT system with cer-
tain target language, the intuition is that we can 
fully exploit linguistic information, making the 
evaluation progress more ?human-like? while leav-
ing the capability of generalization across multiple 
languages (just the case that BLEU considers) out 
of account. 
Following this intuition, from the plentiful lin-
guist information, we take the following factors in 
to consideration: 
? Content words are important to the semantic 
meaning of a sentence. A better translation will 
include more substantives translated from the 
source sentence than worse ones. In a similar way, 
a machine translation should be considered a better 
one, if more content words in human reference 
translations are included in it. 
? At the phrase level, the situation above re-
mains the same, and what is more, real phrases are 
used to measure the quality of the machine transla-
tions instead of merely using n-grams which are of 
little semantic information. 
? In addition, the length of translation is usually 
in good proportion to the source language. We be-
lieve that a human reference translation sentence 
has a moderate byte-length ratio to the source sen-
tence. So a machine translation will be depreciated 
if it has a ratio considerably different from the ratio 
calculated from reference sentences. 
? Finally, a good translation must be a ?well-
formed? sentence, which usually brings a high 
probability score in language models, e.g. n-gram 
model. 
In the next section, using regression SVM, we 
will build a MT evaluation metric for Chinese-
English translation with features selected from 
above aspects. 
3 A Regression SVM Approach Based on 
Linguistic Motivated Features 
Introducing machine learning methods to establish 
MT evaluation metric is a recent trend. Provided 
that we could get many factors of human judg-
ments, machine learning will be a good method to 
combine these factors together. As proved in the 
recent literature, learning from regression is of a 
better quality than from classifier (Albrecht and 
Hwa, 2007; Russo-Lassner et al, 2005; Quirk, 
2004). In this paper, we choose regression support 
vector machine (SVM) as the learning model.  
3.1 Learning from human assessment data 
The machine translated sentences for model train-
ing are provided with human assessment data score 
together with several human references. Each sen-
tence is treated as a training example. We extract 
feature vectors from training examples, and human 
assessment score will act as the output of the target 
function. The regression SVM will generate an 
approximated function which maps multi-
dimensional feature vectors to a continuous real 
value with a minimal error rate according to a loss 
function. This value is the result of the evaluation 
process.  
Figure 1 shows our general framework for re-
gression based learning, in which we train the 
SVM with a number of sentences x1, x2, ? with 
human assessment scores y1, y2, ? and use the 
trained model to evaluate an test sentence x with 
feature vector (f1, f2 ,?, fn). To determine which 
indicators of a sentence are chosen as features is 
research in progress, but we contend that ?the more 
features, the better quality? is not always true. 
Large feature sets require more computation cost, 
though maybe result in a metric with a better corre-
26
lation with human judgments, it can also be 
achieved by introducing a much smaller feature set. 
Moreover, features may conflict with each others, 
and bring down the performance of the metric. We 
will show this in the next section, using less than 
10 features stated in section 3.2. Some details of 
the implementation will also be described.  
Figure 1: SVM based model of automatic MT evalua-
tion metric 
3.2 Feature selection 
A great deal of information can be extracted from 
the MT systems? output using linguistic knowledge. 
Some of them can be very informative while easy 
to obtain.  
As considered in section 2, we choose factors 
from lexical level, phrase level, syntax level and 
sentence-level as features to train the SVM. 
? Features based on translation quality of con-
tent words 
The motivation is that content words are carry-
ing more important information of a sentence 
compared with function words. In this paper, con-
tent words include nouns, verbs, adjectives, adver-
bials, pronouns and cardinal numerals. The 
corresponding features are the precision of content 
words defined in Eq. 1 and the recall defined in Eq. 
2 where ref means reference translation. 
( )
# _ _ _ _
# _ _
conprecision t
correctly translated cons in t
cons in t
=         (1) 
( )
# _ _ _ _ _ _
# _ _ _
conrecall t
cons in ref correctly translated in t
cons in the ref
=     (2) 
? Features based on cognate words matching 
English words have plenty of morphological 
changes. So if a machine translation sentence 
shares with a human reference sentence some cog-
nates, it contains at least some basic information 
correct. And if we look at it in another way, words 
that do not match in the original text maybe match 
after morphological reduction. Thus, differences 
between poor translations will be revealed. Simi-
larly, we here define the content word precision 
and recall after morphological reduction in Eq. 3 
and Eq. 4 where mr_cons means content words 
after morphological reduction: 
_ ( )
# _ _ _ _ _
# _ _ _
mr conprecision t
correctly translated mr cons in t
mr cons in t
=   (3) 
_ ( )
# _ _ _ _ _ _ _
# _ _ _ _
mr conrecall t
mr cons in ref correctly translated in t
mr cons in the ref
=  (4) 
? Features based on translation quality of 
phrases 
Phrases are baring the weight of semantic in-
formation more than words. In manual evaluation, 
or rather, in a human?s mind, phrases are paid spe-
cial attention to. Here we parse every sentence1 and 
extract several types of phrases, then, compute the 
precision and recall of each type of phrase accord-
ing to Eq. 5 and Eq. 62: 
tinphrs
tinphrstranslatedcorrectly
tprecisionphr
__#
____#
)( =      (5) 
reftheinphr
tintranslatedcorrectlyrefinphr
trecallphr
___#
______#
)( =    (6) 
In practice, we found that if we compute these 
two indicators by matching phrases case-
insensitive, we will receive a metric with higher 
performance. We speculate that by doing this the 
difference between poor translations is revealed 
just like morphological reduction. 
? Features based on byte-length ratio 
Gale and Church (1991) noted that he byte-
length ratio of target sentence to source sentence is 
normally distributed. We employ this observation 
by computing the ratio of reference sentences to 
                                                 
1 The parser we used is proposed by Michael Collins in Col-
lins (1999). 
2 Only precision and recall of NP are used so far. Other types 
of phrase will be added in future study. 
Machine 
Translation Sentence 
Feature extraction 
x = (f1, f2 ,?, fn) 
 
Regression SVM 
 
 y = g(x) 
Assessment 
x2=(f1, f2 ,?, fn), y = y2
x1=(f1, f2 ,?, fn), y = y1
Training Set
?
27
source sentences, and then calculating the mean c 
and variance s of this ratio. So if we take the ratio r 
as a random variable, (r-c)/s has a normal distribu-
tion with mean 0 and variance 1. Then we compute 
the same ratio of machine translation sentence to 
source sentence, and take the output of p-norm 
function as a feature: 
)
__/__
()(
s
csrcoflengthtoflenght
Ptf norm
?=      (7) 
? Features based on parse score 
The usual practice to model the ?well-
formedness? of a sentence is to employ the n-gram 
language model or compute the syntactic structure 
similarity (Liu and Gildea 2005). However, the 
language model is widely adopted in MT, resulting 
less discrimination power. And the present parser 
is still not satisfactory, leading much noise in parse 
structure matching.  
To avoid these pitfalls in using LM and parser, 
here we notice that the score of a parse by the 
parser also reflects the quality of a sentence. It may 
be regarded as a syntactic based language model 
score as well as an approximate representation of 
parse structure. Here we introduce the feature 
based on parser?s score as: 
parserbygiventofmark
tscorepaser
_____
100
)(_
?
=            (8) 
4 Experiments 
We use SVM-Light (Joachims 1999) to train our 
learning models. Our main dataset is NIST?s 2003 
Chinese MT evaluations. There are 6?919=5514 
sentences generated by six systems together with 
human assessment data which contains a fluency 
score and adequacy score marked by two human 
judges. Because there is bias in the distributions of 
the two judges? assessment, we normalize the 
scores following Blatz et al (2003). The normal-
ized score is the average of the sum of the normal-
ized fluency score and the normalized adequacy 
score. 
To determine the quality of a metric, we use 
Spearman rank correlation coefficient which is 
distribution-independent between the score given 
to the evaluative data and human assessment data. 
The Spearman coefficient is a real number ranging 
from -1 to +1, indicating perfect negative correla-
tions or perfect positive correlations. We take the 
correlation rates of the metrics reported in Albrecht 
and Hwa (2007) and a standard automatic metric 
BLEU as a baseline comparison.  
Among the features described in section 3.2, we 
finally adopted 6 features: 
? Content words precision and recall after mor-
phological reduction defined in Eq. 3 and Eq. 4. 
? Noun-phrases? case insensitive precision and 
recall. 
? P-norm (Eq. 7) function?s output. 
? Rescaled parser score defined in Eq. 8. Our 
first experiment will compare the correlation rate 
between metric using rescaled parser score and that 
using parser score directly. 
4.1 Different kernels 
Intuitively, features and the resulting assessment 
are not in a linear correlation. We trained two 
SVM, one with linear kernel and the other with 
Gaussian kernel, using NIST 2003 Chinese dataset. 
Then we apply the two metrics on NIST 2002 Chi-
nese Evaluation dataset which has 3?878=2634 
sentences (3 systems total). The results are summa-
rized in Table 1. For comparison, the result from 
BLEU is also included. 
Feature Linear Gaussian  BLEU 
Rescale 0.320 0.329 
Direct 0.317 0.224 
0.244 
Table 1: Spearman rank-correlation coefficients for re-
gression based metrics using linear and Gaussian kernel, 
and using rescaled parser score or directly the parser 
score. Coefficient for BLEU is also involved. 
Table 1 shows that the metric with Gaussian 
kernel using rescaled parser score gains the highest 
correlation rate. That is to say, Gaussian kernel 
function can capture characteristics of the relation 
better, and rescaling the parser score can help to 
increase the correlation with human judgments. 
Moreover, as other features range from 0 to 1, we 
can discover in the second row of Table 1 that 
Gaussian kernel is suffering more seriously from 
the parser score which is ranging distinctly. In fol-
lowing experiments, we will adopt Gaussian kernel 
to train the SVM and rescaled parser score as a 
feature. 
4.2 Comparisons within the year 2003 
We held out 1/6 of the assessment dataset for pa-
rameter turning, and on the other 5/6 of dataset, we 
perform a five-fold cross validation to verify the 
metric?s performance. In comparison we introduce 
28
several metrics? coefficients reported in Albrecht 
and Hwa (2007) including smoothed BLEU (Lin 
and Och, 2004), METEOR (Banerjee and Lavie, 
2005), HWCM (Liu and Gildea 2005), and the me-
tric proposed in Albrecht and Hwa (2007) using 
the full feature set. The results are summarized in 
Table 2: 
Metric Coefficient
Our Metric 0.515 
Albrecht, 2007 0.520 
Smoothed BLEU 0.272 
METEOR 0.318 
HWCM 0.288 
Table 2: Comparison among various metrics. Learning-
based metrics are developed from NIST 2003 Chinese 
Evaluation dataset and tested under five-fold cross vali-
dation. 
Compared with reference based metrics such as 
BLEU, the regression based metrics yield a higher 
correlation rate. Generally speaking, for a given 
source sentence, there is usually a lot of feasible 
translations, but reference translations are always 
limited though this can be eased by adding refer-
ences. On the other hand, regression based metrics 
is independent of references and make the assess-
ment by mapping features to the score, so it can 
make a better judgment even dealing with a trans-
lation that doesn?t match the reference well.  
We can also see that our metric which uses only 
6 features can reach a pretty high correlation rate 
which is close to the metric proposed in Albrecht 
and Hwa (2007) using 53 features. That confirms 
our speculation that a small feature set can also 
result in a metric having a good correlation with 
human judgments. 
4.3 Crossing years  
Though the training set and test set in the experi-
ment described above are not overlapping, in the 
last, they come from the same dataset (NIST 2003). 
The content of this dataset are Xinhua news and 
AFC news from Jan. 2003 to Feb. 2003 which has 
an inherent correlation. To test the capability of 
generalization of our metric, we trained a metric on 
the whole NIST 2003 Chinese dataset (20% data 
are held out for parameter tuning) and applied it 
onto NIST 2002 Chinese Evaluation dataset. We 
use the same metrics introduced in section 4.2 for 
comparison. The results are summarized in Table 3: 
 
Metric Coefficient 
Our Metric 0.329 
Albrecht, 2007 0.309 
Smoothed BLEU 0.269 
METEOR 0.290 
HWCM 0.260 
Table 3: Cross year experiment result. All the learning 
based metrics are developed from NIST 2003.  
The content of NIST 2002 Chinese dataset is 
Xinhua news and Zaobao?s online news from Mar. 
2002 to Apr. 2002. The most remarkable character-
istic of news is its timeliness. News come from the 
year 2002 are nearly totally unrelated to that from 
the year 2003. It can be seen from Table 3 that we 
have got the expected results. Our metric can gen-
eralize well across years and yields a better corre-
lation with human judgments.  
4.4 Discussions 
Albrecht and Hwa (2007) and this paper both 
adopted a regression-based learning method. In 
fact, the preliminary experiment is strictly set ac-
cording to their paper. The most distinguishing 
difference is that the features in Albrecht and Hwa 
(2007) are collections of existing automatic evalua-
tion metrics. The total 53 features are computa-
tionally heavy (for the features from METEOR, 
ROUGE, HWCM and STM). In comparison, our 
metric made use of six features coming from lin-
guistic knowledge which can be easily obtained. 
Moreover, the experiments show that our metric 
can reach a correlation with human judgments 
nearly as good as the metric described in Albrecht 
and Hwa (2007), with a much lower computation 
cost. And when we applied it to a different year?s 
dataset, its correlation rate is much better than that 
of the metric from Albrecht and Hwa (2007), 
showing us a good capability of generalization. 
To account for this, we deem that the regression 
model is not resistant to data overfiting. If pro-
vided too much cross-dependent features for a lim-
ited training data, the model is prone to a less 
generalized result. But, it is difficult in practice to 
locate those key features in human perception of 
translation quality because we are lack of explicit 
evidences on what human actually use in transla-
tion evaluation. In such cases, this paper uses only 
?simple feature in key linguistic aspects?, which 
reduces the risk of overfitting and bring a more 
generalized regression results. 
29
Compared with the literature, the ?byte-length 
ratio between source and translation? and the 
?parse score? are original in automatic MT evalua-
tion modeling. The parse score is proved to be a 
good alternative to LM. And it helps to avoid the 
errors of parser in parse structure (the experiment 
to verify this claim is still on-going). 
It should be noted that feature selection is ac-
complished by empirically exhaustive test on the 
combination of the candidate features. In future 
work, we will test if this strategy will help to get 
better results for MT evaluation, e.g. try-on the 
selection between the 53 features in Albrecht and 
Hwa (2007). And, we will also test to see if lin-
guistic motivated feature augmentation would 
bring further benefit. 
5 Conclusion 
For the metrics based on regressing, it is not al-
ways true that more features and complex features 
will help in performance. If we choose features 
elaborately, simple features are also effective. In 
this paper we proposed a regression based metric 
with a considerably small feature set that yield per-
formance of the same level to the metrics with a 
large set of 53 features. And the experiment of the 
cross-year validation proves that our metric bring a 
more generalized evaluation results by correlating 
with human judgments better. 
Acknowledgements 
This research is support by Natural Science Foun-
dation of China (Grant No. 60773066) and Na-
tional 863 Project (Grant No. 2006AA01Z150) 
References 
Joshua S. Albrecht and Rebecca Hwa. 2007. A Re-
examination of Machine Learning Approaches for 
Sentence-Level MT Evaluation. In Proceedings of 
the 45th Annual Meeting of the Association of Com-
putational Linguistics , pages 880-887, Prague, 
Czech Republic, June. 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summarization 
at the Association for Computational Linguistics 
Conference 2005: 65-73. Ann Arbor, Michigan. 
John Blatz, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto San-
chis, and Nicola Ueffing. 2003. Confidence estima-
tion for machine translation. In Technical Report 
Natural Language Engineering Workshop Final Re-
port, pages 97-100, Johns Hopkins University. 
Simon Corston-Oliver, Michael Gamon, and Chris 
Brockett. 2001. A machine learning approach to the 
automatic evaluation of machine translation. In Pro-
ceedings of the 39th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 140-147, 
Toulouse, France, July. 
W. Gale and K. W. Church. 1991. A Program for Align-
ing Sentences in Bilingual Corpora. In Proceedings 
of the 29th Annual Meeting of the Association for 
Computational Linguistics, pages 177-184, Berkeley. 
Jes?s Gim?nez and Llu?s M?rquez. 2007. Linguistic 
Features for Automatic Evaluation of Heterogenous 
MT Systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 256-
264, Prague, Czech Republic, June. 
Thorsten Joachims. 1999. Making large-scale SVM 
learning practical. In Bernhard Sch?elkopf, Christo-
pher Burges, and Alexander Smola, editors, Ad-
vances in Kernel Methods - Support Vector Learning. 
MIT Press. 
Alex Kulesza and Stuart M. Shieber. 2004. A learning 
approach to improving sentence-level MT evaluation. 
In Proceedings of the 10th International Conference 
on Theoretical and Methodological Issues in Ma-
chine Translation (TMI), pages 75-84, Baltimore, 
MD, October. 
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 
2006. CDER: Efficient MT evaluation using block 
movements. In The Proceedings of the Thirteenth 
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 241-248. 
Chin-Yew Lin & Franz Josef Och. 2004. Automatic 
Evaluation of Machine Translation Quality Using 
Longest Common Subsequence and Skip-Bigram 
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics, 
pages 606-613, Barcelona, Spain, July. 
Ding Liu and Daniel Gildea. 2005. Syntactic features 
for evaluation of machine translation. In ACL 2005 
Workshop on Intrinsic and Extrinsic Evaluation 
Measures for Machine Translation and/or Summari-
zation, pages 25-32, June. 
Christopher B. Quirk. 2004. Training a Sentence-Level 
Machine Translation Confidence Measure, In Pro-
ceedings of LREC 2004, pages 825-828. 
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 
2005. A Paraphrase-Based Approach to Machine 
Translation Evaluation. In Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Univer-
sity of Maryland, College Park, August. 
30
Coling 2010: Poster Volume, pages 1203?1210,
Beijing, August 2010
Utilizing Variability of Time and Term Content, within and across 
Users in Session Detection 
Shuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao1 
1Harbin Institute of Technology, 2Heilongjiang Institute of Technology 
{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
haoliang.qi@gmail.com 
Abstract 
In this paper, we describe a SVM classi-
fication framework of session detection 
task on both Chinese and English query 
logs. With eight features on the aspects 
of temporal and content information ex-
tracted from pairs of successive queries, 
the classification models achieve signifi-
cantly superior performance than the stat-
of-the-art method. Additionally, we find 
through ROC analysis that there exists 
great discrimination power variability 
among different features and within the 
same feature across different users. To 
fully utilize this variability, we build lo-
cal models for individual users and com-
bine their predictions with those from the 
global model. Experiments show that the 
local models do make significant im-
provements to the global model, although 
the amount is small. 
1 Introduction 
To provide users better experiences of search 
engines, inspecting users? activities and inferring 
users? interests are indispensible. Query logs rec-
orded by search engines serves well for these 
purposes. Query log conveys the user interest 
information in the form of slices of the query 
stream. Thus the task of session detection con-
sists in distinguishing slice that corresponds to a 
user interest from other ones, and thus this paper, 
we adopt the definition of a session following 
(Jansen et al, 2007): 
(A session is) a series of interactions by the us-
er toward addressing a single information need. 
This definition is equivalent to that of the 
?search goal? proposed by Jones and Klinkner 
(2008), which corresponds to an atomic infor-
mation need, resulting in one or more queries.  
This paper adopts a classification point of 
view to the task of session detection (Jones and 
Klinkner, 2008). Given a pair of successive que-
ries in a query log, we examine it in various 
viewpoints (i.e. features) such as time proximity 
and similarity of the content of the two queries to 
determine whether these two queries cross a bor-
der of a search session. In other words, we classi-
fy the gap between the two queries into two clas-
ses: session shift and session continuation. In 
practice, search goals in a search mission and 
different search missions could be intermingled, 
and increase the difficulty of correctly identify-
ing them. In this paper, we do not take this issue 
into account and simply treat all boundaries be-
tween intermingled search goals as session shifts. 
The chief advantage in this choice is that we will 
have the opportunity to make classification mod-
el working online without caching user?s queries 
that are pending to be assigned to a session. 
Various studies built accurate models in pre-
dicting session boundaries and in distinguishing 
intermingled sessions, and they are summarized 
in Section 2. However, none of these works ana-
lyzed the contribution of individual features from 
a user-oriented viewpoint, or evaluated a fea-
ture?s discrimination power in a general scenario 
independent of its usage, as this paper does by 
conducting ROC analyses. During these analyses, 
we found that the discrimination power of fea-
tures varies dramatically, and for different users, 
the discrimination power of a particular feature 
also does not remain constant.  
Thus, it is appealing to build local models for 
users with have sufficient size of training exam-
ples, and combine the local models? predictions 
with those made by the global model trained by 
the whole training data. However, few of previ-
1203
ous works build user-specific models for the sake 
of characterizing the variability in user?s search 
activities, except that of Murray et al (2006). To 
fully make use of these two aspects of variability, 
inspired by Murray et al, we build users? local 
models based on a much broader range of evi-
dences, and show that different local models vary 
to a great extent, and experiments show that the 
local models do make significant improvements 
to the global model, although the amount is small. 
The remainder of this paper is organized as 
follows: Section 2 summarizes the related work 
of the session detection task. In Section 3, we 
first describe our classification framework as 
well as the features utilized. Then we conduct 
various evaluations on both English and Chinese 
query logs. Section 4 introduces the approaches 
to building local models based on an analysis of 
the variability of the discrimination power of 
features, and combine predictions of local mod-
els with those of the global model. Section 5 dis-
cusses the experimental results and concludes 
this paper. 
2 Related Work 
The simplest method in session detection is 
defining a timeout threshold and marking any 
time gaps of successive queries that exceed the 
threshold as session shifts. The thresholds 
adopted in different studies were significantly 
different, ranging from 5 minutes to 30 minutes 
(Silverstein et al, 1999; He and G?ker, 2000; 
Radlinski and Joachims, 2005; Downey et al, 
2007). Other study suggested adopting a dynamic 
timeout threshold. Murray et al (2006) proposed 
a user-centered hierarchical agglomerative 
clustering algorithm to determine timeout 
threshold for each user dynamically, other than 
setting a fixed threshold. However, Jones and 
Klinkner (2008) pointed out that single timeout 
criterion is always of limited utility, whatever its 
length is, and incorporating timeout features with 
other various features achieved satisfactory 
classification accuracy.  
An effective approach to combining the time 
out features with various evidences for session 
detection is machine learning. He et al (2002) 
collected statistical information from human an-
notated query logs to predict the probability a 
?New? pattern indicates a session shift according 
to the time gap between successive queries. 
?zmutlu and colleagues re-examined He et al?s 
work, and explored other machine learning tech-
niques such as neural networks, multiple linear 
regression, Monte Carlo simulation, conditional 
probabilities (Gayo-Avello, 2009), and HMMs 
(?zmutlu, 2009). 
In recent studies, Jones and Klinkner (2008) 
built logistic regression models to identify search 
goals and missions, and tackled the intermingled 
search goal/mission issue by examining arbitrary 
pairs of queries in the query log. Another contri-
bution of Jones and Klinkner is that they made a 
thorough analysis of contributions of individual 
features. However, they explored the features? 
contributions from a feature selection point of 
view rather than from a user-oriented one, and 
thus failed to characterize the variability of the 
discrimination power of the features when ap-
plied to different users. 
3 Learning to Detect Session Shifts 
3.1 Feature Extraction 
We adopt eight features covering both the tem-
poral and the content aspect of pairs of succes-
sive queries. Most these features are commonly 
used by previous studies (He and G?ker, 2000; 
?zmutlu, 2006; Jones and Klinkner, 2008). 
However, in this paper, we will analyze their 
contributions to the resulted model in a quite dif-
ferent way from that in previous works. 
Let Q = (q1, q2, ? , qn) denote a query log.  
The features are extracted from every successive 
pair of queries (qi, qi+1). Table 1 summarizes the 
features we adopt. The normalization described 
in Table1 is done according to the type of the 
feature. Features describing characters are nor-
malized by the average length of the two queries, 
while those describing character-n-grams are 
normalized by the average size of the n-gram sets 
of the two queries. Character-n-grams (e.g. bi-
grams ?ca? and ?at? in ?cat?) are robust to dif-
ferent representations of the same topic (e.g. ?IR? 
as Information Retrieval) and typos (e.g. 
?speling? as ?spelling?), and serve as a simple 
stemming method. In practice, character-n-grams 
are accumulative, which means they consist of 
all m-grams with m ? n. 
The feature ?avg_ngram_distance?, a variant 
of the ?lexical distance? in (Gayo-Avello, 2009), 
is more complicated than to be described briefly. 
1204
Here we first define n-gram distance (ND) from 
qi to qj, which is formalized as follows: 
j
ji
ji n
n
ND
qin   gram--char. of #
qin occur   qin   gram--char. of #
1)qq( ?=?  
Note that character-n-grams are accumulative 
and there could be multiple occurrences of a 
character-n-gram in a query, so the number of a 
character-n-gram is the sum of that of all m-
grams with m ? n, and multiple occurrences are 
all considered. At last, the average of character-
n-gram distance (ACD) of the pair (qi, qi+1) is:  
2
)qq()qq(
)q,q( 111
iiii
ii
NDND
ACD
?+?
=
++
+
 
There are seven features describing the content 
aspect of a query pair, and they are more or less 
overlapped (e.g. edit_distance vs. common_char). 
However, we show in the next subsection that all 
these features are beneficial to the final perfor-
mance.  
Feature Description 
time_interval time interval between 
successive queries 
avg_ngram_ 
distance 
avg. of character-n-gram 
distances 
edit_disance normalized Levenshtein 
edit distance 
common_prefix normalized length of pre-
fix shared 
common_suffix normalized length of suf-
fix shared 
common_char normalized number of 
characters shared 
common_ngram normalized number of 
character-n-grams shared 
Jaccard_ngram Jaccard distance between 
character-n-gram sets 
Table 1. Features used in classification models 
3.2 Data Preparation 
The query logs we explored include an English 
search log tracked by AOL from Mar 1, 2006 to 
May, 31 2006 (Pass et al, 2006), and a Chinese 
search log tracked by Sogou.com, which is one 
of the major Chinese Search Engines, from Mar 
1, 2007 to Mar 31, 20071. We applied systematic 
sampling over the user space on the two logs, 
which yielded 223 users and 2809 users, corre-
sponding to 6407 and 6917 query instances re-
                                                 
1 http://www.sogou.com/labs/resources.html 
spectively2. Sampling over the user space instead 
of over the query space avoids the bias to the 
most active users who submit much more queries 
than average users. 
For each sampled dataset, we invited annota-
tors who are familiar with IR and search process 
to determine each pair of successive queries of 
interest is across the border of a session. We 
made trivial pre-split process under two rules: 
 Queries from different users are not in the 
same session. 
 Queries from different days are not in the 
same session.  
Table 2 shows some basic statistics of the an-
notated data set. During the annotation process, 
the annotators were guided to identify the user?s 
information need at the finest granularity ever 
possible, because we focus on the atomic infor-
mation needs as described in Section 1. Conse-
quently, the average numbers of queries in a ses-
sion in both query logs are lower than previous 
studies. 
 AOL log Sogou log 
Queries 6407 6917 
Sessions 4571 5726 
Queries per session 1.40 1.21 
Longest session 21 12 
Table 2. Summary of the annotation results in 
both query logs 
3.3 Learning Framework 
In this section we seek to build accurate global 
classification model based on the whole training 
data obtained in the previous sub-subsection for 
both the query logs. We built the models within 
SVM framework. The implementation of SVM 
we used is libSVM (Chang and Lin, 2001). For 
the sake of evaluations and of model integration 
in the next section, we set the prediction of SVM 
to be probability estimation of the test example 
being positive. All features were pre-scaled into 
[0, 1] interval. We adopted the polynomial kernel, 
and for both datasets, we exhaustively tried each 
of the subset of the eight features using 5-fold 
cross validation. We found that using all the 
eight features yielded the best classification ac-
curacy. Thus in the experiments in rest of this 
                                                 
2 The sampling schema and sample size was deter-
mined following (Gayo-Avello, 2009). 
1205
section and the next section, we adopt the entire 
feature set to build global classification models. 
There is one parameter to be determined for 
feature extraction: the length of character-n-
grams. The proper lengths on AOL log and 
Sogou log are different. We tried the length from 
1 to 9, and according to cross validation accuracy, 
we found the best lengths for the two logs as 6 
and 3 respectively. 
3.4 Experimental Results 
3.4.1 Baseline Methods 
We provide two base line methods for compari-
sons. The first method is the commonly used 
timeout methods. We tried different timeout 
thresholds from 5 minutes to 30 minutes with a 
step of 5 minutes, and found that for both query 
logs the 5 minutes? threshold yield the best over-
all performance.  
The second method achieved the best perfor-
mance on the AOL log (Gayo-Avello, 2009), 
which addresses the session detection problem 
using a geometric interpolation method, in com-
parison to previous studies on this query log. We 
re-implemented this method and evaluated it on 
both the datasets. Similarly, the best parameters 
for the two query logs are different, such as the 
length of a character-n-gram. We only report the 
performance with the best parameter settings. 
3.4.2 Analyzing the Performance  
We analyze the performance of the SVM models 
according to precision, recall, F1-mean and F1.5-
mean of predictions on session shift and continu-
ation against human annotation data. 
The F

-mean is defined as: 
RP
PR
+
+
=
2
2)1(
mean-F ?
?
?  
where P denotes precision and R denotes recall. 
He et al (2002) regards recall more important 
than precision, and set the value of   in F

-mean 
to 1.5. We also report performance under this 
measure. 
In addition to traditional precision / recall 
based measures, we also perform ROC (Receiver 
Operating Characteristic) analysis to determine 
the discrimination power of different methods. 
The best merit of ROC analysis is that given a 
reference set, which is usually the human annota-
tion results, it evaluates a set of indicator?s dis-
crimination power for arbitrary binary classifica-
tion problem independent of the critical value 
with which the class predictions are made.  
Specifically, in the context session detection, 
regardless of the critical value that splits the clas-
sifier outputs into positive ones and negative 
ones (e.g. the 5-minutes? timeout threshold and 
50% probability in SVM?s output), the ROC 
analysis provides the overall discrimination pow-
er evaluation of the output set of a certain meth-
od (by trying to set each output value as the criti-
cal value). For the baseline method by Gayo-
Avello, the core of the decision heuristics also 
had a critical value to be determined. For details, 
readers could refer to (Gayo-Avello, 2009).  
3.4.3 Precision, Recall, and F-means 
Before we examine the discrimination power of 
each session detection method?s output independ-
ent of the threshold value selected. In this sub-
subsection, we begin with a more traditional eval-
uation schema: setting a proper threshold to pro-
duce binary predictions. It is straightforward to set 
the threshold for SVM method to 50%, and as 
described in sub-subsection 3.1.1, the threshold 
for timeout method is 5 minutes. The threshold of 
Gayo-Avello?s method is implied in its heuristics. 
Table 3 and Table 4 show the experimental re-
sults on AOL log and Sogou log respectively. 
For each dataset, we performed 1000-times boot-
strap resampling, generating 1000 bootstrapped 
datasets with the same size as the original dataset. 
To test the statistical significance of performance 
differences, we adopted Wilcoxon signed-rank 
test on the performance measures computed from 
the 1000 bootstrapped dataset, and found com-
parisons between each pair of methods were all 
significant at 95% level. 
The results show that SVM method clearly 
outperforms the baseline methods, and timeout 
method performs poorly. It may be argued that 
the poor performance of timeout method is due 
to the improper threshold value chosen. In this 
case, the ROC analysis, which assesses the dis-
crimination power of a method?s output set inde-
pendent of the threshold value chosen, is more 
suitable for performance evaluation. 
Gayo-Avello method significantly outperforms 
the timeout method. But due to its heuristic na-
ture, it is less likely to do better than the super-
vised-learning methods, although it avoids the 
over fitting issue. The Gayo-Avello method?s 
unstable performance in predicting session con-
1206
tinuations implies that its heuristics did not gen-
eralize well to Chinese query logs. 
 Timeout Gayo-Avello SVM 
P 
shift 75.92 89.35 90.96 
cont. 63.05 85.32 92.06 
R 
shift 64.49 87.85 93.82 
cont. 74.77 87.08 88.50 
F1 
shift 69.74 88.60 92.37 
cont. 68.41 86.19 90.25 
F1.5 
shift 67.62 88.31 92.92 
cont. 70.72 86.53 89.57 
Table 3. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on AOL dataset.  
 Timeout Gayo-Avello SVM 
P 
shift 67.75 75.10 87.53 
cont. 52.82 83.51 81.62 
R 
shift 59.52 91.44 86.17 
cont. 61.53 58.84 83.33 
F1 
shift 63.37 82.47 86.85 
cont. 56.84 69.04 82.47 
F1.5 
shift 61.83 85.71 86.59 
cont. 58.56 64.72 82.80 
Table 4. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on Sogou dataset. 
3.4.4 ROC Analysis 
By setting certain threshold value, we analyzed 
the three method?s performance using precision / 
recall based measures. In this sub-subsection, we 
try to set each value in an output set as the 
threshold value, and evaluate the discrimination 
power of methods by the area under the ROC 
curve. 
Figure 1 shows the ROC curves of the SVM 
method and the two baseline methods: timeout 
and Gayo-Avello, for predicting session shifts. 
ROC curves for predicting session continuations 
are symmetric with respect to the reference line, 
so we omit them in the rest of this paper for the 
sake of space limit.  
The results show that SVM method clearly 
outperforms the baseline methods in the prospec-
tive of discrimination power, with ROC area 
0.9562 on AOL dataset and 0.9154 on Sogou 
dataset. The curves of the two baseline methods 
are clearly under that of SVM method. This 
means baseline methods can never achieve accu-
racy as high as SVM method w.r.t. a fixed false 
alarm (classification error) rate, nor false alarm 
rate as low as SVM method w.r.t. a fixed accura-
cy rate. Again, Gayo-Avello method significantly 
outperforms timeout method, while underper-
forms the SVM method. For the question in the 
previous sub-subsection, coinciding with previ-
ous studies (Murray et al, 2006; Jones and 
Klinkner, 2008), applying single timeout thresh-
old always yields limited discrimination power, 
wherever the operating point on ROC curve (i.e. 
threshold value) is set. 
4 Making Use of the Variability of Dis-
crimination Power 
In this section, we first analyze the amount of 
contribution that each feature makes and show 
that the contribution, i.e. the discrimination pow-
er of each feature varies dramatically across dif-
ferent users. Then, we propose an approach to 
making use of this variability. Finally through 
experimental results, we show that the proposed 
approach makes small, yet significant improve-
ments to the SVM method in Section 3. 
4.1 Variability of Discrimination Power 
The ROC analysis of individual feature provides 
adequate characterizations of the discrimination 
power of the feature. Another advantage of 
adopting ROC analysis is that the results are in-
dependent not only of the critical value, but also 
of the scale of the feature values.  
Figure 2 shows the ROC curves of all the eight 
features in both datasets. Note that some features 
are with a higher value indicating session contin-
uation rather than session shift, so their ROC 
curves are below the reference line. The feature 
?time_interval? behaves exactly the same as the 
timeout method in Figure 1. For the rest of the 
features, ?avg_ngram_distance?, ?common_ngram? 
and ?Jaccard_ngram? achieve the best discrimi-
nation powers, showing the character-n-gram 
representation is effective. The feature ?com-
mon_char? performs significantly better in 
Sogou dataset than in AOL dataset, because Chi-
nese characters convey much more information 
than English characters do. ?common_suffix? 
performing worse than ?common_prefix? reflects 
the custom of users. Users tend to add terms at 
the end of the query in a searching iteration, thus 
predicting session continuations by examining 
the common suffixes is problematic. 
1207
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.7707
Gayo-Avello ROC area: 0.9130
SVM ROC area: 0.9562
Reference
AOL
    
0.
00
0.
25
0.
50
1.
00
0.
75
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.6365
Gayo-Avello ROC area: 0.8463
SVM ROC area: 0.9154
Reference
Sogou
 
Figure 1. ROC analysis of SVM method and two baseline methods for predicting session shifts on 
both AOL and Sogou dataset. All comparisons between ROC areas within the same dataset are at 
least 95% statistically significant, because the corresponding confidence intervals do not overlap. 
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.7707
avg_ngram_distance ROC area: 0.9560
edit_disance ROC area: 0.8848
common_prefix ROC area: 0.2177
common_suffix ROC area: 0.2985
common_char ROC area: 0.1360
common_ngram ROC area: 0.0480
Jaccard_ngram ROC area: 0.0464
Reference
AOL
  
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.6365
avg_ngram_distance ROC area: 0.9108
edit_disance ROC area: 0.8333
common_prefix ROC area: 0.2449
common_suffix ROC area: 0.3745
common_char ROC area: 0.0922
common_ngram ROC area: 0.1018
Jaccard_ngram ROC area: 0.0965
Reference
Sogou
 
Figure 2. ROC analysis of individual features for predicting session shifts on both AOL and Sogou 
dataset. Note that some curves with similar ROC area values overlap each other. 
In spite of the discrimination power a feature 
has, its behavior on different users is worth-
while to be examined. For selecting users that 
have sufficient data to draw stable conclusions, 
we consider only users who issued more than 50 
queries in the datasets. Unfortunately, there are 
too few users (6 users) qualified in Sogou da-
taset, so we show only the statistics of ROC 
area values of each of the features in Table 5 
based on 37 users in AOL dataset. 
The statistics in Table 5 show that for differ-
ent users. Recall that in sub-subsection 3.3.2, a 
0.04 difference of ROC area make the perfor-
mance of the SVM method significantly better 
1208
than that of the Gayo-Avello?s method. Thus, 
the discrimination power of a feature is likely to 
vary significantly, because all the standard de-
viations are at 0.03 or even higher level. Espe-
cially, the minimum and maximum values show 
that for these users, some of the findings above 
from the whole dataset do not hold. This implies 
that it is likely more feasible to build specific 
local models for these users to make full use of 
the variability within the same feature. 
Feature avg. sdev. min. max. 
time_interval 0.780 0.088 0.476 0.912
avg_ngram_ 
distance 
0.954 0.034 0.861 1.000
edit_disance 0.883 0.056 0.733 0.990
common_prefix 0.224 0.069 0.099 0.327
common_suffix 0.299 0.113 0.064 0.578
common_char 0.143 0.082 0.037 0.493
common_ngram 0.051 0.037 0.000 0.187
Jaccard_ngram 0.049 0.036 0.000 0.173
Table 5. Average, standard deviation, minimum, 
and maximum ROC areas of individual features 
4.2 Building Local Models 
We built individual local models for each user 
that issued more than 50 queries in AOL dataset. 
We also performed 5-fold cross validations and 
set the prediction to be the probability estima-
tion of a test example being positive. The fea-
ture selection process showed again that all the 
eight features are beneficial, and none of them 
should be excluded. 
In each fold of cross validation, we per-
formed 90%-bagging on the training set 10 
times to get the variance estimations of the local 
model. For each example in the test set, we set 
the final output on it to be the average of the 10 
outputs, and recorded the standard deviation of 
the outputs on this example which is used dur-
ing the model combination. We also conducted 
the same process for the global model for the 
sake of combination process described below. 
4.3 Combing with the Global Model 
Since the predictions of both the local and the 
global models are probability estimations, it is 
reasonable to combine them using linear combi-
nation. For each example, there are two outputs 
Ol and Og coming from local and global models 
accordingly. For each example e of a user?s sub 
dataset U, we have the outputs Ol(e) and Og(e) 
as well as the normalized deviations Dl(e) and 
Dg(e) (by the largest deviation in U of the corre-
sponding models). The final output O(e) is de-
fined as: 
)()(
)()()()(
)(
eDeD
eOeDeOeD
eO
gl
glgl
+
?+?
=
 
 Global Local Combine 
P 
shift 90.48 88.53 90.43 
cont. 91.75 92.12 92.52 
R 
shift 93.94 94.44 94.56 
cont. 87.20 84.16 87.04 
F1 
shift 92.18 91.39 92.45 
cont. 89.41 87.96 89.69 
F1.5 
shift 92.85 92.54 93.25 
cont. 88.55 86.46 88.65 
Table 6. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of global model (bagging), 
local model (bagging) and combined model  
This combination process is similar to (Osl et 
al., 2008). Note that the more the deviation of a 
model is, the less feasible the corresponding 
model is. We compared the performance of 
three models: global model, local model, and 
combined model. The results are summarized in 
Table 6. All comparisons between different 
models are statistically significant at 95% level, 
based on the same bootstrapping settings in sub-
subsection 3.4.3. The combined model shows 
slight (may due to the inferior performance of 
the local model), yet significant improvement to 
the global model. In spite of the amount of the 
improvement, the local model did correct some 
errors of the global model. It may be not ac-
ceptable to build such an expensive combined 
model for a limited improvement. Nevertheless, 
the results do show that the variability across 
different users is exploitable. 
5 Discussion and Conclusion 
In this paper, we built a learning framework of 
detecting sessions which corresponds to user?s 
interest in a query log. We considered two as-
pect of a pair of successive queries: temporal 
aspect and content aspect, and designed eight 
features based on these two aspects, and the 
SVM models built with these features achieved 
satisfactory performance (92.37% F1-mean on 
session shift, 90.25% F1-mean on session con-
tinuation), significantly better than the best-ever 
approach on AOL query log. 
1209
The analysis of the features? discrimination 
power was conducted not only among different 
features, but also within the same feature when 
applied to different users in the query log. By 
analyzing the statistics of ROC area values of 
each of the features based on 37 users in AOL 
dataset, experimental results showed that there 
is considerable variability in both these aspects. 
To make full use of this variability, we built 
local models for individual user and combine 
the yielded predictions with those yielded by the 
global model. Experiments showed that the lo-
cal model did make significant improvements to 
the global model, although the amount was 
small (92.45% vs. 92.18% F1-mean on session 
shift, 89.69% vs. 89.41% F1-mean on session 
continuation). 
In future studies, we will explore other learn-
ing frameworks which better integrate the local 
model and the global model, and will try to ac-
quire more data to build local models. We will 
also analyze more deeply the characteristics of 
ROC analysis in the feature selection process.  
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project (Grant 
No.2006AA010108). The authors are grateful 
for the anonymous reviewers for their valuable 
comments. 
References 
Chang Chih-Chung and Chih-Jen Lin. 2001. 
LIBSVM : a library for support vector machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Downey Doug, Susan Dumais, and Eric Horvitz. 
2007. Models of searching and brows-
ing: languages, studies, and applications. In Pro-
ceedings of the 20th international joint conference 
on Artificial intelligence, pages 2740-2747, Hy-
derabad, India. 
Gayo-Avello Daniel. 2009. A survey on session de-
tection methods in query logs and a proposal for 
future evaluation, Information Science 
179(12):1822-1843. 
He Daqing and Ayse G?ker. 2000. Detecting Session 
Boundaries from Web User Logs. In BCS/IRSG 
22nd Annual Colloqui-um on Information Re-
trieval Research, pages 57-66.  
He Daqing, Ayse G?ke, and David J. Harper. 2002. 
Combining evidence for automatic web session 
identification. Information Processing and Man-
agement: an International Journal, 38(5):727-742. 
Jansen Bernard J., Amanda Spink, Chris Blakely, 
and Sherry Koshman. 2007. Defining a session on 
Web search engines: Research Articles. Journal of 
the American Society for Information Science and 
Technology, 58(6):862-871 
Jones Rosie and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical 
segmentation of search topics in query logs. In 
Proceedings of the 17th ACM conference on In-
formation and knowledge management, pages 
699-708, Napa Valley, California, USA. 
Murray G. Craig, Jimmy Lin, and Abdur Chowdhury. 
2007. Identification of user sessions with hierar-
chical agglomerative clustering. American Society 
for Information Science and Technology, 43(1):1-
9. 
Osl Melanie, Christian Baumgartner, Bernhard Tilg, 
and Stephan Dreiseitl. 2008. On the combination 
of logistic regression and local probability esti-
mates. In Proceedings of Third International Con-
ference on Broadband Communications, Infor-
mation Technology & Biomedical Applications, 
pages 124-128. 
?zmutlu Seda. 2006. Automatic new topic identifi-
cation using multiple linear regression. Infor-
mation Processing and Management: an Interna-
tional Journal, 42(4):934-950. 
?zmutlu Huseyin C. 2009. Markovian analysis for 
automatic new topic identification in search en-
gine transaction logs. Applied Stochastic Models 
in Business and Industry, 25(6):737-768. 
Pass Greg, Abdur Chowdhury, and Cayley Torgeson. 
2006. A picture of search. In Proceedings of the 
1st international conference on Scalable infor-
mation systems, Hong Kong. 
Radlinski Filip and Thorsten Joachims. 2005. Query 
chains: learning to rank from implicit feedback. In 
Proceedings of the eleventh ACM SIGKDD inter-
national conference on Knowledge discovery in 
data mining, pages 239-248, Chicago, Illinois, 
USA. 
Silverstein Craig, Hannes Marais, Monika Henzinger, 
and Michael Moricz. 1999. Analysis of a very 
large web search engine query log. ACM SIGIR 
Forum, 33(1):6-12. 
1210
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Nonlinear Evidence Fusion and Propagation 
for Hyponymy Relation Mining 
 
Fan Zhang2*     Shuming Shi1     Jing Liu2     Shuqi Sun3*     Chin-Yew Lin1 
1Microsoft Research Asia 
2Nankai University, China 
3Harbin Institute of Technology, China 
{shumings, cyl}@microsoft.com 
 
 
 
Abstract 
This paper focuses on mining the hypon-
ymy (or is-a) relation from large-scale, 
open-domain web documents. A nonlinear 
probabilistic model is exploited to model 
the correlation between sentences in the 
aggregation of pattern matching results. 
Based on the model, we design a set of ev-
idence combination and propagation algo-
rithms. These significantly improve the 
result quality of existing approaches.  Ex-
perimental results conducted on 500 mil-
lion web pages and hypernym labels for 
300 terms show over 20% performance 
improvement in terms of P@5, MAP and 
R-Precision. 
1 Introduction1 
An important task in text mining is the automatic 
extraction of entities and their lexical relations; this 
has wide applications in natural language pro-
cessing and web search. This paper focuses on 
mining the hyponymy (or is-a) relation from large-
scale, open-domain web documents. From the 
viewpoint of entity classification, the problem is to 
automatically assign fine-grained class labels to 
terms. 
There have been a number of approaches 
(Hearst 1992; Pantel & Ravichandran 2004; Snow 
et al, 2005; Durme & Pasca, 2008; Talukdar et al, 
2008) to address the problem. These methods typi-
cally exploited manually-designed or automatical-
                                                          
* This work was performed when Fan Zhang and Shuqi Sun 
were interns at Microsoft Research Asia 
ly-learned patterns (e.g., ?NP such as NP?, ?NP 
like NP?, ?NP is a NP?). Although some degree of 
success has been achieved with these efforts, the 
results are still far from perfect, in terms of both 
recall and precision. As will be demonstrated in 
this paper, even by processing a large corpus of 
500 million web pages with the most popular pat-
terns, we are not able to extract correct labels for 
many (especially rare) entities. Even for popular 
terms, incorrect results often appear in their label 
lists. 
The basic philosophy in existing hyponymy ex-
traction approaches (and also many other text-
mining methods) is counting: count the number of 
supporting sentences. Here a supporting sentence 
of a term-label pair is a sentence from which the 
pair can be extracted via an extraction pattern. We 
demonstrate that the specific way of counting has a 
great impact on result quality, and that the state-of-
the-art counting methods are not optimal. Specifi-
cally, we examine the problem from the viewpoint 
of probabilistic evidence combination and find that 
the probabilistic assumption behind simple count-
ing is the statistical independence between the ob-
servations of supporting sentences. By assuming a 
positive correlation between supporting sentence 
observations and adopting properly designed non-
linear combination functions, the results precision 
can be improved. 
It is hard to extract correct labels for rare terms 
from a web corpus due to the data sparseness prob-
lem. To address this issue, we propose an evidence 
propagation algorithm motivated by the observa-
tion that similar terms tend to share common hy-
pernyms. For example, if we already know that 1) 
Helsinki and Tampere are cities, and 2) Porvoo is 
similar to Helsinki and Tampere, then Porvoo is 
1159
very likely also a city. This intuition, however, 
does not mean that the labels of a term can always 
be transferred to its similar terms. For example, 
Mount Vesuvius and Kilimanjaro are volcanoes 
and Lhotse is similar to them, but Lhotse is not a 
volcano. Therefore we should be very conservative 
and careful in hypernym propagation. In our prop-
agation algorithm, we first construct some pseudo 
supporting sentences for a term from the support-
ing sentences of its similar terms. Then we calcu-
late label scores for terms by performing nonlinear 
evidence combination based on the (pseudo and 
real) supporting sentences. Such a nonlinear prop-
agation algorithm is demonstrated to perform bet-
ter than linear propagation. 
Experimental results on a publicly available col-
lection of 500 million web pages with hypernym 
labels annotated for 300 terms show that our non-
linear evidence fusion and propagation significant-
ly improve the precision and coverage of the 
extracted hyponymy data. This is one of the tech-
nologies adopted in our semantic search and min-
ing system NeedleSeek2. 
In the next section, we discuss major related ef-
forts and how they differ from our work. Section 3 
is a brief description of the baseline approach. The 
probabilistic evidence combination model that we 
exploited is introduced in Section 4. Our main ap-
proach is illustrated in Section 5. Section 6 shows 
our experimental settings and results. Finally, Sec-
tion 7 concludes this paper. 
2 Related Work 
Existing efforts for hyponymy relation extraction 
have been conducted upon various types of data 
sources, including plain-text corpora (Hearst 1992; 
Pantel & Ravichandran, 2004; Snow et al, 2005; 
Snow et al, 2006; Banko, et al, 2007; Durme & 
Pasca, 2008; Talukdar et al, 2008), semi-
structured web pages (Cafarella  et al, 2008; Shin-
zato & Torisawa, 2004), web search results (Geraci 
et al, 2006; Kozareva et al, 2008; Wang & Cohen, 
2009), and query logs (Pasca 2010). Our target for 
optimization in this paper is the approaches that 
use lexico-syntactic patterns to extract hyponymy 
relations from plain-text corpora. Our future work 
will study the application of the proposed algo-
rithms on other types of approaches. 
                                                          
2 http://research.microsoft.com/en-us/projects/needleseek/ or 
http://needleseek.msra.cn/  
The probabilistic evidence combination model 
that we exploit here was first proposed in (Shi et 
al., 2009), for combining the page in-link evidence 
in building a nonlinear static-rank computation 
algorithm. We applied it to the hyponymy extrac-
tion problem because the model takes the depend-
ency between supporting sentences into 
consideration and the resultant evidence fusion 
formulas are quite simple. In (Snow et al, 2006), a 
probabilistic model was adopted to combine evi-
dence from heterogeneous relationships to jointly 
optimize the relationships. The independence of 
evidence was assumed in their model. In compari-
son, we show that better results will be obtained if 
the evidence correlation is modeled appropriately. 
Our evidence propagation is basically about us-
ing term similarity information to help instance 
labeling. There have been several approaches 
which improve hyponymy extraction with instance 
clusters built by distributional similarity. In (Pantel 
& Ravichandran, 2004), labels were assigned to 
the committee (i.e., representative members) of a 
semantic class and used as the hypernyms of the 
whole class. Labels generated by their approach 
tend to be rather coarse-grained, excluding the pos-
sibility of a term having its private labels (consid-
ering the case that one meaning of a term is not 
covered by the input semantic classes). In contrast 
to their method, our label scoring and ranking ap-
proach is applied to every single term rather than a 
semantic class. In addition, we also compute label 
scores in a nonlinear way, which improves results 
quality. In Snow et al (2005), a supervised ap-
proach was proposed to improve hypernym classi-
fication using coordinate terms. In comparison, our 
approach is unsupervised. Durme & Pasca (2008) 
cleaned the set of instance-label pairs with a 
TF*IDF like method, by exploiting clusters of se-
mantically related phrases. The core idea is to keep 
a term-label pair (T, L) only if the number of terms 
having the label L in the term T?s cluster is above a 
threshold and if L is not the label of too many clus-
ters (otherwise the pair will be discarded). In con-
trast, we are able to add new (high-quality) labels 
for a term with our evidence propagation method. 
On the other hand, low quality labels get smaller 
score gains via propagation and are ranked lower. 
Label propagation is performed in (Talukdar et 
al., 2008; Talukdar & Pereira, 2010) based on mul-
tiple instance-label graphs. Term similarity infor-
mation was not used in their approach. 
1160
Most existing work tends to utilize small-scale 
or private corpora, whereas the corpus that we used 
is publicly available and much larger than most of 
the existing work. We published our term sets (re-
fer to Section 6.1) and their corresponding user 
judgments so researchers working on similar topics 
can reproduce our results. 
 
Type Pattern 
Hearst-I NPL {,} (such as) {NP,}
* {and|or} NP  
Hearst-II 
NPL {,} (include(s) | including) {NP,}
* 
{and|or} NP 
Hearst-III NPL {,} (e.g.|e.g) {NP,}
* {and|or} NP 
IsA-I NP (is|are|was|were|being) (a|an) NPL 
IsA-II NP (is|are|was|were|being) {the, those} NPL 
IsA-III NP (is|are|was|were|being) {another, any} NPL 
Table 1. Patterns adopted in this paper (NP: named 
phrase representing an entity; NPL: label) 
3 Preliminaries 
The problem addressed in this paper is corpus-
based is-a relation mining: extracting hypernyms 
(as labels) for entities from a large-scale, open-
domain document corpus. The desired output is a 
mapping from terms to their corresponding hyper-
nyms, which can naturally be represented as a 
weighted bipartite graph (term-label graph). Typi-
cally we are only interested in top labels of a term 
in the graph. 
Following existing efforts, we adopt pattern-
matching as a basic way of extracting hyper-
nymy/hyponymy relations. Two types of patterns 
(refer to Table 1) are employed, including the pop-
ular ?Hearst patterns? (Hearst, 1992) and the IsA 
patterns which are exploited less frequently in ex-
isting hyponym mining efforts. One or more term-
label pairs can be extracted if a pattern matches a 
sentence. In the baseline approach, the weight of 
an edge T?L (from term T to hypernym label L) in 
the term-label graph is computed as, 
 w(T?L)      ( )       
   
    ( )
 (3.1) 
where m is the number of times the pair (T, L) is 
extracted from the corpus, DF(L) is the number of 
in-links of L in the graph, N is total number of 
terms in the graph, and IDF means the ?inverse 
document frequency?. 
A term can only keep its top-k neighbors (ac-
cording to the edge weight) in the graph as its final 
labels. 
Our pattern matching algorithm implemented in 
this paper uses part-of-speech (POS) tagging in-
formation, without adopting a parser or a chunker. 
The noun phrase boundaries (for terms and labels) 
are determined by a manually designed POS tag 
list. 
4 Probabilistic Label-Scoring Model 
Here we model the hyponymy extraction problem 
from the probability theory point of view, aiming 
at estimating the score of a term-label pair (i.e., the 
score of a label w.r.t. a term) with probabilistic 
evidence combination. The model was studied in 
(Shi et al, 2009) to combine the page in-link evi-
dence in building a nonlinear static-rank computa-
tion algorithm. 
We represent the score of a term-label pair by 
the probability of the label being a correct hyper-
nym of the term, and define the following events, 
AT,L: Label L is a hypernym of term T (the ab-
breviated form A is used in this paper unless it is 
ambiguous). 
Ei: The observation that (T, L) is extracted from 
a sentence Si via pattern matching (i.e., Si is a sup-
porting sentence of the pair). 
Assuming that we already know m supporting 
sentences (S1~Sm), our problem is to compute 
P(A|E1,E2,..,Em), the posterior probability that L is 
a hypernym of term T, given evidence E1~Em. 
Formally, we need to find a function f to satisfy, 
 P(A|E1,?,Em) = f(P(A), P(A|E1)?, P(A|Em) ) (4.1) 
For simplicity, we first consider the case of 
m=2. The case of m>2 is quite similar. 
We start from the simple case of independent 
supporting sentences. That is, 
  (     )   (  )   (  ) (4.2) 
  (       )   (    )   (    ) (4.3) 
By applying Bayes rule, we get, 
 
 (       )  
 (       )   ( )
 (     )
 
          
 (    )   ( )
 (  )
 
 (    )   ( )
 (  )
 
 
 ( )
 
          
 (    )   (    )
 ( )
 
(4.4) 
Then define 
 (   )     
 (   )
 ( )
     ( (   ))     ( ( )) 
1161
Here G(A|E) represents the log-probability-gain 
of A given E, with the meaning of the gain in the 
log-probability value of A after the evidence E is 
observed (or known). It is a measure of the impact 
of evidence E to the probability of event A. With 
the definition of G(A|E), Formula 4.4 can be trans-
formed to, 
  (       )   (    )   (    ) (4.5) 
Therefore, if E1 and E2 are independent, the log-
probability-gain of A given both pieces of evidence 
will exactly be the sum of the gains of A given eve-
ry single piece of evidence respectively. It is easy 
to prove (by following a similar procedure) that the 
above Formula holds for the case of m>2, as long 
as the pieces of evidence are mutually independent. 
Therefore for a term-label pair with m mutually 
independent supporting sentences, if we set every 
gain G(A|Ei) to be a constant value g, the posterior 
gain score of the pair will be ?         . If the 
value g is the IDF of label L, the posterior gain will 
be, 
 G(AT,L|E1?,Em) ?    ( )
 
         ( ) (4.6) 
This is exactly the Formula 3.1. By this way, we 
provide a probabilistic explanation of scoring the 
candidate labels for a term via simple counting. 
 
 Hearst-I IsA-I 
E1: Hearst-I 
E2: IsA-I 
RA: 
 (      )
 (    ) (    )
  66.87 17.30 24.38 
R: 
 (    )
 (  ) (  )
  5997 1711 802.7 
RA/R 0.011 0.010 0.030 
Table 2. Evidence dependency estimation for intra-
pattern and inter-pattern supporting sentences 
In the above analysis, we assume the statistical 
independence of the supporting sentence observa-
tions, which may not hold in reality. Intuitively, if 
we already know one supporting sentence S1 for a 
term-label pair (T, L), then we have more chance to 
find another supporting sentence than if we do not 
know S1. The reason is that, before we find S1, we 
have to estimate the probability with the chance of 
discovering a supporting sentence for a random 
term-label pair. The probability is quite low be-
cause most term-label pairs do not have hyponymy 
relations. Once we have observed S1, however, the 
chance of (T, L) having a hyponymy relation in-
creases. Therefore the chance of observing another 
supporting sentence becomes larger than before. 
Table 2 shows the rough estimation of 
 (      )
 (    ) (    )
 (denoted as RA), 
 (    )
 (  ) (  )
 (denoted 
as R), and their ratios. The statistics are obtained 
by performing maximal likelihood estimation 
(MLE) upon our corpus and a random selection of 
term-label pairs from our term sets (see Section 
6.1) together with their top labels3. The data veri-
fies our analysis about the correlation between E1 
and E2 (note that R=1 means independent). In addi-
tion, it can be seen that the conditional independ-
ence assumption of Formula 4.3 does not hold 
(because RA>1). It is hence necessary to consider 
the correlation between supporting sentences in the 
model. The estimation of Table 2 also indicates 
that, 
 
 (     )
 (  ) (  )
 
 (       )
 (    ) (    )
 (4.7) 
By following a similar procedure as above, with 
Formulas 4.2 and 4.3 replaced by 4.7, we have, 
  (       )   (    )   (    ) (4.8) 
This formula indicates that when the supporting 
sentences are positively correlated, the posterior 
score of label L w.r.t. term T (given both the sen-
tences) is smaller than the sum of the gains caused 
by one sentence only. In the extreme case that sen-
tence S2 fully depends on E1 (i.e. P(E2|E1)=1), it is 
easy to prove that 
  (       )   (    )  
It is reasonable, since event E2 does not bring in 
more information than E1. 
Formula 4.8 cannot be used directly for compu-
ting the posterior gain. What we really need is a 
function h satisfying 
  (         )   ( (    )    (    )) (4.9) 
and 
  (      )  ?   
 
     (4.10) 
Shi et al (2009) discussed other constraints to h 
and suggested the following nonlinear functions, 
   (      )    (  ? ( 
    )    )  (4.11) 
                                                          
3 RA is estimated from the labels judged as ?Good?; whereas 
the estimation of R is from all judged labels. 
1162
   (      )  ??   
  
   
 
           (p>1) (4.12) 
In the next section, we use the above two h func-
tions as basic building blocks to compute label 
scores for terms. 
5 Our Approach 
Multiple types of patterns (Table 1) can be adopted 
to extract term-label pairs. For two supporting sen-
tences the correlation between them may depend 
on whether they correspond to the same pattern. In 
Section 5.1, our nonlinear evidence fusion formu-
las are constructed by making specific assumptions 
about the correlation between intra-pattern sup-
porting sentences and inter-pattern ones. 
Then in Section 5.2, we introduce our evidence 
propagation technique in which the evidence of a 
(T, L) pair is propagated to the terms similar to T. 
5.1 Nonlinear evidence fusion 
For a term-label pair (T, L), assuming K patterns 
are used for hyponymy extraction and the support-
ing sentences discovered with pattern i are, 
                  (5.1) 
where mi is the number of supporting sentences 
corresponding to pattern i. Also assume the gain 
score of Si,j is xi,j, i.e., xi,j=G(A|Si,j). 
Generally speaking, supporting sentences corre-
sponding to the same pattern typically have a high-
er correlation than the sentences corresponding to 
different patterns. This can be verified by the data 
in Table-2. By ignoring the inter-pattern correla-
tions, we make the following simplified assump-
tion: 
Assumption: Supporting sentences correspond-
ing to the same pattern are correlated, while those 
of different patterns are independent. 
According to this assumption, our label-scoring 
function is, 
      (   )  ? (               )
 
   
 (5.2) 
In the simple case that         ( ) , if the h 
function of Formula 4.12 is adopted, then, 
      (   )  (? ?  
 
 
   
)     ( ) (5.3) 
We use an example to illustrate the above for-
mula. 
Example: For term T and label L1, assume the 
numbers of the supporting sentences corresponding 
to the six pattern types in Table 1 are (4, 4, 4, 4, 4, 
4), which means the number of supporting sen-
tences discovered by each pattern type is 4. Also 
assume the supporting-sentence-count vector of 
label L2 is (25, 0, 0, 0, 0, 0). If we use Formula 5.3 
to compute the scores of L1 and L2, we can have 
the following (ignoring IDF for simplicity), 
Score(L1)   ?    ; Score(L2) ?     
One the other hand, if we simply count the total 
number of supporting sentences, the score of L2 
will be larger. 
The rationale implied in the formula is: For a 
given term T, the labels supported by multiple 
types of patterns tend to be more reliable than 
those supported by a single pattern type, if they 
have the same number of supporting sentences. 
5.2 Evidence propagation 
According to the evidence fusion algorithm de-
scribed above, in order to extract term labels relia-
bly, it is desirable to have many supporting 
sentences of different types. This is a big challenge 
for rare terms, due to their low frequency in sen-
tences (and even lower frequency in supporting 
sentences because not all occurrences can be cov-
ered by patterns). With evidence propagation, we 
aim at discovering more supporting sentences for 
terms (especially rare terms). Evidence propaga-
tion is motivated by the following two observa-
tions: 
(I) Similar entities or coordinate terms tend to 
share some common hypernyms. 
(II) Large term similarity graphs are able to be 
built efficiently with state-of-the-art techniques 
(Agirre et al, 2009; Pantel et al, 2009; Shi et al, 
2010). With the graphs, we can obtain the similari-
ty between two terms without their hypernyms be-
ing available. 
The first observation motivates us to ?borrow? 
the supporting sentences from other terms as auxil-
iary evidence of the term. The second observation 
means that new information is brought with the 
state-of-the-art term similarity graphs (in addition 
to the term-label information discovered with the 
patterns of Table 1). 
1163
Our evidence propagation algorithm contains 
two phases. In phase I, some pseudo supporting 
sentences are constructed for a term from the sup-
porting sentences of its neighbors in the similarity 
graph. Then we calculate the label scores for terms 
based on their (pseudo and real) supporting sen-
tences. 
Phase I: For every supporting sentence S and 
every similar term T1 of the term T, add a pseudo 
supporting sentence S1 for T1, with the gain score, 
  (         )       (    )   (      ) (5.5) 
where         is the propagation factor, and 
   (   ) is the term similarity function taking val-
ues in [0, 1]. The formula reasonably assumes that 
the gain score of the pseudo supporting sentence 
depends on the gain score of the original real sup-
porting sentence, the similarity between the two 
terms, and the propagation factor. 
Phase II: The nonlinear evidence combination 
formulas in the previous subsection are adopted to 
combine the evidence of pseudo supporting sen-
tences. 
Term similarity graphs can be obtained by dis-
tributional similarity or patterns (Agirre et al, 
2009; Pantel et al, 2009; Shi et al, 2010). We call 
the first type of graph DS and the second type PB. 
DS approaches are based on the distributional hy-
pothesis (Harris, 1985), which says that terms ap-
pearing in analogous contexts tend to be similar. In 
a DS approach, a term is represented by a feature 
vector, with each feature corresponding to a con-
text in which the term appears. The similarity be-
tween two terms is computed as the similarity 
between their corresponding feature vectors. In PB 
approaches, a list of carefully-designed (or auto-
matically learned) patterns is exploited and applied 
to a text collection, with the hypothesis that the 
terms extracted by applying each of the patterns to 
a specific piece of text tend to be similar. Two cat-
egories of patterns have been studied in the litera-
ture (Heast 1992; Pasca 2004; Kozareva et al, 
2008; Zhang et al, 2009): sentence lexical patterns, 
and HTML tag patterns. An example of sentence 
lexical patterns is ?T {, T}*{,} (and|or) T?. HTML 
tag patterns include HTML tables, drop-down lists, 
and other tag repeat patterns. In this paper, we 
generate the DS and PB graphs by adopting the 
best-performed methods studied in (Shi et al, 
2010). We will compare, by experiments, the prop-
agation performance of utilizing the two categories 
of graphs, and also investigate the performance of 
utilizing both graphs for evidence propagation. 
6 Experiments 
6.1 Experimental setup 
Corpus We adopt a publicly available dataset in 
our experiments: ClueWeb094. This is a very large 
dataset collected by Carnegie Mellon University in 
early 2009 and has been used by several tracks of 
the Text Retrieval Conference (TREC)5. The whole 
dataset consists of 1.04 billion web pages in ten 
languages while only those in English, about 500 
million pages, are used in our experiments. The 
reason for selecting such a dataset is twofold: First, 
it is a corpus large enough for conducting web-
scale experiments and getting meaningful results. 
Second, since it is publicly available, it is possible 
for other researchers to reproduce the experiments 
in this paper. 
Term sets Approaches are evaluated by using 
two sets of selected terms: Wiki200, and Ext100. 
For every term in the term sets, each approach 
generates a list of hypernym labels, which are 
manually judged by human annotators. Wiki200 is 
constructed by first randomly selecting 400 Wik-
ipedia6 titles as our candidate terms, with the prob-
ability of a title T being selected to be     (  
 ( )), where F(T) is the frequency of T in our data 
corpus. The reason of adopting such a probability 
formula is to balance popular terms and rare ones 
in our term set. Then 200 terms are manually se-
lected from the 400 candidate terms, with the prin-
ciple of maximizing the diversity of terms in terms 
of length (i.e., number of words) and type (person, 
location, organization, software, movie, song, ani-
mal, plant, etc.). Wiki200 is further divided into 
two subsets: Wiki100H and Wiki100L, containing 
respectively the 100 high-frequency and low-
frequency terms. Ext100 is built by first selecting 
200 non-Wikipedia-title terms at random from the 
term-label graph generated by the baseline ap-
proach (Formula 3.1), then manually selecting 100 
terms. 
Some sample terms in the term sets are listed in 
Table 3. 
 
                                                          
4 http://boston.lti.cs.cmu.edu/Data/clueweb09/  
5 http://trec.nist.gov/  
6 http://www.wikipedia.org/  
1164
Term 
Set 
Sample Terms 
Wiki200 
Canon EOS 400D, Disease management, El Sal-
vador, Excellus Blue Cross Blue Shield, F33, 
Glasstron, Indium, Khandala, Kung Fu, Lake 
Greenwood, Le Gris, Liriope, Lionel Barrymore, 
Milk, Mount Alto, Northern Wei, Pink Lady, 
Shawshank, The Dog Island, White flight, World 
War II? 
Ext100 
A2B, Antique gold, GPTEngine, Jinjiang Inn, 
Moyea SWF to Apple TV Converter, Nanny ser-
vice, Outdoor living, Plasmid DNA, Popon, Spam 
detection, Taylor Ho Bynum, Villa Michelle? 
Table 3. Sample terms in our term sets 
 
Annotation For each term in the term set, the 
top-5 results (i.e., hypernym labels) of various 
methods are mixed and judged by human annota-
tors. Each annotator assigns each result item a 
judgment of ?Good?, ?Fair? or ?Bad?. The annota-
tors do not know the method by which a result item 
is generated. Six annotators participated in the la-
beling with a rough speed of 15 minutes per term. 
We also encourage the annotators to add new good 
results which are not discovered by any method. 
The term sets and their corresponding user anno-
tations are available for download at the following 
links (dataset ID=data.queryset.semcat01): 
http://research.microsoft.com/en-us/projects/needleseek/ 
http://needleseek.msra.cn/datasets/ 
Evaluation We adopt the following metrics to 
evaluate the hypernym list of a term generated by 
each method. The evaluation score on a term set is 
the average over all the terms. 
Precision@k: The percentage of relevant (good 
or fair) labels in the top-k results (labels judged as 
?Fair? are counted as 0.5) 
Recall@k: The ratio of relevant labels in the top-
k results to the total number of relevant labels 
R-Precision: Precision@R where R is the total 
number of labels judged as ?Good? 
Mean average precision (MAP): The average of 
precision values at the positions of all good or fair 
results 
Before annotation and evaluation, the hypernym 
list generated by each method for each term is pre-
processed to remove duplicate items. Two hyper-
nyms are called duplicate items if they share the 
same head word (e.g., ?military conflict? and ?con-
flict?). For duplicate hypernyms, only the first (i.e., 
the highest ranked one) in the list is kept. The goal 
with such a preprocessing step is to partially con-
sider results diversity in evaluation and to make a 
more meaningful comparison among different 
methods. Consider two hypernym lists for ?sub-
way?: 
List-1: restaurant; chain restaurant; worldwide chain 
restaurant; franchise; restaurant franchise? 
List-2: restaurant; franchise; transportation; company; 
fast food? 
There are more detailed hypernyms in the first 
list about ?subway? as a restaurant or a franchise; 
while the second list covers a broader range of 
meanings for the term. It is hard to say which is 
better (without considering the upper-layer appli-
cations). With this preprocessing step, we keep our 
focus on short hypernyms rather than detailed ones. 
 
Term Set Method MAP R-Prec P@1 P@5 
Wiki200 
Linear 0.357 0.376 0.783 0.547 
Log 
0.371 
 3.92% 
0.384 
 2.13% 
0.803 
 2.55% 
0.561 
 2.56% 
PNorm 
0.372 
 4.20% 
0.384 
 2.13% 
0.800 
 2.17% 
0.562 
 2.74% 
Wiki100H 
Linear 0.363 0.382 0.805 0.627 
Log 
0.393 
 8.26% 
0.402 
 5.24% 
0.845 
 4.97% 
0.660 
 5.26% 
PNorm 
0.395 
 8.82% 
0.403 
 5.50% 
0.840 
 4.35% 
0.662 
 5.28% 
Table 4. Performance comparison among various 
evidence fusion methods (Term sets: Wiki200 and 
Wiki100H; p=2 for PNorm) 
6.2 Experimental results 
We first compare the evaluation results of different 
evidence fusion methods mentioned in Section 4.1. 
In Table 4, Linear means that Formula 3.1 is used 
to calculate label scores, whereas Log and PNorm 
represent our nonlinear approach with Formulas 
4.11 and 4.12 being utilized. The performance im-
provement numbers shown in the table are based 
on the linear version; and the upward pointing ar-
rows indicate relative percentage improvement 
over the baseline. From the table, we can see that 
the nonlinear methods outperform the linear ones 
on the Wiki200 term set. It is interesting to note 
that the performance improvement is more signifi-
cant on Wiki100H, the set of high frequency terms. 
By examining the labels and supporting sentences 
for the terms in each term set, we find that for 
many low-frequency terms (in Wiki100L), there 
are only a few supporting sentences (corresponding 
1165
to one or two patterns). So the scores computed by 
various fusion algorithms tend to be similar. In 
contrast, more supporting sentences can be discov-
ered for high-frequency terms. Much information 
is contained in the sentences about the hypernyms 
of the high-frequency terms, but the linear function 
of Formula 3.1 fails to make effective use of it. 
The two nonlinear methods achieve better perfor-
mance by appropriately modeling the dependency 
between supporting sentences and computing the 
log-probability gain in a better way. 
The comparison of the linear and nonlinear 
methods on the Ext100 term set is shown in Table 
5. Please note that the terms in Ext100 do not ap-
pear in Wikipedia titles. Thanks to the scale of the 
data corpus we are using, even the baseline ap-
proach achieves reasonably good performance. 
Please note that the terms (refer to Table 3) we are 
using are ?harder? than those adopted for evalua-
tion in many existing papers. Again, the results 
quality is improved with the nonlinear methods, 
although the performance improvement is not big 
due to the reason that most terms in Ext100 are 
rare. Please note that the recall (R@1, R@5) in this 
paper is pseudo-recall, i.e., we treat the number of 
known relevant (Good or Fair) results as the total 
number of relevant ones. 
 
Method MAP R-Prec P@1 P@5 R@1 R@5 
Linear 0.384 0.429 0.665 0.472 0.116 0.385 
Log 
0.395 0.429 0.715 0.472 0.125 0.385 
 2.86%  0%  7.52%  0%  7.76%  0% 
PNorm 
0.390 0.429 0.700 0.472 0.120 0.385 
 1.56%  0%   5.26%  0%  3.45%  0% 
Table 5. Performance comparison among various 
evidence fusion methods (Term set: Ext100; p=2 
for PNorm) 
The parameter p in the PNorm method is related 
to the degree of correlations among supporting 
sentences. The linear method of Formula 3.1 corre-
sponds to the special case of p=1; while p=  rep-
resents the case that other supporting sentences are 
fully correlated to the supporting sentence with the 
maximal log-probability gain. Figure 1 shows that, 
for most of the term sets, the best performance is 
obtained for   [2.0, 4.0]. The reason may be that 
the sentence correlations are better estimated with 
p values in this range. 
 
 
Figure 1. Performance curves of PNorm with dif-
ferent parameter values (Measure: MAP) 
The experimental results of evidence propaga-
tion are shown in Table 6. The methods for com-
parison are, 
Base: The linear function without propagation. 
NL: Nonlinear evidence fusion (PNorm with 
p=2) without propagation. 
LP: Linear propagation, i.e., the linear function 
is used to combine the evidence of pseudo support-
ing sentences. 
NLP: Nonlinear propagation where PNorm 
(p=2) is used to combine the pseudo supporting 
sentences. 
NL+NLP: The nonlinear method is used to 
combine both supporting sentences and pseudo 
supporting sentences. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL 
0.372 0.384 0.800 0.562 0.325 
 4.20%  2.13%  2.17%  2.74%  2.52% 
LP 
0.357 0.376 0.783 0.547 0.317 
 0%  0%  0%  0%  0% 
NLP 
0.396 0.418 0.785 0.605 0.357 
 10.9%  11.2%  0.26%  10.6%  12.6% 
NL+NLP 
0.447 0.461 0.840 0.667 0.404 
 25.2%  22.6%  7.28%  21.9%  27.4% 
Table 6. Evidence propagation results (Term set: 
Wiki200; Similarity graph: PB; Nonlinear formula: 
PNorm) 
In this paper, we generate the DS (distributional 
similarity) and PB (pattern-based) graphs by adopt-
ing the best-performed methods studied in (Shi et 
al., 2010). The performance improvement numbers 
(indicated by the upward pointing arrows) shown 
in tables 6~9 are relative percentage improvement 
1166
over the base approach (i.e., linear function with-
out propagation). The values of parameter   are set 
to maximize the MAP values. 
Several observations can be made from Table 6. 
First, no performance improvement can be ob-
tained with the linear propagation method (LP), 
while the nonlinear propagation algorithm (NLP) 
works quite well in improving both precision and 
recall. The results demonstrate the high correlation 
between pseudo supporting sentences and the great 
potential of using term similarity to improve hy-
pernymy extraction. The second observation is that 
the NL+NLP approach achieves a much larger per-
formance improvement than NL and NLP. Similar 
results (omitted due to space limitation) can be 
observed on the Ext100 term set. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.357 0.376 0.783 0.547 0.317 
NL+NLP 
(PB) 
0.415 0.439 0.830 0.633 0.379 
 16.2%  16.8%  6.00%  15.7%  19.6% 
NL+NLP 
(DS) 
0.456 0.469 0.843 0.673 0.406 
 27.7%  24.7%  7.66%  23.0%  28.1% 
NL+NLP
(PB+DS) 
0.473 0.487 0.860 0.700 0.434 
 32.5%  29.5%  9.83%  28.0%  36.9% 
Table 7. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki200; Nonlin-
ear formula: Log) 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.351 0.370 0.760 0.467 0.317 
NL+NLP 
(PB) 
0.411 0.448 0.770 0.564 0.401 
?17.1% ?21.1% ?1.32% ?20.8% ?26.5% 
NL+NLP 
(DS) 
0.469 0.490 0.815 0.622 0.438 
 33.6%  32.4%  7.24%  33.2%  38.2% 
NL+NLP
(PB+DS) 
0.491 0.513 0.860 0.654 0.479 
 39.9%  38.6%  13.2%  40.0%  51.1% 
Table 8. Combination of PB and DS graphs for 
evidence propagation (Term set: Wiki100L) 
Now let us study whether it is possible to com-
bine the PB and DS graphs to obtain better results. 
As shown in Tables 7, 8, and 9 (for term sets 
Wiki200, Wiki100L, and Ext100 respectively, us-
ing the Log formula for fusion and propagation), 
utilizing both graphs really yields additional per-
formance gains. We explain this by the fact that the 
information in the two term similarity graphs tends 
to be complimentary. The performance improve-
ment over Wiki100L is especially remarkable. This 
is reasonable because rare terms do not have ade-
quate information in their supporting sentences due 
to data sparseness. As a result, they benefit the 
most from the pseudo supporting sentences propa-
gated with the similarity graphs. 
 
Method MAP R-Prec P@1 P@5 R@5 
Base 0.384 0.429 0.665 0.472 0.385 
NL+NLP 
(PB) 
0.454 0.479 0.745 0.550 0.456 
 18.3%  11.7%  12.0%  16.5%  18.4% 
NL+NLP 
(DS) 
0.404 0.441 0.720 0.486 0.402 
 5.18%  2.66%  8.27%  2.97%  4.37% 
NL+NLP(P
B+DS) 
0.483 0.518 0.760 0.586 0.492 
 26.0%  20.6%  14.3%  24.2%  27.6% 
Table 9. Combination of PB and DS graphs for 
evidence propagation (Term set: Ext100) 
7 Conclusion 
We demonstrated that the way of aggregating sup-
porting sentences has considerable impact on re-
sults quality of the hyponym extraction task using 
lexico-syntactic patterns, and the widely-used 
counting method is not optimal. We applied a se-
ries of nonlinear evidence fusion formulas to the 
problem and saw noticeable performance im-
provement. The data quality is improved further 
with the combination of nonlinear evidence fusion 
and evidence propagation. We also introduced a 
new evaluation corpus with annotated hypernym 
labels for 300 terms, which were shared with the 
research community. 
Acknowledgments 
We would like to thank Matt Callcut for reading 
through the paper. Thanks to the annotators for 
their efforts in judging the hypernym labels. 
Thanks to Yueguo Chen, Siyu Lei, and the anony-
mous reviewers for their helpful comments and 
suggestions. The first author is partially supported 
by the NSF of China (60903028,61070014), and 
Key Projects in the Tianjin Science and Technolo-
gy Pillar Program. 
 
 
 
 
1167
References  
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas-
ca, and A. Soroa. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-based 
Approaches. In Proc. of NAACL-HLT?2009. 
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead, 
and O. Etzioni. 2007. Open Information Extraction 
from the Web. In Proc. of IJCAI?2007. 
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. 
Zhang. 2008. WebTables: Exploring the Power of 
Tables on the Web. In Proceedings of the 34th Con-
ference on Very Large Data Bases (VLDB?2008), 
pages 538?549, Auckland, New Zealand. 
B. Van Durme and M. Pasca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of 
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence. 
F. Geraci, M. Pellegrini, M. Maggini, and F. Sebastiani. 
2006. Cluster Generation and Cluster Labelling for 
Web Snippets: A Fast and Accurate Hierarchical So-
lution. In Proceedings of the 13th Conference on 
String Processing and Information Retrieval 
(SPIRE?2006), pages 25?36, Glasgow, Scotland. 
Z. S. Harris. 1985. Distributional Structure. The Philos-
ophy of Linguistics. New York: Oxford University 
Press. 
M. Hearst. 1992. Automatic Acquisition of Hyponyms 
from Large Text Corpora. In Fourteenth International 
Conference on Computational Linguistics, Nantes, 
France. 
Z. Kozareva, E. Riloff, E.H. Hovy. 2008. Semantic 
Class Learning from the Web with Hyponym Pattern 
Linkage Graphs. In Proc. of ACL'2008. 
P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu and 
V. Vyas. 2009. Web-Scale Distributional Similarity 
and Entity Set Expansion. EMNLP?2009. Singapore. 
P. Pantel and D. Ravichandran. 2004. Automatically 
Labeling Semantic Classes. In Proc. of the 2004 Hu-
man Language Technology Conference (HLT-
NAACL?2004), 321?328. 
M. Pasca. 2004. Acquisition of Categorized Named 
Entities for Web Search. In Proc. of CIKM?2004. 
M. Pasca. 2010. The Role of Queries in Ranking La-
beled Instances Extracted from Text. In Proc. of 
COLING?2010, Beijing, China. 
S. Shi, B. Lu, Y. Ma, and J.-R. Wen. 2009. Nonlinear 
Static-Rank Computation. In Proc. of CIKM?2009, 
Kong Kong. 
S. Shi, H. Zhang, X. Yuan, J.-R. Wen. 2010. Corpus-
based Semantic Class Mining: Distributional vs. Pat-
tern-Based Approaches. In Proc. of COLING?2010, 
Beijing, China. 
K. Shinzato and K. Torisawa. 2004. Acquiring Hypon-
ymy Relations from Web Documents. In Proc. of the 
2004 Human Language Technology Conference 
(HLT-NAACL?2004). 
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning 
Syntactic Patterns for Automatic Hypernym Discov-
ery. In Proceedings of the 19th Conference on Neural 
Information Processing Systems. 
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic 
Taxonomy Induction from Heterogenous Evidence. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics 
(COLING-ACL-06), 801?808. 
P. P. Talukdar and F. Pereira. 2010. Experiments in 
Graph-based Semi-Supervised Learning Methods for 
Class-Instance Acquisition. In 48th Annual Meeting 
of the Association for Computational Linguistics 
(ACL?2010). 
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, 
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised 
Acquisition of Labeled Class Instances using Graph 
Random Walks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP?2008), pages 581?589. 
R.C. Wang. W.W. Cohen. Automatic Set Instance Ex-
traction using the Web. In Proc. of the 47th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-IJCNLP?2009), pages 441?449, Sin-
gapore. 
H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. 2009. Em-
ploying Topic Models for Pattern-based Semantic 
Class Discovery. In Proc. of the 47th Annual Meet-
ing of the Association for Computational Linguistics 
(ACL-IJCNLP?2009), pages 441?449, Singapore. 
 
1168
