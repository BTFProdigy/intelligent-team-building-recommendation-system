Discourse Parsing: Learning FOL Rules based on Rich Verb Semantic
Representations to automatically label Rhetorical Relations
Rajen Subba
Computer Science
University of Illinois
Chicago, IL, USA
rsubba@cs.uic.edu
Barbara Di Eugenio
Computer Science
University of Illinois
Chicago, IL, USA
bdieugen@cs.uic.edu
Su Nam Kim
Department of CSSE
University of Melbourne
Carlton, VIC, Australia
snkim@csse.unimelb.edu.au
Abstract
We report on our work to build a dis-
course parser (SemDP) that uses seman-
tic features of sentences. We use an In-
ductive Logic Programming (ILP) System
to exploit rich verb semantics of clauses
to induce rules for discourse parsing. We
demonstrate that ILP can be used to learn
from highly structured natural language
data and that the performance of a dis-
course parsing model that only uses se-
mantic information is comparable to that
of the state of the art syntactic discourse
parsers.
1 Introduction
The availability of corpora annotated with syntac-
tic information have facilitated the use of prob-
abilistic models on tasks such as syntactic pars-
ing. Current state of the art syntactic parsers
reach accuracies between 86% and 90%, as mea-
sured by different types of precision and recall
(for more details see (Collins, 2003)). Recent
semantic (Kingsbury and Palmer, 2002) and dis-
course (Carlson et al, 2003) annotation projects
are paving the way for developments in seman-
tic and discourse parsing as well. However unlike
syntactic parsing, significant development in dis-
course parsing remains at large.
Previous work on discourse parsing ((Soricut
and Marcu, 2003) and (Forbes et al, 2001))
have focused on syntactic and lexical features
only. However, discourse relations connect
clauses/sentences, hence, descriptions of events
and states. It makes linguistic sense that the
semantics of the two clauses ?generally built
around the semantics of the verbs, composed with
that of their arguments? affects the discourse re-
lation(s) connecting the clauses. This may be
even more evident in our instructional domain,
where relations derived from planning such as
Precondition-Act may relate clauses.
Of course, since semantic information is hard
to come by, it is not surprising that previous work
on discourse parsing did not use it, or only used
shallow word level ontological semantics as spec-
ified in WordNet (Polanyi et al, 2004). But when
rich sentence level semantics is available, it makes
sense to experiment with it for discourse parsing.
A second major difficulty with using such rich
verb semantic information, is that it is rep-
resented using complex data structures. Tradi-
tional Machine Learning methods cannot han-
dle highly structured data such as First Or-
der Logic (FOL), a representation that is suit-
ably used to represent sentence level seman-
tics. Such FOL representations cannot be reduced
to a vector of attribute/value pairs as the rela-
tions/interdependencies that exist among the pred-
icates would be lost.
Inductive Logic Programming (ILP) can learn
structured descriptions since it learns FOL de-
scriptions. In this paper, we present our first steps
using ILP to learn semantic descriptions of dis-
course relations. Also of relevance to the topic of
this workshop, is that discourse structure is inher-
ently highly structured, since discourse structure
is generally described in hierarchical terms: ba-
sic units of analysis, generally clauses, are related
by discourse relations, resulting in more complex
units, which in turn can be related via discourse re-
lations. At the moment, we do not yet address the
problem of parsing at higher levels of discourse.
We intend to build on the work we present in this
paper to achieve that goal.
The task of discourse parsing can be di-
vided into two disjoint sub-problems ((Soricut and
Marcu, 2003) and (Polanyi et al, 2004)). The two
sub-problems are automatic identification of seg-
ment boundaries and the labeling of rhetorical re-
lations. Though we consider the problem of auto-
matic segmentation to be an important part in dis-
course parsing, we have focused entirely on the
latter problem of automatically labeling rhetorical
33
Figure 1: SemDP System Architecture (Discourse Parser)
relations only. Our approach uses rich verb seman-
tics1 of elementary discourse units (EDUs)2 based
on VerbNet(Kipper et al, 2000) as background
knowledge and manually annotated rhetorical re-
lations as training examples. It is trained on a lot
fewer examples than the state of the art syntax-
based discourse parser (Soricut and Marcu, 2003).
Nevertheless, it achieves a comparable level of
performance with an F-Score of 60.24. Figure 1
shows a block diagram of SemDP?s system archi-
tecture. Segmentation, annotation of rhetorical re-
lations and parsing constitute the data collection
phase of the system. Learning is accomplished
using an ILP based system, Progol (Muggleton,
1995). As can be seen in Figure 1, Progol takes
as input both rich verb semantic information of
pairs of EDUs and the rhetorical relations between
them. The goal was to learn rules using the se-
mantic information from pairs of EDUs as in Ex-
ample 1:
(1) EDU1: ?Sometimes, you can add a liquid to the water
EDU2: ?to hasten the process?
relation(EDU1,EDU2,?Act:goal?).
to automatically label unseen examples with the
correct rhetorical relation.
The rest of the paper is organized as follows.
Section 2 describes our data collection methodol-
ogy. In section 3, Progol, the ILP system that we
1The semantic information we used is composed of Verb-
Net semantic predicates that capture event semantics as well
as thematic roles.
2EDUs are minimal discourse units produced as a result
of discourse segmentation.
used to induce rules for discourse parsing is de-
tailed. Evaluation results are presented in section
4 followed by the conclusion in section 5.
2 Data Collection
The lack of corpora annotated with both rhetorical
relations as well as sentence level semantic rep-
resentation led us to create our own corpus. Re-
sources such as (Kingsbury and Palmer, 2002) and
(Carlson et al, 2003) have been developed man-
ually. Since such efforts are time consuming and
costly, we decided to semi-automatically build our
annotated corpus. We used an existing corpus of
instructional text that is about 9MB in size and is
made up entirely of written English instructions.
The two largest components are home repair man-
uals (5Mb) and cooking recipes (1.7Mb). 3
Segmentation. The segmentation of the corpus
was done manually by a human coder. Our seg-
mentation rules are based on those defined in
(Mann and Thompson, 1988). For example, (as
shown in Example 2) we segment sentences in
which a conjunction is used with a clause at the
conjunction site.
(2) You can copy files (//) as well as cut messages.
(//) is the segmentation marker. Sentences are
segmented into EDUs. Not all the segmentation
3It was collected opportunistically off the internet and
from other sources, and originally assembled at the Informa-
tion Technology Research Institute, University of Brighton.
34
rules from (Mann and Thompson, 1988) are im-
ported into our coding scheme. For example, we
do not segment relative clauses. In total, our seg-
mentation resulted in 10,084 EDUs. The seg-
mented EDUs were then annotated with rhetorical
relations by the human coder4 and also forwarded
to the parser as they had to be annotated with se-
mantic information.
2.1 Parsing of Verb Semantics
We integrated LCFLEX (Rose? and Lavie, 2000),
a robust left-corner parser, with VerbNet (Kipper
et al, 2000) and CoreLex (Buitelaar, 1998). Our
interest in decompositional theories of lexical se-
mantics led us to base our semantic representation
on VerbNet.
VerbNet operationalizes Levin?s work and ac-
counts for 4962 distinct verbs classified into 237
main classes. Moreover, VerbNet?s strong syntac-
tic components allow it to be easily coupled with a
parser in order to automatically generate a seman-
tically annotated corpus.
To provide semantics for nouns, we use
CoreLex (Buitelaar, 1998), in turn based on the
generative lexicon(Pustejovsky, 1991). CoreLex
defines basic types such as art (artifact) or com
(communication). Nouns that share the same bun-
dle of basic types are grouped in the same System-
atic Polysemous Class (SPC). The resulting 126
SPCs cover about 40,000 nouns.
We modified and augmented LCFLEX?s exist-
ing lexicon to incorporate VerbNet and CoreLex.
The lexicon is based on COMLEX (Grishman et
al., 1994). Verb and noun entries in the lexicon
contain a link to a semantic type defined in the on-
tology. VerbNet classes (including subclasses and
frames) and CoreLex SPCs are realized as types in
the ontology. The deep syntactic roles are mapped
to the thematic roles, which are defined as vari-
ables in the ontology types. For more details on
the parser see (Terenzi and Di Eugenio, 2003).
Each of the 10,084 EDUs was parsed using the
parser. The parser generates both a syntactic tree
and the associated semantic representation ? for
the purpose of this paper, we only focus on the
latter. Figure 2 shows the semantic representation
generated for EDU1 from Example 1, ?sometimes,
you can add a liquid to the water?.
The semantic representation in Figure 2 is part
4Double annotation and segmentation is currently being
done to assess inter-annotator agreement using kappa.
(*SEM*
((AGENT YOU)
(VERBCLASS ((VNCLASS MIX-22.1-2))) (EVENT +)
(EVENT0
((END
((ARG1 (LIQUID))
(FRAME *TOGETHER) (ARG0 PHYSICAL)
(ARG2 (WATER)))))))
(EVENTSEM
((FRAME *CAUSE) (ARG1 E) (ARG0 (YOU)))))
(PATIENT1 LIQUID)
(PATIENT2 WATER)
(ROOT-VERB ADD))
Figure 2: Parser Output (Semantic Information)
of the F-Structure produced by the parser. The
verb add is parsed for a transitive frame with a PP
modifier that belongs to the verb class ?MIX-22.1-
2?. The sentence contains two PATIENTs, namely
liquid and water. you is identified as the AGENT
by the parser. *TOGETHER and *CAUSE are the
primitive semantic predicates used by VerbNet.
Verb Semantics in VerbNet are defined as events
that are decomposed into stages, namely start, end,
during and result. The semantic representation in
Figure 2 states that there is an event EVENT0 in
which the two PATIENTs are together at the end.
An independent evaluation on a set of 200 sen-
tences from our instructional corpus was con-
ducted. 5 It was able to generate complete parses
for 72.2% and partial parses for 10.9% of the verb
frames that we expected it to parse, given the re-
sources. The parser cannot parse those sentences
(or EDUs) that contain a verb that is not cov-
ered by VerbNet. This coverage issue, coupled
with parser errors, exacerbates the problem of data
sparseness. This is further worsened by the fact
that we require both the EDUs in a relation set
to be parsed for the Machine Learning part of our
work. Addressing data sparseness is an issue left
for future work.
2.2 Annotation of Rhetorical Relations
The annotation of rhetorical relations was done
manually by a human coder. Our coding scheme
builds on Relational Discourse Analysis (RDA)
(Moser and Moore, 1995), to which we made mi-
5The parser evaluation was not based on EDUs but rather
on unsegmented sentences. A sentence contained one or
more EDUs.
35
nor modifications; in turn, as far as discourse rela-
tions are concerned, RDA was inspired by Rhetor-
ical Structure Theory (RST) (Mann and Thomp-
son, 1988).
Rhetorical relations were categorized as infor-
mational, elaborational, temporal and others. In-
formational relations describe how contents in
two relata are related in the domain. These re-
lations are further subdivided into two groups;
causality and similarity. The former group con-
sists of relations between an action and other ac-
tions or between actions and their conditions or
effects. Relations like ?act:goal?, ?criterion:act?
fall under this group. The latter group con-
sists of relations between two EDUs according
to some notion of similarity such as ?restate-
ment? and ?contrast1:contrast2?. Elaborational
relations are interpropositional relations in which
a proposition(s) provides detail relating to some
aspect of another proposition (Mann and Thomp-
son, 1988). Relations like ?general:specific? and
?circumstance:situation? belong to this category.
Temporal relations like ?before:after? capture time
differences between two EDUs. Lastly, the cate-
gory others includes relations not covered by the
previous three categories such as ?joint? and ?inde-
terminate?.
Based on the modified coding scheme manual,
we segmented and annotated our instructional cor-
pus using the augmented RST tool from (Marcu et
al., 1999). The RST tool was modified to incor-
porate our relation set. Since we were only inter-
ested in rhetorical relations that spanned between
two adjacent EDUs 6, we obtained 3115 sets of
potential relations from the set of all relations that
we could use as training and testing data.
The parser was able to provide complete parses
for both EDUs in 908 of the 3115 relation sets.
These constitute the training and test set for Pro-
gol.
The semantic representation for the EDUs along
with the manually annotated rhetorical relations
were further processed (as shown in Figure 4) and
used by Progol as input.
3 The Inductive Logic Programming
Framework
We chose to use Progol, an Inductive Logic Pro-
gramming system (ILP), to learn rules based on
6At the moment, we are concerned with learning relations
between two EDUs at the base level of a Discourse Parse Tree
(DPT) and not at higher levels of the hierarchy.
the data we collected. ILP is an area of research
at the intersection of Machine Learning (ML) and
Logic Programming. The general problem speci-
fication in ILP is given by the following property:
B ?H |= E (3)
Given the background knowledge B and the ex-
amples E, ILP systems find the simplest consistent
hypothesis H, such that B and H entails E.
While most of the work in NLP that involves
learning has used more traditional ML paradigms
like decision-tree algorithms and SVMs, we did
not find them suitable for our data which is rep-
resented as Horn clauses. The requirement of us-
ing a ML system that could handle first order logic
data led us to explore ILP based systems of which
we found Progol most appropriate.
Progol combines Inverse Entailment with
general-to-specific search through a refinement
graph. A most specific clause is derived using
mode declarations along with Inverse Entailment.
All clauses that subsume the most specific clause
form the hypothesis space. An A*-like search
is used to search for the most probable theory
through the hypothesis space. Progol allows arbi-
trary programs as background knowledge and ar-
bitrary definite clauses as examples.
3.1 Learning from positive data only
One of the features we found appealing about Pro-
gol, besides being able to handle first order logic
data, is that it can learn from positive examples
alone.
Learning in natural language is a universal hu-
man process based on positive data only. How-
ever, the usual traditional learning models do not
work well without negative examples. On the
other hand, negative examples are not easy to ob-
tain. Moreover, we found learning from positive
data only to be a natural way to model the task of
discourse parsing.
To make the learning from positive data only
feasible, Progol uses a Bayesian framework. Pro-
gol learns logic programs with an arbitrarily low
expected error using only positive data. Of course,
we could have synthetically labeled examples of
relation sets (pairs of EDUs), that did not belong
to a particular relation, as negative examples. We
plan to explore this approach in the future.
A key issue in learning from positive data
only using a Bayesian framework is the ability
to learn complex logic programs. Without any
36
negative examples, the simplest rule or logic
program, which in our case would be a single
definite clause, would be assigned the highest
score as it captures the most number of examples.
In order to handle this problem, Progol?s scoring
function exercises a trade-off between the size of
the function and the generality of the hypothesis.
The score for a given hypothesis is calculated
according to formula 4.
ln p(H | E) = m ln
( 1
g(H)
)
?sz(H)+dm (4)
sz(H) and g(H) computes the size of the hy-
pothesis and the its generality respectively. The
size of a hypothesis is measured as the number
of atoms in the hypothesis whereas generality is
measured by the number of positive examples the
hypothesis covers. m is the number of examples
covered by the hypothesis and dm is a normaliz-
ing constant. The function ln p(H|E) decreases
with increases in sz(H) and g(H). As the number
of examples covered (m) grow, the requirements
on g(H) become even stricter. This property fa-
cilitates the ability to learn more complex rules
as they are supported by more positive examples.
For more information on Progol and the computa-
tion of Bayes? posterior estimation, please refer to
(Muggleton, 1995).
3.2 Discourse Parsing with Progol
We model the problem of assigning the correct
rhetorical relation as a classification task within
the ILP framework. The rich verb semantic repre-
sentation of pairs of EDUs, as shown in Figure 3 7,
form the background knowledge and the manually
annotated rhetorical relations between the pairs of
EDUs, as shown in Figure 4, serve as the positive
examples in our learning framework. The num-
bers in the definite clauses are ids used to identify
the EDUs.
Progol constructs logic programs based on the
background knowledge and the examples in Fig-
ures 3 and 4. Mode declarations in the Progol in-
put file determines which clause to be used as the
head (i.e. modeh) and which ones to be used in
the body (i.e. modeb) of the hypotheses. Figure 5
shows an abridged set of our mode declarations.
7The output from the parser was further processed into
definite clauses.
...
agent(97,you).
together(97,event0,end,physical,liquid,water).
cause(97,you,e).
patient1(97,liquid).
patient2(97,water).
theme(98,process).
rushed(98,event0,during,process).
cause(98,AGENT98,e).
...
Figure 3: Background Knowledge for Example 1
...
relation(18,19,?Act:goal?).
relation(97,98,?Act:goal?).
relation(1279,1280,?Step1:step2?).
relation(1300,1301,?Step1:step2?).
relation(1310,1311,?Step1:step2?).
relation(412,413,?Before:after?).
relation(441,442,?Before:after?).
...
Figure 4: Positive Examples
Our mode declarations dictate that the predicate
relation be used as the head and the other pred-
icates (has possession, transfer and visible) form
the body of the hypotheses. ?*? indicates that the
number of hypotheses to learn for a given relation
is unlimited. ?+? and ?-? signs indicate variables
within the predicates of which the former is an in-
put variable and the latter an output variable. ?#?
is used to denote a constant. Each argument of the
predicate is a type, whether a constant or a vari-
able. Types are defined as a single definite clause.
Our goal is to learn rules where the LHS of the
rule contains the relation that we wish to learn and
:- modeh(*,relation(+edu,+edu,#relationtype))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,+verbarg))?
:- modeb(*,has possession(+edu,#event,
#eventstage,+verbarg,-verbarg))?
:- modeb(*,transfer(+edu,#event,#eventstage,-verbarg))?
:- modeb(*,visible(+edu,#event,#eventstage,+verbarg))?
:- modeb(*,together(+edu,#event,
#eventstage,+verbarg,+verbarg,+verbarg))?
:- modeb(*,rushed(+edu,#event,#eventstage,+verbarg))?
Figure 5: Mode Declarations
37
RULE1:
relation(EDU1,EDU2,?Act:goal?) :-
degradation material integrity(EDU1,event0,result,C),
allow(EDU2,event0,during,C,D).
RULE2:
relation(EDU1,EDU2,?Act:goal?) :-
cause(EDU1,C,D),
together(EDU1,event0,end,E,F,G),
cause(EDU2,C,D).
RULE3:
relation(EDU1,EDU2,?Step1:step2?) :-
together(EDU2,event0,end,C,D,E),
has possession(EDU1,event0,during,C,F).
RULE4:
relation(EDU1,EDU2,?Before:after?) :-
motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE6:
relation(EDU1,EDU2,?Act:goal?) :-
motion(EDU1,event0,during,C).
Figure 6: Rules Learned
the RHS is a CNF of the semantic predicates de-
fined in VerbNet with their arguments. Given the
amount of training data we have, the nature of the
data itself and the Bayesian framework used, Pro-
gol learns simple rules that contain just one or two
clauses on the RHS. 6 of the 68 rules that Progol
manages to learn are shown in Figure 6. RULE4
states that there is a theme in motion during the
event in EDU A (which is the first EDU) and that
the theme is located in location D at the start of
the event in EDU B (the second EDU). RULE2 is
learned from pairs of EDUs such as in Example
1. The simple rules in Figure 6 may not readily
appeal to our intuitive notion of what such rules
should include. It is not clear at this point as to
how elaborate these rules should be, in order to
correctly identify the relation in question. One
of the reasons why more complex rules are not
learned by Progol is that there aren?t enough train-
ing examples. As we add more training data in the
future, we will see if rules that are more elaborate
than the ones in Figure 6 are learned .
4 Evaluation of the Discourse Parser
Table 1 shows the sets of relations for which we
managed to obtain semantic representations (i.e.
for both the EDUs).
Relations like Preparation:act did not yield any
Relation Total Train Test
Set Set
Step1:step2: 232 188 44
Joint: 190
Goal:act: 170 147 23
General:specific: 77
Criterion:act: 53 46 7
Before:after: 53 42 11
Act:side-effect: 38
Co-temp1:co-temp2: 22
Cause:effect: 19
Prescribe-act:wrong-act: 14
Obstacle:situation: 11
Reason:act: 9
Restatement: 6
Contrast1:contrast2: 6
Circumstance:situation: 3
Act:constraint: 2
Criterion:wrong-act: 2
Set:member: 1
Act:justification: 0
Comparison: 0
Preparation:act: 0
Object:attribute: 0
Part:whole: 0
Same-unit: 0
Indeterminate: 0
908 423 85
Table 1: Relation Set Count (Total Counts include ex-
amples that yielded semantic representations for both EDUs)
examples that could potentially be used. For a
number of relations, the total number of examples
we could use were less than 50. For the time being,
we decided to use only those relation sets that had
more than 50 examples. In addition, we chose not
to use Joint and General:specific relations. They
will be included in the future. Hence, our training
and testing data consisted of the following four re-
lations: Goal:act, Step1:step2, Criterion:act and
Before:after. The total number of examples we
used was 508 of which 423 were used for training
and 85 were used for testing.
Table 2, Table 3 and Table 4 show the results
from running the system on our test data. A total
of 85 positive examples were used for testing the
system.
Table 2 evaluates our SemDP system against a
baseline. Our baseline is the majority function,
which performs at a 51.7 F-Score. SemDP outper-
forms the baseline by almost 10 percentage points
38
Discourse Precision Recall F-Score
Parser
SemDP 61.7 58.8 60.24
Baseline* 51.7 51.7 51.7
Table 2: Evaluation vs Baseline (* our baseline is
the majority function)
Relation Precision Recall F-Score
Goal:act 31.57 26.08 28.57
Step1:step2 75 75 75
Before:after 54.5 54.5 54.5
Criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
Table 3: Test Results for SemDP
with an F-Score of 60.24. To the best of our
knowledge, we are also not aware of any work that
uses rich semantic information for discourse pars-
ing. (Polanyi et al, 2004) do not provide any eval-
uation results at all. (Soricut and Marcu, 2003) re-
port that their SynDP parser achieved up to 63.8 F-
Score on human-segmented test data. Our result of
60.24 F-Score shows that a Discourse Parser based
purely on semantics can perform as well. How-
ever, since the corpus, the size of training data and
the set of rhetorical relations we have used differ
from (Soricut and Marcu, 2003), a direct compar-
ison cannot be made.
Table 3 breaks down the results in detail for
each of the four rhetorical relations we tested on.
Since we are learning from positive data only and
the rules we learn depend heavily on the amount
of training data we have, we expected the system
to be more accurate with the relations that have
more training examples. As expected, SemDP did
very well in labeling Step1:step2 relations. Sur-
prisingly though, it did not perform as well with
Goal:act, even though it had the second highest
number of training examples (147 in total). In fact,
SemDP misclassified more positive test examples
for Goal:act than Before:after or Criterion:act, re-
lations which had almost one third the number of
Relation Goal:act Step1:step2 Before:after Criterion:act
Goal:act 6 8 5 0
Step1:step2 6 33 5 0
Before:after 0 4 6 1
Criterion:act 0 0 2 5
Table 4: Confusion Matrix for SemDP Test Result
training examples. Overall SemDP achieved a pre-
cision of 61.7 and a Recall of 58.8.
In order to find out how the positive test exam-
ples were misclassified, we investigated the dis-
tribution of the relations classified by SemDP. Ta-
ble 4 is the confusion matrix that highlights this
issue. A majority of the actual Goal:act relations
are incorrectly classified as Step1:step1 and Be-
fore:after. Likewise, most of the misclassification
of actual Step1:step1 seems to labeled as Goal:act
or Before:after. Such misclassification occurs be-
cause the simple rules learned by SemDP are not
able to accurately distinguish cases where positive
examples of two different relations share similar
semantic predicates. Moreover, since we are learn-
ing using positive examples only, it is possible that
a positive example may satisfy two or more rules
for different relations. In such cases, the rule that
has the highest score (as calculated by formula 4)
is used to label the unseen example.
5 Conclusions and Future Work
We have shown that it is possible to learn First Or-
der Logic rules from complex semantic data us-
ing an ILP based methodology. These rules can
be used to automatically label rhetorical relations.
Moreover, our results show that a Discourse Parser
that uses only semantic information can perform
as well as the state of the art Discourse Parsers
based on syntactic and lexical information.
Future work will involve the use of syntactic in-
formation as well. We also plan to run a more thor-
ough evaluation on the complete set of relations
that we have used in our coding scheme. It is also
important that the manual segmentation and an-
notation of rhetorical relations be subject to inter-
annotator agreement. A second human annotator
is currently annotating a sample of the annotated
corpus. Upon completion, the annotated corpus
will be checked for reliability.
Data sparseness is a well known problem inMa-
chine Learning. Like most paradigms, our learn-
ing model is also affected by it. We also plan to
explore techniques to deal with this issue.
39
Lastly, we have not tackled the problem of dis-
course parsing at higher levels of the DPT and seg-
mentation in this paper. Our ultimate goal is to
build a Discourse Parser that will automatically
segment a full text as well as annotate it with
rhetorical relations at every level of the DPT using
semantic as well as syntactic information. Much
work needs to be done but we are excited to see
what the aforesaid future work will yield.
Acknowledgments
This work is supported by award 0133123 from the National
Science Foundation. Thanks to C.P. Rose? for LCFLEX, M.
Palmer and K. Kipper for VerbNet, C. Buitelaar for CoreLex,
and Stephen Muggleton for Progol.
References
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy
and Underspecification. Ph.D. thesis, Computer Science,
Brandeis University, February.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.
2003. Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. In Current Direc-
tions in Discourse and Dialogue, pp. 85-112, Jan van Kup-
pevelt and Ronnie Smith eds., Kluwer Academic Publish-
ers.
Michael Collins. 2003. Head-driven statistical methods for
natural language parsing. Computational Linguistics, 29.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop
Sarkar, Aravind Joshi and Bonnie Webber. 2001. D-
LTAG System - Discourse Parsing with a Lexicalized Tree
Adjoining Grammar. Information Stucture, Discourse
Structure and Discourse Semantics, ESSLLI, 2001.
Ralph Grishman, Catherine Macleod, and Adam Meyers.
1994. COMLEX syntax: Building a computational lex-
icon. In COLING 94, Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, pages
472?477, Kyoto, Japan, August.
Paul Kingsbury and Martha Palmer. 2000. From Treebank
to Propbank. In Third International Conference on Lan-
guage Resources and Evaluation, LREC-02, Las Palmas,
Canary Islands, Spain, May 28 - June 3, 2002.
Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000.
Class-based construction of a verb lexicon. In AAAI-2000,
Proceedings of the Seventeenth National Conference on
Artificial Intelligence, Austin, TX.
Beth Levin and Malka Rappaport Hovav. 1992. Wiping the
slate clean: a lexical semantic exploration. In Beth Levin
and Steven Pinker, editors, Lexical and Conceptual Se-
mantics, Special Issue of Cognition: International Journal
of Cognitive Science. Blackwell Publishers.
William C. Mann and Sandra Thompson. 1988. Rhetorical
Structure Theory: toward a Functional Theory of Text Or-
ganization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsuper-
vised approach to recognizing discourse relations. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL-2002), Philadelphia, PA,
July.
Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu.
1999. Experiments in Constructing a Corpus of Discourse
Trees: Problems, Annotation Choices, Issues. In The
Workshop on Levels of Representation in Discourse, pages
71-78, Edinburgh, Scotland, July.
M. G. Moser, and J. D. Moore. 1995. Using Discourse
Analysis and Automatic Text Generation to Study Dis-
course Cue Usage. In AAAI Spring Symposium on Empir-
ical Methods in Discourse Interpretation and Generation,
1995.
Stephen H. Muggleton. 1995. Inverse Entailment and Pro-
gol. In New Generation Computing Journal, Vol. 13, pp.
245-286, 1995.
Martha Palmer, Daniel Gildea and, Paul Kingsbury. 2005.
The Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?105.
Livia Polanyi, Christopher Culy, Martin H. van den Berg,
Gian Lorenzo Thione, and David Ahn. 2004. Senten-
tial Structure and Discourse Parsing. Proceedings of the
ACL2004 Workshop on Discourse Annotation, Barcelona,
Spain, July 25, 2004.
James Pustejovsky. 1991. The generative lexicon. Computa-
tional Linguistics, 17(4):409?441.
Carolyn Penstein Rose? and Alon Lavie. 2000. Balancing ro-
bustness and efficiency in unification-augmented context-
free parsers for large practical applications. In Jean-
Clause Junqua and Gertjan van Noord, editors, Robustness
in Language and Speech Technology. Kluwer Academic
Press.
Radu Soricut and Daniel Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Information.
In Proceedings of the Human Language Technology and
North American Assiciation for Computational Linguis-
tics Conference (HLT/NAACL-2003), Edmonton, Canada,
May-June.
Elena Terenzi and Barbara Di Eugenio. 2003. Building lex-
ical semantic representations for natural language instruc-
tions. In HLT-NAACL03, 2003 Human Language Tech-
nology Conference, pages 100?102, Edmonton, Canada,
May. (Short Paper).
40
Benchmarking Noun Compound Interpretation
Su Nam Kim and Timothy Baldwin
Department of Computer Science and Software Engineering
and
NICTA Victoria Lab
University of Melbourne, VIC 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper we provide benchmark results
for two classes of methods used in inter-
preting noun compounds (NCs): semantic
similarity-based methods and their hybrids.
We evaluate the methods using 7-way and
binary class data from the nominal pair in-
terpretation task of SEMEVAL-2007.1 We
summarize and analyse our results, with
the intention of providing a framework for
benchmarking future research in this area.
1 Introduction
This paper reviews a range of simple and hybrid
approaches to noun compound (NC) interpretation.
The interpretation of NCs such as computer science
and paper submission involves predicting the se-
mantic relation (SR) that underlies a given NC. For
example, student price conventionally expresses the
meaning that a student benefits from the price (SR
= BENEFICIARY), while student protest conven-
tionally means a student undertaking a protest (SR
= AGENT).2
NCs are formed from simplex nouns with high
productivity. The huge number of possible NCs and
potentially large number of SRs makes NC interpre-
tation a very difficult problem. In the past, much NC
interpretation work has been carried out which tar-
gets particular NLP applications such as information
extraction, question-answering and machine trans-
lation. Unfortunately, much of it has not gained
1The 4th International Workshop on Semantic Evaluation
2SRs used in the examples are taken from Barker and Sz-
pakowicz (1998).
traction in real-world applications as the accuracy
of the methods has not been sufficiently high over
open-domain data. Most prior work has been car-
ried out under specific assumptions and with one-
off datasets, which makes it hard to analyze perfor-
mance and to build hybrid methods. Additionally,
disagreement in the inventory of SRs and a lack of
resource sharing has hampered comparative evalua-
tion of different methods.
The first step in NC interpretation is to define a set
of SRs. Levi (1979), for example, proposed a system
of 9 SRs, while others have proposed classifications
with 20-30 SRs (Finin, 1980; Barker and Szpakow-
icz, 1998; Moldovan et al, 2004). Smaller sets tend
to have reduced coverage due to coarse granularity,
whereas larger sets tend to be too fine grained and
suffer from low inter-annotator agreement. Addi-
tionally pragmatic/contextual differentiation leads to
difficulties in defining and interpreting SRs (Down-
ing, 1977; SparckJones, 1983).
Recent attempts in the area of NC interpretation
have taken two basic approaches: analogy-base in-
terpretation (Rosario, 2001; Moldovan et al, 2004;
Kim and Baldwin, 2005; Girju, 2007) and seman-
tic disambiguation relative to an underlying predi-
cate or semantically-unambiguous paraphrase (Van-
derwende, 1994; Lapata, 2002; Kim and Baldwin,
2006; Nakov, 2006). Most methods employ rich on-
tologies and ignore the context of use, supporting
the claim by Fan (2003) that axioms and ontological
distinctions are more important than detailed knowl-
edge of specific nouns for NC interpretation. Addi-
tionally, most approaches use supervised learning,
raising questions about the generality of the test and
569
training data sets and the effectiveness of the algo-
rithms in different domains (coverage of SRs over
the NCs is another issue).
Our aim in this paper is to compare and analyze
existing NC interpretation methods over a common,
publicly available dataset. While recent research
has made significant progress, bringing us one step
closer to practical applicability in NLP applications,
no direct comparison or analysis of the approaches
has been attempted to date. As a result, it is hard to
determine which approach is appropriate in a given
domain or build hybrid methods based on prior ap-
proaches. We also investigate the impact on perfor-
mance of relaxing assumptions made in the origi-
nal research, to compare different approaches in an
identical setting.
In the remainder of the paper, we review the re-
search background and NC interpretation methods
in Section 2, describe the methods and system archi-
tectures in Section 3, detail the datasets used in our
experiments in Section 4, carry out a system evalu-
ation in Section 5 and Section 6, and finally present
a discussion and conclusions in Section 7 and Sec-
tion 8, respectively.
2 Background and Methods
2.1 Research Background
In this study, we selected three semantic similar-
ity based models which had been found to perform
strongly in previous research, and which were easy
to re-implement: SENSE COLLOCATION (Moldovan
et al, 2004), CONSTITUENT SIMILARITY (Kim
and Baldwin, 2005) and CO-TRAINING, e.g. using
SENSE COLLOCATION or CONSTITUENT SIMILAR-
ITY (Kim and Baldwin, 2007). These approaches
were evaluated over a 7-way classification using
open-domain data from the nominal pair interpre-
tation task of SEMEVAL-2007 (Girju et al, 2007).
We test their performance in both 7-way and binary-
class classification settings.
2.2 Sense Collocation Method
The SENSE COLLOCATION method of Moldovan et
al. (2004) is based on the pair of word senses of NC
constituents. The basic idea is that NCs which have
the same or similar sense collocation tend to have
the same SR. As an example, car factory and auto-
mobile factory share the conventional interpretation
of MAKE, which is predicted by car and automo-
bile having the same sense across the two NCs, and
factory being used with the same sense in each in-
stance. This intuition is formulated in Equations 1
and 2 below.
The probability P (r|fifj) (simplified to
P (r|fij)) of a SR r for word senses fi and fj
is calculated based on simple maximum likelihood
estimation:
P (r|fij) = n(r, fij)n(fij) (1)
The preferred SR r? for the given sense combina-
tion is that which maximises the probability:
r? = argmaxr?RP (r|fij)
= argmaxr?RP (fij |r)P (r) (2)
2.3 Constituent Similarity Method
The intuition behind the CONSTITUENT SIMILAR-
ITY method is similar to the SENSE COLLOCATION
method, in that NCs made up of similar words tend
to share the same SR. The principal difference is that
it doesn?t presuppose that we know the word sense
of each constituent word (i.e. the similarity is cal-
culated at the word rather than sense level). The
method takes the form of a 1-nearest neighbour clas-
sifier, with the best-matching training instance for
each test instance predicting its SR. For example,
we may find that test instance chocolate milk most
closely matches apple juice and hence predict that
the SR is MATERIAL.
This idea is formulated in Equation 3 below. For-
mally, SA is the similarity between NCs (Ni,1, Ni,2)
and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1)? ((1? ?)S2 + S2))
2 (3)
where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is the head noun similarity
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
The similarity scores are calculated across the bag
of WordNet senses (without choosing between
570
them) using the method of Wu and Palmer (1994) as
implemented in WordNet::Similarity (Pat-
wardhan et al, 2003). This is done for each pairing
of WordNet senses of the two words in question,
and the overall lexical similarity is calculated as the
average across the pairwise sense similarities.
2.4 Co-Training by Sense Collocation
Co-training by sense collocation (SCOLL CO-
TRAINING) is based on the SENSE COLLOCATION
method and lexical substitution (Kim and Baldwin,
2007). It expands the set of training NCs from
a relatively small number of manually-tagged seed
instances. That is, it makes use of extra train-
ing instances fashioned through a bootstrap process.
For example, assuming automobile factory with the
SR MAKE were a seed instance, NCs generated
from synonyms, hypernyms and sister words of its
constituents would be added as extra training in-
stances, with the same SR of MAKE. That is, we
would add car factory (SYNONYM), vehicle fac-
tory (HYPERNYM) and truck factory (SISTER
WORD), for example. Note that the substitution
takes place only for one constituent at a time to avoid
extreme variation.
2.5 Co-training by Constituent Similarity
Co-training by Constituent Similarity (CS CO-
TRAINING) is also a co-training method, but based
on CONSTITUENT SIMILARITY rather than SENSE
COLLOCATION. The basic idea is that when NCs
are interpreted using the CONSTITUENT SIMILAR-
ITY method, the predictions are more reliable when
the lexical similarity is higher. Hence, we progres-
sively reduce the similarity threshold, and incorpo-
rate higher-similarity instances into our training data
earlier in the bootstrap process. That is, we run
the CONSTITUENT SIMILARITY method and acquire
NCs with similarity equal to or greater than a fixed
threshold. Then in the next iteration, we add the ac-
quired NCs into the training dataset for use in clas-
sifying more instances. As a result, in each step,
the number of training instances increases monoton-
ically. We ?cascade? through a series of decreas-
ing similarity thresholds until we reach a saturation
point. As our threshold, we used a starting value of
0.90, which was decremented down to 0.65 in steps
of 0.05.
Method Description
SCOLL sense collocation
SCOLLCT sense collocation + SCOLL co-training
CSIM constituent similarity
CSIM +SCOLLCT constituent similarity + SCOLL co-training
HYBRID SCOLL + CSIM + SCOLLCT
CSIMCT constituent similarity + CSIM co-training
Table 1: Systems used in our experiments
TEST
untagged test data
untagged test data
untagged test data
untagged test data
tagged data
tagged data
tagged data
tagged data
tagged data
TRAIN
Extension ofTraining databy similar words
? Synonym
? Hypernym
? Sister word
Extended TRAIN
Sense Collcation
Step 1
Similarity
Step 2
Step 3
Step 4
Similarity
Step 5
Sense Collcation
Similarity
Figure 1: Architecture of the HYBRID method
3 Systems and Architectures
We tested the original methods of Moldovan et al
(2004) and Kim and Baldwin (2005), and combined
them with the co-training methods of Kim and Bald-
win (2007) to come up with six different hybrid sys-
tems for evaluation, as detailed in Table 1. To build
the classifiers, we used the TIMBL5.0 memory-
based learner (Daelemans et al, 2004).
The HYBRID method consists of five interpreta-
tion steps. The first step is to use the SENSE COL-
LOCATION method over the original training data.
When the sense collocation of the test and train-
ing instances is the same, we judge the predicted
SR to be correct. The second step is to apply the
CONSTITUENT SIMILARITY method over the origi-
nal training data. In order to confirm that the pre-
dicted SR is correct, we use a threshold of 0.8 to
interpret the test instances. The third step is to ap-
ply SENSE COLLOCATION over the expanded train-
571
TRAIN
#of Tagged
>= 10% of testThreshold
Tagged
finalize current
tags and end
reduce Threshold
TEST
get Similarity
Sim >= TN Y
Y
N
if T == 0.6 &(#of Tagged <
10% of test)
N
Y
Figure 2: Architecture of the CSIMCT system
ing data through the advent of hypernyms and sis-
ter words, using the SCOLL CO-TRAINING method.
This step benefits from a larger amount of training
data (17,613 vs. 937). The fourth step is to apply
the CONSTITUENT SIMILARITY method (EXTCS)
over the consolidated training data, with the thresh-
old unchanged at 0.8. The final step is to apply the
CONSTITUENT SIMILARITY (CSTT) method over
the combined training data without any restriction
on the threshold (to guarantee a SR prediction for
every test instance). We select SRs from the training
instances whose similarity is higher than the origi-
nal training data and expanded training data. How-
ever, since the generated training instances are more
likely to contain errors, we apply a linear weight of
0.8 to the similarity values for the expanded train-
ing instances. This gives preferential treatment to
predictions based on the original training instances.
Note that this weight was based on analysis of the
error rate in the expanded training instances. In pre-
vious work (Kim and Baldwin, 2007), we found the
overall classification accuracy rate after the first it-
eration to be 70-80%. Hence, we settled on a weight
of 0.8.
The CSIMCT system is based solely on the CON-
STITUENT SIMILARITY method with cascading. We
perform iterative CS co-training as described in Sec-
tion 2.5, with the slight variation that we hold off
Binary 7-way
SR Test Train Train* Test Train Train*
CE 80 136 2,588 36 71 1,854
IA 78 135 1,400 36 68 1,001
PP 93 126 2,591 55 78 2,089
OE 81 136 3,085 35 52 1,560
TT 71 129 2,994 27 50 1,718
PW 72 138 2,577 28 64 1,510
CC 74 137 2,378 37 63 1,934
Total 549 937 17,613 254 446 11,664
Table 3: Number of instances associated with each
SR (Train* is the number of expanded train in-
stances)
on reducing the threshold if less than 10% of the
test instances are tagged on a given iteration, giving
other test instances a chance to be tagged at a higher
threshold level relative to newly generated training
instances. The residue of test instances on comple-
tion of the final iteration (threshold = 0.6) are tagged
according to the best-matching training instance, ir-
respective of the magnitude of the similarity.
4 Data
We used the dataset from the SEMEVAL-2007
nominal pair interpretation task, which is based
on 7 SRs: CAUSE-EFFECT (CE), INSTRUMENT-
AGENCY (IA), PRODUCT-PRODUCER (PP),
ORIGIN-ENTITY (OE), THEME-TOOL (TT),
PART-WHOLE (PW), CONTENT-CONTAINER
(CC). The task in SEMEVAL-2007 was to identify
the compatibility of a given SR for each test
instances using word senses retrieved from WORD-
NET 3.0 (Fellbaum, 1998) and queries. Table 2
shows the definition of the SRs.
In our research, we interpret the dataset in two
ways: (1) as a binary classification task for each SR
based on the original data; and (2) as a 7-way clas-
sification task, combining together all positive test
and training instances for each of the 7 SR datasets
into a single dataset. Hence, the size of the dataset
for 7-way classification is much smaller than that of
the original dataset. We also expand the training in-
stances using SCOLL CO-TRAINING. Table 3 de-
scribes the number of test and train instances for NC
interpretation for the binary and 7-way classification
tasks.
Our analysis shows that only 5 NCs are repeated
572
Semantic relation Definition Examples
Cause-Effect (CE) N1 is the cause of N2 virus flu, hormone growth
Instrument-Agency (IA) N1 is the instrument of N2; N2 uses N1 laser printer, axe murderer
Product-Producer (PP) N1 is a product of N2; N2 produces N1 honey bee, music clock
Origin-Entity (OE) N1 is the origin of N2 bacon grease, desert storm
Theme-Tool (TT) N2 is intended for N1 reorganization process, copyright law
Part-Whole (PW) N1 is part of N2 table leg, daisy flower
Content-Container (CC) N1 is stored or carried inside N2 apple basket, wine bottle
Table 2: The set of 7 semantic relations, where N1 is the modifier and N2 is the head noun
across multiple SR datasets (i.e. occur as an instance
in more than one of the 7 datasets), none of which
occur as positive instances for multiple SRs. As
such, no NC instances in the 7-way classification
task end up with a multiclass classification. Also
note that some of NCs are contained within ternary
or higher-order NCs: 40 test NCs and 81 training
NCs for the binary classification task, and 24 test
NCs and 42 training NCs for the 7-way classification
task. For these NCs, we extracted a ?base? binary
NC based on the provided bracketing. The follow-
ing are examples of extraction of binary NCs from
ternary or higher-order NCs.
((billiard table) room) ? table room
(body (bath towel)) ? body towel
In order to extract a binary NC, we take the head
noun of each embedded NC and combine this with
the corresponding head noun or modifier. E.g., table
is the head noun of billiard table, which combines
with the head noun of the complex NC room to form
table room.
5 Experiment 1: 7-way classification
Our first experiment was carried out over the 7-way
classification task?i.e. all 7 SRs in a single classifi-
cation task?using the 6 systems from Section 3. In
our results in Table 4, we use the system categories
from SEMEVAL-2007 of A4 and B4, where A4 sys-
tems use none of the provided word senses, and B4
systems use the word senses.3 We categorized our
systems into these two groups in order to evaluate
them separately within the bounds of the original
SEMEVAL-2007 task. In each case, the baseline is
a majority class classifier.
3In the original SEMEVAL-2007 task, there were two fur-
ther categories, which incorporated the ?query? with or without
the sense information.
Class Method P R F1 A
? Majority .217
A4 CSIM .518 .522 .449 .528
CSIMCT .517 .511 .426 .522
B4 SCOLL .705 .444 .477 .496
SCOLLCT .646 .466 .498 .508
CSIM +SCOLLCT .523 .520 .454 .528
HYBRID .500 .505 .416 .516
Table 4: Experiment 1: Results (P=precision,
R=recall, F1=F-score, A=accuracy)
Step Method Tagged Ai Untagged
1 SCOLL 12 1.000 242
2 CSIM 57 .719 185
3 extSCOLL 0 .000 185
4 extCSIM 78 .462 107
5 CSIMREST 107 .393 0
Table 5: Experiment 1: Classifications for each
step of the HYBRID method (CSREST=the final ap-
plication of CS over the remaining test instances,
Ai=accuracy for classifications made at step i)
Tables 5 and 6 show the results at each step for
the HYBRID and CSIMCT methods, respectively. As
each method proceeds, the amount of tagged data in-
creases but the classification accuracy of the system
decreases, due to the inclusion of increasingly noisy
training instances in the previous step. The perfor-
mance of each individual relation is shown in Fig-
ure 3, which largely mirrors the findings of the sys-
tems in the original SEMEVAL-2007 task in terms
of the relative difficulty to predict each of the 7 SRs.
6 Experiment 2: binary classification
In the second experiment, we performed a separate
binary classification task for each of the 7 SRs, in
the manner of the original SEMEVAL-2007 task.
Table 7 shows the three baselines provided by the
SEMEVAL-2007 organisers and performance of our
573
Iteration ? Tagged Ai Untagged
1 .90 29 .897 225
2 .85 12 .750 213
3 .80 31 .613 182
4 .75 43 .535 139
5 .70 63 .540 76
6 .65 26 .346 50
7 <.65 49 .250 1
Table 6: Experiment 1: Classifications at each step
of the CSIMCT method (?=threshold, Ai=accuracy
for classifications made at iteration i)
CE IA OEPP TT PW CCRelations
Accuracy(%)
KE w/ multiple classes
 0
 20
 40
 60
 80
 100 precision
recallFscore
Figure 3: Experiment 1: Performance over each SR
(CSIM +SCOLLCT method)
6 systems. We also present the best-performing sys-
tem within each group from the SEMEVAL-2007
task. The methods for computing the baselines are
described in Girju et al (2007).
As with the first experiment, we analyzed the
number of tagged instances and accuracy for the HY-
BRID and CSIMCT methods, as shown in Tables 8
and 9, respectively. The overall results are similar to
those for the 7-way classification task.
Figures 4 and 5 show the performance for posi-
tive and negative classifications for each individual
SR. The performance when the classifier outputs are
mapped onto the 7-way classification task are simi-
lar to those in Figure 3.
7 Discussion and Conclusion
We compared the performance of the 6 systems in
Tables 4 and 7 over the 7-way and binary clas-
sification tasks, respectively. The performance of
all methods exceeded the baseline. The CON-
STITUENT SIMILARITY (CSIM) system performed
the best in group A4 and CONSTITUENT SIMILAR-
Class Method P R F1 A
? All True .485 1.000 .648 .485
? Probability .485 .485 .485 .517
? Majority .813 .429 .308 .570
A4 Best .661 .667 .648 .660
CSIM .632 .628 .627 .650
CSIMCT .615 .557 .578 .627
B4 Best .797 .698 .724 .763
SCOLL .672 .584 .545 .634
SCOLLCT .602 .571 .554 .619
CSIM +SCOLLCT .660 .657 .654 .669
HYBRID .617 .568 .587 .625
Table 7: Experiment 2: Binary classification results
(P=precision, R=recall, F1=F-score, A=accuracy)
Step Method Tagged Ai Untagged
1 SCOLL 21 .810 526
2 CSIM 106 .689 420
3 extSCOLL 0 .000 420
4 extCSIM 61 .607 359
5 CSIMREST 359 .619 0
Table 8: Experiment 2: Classifications for each
step of the HYBRID method (CSREST=the final ap-
plication of CS over the remaining test instances,
Ai=accuracy for classifications made at step i)
ITY + SCOLLCT (CSIM +SCOLLCT ) system per-
formed the best in group B4 for both classification
tasks. In general, the performance of CONSTITUENT
SIMILARITY is marginally better than that of SENSE
COLLOCATION. Also, the utility of co-training is
confirmed by it outperforming both CONSTITUENT
SIMILARITY and SENSE COLLOCATION.
In order to compare the original methods with
the hybrid methods, we observed that the original
methods, SCOLL and K, and their co-training vari-
ants performed consistently better than the hybrid
methods, HYBRID and CSIMCT . We found that the
combination of the methods lowers overall perfor-
mance. We also found that the number of training
instances contributes to improved performance, pre-
dictably in the sense that the methods are supervised,
but encouraging in the sense that the extra training
data is generated automatically. As expected, the
step-wise performance of HYBRID and CSIMCT de-
grades with each iteration, although there were in-
stances where the performance didn?t drop from one
iteration to the next (e.g. iteration 3 = 59.46% vs. it-
eration 4 = 72.23% in Experiment 2). This confirms
574
Iteration ? Tagged Ai Untagged
1 .90 21 .810 526
2 .85 52 .726 474
3 .80 56 .714 418
4 .75 74 .595 344
5 .70 101 .722 243
6 .65 222 .572 21
7 <.65 21 .996 0
Table 9: Experiment 2: Classifications at each step
of the CSIMCT method (?=threshold, Ai=accuracy
for classifications made at iteration i)
CE IA PP OE TT PW CC
relations
Accuracy(%)
KE w/ binary classes & tagged as "true"
Fscorerecall
precision
 0
 20
 40
 60
 80
 100
Figure 4: TPR for each SR for the binary task (pos-
itive instances, CSIM +SCOLLCT method)
our expectation that: (a) the similarity threshold is
strongly correlated with the quality of the resultant
data; and (b) the method is susceptible to noisy train-
ing data.
Our performance comparison over the binary
classification task from the SEMEVAL-2007 task
shows that our 6 systems performed below the best
performing system in the competition, to varying de-
grees. This is partly because the methods were origi-
nally designed for multi-way (positive) classification
and require adjustment for the binary task reformu-
lation, although their performance is competitive.
Finally, comparing the SCOLL and CSIM meth-
ods, we found that the methods interpret SRs with
100% accuracy when the sense collocations are
found in both the test and training data. However,
the CSIM method is more sensitive than the SCOLL
method to variation in the sense collocations, which
leads to better performance. Also, the CSIM method
interprets NCs with high accuracy when the com-
puted similarity is sufficiently high (e.g. with simi-
larity ? 0.9 the accuracy is 89.7%). Another benefit
CE IA PP OE TT PW CCRelations
Accuracy(%)
KE w/ binary classes & tagged as "false"
 0
 20
 40
 60
 80
 100 precision
recallFscore
Figure 5: TNR for each SR for the binary task (neg-
ative instances, CSIM +SCOLLCT method)
of this method is that it interprets NCs without word
sense information. As a result, we conclude that the
CSIM method is more flexible and robust. One pos-
sible weakness of CSIM is its reliance on the simi-
larity measure.
8 Conclusions and Future Work
In this paper, we have benchmarked and hybridised
existing NC interpretation methods over data from
the SEMEVAL-2007 nominal pair interpretation
task. In this, we have established guidelines for the
use of the different methods, and also for the rein-
terpretation of the SEMEVAL-2007 data as a more
conventional multi-way classification task. We con-
firmed that CONSTITUENT SIMILARITY is the best
method due to its insensitivity to varied sense col-
locations. We also confirmed that co-training im-
proves the performance of the methods by expand-
ing the number of training instances.
Looking to the future, there is room for improve-
ment for all the methods through such factors as
threshold tweaking and expanding the training in-
stances further.
References
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pp. 805?810,
Acapulco, Mexico.
Ken Barker and Stan Szpakowicz. 1998. Semi-
automatic recognition of noun modifier relationships.
In Proceedings of the 17th International Conference
575
on Computational Linguistics, pp. 96?102, Montreal,
Canada.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. TiMBL: Tilburg Mem-
ory Based Learner, version 5.1, Reference Guide. ILK
Technical Report 04-02.
Pamela Downing. 1977. On the Creation and Use of En-
glish Compound Nouns. Language, 53(4):810?842.
James Fan and Ken Barker and Bruce W. Porter. 2003.
The knowledge required to interpret noun compounds.
In In Proceedings of the 7th International Joint Con-
ference on Artificial Intelligence, Acapulco, Mexico,
1483?1485.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Timothy W. Finin. 1980. The Semantic Interpretation
of Compound Nominals. Ph.D. thesis, University of
Illinois.
Roxana Girju. 2007. Improving the Interpretation of
Noun Phrases with Cross-linguistic Information. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pp. 568?575,
Prague, Czech Republic.
Roxana Girju and Preslav Nakov and Vivi Nastase and
Stan Szpakowicz and Peter Turney and Deniz Yuret.
2007. SemEval-2007 Task 04: Classification of Se-
mantic Relations between Nominals. In Proceedings
of the 4th Semantic Evaluation Workshop (SemEval-
2007), Prague, Czech Republic, pp.13?18.
Su Nam Kim and Timothy Baldwin. 2005. Auto-
matic interpretation of Noun Compounds using Word-
Net similarity. In Proceedings of the 2nd International
Joint Conference On Natural Language Processing,
pp. 945?956, JeJu, Korea.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting
Semantic Relations in Noun Compounds via Verb Se-
mantics. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics and
21st International Conference on Computational Lin-
guistics (COLING/ACL-2006). pp. 491?498, Sydney,
Australia.
Su Nam Kim and Timothy Baldwin. 2007. Interpreting
Noun Compound Using Bootstrapping and Sense Col-
location. In Proceedings of the Pacific Association for
Computational Linguistics (PACLING), pp. 129?136,
Melbourne, Australia.
Maria Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
Judith Levi. 1979. The syntax and semantics of complex
nominals. In The Syntax and Semantics of Complex
Nominals. New York:Academic Press.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In Proceedings
of the HLT-NAACL 2004 Workshop on Computational
Lexical Semantics, pp. 60?67, Boston, USA.
Preslav Nakov and Marti Hearst. 2006. Using Verbs to
Characterize Noun-Noun Relations. In Proceedings of
the 12th International Conference on Artificial Intelli-
gence: Methodology, Systems, Applications (AIMSA),
Bularia.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proc. of the ACL-2007 Workshop on
A Broader Perspective on Multiword Expressions,
Prague, Czech Republic.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics.
Barbara Rosario and Hearst Marti. 2001. Classify-
ing the Semantic Relations in Noun Compounds via
a Domain-Specific Lexical Hierarchy. In In Proceed-
ings of the 6th Conference on Empirical Methods in
Natural Language Processing (EMNLP-2001), 82?90.
Karen Sparck Jones. 1983. Compound noun interpre-
tation problems. Computer Speech Processing, Frank
Fallside and William A. Woods, Prentice-Hall, Engle-
wood Cliffs, NJ.
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings of
the 15th Conference on Computational linguistics, pp.
782?788.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Computational
Linguistics, pp. 133?138, Las Cruces, USA.
576
MRD-based Word Sense Disambiguation: Further#2 Extending#1 Lesk
Timothy Baldwin,? Su Nam Kim,? Francis Bond,? Sanae Fujita,?
David Martinez? and Takaaki Tanaka?
? CSSE
University of Melbourne
VIC 3010 Australia
? NICT
3-5 Hikaridai, Seika-cho
Soraku-gun, Kyoto
619-0289 Japan
? NTT CS Labs
2-4 Hikari-dai, Seika-cho
Soraku-gun, Kyoto
619-0237 Japan
Abstract
This paper reconsiders the task of MRD-
based word sense disambiguation, in extend-
ing the basic Lesk algorithm to investigate
the impact onWSD performance of different
tokenisation schemes, scoring mechanisms,
methods of gloss extension and filtering
methods. In experimentation over the Lex-
eed Sensebank and the Japanese Senseval-
2 dictionary task, we demonstrate that char-
acter bigrams with sense-sensitive gloss ex-
tension over hyponyms and hypernyms en-
hances WSD performance.
1 Introduction
The aim of this work is to develop and extend word
sense disambiguation (WSD) techniques to be ap-
plied to all words in a text. The goal of WSD is
to link occurrences of ambiguous words in specific
contexts to their meanings, usually represented by
a machine readable dictionary (MRD) or a similar
lexical repository. For instance, given the following
Japanese input:
(1) ?????
quiet
?
dog
?
ACC
????
want to keep
?(I) want to keep a quiet dog?
we would hope to identify each component word as
occurring with the sense corresponding to the indi-
cated English glosses.
WSD systems can be classified according to the
knowledge sources they use to build their models. A
top-level distinction is made between supervised and
unsupervised systems. The former rely on training
instances that have been hand-tagged, while the lat-
ter rely on other types of knowledge, such as lexical
databases or untagged corpora. The Senseval evalu-
ation tracks have shown that supervised systems per-
form better when sufficient training data is available,
but they do not scale well to all words in context.
This is known as the knowledge acquisition bottle-
neck, and is the main motivation behind research on
unsupervised techniques (Mihalcea and Chklovski,
2003).
In this paper, we aim to exploit an existing lexical
resource to build an all-words Japanese word-sense
disambiguator. The resource in question is the Lex-
eed Sensebank (Tanaka et al, 2006) and consists of
the 28,000 most familiar words of Japanese, each of
which has one or more basic senses. The senses take
the form of a dictionary definition composed from
the closed vocabulary of the 28,000 words contained
in the dictionary, each of which is further manually
sense annotated according to the Lexeed sense in-
ventory. Lexeed also has a semi-automatically con-
structed ontology.
Through the Lexeed sensebank, we investigate a
number of areas of general interest to theWSD com-
munity. First, we test extensions of the Lesk algo-
rithm (Lesk, 1986) over Japanese, focusing specif-
ically on the impact of the overlap metric and seg-
ment representation on WSD performance. Second,
we propose further extensions of the Lesk algorithm
that make use of disambiguated definitions. In this,
we shed light on the relative benefits we can expect
from hand-tagging dictionary definitions, i.e. in in-
troducing ?semi-supervision? to the disambiguation
task. The proposed method is language independent,
and is equally applicable to the Extended WordNet1
for English, for example.
2 Related work
Our work focuses on unsupervised and semi-
supervised methods that target al words and parts
of speech (POS) in context. We use the term
?unsupervised? to refer to systems that do not
use hand-tagged example sets for each word, in
line with the standard usage in the WSD litera-
ture (Agirre and Edmonds, 2006). We blur the su-
pervised/unsupervised boundary somewhat in com-
bining the basic unsupervised methods with hand-
tagged definitions from Lexeed, in order to measure
the improvement we can expect from sense-tagged
data. We qualify our use of hand-tagged definition
1 http://xwn.hlt.utdallas.edu
775
sentences by claiming that this kind of resource is
less costly to produce than sense-annotated open text
because: (1) the effects of discourse are limited, (2)
syntax is relatively simple, (3) there is significant se-
mantic priming relative to the word being defined,
and (4) there is generally explicit meta-tagging of
the domain in technical definitions. In our experi-
ments, we will make clear when hand-tagged sense
information is being used.
Unsupervised methods rely on different knowl-
edge sources to build their models. Primarily
the following types of lexical resources have been
used for WSD: MRDs, lexical ontologies, and un-
tagged corpora (monolingual corpora, second lan-
guage corpora, and parallel corpora). Although
early approaches focused on exploiting a single re-
source (Lesk, 1986), recent trends show the bene-
fits of combining different knowledge sources, such
as hierarchical relations from an ontology and un-
tagged corpora (McCarthy et al, 2004). In this sum-
mary, we will focus on a few representative systems
that make use of different resources, noting that this
is an area of very active research which we cannot
do true justice to within the confines of this paper.
The Lesk method (Lesk, 1986) is an MRD-based
system that relies on counting the overlap between
the words in the target context and the dictionary
definitions of the senses. In spite of its simplicity,
it has been shown to be a hard baseline for unsu-
pervised methods in Senseval, and it is applicable to
all-words with minimal effort. Banerjee and Peder-
sen (2002) extended the Lesk method for WordNet-
based WSD tasks, to include hierarchical data from
the WordNet ontology (Fellbaum, 1998). They ob-
served that the hierarchical relations significantly
enhance the basic model. Both these methods will
be described extensively in Section 3.1, as our ap-
proach is based on them.
Other notable unsupervised and semi-supervised
approaches are those of McCarthy et al (2004), who
combine ontological relations and untagged corpora
to automatically rank word senses in relation to a
corpus, and Leacock et al (1998) who use untagged
data to build sense-tagged data automatically based
on monosemous words. Parallel corpora have also
been used to avoid the need for hand-tagged data,
e.g. by Chan and Ng (2005).
3 Background
As background to our work, we first describe the ba-
sic and extended Lesk algorithms that form the core
of our approach. Then we present the Lexeed lex-
ical resource we have used in our experiments, and
finally we outline aspects of Japanese relevant for
this work.
3.1 Basic and Extended Lesk
The original Lesk algorithm (Lesk, 1986) performs
WSD by calculating the relative word overlap be-
tween the context of usage of a target word, and the
dictionary definition of each of its senses in a given
MRD. The sense with the highest overlap is then se-
lected as the most plausible hypothesis.
An obvious shortcoming of the original Lesk al-
gorithm is that it requires that the exact words used
in the definitions be included in each usage of the
target word. To redress this shortcoming, Banerjee
and Pedersen (2002) extended the basic algorithm
for WordNet-based WSD tasks to include hierarchi-
cal information, i.e. expanding the definitions to in-
clude definitions of hypernyms and hyponyms of the
synset containing a given sense, and assigning the
same weight to the words sourced from the different
definitions.
Both of these methods can be formalised accord-
ing to the following algorithm, which also forms the
basis of our proposed method:
for each word wi in context w = w1w2...wn do
for each sense si,j and definition di,j of wi do
score(si,j) = overlap(w, di,j)
end for
s?i = arg maxj score(si,j)
end for
3.2 The Lexeed Sensebank
All our experimentation is based on the Lexeed
Sensebank (Tanaka et al, 2006). The Lexeed Sense-
bank consists of all Japanese words above a certain
level of familiarity (as defined by Kasahara et al
(2004)), giving rise to 28,000 words in all, with a to-
tal of 46,000 senses which are similarly filtered for
similarity. The sense granularity is relatively coarse
for most words, with the possible exception of light
verbs, making it well suited to open-domain appli-
cations. Definition sentences for these senses were
rewritten to use only the closed vocabulary of the
28,000 familiar words (and some function words).
Additionally, a single example sentence was man-
ually constructed to exemplify each of the 46,000
senses, once again using the closed vocabulary of the
Lexeed dictionary. Both the definition sentences and
example sentences were then manually sense anno-
tated by 5 native speakers of Japanese, from which a
majority sense was extracted.
776
In addition, an ontology was induced from the
Lexeed dictionary, by parsing the first definition sen-
tence for each sense (Nichols et al, 2005). Hy-
pernyms were determined by identifying the highest
scoping real predicate (i.e. the genus). Other rela-
tion types such as synonymy and domain were also
induced based on trigger patterns in the definition
sentences, although these are too few to be useful
in our research. Because each word is sense tagged,
the relations link senses rather than just words.
3.3 Peculiarities of Japanese
The experiments in this paper focus exclusively
on Japanese WSD. Below, we outline aspects of
Japanese which are relevant to the task.
First, Japanese is a non-segmenting language, i.e.
there is no explicit orthographic representation of
word boundaries. The native rendering of (1), e.g., is???????????. Various packages exist to
automatically segment Japanese strings into words,
and the Lexeed data has been pre-segmented using
ChaSen (Matsumoto et al, 2003).
Second, Japanese is made up of 3 basic alpha-
bets: hiragana, katakana (both syllabic in nature)
and kanji (logographic in nature). The relevance of
these first two observations to WSD is that we can
choose to represent the context of a target word by
way of characters or words.
Third, Japanese has relatively free word order,
or strictly speaking, word order within phrases is
largely fixed but the ordering of phrases governed
by a given predicate is relatively free.
4 Proposed Extensions
We propose extensions to the basic Lesk algorithm
in the orthogonal areas of the scoring mechanism,
tokenisation, extended glosses and filtering.
4.1 Scoring Mechanism
In our algorithm, overlap provides the means to
score a given pairing of context w and definition
di,j . In the original Lesk algorithm, overlap was
simply the sum of words in common between the
two, which Banerjee and Pedersen (2002) modified
by squaring the size of each overlapping sub-string.
While squaring is well motivated in terms of prefer-
ring larger substring matches, it makes the algorithm
computationally expensive. We thus adopt a cheaper
scoring mechanism which normalises relative to the
length of w and di,j , but ignores the length of sub-
string matches. Namely, we use the Dice coefficient.
4.2 Tokenisation
Tokenisation is particularly important in Japanese
because it is a non-segmenting language with a lo-
gographic orthography (kanji). As such, we can
chose to either word tokenise via a word splitter
such as ChaSen, or character tokenise. Charac-
ter and word tokenisation have been compared in
the context of Japanese information retrieval (Fujii
and Croft, 1993) and translation retrieval (Baldwin,
2001), and in both cases, characters have been found
to be the superior representation overall.
Orthogonal to the question of whether to tokenise
into words or characters, we adopt an n-gram seg-
ment representation, in the form of simple unigrams
and simple bigrams. In the case of word tokenisa-
tion and simple bigrams, e.g., example (1) would be
represented as {?????? ,?? ,????? }.
4.3 Extended Glosses
The main direction in which Banerjee and Peder-
sen (2002) successfully extended the Lesk algorithm
was in including hierarchically-adjacent glosses (i.e.
hyponyms and hypernyms). We take this a step
further, in using both the Lexeed ontology and the
sense-disambiguated words in the definition sen-
tences.
The basic form of extended glossing is the simple
Lesk method, where we take the simple definitions
for each sense si,j (i.e. without any gloss extension).
Next, we replicate the Banerjee and Pedersen
(2002) method in extending the glosses to include
words from the definitions for the (immediate) hy-
pernyms and/or hyponyms of each sense si,j .
An extension of the Banerjee and Pedersen (2002)
method which makes use of the sense-annotated def-
initions is to include the words in the definition of
each sense-annotated word dk contained in defini-
tion di,j = d1d2...dm of word sense si,j . That is,
rather than traversing the ontology relative to each
word sense candidate si,j for the target word wi,
we represent each word sense via the original def-
inition plus all definitions of word senses contained
in it (weighting each to give the words in the original
definition greater import than those from definitions
of those word senses). We can then optionally adopt
a similar policy to Banerjee and Pedersen (2002) in
expanding each sense-annotated word dk in the orig-
inal definition relative to the ontology, to include the
immediate hypernyms and/or hyponyms.
We further expand the definitions (+extdef) by
adding the full definition for each sense-tagged word
in the original definition. This can be combined
with the Banerjee and Pedersen (2002) method by
777
also expanding each sense-annotated word dk in the
original definition relative to the ontology, to in-
clude the immediate hypernyms (+hyper) and/or hy-
ponyms (+hypo).
4.4 Filtering
Each word sense in the dictionary is marked with a
word class, and the word splitter similarly POS tags
every definition and input to the system. It is nat-
ural to expect that the POS tag of the target word
should match the word class of the word sense, and
this provides a coarse-grained filter for discriminat-
ing homographs with different word classes.
We also experiment with a stop word-based filter
which ignores a closed set of 18 lexicographic mark-
ers commonly found in definitions (e.g. ? [ryaku]
?an abbreviation for ...?), in line with those used by
Nichols et al (2005) in inducing the ontology.
5 Evaluation
We evaluate our various extensions over two
datasets: (1) the example sentences in the Lexeed
sensebank, and (2) the Senseval-2 Japanese dictio-
nary task (Shirai, 2002).
All results below are reported in terms of sim-
ple precision, following the conventions of Senseval
evaluations. For all experiments, precision and re-
call are identical as our systems have full coverage.
For the two datasets, we use two baselines: a ran-
dom baseline and the first-sense baseline. Note that
the first-sense baseline has been shown to be hard
to beat for unsupervised systems (McCarthy et al,
2004), and it is considered supervised when, as in
this case, the first-sense is the most frequent sense
from hand-tagged corpora.
5.1 Lexeed Example Sentences
The goal of these experiments is to tag all the words
that occur in the example sentences in the Lexeed
Sensebank. The first set of experiments over the
Lexeed Sensebank explores three parameters: the
use of characters vs. words, unigrams vs. bigrams,
and original vs. extended definitions. The results of
the experiments and the baselines are presented in
Table 1.
First, characters are in all cases superior to words
as our segment granularity. The introduction of bi-
grams has a uniformly negative impact for both char-
acters and words, due to the effects of data sparse-
ness. This is somewhat surprising for characters,
given that the median word length is 2 characters,
although the difference between character unigrams
and bigrams is slight.
Extended definitions are also shown to be superior
to simple definitions, although the relative increment
in making use of large amounts of sense annotations
is smaller than that of characters vs. words, suggest-
ing that the considerable effort in sense annotating
the definitions is not commensurate with the final
gain for this simple method.
Note that at this stage, our best-performing
method is roughly equivalent to the unsupervised
(random) baseline, but well below the supervised
(first sense) baseline.
Having found that extended definitions improve
results to a small degree, we turn to our next exper-
iment were we investigate whether the introduction
of ontological relations to expand the original def-
initions further enhances our precision. Here, we
persevere with the use of word and characters (all
unigrams), and experiment with the addition of hy-
pernyms and/or hyponyms, with and without the ex-
tended definitions. We also compare our method
directly with that of Banerjee and Pedersen (2002)
over the Lexeed data, and further test the impact
of the sense annotations, in rerunning our experi-
ments with the ontology in a sense-insensitive man-
ner, i.e. by adding in the union of word-level hyper-
nyms and/or hyponyms. The results are described in
Table 2. The results in brackets are reproduced from
earlier tables.
Adding in the ontology makes a significant dif-
ference to our results, in line with the findings of
Banerjee and Pedersen (2002). Hyponyms are better
discriminators than hypernyms (assuming a given
word sense has a hyponym ? the Lexeed ontology
is relatively flat), partly because while a given word
sense will have (at most) one hypernym, it often has
multiple hyponyms (if any at all). Adding in hyper-
nyms or hyponyms, in fact, has a greater impact on
results than simple extended definitions (+extdef),
especially for words. The best overall results are
produced for the (weighted) combination of all on-
tological relations (i.e. extended definitions, hyper-
nyms and hyponyms), achieving a precision level
above both the unsupervised (random) and super-
vised (first-sense) baselines.
In the interests of getting additional insights into
the import of sense annotations in our method, we
ran both the original Banerjee and Pedersen (2002)
method and a sense-insensitive variant of our pro-
posed method over the same data, the results for
which are also included in Table 2. Simple hy-
ponyms (without extended definitions) and word-
based segments returned the best results out of all
the variants tried, at a precision of 0.656. This com-
pares with a precision of 0.683 achieved for the best
778
UNIGRAMS BIGRAMS
ALL WORDS POLYSEMOUS ALL WORDS POLYSEMOUS
Simple Definitions
CHARACTERS 0.523 0.309 0.486 0.262
WORDS 0.469 0.229 0.444 0.201
Extended Definitions
CHARACTERS 0.526 0.313 0.529 0.323
WORDS 0.489 0.258 0.463 0.227
Table 1: Precision over the Lexeed example sentences using simple/extended definitions and word/character
unigrams and bigrams (best-performing method in boldface)
ALL WORDS POLYSEMOUS
UNSUPERVISED BASELINE: 0.527 0.315
SUPERVISED BASELINE: 0.633 0.460
Banerjee and Pedersen (2002) 0.648 0.492
Ontology expansion (sense-sensitive)
simple (0.469) (0.229)
+extdef (0.489) (0.258)
+hypernyms 0.559 0.363
W +hyponyms 0.655 0.503
+def +hyper 0.577 0.386
+def +hypo 0.649 0.490
+def +hyper +hypo 0.683 0.539
simple (0.523) (0.309)
+extdef (0.526) (0.313)
+hypernyms 0.539 0.334
C +hyponyms 0.641 0.481
+def +hyper 0.563 0.365
+def +hypo 0.671 0.522
+def +hyper +hypo 0.671 0.522
Ontology expansion (sense-insensitive)
+hypernyms 0.548 0.348
+hyponyms 0.656 0.503
W +def +hyper 0.551 0.347
+def +hypo 0.649 0.490
+def + hyper +hypo 0.631 0.464
+hypernyms 0.537 0.332
+hyponyms 0.644 0.485
C +def +hyper 0.542 0.335
+def +hypo 0.644 0.484
+def + hyper +hypo 0.628 0.460
Table 2: Precision over the Lexeed exam-
ple sentences using ontology-based gloss extension
(with/without word sense information) and word
(W) and character (C) unigrams (best-performing
method in boldface)
of the sense-sensitive methods, indicating that sense
information enhances WSD performance. This rein-
forces our expectation that richly annotated lexical
resources improve performance. With richer infor-
mation to work with, character based methods uni-
formly give worse results.
While we don?t present the results here due to rea-
sons of space, POS-based filtering had very little im-
pact on results, due to very few POS-differentiated
homographs in Japanese. Stop word filtering leads
ALL
WORDS
POLYSEMOUS
Baselines
Unsupervised (random) 0.310 0.260
Supervised (first-sense) 0.577 0.555
Ontology expansion (sense-sensitive)
W +def +hyper +hypo 0.624 0.605
C +def +hyper +hypo 0.624 0.605
Ontology expansion (sense-insensitive)
W +def +hyper +hypo 0.602 0.581
C +def +hyper +hypo 0.593 0.572
Table 3: Precision over the Senseval-2 data
to a very slight increment in precision across the
board (of the order of 0.001).
5.2 Senseval-2 Japanese Dictionary Task
In our second set of experiments we apply our pro-
posed method to the Senseval-2 Japanese dictionary
task (Shirai, 2002) in order to calibrate our results
against previously published results for Japanese
WSD. Recall that this is a lexical sample task,
and that our evaluation is relative to Lexeed re-
annotations of the same dataset, although the relative
polysemy for the original data and the re-annotated
version are largely the same (Tanaka et al, 2006).
The first sense baselines (i.e. sense skewing) for the
two sets of annotations differ significantly, however,
with a precision of 0.726 reported for the original
task, and 0.577 for the re-annotated Lexeed vari-
ant. System comparison (Senseval-2 systems vs. our
method) will thus be reported in terms of error rate
reduction relative to the respective first sense base-
lines.
In Table 3, we present the results over the
Senseval-2 data for the best-performing systems
from our earlier experiments. As before, we in-
clude results over both words and characters, and
with sense-sensitive and sense-insensitive ontology
expansion.
Our results largely mirror those of Table 2, al-
though here there is very little to separate words
and characters. All methods surpassed both the ran-
dom and first sense baselines, but the relative impact
779
of sense annotations was if anything even less pro-
nounced than for the example sentence task.
Both sense-sensitiveWSDmethods achieve a pre-
cision of 0.624 over all the target words (with one
target word per sentence), an error reduction rate
of 11.1%. This compares favourably with an error
rate reduction of 21.9% for the best of the WSD
systems in the original Senseval-2 task (Kurohashi
and Shirai, 2001), particularly given that our method
is semi-supervised while the Senseval-2 system is a
conventional supervised word sense disambiguator.
6 Conclusion
In our experiments extending the Lesk algorithm
over Japanese data, we have shown that definition
expansion via an ontology produces a significant
performance gain, confirming results by Banerjee
and Pedersen (2002) for English. We also explored
a new expansion of the Lesk method, by measuring
the contribution of sense-tagged definitions to over-
all disambiguation performance. Using sense infor-
mation doubles the error reduction compared to the
supervised baseline, a constant gain that shows the
importance of precise sense information for error re-
duction.
Our WSD system can be applied to all words in
running text, and is able to improve over the first-
sense baseline for two separate WSD tasks, using
only existing Japanese resources. This full-coverage
system opens the way to explore further enhance-
ments, such as the contribution of extra sense-tagged
examples to the expansion, or the combination of
different WSD algorithms.
For future work, we are also studying the in-
tegration of the WSD tool with other applications
that deal with Japanese text, such as a cross-lingual
glossing tool that aids Japanese learners reading text.
Another application we are working on is the inte-
gration of the WSD system with parse selection for
Japanese grammars.
Acknowledgements
This material is supported by the Research Collaboration be-
tween NTT Communication Science Laboratories, Nippon
Telegraph and Telephone Corporation and the University of
Melbourne. We would like to thank members of the NTT Ma-
chine Translation Group and the three anonymous reviewers for
their valuable input on this research.
References
Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense
Disambiguation: Algorithms and Applications. Springer,
Dordrecht, Netherlands.
Timothy Baldwin. 2001. Low-cost, high-performance transla-
tion retrieval: Dumber is better. In Proc. of the 39th Annual
Meeting of the ACL and 10th Conference of the EACL (ACL-
EACL 2001), pages 18?25, Toulouse, France.
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk
algorithm for word sense disambiguation using WordNet. In
Proc. of the 3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing-2002),
pages 136?45, Mexico City, Mexico.
Yee Seng Chan and Hwee Tou Ng. 2005. Scaling up word
sense disambiguation via parallel texts. In Proc. of the 20th
National Conference on Artificial Intelligence (AAAI 2005),
pages 1037?42, Pittsburgh, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Hideo Fujii and W. Bruce Croft. 1993. A comparison of index-
ing techniques for Japanese text retrieval. In Proc. of 16th
International ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?93), pages 237?
46, Pittsburgh, USA.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In Proc. of SIG NLC-159, Tokyo, Japan.
Sadao Kurohashi and Kiyoaki Shirai. 2001. SENSEVAL-2
Japanese tasks. In IEICE Technical Report NLC 2001-10,
pages 1?8. (in Japanese).
Claudia Leacock, Martin Chodorow, and George A. Miller.
1998. Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics, 24(1):147?
65.
Michael Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone from
an ice cream cone. In Proc. of the 1986 SIGDOC Confer-
ence, pages 24?6, Ontario, Canada.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka
Hirano, Hiroshi Matsuda, Kazuma Takaoka, and Masayuki
Asahara. 2003. Japanese Morphological Analysis System
ChaSen Version 2.3.3 Manual. Technical report, NAIST.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll.
2004. Finding predominant senses in untagged text. In
Proc. of the 42nd Annual Meeting of the ACL, pages 280?
7, Barcelona, Spain.
Rada Mihalcea and Timothy Chklovski. 2003. Open Mind
Word Expert: Creating Large Annotated Data Collections
with Web Users? Help. In Proceedings of the EACL
2003 Workshop on Linguistically Annotated Corpora (LINC
2003), pages 53?61, Budapest, Hungary.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI-2005), pages 1111?6, Ed-
inburgh, UK.
Kiyoaki Shirai. 2002. Construction of a word sense tagged
corpus for SENSEVAL-2 japanese dictionary task. In Proc.
of the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 605?8, Las Palmas,
Spain.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Proc. of the Workshop on Frontiers
in Linguistically Annotated Corpora 2006, pages 62?9, Syd-
ney, Australia.
780
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 491?498,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Interpreting Semantic Relations in Noun Compounds via Verb Semantics
Su Nam Kim? and Timothy Baldwin??
? Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
and
? NICTA Victoria Research Lab
University of Melbourne, Victoria 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
We propose a novel method for automat-
ically interpreting compound nouns based
on a predefined set of semantic relations.
First we map verb tokens in sentential con-
texts to a fixed set of seed verbs using
WordNet::Similarity and Moby?s
Thesaurus. We then match the sentences
with semantic relations based on the se-
mantics of the seed verbs and grammatical
roles of the head noun and modifier. Based
on the semantics of the matched sentences,
we then build a classifier using TiMBL.
The performance of our final system at in-
terpreting NCs is 52.6%.
1 Introduction
The interpretation of noun compounds (hereafter,
NCs) such as apple pie or family car is a well-
established sub-task of language understanding.
Conventionally, the NC interpretation task is de-
fined in terms of unearthing the underspecified se-
mantic relation between the head noun and modi-
fier(s), e.g. pie and apple respectively in the case
of apple pie.
NC interpretation has been studied in the con-
text of applications including question-answering
and machine translation (Moldovan et al, 2004;
Cao and Li, 2002; Baldwin and Tanaka, 2004;
Lauer, 1995). Recent work on the automatic/semi-
automatic interpretation of NCs (e.g., Lapata
(2002), Rosario and Marti (2001), Moldovan et al
(2004) and Kim and Baldwin (2005)) has made as-
sumptions about the scope of semantic relations or
restricted the domain of interpretation. This makes
it difficult to gauge the general-purpose utility of
the different methods. Our method avoids any
such assumptions while outperforming previous
methods.
In seminal work on NC interpretation, Finin
(1980) tried to interpret NCs based on hand-coded
rules. Vanderwende (1994) attempted the auto-
matic interpretation of NCs using hand-written
rules, with the obvious cost of manual interven-
tion. Fan et al (2003) estimated the knowledge
required to interpret NCs and claimed that perfor-
mance was closely tied to the volume of data ac-
quired.
In more recent work, Barker and Szpakowicz
(1998) used a semi-automatic method for NC in-
terpretation in a fixed domain. Lapata (2002)
developed a fully automatic method but focused
on nominalizations, a proper subclass of NCs.1
Rosario and Marti (2001) classified the nouns in
medical texts by tagging hierarchical information
using neural networks. Moldovan et al (2004)
used the word senses of nouns based on the do-
main or range of interpretation of an NC, leading
to questions of scalability and portability to novel
domains/NC types. Kim and Baldwin (2005) pro-
posed a simplistic general-purpose method based
on the lexical similarity of unseen NCs with train-
ing instances.
The aim of this paper is to develop an automatic
method for interpreting NCs based on semantic re-
lations. We interpret semantic relations relative to
a fixed set of constructions involving the modifier
and head noun and a set of seed verbs for each
semantic relation: e.g. (the) family owns (a) car
is taken as evidence for family car being an in-
stance of the POSSESSOR relation. We then at-
tempt to map all instances of the modifier and head
noun as the heads of NPs in a transitive senten-
tial context onto our set of constructions via lex-
ical similarity over the verb, to arrive at an inter-
pretation: e.g. we would hope to predict that pos-
sess is sufficiently similar to own that (the) family
possesses (a) car would be recognised as support-
1With nominalizations, the head noun is deverbal, and in
the case of Lapata (2002), nominalisations are assumed to
be interpretable as the modifier being either the subject (e.g.
child behavior) or object (e.g. car lover) of the base verb of
the head noun.
491
ing evidence for the POSSESSOR relation. We use
a supervised classifier to combine together the evi-
dence contributed by individual sentential contexts
of a given modifier?head noun combination, and
arrive at a final interpretation for a given NC.
Mapping the actual verbs in sentences to ap-
propriate seed verbs is obviously crucial to the
success of our method. This is particularly im-
portant as there is no guarantee that we will find
large numbers of modifier?head noun pairings in
the sorts of sentential contexts required by our
method, nor that we will find attested instances
based on the seed verbs. Thus an error in map-
ping an attested verb to the seed verbs could result
in a wrong interpretation or no classification at all.
In this paper, we experiment with the use of Word-
Net (Fellbaum, 1998) and word clusters (based on
Moby?s Thesaurus) in mapping attested verbs to
the seed verbs. We also make use of CoreLex in
dealing with the semantic relation TIME and the
RASP parser (Briscoe and Carroll, 2002) to de-
termine the dependency structure of corpus data.
The data source for our set of NCs is binary
NCs (i.e. NCs with a single modifier) from the
Wall Street Journal component of the Penn Tree-
bank. We deliberately choose to ignore NCs with
multiple modifiers on the grounds that: (a) 88.4%
of NC types in the Wall Street Journal component
of the Penn Treebank and 90.6% of NC types in
the British National Corpus are binary; and (b) we
expect to be able to interpret NCs with multiple
modifiers by decomposing them into binary NCs.
Another simplifying assumption we make is to re-
move NCs incorporating proper nouns since: (a)
the lexical resources we employ in this research
do not contain them in large numbers; and (b)
there is some doubt as to whether the set of seman-
tic relations required to interpret NCs incorporat-
ing proper nouns is that same as that for common
nouns.
The paper is structured as follows. Section 2
takes a brief look at the semantics of NCs and the
basic idea behind the work. Section 3 details the
set of NC semantic relations that is used in our
research, Section 4 presents an extended discus-
sion of our approach, Section 5 briefly explains the
tools we use, Section 6.1 describes how we gather
and process the data, Section 6.2 explains how we
map the verbs to seed verbs, and Section 7 and
Section 8 present the results and analysis of our
approach. Finally we conclude our work in Sec-
tion 9.
2 Motivation
The semantic relation in NCs is the relation be-
tween the head noun (denoted ?H?) and the mod-
ifier(s) (denoted ?M?). We can find this semantic
relation expressed in certain sentential construc-
tions involving the head noun and modifier.
(1) family car
CASE: family owns the car.
FORM: H own M
RELATION: POSSESSOR
(2) student protest
CASE: protest is performed by student.
FORM: M is performed by H
RELATION: AGENT
In the examples above, the semantic relation
(e.g. POSSESSOR) provides an interpretation of
how the head noun and modifiers relate to each
other, and the seed verb (e.g. own) provides a para-
phrase evidencing that relation. For example, in
the case of family car, the family is the POSSES-
SOR of the car, and in student protest, student(s)
are the AGENT of the protest. Note that voice is im-
portant in aligning sentential contexts with seman-
tic relations. For instance, family car can be repre-
sented as car is owned by family (passive) and stu-
dent protest as student performs protest (active).
The exact nature of the sentential context varies
with different synonyms of the seed verbs.
(3) family car
CASE: Synonym=have/possess/belong to
FORM: H own M
RELATION: POSSESSOR
(4) student protest
CASE: Synonym=act/execute/do
FORM: M is performed by H
RELATION: AGENT
The verb own in the POSSESSOR relation has
the synonyms have, possess and belong to. In
the context of have and possess, the form of re-
lation would be same as the form with verb, own.
However, in the context of belong to, family car
492
would mean that the car belongs to family. Thus,
even when the voice of the verb is the same
(voice=active), the grammatical role of the head
noun and modifier can change.
In our approach we map the actual verbs in sen-
tences containing the head noun and modifiers to
seed verbs corresponding to the relation forms.
The set of seed verbs contains verbs representa-
tive of each semantic relation form. We chose two
sets of seed verbs of size 57 and 84, to examine
how the coverage of actual verbs by seed verbs af-
fects the performance of our method. Initially, we
manually chose a set of 60 seed verbs. We then
added synonyms from Moby?s thesaurus for some
of the 60 verbs. Finally, we filtered verbs from the
two expanded sets, since these verbs occur very
frequently in the corpus (as this might skew the
results). The set of seed verbs {have, own, pos-
sess, belong to} are in the set of 57 seed verbs,
and {acquire, grab, occupy} are added to the set
of 84 seed verbs; all correspond to the POSSES-
SOR relation.
For each relation, we generate a set of con-
structional templates associating a subset of seed
verbs with appropriate grammatical relations for
the head noun and modifier. Examples for POS-
SESSOR are:
S({have, own, possess}V,M SUBJ,H OBJ) (5)
S({belong to}V,H SUBJ,M OBJ) (6)
where V is the set of seed verbs, M is the modifier
and H is the head noun.
Two relations which do not map readily onto
seed verbs are TIME (e.g. winter semester) and
EQUATIVE (e.g. composer arranger). Here, we
rely on an independent set of contextual evidence,
as outlined in Section 6.1.
Through matching actual verbs attested in cor-
pus data onto seed verbs, we can match sentences
with relations (see Section 6.2). Using this method
we can identify the matching relation forms of se-
mantic relations to decide the semantic relation for
NCs.
3 Semantic Relations in Compound
Nouns
While there has been wide recognition of the need
for a system of semantic relations with which to
classify NCs, there is still active debate as to what
the composition of that set should be, or indeed
RASP parser
Raw Sentences
Modified Sentences
Final Sentences
Classifier
Semantic Relation
Pre?processing
Collect Subj, Obj, PP, PPN, V, T
Filter sentences
Get sentences with H,M
Verb?Mapping
map verbs onto seed verbs
Match modified sentences
wrt relation forms
Moby?s Thesaurus
WordNet::Similarity
Classifier:Timbl
Noun Compound
Figure 1: System Architecture
whether it is reasonable to expect that all NCs
should be interpretable with a fixed set of semantic
relations.
Based on the pioneering work on Levi (1979)
and Finin (1980), there have been efforts in com-
putational linguistics to arrive at largely task-
specific sets of semantic relations, driven by the
annotation of a representative sample of NCs from
a given corpus type (Vanderwende, 1994; Barker
and Szpakowicz, 1998; Rosario and Marti, 2001;
Moldovan et al, 2004). In this paper, we use the
set of 20 semantic relations defined by Barker and
Szpakowicz (1998), rather than defining a new set
of relations. The main reasons we chose this set
are: (a) that it clearly distinguishes between the
head noun and modifiers, and (b) there is clear
documentation of each relation, which is vital for
NC annotation effort. The one change we make
to the original set of 20 semantic relations is to ex-
clude the PROPERTY relation since it is too general
and a more general form of several other relations
including MATERIAL (e.g. apple pie).
4 Method
Figure 1 outlines the system architecture of our
approach. We used three corpora: the Brown
corpus (as contained in the Penn Treebank), the
Wall Street Journal corpus (also taken from the
Penn treebank), and the written component of the
British National Corpus (BNC). We first parsed
each of these corpora using RASP (Briscoe and
Carroll, 2002), and identified for each verb to-
ken the voice, head nouns of the subject and
object, and also, for each PP attached to that
verb, the head preposition and head noun of the
493
NP (hereafter, PPN). Next, for our test NCs, we
identified all verbs for which the modifier and
head noun co-occur as subject, object, or PPN.
We then mapped these verbs to seed verbs us-
ing WordNet::Similarity and Moby?s The-
saurus (see Section 5 for details). Finally, we iden-
tified the corresponding relation for each seed verb
and selected the best-fitting semantic relation us-
ing a classifier. To evaluate our approach, we built
a classifier using TiMBL (Daelemans et al, 2004).
5 Resources
In this section, we outline the tools and resources
employed in our method.
As our parser, we used RASP, generating a
dependency representation for the most probable
parse for each sentence. Note that RASP also lem-
matises all words in a POS-sensitive manner.
To map actual verbs onto seed verbs,
we experimented with two resources:
WordNet::Similarity and Moby?s the-
saurus. WordNet::Similarity2 is an open
source software package that allows the user
to measure the semantic similarity or related-
ness between two words (Patwardhan et al,
2003). Of the many methods implemented in
WordNet::Similarity, we report on results
for one path-based method (WUP, Wu and Palmer
(1994)), one content-information based method
(JCN, Jiang and Conrath (1998)) and two semantic
relatedness methods (LESK, Banerjee and Peder-
sen (2003), and VECTOR, (Patwardhan, 2003)).
We also used a random similarity-generating
method as a baseline (RANDOM).
The second semantic resource we use for verb-
mapping method is Moby?s thesaurus. Moby?s
thesaurus is based on Roget?s thesaurus, and con-
tains 30K root words, and 2.5M synonyms and re-
lated words. Since the direct synonyms of seed
verbs have limited coverage over the set of sen-
tences used in our experiment, we also experi-
mented with using second-level indirect synonyms
of seed verbs.
In order to deal with the TIME relation, we used
CoreLex (Buitelaar, 1998). CoreLex is based on a
unified approach to systematic polysemy and the
semantic underspecification of nouns, and derives
from WordNet 1.5. It contains 45 basic CoreLex
types, systematic polysemous classes and 39,937
nouns with tags.
2www.d.umn.edu/ tpederse/similarity.html
As mentioned earlier, we built our supervised
classifier using TiMBL.
6 Data Collection
6.1 Data Processing
To test our method, we extracted 2,166 NC types
from the Wall Street Journal (WSJ) component of
the Penn Treebank. We additionally extracted sen-
tences containing the head noun and modifier in
pre-defined constructional contexts from the amal-
gam of: (1) the Brown Corpus subset contained
in the Penn Treebank, (2) the WSJ portion of the
Penn Treebank, and (3) the British National Cor-
pus (BNC). Note that while these pre-defined con-
structional contexts are based on the contexts in
which our seed verbs are predicted to correlate
with a given semantic relation, we instances of all
verbs occurring in those contexts. For example,
based on the construction in Equation 5, we ex-
tract all instances of S(Vi,M SUBJj ,H OBJj ) for all
verbs Vi and all instances of NCj = (Mj ,Hj) in
our dataset.
Two annotators tagged the 2,166 NC types in-
dependently at 52.3% inter-annotator agreement,
and then met to discus all contentious annotations
and arrive at a mutually-acceptable gold-standard
annotation for each NC. The Brown, WSJ and
BNC data was pre-parsed with RASP, and sen-
tences were extracted which contained the head
noun and modifier of one of our 2,166 NCs in sub-
ject or object position, or as (head) noun within the
NP of an PP. After extracting these sentences, we
counted the frequencies of the different modifier?
head noun pairs, and filtered out: (a) all construc-
tional contexts not involving a verb contained in
WordNet 2.0, and (b) all NCs for which the modi-
fier and head noun did not co-occur in at least five
sentential contexts. This left us with a total of 453
NCs for training and testing. The combined total
number of sentential contexts for our 453 NCs was
7,714, containing 1,165 distinct main verbs.
We next randomly split the NC data into 80%
training data and 20% test data. The final number
of test NCs is 88; the final number of training NCs
varies depending on the verb-mapping method.
As noted in Section 2, the relations TIME and
EQUATIVE are not associated with seed verbs. For
TIME, rather than using contextual evidence, we
simply flag the possibility of a TIME if the modifier
is found to occur in the TIME class of CoreLex. In
the case of TIME, we consider coordinated occur-
494
ACTBENEFITHAVE
USE
PLAYPERFORM
...
...
Seed verbs
accept
act
agree HOLD
.
.
.
.
.
Verb?MappingMethods
AGENTBENEFICIARYCONTAINER
OBJECTPOSSESSOR
INSTRUMENT...
...
...
Semantic RelationsOriginal verbs
accommodate
Figure 2: Verb mapping
rences of the modifier and head noun (e.g. coach
and player for player coach) as evidence for the
relation.3 We thus separately collate statistics
from coordinated NPs for each NC, and from this
compute a weight for each NC based on mutual
information:
TIME(NCi) = ?log2
freq(coord(Mi, Hi))
freqMi ? freq(Hi)
(7)
where Mi and Hi are the modifier and head of
NCi, respectively, and freq(coord(Mi,Hi)) is the
frequency of occurrence of Mi and Hi in coordi-
nated NPs.
Finally, we calculate a normalised weight for
each seed verb by determining the proportion of
head verbs each seed verb occurs with.
6.2 Verb Mapping
The sentential contexts gathered from corpus
data contain a wide range of verbs, not just
the seed verbs. To map the verbs onto seed
verbs, and hence estimate which semantic rela-
tion(s) each is a predictor of, we experimented
with two different methods. First we used the
WordNet::Similarity package to calculate
the similarity between a given verb and each
of the seed verbs, experimenting with the 5
methods mentioned in Section 5. Second, we
used Moby?s thesaurus to extract both direct syn-
onyms (D-SYNONYM) and a combination of direct
and second-level indirect synonyms of verbs (I-
SYNONYM), and from this, calculate the closest-
matching seed verb(s) for a given verb.
Figure 2 depicts the procedure for mapping
verbs in constructional contexts onto the seed
verbs. Verbs found in the various contexts in the
3Note the order of the modifier and head in coordinated
NPs is considered to be irrelevant, i.e. player and coach and
coach and player are equally evidence for an EQUATIVE inter-
pretation for player coach (and coach player).
accomplish achieve behave conduct ...
ACT
act conduct deadl with function perform play
LEVEL=1
LEVEL=2
synonym in level1 synonym in level2 not found in level1
Figure 3: Expanding synonyms
# of SeedVB D-Synonyms D/I-Synonyms
57 6,755(87.6%) 7,388(95.8%)
84 6,987(90.6%) 7,389(95.8%)
Table 1: Coverage of D and D/I-Synonyms
corpus (on the left side of the figure) map onto one
or more seed verbs, which in turn map onto one
or more semantic relations.4 We replace all non-
seed verbs in the corpus data with the seed verb(s)
they map onto, potentially increasing the number
of corpus instances.
Since direct (i.e. level 1) synonyms from
Moby?s thesaurus are not sufficient to map all
verbs onto seed verbs, we also include second-
level (i.e. level 2) synonyms, expanding from di-
rect synonyms. Table 1 shows the coverage of
sentences for test NCs, in which D indicates direct
synonyms and I indicates indirect synonyms.
7 Evaluation
We evaluated our method over both 17 semantic
relations (without EQUATIVE and TIME) and the full
19 semantic relations, due to the low frequency
and lack of verb-based constructional contexts for
EQUATIVE and TIME, as indicated in Table 2. Note
that the test data set is the same for both sets of
semantic relations, but that the training data in
the case of 17 semantic relations will not con-
tain any instances for the EQUATIVE and TIME re-
lations, meaning that all such test instances will
be misclassified. The baseline for all verb map-
ping methods is a simple majority-class classifier,
which leads to an accuracy of 42.4% for the TOPIC
relation. In evaluation, we use two different val-
ues for our method: Count and Weight. Count
is based on the raw number of corpus instances,
while Weight employs the seed verb weight de-
scribed in Section 6.1.
4There is only one instance of a seed verb mapping to
multiple semantic relations, namely perform which corre-
sponds to the two relations AGENT and OBJECT.
495
# of SR # SeedV Method WUP JCN RANDOM LESK VECTOR D-SYNONYM I-SYNONYM
17 Baseline .423 .423 .423 .423 .423 .423 .423
57 Count .324 .408 .379 .416 .466 .337 .337
Weight .320 .408 .371 .416 .466 .337 .342
84 Count .406 .470 .184 .430 .413 .317 .333
Weight .424 .426 .259 .457 .526 .341 .406
19 Baseline .409 .409 .409 .409 .409 .409 .409
57 Count .315 .420 .384 .440 .466 .350 .337
Weight .311 .420 .376 .440 .466 .350 .342
84 Count .413 .470 .200 .414 .413 .321 .333
Weight .439 .446 .280 .486 .526 .356 .393
Table 2: Results with 17 relations and with 19 relations
#of SR # SeedVB Method WUP JCN RANDOM LESK VECTOR
17 Baseline .423 .423 .423 .423 .423
57 Count .423 .385 .379 .413 .466
Weight .423 .385 .379 .413 .466
84 Count .325 .439 .420 .484 .466
Weight .281 .393 .317 .476 .466
19 Baseline .409 .409 .409 .409 .409
57 Count .423 .397 .392 .413 .413
Weight .423 .397 .392 .413 .500
84 Count .333 .439 .425 .484 .413
Weight .290 .410 .317 .484 .413
Table 3: Results of combining the proposed method and with the method of Kim and Baldwin (2005)
As noted above, we excluded all NCs for which
we were unable to find at least 5 instances of the
modifier and head noun in an appropriate senten-
tial context. This exclusion reduced the original
set of 2,166 NCs to only 453, meaning that the
proposed method is unable to classify up to 80% of
NCs. For real-world applications, a method which
is only able to arrive at a classification for 20% of
instances is clearly of limited utility, and we need
some way of expanding the coverage of the pro-
posed method. This is achieved by adapting the
similarity method proposed by Kim and Baldwin
(2005) to our task, wherein we use lexical simi-
larity to identify the nearest-neighbour NC for a
given NC, and classify the given NC according to
the classification for the nearest-neighbour. The
results for the combined method are presented in
Table 3.
8 Discussion
For the basic method, as presented in Table 2, the
classifier produced similar results over the 17 se-
mantic relations to the 19 semantic relations. Us-
ing data from Weight and Count for both 17 and
19 semantic relations, the classifier achieved the
best performance with VECTOR (context vector-
based distributional similarity), followed by JCN
and LESK. The main reason is that VECTOR is
more conservative than the other methods at map-
ping (original) verbs onto seed verbs, i.e. the aver-
age number of seed verbs a given verb maps onto
is small. For the other methods, the semantics of
the original sentences are often not preserved un-
der verb mapping, introducing noise to the classi-
fication task.
Comparing the two sets of semantic relations
(17 vs. 19), the set with more semantic rela-
tions achieved slightly better performance in most
cases. A detailed breakdown of the results re-
vealed that TIME both has an above-average clas-
sification accuracy and is associated with a rela-
tively large number of test NCs, while EQUATIVE
has a below-average classification accuracy but is
associated with relatively few instances.
While an increased number of seed verbs gener-
ates more training instances under verb mapping,
it is imperative that the choice of seed verbs be
made carefully so that they not introduce noise
into the classifier and reducing overall perfor-
mance. Figure 4 is an alternate representation of
the numbers from Table 2, with results for each in-
dividual method over 57 and 84 seed verbs juxta-
posed for each of Count andWeight. From this, we
get the intriguing result that Count generally per-
forms better over fewer seed verbs, while Weight
performs better over more seed verbs.
496
WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,I
Result with Count
Verb?mapping method
Accuracy(%)
WUP JCN RANDOM LESK VECTOR SYN?D SYN?D,IVerb?mapping method
Accuracy(%)
Result with Weight
 0
 20
 40
 60
 80
 100
w/ 57 seed verbs
w/ 84 seed verbs
 0
 20
 40
 60
 80
 100
w/ 57 seed verbs
w/ 84 seed verbs
Figure 4: Performance with 57 vs. 84 seed verbs
#of SR # SeedVB WUP LCH JCN LIN RANDOM LESK VECTOR
17 Baseline .433 .433 .441 .441 .433 .477 .428
57 .449 .421 .415 .337 .409 .469 .344
Baseline .433 .433 .433 .433 .428 .438 .444
84 .476 .416 .409 .349 .226 .465 .333
19 Baseline .418 .418 .430 .430 .418 .477 .413
57 .465 .418 .417 .341 .232 .462 .344
Baseline .413 .413 .418 .418 .413 .438 .426
84 .471 .413 .407 .348 .218 .465 .320
Table 4: Results for the method of Kim and Baldwin (2005) over the test set used in this research
For the experiment where we combine our
method with that of Kim and Baldwin (2005), as
presented in Table 3, we find a similar pattern of
results to the proposed method. Namely, VECTOR
and LESK achieve the best performance, with mi-
nor variations in the absolute performance relative
to the original method but the best results for each
relation set actually dropping marginally over the
original method. This drop is not surprising when
we consider that we use an imperfect method to
identify the nearest neighbour for an NC for which
we are unable to find corpus instances in sufficient
numbers, and then a second imperfect method to
classify the instance.
Compared to previous work, our method pro-
duces reasonably stable performance when op-
erated over the open-domain data with small
amounts of training data. Rosario and Marti
(2001) achieved about 60% using a neural net-
work in a closed domain, Moldovan et al (2004)
achieved 43% using word sense disambiguation
of the head noun and modifier over open domain
data, and Kim and Baldwin (2005) produced 53%
using lexical similarities of the head noun and
modifier (using the same relation set, but evaluated
over a different dataset). The best result achieved
by our system was 52.6% over open-domain data,
using a general-purpose relation set.
To get a better understanding of how our
method compares with that of Kim and Baldwin
(2005), we evaluated the method of Kim and Bald-
win (2005) over the same data set as used in this
research, the results of which are presented in Ta-
ble 4. The relative results for the different sim-
ilarity metrics mirror those reported in Kim and
Baldwin (2005). WUP produced the best perfor-
mance at 47-48% for the two relation sets, sig-
nificantly below the accuracy of our method at
53.3%. Perhaps more encouraging is the result
that the combined method?where we classify at-
tested instances according to the proposed method,
and classify unattested instances according to the
nearest-neighbour method of Kim and Baldwin
(2005) and the classifications from the proposed
method?outperforms the method of Kim and
Baldwin (2005). That is, the combined method
has the coverage of the method of Kim and Bald-
win (2005), but inherits the higher accuracy of the
method proposed herein. Having said this, the per-
formance of the Kim and Baldwin (2005) method
over PRODUCT, TOPIC, LOCATION and SOURCE is
superior to that of our method. In this sense,
we believe that alternate methods of hybridisation
may lead to even better results.
Finally, we wish to point out that the method
as presented here is still relatively immature, with
considerable scope for improvement. In its current
form, we do not weight the different seed verbs
497
based on their relative similarity to the original
verb. We also use the same weight and frequency
for each seed verb relative to a given relation, de-
spite seed verbs being more indicative of a given
relation and also potentially occurring more often
in the corpus. For instance, possess is more related
to POSSESSOR than occupy. Also possess occurs
more often in the corpus than belong to. As future
work, we intend to investigate whether allowances
for these considerations can improve the perfor-
mance of our method.
9 Conclusion
In this paper, we proposed a method for au-
tomatically interpreting noun compounds based
on seed verbs indicative of each semantic re-
lation. For a given modifier and head noun,
our method extracted corpus instances of the
two nouns in a range of constructional contexts,
and then mapped the original verbs onto seed
verbs based on lexical similarity derived from
WordNet::Similarity, and Moby?s The-
saurus. These instances were then fed into the
TiMBL learner to build a classifier. The best-
performing method was VECTOR, which is a con-
text vector distributional similarity method. We
also experimented with varying numbers of seed
verbs, and found that generally the more seed
verbs, the better the performance. Overall, the
best-performing system achieved an accuracy of
52.6% with 84 seed verbs and the VECTOR verb-
mapping method.
Acknowledgements
Wewould like to thank the members of the Univer-
sity of Melbourne LT group and the three anony-
mous reviewers for their valuable input on this re-
search.
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it right.
In In Proceedings of the ACL 2004 Workshop on Multi-
word Expressions: Integrating Processing, pages 24?31,
Barcelona, Spain.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss
overlaps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805?810, Acapulco, Mex-
ico.
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics, pages 96?102, Quebec, Canada.
Ted Briscoe and John Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of the 3rd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-
lands.
Paul Buitelaar. 1998. CoreLex: Systematic Polysemy and
Underspecification. Ph.D. thesis, Brandeis University,
USA.
Yunbo Cao and Hang Li. 2002. Base noun phrase translation
using web data and the EM algorithm. In 19th Interna-
tional Conference on Computational Linguistics, Taipei,
Taiwan.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. ILK Technical Re-
port 04-02.
James Fan, Ken Barker, and Bruce W. Porter. 2003. The
knowledge required to interpret noun compounds. In 7th
International Joint Conference on Artificial Intelligence,
pages 1483?1485, Acapulco, Mexico.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Timothy W. Finin. 1980. The semantic interpretation of
compound nominals. Ph.D. thesis, University of Illinois,
Urbana, Illinois, USA.
Jay Jiang and David Conrath. 1998. Semantic similar-
ity based on corpus statistics and lexical taxonomy. In
Proceedings on International Conference on Research in
Computational Linguistics, pages 19?33.
Su Nam Kim and Timothy Baldwin. 2005. Automatic in-
terpretation of noun compounds using WordNet similar-
ity. In Second International Joint Conference On Natural
Language Processing, pages 945?956, JeJu, Korea.
Maria Lapata. 2002. The disambiguation of nominalizations.
Computational Linguistics, 28(3):357?388.
Mark Lauer. 1995. Designing Statistical Language Learn-
ers: Experiments on Noun Compounds. Ph.D. thesis,
Macquarie University.
Judith Levi. 1979. The Syntax and Semantics of Complex
Nominals. New York: Academic Press.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel An-
tohe, and Roxana Girju. 2004. Models for the seman-
tic classification of noun phrases. In HLT-NAACL 2004:
Workshop on Computational Lexical Semantics, pages 60?
67, Boston, USA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. 2003. Using measures of semantic relatedness for
word sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Processing
and Computational Linguistics.
Siddharth Patwardhan. 2003. Incorporating dictionary and
corpus information into a context vector measure of se-
mantic relatedness. Master?s thesis, University of Min-
nesota, USA.
Barbara Rosario and Hearst Marti. 2001. Classifying the se-
mantic relations in noun compounds via a domain-specific
lexical hierarchy. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Processing,
pages 82?90.
Lucy Vanderwende. 1994. Algorithm for automatic inter-
pretation of noun sequences. In Proceedings of the 15th
Conference on Computational linguistics, pages 782?788.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics and
lexical selection. In 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 133 ?138.
498
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 65?72,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Automatic Identification of English Verb Particle Constructions
using Linguistic Features
Su Nam Kim and Timothy Baldwin
Department of Computer Science and Software Engineering
University of Melbourne, Victoria 3010 Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
This paper presents a method for identify-
ing token instances of verb particle con-
structions (VPCs) automatically, based on
the output of the RASP parser. The pro-
posed method pools together instances of
VPCs and verb-PPs from the parser out-
put and uses the sentential context of each
such instance to differentiate VPCs from
verb-PPs. We show our technique to per-
form at an F-score of 97.4% at identifying
VPCs in Wall Street Journal and Brown
Corpus data taken from the Penn Tree-
bank.
1 Introduction
Multiword expressions (hereafter MWEs) are
lexical items that can be decomposed into multi-
ple simplex words and display lexical, syntactic
and/or semantic idiosyncracies (Sag et al, 2002;
Calzolari et al, 2002). In the case of English,
MWEs are conventionally categorised syntactico-
semantically into classes such as compound nom-
inals (e.g. New York, apple juice, GM car), verb
particle constructions (e.g. hand in, battle on),
non-decomposable idioms (e.g. a piece of cake,
kick the bucket) and light-verb constructions (e.g.
make a mistake). MWE research has focussed
largely on their implications in language under-
standing, fluency and robustness (Pearce, 2001;
Sag et al, 2002; Copestake and Lascarides, 1997;
Bannard et al, 2003; McCarthy et al, 2003; Wid-
dows and Dorow, 2005). In this paper, our goal
is to identify individual token instances of En-
glish verb particle constructions (VPCs hereafter)
in running text.
For the purposes of this paper, we follow Bald-
win (2005) in adopting the simplifying assump-
tion that VPCs: (a) consist of a head verb and a
unique prepositional particle (e.g. hand in, walk
off); and (b) are either transitive (e.g. hand in, put
on) or intransitive (e.g. battle on). A defining char-
acteristic of transitive VPCs is that they can gen-
erally occur with either joined (e.g. He put on the
sweater) or split (e.g. He put the sweater on) word
order. In the case that the object is pronominal,
however, the VPC must occur in split word order
(c.f. *He handed in it) (Huddleston and Pullum,
2002; Villavicencio, 2003).
The semantics of the VPC can either derive
transparently from the semantics of the head verb
and particle (e.g. walk off ) or be significantly re-
moved from the semantics of the head verb and/or
particle (e.g. look up); analogously, the selectional
preferences of VPCs can mirror those of their head
verbs or alternatively diverge markedly. The syn-
tax of the VPC can also coincide with that of the
head verb (e.g. walk off ) or alternatively diverge
(e.g. lift off ).
In the following, we review relevant past
research on VPCs, focusing on the extrac-
tion/identification of VPCs and the prediction of
the compositionality/productivity of VPCs.
There is a modest body of research on the iden-
tification and extraction of VPCs. Note that in
the case of VPC identification we seek to detect
individual VPC token instances in corpus data,
whereas in the case of VPC extraction we seek
to arrive at an inventory of VPC types/lexical
items based on analysis of token instances in cor-
pus data. Li et al (2003) identify English VPCs
(or ?phrasal verbs? in their parlance) using hand-
coded regular expressions. Baldwin and Villavi-
cencio (2002) extract a simple list of VPCs from
corpus data, while Baldwin (2005) extracts VPCs
with valence information under the umbrella of
deep lexical acquisition.1 The method of Baldwin
(2005) is aimed at VPC extraction and takes into
account only the syntactic features of verbs. In this
paper, our interest is in VPC identification, and we
make use of deeper semantic information.
In Fraser (1976) and Villavicencio (2006) it is
argued that the semantic properties of verbs can
determine the likelihood of their occurrence with
1The learning of lexical items in a form that can be fed
directly into a deep grammar or other richly-annotated lexical
resource
65
particles. Bannard et al (2003) and McCarthy et
al. (2003) investigate methods for estimating the
compositionality of VPCs based largely on dis-
tributional similarity of the head verb and VPC.
O?Hara and Wiebe (2003) propose a method for
disambiguating the verb sense of verb-PPs. While
our interest is in VPC identification?a fundamen-
tally syntactic task?we draw on the shallow se-
mantic processing employed in these methods in
modelling the semantics of VPCs relative to their
base verbs.
The contribution of this paper is to combine
syntactic and semantic features in the task of VPC
identification. The basic intuition behind the pro-
posed method is that the selectional preferences of
VPCs over predefined argument positions,2 should
provide insight into whether a verb and preposi-
tion in a given sentential context combine to form
a VPC (e.g. Kim handed in the paper) or alter-
natively constitute a verb-PP (e.g. Kim walked in
the room). That is, we seek to identify individual
preposition token instances as intransitive preposi-
tions (i.e. prepositional particles) or transitive par-
ticles based on analysis of the governing verb.
The remainder of the paper is structured as fol-
lows. Section 2 outlines the linguistic features of
verbs and their co-occuring nouns. Section 3 pro-
vides a detailed description of our technique. Sec-
tion 4 describes the data properties and the identi-
fication method. Section 5 contains detailed evalu-
ation of the proposed method. Section 6 discusses
the effectiveness of our approach. Finally, Sec-
tion 7 summarizes the paper and outlines future
work.
2 Linguistic Features
When verbs co-occur with particles to form VPCs,
their meaning can be significantly different from
the semantics of the head verb in isolation. Ac-
cording to Baldwin et al (2003), divergences in
VPC and head verb semantics are often reflected
in differing selectional preferences, as manifested
in patterns of noun co-occurrence. In one example
cited in the paper, the cosine similarity between
cut and cut out, based on word co-occurrence vec-
tors, was found to be greater than that between cut
and cut off, mirroring the intuitive compositional-
ity of these VPCs.
(1) and (2) illustrate the difference in the selec-
tional preferences of the verb put in isolation as
compared with the VPC put on.3
2Focusing exclusively on the subject and object argument
positions.
3All sense definitions are derived from WordNet 2.1.
(1) put = place
EX: Put the book on the table.
ARGS: bookOBJ = book, publication, object
ANALYSIS: verb-PP
(2) put on = wear
EX: Put on the sweater .
ARGS: sweaterOBJ = garment, clothing
ANALYSIS: verb particle construction
While put on is generally used in the context of
wearing something, it usually occurs with clothing-
type nouns such as sweater and coat, whereas the
simplex put has less sharply defined selectional re-
strictions and can occur with any noun. In terms
of the word senses of the head nouns of the ob-
ject NPs, the VPC put on will tend to co-occur
with objects which have the semantics of clothes
or garment. On the other hand, the simplex verb
put in isolation tends to be used with objects with
the semantics of object and prepositional phrases
containing NPs with the semantics of place.
Also, as observed above, the valence of a VPC
can differ from that of the head verb. (3) and (4)
illustrate two different senses of take off with in-
transitive and transitive syntax, respectively. Note
that take cannot occur as a simplex intransitive
verb.
(3) take off = lift off
EX: The airplane takes off.
ARGS: airplaneSUBJ = airplane, aeroplane
ANALYSIS: verb particle construction
(4) take off = remove
EX: They take off the cape .
ARGS: theySUBJ = person, individual
capeOBJ = garment, clothing
ANALYSIS: verb particle construction
Note that in (3), take off = lift off co-occurs with
a subject of the class airplane, aeroplane. In (4), on
the other hand, take off = remove and the corre-
sponding object noun is of class garment or cloth-
ing. From the above, we can see that head nouns
in the subject and object argument positions can
be used to distinguish VPCs from simplex verbs
with prepositional phrases (i.e. verb-PPs).
66
3 Approach
Our goal is to distinguish VPCs from verb-PPs in
corpus data, i.e. to take individual inputs such as
Kim handed the paper in today and tag each as
either a VPC or a verb-PP. Our basic approach is
to parse each sentence with RASP (Briscoe and
Carroll, 2002) to obtain a first-gloss estimate of
the VPC and verb-PP token instances, and also
identify the head nouns of the arguments of each
VPC and simplex verb. For the head noun of each
subject and object, as identified by RASP, we use
WordNet 2.1 (Fellbaum, 1998) to obtain the word
sense. Finally we build a supervised classifier us-
ing TiMBL 5.1 (Daelemans et al, 2004).
3.1 Method
Compared to the method proposed by Baldwin
(2005), our approach (a) tackles the task of VPC
identification rather than VPC extraction, and (b)
uses both syntactic and semantic features, employ-
ing the WordNet 2.1 senses of the subject and/or
object(s) of the verb. In the sentence He put the
coat on the table, e.g., to distinguish the VPC put
on from the verb put occurring with the preposi-
tional phrase on the table, we identify the senses
of the head nouns of the subject and object(s) of
the verb put (i.e. he and coat, respectively).
First, we parse all sentences in the given corpus
using RASP, and identify verbs and prepositions
in the RASP output. This is a simple process of
checking the POS tags in the most-probable parse,
and for both particles (tagged RP) and transitive
prepositions (tagged II) reading off the governing
verb from the dependency tuple output (see Sec-
tion 3.2 for details). We also retrieved the head
nouns of the subject and object(s) of each head
verb directly from the dependency tuples. Using
WordNet 2.1, we then obtain the word sense of the
head nouns.
The VPCs or verb-PPs are represented with cor-
responding information as given below:
P (type|v, p,wsSUBJ,wsDOBJ,ws IOBJ)
where type denotes either a VPC or verb-PP, v is
the head verb, p is the preposition, and ws* is the
word sense of the subject, direct object or indirect
object.
Once all the data was gathered, we separated it
into test and training data. We then used TiMBL
5.1 to learn a classifier from the training data,
which was then run and evaluated over the test
data. See Section 5 for full details of the results.
Figure 1 depicts the complete process used to
distinguish VPCs from verb-PPs.
text
raw
Particles Objects
Senses
corpus
Subjects
WordNet
Word
v+p with Semantics
Verbs
TiMBL Classifier
look_after := [..
put_on := [..
take_off := [..
e.g.
Preprocessing RASPparser
Figure 1: System Architecture
3.2 On the use of RASP, WordNet and
TiMBL
RASP is used to identify the syntactic structure
of each sentence, including the head nouns of ar-
guments and first-gloss determination of whether
a given preposition is incorporated in a VPC or
verb-PP. The RASP output contains dependency
tuples derived from the most probable parse, each
of which includes a label identifying the nature
of the dependency (e.g. SUBJ, DOBJ), the head
word of the modifying constituent, and the head of
the modified constituent. In addition, each word
is tagged with a POS tag from which it is possi-
ble to determine the valence of any prepositions.
McCarthy et al (2003) evaluate the precision of
RASP at identifying VPCs to be 87.6% and the re-
call to be 49.4%. However the paper does not eval-
uate the parser?s ability to distinguish sentences
containing VPCs and sentences with verb-PPs.
To better understand the baseline performance
of RASP, we counted the number of false-positive
examples tagged with RP and false-negative ex-
amples tagged with II, relative to gold-standard
data. See Section 5 for details.
We use WordNet to obtain the first-sense word
sense of the head nouns of subject and object
phrases, according to the default word sense rank-
ing provided within WordNet. McCarthy et al
(2004) found that 54% of word tokens are used
with their first (or default) sense. With the per-
formance of current word sense disambiguation
(WSD) systems hovering around 60-70%, a sim-
ple first-sense WSD system has room for improve-
ment, but is sufficient for our immediate purposes
67
in this paper.
To evaluate our approach, we built a super-
vised classifier using the TiMBL 5.1 memory-
based learner and training data extracted from the
Brown and WSJ corpora.
4 Data Collection
We evaluated out method by running RASP over
Brown Corpus and Wall Street Journal, as con-
tained in the Penn Treebank (Marcus et al, 1993).
4.1 Data Classification
The data we consider is sentences containing
prepositions tagged as either RP or II. Based on
the output of RASP, we divide the data into four
groups:
Group A Group BGroup C
RP & II tagged dataRP tagged data II tagged data
Group D
Group A contains the verb?preposition token
instances tagged tagged exclusively as VPCs (i.e.
the preposition is never tagged as II in combi-
nation with the given head verb). Group B con-
tains the verb?preposition token instances iden-
tified as VPCs by RASP where there were also
instances of that same combination identified as
verb-PPs. Group C contains the verb?preposition
token instances identified as verb-PPs by RASP
where there were also instances of that same com-
bination identified as VPCs. Finally, group D
contains the verb-preposition combinations which
were tagged exclusively as verb-PPs by RASP.
We focus particularly on disambiguating verb?
preposition token instances falling into groups B
and C, where RASP has identified an ambiguity
for that particular combination. We do not further
classify token instances in group D, on the grounds
that (a) for high-frequency verb?preposition com-
binations, RASP was unable to find a single in-
stance warranting a VPC analysis, suggesting it
had high confidence in its ability to correctly iden-
tify instances of this lexical type, and (b) for low-
frequency verb?preposition combinations where
the confidence of there definitively no being a
VPC usage is low, the token sample is too small
to disambiguate effectively and the overall impact
would be negligible even if we tried. We do, how-
ever, return to considered data in group D in com-
puting the precision and recall of RASP.
Naturally, the output of RASP parser is not
error-free, i.e. VPCs may be parsed as verb-PPs
FPR FNR Agreement
Group A 4.08% ? 95.24%
Group B 3.96% ? 99.61%
Group C ? 10.15% 93.27%
Group D ? 3.4% 99.20%
Table 1: False positive rate (FPR), false negative
rate (FNR) and inter-annotator agreement across
the four groups of token instances
f ? 1 f ? 5
VPC V-PP VPC V-PP
Group A 5,223 0 3,787 0
Group B 1,312 0 1,108 0
Group C 0 995 0 217
Total 6,535 995 4,895 217
Table 2: The number of VPC and verb-PP token
instances occurring in groups A, B and C at vary-
ing frequency cut-offs
and vice versa. In particular, other than the re-
ported results of McCarthy et al (2003) targeting
VPCs vs. all other analyses, we had no a priori
sense of RASP?s ability to distinguish VPCs and
verb-PPs. Therefore, we manually checked the
false-positive and false-negative rates in all four
groups and obtained the performance of parser
with respect to VPCs. The verb-PPs in group A
and B are false-positives while the VPCs in group
C and D are false-negatives (we consider the VPCs
to be positive examples).
To calculate the number of incorrect examples,
two human annotators independently checked
each verb?preposition instance. Table 1 details the
rate of false-positives and false-negative examples
in each data group, as well as the inter-annotator
agreement (calculated over the entire group).
4.2 Collection
We combined together the 6,535 (putative) VPCs
and 995 (putative) verb-PPs from groups A, B and
C, as identified by RASP over the corpus data. Ta-
ble 2 shows the number of VPCs in groups A and
B and the number of verb-PPs in group C. The
first number is the number of examples occuring
at least once and the second number that of exam-
ples occurring five or more times.
From the sentences containing VPCs and verb-
PPs, we retrieved a total of 8,165 nouns, including
68
Type Groups A&B Group C
common noun 7,116 1,239
personal pronoun 629 79
demonstrative pronoun 127 1
proper noun 156 18
who 94 6
which 32 0
No sense (what) 11 0
Table 3: Breakdown of subject and object head
nouns in group A&B, and group C
pronouns (e.g. I, he, she), proper nouns (e.g. CITI,
Canada, Ford) and demonstrative pronouns (e.g.
one, some, this), which occurred as the head noun
of a subject or object of a VPC in group A or B.
We similarly retrieved 1,343 nouns for verb-PPs in
group C. Table 3 shows the distribution of different
noun types in these two sets.
We found that about 10% of the nouns are pro-
nouns (personal or demonstrative), proper nouns
or WH words. For pronouns, we manually re-
solved the antecedent and took this as the head
noun. When which is used as a relative pronoun,
we identified if it was coindexed with an argument
position of a VPC or verb-PP, and if so, manually
identified the antecedent, as illustrated in (5).
(5) EX: Tom likes the books which he sold off.
ARGS: heSUBJ = person
whichOBJ = book
With what, on the other hand, we were gener-
ally not able to identify an antecedent, in which
case the argument position was left without a word
sense (we come back to this in Section 6).
(6) Tom didn?t look up what to do.
What went on?
We also replaced all proper nouns with cor-
responding common noun hypernyms based on
manual disambiguation, as the coverage of proper
nouns in WordNet is (intentionally) poor. The fol-
lowing are examples of proper nouns and their
common noun hypernyms:
Proper noun Common noun hypernym
CITI bank
Canada country
Ford company
Smith human
produce, green goods, ...
food(3rd)
...
reproductive structure
...
pome, false fruit
reproductive structure
fruit
fruit(2nd)
citrus, citrus fruit, citrous fruit
edible fruit(2nd)
edible fruit(1st)apple
Sense 1
Sense 1
orange
produce, green goods, ...
food(4th)
...
..
fruit(3rd)
Figure 2: Senses of apple and orange
When we retrieved the first word sense of nouns
from WordNet, we selected the first sense and the
associated hypernyms (up to) three levels up the
WordNet hierarchy. This is intended as a crude
form of smoothing for closely-related word senses
which occur in the same basic region of the Word-
Net hierarchy. As an illustration of this process,
in Figure 2, apple and orange are used as edi-
ble fruit, fruit or food, and the semantic overlap is
picked up on by the fact that edible fruit is a hy-
pernym of both apple and orange. On the other
hand, food is the fourth hypernym for orange so it
is ignored by our method. However, because we
use the four senses, the common senses of nouns
are extracted properly. This approach works rea-
sonably well for retrieving common word senses
of nouns which are in the immediate vicinity of
each other in the WordNet hierarchy, as was the
case with apple and orange. In terms of feature
representation, we generate an individual instance
for each noun sense generated based on the above
method, and in the case that we have multiple ar-
guments for a given VPC or verb-PP (e.g. both a
subject and a direct object), we generate an indi-
vidual instance for the cross product of all sense
combinations between the arguments.
We use 80% of the data for training and 20%
for testing. The following is the total number of
training instances, before and after performing hy-
pernym expansion:
Training Instances
Before expansion After expansion
Group A 5,223 24,602
Group B 1,312 4,158
Group C 995 5,985
69
Group Frequency of VPCs Size
B (f?1 ) test:272
(f?5 ) train:1,040
BA (f?1 & f?1 ) test:1,327
(f?5 & f?5 ) train:4,163
BC (f?1 & f?1 ) test:498
(f?5 & f?1 ) train:1,809
BAC (f?1 & f?1 & f?1 ) test:1,598
(f?5 & f?5 & f?1 ) train:5,932
Table 4: Data set sizes at different frequency cut-
offs
5 Evaluation
We selected 20% of the test data from different
combinations of the four groups and over the two
frequency thresholds, leading to a total of 8 test
data sets. The first data set contains examples from
group B only, the second set is from groups B and
A, the third set is from groups B and C, and the
fourth set is from groups B, A and C. Addition-
ally, each data set is divided into: (1) f ? 1, i.e.
verb?preposition combinations occurring at least
once, and (2) f ? 5, i.e. verb?preposition com-
binations occurring at least five times (hereafter,
f ? 1 is labelled f?1 and f ? 5 is labelled f?5 ).
In the group C data, there are 217 verb-PPs with
f?5 , which is slightly more than 20% of the data
so we use verb-PPs with f?1 for experiments in-
stead of verb-PP with f?5 . The first and second
data sets do not contain negative examples while
the third and fourth data sets contain both positive
and negative examples. As a result, the precision
for the first two data sets is 1.0.
Table 5 shows the precision, recall and F-score
of our method over each data set, relative to the
identification of VPCs only. A,B,C are groups and
f# is the frequency of examples.
Table 6 compares the performance of VPC iden-
tification and verb-PP identification.
Table 7 indicates the result using four word
senses (i.e. with hypernym expansion) and only
one word sense (i.e. the first sense only).
6 Discussion
The performance of RASP as shown in Tables 5
and 6 is based on human judgement. Note that
we only consider the ability of the parser to distin-
guish sentences with prepositions as either VPCs
or verb-PPs (i.e. we judge the parse to be correct if
the preposition is classified correctly, irrespective
of whether there are other errors in the output).
Data Freq P R F
RASP f?1 .959 .955 .957
B f?1 1.0 .819 .901
f?5 1.0 .919 .957
BA f?1 f?1 1.0 .959 .979
f?5 f?5 1.0 .962 .980
BC f?1 f?1 .809 .845 .827
f?5 f?1 .836 .922 .877
BAC f?1 f?1 f?1 .962 .962 .962
f?5 f?5 f?1 .964 .983 .974
Table 5: Results for VPC identification only (P =
precision, R = recall, F = F-score)
Data Freq Type P R F
RASP f?1 P+V .933 ? ?
BC f?1 f?1 P+V .8068 .8033 .8051
f?5 f?1 P+V .8653 .8529 .8591
BAC f?1 f?1 P+V .8660 .8660 .8660
f?5 f?1 P+V .9272 .8836 .9054
Table 6: Results for VPC (=V) and verb-PP (=P)
identification (P = precision, R = recall, F = F-
score)
Also, we ignore the ambiguity between particles
and adverbs, which is the principal reason for our
evaluation being much higher than that reported
by McCarthy et al (2003). In Table 5, the preci-
sion (P) and recall (R) for VPCs are computed as
follows:
P = Data Correctly Tagged as VPCs
Data Retrieved as VPCs
R = Data Correctly Tagged as VPCs
All VPCs in Data Set
The performance of RASP in Table 6 shows
how well it distinguishes between VPCs and verb-
PPs for ambiguous verb?preposition combina-
tions. Since Table 6 shows the comparative per-
formance of our method between VPCs and verb-
PPs, the performance of RASP with examples
which are misrecognized as each other should be
the guideline. Note, the baseline RASP accuracy,
based on assigning the majority class to instances
in each of groups A, B and C, is 83.04%.
In Table 5, the performance over high-
frequency data identified from groups B, A and
C is the highest (F-score = .974). In general, we
would expect the data set containing the high fre-
quency and both positive and negative examples
70
Freq Type # P R F
f?1 V 4WS .962 .962 .962
1WS .958 .969 .963
f?1 P 4WS .769 .769 .769
1WS .800 .743 .770
f?5 V 4WS .964 .983 .974
1WS .950 .973 .962
f?5 P 4WS .889 .783 .832
1WS .813 .614 .749
Table 7: Results with hypernym expansion (4WS)
and only the first sense (1WS), in terms of preci-
sion (P), recall (R) and F-score (F)
to give us the best performance at VPC identifi-
cation. We achieved a slightly better result than
the 95.8%-97.5% performance reported by Li et
al. (2003). However, considering that Li et al
(2003) need considerable time and human labour
to generate hand-coded rules, our method has ad-
vantages in terms of both raw performance and
labour efficiency.
Combining the results for Table 5 and Table 6,
we see that our method performs better for VPC
identification than verb-PP identification. Since
we do not take into account the data from group
D with our method, the performance of verb-PP
identification is low compared to that for RASP,
which in turn leads to a decrement in the overall
performance.
Since we ignored the data from group D con-
taining unambiguous verb-PPs, the number of pos-
itive training instances for verb-PP identification
was relatively small. As for the different number
of word senses in Table 7, we conclude that the
more word senses the better the performance, par-
ticularly for higher-frequency data items.
In order to get a clearer sense of the impact of
selectional preferences on the results, we investi-
gated the relative performance over VPCs of vary-
ing semantic compositionality, based on 117 VPCs
(f?1 ) attested in the data set of McCarthy et al
(2003). According to our hypothesis from above,
we would expect VPCs with low composition-
ality to have markedly different selectional pref-
erences to the corresponding simplex verb, and
VPCs with high compositionality to have similar
selectional preferences to the simplex verb. In
terms of the performance of our method, therefore,
we would expect the degree of compositionality
to be inversely proportional to the system perfor-
mance. We test this hypothesis in Figure 3, where
we calculate the error rate reduction (in F-score)
 0
 20
 40
 60
 80
 100
 0  1  2  3  4  5  6  7  8  9  10 0
 20
 40
 60
 80
 100
Err
or R
ate
 Re
duc
tion
 (%
)
Typ
es
Compositionality
Figure 3: Error rate reduction for VPCs of varying
compositionality
for the proposed method relative to the majority-
class baseline, at various degrees of composition-
ality. McCarthy et al (2003) provides compo-
sitionality judgements from three human judges,
which we take the average of and bin into 11 cate-
gories (with 0 = non-compositional and 10 = fully
compositional). In Figure 3, we plot both the er-
ror rate reduction in each bin (both the raw num-
bers and a smoothed curve), and also the number
of attested VPC types found in each bin. From
the graph, we see our hypothesis born out that,
with perfect performance over non-compositional
VPCs and near-baseline performance over fully
compositional VPCs. Combining this result with
the overall results from above, we conclude that
our method is highly successful at distinguishing
non-compositional VPCs from verb-PPs, and fur-
ther that there is a direct correlation between the
degree of compositionality and the similarity of
the selectional preferences of VPCs and their verb
counterparts.
Several factors are considered to have influ-
enced performance. Some data instances are miss-
ing head nouns which would assist us in determin-
ing the semantics of the verb?preposition combi-
nation. Particular examples of this are imperative
and abbreviated sentences:
(7) a. Come in.
b. (How is your cold?) Broiled out.
Another confounding factor is the lack of word
sense data, particularly in WH questions:
(8) a. What do I hand in?
b. You can add up anything .
71
7 Conclusion
In this paper, we have proposed a method for iden-
tifying VPCs automatically from raw corpus data.
We first used the RASP parser to identify VPC
and verb-PP candidates. Then, we used analysis of
the head nouns of the arguments of the head verbs
to model selectional preferences, and in doing so,
distinguish between VPCs and verb-PPs. Using
TiMBL 5.1, we built a classifier which achieved
an F-score of 97.4% at identifying frequent VPC
examples. We also investigated the comparative
performance of RASP at VPC identification.
The principal drawback of our method is that it
relies on the performance of RASP and we assume
a pronoun resolution oracle to access the word
senses of pronouns. Since the performance of such
systems is improving, however, we consider our
approach to be a promising, stable method of iden-
tifying VPCs.
Acknowledgements
This material is based upon work supported in part by the
Australian Research Council under Discovery Grant No.
DP0663879 and NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation. We would
like to thank the three anonymous reviewers for their valu-
able input on this research.
References
Timothy Baldwin and Aline Villavicencio. 2002. Extract-
ing the unextractable: A case study on verb-particles. In
Proc. of the 6th Conference on Natural Language Learn-
ing (CoNLL-2002), pages 98?104, Taipei, Taiwan.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Do-
minic Widdows. 2003. An empirical model of multiword
expression decomposability. In Proc. of the ACL-2003
Workshop on Multiword Expressions: Analysis, Acquisi-
tion and Treatment, pages 89?96, Sapporo, Japan.
Timothy Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer Speech
and Language, Special Issue on Multiword Expressions,
19(4):398?414.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of the ACL-2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treatment,
pages 65?72, Sapporo, Japan.
Ted Briscoe and John Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proc. of the 3rd Interna-
tional Conference on Language Resources and Evaluation
(LREC 2002), pages 1499?1504, Las Palmas, Canary Is-
lands.
Nicoletta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine MacLeod, and
Antonio Zampolli. 2002. Towards best practice for mul-
tiword expressions in computational lexicons. In Proc. of
the 3rd International Conference on Language Resources
and Evaluation (LREC 2002), pages 1934?40, Las Pal-
mas, Canary Islands.
Ann Copestake and Alex Lascarides. 1997. Integrating sym-
bolic and statistical representations: The lexicon pragmat-
ics interface. In Proc. of the 35th Annual Meeting of the
ACL and 8th Conference of the EACL (ACL-EACL?97),
pages 136?43, Madrid, Spain.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and An-
tal van den Bosch. 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide. ILK Technical Re-
port 04-02.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
B. Fraser. 1976. The Verb-Particle Combination in English.
The Hague: Mouton.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang, and Ro-
hini K. Srihari. 2003. An expert lexicon approach to iden-
tifying English phrasal verbs. In Proc. of the 41st Annual
Meeting of the ACL, pages 513?20, Sapporo, Japan.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313?30.
Diana McCarthy, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proc. of the ACL-2003 Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment, Sapporo,
Japan.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proc. of the 42nd Annual Meeting of the ACL, pages
280?7, Barcelona, Spain.
Tom O?Hara and Janyce Wiebe. 2003. Preposition semantic
classification via Treebank and FrameNet. In Proc. of the
7th Conference on Natural Language Learning (CoNLL-
2003), pages 79?86, Edmonton, Canada.
Darren Pearce. 2001. Synonymy in collocation extraction.
In Proceedings of the NAACL 2001 Workshop on WordNet
and Other Lexical Resources: Applications, Extensions
and Customizations, Pittsburgh, USA.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
A pain in the neck for NLP. In Proc. of the 3rd Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-2002), pages 1?15, Mex-
ico City, Mexico.
Aline Villavicencio. 2003. Verb-particle constructions and
lexical resources. In Proc. of the ACL-2003 Workshop on
Multiword Expressions: Analysis, Acquisition and Treat-
ment, pages 57?64, Sapporo, Japan.
Aline Villavicencio. 2006. Verb-particle constructions in the
world wide web. In Patrick Saint-Dizier, editor, Compu-
tational Linguistics Dimensions of Syntax and Semantics
of Prepositions. Springer, Dordrecht, Netherlands.
Dominic Widdows and Beate Dorow. 2005. Automatic ex-
traction of idioms using graph analysis and asymmetric
lexicosyntactic patterns. In Proc. of the ACL-SIGLEX
2005 Workshop on Deep Lexical Acquisition, pages 48?
56, Ann Arbor, USA.
72
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 231?236,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-KB: Nominal Classification as Noun Compound Interpretation
Su Nam Kim and Timothy Baldwin
Computer Science and Software Engineering
University of Melbourne, Australia
{snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper, we outline our approach to
interpreting semantic relations in nominal
pairs in SemEval-2007 task #4: Classifica-
tion of Semantic Relations between Nomi-
nals. We build on two baseline approaches
to interpreting noun compounds: sense col-
location, and constituent similarity. These
are consolidated into an overall system in
combination with co-training, to expand the
training data. Our two systems attained an
average F-score over the test data of 58.7%
and 57.8%, respectively.
1 Introduction
This paper describes two systems entered in
SemEval-2007 task #4: Classification of Semantic
Relations between Nominals. A key contribution of
this research is that we examine the compatibility of
noun compound (NC) interpretation methods over
the extended task of nominal classification, to gain
empirical insight into the relative complexity of the
two tasks.
The goal of the nominal classification task is to
identify the compatibility of a given semantic re-
lation with each of a set of test nominal pairs,
e.g. between climate and forest in the fragment the
climate in the forest with respect to the CONTENT-
CONTAINER relation. Semantic relations (or SRs)
in nominals represent the underlying interpretation
of the nominal, in the form of the directed relation
between the two nominals.
The proposed task is a generalisation of the more
conventional task of interpreting noun compounds
(NCs), in which we take a NC such as cookie jar and
interpret it according to a pre-defined inventory of
semantic relations (Levi, 1979; Vanderwende, 1994;
Barker and Szpakowicz, 1998). Examples of seman-
tic relations are MAKE,1, as exemplified in apple pie
where the pie is made from apple(s), and POSSES-
SOR, as exemplified in family car where the car is
possessed by a family.
In the SemEval-2007 task, SR interpretation
takes the form of a binary decision for a
given nominal pair in context and a given SR,
in judging whether that nominal pair conforms
to the SR. Seven relations were used in the
task: CAUSE-EFFECT, INSTRUMENT-AGENCY,
PRODUCT-PRODUCER, ORIGIN-ENTITY, THEME-
TOOL, PART-WHOLE and CONTENT-CONTAINER.
Our approach to the task was to: (1) naively treat
all nominal pairs as NCs (e.g. the climate in the for-
est is treated as an instance of climate forest); and
(2) translate the individual binary classification tasks
into a single multiclass classification task, in the in-
terests of benchmarking existing SR interpretation
methods over a common dataset. That is, we take
all positive training instances for each SR and pool
them together into a single training dataset. For each
test instance, we make a prediction according to one
of the seven relations in the task, which we then
map onto a binary classification for final evaluation
purposes. This mapping is achieved by determining
which binary SR classification the test instance was
sourced from, and returning a positive classification
if the predicted SR coincides with the target SR, and
a negative classification if not.
We make three (deliberately naive) assumptions
in our approach to the nominal interpretation task.
First, we assume that all the positive training in-
1For direct comparability with our earlier research, seman-
tic relations used in our examples are taken from (Barker and
Szpakowicz, 1998), and differ slightly from those used in the
SemEval-2007 task.
231
stances correspond uniquely to the SR in question,
despite the task organisers making it plain that there
is semantic overlap between the SRs. As a machine
learning task, this makes the task considerably more
difficult, as the performance for the standard base-
lines drops considerably from that for the binary
tasks. Second, we assume that each nominal pair
maps onto a NC. This is clearly a misconstrual of the
task, and intended to empirically validate whether
such an approach is viable. In line with this assump-
tion, we will refer to nominal pairs as NCs for the
remainder of the paper. Third and finally, we assume
that the SR annotation of each training and test in-
stance is insensitive to the original context, and use
only the constituent words in the NC to make our
prediction. This is for direct comparability with ear-
lier research, and we acknowledge that the context
(and word sense) is a strong determinant of the SR
in practice.
Our aim in this paper is to demonstrate the effec-
tiveness of general-purpose SR interpretation over
the nominal classification task, and establish a new
baseline for the task.
The remainder of this paper is structured as fol-
lows. We present our methods in Section 2 and de-
pict the system architectures in Section 4. We then
describe and discuss the performance of our meth-
ods in Section 5 and conclude the paper in Section 6.
2 Approach
We used two basic NC interpretation methods. The
first method uses sense collocations as proposed by
Moldovan et al (2004), and the second method uses
the lexical similarity of the component words in the
NC as proposed by Kim and Baldwin (2005). Note
that neither method uses the context of usage of the
NC, i.e. the only features are the words contained in
the NC.
2.1 Sense Collocation Method
Moldovan et al (2004) proposed a method called se-
mantic scattering for interpreting NCs. The intuition
behind this method is that when the sense colloca-
tion of NCs is the same, their SR is most likely the
same. For example, the sense collocation of auto-
mobile factory is the same as that of car factory, be-
cause the senses of automobile and car, and factory
in the two instances, are identical. As a result, the
two NCs have the semantic relation MAKE.
The semantic scattering model is outlined below.
The probability P (r|fifj) (simplified to
P (r|fij)) of a semantic relation r for word
senses fi and fj is calculated based on simple
maximum likelihood estimation:
P (r|fij) =
n(r, fij)
n(fij)
(1)
and the preferred SR r? for the given word sense
combination is that which maximises the probabil-
ity:
r? = argmaxr?RP (r|fij)
= argmaxr?RP (fij |r)P (r) (2)
Note that in limited cases, the same sense collo-
cation can lead to multiple SRs. However, since we
do not take context into account in our method, we
make the simplifying assumption that a given sense
collocation leads to a unique SR.
2.2 Constituent Similarity Method
In earlier work (Kim and Baldwin, 2005), we pro-
posed a simplistic general-purpose method based on
the lexical similarity of unseen NCs with training
instances. That is, the semantic relation of a test
instance is derived from the train instance which
has the highest similarity with the test instance, in
the form of a 1-nearest neighbour classifier. For
example, assuming the test instance chocolate milk
and training instances apple juice and morning milk,
we would calculate the similarity between modifier
chocolate and each of apple and morning, and head
noun milk and each of juice and milk, and find, e.g.,
the similarities .71 and .27, and .83 and 1.00 respec-
tively. We would then add these up to derive the
overall similarity for a given NC and find that apple
juice is a better match. From this, we would assign
the SR of MAKE from apple juice to chocolate milk.
Formally, SA is the similarity between NCs
(Ni,1, Ni,2) and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1)? ((1? ?)S2 + S2))
2
(3)
where S1 is the modifier similarity (i.e.
S(Ni,1, Bj1)) and S2 is head noun similarity
232
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
The similarity scores are calculated using the
method of Wu and Palmer (1994) as implemented
in WordNet::Similarity (Patwardhan et al,
2003). This is done for each pairing of WordNet
senses of each of the two words in question, and the
overall lexical similarity is calculated as the average
across the pairwise sense similarities.
The final classification is derived from the training
instance which has the highest lexical similarity with
the test instance in question.
3 Co-Training
As with many semantic annotation tasks, SR tag-
ging is a time-consuming and expensive process. At
the same time, due to the inherent complexity of the
SR interpretation task, we require large amounts of
training data in order for our methods to perform
well. In order to generate additional training data to
train our methods over, we experiment with different
co-training methodologies for each of our two basic
methods.
3.1 Co-Training for the Sense Collocation
Method
For the sense collocation method, we experiment
with a substitution method whereby we replace one
constituent in a training NC instance by a similar
word, and annotate the new instance with the same
SR as the original NC. For example, car in car fac-
tory (SR = MAKE) has similar words automobile,
vehicle, truck from the synonym, hypernym and sis-
ter word taxonomic relations, respectively. When
car is replaced by a similar word, the new noun
compound(s) (i.e. automobile/vehicle/truck factory)
share the same SR as the original car factory. Note
that each constituent in our original example is
tagged for word sense, which we use both in ac-
cessing sense-specific substitution candidates (via
WordNet), and sense-annotating the newly gener-
ated NCs.
Substitution is restricted to one constituent at a
time in order to avoid extreme semantic variation.
This procedure can be repeated to generate more
training data. However, as the procedure goes fur-
ther, we introduce increasingly more noise.
In our experiments, we use this co-training
method with the sense collocation method to expand
the size and variation of training data, using syn-
onym, hypernym and sister word relations. For our
experiment, we ran the expansion procedure for only
one iteration in order to avoid generating excessive
amounts of incorrectly-tagged NCs.
3.2 Co-Training for the Constituent Similarity
Method
Our experiments with the constituent similarity
method over the trial data showed, encouragingly,
that there is a strong correlation between the strength
of overall similarity with the best-matching training
NC, and the accuracy of the prediction. From this,
we experimented with implementing the constituent
similarity method in a cascading architecture. That
is, we batch evaluate all test instances on each it-
eration, and tag those test instances for which the
best match with a training instance is above a pre-
set threshold, which we decrease on each iteration.
In subsequent iterations, all tagged test instances are
included in the training data. Hence, on each itera-
tion, the number of training instances is increasing.
As our threshold, we used a starting value of 0.85,
which was decreased down to 0.65 in increments of
0.05.
4 Architectures
In Section 4.1 and Section 4.2, we describe the ar-
chitecture of our two systems.
4.1 Architecture (I)
Figure 1 presents the architecture of our first system,
which interleaves sense collocation and constituent
similarity, and includes co-training for each. There
are five steps in this system.
First, we apply the basic sense collocation method
relative to the original training data. If the sense col-
location between the test and training instances is
the same, we judge the predicted SR to be correct.
Second, we apply the similarity method described
in Section 2.2 over the original training data. How-
ever, we only classify test instances where the final
similarity is above a threshold of 0.8.
Third, we apply the sense collocation co-training
method and re-run the sense collocation method
over the expanded training data from the first two
steps. Since the sense collocations in the expanded
233
TEST
untagged test data
untagged test data
untagged test data
untagged test data
tagged data
tagged data
tagged data
tagged data
tagged data
TRAIN
Extension of
Training databy similar words
? Synonym
? Hypernym
? Sister word
Extended TRAIN
Sense Collcation
Step 1
Similarity
Step 2
Step 3
Step 4
Similarity
Step 5
Sense Collcation
Similarity
Figure 1: System Architecture (I)
training data have been varied through the advent of
hypernyms and sister words, the number of sense
collocations in the expanded training data is much
greater than that of the original training data (937
vs. 16,676).
Fourth, we apply the constituent similarity co-
training method over the consolidated training data
(from both sense collocation and constituent simi-
larity co-training) with the threshold unchanged at
0.8.
Finally, we apply the constituent similarity
method over the combined training data, without any
threshold (to guarantee a SR prediction for every
test instance). However, since the generated train-
ing instances are more likely to contain errors, we
decrement the similarity values for generated train-
ing instances by 0.2, to prefer predictions based on
the original training instances.
4.2 Architecture (II)
Figure 2 depicts our second system, which is based
solely on the constituent similarity method, with co-
training.
We perform iterative co-training as described in
TRAIN
#of Tagged
>= 10% of testThreshold
Tagged
finalize current
tags and end
reduce Threshold
TEST
get Similarity
Sim >= TN Y
Y
N
if T == 0.6 &(#of Tagged <
10% of test)
N
Y
Figure 2: System Architecture (II)
Section 3.2, with the slight variation that we hold
off reducing the threshold if more than 10% of the
test instances are tagged on a given iteration, giving
other test instances a chance to be tagged at a higher
threshold level relative to newly generated training
instances. The residue of test instances on comple-
tion of the final iteration (threshold = 0.6) are tagged
according to the best-matching training instance, ir-
respective of the magnitude of the similarity.
5 Evaluation
We group our evaluation into two categories: (A)
doesn?t use WordNet 2.1 or the query context;
and (B) uses WordNet 2.1 only (again with-
out the query context). Of our two basic meth-
ods the sense collocation method and co-training
method are based on WordNet 2.1 only, while
the constituent similarity method is based indirectly
on WordNet 2.1, but doesn?t preserve WordNet
2.1 sense information. Hence, our first system is
category B while our second system is (arguably)
category A.
Table 1 presents the three baselines for the task,
and the results for our two systems (System I and
System II). The performance for both systems ex-
ceeded all three baselines in terms of accuracy, and
all but the All True baseline (i.e. every instance is
judged to be compatible with the given SR) in terms
234
Method P R F A
All True 48.5 100.0 64.8 48.5
Probability 48.5 48.5 48.5 51.7
Majority 81.3 42.9 30.8 57.0
System I 61.7 56.8 58.7 62.5
System II 61.5 55.7 57.8 62.7
Table 1: System results (P = precision, R = recall, F
= F-score, and A = accuracy)
Team P R F A
759 66.1 66.7 64.8 66.0
281 60.5 69.5 63.8 63.5
633 62.7 63.0 62.7 65.4
220 61.5 55.7 57.8 62.7
161 56.1 57.1 55.9 58.8
538 48.2 40.3 43.1 49.9
Table 2: Results of category A systems
of F-score and recall.
Tables 2 and 3 show the performance of the teams
which performed in the task, in categories A and B.
Team 220 in Table 2 is our second system, and team
220 in Table 3 is our first system.
In Figures 3 and 4, we present a breakdown of
the performance our first and second system, re-
spectively, over the individual semantic relations.
Our approaches performed best for the PRODUCT-
PRODUCER SR, and worst for the PART-WHOLE
SR. In general, our systems achieved similar perfor-
mance on most SRs, with only PART-WHOLE be-
ing notably worse. The lower performance of PART-
WHOLE pulls down our overall performance consid-
erably.
Tables 4 and 5 show the number of tagged and un-
tagged instances for each step of System I and Sys-
tem II, respectively. The first system tagged more
than half of the data in the fifth (and final) step,
where it weighs up predictions from the original and
expanded training data. Hence, the performance of
this approach relies heavily on the similarity method
and expanded training data. Additionally, the differ-
ence in quality between the original and expanded
training data will influence the performance of the
approach appreciably. On the other hand, the num-
ber of instances tagged by the second system is well
distributed across each iteration. However, since
we accumulate generated training instances on each
step, the relative noise level in the training data will
Team P R F A
901 79.7 69.8 72.4 76.3
777 70.9 73.4 71.8 72.9
281 72.8 70.6 71.5 73.2
129 69.9 64.6 66.8 71.4
333 62.0 71.7 65.4 67.0
538 66.7 62.8 64.3 67.2
571 55.7 66.7 60.4 59.1
759 66.4 58.1 60.3 63.6
220 61.7 56.8 58.7 62.5
371 56.8 56.3 56.1 57.7
495 55.9 57.8 51.4 53.7
Table 3: Results of category B systems
CE IA PP OE TT PW CC
relations
(%) F?scorerecallprecision accuracy
 0
 20
 40
 60
 80
 100
Figure 3: System I performance for each rela-
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
increase across iterations, impacting on the final per-
formance of the system.
Over the trial data, we noticed that the system pre-
dictions are appreciably worse when the similarity
value is low. In future work, we intend to analyse
what is happening in terms of the overall system
performance at each step. This analysis is key to
improving the performance of our systems.
Recall that we are generalising from the set of
binary classification tasks in the original task, to a
multiclass classification task. As such, a direct com-
parison with the binary classification baselines is
perhaps unfair (particularly All True, which has no
correlate in a multiclass setting), and it is if anything
remarkable that our system compares favourably
compared to the baselines. Similarly, while we
clearly lag behind other systems participating in the
235
(%)
CE IA PP OE TT PW CC
relations
F?scorerecallprecision accuracy
 0
 20
 40
 60
 80
 100
Figure 4: System II performance for each rela-
tion (CC=CAUSE-EFFECT, IA=INSTRUMENT-
AGENCY, PP=PRODUCT-PRODUCER,
OE=ORIGIN-ENTITY, TT=THEME-TOOL,
PW=PART-WHOLE, CC=CONTENT-CONTAINER)
step method tagged accumulated untagged
s1 SC 21 3.8% 528
s2 Sim 106 23.1% 422
s3 extSC 0 23.1% 422
s4 extSim 61 34.2% 361
s5 SvsExtS 359 99.6% 2
Table 4: System I: Tagged data from each step
(SC= sense collocation; Sim = the similarity method;
extSC = SC over the expanded training data; extSim
= similarity over the expanded training data; SvsExtS
= the final step over both the original and expanded
training data)
task, we believe we have demonstrated that NC in-
terpretation methods can be successfully deployed
over the more general task of nominal pair classifi-
cation.
6 Conclusion
In this paper, we presented two systems entered in
the SemEval-2007 Classification of Semantic Re-
lations between Nominals task. Both systems are
based on baseline NC interpretation methods, and
the naive assumption that the nominal classification
task is analogous to a conventional multiclass NC
interpretation task. Our results compare favourably
with the established baselines, and demonstrate that
NC interpretation methods are compatible with the
more general task of nominal classification.
I T tagged accumulated untagged
i1 .85 73 13.3% 476
i2 .80 56 23.5% 420
i3 .75 74 37.0% 346
i4 .70 101 55.4% 245
i5 .65 222 95.8% 23
? <.65 21 99.6% 2
Table 5: System II: data tagged on each iteration (T
= the threshold; iX = the iteration number)
Acknowledgments
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proc. of the
17th International Conference on Computational Linguis-
tics, pages 96?102, Montreal, Canada.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Timothy W. Finin. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D. thesis, University of Illinois.
Su Nam Kim and Timothy Baldwin. 2005. Automatic inter-
pretation of Noun Compounds using WordNet similarity. In
Proc. of the 2nd International Joint Conference On Natural
Language Processing, pages 945?956, JeJu, Korea.
Judith Levi. 1979. The syntax and semantics of complex nom-
inals. In The Syntax and Semantics of Complex Nominals.
New York:Academic Press.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe,
and Roxana Girju. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of the HLT-NAACL 2004
Workshop on Computational Lexical Semantics, pages 60?
67, Boston, USA.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen.
2003. Using measures of semantic relatedness for word
sense disambiguation. In Proc. of the Fourth International
Conference on Intelligent Text Processing and Computa-
tional Linguistics, pages 241?57, Mexico City, Mexico.
Lucy Vanderwende. 1994. Algorithm for automatic interpreta-
tion of noun sequences. In Proc. of the 15th conference on
Computational linguistics, pages 782?788, Kyoto, Japan.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics and
lexical selection. In Proc. of the 32nd Annual Meeting of the
Association for Computational Linguistics, pages 133?138,
Las Cruces, USA.
236
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 237?240,
Prague, June 2007. c?2007 Association for Computational Linguistics
MELB-MKB: Lexical Substitution System based on Relatives in Context
David Martinez, Su Nam Kim and Timothy Baldwin
LT Group, CSSE
University of Melbourne
Victoria 3010 Australia
{davidm,snkim,tim}@csse.unimelb.edu.au
Abstract
In this paper we describe the MELB-MKB
system, as entered in the SemEval-2007 lex-
ical substitution task. The core of our sys-
tem was the ?Relatives in Context? unsuper-
vised approach, which ranked the candidate
substitutes by web-lookup of the word se-
quences built combining the target context
and each substitute. Our system ranked third
in the final evaluation, performing close to
the top-ranked system.
1 Introduction
This paper describes the system we developed for
the SemEval lexical substitution task, a new task in
SemEval-2007. Although we tested different con-
figurations on the trial data, our basic system relied
on WordNet relatives (Fellbaum, 1998) and Google
queries in order to identify the most plausible sub-
stitutes in the context.
The main goal when building our system was to
study the following factors: (i) substitution candi-
date set, (ii) settings of the relative-based algorithm,
and (iii) syntactic filtering. We analysed these fac-
tors over the trial data provided by the organisation,
and used the BEST metric to tune our system. This
metric accepts multiple answers, and averages the
score across the answers. We did not experiment
with the OOT (top 10 answers) and MULTIWORD
metrics.
In the remainder of this paper we briefly intro-
duce the basic Relatives in Context algorithm in Sec-
tion 2. Next we describe our experiments on the trial
data in Section 3. Our final system and its results are
described in Section 4. Finally, our conclusions are
outlined in Section 5.
2 Algorithm
Our basic algorithm is an unsupervised method pre-
sented in Martinez et al (2006). This technique
makes use of the WordNet relatives of the target
word for disambiguation, by way of the following
steps: (i) obtain a set of close relatives from Word-
Net for each sense of the target word; (ii) for each
test instance define all possible word sequences that
include the target word; (iii) for each word sequence,
substitute the target word with each relative, and
then query Google; (iv) rank queries according to
the following factors: length of the query, distance
of the relative to the target word, and number of hits;
and (v) select the relative from the highest ranked
query.1
For the querying step, first we tokenise each tar-
get sentence, and then we apply sliding windows of
different sizes (up to 6 tokens) that include the tar-
get word. For each window and each relative in the
pool, we substitute the target word for the relative,
and query Google. The algorithm stops augment-
ing the window for the relative when one of its sub-
strings returns zero hits. The length of the query is
measured as the number of words, and the distance
of the relative to the target words gives preference
to synonyms over hypernyms, and immediate hyper-
nyms over further ones.
One important parameter in this method is the
candidate set. We performed different experiments
to measure the expected score we could achieve
1In the case of WSD we would use the relative to chose the
sense it relates to.
237
from WordNet relatives, and the contribution of dif-
ferent types of filters (syntactic, frequency-based,
etc.) to the overall result. We also explored other
settings of the algorithm, such as the ranking crite-
ria, and the number of answers to return. These ex-
periments and some other modifications of the basic
algorithm are covered in Section 3.
3 Development on Trial data
In this section we analyse the coverage of WordNet
over the data, the basic parameter exploration pro-
cess, a syntactic filter, and finally the extra experi-
ments we carried out before submission. The trial
data consisted on 300 instances of 34 words with
gold-standard annotations.
3.1 WordNet coverage
The most obvious resource for selecting substitution
candidates was WordNet, due to its size and avail-
ability. We used version 2.0 throughout this work.
In our first experiment, we tried to determine which
kind of relationships to use, and the coverage of the
gold-standard annotations that we could expect from
WordNet relations only. As a basic set of relations,
we used the following: SYNONYMY, SIMILAR-TO,
ENTAILMENT, CAUSE, ALSO-SEE, and INSTANCE.
We created two extended candidate sets using im-
mediate and 2-step hypernyms (hype and hype2, re-
spectively, in Table 1).
Given that we are committed to using Word-
Net, we set out to measure the percentage of gold-
standard substitutes that were ?reachable? using dif-
ferent WordNet relations. Table 1 shows the cov-
erage for the three sets of candidates. Instance-
coverage indicates the percentage of instances that
have at least one of the gold-standard instances cov-
ered from the candidate set. We can see that the per-
centage is surprisingly low.
Any shortcoming in coverage will have a direct
impact on performance, suggesting the need for al-
ternate means to obtain substitution candidates. One
possibility is to extend the candidates from Word-
Net by following links from the relatives (e.g. col-
lect all synonyms of the synonymous words), but
this could add many noisy candidates. We can also
use other lexical repositories built by hand or auto-
matically, such as the distributional theusauri built
Candidate Set Subs. Cov. Inst. Cov.
basic 344/1152 (30%) 197 / 300 (66%)
hype 404/1152 (35%) 229/300 (76%)
hype2 419/1152 (36%) 229/300 (76%)
Table 1: WordNet coverage for different candidate
sets, based on substitute (Subs.) and instance (Inst.)
coverage.
in Lin (1998). A different approach that we are test-
ing for future work is to adapt the algorithm to work
with wildcards instead of explicit candidates. Due to
time constraints, we only relied on WordNet for our
submission.
3.2 Parameter Tuning
In this experiment we tuned different parameters of
the basic algorithm. First, we observed the data in
order to identify the most relevant variables for this
task. We tried to avoid including too many parame-
ters and overfitting the system to the trial dataset. At
this point, we separated the instances by PoS, and
studied the following parameters:
Candidate set: From WordNet, we tested four
possible datasets for each target word: basic-set, 1st-
sense (basic relations from the first sense only), hype
(basic set and immediate hypernyms), and hype2
(basic set and up to two-step hypernyms).
Semcor-based filters: Semcor provides frequency
information for WordNet senses, and can be used
to identify rare senses. As each candidate is ob-
tained via WordNet semantic relations with the tar-
get word, we can filter out those candidates that are
related with unfrequent senses in Semcor. We tested
three configurations: (1) no filter, (2) filter out candi-
dates when the candidate-sense in the relation does
not occur in Semcor, (3) and filter out candidates
when the target-sense in the relation does not oc-
cur in Semcor. The filters can potentially lead to the
removal of all candidates, in which case a back-off
is applied (see below).
Relative-ranking criteria: Our algorithm ranks
relatives according to the length in words of their
context-match. In the case of ties, the number of re-
turned hits from Google is applied. The length can
be different depending on whether we count punc-
tuation marks as separate tokens, and whether the
word-length of substitute multiwords is included.
238
We tested three options: including the target word,
not including the target word (multiwords count as a
single word), and not counting punctuation marks.
Back-off: We need a back-off method in case the
basic algorithm does not find any matches. We
tested the following: sense-ordered synonyms from
WordNet (highest sense first, randomly breaking
ties), and most frequent synonyms from the first sys-
tem (using two corpora: Semcor and BNC).
Number of answers: We also measured the per-
formance for different numbers of system outputs
(1, 2, or 3).
All in all, we performed 324 (4x3x3x3x3) runs
for each PoS, based on the different combinations.
The best scores for each PoS are shown in Table 2,
together with the baselines. We can see that the pre-
cision is above the official WordNet baseline, but is
still very low. The results illustrate the difficulty of
the task. In error analysis, we observed that the per-
formance and settings varied greatly depending on
the PoS of the target word. Adverbs produced the
best performance, followed by nouns. The scores
were very low for adjectives and verbs (the baseline
score for verbs was only 2%).
We will now explain the main conclusions ex-
tracted from the parameter analysis. Regarding the
candidate set, we observed that using synonyms only
was the best approach for all PoS, except for verbs,
where hypernyms helped. The option of limiting the
candidates to the first sense only helped for adjec-
tives, but not for other PoS.
For the Semcor-based filter, our results showed
that the target-sense filter improved the performance
for verbs and adverbs. For nouns and adjectives, the
candidate-sense filter worked best. All in all, apply-
ing the Semcor filters was effective in removing rare
senses and improving performance.
The length criteria did not affect the results signif-
icantly, and only made a difference in some extreme
cases. Not counting the length of the target word
helped slightly for nouns and adverbs, and removing
punctuation improved results for adjectives. Regard-
ing the back-off method, we observed that the count
of frequencies in Semcor was the best approach for
all PoS except verbs, which reached their best per-
formance with BNC frequencies.
PoS Relatives in Context WordNet Baseline
Nouns 18.4 14.9
Verbs 6.7 2.0
Adjectives 9.6 7.5
Adverbs 31.1 29.9
Overall 14.4 10.4
Table 2: Experiments to tune parameters on the trial
data, based on the BEST metric. Scores correspond
to precision (which is the same as recall).
Finally, we observed that the performance for the
BEST score decreased significantly when more than
one answer was returned, probably due to the diffi-
culty of the task.
3.3 Syntactic Filter
After the basic parameter analysis, we studied the
contribution of a syntactic filter to remove those can-
didates that, when substituted, generate an ungram-
matical sentence. Intuitively, we would expect this
to have a high impact for verbs, which vary consid-
erably in their subcategorisation properties. For ex-
ample, in the case of the (reduced) target If we order
our lives well ..., the syntactic filter should ideally
disallow candidates such as If we range our lives
well ...
In order to apply this filter, we require a parser
which has an explicit notion of grammaticality, rul-
ing out the standard treebank parsers. We experi-
mented briefly with RASP, but found that the En-
glish Resource Grammar (ERG: Flickinger (2002)),
combined with the PET run-time engine, was the
best fit for out needs. Unfortunately we could not get
unknown word handling working within the ERG
for our submission, such that we get a meaningful
output for a given input string only in the case that
the ERG has full lexical coverage over that string
(we will never get a spanning parse for an input
where we are missing lexical entries). As such, the
syntactic filter is limited in coverage only to strings
where the ERG has lexical coverage.
Ideally, we would have tested this filter on trial
data, but unfortunately we ran out of time. Thus, we
simply eyeballed a sample of examples, and we de-
cided to include this filter in our final submission. As
we will see in Section 4, its effect was minimal. We
plan to perform a complete evaluation of this module
in the near future.
239
3.4 Extra experiments
One of the limitations of the ?Relatives in Context?
algorithm is that it only relies on the local con-
text. We wanted to explore the contribution of other
words in the context for the task, and we performed
an experiment including the Topical Signatures re-
source (Agirre and Lopez de Lacalle, 2004). We
simply counted the overlapping of words shared be-
tween the context and the different candidates. We
only tested this for nouns, for which the results were
below baseline. We then tried to integrate the topic-
signature scores with the ?Relatives in Context? al-
gorithm, but we did not improve our basic system?s
results on the trial data. Thus, this approach was not
included in our final submission.
Another problem we observed in error analysis
was that the Semcor-based filters were too strict in
some cases, and it was desirable to have a way of
penalising low frequency senses without removing
them completely. Thus, we weighted senses by the
inverse of their sense-rank. As we did not have time
to test this intuition properly, we opted for applying
the sense-weighting only when the candidates had
the same context-match length, instead of using the
number of hits. We will see the effect of this method
in the next section.
4 Final system
The test data consisted of 1,710 instances. For our
final system we applied the best configuration for
each PoS as observed in the development experi-
ments, and the syntactic filter. We also incorpo-
rated the sense-weighting to solve ties. The results
of our system, the best competing system, and the
best baseline (WordNet) are shown in Table 3 for the
BEST metric. Precision and recall are provided for
all the instances, and also for the ?Mode? instances
(those that have a single preferred candidate).
Our method outperforms the baseline in all cases,
and performs very close to the top system, ranking
third out of eight systems. This result is consistent
in the ?further analysis? tables provided by the task
organisers for subsets of data, where our system al-
ways performs close to the top score. The overall
scores are below 13% recall for all systems when
targeting all instances. This illustrates the difficulty
of the task, and the similarity of the top-3 scores sug-
All instances Mode
System P R P R
Best 12.90 12.90 20.65 20.65
Relat. in Context 12.68 12.68 20.41 20.41
WordNet baseline 9.95 9.95 15.28 15.28
Table 3: Official results based on the BEST metric.
gests that similar resources (i.e. WordNet) have been
used in the development of the systems.
After the release of the gold-standard data, we
tested two extra settings to measure the effect of the
syntactic filter and the sense-weighting in the final
score. We observed that our application of the syn-
tactic filter had almost no effect in the performance,
but sense-weighting increased the overall recall by
0.4% (from 12.3% to 12.7%).
5 Conclusions
Although the task was difficult and the scores were
low, we showed that by using WordNet and the lo-
cal context we are able to outperform the baselines
and achieve close to top performance. For future
work, we would like to integrate a parser with un-
known word handling in our system. We also aim to
adapt the algorithm to match the target context with
wildcards, in order to avoid explicitly defining the
candidate set.
Acknowledgments
This research was carried out with support from Australian Re-
search Council grant no. DP0663879.
References
Eneko Agirre and Oier Lopez de Lacalle. 2004. Publicly avail-
able topic signatures for all WordNet nominal senses. In
Proc. of the 4rd International Conference on Languages Re-
sources and Evaluations (LREC 2004), pages 1123?6, Lis-
bon, Portugal.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, USA.
Dan Flickinger. 2002. On building a more efficient grammar by
exploiting types. In Stephan Oepen, Dan Flickinger, Jun?ichi
Tsujii, and Hans Uszkoreit, editors, Collaborative Language
Engineering. CSLI Publications, Stanford, USA.
Dekang Lin. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL, pages 768?74,
Montreal, Canada.
David Martinez, Eneko Agirre, and Xinglong Wang. 2006.
Word relatives in context for word sense disambiguation. In
Proc. of the 2006 Australasian Language Technology Work-
shop, pages 42?50, Sydney, Australia.
240
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94?99,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx? , Su Nam Kim? , Zornitsa Kozareva? , Preslav Nakov? ,
Diarmuid O? Se?aghdha?, Sebastian Pado?? , Marco Pennacchiotti??,
Lorenza Romano??, Stan Szpakowicz??
Abstract
We present a brief overview of the main
challenges in the extraction of semantic
relations from English text, and discuss the
shortcomings of previous data sets and shared
tasks. This leads us to introduce a new
task, which will be part of SemEval-2010:
multi-way classification of mutually exclusive
semantic relations between pairs of common
nominals. The task is designed to compare
different approaches to the problem and to
provide a standard testbed for future research,
which can benefit many applications in
Natural Language Processing.
1 Introduction
The computational linguistics community has a con-
siderable interest in robust knowledge extraction,
both as an end in itself and as an intermediate step
in a variety of Natural Language Processing (NLP)
applications. Semantic relations between pairs of
words are an interesting case of such semantic
knowledge. It can guide the recovery of useful facts
about the world, the interpretation of a sentence, or
even discourse processing. For example, pears and
bowl are connected in a CONTENT-CONTAINER re-
lation in the sentence ?The bowl contained apples,
?University of Antwerp, iris.hendrickx@ua.ac.be
?University of Melbourne, snkim@csse.unimelb.edu.au
?University of Alicante, zkozareva@dlsi.ua.es
?National University of Singapore, nakov@comp.nus.edu.sg
?University of Cambridge, do242@cl.cam.ac.uk
?University of Stuttgart, pado@stanford.edu
??Yahoo! Inc., pennacc@yahoo-inc.com
??Fondazione Bruno Kessler, romano@fbk.eu
??University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
pears, and oranges.?, while ginseng and taste are in
an ENTITY-ORIGIN relation in ?The taste is not from
alcohol, but from the ginseng.?.
The automatic recognition of semantic relations
can have many applications, such as information
extraction (IE), document summarization, machine
translation, or construction of thesauri and seman-
tic networks. It can also facilitate auxiliary tasks
such as word sense disambiguation, language mod-
eling, paraphrasing or recognizing textual entail-
ment. For example, semantic network construction
can benefit from detecting a FUNCTION relation be-
tween airplane and transportation in ?the airplane
is used for transportation? or a PART-WHOLE rela-
tion in ?the car has an engine?. Similarly, all do-
mains that require deep understanding of text rela-
tions can benefit from knowing the relations that de-
scribe events like ACQUISITION between named en-
tities in ?Yahoo has made a definitive agreement to
acquire Flickr?.
In this paper, we focus on the recognition of se-
mantic relations between pairs of common nomi-
nals. We present a task which will be part of the
SemEval-2010 evaluation exercise and for which we
are developing a new benchmark data set. This data
set and the associated task address three significant
problems encountered in previous work: (1) the def-
inition of a suitable set of relations; (2) the incorpo-
ration of context; (3) the desire for a realistic exper-
imental design. We outline these issues in Section
2. Section 3 describes the inventory of relations we
adopted for the task. The annotation process, the
design of the task itself and the evaluation method-
ology are presented in Sections 4-6.
94
2 Semantic Relation Classification: Issues
2.1 Defining the Relation Inventory
A wide variety of relation classification schemes ex-
ist in the literature, reflecting the needs and granular-
ities of various applications. Some researchers only
investigate relations between named entities or in-
ternal to noun-noun compounds, while others have a
more general focus. Some schemes are specific to a
domain such as biomedical text.
Rosario and Hearst (2001) classify noun com-
pounds from the domain of medicine into 13 classes
that describe the semantic relation between the head
noun and the modifier. Rosario et al (2002) classify
noun compounds using the MeSH hierarchy and a
multi-level hierarchy of semantic relations, with 15
classes at the top level. Stephens et al (2001) pro-
pose 17 very specific classes targeting relations be-
tween genes. Nastase and Szpakowicz (2003) ad-
dress the problem of classifying noun-modifier rela-
tions in general text. They propose a two-level hier-
archy, with 5 classes at the first level and 30 classes
at the second one; other researchers (Kim and Bald-
win, 2005; Nakov and Hearst, 2008; Nastase et al,
2006; Turney, 2005; Turney and Littman, 2005)
have used their class scheme and data set. Moldovan
et al (2004) propose a 35-class scheme to classify
relations in various phrases; the same scheme has
been applied to noun compounds and other noun
phrases (Girju et al, 2005). Lapata (2002) presents a
binary classification of relations in nominalizations.
Pantel and Pennacchiotti (2006) concentrate on five
relations in an IE-style setting. In short, there is little
agreement on relation inventories.
2.2 The Role of Context
A fundamental question in relation classification is
whether the relations between nominals should be
considered out of context or in context. When one
looks at real data, it becomes clear that context does
indeed play a role. Consider, for example, the noun
compound wood shed : it may refer either to a shed
made of wood, or to a shed of any material used to
store wood. This ambiguity is likely to be resolved
in particular contexts. In fact, most NLP applica-
tions will want to determine not all possible relations
between two words, but rather the relation between
two instances in a particular context. While the in-
tegration of context is common in the field of IE (cf.
work in the context of ACE1), much of the exist-
ing literature on relation extraction considers word
pairs out of context (thus, types rather than tokens).
A notable exception is SemEval-2007 Task 4 Clas-
sification of Semantic Relations between Nominals
(Girju et al, 2007; Girju et al, 2008), the first to of-
fer a standard benchmark data set for seven semantic
relations between common nouns in context.
2.3 Style of Classification
The design of SemEval-2007 Task 4 had an im-
portant limitation. The data set avoided the chal-
lenge of defining a single unified standard classifi-
cation scheme by creating seven separate training
and test sets, one for each semantic relation. That
made the relation recognition task on each data set
a simple binary (positive / negative) classification
task.2 Clearly, this does not easily transfer to prac-
tical NLP settings, where any relation can hold be-
tween a pair of nominals which occur in a sentence
or a discourse.
2.4 Summary
While there is a substantial amount of work on re-
lation extraction, the lack of standardization makes
it difficult to compare different approaches. It is
known from other fields that the availability of stan-
dard benchmark data sets can provide a boost to the
advancement of a field. As a first step, SemEval-
2007 Task 4 offered many useful insights into the
performance of different approaches to semantic re-
lation classification; it has also motivated follow-
up research (Davidov and Rappoport, 2008; Ka-
trenko and Adriaans, 2008; Nakov and Hearst, 2008;
O? Se?aghdha and Copestake, 2008).
Our objective is to build on the achievements of
SemEval-2007 Task 4 while addressing its short-
comings. In particular, we consider a larger set of
semantic relations (9 instead of 7), we assume a
proper multi-class classification setting, we emulate
the effect of an ?open? relation inventory by means
of a tenth class OTHER, and we will release to the
research community a data set with a considerably
1http://www.itl.nist.gov/iad/mig/tests/
ace/
2Although it was not designed for a multi-class set-up, some
subsequent publications tried to use the data sets in that manner.
95
larger number of examples than SemEval-2007 Task
4 or other comparable data sets. The last point is cru-
cial for ensuring the robustness of the performance
estimates for competing systems.
3 Designing an Inventory of Semantic Re-
lations Between Nominals
We begin by considering the first of the problems
listed above: defining of an inventory of semantic
relations. Ideally, it should be exhaustive (should al-
low the description of relations between any pair of
nominals) and mutually exclusive (each pair of nom-
inals in context should map onto only one relation).
The literature, however, suggests no such inventory
that could satisfy all needs. In practice, one always
must decide on a trade-off between these two prop-
erties. For example, the gene-gene relation inven-
tory of Stephens et al (2001), with relations like X
phosphorylates Y, arguably allows no overlaps, but
is too specific for applications to general text.
On the other hand, schemes aimed at exhaus-
tiveness tend to run into overlap issues, due
to such fundamental linguistic phenomena as
metaphor (Lakoff, 1987). For example, in the sen-
tence Dark clouds gather over Nepal., the relation
between dark clouds and Nepal is literally a type of
ENTITY-DESTINATION, but in fact it refers to the
ethnic unrest in Nepal.
We seek a pragmatic compromise between the
two extremes. We have selected nine relations with
sufficiently broad coverage to be of general and
practical interest. We aim at avoiding ?real? overlap
to the extent that this is possible, but we include two
sets of similar relations (ENTITY-ORIGIN/ENTITY-
DESTINATION and CONTENT-CONTAINER/COM-
PONENT-WHOLE/MEMBER-COLLECTION), which
can help assess the models? ability to make such
fine-grained distinctions.3
As in Semeval-2007 Task 4, we give ordered two-
word names to the relations, where each word de-
scribes the role of the corresponding argument. The
full list of our nine relations follows4 (the definitions
we show here are intended to be indicative rather
than complete):
3COMPONENT-WHOLE and MEMBER-COLLECTION are
proper subsets of PART-WHOLE, one of the relations in
SemEval-2007 Task 4.
4We have taken the first five from SemEval-2007 Task 4.
Cause-Effect. An event or object leads to an effect.
Example: Smoking causes cancer.
Instrument-Agency. An agent uses an instrument.
Example: laser printer
Product-Producer. A producer causes a product to
exist. Example: The farmer grows apples.
Content-Container. An object is physically stored
in a delineated area of space, the container. Ex-
ample: Earth is located in the Milky Way.
Entity-Origin. An entity is coming or is derived
from an origin (e.g., position or material). Ex-
ample: letters from foreign countries
Entity-Destination. An entity is moving towards a
destination. Example: The boy went to bed.
Component-Whole. An object is a component of a
larger whole. Example: My apartment has a
large kitchen.
Member-Collection. A member forms a nonfunc-
tional part of a collection. Example: There are
many trees in the forest.
Communication-Topic. An act of communication,
whether written or spoken, is about a topic. Ex-
ample: The lecture was about semantics.
We add a tenth element to this set, the pseudo-
relation OTHER. It stands for any relation which
is not one of the nine explicitly annotated relations.
This is motivated by modelling considerations. Pre-
sumably, the data for OTHER will be very nonho-
mogeneous. By including it, we force any model of
the complete data set to correctly identify the deci-
sion boundaries between the individual relations and
?everything else?. This encourages good generaliza-
tion behaviour to larger, noisier data sets commonly
seen in real-world applications.
3.1 Semantic Relations versus Semantic Roles
There are three main differences between our task
(classification of semantic relations between nomi-
nals) and the related task of automatic labeling of
semantic roles (Gildea and Jurafsky, 2002).
The first difference is to do with the linguistic
phenomena described. Lexical resources for theo-
ries of semantic roles such as FrameNet (Fillmore et
96
al., 2003) and PropBank (Palmer et al, 2005) have
been developed to describe the linguistic realization
patterns of events and states. Thus, they target pri-
marily verbs (or event nominalizations) and their de-
pendents, which are typically nouns. In contrast,
semantic relations may occur between all parts of
speech, although we limit our attention to nominals
in this task. Also, semantic role descriptions typi-
cally relate an event to a set of multiple participants
and props, while semantic relations are in practice
(although not necessarily) binary.
The second major difference is the syntactic con-
text. Theories of semantic roles usually developed
out of syntactic descriptions of verb valencies, and
thus they focus on describing the linking patterns of
verbs and their direct dependents, phenomena like
raising and noninstantiations notwithstanding (Fill-
more, 2002). Semantic relations are not tied to
predicate-argument structures. They can also be es-
tablished within noun phrases, noun compounds, or
sentences more generally (cf. the examples above).
The third difference is that of the level of gen-
eralization. FrameNet currently contains more than
825 different frames (event classes). Since the se-
mantic roles are designed to be interpreted at the
frame level, there is a priori a very large number
of unrelated semantic roles. There is a rudimen-
tary frame hierarchy that defines mappings between
roles of individual frames,5 but it is far from com-
plete. The situation is similar in PropBank. Prop-
Bank does use a small number of semantic roles, but
these are again to be interpreted at the level of in-
dividual predicates, with little cross-predicate gen-
eralization. In contrast, all of the semantic relation
inventories discussed in Section 1 contain fewer than
50 types of semantic relations. More generally, se-
mantic relation inventories attempt to generalize re-
lations across wide groups of verbs (Chklovski and
Pantel, 2004) and include relations that are not verb-
centered (Nastase and Szpakowicz, 2003; Moldovan
et al, 2004). Using the same labels for similar se-
mantic relations facilitates supervised learning. For
example, a model trained with examples of sell re-
lations should be able to transfer what it has learned
to give relations. This has the potential of adding
5For example, it relates the BUYER role of the COM-
MERCE SELL frame (verb sell ) to the RECIPIENT role of the
GIVING frame (verb give).
1. People in Hawaii might be feeling
<e1>aftershocks</e1> from that power-
ful <e2>earthquake</e2> for weeks.
2. My new <e1>apartment</e1> has a
<e2>large kitchen</e2>.
Figure 1: Two example sentences with annotation
crucial robustness and coverage to analysis tools in
NLP applications based on semantic relations.
4 Annotation
The next step in our study will be the actual annota-
tion of relations between nominals. For the purpose
of annotation, we define a nominal as a noun or a
base noun phrase. A base noun phrase is a noun and
its pre-modifiers (e.g., nouns, adjectives, determin-
ers). We do not include complex noun phrases (e.g.,
noun phrases with attached prepositional phrases or
relative clauses). For example, lawn is a noun, lawn
mower is a base noun phrase, and the engine of the
lawn mower is a complex noun phrase.
We focus on heads that are common nouns. This
emphasis distinguishes our task from much work in
IE, which focuses on named entities and on consid-
erably more fine-grained relations than we do. For
example, Patwardhan and Riloff (2007) identify cat-
egories like Terrorist organization as participants in
terror-related semantic relations, which consists pre-
dominantly of named entities. We feel that named
entities are a specific category of nominal expres-
sions best dealt with using techniques which do not
apply to common nouns; for example, they do not
lend themselves well to semantic generalization.
Figure 1 shows two examples of annotated sen-
tences. The XML tags <e1> and <e2> mark the
target nominals. Since all nine proper semantic re-
lations in this task are asymmetric, the ordering of
the two nominals must be taken into account. In
example 1, CAUSE-EFFECT(e1, e2) does not hold,
although CAUSE-EFFECT(e2, e1) would. In exam-
ple 2, COMPONENT-WHOLE(e2, e1) holds.
We are currently developing annotation guide-
lines for each of the relations. They will give a pre-
cise definition for each relation and some prototypi-
cal examples, similarly to SemEval-2007 Task 4.
The annotation will take place in two rounds. In
the first round, we will do a coarse-grained search
97
for positive examples for each relation. We will
collect data from the Web using a semi-automatic,
pattern-based search procedure. In order to ensure
a wide variety of example sentences, we will use
several dozen patterns per relation. We will also
ensure that patterns retrieve both positive and nega-
tive example sentences; the latter will help populate
the OTHER relation with realistic near-miss negative
examples of the other relations. The patterns will
be manually constructed following the approach of
Hearst (1992) and Nakov and Hearst (2008).6
The example collection for each relation R will
be passed to two independent annotators. In order to
maintain exclusivity of relations, only examples that
are negative for all relations but R will be included
as positive and only examples that are negative for
all nine relations will be included as OTHER. Next,
the annotators will compare their decisions and as-
sess inter-annotator agreement. Consensus will be
sought; if the annotators cannot agree on an exam-
ple it will not be included in the data set, but it will
be recorded for future analysis.
Finally, two other task organizers will look for
overlap across all relations. They will discard any
example marked as positive in two or more relations,
as well as examples in OTHER marked as positive in
any of the other classes. The OTHER relation will,
then, consist of examples that are negatives for all
other relations and near-misses for any relation.
Data sets. The annotated data will be divided into
a training set, a development set and a test set. There
will be 1000 annotated examples for each of the
ten relations: 700 for training, 100 for development
and 200 for testing. All data will be released under
the Creative Commons Attribution 3.0 Unported Li-
cense7. The annotation guidelines will be included
in the distribution.
5 The Classification Task
The actual task that we will run at SemEval-2010
will be a multi-way classification task. Not all pairs
of nominals in each sentence will be labeled, so the
gold-standard boundaries of the nominals to be clas-
sified will be provided as part of the test data.
6Note that, unlike in Semeval 2007 Task 4, we will not re-
lease the patterns to the participants.
7http://creativecommons.org/licenses/by/
3.0/
In contrast with Semeval 2007 Task 4, in which
the ordering of the entities was provided with each
example, we aim at a more realistic scenario in
which the ordering of the labels is not given. Par-
ticipants in the task will be asked to discover both
the relation and the order of the arguments. Thus,
the more challenging task is to identify the most
informative ordering and relation between a pair
of nominals. The stipulation ?most informative?
is necessary since with our current set of asym-
metrical relations that includes OTHER, each pair
of nominals that instantiates a relation in one di-
rection (e.g., REL(e1, e2)), instantiates OTHER in
the inverse direction (OTHER (e2, e1)). Thus, the
correct answers for the two examples in Figure 1
are CAUSE-EFFECT (earthquake, aftershocks) and
COMPONENT-WHOLE (large kitchen, apartment).
Note that unlike in SemEval-2007 Task 4, we will
not provide manually annotated WordNet senses,
thus making the task more realistic. WordNet senses
did, however, serve for disambiguation purposes in
SemEval-2007 Task 4. We will therefore have to
assess the effect of this change on inter-annotator
agreement.
6 Evaluation Methodology
The official ranking of the participating systems will
be based on their macro-averaged F-scores for the
nine proper relations. We will also compute and re-
port their accuracy over all ten relations, including
OTHER. We will further analyze the results quan-
titatively and qualitatively to gauge which relations
are most difficult to classify.
Similarly to SemEval-2007 Task 4, in order to
assess the effect of varying quantities of training
data, we will ask the teams to submit several sets of
guesses for the labels for the test data, using varying
fractions of the training data. We may, for example,
request test results when training on the first 50, 100,
200, 400 and all 700 examples from each relation.
We will provide a Perl-based automatic evalua-
tion tool that the participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
7 Conclusion
We have introduced a new task, which will be part of
SemEval-2010: multi-way classification of semantic
98
relations between pairs of common nominals. The
task will compare different approaches to the prob-
lem and provide a standard testbed for future re-
search, which can benefit many NLP applications.
The description we have presented here should
be considered preliminary. We invite the in-
terested reader to visit the official task web-
site http://semeval2.fbk.eu/semeval2.
php?location=tasks\#T11, where up-to-
date information will be published; there is also a
discussion group and a mailing list.
References
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proc. EMNLP 2004, pages 33?40.
Dmitry Davidov and Ari Rappoport. 2008. Classifica-
tion of semantic relationships between nominals using
pattern clusters. In Proc. ACL-08: HLT, pages 227?
235.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235?250.
Charles J. Fillmore. 2002. FrameNet and the linking be-
tween semantic and syntactic relations. In Proc. COL-
ING 2002, pages 28?36.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Roxana Girju, Dan Moldovan, Marta Tatu, , and Dan An-
tohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19:479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 task 04: Classification of semantic re-
lations between nominals. In Proc. 4th Semantic Eval-
uation Workshop (SemEval-2007).
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2008.
Classification of semantic relations between nominals.
Language Resources and Evaluation. In print.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. COLING
92, pages 539?545.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
types of some generic relation arguments: Detection
and evaluation. In Proc. ACL-08: HLT, Short Papers,
pages 185?188.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet sim-
ilarity. In Proc. IJCAI, pages 945?956.
George Lakoff. 1987. Women, fire, and dangerous
things. University of Chicago Press, Chicago, IL.
Maria Lapata. 2002. The disambiguation of nominalisa-
tions. Computational Linguistics, 28:357?388.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In HLT-NAACL
2004: Workshop on Computational Lexical Semantics,
pages 60?67.
Preslav Nakov and Marti A. Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proc. ACL-08: HLT, pages 452?460.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Fifth Interna-
tional Workshop on Computational Semantics (IWCS-
5), pages 285?301.
Vivi Nastase, Jelber Sayyad-Shirabad, Marina Sokolova,
and Stan Szpakowicz. 2006. Learning noun-modifier
semantic relations with corpus-based and WordNet-
based features. In Proc. AAAI, pages 781?787.
Diarmuid O? Se?aghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proc. COLING 2008, pages 649?656.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of seman-
tic roles. Computational Linguistics, 31(1):71?106.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. COLING/ACL, pages
113?120.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
information extraction with semantic affinity patterns
and relevant regions. In Proc. EMNLP-CoNLL), pages
717?727.
Barbara Rosario and Marti Hearst. 2001. Classifying the
semantic relations in noun compounds via a domain-
specific lexical hierarchy. In Proc. EMNLP 2001,
pages 82?90.
Barbara Rosario, Marti Hearst, and Charles Fillmore.
2002. The descent of hierarchy, and selection in re-
lational semantics. In Proc. ACL-02, pages 247?254.
Matthew Stephens, Mathew Palakal, Snehasis
Mukhopadhyay, Rajeev Raje, and Javed Mostafa.
2001. Detecting gene relations from Medline ab-
stracts. In Pacific Symposium on Biocomputing, pages
483?495.
Peter D. Turney and Michael L. Littman. 2005. Corpus-
based learning of analogies and semantic relations.
Machine Learning, 60(1-3):251?278.
Peter D. Turney. 2005. Measuring semantic similarity by
latent relational analysis. In Proc. IJCAI, pages 1136?
1141.
99
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Re-examining Automatic Keyphrase Extraction Approaches
in Scientific Articles
Su Nam Kim
CSSE dept.
University of Melbourne
snkim@csse.unimelb.edu.au
Min-Yen Kan
Department of Computer Science
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
We tackle two major issues in automatic
keyphrase extraction using scientific arti-
cles: candidate selection and feature engi-
neering. To develop an efficient candidate
selection method, we analyze the nature
and variation of keyphrases and then se-
lect candidates using regular expressions.
Secondly, we re-examine the existing fea-
tures broadly used for the supervised ap-
proach, exploring different ways to en-
hance their performance. While most
other approaches are supervised, we also
study the optimal features for unsuper-
vised keyphrase extraction. Our research
has shown that effective candidate selec-
tion leads to better performance as evalua-
tion accounts for candidate coverage. Our
work also attests that many of existing fea-
tures are also usable in unsupervised ex-
traction.
1 Introduction
Keyphrases are simplex nouns or noun phrases
(NPs) that represent the key ideas of the document.
Keyphrases can serve as a representative summary
of the document and also serve as high quality in-
dex terms. It is thus no surprise that keyphrases
have been utilized to acquire critical information
as well as to improve the quality of natural lan-
guage processing (NLP) applications such as doc-
ument summarizer(Da?vanzo and Magnini, 2005),
information retrieval (IR)(Gutwin et al, 1999) and
document clustering(Hammouda et al, 2005).
In the past, various attempts have been made to
boost automatic keyphrase extraction performance
based primarily on statistics(Frank et al, 1999;
Turney, 2003; Park et al, 2004; Wan and Xiao,
2008) and a rich set of heuristic features(Barker
and Corrnacchia, 2000; Medelyan and Witten,
2006; Nguyen and Kan, 2007). In Section 2, we
give a more comprehensive overview of previous
attempts.
Current keyphrase technology still has much
room for improvement. First of all, although sev-
eral candidate selection methods have been pro-
posed for automatic keyphrase extraction in the
past (e.g. (Frank et al, 1999; Park et al, 2004;
Nguyen and Kan, 2007)), most of them do not ef-
fectively deal with various keyphrase forms which
results in the ignorance of some keyphrases as can-
didates. Moreover, no studies thus far have done
a detailed investigation of the nature and varia-
tion of manually-provided keyphrases. As a con-
sequence, the community lacks a standardized list
of candidate forms, which leads to difficulties in
direct comparison across techniques during evalu-
ation and hinders re-usability.
Secondly, previous studies have shown the ef-
fectiveness of their own features but not many
compared their features with other existing fea-
tures. That leads to a redundancy in studies and
hinders direct comparison. In addition, existing
features are specifically designed for supervised
approaches with few exceptions. However, this
approach involves a large amount of manual labor,
thus reducing its utility for real-world application.
Hence, unsupervised approach is inevitable in or-
der to minimize manual tasks and to encourage
utilization. It is a worthy study to attest the re-
liability and re-usability for the unsupervised ap-
proach in order to set up the tentative guideline for
applications.
This paper targets to resolve these issues of
candidate selection and feature engineering. In
our work on candidate selection, we analyze the
nature and variation of keyphrases with the pur-
pose of proposing a candidate selection method
which improves the coverage of candidates that
occur in various forms. Our second contribution
re-examines existing keyphrase extraction features
9
reported in the literature, in terms of their effec-
tiveness and re-usability. We test and compare
the usefulness of each feature for further improve-
ment. In addition, we assess how well these fea-
tures can be applied in an unsupervised approach.
In the remaining sections, we describe an
overview of related work in Section 2, our propos-
als on candidate selection and feature engineering
in Section 4 and 5, our system architecture and
data in Section 6. Then, we evaluate our propos-
als, discuss outcomes and conclude our work in
Section 7, 8 and 9, respectively.
2 Related Work
The majority of related work has been carried
out using statistical approaches, a rich set of
symbolic resources and linguistically-motivated
heuristics(Frank et al, 1999; Turney, 1999; Barker
and Corrnacchia, 2000; Matsuo and Ishizuka,
2004; Nguyen and Kan, 2007). Features used can
be categorized into three broad groups: (1) docu-
ment cohesion features (i.e. relationship between
document and keyphrases)(Frank et al, 1999;
Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007), and to lesser,
(2) keyphrase cohesion features (i.e. relationship
among keyphrases)(Turney, 2003) and (3) term
cohesion features (i.e. relationship among compo-
nents in a keyphrase)(Park et al, 2004).
The simplest system is KEA (Frank et al,
1999; Witten et al, 1999) that uses TF*IDF (i.e.
term frequency * inverse document frequency) and
first occurrence in the document. TF*IDF mea-
sures the document cohesion and the first occur-
rence implies the importance of the abstract or
introduction which indicates the keyphrases have
a locality. Turney (2003) added the notion of
keyphrase cohesion to KEA features and Nguyen
and Kan (2007) added linguistic features such
as section information and suffix sequence. The
GenEx system(Turney, 1999) employed an inven-
tory of nine syntactic features, such as length in
words and frequency of stemming phrase as a
set of parametrized heuristic rules. Barker and
Corrnacchia (2000) introduced a method based
on head noun heuristics that took three features:
length of candidate, frequency and head noun fre-
quency. To take advantage of domain knowledge,
Hulth et al (2001) used a hierarchically-organized
domain-specific thesaurus from Swedish Parlia-
ment as a secondary knowledge source. The
Textract (Park et al, 2004) also ranks the can-
didate keyphrases by its judgment of keyphrases?
degree of domain specificity based on subject-
specific collocations(Damerau, 1993), in addi-
tion to term cohesion using Dice coefficient(Dice,
1945). Recently, Wan and Xiao (2008) extracts
automatic keyphrases from single documents, uti-
lizing document clustering information. The as-
sumption behind this work is that the documents
with the same or similar topics interact with each
other in terms of salience of words. The authors
first clustered the documents then used the graph-
based ranking algorithm to rank the candidates in
a document by making use of mutual influences of
other documents in the same cluster.
3 Keyphrase Analysis
In previous study, KEA employed the index-
ing words as candidates whereas others such as
(Park et al, 2004; Nguyen and Kan, 2007) gen-
erated handcrafted regular expression rules. How-
ever, none carefully undertook the analysis of
keyphrases. We believe there is more to be learned
from the reference keyphrases themselves by do-
ing a fine-grained, careful analysis of their form
and composition. Note that we used the articles
collected from ACM digital library for both ana-
lyzing keyphrases as well as evaluating methods.
See Section 6 for data in detail.
Syntactically, keyphrases can be formed by ei-
ther simplex nouns (e.g. algorithm, keyphrase,
multi-agent) or noun phrases (NPs) which can be a
sequence of nouns and their auxiliary words such
as adjectives and adverbs (e.g. mobile network,
fast computing, partially observable Markov de-
cision process) despite few incidences. They can
also incorporate a prepositional phrase (PP) (e.g.
quality of service, policy of distributed caching).
When keyphrases take the form of an NP with an
attached PP (i.e. NPs in of-PP form), the preposi-
tion of is most common, but others such as for, in,
via also occur (e.g. incentive for cooperation, in-
equality in welfare, agent security via approximate
policy, trade in financial instrument based on log-
ical formula). The patterns above correlate well
to part-of-speech (POS) patterns used in modern
keyphrase extraction systems.
However, our analysis uncovered additional lin-
guistic patterns and alternations which other stud-
ies may have overlooked. In our study we also
found that keyphrases also occur as a simple con-
10
Criteria Rules
Frequency (Rule1) Frequency heuristic i.e. frequency ? 2 for simplex words vs. frequency ? 1 for NPs
Length (Rule2) Length heuristic i.e. up to length 3 for NPs in non-of-PP form vs. up to length 4 for NPs in of-PP form
(e.g. synchronous concurrent program vs. model of multiagent interaction)
Alternation (Rule3) of-PP form alternation
(e.g. number of sensor = sensor number, history of past encounter = past encounter history)
(Rule4) Possessive alternation
(e.g. agent?s goal = goal of agent, security?s value = value of security)
Extraction (Rule5) Noun Phrase = (NN |NNS|NNP |NNPS|JJ |JJR|JJS)?(NN |NNS|NNP |NNPS)
(e.g. complexity, effective algorithm, grid computing, distributed web-service discovery architecture)
(Rule6) Simplex Word/NP IN Simplex Word/NP
(e.g. quality of service, sensitivity of VOIP traffic (VOIP traffic extracted),
simplified instantiation of zebroid (simplified instantiation extracted))
Table 1: Candidate Selection Rules
junctions (e.g. search and rescue, propagation and
delivery), and much more rarely, as conjunctions
of more complex NPs (e.g. history of past en-
counter and transitivity). Some keyphrases appear
to be more complex (e.g. pervasive document edit
and management system, task and resource allo-
cation in agent system). Similarly, abbreviations
and possessive forms figure as common patterns
(e.g. belief desire intention = BDI, inverse docu-
ment frequency = (IDF); Bayes? theorem, agent?s
dominant strategy).
A critical insight of our work is that keyphrases
can be morphologically and semantically altered.
Keyphrases that incorporate a PP or have an un-
derlying genitive composition are often easily var-
ied by word order alternation. Previous studies
have used the altered keyphrases when forming in
of-PP form. For example, quality of service can
be altered to service quality, sometimes with lit-
tle semantic difference. Also, as most morpho-
logical variation in English relates to noun num-
ber and verb inflection, keyphrases are subject to
these rules as well (e.g. distributed system 6= dis-
tributing system, dynamical caching 6= dynamical
cache). In addition, possessives tend to alternate
with of-PP form (e.g. agent?s goal = goal of agent,
security?s value = value of security).
4 Candidate Selection
We now describe our proposed candidate selection
process. Candidate selection is a crucial step for
automatic keyphrase extraction. This step is corre-
lated to term extraction study since top Nth ranked
terms become keyphrases in documents. In pre-
vious study, KEA employed the indexing words
as candidates whereas others such as (Park et al,
2004; Nguyen and Kan, 2007) generated hand-
crafted regular expression rules. However, none
carefully undertook the analysis of keyphrases. In
this section, before we present our method, we first
describe the detail of keyphrase analysis.
In our keyphrase analysis, we observed that
most of author assigned keyphrase and/or reader
assigned keyphrase are syntactically more of-
ten simplex words and less often NPs. When
keyphrases take an NP form, they tend to be a sim-
ple form of NPs. i.e. either without a PP or with
only a PP or with a conjunction, but few appear as
a mixture of such forms. We also noticed that the
components of NPs are normally nouns and adjec-
tives but rarely, are adverbs and verbs. As a re-
sult, we decided to ignore NPs containing adverbs
and verbs in this study as our candidates since they
tend to produce more errors and to require more
complexity.
Another observation is that keyphrases contain-
ing more than three words are rare (i.e. 6% in our
data set), validating what Paukkeri et al (2008)
observed. Hence, we apply a length heuristic. Our
candidate selection rule collects candidates up to
length 3, but also of length 4 for NPs in of-PP
form, since they may have a non-genetive alter-
nation that reduces its length to 3 (e.g. perfor-
mance of distributed system = distributed system
performance). In previous studies, words occur-
ring at least twice are selected as candidates. How-
ever, during our acquisition of reader assigned
keyphrase, we observed that readers tend to collect
NPs as keyphrases, regardless of their frequency.
Due to this, we apply different frequency thresh-
olds for simplex words (>= 2) and NPs (>= 1).
Note that 30% of NPs occurred only once in our
data.
Finally, we generated regular expression rules
to extract candidates, as presented in Table 1. Our
candidate extraction rules are based on those in
Nguyen and Kan (2007). However, our Rule6
for NPs in of-PP form broadens the coverage of
11
possible candidates. i.e. with a given NPs in of-
PP form, not only we collect simplex word(s),
but we also extract non-of-PP form of NPs from
noun phrases governing the PP and the PP. For
example, our rule extracts effective algorithm of
grid computing as well as effective algorithm and
grid computing as candidates while the previous
works? rules do not.
5 Feature Engineering
With a wider candidate selection criteria, the onus
of filtering out irrelevant candidates becomes the
responsibility of careful feature engineering. We
list 25 features that we have found useful in ex-
tracting keyphrases, comprising of 9 existing and
16 novel and/or modified features that we intro-
duce in our work (marked with ?). As one of
our goals in feature engineering is to assess the
suitability of features in the unsupervised setting,
we have also indicated which features are suitable
only for the supervised setting (S) or applicable to
both (S, U).
5.1 Document Cohesion
Document cohesion indicates how important the
candidates are for the given document. The most
popular feature for this cohesion is TF*IDF but
some works have also used context words to check
the correlation between candidates and the given
document. Other features for document cohesion
are distance, section information and so on. We
note that listed features other than TF*IDF are re-
lated to locality. That is, the intuition behind these
features is that keyphrases tend to appear in spe-
cific area such as the beginning and the end of doc-
uments.
F1 : TF*IDF (S,U) TF*IDF indicates doc-
ument cohesion by looking at the frequency of
terms in the documents and is broadly used in pre-
vious work(Frank et al, 1999; Witten et al, 1999;
Nguyen and Kan, 2007). However, a disadvan-
tage of the feature is in requiring a large corpus
to compute useful IDF. As an alternative, con-
text words(Matsuo and Ishizuka, 2004) can also
be used to measure document cohesion. From our
study of keyphrases, we saw that substrings within
longer candidates need to be properly counted, and
as such our method measures TF in substrings as
well as in exact matches. For example, grid com-
puting is often a substring of other phrases such as
grid computing algorithm and efficient grid com-
puting algorithm. We also normalize TF with re-
spect to candidate types: i.e. we separately treat
simplex words and NPs to compute TF. To make
our IDFs broadly representative, we employed the
Google n-gram counts, that were computed
over terabytes of data. Given this large, generic
source of word count, IDF can be incorporated
without corpus-dependent processing, hence such
features are useful in unsupervised approaches
as well. The following list shows variations of
TF*IDF, employed as features in our system.
? (F1a) TF*IDF
? (F1b*) TF including counts of substrings
? (F1c*) TF of substring as a separate feature
? (F1d*) normalized TF by candidate types
(i.e. simplex words vs. NPs)
? (F1e*) normalized TF by candidate types as
a separate feature
? (F1f*) IDF using Google n-gram
F2 : First Occurrence (S,U) KEA used the first
appearance of the word in the document(Frank et
al., 1999; Witten et al, 1999). The main idea
behind this feature is that keyphrases tend to oc-
cur in the beginning of documents, especially in
structured reports (e.g., in abstract and introduc-
tion sections) and newswire.
F3 : Section Information (S,U) Nguyen and
Kan (2007) used the identity of which specific
document section a candidate occurs in. This lo-
cality feature attempts to identify key sections. For
example, in their study of scientific papers, the
authors weighted candidates differently depending
on whether they occurred in the abstract, introduc-
tion, conclusion, section head, title and/or refer-
ences.
F4* : Additional Section Information (S,U)
We first added the related work or previous work
as one of section information not included in
Nguyen and Kan (2007). We also propose and test
a number of variations. We used the substrings
that occur in section headers and reference titles
as keyphrases. We counted the co-occurrence of
candidates (i.e. the section TF) across all key sec-
tions that indicates the correlation among key sec-
tions. We assign section-specific weights as in-
dividual sections exhibit different propensities for
generating keyphrases. For example, introduction
12
contains the majority of keyphrases while the ti-
tle or section head contains many fewer due to the
variation in size.
? (F4a*) section, ?related/previous work?
? (F4b*) counting substring occurring in key
sections
? (F4c*) section TF across all key sections
? (F4d*) weighting key sections according to
the portion of keyphrases found
F5* : Last Occurrence (S,U) Similar to dis-
tance in KEA , the position of the last occurrence
of a candidate may also imply the importance of
keyphrases, as keyphrases tend to appear in the
last part of document such as the conclusion and
discussion.
5.2 Keyphrase Cohesion
The intuition behind using keyphrase cohesion is
that actual keyphrases are often associated with
each other, since they are semantically related to
topic of the document. Note that this assumption
holds only when the document describes a single,
coherent topic ? a document that represents a col-
lection may be first need to be segmented into its
constituent topics.
F6* : Co-occurrence of Another Candidate
in Section (S,U) When candidates co-occur in
several key sections together, then they are more
likely keyphrases. Hence, we used the number of
sections that candidates co-occur.
F7* : Title overlap (S) In a way, titles also rep-
resent the topics of their documents. A large col-
lection of titles in the domain can act as a prob-
abilistic prior of what words could stand as con-
stituent words in keyphrases. In our work, as we
examined scientific papers from computer science,
we used a collection of titles obtained from the
large CiteSeer1 collection to create this feature.
? (F7a*) co-occurrence (Boolean) in title col-
location
? (F7b*) co-occurrence (TF) in title collection
F8 : Keyphrase Cohesion (S,U) Turney (2003)
integrated keyphrase cohesion into his system by
checking the semantic similarity between top N
ranked candidates against the remainder. In the
1It contains 1.3M titles from articles, papers and reports.
original work, a large, external web corpus was
used to obtain the similarity judgments. As we
did not have access to the same web corpus and
all candidates/keyphrases were not found in the
Google n-gram corpus, we approximated this fea-
ture using a similar notion of contextual similarity.
We simulated a latent 2-dimensional matrix (simi-
lar to latent semantic analysis) by listing all candi-
date words in rows and their neighboring words
(nouns, verbs, and adjectives only) in columns.
The cosine measure is then used to compute the
similarity among keyphrases.
5.3 Term Cohesion
Term cohesion further refines the candidacy judg-
ment, by incorporating an internal analysis of the
candidate?s constituent words. Term cohesion
posits that high values for internal word associa-
tion measures correlates indicates that the candi-
date is a keyphrase (Church and Hanks, 1989).
F9 : Term Cohesion (S,U) Park et al (2004)
used in the Dice coefficient (Dice, 1945)
to measure term cohesion particularly for multi-
word terms. In their work, as NPs are longer than
simplex words, they simply discounted simplex
word cohesion by 10%. In our work, we vary the
measure of TF used in Dice coefficient,
similar to our discussion earlier.
? (F9a) term cohesion by (Park et al, 2004),
? (F9b*) normalized TF by candidate types
(i.e. simplex words vs. NPs),
? (F9c*) applying different weight by candi-
date types,
? (F9d*) normalized TF and different weight-
ing by candidate types
5.4 Other Features
F10 : Acronym (S) Nguyen and Kan (2007) ac-
counted for the importance of acronym as a fea-
ture. We found that this feature is heavily depen-
dent on the data set. Hence, we used it only for
N&K to attest our candidate selection method.
F11 : POS sequence (S) Hulth and Megyesi
(2006) pointed out that POS sequences of
keyphrases are similar. It showed the distinctive
distribution of POS sequences of keyphrases and
use them as a feature. Like acronym, this is also
subject to the data set.
13
F12 : Suffix sequence (S) Similar to acronym,
Nguyen and Kan (2007) also used a candidate?s
suffix sequence as a feature, to capture the propen-
sity of English to use certain Latin derivational
morphology for technical keyphrases. This fea-
ture is also a data dependent features, thus used in
supervised approach only.
F13 : Length of Keyphrases (S,U) Barker and
Corrnacchia (2000) showed that candidate length
is also a useful feature in extraction as well as in
candidate selection, as the majority of keyphrases
are one or two terms in length.
6 System and Data
To assess the performance of the proposed candi-
date selection rules and features, we implemented
a keyphrase extraction pipe line. We start with
raw text of computer science articles converted
from PDF by pdftotext. Then, we parti-
tioned the into section such as title and sections
via heuristic rules and applied sentence segmenter
2, ParsCit3(Councill et al, 2008) for refer-
ence collection, part-of-speech tagger4 and lem-
matizer5(Minnen et al, 2001) of the input. Af-
ter preprocessing, we built both supervised and
unsupervised classifiers using Naive Bayes from
the WEKA machine learning toolkit(Witten and
Frank, 2005), Maximum Entropy6, and simple
weighting.
In evaluation, we collected 250 papers from
four different categories7 of the ACM digital li-
brary. Each paper was 6 to 8 pages on average.
In author assigned keyphrase, we found many
were missing or found as substrings. To rem-
edy this, we collected reader assigned keyphrase
by hiring senior year undergraduates in computer
science, each whom annotated five of the papers
with an annotation guideline and on average, took
about 15 minutes to annotate each paper. The fi-
nal statistics of keyphrases is presented in Table
2 where Combined represents the total number of
keyphrases. The numbers in () denotes the num-
ber of keyphrases in of-PP form. Found means the
2http://www.eng.ritsumei.ac.jp/asao/resources/sentseg/
3http://wing.comp.nus.edu.sg/parsCit/
4http://search.cpan.org/dist/Lingua-EN-
Tagger/Tagger.pm
5http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html
6http://maxent.sourceforge.net/index.html
7C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial Intelligence-
Multiagent Systems) and J4 (Social and Behavioral Sciences-
Economics)
number of author assigned keyphrase and reader
assigned keyphrase found in the documents.
Author Reader Combined
Total 1252 (53) 3110 (111) 3816 (146)
NPs 904 2537 3027
Average 3.85 (4.01) 12.44 (12.88) 15.26 (15.85)
Found 769 2509 2864
Table 2: Statistics in Keyphrases
7 Evaluation
The baseline system for both the supervised and
unsupervised approaches is modified N&K which
uses TF*IDF, distance, section information and
additional section information (i.e. F1-4). Apart
from baseline , we also implemented basic
KEA and N&K to compare. Note that N&K is con-
sidered a supervised approach, as it utilizes fea-
tures like acronym, POS sequence, and suffix se-
quence.
Table 3 and 4 shows the performance of our can-
didate selection method and features with respect
to supervised and unsupervised approaches using
the current standard evaluation method (i.e. exact
matching scheme) over top 5th, 10th, 15th candi-
dates.
BestFeatures includes F1c:TF of substring as
a separate feature, F2:first occurrence, F3:section
information, F4d:weighting key sections, F5:last
occurrence, F6:co-occurrence of another candi-
date in section, F7b:title overlap, F9a:term co-
hesion by (Park et al, 2004), F13:length of
keyphrases. Best-TF*IDF means using all best
features but TF*IDF.
In Tables 3 and 4, C denotes the classifier tech-
nique: unsupervised (U) or supervised using Max-
imum Entropy (S)8.
In Table 5, the performance of each feature is
measured using N&K system and the target fea-
ture. + indicates an improvement, - indicates a
performance decline, and ? indicates no effect
or unconfirmed due to small changes of perfor-
mances. Again, supervised denotes Maximum
Entropy training and Unsupervised is our unsu-
pervised approach.
8 Discussion
We compared the performances over our candi-
date selection and feature engineering with sim-
ple KEA , N&K and our baseline system. In eval-
uating candidate selection, we found that longer
8Due to the page limits, we present the best performance.
14
Method Features C Five Ten Fifteen
Match Precision Recall Fscore Match Precising Recall Fscore Match Precision Recall Fscore
All KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.79 15.84% 5.19% 7.82% 1.39 13.88% 9.09% 10.99% 1.84 12.24% 12.03% 12.13%
N&K S 1.32 26.48% 8.67% 13.06% 2.04 20.36% 13.34% 16.12% 2.54 16.93% 16.64% 16.78%
baseline U 0.92 18.32% 6.00% 9.04% 1.57 15.68% 10.27% 12.41% 2.20 14.64% 14.39% 14.51%
S 1.15 23.04% 7.55% 11.37% 1.90 18.96% 12.42% 15.01% 2.44 16.24% 15.96% 16.10%
Length<=3 KEA U 0.03 0.64% 0.21% 0.32% 0.09 0.92% 0.60% 0.73% 0.13 0.88% 0.86% 0.87%
Candidates S 0.81 16.16% 5.29% 7.97% 1.40 14.00% 9.17% 11.08% 1.84 12.24% 12.03% 12.13%
N&K S 1.40 27.92% 9.15% 13.78% 2.10 21.04% 13.78% 16.65% 2.62 17.49% 17.19% 17.34%
baseline U 0.92 18.4% 6.03% 9.08% 1.58 15.76% 10.32% 12.47% 2.20 14.64% 14.39% 14.51%
S 1.18 23.68% 7.76% 11.69% 1.90 19.00% 12.45% 15.04% 2.40 16.00% 15.72% 15.86%
Length<=3 KEA U 0.01 0.24% 0.08% 0.12% 0.05 0.52% 0.34% 0.41% 0.07 0.48% 0.47% 0.47%
Candidates S 0.83 16.64% 5.45% 8.21% 1.42 14.24% 9.33% 11.27% 1.87 12.45% 12.24% 12.34%
+ Alternation N&K S 1.53 30.64% 10.04% 15.12% 2.31 23.08% 15.12% 18.27% 2.88 19.20% 18.87% 19.03%
baseline U 0.98 19.68% 6.45% 9.72% 1.72 17.24% 11.29% 13.64% 2.37 15.79% 15.51% 15.65%
S 1.33 26.56% 8.70% 13.11% 2.09 20.88% 13.68% 16.53% 2.69 17.92% 17.61% 17.76%
Table 3: Performance on Proposed Candidate Selection
Features C Five Ten Fifteen
Match Prec. Recall Fscore Match Prec. Recall Fscore Match Prec. Recall Fscore
Best U 1.14 .228 .747 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
S 1.56 .312 .102 .154 2.50 .250 .164 .198 3.15 .210 .206 .208
Best U 1.14 .228 .74 .113 1.92 .192 .126 .152 2.61 .174 .171 .173
w/o TF*IDF S 1.56 .311 .102 .154 2.46 .246 .161 .194 3.12 .208 .204 .206
Table 4: Performance on Feature Engineering
A Method Feature
+ S F1a,F2,F3,F4a,F4d,F9a
U F1a,F1c,F2,F3,F4a,F4d,F5,F7b,F9a
- S F1b,F1c,F1d,F1f,F4b,F4c,F7a,F7b,F9b-d,F13
U F1d,F1e,F1f,F4b,F4c,F6,F7a,F9b-d
? S F1e,F10,F11,F12
U F1b
Table 5: Performance on Each Feature
length candidates play a role to be noises so de-
creased the overall performance. We also con-
firmed that candidate alternation offered the flexi-
bility of keyphrases leading higher candidate cov-
erage as well as better performance.
To re-examine features, we analyzed the impact
of existing and new features and their variations.
First of all, unlike previous studies, we found that
the performance with and without TF*IDF did not
lead to a large difference which indicates the im-
pact of TF*IDF was minor, as long as other fea-
tures are incorporated. Secondly, counting sub-
strings for TF improved performance, while ap-
plying term weighting for TF and/or IDF did not
impact on the performance. We estimated the
cause that many of keyphrases are substrings of
candidates and vice versa. Thirdly, section in-
formation was also validated to improve perfor-
mance, as in Nguyen and Kan (2007). Extend-
ing this logic, modeling additional section infor-
mation (related work) and weighting sections both
turned out to be useful features. Other locality
features were also validated as helpful: both first
occurrence and last occurrence are helpful as it
implies the locality of the key ideas. In addi-
tion, keyphrase co-occurrence with selected sec-
tions was proposed in our work and found empiri-
cally useful. Term cohesion (Park et al, 2004) is a
useful feature although it has a heuristic factor that
reduce the weight by 10% for simplex words. Nor-
mally, term cohesion is subject to NPs only, hence
it needs to be extended to work with multi-word
NPs as well. Table 5 summarizes the reflections
on each feature.
As unsupervised methods have the appeal of not
needing to be trained on expensive hand-annotated
data, we also compared the performance of super-
vised and unsupervised methods. Given the fea-
tures initially introduced for supervised learning,
unsupervised performance is surprisingly high.
While supervised classifier produced a matching
count of 3.15, the unsupervised classifier obtains a
count of 2.61. We feel this indicates that the exist-
ing features for supervised methods are also suit-
able for use in unsupervised methods, with slightly
reduced performance. In general, we observed that
the best features in both supervised and unsuper-
vised methods are the same ? section information
and candidate length. In our analysis of the im-
pact of individual features, we observed that most
features affect performance in the same way for
both supervised and unsupervised approaches, as
shown in Table 5. These findings indicate that al-
though these features may be been originally de-
signed for use in a supervised approach, they are
stable and can be expected to perform similar in
unsupervised approaches.
15
9 Conclusion
We have identified and tackled two core issues
in automatic keyphrase extraction: candidate se-
lection and feature engineering. In the area of
candidate selection, we observe variations and al-
ternations that were previously unaccounted for.
Our selection rules expand the scope of possible
keyphrase coverage, while not overly expanding
the total number candidates to consider. In our
re-examination of feature engineering, we com-
piled a comprehensive feature list from previous
works while exploring the use of substrings in de-
vising new features. Moreover, we also attested to
each feature?s fitness for use in unsupervised ap-
proaches, in order to utilize them in real-world ap-
plications with minimal cost.
10 Acknowledgement
This work was partially supported by a National Research
Foundation grant, Interactive Media Search (grant # R 252
000 325 279), while the first author was a postdoctoral fellow
at the National University of Singapore.
References
Ken Barker and Nadia Corrnacchia. Using noun phrase
heads to extract document keyphrases. In Proceedings of
the 13th Biennial Conference of the Canadian Society on
Computational Studies of Intelligence: Advances in Arti-
ficial Intelligence. 2000.
Regina Barzilay and Michael Elhadad. Using lexical chains
for text summarization. In Proceedings of the ACL/EACL
1997 Workshop on Intelligent Scalable Text Summariza-
tion. 1997, pp. 10?17.
Kenneth Church and Patrick Hanks. Word association
norms, mutual information and lexicography. In Proceed-
ings of ACL. 1989, 76?83.
Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit:
An open-source CRF reference string parsing package. In
Proceedings of LREC. 2008, 28?30.
Ernesto DA?vanzo and Bernado Magnini. A Keyphrase-
Based Approach to Summarization:the LAKE System at
DUC-2005. In Proceedings of DUC. 2005.
F. Damerau. Generating and evaluating domain-oriented
multi-word terms from texts. Information Processing and
Management. 1993, 29, pp.43?447.
Lee Dice. Measures of the amount of ecologic associations
between species. Journal of Ecology. 1945, 2.
Eibe Frank and Gordon Paynter and Ian Witten and Carl
Gutwin and Craig Nevill-manning. Domain Specific
Keyphrase Extraction. In Proceedings of IJCAI. 1999,
pp.668?673.
Carl Gutwin and Gordon Paynter and Ian Witten and Craig
Nevill-Manning and Eibe Frank. Improving browsing in
digital libraries with keyphrase indexes. Journal of Deci-
sion Support Systems. 1999, 27, pp.81?104.
Khaled Hammouda and Diego Matute and Mohamed Kamel.
CorePhrase: keyphrase extraction for document cluster-
ing. In Proceedings of MLDM. 2005.
Annette Hulth and Jussi Karlgren and Anna Jonsson and
Henrik Bostrm and Lars Asker. Automatic Keyword Ex-
traction using Domain Knowledge. In Proceedings of CI-
CLing. 2001.
Annette Hulth and Beata Megyesi. A study on automatically
extracted keywords in text categorization. In Proceedings
of ACL/COLING. 2006, 537?544.
Mario Jarmasz and Caroline Barriere. Using semantic sim-
ilarity over tera-byte corpus, compute the performance of
keyphrase extraction. In Proceedings of CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summarization. In
Proceedings of SIGIR. 2001, pp. 349?357.
Y. Matsuo and M. Ishizuka. Keyword Extraction from a Sin-
gle Document using Word Co-occurrence Statistical Infor-
mation. In International Journal on Artificial Intelligence
Tools. 2004, 13(1), pp. 157?169.
Olena Medelyan and Ian Witten. Thesaurus based automatic
keyphrase indexing. In Proceedings of ACM/IEED-CS
joint conference on Digital libraries. 2006, pp.296?297.
Guido Minnen and John Carroll and Darren Pearce. Applied
morphological processing of English. NLE. 2001, 7(3),
pp.207?223.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extrac-
tion in Scientific Publications. In Proceeding of ICADL.
2007, pp.317-326.
Youngja Park and Roy Byrd and Branimir Boguraev. Auto-
matic Glossary Extraction Beyond Terminology Identifi-
cation. In Proceedings of COLING. 2004, pp.48?55.
Mari-Sanna Paukkeri and Ilari Nieminen and Matti Polla
and Timo Honkela. A Language-Independent Approach
to Keyphrase Extraction and Evaluation. In Proceedings
of COLING. 2008.
Peter Turney. Learning to Extract Keyphrases from Text.
In National Research Council, Institute for Information
Technology, Technical Report ERB-1057. 1999.
Peter Turney. Coherent keyphrase extraction via Web min-
ing. In Proceedings of IJCAI. 2003, pp. 434?439.
Xiaojun Wan and Jianguo Xiao. CollabRank: towards a col-
laborative approach to single-document keyphrase extrac-
tion. In Proceedings of COLING. 2008.
Ian Witten and Gordon Paynter and Eibe Frank and Car
Gutwin and Graig Nevill-Manning. KEA:Practical Au-
tomatic Key phrase Extraction. In Proceedings of ACM
DL. 1999, pp.254?256.
Ian Witten and Eibe Frank. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kauf-
mann, 2005.
Yongzheng Zhang and Nur Zinchir-Heywood and Evange-
los Milios. Term based Clustering and Summarization of
Web Page Collections. In Proceedings of Conference of
the Canadian Society for Computational Studies of Intel-
ligence. 2004.
16
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 31?39,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A Re-examination of Lexical Association Measures  
 
 
Abstract 
We review lexical Association Measures 
(AMs) that have been employed by past 
work in extracting multiword expressions. 
Our work contributes to the understanding 
of these AMs by categorizing them into 
two groups and suggesting the use of rank 
equivalence to group AMs with the same 
ranking performance. We also examine 
how existing AMs can be adapted to better 
rank English verb particle constructions 
and light verb constructions. Specifically, 
we suggest normalizing (Pointwise) 
Mutual Information and using marginal 
frequencies to construct penalization 
terms.  We empirically validate the 
effectiveness of these modified AMs in 
detection tasks in English, performed on 
the Penn Treebank, which shows 
significant improvement over the original 
AMs. 
1 Introduction 
Recently, the NLP community has witnessed a 
renewed interest in the use of lexical association 
measures in extracting Multiword Expressions 
(MWEs). Lexical Association Measures 
(hereafter, AMs) are mathematical formulas 
which can be used to capture the degree of 
connection or association between constituents 
of a given phrase. Well-known AMs include 
Pointwise Mutual Information (PMI), 
Pearson?s 2? and the Odds Ratio. These AMs 
have been applied in many different fields of 
study, from information retrieval to hypothesis 
testing. In the context of MWE extraction, many 
published works have been devoted to comparing 
their effectiveness. Krenn and Evert (2001) 
evaluate Mutual Information (MI), Dice, 
Pearson?s 2? , log-likelihood  
ratio and the T score. In Pearce (2002), AMs 
such as Z score, Pointwise MI, cost reduction, 
left and right context entropy, odds ratio are 
evaluated. Evert (2004) discussed a wide range 
of AMs, including exact hypothesis tests such as 
the binomial test and Fisher?s exact tests, various 
coefficients such as Dice and Jaccard. Later, 
Ramisch et al (2008) evaluated MI, 
Pearson?s 2? and Permutation Entropy. Probably 
the most comprehensive evaluation of AMs was 
presented in Pecina and Schlesinger (2006), 
where 82 AMs were assembled and evaluated 
over Czech collocations. These collocations 
contained a mix of idiomatic expressions, 
technical terms, light verb constructions and 
stock phrases. In their work, the best 
combination of AMs was selected using machine 
learning.  
While the previous works have evaluated AMs, 
there have been few details on why the AMs 
perform as they do.  A detailed analysis of why 
these AMs perform as they do is needed in order 
to explain their identification performance, and 
to help us recommend AMs for future tasks. This 
weakness of previous works motivated us to 
address this issue. In this work, we contribute to 
further understanding of association measures, 
using two different MWE extraction tasks to 
motivate and concretize our discussion. Our goal 
is to be able to predict, a priori, what types of 
AMs are likely to perform well for a particular 
MWE class. 
We focus on the extraction of two common 
types of English MWEs that can be captured by 
bigram model: Verb Particle Constructions 
(VPCs) and Light Verb Constructions (LVCs). 
VPCs consist of a verb and one or more particles, 
which can be prepositions (e.g. put on, bolster 
up), adjectives (cut short) or verbs (make do). 
For simplicity, we focus only on bigram VPCs 
that take prepositional particles, the most 
common class of VPCs. A special characteristic 
of VPCs that affects their extraction is the 
Hung Huu Hoang 
Dept. of Computer Science 
National University 
of  Singapore 
hoanghuu@comp.nus.edu.sg 
Su Nam Kim 
Dept. of Computer Science 
and Software Engineering 
University of Melbourne 
snkim@csse.unimelb.edu.au
Min-Yen Kan 
Dept. of Computer Science
National University 
of  Singapore 
kanmy@comp.nus.edu.sg
31
mobility of noun phrase complements in 
transitive VPCs. They can appear after the 
particle (Take off your hat) or between the verb 
and the particle (Take your hat off). However, a 
pronominal complement can only appear in the 
latter configuration (Take it off).  
In comparison, LVCs comprise of a verb and a 
complement, which is usually a noun phrase 
(make a presentation, give a demonstration). 
Their meanings come mostly from their 
complements and, as such, verbs in LVCs are 
termed semantically light, hence the name light 
verb. This explains why modifiers of LVCs 
modify the complement instead of the verb 
(make a serious mistake vs. *make a mistake 
seriously).  This phenomenon also shows that an 
LVC?s constituents may not occur contiguously. 
2 Classification of Association Measures 
Although different AMs have different 
approaches to measuring association, we 
observed that they can effectively be classified 
into two broad classes. Class I AMs look at the 
degree of institutionalization; i.e., the extent to 
which the phrase is a semantic unit rather than a 
free combination of words. Some of the AMs in 
this class directly measure this association 
between constituents using various combinations 
of co-occurrence and marginal frequencies. 
Examples include MI, PMI and their variants as 
well as most of the association coefficients such 
as Jaccard, Hamann, Brawn-Blanquet, and 
others. Other Class I AMs estimate a phrase?s 
MWE-hood by judging the significance of the 
difference between observed and expected 
frequencies. These AMs include, among others, 
statistical hypothesis tests such as T score, Z 
score and Pearson?s 2? test.  
Class II AMs feature the use of context to 
measure non-compositionality, a peculiar 
characteristic of many types of MWEs, including 
VPCs and idioms. This is commonly done in one 
of the following two ways. First, non-
compositionality can be modeled through the 
diversity of contexts, measured using entropy. 
The underlying assumption of this approach is 
that non-compositional phrases appear in a more 
restricted set of contexts than compositional ones. 
Second, non-compositionality can also be 
measured through context similarity between the 
phrase and its constituents. The observation here 
is that non-compositional phrases have different 
semantics from those of their constituents. It then 
follows that contexts in which the phrase and its 
constituents appear would be different (Zhai, 
1997). Some VPC examples include carry out, 
give up. A close approximation stipulates that 
contexts of a non-compositional phrase?s 
constituents are also different. For instance, 
phrases such as hot dog and Dutch courage are 
comprised of constituents that have unrelated 
meanings. Metrics that are commonly used to 
compute context similarity include cosine and 
dice similarity; distance metrics such as 
Euclidean and Manhattan norm; and probability 
distribution measures such as Kullback-Leibler 
divergence and Jensen-Shannon divergence.  
 
Table 1 lists all AMs used in our discussion. 
The lower left legend defines the variables a, b, c, 
and d with respect to the raw co-occurrence 
statistics observed in the corpus data.  When an 
AM is introduced, it is prefixed with its index 
given in Table 1(e.g., [M2] Mutual Information) 
for the reader?s convenience.  
3 Evaluation   
We will first present how VPC and LVC 
candidates are extracted and used to form our 
evaluation data set. Second, we will discuss how 
performances of AMs are measured in our 
experiments. 
3.1 Evaluation Data 
In this study, we employ the Wall Street Journal 
(WSJ) section of one million words in the Penn 
Tree Bank. To create the evaluation data set, we 
first extract the VPC and LVC candidates from 
our corpus as described below. We note here that 
the mobility property of both VPC and LVC 
constituents have been used in the extraction 
process. 
For VPCs, we first identify particles using a 
pre-compiled set of 38 particles based on 
Baldwin (2005) and Quirk et al (1985) 
(Appendix A). Here we do not use the WSJ 
particle tag to avoid possible inconsistencies 
pointed out in Baldwin (2005). Next, we search 
to the left of the located particle for the nearest 
verb. As verbs and particles in transitive VPCs 
may not occur contiguously, we allow an 
intervening NP of up to 5 words, similar to 
Baldwin and Villavicencio (2002) and Smadja 
(1993), since longer NPs tend to be located after 
particles.  
32
Extraction of LVCs is carried out in a similar 
fashion. First, occurrences of light verbs are 
located based on the following set of seven 
frequently used English light verbs: do, get, give, 
have, make, put and take. Next, we search to the      
right of the light verbs for the nearest noun, 
AM Name Formula AM Name Formula 
M1. Joint Probability ( ) /f xy N  M2. Mutual  Information 
,
1
log
?
ij
ij
i j ij
f
f
N f
?  
M3. Log likelihood      
        ratio 
,
2 log
?
ij
ij
i j ij
f
f
f
?  M4. Pointwise MI (PMI) ( )log
( ) ( )
P xy
P x P y? ?
 
M5. Local-PMI ( ) PMIf xy ?  M6. PMIk ( )
log
( ) ( )
kNf xy
f x f y? ?
 
M7. PMI2 2( )
log
( ) ( )
Nf xy
f x f y? ?
 
M8. Mutual Dependency 2( )
log
( *) (* )
P xy
P x P y
 
M9. Driver-Kroeber  
( )( )
a
a b a c+ +
 
M10. Normalized   
          expectation 
2
2
a
a b c+ +
 
M11. Jaccard a
a b c+ +
 
M12. First Kulczynski a
b c+
 
M13. Second  
         Sokal-Sneath 2( )
a
a b c+ +
 
M14. Third  
          Sokal-Sneath 
a d
b c
+
+
 
M15. Sokal-Michiner a d
a b c d
+
+ + +
 
M16. Rogers-Tanimoto 
2 2
a d
a b c d
+
+ + +
 
M17. Hamann ( ) ( )a d b c
a b c d
+ ? +
+ + +
 
M18. Odds ratio ad
bc
 
M19. Yule?s ?  ad bc
ad bc
?
+
 
M20. Yule?s Q ad bc
ad bc
?
+
 
M21. Brawn-     
          Blanquet max( , )
a
a b a c+ +
 
M22. Simpson 
min( , )
a
a b a c+ +
 
M23. S cost 1
2
min( , )
log(1 )
1
b c
a
?
+
+
 
M24*. Adjusted S Cost 1
2
max( , )
log(1 )
1
b c
a
?
+
+
 
M25. Laplace 1
 min( ,  ) 2
a
a b c
+
+ +
 
M26*. Adjusted Laplace 1
 max( ,  ) 2
a
a b c
+
+ +
 
M27. Fager  
[M9]
1
max( , )
2
b c?  
M28*. Adjusted Fager 
[M9]
1
max( , )b c
aN
?  
M29*. Normalized     
            PMIs  
PMI / NF( )?  
PMI / NFMax 
M30*. Simplified 
normalized PMI for 
VPCs 
log( )
(1 )
ad
b c? ?? + ? ?
 
M31*. Normalized  
           MIs 
MI / NF( )?  
MI / NFMax 
NF( )?  = ( )P x? ?  + (1 ) ( )P y?? ?   [0,  1]? ?  
NFMax = max( ( ),  ( ))P x P y? ?  
11 ( )a f f xy= =   12 ( )b f f xy= =    
21 ( )c f f xy= =  22 ( )d f f xy= =  
( )f x?  
( )f x?  
            ( )f y?               ( )f y?  N 
 
Table 1. Association measures discussed in this paper. Starred AMs (*) are developed in this work. 
Contingency table of a bigram (x y), recording co-
occurrence and marginal frequencies; w  stands for all 
words except w; * stands for all words; N is total 
number of bigrams. The expected frequency under the 
independence assumption is ? ( ) ( ) ( ) / .f xy f x f y N= ? ?  
33
permitting a maximum of 4 intervening words to 
allow for quantifiers (a/an, the, many, etc.), 
adjectival and adverbial modifiers, etc. If this 
search fails to find a noun, as when LVCs are 
used in the passive (e.g. the presentation was 
made), we search to the right of the light verb, 
also allowing a maximum of 4 intervening words. 
The above extraction process produced a total of 
8,652 VPC and 11,465 LVC candidates when 
run on the corpus. We then filter out candidates 
with observed frequencies less than 6, as 
suggested in Pecina and Schlesinger (2006), to 
obtain a set of 1,629 VPCs and 1,254 LVCs.  
Separately, we use the following two available 
sources of annotations: 3,078 VPC candidates 
extracted and annotated in (Baldwin, 2005) and 
464 annotated LVC candidates used in (Tan et 
al., 2006). Both sets of annotations give both 
positive and negative examples. 
Our final VPC and LVC evaluation datasets 
were then constructed by intersecting the gold-
standard datasets with our corresponding sets of 
extracted candidates. We also concatenated both 
sets of evaluation data for composite evaluation.  
This set is referred to as ?Mixed?. Statistics of 
our three evaluation datasets are summarized in 
Table 2.  
 
 VPC data LVC data Mixed 
Total  
(freq  ? 6) 
413 100 513 
Positive  
instances 
117 
(28.33%)
28 
(28%) 
145 
(23.26%)
Table 2. Evaluation data sizes (type count, not token). 
 
While these datasets are small, our primary 
goal in this work is to establish initial 
comparable baselines and describe interesting 
phenomena that we plan to investigate over 
larger datasets in future work.  
3.2 Evaluation Metric  
To evaluate the performance of AMs, we can use 
the standard precision and recall measures, as in 
much past work.  We note that the ranked list of 
candidates generated by an AM is often used as a 
classifier by setting a threshold. However, setting 
a threshold is problematic and optimal threshold 
values vary for different AMs. Additionally, 
using the list of ranked candidates directly as a 
classifier does not consider the confidence 
indicated by actual scores. Another way to avoid 
setting threshold values is to measure precision 
and recall of only the n most likely candidates 
(the n-best method). However, as discussed in 
Evert and Krenn (2001), this method depends 
heavily on the choice of n. In this paper, we opt 
for average precision (AP), which is the average 
of precisions at all possible recall values. This 
choice also makes our results comparable to 
those of Pecina and Schlesinger (2006).  
3.3 Evaluation Results 
Figure 1(a, b) gives the two average precision 
profiles of the 82 AMs presented in Pecina and 
Schlesinger (2006) when we replicated their 
experiments over our English VPC and LVC 
datasets. We observe that the average precision 
profile for VPCs is slightly concave while the 
one for LVCs is more convex. This can be 
interpreted as VPCs being more sensitive to the 
choice of AM than LVCs. Another point we 
observed is that a vast majority of Class I AMs, 
including PMI, its variants and association 
coefficients (excluding hypothesis tests), perform 
reasonably well in our application. In contrast, 
the performances of most of context-based and 
hypothesis test AMs are very modest. Their 
mediocre performance indicates their 
inapplicability to our VPC and LVC tasks. In 
particular, the high frequencies of particles in 
VPCs and light verbs in LVCs both undermine 
their contexts? discriminative power and skew 
the difference between observed and expected 
frequencies that are relied on in hypothesis tests.  
4 Rank Equivalence 
We note that some AMs, although not 
mathematically equivalent (i.e., assigning 
identical scores to input candidates) produce the 
same lists of ranked candidates on our datasets. 
Hence, they achieve the same average precision. 
The ability to identify such groups of AMs is 
helpful in simplifying their formulas, which in 
turn assisting in analyzing their meanings. 
 
Definition: Association measures M1 and M2 are 
rank equivalent over a set C, denoted by M1 
r
C
?  
M2, if and only if M1(cj) > M1(ck) ? M2(cj) > 
M2(ck) and M1(cj) = M1(ck) ? M2(cj) = M2(ck) for 
all cj, ck belongs to C where Mk(ci) denotes the 
score assigned to ci by the measure Mk.  
 
As a corollary, the following also holds for rank 
equivalent AMs:  
34
Corollary: If M1 
r
C
?  M2 then APC(M1) = APC(M2) 
where APC(Mi) stands for the average precision 
of the AM Mi over the data set C.  
 
Essentially, M1 and M2 are rank equivalent over 
a set C if their ranked lists of all candidates taken 
from C are the same, ignoring the actual 
calculated scores1. As an example, the following 
3 AMs: Odds ratio, Yule?s ? and Yule?s Q (Table 
3, row 5), though not mathematically equivalent, 
can be shown to be rank equivalent. Five groups 
of rank equivalent AMs that we have found are 
listed in Table 3.  This allows us to replace the 
below 15 AMs with their (most simple) 
representatives from each rank equivalent group. 
 
 
 
                                                 
1 Two AMs may be rank equivalent with the exception of 
some candidates where one AM is undefined due to a zero 
in the denominator while the other AM is still well-defined. 
We call these cases weakly rank equivalent. With a 
reasonably large corpus, such candidates are rare for our 
VPC and LVC types. Hence, we still consider such AM 
pairs to be rank equivalent. 
1) [M2] Mutual Information,  
    [M3] Log likelihood ratio 
2) [M7] PMI2, [M8] Mutual Dependency,  
    [M9] Driver-Kroeber (a.k.a. Ochiai) 
3) [M10] Normalized expectation,  
    [M11] Jaccard, [M12] First Kulczynski,  
[M13]Second Sokal-Sneath  
          (a.k.a. Anderberg) 
4) [M14] Third Sokal-Sneath,  
    [M15] Sokal-Michiner, 
    [M16] Rogers-Tanimoto, [M17] Hamann 
5) [M18] Odds ratio, [M19] Yule?s ,?       
    [M20] Yule?s Q 
Table 3. Five groups of rank equivalent AMs. 
5 Examination of Association Measures 
We highlight two important findings in our 
analysis of the AMs over our English datasets. 
Section 5.1 focuses on MI and PMI and Section 
5.2 discusses penalization terms.   
5.1 Mutual Information and Pointwise 
Mutual Information 
In Figure 1, over 82 AMs, PMI ranks 11th in 
identifying VPCs while MI ranks 35th in 
0.1
0.2
0.3
0.4
0.5
0.6
AP
 
Figure 1a. AP profile of AMs examined over our VPC data set. 
0.1
0.2
0.3
0.4
0.5
0.6
AP
 
Figure 1b. AP profile of AMs examined over our LVC data set. 
Figure 1. Average precision (AP) performance of the 82 AMs from Pecina and Schlesinger (2006), on our 
English VPC and LVC datasets. Bold points indicate AMs discussed in this paper.  
? Hypothesis test AMs     ? Class I AMs, excluding hypothesis test AMs     + Context-based AMs. 
35
identifying LVCs. In this section, we show how 
their performances can be improved significantly.  
Mutual Information (MI) measures the 
common information between two variables or 
the reduction in uncertainty of one variable given 
knowledge of the 
other.
,
( )MI( ; ) ( )log
( ) ( )u v
p uvU V p uv
p u p v
=
? ?
? . In the 
context of bigrams, the above formula can be 
simplified to [M2] MI =
,
1
log
?N
ij
ij
i j
ij
f
f
f
? . While MI 
holds between random variables, [M4] Pointwise 
MI (PMI) holds between specific values: PMI(x, 
y) =
( )
log
( ) ( )
P xy
P x P y? ?
( )
log
( ) ( )
Nf xy
f x f y
=
? ?
. It has long 
been pointed out that PMI favors bigrams with 
low-frequency constituents, as evidenced by the 
product of two marginal frequencies in its 
denominator. To reduce this bias, a common 
solution is to assign more weight to the co-
occurrence frequency ( )f xy in the numerator by 
either raising it to some power k (Daille, 1994) or 
multiplying PMI with ( )f xy . Table 4 lists these 
adjusted versions of PMI and their performance 
over our datasets. We can see from Table 4 that 
the best performance of PMIk is obtained at k 
values less than one, indicating that it is better to 
rely less on ( )f xy . Similarly, multiplying 
( )f xy directly to PMI reduces the performance of 
PMI. As such, assigning more weight to ( )f xy  
does not improve the AP performance of PMI.  
 
AM VPCs LVCs Mixed
Best [M6] PMIk .547 
(k = .13) 
.573 
(k = .85) 
.544 
(k = .32)
[M4] PMI .510 .566 .515 
[M5] Local-PMI  .259 .393 .272 
[M1] Joint Prob. .170 .28 .175 
Table 4. AP performance of PMI and its variants. Best 
alpha settings shown in parentheses. 
 
Another shortcoming of (P)MI is that both 
grow not only with the degree of dependence but 
also with frequency (Manning and Schutze,&& 1999, 
p. 66). In particular, we can show that MI(X; Y) ? 
min(H(X), H(Y)), where H(.) denotes entropy, 
and PMI(x,y) ? min( log ( ),P x? ?  log ( )P y? ? ). 
These two inequalities suggest that the 
allowed score ranges of different candidates vary 
and consequently, MI and PMI scores are not 
directly comparable. Furthermore, in the case of 
VPCs and LVCs, the differences among score 
ranges of different candidates are large, due to 
high frequencies of particles and light verbs. This 
has motivated us to normalize these scores 
before using them for comparison. We suggest 
MI and PMI be divided by one of the following 
two normalization factors: NF( )? = ( )P x? ?  + 
(1 ) ( )P y?? ? with [0,  1]? ? and NFmax 
= max( ( ),  ( ))P x P y? ? . NF( )? , being dependent on 
alpha, can be optimized by setting an appropriate 
alpha value, which is inevitably affected by the 
MWE type and the corpus statistics. On the other 
hand, NFmax is independent of alpha and is 
recommended when one needs to apply 
normalized (P)MI to a mixed set of different 
MWE types or when sufficient data for 
parameter tuning is unavailable. As shown in 
Table 5, normalized MI and PMI show 
considerable improvements of up to 80%. Also, 
PMI and MI, after being normalized with NFmax, 
rank number one in VPC and LVC task, 
respectively. If one re-writes MI as = (1/ 
N) ij ij
i, j
PMIf ?? , it is easy to see the heavy 
dependence of MI on direct frequencies 
compared with PMI and this explains why 
normalization is a pressing need for MI.  
 
AM VPCs LVCs Mixed
MI / NF( )?  .508 
(? = .48)  
.583 
(? = .47) 
.516 
(? = .5)
MI / NFmax .508 .584 .518 
[M2] MI .273 .435 .289 
PMI / NF( )?  .592 
(? = .8)  
.554 
(? = .48)  
.588 
(? = .77)
PMI / NFmax .565 .517 .556 
[M4] PMI .510 .566 .515 
Table 5. AP performance of normalized (P)MI versus 
standard (P)MI. Best alpha settings shown in 
parentheses. 
5.2 Penalization Terms  
It can be seen that given equal co-occurrence 
frequencies, higher marginal frequencies reduce 
the likelihood of being MWEs. This motivates us 
to use marginal frequencies to synthesize 
penalization terms which are formulae whose 
values are inversely proportional to the 
likelihood of being MWEs. We hypothesize that 
incorporating such penalization terms can 
improve the respective AMs detection AP.  
Take as an example, the AMs [M21] Brawn-
Blanquet (a.k.a. Minimum Sensitivity) and [M22] 
Simpson. These two AMs are identical, except 
36
for one difference in the denominator: Brawn-
Blanquet uses max(b, c); Simpson uses min(b, c). 
It is intuitive and confirmed by our experiments 
that penalizing against the more frequent 
constituent by choosing max(b, c) is more 
effective. This is further attested in AMs [M23] 
S Cost and [M25] Laplace, where we tried to 
replace the min(b, c) term with max(b, c). Table 
6 shows the average precision on our datasets for 
all these AMs.   
 
AM VPCs LVCs Mixed
[M21]Brawn-    
          Blanquet 
.478 .578 .486 
[M22] Simpson .249 .382 .260 
[M24] Adjusted  
            S Cost 
.485 .577 .492 
[M23] S cost .249 .388 .260 
[M26] Adjusted  
           Laplace 
.486 .577 .493 
[M25] Laplace .241 .388 .254 
Table 6. Replacing min() with max() in selected AMs. 
 
In the [M27] Fager AM, the penalization term 
max(b, c) is subtracted from the first term, which 
is no stranger but rank equivalent to [M7] PMI2. 
In our application, this AM is not good since the 
second term is far larger than the first term, 
which is less than 1. As such, Fager is largely 
equivalent to just ?? max(b, c). In order to make 
use of the first term, we need to replace the 
constant ? by a scaled down version of max(b, 
c). We have approximately derived 1/ aN as a 
lower bound estimate of max(b, c) using the 
independence assumption, producing [M28] 
Adjusted Fager. We can see from Table 7 that 
this adjustment improves Fager on both datasets. 
 
AM VPCs LVCs Mixed
[M28] Adjusted  
           Fager 
.564 .543 .554 
[M27] Fager .552 .439 .525 
Table 7. Performance of Fager and its adjusted 
version. 
 
The next experiment involves [M14] Third 
Sokal Sneath, which can be shown to be rank 
equivalent to ?b ?c. We further notice that 
frequencies c of particles are normally much 
larger than frequencies b of verbs. Thus, this AM 
runs the risk of ranking VPC candidates based on 
only frequencies of particles. So, it is necessary 
that we scale b and c properly as in 
[M14'] b?? ? ? (1 ) c?? ? . Having scaled the 
constituents properly, we still see that [M14'] by 
itself is not a good measure as it uses only 
constituent frequencies and does not take into 
consideration the co-occurrence frequency of the 
two constituents. This has led us to experiment 
with [MR14'']
PMI
(1 )b c? ?? + ? ?
. The 
denominator of [MR14''] is obtained by 
removing the minus sign from [MR14'] so that it 
can be used as a penalization term. The choice of 
PMI in the numerator is due to the fact that the 
denominator of [MR14''] is in essence similar to 
NF( )? = ( )P x? ?  + (1 ) ( )P y?? ? , which has 
been successfully used to divide PMI in the 
normalized PMI experiment. We heuristically 
tried to simplify [MR14''] to the following AM 
[M30]
log( )
(1 )
ad
b c? ?? + ? ?
. The setting of alpha in 
Table 8 below is taken from the best alpha 
setting obtained the experiment on the 
normalized PMI (Table 5). It can be observed 
from Table 8 that [MR14'''], being 
computationally simpler than normalized PMI, 
performs as well as normalized PMI and better 
than Third Sokal-Sneath over the VPC data set. 
 
AM VPCs LVCs Mixed
PMI / NF( )?  .592 
(? =.8) 
.554 
(? =.48) 
.588 
(? =.77)
[M30]
log( )
(1 )
ad
b c? ?? + ? ?
.600 
(? =.8) 
.484 
(? =.48) 
.588 
(? =.77)
[M14] Third  
            Sokal Sneath  
.565 .453 .546 
Table 8. AP performance of suggested VPCs?  
penalization terms and AMs.  
 
With the same intention and method, we have 
found that while addition of marginal frequencies 
is a good penalization term for VPCs, the 
product of marginal frequencies is more suitable 
for LVCs (rows 1 and 2, Table 9). As with the 
linear combination, the product bc should also be 
weighted accordingly as (1 )b c? ?? . The best alpha 
value is also taken from the normalized PMI 
experiments (Table 5), which is nearly .5. Under 
this setting, this penalization term is exactly the 
denominator of the [M18] Odds Ratio. Table 9 
below show our experiment results in deriving 
the penalization term for LVCs.  
 
37
AM VPCs LVCs Mixed
?b ?c .565 .453 .546 
1/bc .502 .532 .502 
[M18] Odds ratio .443 .567 .456 
Table 9. AP performance of suggested LVCs?  
penalization terms and AMs.  
6 Conclusions 
We have conducted an analysis of the 82 AMs 
assembled in Pecina and Schlesinger (2006) for 
the tasks of English VPC and LVC extraction 
over the Wall Street Journal Penn Treebank data.  
In our work, we have observed that AMs can be 
divided into two classes: ones that do not use 
context (Class I) and ones that do (Class II), and 
find that the latter is not suitable for our VPC and 
LVC detection tasks as the size of our corpus is 
too small to rely on the frequency of candidates? 
contexts. This phenomenon also revealed the 
inappropriateness of hypothesis tests for our 
detection task. We have also introduced the 
novel notion of rank equivalence to MWE 
detection, in which we show that complex AMs 
may be replaced by simpler AMs that yield the 
same average precision performance. 
We further observed that certain modifications 
to some AMs are necessary. First, in the context 
of ranking, we have proposed normalizing scores 
produced by MI and PMI in cases where the 
distributions of the two events are markedly 
different, as is the case for light verbs and 
particles. While our claims are limited to the 
datasets analyzed, they show clear 
improvements: normalized PMI produces better 
performance over our mixed MWE dataset, 
yielding an average precision of 58.8% 
compared to 51.5% when using standard PMI, a 
significant improvement as judged by paired T 
test.  Normalized MI also yields the best 
performance over our LVC dataset with a 
significantly improved AP of 58.3%. 
We also show that marginal frequencies can 
be used to form effective penalization terms. In 
particular, we find that (1 )b c? ?? + ? ? is a good 
penalization term for VPCs, while (1 )b c? ?? is 
suitable for LVCs. Our introduced alpha tuning 
parameter should be set to properly scale the 
values b and c, and should be optimized per 
MWE type. In cases where a common factor is 
applied to different MWE types, max(b, c) is a 
better choice than min(b, c).  In future work, we 
plan to expand our investigations over larger, 
web-based datasets of English, to verify the 
performance gains of our modified AMs. 
Acknowledgement 
This work was partially supported by a National 
Research Foundation grant ?Interactive Media 
Search? (grant # R 252 000 325 279). 
References  
Baldwin, Timothy (2005). The deep lexical 
acquisition of English verb-particle constructions. 
Computer Speech and Language, Special 
Issue on Multiword Expressions, 19(4):398?414. 
Baldwin, Timothy and Villavicencio, Aline (2002). 
Extracting the unextractable: A case study on verb-
particles. In Proceedings of the 6th Conference 
on Natural Language Learning (CoNLL-2002), 
pages 98?104, Taipei, Taiwan.  
Daille, B?atrice (1994). Approche mixte pour 
l'extraction automatique de terminologie: 
statistiques lexicales et filtres linguistiques. 
PhD thesis, Universit? Paris 7. 
Evert, Stefan (2004). Online repository of association 
measures http://www.collocations.de/, a 
companion to The Statistics of Word 
Cooccurrences: Word Pairs and Collocations. 
Ph.D. dissertation, University of Stuttgart. 
Evert, Stefan and Krenn, Brigitte (2001) Methods for 
qualitative evaluation of lexical association 
measures. In Proceedings of the 39th Annual 
Meeting of the Association of Computational 
Linguistics, pages 188-915, Toulouse, France. 
Katz, Graham and Giesbrecht, Eugenie (2006). 
Automatic identification of non-compositional 
multi-word expressions using latent semantic 
analysis. In Proceedings of the ACL Workshop 
on Multiword Expressions: Identifying and 
Exploiting Underlying Properties, pages 12-19, 
Sydney, Australia. 
Krenn, Brigitte and Evert, Stefan (2001). Can we do 
better than frequency? A case study on extracting 
PP-verb collocations. In Proceedings of the 
ACL/EACL 2001 Workshop on the 
Computational Extraction, Analysis and 
Exploitation of Collocations, pages 39?46, 
Toulouse, France. 
Manning D. Christopher and Schutze, && Hinrich  
(1999). Foundations of Statistical Natural 
Language Processing. The MIT Press, Cambridge, 
Massachusetts.  
Pearce, Darren (2002). A comparative evaluation of 
collocation extraction techniques. In Proc. of the 
38
3rd International Conference on Language 
Resources and Evaluation (LREC 2002), Las 
Palmas, pages 1530-1536, Canary Islands.  
Pecina, Pavel and Schlesinger, Pavel (2006). 
Combining association measures for collocation 
extraction. In Proceedings of the 21th 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics 
(COLING/ACL 2006), pages 651-658, Sydney, 
Australia. 
Quirk Randolph, Greenbaum Sidney, Leech Geoffrey 
and Svartvik Jan (1985). A Comprehensive 
Grammar of the English Language. Longman, 
London, UK.  
Ramisch Carlos, Schreiner Paulo, Idiart Marco and 
Villavicencio Aline (2008). An Evaluation of 
Methods for the extraction of Multiword 
Expressions. In Proceedings of the LREC-2008 
Workshop on Multiword Expressions: 
Towards a Shared Task for Multiword 
Expressions, pages 50-53, Marrakech, Morocco.  
Smadja, Frank (1993). Retrieving collocations from 
text: Xtract. Computational Linguistics 19(1): 
143?77. 
Tan, Y. Fan, Kan M. Yen and Cui, Hang (2006). 
Extending corpus-based identification of light verb 
constructions using a supervised learning 
framework. In Proceedings of the EACL 2006 
Workshop on Multi-word-expressions in a 
multilingual context, pages 49?56, Trento, Italy. 
Zhai, Chengxiang (1997). Exploiting context to 
identify lexical atoms ? A statistical view of 
linguistic context. In International and 
Interdisciplinary Conference on Modelling 
and Using Context (CONTEXT-97), pages 119-
129, Rio de Janeiro, Brazil.  
Appendix A. List of particles used in 
identifying verb particle constructions. 
about,  aback,  aboard,  above,  abroad,  across,  adrift,  
ahead,  along,  apart,  around,  aside,  astray,  away,  
back,  backward,  backwards,  behind, by, down,  
forth,  forward, forwards, in,  into,  off,  on,  out,  over,   
past,  round,  through, to, together, under, up,  upon,  
without. 
 
39
Automatic Interpretation of Noun Compounds
Using WordNet Similarity
Su Nam Kim1,2 and Timothy Baldwin2,3
1 Computer Science, University of Illinois, Chicago, IL 60607 USA
sunamkim@gmail.com
2 Computer Science and Software Engineering,
University of Melbourne, Victoria 3010 Australia
3 NICTA Victoria Lab, University of Melbourne, Victoria 3010 Australia
tim@csse.unimelb.edu.au
Abstract. The paper introduces a method for interpreting novel noun compounds
with semantic relations. The method is built around word similarity with pre-
tagged noun compounds, based on WordNet::Similarity. Over 1,088
training instances and 1,081 test instances from the Wall Street Journal in the
Penn Treebank, the proposed method was able to correctly classify 53.3% of the
test noun compounds. We also investigated the relative contribution of the modi-
fier and the head noun in noun compounds of different semantic types.
1 Introduction
A noun compound (NC) is an ?N made up of two or more nouns, such as golf club or
paper submission; we will refer to the rightmost noun as the head noun and the re-
mainder of nouns in the NC as modifiers. The interpretation of noun compounds is a
well-researched area in natural language processing, and has been applied in applica-
tions such as question answering and machine translation [1,2,3]. Three basic properties
make the interpretation of NCs difficult [4]: (1) the compounding process is extremely
productive; (2) the semantic relationship between head noun and modifier in the noun
compounds is implicit; and (3) the interpretation can be influenced by contextual and
pragmatic factors.
In this paper, we are interested in recognizing the semantic relationship between the
head noun and modifier(s) of noun compounds. We introduce a method based on word
similarity between the component nouns in an unseen test instance NC and annotated
training instance NCs. Due to its simplicity, our method is able to interpret NCs with
significantly reduced cost. We also investigate the relative contribution of the head noun
and modifier in determining the semantic relation.
For the purposes of this paper, we focus exclusively on binary NCs, that is NCs
made up of two nouns. This is partly an empirical decision, in that the majority of
NCs occurring in unrestricted text are binary,1 and also partly due to there being ex-
isting methods for disambiguating the syntactic structure of higher-arity NCs, effec-
tively decomposing them into multiple binary NCs [3]. Note also that in this paper, we
1 We estimate that 88.4% of NCs in the Wall Street Journal section of the Penn Treebank and
90.6% of NCs in the British National Corpus are binary.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 945?956, 2005.
c? Springer-Verlag Berlin Heidelberg 2005
946 S.N. Kim and T. Baldwin
distinguish semantic relations from semantic roles. The semantic relation in an NC is
the underlying relation between the head noun and its modifier, whereas its semantic
role is an indication of its relation to the governing verb and other constituents in the
sentence context.
There is a significant body of closely-related research on interpreting semantic rela-
tions in NCs which relies on hand-written rules. [5] examined the problem of interpre-
tation of NCs and constructed a set of hand-written rules. [6] automatically extracted
semantic information from an on-line dictionary and manipulated a set of hand-written
rules to assign weights to semantic relations. Recently, there has been work on the auto-
matic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is
based on a simplifying assumption as to the scope of semantic relations or the domain
of interpretation, making it difficult to compare the performance of NC interpretation
in a broader context.
In the remainder of the paper, we detail the motivation for our work (Section 2),
introduce the WordNet::Similarity system which we use to calculate word sim-
ilarity (Section 3), outline the set of semantic relations used (Section 4), detail how we
collected the data (Section 5), introduce the proposed method (Section 6), and describe
experimental results (Section 7).
2 Motivation
Most work related to interpreting NCs depends on hand-coded rules [5]. The first at-
tempt at automatic interpretation by [6] showed that it was possible to successfully
interpret NCs. However, the system involved costly hand-written rules involving man-
ual intervention. [9] estimated the amount of world knowledge required to interpret
NCs and claimed that the high cost of data acquisition offsets the benefits of automatic
interpretation of NCs.
Recent work [4,7,8] has investigated methods for interpreting NCs automatically
with minimal human effort. [10] introduced a semi-automatic method for recogniz-
ing noun?modifier relations. [4] examined nominalizations (a proper subset of NCs) in
terms of whether the modifier is a subject or object of the verb the head noun is derived
from (e.g. language understanding = understand language). [7] assigned hierarchical
tags to nouns in medical texts and classified them according to their semantic relations
using neural networks. [8] used the word senses of nouns to classify the semantic re-
lations of NCs. However, in all this work, there has been some underlying simplifying
assumption, in terms of the domain or range of interpretations an NC can occur with,
leading to questions of scalability and portability to novel domains/NC types.
In this paper, we introduce a method which uses word similarity based on WordNet.
Word similarity has been used previously in various lexical semantic tasks, including
word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a con-
text space can be used to disambiguate word senses. [12] measured the relatedness of
concepts using similarity based on WordNet. [13] examined the task of disambiguating
noun groupings with respect to word senses using similarity between nouns in NCs.
Our research uses similarities between nouns in the training and test data to interpret
the semantic relations of novel NCs.
Automatic Interpretation of Noun Compounds 947
apple juice morning milk
chocolate milk
MATERIAL TIME
s12s11 s21 s22
Fig. 1. Similarity between test NC chocolate milk and training NCs apple juice and morning milk
Table 1. WordNet-based similarities for component nouns in the training and test data
Training noun Test noun Sij
t1 apple chocolate 0.71
t2 juice milk 0.83
t1 morning chocolate 0.27
t2 milk milk 1.00
Figure 1 shows the correspondences between two training NCs, apple juice and
morning milk, and a test NC, chocolate milk; Table 1 lists the noun pairings and noun?
noun similarities based on WordNet. Each training noun is a component noun from
the training data, each test noun is a component noun in the input, and Sij provides
a measure of the noun?noun similarity in training and test, where t1 is the modifier
and t2 is the head noun in the NC in question. The similarities in Table 1 were com-
puted by the WUP method [14] as implemented in WordNet::Similarity (see
Section 3).
The simple product of the individual similarities (of each modifier and head noun,
respectively) gives the similarity of the NC pairing. For example, the similarity between
chocolate milk and apple juice is 0.60, while that between chocolate milk and morning
milk is 0.27. Note that although milk in the input NC also occurs in a training exemplar,
the semantic relations for the individual NCs differ. That is, while apple juice is juice
made from apples (MATERIAL), morning milk is milk served in the morning (TIME).
By comparing the similarity of both elements of the input NC, we are able to arrive
at the conclusion that chocolate milk is more closely related to chocolate milk, which
provides the correct semantic relation of MATERIAL (i.e. milk made from/flavored with
chocolate). Unlike word sense disambiguation systems, our method does not need to
determine the particular sense in which each noun is used. The next example (Table 2)
shows how our method interprets NCs containing ambiguous nouns correctly.
One potential pitfall when dealing with WordNet is the high level of polysemy for
many lexemes. We analyze the effects of polysemy with respect to interest. Assume that
we have the two NCs personal interest (POSSESSION) and bank interest (CAUSE/TOPIC)
in the training data. Both contain the noun interest, with the meaning of a state of cu-
948 S.N. Kim and T. Baldwin
Table 2. The effects of polysemy on the similarities between nouns in the training and test data
Training noun Test noun Sij
t1 personal loan 0.32
t2 interest rate 0.84
t1 bank loan 0.75
t2 interest rate 0.84
Table 3. Varying contribution of the head noun and modifier in predicting the semantic relation
Relative contribution of modifier/head noun Relation Example
modifier < head noun PROPERTY elephant seal
modifier = head noun EQUATIVE composer arranger
modifier > head noun TIME morning class
riosity or concern about something in personal interest, and an excess or bonus beyond
what is expected or due in bank interest. Given the test NC loan rate, we would get the
desired result of bank interest being the training instance of highest similarity, leading
to loan rate being classified with the semantic relation of CAUSE/TOPIC. The similar-
ity between the head nouns interest and rate for each pairing of training and test NC is
identical, as the proposed method makes no attempt to disambiguate the sense of a noun
in each NC context, and instead aggregates the overall word-to-word similarity across
the different sense pairings. The determining factor is therefore the similarity between
the different modifier pairings, and the fact that bank is more similar to loan than is the
case for personal.
We also investigate the weight of the head noun and the modifier in determining
overall similarity. We expect for different relations, the weight of the head noun and
the modifier will be different. In the relation EQUATIVE, e.g., we would expect the
significance of the head noun to be the same as that of the modifier. In relations such as
PROPERTY, on the other hand, we would expect the head noun to play a more important
role than the modifier. Conversely, with relations such as TIME, we would expect the
modifier to be more important, as detailed in Table 3.
3 WordNet::Similarity
WordNet::Similarity2 [12] is an open source software package developed at
the University of Minnesota. It allows the user to measure the semantic similarity or
relatedness between a pair of concepts (or word senses), and by extension, between a
pair of words. The system provides six measures of similarity and three measures of
relatedness based on the WordNet lexical database [15]. The measures of similarity are
based on analysis of the WordNet isa hierarchy.
2 www.d.umn.edu/?tpederse/similarity.html
Automatic Interpretation of Noun Compounds 949
The measures of similarity are divided into two groups: path-based and information
content-based. We chose four of the similarity measures in WordNet::Similarity
for our experiments: WUP and LCH as path-based similarity measures, and JCN and
LIN as information content-based similarity measures. LCH finds the shortest path be-
tween nouns [16]; WUP finds the path length to the root node from the least com-
mon subsumer (LCS) of the two word senses that is the most specific word sense
they share as an ancestor [14]; JCN subtracts the information content of the LCS
from the sum [17]; and LIN scales the information content of the LCS relative to the
sum [18].
In WordNet::Similarity, relatedness goes beyond concepts being similar to
each other. That is, WordNet provides additional (non-hierarchical) relations such as
has-part and made-of. It supports our idea of interpretation of NCs by similarity.
However, as [19] point out, information on relatedness has not been developed as ac-
tively as conceptual similarity. Besides, the speed of simulating these relatedness effects
is too slow to use in practice. Hence, we did not use any of the relatedness measures in
this paper.
4 Semantic Relations
A semantic relation in the context of NC interpretation is the relation between the mod-
ifier and the head noun. For instance, family car relates to POSSESSION whereas sports
car relates to PURPOSE. [20] defined complex nominals as expressions that have a
head noun preceded by one or more modifying nouns or denominal adjectives, and
offered nine semantic labels after removing opaque compounds and adding nominal
non-predicating adjectives. [5] produced a diverse set of NC interpretations. Other re-
searchers have identified alternate sets of semantic relations, or conversely cast doubts
on the possibility of devising an all-purpose system of NC interpretations [21]. For our
work, we do not intend to create a new set of semantic relations. Based on our data,
we chose a pre-existing set of semantic relations that had previously been used for au-
tomatic (or semi-automatic) NC interpretation, namely the 20-member classification of
[10] (see Appendix). Other notable classifications include that of [6] which contains 13
relations based on WH questions, making it ideally suited to question answering appli-
cations. However, some relations such as TOPIC are absent. [7] proposed 38 relations for
the medical domain. Such relations are too highly specialized to this domain, and not
suitable for more general applications. [8] defined 35 semantic relations for complex
nominals and adjective phrases.
5 Data Collection
We retrieved binary NCs from the Wall Street Journal component of the Penn treebank.
We excluded proper nouns since WordNet does not contain even high-frequency proper
nouns such as Honda. We also excluded binary NCs that are part of larger NCs. In
tagging the semantic relations of noun compounds, we hired two annotators: two com-
puter science Ph.D students. In many cases, even human annotators disagree on the tag
allocation. For NCs containing more than one semantic relation, the annotators were
950 S.N. Kim and T. Baldwin
judged to have agreed is there was overlap in at least one of the relations specified by
them for a given NC. The initial agreement for the two annotators was 52.31%. From
the disagreement of tagged relations, we observed that decisions between SOURCE and
CAUSE, PURPOSE and TOPIC, and OBJECT and TOPIC frequently have lower agree-
ment. For the NCs where there was no agreement, the annotators decided on a set of
relations through consultation. The distribution of semantic relations is shown in the
Appendix. Overall, we used 1,088 NCs for the training data and 1,081 NCs for the
test data.
6 Method
Figure 2 shows how to compute the similarity between the ith NC in the test data
and jth NC in the training data. We calculate similarities for the component nouns of
the ith NC in the test data with all NCs in the training data. As a result, the modifier
and head noun in the ith test NC are each associated with a total of m similarities,
where m is the number of NCs in the training data. The second step is to multiply
the similarities of the modifier and head noun for all NCs in the training data; we ex-
periment with two methods for calculating the combined similarity. The third step is
to choose the NC in the training data which is most similar to the test instance, and
tag the test instance according to the semantic relation associated with that training
instance.
Formally, SA is the similarity between NCs (Ni,1, Ni,2) and (Bj,1, Bj,2):
SA((Ni,1, Ni,2), (Bj,1, Bj,2)) =
((?S1 + S1) ? ((1 ? ?)S2 + S2))
2
(1)
where S1 is the modifier similarity (i.e. S(Ni,1, Bj1)) and S2 is head noun similarity
(i.e. S(Ni,2, Bj2)); ? ? [0, 1] is a weighting factor.
SB is an analogous similarity function, based on the F-score:
Bj1   Bj2
Bm1 Bm2
B31  B32
B21  B22
B11  B12
Relation2
Relation3
Relation19
Relation_k
Relation3
Ni1   Ni2
Nn1  Nn2
S(Ni1,B11)
S(Ni1,B21)
S(Ni1,Bj1)
S(Ni1,Bm1)
S(Ni2,B12)
S(Ni2,B22)
S(Ni2,Bj2)
S(Ni2,Bm2)
RELATIONNN
N11  N12
N21  N22 Similarity in detail
Fig. 2. Similarity between the ith NC in the test data and jth NC in the training data
Automatic Interpretation of Noun Compounds 951
SB((Ni,1, Ni,2), B(j,1, Bj,2)) = 2 ?
(S1 + ?S1) ? (S2 + (1 ? ?)S2)
(S1 + ?S1) + (S2 + (1 ? ?S2)) (2)
The semantic relation is determined by rel:
rel(Ni,1, Ni,2) = rel(Bm,1, Bm,2) (3)
where m = argmax
j
S((Ni,1, Ni,2), (Bj,1, Bj,2))
7 Experimental Results
7.1 Automatic Tagging Using Similarity
In our first experiment, we tag the test NCs with semantic relations using four different
measures of noun similarity, assuming for the time being that the contribution of the
modifier and head noun is equal (i.e. ? = 0.5). The baseline for this experiment is a
majority-class classifier, in which all NCs are tagged according to the TOPIC class.
Table 4. Accuracy of NC interpretation for the different WordNet-based similarity measures
Basis Method SA SB
majority class Baseline 465 (43.0%) 465 (43.0%)
path-based WUP 576 (53.3%) 557 (51.5%)
path-based LCH 572 (52.9%) 565 (52.3%)
information content-based JCN 505 (46.7%) 470 (43.5%)
information content-based LIN 512 (47.4%) 455 (42.1%)
human annotation Inter-annotator agreement 565 (52.3%) 565 (52.3%)
Table 4 shows that WUP, using the SA multiplicative method of combination, pro-
vides the highest NC interpretation accuracy, significantly above the majority-class
baseline. It is particularly encouraging to see that WUP performs at or above the level
of inter-annotator agreement (52.3%), which could be construed as a theoretical upper
bound for the task as defined here. Using the F-score measure of similarity, LCH has
nearly the same performance as WUP. Among the four measures of similarity used in
this first experiment, the path-based similarity measures have higher performance than
the information content-based methods over both similarity combination methods.
Compared to prior work on the automatic interpretation of NCs, our method
achieves relatively good results. [7] achieved about 60% performance over the medical
domain. [8] used a word sense disambiguation system to achieve around 43% accuracy
interpreting NCs in the open domain. Our accuracy of 53% compares favourably to both
of these sets of results, given that we are operating over open domain data.
7.2 Relative Contribution of Modifier and Head Noun
In the second experiment, we investigated the relative impact of the modifier and head
noun in determining the overall similarity of the NC. While tagging the NCs, we got
952 S.N. Kim and T. Baldwin
(accuracy %)
alpha value
 48.5
 49
 49.5
 50
 50.5
 51
 51.5
 52
0.0 0.2 0.4 0.6 0.8 1.0
% w/ different weight
Fig. 3. Classifier accuracy at different ? values
be
ne
fic
ia
ry
a g
e n
t
ca
u
se
co
n
ta
in
er
co
n
te
nt
de
sti
na
tio
n
eq
ua
tiv
e
in
str
um
en
t
lo
ca
te
d
lo
ca
tio
n
m
at
er
ia
l
o
bje
ct
po
ss
es
so
r
pr
od
uc
t
pr
op
er
ty
re
su
lt
pu
rp
os
e
so
u
rc
e
tim
e
to
pi
c
(accuracy %)
(relation) 0
 20
 40
 60
 80
 100
 0
?wup 5:5
?wup 8:2
?wup 2:8
Fig. 4. Classification accuracy for each semantic relation at different ? values
a sense of modifiers and head nouns having variable impact on the determination of
the overall NC semantic relation. For this test, we used the WUP method based on our
results from above and also because it operates over the scale [0, 1], removing any need
for normalization. In this experiment, modifiers and head nouns were assigned weights
(? in Equations 1 and 2) in the range 0.0, 0.1, ...1.0.
Figure 3 shows the relative contribution of the modifier and head noun in the over-
all NC interpretation process. Interestingly, the head noun seems to be a more reliable
predictor of the overall NC interpretation than the modifier, and yet the best accuracy
is achieved when each noun makes an equal contribution to the overall interpretation
(i.e. ? = 0.5). Thus suggests that, despite any localized biases for individual NC inter-
pretation types, the modifier and head noun have an equal impact on NC interpretation
overall.
Automatic Interpretation of Noun Compounds 953
Bm1 Bm2
Bn1 Bn2
Bn1 Bn2
Bm1 Bm2Ni1 Ni2 Correct
Answer
0.82
0.79
Ni1 Ni2
0.45 Incorrect
Answer
Nj1 Nj2
Nj1 Nj2
0.79
0.45
Correct
Answer
ith step (i+1)th step
Fig. 5. Accumulating correctly tagged data
Figure 4 shows a breakdown of accuracy across the different semantic relation types
for different weights. In Figure 4, we have shown only the weights 0.2, 0.5 and 0.8 (to
show the general effect of variation in ?). The dashed line shows the performance when
the weight of modifiers and head nouns is the same (? = 0.5). The ? symbol shows the
results of modifier-biased interpretation (? = 0.8) and the + symbol shows the results
of head noun-biased interpretation (? = 0.2). From Figure 4, we can see that for rela-
tions such as CAUSE and INSTRUMENT, the modifier plays a more important role in the
determination of the semantic relation of the NC. On the other hand, for the CONTENT
and PROPERTY relations, the head noun contributes more to NC interpretation. Unex-
pectedly, for EQUATIVE, the head noun contributes more than the modifier, although
only 9 examples were tagged with EQUATIVE, such that the result shown may not be
very representative of the general behavior.
8 Discussion
We have presented a method for interpreting the semantic relations of novel NCs using
word similarity. We achieved about 53% interpretation accuracy using a path-based
measure of similarity. Since our system was tested over raw test data from a general
domain, we demonstrated that word similarity has surprising potential for interpreting
the semantic relations of NCs. We also investigated using different weights for the head
noun and modifier to find out how much the modifier and head noun contributes in NC
interpretation and found that, with the exception of some isolated semantic relations,
their relative contribution is equal.
Our method has advantages such its relative simplicity and ability to run over small
amounts of training data, but there are also a few weaknesses. The main bottleneck is
the availability of training data to use in classifying test instances. We suggest that we
could use a bootstrap method to overcome this problem: in each step of classification,
NCs which are highly similar to training instances, as determined by some threshold on
similarity, are added to the training data to use in the next iteration of classification. One
way to arrive at such a threshold is to analyze the relative proportion of correctly- and
incorrectly-classified instances at different similarity levels, through cross-validation
over the training data. We generate such a curve for the test data, as detailed in Fig-
ure 6.
If we were to use the crossover point (similarity ? 0.57), we would clearly ?infect?
the training data with a significant number of misclassified instances, namely 30.69%
of the new training instances; this would have an unpredictable impact on classifica-
tion performance. On the other hand, if we were to select a higher threshold based on
a higher estimated proportion of correctly-classified instances (e.g. 70%), the relative
954 S.N. Kim and T. Baldwin
(accuracy %)
(similarity)
Error
Similarity=0.57
THRESHOLD
(a) error rate with similarity 0.57
 0
 20
 40
 60
 80
 100
 0  0.2  0.4  0.6  0.8  1
?correct.percent?
?incorrect.percent?
Fig. 6. The relative proportion of correctly- and incorrectly-classified NCs at different similarity
values, and the estimated impact of threshold-based bootstrapping
increase in training examples would be slight, and there would be little hope for much
impact on the overall classifier accuracy. Clearly, therefore, there is a trade-off here be-
tween how much training data we wish to acquire automatically and whether this will
impact negatively or positively on classification performance. We leave investigation
of this trade-off as an item for future research. Interestingly, in Figure 6 the propor-
tion of misclassified examples is monotonically decreasing, providing evidence for the
soundness of the proposed similarity-based model.
In the first experiment (where the weight of the modifier and head noun was the
same), we observed that some of the test NCs matched with several training NCs with
high similarity. However, since we chose only the NC with the highest similarity, we
ignored any insight other closely-matching training NCs may have provided into the
semantics of the test NC. One possible workaround here would be to employ a voting
strategy, for example, in taking the k most-similar training instances and determin-
ing the majority class amongst them. Once again, we leave this as an item for future
research.
Acknowledgements
We would like to express our thanks to Bharaneedharan Rathnasabapathy for helping
to tag the noun compound semantic relations, and the anonymous reviewers for their
comments and suggestions.
References
1. Cao, Y., Li, H.: Base noun phrase translation using web data and the em algorithm. In:
COLING2002. (2002)
2. Baldwin, T., Tanaka, T.: Translation by machine of compound nominals: Getting it right. In:
ACL2004-MWE, Barcelona, Spain (2004) 24?31
Automatic Interpretation of Noun Compounds 955
3. Lauer, M.: Designing Statistical Language Learners: Experiments on Noun Compounds.
PhD thesis, Macquarie University (1995)
4. Lapata, M.: The disambiguation of nominalizations. Comput. Linguist. 28 (2002) 357?388
5. Finin, T.W.: The semantic interpretation of compound nominals. PhD thesis, University of
Illinois, Urbana, Illinois, USA (1980)
6. Vanderwende, L.: Algorithm for automatic interpretation of noun sequences. In: Proceedings
of the 15th conference on Computational linguistics. (1994) 782?788
7. Rosario, B., Marti, H.: Classifying the semantic relations in noun compounds via a domain-
specific lexical hierarchy. In: Proceedings of the 2001 Conference on Empirical Methods in
Natural Language Processing. (2001) 82?90
8. Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., Girju, R.: Models for the semantic
classification of noun phrases. HLT-NAACL 2004: Workshop on Computational Lexical
Semantics (2004) 60?67
9. Fan, J., Barker, K., Porter, B.W.: The knowledge required to interpret noun com-
pounds. In: Seventh International Joint Conference on Artificial Intelligence. (2003)
1483?1485
10. Barker, K., Szpakowicz, S.: Semi-automatic recognition of noun modifier relationships. In:
Proceedings of the 17th international conference on Computational linguistics. (1998) 96?
102
11. Artiles, J., Penas, A., Verdejo, F.: Word sense disambiguation based on term to term simi-
larity in a context space. In: Senseval-3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text. (2004) 58?63
12. Patwardhan, S., Banerjee, S., Pedersen, T.: Using measures of semantic relatedness for word
sense disambiguation. In: Proceedings of the Fourth International Conference on Intelligent
Text Processing and Computational Linguistics. (2003)
13. Resnik, P.: Disambiguating noun groupings with respect to wordnet senses. In: Proceedings
of the 3rd Workship on Very Large Corpus. (1995) 77?98
14. Wu, Z., Palmer, M.: Verb semantics and lexical selection. In: 32nd. Annual Meeting of the
Association for Computational Linguistics. (1994) 133 ?138
15. Fellbaum, C., ed.: WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA
(1998)
16. Leacock, C., Chodorow, N.: Combining local context and wordnet similarity for word sense
identification. [15]
17. Jiang, J., Conrath, D.: Semantic similarity based on corpus statistics and lexical taxon-
omy. In: Proceedings on International Conference on Research in Computational Linguistics.
(1998) 19?33
18. Lin, D.: An information-theoretic definition of similarity. In: Proceedings of the International
Conference on Machine Learning. (1998)
19. Banerjee, S., Pedersen, T.: Extended gloss overlaps as a measure of semantic relatedness.
In: Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence.
(2003) 805?810
20. Levi, J.: The syntax and semantics of complex nominals. In: New York:Academic Press.
(1979)
21. Downing, P.: On the creation and use of English compound nouns. Language 53 (1977)
810?42
956 S.N. Kim and T. Baldwin
Appendix
Table 5. The Semantic Relations in Noun Compounds (N1 = modifier, N2 = head noun)
Relation Definition Example # of test/training
AGENT N2 is performed by N1 student protest, band concert 10(2)/5
BENEFICIARY N1 benefits from N2 student price, charitable compound 10(1)/7(1)
CAUSE N1 causes N2 printer tray, flood water 54(10)/74(11)
CONTAINER N1 contains N2 exam anxiety 13(6)/19(5)
CONTENT N1 is contained in N2 paper tray, eviction notice 40(5)/34(7)
DESTINATION N1 is destination of N2 game bus, exit route 2(1)/2
EQUATIVE N1 is also head composer arranger, player coach 9/17(3)
INSTRUMENT N1 is used in N2 electron microscope, diesel engine 6/11(2)
LOCATED N1 is located at N2 building site, home town 12(2)/16(4)
LOCATION N1 is the location of N2 lab printer, desert storm 29(10)/24(5)
MATERIAL N2 is made of N1 carbon deposit, gingerbread man 12(1)/15(2)
OBJECT N1 is acted on by N2 engine repair, horse doctor 88(16)/88(21)
POSSESSOR N1 has N2 student loan, company car 32(3)/22(4)
PRODUCT N1 is a product of N2 automobile factory, light bulb 27(1)/32(9)
PROPERTY N2 is N1 elephant seal 76(5)/85(7)
PURPOSE N2 is meant for N1 concert hall, soup pot 160(23)/160(23)
RESULT N1 is a result of N2 storm cloud, cold virus 7(4)/8(1)
SOURCE N1 is the source of N2 chest pain, north wind 86(21)/99(18)
TIME N1 is the time of N2 winter semester, morning class 26(2)/19
TOPIC N2 is concerned with N1 computer expert, safety standard 465(51)/446(60)
The 4thcolumn gives us the number of words tagged with the corresponding relation in the
1stcolumn. The numbers within the parenthesis gives us the number of words that are tagged
with multiple relations( i.e. those that are tagged with the relation in the 1stcolumn and other
relations as well). In the training data, 94 NCs have multiple relations and in test data, 81 NCs
have multiple relations.
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 572?580,
Beijing, August 2010
Evaluating N-gram based Evaluation Metrics for
Automatic Keyphrase Extraction
Su Nam Kim, Timothy Baldwin
CSSE
University of Melbourne
sunamkim@gmail.com, tb@ldwin.net
Min-Yen Kan
School of Computing
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
This paper describes a feasibility study
of n-gram-based evaluation metrics for
automatic keyphrase extraction. To ac-
count for near-misses currently ignored
by standard evaluation metrics, we adapt
various evaluation metrics developed for
machine translation and summarization,
and also the R-precision evaluation metric
from keyphrase evaluation. In evaluation,
the R-precision metric is found to achieve
the highest correlation with human anno-
tations. We also provide evidence that
the degree of semantic similarity varies
with the location of the partially-matching
component words.
1 Introduction
Keyphrases are noun phrases (NPs) that are repre-
sentative of the main content of documents. Since
they represent the key topics in documents, ex-
tracting good keyphrases benefits various natu-
ral language processing (NLP) applications such
as summarization, information retrieval (IR) and
question-answering (QA). Keyphrases can also be
used in text summarization as semantic metadata
(Barzilay and Elhadad, 1997; Lawrie et al, 2001;
D?Avanzo and Magnini, 2005). In search engines,
keyphrases supplement full-text indexing and as-
sist users in creating good queries.
In the past, a large body of work on keyphrases
has been carried out as an extraction task, uti-
lizing three types of cohesion: (1) document
cohesion, i.e. cohesion between documents and
keyphrases (Frank et al, 1999; Witten et al, 1999;
Matsuo and Ishizuka, 2004; Medelyan and Wit-
ten, 2006; Nguyen and Kan, 2007; Wan and
Xiao, 2008); (2) keyphrase cohesion, i.e. cohe-
sion among keyphrases (Turney, 2003); and (3)
term cohesion, i.e. cohesion among terms in a
keyphrase (Park et al, 2004).
Despite recent successes in keyphrase extrac-
tion (Frank et al, 1999; Turney, 2003; Park et al,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007), current work is hampered by the in-
flexibility of standard metrics in evaluating differ-
ent approaches. As seen in other fields, e.g. ma-
chine translation (MT) and multi-document sum-
marization, the advent of standardized automatic
evaluation metrics, combined with standardized
datasets, has enabled easy comparison of sys-
tems and catalyzed the respective research ar-
eas. Traditionally, the evaluation of automatic
keyphrase extraction has relied on the number
of exact matches in author-assigned keyphrases
and reader-assigned keyphrases. The main prob-
lem with this approach is that even small vari-
ants in the keyphrases are not given any credit.
For example, given the gold-standard keyphrase
effective grid computing algorithm, grid com-
puting algorithm is a plausible keyphrase candi-
date and should be scored appropriately, rather
than being naively evaluated as wrong. Addition-
ally, author-assigned keyphrases and even reader-
assigned keyphrases often have their own prob-
lems in this type of evaluation (Medelyan andWit-
ten, 2006). For example, some keyphrases are
often partly or wholly subsumed by other can-
didates or may not even occur in the document.
Therefore, counting the exactly-matching candi-
dates has been shown to be suboptimal (Jarmasz
572
and Barriere, 2004).
Our goal in this paper is to evaluate the relia-
bility of automatic evaluation metrics that better
account for near-misses. Prior research based on
semantic similarity (Jarmasz and Barriere, 2004;
Mihalcea and Tarau, 2004; Medelyan and Wit-
ten, 2006) has taken the approach of using ex-
ternal resources such as large corpora, Wikipedia
or manually-curated index words. While we ac-
knowledge that these methods can help address
the near-miss problem, they are impractical due
to the effort required to compile the requisite re-
sources for each individual evaluation exercise,
and furthermore, the resources tend to be domain-
specific. In order to design a cheap, practical and
stable keyphrase evaluation metric, our aim is to
properly account for these near-misses without re-
liance on costly external resources.
According to our analysis, the degree of se-
mantic similarity of keyphrase candidates varies
relative to the location of overlap. For exam-
ple, the candidate grid computing algorithm has
higher semantic similarity than computing algo-
rithm with the gold-standard keyphrase effective
grid computing algorithm. Also, computing algo-
rithm is closer than effective grid to the same gold-
standard keyphrase. From these observations, we
infer that n-gram-based evaluation metrics can
be applied to evaluating keyphrase extraction, but
also that candidates with the same relative n-gram
overlap are not necessarily equally good.
Our primary goal is to test the utility of n-gram
based evaluation metrics to the task of keyphrase
extraction evaluation. We test the following eval-
uation metrics: (1) evaluation metrics from MT
and multi-document summarization (BLEU, NIST,
METEOR and ROUGE); and (2) R-precision (Zesch
and Gurevych, 2009), an n-gram-based evalua-
tion metric developed specifically for keyphrase
extraction evaluation which has yet to be evalu-
ated against humans at the extraction task. Sec-
ondarily, we attempt to shed light on the bigger
question of whether it is feasible to expect that
n-gram-based metrics without access to external
resources should be able to capture subtle seman-
tic differences in keyphrase candidates. To this
end, we experimentally verify the impact of lex-
ical overlap of different types on keyphrase sim-
ilarity, and use this as the basis for proposing a
variant of R-precision.
In the next section, we present a brief primer on
keyphrases. We then describe the MT and sum-
marization evaluation metrics trialled in this re-
search, along with R-precision, modified R-precision
and a semantic similarity-based evaluation metric
for keyphrase evaluation (Section 3). In Section 4,
we discuss our gold-standard and candidate ex-
traction method. We compare the evaluation met-
rics with human assigned scores for suitability in
Section 5, before concluding the paper.
2 A Primer on Keyphrases
Keyphrases can be either simplex words (e.g.
query, discovery, or context-awareness)1 or larger
N-bars/noun phrases (e.g. intrusion detection,
mobile ad-hoc network, or quality of service).
The majority of keyphrases are 1?4 words long
(Paukkeri et al, 2008).
Keyphrases are normally composed of nouns
and adjectives, but may occasionally contain ad-
verbs (e.g. dynamically allocated task, or partially
observable Markov decision process) or other
parts of speech. They may also contain hyphens
(e.g. sensor-grouping or multi-agent system) and
apostrophes for possessives (e.g. Bayes? theorem
or agent?s goal).
Keyphrases can optionally incorporate PPs (e.g.
service quality vs. quality of service). A variety of
prepositions can be used (e.g. incentive for coop-
eration, inequality in welfare, agent security via
approximate policy), although the genetive of is
the most common.
Keyphrases can also be coordinated, either as
simple nouns at the top level (e.g. performance
and scalability or group and partition) or within
more complex NPs or between N-bars (e.g. his-
tory of past encounter and transitivity or task and
resource allocation in agent system).
When candidate phrases get too long, abbre-
viations also help to form valid keyphrases (e.g.
computer support collaborative work vs. CSCW,
or partially observable Markov decision process
vs. POMDP).
1All examples in this section are taken from the data set
outlined in Section 4.
573
3 Evaluation Metrics
There have been various evaluation metrics de-
veloped and validated for reliability in fields such
as MT and summarization (Callison-Burch et al,
2009). While n-gram-based metrics don?t cap-
ture systematic alternations in keyphrases, they do
support partial match between keyphrase candi-
dates and the reference keyphrases.
In this section, we first introduce a range of
popular n-gram-based evaluation metrics from
the MT and automatic summarization literature,
which we naively apply to the task of keyphrase
evaluation. We then present R-precision, an n-
gram-based evaluation metric developed specif-
ically for keyphrase evaluation, and propose a
modified version of R-precision which weights n-
grams according to their relative position in the
keyphrase. Finally, we present a semantic similar-
ity method.
3.1 Machine Translation and Summarization
Evaluation Metrics
In this research, we experiment with four popu-
lar n-gram-based metrics from the MT and au-
tomatic summarization fields ? BLEU, METEOR,
NIST and ROUGE. The basic task performed by the
respective evaluation metrics is empirical determi-
nation of how good an approximation is string1 of
string2?, which is not far removed from the re-
quirements of keyphrase evaluation. We briefly
outline each of the methods below.
One subtle property of keyphrase evaluation is
that there is no a priori preference for shorter
keyphrases over longer keyphrases, unlike MT
where shorter strings tend to be preferred. Hence,
we use the longer NP as reference and the shorter
NP as a translation, to avoid the length penalty in
most MT metrics.2
BLEU (Papineni et al, 2002) is an evaluation
metric for measuring the relative similarity be-
tween a candidate translation and a set of ref-
erence translations, based on n-gram composi-
tion. It calculates the number of overlapping n-
grams between the candidate translation and the
2While we don?t present the numbers in this paper, the
results were lower for the MT evaluation metrics without this
reordering of the reference and candidate keyphrases.
set of reference translations. In order to avoid hav-
ing very short translations receive artificially high
scores, BLEU adds a brevity penalty to the scoring
equation.
METEOR (Agarwal and Lavie, 2008) is similar
to BLEU, in that it measures string-level similarity
between the reference and candidate translations.
The difference is that it allows for more match
flexibility, including stem variation and WordNet
synonymy. The basic metric is based on the num-
ber of mapped unigrams found between the two
strings, the total number of unigrams in the trans-
lation, and the total number of unigrams in the ref-
erence.
NIST (Martin and Przybocki, 1999) is once
again similar to BLEU, but integrates a propor-
tional difference in the co-occurrences for all n-
grams while weighting more heavily n-grams that
occur less frequently, according to their informa-
tion value.
ROUGE (Lin and Hovy, 2003) ? and its vari-
ants including ROUGE-N and ROUGE-L ? is simi-
larly based on n-gram overlap between the can-
didate and reference summaries. For example,
ROUGE-N is based on co-occurrence statistics,
using higher-order n-grams (n > 1) to esti-
mate the fluency of summaries. ROUGE-L uses
longest common subsequence (LCS)-based statis-
tics, based on the assumption that the longer the
substring overlap between the two strings, the
greater the similar Saggion et al (2002). ROUGE-
W is a weighted LCS-based statistic that priori-
tizes consecutive LCSes. In this research, we ex-
periment exclusively with the basic ROUGE met-
ric, and unigrams (i.e. ROUGE-1).
3.2 R-precision
In order to analyze near-misses in keyphrase ex-
traction evaluation, Zesch and Gurevych (2009)
proposed R-precision, an n-gram-based evalua-
tion metric for keyphrase evaluation.3 R-precision
contrasts with the majority of previous work on
keyphrase extraction evaluation, which has used
semantic similarity based on external resources
3Zesch and Gurevych?s R-precision has nothing to do with
the information retrieval evaluation metric of the same name,
where P@N is calculated forN equal to the number of rele-
vant documents.
574
(Jarmasz and Barriere, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006). As our inter-
est is in fully automated evaluation metrics which
don?t require external resources and are domain
independent (for maximal reproducibility of re-
sults), we experiment only with R-precision in this
paper.
R-precision is based on the number of overlap-
ping words between a keyphrase and a candi-
date, as well as the length of each. The met-
ric differentiates three types of near-misses: IN-
CLUDE, PARTOF and MORPH. The first two
types are based on an n-gram approach, while
the third relies on lexical variation. As we use
stemming, in line with the majority of previous
work on keyphrase extraction evaluation, we fo-
cus exclusively on the first two cases, namely IN-
CLUDE, and PARTOF. The final score returned
by R-precision is:
number of overlapping word(s)
length of keyphrase/candidate
where the denominator is the longer of the
keyphrase and candidate.
Zesch and Gurevych (2009) evaluated R-
precision over three corpora (Inspec, DUC and SP)
based on 566 non-exact matching candidates. In
order to evaluate the human agreement, they hired
4 human annotators to rate the near-miss candi-
dates, and reported agreements of 80% and 44%
for the INCLUDE and PARTOF types, respec-
tively. They did not, however, perform holistic
evaluation with human scores to verify its relia-
bility in full system evaluation. This is one of our
contributions in this paper.
3.3 Modified R-precision
In this section, we describe a modification to
R-precision which assigns different weights for
component words based on their position in the
keyphrase (unlike R-precision which assigns the
same score for each matching component word).
The head noun generally encodes the core seman-
tics of the keyphrase, and as a very rough heuris-
tic, the further a word is from the head noun,
the less semantic import on the keyphrase it has.
As such, modified R-precision assigns a score to
each component word relative to its position as
CW = 1N?i+1 where N is the number of com-ponent words in the keyphrase and i is the posi-
tion of the component word in the keyphrase (1 =
leftmost word).
For example, AB and BC from ABC would be
scored as 13+ 121
3+
1
2+
1
1
= 511 and
1
2+
1
1
1
3+
1
2+
1
1
= 911 , re-
spectively. Thus, with the keyphrase effective
grid computing algorithm and candidates effec-
tive grid, grid computing and computing algo-
rithm, modified R-precision assigns different scores
for each candidate (computing algorithm > grid
computing > effective grid). In contrast, the orig-
inal R-precision assigns the same score to all can-
didates.
3.4 Semantic Similarity
In Jarmasz and Barriere (2004) and Mihalcea and
Tarau (2004), the authors used a large data set
to compute the semantic similarity of two NPs
to assign partial credits for semantically similar
candidate keyphrases. To simulate these meth-
ods, we adopted the distributional semantic simi-
larity using web documents. That is, we computed
the similarity between a keyphrase and its sub-
string by cosine measure over collected the snip-
pets from Yahoo! BOSS.4 We use the computed
similarity as our score for near-misses.
4 Data
4.1 Data Collection
We constructed a keyphrase extraction dataset us-
ing papers across 4 different categories5 of the
ACM Digital Library.6 In addition to author-
assigned keyphrases provided as part of the ACM
Digital Library, we generated reader-assigned
keyphrases by assigning 250 students 5 papers
each, a list of candidate keyphrases (see below for
details), and standardized instructions on how to
assign keyphrases. It took them an average of 15
minutes to annotate each paper. This is the same
4http://developer.yahoo.com/search/
boss/
5C2.4 (Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial Intelligence ?
Multiagent Systems) and J4 (Social and Behavioral Sciences
? Economics).
6http://portal.acm.org/dl.cfm
575
Author Reader Total
Total 1298/1305 3110/3221 3816/3962
NPs 937 2537 3027
Average 3.85/4.01 12.44/12.88 15.26/15.85
Found 769 2509 2864
Table 1: Details of the keyphrase dataset
(Rule1) NBAR = (NN*|JJ*)?(NN*)
e.g. complexity, effective algorithm,
distributed web-service discovery architecture
(Rule2) NBAR IN NBAR
e.g. quality of service, sensitivity of VOIP traffic,
simplified instantiation of zebroid
Table 2: Regular expressions for candidate selec-
tion
document collection and set of keyphrase annota-
tions as was used in the SemEval 2010 keyphrase
extraction task (Kim et al, 2010).
Table 1 shows the details of the final dataset.
The numbers after the slashes indicate the number
of keyphrases after including alternate keyphrases
based on of -PPs. Despite the reliability of author-
assigned keyphrases discussed in Medelyan and
Witten (2006), many author-assigned keyphrases
and some reader-assigned keyphrases are not
found verbatim in the source documents because:
(1) many of them are substrings of the candidates
or vice versa (about 75% of the total keyphrases
are found in the documents); and (2) our candi-
date selection method does not extract keyphrases
in forms such as coordinated NPs or adverbial
phrases.
4.2 Candidate Selection
During preprocessing, we first converted the
PDF versions of the papers into text using
pdftotext. We then lemmatized and POS
tagged all words using morpha and the Lingua
POS tagger. Next, we applied the regular expres-
sions in Table 2 to extract candidates, based on
Nguyen and Kan (2007). Finally, we selected can-
didates in terms of their frequency: simplex words
with frequency ? 2 and NPs with frequency ? 1.
We observed that for reader-assigned keyphrases,
NPs were often selected regardless of their fre-
quency in the source document. In addition, we
allowed variation in the possessive form, noun
number and abbreviations.
Rule1 detects simplex nouns or N-bars as candi-
dates. Rule2 extracts N-bars with post-modifying
PPs. In Nguyen and Kan (2007), Rule2 was not
used to additionally extract N-bars inside modify-
ing PPs. For example, our rules extract not only
performance of grid computing as a candidate, but
also grid computing. However, we did not extend
the candidate selection rules to cover NPs includ-
ing adverbs (e.g. partially-observable Markov de-
cision process) or conjunctions (e.g. behavioral
evolution and extrapolation), as they are rare.
4.3 Human Assigned Score
We hired four graduate students working in NLP
to assign human scores to substrings in the gold-
standard data. The scores are between 0 and 4
(0 means no semantic overlap between a NP and
its substring, while 4 means semantically indistin-
guishable).
We broke down the candidate?keyphrases pairs
into subtypes, based on where the overlap oc-
curs relative to the keyphrase (e.g. ABCD): (1)
Head: the candidate contains the head noun of
the keyphrase (e.g. CD); (2) First: the candi-
date contains the first word of the keyphrase (e.g.
AB); and (3) Middle: the candidate overlaps with
the keyphrase, but contains neither its first word
nor its head word (e.g. BC). The average human
scores are 1.94 and 2.11 for First and Head, re-
spectively, when the candidate is shorter, while
they are 2.00, 1.89 and 2.15 for First, Middle, and
Head, respectively when the candidate is longer.
Note that we did not have Middle instances with
candidates as the shorter string. The scores are
slightly higher for the keyphrases as substrings
than for the candidates as substrings.
5 Correlation
To check the feasibility of metrics for keyphrase
evaluation, we checked the Spearman rank corre-
lation between the machine-generated score and
the human-assigned score for each keyphrase?
candidate pairing.
As the percentage of annotators who agree on
the exact score is low (i.e. 2 subjects agree ex-
576
Human R-precision BLEU METEOR NIST ROUGE SemanticOrig Mod Similarity
Average
All .4506 .4763 .2840 .3250 .3246 .3366 .3246 .2116
L ? 4 .4510 .5264 .2806 .3242 .3238 .3369 .3240 .2050
L ? 3 .4551 .4834 .2893 .3439 .3437 .3584 .3437 .1980
Majority
All .4603 .4763 .3438 .3407 .3403 .3514 .3404 .2224
L ? 4 .4604 .5264 .3434 .3423 .3421 .3547 .3422 .2168
L ? 3 .4638 .4838 .3547 .3679 .3675 .3820 .3676 .2123
Table 3: Rank correlation between humans and the different evaluation metrics, based on the human
average (top half) and majority (bottom half)
Human R-precision BLEU METEOR NIST ROUGEOrig Mod
LOCATION
First .5508 .5032 .5033 .3844 .3844 .4057 .3844
Middle .5329 .5741 .5988 .4669 .4669 .4055 .4669
Head .3783 .4838 .4838 .3865 .3860 .3780 .3864
COMPLEXITY
Simple .4452 .4715 .2790 .3653 .3445 .3527 .3445
PP .4771 .4814 .1484 .3367 .3122 .3443 .3123
CC .3645 .3810 .3140 .3748 .3446 .3384 .3748
POS AdjN .4616 .4844 .3507 .3147 .3132 .3115 .3133NN .4467 .4586 .2581 .3321 .3321 .3488 .3322
Table 4: Rank correlation between human average judgments and n-gram-based metrics
actly on 55%-70% of instances, 3 subjects agree
exactly on 25%-35% of instances), we require a
method for combining the annotations. We ex-
periment with two combination methods: major-
ity and average. The majority is simply the label
with the majority of annotations associated with
it; in the case of a tie, we break the tie by select-
ing that annotation which is closest to the median.
The average is simply the average score across all
annotators.
5.1 Overall Correlation with Human Scores
Table 3 presents the correlations between the hu-
man scores (acting as an upper bound for the
task), as well as those between human scores
with machine-generated scores. We first present
the overall results, then results over the subset of
keyphrases of length 4 words or less, and also 3
words or less. We present the results for the anno-
tator average and majority in top and bottom half,
respectively, of the table.
To compute the correlation between the hu-
man annotators, we used leave-one-out cross-
validation, holding out one annotator, and com-
paring them to the combination of the remaining
annotators (using either the majority or average
method to combine the remaining annotations).
This was repeated across all annotators, and the
Spearman?s ? was averaged across the annotators.
Overall, we found that R-precision achieved the
highest correlation with humans, above the inter-
annotator correlation in all instances. That is,
based on the evaluation methodology employed,
it is performing slightly above the average level
of a single annotator. The relatively low inter-
annotator correlation is, no doubt, due to the dif-
ficulty of the task, as all of our near-misses have
2 or more terms, and the annotators have to make
very fine-grained, and ultimately subjective, deci-
sions about the true quality of the candidate.
Comparing the n-gram-based methods with the
semantic similarity-based method, the n-gram-
based metrics achieved higher correlations across
the board, with BLEU, METEOR, NIST and ROUGE
all performing remarkably consistently, but well
577
Human R-precision BLEU METEOR NIST ROUGEOrig Mod
LOCATION
First .5642 .5162 .5163 .4032 .4032 .4297 .4032
Middle .5510 .4991 .5320 .4175 .4175 .3653 .4175
Head .4147 .5073 .5074 .4156 .4153 .4042 .4156
COMPLEXITY
Simple .4580 .4869 .3394 .3653 .3651 .3715 .3651
PP .4715 .5068 .3724 .3367 .3367 .3652 .3367
CC .5777 .5513 .3841 .5745 .5571 .5600 .5745
POS AdjN .4501 .4861 .3968 .3266 .3251 .3246 .3252NN .4631 .4733 .3244 .3499 .3499 .3648 .3500
Table 5: Rank correlation between human majority and n-gram-based metrics
below the level of R-precision. Due to the markedly
lower performance of the semantic similarity-
based method, we do not consider it for the re-
mainder of our experiments. A general finding
was that as the length of the keyphrase (L) got
longer, the correlation tended to be higher across
all n-gram-based metrics.
One disappointment at this stage is that the re-
sults for modified R-precision are well below those
of the original, especially over the average of the
human annotators.
5.2 Correlation with Different NP Subtypes
To get a clearer sense of how the different eval-
uation metrics are performing, we broke down
the keyphrases according to three syntactic sub-
classifications: (1) the location of overlap (see
Section 4.3); (2) the complexity of the NP (does
the keyphrase contain a preposition [PP], a con-
junction [CC] or neither a preposition nor a con-
junction [Simple]?); and (3) the word class se-
quence of the keyphrase (is the keyphrase an NN
[NN] or an AdjN sequence [AdjN]?). We present
the results in Tables 4 and Table 4 for the human
average and majority, respectively, presenting re-
sults in boldface when the correlation for a given
method is higher than for that same method in
our holistic evaluation in Table 3 (i.e. .4506 and
.4603, for the average and majority human scores,
respectively).
All methods, including inter-annotator correla-
tion, improve in raw numbers over the subsets
of the data based on overlap location, indicating
that the data was partitioned into more internally-
consistent subsets. Encouragingly, modified R-
precision equalled or bettered the performance of
the original R-precision over each subset of the
data based on overlap location. Where modified
R-precision appears to fall down most noticeably
is over keyphrases including prepositions, as our
assumption about the semantic import based on
linear ordering clearly breaks down in the face of
post-modifying PPs. It is also telling that it does
worse over noun?noun sequences than adjective?
noun sequences. In being agnostic to the effects
of syntax, the original R-precision appears to bene-
fit overall. Another interesting effect is that the
performance of BLEU, METEOR and ROUGE is
notably better over candidates which match with
non-initial and non-final words in the keyphrase.
We conclude from this analysis that keyphrase
scoring should be sensitive to overlap location.
Furthermore, our study also shows that n-gram-
based MT and summarization metrics are sur-
prisingly adept at capturing partial matches in
keyphrases, despite them being much shorter than
the strings they are standardly applied to. More
compellingly, we found that R-precision is the best
overall performer, and that it matches the perfor-
mance of our human annotators across the board.
This is the first research to establish this fact. Our
findings for modified R-precision were more sober-
ing, but its location sensitivity was shown to im-
prove over R-precision for instances of overlap in
the middle or with the head of the keyphrase.
578
6 Conclusion
In this work, we have shown that preexisting n-
gram-based evaluation metrics from MT, summa-
rization and keyphrase extraction evaluation are
able to handle the effects of near-misses, and that
R-precision performs at or above the average level
of a human annotator. We have also shown that
a semantic similarity-based method which uses
web data to model distributional similarity per-
formed below the level of all of the n-gram-based
methods, despite them requiring no external re-
sources (web or otherwise). We proposed a mod-
ification to R-precision based on the location of
match, but found that while it could achieve better
performance over certain classes of keyphrases,
its net effect was to drag the performance of R-
precision down. Other methods were found to be
remarkably consistent across different subtypes of
keyphrase.
Acknowledgements
Many thanks to the anonymous reviewers for their
insightful comments. We wish to acknowledge
the generous funding from National Research
Foundation grant R 252-000-279-325 in support-
ing Min-Yen Kan?s work.
References
Abhaya Agrwal and Alon Lavie. METEOR, M-
BLEU and M-TER: Evaluation Metrics for High-
Correlation with Human Rankings of Machine
Translation Output. In Proceedings of ACL Work-
shop on Statistical Machine Translation. 2008.
Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases. In
Proceedings of BCCSCSI : Advances in Artificial
Intelligence. 2000, pp.96?103.
Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10?17.
Chris Callison-Burch, Philipp Koehn, Christof Monz
and Josh Schroeder. Proceedings of 4th Workshop
on Statistical Machine Translation. 2009.
Ernesto D?Avanzo and Bernado Magnini. A Key-
phrase-Based Approach to Summarization: the
LAKE System at DUC-2005. In Proceedings of
DUC. 2005.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin and Craig G. Nevill-Manning. Domain
Specific Keyphrase Extraction. In Proceedings of
IJCAI. 1999, pp.668?673.
Mario Jarmasz and Caroline Barriere. Using semantic
similarity over Tera-byte corpus, compute the per-
formance of keyphrase extraction. In Proceedings
of CLINE. 2004.
Su Nam Kim, Olena Medelyan, Min-Yen Kan and
Timothy Baldwin. SemEval-2010 Task 5: Auto-
matic Keyphrase Extraction from Scientific Arti-
cles. In Proceedings of SemEval-2: Evaluation Ex-
ercises on Semantic Evaluation. to appear.
Dawn Lawrie, W. Bruce Croft and Arnold Rosenberg.
Finding Topic Words for Hierarchical Summariza-
tion. In Proceedings of SIGIR. 2001, pp. 349?357.
Chin-Yew Lin and Edward H. Hovy. Automatic Eval-
uation of Summaries Using N-gram Co-occurrence
Statistics. In In Proceedings of HLT-NAACL. 2003.
Alvin Martin and Mark Przybocki. The 1999 NIST
Speaker Recognition Evaluation, Using Summed
Two-Channel Telephone Data for Speaker Detec-
tion and Speaker Tracking. In Proceedings of Eu-
roSpeech. 1999.
Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
occurrence Statistical Information. International
Journal on Artificial Intelligence Tools. 2004,
13(1), pp. 157?169.
Olena Medelyan and Ian Witten. Thesaurus based
automatic keyphrase indexing. In Proceedings of
ACM/IEED-CS JCDL. 2006, pp. 296?297.
Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP 2004.
2004, pp. 404?411.
Guido Minnen, John Carroll and Darren Pearce. Ap-
plied morphological processing of English. NLE.
2001, 7(3), pp. 207?223.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientific Publications. In Proceeding
of ICADL. 2007, pp. 317?326.
Sebastian Pado?, Michel Galley, Dan Jurafsky and
Christopher D. Manning. Textual Entailment Fea-
tures for Machine Translation Evaluation. In Pro-
ceedings of ACL Workshop on Statistical Machine
Translation. 2009, pp. 37?41.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
2001, pp. 311?318.
579
Youngja Park, Roy J. Byrd and Branimir Boguraev.
Automatic Glossary Extraction Beyond Terminol-
ogy Identification. In Proceedings of COLING.
2004, pp. 48?55.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti Polla
and Timo Honkela. A Language-Independent Ap-
proach to Keyphrase Extraction and Evaluation. In
Proceedings of COLING. 2008, pp. 83?86.
Horacio Saggion, Dragomir Radev, Simon Teufel,
Wai Lam and Stephanie Strassel. Meta-evaluation
of Summaries in a Cross-lingual Environment us-
ing Content-based Metrics. In Proceedings of COL-
ING. 2002, pp. 1?7.
Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434?
439.
Xiaojun Wan and Jianguo Xiao. CollabRank: to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969?976.
Ian Witten, Gordon Paynter, Eibe Frank, Car Gutwin
and Craig Nevill-Manning. KEA:Practical Auto-
matic Key phrase Extraction. In Proceedings of
ACM conference on Digital libraries. 1999, pp.
254?256.
Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In-
ternational Conference on Recent Advances in Nat-
ural Language Processing. 2009.
580
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 862?871,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Classifying Dialogue Acts in One-on-one Live Chats
Su Nam Kim,? Lawrence Cavedon? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne
? School of Computer Science and IT, RMIT University
sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net
Abstract
We explore the task of automatically classify-
ing dialogue acts in 1-on-1 online chat forums,
an increasingly popular means of providing
customer service. In particular, we investi-
gate the effectiveness of various features and
machine learners for this task. While a sim-
ple bag-of-words approach provides a solid
baseline, we find that adding information from
dialogue structure and inter-utterance depen-
dency provides some increase in performance;
learners that account for sequential dependen-
cies (CRFs) show the best performance. We
report our results from testing using a corpus
of chat dialogues derived from online shop-
ping customer-feedback data.
1 Introduction
Recently, live chats have received attention due to
the growing popularity of chat services and the in-
creasing body of applications. For example, large
organizations are increasingly providing support or
information services through live chat. One advan-
tage of chat-based customer service over conven-
tional telephone-based customer service is that it
becomes possible to semi-automate aspects of the
interaction (e.g. conventional openings or canned
responses to standard questions) without the cus-
tomer being aware of it taking place, something that
is not possible with speech-based dialogue systems
(as synthesised speech is still easily distinguishable
from natural speech). Potentially huge savings can
be made by organisations providing customer help
services if we can increase the degree of automation
of live chat.
Given the increasing impact of live chat services,
there is surprisingly little published computational
linguistic research on the topic. There has been sub-
stantially more work done on dialogue and dialogue
corpora, mostly in spoken dialogue (e.g. Stolcke et
al. (2000)) but also multimodal dialogue systems in
application areas such as telephone support service
(Bangalore et al, 2006) and tutoring systems (Lit-
man and Silliman, 2004). Spoken dialogue analysis
introduces many complications related to the error
inherent in current speech recognition technologies.
As an instance of written dialogue, an advantage of
live chats is that recognition errors are not such an is-
sue, although the nature of language used in chat is
typically ill-formed and turn-taking is complicated
by the semi-asynchronous nature of the interaction
(e.g. Werry (1996)).
In this paper, we investigate the task of automatic
classification of dialogue acts in 1-on-1 live chats,
focusing on ?information delivery? chats since these
are proving increasingly popular as part of enter-
prise customer-service solutions. Our main chal-
lenge is to develop effective features and classifiers
for classifying aspects of 1-on-1 live chat. Much of
the work on analysing dialogue acts in spoken di-
alogues has relied on non-lexical features, such as
prosody and acoustic features (Stolcke et al, 2000;
Julia and Iftekharuddin, 2008; Sridhar et al, 2009),
which are not available for written dialogues. Pre-
vious dialogue-act detection for chat systems has
used bags-of-words (hereafter, BoW) as features
for dialogue-act detection; this simple approach
has shown some promise (e.g. Bangalore et al
(2006), Louwerse and Crossley (2006) and Ivanovic
(2008)). Other features such as keywords/ontologies
(Purver et al, 2005; Forsyth, 2007) and lexical cues
(Ang et al, 2005) have also been used for dialogue
act classification.
862
In this paper, we first re-examine BoW features
for dialogue act classification. As a baseline, we
use the work of Ivanovic (2008), which explored 1-
grams and 2-grams with Boolean values in 1-on-1
live chats in the MSN Online Shopping domain (this
dataset is described in Section 5). Although this
work achieved reasonably high performance (up to
a micro-averaged F-score of around 80%), we be-
lieve that there is still room for improvement using
BoW only. We extend this work by using ideas from
related research such as text categorization (Debole
and Sebastiani, 2003), and explore variants of BoW
based on analysis of live chats, along with feature
weighting. Finally, our main aim is to explore new
features based on dialogue structure and dependen-
cies between utterances1 that can enhance the use of
BoW for dialogue act classification. Our hypothesis
is that, for task-oriented 1-on-1 live chats, the struc-
ture and interactions among utterances are useful in
predicting future dialogue acts: for example, conver-
sations typically start with a greeting, and questions
and answers typically appear as adjacency pairs in
a conversation. Therefore, we propose new features
based on structural and dependency information de-
rived from utterances (Sections 4.2 and 4.3).
2 Related Work
While there has been significant work on classify-
ing dialogue acts, the bulk of this has been for spo-
ken dialogue. Most such work has considered: (1)
defining taxonomies of dialogue acts; (2) discover-
ing useful features for the classification task; and (3)
experimenting with different machine learning tech-
niques. We focus here on (2) and (3); we return to
(1) in Section 3.
For classifying dialogue acts in spoken dialogue,
various features such as dialogue cues, speech char-
acteristics, and n-grams have been proposed. For
example, Samuel et al (1998) utilized the charac-
teristics of spoken dialogues and examined speaker
direction, punctuation marks, cue phrases and n-
grams for classifying spoken dialogues. Jurafsky et
al. (1998) used prosodic, lexical and syntactic fea-
tures for spoken dialogue classification. More re-
cently, Julia and Iftekharuddin (2008) and Sridhar et
1An utterance is the smallest unit to deliver a participant?s
message(s) in a turn.
al. (2009) achieved high performance using acous-
tic and prosodic features. Louwerse and Cross-
ley (2006), on the other hand, used various n-gram
features?which could be adapted to both spoken
and written dialogue?and tested them using the
Map Task Corpus (Anderson et al, 1991). Extend-
ing the discourse model used in previous work, Ban-
galore et al (2006) used n-grams from the previous
1?3 utterances in order to classify dialogue acts for
the target utterance.
There has been substantially less effort on clas-
sifying dialogue acts in written dialogue: Wu et al
(2002) and Forsyth (2007) have used keyword-based
approaches for classifying online chats; Ivanovic
(2008) tested the use of n-gram features for 1-on-1
live chats with MSN Online Shopping assistants.
Various machine learning techniques have been
investigated for the dialogue classification task.
Samuel et al (1998) used transformation-based
learning to classify spoken dialogues, incorporat-
ing Monte Carlo sampling for training efficiency.
Stolcke et al (2000) used Hidden Markov Mod-
els (HMMs) to account for the structure of spo-
ken dialogues, while Wu et al (2002) also used
transformation- and rule-based approaches plus
HMMs for written dialogues. Other researchers
have used Bayesian based approaches, such as
naive Bayes (e.g. (Grau et al, 2004; Forsyth,
2007; Ivanovic, 2008)) and Bayesian networks (e.g.
(Keizer, 2001; Forsyth, 2007)). Maximum entropy
(e.g. (Ivanovic, 2008)), support vector machines
(e.g. (Ivanovic, 2008)), and hidden Markov models
(e.g. (Bui, 2003)) have also all been applied to auto-
matic dialogue act classification.
3 Dialogue Acts
A number of dialogue act taxonomies have been pro-
posed, designed mainly for spoken dialogue. Many
of these use the Dialogue Act Markup in Several
Layers (DAMSL) scheme (Allen and Core, 1997).
DAMSL was originally applied to the TRAINS cor-
pus of (transcribed) spoken task-oriented dialogues,
but various adaptations of it have since been pro-
posed for specific types of dialogue. The Switch-
board corpus (Godfrey et al, 1992) defines 42 types
of dialogue acts from human-to-human telephone
conversations. The HCRCMap Task corpus (Ander-
863
son et al, 1991) defines a set of 128 dialogue acts to
model task-based spoken conversations.
For casual online chat dialogues, Wu et al (2002)
define 15 dialogue act tags based on previously-
defined dialogue act sets (Samuel et al, 1998;
Shriberg et al, 1998; Jurafsky et al, 1998; Stolcke
et al, 2000). Forsyth (2007) defines 15 dialogue acts
for casual online conversations, based on 16 conver-
sations with 10,567 utterances. Ivanovic (2008) pro-
poses 12 dialogue acts based on DAMSL for 1-on-1
online customer service chats.
Ivanovic?s set of dialogue acts for chat dia-
logues has significant overlap with the dialogue act
sets of Wu et al (2002) and Forsyth (2007) (e.g.
GREETING, EMOTION/EXPRESSION, STATEMENT,
QUESTION). In our work, we re-use the set of dia-
logue acts proposed in Ivanovic (2008), due to our
targeting the same task of 1-on-1 IM chats, and in-
deed experimenting over the same dataset. The def-
initions of the dialogue acts are provided in Table 1,
along with examples.
4 Feature Selection
In this section, we describe our initial dialogue-act
classification experiments using simple BoW fea-
tures, and then introduce two groups of new fea-
tures based on structural information and dependen-
cies between utterances.
4.1 Bag-of-Words
n-gram-based BoW features are simple yet effec-
tive for identifying similarities between two utter-
ances, and have been used widely in previous work
on dialogue act classification for online chat di-
alogues (Louwerse and Crossley, 2006; Ivanovic,
2008). However, chats containing large amounts of
noise such as typos and emoticons pose a greater
challenge for simple BoW approaches. On the other
hand, keyword-based features (Forsyth, 2007) have
achieved high performance; however, keyword-
based approaches are more domain-dependent. In
this work, we chose to start with a BoW approach
based on our observation that commercial live chat
services contain relatively less noise; in particular,
the commercial agent tends to use well-formed, for-
mulaic prose.
Previously, Ivanovic (2008) explored Boolean 1-
gram and 2-gram features to classify MSN Online
Shopping live chats, where a user requests assis-
tance in purchasing an item, in response to which the
commercial agent asks the customer questions and
makes suggestions. Ivanovic (2008) achieved solid
performance over this data (around 80% F-score).
While 1-grams performed well (as live chat utter-
ances are generally shorter than, e.g., sentences in
news articles), we expect 2- and 3-grams are needed
to detect formulaic expressions, such as No problem
and You are welcome. We would also expect a pos-
itive effect from combining n-grams due to increas-
ing the coverage of feature words. We thus test 1-,
2- and 3-grams individually, as well as the combi-
nation of 1- and 2-grams together (i.e. 1+2-grams)
and 1-, 2- and 3-grams (i.e. 1+2+3-grams); this re-
sults in five BoW sets. Also, unlike Ivanovic (2008),
we test both raw words and lemmas; we expect the
use of lemmas to perform better than raw words as
our data is less noisy. As the feature weight, in addi-
tion to simple Boolean, we also experiment with TF,
TF?IDF and Information Gain (IG).
4.2 Structural Information
Our motivation for using structural information as
a feature is that the location of an utterance can be
a strong predictor of the dialogue act. That is, dia-
logues are sequenced, comprising turns (i.e. a given
user is sending text), each of which is made up of
one or more messages (i.e. strings sent by the user).
Structured classification methods which make use of
this sequential information have been applied to re-
lated tasks such as tagging semantic labels of key
sentences in biomedical domains (Chung, 2009) and
post labels in web forums (Kim et al, 2010).
Based on the nature of live chats, we observed that
the utterance position in the chat, as well as in a turn,
plays an important role when identifying its dialogue
act. For example, an utterance such as Hello will oc-
cur at the beginning of a chat while an utterance such
as Have a nice day will typically appear at the end.
The position of utterances in a turn can also help
identify the dialogue act; i.e. when there are several
utterances in a turn, utterances are related to each
other, and thus examining the previous utterances in
the same turn can help correctly predict the target
utterance. For example, the greeting (Welcome to ..)
and question (How may I help you?) could occur in
864
Dialogue Act, Definition and Examples
CONVENTIONAL CLOSING: Various ways of ending a conversation e.g. Bye Bye
CONVENTIONAL OPENING: Greeting and other ways of starting a conversation e.g. Hello Customer
DOWNPLAYER: A backwards-linking label often used after THANKS to down play the contribution
e.g. You are welcome, my pleasure
EXPRESSIVE: An acknowledgement of a previous utterance or an indication of the speaker?s mood.
e.g. haha, : ?) wow
NO ANSWER: A backward-linking label in the form of a negative response to a YESNO-QUESTION e.g. no, nope
OPEN QUESTION: A question that cannot be answered with only a yes or no. The answer is usually
some form of explanation or statement. e.g. how do I use the international version?
REQUEST: Used to express a speaker?s desire that the learner do something ? either performing some action
or simply waiting. e.g. Please let me know how I can assist you on MSN Shopping today.
RESPONSE ACK: A backward-linking acknowledgement of the previous utterance. Used to confirm
that the previous utterance was received/accepted. e.g. Sure
STATEMENT: Used for assertions that may state a belief or commit the speaker to doing something
e.g. I am sending you the page which will pop up in a new window on your screen.
THANKS: Conventional thanks e.g. Thank you for contacting us.
YES ANSWER: A backward-linking label in the form of an affirmative response to a YESNO-QUESTION e.g. yes, yeah
YESNO QUESTION: A closed question which can be answered in the affirmative or negative.
e.g. Did you receive the page, Customer?
Table 1: The set of dialogue acts used in this research, taken from Ivanovic (2008)
the same turn. We also noticed that identifying the
utterance author can help classify the dialogue act
(previously used in Ivanovic (2008)).
Based on these observations, we tested the follow-
ing four structural features:
? Author information,
? Relative position in the chat,
? Author + Relative position,
? Author + Turn-relative position among utter-
ances in a given turn.
We illustrate our structural features in Table 2,
which shows an example of a 1-on-1 live chat. The
participants are the agent (A) and customer (C); Uxx
indicates an utterance (U) with ID number xx. This
conversation has 42 utterances in total. The relative
position is calculated by dividing the utterance num-
ber by the total number of utterances in the dialogue;
the turn-relative position is calculated by dividing
the utterance position by the number of utterances
in that turn. For example, for utterance 4 (U4), the
relative position is 442 , while its turn-relative position
is 23 since U4 is the second utterance among U3,4,5
that the customer makes in a single turn.
4.3 Utterance Dependency
In recent work, Kim et al (2010) demonstrated the
importance of dependencies between post labels in
web forums. The authors introduced series of fea-
tures based on structural dependencies among posts.
They used relative position, author information and
automatically predicted labels from previous post(s)
as dependency features for assigning a semantic la-
bel to the current target post.
Similarly, by examining our chat corpus, we ob-
served significant dependencies between utterances.
First, 1-on-1 (i.e. agent-to-user) dialogues often con-
tain dependencies between adjacent utterances by
different authors. For example, in Table 2, when the
agent asks Is that correct?, the expected response
from the user is a Yes or No. Another example is
that when the agent makes a greeting, such as Have
a nice day, then the customer will typically respond
with a greeting or closing remark, and not a Yes or
No. Second, the flow of dialogues is in general co-
hesive, unless the topic of utterances changes dra-
matically (e.g. U5: Are you still there?, U22: brb
in 1 min in Table 2). Third, we observed that be-
tween utterances made by the same author (either
agent or user), the target utterance relies on previous
utterances made by the same author, especially when
865
ID Utterance
A:U1 Hello Customer, welcome to MSN Shopping.
A:U2 My name is Krishna and I am your
online Shopping assistant today.
C:U3 Hello!
C:U4 I?m trying to find a sports watch.
C:U5 are you still there?
A:U6 I understand that you are looking for sports
watch.
A:U7 Is that correct?
C:U8 yes, that is correct.
..
C:U22 brb in 1 min
C:U23 Thank you for waiting
..
A:U37 Thank you for allowing us to assist
you regarding wrist watch.
A:U38 I hope you found our session today helpful.
A:U39 If you have any additional questions or
you need additional information,
please log in again to chat with us.
We are available 24 hours a day, 7 days a
week for your help.
A:U40 Thank you for contacting MSN Shopping.
A:U41 Have a nice day! Good Bye and Take Care.
C:U42 You too.
Table 2: An example of a 1-on-1 live chat, with turn and
utterance structure
the agent and user repeatedly question and answer.
With these observations, we checked the likelihood
of dialogue act pairings between two adjacent utter-
ances, as well as between two adjacent utterances
made by the same author. Overall, we found strong
co-occurrence (as measured by number of occur-
rences of labels across adjacency pairs) between cer-
tain pairs of dialogue acts (e.g. (YESNO QUESTION
?YES ANSWER/NO ANSWER) and (REQUEST
?YES ANSWER)). STATEMENT, on the other
hand, can associate with most other dialogue acts.
Based on this, we designed the following five ut-
terance dependency features; by combining these,
we obtain 31 feature sets.
1. Dependency of utterances regardless of author
(a) Dialogue act of previous utterance
(b) Accumulated dialogue act(s) of previous
utterances
(c) Accumulated dialogue acts of previous ut-
terances in a given turn
2. Dependency of utterances made by a single au-
thor
(a) Dialogue act of previous utterance
by same author; a dialogue act can be in
the same turn or in the previous turn
(b) Accumulated dialogue acts of previous
utterances by same author; dialogue acts
can be in the same turn or in the previous
turn
To capture utterance dependency, Bangalore et al
(2006) previously used n-gram BoW features from
the previous 1?3 utterances. In contrast, instead of
using utterances which indirectly encode dialogue
acts, we directly use the dialogue act classifications,
as done in Stolcke et al (2000). The motivation is
that, due to the high performance of simple BoW
features, using dialogue acts directly would cap-
ture the dependency better than indirect information
from utterances, despite introducing some noise. We
do not build a probabilistic model of dialogue tran-
sitions the way Stolcke et al (2000) does, but follow
an approach similar to that used in Kim et al (2010)
in using predicted dialogue act(s) labels learned in
previous step(s) as a feature.
5 Experiment Setup
As stated earlier, we use the data set from Ivanovic
(2008) for our experiments; it contains 1-on-1 live
chats from an information delivery task. This dataset
contains 8 live chats, including 542 manually-
segmented utterances. The maximum and minimum
number of utterances in a dialogue are 84 and 42,
respectively; the maximum number of utterances in
a turn is 14. The live chats were manually tagged
with the 12 dialogue acts described in Section 3.
The utterance distribution over the dialogue acts is
described in Table 3.
For our experiments, we calculated TF, TF?IDF
and IG (Information Gain) over the utterances,
which were optionally lemmatized with the morph
tool (Minnen et al, 2000). We then built a dialogue
act classifier using three different machine learn-
ers: SVM-HMM (Joachims, 1998),2 naive Bayes
2http://www.cs.cornell.edu/People/tj/svm light/svm hmm.html
866
Dialogue Act Utterance number
CONVENTIONAL CLOSING 15
CONVENTIONAL OPENING 12
DOWNPLAYER 15
EXPRESSIVE 5
NO ANSWER 12
OPEN QUESTION 17
REQUEST 28
RESPONSE ACK 27
STATEMENT 198
THANKS 79
YES ANSWER 35
YESNO QUESTION 99
Table 3: Dialogue act distribution in the corpus
Index Learner Ours Ivanovic
Feature Acc. Feature Acc.
Word SVM 1+2+3/B .790 1/B .751
NB 1/B .673 1/B .673
CRF 1/IG .839 1/B .825
Lemma SVM 1+2+3/IG .777 N/A N/A
NB 1/B .672 N/A N/A
CRF 1/B .862 N/A N/A
Table 4: Best accuracy achieved by the different learn-
ers over different feature sets and weighting methods (1
= 1-gram; 1+2+3 = 1/2/3-grams; B = Boolean; IG = in-
formation gain)
from the WEKA machine learning toolkit (Wit-
ten and Frank, 2005), and Conditional Random
Fields (CRF) using CRF++.3 Note that we chose
to test CRF and SVM-HMM as previous work (e.g.
(Samuel et al, 1998; Stolcke et al, 2000; Chung,
2009)) has shown the effectiveness of structured
classification models on sequential dependencies.
Thus, we expect similar effects with CRF and SVM-
HMM. Finally, we ran 8-fold cross-validation using
the feature sets described above (partitioning across
the 8 sessions). All results are presented in terms
of classification accuracy. The accuracy of a zero-R
(i.e. majority vote) baseline is 0.36.
6 Evaluation
6.1 Testing Bag-of-Words Features
Table 4 shows the best accuracy achieved by the dif-
ferent learners, in combination with BoW represen-
3http://crfpp.sourceforge.net/
n-gram Boolean TF TF?IDF IG
1 .731 .511 .517 .766
2 .603 .530 .601 .614
3 .474 .463 .472 .482
1+2 .756 .511 .522 .777
1+2+3 .773 .511 .528 .777
Table 5: Accuracy of different feature representations and
weighting methods for SVM-HMM
tations and feature weighting methods. Note that the
CRF learner ran using 1-grams only, as CRF++ does
not accept large numbers of features. As a bench-
mark, we also tested the method in Ivanovic (2008)
and present the best performance over words (rather
than lemmas). Overall, we found using just 1-grams
produced the best performance for all learners, al-
though SVM achieved the best performance when
using all three n-gram orders (i.e. 1+2+3). Since the
utterances are very short, 2-grams or 3-grams alone
are too sparse to be effective. Among the feature
weighting methods, Boolean and IG achieved higher
accuracy than TF and TF?IDF. Likewise, due to the
short utterances, simple Boolean values were often
the most effective. However, as IG was computed
using the training data, it also achieved high perfor-
mance. When comparing the learners, we found that
CRF produced the best performance, due to its abil-
ity to capture inter-utterance dependencies. Finally,
we confirmed that using lemmas results in higher ac-
curacy.
Table 5 shows the accuracy over all feature sets;
for brevity, we show this for SVM only since the
pattern is similar across all learners.
6.2 Using Structural Information
In this section, we describe experiments using struc-
tural information?i.e. author and/or position?with
BoWs. As with the base BoW technique, we used
1-gram lemmas with Boolean values, based on the
results from Section 6.1. Table 6 shows the results:
Pos indicates the relative position of an utterance in
the whole dialogue, Author means author informa-
tion, and Posturn indicates the relative position of
the utterance in a turn. All methods outperformed
the baseline; methods that surpassed the results for
the simple BoW method (for the given learner) at a
867
Feature Learners
CRF SVM NB
BoW .862 .731 .672
BoW+Author .860 .655 .649
BoW+Pos .862 .721 .655
BoW+Posabsolute .863 .631 .524
BoW+Author+Pos .875 .700 .642
BoW+Author+Posturn .871 .651 .631
Table 6: Accuracy with structural information
level of statistical significance (based on randomised
estimation, p < 0.05) are boldfaced.
Overall, using CRFs with Author and Position in-
formation produced better performance than using
BoW alone. Clearly, the ability of CRFs to natively
optimise over structural dependencies provides an
advantage over other learners.
Relative position cannot of course be measured
directly in an actual online application; hence Ta-
ble 6 also includes the use of ?absolute position? as
a feature. We see that, for CRF, the absolute posi-
tion feature shows an insignificant drop in accuracy
as compared to the use of relative position. (How-
ever, we do see a significant drop in performance
when using this feature with SVM and NB.)
6.3 Using Utterance Dependency
We next combined the inter-utterance dependency
features with the BoW features. Since we use the
dialogue acts directly in utterance dependency, we
first experimented using gold-standard dialogue act
labels. We also tested using the dialogue acts which
were automatically learned in previous steps.
Table 7 shows performance using both the gold-
standard and learned dialogue acts. The differ-
ent features listed are as follows: LabelList/L in-
dicates those corresponding to all utterances in
a dialogue preceding the target utterance; Label-
Prev/P indicates a dialogue act from a previous
utterance; LabelAuthor/A indicates a dialogue act
from a previous utterance by the same author;
and LabelPrevt/LabelAuthort indicates the previ-
ous utterance(s) and previously same-authored ut-
terance(s) in a turn, respectively. Since the accuracy
for SVM and NB using learned labels is similar to
that using gold standard labels, for brevity we report
Features Dialogue Acts
Goldstandard Learned
CRF HMM NB CRF
BoW .862 .731 .672 .862
BoW+LabelList(L) .795 .435 .225 .803
BoW+LabelPrev(P) .875 .661 .364 .876
BoW+LabelAuthor(A) .865 .633 .559 .865
BoW+LabelPrevt(Pt) .873 .603 .557 .873
BoW+LabelAuthort(At) .862 .587 .535 .851
BoW+L+P .804 .428 .227 .808
BoW+L+A .799 .404 .225 .804
BoW+L+Pt .803 .413 .229 .804
BoW+L+At .808 .408 .216 .801
BoW+P+A .873 .631 .517 .869
BoW+P+Pt .878 .579 .539 .875
BoW+P+At .871 .603 .519 .867
BoW+A+Pt .847 .594 .519 .849
BoW+A+At .869 .594 .530 .871
BoW+Pt+At .871 .592 .519 .867
BoW+L+P+A .812 .419 .231 .804
BoW+L+P+Pt .816 .423 .229 .812
BoW+L+P+At .808 .397 .225 .806
BoW+L+A+Pt .810 .388 .225 .810
BoW+L+A+At .812 .415 .216 .801
BoW+L+Pt+At .810 .375 .205 .816
BoW+P+A+Pt .875 .602 .522 .876
BoW+P+A+At .862 .609 .511 .864
BoW+P+Pt+At .873 .594 .515 .867
BoW+A+Pt+At .865 .594 .517 .864
BoW+L+P+A+Pt .817 .410 .231 .810
BoW+L+P+A+At .814 .411 .223 .810
BoW+L+P+Pt+At .816 .382 .205 .806
BoW+L+A+Pt+At .812 .406 .203 .808
BoW+P+A+Pt+At .865 .583 .513 .865
BoW+L+P+A+Pt+At .816 .399 .205 .803
Table 7: Accuracy for the different learners with depen-
dency features
the performance for CRF using learned labels only.
Results that exceed the BoW accuracy at a level of
statistical significance (p < 0.05) are boldfaced.
Utterance dependency features worked well in
combination with CRF only. Individually, Prev and
Prevt (i.e. BoW+P+Pt) helped to achieve higher ac-
curacies, and the Author feature was also benefi-
cial. However, List decreased the performance, as
the flow of dialogues can change, and when a larger
history of dialogue acts is included, it tends to in-
troduce noise. Comparing use of gold-standard and
learned dialogue acts, the reduction in accuracy was
not statistically significant, indicating that we can
868
Feature CRF SVM NB
C+LabelList .9557 .4613 .2565
C+LabelPrev .9649 .6365 .5720
C+LabelAuthor .9686 .6310 .5424
C+LabelPrevt .9686 .5738 .5738
C+LabelAuthort .9561 .6125 .5332
Table 8: Accuracy with Structural and Dependency Infor-
mation: C means lemmatized Unigram+Position+Author
achieve high performance on dialogue act classifi-
cation even with interactively-learned dialogue acts.
We believe this demonstrates the robustness of the
proposed techniques.
Finally, we tested the combination of features
from structural and dependency information. That
is, we used a base feature (unigrams with Boolean
value), relative position, author information, com-
bined with each of the different dependency features
? LabelList, LabelPrev, LabelAuthor, LabelPrevt
and LabelAuthort.
Table 8 shows the performance when using these
combinations, for each dependency feature. As we
would expect, CRFs performed well with the com-
bined features since CRFs can incorporate the struc-
tural and dependency information; the achieved the
highest accuracy of 96.86%.
6.4 Error Analysis and Future Work
Finally, we analyzed the errors of
the best-performing feature set (i.e.
BoW+Position+Author+LabelAuthor). In Ta-
ble 9, we present a confusion matrix of errors,
for CONVENTIONAL CLOSING (Cl), CON-
VENTIONAL OPENING (Op), DOWNPLAYER
(Dp), EXPRESSIVE (Ex), NO ANSWER (No),
OPEN QUESTION (Qu), REQUEST (Rq), RE-
SPONSE ACK (Ack), STATEMENT (St), THANKS
(Ta), YES ANSWER (Yes), and YESNO QUESTION
(YN). Rows indicate the correct dialogue acts and
columns indicate misclassified dialogue acts.
Looking over the data, STATEMENT is a common
source of misclassification, as it is the majority class
in the data. In particularly, a large number of RE-
QUEST and RESPONSE ACK utterances were tagged
as STATEMENT. We did not include punctuation
such as question marks in our feature sets; includ-
ing this would likely improve results further.
In future work, we plan to investigate methods for
automatically cleansing the data to remove typos,
and taking account of temporal gaps that can some-
times arise in online chats (e.g. in Table 2, there is
a time gap between C:U22 brb in 1 min and C:U23
Thank you for waiting).
7 Conclusion
We have explored an automated approach for classi-
fying dialogue acts in 1-on-1 live chats in the shop-
ping domain, using bag-of-words (BoW), structural
information and utterance dependency features. We
found that the BoW features perform remarkably
well, with slight improvements when using lemmas
rather than words. Including structural and inter-
utterance dependency information further improved
performance. Of the learners we experimented with,
CRFs performed best, due to their ability to natively
capture sequential dialogue act dependencies.
Acknowledgements
This research was supported in part by funding from
Microsoft Research Asia.
References
J.Allen and M.Core. Draft of DAMSL: Dialog Act
Markup in Several Layers. The Multiparty Dis-
course Group. University of Rochester, Rochester,
USA. 1997.
A. Anderson, M. Bader, E. Bard, E. Boyle G.M. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, C. Sotillo, H.S. Thompson, R. and Weinert.
The HCRC Map Task Corpus. Language and Speech.
1991, 34, pp. 351?366.
J. Ang, Y. Liu and E. Shriberg. Automatic Dialog Act
Segmentation and Classification in Multiparty Meet-
ings. IEEE International Conference on Acoustics,
Speech, and Signal Processing. 2005, pp, 1061?1064.
S. Bangalore, G. Di Fabbrizio and A. Stent. Learning
the Structure of Task-Driven Human-Human Dialogs.
Proceedings of the 21st COLING and 44th ACL. 2006,
pp. 201?208.
H. H. Bui. A general model for online probabilistic plan
recognition. IJCAI. 2003, pp. 1309?1318.
G.Y Chung. Sentence retrieval for abstracts of random-
ized controlled trials. BMC Medical Informatics and
Decision Making. 2009, 9(10), pp. 1?13.
F. Debole and F. Sebastiani. Supervised term weighting
for automated text categorization. 18th ACM Sympo-
sium on Applied Computing. 2003, pp. 784?788.
869
Cl Op Dp Ex No Qu Rq Ack St Ta Yes YN
Op 0 0 0 0 0 0 0 0 0 0 0 2
Cl 0 0 0 0 0 0 0 0 1 1 0 0
Dp 0 0 0 0 0 0 0 0 0 0 0 0
Ex 0 0 0 0 0 0 0 0 0 0 0 0
No 0 0 0 0 0 0 0 0 0 0 0 0
Qu 0 0 0 0 0 0 0 0 0 0 0 0
Rq 0 0 0 0 0 0 0 0 3 0 0 0
Ack 0 0 1 0 1 0 0 0 5 0 0 0
St 0 0 0 0 0 0 1 0 0 0 0 0
Ta 1 0 0 0 0 0 0 0 0 0 0 0
Yes 0 0 0 0 0 0 0 0 0 0 0 0
YN 0 1 0 0 0 0 0 0 0 0 0 0
Table 9: Confusion matrix for errors from the CRF with BoW+Position+Author+LabelAuthor (rows = correct clas-
sification; columns = misclassification; CONVENTIONAL CLOSING = Cl; CONVENTIONAL OPENING = Op; DOWN-
PLAYER = Dp; EXPRESSIVE = Ex; NO ANSWER = No; OPEN QUESTION = Qu; REQUEST = Rq; RESPONSE ACK
= Ack; STATEMENT = St; THANKS = Ta; YES ANSWER = Yes; and YESNO QUESTION = YN)
E. N. Forsyth. Improving Automated Lexical and Dis-
course Analysis of Online Chat Dialog. Master?s the-
sis. Naval Postgraduate School, 2007.
J. Godfrey and E. Holliman and J. McDaniel. SWITCH-
BOARD: Telephone speech corpus for research and
development. Proceedings of IEEE International
Conference on Acoustics, Speech, and Signal Process-
ing. 1992, pp. 517?520.
S. Grau, E. Sanchis, M. Jose and D. Vilar. Dialogue act
classification using a Bayesian approach. Proceedings
of the 9th Conference on Speech and Computer. 2004.
P. A. Heeman and J. Allen. The Trains 93 Dialogues.
Trains Technical Note 94-2. Computer Science Dept.,
University of Rochester, March 1995.
T. Joachims. Text categorization with support vector ma-
chines: Learning with many relevant features. Pro-
ceedings of European Conference on Machine Learn-
ing. 1998, pp. 137?142.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker and P. Maloor.
MATCH: An Architecture for Multimodal Dialogue
Systems. Proceedings of 40th ACL. 2002, pp. 376?
383.
F. N. Julia and K. M. Iftekharuddin. Dialog Act clas-
sification using acoustic and discourse information of
MapTask Data. Proceedings of the International Joint
Conference on Neural Networks. 2008, pp. 1472?
1479.
D. Jurafsky, E. Shriberg, B Fox and T. Curl. Lexical,
Prosodic, and Syntactic Cues for Dialog Acts. Pro-
ceedings of ACL/COLING-98 Workshop on Discourse
Relations and Discourse Markers. 1998, pp. 114?120.
E. Ivanovic. Automatic instant messaging dialogue us-
ing statistical models and dialogue acts. Master?s The-
sis. The University of Melbourne. 2008.
S. Keizer. A Bayesian Approach to Dialogue Act Clas-
sification. 5th Workshop on Formal Semantics and
Pragmatics of Dialogue. 2001, pp. 210?218.
S.N. Kim and L. Wang and T. Baldwin. Tagging and
Linking Web Forum Posts. Fourteenth Conference on
Computational Natural Language Learning. 2010.
J. Lafferty, A. McCallum and F. Pereira. Conditional
random fields: Probabilistic models for segmenting
and labeling sequence data. Proceedings of ICML.
2001, pp. 282?289.
D. J. Litman and S. Silliman. ITSPOKE: An Intelligent
Tutoring Spoken Dialogue SYstem. Proceedings of
the HLT/NAACL. 2004.
M. M. Louwerse and S. Crossley. Dialog Act Classifica-
tion Using N -Gram Algorithms. FLAIRS Conference,
2006, pp. 758?763.
G. Minnen, J. Carroll and D. Pearce. Applied morpho-
logical processing of English Natural Language Engi-
neering 2000, 7(3), pp. 77?80.
M. Purver, J. Niekrasz and S. Peters. Ontology-Based
Discourse Understanding for a Persistent Meeting As-
sistant. Proc. CHI 2005 Workshop on The Virtuality
Continuum Revisited. 2005.
K. Samuel, Sandra Carberry and K. Vijay-Shanker. Dia-
logue Act Tagging with Transformation-Based Learn-
ing. Proceedings of COLING/ACL 1998. 1998, pp.
1150-1156.
E. Shriberg, R. Bates, P. Taylor, A. Stolcke, D. Jurafsky,
K. Ries, N. Coccaro, R. Martin, M. Meteer and C. Van
870
Ess-Dykema. Can Prosody Aid the Automatic Clas-
sification of Dialog Acts in Conversational Speech?.
Language and Speech. 1998, 41(3-4), pp. 439?487.
V. R. Sridhar, S. Bangalore and S. Narayanan. Combin-
ing lexical, syntactic and prosodic cues for improved
online dialog act tagging. Computer Speech and Lan-
guage. 2009, 23(4), pp. 407?422.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema
and M. Meteer. Dialogue Act Modeling for Automatic
Tagging and Recognition of Conversational Speech.
Computational Linguistics. 2000, 26(3), pp. 339?373.
A. Stolcke and E. Shriberg. Markovian Combination of
Language and Prosodic Models for better Speech Un-
derstanding and Recognition . Invited talk at the IEEE
Workshop on Speech Recognition and Understanding,
Madonna di Campiglio, Italy, December 2001 2001,
C. C. Werry. Linguistic and interactional features of In-
ternet Relay Chat. In S. C. Herring (ed.). Computer-
Mediated Communication. Benjamins, 1996.
I. Witten and E. Frank. Data Mining: Practical Machine
Learning Tools and Techniques. Morgan Kaufmann,
2005.
T. Wu, F. M. Khan, T. A. Fisher, L. A. Shuler and W. M.
Pottenger. Posting act tagging using transformation-
based learning. Proceedings of the Workshop on Foun-
dations of Data Mining and Discovery, IEEE Interna-
tional Conference on Data Mining. 2002.
871
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13?25,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting Thread Discourse Structure over Technical Web Forums
Li Wang,?? Marco Lui,?? Su Nam Kim,?? Joakim Nivre? and Timothy Baldwin??
? Dept. of Computer Science and Software Engineering, University of Melbourne
? NICTA Victoria Research Laboratory
? Dept. of Linguistics and Philology, Uppsala University
li.wang.d@gmail.com, saffsd@gmail.com,
sunamkim@gmail.com, joakim.nivre@lingfil.uu.se, tb@ldwin.net
Abstract
Online discussion forums are a valuable
means for users to resolve specific information
needs, both interactively for the participants
and statically for users who search/browse
over historical thread data. However, the com-
plex structure of forum threads can make it
difficult for users to extract relevant informa-
tion. The discourse structure of web forum
threads, in the form of labelled dependency re-
lationships between posts, has the potential to
greatly improve information access over web
forum archives. In this paper, we present the
task of parsing user forum threads to deter-
mine the labelled dependencies between posts.
Three methods, including a dependency pars-
ing approach, are proposed to jointly clas-
sify the links (relationships) between posts
and the dialogue act (type) of each link. The
proposed methods significantly surpass an in-
formed baseline. We also experiment with ?in
situ? classification of evolving threads, and es-
tablish that our best methods are able to per-
form equivalently well over partial threads as
complete threads.
1 Introduction
Web user forums (or simply ?forums?) are online
platforms for people to discuss information and ob-
tain information via a text-based threaded discourse,
generally in a pre-determined domain (e.g. IT sup-
port or DSLR cameras). With the advent of Web
2.0, there has been an explosion of web authorship in
this area, and forums are now widely used in various
areas such as customer support, community devel-
opment, interactive reporting and online eduction.
In addition to providing the means to interactively
participate in discussions or obtain/provide answers
to questions, the vast volumes of data contained in
forums make them a valuable resource for ?support
sharing?, i.e. looking over records of past user inter-
actions to potentially find an immediately applica-
ble solution to a current problem. On the one hand,
more and more answers to questions over a wide
range of domains are becoming available on forums;
on the other hand, it is becoming harder and harder
to extract and access relevant information due to the
sheer scale and diversity of the data.
This research aims at enhancing information ac-
cess and support sharing, by mining the discourse
structure of troubleshooting-oriented web user fo-
rum threads. Previous research has shown that sim-
ple thread structure information (e.g. reply-to struc-
ture) can enhance tasks such as forum information
retrieval (Seo et al, 2009) and post quality assess-
ment (Lui and Baldwin, 2009). We aim to move be-
yond simple threading, to predict not only the links
between posts, but also show the manner of each
link, in the form of the discourse structure of the
thread. In doing so, we hope to be able to perform
richer visualisation of thread structure (e.g. high-
lighting the key posts which appear to have led to
a successful resolution to a problem), and more fine-
grained weighting of posts in threads for search pur-
poses.
To illustrate the task, we use an example thread,
made up of 5 posts from 4 distinct participants, from
the CNET forum dataset of Kim et al (2010b), as
shown in Figure 1. The discourse structure of the
thread is modelled as a rooted directed acyclic graph
13
HTML Input Code...Please can someone tell me how to create an input box that asks the user to enter their ID, and then allows them to press go. It will then redirect to the page ...
User APost 1
User BPost 2
User CPost 3
Re: html input codePart 1: create a form with a text field. See ... Part 2: give it a Javascript action
asp.net c\# videoI?ve prepared for you video.link click ...
Thank You!Thanks a lot for that ... I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site? ...
A little more help... You would simply do it this way: ... You could also just ... An example of this is ...
User APost 4
User DPost 5
0+Question-Question
2+Answer-Answer
4+Answer-Answer
1+Answer-Answer
1+Answer-Confirmation
3+Question-Add
?
Figure 1: A snippeted and annotated CNET thread
(DAG) with a dialogue act label associated with each
edge of the graph. In this example, UserA initiates
the thread with a question (dialogue act = Question-
Question) in the first post, by asking how to create
an interactive input box on a webpage. In response,
UserB and UserC provide independent answers (di-
alogue act = Answer-Answer). UserA responds to
UserC to confirm the details of the solution (dia-
logue act = Answer-Confirmation), and at the same
time, adds extra information to his/her original ques-
tion (dialogue act = Question-Add); i.e., this one
post has two distinct dependency links associated
with it. Finally, UserD proposes a different solution
again to the original question.
To predict thread discourse structure of this type,
we jointly classify the links and dialogue acts be-
tween posts, experimenting with a variety of su-
pervised classification methods, namely dependency
parsing and linear-chain conditional random fields.
In this, we build on the earlier work of Kim et al
(2010b) who first proposed the task of thread dis-
course analysis, but only carried out experiments on
post linking and post dialogue act classification as
separate tasks. In addition to achieving state-of-the-
art accuracy over the task, we carry out in-depth
analysis of classification effectiveness at different
thread depths, and establish that the accuracy of our
method over partial threads is equivalent to that over
full threads, indicating that the method is applica-
ble to in-situ thread classification. Finally, we in-
vestigate the role of user-level features in discourse
structure analysis.
2 Related Work
This work builds directly on earlier work of a subset
of the authors (Kim et al, 2010b), whereby a novel
post-level dialogue act set was proposed, and used
as the basis for annotation of a set of threads taken
from CNET. In the original work, we proposed a set
of novel features, which we applied to the separate
tasks of post link classification and dialogue act clas-
sification. We later applied the same basic method-
ology to dialogue act classification over one-on-one
live chat data with provided message dependencies
(Kim et al, 2010a), demonstrating the generalisabil-
ity of the original method. In both cases, however,
we tackled only a single task, either link classifica-
tion (optionally given dialogue act tags) or dialogue
act classification, but never the two together. In this
paper, we take the obvious step of exploring joint
classification of post link and dialogue act tags, to
generate full thread discourse structures.
Discourse disentanglement (i.e. link classifica-
tion) and dialogue act tagging have been studied
largely as independent tasks. Discourse disentangle-
ment is the task of dividing a conversation thread
(Elsner and Charniak, 2008; Lemon et al, 2002)
or document thread (Wolf and Gibson, 2005) into
a set of distinct sub-discourses. The disentangled
discourse is sometimes assumed to take the form of
a tree structure (Grosz and Sidner, 1986; Lemon et
al., 2002; Seo et al, 2009), an acyclic graph struc-
ture (Rose? et al, 1995; Schuth et al, 2007; Elsner
and Charniak, 2008; Wang et al, 2008; Lin et al,
2009), or a more general cyclic chain graph struc-
ture (Wolf and Gibson, 2005). Dialogue acts are
used to describe the function or role of an utterance
in a discourse, and have been applied to the anal-
ysis of mediums of communication including con-
versational speech (Stolcke et al, 2000; Shriberg et
al., 2004; Murray et al, 2006), email (Cohen et al,
2004; Carvalho and Cohen, 2005; Lampert et al,
2008), instant messaging (Ivanovic, 2008; Kim et
al., 2010a), edited documents (Soricut and Marcu,
2003; Sagae, 2009) and online forums (Xi et al,
14
2004; Weinberger and Fischer, 2006; Wang et al,
2007; Fortuna et al, 2007; Kim et al, 2010b). For a
more complete review of models for discourse dis-
entanglement and dialogue act tagging, see Kim et
al. (2010b).
Joint classification has been applied in a number
of different contexts, based on the intuition that it
should be possible to harness interactions between
different sub-tasks to the mutual benefit of both.
Warnke et al (1997) jointly performed segmenta-
tion and dialogue act classification over a German
spontaneous speech corpus. In their approach, the
predictions of a multi-layer perceptron classifier on
dialogue act boundaries were fed into an n-gram
language model, which was used for the joint seg-
mentation and classification of dialogue acts. Sut-
ton and McCallum (2005) performed joint parsing
and semantic role labelling (SRL), using the results
of a probabilistic SRL system to improve the accu-
racy of a probabilistic parser. Finkel and Manning
(2009) built a joint, discriminative model for pars-
ing and named entity recognition (NER), address-
ing the problem of inconsistent annotations across
the two tasks, and demonstrating that NER bene-
fited considerably from the interaction with parsing.
Dahlmeier et al (2009) proposed a joint probabilis-
tic model for word sense disambiguation (WSD) of
prepositions and SRL of prepositional phrases (PPs),
and achieved state-of-the-art results over both tasks.
There has been a recent growth in user-level
research over forums. Lui and Baldwin (2009)
explored a range of user-level features, including
replies-to and co-participation graph analysis, for
post quality classification. Lui and Baldwin (2010)
introduced a novel user classification task where
each user is classified against four attributes: clar-
ity, proficiency, positivity and effort. User commu-
nication roles in web forums have also been studied
(Chan and Hayes, 2010; Chan et al, 2010).
Threading information has been shown to en-
hance retrieval effectiveness for post-level retrieval
(Xi et al, 2004; Seo et al, 2009), thread-level
retrieval (Seo et al, 2009; Elsas and Carbonell,
2009), sentence-level shallow information extrac-
tion (Sondhi et al, 2010), and near-duplicate thread
detection (Muthmann et al, 2009). These results
suggest that the thread structural representation used
in this research, which includes both linking struc-
ture and the dialogue act associated with each link,
could potentially provide even greater leverage in
these retrieval tasks.
Another related research area is post-level classi-
fication, such as general post quality classification
(Weimer et al, 2007; Weimer and Gurevych, 2007;
Wanas et al, 2008; Lui and Baldwin, 2009), and
post descriptiveness in particular domains (e.g. med-
ical forums: Leaman et al (2010)). It has been
demonstrated (Wanas et al, 2008; Lui and Bald-
win, 2009) that thread discourse structure can signif-
icantly improve the classification accuracy for post-
level tasks.
Initiation?response pairs (e.g. question?answer,
assessment?agreement, and blame?denial) from on-
line forums have the potential to enhance thread
summarisation or automatically generate knowledge
bases for Community Question Answering (cQA)
services such as Yahoo! Answers. While initiation?
response pair identification has been explored as a
pairwise ranking problem (Wang and Rose?, 2010),
question?answer pair identification has been ap-
proached via the two separate sub-tasks of ques-
tion classification and answer detection (Cong et al,
2008; Ding et al, 2008; Cao et al, 2009). Our
thread discourse structure prediction task includes
joint classification of post roles (i.e. dialogue acts)
and links, and could potentially be performed at the
sub-post sentence level to extract initiation?response
pairs.
3 Task Description and Data Set
The main task performed in this research is joint
classification of inter-post links (Link) and dialogue
acts (DA) within forum threads. In this, we assume
that a post can only link to an earlier post (or a vir-
tual root node), and that dialogue acts are labels on
edges. It is possible for there to be multiple edges
from a given post, e.g. if a post both confirms the va-
lidity of an answer and adds extra information to the
original question (as happens in Post4 in Figure 1).
We experiment with two different approaches to
joint classification: (1) a linear-chain CRF over
combined Link/DA post labels; and (2) a depen-
dency parser. The joint classification task is a nat-
ural fit for dependency parsing, in that the task is
intrinsically one of inferring labelled dependencies
15
between posts, but it has a number of special prop-
erties that distinguish it from standard dependency
parsing:
strict reverse-chronological directionality: the
head always precedes the dependent, in terms
of the chronological sequencing of posts.
non-projective dependencies: threads can contain
non-projective dependencies, e.g. in a 4-post
thread, posts 2 and 3 may be dependent on
post 1, and post 4 dependent on post 2; around
2% of the threads in our dataset contain non-
projective dependencies.
multi-headedness: it is possible for a given post to
have multiple heads, including the possibility
of multiple dependency links to the same post
(e.g. adding extra information to a question
[Question-Add] as well as retracting infor-
mation from the original question [Question-
Correction]); around 6% of the threads in our
dataset contain multi-headed dependencies.
disconnected sub-graphs: it is possible for there to
be disconnected sub-graphs, e.g. in instances
where a user hijacks a thread to ask their
own unrelated question, or submit an unrelated
spam post; around 2% of the threads in our
dataset contain disconnected sub-graphs.
The first constraint potentially simplifies depen-
dency parsing, and non-projective dependencies are
relatively well understood in the dependency parsing
community (Tapanainen and Jarvinen, 1997; Mc-
Donald et al, 2005). Multi-headedness and dis-
connected sub-graphs pose greater challenges to de-
pendency parsing, although there has been research
done on both (McDonald and Pereira, 2006; Sagae
and Tsujii, 2008; Eisner and Smith, 2005). The
combination of non-projectivity, multi-headedness
and disconnected sub-graphs in a single dataset,
however, poses a challenge for dependency parsing.
In addition to performing evaluation in batch
mode over complete threads, we consider the task of
?in situ thread classification?, whereby we predict
the discourse structure of a thread after each post.
This is intended to simulate the more realistic set-
ting of incrementally crawling/updating thread data,
but needing to predict discourse structure for partial
threads. We are interested in determining the rela-
tive degradation in accuracy for in situ classification
vs. batch classification.
As our dataset, we use the CNET forum dataset
of Kim et al (2010b),1 which contains 1332 an-
notated posts spanning 315 threads, collected from
the Operating System, Software, Hardware and Web
Development sub-forums of cnet.2 Each post is la-
belled with one or more links (including the possi-
bility of null-links, where the post doesn?t link to
any other post), and each link is labelled with a di-
alogue act. The dialogue act set is made up of 5
super-categories: Question, Answer, Resolution
(confirmation of the question being resolved), Re-
production (external confirmation of a proposed so-
lution working) and Other. The Question category
contains 4 sub-classes: Question, Add, Confirma-
tion and Correction. Similarly, the Answer cate-
gory contains 5 sub-classes: Answer, Add, Confir-
mation, Correction and Objection. For example,
the label Question-Add signifies the Question su-
perclass and Add subclass, i.e. addition of extra in-
formation to a question. For full details of the dia-
logue act tagset, see Kim et al (2010b).
Dependency links are represented by their relative
position in the chronologically-sorted list of posts,
e.g. 1 indicates a link back to the preceding post,
and 2 indicates a link back two posts.
Unless otherwise noted, evaluation is over the
combined link and dialogue act tag, including the
combination of superclass and subclass for the
Question and Answer dialogue acts. For ex-
ample, 1+Answer-Answer indicates a dependency
link back one post, which is an answer to a question.
The most common label in the dataset is 1+Answer-
answer (28.4%).
4 Learners and Features
4.1 Learners
To predict thread discourse structure, we use a struc-
tured classification approach ? based on the find-
ings of Kim et al (2010b) and Kim et al (2010a)
? and a dependency parser. The structured clas-
sification approach we experiment with is a linear-
1Available from http://www.csse.unimelb.edu.
au/research/lt/resources/conll2010-thread/
2http://forums.cnet.com/
16
chain conditional random field learner (CRF: Laf-
ferty et al (2001)), within which we explore two
simple approaches to joint classification, as is ex-
plained in Section 5.1. Dependency parsing (Ku?bler
et al, 2009) is the task of automatically predicting
the dependency structure of a token sequence, in
the form of binary asymmetric dependency relations
with dependency types.
Standardly, CRFs have been applied to tasks such
as part-of-speech tagging, named entity recognition,
semantic role labelling and supertagging, where the
individual tokens are single words. Similarly, de-
pendency parsing is conventionally applied to sen-
tences, with single-word tokens. In our case, our
tokens are thread posts, with much greater scope for
feature engineering than single words, and techni-
cal challenges in scaling the underlying implemen-
tations to handle potentially much larger feature sets.
As our learners, we deployed CRFSGD (Bot-
tou, 2011) to learn the CRF, and MaltParser (Nivre
et al, 2007) as our dependency parser. CRFSGD
uses stochastic gradient descent to efficiently solve
the convex optimisation problem, and scales well to
large feature sets. We used the default parameter set-
tings for CRFSGD, with feature templates includ-
ing all unigram features of the current token as well
as bigram features combining the previous output to-
ken with the current token.
MaltParser implements transition-based parsing,
where no formal grammar is considered, and a tran-
sition system, or state machine, is learned to map a
sentence onto its dependency graph. One feature of
MaltParser that makes it well suited to our task is
that it is possible to define feature models of arbi-
trary complexity for each token. In presenting the
thread data to MaltParser, we represent the null-
link from the initial post of each thread, as well as
any disconnected posts, as the root.
To the best of our knowledge, there is no past
work on using dependency parsing to learn thread
discourse structure. Based on extensive experimen-
tation, we determined that the MaltParser configu-
ration that obtains the best results for our task is the
Nivre algorithm in arc-standard mode (Nivre, 2003;
Nivre, 2004), using LIBSVM (Chang and Lin, 2011)
with a linear kernel as the learner, and a feature
model with exhaustive combinations of features re-
lating to the features and predictions of the first/top
three tokens from both ?Input? and ?Stack?.3 As
such, MaltParser is actually unable to predict any
non-projective structures, as experiments with algo-
rithms supporting non-projective structures invari-
ably led to lower results. In our choice of parsing al-
gorithm, we are also unable to detect posts with mul-
tiple heads, but can potentially detect disconnected
sub-graphs.
4.2 Features
The features used in our classifiers are as follows:
Structural Features:
Initiator a binary feature indicating whether the
current post?s author is the thread initiator.
Position the relative position of the current post,
as a ratio over the total number of posts in the
thread.
Semantic Features:
TitSim the relative location of the post which has
the most similar title (based on unweighted co-
sine similarity) to the current post.
PostSim the relative location of the post which
has the most similar content (based on un-
weighted cosine similarity) to the current post.
Punct the number of question marks (QuCount),
exclamation marks (ExCount) and URLs
(UrlCount) in the current post.
UserProf the class distribution (in the training
thread) of the author of the current post.
These features are drawn largely from the work
of Kim et al (2010b), with two major differences:
(1) we do not use post context features because our
learners (i.e. CRFSGD and MaltParser) inherently
capture Markov chains; and (2) our UserProf fea-
tures are customised to the class set associated with
the task at hand, e.g. the UserProf features for the
standalone linking task take the form of the link la-
bels (and not dialogue act labels) of the posts by the
relevant author in the training data. Table 1 shows
the feature representation of the third post in a thread
17
Feature Value Explanation
Initiator 1.0 post from the initiator
ExCount 4.0 4 exclamation marks
QuCount 0.0 0 question marks
UrlCount 0.0 0 URLs
Position 0.25 i?1n = 3?18PostSim 2.0 most similar to post 1
TitSim 2.0 most similar to post 1
UserProf ~x counts for posts of each
class from the same author
in the training data
Table 1: The feature presentation of the third post in a
thread of length 8
of length 8. The values of each feature are scaled to
the range [0, 1] before being fed into the learners.
We also experimented with other features,
including raw bag-of-words lexical features,
dimensionality-reduced lexical features (using
principal components analysis), and different post
similarity measures such as longest common subse-
quence (LCS) match. While we were able to obtain
gains in isolation, when combined with the other
features, these features had no impact, and are thus
not included in the results presented in this paper.
5 Classification Methodology
All our experiments were carried out based on strati-
fied 10-fold cross-validation, stratifying at the thread
level to ensure that all posts from a given thread
occur in a single fold. The results are primarily
evaluated using post-level micro-averaged F-score
(F?: ? = 1), and additionally with thread-level F-
score/classification accuracy (i.e. the proportion of
threads where all posts have been correctly classi-
fied4), where space allows. Statistical significance
is tested using randomised estimation (Yeh, 2000)
with p < 0.05. Initial experiments showed it is
hard for learners to discover which posts have multi-
ple links, largely due to the sparsity of multi-headed
posts (which account for less than 5% of the total
posts). Therefore, only the the most recent link for
3http://maltparser.org/userguide.html#
parsingalg
4Classification accuracy = F-score at the thread-level, as
each thread is assigned a single label of correct or incorrect.
each multi-headed post was included in training, but
evaluation still considers all links.
5.1 Joint classification
In our experiments, we test two basic approaches to
joint classification for the CRF: (1) classifying the
Link and DA separately, and composing the predic-
tions to form the joint classification (Composition);
and (2) combining the Link and DA labels into a sin-
gle class, and applying the learner over the posts
with the combined class (Combine). Note that
Composition has the potential for mismatches in
the number of Link and DA predictions it gener-
ates, causing complications in the class composition.
Even if the same number of labels is predicted for
both Link and DA, if multiple tags are predicted in
both cases, we are left with the problem of determin-
ing which link label to combine with which dialogue
act label. As such, we have our reservations about
Composition, but as the CRF performs strict 1-of-
n labelling, these are not issues in the experiments
reported herein.
MaltParser natively handles the combination of
Link and DA in its dependency parsing formulation.
5.2 In Situ Thread Classification
One of the biggest challenges in classifying the dis-
course structure of a forum thread is that threads
evolve over time, as new posts are posted. In or-
der to capture this phenomenon, and compare the
accuracy of different models when applied to partial
thread data (artificially cutting off a thread at post
N ) vs. complete threads.5 This is done in the fol-
lowing way: classification over the first two posts
only ([1, 2]), the first four posts ([1, 4]), the first six
posts ([1, 6]), the first eight posts ([1, 8]), and all
posts ([all]). In each case, we limit the test data
only, meaning that the only variable in play is the
extent of thread context used to learn the thread dis-
course structure for the given set of posts. We break
down the results in each case into the indicated sub-
threads, e.g. we take the predictions for [all], and
break them down into the results for [1, 2], [1, 4],
[1, 6], [1, 8] and [all], for direct comparison with the
predictions over the respective sub-thread data.
5In practice, completeness is defined at a given point in time,
when the crawl was done, and it is highly likely that some of the
?complete? threads had extra posts after the crawl.
18
Method Link DA
Kim et al (2010b) .863 / .676 .751 / .543
CRFSGD .891 / .727 .795 / .609
Table 2: Post/thread-level component-wise classification
F-scores for Link and DA classes
6 Experiments and Analysis
6.1 Joint classification
As our baseline for the task, we first use a sim-
ple majority class classifier in the form of the sin-
gle joint class of 1+Answer-Answer for all posts,
which has a post-level F-score of 0.284. A stronger
baseline is to classify all first posts as 0+Question-
Question and all subsequent posts as 1+Answer-
answer, which achieves a post-level F-score of
0.515 (labelled as Heuristic).
As described in Section 5.1, one approach to joint
classification with CRFSGD is to firstly conduct
component-wise classification over Link and DA
separately, and compose the predictions. The results
for the separate Link and DA classification tasks are
presented in Table 2, along with the best results for
Link and DA classification from Kim et al (2010b).
At the component-wise tasks, our method is superior
to Kim et al (2010b), based on a different learner
and slightly different feature set.
Next, we compose the component-wise clas-
sifications for the CRF into joint classifications
(Composition). We contrast this with the com-
bined class approach for CRFSGD and MaltParser
(jointly presented as Joint in Table 3). With the
combined class results, we additionally ablate each
of the feature types from Section 4.2, and also
present results for a dummy model, where no fea-
tures are provided and the prediction is based simply
on sequential priors (Dummy). The results are pre-
sented in Table 3, along with the Heuristic baseline
result.
Several interesting things can be observed from
the post-level F-score results in Table 3. First, with
no features (Dummy), while CRFSGD performs
slightly worse than the Heuristic baseline, Malt-
Parser significantly surpasses the baseline. This is
due to the richer sequential context model of Malt-
Parser. Second, the single feature with the greatest
impact on results is UserProf, i.e. user profile fea-
Method CRFSGD MaltParser
Heuristic .515?/ .311?
Dummy .508?/ .394? .533?/ .356?
Composition .728?/ .553? ?
Joint +ALL .756 / .578 .738 / .578
?Initiator .745 / .569 .708?/ .534?
?Position .750 / .565 .736 / .568
?PostSim .753 / .578 .737 / .568
?TitSim .760 / .587 .734 / .571
?Punct .745 / .571 .735 / .578
?UserProf .672?/ .527? .701?/ .536?
Table 3: Post/thread-level Link-DA joint classification F-
scores (??? signifies a significantly worse result than that
for the same learner with ALL features)
tures extracted from the training data; CRFSGD in
particular benefits from this feature. We return to ex-
plore this effect in Section 6.4. Third, although the
Initiator feature does not have much effect on CRF-
SGD, it affects the performance of MaltParser sig-
nificantly. Further experiments shown that the com-
bination of Initiator and UserProf is sufficient to
achieve a competitive result (i.e. 0.731). It therefore
seems that MaltParser is more robust than CRF-
SGD, whose performance relies crucially on user-
level features which must be learned from the train-
ing data (i.e. UserProf).
Looking to the thread-level F-scores, we observe
some interesting divergences from the post-level F-
score results. First, with no features (Dummy),
CRFSGD significantly outperforms both the base-
line and MaltParser. This appears to be because
CRFSGD performs particularly well over short
threads (e.g. of length 3 and 4), but worse over
longer threads. Second, the best thread-level F-
scores from CRFSGD (i.e. 0.587) and MaltParser
(i.e. 0.578) are not significantly different, despite the
discrepancy in post-level F-score (where CRFSGD
is markedly superior in this case). With the extra
features, the performance of MaltParser on short
threads appears to pick up noticeably, and the differ-
ence in post-level predictions is over longer threads.
If we evaluate the two models over DA super-
classes only (ignoring mismatches at the subclass
level for Question and Answer), the post-level F-
scores for joint classification with ALL features for
CRFSGD and MaltParser are 0.803 and 0.787, re-
spectively.
19
Approaches Link DA
Component-wise .891 / .727? .795 / .609
CRFSGD decomp .893 / .749 .785 / .603
MaltParser decomp .870?/ .730? .766?/ .571?
Table 4: Post/thread-level Link and DA F-scores from
component-wise classification, and from Link-DA clas-
sification decomposition (??? signifies a significantly
worse result than the best result in that column)
Looking at the performance of CRFSGD (in
Combine mode) and MaltParser on disconnected
sub-graphs, while both models did predict a small
number of non-initial posts with null-links (includ-
ing MaltParser predicting 5 out of 6 posts in a sin-
gle thread as having null-links), none were correct,
and neither model was able to correctly predict any
of the 6 actual non-initial instances of null-links in
the dataset.
Finally, we took the joint classification results
from CRFSGD and MaltParser using ALL fea-
tures, and decomposed the predictions into Link and
DA. The results are presented in Table 4, along with
the results for component-wise classification from
Table 2. Somewhat surprisingly, the decomposed
predictions are mostly slightly worse than the re-
sults for the component-wise classification, despite
achieving higher F-score for the joint classification
task. This is simply due to the combined method
tending to get both labels correct or both labels
wrong, for a given post.
6.2 Post Position-based Result Breakdown
One question in thread discourse structure classifica-
tion is how accurate the predictions are at different
depths in a thread (e.g. the first two posts vs. the sec-
ond two posts). A breakdown of results across posts
at different positions is presented in Figure 2.
The overall trend for both CRFSGD and Malt-
Parser is that it becomes increasingly hard to clas-
sify posts as we continue through a thread, due to
greater variability in discourse structure and greater
sparsity in the data. However, it is interesting to note
that the results for CRFSGD actually improve from
posts 7 and 8 ([7, 8]) to posts 9 and onwards ([9, ]).
To further investigate this effect, we performed class
decomposition over the joint classification predic-
tions, and performed a similar breakdown of posts
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Posts
F ?
 
 CRFSGDMaltParser
Figure 2: Breakdown of post-level Link-DA results for
CRFSGD and MaltParser based on post position
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed Link
 
 CRFSGDMaltParser
[1,2] [3,4] [5,6] [7,8] [9,] All0
0.5
1
Posts
F ?
Decomposed DA
 
 CRFSGDMaltParser
Figure 3: Breakdown of post-level Link and DA F-score
based on the decomposition of CRFSGD and Malt-
Parser classifications
for Link and DA; the results are presented in Fig-
ure 3. It is clear that the anomaly for CRFSGD
comes from the DA component, due to there being
greater predictability in the dialogue for final posts
in a thread (users tend to confirm a successful reso-
lution of the problem, or report on successful exter-
nal reproduction of the solution). MaltParser seems
less adept at identifying that a post is at the end
of a thread, and predicting the dialogue act accord-
ingly. This observation is congruous with the find-
ings of McDonald and Nivre (2007) that errors prop-
agate, due to MaltParser?s greedy inference strat-
egy. The higher results for Link are to be expected,
as throughout the thread, most posts tend to link lo-
cally.
20
XXXXXXXXXTest
B/down [1, 2] [1, 4] [1, 6] [1, 8] [All]
[1, 2] .947/.947 ? ? ? ?
[1, 4] .946/.947 .836/.841 ? ?
[1, 6] .946/.947 .840/.841 .800/.794 ? ?
[1, 8] .946/.947 .840/.841 .800/.794 .780/.769 ?
[All] .946/.946 .840/.838 .800/.791 .776/.767 .756/.738
Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of
different lengths (indicated in the rows), broken down over different post extents (indicated in the columns)
6.3 In Situ Structure Prediction
As described in Section 5.2, we simulate in situ
thread discourse structure prediction by removing
differing numbers of posts from the tail of the thread,
and applying the trained model over the resultant
sub-threads. The results for in situ classification are
presented in Table 5, with the rows indicating the
size of the test sub-thread, and the columns being a
breakdown of results over different portions of the
classified thread. The reason that we do not pro-
vide numbers for all cells in the table is that the size
of the test sub-thread determines the post extents we
can breakdown the results into, e.g. we cannot return
results for posts 1?4 ([1, 4]) when the size of the test
thread was only two posts ([1, 2]).
From the results, we can see that both CRFSGD
and MaltParser are very robust when applied to par-
tial threads, to the extent that we actually achieve
higher results over shortened versions of the thread
than over the complete thread in some instances, al-
though the only difference that is statistically signif-
icant is over [1, 8] for CRFSGD, where the predic-
tion over the partial thread is actually superior to that
over the complete thread. From this, we can con-
clude that it is possible to apply our method to partial
threads without any reduction in effectiveness rela-
tive to classification over complete threads. As such,
our method is shown to be robust when applied to
real-time analysis of dynamically evolving threads.
6.4 User profile feature analysis
In our experiments, we noticed that the user profile
feature (UserProf) is the most effective feature for
both CRFSGD and MaltParser. To gain a deeper
insight into the behaviour of the feature, we binned
the posts according to the number of times the author
had posted in the training data, evaluated based on a
Bin uscore Posts Total Totalper user users posts
High 224.6 251 1 251
Medium 1?41.7 4?48 45 395
Low 0 2?4 157 377
Very Low 0 1 309 309
Table 6: Statistics for the 4 groups of users
user score (uscore) for each user:
uscorei =
?ni
j=1 spi,j
ni
where ni is the number of posts by user i, and spi,j is
the number of posts by user i that occur as training
instances for other posts by the same author. uscore
reflects the average training?test post ratio per user
in cross-validation. Note that as we include all posts
from a given thread in a single partition during cross-
validation, it is possible for an author to have posted
4 times, but have a uscore of 0 due to those posts all
occurring in the same thread.
We ranked the users in the dataset in descending
order of uscore, sub-ranking on ni in cases of a tie
in uscore. The users were binned into 4 groups
of roughly equal post size. The detailed statistics
are shown in Table 6, noting that the high-frequency
bin (?High?) contains posts from a single user. We
present the post-level micro-averaged F-score for
posts in each bin based on CRFSGD, with and with-
out user profile features, in Figure 4.
Contrary to expectation, the UserProf features
have the greatest impact for users with fewer posts.
In fact, a statistically significant difference was ob-
served only for users with no posts in the training
data (uscore = 0), where the F-score jumped over
10% in absolute terms for both the Low and Very
Low bins. Our explanation for this effect is that the
21
High Median Low Very Low0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
User Group
F ?
 
 With UserProfWithout UserProf
Figure 4: Post-level joint classification results for users
binned by uscore, based on CRFSGD with and without
UserProf features)
lack of user profile information is predictive of the
sort of posts we can expect from a user (i.e. they
tend to be newbie users, asking questions).
7 Conclusions and Future Work
In this research, we explored the joint classification
of web user forum thread discourse structure, in the
form of a rooted directed acyclic graph over posts,
with edges labelled with dialogue acts. Three classi-
fication approaches were proposed: separately pre-
dicting Link and DA labels, and composing them
into a joint class; predicting a combined Link-DA
class using a structured classifier; and applying de-
pendency parsing to the problem. We found the
combined approach based on CRFSGD to perform
best over the task, closely followed by dependency
parsing with MaltParser.
We also examined the task of in situ classification
of dialogue structure, in the form of predicting the
discourse structure of partial threads, as contrasted
with classifying only complete threads. We found
that there was no drop in F-score over different sub-
extents of the thread in classifying partial threads,
despite the relative lack of thread context.
In future work, we plan to delve further into de-
pendency parsing, looking specifically at the impli-
cations of multi-headedness and disconnected sub-
graphs on dependency parsing. We also intend to
carry out meta-classification, combining the predic-
tions of CRFSGD and MaltParser.
Our user profile features were found to be the
pick of our features, but counter-intuitively, to bene-
fit users with no posts in the training data, rather than
prolific users. We wish to explore this effect further,
including incorporating unsupervised user-level fea-
tures into our classifiers.
Acknowledgements
The authors wish to acknowledge the development
efforts of Johan Hall in configuring MaltParser to
handle numeric features, and be able to parse thread
structures. NICTA is funded by the Australian gov-
ernment as represented by Department of Broad-
band, Communication and Digital Economy, and the
Australian Research Council through the ICT Centre
of Excellence programme.
References
Le?on Bottou. 2011. CRFSGD software. http://
leon.bottou.org/projects/sgd.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Informa-
tion and Knowledge Management (CIKM 2009), pages
265?274, Hong Kong, China.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?. In
Proceedings of 28th International ACM-SIGIR Con-
ference on Research and Development in Information
Retrieval (SIGIR 2005), pages 345?352.
Jeffrey Chan and Conor Hayes. 2010. Decomposing dis-
cussion forums using user roles. In Proceedings of the
WebSci10: Extending the Frontiers of Society On-Line
(WebSci10), pages 1?8, Raleigh, USA.
Jeffrey Chan, Conor Hayes, and Elizabeth M. Daly.
2010. Decomposing discussion forums using user
roles. In Proceedings of the Fourth International AAAI
Conference on Weblogs and Social Media (ICWSM
2010), pages 215?8, Washington, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP 2004), pages 309?316, Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In Song,
and Yueheng Sun. 2008. Finding question-answer
22
pairs from online forums. In Proceedings of 31st Inter-
national ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?08), pages
467?474, Singapore.
Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz.
2009. Joint learning of preposition senses and seman-
tic roles of prepositional phrases. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2009), pages 450?458,
Singapore. Association for Computational Linguistics.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract con-
text and answers of questions from online forums. In
Proceedings of the 46th Annual Meeting of the ACL:
HLT (ACL 2008), pages 710?718, Columbus, USA.
Jason Eisner and Noah A. Smith. 2005. Parsing with soft
and hard constraints on dependency length. In Pro-
ceedings of the Ninth International Workshop on Pars-
ing Technology, pages 30?41, Vancouver, Canada.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval (SIGIR?09), pages
714?715, Boston, USA.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?842,
Columbus, USA.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2009), pages 326?334, Boulder, Col-
orado. Association for Computational Linguistics.
Blaz Fortuna, Eduarda Mendes Rodrigues, and Natasa
Milic-Frayling. 2007. Improving the classification of
newsgroup messages through social network analysis.
In Proceedings of the 16th ACM Conference on In-
formation and Knowledge Management (CIKM 2007),
pages 877?880, Lisbon, Portugal.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010a. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2010), pages 862?871, Boston, USA.
Su Nam Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. In Proceedings
of the 14th Conference on Computational Natural Lan-
guage Learning (CoNLL-2010), pages 192?202, Upp-
sala, Sweden.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on Hu-
man Language Technologies, 2(1):1?127.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, pages 282?289, Williamstown, USA.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008.
The nature of requests and commitments in email mes-
sages. In Proceedings of the AAAI 2008 Workshop on
Enhanced Messaging, pages 42?47, Chicago, USA.
Robert Leaman, Laura Wojtulewicz, Ryan Sullivan, An-
nie Skariah, Jian Yang, and Graciela Gonzalez. 2010.
Towards internet-age pharmacovigilance: Extracting
adverse drug reactions from user posts in health-
related social networks. In Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing (ACL 2010), pages 117?125, Uppsala, Sweden.
Oliver Lemon, Alex Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL), Special Issue on Dialogue, 43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling semantics
and structure of discussion threads. In Proceedings of
the 18th International Conference on the World Wide
Web (WWW 2009), pages 1103?1104, Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse. In
Proceedings of the 14th Australasian Document Com-
puting Symposium (ADCS 2009), Sydney, Australia.
Marco Lui and Timothy Baldwin. 2010. Classifying
user forum participants: Separating the gurus from the
hacks, and other tales of the internet. In Proceedings
of the 2010 Australasian Language Technology Work-
shop (ALTW 2010), pages 49?57, Melbourne, Aus-
tralia.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency parsing
models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL 2007), pages 122?131, Prague,
Czech Republic.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of the 11th Conference of
23
the European Chapter of the Association for Computa-
tional Linguistics (EACL 2006), pages 81?88, Trento,
Italy.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, Canada.
Gabriel Murray, Steve Renals, Jean Carletta, and Johanna
Moore. 2006. Incorporating speaker and discourse
features into speech summarization. In Proceedings
of the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 367?
374.
Klemens Muthmann, Wojciech M. Barczyn?ski, Falk
Brauer, and Alexander Lo?ser. 2009. Near-duplicate
detection for web-forums. In Proceedings of the 2009
International Database Engineering & Applications
Symposium (IDEAS 2009), pages 142?151, Cetraro,
Italy.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(02):95?135.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149?160, Nancy, France.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop Incremental Parsing: Bringing Engineer-
ing and Cognition Together (ACL-2004), pages 50?57,
Barcelona, Spain.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995. Discourse
processing of dialogues with multiple threads. In Pro-
ceedings of the 33rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 31?38,
Cambridge, USA.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), pages 753?760, Manchester,
UK.
Kenji Sagae. 2009. Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International Con-
ference on Parsing Technologies (IWPT-09), pages 81?
84, Paris, France.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in comments
on news-articles. In Proceedings of the 9th Annual
ACM International Workshop on Web Information and
Data Management, pages 97?104, Lisboa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, USA.
Parikshit Sondhi, Manish Gupta, ChengXiang Zhai, and
Julia Hockenmaier. 2010. Shallow information ex-
traction from medical forum data. In Proceedings of
the 23rd International Conference on Computational
Linguistics (COLING 2010), Posters Volume, pages
1158?1166, Beijing, China.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics (HLT-NAACL 2003), pages 149?156, Edmonton,
Canada.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Pail
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Charles Sutton and Andrew McCallum. 2005. Joint
parsing and semantic role labeling. In Proceedings of
the Ninth Conference on Computational Natural Lan-
guage Learning (CoNLL-2005), pages 225?228, Ann
Arbor, Michigan. Association for Computational Lin-
guistics.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of the
Fifth Conference on Applied Natural Language Pro-
cessing, pages 64?71, Washington, USA.
Nayer Wanas, Motaz El-Saban, Heba Ashour, and
Waleed Ammar. 2008. Automatic scoring of online
discussion posts. In Proceeding of the 2nd ACM work-
shop on Information credibility on the web (WICOW
?08), pages 19?26, Napa Valley, USA.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Mak-
ing conversational structure explicit: identification of
initiation-response pairs within online discussions. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL HLT
2010), pages 673?676.
24
Yi-Chia Wang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions
(ACL 2007), pages 73?76, Prague, Czech Republic.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages 152?
160, Seattle, USA.
V. Warnke, R. Kompe, H. Niemann, and E. No?th. 1997.
Integrated dialog act segmentation and classification
using prosodic features and language models. In Proc.
Eurospeech, volume 1, pages 207?210.
Markus Weimer and Iryna Gurevych. 2007. Predicting
the perceived quality of web forum posts. In Proceed-
ings of the 2007 International Conference on Recent
Advances in Natural Language Processing (RANLP
2007), pages 643?648, Borovets, Bulgaria.
Markus Weimer, Iryna Gurevych, and Max Mu?hlha?user.
2007. Automatically assessing the post quality in on-
line discussions on software. In Proceedings of the
45th Annual Meeting of the ACL: Interactive Poster
and Demonstration Sessions, pages 125?128, Prague,
Czech Republic.
Armin Weinberger and Frank Fischer. 2006. A
framework to analyze argumentative knowledge con-
struction in computer-supported collaborative learn-
ing. Computers & Education, 46:71?95, January.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search. In
Proceedings of 27th International ACM-SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR 2004), pages 394?401. Sheffield,
UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 947?953,
Saarbru?cken, Germany.
25
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 648?658,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Large-Scale Noun Compound Interpretation
Using Bootstrapping and the Web as a Corpus
Su Nam Kim
Computer Science & Software Engineering
University of Melbourne
Melbourne, VIC 3010
Australia
snkim@csse.unimelb.edu.au
Preslav Nakov
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nakov@comp.nus.edu.sg
Abstract
Responding to the need for semantic lexical
resources in natural language processing ap-
plications, we examine methods to acquire
noun compounds (NCs), e.g., orange juice, to-
gether with suitable fine-grained semantic in-
terpretations, e.g., squeezed from, which are
directly usable as paraphrases. We employ
bootstrapping and web statistics, and utilize
the relationship between NCs and paraphras-
ing patterns to jointly extract NCs and such
patterns in multiple alternating iterations. In
evaluation, we found that having one com-
pound noun fixed yields both a higher number
of semantically interpreted NCs and improved
accuracy due to stronger semantic restrictions.
1 Introduction
Noun compounds (NCs) such as malaria mosquito
and colon cancer tumor suppressor protein are chal-
lenging for text processing since the relationship
between the nouns they are composed of is im-
plicit. NCs are abundant in English and understand-
ing their semantics is important in many natural lan-
guage processing (NLP) applications. For example,
a question answering system might need to know
whether protein acting as a tumor suppressor is a
good paraphrase for tumor suppressor protein. Sim-
ilarly, a machine translation system facing the un-
known noun compound Geneva headquarters might
translate it better if it could first paraphrase it as
Geneva headquarters of the WTO. Given a query
for ?migraine treatment?, an information retrieval
system could use paraphrasing verbs like relieve and
prevent for query expansion and result ranking.
Most work on noun compound interpretation has
focused on two-word NCs. There have been two
general lines of research: the first one derives the NC
semantics from the semantics of the nouns it is made
of (Rosario and Hearst, 2002; Moldovan et al, 2004;
Kim and Baldwin, 2005; Girju, 2007; Se?aghdha,
2009; Tratz and Hovy, 2010), while the second one
models the relationship between the nouns directly
(Vanderwende, 1994; Lapata, 2002; Kim and Bald-
win, 2006; Nakov and Hearst, 2006; Nakov and
Hearst, 2008; Butnariu and Veale, 2008).
In either case, the semantics of an NC is typi-
cally expressed by an abstract relation like CAUSE
(e.g., malaria mosquito), SOURCE (e.g., olive oil),
or PURPOSE (e.g., migraine drug), coming from a
small fixed inventory. Some researchers however,
have argued for a more fine-grained, even infinite,
inventory (Finin, 1980). Verbs are particularly use-
ful in this respect and can capture elements of the
semantics that the abstract relations cannot. For ex-
ample, while most NCs expressing MAKE, can be
paraphrased by common patterns like be made of
and be composed of, some NCs allow more specific
patterns, e.g., be squeezed from for orange juice, and
be topped with for bacon pizza.
Recently, the idea of using fine-grained para-
phrasing verbs for NC semantics has been gain-
ing popularity (Butnariu and Veale, 2008; Nakov,
2008b); there has also been a related shared task at
SemEval-2010 (Butnariu et al, 2010). This interest
is partly driven by practicality: verbs are directly us-
able as paraphrases. Still, abstract relations remain
dominant since they offer a more natural generaliza-
tion, which is useful for many NLP applications.
648
One good contribution to this debate would be a
direct study of the relationship between fine-grained
and coarse-grained relations for NC interpretation.
Unfortunately, the existing datasets do not allow
this since they are tied to one particular granular-
ity; moreover, they only contain a few hundred NCs.
Thus, our objective is to build a large-scale dataset
of hundreds of thousands of NCs, each interpreted
(1) by an abstract semantic relation and (2) by a set
of paraphrasing verbs. Having such a large dataset
would also help the overall advancement of the field.
Since there is no universally accepted abstract re-
lation inventory in NLP, and since we are interested
in NC semantics from both a theoretical and a prac-
tical viewpoint, we chose the set of abstract relations
proposed in the theory of Levi (1978), which is dom-
inant in theoretical linguistics and has been also used
in NLP (Nakov and Hearst, 2008).
We use a two-step algorithm to jointly harvest
NCs and patterns (verbs and prepositions) that in-
terpret them for a given abstract relation. First,
we extract NCs using a small number of seed pat-
terns from a given abstract relation. Then, using
the extracted NCs, we harvest more patterns. This
is repeated until no new NCs and patterns can be
extracted or for a pre-specified number of itera-
tions. Our approach combines pattern-based extrac-
tion and bootstrapping, which is novel for NC in-
terpretation; however, such combinations have been
used in other areas, e.g., named entity recognition
(Riloff and Jones, 1999; Thelen and Riloff, 2002;
Curran et al, 2007; McIntosh and Curran, 2009).
The remainder of the paper is organized as fol-
lows: Section 2 gives an overview of related work,
Section 3 motivates our semantic representation,
Sections 4, 5, and 6 explain our method, dataset and
experiments, respectively, Section 7 discusses the
results, Section 8 provides error analysis, and Sec-
tion 9 concludes with suggestions for future work.
2 Related Work
As we mentioned above, the implicit relation be-
tween the two nouns forming a noun compound can
often be expressed overtly using verbal and prepo-
sitional paraphrases. For example, student loan is
?loan given to a student?, while morning tea can be
paraphrased as ?tea in the morning?.
Thus, many NLP approaches to NC semantics
have used verbs and prepositions as a fine-grained
semantic representation or as features when pre-
dicting coarse-grained abstract relations. For ex-
ample, Vanderwende (1994) associated verbs ex-
tracted from definitions in an online dictionary with
abstract relations. Lauer (1995) expressed NC se-
mantics using eight prepositions. Kim and Baldwin
(2006) predicted abstract relations using verbs as
features. Nakov and Hearst (2008) proposed a fine-
grained NC interpretation using a distribution over
Web-derived verbs, prepositions and coordinating
conjunctions; they also used this distribution to pre-
dict coarse-grained abstract relations. Butnariu and
Veale (2008) adopted a similar fine-grained verb-
centered approach to NC semantics. Using a dis-
tribution over verbs as a semantic interpretation was
also carried out in a recent challenge: SemEval-2010
Task 9 (Butnariu et al, 2009; Butnariu et al, 2010).
In noun compound interpretation, verbs and
prepositions can be seen as patterns connecting the
two nouns in a paraphrase. Similar pattern-based ap-
proaches have been popular in information extrac-
tion and ontology learning. For example, Hearst
(1992) extracted hyponyms using patterns such as
X, Y, and/or other Zs, where Z is a hypernym of
X and Y. Berland and Charniak (1999) used sim-
ilar patterns to extract meronymy (part-whole) re-
lations, e.g., parts/NNS of/IN wholes/NNS matches
basements of buildings. Unfortunately, matches are
rare, which makes it difficult to build large semantic
inventories. In order to overcome data sparseness,
pattern-based approaches are often combined with
bootstrapping. For example, Riloff and Jones (1999)
used a multi-level bootstrapping algorithm to learn
both a semantic lexicon and extraction patterns, e.g.,
owned by X extracts COMPANY and facilities in X
extracts LOCATION. That is, they learned seman-
tic lexicons using extraction patterns, and then, al-
ternatively, they extracted new patterns using these
lexicons. They also introduced a second level of
bootstrapping to retain the most reliable examples
only. While the method enables the extraction of
large lexicons, its quality degrades rapidly, which
makes it impossible to run for too many iterations.
Recently, Curran et al (2007) and McIntosh and
Curran (2009) proposed ways to control degradation
using simultaneous learning and weighting.
649
Bootstrapping has been applied to noun com-
pound extraction as well. For example, Kim and
Baldwin (2007) used it to produce a large number
of semantically interpreted noun compounds from
a small number of seeds. In each iteration, the
method replaced one component of an NC with its
synonyms, hypernyms and hyponyms to generate a
new NC. These new NCs were further filtered based
on their semantic similarity with the original NC.
While the method acquired a large number of noun
compounds without significant semantic drifting, its
accuracy degraded rapidly after each iteration. More
importantly, the variation of the sense pairs was lim-
ited since new NCs had to be semantically similar to
the original NCs.
Recently, Kozareva and Hovy (2010) combined
patterns and bootstrapping to learn the selectional
restrictions for various semantic relations. They
used patterns involving the coordinating conjunction
and, e.g., ?* and John fly to *?, and learned argu-
ments such as Mary/Tom and France/New York. Un-
like in NC interpretation, it is not necessary for their
arguments to form an NC, e.g., Mary France and
France Mary are not NCs. Rather, they were in-
terested in building a semantic ontology with a pre-
defined set of semantic relations, similar to YAGO
(Suchanek et al, 2007), where the pattern work for
would have arguments like a company/UNICEF.
3 Semantic Representation
Inspired by (Finin, 1980), Nakov and Hearst (2006)
and (Nakov, 2008b) proposed that NC semantics is
best expressible using paraphrases involving verbs
and/or prepositions. For example, bronze statue is
a statue that is made of, is composed of, consists of,
contains, is of, is, is handcrafted from, is dipped in,
looks like bronze. They further proposed that se-
lecting one such paraphrase is not enough and that
multiple paraphrases are needed for a fine-grained
representation. Finally, they observed that not all
paraphrases are equally good (e.g., is made of is
arguably better than looks like or is dipped in for
MAKE), and thus proposed that the semantics of a
noun compound should be expressed as a distribu-
tion over multiple possible paraphrases. This line of
research was later adopted by SemEval-2010 Task 9
(Butnariu et al, 2010).
It easily follows that the semantics of abstract re-
lations such as MAKE that can hold between the
nouns in an NC can be represented in the same way:
as a distribution over paraphrasing verbs and prepo-
sitions. Note, however, that some NCs are para-
phrasable by more specific verbs that do not nec-
essarily support the target abstract relation. For ex-
ample, malaria mosquito, which expresses CAUSE,
can be paraphrased using verbs like carry, which do
not imply direct causation. Thus, while we will be
focusing on extracting NCs for a particular abstract
relation, we are interested in building semantic rep-
resentations that are specific for these NCs and do
not necessarily apply to all instances of that relation.
Traditionally, the semantics of a noun compound
have been represented as an abstract relation drawn
from a small closed set. Unfortunately, no such set is
universally accepted, and mapping between sets has
proven challenging (Girju et al, 2005). Moreover,
being both abstract and limited, such sets capture
only part of the semantics; often multiple meanings
are possible, and sometimes none of the pre-defined
ones suits a given example. Finally, it is unclear
how useful these sets are since researchers have of-
ten fallen short of demonstrating practical uses.
Arguably, verbs have more expressive power and
are more suitable for semantic representation: there
is an infinite number of them (Downing, 1977), and
they can capture fine-grained aspects of the mean-
ing. For example, while both wrinkle treatment and
migraine treatment express the same abstract rela-
tion TREATMENT-FOR-DISEASE, fine-grained dif-
ferences can be revealed using verbs, e.g., smooth
can paraphrase the former, but not the latter.
In many theories, verbs play an important role in
NC derivation (Levi, 1978). Moreover, speakers of-
ten use verbs to make the hidden relation between
the noun in a noun compound overt. This allows for
simple extraction and for straightforward use in NLP
tasks like textual entailment (Tatu and Moldovan,
2005) and machine translation (Nakov, 2008a).
Finally, a single verb is often not enough, and
the meaning is better approximated by a collection
of verbs. For example, while malaria mosquito ex-
presses CAUSE (and is paraphrasable using cause),
further aspects of the meaning can be captured with
more verbs, e.g., carry, spread, be responsible for,
be infected with, transmit, pass on, etc.
650
4 Method
We harvest noun compounds expressing some target
abstract semantic relation (in the experiments below,
this is Levi?s MAKE2), starting from a small number
of initial seed patterns: paraphrasing verbs and/or
prepositions. Optionally, we might also be given
a small number of noun compounds that instanti-
ate the target abstract relation. We then learn more
noun compounds and patterns for the relation by al-
ternating between the following two bootstrapping
steps, using the Web as a corpus. First, we extract
more noun compounds that are paraphrasable with
the available patterns (see Section 4.1). We then
look for new patterns that can paraphrase the newly-
extracted noun compounds (see Section 4.2). These
two steps are repeated until no new noun compounds
can be extracted or until a pre-determined number of
iterations has been reached. A schematic description
of the algorithm is shown in Figure 1.
(+ H/M of NCs)Patterns
Query Generation
NC Extraction
Pattern
Filtering
Rules
Filtering
NC
Rules
repeat
collected NCs^
Query Generation
w/ NCs^
collected Patterns
stop
if newNCs = 0
or
Iteration limit exceeded
Snippet by Yahoo!
Pattern Extraction
Snippet by Yahoo!
Figure 1: Our bootstrapping algorithm.
4.1 Bootstrapping Step 1: Noun Compound
Extraction
Given a list of patterns (verbs and/or prepositions),
we mine the Web to extract noun compounds that
match these patterns. We experiment with the fol-
lowing three bootstrapping strategies for this step:
? Loose bootstrapping uses the available pat-
terns and imposes no further restrictions.
? Strict bootstrapping requires that, in addition
to the patterns themselves, some noun com-
pounds matching each pattern be made avail-
able as well. A pattern is only instantiated in
the context of either the head or the modifier of
a noun compound that is known to match it.
? NC-only strict bootstrapping is a stricter ver-
sion of strict bootstrapping, where the list of
patterns is limited to the initial seeds.
Below we describe each of the sub-steps of the NC
extraction process: query generation, snippet har-
vesting, and noun compound acquisition & filtering.
4.1.1 Query Generation
We generate generalized exact-phrase queries to
be used in a Web search engine (we use Yahoo!):
"* that PATTERN *" (loose)
"HEAD that PATTERN *" (strict)
"* that PATTERN MOD" (strict)
where PATTERN is an inflected form of a verb, MOD
and HEAD are inflected forms the modifier and the
head of a noun compound that is paraphrasable by
the pattern, that is the word that, and * is the
search engine?s star operator.
We use the first pattern for loose bootstrapping
and the other two for both strict bootstrapping and
NC-only strict bootstrapping.
Note that the above queries are generalizations of
the actual queries we use against the search engine.
In order to instantiate these generalizations, we fur-
ther generate the possible inflections for the verbs
and the nouns involved. For nouns, we produce sin-
gular and plural forms, while for verbs, we vary not
only the number (singular and plural), but also the
tense (we allow present, past, and present perfect).
When inflecting verbs, we distinguish between ac-
tive verb forms like consist of and passive ones like
be made from and we treat them accordingly. Over-
all, in the case of loose bootstrapping, we generate
about 14 and 20 queries per pattern for active and
passive patterns, respectively, while for strict boot-
strapping and NC-only strict bootstrapping, the in-
stantiations yield about 28 and 40 queries for active
and passive patterns, respectively.
651
For example, given the seed be made of, we could
generate "* that were made of *". If we
are further given the NC orange juice, we could also
produce "juice that was made of *" and
"* that is made of oranges".
4.1.2 Snippet Extraction
We execute the above-described instantiations of
the generalized queries against a search engine as
exact phrase queries, and, for each one, we collect
the snippets for the top 1,000 returned results.
4.1.3 NC Extraction and Filtering
Next, we process the snippets returned by the
search engine and we acquire potential noun com-
pounds from them. Then, in each snippet, we look
for an instantiation of the pattern used in the query
and we try to extract suitable noun(s) that occupy the
position(s) of the *.
For loose bootstrapping, we extract two nouns,
one from each end of the matched pattern, while
for strict bootstrapping and for NC-only strict boot-
strapping, we only extract one noun, either preced-
ing or following the pattern, since the other noun
is already fixed. We then lemmatize the extracted
noun(s) and we form NC candidates from the two
arguments of the instantiated pattern, taking into ac-
count whether the pattern is active or passive.
Due to the vast number of snippets we have to
process, we decided not to use a syntactic parser or a
part-of-speech (POS) tagger1; thus, we use heuristic
rules instead. We extract ?phrases? using simple in-
dicators such as punctuation (e.g., comma, period),
coordinating conjunctions2 (e.g., and, or), preposi-
tions (e.g., at, of, from), subordinating conjunctions
(e.g., because, since, although), and relative pro-
nouns (e.g., that, which, who). We then extract the
nouns from these phrases, we lemmatize them using
WordNet, and we form a list of NC candidates.
While the above heuristics work reasonably well
in practice, we perform some further filtering, re-
moving all NC candidates for which one or more of
the following conditions are met:
1In fact, POS taggers and parsers are unreliable for Web-
derived snippets, which often represent parts of sentences and
contain errors in spelling, capitalization and punctuation.
2Note that filtering the arguments using such indicators indi-
rectly subsumes the pattern "X PATTERN Y and" proposed
in (Kozareva and Hovy, 2010).
1. the candidate NC is one of the seed examples
or has been extracted on a previous iteration;
2. the head and the modifier are the same;
3. the head or the modifier are not both listed as
nouns in WordNet (Fellbaum, 1998);
4. the candidate NC occurs less than 100 times in
the Google Web 1T 5-gram corpus3;
5. the NC is extracted less than N times (we tried
5 and 10) in the context of the pattern for all
instantiations of the pattern.
4.2 Bootstrapping Step 2: Pattern Extraction
This is the second step of our bootstrapping algo-
rithm as shown on Figure 1. Given a list of noun
compounds, we mine the Web to extract patterns:
verbs and/or prepositions that can paraphrase each
NC. The idea is to turn the NC?s pre-modifier into
a post-modifying relative clause and to collect the
verbs and prepositions that are used in such clauses.
Below we describe each of the sub-steps of the NC
extraction process: query generation, snippet har-
vesting, and NC extraction & filtering.
4.2.1 Query Generation
The process of extraction starts with exact-phrase
queries issued against a Web search engine (again
Yahoo!) using the following generalized pattern:
"HEAD THAT? * MOD"
where MOD and HEAD are inflected forms of NC?s
modifier and head, respectively, THAT? stands for
that, which, who or the empty string, and * stands
for 1-6 instances of search engine?s star operator.
For example, given orange juice, we could gen-
erate queries like "juice that * oranges",
"juices which * * * * * * oranges",
and "juices * * * orange".
4.2.2 Snippet Extraction
The same as in Section 4.1.2 above.
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
652
4.2.3 Pattern Extraction and Filtering
We split the extracted snippets into sentences, and
filter out all incomplete ones and those that do not
contain (a possibly inflected version of) the target
nouns. We further make sure that the word sequence
following the second mentioned target noun is non-
empty and contains at least one non-noun, thus en-
suring the snippet includes the entire noun phrase.
We then perform shallow parsing, and we extract all
verb forms, and the following preposition, between
the target nouns. We allow for adjectives and partici-
ples to fall between the verb and the preposition but
not nouns; we further ignore modal verbs and aux-
iliaries, but we retain the passive be, and we make
sure there is exactly one verb phrase between the tar-
get nouns. Finally, we lemmatize the verbs to form
the patterns candidates, and we apply the following
pattern selection rules:
1. we filter out all patterns that were provided as
initial seeds or were extracted previously;
2. we select the top 20 most frequent patterns;
3. we filter out all patterns that were extracted less
than N times (we tried 5 and 10) and with less
thanM NCs per pattern (we tried 20 and 50).
5 Target Relation and Seed Examples
As we mentioned above, we use the inventory of
abstract relations proposed in the popular theoreti-
cal linguistics theory of Levi (1978). In this theory,
noun compounds are derived from underlying rel-
ative clauses or noun phrase complement construc-
tions by means of two general processes: predicate
deletion and predicate nominalization. Given a two-
argument predicate, predicate deletion removes that
predicate, but retains its arguments to form an NC,
e.g., pie made of apples ? apple pie. In contrast,
predicate nominalization creates an NC whose head
is a nominalization of the underlying predicate and
whose modifier is either the subject or the object of
that predicate, e.g., The President refused General
MacArthur?s request. ? presidential refusal.
According to Levi, predicate deletion can be ap-
plied to abstract predicates, whose semantics can be
roughly approximated using five paraphrasing verbs
(CAUSE, HAVE, MAKE, USE, and BE) and four
prepositions (IN, FOR, FROM, and ABOUT).
Typically, in predicate deletion, the modifier is
derived from the object of the underlying relative
clause; however, the first three verbs also allow for
it to be derived from the subject. Levi expresses the
distinction using indexes. For example, music box is
MAKE1 (object-derived), i.e., the box makes music,
while chocolate bar is MAKE2 (subject-derived),
i.e., the bar is made of chocolate (note the passive).
Due to time constraints, we focused on one re-
lation of Levi?s, MAKE2, which is among the most
frequent relations an NC can express and is present
in some form in many relation inventories (Warren,
1978; Barker and Szpakowicz, 1998; Rosario and
Hearst, 2001; Nastase and Szpakowicz, 2003; Girju
et al, 2005; Girju et al, 2007; Girju et al, 2009;
Hendrickx et al, 2010; Tratz and Hovy, 2010).
In Levi?s theory, MAKE2 means that the head of
the noun compound is made up of or is a product of
its modifier. There are three subtypes of this relation
(we do not attempt to distinguish between them):
(a) the modifier is a unit and the head is a configu-
ration, e.g., root system;
(b) the modifier represents a material and the head
is a mass or an artefact, e.g., chocolate bar;
(c) the head represents human collectives and
the modifier specifies their membership, e.g.,
worker teams.
There are 20 instances of MAKE2 in the appendix
of (Levi, 1978), and we use them all as seed NCs.
As seed patterns, we use a subset of the human-
proposed paraphrasing verbs and prepositions cor-
responding to these 20 NCs in the dataset in (Nakov,
2008b), where each NC is paraphrased by 25-30 an-
notators. For example, for chocolate bar, we find
the following list of verbs (the number of annotators
who proposed each verb is shown in parentheses):
be made of (16), contain (16), be made from
(10), be composed of (7), taste like (7), con-
sist of (5), be (3), have (2), melt into (2), be
manufactured from (2), be formed from (2),
smell of (2), be flavored with (1), sell (1), taste
of (1), be constituted by (1), incorporate (1),
serve (1), contain (1), store (1), be made with
(1), be solidified from (1), be created from (1),
be flavoured with (1), be comprised of (1).
653
Seed NCs: bronze statue, cable network, candy cigarette, chocolate bar, concrete desert, copper coin, daisy chain, glass eye,
immigrant minority, mountain range, paper money, plastic toy, sand dune, steel helmet, stone tool, student committee,
sugar cube, warrior castle, water drop, worker team
Seed patterns: be composed of, be comprised of, be inhabited by, be lived in by, be made from, be made of, be made up of,
be manufactured from, be printed on, consist of, contain, have, house, include, involve, look like, resemble, taste like
Table 1: Our seed examples: 20 noun compounds and 18 verb patterns.
As we can see, the most frequent patterns are of
highest quality, e.g., be made of (16), while the less
frequent ones can be wrong, e.g., serve (1). There-
fore, we filtered out all verbs that were proposed less
than five times with the 20 seed NCs. We further re-
moved the verb be, which is too general, thus ending
up with 18 seed patterns. Note that some patterns
can paraphrase multiple NCs: the total number of
seed NC-pattern pairs is 84.
The seed NCs and patterns are shown in Table 1.
While some patterns, e.g., taste like do not express
the target relation MAKE2, we kept them anyway
since they were proposed by several human anno-
tators and since they do express the fine-grained se-
mantics of some particular instances of that relation;
thus, we thought they might be useful, even for the
general relation. For example, taste like has been
proposed 8 times for candy cigarette, 7 times for
chocolate bar, and 2 times for sugar cube, and thus
it clearly correlates well with some seed examples,
even if it does not express MAKE2 in general.
6 Experiments and Evaluation
Using the NCs and patterns in Table 1 as initial
seeds, we ran our algorithm for three iterations of
loose bootstrapping and strict bootstrapping, and
for two iterations of NC-only strict bootstrapping.
We only performed up to three iterations because
of the huge number of noun compounds extracted
for NC-only strict bootstrapping (which we only ran
for two iterations) and because of the low number of
new NCs extracted by loose bootstrapping on itera-
tion 3. While we could have run strict bootstrapping
for more iterations, we opted for a comparable num-
ber of iterations for all three methods.
Examples of noun compounds that we have ex-
tracted are bronze bell (be made of, be made from)
and child team (be composed of, include). Exam-
ple patterns are be filled with (cotton bag, water cup)
and use (water sculpture, wood statue).
Limits Extracted & Retained
(see 4.2.3) NCs Patterns Patt.+NC
Loose Bootstrapping
N=5,M=50 1,662 / 61.67 12 / 65.83 1,337
N=10,M=20 590 / 61.52 9 / 65.56 316
Strict Bootstrapping
N=5,M=50 25,375 / 67.42 16 / 71.43 9,760
N=10,M=20 16,090 / 68.27 16 / 78.98 5,026
NC-only Strict Bootstrapping
N=5 205,459 / 69.59 ? ?
N=10 100,550 / 70.43 ? ?
Table 2: Total number and accuracy in % for NCs, pat-
terns and NC-pattern pairs extracted and retained for each
of the three methods over all iterations.
Tables 2 and 3 show the overall results. As we
mentioned in section 4.2.3, at each iteration, we fil-
tered out all patterns that were extracted less thanN
times or with less than M NCs. Note that we only
used the 10 most frequent NCs per pattern as NC
seeds for NC extraction in the next iteration of strict
bootstrapping and NC-only strict bootstrapping. Ta-
ble 3 shows the results for two value combinations
of (N ;M ): (5;50) and (10;20). Note also that if
some NC was extracted by several different patterns,
it was only counted once. Patterns are subject to
particular NCs, and thus we show (1) the number
of patterns extracted with all NCs, i.e., unique NC-
pattern pairs, (2) the accuracy of these pairs,4 and
(3) the number of unique patterns retained after fil-
tering, which will be used to extract new noun com-
pounds on the second step of the current iteration.
4One of the reviewers suggested that evaluating the accuracy
of NC-pattern pairs could potentially conceal some of the drift
of our algorithm. For example, while water cup / be filled with
is a correct NC-pattern pair, water cup is incorrect for MAKE2;
it is probably an instance of Levi?s FOR. Thus, the same boot-
strapping technique evaluated against a fixed set of semantic re-
lations (which is the more traditional approach) could arguably
show bootstrapping going ?off the rails? more quickly than what
we observe here. However, our goal, as stated in Section 3, is to
find NC-specific paraphrases, and our evaluation methodology
is more adequate with respect to this goal.
654
Limits Seeds Iteration 1 Iteration 2 Iteration 3
(see 4.2.3) Patt. NCs Patt. NCs Patterns NCs Patterns NCs
Loose Bootstrapping
N=5,M=50 ? 18 ? 1,144 / 63.11 1,136 / 64.44 / 9 390 / 58.72 201 / 70.00 / 3 128 / 57.03
N=10,M=20 ? 18 ? 502 / 61.55 294 / 62.50 / 8 78 / 60.26 22 / 90.00 / 1 10 / 70.00
Strict Bootstrapping
N=5,M=50 20 18 ? 7,011 / 70.65 5,312 / 74.00 / 10 11,214 / 67.15 4,448 / 60.00 / 6 7,150 / 64.69
N=10,M=20 20 18 ? 4,826 / 71.26 2,838 / 79.38 / 10 7,371 / 67.26 2,188 / 78.33 / 6 3,893 / 66.48
NC-only Strict Bootstrapping
N=5 20 18 ? 7,011 / 70.65 ? 198,448 / 69.55 ? ?
N=10 20 18 ? 4,826 / 71.26 ? 95,524 / 70.59 ? ?
Table 3: Evaluation results for up to three iterations. For NCs, we show the number of unique NCs extracted and
their accuracy in %. For patterns, we show the number of unique NC-pattern pairs extracted, their accuracy in %, and
the number of unique patterns retained and used to extract NCs on the second step of the current iteration. The first
column shows the pattern filtering thresholds used (see Section 4.2.3 for details).
The above accuracies were calculated based on
human judgments by an experienced, well-trained
annotator. We also hired a second annotator for a
small subset of the examples.
For NCs, the first annotator judged whether each
NC is an instance of MAKE2. All NCs were judged,
except for iteration 2 of NC-only strict bootstrap-
ping, where their number was prohibitively high and
only the most frequent noun compounds extracted
for each modifier and for each head were checked:
9,004 NCs for N=5 and 4,262 NCs for N=10.
For patterns, our first annotator judged the cor-
rectness of the unique NC-pattern pairs, i.e., whether
the NC is paraphrasable with the target pattern.
Given the large number of NC-pattern pairs, the an-
notator only judged patterns with their top 10 most
frequent NCs. For example, if there were 5 patterns
extracted, then the NC-pattern pairs to be judged
would be no more than 5 ? 10 = 50.
Our second annotator judged 340 random exam-
ples: 100 NCs and 20 patterns with their top 10 NCs
for each iteration. The Cohen?s kappa (Cohen, 1960)
between the two annotators is .66 (85% initial agree-
ment), which corresponds to substantial agreement
(Landis and Koch, 1977).
7 Discussion
Tables 2 and 3 show that fixing one of the two nouns
in the pattern, as in strict bootstrapping and NC-only
strict bootstrapping, yields significantly higher ac-
curacy (?2 test) for both NC and NC-pattern pair
extraction compared to loose bootstrapping.
The accuracy for NC-only strict bootstrapping is
a bit higher than for strict bootstrapping, but the ac-
tual differences are probably smaller since the eval-
uation of the former on iteration 2 was done for the
most frequent NCs, which are more accurate.
Note that the number of extracted NCs is much
higher with the strict methods because of the higher
number of possible instantiations of the generalized
query patterns. For NC-only strict bootstrapping,
the number of extracted NCs grows exponentially
since the number of patterns does not diminish as
in the other two methods. The number of extracted
patterns is similar for the different methods since we
select no more than 20 of them per iteration.
Overall, the accuracy for all methods decreases
from one iteration to the next since errors accumu-
late; still, the degradation is slow. Note also the ex-
ception of loose bootstrapping on iteration 3.
Comparing the results for N=5 and N=10, we
can see that, for all three methods, using the latter
yields a sizable drop in the number of extracted NCs
and NC-pattern pairs; it also tends to yield a slightly
improved accuracy. Note, however, the exception
of loose bootstrapping for the first two iterations,
where the less restrictive N=5 is more accurate.
As a comparison, we implemented the method
of Kim and Baldwin (2007), which generates new
semantically interpreted NCs by replacing either
the head or the modifier of a seed NC with suit-
able synonyms, hypernyms and sister words from
WordNet, followed by similarity filtering using
WordNet::Similarity (Pedersen et al, 2004).
655
Rep. Iter. 1 Iter. 2 Iter. 3 All
Syn. 11/81.81 3/66.67 0 14/78.57
Hyp. 27/85.19 35/77.14 33/66.67 95/75.79
Sis. 381/82.05 1,736/69.33 17/52.94 2,134/75.12
All 419/82.58 1,774/71.68 50/62.00 2,243/75.47
Table 4: Number of extracted noun compounds and ac-
curacy in % for the method of Kim and Baldwin (2007).
The abbreviations Syn., Hyp., and Sis. indicate using syn-
onyms, hypernyms, and sister words, respectively.
The results for three bootstrapping iterations us-
ing the same list of 20 initial seed NCs as in our pre-
vious experiments, are shown in Table 4. We can see
that the overall accuracy of their method is slightly
better than ours. Note, however, that our method ac-
quired a much larger number of NCs, while allow-
ing more variety in the NC semantics. Moreover, for
each extracted noun compound, we also generated a
list of fine-grained paraphrasing verbs.
8 Error Analysis
Below we analyze the errors of our method.
Many problems were due to wrong POS assign-
ment. For example, on Step 2, because of the omis-
sion of that in ?the statue has such high quality gold
(that) demand is ...?, demand was tagged as a noun
and thus extracted as an NCmodifier instead of gold.
The problem also arose on Step 1, where we used
WordNet to check whether the NC candidates were
composed of two nouns. Since words like clear,
friendly, and single are listed in WordNet as nouns
(which is possible in some contexts), we extracted
wrong NCs such as clear cube, friendly team, and
single chain. There were similar issues with verb-
particle constructions since some particles can be
used as nouns as well, e.g., give back, break down.
Some errors were due to semantic transparency
issues, where the syntactic and the semantic head of
a target NP were mismatched (Fillmore et al, 2002;
Fontenelle, 1999). For example, from the sentence
?This wine is made from a range of white grapes.?,
we would extract range rather than grapes as the po-
tential modifier of wine.
In some cases, the NC-pattern pair was correct,
but the NC did not express the target relation, e.g.,
while contain is a good paraphrase for toy box, the
noun compound itself is not an instance of MAKE2.
There were also cases where the pair of extracted
nouns did not make a good NC, e.g., worker work or
year toy. Note that this is despite our checking that
the candidate NC occurred at least 100 times in the
Google Web 1T 5-gram corpus (see Section 4.1.3).
We hypothesized that such bad NCs would tend to
have a low collocation strength. We tested this hy-
pothesis using the Dice coefficient, calculated using
the Google Web 1T 5-gram corpus. Figure 2 shows a
plot of the NC accuracy vs. collocation strength for
strict bootstrapping with N=5, M=50 for all three
iterations (the results for the other experiments show
a similar trend). We can see that the accuracy im-
proves slightly as the collocation strength increases:
compare the left and the right ends of the graph (the
results are mixed in the middle though).
?Acc.i1?
?Acc.i2?
?Acc.i3?
 40
 50
 60
 70
 80
 90
 100
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Figure 2: NC accuracy vs. collocation strength.
9 Conclusion and Future Work
We have presented a framework for building a very
large dataset of noun compounds expressing a given
target abstract semantic relation. For each extracted
noun compound, we generated a corresponding fine-
grained semantic interpretation: a frequency distri-
bution over suitable paraphrasing verbs.
In future work, we plan to apply our frame-
work to the remaining relations in the inventory of
Levi (1978), and to release the resulting dataset to
the research community. We believe that having a
large-scale dataset of noun compounds interpreted
with both fine- and coarse-grained semantic rela-
tions would be an important contribution to the de-
bate about which representation is preferable for dif-
ferent tasks. It should also help the overall advance-
ment of the field of noun compound interpretation.
656
Acknowledgments
This research is partially supported (for the sec-
ond author) by the SmartBook project, funded by
the Bulgarian National Science Fund under Grant
D002-111/15.12.2008.
We would like to thank the anonymous reviewers
for their detailed and constructive comments, which
have helped us improve the paper.
References
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Pro-
ceedings of the 17th International Conference on
Computational Linguistics, pages 96?102.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, ACL ?99, pages 57?64.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics, COLING ?08, pages
81?88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2009. Semeval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the Workshop on Semantic
Evaluations: Recent Achievements and Future Direc-
tions, SEW ?09, pages 100?105.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-
muid O? Se?aghdha, Stan Szpakowicz, and Tony Veale.
2010. SemEval-2010 task 9: The interpretation of
noun compounds using paraphrasing verbs and prepo-
sitions. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval-2, pages 39?
44.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 1(20):37?46.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the Conference
of the Pacific Association for Computational Linguis-
tics, PACLING ?07, pages 172?180.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53:810?842.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato.
2002. Seeing arguments through transparent struc-
tures. In Proceedings of the Third International Con-
ference on Language Resources and Evaluation, vol-
ume III of LREC ?02, pages 787?791.
Timothy Wilking Finin. 1980. The semantic interpre-
tation of compound nominals. Ph.D. thesis, Univer-
sity of Illinois at Urbana-Champaign, Champaign, IL,
USA. AAI8026491.
Thierry Fontenelle. 1999. Semantic resources for word
sense disambiguation: a sine qua non. Linguistica e
Filologia, 9:25?41.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Computer Speech and Language, 19(44):479?496.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic rela-
tions between nominals. In Proceedings of the 4th Se-
mantic Evaluation Workshop, SemEval-1, pages 13?
18.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2009.
Classification of semantic relations between nominals.
Language Resources and Evaluation, 43(2):105?121.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, ACL ?07, pages
568?575.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, COLING ?92, pages 539?545.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O? Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 task 8: Multi-way classifi-
cation of semantic relations between pairs of nominals.
In Proceedings of the 5th International Workshop on
Semantic Evaluation, SemEval-2, pages 33?38.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of compound nouns using WordNet sim-
ilarity. In Proceedings of 2nd International Joint Con-
ference on Natural Language Processing, IJCNLP ?05,
pages 945?956.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb se-
mantics. In Proceedings of the 44th Annual Meeting
of the Association for Computational Linguistics and
21st International Conference on Computational Lin-
guistics, ACL-COLING ?06, pages 491?498.
657
Su Nam Kim and Timothy Baldwin. 2007. Interpreting
noun compounds using bootstrapping and sense col-
location. In Proceedings of Conference of the Pacific
Association for Computational Linguistics, PACLING
?07, pages 129?136.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 1482?1491.
Richard J. Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159?174.
Maria Lapata. 2002. The disambiguation of nominaliza-
tions. Computational Linguistics, 28(3):357?388.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York, USA.
Tara McIntosh and James Curran. 2009. Reducing se-
mantic drift with bagging and distributional similar-
ity. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, ACL-IJCNLP ?09, pages 396?404.
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. In Proceedings
of the HLT-NAACL?04 Workshop on Computational
Lexical Semantics, pages 60?67.
Preslav Nakov and Marti A. Hearst. 2006. Using verbs
to characterize noun-noun relations. In Proceedings
of the 12th International Conference on Artificial In-
telligence: Methodology, Systems, and Applications,
AIMSA ?06, pages 233?244.
Preslav Nakov and Marti Hearst. 2008. Solving rela-
tional similarity problems using the web as a corpus.
In Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics, ACL ?08, pages
452?460.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence, ECAI ?08, pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In Pro-
ceedings of the 13th international conference on Arti-
ficial Intelligence: Methodology, Systems, and Appli-
cations, AIMSA ?08, pages 103?117.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence, AAAI
?04, pages 1024?1025.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, AAAI ?99, pages
474?479.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 6th Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01, pages 82?90.
Barbara Rosario and Marti Hearst. 2002. The descent
of hierarchy, and selection in relational semantics. In
Proceedings of Annual Meeting of the Association for
Computational Linguistics, ACL ?02, pages 247?254.
Diarmuid O? Se?aghdha. 2009. Semantic classification
with WordNet kernels. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, NAACL ?09, pages 237?240.
Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge - unify-
ing WordNet and Wikipedia. In Proceedings of 16th
International World Wide Web Conference, WWW
?07, pages 697?706.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, HLT-EMNLP ?05, pages 371?378.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?02, pages 214?221.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL ?10, pages 678?687.
Lucy Vanderwende. 1994. Algorithm for automatic in-
terpretation of noun sequences. In Proceedings of the
15th Conference on Computational linguistics, pages
782?788.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
658
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21?26,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific
Articles
Su Nam Kim,? Olena Medelyan,? Min-Yen Kan? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Pingar LP, Auckland, New Zealand
? School of Computing, National University of Singapore, Singapore
sunamkim@gmail.com, medelyan@gmail.com,
kanmy@comp.nus.edu.sg, tb@ldwin.net
Abstract
This paper describes Task 5 of the
Workshop on Semantic Evaluation 2010
(SemEval-2010). Systems are to automat-
ically assign keyphrases or keywords to
given scientific articles. The participating
systems were evaluated by matching their
extracted keyphrases against manually as-
signed ones. We present the overall rank-
ing of the submitted systems and discuss
our findings to suggest future directions
for this task.
1 Task Description
Keyphrases
1
are words that capture the main top-
ics of a document. As they represent these key
ideas, extracting high-quality keyphrases can ben-
efit various natural language processing (NLP) ap-
plications such as summarization, information re-
trieval and question-answering. In summariza-
tion, keyphrases can be used as a form of se-
mantic metadata (Barzilay and Elhadad, 1997;
Lawrie et al, 2001; D?Avanzo and Magnini,
2005). In search engines, keyphrases can supple-
ment full-text indexing and assist users in formu-
lating queries.
Recently, a resurgence of interest in keyphrase
extraction has led to the development of several
new systems and techniques for the task (Frank
et al, 1999; Witten et al, 1999; Turney, 1999;
Hulth, 2003; Turney, 2003; Park et al, 2004;
Barker and Corrnacchia, 2000; Hulth, 2004; Mat-
suo and Ishizuka, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007; Wan and Xiao, 2008; Liu et al, 2009;
Medelyan, 2009; Nguyen and Phan, 2009). These
1
We use ?keyphrase? and ?keywords? interchangeably to
refer to both single words and phrases.
?
Min-Yen Kan?s work was funded by National Research
Foundation grant ?Interactive Media Search? (grant # R-252-
000-325-279).
have showcased the potential benefits of keyphrase
extraction to downstream NLP applications.
In light of these developments, we felt that this
was an appropriate time to conduct a shared task
for keyphrase extraction, to provide a standard as-
sessment to benchmark current approaches. A sec-
ond goal of the task was to contribute an additional
public dataset to spur future research in the area.
Currently, there are several publicly available
data sets.
2
For example, Hulth (2003) contributed
2,000 abstracts of journal articles present in In-
spec between the years 1998 and 2002. The data
set contains keyphrases (i.e. controlled and un-
controlled terms) assigned by professional index-
ers ? 1,000 for training, 500 for validation and
500 for testing. Nguyen and Kan (2007) col-
lected a dataset containing 120 computer science
articles, ranging in length from 4 to 12 pages.
The articles contain author-assigned keyphrases
as well as reader-assigned keyphrases contributed
by undergraduate CS students. In the general
newswire domain, Wan and Xiao (2008) devel-
oped a dataset of 308 documents taken from DUC
2001 which contain up to 10 manually-assigned
keyphrases per document. Several databases, in-
cluding the ACM Digital Library, IEEE Xplore,
Inspec and PubMed provide articles with author-
assigned keyphrases and, occasionally, reader-
assigned ones. Medelyan (2009) automatically
generated a dataset using tags assigned by the
users of the collaborative citation platform CiteU-
Like. This dataset additionally records how many
people have assigned the same keyword to the
same publication. In total, 180 full-text publi-
cations were annotated by over 300 users.
3
De-
spite the availability of these datasets, a standard-
ized benchmark dataset with a well-defined train-
2
All data sets listed below are available for
download from http://github.com/snkim/
AutomaticKeyphraseExtraction
3
http://bit.ly/maui-datasets
21
ing and test split is needed to maximize compara-
bility of results.
For the SemEval-2010 Task 5, we have
compiled a set of 284 scientific articles with
keyphrases carefully chosen by both their authors
and readers. The participants? task was to develop
systems which automatically produce keyphrases
for each paper. Each team was allowed to sub-
mit up to three system runs, to benchmark the
contributions of different parameter settings and
approaches. Each run consisted of extracting a
ranked list of 15 keyphrases from each docu-
ment, ranked by their probability of being reader-
assigned keyphrases.
In the remainder of the paper, we describe
the competition setup, including how data collec-
tion was managed and the evaluation methodol-
ogy (Section 2). We present the results of the
shared task, and discuss the immediate findings of
the competition in Section 3. In Section 4 we as-
sess the human performance by comparing reader-
assigned keyphrases to those assigned by the au-
thors. This gives an approximation of an upper-
bound performance for this task.
2 Competition Setup
2.1 Data
We collected trial, training and test data from
the ACM Digital Library (conference and work-
shop papers). The input papers ranged from 6
to 8 pages, including tables and pictures. To en-
sure a variety of different topics was represented
in the corpus, we purposefully selected papers
from four different research areas for the dataset.
In particular, the selected articles belong to the
following four 1998 ACM classifications: C2.4
(Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial In-
telligence ? Multiagent Systems) and J4 (Social
and Behavioral Sciences ? Economics). All three
datasets (trial, training and test) had an equal dis-
tribution of documents from among the categories
(see Table 1). This domain specific information
was provided with the papers (e.g. I2.4-1 or H3.3-
2), in case participant systems wanted to utilize
this information. We specifically decided to strad-
dle different areas to see whether participant ap-
proaches would work better within specific areas.
Participants were provided with 40, 144, and
100 articles, respectively, in the trial, training and
test data, distributed evenly across the four re-
search areas in each case. Note that the trial data is
a subset of the training data. Since the original for-
mat for the articles was PDF, we converted them
into (UTF-8) plain text using pdftotext, and sys-
tematically restored full words that were originally
hyphenated and broken across two lines. This pol-
icy potentially resulted in valid hyphenated forms
having their hyphen (-) removed.
All collected papers contain author-assigned
keyphrases, part of the original PDF file. We addi-
tionally collected reader-assigned keyphrases for
each paper. We first performed a pilot annotation
task with a group of students to check the stabil-
ity of the annotations, finalize the guidelines, and
discover and resolve potential issues that may oc-
cur during the actual annotation. To collect the ac-
tual reader-assigned keyphrases, we then hired 50
student annotators from the Computer Science de-
partment of the National University of Singapore.
We assigned 5 papers to each annotator, esti-
mating that assigning keyphrases to each paper
should take about 10-15 minutes. Annotators were
explicitly told to extract keyphrases that actually
appear in the text of each paper, rather than to cre-
ate semantically-equivalent phrases, but could ex-
tract phrases from any part of the document (in-
cluding headers and captions). In reality, on av-
erage 15% of the reader-assigned keyphrases did
not appear in the text of the paper, but this is still
less than the 19% of author-assigned keyphrases
that did not appear in the papers. These values
were computed using the test documents only. In
other words, the maximum recall that the partici-
pating systems can achieve on these documents is
85% and 81% for the reader- and author-assigned
keyphrases, respectively.
As some keyphrases may occur in multiple
forms, in our evaluation we accepted two differ-
ent versions of genitive keyphrases: A of B ? B
A (e.g. policy of school = school policy) and A?s
B ? A B (e.g. school?s policy = school pol-
icy). In certain cases, such alternations change the
semantics of the candidate phrase (e.g., matter of
fact vs. ?fact matter). We judged borderline cases
by committee and do not include alternations that
were judged to be semantically distinct.
Table 1 shows the distribution of the trial, train-
ing and test documents over the four different re-
search areas, while Table 2 shows the distribution
of author- and reader-assigned keyphrases.
Interestingly, among the 387 author-assigned
22
Dataset Total Document Topic
C H I J
Trial 40 10 10 10 10
Training 144 34 39 35 36
Test 100 25 25 25 25
Table 1: Number of documents per topic in the
trial, training and test datasets, across the four
ACM document classifications
Dataset Author Reader Combined
Trial 149 526 621
Training 559 1824 2223
Test 387 1217 1482
Table 2: Number of author- and reader-assigned
keyphrases in the different datasets
keywords, 125 keywords match exactly with
reader-assigned keywords, while many more near-
misses (i.e. partial matches) occur.
2.2 Evaluation Method and Baseline
Traditionally, automatic keyphrase extraction sys-
tems have been assessed using the proportion of
top-N candidates that exactly match the gold-
standard keyphrases (Frank et al, 1999; Witten et
al., 1999; Turney, 1999). In some cases, inexact
matches, or near-misses, have also been consid-
ered. Some have suggested treating semantically-
similar keyphrases as correct based on simi-
larities computed over a large corpus (Jarmasz
and Barriere, 2004; Mihalcea and Tarau, 2004),
or using semantic relations defined in a the-
saurus (Medelyan and Witten, 2006). Zesch and
Gurevych (2009) compute near-misses using an n-
gram based approach relative to the gold standard.
For our shared task, we follow the traditional ex-
act match evaluation metric. That is, we match the
keyphrases in the answer set with those the sys-
tems provide, and calculate micro-averaged preci-
sion, recall and F-score (? = 1). In the evaluation,
we check the performance over the top 5, 10 and
15 candidates returned by each system. We rank
the participating systems by F-score over the top
15 candidates.
Participants were required to extract ex-
isting phrases from the documents. Since
it is theoretically possible to retrieve author-
assigned keyphrases from the original PDF arti-
cles, we evaluate the participating systems over
the independently-generated and held-out reader-
assigned keyphrases, as well as the combined set
of keyphrases (author- and reader-assigned).
All keyphrases in the answer set are stemmed
using the English Porter stemmer for both the
training and test dataset.
4
We computed a TF?IDF n-gram based baseline
using both supervised and unsupervised learning
systems. We use 1, 2, 3-grams as keyphrase can-
didates, used Na??ve Bayes (NB) and Maximum
Entropy (ME) classifiers to learn two supervised
baseline systems based on the keyphrase candi-
dates and gold-standard annotations for the train-
ing documents. In total, there are three baselines:
two supervised and one unsupervised. The per-
formance of the baselines is presented in Table 3,
where R indicates reader-assigned keyphrases and
C indicates combined (both author- and reader-
assigned) keyphrases.
3 Competition Results
The trial data was downloaded by 73 different
teams, of which 36 teams subsequently down-
loaded the training and test data. 21 teams partici-
pated in the final competition, of which two teams
withdrew their systems.
Table 4 shows the performance of the final 19
submitted systems. 5 teams submitted one run,
6 teams submitted two runs and 8 teams sub-
mitted the maximum number of three runs. We
rank the best-performing system from each team
by micro-averaged F-score over the top 15 can-
didates. We also show system performance over
reader-assigned keywords in Table 5, and over
author-assigned keywords in Table 6. In all these
tables, P, R and F denote precision, recall and F-
score, respectively.
The best results over the reader-assigned and
combined keyphrase sets are 23.5% and 27.5%,
respectively, achieved by the HUMB team. Most
systems outperformed the baselines. Systems also
generally did better over the combined set, as the
presence of a larger gold-standard answer set im-
proved recall.
In Tables 7 and 8, we ranked the teams by F-
score, computed over the top 15 candidates for
each of the four ACM document classifications.
The numbers in brackets are the actual F-scores
4
Using the Perl implementation available at http://
tartarus.org/
?
martin/PorterStemmer/; we in-
formed participants that this was the stemmer we would be
using for the task, to avoid possible stemming variations be-
tween implementations.
23
Method Top 5 candidates Top 10 candidates Top 15 candidates
by P R F P R F P R F
TF?IDF R 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
C 22.0% 7.5% 11.2% 17.7% 12.1% 14.4% 14.9% 15.3% 15.1%
NB R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
ME R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
Table 3: Baseline keyphrase extraction performance for one unsupervised (TF?IDF) and two supervised
(NB and ME) systems
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 39.0% 13.3% 19.8% 32.0% 21.8% 26.0% 27.2% 27.8% 27.5%
WINGNUS 2 40.2% 13.7% 20.5% 30.5% 20.8% 24.7% 24.9% 25.5% 25.2%
KP-Miner 3 36.0% 12.3% 18.3% 28.6% 19.5% 23.2% 24.9% 25.5% 25.2%
SZTERGAK 4 34.2% 11.7% 17.4% 28.5% 19.4% 23.1% 24.8% 25.4% 25.1%
ICL 5 34.4% 11.7% 17.5% 29.2% 19.9% 23.7% 24.6% 25.2% 24.9%
SEERLAB 6 39.0% 13.3% 19.8% 29.7% 20.3% 24.1% 24.1% 24.6% 24.3%
KX FBK 7 34.2% 11.7% 17.4% 27.0% 18.4% 21.9% 23.6% 24.2% 23.9%
DERIUNLP 8 27.4% 9.4% 13.9% 23.0% 15.7% 18.7% 22.0% 22.5% 22.3%
Maui 9 35.0% 11.9% 17.8% 25.2% 17.2% 20.4% 20.3% 20.8% 20.6%
DFKI 10 29.2% 10.0% 14.9% 23.3% 15.9% 18.9% 20.3% 20.7% 20.5%
BUAP 11 13.6% 4.6% 6.9% 17.6% 12.0% 14.3% 19.0% 19.4% 19.2%
SJTULTLAB 12 30.2% 10.3% 15.4% 22.7% 15.5% 18.4% 18.4% 18.8% 18.6%
UNICE 13 27.4% 9.4% 13.9% 22.4% 15.3% 18.2% 18.3% 18.8% 18.5%
UNPMC 14 18.0% 6.1% 9.2% 19.0% 13.0% 15.4% 18.1% 18.6% 18.3%
JU CSE 15 28.4% 9.7% 14.5% 21.5% 14.7% 17.4% 17.8% 18.2% 18.0%
LIKEY 16 29.2% 10.0% 14.9% 21.1% 14.4% 17.1% 16.3% 16.7% 16.5%
UvT 17 24.8% 8.5% 12.6% 18.6% 12.7% 15.1% 14.6% 14.9% 14.8%
POLYU 18 15.6% 5.3% 7.9% 14.6% 10.0% 11.8% 13.9% 14.2% 14.0%
UKP 19 9.4% 3.2% 4.8% 5.9% 4.0% 4.8% 5.3% 5.4% 5.3%
Table 4: Performance of the submitted systems over the combined author- and reader-assigned keywords,
ranked by F-score
for each team. Note that in the case of a tie in
F-score, we ordered teams by descending F-score
over all the data.
4 Discussion of the Upper-Bound
Performance
The current evaluation is a testament to the gains
made by keyphrase extraction systems. The sys-
tem performance over the different keyword cat-
egories (reader-assigned and author-assigned) and
numbers of keyword candidates (top 5, 10 and 15
candidates) attest to this fact.
The top-performing systems return F-scores in
the upper twenties. Superficially, this number is
low, and it is instructive to examine how much
room there is for improvement. Keyphrase extrac-
tion is a subjective task, and an F-score of 100% is
infeasible. On the author-assigned keyphrases in
our test collection, the highest a system could the-
oretically achieve was 81% recall
5
and 100% pre-
cision, which gives a maximum F-score of 89%.
However, such a high value would only be possi-
ble if the number of keyphrases extracted per doc-
ument could vary; in our task, we fixed the thresh-
olds at 5, 10 and 15 keyphrases.
5
The remaining 19% of keyphrases do not actually appear
in the documents and thus cannot be extracted.
Another way of computing the upper-bound
performance would be to look into how well peo-
ple perform the same task. We analyzed the
performance of our readers, taking the author-
assigned keyphrases as the gold standard. The au-
thors assigned an average of 4 keyphrases to each
paper, whereas the readers assigned 12 on average.
These 12 keyphrases cover 77.8% of the authors?
keyphrases, which corresponds to a precision of
21.5%. The F-score achieved by the readers on the
author-assigned keyphrases is 33.6%, whereas the
F-score of the best-performing system on the same
data is 19.3% (for top 15, not top 12 keyphrases,
see Table 6).
We conclude that there is definitely still room
for improvement, and for any future shared tasks,
we recommend against fixing any threshold on the
number of keyphrases to be extracted per docu-
ment. Finally, as we use a strict exact matching
metric for evaluation, the presented evaluation fig-
ures are a lower bound for performance, as se-
mantically equivalent keyphrases are not counted
as correct. For future runs of this challenge, we
believe a more semantically-motivated evaluation
should be employed to give a more accurate im-
pression of keyphrase acceptability.
24
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 30.4% 12.6% 17.8% 24.8% 20.6% 22.5% 21.2% 26.4% 23.5%
KX FBK 2 29.2% 12.1% 17.1% 23.2% 19.3% 21.1% 20.3% 25.3% 22.6%
SZTERGAK 3 28.2% 11.7% 16.6% 23.2% 19.3% 21.1% 19.9% 24.8% 22.1%
WINGNUS 4 30.6% 12.7% 18.0% 23.6% 19.6% 21.4% 19.8% 24.7 22.0%
ICL 5 27.2% 11.3% 16.0% 22.4% 18.6% 20.3% 19.5% 24.3% 21.6%
SEERLAB 6 31.0% 12.9% 18.2% 24.1% 20.0% 21.9% 19.3% 24.1% 21.5%
KP-Miner 7 28.2% 11.7% 16.5% 22.0% 18.3% 20.0% 19.3% 24.1% 21.5%
DERIUNLP 8 22.2% 9.2% 13.0% 18.9% 15.7% 17.2% 17.5% 21.8% 19.5%
DFKI 9 24.4% 10.1% 14.3% 19.8% 16.5% 18.0% 17.4% 21.7% 19.3%
UNICE 10 25.0% 10.4% 14.7% 20.1% 16.7% 18.2% 16.0% 19.9% 17.8%
SJTULTLAB 11 26.6% 11.1% 15.6% 19.4% 16.1% 17.6% 15.6% 19.4% 17.3%
BUAP 12 10.4% 4.3% 6.1% 13.9% 11.5% 12.6% 14.9% 18.6% 16.6%
Maui 13 25.0% 10.4% 14.7% 18.1% 15.0% 16.4% 14.9% 18.5% 16.1%
UNPMC 14 13.8% 5.7% 8.1% 15.1% 12.5% 13.7% 14.5% 18.0% 16.1%
JU CSE 15 23.4% 9.7% 13.7% 18.1% 15.0% 16.4% 14.4% 17.9% 16.0%
LIKEY 16 24.6% 10.2% 14.4% 17.9% 14.9% 16.2% 13.8% 17.2% 15.3%
POLYU 17 13.6% 5.7% 8.0% 12.6% 10.5% 11.4% 12.0% 14.9% 13.3%
UvT 18 20.4% 8.5% 12.0% 15.6% 13.0% 14.2% 11.9% 14.9% 13.2%
UKP 19 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
Table 5: Performance of the submitted systems over the reader-assigned keywords, ranked by F-score
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 21.2% 27.4% 23.9% 15.4% 39.8% 22.2% 12.1% 47.0% 19.3%
KP-Miner 2 19.0% 24.6% 21.4% 13.4% 34.6% 19.3% 10.7% 41.6% 17.1%
ICL 3 17.0% 22.0% 19.2% 13.5% 34.9% 19.5% 10.5% 40.6% 16.6%
Maui 4 20.4% 26.4% 23.0% 13.7% 35.4% 19.8% 10.2% 39.5% 16.2%
SEERLAB 5 18.8% 24.3% 21.2% 13.1% 33.9% 18.9% 10.1% 39.0% 16.0%
SZTERGAK 6 14.6% 18.9% 16.5% 12.2% 31.5% 17.6% 9.9% 38.5% 15.8%
WINGNUS 7 18.6% 24.0% 21.0% 12.6% 32.6% 18.2% 9.3% 36.2% 14.8%
DERIUNLP 8 12.6% 16.3% 14.2% 9.7% 25.1% 14.0% 9.3% 35.9% 14.7%
KX FBK 9 13.6% 17.6% 15.3% 10.0% 25.8% 14.4% 8.5% 32.8% 13.5%
BUAP 10 5.6% 7.2% 6.3% 8.1% 20.9% 11.7% 8.3% 32.0% 13.2%
JU CSE 11 12.0% 15.5% 13.5% 8.5% 22.0% 12.3% 7.5% 29.0% 11.9%
UNPMC 12 7.0% 9.0% 7.9% 7.7% 19.9% 11.1% 7.1% 27.4% 11.2%
DFKI 13 12.8% 16.5% 14.4% 8.5% 22.0% 12.3% 6.6% 25.6% 10.5%
SJTULTLAB 14 9.6% 12.4% 10.8% 7.8% 20.2% 11.3% 6.2% 24.0% 9.9%
Likey 15 11.6% 15.0% 13.1% 7.9% 20.4% 11.4% 5.9% 22.7% 9.3%
UvT 16 11.4% 14.7% 12.9% 7.6% 19.6% 11.0% 5.8% 22.5% 9.2%
UNICE 17 8.8% 11.4% 9.9% 6.4% 16.5% 9.2% 5.5% 21.5% 8.8%
POLYU 18 3.8% 4.9% 4.3% 4.1% 10.6% 5.9% 4.1% 16.0% 6.6%
UKP 19 1.6% 2.1% 1.8% 0.9% 2.3% 1.3% 0.8% 3.1% 1.3%
Table 6: Performance of the submitted systems over the author-assigned keywords, ranked by F-score
5 Conclusion
This paper has described Task 5 of the Workshop
on Semantic Evaluation 2010 (SemEval-2010), fo-
cusing on keyphrase extraction. We outlined the
design of the datasets used in the shared task and
the evaluation metrics, before presenting the offi-
cial results for the task and summarising the im-
mediate findings. We also analyzed the upper-
bound performance for this task, and demon-
strated that there is still room for improvement
over the task. We look forward to future advances
in automatic keyphrase extraction based on this
and other datasets.
References
Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases. In
Proceedings of BCCSCSI : Advances in Artificial In-
telligence. 2000, pp.96?103.
Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10?17.
Ernesto D?Avanzo and Bernado Magnini. A
Keyphrase-Based Approach to Summarization: the
LAKE System at DUC-2005. In Proceedings of
DUC. 2005.
Eibe Frank and Gordon W. Paynter and Ian H. Witten
and Carl Gutwin and Craig G. Nevill-Manning. Do-
main Specific Keyphrase Extraction. In Proceedings
of IJCAI. 1999, pp.668?673.
Annette Hulth. Improved automatic keyword extrac-
tion given more linguistic knowledge. In Proceed-
ings of EMNLP. 2003, 216?223.
Annette Hulth. Enhancing Linguistically Oriented
Automatic Keyword Extraction. In Proceedings of
HLT/NAACL. 2004, pp. 17?20.
Mario Jarmasz and Caroline Barriere. Using semantic
similarity over tera-byte corpus, compute the perfor-
mance of keyphrase extraction. In Proceedings of
CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosen-
berg. Finding Topic Words for Hierarchical Summa-
rization. In Proceedings of SIGIR. 2001, pp. 349?
357.
25
Rank Group C Group H Group I Group J
1 HUMB(28.3%) HUMB(30.2%) HUMB(24.2%) HUMB(27.4%)
2 ICL(27.2%) WINGNUS(28.9%) SEERLAB(24.2%) WINGNUS(25.4%)
3 KP-Miner(25.5%) SEERLAB(27.8%) KP-Miner(22.8%) ICL(25.4%)
4 SZTERGAK(25.3%) KP-Miner(27.6%) KX FBK(22.8%) SZTERGAK(25.17%)
5 WINGNUS(24.2%) SZTERGAK(27.6%) WINGNUS(22.3%) KP-Miner(24.9%)
6 KX FBK(24.2%) ICL(25.5%) SZTERGAK(22.25%) KX FBK(24.6%)
7 DERIUNLP(23.6%) KX FBK(23.9%) ICL(21.4%) UNICE(23.5%)
8 SEERLAB(22.0%) Maui(23.9%) DERIUNLP(20.1%) SEERLAB(23.3%)
9 DFKI(21.7%) DERIUNLP(23.6%) DFKI(19.3%) DFKI(22.2%)
10 Maui(19.3%) UNPMC(22.6%) BUAP(18.5%) Maui(21.3%)
11 BUAP(18.5%) SJTULTLAB(22.1%) SJTULTLAB(17.9%) DERIUNLP(20.3%)
12 JU CSE(18.2%) UNICE(21.8%) JU CSE(17.9%) BUAP(19.7%)
13 Likey(18.2%) DFKI(20.5%) Maui(17.6%) JU CSE(18.6%)
14 SJTULTLAB(17.7%) BUAP(20.2%) UNPMC(17.6%) UNPMC(17.8%)
15 UvT(15.8%) UvT(20.2%) UNICE(14.7%) Likey(17.2%)
16 UNPMC(15.2%) Likey(19.4%) Likey(11.3%) SJTULTLAB(16.7%)
17 UNIC(14.3%) JU CSE(17.3%) POLYU(13.6%) POLYU(14.3%)
18 POLYU(12.5%) POLYU(15.8%) UvT(10.3%) UvT(12.6%)
19 UKP(4.4%) UKP(5.0%) UKP(5.4%) UKP(6.8%)
Table 7: System ranking (and F-score) for each ACM classification: combined keywords
Rank Group C Group H Group I Group J
1 ICL(23.3%) HUMB(25.0%) HUMB(21.7%) HUMB(24.7%)
2 KX FBK(23.3%) WINGNUS(23.5%) KX FBK(21.4%) WINGNUS(24.4%)
3 HUMB(22.7%) SEERLAB(23.2%) SEERLAB(21.1%) SZTERGAK(24.4%)
4 SZTERGAK(22.7%) KP-Miner(22.4%) WINGNUS(19.9%) KX FBK(24.4%)
5 DERIUNLP(21.5%) SZTERGAK(21.8%) KP-Miner(19.6%) UNICE(23.8%)
6 KP-Miner(21.2%) KX FBK(21.2%) SZTERGAK(19.6%) ICL(23.5%)
7 WINGNUS(20.0%) ICL(20.1%) ICL(19.6%) KP-Miner(22.6%)
8 SEERLAB(19.4%) DERIUNLP(20.1%) DFKI(18.5%) SEERLAB(22.0%)
9 DFKI(19.4%) DFKI(19.5%) SJTULTLAB(17.6%) DFKI(21.7%)
10 JU CSE(17.0%) SJTULTLAB(19.5%) DERIUNLP(17.3%) BUAP(19.6%)
11 Likey(16.4%) UNICE(19.2%) JU CSE(16.7%) DERIUNLP(19.0%)
12 SJTULTLAB(15.8%) Maui(18.1%) BUAP(16.4%) Maui(17.8%)
13 BUAP(15.5%) UNPMC(18.1%) UNPMC(16.1%) JU CSE(17.9%)
14 Maui(15.2%) Likey(16.9%) Maui(14.9%) Likey(17.5%)
15 UNICE(14.0%) UvT(16.4%) UNICE(14.0%) UNPMC(16.6%)
16 UvT(14.0%) POLYU(15.5%) POLYU(11.9%) SJTULTLAB(16.3%)
17 UNPMC(13.4%) BUAP(14.9%) Likey(10.4%) POLYU(13.3%)
18 POLYU(12.5%) JU CSE(12.6%) UvT(9.5%) UvT(13.0%)
19 UKP(4.5%) UKP(4.3%) UKP(5.4%) UKP(6.9%)
Table 8: System ranking (and F-score) for each ACM classification: reader-assigned keywords
Zhiyuan Liu and Peng Li and Yabin Zheng and Sun
Maosong. Clustering to Find Exemplar Terms for
Keyphrase Extraction. In Proceedings of EMNLP.
2009, pp. 257?266.
Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
occurrence Statistical Information. International
Journal on Artificial Intelligence Tools. 2004, 13(1),
pp. 157?169.
Olena Medelyan and Ian H. Witten. Thesaurus based
automatic keyphrase indexing. In Proceedings of
ACM/IEED-CS JCDL. 2006, pp. 296?297.
Olena Medelyan. Human-competitive automatic topic
indexing. PhD Thesis. University of Waikato. 2009.
Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP. 2004,
pp. 404?411.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientific Publications. In Proceed-
ings of ICADL. 2007, pp. 317?326.
Chau Q. Nguyen and Tuoi T. Phan. An ontology-based
approach for key phrase extraction. In Proceedings
of the ACL-IJCNLP. 2009, pp. 181?184.
Youngja Park and Roy J. Byrd and Branimir Bogu-
raev. Automatic Glossary Extraction Beyond Termi-
nology Identification. In Proceedings of COLING.
2004, pp. 48?55.
Peter Turney. Learning to Extract Keyphrases from
Text. In National Research Council, Institute for In-
formation Technology, Technical Report ERB-1057.
1999.
Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434?
439.
Xiaojun Wan and Jianguo Xiao. CollabRank: to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969?976.
Ian H. Witten and Gordon Paynter and Eibe
Frank and Car Gutwin and Graig Nevill-Manning.
KEA:Practical Automatic Key phrase Extraction.
In Proceedings of ACM conference on Digital li-
braries. 1999, pp. 254?256.
Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In
Proceedings of RANLP. 2009.
26
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 33?38,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 8: Multi-Way Classification
of Semantic Relations Between Pairs of Nominals
Iris Hendrickx
?
, Su Nam Kim
?
, Zornitsa Kozareva
?
, Preslav Nakov
?
,
Diarmuid
?
O S
?
eaghdha
?
, Sebastian Pad
?
o
?
, Marco Pennacchiotti
??
,
Lorenza Romano
??
, Stan Szpakowicz
??
Abstract
SemEval-2 Task 8 focuses on Multi-way
classification of semantic relations between
pairs of nominals. The task was designed
to compare different approaches to seman-
tic relation classification and to provide a
standard testbed for future research. This
paper defines the task, describes the train-
ing and test data and the process of their
creation, lists the participating systems (10
teams, 28 runs), and discusses their results.
1 Introduction
SemEval-2010 Task 8 focused on semantic rela-
tions between pairs of nominals. For example, tea
and ginseng are in an ENTITY-ORIGIN relation in
?The cup contained tea from dried ginseng.?. The
automatic recognition of semantic relations has
many applications, such as information extraction,
document summarization, machine translation, or
construction of thesauri and semantic networks.
It can also facilitate auxiliary tasks such as word
sense disambiguation, language modeling, para-
phrasing, and recognizing textual entailment.
Our goal was to create a testbed for automatic
classification of semantic relations. In developing
the task we met several challenges: selecting a
suitable set of relations, specifying the annotation
procedure, and deciding on the details of the task
itself. They are discussed briefly in Section 2; see
also Hendrickx et al (2009), which includes a sur-
vey of related work. The direct predecessor of Task
8 was Classification of semantic relations between
nominals, Task 4 at SemEval-1 (Girju et al, 2009),
?
University of Lisbon, iris@clul.ul.pt
?
University of Melbourne, snkim@csse.unimelb.edu.au
?
Information Sciences Institute/University of Southern
California, kozareva@isi.edu
?
National University of Singapore, nakov@comp.nus.edu.sg
?
University of Cambridge, do242@cl.cam.ac.uk
?
University of Stuttgart, pado@ims.uni-stuttgart.de
??
Yahoo! Inc., pennacc@yahoo-inc.com
??
Fondazione Bruno Kessler, romano@fbk.eu
??
University of Ottawa and Polish Academy of Sciences,
szpak@site.uottawa.ca
which had a separate binary-labeled dataset for
each of seven relations. We have defined SemEval-
2010 Task 8 as a multi-way classification task in
which the label for each example must be chosen
from the complete set of ten relations and the map-
ping from nouns to argument slots is not provided
in advance. We also provide more data: 10,717 an-
notated examples, compared to 1,529 in SemEval-1
Task 4.
2 Dataset Creation
2.1 The Inventory of Semantic Relations
We first decided on an inventory of semantic rela-
tions. Ideally, it should be exhaustive (enable the
description of relations between any pair of nomi-
nals) and mutually exclusive (each pair of nominals
in context should map onto only one relation). The
literature, however, suggests that no relation inven-
tory satisfies both needs, and, in practice, some
trade-off between them must be accepted.
As a pragmatic compromise, we selected nine
relations with coverage sufficiently broad to be of
general and practical interest. We aimed at avoid-
ing semantic overlap as much as possible. We
included, however, two groups of strongly related
relations (ENTITY-ORIGIN / ENTITY-DESTINA-
TION and CONTENT-CONTAINER / COMPONENT-
WHOLE / MEMBER-COLLECTION) to assess mod-
els? ability to make such fine-grained distinctions.
Our inventory is given below. The first four were
also used in SemEval-1 Task 4, but the annotation
guidelines have been revised, and thus no complete
continuity should be assumed.
Cause-Effect (CE). An event or object leads to an
effect. Example: those cancers were caused
by radiation exposures
Instrument-Agency (IA). An agent uses an in-
strument. Example: phone operator
Product-Producer (PP). A producer causes a
product to exist. Example: a factory manu-
factures suits
33
Content-Container (CC). An object is physically
stored in a delineated area of space. Example:
a bottle full of honey was weighed
Entity-Origin (EO). An entity is coming or is de-
rived from an origin (e.g., position or mate-
rial). Example: letters from foreign countries
Entity-Destination (ED). An entity is moving to-
wards a destination. Example: the boy went
to bed
Component-Whole (CW). An object is a com-
ponent of a larger whole. Example: my
apartment has a large kitchen
Member-Collection (MC). A member forms a
nonfunctional part of a collection. Example:
there are many trees in the forest
Message-Topic (MT). A message, written or spo-
ken, is about a topic. Example: the lecture
was about semantics
2.2 Annotation Guidelines
We defined a set of general annotation guidelines
as well as detailed guidelines for each semantic
relation. Here, we describe the general guidelines,
which delineate the scope of the data to be col-
lected and state general principles relevant to the
annotation of all relations.
1
Our objective is to annotate instances of seman-
tic relations which are true in the sense of hold-
ing in the most plausible truth-conditional inter-
pretation of the sentence. This is in the tradition
of the Textual Entailment or Information Valida-
tion paradigm (Dagan et al, 2009), and in con-
trast to ?aboutness? annotation such as semantic
roles (Carreras and M`arquez, 2004) or the BioNLP
2009 task (Kim et al, 2009) where negated rela-
tions are also labelled as positive. Similarly, we
exclude instances of semantic relations which hold
only in speculative or counterfactural scenarios. In
practice, this means disallowing annotations within
the scope of modals or negations, e.g., ?Smoking
may/may not have caused cancer in this case.?
We accept as relation arguments only noun
phrases with common-noun heads. This distin-
guishes our task from much work in Information
Extraction, which tends to focus on specific classes
of named entities and on considerably more fine-
grained relations than we do. Named entities are a
specific category of nominal expressions best dealt
1
The full task guidelines are available at http://docs.
google.com/View?id=dfhkmm46_0f63mfvf7
with using techniques which do not apply to com-
mon nouns. We only mark up the semantic heads of
nominals, which usually span a single word, except
for lexicalized terms such as science fiction.
We also impose a syntactic locality requirement
on example candidates, thus excluding instances
where the relation arguments occur in separate sen-
tential clauses. Permissible syntactic patterns in-
clude simple and relative clauses, compounds, and
pre- and post-nominal modification. In addition,
we did not annotate examples whose interpretation
relied on discourse knowledge, which led to the
exclusion of pronouns as arguments. Please see
the guidelines for details on other issues, includ-
ing noun compounds, aspectual phenomena and
temporal relations.
2.3 The Annotation Process
The annotation took place in three rounds. First,
we manually collected around 1,200 sentences for
each relation through pattern-based Web search. In
order to ensure a wide variety of example sentences,
we used a substantial number of patterns for each
relation, typically between one hundred and several
hundred. Importantly, in the first round, the relation
itself was not annotated: the goal was merely to
collect positive and near-miss candidate instances.
A rough aim was to have 90% of candidates which
instantiate the target relation (?positive instances?).
In the second round, the collected candidates for
each relation went to two independent annotators
for labeling. Since we have a multi-way classifi-
cation task, the annotators used the full inventory
of nine relations plus OTHER. The annotation was
made easier by the fact that the cases of overlap
were largely systematic, arising from general phe-
nomena like metaphorical use and situations where
more than one relation holds. For example, there is
a systematic potential overlap between CONTENT-
CONTAINER and ENTITY-DESTINATION depend-
ing on whether the situation described in the sen-
tence is static or dynamic, e.g., ?When I came,
the <e1>apples</e1> were already put in the
<e2>basket</e2>.? is CC(e1, e2), while ?Then,
the <e1>apples</e1> were quickly put in the
<e2>basket</e2>.? is ED(e1, e2).
In the third round, the remaining disagreements
were resolved, and, if no consensus could be
achieved, the examples were removed. Finally, we
merged all nine datasets to create a set of 10,717
instances. We released 8,000 for training and kept
34
the rest for testing.
2
Table 1 shows some statistics about the dataset.
The first column (Freq) shows the absolute and rel-
ative frequencies of each relation. The second col-
umn (Pos) shows that the average share of positive
instances was closer to 75% than to 90%, indicating
that the patterns catch a substantial amount of ?near-
miss? cases. However, this effect varies a lot across
relations, causing the non-uniform relation distribu-
tion in the dataset (first column).
3
After the second
round, we also computed inter-annotator agreement
(third column, IAA). Inter-annotator agreement
was computed on the sentence level, as the per-
centage of sentences for which the two annotations
were identical. That is, these figures can be inter-
preted as exact-match accuracies. We do not report
Kappa, since chance agreement on preselected can-
didates is difficult to estimate.
4
IAA is between
60% and 95%, again with large relation-dependent
variation. Some of the relations were particularly
easy to annotate, notably CONTENT-CONTAINER,
which can be resolved through relatively clear cri-
teria, despite the systematic ambiguity mentioned
above. ENTITY-ORIGIN was the hardest relation to
annotate. We encountered ontological difficulties
in defining both Entity (e.g., in contrast to Effect)
and Origin (as opposed to Cause). Our numbers
are on average around 10% higher than those re-
ported by Girju et al (2009). This may be a side
effect of our data collection method. To gather
1,200 examples in realistic time, we had to seek
productive search query patterns, which invited
certain homogeneity. For example, many queries
for CONTENT-CONTAINER centered on ?usual sus-
pect? such as box or suitcase. Many instances of
MEMBER-COLLECTION were collected on the ba-
sis of from available lists of collective names.
3 The Task
The participating systems had to solve the follow-
ing task: given a sentence and two tagged nominals,
predict the relation between those nominals and the
direction of the relation.
We released a detailed scorer which outputs (1) a
confusion matrix, (2) accuracy and coverage, (3)
2
This set includes 891 examples from SemEval-1 Task 4.
We re-annotated them and assigned them as the last examples
of our training dataset to ensure that the test set was unseen.
3
To what extent our candidate selection produces a biased
sample is a question that we cannot address within this paper.
4
We do not report Pos or IAA for OTHER, since OTHER is
a pseudo-relation that was not annotated in its own right. The
numbers would therefore not be comparable to other relations.
Relation Freq Pos IAA
Cause-Effect 1331 (12.4%) 91.2% 79.0%
Component-Whole 1253 (11.7%) 84.3% 70.0%
Entity-Destination 1137 (10.6%) 80.1% 75.2%
Entity-Origin 974 (9.1%) 69.2% 58.2%
Product-Producer 948 (8.8%) 66.3% 84.8%
Member-Collection 923 (8.6%) 74.7% 68.2%
Message-Topic 895 (8.4%) 74.4% 72.4%
Content-Container 732 (6.8%) 59.3% 95.8%
Instrument-Agency 660 (6.2%) 60.8% 65.0%
Other 1864 (17.4%) N/A
4
N/A
4
Total 10717 (100%)
Table 1: Annotation Statistics. Freq: Absolute and
relative frequency in the dataset; Pos: percentage
of ?positive? relation instances in the candidate set;
IAA: inter-annotator agreement
precision (P), recall (R), and F
1
-Score for each
relation, (4) micro-averaged P, R, F
1
, (5) macro-
averaged P, R, F
1
. For (4) and (5), the calculations
ignored the OTHER relation. Our official scoring
metric is macro-averaged F
1
-Score for (9+1)-way
classification, taking directionality into account.
The teams were asked to submit test data pre-
dictions for varying fractions of the training data.
Specifically, we requested results for the first 1000,
2000, 4000, and 8000 training instances, called
TD1 through TD4. TD4 was the full training set.
4 Participants and Results
Table 2 lists the participants and provides a rough
overview of the system features. Table 3 shows the
results. Unless noted otherwise, all quoted numbers
are F
1
-Scores.
Overall Ranking and Training Data. We rank
the teams by the performance of their best system
on TD4, since a per-system ranking would favor
teams with many submitted runs. UTD submit-
ted the best system, with a performance of over
82%, more than 4% better than the second-best
system. FBK IRST places second, with 77.62%,
a tiny margin ahead of ISI (77.57%). Notably, the
ISI system outperforms the FBK IRST system for
TD1 to TD3, where it was second-best. The accu-
racy numbers for TD4 (Acc TD4) lead to the same
overall ranking: micro- versus macro-averaging
does not appear to make much difference either.
A random baseline gives an uninteresting score of
6%. Our competitive baseline system is a simple
Naive Bayes classifier which relies on words in the
sentential context only; two systems scored below
this baseline.
35
System Institution Team Description Res. Class.
Baseline Task organizers local context of 2 words only BN
ECNU-SR-1 East China Normal
University
Man Lan, Yuan
Chen, Zhimin
Zhou, Yu Xu
stem, POS, syntactic patterns S SVM
(multi)
ECNU-SR-2,3 features like ECNU-SR-1, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-4 stem, POS, syntactic patterns,
hyponymy and meronymy rela-
tions
WN,
S
SVM
(multi)
ECNU-SR-5,6 features like ECNU-SR-4, dif-
ferent prob. thresholds
SVM
(binary)
ECNU-SR-7 majority vote of ECNU-1,2,4,5
FBK IRST-6C32 Fondazione Bruno
Kessler
Claudio Giu-
liano, Kateryna
Tymoshenko
3-word window context features
(word form, part of speech, or-
thography) + Cyc; parameter
estimation by optimization on
training set
Cyc SVM
FBK IRST-12C32 FBK IRST-6C32 + distance fea-
tures
FBK IRST-12VBC32 FBK IRST-12C32 + verbs
FBK IRST-6CA,
-12CA, -12VBCA
features as above, parameter es-
timation by cross-validation
FBK NK-RES1 Fondazione Bruno
Kessler
Matteo Negri,
Milen Kouylekov
collocations, glosses, semantic
relations of nominals + context
features
WN BN
FBK NK-RES 2,3,4 like FBK NK-RES1 with differ-
ent context windows and collo-
cation cutoffs
ISI Information Sci-
ences Institute,
University of
Southern Califor-
nia
Stephen Tratz features from different re-
sources, a noun compound
relation system, and various
feature related to capitalization,
affixes, closed-class words
WN,
RT, G
ME
ISTI-1,2 Istituto di sci-
enca e tecnologie
dell?informazione
?A. Faedo?
Andrea Esuli,
Diego Marcheg-
giani, Fabrizio
Sebastiani
Boosting-based classification.
Runs differ in their initializa-
tion.
WN 2S
JU Jadavpur Univer-
sity
Santanu Pal, Partha
Pakray, Dipankar
Das, Sivaji Bandy-
opadhyay
Verbs, nouns, and prepositions;
seed lists for semantic relations;
parse features and NEs
WN,
S
CRF
SEKA Hungarian
Academy of
Sciences
Eszter Simon, An-
dras Kornai
Levin and Roget classes, n-
grams; other grammatical and
formal features
RT,
LC
ME
TUD-base Technische Univer-
sit?at Darmstadt
Gy?orgy Szarvas,
Iryna Gurevych
word, POS n-grams, depen-
dency path, distance
S ME
TUD-wp TUD-base + ESA semantic re-
latedness scores
+WP
TUD-comb TUD-base + own semantic relat-
edness scores
+WP,WN
TUD-comb-threshold TUD-comb with higher thresh-
old for OTHER
UNITN University of
Trento
Fabio Celli punctuation, context words,
prepositional patterns, estima-
tion of semantic relation
? DR
UTD University of Texas
at Dallas
Bryan Rink, Sanda
Harabagiu
context wods, hypernyms, POS,
dependencies, distance, seman-
tic roles, Levin classes, para-
phrases
WN,
S, G,
PB/NB,
LC
SVM,
2S
Table 2: Participants of SemEval-2010 Task 8. Res: Resources used (WN: WordNet data; WP:
Wikipedia data; S: syntax; LC: Levin classes; G: Google n-grams, RT: Roget?s Thesaurus, PB/NB:
PropBank/NomBank). Class: Classification style (ME: Maximum Entropy; BN: Bayes Net; DR: Decision
Rules/Trees; CRF: Conditional Random Fields; 2S: two-step classification)
36
System TD1 TD2 TD3 TD4 Acc TD4 Rank Best Cat Worst Cat-9
Baseline 33.04 42.41 50.89 57.52 50.0 - MC (75.1) IA (28.0)
ECNU-SR-1 52.13 56.58 58.16 60.08 57.1
4
CE (79.7) IA (32.2)
ECNU-SR-2 46.24 47.99 69.83 72.59 67.1 CE (84.4) IA (52.2)
ECNU-SR-3 39.89 42.29 65.47 68.50 62.0 CE (83.4) IA (46.5)
ECNU-SR-4 67.95 70.58 72.99 74.82 70.5 CE (84.6) IA (61.4)
ECNU-SR-5 49.32 50.70 72.63 75.43 70.2 CE (85.1) IA (60.7)
ECNU-SR-6 42.88 45.54 68.87 72.19 65.8 CE (85.2) IA (56.7)
ECNU-SR-7 58.67 58.87 72.79 75.21 70.2 CE (86.1) IA (61.8)
FBK IRST-6C32 60.19 67.31 71.78 76.81 72.4
2
ED (82.6) IA (69.4)
FBK IRST-12C32 60.66 67.91 72.04 76.91 72.4 MC (84.2) IA (68.8)
FBK IRST-12VBC32 62.64 69.86 73.19 77.11 72.3 ED (85.9) PP (68.1)
FBK IRST-6CA 60.58 67.14 71.63 76.28 71.4 CE (82.3) IA (67.7)
FBK IRST-12CA 61.33 67.80 71.65 76.39 71.4 ED (81.8) IA (67.5)
FBK IRST-12VBCA 63.61 70.20 73.40 77.62 72.8 ED (86.5) IA (67.3)
FBK NK-RES1 55.71
?
64.06
?
67.80
?
68.02 62.1
7
ED (77.6) IA (52.9)
FBK NK-RES2 54.27
?
63.68
?
67.08
?
67.48 61.4 ED (77.4) PP (55.2)
FBK NK-RES3 54.25
?
62.73
?
66.11
?
66.90 60.5 MC (76.7) IA (56.3)
FBK NK-RES4 44.11
?
58.85
?
63.06
?
65.84 59.4 MC (76.1) IA/PP (58.0)
ISI 66.68 71.01 75.51 77.57 72.7 3 CE (87.6) IA (61.5)
ISTI-1 50.49
?
55.80
?
61.14
?
68.42 63.2
6
ED (80.7) PP (53.8)
ISTI-2 50.69
?
54.29
?
59.77
?
66.65 61.5 ED (80.2) IA (48.9)
JU 41.62
?
44.98
?
47.81
?
52.16 50.2 9 CE (75.6) IA (27.8)
SEKA 51.81 56.34 61.10 66.33 61.9 8 CE (84.0) PP (43.7)
TUD-base 50.81 54.61 56.98 60.50 56.1
5
CE (80.7) IA (31.1)
TUD-wp 55.34 60.90 63.78 68.00 63.5 ED (82.9) IA (44.1)
TUD-comb 57.84 62.52 66.41 68.88 64.6 CE (83.8) IA (46.8)
TUD-comb-? 58.35 62.45 66.86 69.23 65.4 CE (83.4) IA (46.9)
UNITN 16.57
?
18.56
?
22.45
?
26.67 27.4 10 ED (46.4) PP (0)
UTD 73.08 77.02 79.93 82.19 77.9 1 CE (89.6) IA (68.5)
Table 3: F
1
-Score of all submitted systems on the test dataset as a function of training data: TD1=1000,
TD2=2000, TD3=4000, TD4=8000 training examples. Official results are calculated on TD4. The results
marked with
?
were submitted after the deadline. The best-performing run for each participant is italicized.
As for the amount of training data, we see a sub-
stantial improvement for all systems between TD1
and TD4, with diminishing returns for the transi-
tion between TD3 and TD4 for many, but not all,
systems. Overall, the differences between systems
are smaller for TD4 than they are for TD1. The
spread between the top three systems is around 10%
at TD1, but below 5% at TD4. Still, there are clear
differences in the influence of training data size
even among systems with the same overall archi-
tecture. Notably, ECNU-SR-4 is the second-best
system at TD1 (67.95%), but gains only 7% from
the eightfold increase of the size of the training data.
At the same time, ECNU-SR-3 improves from less
than 40% to almost 69%. The difference between
the systems is that ECNU-SR-4 uses a multi-way
classifier including the class OTHER, while ECNU-
SR-3 uses binary classifiers and assigns OTHER
if no other relation was assigned with p>0.5. It
appears that these probability estimates for classes
are only reliable enough for TD3 and TD4.
The Influence of System Architecture. Almost
all systems used either MaxEnt or SVM classifiers,
with no clear advantage for either. Similarly, two
systems, UTD and ISTI (rank 1 and 6) split the task
into two classification steps (relation and direction),
but the 2nd- and 3rd-ranked systems do not. The
use of a sequence model such as a CRF did not
show a benefit either.
The systems use a variety of resources. Gener-
ally, richer feature sets lead to better performance
(although the differences are often small ? compare
the different FBK IRST systems). This improve-
ment can be explained by the need for semantic
generalization from training to test data. This need
can be addressed using WordNet (contrast ECNU-1
to -3 with ECNU-4 to -6), the Google n-gram col-
lection (see ISI and UTD), or a ?deep? semantic
resource (FBK IRST uses Cyc). Yet, most of these
resources are also included in the less successful
systems, so beneficial integration of knowledge
sources into semantic relation classification seems
to be difficult.
System Combination. The differences between
the systems suggest that it might be possible to
achieve improvements by building an ensemble
37
system. When we combine the top three systems
(UTD, FBK IRST-12VBCA, and ISI) by predict-
ing their majority vote, or OTHER if there was none,
we obtain a small improvement over the UTD sys-
tem with an F
1
-Score of 82.79%. A combination of
the top five systems using the same method shows
a worse performance, however (80.42%). This sug-
gests that the best system outperforms the rest by
a margin that cannot be compensated with system
combination, at least not with a crude majority vote.
We see a similar pattern among the ECNU systems,
where the ECNU-SR-7 combination system is out-
performed by ECNU-SR-5, presumably since it
incorporates the inferior ECNU-SR-1 system.
Relation-specific Analysis. We also analyze the
performance on individual relations, especially the
extremes. There are very stable patterns across all
systems. The best relation (presumably the eas-
iest to classify) is CE, far ahead of ED and MC.
Notably, the performance for the best relation is
75% or above for almost all systems, with compar-
atively small differences between the systems. The
hardest relation is generally IA, followed by PP.
5
Here, the spread among the systems is much larger:
the highest-ranking systems outperform others on
the difficult relations. Recall was the main prob-
lem for both IA and PP: many examples of these
two relations are misclassified, most frequently as
OTHER. Even at TD4, these datasets seem to be
less homogeneous than the others. Intriguingly, PP
shows a very high inter-annotator agreement (Ta-
ble 1). Its difficulty may therefore be due not to
questionable annotation, but to genuine variability,
or at least the selection of difficult patterns by the
dataset creator. Conversely, MC, among the easiest
relations to model, shows only a modest IAA.
Difficult Instances. There were 152 examples
that are classified incorrectly by all systems. We
analyze them, looking for sources of errors. In ad-
dition to a handful of annotation errors and some
borderline cases, they are made up of instances
which illustrate the limits of current shallow mod-
eling approaches in that they require more lexical
knowledge and complex reasoning. A case in point:
The bottle carrier converts your <e1>bottle</e1>
into a <e2>canteen</e2>. This instance of
OTHER is misclassified either as CC (due to the
5
The relation OTHER, which we ignore in the overall F
1
-
score, does even worse, often below 40%. This is to be ex-
pected, since the OTHER examples in our datasets are near
misses for other relations, thus making a very incoherent class.
nominals) or as ED (because of the preposition
into). Another example: [...] <e1>Rudders</e1>
are used by <e2>towboats</e2> and other ves-
sels that require a high degree of manoeuvrability.
This is an instance of CW misclassified as IA, prob-
ably on account of the verb use which is a frequent
indicator of an agentive relation.
5 Discussion and Conclusion
There is little doubt that 19-way classification is a
non-trivial challenge. It is even harder when the
domain is lexical semantics, with its idiosyncrasies,
and when the classes are not necessarily disjoint,
despite our best intentions. It speaks to the success
of the exercise that the participating systems? per-
formance was generally high, well over an order
of magnitude above random guessing. This may
be due to the impressive array of tools and lexical-
semantic resources deployed by the participants.
Section 4 suggests a few ways of interpreting
and analyzing the results. Long-term lessons will
undoubtedly emerge from the workshop discussion.
One optimistic-pessimistic conclusion concerns the
size of the training data. The notable gain TD3?
TD4 suggests that even more data would be helpful,
but that is so much easier said than done: it took
the organizers well in excess of 1000 person-hours
to pin down the problem, hone the guidelines and
relation definitions, construct sufficient amounts of
trustworthy training data, and run the task.
References
X. Carreras and L. M`arquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role label-
ing. In Proc. CoNLL-04, Boston, MA.
I. Dagan, B. Dolan, B. Magnini, and D. Roth. 2009.
Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineer-
ing, 15(4):i?xvii.
R. Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2009. Classification of semantic
relations between nominals. Language Resources
and Evaluation, 43(2):105?121.
I. Hendrickx, S. Kim, Z. Kozareva, P. Nakov, D.
?
O
S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Romano,
and S. Szpakowicz. 2009. SemEval-2010 Task
8: Multi-way classification of semantic relations be-
tween pairs of nominals. In Proc. NAACL Workshop
on Semantic Evaluations, Boulder, CO.
J. Kim, T. Ohta, S. Pyysalo, Y. Kano, and J. Tsujii.
2009. Overview of BioNLP?09 shared task on event
extraction. In Proc. BioNLP-09, Boulder, CO.
38
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 15?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Intelligent Linux Information Access by Data Mining: the ILIAD Project
Timothy Baldwin,? David Martinez,? Richard B. Penman,? Su Nam Kim,?
Marco Lui,? Li Wang? and Andrew MacKinlay?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? NICTA Victoria Research Laboratory
Abstract
We propose an alternative to conventional in-
formation retrieval over Linux forum data,
based on thread-, post- and user-level analysis,
interfaced with an information retrieval engine
via reranking.
1 Introduction
Due to the sheer scale of web data, simple keyword
matching is an effective means of information ac-
cess for many informational web queries. There
still remain significant clusters of information access
needs, however, where keyword matching is less
successful. One such instance is technical web fo-
rums and mailing lists (collectively termed ?forums?
for the purposes of this paper): technical forums
are a rich source of information when troubleshoot-
ing, and it is often possible to resolve technical
queries/problems via web-archived data. The search
facilities provided by forums and web search en-
gines tend to be over-simplistic, however, and there
is a desperate need for more sophisticated search (Xi
et al, 2004; Seo et al, 2009), including: favour-
ing threads which have led to a successful resolu-
tion; reflecting the degree of clarity/reproducibility
of the proposed solution in a given thread; repre-
senting threads via their threaded rather than sim-
ple chronological structure; the ability to highlight
key aspects of the thread, in terms of the problem
description and solution which led to a successful
resolution; and ideally, the ability to represent the
problem and solution in normalised form via infor-
mation extraction.
This paper provides a brief outline of an attempt
to achieve these and other goals in the context of
Linux web user forum data, in the form of the IL-
IAD (Intelligent Linux Information Access by Data
Mining) project. Linux users and developers rely
particularly heavily on web user forums and mail-
ing lists, due to the nature of the community, which
is highly decentralised ? with massive proliferation
of packages and distributions? and notoriously bad
at maintaining up-to-date documentation at a level
suitable for newbie and even intermediate users.
2 Project Outline
Our proposed solution is as follows: (1) crawl data
from a variety of web user forums; (2) analyse each
thread, to identify named entities and generate meta-
data; (3) analyse post-level linkages; (4) predict
user-level features which are expected to impinge on
the quality of search results; and finally (5) draw to-
gether the features from (1) to (4) to enhance the
quality of a traditional ranked IR approach. We
briefly review each step below. Given space limi-
tations, we focus on outlining our interpretation of
the task in this paper. For further details and results,
the reader is referred to the key papers cited herein.
2.1 Crawling
The first step is to crawl data from a variety of fo-
rums and mailing lists, for which we have developed
open-source scraping software in the form of SITE-
SCRAPER.1 SITESCRAPER is designed such that the
user simply copies relevant content from a browser-
rendered version of a given set of pages, which it
interprets as a structured record, and translates into
a generalised XPATH query.
2.2 Thread-level analysis
Next, we perform named entity recognition (NER)
over each thread to identify entities such as package
and distribution names, version numbers and snip-
pets of code; as part of this, we perform version
1http://sitescraper.googlecode.com/
15
anchoring, in identifying what entity each version
number relates to.
To generate thread-level metadata, we classify
each thread for the following three features, based
on an ordinal scale of 1?5 (Baldwin et al, 2007):
Complete: Is the problem description complete?
Solved: Is a solution provided in the thread?
Task Oriented: Is the thread about a specific
problem?
We additionally automatically classify the nature
of the thread content, in terms of, e.g., whether it
contains documentation or installation details, or re-
lates to software, hardware or programming.
Our experiments on thread-level classification are
based on a set of 250 annotated threads from Lin-
uxQuestions and other forums, as well as a dataset
from CNET.
2.3 Post-level analysis
We automatically analyse the post-to-post discourse
structure of each thread, in terms of which (preced-
ing) post(s) each post relates to, and how, building
off the work of Rose? et al (1995) and Wolf and Gib-
son (2005). For example, a given post may refute
the solution proposed in an earlier post, and also
propose a novel solution in response to the initiat-
ing post.
Separately, we are developing techniques for
identifying whether a new post to a given forum
is sufficiently similar to other (ideally resolved)
threads that the author should be prompted to first
check the existing threads for redundancy before a
new thread is initiated.
Our experiments on post-level analysis are, once
again, based on data from LinuxQuestions and
CNET.
2.4 User-level analysis
We are also experimenting with profiling users vari-
ously, based on a 5-point ordinal scale across a range
of user characteristics. Our experiments are based
on data from LinuxQuestions (Lui, 2009).
2.5 IR ranking
The various features are interfaced with an ad hoc
information retrieval (IR) system via a learning-to-
rank approach (Cao et al, 2007). In order to carry
out IR evaluation, we have developed a set of queries
and relevance judgements over a large-scale set of
forum data.
Our experiments to date have been based on com-
bination over three IR engines (LUCENE, ZETTAIR
and LEMUR), and involved thread-level metadata
only, but we have achieved encouraging results, sug-
gesting that thread-level metadata can enhance IR
effectiveness.
3 Conclusions
This paper provides an outline of the ILIAD project,
focusing on the tasks of crawling, thread-level anal-
ysis, post-level analysis, user-level analysis and IR
reranking. We have designed a series of class sets
for the component tasks, and carried out experimen-
tation over a range of data sources, achieving en-
couraging results.
Acknowledgements
NICTA is funded by the Australian Government as rep-
resented by the Department of Broadband, Communica-
tions and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence pro-
gram.
References
T Baldwin, D Martinez, and RB Penman. 2007. Auto-
matic thread classification for Linux user forum infor-
mation access. In Proc of ADCS 2007.
Z Cao, T Qin, TY Liu, MF Tsai, and H Li. 2007. Learn-
ing to rank: from pairwise approach to listwise ap-
proach. In Proc of ICML 2007.
M Lui. 2009. Impact of user characteristics on online fo-
rum classification tasks. Honours thesis, University of
Melbourne. http://repository.unimelb.edu.
au/10187/5745.
CP Rose?, B Di Eugenio, LS Levin, and C Van Ess-
Dykema. 1995. Discourse processing of dialogues
with multiple threads. In Proc of ACL 1995.
J Seo, WB Croft, and DA Smith. 2009. Online commu-
nity search using thread structure. In Proc of CIKM
2009.
F Wolf and E Gibson. 2005. Representing discourse co-
herence: A corpus-based study. Comp Ling, 31(2).
W Xi, J Lind, and E Brill. 2004. Learning effective rank-
ing functions for newsgroup search. In Proc of SIGIR
2004.
16
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 192?202,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Tagging and Linking Web Forum Posts
Su Nam Kim, Li Wang and Timothy Baldwin
Dept of Computer Science and Software Engineering
University of Melbourne, Australia
sunamkim@gmail.com, li.wang.d@gmail.com, tb@ldwin.net
Abstract
We propose a method for annotating post-
to-post discourse structure in online user
forum data, in the hopes of improving
troubleshooting-oriented information ac-
cess. We introduce the tasks of: (1) post
classification, based on a novel dialogue
act tag set; and (2) link classification. We
also introduce three feature sets (structural
features, post context features and seman-
tic features) and experiment with three dis-
criminative learners (maximum entropy,
SVM-HMM and CRF).We achieve above-
baseline results for both dialogue act and
link classification, with interesting diver-
gences in which feature sets perform well
over the two sub-tasks, and go on to per-
form preliminary investigation of the inter-
action between post tagging and linking.
1 Introduction
With the advent of Web 2.0, there has been an ex-
plosion of web authorship from individuals of all
walks of life. Notably, social networks, blogs and
web user forums have entered the mainstream of
modern-day society, creating both new opportuni-
ties and challenges for organisations seeking to en-
gage with clients or users of any description. One
area of particular interest is web-based user sup-
port, e.g. to aid a user in purchasing a gift for a
friend, or advising a customer on how to config-
ure a newly-acquired wireless router. While such
interactions traditionally took place on an indi-
vidual basis, leading to considerable redundancy
for frequently-arising requests or problems, user
forums support near-real-time user interaction in
the form of a targeted thread made up of individ-
ual user posts. Additionally, they have the poten-
tial for perpetual logging to allow other users to
benefit from them. This in turn facilitates ?sup-
port sharing??i.e. the ability for users to look
over the logs of past support interactions to deter-
mine whether there is a documented, immediately-
applicable solution to their current problem?on a
scale previously unimaginable. This research is
targeted at this task of enhanced support sharing,
in the form of text mining over troubleshooting-
oriented web user forum data (Baldwin et al, to
appear).
One facet of our proposed strategy for enhanc-
ing information access to troubleshooting-oriented
web user forum data is to preprocess threads to
uncover the ?content structure? of the thread, in
the form of its post-to-post discourse structure.
Specifically, we identify which earlier post(s) a
given post responds to (linking) and in what man-
ner (tagging), in an amalgam of dialogue act tag-
ging (Stolcke et al, 2000) and coherence-based
discourse analysis (Carlson et al, 2001; Wolf and
Gibson, 2005). The reason we do this is gauge
the relative role/import of individual posts, to in-
dex and weight component terms accordingly, ul-
timately in an attempt to enhance information ac-
cess. Evidence to suggest that this structure can
enhance information retrieval effectiveness comes
from Xi et al (2004) and Seo et al (2009) (see
Section 2).
To illustrate the task, consider the thread from
the CNET forum shown in Figure 1, made up of
5 posts (Post 1, ..., Post 5) with 4 distinct partici-
pants (A, B, C, D). In the first post, A initiates the
thread by requesting assistance in creating a web
form. In response, B proposes a Javascript-based
solution (i.e. responds to the first post with a pro-
posed solution), and C proposes an independent
solution based on .NET (i.e. also responds to the
first post with a proposed solution). Next, A re-
sponds to C?s post asking for details of how to in-
clude this in a web page (i.e. responds to the third
post asking for clarification), and in the final post,
D proposes a different solution again (i.e. responds
to the first post with a different solution again).
192
HTML Input Code - CNET Coding & scripting Forums
User A HTML Input Code
Post 1 . . . Please can someone tell me how to create an
input box that asks the user to enter their ID,
and then allows them to press go. It will then
redirect to the page . . .
User B Re: html input code
Post 2 Part 1: create a form with a text field. See
. . . Part 2: give it a Javascript action . . .
User C asp.net c# video
Post 3 Ive prepared for you video.link click . . .
User A Thank You!
Post 4 Thanks a lot for that . . . I have Microsoft Vi-
sual Studio 6, what program should I do this
in? Lastly, how do I actually include this in my
site?. . .
User D A little more help
Post 5 . . . You would simply do it this way: . . . You
could also just . . . An example of this is:. . .
Figure 1: Snippeted posts in a CNET thread



	
A



	ABCDAEBF
EBFEBF
Figure 2: Post links and dialogue act labels for the
example thread in Figure 1
In this, we therefore end up with a tree-based de-
pendency link structure, with each post (other than
the initial post) relating back to a unique preced-
ing post via a range of link types, as indicated in
Figure 2. Note, however, that more generally, it
is possible for a post to link to multiple preced-
ing posts (e.g. refuting one proposed solution, and
proposing a different solution to the problem in the
initial post).
Our primary contributions in this paper are: (1)
a novel post label set for post structure in web
forum data, and associated dataset; and (2) a se-
ries of results for post dependency linking and la-
belling, which achieve strong results for the re-
spective tasks.
2 Related Work
Related work exists in the broad fields of dialogue
processing, discourse analysis and information re-
trieval, and can be broken down into the following
tasks: (1) dialogue act tagging; (2) discourse ?dis-
entanglement?; (3) community question answer-
ing; and (4) newsgroup/user forum search.
Dialogue act (DA) tagging is a means of cap-
turing the function of a given utterance relative
to an encompassing discourse, and has been pro-
posed variously as a means of enhancing dialogue
summarisation (Murray et al, 2006), and track-
ing commitments and promises in email (Cohen
et al, 2004; Lampert et al, 2008), as well as be-
ing shown to improve speech recognition accu-
racy (Stolcke et al, 2000). A wide range of DA
tag sets have been proposed, usually customised
to a particular medium such as speech dialogue
(Stolcke et al, 2000; Shriberg et al, 2004), task-
focused email (Cohen et al, 2004; Wang et al,
2007; Lampert et al, 2008) or instant messag-
ing (Ivanovic, 2008). The most immediately rel-
evant DA-based work we are aware of is that of
Xi et al (2004), who proposed a 5-way classifi-
cation for newsgroup data (including QUESTION
and AGREEMENT/AMMENDMENT), but did not
present any results based on the tagset.
A range of supervised models have been applied
to DA classification, including graphical mod-
els (Ji and Bilmes, 2005), kernel methods (Wang
et al, 2007), dependency networks (Carvalho
and Cohen, 2005), transformation-based learning
(Samuel et al, 1998), maxent models (Ang et
al., 2005) and HMMs (Ivanovic, 2008). There is
some contention about the import of context in DA
classification, with the prevailing view being that
context aids classification (Carvalho and Cohen,
2005; Ang et al, 2005; Ji and Bilmes, 2005), but
also evidence to suggest that strictly local mod-
elling is superior (Ries, 1999; Serafin and Di Eu-
genio, 2004).
In this work, we draw on existing work (esp.
Xi et al (2004)) in proposing a novel DA tag
set customised to the analysis of troubleshooting-
oriented web user forums (Section 3), and com-
pare a range of text classification and structured
classification methods for post-level DA classifi-
cation.
Discourse disentanglement is the process of
automatically identifying coherent sub-discourses
in a single thread (in the context of user fo-
rums/mailing lists), chat session (in the context of
IRC chat data: Elsner and Charniak (2008)), sys-
tem interaction (in the context of HCI: Lemon et
al. (2002)) or document (Wolf and Gibson, 2005).
The exact definition of what constitutes a sub-
discourse varies across domains, but for our pur-
poses, entails an attempt to resolve the informa-
193
tion need of the initiator by a particular approach;
if there are competing approaches proposed in a
single thread, multiple sub-discourses will neces-
sarily arise. The data structure used to represent
the disentangled discourse varies from a simple
connected sub-graph (Elsner and Charniak, 2008),
to a stack/tree (Grosz and Sidner, 1986; Lemon
et al, 2002; Seo et al, 2009), to a full directed
acyclic graph (DAG: Rose? et al (1995), Wolf and
Gibson (2005), Schuth et al (2007)). Disentan-
glement has been carried out via analysis of di-
rect citation/user name references (Schuth et al,
2007; Seo et al, 2009), topic modelling (Lin et al,
2009), and clustering over content-based features
for pairs of posts, optionally incorporating various
constraints on post recency (Elsner and Charniak,
2008; Wang et al, 2008; Seo et al, 2009).
In this work, we follow Rose? et al (1995) and
Wolf and Gibson (2005) in adopting a DAG repre-
sentation of discourse structure, and draw on the
wide set of features used in discourse entangle-
ment to model coherence.
Community question answering (cQA) is the
task of identifying question?answer pairs in a
given thread, e.g. for the purposes of thread sum-
marisation (Shrestha and McKeown, 2004) or au-
tomated compilation of resources akin to Yahoo!
Answers. cQA has been applied to both mail-
ing list and user forum threads, conventionally
based on question classification, followed by rank-
ing of candidate answers relative to each question
(Shrestha and McKeown, 2004; Ding et al, 2008;
Cong et al, 2008; Cao et al, 2009). The task is
somewhat peripheral to our work, but relevant in
that it involves the implicit tagging of certain posts
as containing questions/answers, as well as link-
ing the posts together. Once again, we draw on the
features used in cQA in this research.
There has been a spike of recent interest in
newsgroup/user forum search. Xi et al (2004)
proposed a structured information retrieval (IR)
model for newsgroup search, based on author fea-
tures, thread structure (based on the tree defined by
the reply-to structure), thread ?topology? features
and content-based features, and used a supervised
ranking method to improve over a baseline IR sys-
tem. Elsas and Carbonell (2009) ? building on
earlier work on blog search (Elsas et al, 2008) ?
proposed a probabilistic IR approach which ranks
user forum threads relative to selected posts in the
overall thread, and again demonstrated the superi-
ority of this method over a model which ignores
thread structure. Finally, Seo et al (2009) auto-
matically derived thread structure from user forum
threads, and demonstrated that the IR effectiveness
over the ?threaded? structure was superior to that
using a monolithic document representation.
The observations and results of Xi et al (2004)
and Seo et al (2009) that threading information
(or in our case ?disentangled? DAG structure) en-
hances IR effectiveness is a core motivator for this
research.
3 Post Label Set
Our post label set contains 12 categories, intended
to capture the typical interactions that take place in
troubleshooting-oriented threads on technical fo-
rums. There are 2 super-categories (QUESTION,
ANSWER) and 3 singleton classes (RESOLUTION,
REPRODUCTION, and OTHER). QUESTION, in
turn, contains 4 sub-classes (QUESTION, ADD,
CONFIRMATION, CORRECTION), while ANSWER
contains 5 sub-classes (ANSWER, ADD, CONFIR-
MATION, CORRECTION, and OBJECTION), par-
tially mirroring the sub-structure of QUESTION.
We represent the amalgam of a super- and sub-
class as QUESTION-ADD, for example.
All tags other than QUESTION-QUESTION and
OTHER are relational, i.e. relate a given post to a
unique earlier post. A given post can potentially
be labelled with multiple tags (e.g. confirm details
of a proposed solution, in addition to providing ex-
tra details of the problem), although, based on the
strictly chronological ordering of posts in threads,
a post can only link to posts earlier in the thread
(and can also not cross thread boundaries). Addi-
tionally, the link structure is assumed to be tran-
sitive, in that if post A links to post B and post B
to post C, post A is implicitly linked to post C. As
such, an explicit link from post A to post C should
exist only in the case that the link between them is
not inferrable transitively.
Detailed definitions of each post tag are given
below. Note that initiator refers to the user who
started the thread with the first post.
QUESTION-QUESTION (Q-Q): the post con-
tains a new question, independent of the
thread context that precedes it. In general,
QUESTION-QUESTION is reserved for the
first post in a given thread.
QUESTION-ADD (Q-ADD): the post supple-
194
ments a question by providing additional
information, or asking a follow-up question.
QUESTION-CONFIRMATION (Q-CONF): the
post points out error(s) in a question without
correcting them, or confirms details of the
question.
QUESTION-CORRECTION (Q-CORR): the post
corrects error(s) in a question.
ANSWER-ANSWER (A-A): the post proposes an
answer to a question.
ANSWER-ADD (A-ADD): the post supplements
an answer by providing additional informa-
tion.
ANSWER-CONFIRMATION (A-CONF): the
post points out error(s) in an answer without
correcting them, or confirms details of the
answer.
ANSWER-CORRECTION (A-CORR): the post
corrects error(s) in an answer.
ANSWER-OBJECTION (A-OBJ): the post ob-
jects to an answer on experiential or theoreti-
cal grounds (e.g. It won?t work.).
RESOLUTION (RES): the post confirms that an
answer works, on the basis of implementing
it.
REPRODUCTION (REP): the post either: (1)
confirms that the same problem is being ex-
perienced (by a non-initiator, e.g. I?m seeing
the same thing.); or (2) confirms that the an-
swer should work.
OTHER (OTHER): the post does not belong to
any of the above classes.
4 Feature Description
In this section, we describe our post feature repre-
sentation, in the form of four feature types.
4.1 Lexical features
As our first feature type, we use simple lexical fea-
tures, in the form of unigram and bigram tokens
contained within a given post (without stopping).
We also POS tagged and lemmatised the posts,
postfixing the lemmatised token with its POS tag
(using Lingua::EN::Tagger and morpha (Min-
nen et al, 2001)). Finally, we bin together the
counts for each token, and represent it via its raw
frequency.
4.2 Structural features
The identity of the post author, and position of the
post within the thread, can be indicators of the
post/link structure of a given post. We represent
the post author as a simple binary feature indicat-
ing whether s/he is the thread initiator, and the post
position via its relative position in the thread (as a
ratio, relative to the total number of posts).
4.3 Post context features
As mentioned in Section 2, post context has gen-
erally (but not always) been shown to enhance the
classification accuracy of DA tagging tasks, in the
form of Markov features providing predicted post
labels for previous posts, or more simply, post-to-
post similarity. We experiment with a range of
post context features, all of which are compatible
with features both from the same label set as that
being classified (e.g. link features for link classifi-
cation), as well as features from a second label set
(e.g. DA label features for link classification).
Previous Post: There is a strong prior for posts
to link to their immediately preceding post (as ob-
served for 79.9% of the data in our dataset), and
also strong sequentiality in our post label set (e.g.
a post following a Q-Q is most likely to be an A-
A). As such, we represent the predicted post label
of the immediately preceding post, as a first-order
Markov feature, as well as a binary feature to in-
dicate whether the author of the previous post also
authored the current post.
Previous Post from Same Author: A given
user tends to author posts of the same basic type
(e.g. QUESTION or ANSWER) in a given thread,
and pairings such as A-A and A-CONF from a
given author are very rare. To capture this obser-
vation, we look to see if the author of the current
post has posted earlier in the thread, and if so, in-
clude the label and relative location (in posts) of
their most recent previous post.
Full History: As a final option, we include the
predictions for all posts P1, ..., Pi?1 preceding the
current post Pi.
4.4 Semantic features
We tested four semantic features based on post
content and title.
195
Title Similarity: For forums such as CNET
which include titles for individual posts (as rep-
resented in Figure 1), a post having the same or
similar title as a previous post is often a strong
indicator that it responds to that post. This both
provides a strong indicator of which post a given
post responds (links) to, and can aid in DA tag-
ging. We use simple cosine similarity to find the
post with the most-similar title, and represent its
relative location to the current post.
Post Similarity: Posts of the same general type
tend to have similar content and be linked. For
example, A-A and A-ADD posts tend to share
content. We capture this by identifying the post
with most-similar content based on cosine similar-
ity, and represent its relative location to the current
post.
Post Characteristics: We separately represent
the number of question marks, exclamation marks
and URLs in the current post. In general, ques-
tion marks occur in QUESTION and CONFIRMA-
TION posts, exclamation marks occur in RES and
OBJECTION posts, and URLs occur in A-A and
A-ADD posts.
User Profile: Some authors tend to answer ques-
tions more, while others tend to ask more ques-
tions. We capture the class priors for the author of
the current post by the distribution of post labels
in their posts in the training data.
5 Experimental Setup
As our dataset, we collected 320 threads contain-
ing a total of 1,332 posts from the Operating Sys-
tem, Software, Hardware, and Web Development
sub-forums of CNET.1
The annotation of post labels and links was car-
ried by two annotators in a custom-built web inter-
face which supported multiple labels and links for
a given post. For posts with multilabels, we used
a modified version of Cohen?s Kappa, which re-
turned ? values of 0.59 and 0.78 for the post label
and link annotations, respectively. Any disagree-
ments in labelling were resolved through adjudi-
cation.
Of the 1332 posts, 65 posts have multiple labels
(which possibly link to a common post) and 22
posts link to two different links. The majority post
label in the dataset is A-A (40.30%).
1http://forums.cnet.com/?tag=
TOCleftColumn.0
We built machine learners using a conven-
tional Maximum Entropy (ME) learner,2 as well as
two structural learners, namely: (1) SVM-HMMs
(Joachims et al, 2009), as implemented in SVM-
struct3, with a linear kernel; and (2) conditional
random fields (CRFs) using CRF++.4 SVM-
HMMs and CRFs have been successfully applied
to a range of sequential tagging tasks such as
syllabification (Bartlett et al, 2009), chunk pars-
ing (Sha and Pereira, 2003) and word segmen-
tation (Zhao et al, 2006). Both are discrimina-
tive models which capture structural dependen-
cies, which is highly desirable in terms of mod-
elling sequential preferences between post labels
(e.g. A-CONF typically following a A-A). SVM-
HMM has the additional advantage of scaling to
large numbers of features (namely the lexical fea-
tures). As such, we only experiment with lexical
features for SVM-HMM and ME.
All of our evaluation is based on stratified 10-
fold cross-validation, stratifying at the thread level
to ensure that if a given post is contained in the
test data for a given iteration, all other posts in
that same thread are also in the test data (or more
pertinently, not in the training data). We evalu-
ate using micro-averaged precision, recall and F-
score (? = 1). We test the statistical significance
of all above-baseline results using randomised es-
timation (p < 0.05; Yeh (2000)), and present all
such results in bold in our results tables.
In our experiments, we first look at the post
classification task in isolation (i.e. we predict
which labels to associate with each post, under-
specifying which posts those labels relate to). We
then move on to look at the link classification task,
again in isolation (i.e. we predict which previous
posts each post links to, underspecifying the na-
ture of the link). Finally, we perform preliminary
investigation of the joint task of DA and link clas-
sification, by incorporating DA class features into
the link classification task.
6 DA Classification Results
Our first experiment is based on post-level dia-
logue act (DA) classification, ignoring link struc-
ture in the first instance. That is, we predict the
labels on edges emanating from each post in the
DAG representation of the post structure, without
2http://maxent.sourceforge.net/
3http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
4http://crfpp.sourceforge.net/
196
Features CRF SVM-HMM ME
Lexical ? .566 .410
Structural .742 .638 .723
Table 1: DA classification F-score with lexical and
structural features (above-baseline results in bold)
specifying the edge destination. Returning to our
example in Figure 2, e.g., the gold-standard clas-
sification for Post 1 would be Q-Q, Post 2 would
be A-A, etc.
As a baseline for DA classification, simple ma-
jority voting attains an F-score of 0.403, based on
the A-A class. A more realistic baseline, how-
ever, is a position-conditioned variant, where the
first post is always classified as Q-Q, and all sub-
sequent posts are classified as A-A, achieving an
F-score of 0.641.
6.1 Lexical and structural features
First, we experiment with lexical and structural
features (recalling that we are unable to scale the
CRF model to full lexical features). Lexical fea-
tures produce below-baseline performance, while
simple structural features immediately lead to an
improvement over the baseline for CRF and ME.
The reason for the poor performance with lex-
ical features is that our dataset contains only
around 1300 posts, each of which is less than 100
words in length on average. The models are sim-
ply unable to generalise over this small amount of
data, and in the case of SVM-HMM, the presence
of lexical features, if anything, appears to obscure
the structured nature of the labelling task (i.e. the
classifier is unable to learn the simple heuristic
used by the modified majority class baseline).
The success of the structural features, on the
other hand, points to the presence of predictable
sequences of post labels in the data. That SVM-
HMM is unable to achieve baseline performance
with structural features is slightly troubling.
6.2 Post context features
Next, we test the two post context features: Previ-
ous Post (P) and Previous Post from Same Author
(A). Given the success of structural features, we
retain these in our experiments. Note that the la-
bels used in the post context are those which are
interactively learned by that model for the previ-
ous posts.
Table 2 presents the results for structural fea-
Features CRF SVM-HMM ME
Struct+R .740 .640 .632
Struct+A .742 .676 .693
Struct+F .744 .641 .577
Struct+RA .397 .636 .665
Struct+AF .405 .642 .586
Table 2: DA classification F-score with structural
and DA-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
tures combined with DA-based post context; we
do not present any combinations of Previous Post
and Full History, as Full History includes the Pre-
vious Post.
Comparing back to the original results using
only the structural results, we can observe that Pre-
vious Post from Same Author and Full History (A
and F, resp., in the table) lead to a slight incre-
ment in F-score for both CRF and SVM-HMM,
but degrade the performance of ME. Previous Post
leads to either a marginal improvement, or a drop
in results, most noticeably for ME. It is slightly
surprising that the CRF should benefit from con-
text features at all, given that it is optimising over
the full tag sequence, but the impact is relatively
localised, and when all sets of context features
are used, the combined weight of noisy features
appears to swamp the learner, leading to a sharp
degradation in F-score.
6.3 Semantic features
We next investigate the relative impact of the se-
mantic features, once again including structural
features in all experiments. Table 3 presents the F-
score using the different combinations of semantic
features.
Similarly to the post context features, the se-
mantic features produced slight increments over
the structural features in isolation, especially for
CRF and ME. For the first time, SVM-HMM
achieved above-baseline results, when incorporat-
ing title similarity and post characteristics. Of the
individual semantic features, title and post simi-
larity appear to be the best performers. Slightly
disappointingly, the combination of semantic fea-
tures generally led to a degradation in F-score, al-
most certainly due to data sparseness. The best
overall result was achieved with CRF, incorporat-
197
Features CRF SVM-HMM ME
Struct+T .751 .636 .660
Struct+P .747 .636 .662
Struct+C .738 .587 .630
Struct+U .722 .564 .620
Struct+TP .740 .627 .720
Struct+TC .744 .646 .589
Struct+TU .738 .600 .609
Struct+PC .745 .630 .583
Struct+PU .736 .626 .605
Struct+CU .730 .599 .619
Struct+TPC .739 .622 .580
Struct+TPU .729 .613 .6120
Struct+TCU .750 .611 .6120
Struct+PCU .738 .616 .614
Struct+TPCU .737 .619 .605
Table 3: DA classification F-score with semantic
features (T = ?Title Similarity?, P = ?Post Simi-
larity?, C = ?Post Characteristics?, and U = ?User
Profile?; above-baseline results in bold)
ing structural features and title similarity, at an F-
score of 0.751.
To further explore the interaction between post
context and semantic features, we built CRF clas-
sifiers for different combinations of post context
and semantic features, and present the results in
Table 4.5 We achieved moderate gains in F-score,
with all post context features, in combination with
structural features, post similarity and post char-
acteristics achieving an F-score of 0.753, slightly
higher than the best result achieved for just struc-
tural and post context features.
It is important to refer back to the results for
lexical features (comparable to what would have
been achieved with a standard text categorisation
approach to the task), and observe that we have
achieved far higher F-scores using features cus-
tomised to user forum data. It is also important
to reflect that post context (in terms of the features
and the structured classification results of CRF)
appears to markedly improve our results, contrast-
ing with the results of Ries (1999) and Serafin and
Di Eugenio (2004).
5We omit the results for Full History post context for rea-
sons of space, but there is relatively little deviation from the
numbers presented.
Features R A RA
Struct+T .649 .649 .649
Struct+P .737 .736 .742
Struct+C .741 .741 .742
Struct+U .745 .742 .737
Struct+TP .645 .656 .658
Struct+TC .383 .402 .408
Struct+TU .650 .652 .652
Struct+PC .730 .743 .753
Struct+PU .232 .232 .286
Struct+CU .719 .471 .710
Struct+TPC .498 .469 .579
Struct+TPU .248 .232 .248
Struct+TCU .388 .377 .380
Struct+PCU .231 .231 .261
Struct+TPCU .231 .231 .231
Table 4: DA classification F-score for CRF with
different combinations of post context features and
semantic features (R = ?Previous Post?, and A
= ?Previous Post from Same Author?; T = ?Ti-
tle Similarity?, P = ?Post Similarity?, C = ?Post
Characteristics?, and U = ?User Profile?; above-
baseline results in bold)
7 Link Classification Results
Our second experiment is based on link classifi-
cation in isolation. Here, we predict unlabelled
edges, e.g. in Figure 2, the gold-standard classifi-
cation for Post 1 would be NULL, Post 2 would be
Post 1, Post 3 would be Post 1, etc.
Note that the initial post cannot link to any other
post, and also that the second post always links
to the first post. As this is a hard constraint on
the data, and these posts simply act to inflate the
overall numbers, we exclude all first and second
posts from our evaluation of link classification.
We experimented with a range of baselines as
presented in Table 5, but found that the best per-
former by far was the simple heuristic of linking
each post (except for the initial post) to its imme-
diately preceding post. This leads to an F-score of
0.631, comparable to that for the post classifica-
tion task.
7.1 Lexical and structural features
Once again, we started by exploring the effective-
ness of lexical and structural features using the
three learners, as detailed in Table 6.
Similarly to the results for post classification,
198
Baseline Prec Rec F-score
Previous post .641 .622 .631
First post .278 .269 .274
Title similarity .311 .301 .306
Post similarity .255 .247 .251
Table 5: Baselines for link classification
Features CRF SVM-HMM ME
Lexical ? .154 .274
Structural .446 .220 .478
Table 6: Link classification F-score with lexical
and structural features (above-baseline results in
bold)
structural features are more effective than lexical
features for link classification, but this time, nei-
ther feature set approaches the baseline F-score
for any of the learners. Once again, the results for
SVM-HMM are well below those for the other two
learners.
7.2 Post context features
Next, we experiment with link-based post con-
text features, in combination with the structural
features, as the results were found to be consis-
tently better when combined with the structural
features (despite the below-baseline performance
of the structural features in this case). The link-
based post context features in all cases are gener-
ated using the CRF with structural features from
Table 6. As before, we do not present any combi-
nations of Previous Post and Full History, as Full
History includes the Previous Post
As seen in Table 9, here, for the first time, we
achieve an above-baseline result for link classifi-
cation, for SVM and ME based on Previous Post
from Same Author in isolation, and also some-
times in combination with the other feature sets.
The results for CRF also improve, but not to a
level of statistical significance over the baseline.
Similarly to the results for DA classification, the
results for CRF drop appreciably when we com-
bine feature sets.
7.3 Semantic features
Finally, we experiment with semantic features,
once again in combination with structural features.
The results are presented in Table 8.
The results for semantic features largely mir-
Features CRF SVM-HMM ME
Struct+R .234 .605 .618
Struct+A .365 .665 .665
Struct+F .624 .648 .615
Struct+RA .230 .615 .661
Struct+AF .359 .663 .621
Table 7: Link classification F-score with structural
and link-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
Features CRF SVM-HMM ME
Struct+T .464 .223 .477
Struct+P .433 .198 .453
Struct+C .438 .213 .419
Struct+U .407 .160 .376
Struct+TP .459 .194 .491
Struct+TC .449 .229 .404
Struct+TU .456 .174 .353
Struct+PC .422 .152 .387
Struct+PU .439 .166 .349
Struct+CU .397 .178 .366
Struct+TPC .449 .185 .418
Struct+TPU .449 .160 .365
Struct+TCU .459 .185 .358
Struct+PCU .439 .161 .358
Struct+TPCU .443 .163 .365
Table 8: Link classification F-score with semantic
features (T = ?Title Similarity?, P = ?Post Simi-
larity?, C = ?Post Characteristics?, and U = ?User
Profile?; above-baseline results in bold)
ror those for post classification: small improve-
ments are observed for title similarity with CRF,
but otherwise, the results degrade across the board,
and the combination of different feature sets com-
pounds this effect.
The best overall result achieved for link classifi-
cation is thus the 0.743 for CRF with the structural
and post context features.
We additionally experimented with combina-
tions of features as for post classification, but were
unable to improve on this result.
7.4 Link Classification using DA Features
Ultimately, we require both DA and link classifica-
tion of each post, which is possible by combining
the outputs of the component classifiers described
199
Features CRF SVM-HMM ME
Struct+R .586 .352 .430
Struct+A .591 .278 .568
Struct+F .704 .477 .546
Struct+RA .637 .384 .551
Struct+AF .743 .527 .603
Table 9: Link classification F-score with structural
and post-based post context features (R = ?Previ-
ous Post?, A = ?Previous Post from Same Author?,
and F = ?Full History?; above-baseline results in
bold)
above, by rolling the two tasks into a single clas-
sification task, or alternatively by looking to joint
modelling methods. As a preliminary step in this
direction, and means of exploring the interaction
between the two tasks, we repeat the experiment
based on post context features from above (see
Section 7.2), but rather than using link-based post
context, we use DA-based post context.
As can be seen in Table 9, the results for SVM-
HMM and ME drop appreciably as compared to
the results using link-based post context in Table 9,
while the results for CRF jump to the highest level
achieved for the task for all three learners. The
effect can be ascribed to the ability of CRF to
natively model the (bidirectional) link classifica-
tion history in the process of performing structured
learning, and the newly-introduced post features
complementing the link classification task.
8 Discussion and Future Work
Ultimately, we require both DA and link classifica-
tion of each post, which is possible in (at least) the
following three ways: (1) by combining the out-
puts of the component classifiers described above;
(2) by rolling the two tasks into a single classifi-
cation task; or (3) by looking to joint modelling
methods. Our results in Section 7.4 are suggestive
of the empirical potential of performing the two
tasks jointly, which we hope to explore in future
work.
One puzzling effect observed in our experi-
ments was the generally poor results for SVM. Er-
ror analysis indicates that the classifier was heav-
ily biased towards the high-frequency classes, e.g.
classifying all posts as either Q-Q or A-A for DA
classification. The classifications for the other two
learners were much more evenly spread across the
different classes.
CRF was limited in that it was unable to cap-
ture lexical features, but ultimately, lexical fea-
tures were found to be considerably less effec-
tive than structural and post context features for
both tasks, and the ability of the CRF to opti-
mise the post labelling over the full sequence of
posts in a thread more than compensated for this
shortcoming. Having said this, there is more work
to be done exploring synergies between the dif-
ferent feature sets, especially for DA classifica-
tion where all feature sets were found to produce
above-baseline results.
Another possible direction for future research is
to explore the impact of inter-post time on link
structure, based on the observation that follow-
up posts from the initiator tend to be tempo-
rally adjacent to posts they respond to with rela-
tively short time intervals, while posts from non-
initiators which are well spaced out tend not to re-
spond to one another. Combining this with pro-
filing of the cross-thread behaviour of individual
forum participants (Weimer et al, 2007; Lui and
Baldwin, 2009), and formal modelling of ?forum
behaviour? is also a promising line of research,
taking the lead from the work of Go?tz et al (2009),
inter alia.
9 Conclusion
In this work, we have proposed a method for
analysing post-to-post discourse structure in on-
line user forum data, in the form of post link-
ing and dialogue act tagging. We introduced
three feature sets: structural features, post con-
text features and semantic features. We exper-
imented with three learners (maximum entropy,
SVM-HMM and CRF), and established that CRF
is the superior approach to the task, achieving
above-baseline results for both post and link clas-
sification. We also demonstrated the complemen-
tarity of the proposed feature sets, especially for
the post classification task, and carried out a pre-
liminary exploration of the interaction between the
linking and dialogue act tagging tasks.
Acknowledgements
This research was supported in part by funding
from Microsoft Research Asia.
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
200
tion in multiparty meetings. In Proceedings of
the 2005 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2005),
pages 1061?1064, Philadelphia, USA.
Timothy Baldwin, David Martinez, Richard Penman,
Su Nam Kim, Marco Lui, Li Wang, and Andrew
MacKinlay. to appear. Intelligent Linux informa-
tion access by data mining: the ILIAD project. In
Proceedings of the NAACL 2010 Workshop on Com-
putational Linguistics in a World of Social Media:
#SocialMedia, Los Angeles, USA.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2009. On the syllabification of phonemes. In Pro-
ceedings of the North American Chapter of the As-
sociation for Computational Linguistics ? Human
Language Technologies 2009 (NAACL HLT 2009),
pages 308?316, Boulder, USA.
Xin Cao, Gao Cong, Bin Cui, Christian S. Jensen, and
Ce Zhang. 2009. The use of categorization infor-
mation in language models for question retrieval. In
Proceedings of the 18th ACM Conference on Infor-
mation and Knowledge Management (CIKM 2009),
pages 265?274, Hong Kong, China.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged
corpus in the framework of rhetorical structure the-
ory. In Proceedings of the Second SIGdial Work-
shop on Discourse and Dialogue, pages 1?10, Aal-
borg, Denmark. Association for Computational Lin-
guistics Morristown, NJ, USA.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email ?speech acts?.
In Proceedings of 28th International ACM-SIGIR
Conference on Research and Development in Infor-
mation Retrieval (SIGIR 2005), pages 345?352.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 309?316,
Barcelona, Spain.
Gao Cong, Long Wang, Chin-Yew Lin, Young-In
Song, and Yueheng Sun. 2008. Finding question-
answer pairs from online forums. In Proceedings of
31st International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?08), pages 467?474, Singapore.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan
Zhu. 2008. Using conditional random fields to ex-
tract context and answers of questions from online
forums. In Proceedings of the 46th Annual Meet-
ing of the ACL: HLT (ACL 2008), pages 710?718,
Columbus, USA.
Jonathan L. Elsas and Jaime G. Carbonell. 2009. It
pays to be picky: An evaluation of thread retrieval
in online forums. In Proceedings of 32nd Inter-
national ACM-SIGIR Conference on Research and
Development in Information Retrieval (SIGIR?09),
pages 714?715, Boston, USA.
Jonathan L. Elsas, Jaime Arguello, Jamie Callan, and
Jaime G. Carbonell. 2008. Retrieval and feed-
back models for blog feed search. In Proceedings of
31st International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR?08), pages 347?354, Singapore.
Micha Elsner and Eugene Charniak. 2008. You talk-
ing to me? a corpus and algorithm for conversation
disentanglement. In Proceedings of the 46th Annual
Meeting of the ACL: HLT (ACL 2008), pages 834?
842, Columbus, USA.
Michaela Go?tz, Jure Leskovec, Mary McGlohon, and
Christos Faloutsos. 2009. Modeling blog dynamics.
In Proceedings of the Third International Confer-
ence on Weblogs and Social Media (ICWSM 2009),
pages 26?33, San Jose, USA.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Edward Ivanovic. 2008. Automatic instant messaging
dialogue using statistical models and dialogue acts.
Master?s thesis, University of Melbourne.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of
the 2005 IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP 2005),
pages 33?36, Philadelphia, USA.
Thorsten Joachims, Thomas Finley, and Chun-
Nam John Yu. 2009. Cutting-plane training of
structural SVMs. Machine Learning, 77(1):27?59.
Andrew Lampert, Robert Dale, and Ce?cile Paris.
2008. The nature of requests and commitments in
email messages. In Proceedings of the AAAI 2008
Workshop on Enhanced Messaging, pages 42?47,
Chicago, USA.
Oliver Lemon, Alex Gruenstein, and Stanley Pe-
ters. 2002. Collaborative activities and multi-
tasking in dialogue systems. Traitement Automa-
tique des Langues (TAL), Special Issue on Dialogue,
43(2):131?154.
Chen Lin, Jiang-Ming Yang, Rui Cai, Xin-Jing Wang,
Wei Wang, and Lei Zhang. 2009. Modeling se-
mantics and structure of discussion threads. In Pro-
ceedings of the 18th International Conference on the
World Wide Web (WWW 2009), pages 1103?1104,
Madrid, Spain.
Marco Lui and Timothy Baldwin. 2009. You are what
you post: User-level features in threaded discourse.
In Proceedings of the Fourteenth Australasian Doc-
ument Computing Symposium (ADCS 2009), Syd-
ney, Australia.
201
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and dis-
course features into speech summarization. In Pro-
ceedings of the Main Conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367?374.
Klaus Ries. 1999. HMM and neural network
based speech act detection. In Proceedings of the
1999 IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP-99), pages
497?500, Phoenix, USA.
Carolyn Penstein Rose?, Barbara Di Eugenio, Lori S.
Levin, and Carol Van Ess-Dykema. 1995.
Discourse processing of dialogues with multiple
threads. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 31?38, Cambridge, USA.
Ken Samuel, Carbeery Sandra Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proceedings of
the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguis-
tics (COLING/ACL-98), pages 1150?1156, Mon-
treal, Canada.
Anne Schuth, Maarten Marx, and Maarten de Rijke.
2007. Extracting the discussion structure in com-
ments on news-articles. In Proceedings of the 9th
Annual ACM International Workshop on Web Infor-
mation and Data Management, pages 97?104, Lis-
boa, Portugal.
Jangwon Seo, W. Bruce Croft, and David A. Smith.
2009. Online community search using thread struc-
ture. In Proceedings of the 18th ACM Conference
on Information and Knowledge Management (CIKM
2009), pages 1907?1910, Hong Kong, China.
Riccardo Serafin and Barbara Di Eugenio. 2004.
FLSA: Extending latent semantic analysis with fea-
tures for dialogue act classification. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL 2004), pages 692?
699, Barcelona, Spain.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 3rd International Conference on Human Lan-
guage Technology Research and 4th Annual Meeting
of the NAACL (HLT-NAACL 2003), pages 213?220,
Edmonton, Canada.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conver-
sations. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 889?895, Geneva, Switzerland.
Elinzabeth Shriberg, Raj Dhillon, Sonali Bhagat,
Jeremy Ang, and Hannah Carvey. 2004. The ICSI
meeting recorder dialog act (MRDA) corpus. In
Proceedings of the 5th SIGdial Workshop on Dis-
course and Dialogue, pages 97?100, Cambridge,
USA.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Pail Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialogue Act Mod-
eling for Automatic Tagging and Recognition of
Conversational Speech. Computational Linguistics,
26(3):339?373.
Yi-ChiaWang, Mahesh Joshi, and Carolyn Rose?. 2007.
A feature based approach to leveraging context for
classifying newsgroup style discussion segments. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions (ACL 2007), pages 73?76, Prague, Czech Re-
public.
Yi-Chia Wang, Mahesh Joshi, William W. Cohen, and
Carolyn Rose?. 2008. Recovering implicit thread
structure in newsgroup style conversations. In Pro-
ceedings of the Second International Conference on
Weblogs and Social Media (ICWSM 2008), pages
152?160, Seattle, USA.
Markus Weimer, Iryna Gurevych, and Max
Mu?hlha?user. 2007. Automatically assessing
the post quality in online discussions on software.
In Proceedings of the 45th Annual Meeting of
the ACL: Interactive Poster and Demonstration
Sessions, pages 125?128, Prague, Czech Republic.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Com-
putational Linguistics, 31(2):249?287.
Wensi Xi, Jesper Lind, and Eric Brill. 2004. Learning
effective ranking functions for newsgroup search.
In Proceedings of 27th International ACM-SIGIR
Conference on Research and Development in In-
formation Retrieval (SIGIR 2004), pages 394?401.
Sheffield, UK.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th International Conference on
Computational Linguistics (COLING 2000), pages
947?953, Saarbru?cken, Germany.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved Chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162?165. Sydney, Australia.
202
