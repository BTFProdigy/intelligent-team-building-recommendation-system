A New Probabilistic Model for Title Generation 
 
Rong Jin 
Language Technology Institute 
Carnegie Mellon University  
5000 Forbes Ave. 
Pittsburgh, PA15213, U. S. A. 
rong+@cs.cmu.edu 
Alexander G. Hauptmann 
Department of Computer Science 
Carnegie Mellon University 
5000 Forbes Ave. 
Pittsburgh, PA15213, U. S. A. 
alex+@cs.cmu.edu 
 
Abstract  
Title generation is a complex task involving 
both natural language understanding and 
natural language synthesis. In this paper, we 
propose a new probabilistic model for title 
generation. Different from the previous 
statistical models for title generation, which 
treat title generation as a generation process 
that converts the ?document representation? 
of information directly into a ?title 
representation? of the same information, this 
model introduces a hidden state called 
?information source? and divides title 
generation into two steps, namely the step of 
distilling the ?information source? from the 
observation of a document and the step of 
generating a title from the estimated 
?information source?. In our experiment, the 
new probabilistic model outperforms the 
previous model for title generation in terms 
of both automatic evaluations and human 
judgments. 
Introduction 
Compared with a document, a title provides a 
compact representation of the information and 
therefore helps people quickly capture the main 
idea of a document without spending time on the 
details. Automatic title generation is a complex 
task, which not only requires finding the title 
words that reflects the document content but also 
demands ordering the selected title words into 
human readable sequence. Therefore, it involves 
in both nature language understanding and 
nature language synthesis, which distinguishes 
title generation from other seemingly similar 
tasks such as key phrase extraction or automatic 
text summarization where the main concern of 
tasks is identify important information units 
from documents (Mani & Maybury., 1999). 
 
The statistical approach toward title generation 
has been proposed and studied in the recent 
publications (Witbrock & Mittal, 1999; Kennedy 
& Hauptmann, 2000; Jin & Hauptmann, 2001). 
The basic idea is to first learn the correlation 
between the words in titles (title words) and the 
words in the corresponding documents 
(document words) from a given training corpus 
consisting of document-title pairs, and then 
apply the learned title-word-document-word 
correlations to generate titles for unseen 
documents.  
 
Witbrock and Mittal (1999) proposed a 
statistical framework for title generation where 
the task of title generation is decomposed into 
two phases, namely the title word selection 
phase and the title word ordering phase. In the 
phase of title word selection, each title word is 
scored based on its indication of the document 
content. During the title word ordering phase, 
the ?appropriateness? of the word order in a title 
is scored using ngram statistical language model. 
The sequence of title words with highest score in 
both title word selection phase and title word 
ordering phase is chosen as the title for the 
document. The follow-ups within this 
framework mainly focus on applying different 
approaches to the title word selection phase (Jin 
& Hauptmann, 2001; Kennedy & Hauptmann, 
2000). 
 
However, there are two problems with this 
framework for title generation. They are: 
? A problem with the title word ordering 
phase. The goal of title word selection phase is 
to find the appropriate title words for document 
and the goal of title word ordering phase is to 
find the appropriate word order for the selected 
title words. In the framework proposed by 
Witbrock and Mittal (1999), the title word 
ordering phase is accomplished by using ngram 
language model (Clarkson & Rosenfeld, 1997) 
to predict the probability P(T), i.e. how 
frequently the word sequence T is used as a title 
for a document. Of course, the probability for 
the word sequence T to be used as a title for any 
document is definitely influenced by the 
correctness of the word order in T. However, the 
factor whether the words in the sequence T are 
common words or not will also have great 
influence on the chance of seeing the sequence T 
as a title. Word sequence T with many rare 
words, even with a perfect word order, will be 
difficult to match with the content of most 
documents and has small chance to be used as a 
title. As the result, using probability P(T) for the 
purpose of ordering title words can cause the 
generated titles to include unrelated common 
title words. The obvious solution to this problem 
is to somehow eliminate the bias of favouring 
common title words from probability P(T) and 
leave it only with the task of the word ordering.  
? A problem with the title word selection 
phase. The title word selection phase is 
responsible for coming up with a set of title 
words that reflect the meaning of the document. 
In the framework proposed by Witbrock and 
Mittal (1999), every document word has an 
equal vote for title words. However, title only 
needs to reflect the main content of a document 
not every single detail of that document. 
Therefore, letting all the words in the document 
participate equally in the selection of title words 
can cause a large variance in choosing title 
words. For example, common words usually 
have little to do with the content of documents. 
Therefore, allowing common words of a 
document equally compete with the content 
words in the same document in choosing title 
words can seriously degrade the quality of 
generated titles. 
 
The solution we proposed to this problem is to 
introduce a hidden state called ?information 
source?. This ?information source? will sample 
the important content word out of a document 
and a title will be computed based on the 
sampled ?information source? instead of the 
original document. By striping off the common 
words through the ?information source? state, we 
are able to reduce the noise introduced by 
common words to the documents in selecting 
title words. The schematic diagram for the idea 
is shown in Figure 1, together with the 
schematic diagram for the framework by 
Witbrock and Mittal. As indicated by Figure 1, 
the old framework for title generation has only a 
single ?channel? connecting the document words 
to the title words while the new model contains 
two ?channels? with one connecting the 
document words to the ?information source? 
state and the other connecting the ?information 
source? state to the title words.  
 
T itle  W o rd s 
{ T W }  
D o c u m e n t W o rd s  
{ D W }  
P (T W |D W ) 
O ld  M o d e l 
T itle  W ord s 
{ T W }  
D o c u m e n t W o rd s  
{ D W }  
In fo rm a tio n  S o u rc e  
{ D W ?: c o n te n t w o rd }  
N e w  M o d e l 
P (D W ?|D W ) 
P (T W |D W ) 
 
Fig. 1: Graphic representation for previous title generation 
model and new model for title generation. 
1 Probabilistic Title Generation Model 
In the language of probabilistic theory, the goal 
of creating a title T for a document D can be 
formalized as the search of the word sequence T 
that can be best generated by the document D, or 
)|?(maxarg
?
DTPT
T
=  (1) 
Therefore, the key of a probabilistic model for 
title generation is how to estimate the probability 
P(T|D). i.e. the probability of having a word 
sequence T as the title for the document D.  
 
In this section, we will first describe the old 
framework using probability theory and 
associate the two problems of the old framework 
with the flaw in estimation of the probability 
P(T|D). Then a solution to each of the two 
problems will be presented and the new model 
based on the old framework for title generation 
with the adaptation of the solutions will be 
described at the end of this section. 
1.1 Formal Description of Old 
Framework for Title Generation 
In terms of probability theory, the old 
framework can be interpreted as approximating 
the probability P(T|D) as a product of two terms 
with term P({tw?T}|D) responsible for the title 
word selection and term P(T) responsible for the 
title word ordering and the probability P(T|D) 
can be written as: 
)()|}({)|( TPDTtwPDTP ??
 
(2) 
where {tw?T} stands for the set of words in the 
title T. Since P({tw?T}|D) stands for the 
probability of using the set of  words tw in word 
sequence T as title words given the observation 
of the document D, it corresponds to the title 
word selection phase. P(T) stands for the 
probability of using word sequence T as a title 
for any document. Since word sequences with 
wrong word orders are rarely seen as titles for 
any document, the word order in word sequence 
T is an important factor in determining the 
frequency of seeing word sequence T as a title 
for documents and therefore it can be associated 
with the title word ordering phase. 
1.2 Problem with the title word ordering 
phase 
In the old framework for title generation, term 
P(T) is used for ordering title words into a 
correct sequence. However, term P(T) is not 
only influenced by the word order in T, but also 
whether words in T are common words. A word 
sequence T with a set of rare words will have 
small chance to be used as a title for any 
document even if the word order in T is 
perfectly correct. On the other side, a title T with 
a set of common words can have a good chance 
to be a title for some documents even its word 
order is problematic. Therefore, the probability 
for a word sequence T to be used as a title, i.e. 
P(T), is determined by both the 
?appropriateness? of the word order of T and the 
?rareness? of the words in T and doesn?t 
appropriately represent the process of title word 
ordering whose only goal is to identify a correct 
word order with the given words.  
In terms of formal analysis, the problem with the 
title word selection phase can be attributed to the 
oversimplified approximation for probability 
P(T|D). According to the chain rule in 
probability theory, the approximation for P(T|D) 
in Equation (2) is quite problematic and a more 
reasonable expansion for probability P(T|D) 
should be following: 
})({/)()|}({
})?{|()|}({)|(
TtwPTPDTtwP
TtwTPDTtwPDTP
??=
???
 (3) 
where P({tw?T}) stands for the probability of 
using the set of word {tw?T} in titles without 
considering the word order. The difference 
between Equations (3) and (2) is that, Equation 
(2) uses term P(T) directly for title word 
ordering phase while Equation (3) divides term 
P(T) by term P({tw?T}) and uses the result of 
division for title word ordering process. Because 
term P({tw?T}) concerns only with the 
popularity of the words tw in sequence T, 
dividing P(T) by P({tw?T}) has the effect of 
removing the bias of favouring popular title 
words from term P(T). Therefore, term 
P(T)/P({tw?T}) is determined mainly by the 
word order in T and not influenced by the 
popularity of title words in T.  
1.3 Problem with title word selection 
phase 
As already discussed in the introduction section, 
the old framework for title generation allows all 
the words in the document equally participate in 
selecting title words and therefore, the final 
choice of title words may be influenced 
significantly by the common words in the 
document which have nothing to do with the 
content of the document. Thus, we suggest a 
solution to this problem by introducing a hidden 
state called ?information source? which is able to 
sampled the important content words from the 
original document. To find an optimal title for a 
document, we will create the title from the 
?distilled information source? instead of the 
original document.  
 
To allow titles being generated from the 
?distilled information source? instead of the 
original document, we can expand the 
probability P(T|D) as the sum of the 
probabilities P(T| ?information source? S) over 
all the possible ?information sources? S, where 
probability P(T|S) stands for the probability of 
using the word sequence T as the title for the 
?information source? S. Formally, this idea can 
be expressed as: 
?=
S
DSPSTPDTP )|()|()|(  (4) 
where symbol S stands for a possible 
?information source? S for the document D. In 
Equation (4), term P(T|S)P(S|D) represents the 
idea of two noisy channels, with term P(S|D) 
corresponding to the first channel that samples 
?information source? S out of the original 
document D and term P(T|S) corresponding to 
the second noisy channel that creates title T 
from the ?distilled information source? S. Since 
the first noisy channel, i.e. P(S|D), is new to the 
old framework for title generation, we will focus 
on the discussion of the noisy channel P(S|D). 
 
Since the motivation of introducing the hidden 
state ?information source? S is to strip off the 
common words and have important content 
words kept, we want the noisy channel P(S|D) to 
be a sampling process where important content 
words have higher chances to be selected than 
common words. Let function g(dw,D) stands for 
the importance of the word dw related to the 
document D. Then, the word sampling 
distribution should be proportional to the word 
importance function g(dw,D). Therefore, we can 
write the probability P(S|D) 
?
?
?
Sdw
DdwgDSP ),()|(  (5) 
As indicated by Equation (5), the probability for 
?information source? S to represent the content 
of the document D, i.e. P(S|D), is proportional to 
the product of the importance function values for 
all the words selected by ?information source? S. 
1.4 A New Model for Title Generation 
The new model is based on the old framework 
with the proposed solutions to the problems of 
the old framework. As the summary of 
discussions in the previous two subsections, the 
essential idea of this new model is in two 
aspects: 
? Creating titles from the distilled 
?information source?. To prevent the 
common words in the document from voting 
for title words, in the new model, titles will 
be created from the estimated ?information 
source? which has common document words 
stripped off.  
? Subtract the influence of the 
?commonness? of title words from P(T). In 
the old framework for title generation, term 
P(T) is associated with the title word 
ordering phase. Since both the word order 
and the word ?commonness? can influence 
the occurring probability of the sequence T, 
i.e. P(T), we need to subtract the factor of 
word ?commonness? from term P(T), which 
results in term P(T)/P({tw?T}) for the title 
word ordering phase. 
 
T itle  W ords 
{T W 1, T W 2, ? , T W m } 
D ocum ent W ords 
{D W 1, D W 2, ? , D W n} 
Info rm ation So urce 
{D W ?1 , D W ?2 , ? , D W ?m  } 
Sam ple content w ords D W ? o ut o f 
all the w ords D W  using g(D W ,D ) 
C rea te  title  w ord T W  from  D W ?s  
using P (T W |D W ?) 
W ord  Sequence T  
O rder selected  title  w ord  in  a  sequence  
using P (T )/P ({tw ? T }) 
 
Fig. 2: Representation of the title generation scheme used 
by the new model. n is the number of words in the 
document and m is the number of words in the title. 
Therefore, by putting Equations (2), (4) and (5) 
together, our new model for title generation can 
be expressed as 
? ??
?
?
S dw
DdwgSTtwP
TtwP
TPDTP ),()|}({})({
)()|?(  (6) 
By further assuming that the number of words in 
any ?information source? S is equal to the 
number of words in the title T and, words in title 
T are created from the ?information source? S by 
first aligning every title word with a different 
word in the ?information source? S and then 
generating every title word tw from its aligned 
document word dw according to the probability 
distribution P(tw|dw), Equation (5) can be 
simplified as 
? ?
? ??
?
Ttw Ddw
DdwgdwtwP
TtwP
TPDTP ),()|(})({
)()|(  (7) 
Equation (7) is the center of the new 
probabilistic model for title generation. There 
are three components in Equation (7). They are 
word importance function g(dw,D), title-word-
document-word translation probability P(tw|dw) 
and the word ordering component P(T)/ 
P({tw?T}). A schematic diagram in Figure 2 
shows how a title is generated from a document 
in the new model through the three components. 
As shown in Figure 2, a sampling process based 
on the word importance function g(dw,D) will 
be applied to the original document to generate 
the ?information source? set containing most 
content words. Then, a set of title words will be 
scored according to probability P(tw|dw?) based 
on the words dw? selected by the ?information 
source?. Finally, the word ordering process is 
applied to the chosen title words tw using 
P(T)/P({tw?T}). 
1.5 Estimation of Components 
To implement the new model for title 
generation, we need to know how to estimate 
each of the three components.  
? The word importance function g(dw,D). In 
information retrieval, normalized tf.idf value has 
been used as the measurement of the importance 
of a term to a document (Salton & Buckley, 
1988). Therefore, we can adapt normalized tf.idf 
value as the word importance function g(dw,D). 
Therefore, function g(dw,D) can be written as 
?= dw dwidfDdwtfdwidfDdwtfDdwg )(),(/)(),(),(  (8) 
? The title-word-document-word ?translation? 
probability P(tw|dw). The title-word-document-
word ?translation? probability can be estimated 
using statistical translation model. Similar to the 
work of Kennedy and Hauptmann (2000), we 
can treat a document and its title as a 
?translation? pair with the document as in 
?verbose? language and the title as in ?concise? 
language. Therefore, title-word-document-word 
?translation? probability P(tw|dw) can be learned 
from the training corpus using statistical 
translation model (Brown et al, 1990). 
? Word ordering component P(T)/P({tw?T}). 
There are two terms in this component, namely 
P(T) and P({tw?T}). As already used by the old 
framework for title generation, P(T) can be 
estimated using a ngram statistical language 
model (Clarkson & Rosenfeld, 1997). The term 
P({tw?T}), by assuming the independence 
between words tw, can be written as the product 
of the occurring probability of each tw in T, i.e. 
? ??? Ttw twPTtwP )(})({ .  
 
With the expressions for g(dw,D) and 
P({tw?T}) substituted into Equation (6), we 
have the final expression for our model, i.e 
???
?
???
?
?
??
??
??
?
?
??
??
??
?
?
???
?
???
??
? ?
??
? ?
??
Ttw Ddw
Ttw
T
Ddw
dwidfDdwtfdwtwP
twPdwidfDdwtf
TPDTP
)(),()|(
)()(),(
)()|( ||
 (9) 
2 Evaluation 
In this experiment, we introduce two 
different of evaluations, i.e. a F1 metric for 
automatic evaluation and human judgments 
to evaluate the quality of machine-generated 
titles.  
F1 metric is a common evaluation metric 
that has been widely used in information 
retrieval and automatic text summarization. 
Witbrock and Mittal (1999) used the F1 
measurement (Rjiesbergen, 1979) as their 
performance metric. For an automatically 
generated title Tauto, F1 is measured against 
the correspondent human assigned title 
Thuman as follows: 
recallprecision
recallprecision2
  F1
+
??
=  (10) 
Here, precision and recall is measured as the 
number of identical words shared by title 
Tauto and Thuman over the number of words in 
title Tauto and the number of words in title 
Thuman respectively. 
Unfortunately, this metric ignores syntax and 
human readability. In this paper, we also asked 
people to judge the quality of machine-generated 
titles. There are five different quality categories, 
namely ?very good?, ?good?, ?ok?, ?bad?, 
?extremely bad?. A simple score scheme is 
developed with score 5 for the category ?very 
good?, score 4 for ?good?, score 3 for ?ok?, score 
2 for ?bad? and score 1 for ?extremely bad?. The 
average score of human judgment is used as 
another evaluation metric. 
3 Experiment 
3.1 Experiment Design 
The experimental dataset comes from a CD of 
1997 broadcast news transcriptions published by 
Primary Source Media [PrimarySourceMedia, 
1997]. There were a total of 50,000 documents 
and corresponding titles in the dataset. The 
training dataset was formed by randomly 
picking four documents-title pairs from every 
five pairs in the original dataset. Thus, the size 
of training corpus was 40,000 documents with 
corresponding titles. Only 1000 documents 
randomly selected from the remaining 10,000 
documents are used as test collection because of 
computation expensiveness of applying 
language model to sequentialize the title words.  
 
To see the effectiveness of our new model for 
title generation, we implemented the framework 
proposed by Witbrock and Mittal (1999) and 
conducted a contrastive experiment. The length 
of generated titles was fixed to be 6 for both 
methods and all the stop words in the title are 
removed.  
3.2 Examples of Machine-Generated 
Titles 
Table 1 and 2 give 5 examples of the titles 
generated by the old framework and the new 
probabilistic model, respectively. The true titles 
are also listed in Table 1 and 2 for the purpose of 
comparison.  
 
As shown in Table 1, one common problem with 
this set of machine-generated titles is that 
common title words are highly favoured. For 
example, the phrase ?president clinton? is a 
common title phrase and appears in 3 out of 5 
titles and frequently is not necessary. As already 
discussed in previous sections, the problem of 
over-favouring common title words in the old 
framework can be attributed to the use of term 
P(T) for the title word ordering phase. The other 
problem with the set of generated titles in Table 
1 is that, sometimes machine-generated titles 
contain words that have nothing to do with the 
content of the document. For example, the third 
machine-generated title in Table 1 is ?president 
clinton budget tax tobacco settlement? while the 
original corresponding title is ?senate funds fight 
against underage smoking?. By the comparison 
of the two titles, we can see that the word 
?budget? has little to do with the content of the 
story and shouldn?t be selected as title words. 
We think this problem is due to the fact that in 
the old framework for title generation, all the 
words in the document have an equal chance to 
vote for their favourite title words and the votes 
of common words in the document can cause 
unrelated title words to be selected.  
Table 1: Examples of titles generated by the old 
framework. Stopwords are removed 
Original Titles Machine-generated Titles 
bill lann lee president clinton affirmative action 
supreme court 
researchers say stress can 
cause heart disease 
stress heart disease medical news 
day 
senate funds fight against 
underage smoking 
president clinton budget tax 
tobacco settlement 
reaction to john f. 
kennedy jr. speaking out 
about his family 
joe kennedy family reaction 
entertainment news 
clinton?s fast track quest 
and other stories 
vice president clinton gore 
campaign fundraising 
As shown in Table 2, the titles generated by the 
new model appear to be more relevant to the 
content of the document by comparison to the 
original titles. Furthermore, the titles in Table 2 
appear to ?smoother? than the titles listed in 
Table 1 and don?t have unnecessary common 
words in titles. We believe it is due to the effects 
of both modified process for the title word 
ordering and dual noisy channel model. By 
replacing term P(T)/P({tw?T}) with term P(T), 
we make the title word selection phase 
concentrate on finding the correct word order 
and therefore avoid the problem of overly 
favouring common title words. With the 
introduction of the hidden state ?information 
source?, the title words will be selected based on 
the sampled important content words and 
therefore the noise introduced by common 
words in the document is reduced dramatically. 
Table 2: Examples of titles generated by new 
probabilistic model. Stopwords are removed 
Original Titles Machine-generated Titles 
bill lann lee civil rights nominee bill lann lee 
researchers say stress can 
cause heart disease 
study links everyday stress heart 
disease 
senate funds fight against 
underage smoking 
companies settlement tobacco 
deal tax laws 
reaction to john f. kennedy 
jr. speaking out about his 
family 
george magazine discusses joe 
kennedy family 
clinton?s fast track quest 
and other stories 
senate vote fast track trade 
authority 
3.3 Results and Discussions 
The F1 score of each method is computed based 
on the comparison of the 1000 generated titles to 
their original titles using Equation (10). To 
collect human judgments for machine-generated 
titles, we randomly chose 100 documents out of 
the 1000 test documents and sent the machine-
generated titles by both methods to the assessor 
for the quality judgment. The F1 scores and the 
average scores of human judgments for the old 
framework and the new probabilistic model are 
listed in Table 3. 
 
Table 3: Evaluation results of the old framework 
and the new probabilistic model 
 F1 Human Judg. 
Old model 0.21 2.09 
New model 0.26 3.07 
 
 As seen from Table 1, the F1 score for the new 
probabilistic model is better than the score for 
the old model with 0.26 for the new model and 
0.21 for the old model. Since the F1 metric 
basically measures the word overlapping 
between machine-generated titles and the 
original titles, the fact that the new model is 
better than the old model in terms of F1 metric 
indicates that the new model does a better job 
than the old model in terms of finding title 
words appropriate for documents. More 
important, in terms of human judgments, the 
new model also outperforms the old model 
significantly, which implies that titles generated 
by the new model is more readable than the titles 
generated by the old model. Based on these two 
observations, we can conclude that the new 
probabilistic model for title generation is 
effective in generating human readable titles. 
Conclusion 
In this paper, we propose a new probabilistic 
model for title generation. The advantages of the 
new model over the old framework are on the 
modification of the title word ordering phase and 
the introduction of the hidden state ?information 
source?. In the contrastive experiment, the new 
model outperforms the old model significantly 
in terms of both the automatic evaluation metric 
and the human judgments of the qualities of the 
generated titles. Therefore, we conclude that our 
new probabilistic model is effective in creating 
human readable titles. 
Acknowledgements 
The authors are grateful to the anonymous 
reviewers for their comments, which have 
helped improve the quality of the paper. This 
material is based in part on work supported by 
National Science Foundation under Cooperative 
Agreement No. IRI-9817496. Partial support for 
this work was provided by the National Science 
Foundation's National Science, Mathematics, 
Engineering, and Technology Education Digital 
Library Program under grant DUE-0085834. 
This work was also supported in part by the 
Advanced Research and Development Activity 
(ARDA) under contract number MDA908-00-C-
0037. Any opinions, findings, and conclusions 
or recommendations expressed in this material 
are those of the authors and do not necessarily 
reflect the views of the National Science 
Foundation or ARDA. 
References  
I. Mani and M. T. Maybury (1999) Advances in 
Automatic Text. MIT press, pp 51?53. 
M. Witbrock and V. Mittal (1999) Ultra-
Summarization: A Statistical Approach to 
Generating Highly Condensed Non-Extractive 
Summaries, Proceedings of SIGIR 99, Berkeley, 
CA 
R. Jin and A. G. Hauptmann (2001) Learn to Select 
Good Title Word: A New Approach based on 
Reverse Information Retrieval, ICML 2001. 
P. Kennedy and A. G. Hauptmann (2000) Automatic 
Title Generation for the Informedia Multimedia 
Digital Library, ACM Digital Libraries, DL-2000, 
San Antonio Texas 
P. R. Clarkson and R. Rosenfeld (1997) Statistical 
Language Modeling Using the CMU-Cambridge 
Toolkit. Proceedings ESCA Eurospeech. 
G. Salton and C. Buckeley (1988) Term-weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24, 513?523. 
P. Brown, S. Cocke, S. Della Pietra, Della Pietra, F. 
Jelinek, J. Lafferty, R. Mercer, and Roossin (1990) 
A Statistical Approach to Machine Translation. 
Computational Linguistics V. 16, No. 2. 
V. Rjiesbergen (1979) Information Retrieval. Chapter 
7. Butterworths, London. 
Automatic Title Generation for Spoken Broadcast News
Rong Jin
Language Technology Institute
Carnegie Mellon University
Pittsburgh, PA 15213
412-268-7003
rong+@cs.cmu.edu
Alexander G. Hauptmann
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
412-268-1448
alex+@cs.cmu.edu
ABSTRACT
In this paper, we implemented a set of title generation methods
using training set of 21190 news stories and evaluated them on an
independent test corpus of 1006 broadcast news documents,
comparing the results over manual transcription to the results over
automatically recognized speech. We use both F1 and the average
number of correct title words in the correct order as metric.
Overall, the results show that title generation for speech
recognized news documents is possible at a level approaching the
accuracy of titles generated for perfect text transcriptions.
Keywords
Machine learning, title generation
1. INTRODUCTION
To create a title for a document is a complex task. To generate a
title for a spoken document becomes even more challenging
because we have to deal with word errors generated by speech
recognition.
Historically, the title generation task is strongly connected to
traditional summarization because it can be thought of extremely
short summarization. Traditional summarization has emphasized
the extractive approach, using selected sentences or paragraphs
from the document to provide a summary. The weaknesses of this
approach are inability of taking advantage of the training corpus
and producing summarization with small ratio. Thus, it will not be
suitable for title generation tasks.
More recently, some researchers have moved toward ?learning
approaches? that take advantage of training data. Witbrock and
Mittal [1] have used Na?ve Bayesian approach for learning the
document word and title word correlation. However they limited
their statistics to the case that the document word and the title
word are same surface string. Hauptmann and Jin [2] extended
this approach by relaxing the restriction. Treating title generation
problem as a variant of Machine translation problem, Kennedy
and Hauptmann [3] tried the iterative Expectation-Maximization
algorithm. To avoid struggling with organizing selected title
words into human readable sentence, Hauptmann [2] used K
nearest neighbour method for generating titles. In this paper, we
put all those methods together and compare their performance
over 1000 speech recognition documents.
We decompose the title generation problem into two parts:
learning and analysis from the training corpus and generating a
sequence of title words to form the title.
For learning and analysis of training corpus, we present five
different learning methods for comparison: Na?ve Bayesian
approach with limited vocabulary, Na?ve Bayesian approach with
full vocabulary, K nearest neighbors, Iterative Expectation-
Maximization approach, Term frequency and inverse document
frequency method. More details of each approach will be
presented in Section 2.
For the generating part, we decompose the issues involved as
follows: choosing appropriate title words, deciding how many title
words are appropriate for this document title, and finding the
correct sequence of title words that forms a readable title
?sentence?.
The outline of this paper is as follows: Section 1 gave an
introduction to the title generation problem. The details of the
experiment and analysis of results are presented in Section 2.
Section 3 discusses our conclusions drawn from the experiment
and suggests possible improvements.
2. THE CONTRASTIVE TITLE
GENERATION EXPERIMENT
In this section we describe the experiment and present the results.
Section 2.1 describes the data. Section 2.2 discusses the
evaluation method. Section 2.3 gives a detailed description of all
the methods, which were compared. Results and analysis are
presented in section 2.4.
2.1 Data Description
In our experiment, the training set, consisting of 21190 perfectly
transcribed documents, are obtain from CNN web site during
1999. Included with each training document text was a human
assigned title. The test set, consisting of 1006 CNN TV news
story documents for the same year (1999), are randomly selected
from the Informedia Digital Video Library. Each document has a
closed captioned transcript, an alternative transcript generated
with CMU Sphinx speech recognition system with a 64000-word
broadcast news language model and a human assigned title.
2.2 Evaluation
First, we evaluate title generation by different approaches using
the F1 metric. For an automatically generated title Tauto, F1 is
measured against corresponding human assigned title Thuman as
follows:
F1 = 2?precision?recall / (precision + recall)
Here, precision and recall is measured respectively as the number
of identical words in Tauto and Thuman over the number of
words in Tauto and the number of words in Thuman. Obviously
the sequential word order of the generated title words is ignored
by this metric.
To measure how well a generated title compared to the original
human generated title in terms of word order, we also measured
the number of correct title words in the hypothesis titles that were
in the same order as in the reference titles.
We restrict all approaches to generate only 6 title words, which is
the average number of title words in the training corpus. Stop
words were removed throughout the training and testing
documents and also removed from the titles.
2.3 Description of the Compared Title
Generation Approaches
The five different title generation methods are:
1. Na?ve Bayesian approach with limited vocabulary (NBL).
It tries to capture the correlation between the words in the
document and the words in the title. For each document word
DW, it counts the occurrence of title word same as DW and
apply the statistics to the test documents for generating titles.
2. Na?ve Bayesian approach with full vocabulary (NBF). It
relaxes the constraint in the previous approach and counts all
the document-word-title-word pairs. Then this full statistics
will be applied on generating titles for the test documents.
3. Term frequency and inverse document frequency
approach (TF.IDF). TF is the frequency of words occurring
in the document and IDF is logarithm of the total number of
documents divided by the number of documents containing
this word. The document words with highest TF.IDF were
chosen for the title word candidates.
4. K nearest neighbor approach (KNN). This algorithm is
similar to the KNN algorithm applied to topic classification.
It searches the training document set for the closest related
document and assign the training document title to the new
document as title.
5. Iterative Expectation-Maximization approach (EM). It
views documents as written in a ?verbal? language and their
titles as written a ?concise? language. It builds the translation
model between the ?verbal? language and the ?concise?
language from the documents and titles in the training corpus
and ?translate? each testing document into title.
2.4 The sequentializing process for title word
candidates
To generate an ordered set of candidates, equivalent to what we
would expect to read from left to right, we built a statistical
trigram language model using the SLM tool-kit (Clarkson, 1997)
and the 40,000 titles in the training set. This language model was
used to determine the most likely order of the title word
candidates generated by the NBL, NBF, EM and TF.IDF methods.
3. RESULTS AND OBSERVATIONS
The experiment was conducted both on the closed caption
transcripts and automatic speech recognized transcripts. The F1
results and the average number of correct title word in correct
order are shown in Figure 1 and 2 respectively.
KNN works surprisingly well.  KNN generates titles for a new
document by choosing from the titles in the training corpus. This
works fairly well because both the training set and test set come
from CNN news of the same year. Compared to other methods,
KNN degrades much less with speech-recognized transcripts.
Meanwhile, even though KNN performance not as well as TF.IDF
and NBL in terms of F1 metric, it performances best in terms of
the average number of correct title words in the correct order. If
consideration of human readability matters, we would expect
KNN to outperform considerately all the other approaches since it
is guaranteed to generate human readable title.
Comparison of F1 
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
30.00%
KN
N
TF
IDF NB
L
NB
F EM
Methods
F1
original
documents
spoken
documents
Figure 1: Comparison of Title Generation Approaches on a
test corpus of 1006 documents with either perfect transcript or
speech recognized transcripts using the F1 score.
NBF performs much worse than NBL. NBF performances much
worse than NBL in both metrics. The difference between NBF and
NBL is that NBL assumes a document word can only generate a
title word with the same surface string. Though it appears that
NBL loses information with this very strong assumption, the
results tell us that some information can safely be ignored. In
NBF, nothing distinguishes between important words and trivial
words. This lets frequent, but unimportant words dominate the
document-word-title-word correlation.
Light learning approach TF.IDF performances considerably
well compared with heavy learning approaches. Surprisingly,
heavy learning approaches, NBL, NBF and EM algorithm didn?t
out performance the light learning approach TF.IDF. We think
learning the association between document words and title words
by inspecting directly the document and its title is very
problematic since many words in the document don?t reflect its
content. The better strategy should be distilling the document first
before learning the correlation between document words and title
words.
Comparison of # of Correct Words in 
Correct Order
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
KN
N
TF
IDF NB
L
NB
F EM
Methods
# 
of
 
Co
rr
ec
t W
o
rd
s 
in
 
th
e 
Co
rr
ec
t O
rd
er
original
documents
spoken
documents
Figure 1: Comparison of Title Generation Approaches on a
test corpus of 1006 documents with either perfect transcript or
speech recognized transcripts using the average number of
correct words in the correct order.
4. CONCLUSION
From the analysis discussed in previous section, we draw the
following conclusions:
1. The KNN approach works well for title generation especially
when overlap in content between training dataset and test
collection is large.
2. The fact that NBL out performances NBF and TF.IDF out
performance NBL and suggests that we need to distinguish
important document words from those trivial words.
5. ACKNOWLEDGMENTS
This material is based in part on work supported by National
Science Foundation under Cooperative Agreement No. IRI-
9817496. Partial support for this work was provided by the
National Science Foundation?s National Science, Mathematics,
Engineering, and Technology Education Digital Library Program
under grant DUE-0085834. This work was also supported in part
by the Advanced Research and Development Activity (ARDA)
under contract number MDA908-00-C-0037. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect the
views of the National Science Foundation or ARDA.
6. REFERENCES
[1] Michael Witbrock and Vibhu Mittal. Ultra-Summarization:
A Statistical Approach to Generating Highly Condensed
Non-Extractive Summaries. Proceedings of SIGIR 99,
Berkeley, CA, August 1999.
[2] R. Jin and A.G. Hauptmann. Title Generation for Spoken
Broadcast News using a Training Corpus. Proceedings of 6th
Internal Conference on Language Processing (ICSLP 2000),
Beijing China. 2000.
[3] P. Kennedy and A.G. Hauptmann. Automatic Title
Generation for the Informedia Multimedia Digital Library.
ACM Digital Libraries, DL-2000, San Antonio Texas, May
2000.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368?375,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Automated Vocabulary Acquisition and Interpretation
in Multimodal Conversational Systems
Yi Liu Joyce Y. Chai Rong Jin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{liuyi3, jchai, rongjin}@cse.msu.edu
Abstract
Motivated by psycholinguistic findings that
eye gaze is tightly linked to human lan-
guage production, we developed an unsuper-
vised approach based on translation models
to automatically learn the mappings between
words and objects on a graphic display dur-
ing human machine conversation. The ex-
perimental results indicate that user eye gaze
can provide useful information to establish
such mappings, which have important impli-
cations in automatically acquiring and inter-
preting user vocabularies for conversational
systems.
1 Introduction
To facilitate effective human machine conversation,
it is important for a conversational system to have
knowledge about user vocabularies and understand
how these vocabularies are mapped to the internal
entities for which the system has representations.
For example, in a multimodal conversational system
that allows users to converse with a graphic inter-
face, the system needs to know what vocabularies
users tend to use to describe objects on the graphic
display and what (type of) object(s) a user is attend-
ing to when a particular word is expressed. Here,
we use acquisition to refer to the process of acquir-
ing relevant vocabularies describing internal entities,
and interpretation to refer to the process of automat-
ically identifying internal entities given a particular
word. Both acquisition and interpretation have been
traditionally approached by either knowledge engi-
neering (e.g., manually created lexicons) or super-
vised learning from annotated data. In this paper,
we describe an unsupervised approach that relies
on naturally co-occurred eye gaze and spoken utter-
ances during human machine conversation to auto-
matically acquire and interpret vocabularies.
Motivated by psycholinguistic studies (Just and
Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus
et al, 1995) and recent investigations on computa-
tional models for language acquisition and ground-
ing (Siskind, 1995; Roy and Pentland, 2002; Yu
and Ballard, 2004), we are particularly interested in
two unique questions related to multimodal conver-
sational systems: (1) In a multimodal conversation
that involves more complex tasks (e.g., both user
initiated tasks and system initiated tasks), is there
a reliable temporal alignment between eye gaze and
spoken references so that the coupled inputs can be
used for automated vocabulary acquisition and inter-
pretation? (2) If such an alignment exists, how can
we model this alignment and automatically acquire
and interpret the vocabularies?
To address the first question, we conducted an
empirical study to examine the temporal relation-
ships between eye fixations and their correspond-
ing spoken references. As shown later in section 4,
although a larger variance (compared to the find-
ings from psycholinguistic studies) exists in terms of
how eye gaze is linked to speech production during
human machine conversation, eye fixations and the
corresponding spoken references still occur in a very
close vicinity to each other. This natural coupling
between eye gaze and speech provides an opportu-
nity to automatically learn the mappings between
368
words and objects without any human supervision.
Because of the larger variance, it is difficult to
apply rule-based approaches to quantify this align-
ment. Therefore, to address the second question,
we developed an approach based on statistical trans-
lation models to explore the co-occurrence patterns
between eye fixated objects and spoken references.
Our preliminary experiment results indicate that the
translation model can reliably capture the mappings
between the eye fixated objects and the correspond-
ing spoken references. Given an object, this model
can provide possible words describing this object,
which represents the acquisition process; given a
word, this model can also provide possible objects
that are likely to be described, which represents the
interpretation process.
In the following sections, we first review some re-
lated work and introduce the procedures used to col-
lect eye gaze and speech data during human machine
conversation. We then describe our empirical study
and the unsupervised approach based on translation
models. Finally, we present experiment results and
discuss their implications in natural language pro-
cessing applications.
2 Related Work
Our work is motivated by previous work in the fol-
lowing three areas: psycholinguistics studies, multi-
modal interactive systems, and computational mod-
eling of language acquisition and grounding.
Previous psycholinguistics studies have shown
that the direction of gaze carries information about
the focus of the user?s attention (Just and Carpenter,
1976). Specifically, in human language processing
tasks, eye gaze is tightly linked to language produc-
tion. The perceived visual context influences spo-
ken word recognition and mediates syntactic pro-
cessing (Tenenhaus et al, 1995). Additionally, be-
fore speaking a word, the eyes usually move to the
objects to be mentioned (Griffin and Bock, 2000).
These psycholinguistics findings have provided a
foundation for our investigation.
In research on multimodal interactive systems, re-
cent work indicates that the speech and gaze inte-
gration patterns can be modeled reliably for indi-
vidual users and therefore be used to improve mul-
timodal system performances (Kaur et al, 2003).
Studies have also shown that eye gaze has a poten-
tial to improve resolution of underspecified referring
expressions in spoken dialog systems (Campana et
al., 2001) and to disambiguate speech input (Tanaka,
1999). In contrast to these earlier studies, our work
focuses on a different goal of using eye gaze for au-
tomated vocabulary acquisition and interpretation.
The third area of research that influenced our
work is computational modeling of language acqui-
sition and grounding. Recent studies have shown
that multisensory information (e.g., through vision
and language processing) can be combined to effec-
tively acquire words to their perceptually grounded
objects in the environment (Siskind, 1995; Roy and
Pentland, 2002; Yu and Ballard, 2004). Especially in
(Yu and Ballard, 2004), an unsupervised approach
based on a generative correspondence model was
developed to capture the mapping between spoken
words and the occurring perceptual features of ob-
jects. This approach is most similar to the transla-
tion model used in our work. However, compared
to this work where multisensory information comes
from vision and language processing, our work fo-
cuses on a different aspect. Here, instead of applying
vision processing on objects, we are interested in eye
gaze behavior when users interact with a graphic dis-
play. Eye gaze is an implicit and subconscious input
modality during human machine interaction. Eye
gaze data inevitably contain a significant amount of
noise. Therefore, it is the goal of this paper to exam-
ine whether this modality can be utilized for vocab-
ulary acquisition for conversational systems.
3 Data Collection
We used a simplified multimodal conversational sys-
tem to collect synchronized speech and eye gaze
data. A room interior scene was displayed on a com-
puter screen, as shown in Figure 1. While watching
the graphical display, users were asked to communi-
cate with the system on topics about the room dec-
orations. A total of 28 objects (e.g., multiple lamps
and picture frames, a bed, two chairs, a candle, a
dresser, etc., as marked in Figure 1) are explicitly
modeled in this scene. The system is simplified in
the sense that it only supports 14 tasks during human
machine interaction. These tasks are designed to
cover both open-ended utterances (e.g., the system
369
Figure 1: The room interior scene for user studies.
For easy reference, we give each object an ID. These
IDs are hidden from the system users.
asks users to describe the room) and more restricted
utterances (e.g., the system asks the user whether
he/she likes the bed) that are commonly supported in
conversational systems. Seven human subjects par-
ticipated in our study.
User speech inputs were recorded using the Au-
dacity software1, with each utterance time-stamped.
Eye movements were recorded using an EyeLink II
eye tracker sampled at 250Hz. The eye tracker au-
tomatically saved two-dimensional coordinates of a
user?s eye fixations as well as the time-stamps when
the fixations occurred.
The collected raw gaze data is extremely noisy.
To refine the gaze data, we further eliminated in-
valid and saccadic gaze points (known as ?saccadic
suppression? in vision studies). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we also smoothed the data by averaging
nearby gaze locations to identify fixations.
4 Empirical Study on Speech-Gaze
Alignment
Based on the data collected, we investigated the tem-
poral alignment between co-occurred eye gaze and
spoken utterances. In particular, we examined the
temporal alignment between eye gaze fixations and
the corresponding spoken references (i.e., the spo-
ken words that are used to refer to the objects on the
graphic display).
According to the time-stamp information, we can
1http://audacity.sourceforge.net/
measure the length of time gap between a user?s eye
fixation falling on an object and the corresponding
spoken reference being uttered (which we refer to
as ?length of time gap? for brevity). Also, we can
count the number of times that user fixations hap-
pen to change their target objects during this time
gap (which we refer to as ?number of fixated object
changes? for brevity). The nine most frequently oc-
curred spoken references in utterances from all users
(as shown in Table 1) are chosen for this empirical
study. For each of those spoken references, we use
human judgment to decide which object is referred
to. Then, from both before and after the onset of
the spoken reference, we find the closest occurrence
of the fixation falling on that particular object. Al-
together we have 96 such speech-gaze pairs. In 54
pairs, the eye gaze fixation occurred before the cor-
responding speech reference was uttered; and in the
other 42 pairs, the eye fixation occurred after the
corresponding speech reference was uttered. This
observation suggests that in human machine conver-
sation, eye fixation on an object does not necessarily
always proceed the utterance of the corresponding
speech reference.
Further, we computed the average absolute length
of the time gap and the average number of fixated
object changes, as well as their variances for each of
5 selected users2 as shown in Table 1. From Table 1,
it is easy to observe that: (I) A spoken reference al-
ways appears within a short period of time (usually
1-2 seconds) before or after the corresponding eye
gaze fixation. But, the exact length of the period is
far from constant. (II) It is not necessary for a user
to utter the corresponding spoken reference imme-
diately before or after the eye gaze fixation falls on
that particular object. Eye gaze fixations may move
back and forth. Between the time an object is fixated
and the corresponding spoken reference is uttered, a
user?s eye gaze may fixate on a few other objects
(reflected by the average number of eye fixated ob-
ject changes shown in the table). (III) There is a
large variance in both the length of time gap and the
number of fixated object changes in terms of 1) the
same user and the same spoken reference at differ-
ent time-stamps, 2) the same user but different spo-
2The other two users are not selected because the nine se-
lected words do not appear frequently in their utterances.
370
Spoken Average Absolute Length of Time Gap (in seconds) Average Number of Eye Fixated Object Changes
Reference User 1 User 2 User 3 User 4 User 5 User 1 User 2 User 3 User 4 User 5
bed 1.27? 1.40 1.02? 0.65 0.32? 0.21 0.59? 0.77 2.57? 3.25 2.1? 3.2 2.1? 2.2 0.4? 0.5 1.4? 2.2 5.3? 7.9
tree - 0.24? 0.24 - - - - 0.0? 0.0 - - -
window - 0.67? 0.74 - - 1.95? 3.20 - 0.0? 0.0 - - 3.3? 5.9
mirror - 1.04? 1.36 - - - - 1.0? 1.4 - - -
candle - - 3.64? 0.59 - - - - 8.5? 2.1 - -
waterfall 1.80? 1.12 - - - - 5.5? 4.9 - - - -
painting 0.10? 0.10 - - - - 0.2? 0.4 - - - -
lamp 0.74? 0.54 1.70? 0.99 0.26? 0.35 1.98? 1.72 2.84? 2.42 1.3? 1.3 1.8? 1.5 0.3? 0.6 4.8? 4.3 2.7? 2.2
door 2.47? 0.84 - - 2.49? 1.90 6.36? 2.29 5.0? 2.6 - - 6.7? 5.5 13.3? 6.7
Table 1: The average absolute length of time and the number of eye fixated object changes within the time
gap of eye gaze and corresponding spoken references. Variances are also listed. Some of the entries are not
available because the spoken references were never or rarely used by the corresponding users.
ken references, and 3) the same spoken reference but
different users. We believe this is due to the different
dialog scenarios and user language habits.
To summarize our empirical study, we find that
in human machine conversation, there still exists a
natural temporal coupling between user speech and
eye gaze, i.e. the spoken reference and the corre-
sponding eye fixation happen within a close vicinity
of each other. However, a large variance is also ob-
served in terms of these temporal vicinities, which
indicates an intrinsically more complex gaze-speech
pattern. Therefore, it is hard to directly quantify
the temporal or ordering relationship between spo-
ken references and corresponding eye fixated objects
(for example, through rules).
To better handle the complexity in the gaze-
speech pattern, we propose to use statistical transla-
tion models. Given a time window of enough length,
a speech input that contains a list of spoken refer-
ences (e.g., definite noun phrases) is always accom-
panied by a list of naturally occurred eye fixations
and therefore a list of objects receiving those fixa-
tions. All those pairs of speech references and cor-
responding fixated objects could be viewed as paral-
lel, i.e. they co-occur within the time window. This
situation is very similar to the training process of
translation models in statistical machine translation
(Brown et al, 1993), where parallel corpus is used to
find the mappings between words from different lan-
guages by exploiting their co-occurrence patterns.
The same idea can be borrowed here: by exploring
the co-occurrence statistics, we hope to uncover the
exact mapping between those eye fixated objects and
spoken references. The intuition is that, the more of-
ten a fixation is found to exclusively co-occur with a
spoken reference, the more likely a mapping should
be established between them.
5 Translation Models for Vocabulary
Acquisition and Interpretation
Formally, we denote the set of observations by
D = {wi,oi}Ni=1 where wi and oi refers to
the i-th speech utterance (i.e., a list of words
of spoken references) and the i-th corresponding
eye gaze pattern (i.e., a list of eye fixated ob-
jects) respectively. When we study the prob-
lem of mapping given objects to words (for vo-
cabulary acquisition), the parameter space ? =
{Pr(wj |ok), 1 ? j ? mw, 1 ? k ? mo} consists of
the mapping probabilities of an arbitrary word wj
to an arbitrary object ok, where mw and mo repre-
sent the total number of unique words and objects
respectively. Those mapping probabilities are sub-
ject to constraints ?mwj=1 Pr(wj |ok) = 1. Note that
Pr(wj |ok) = 0 if the corresponding word wj and ok
never co-occur in any observed list pair (wi,oi).
Let lwi and loi denote the length of lists wi and
oi respectively. To distinguish with the notations
wj and ok whose subscripts are indices for unique
words and objects respectively, we use w?i,j to de-
note the word in the j-th position of the list wi and
o?i,k to denote the object in the k-th position of the
list oi. In translation models, we assume that any
word in the list wi is mapped to an object in the cor-
responding list oi or a null object (we reserve the
position 0 for it in every object list). To denote all
the word-object mappings in the i-th list pair, we in-
troduce an alignment vector ai, whose element ai,j
takes the value k if the word w?i,j is mapped to o?i,k.
Then, the likelihood of the observations given the
371
parameters can be computed as follows
Pr(D;?) =
N
?
i=1
Pr(wi|oi) =
N
?
i=1
?
ai
Pr(wi,ai|oi)
=
N
?
i=1
?
ai
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
Pr(w?i,j |o?ai,j )
=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
?
ai
lwi
?
j=1
Pr(w?i,j |o?ai,j )
Note that the following equation holds:
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k) =
loi
?
ai,1=1
? ? ?
loi
?
ai,lwi =1
lwi
?
j=1
Pr(w?i,j |o?ai,j )
where the right-hand side is actually the expansion
of
?
ai
?lwi
j Pr(w?i,j |o?ai,j ). Therefore, the likelihood
can be simplified as
Pr(D;?) =
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k)
Switching to the notations wj and ok, we have
Pr(D;?)=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
mw
?
j=1
[ mo
?
k=0
Pr(wj |ok)?oi,k
]?wi,j
where ?wi,j = 1 if w?i,j ? wi and ?wi,j = 0 otherwise,
and ?oi,k = 1 if o?i,k ? oi and ?oi,k = 0 otherwise.
Finally, the translation model can be formalized
as the following optimization problem
arg max? log Pr(D;?)
s.t.
mw
?
j=1
Pr(wj |ok) = 1,?k
This optimization problem can be solved by the EM
algorithm (Brown et al, 1993).
The above model is developed in the con-
text of mapping given objects to words, i.e., its
solution yields a set of conditional probabilities
{Pr(wj |ok),?j} for each object ok, indicating how
likely every word is mapped to it. Similarly, we
can develop the model in the context of mapping
given words to objects (for vocabulary interpreta-
tion), whose solution leads to another set of prob-
abilities {Pr(ok|wj),?k} for each word wj indicat-
ing how likely every object is mapped to it. In our
experiments, both models are implemented and we
will present the results later.
6 Experiments
We experimented our proposed statistical translation
model on the collected data mentioned in Section 3.
6.1 Preprocessing
The main purpose of preprocessing is to create a
?parallel corpus? for training a translation model.
Here, the ?parallel corpus? refers to a series of
speech-gaze pairs, each of them consisting of a list
of words from the spoken references in the user ut-
terances and a list of objects that are fixated upon
within the same time window.
Specifically, we first transcribed the user speech
into scripts by automatic speech recognition soft-
ware and then refined them manually. A time-stamp
was associated with each word in the speech script.
Further, we detected long pauses in the speech script
as splitting points to create time windows, since a
long pause usually marks the start of a sentence
that indicates a user?s attention shift. In our exper-
iment, we set the threshold of judging a long pause
to be 1 second. From all the data gathered from 7
users, we get 357 such time windows (which typi-
cally contain 10-20 spoken words and 5-10 fixated
object changes).
Given a time window, we then found the objects
being fixated upon by eye gaze (represented by their
IDs as shown in Figure 1). Considering that eye gaze
fixation could occur during the pauses in speech, we
expanded each time window by a fixed length at both
its start and end to find the fixations. In our experi-
ments, the expansion length is set to 0.5 seconds.
Finally, we applied a part-of-speech tagger to
each sentence in the user script and only singled out
nouns as potential spoken references in the word list.
The Porter stemming algorithm was also used to get
the normalized forms of those nouns.
The translation model was trained based on this
preprocessed parallel data.
6.2 Evaluation Metrics
As described in Section 5, by using a statistical
translation model we can get a set of translation
probabilities, either from any given spoken word to
all the objects, or from any given object to all the
spoken words. To evaluate the two sets of trans-
lation probabilities, we use precision and recall as
372
#Rank Precision Recall #Rank Precision Recall
1 0.6667 0.2593 6 0.2302 0.5370
2 0.4524 0.3519 7 0.2041 0.5556
3 0.3810 0.4444 8 0.1905 0.5926
4 0.3095 0.4815 9 0.1799 0.6296
5 0.2667 0.5185 10 0.1619 0.6296
Table 2: Average precision/recall of mapping given
objects to words (i.e., acquisition)
#Rank Precision Recall #Rank Precision Recall
1 0.7826 0.3214 6 0.3043 0.7500
2 0.5870 0.4821 7 0.2671 0.7679
3 0.4638 0.5714 8 0.2446 0.8036
4 0.3804 0.6250 9 0.2293 0.8393
5 0.3478 0.7143 10 0.2124 0.8571
Table 3: Average precision/recall of mapping given
words to objects.(i.e., interpretation)
evaluation metrics.
Specifically, for a given object ok the trans-
lation model will yield a set of probabilities
{Pr(wj |ok),?j}. We can sort the probabilities and
get a ranked list. Let us assume that we have the
ground truth about all the spoken words to which
the given object should be mapped. Then, at a given
number n of top ranked words, the precision of map-
ping the given object ok to words is defined as
# words that ok is correctly mapped to
# words that ok is mapped to
and the recall is defined as
# words that ok is correctly mapped to
# words that ok should be mapped to
All the counting above is done within the top n rank.
Therefore, we can get different precision/recall at
different ranks. At each rank, the overall perfor-
mance can be evaluated by averaging the preci-
sion/recall for all the given objects. Human judg-
ment is used to decide whether an object-word map-
ping is correct or not, as ground truth for evaluation.
Similarly, based on the set of probabilities of map-
ping a given object with spoken words, we can
find a ranked list of objects for a given word, i.e.
{Pr(ok|wj),?k}. Thus, at a given rank the preci-
sion and recall of mapping a given word wj to ob-
jects can be measured.
6.3 Experiment Results
Vocabulary acquisition is the process of finding
the appropriate word(s) for any given object. For
the sake of statistical significance, our evaluation is
done on 21 objects that were mentioned at least 3
times by the users.
Table 2 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use
the most probable word acquired for each object,
about 66.67% of them are appropriate. With the
rank increasing, more and more appropriate words
can be acquired. About 62.96% of all the appropri-
ate words are included within the top 10 probable
words found. The results indicate that by using a
translation model, we can obtain the words that are
used by the users to describe the objects with rea-
sonable accuracy.
Table 4 presents the top 3 most probable words
found for each object. It shows that although there
may be more than one word appropriate to describe
a given object, those words with highest probabil-
ities always suggest the most popular way of de-
scribing the corresponding object among the users.
For example, for the object with ID 26, the word
candle gets a higher probability than the word
candlestick, which is in accordance with our
observation that in our user study, on most occasions
users tend to use the word candle rather than the
word candlestick.
Vocabulary interpretation is the process of find-
ing the appropriate object(s) for any given spoken
word. Out of 176 nouns in the user vocabulary,
we only evaluate those used at least three times for
statistical significance concerns. Further, abstract
words (such as reason, position) and general
words (such as room, furniture) are not eval-
uated since they do not refer to any particular objects
in the scene. Finally, 23 nouns remain for evalua-
tion.
We manually enumerated all the object(s) that
those 23 nouns refer to as the ground truth in our
evaluation. Note that a given noun can possibly
be used to refer to multiple objects, such as lamp,
since we have several lamps (with object ID 3, 8, 17,
and 23) in the experiment setting, and bed, since
bed frame, bed spread, and pillows (with object ID
19, 21, and 20 respectively) are all part of a bed.
Also, an object can be referred to by multiple nouns.
For example, the words painting, picture,
or waterfall can all be used to refer to the ob-
ject with ID 15.
373
Object Rank 1 Rank 2 Rank 3
1 paint (0.254) * wall (0.191) left (0.150)
2 pictur (0.305) * girl (0.122) niagara (0.095) *
3 wall (0.109) lamp (0.093) * floor (0.084)
4 upsid (0.174) * left (0.151) * paint (0.149) *
5 pictur (0.172) window (0.157) * wall (0.116)
6 window (0.287) * curtain (0.115) pictur (0.076)
7 chair (0.287) * tabl (0.088) bird (0.083)
9 mirror (0.161) * dresser (0.137) bird (0.098) *
12 room (0.131) lamp (0.127) left (0.069)
14 hang (0.104) favourit (0.085) natur (0.064)
15 thing (0.066) size (0.059) queen (0.057)
16 paint (0.211) * pictur (0.116) * forest (0.076) *
17 lamp (0.354) * end (0.154) tabl (0.097)
18 bedroom (0.158) side (0.128) bed (0.104)
19 bed (0.576) * room (0.059) candl (0.049)
20 bed (0.396) * queen (0.211) * size (0.176)
21 bed (0.180) * chair (0.097) orang (0.078)
22 bed (0.282) door (0.235) * chair (0.128)
25 chair (0.215) * bed (0.162) candlestick (0.124)
26 candl (0.145) * chair (0.114) candlestick (0.092) *
27 tree (0.246) * chair (0.107) floor (0.096)
Table 4: Words found for given objects. Each row
lists the top 3 most probable spoken words (being
stemmed) for the corresponding given object, with
the mapping probabilities in parentheses. Asterisks
indicate correctly identified spoken words. Note
that some objects are heavily overlapped, so the cor-
responding words are considered correct for all the
overlapping objects, such as bed being considered
correct for objects with ID 19, 20, and 21.
Word Rank 1 Rank 2 Rank 3 Rank 4
curtain 6 (0.305) * 5 (0.305) * 7 (0.133) 1 (0.121)
candlestick 25 (0.147) * 28 (0.135) 24 (0.131) 22 (0.117)
lamp 22 (0.126) 12 (0.094) 17 (0.093) * 25 (0.093)
dresser 12 (0.298) * 9 (0.294) * 13 (0.173) * 7 (0.104)
queen 20 (0.187) * 21 (0.182) * 22 (0.136) 19 (0.136) *
door 22 (0.200) * 27 (0.124) 25 (0.108) 24 (0.106)
tabl 9 (0.152) * 12 (0.125) * 13 (0.112) * 22 (0.107)
mirror 9 (0.251) * 12 (0.238) 8 (0.109) 13 (0.081)
girl 2 (0.173) 22 (0.128) 16 (0.099) 10 (0.074)
chair 22 (0.132) 25 (0.099) * 28 (0.085) 24 (0.082)
waterfal 6 (0.226) 5 (0.215) 1 (0.118) 9 (0.083)
candl 19 (0.156) 22 (0.139) 28 (0.134) 24 (0.131)
niagara 4 (0.359) * 2 (0.262) * 1 (0.226) 7 (0.045)
plant 27 (0.230) * 22 (0.181) 23 (0.131) 28 (0.117)
tree 27 (0.352) * 22 (0.218) 26 (0.100) 13 (0.062)
upsid 4 (0.204) * 12 (0.188) 9 (0.153) 1 (0.104) *
bird 9 (0.142) * 10 (0.138) 12 (0.131) 7 (0.121)
desk 12 (0.170) * 9 (0.141) * 19 (0.118) 8 (0.118)
bed 19 (0.207) * 22 (0.141) 20 (0.111) * 28 (0.090)
upsidedown 4 (0.243) * 3 (0.219) 6 (0.203) 5 (0.188)
paint 4 (0.188) * 16 (0.148) * 1 (0.137) * 15 (0.118) *
window 6 (0.305) * 5 (0.290) * 3 (0.085) 22 (0.065)
lampshad 3 (0.223) * 7 (0.137) 11 (0.137) 10 (0.137)
Table 5: Objects found for given words. Each row
lists the 4 most probable object IDs for the corre-
sponding given words (being stemmed), with the
mapping probabilities in parentheses. Asterisks in-
dicate correctly identified objects. Note that some
objects are heavily overlapped, such as the candle
(with object ID 26) and the chair (with object ID
25), and both were considered correct for the re-
spective spoken words.
Table 3 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use the
most probable object found for each speech word,
about 78.26% of them are appropriate. With the rank
increasing, more and more appropriate objects can
be found. About 85.71% of all the appropriate ob-
jects are included within the top 10 probable objects
found. The results indicate that by using a trans-
lation model, we can predict the objects from user
spoken words with reasonable accuracy.
Table 5 lists the top 4 probable objects found for
each spoken word being evaluated. A close look re-
veals that in general, the top ranked objects tend to
gather around the correct object for a given spoken
word. This is consistent with the fact that eye gaze
tends to move back and forth. It also indicates that
the mappings established by the translation model
can effectively find the approximate area of the cor-
responding fixated object, even if it cannot find the
object due to the noisy and jerky nature of eye gaze.
The precision/recall in vocabulary acquisition is
not as high as that in vocabulary interpretation, par-
tially due to the relatively small scale of our exper-
iment data. For example, with only 7 users? speech
data on 14 conversational tasks, some words were
only spoken a few times to refer to an object, which
prevented them from getting a significant portion of
probability mass among all the words in the vocab-
ulary. This degrades both precision and recall. We
believe that in large scale experiments or real-world
applications, the performance will be improved.
7 Discussion and Conclusion
Previous psycholinguistic findings have shown that
eye gaze is tightly linked with human language pro-
duction. During human machine conversation, our
study shows that although a larger variance is ob-
served on how eye fixations are exactly linked with
corresponding spoken references (compared to the
psycholinguistic findings), eye gaze in general is
closely coupled with corresponding referring ex-
pressions in the utterances. This close coupling na-
ture between eye gaze and speech utterances pro-
vides an opportunity for the system to automatically
374
acquire different words related to different objects
without any human supervision. To further explore
this idea, we developed a novel unsupervised ap-
proach using statistical translation models.
Our experimental results have shown that this ap-
proach can reasonably uncover the mappings be-
tween words and objects on the graphical display.
The main advantages of this approach include: 1) It
is an unsupervised approach with minimum human
inference; 2) It does not need any prior knowledge to
train a statistical translation model; 3) It yields prob-
abilities that indicate the reliability of the mappings.
Certainly, our current approach is built upon sim-
plified assumptions. It is quite challenging to in-
corporate eye gaze information since it is extremely
noisy with large variances. Recent work has shown
that the effect of eye gaze in facilitating spoken lan-
guage processing varies among different users (Qu
and Chai, 2007). In addition, visual properties of
the interface also affect user gaze behavior and thus
influence the predication of attention (Prasov et al,
2007) based on eye gaze. Our future work will de-
velop models to address these variations.
Nevertheless, the results from our current work
have several important implications in building ro-
bust conversational interfaces. First of all, most
conversational systems are built with static knowl-
edge space (e.g., vocabularies) and can only be up-
dated by the system developers. Our approach can
potentially allow the system to automatically ac-
quire knowledge and vocabularies based on the nat-
ural interactions with the users without human in-
tervention. Furthermore, the automatically acquired
mappings between words and objects can also help
language interpretation tasks such as reference res-
olution. Given the recent advances in eye track-
ing technology (Duchowski, 2002), integrating non-
intrusive and high performance eye trackers with
conversational interfaces becomes feasible. The
work reported here can potentially be integrated in
practical systems to improve the overall robustness
of human machine conversation.
Acknowledgment
This work was supported by funding from National
Science Foundation (IIS-0347548, IIS-0535112,
and IIS-0643494) and Disruptive Technology Of-
fice. The authors would like to thank Zahar Prasov
for his contribution to data collection.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
E. Campana, J. Baldridge, J. Dowding, B. A. Hockey,
R. Remington, and L. S. Stone. 2001. Using eye
movements to determine referents in a spoken dialog
system. In Proceedings of PUI?01.
A. T. Duchowski. 2002. A breath-first survey of eye
tracking applications. Behavior Research methods, In-
struments, and Computers, 33(4).
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. Psychological Science, 11:274?279.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. Cognitive Psychology, 8:441?
480.
M. Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is ?it??
Event synchronization in gaze-speech input systems.
In Proceedings of ICMI?03, pages 151?157.
Z. Prasov, J. Y. Chai, and H. Jeong. 2007. Eye gaze
for attention prediction in multimodal human-machine
conversation. In 2007 Spring Symposium on Inter-
action Challenges for Artificial Assistants, Palo Alto,
California, March.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze
in spoken language processing for multimodal con-
versational interfaces. In NAACL?07, pages 284?291,
Rochester, New York, April.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cognitive
Science, 26(1):113?1146.
J. M. Siskind. 1995. Grounding language in perception.
Artificial Intelligence Review, 8:371?391.
K. Tanaka. 1999. A robust selection system using real-
time multi-modal user-agent interactions. In Proceed-
ings of IUI?99, pages 105?108.
M. K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
Science, 268:1632?1634.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. Proceedings
of AAAI?04.
375
Discourse Structure for Context Question Answering 
Joyce Y. Chai Rong Jin 
Department of Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48864 
jchai@cse.msu.edu, rongjin@cse.msu.edu 
 
 
 
Abstract 
In a real-world setting, questions are not 
asked in isolation, but rather in a cohesive 
manner that involves a sequence of related 
questions to meet user?s information needs. 
The capability to interpret and answer 
questions based on context is important. In 
this paper, we discuss the role of discourse 
modeling in context question answering.  In 
particular, we motivate a semantic-rich 
discourse representation and discuss the 
impact of refined discourse structure on 
question answering.   
1 Introduction 
In a real-world setting, questions are not asked in 
isolation, but rather in a cohesive manner that 
involves a sequence of related questions to meet 
user?s information needs. The capability to interpret 
and answer questions based on context is important. 
For example, Figure 1 shows an example of a series 
of context questions. In this example, the 
interpretation of Q2 and Q4 depends on the 
resolution of ?it? and ?this? from the context 
respectively. Although neither Q3 nor Q6 requires 
any anaphora resolution, the interpretation of Q3 
depends on Q2 while the interpretation of Q6 
depends solely on itself. Furthermore, in Q5, there 
are no explicit references. Its interpretation depends 
on a preceding question (e.g.,Q4), however, in a 
different manner.     
This example indicates that interpreting each of 
these questions and extracting answers needs to be 
situated in a particular context as the QA session 
proceeds. There are situations where a question is 
?complete? enough and its interpretation does not 
depend on the previous questions (Q6).  There are 
also situations where the interpretation of a question 
depends on preceding questions no matter whether it 
requires anaphora or ellipsis resolution. Based on 
these observations, a natural question to ask is what 
makes the use of discourse differently in different 
situations? What is the role of discourse in context 
question answering? 
To address these questions, a key issue, in our 
mind, is that every question and its answer have a 
discourse status with respect to an entire QA session. 
This discourse status includes two aspects. The first 
aspect relates to discourse roles of entities in a 
question and the corresponding answer. Entities (such 
as noun phrase, verb phrase, preposition phrase, etc) 
in a question carry distinctive roles that indicate what 
is the topic or focus of a question in terms of the 
overall information seeking discourse. Topic relates 
to the ?aboutness? of a question and focus relates to a 
specific perspective of the topic. The second aspect 
of discourse status relates to discourse transitions that 
indicate how discourse roles are changed from one 
question to another as the interaction proceeds and 
how such changes reflect the progress of user 
information needs. Both discourse roles and 
discourse transitions determine whether the context is 
useful, and if so, how to use the context to interpret a 
question. 
This paper takes an initial attempt to investigate 
the discourse status for context question answering. 
In particular, it motivates a semantic-rich discourse 
representation that captures both discourse roles of a 
question and discourse transitions between questions. 
Through examples, this paper further discusses the 
potential impact of this refined discourse structure on 
context question answering.   
Q1: What is the name of the volcano that destroyed 
the ancient city of Pompeii?  
Q2: When did it happen?  
Q3: how many people were killed?  
Q4: how tall was this volcano? 
Q5: Any pictures?  
Q6: Where is Mount Rainier?  
 
Figure 1: An example of context questions 
2 Semantic-rich Discourse Modeling 
For processing single questions, an earlier study 
shows that an impressive improvement can be 
achieved when more knowledge-intensive NLP 
techniques are applied at both question and answer 
processing level (Harabagiu et al, 2000). For context 
questions, a parallel question would be whether rich 
contextual knowledge will help interpret subsequent 
questions and extracting answers. To address this 
question, we propose a semantic-rich discourse 
modeling that captures both discourse roles of 
questions and discourse transitions between 
questions, and investigate its usefulness in context 
question answering.  
2.1 Discourse Roles  
In context question answering, each question is 
situated in a context. In addition to the semantic 
information carried by important syntactic entities 
(such as noun phrase, verb phrase, preposition 
phrase, etc), each question also carries distinctive 
discourse roles with respect to the whole question 
answering discourse. Specifically, the discourse roles 
can be categorized based on both the informational 
and intentional perspectives of discourse (Hobbs, 
1996), as well as the presentation aspect of both 
questions and answers.   
The intentional perspective relates to the purpose 
of a question. In a fully interactive question 
answering environment, instead of asking questions, 
a user may need to reply to a clarification question 
prompted by the system or may need to simply ask 
for a confirmation. Therefore, it is important to 
capture the intention from the user (Grosz and Sidner 
1986). The informational perspective relates to the 
information content of a question, in particular, the 
topic and the focus based on the semantics of the 
content. In addition to the intentional and 
informational aspects, there is also a presentational 
aspect of discourse that relates to both the input 
modality (i.e., questions) and the output modality 
(i.e., answers). For example, a user may explicitly ask 
for images or pictures of a person or event. The 
presentation aspect is particularly important to 
facilitate multimodal multimedia question answering. 
Therefore, for a given question, three types of 
discourse roles: Intent, Content, and Media can be 
captured to reflect the intentional, informational, and 
presentational perspectives of discourse respectively.   
These discourse roles can be further characterized 
by a set of features. For example, Intent can be 
represented by Act and Motivator, where Act indicates 
whether the user is requesting information from the 
system or replying to a system question. Motivator 
corresponds to the information goal as to what type 
of action is expected from the system, for example, 
whether information retrieval or confirmation (Chai 
et al 2003). We will not elaborate Intent here since it 
has been widely modeled for most dialog systems.  
 Content can be characterized as Target, Topic and 
Focus. Target indicates the expected answer type such 
as whether it should be a proposition (e.g., for why 
and how questions), or a specific type of entity (e.g., 
TIME and PLACE).  
Topic indicates the ?aboutness? or the scope 
related to a question. Focus indicates the current 
focus of attention given a particular topic. Focus 
always refers to a particular aspect of Topic. Since the 
informational perspective of discourse should capture 
the semantics of what has been conveyed, Topic and 
Focus are linked with the semantic information of a 
question, for example, semantic roles as described in 
(Gildea and Jurafsky 2002). Semantic roles concern 
with the roles of constitutes in a question in terms of 
its predicate-argument structure. The discourse roles 
link the semantic roles of individual questions 
together with respect to the discourse progress 
through Topic and Focus. 
For example, Topic can be of type Activity or Entity. 
Activity can be further categorized by ActType, 
Participant, and Peripheral.  ActType indicates the type 
of the activity; Participant indicates entities that are 
participating in the activity with different semantic 
roles. Peripheral captures auxiliary information such 
 
Content
Target: $NAME 
Topic: Activity
ActType: Destroy [Term: ?destroy?]
Participant1: Entity
SemRole: Agent
SemType: volcano
Id: ?
Term: ?the volcano? 
Participant2: Entity
SemRole: Theme
SemType: city
Id: ?Pompeii?
Term: ?Pompeii?        
Focus: Topic.Activity-Participant1
[Element: Name 
[Value: ?; Term: ?name? ]]
Intent
Act: Request
Motivator: AnsRequest
Media
Format: Text
Genre: Default
 
Figure 2: Discourse roles for Q1.   
as the time, place, purpose, and reason for such an 
activity. Entity can be categorized by SemRole, 
SemType, Id, Element, and Constraint. SemRole 
indicates the semantic role of the entity in a particular 
activity (if any). SemType represents the semantic 
type of the entity. Element indicates the specific 
features associated with the entity. Constraint 
specifies the constraints need to be satisfied to 
identify the entity, and Id specifies the particular 
identifier of the entity that particularly corresponds to 
pronouns, demonstratives, and definite noun phrases. 
Media indicates the desired information media, 
which can be further characterized as Format and 
Genre as shown in Figure 2. Format indicates whether 
it is an image, a table, or text, etc. Genre specifies the 
answer needs such as summary or list. If it is a list, 
how many should be in the list as in the question 
?number ten largest cities in the world.? 
Figure 2 shows the representation of discourse 
roles of Q1 using typed feature structures (Carpenter 
1992), where Intent indicates that the user is 
requesting for the system to retrieve an answer. Topic 
indicates the topic of Q1 is a Destroy Activity, which 
has two participants. The first participant is some 
kind of unknown volcano that takes the role of Agent 
in the activity (i.e., the destroyer). The second 
participant is the city of Pompeii that takes the role of 
Theme indicating the thing destroyed. The Focus of 
Q1 is about the name (i.e., Element) of the entity in 
the first participant (i.e., Participant1) in the Topic 
representation.  
The granularity of discourse roles can be varied. 
The finer the granularity, the better is the use of 
context for inference (as discussed later). However, 
the finer granularity also implies deeper semantic 
processing. This semantic rich representation can be 
used to generate other representations such as queries 
based on weighted terms for information retrieval or 
logical forms for deduction and inference (Waldinger 
et al, 2003). Furthermore, this representation is able 
to keep all QA sessions in a structured way to support 
inference, summarization, and collaborative fusion as 
described later.   
2.2 Discourse Transitions 
Transitions from one question to another also 
determine how context will be used in interpreting 
questions and retrieving answers. In this section, we 
use query formation as an example to illustrate the 
role of different types of discourse transitions.  
Discourse transitions also correspond to the 
intentional, informational, and presentational 
perspectives of discourse. Intentional transitions are 
closely related to Grosz and Sidner?s ?dominance? 
and ?satisfaction precedence? relations, which are 
more relevant to plan-based discourse (Grosz and 
Sidner, 1986). Here we focus on informational 
transitions and presentational transitions that are 
more relevant to QA systems since they are targeted 
for information exchange.   
Informational transitions are mainly centered 
around Topics of questions. In context question 
answering, how questions are related to each other 
depends on how ?topics? of those questions evolve. 
Currently, we categorize information transitions into 
three types: Topic Extension, Topic Exploration, and 
Topic Shift.  
 
Topic Extension 
A question concerns a similar topic as that of a 
previous question, but with different participants, 
peripheral, or constraints. It has the following 
subcategories: 
Constraint Refinement 
A question is about a similar topic with additional or 
revised constraints. For example:  
Q7: What?s the crime rate in Maryland and Virginia? 
Q8: What is it ten years ago?  
For another example:  
Q9: What?s the crime rate in Maryland and Virginia? 
Q10: What was it in Alabama and Florida? 
In both examples, both questions share the topic of 
?crime rate?, but concerning different crime rates 
with different constraints.  Interpreting the second 
question requires not only identifying constraints, but 
also the relations between constraints. In the first 
example, the constraints from Q7 need to be used to 
form a query for Q8. However, constraints from Q9 
should not be used for Q10.  
 
Participant Shift 
A question is about a similar topic with different 
participants.  
For example:  
Q11: In what country did the game of croquet 
originate?  
Q12: What about soccer? 
In this example, both questions are about the 
origination of a certain sport. The Content structure 
for both questions are the same except for the 
Participant role, which in Q11 is ?croquet? and in Q12 
is ?soccer?. Therefore, the query created for Q12 
would be {country, soccer, originate}, the keyword 
?croquet? should not appear in the query list.   
 
Topic Exploration  
Two questions are concerning the same topic, but 
with different focus (i.e., asking about different 
aspects of the topic).  For example,  
Q13: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q14: When did this happen?  
In this example, ?this? in Q14 refers to the same 
activity topic in Q13, but focus on the TIME 
peripheral information about the activity.  
In the following example,  
Q15: Where is Mount Rainier?  
Q16: How tall is it?  
Q15 asks about the location of Mount Rainier (which 
is an entity topic) and Q16 asks about a different 
aspect (i.e., the height) of the same entity topic. In 
both examples, significant terms representing the 
Topic from the preceding question can be merged 
with the significant terms in the current question to 
form a query.   
 
Topic Shift  
Two consecutive questions could ask about two 
different topics.  Different topic shifts indicate 
different semantic relations between two questions.  
Activity Topic shifts to another Activity Topic 
In the following example,  
Q17: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q18: How many people were killed?  
The topic of both questions concerns about certain 
activities. This activity shift indicates that ?kill? 
activity is a consequence of ?destroy? activity (i.e., 
Q18 is a consequence of Q17).  
Other relations can also be entailed from such a 
transition such as ?effect-cause? relation as in the 
following example (Harabagiu et al 2001):  
Q19: Which museum in Florence was damaged by a 
major bomb explosion in 1993? 
Q20: How much explosive was used?  
 
Activity Topic shifts to Entity Topic 
In the example: 
Q21: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q22: How tall is this volcano?  
The topic of Q21 is an activity of ?destroying? and 
the focus is the agent of the activity ?the volcano?. 
This focus becomes the topic of Q22. This transition 
indicates a further probing of a particular participant 
in an activity that can be independent of the activity 
itself. Therefore, the terms in Q21 will not be helpful 
in setting up the stage for processing Q22. Q21 
should be used only to resolve reference to the 
definite noun phrase ?this volcano?.  
Related to the presentational perspective of a QA 
discourse, we currently only identify: Media Shift.  
This relation indicates that two questions are about 
the same information content, but with different 
preference of media presentation.  
For example,  
Q25: how tall is Mount Vesuvius? 
Q26: Any pictures? 
Q26 is asking for the images of the Mount Vesuvius. 
This indicates that the backend should perform image 
retrieval rather than text retrieval.  
In summary, given two consecutive questions (Qi, 
Qi+1), a certain transition exists from Qi to Qi+1. 
These transitions determine how the context, for 
example, proceeding questions and answers can be 
used in interpreting the following question and 
identifying the potential answers. Here we only list 
several examples to show the importance of these 
transitions, which are by no means complete. We 
plan to identify a list of salient transitions for 
processing context questions as well as their 
implications (e.g., semantic relations) in interpreting 
context questions.  
2.3 Discourse Processing 
Given the above discussion, the goal of discourse 
modeling for context question answering is to 
automatically identify the discourse roles of a 
question and discourse relations between questions as 
the QA session proceeds. This may be a difficult task 
that requires rich knowledge and deep semantic 
processing. However, the recent advancement in 
semantic processing and discourse parsing has 
provided an excellent foundation for this task.  
The discourse roles are higher-level abstracts of 
the semantic roles as those provided in FrameNet 
(Baker et al, 1998) and Propbank (Kingsbury and 
Palmer 2002). Recent corpus-based approaches to 
identify semantic roles (Roth et al2002, Gildea and 
Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et 
al., 2003) have been successful in identifying domain 
independent semantic relations with respect to the 
predicate-argument structure. Furthermore, recent 
work also provides discourse annotated corpora with 
rhetorical relations (Carlson, et al, 2003) and 
techniques for discourse paring for texts (Soricut and 
Marcu, 2003).  All these recent advances make the 
semantic-rich discourse modeling possible.  
For example, a collection of context questions 
(and answers) can be annotated in terms of their 
discourse roles and relations. Specifically, the 
following information can be either automatically 
identified or manually annotated: 
? Syntactic structures automatically identified from 
a parser (Collins, 1997); 
? Semantic roles of entities in the question (Gildea 
and Jurafsky 2002; Gildea and Palmer 2002; 
Surdeanu et al, 2003); 
? Discourse roles either manually annotated or 
identified by rules that map directly from semantic 
roles to discourse roles.  
? Discourse transitions automatically determined 
once discourse roles are identified for each 
question.  
? Semantic relations between questions manually 
annotated. 
? Answers provided by the system.  
Based on this information, important features can be 
identified. Different learning models such as decision 
trees or Bayesian classifier can be applied to learn the 
classifier for discourse roles and relations. Strategies 
can be built to take into account of discourse roles 
and relations from preceding questions and answers 
to process a subsequent question and extract answers. 
These models can then be applied to process new 
context questions.  
3 Refined Discourse Structure in Context 
Question Answering 
Based on the above discussion, during the question 
answering process, a discourse structure can be 
created to capture the discourse roles of each 
ActType
Activity
City
?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
Focus
???
    
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
?
Peripheral
Value
Focus
 
 
(a) Discourse repreentation after processing Q1                (b) Discourse representation after processing Q2             
 
 
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
79 AD
Peripheral1
Value
Focus
Consequence
Activity
EntityKill
Patient
Person SizeOfSet ?
Peripheral2
Type
ActType Participant1
SemType
SemRole
Element
Value
Value
        
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
79 AD
Peripheral
Value
Focus
Height
Element
Value
?
Activity
EntityKill
Patient
Person SizeOfSet Thousands
Peripheral2
Type
ActType Participant1
SemType
SemRole
Element
Value
Consequence
 
 
(c) Discourse representation after processing Q3              (d) Discourse representation after processing Q4             
question and discourse relations between questions.  
Similar to information extraction for free texts, this 
refined discourse structure captures the salient 
information extracted from the question answering 
process. This discourse provides a structured 
information space that indicates what type of 
information has been exchanged and how 
information obtained at different stages is related. In 
other words, we can also consider this representation 
as the ?mental map? of user information needs. This 
mental map will potentially provide a basis to 
improve question interpretation and answer 
extraction through inference, summarization, and 
collaborative question answering.    
3.1 Discourse Representation 
The typed feature structures can be represented as 
Directed Acyclic Graph (DAG). Thus the described 
discourse structure can be represented as semantic 
networks using DAGs. For example, Figure 3 shows 
the discourse representation after processing the each 
of the first four questions in Figure 1. In this network, 
each node is either a specific value (i.e., leaf nodes) 
or a typed feature structure itself (i.e., internal node). 
Each directed link corresponds to a particular feature. 
Note that because of the space limit, not everything 
represented in the feature structure in Figure 2 is 
shown here in the semantic network. For example, 
the type of an activity (e.g., Destroy) by itself is a 
feature structure (in Figure 2) that further consists of 
the specific term used in the question.  This term is 
not shown but is included in the semantic nets.  
As context question answering proceeds, the 
semantic network(s) for discourse grows, with 
different pointers of Topic and Focus. For example, 
Figure 3(a) represents Q1, where Topic points an 
Activity feature structure and Focus points to the Name 
Element of the Participant1 in the Activity. From Q1 to 
Q2, there is a transition of Topic Exploration which 
indicates that Q2 is about the same topic, but with a 
different focus. Therefore, in Figure 3(b), the Topic 
points to the same activity, but the Focus now points 
to the peripheral Time information of that activity.  
Next, Q3 is about a different topic involving activity 
Kill. However, since there is a consequence relation 
from Q2 to Q3, the activity asked in Q3 actually 
fulfills the Peripheral role of Consequence for the 
previous activity as shown in Figure 3(c). Finally, in 
Q4, there is a gap between Q3 and Q4, however, 
there is a transition of Probing from Q2 to Q4. Now 
the Topic becomes the Participant in Q1 as shown in 
Figure 3(d).  
3.2 Potential Impacts 
The growth of the semantic networks represents the 
overall information needs of a user and how such 
information needs are related. Since this is a 
structured representation, it can be queried and used 
to facilitate context question answering, for example, 
in the following aspects: 
? Query expansion and answer retrieval 
? Inference and summary for question answering 
? Collaborative question answering 
To process questions, most systems will first form 
a query of keywords to represent the current question 
and to retrieve relevant passages that may contain the 
potential answers. In context question answering, 
since the interpretation of a question may depend on 
preceding questions, some keywords from preceding 
questions may need to be included in the query for 
the current question. The fine-grained discourse 
structure will enhance answer retrieval through more 
controlled selection of terms from preceding 
questions and answers. For example, strategies can be 
developed to select query terms depending on the 
discourse relations. Different discourse roles and 
transitions may lead to different weighting schemes 
for query expansion.  
Furthermore, the information captured in the 
discourse structure can help make predication about 
what the user information need is and therefore 
provide more intelligent services to help user find 
answers. For example, semantic and discourse 
relations between different topics and focuses of a 
series of questions can help a system infer and predict 
the overall interest of a user. Although answers to 
each question may come from different sources, 
based on the structured discourse (e.g., in semantic 
network), the system can aggregate information and 
generate summaries.  
Another potential impact of the refined structured 
discourse is to facilitate collaborative question 
answering. Very often, various users may have a 
similar interest about a set of topics. The structured 
discourse built for one user can be used to help 
answers questions from another user. A user may 
have a certain information goal in mind, but does not 
know what types of questions to ask.  Therefore, a 
user?s question may be very general and vague, such 
as ?what happened to Pompeii?? This question needs 
to be decomposed into a set of smaller questions. The 
discourse structure that connects different aspects of 
topics together can provide some insight on how such 
decomposition should be made. Furthermore, the 
discourse structure from a skilled user can enable the 
system to intelligently direct a novice user in his 
information seeking process.  
4 Discussion 
TREC 10 Question Answering Track initiated a 
context-task that was designed to investigate the 
system capability to track context through a series of 
questions. As described in (Voorhees 2001), there 
were two unexpected results of this task. First, the 
ability to identify the correct answer to a question in 
the later series had no correlation with the capability 
to identify correct answers to preceding questions. 
Second, since the first question in a series already 
restricted answers to a small set of documents, the 
performance was determined by whether the system 
could answer a particular type of question, rather than 
the ability to track context. Because of these 
unexpected results, the context task has been stopped 
in the following TREC evaluations (Voorhees 2002).  
The reasons that TREC 10 did not achieve the 
expected results, in our opinion, lie in two aspects. 
The first aspect relates to the uniqueness of open 
domain context question answering. In open domain 
QA, first, there may be many occurrences of correct 
answers in various part(s) of document(s). Second, 
there may be multiple paths (e.g., different 
combination of key query terms) that can lead to one 
occurrence of the correct answer. Therefore, the 
correct answer to a previous question may not be 
critical in finding answers to subsequent questions. 
This phenomenon may provide an opportunity to find 
answers without explicitly modeling context (i.e., by 
keeping track of the discourse objects from answers), 
but rather identifying and using relevant context.  
For example, in the LCC system (that achieved 
the best result for the context task in TREC 10), the 
discourse was not explicitly represented (Harabagiu 
et al2001). Instead of resolving references using 
discourse information, the LCC system first identifies 
the question that contains the potential referents and 
uses those questions and the current question to 
identify the target paragraph. Thus, question 
interpretation does not depend on the answers, but 
rather depends on the context that is dynamically 
identified as a list of preceding questions. Now the 
question is whether the system will achieve even 
better results (e.g., correctly find answers to the rest 
of the eight questions) with some context 
representation?  
Another more important question to be asked is 
whether the design of the context task just happened 
to provide an opportunity to achieve good results 
without modeling the context. As discussed in 
(Harabagiu et al, 2001), answers to 85% of context 
questions actually occurred within the same 
paragraph as the answers to the previous questions. 
Therefore, just using preceding questions, the system 
was able to find the target paragraph and the final 
answer really depended on the capability to identify 
different types of answers in that paragraph. What if 
a series of questions were designed differently so that 
questions are related but answers are scattered in 
different documents or paragraphs.  Will the shallow 
processing of discourse succeed in finding the 
answers?  
Furthermore, the ultimate goal of QA systems is 
to be able to access information from different 
sources (e.g., unstructured text or structured 
database) and to provide intelligent dialog capability.  
One important question we need to address is what 
kind of discourse representation will be sufficient to 
support these capabilities? For example, to access 
structured databases, the answer to a previous 
question usually narrows down the search spaces in 
the database for subsequent questions. Thus, previous 
answers usually determine where in the database an 
answer can be found. Therefore, it is important to 
keep track of previous questions and answers in some 
kind of structure for later use.  
The second reason that TREC 10 did not achieve 
expected results relates to evaluation methodology. In 
context QA, good performance depends on two 
important components: the capability of representing 
and using the relevant context (both explicitly or 
implicitly) and the general capability of interpreting 
questions and extracting answers. The level of 
sophistication in either component will influence the 
final performance. Thus, by comparing the final 
answers to each context question, the evaluation of 
the context task in TREC 10 was not able to isolate 
the effect of one component from another. It is not 
feasible to identify that when an answer is not 
identified, whether it is because of poor 
representation and use of the discourse information or 
it is because of the general limitations of the 
capability to process certain types of questions. 
Therefore, to study the role of discourse in context 
question answering, a more controlled evaluation 
mechanism is desired. For example, one approach is 
to keep the general processing capability as a 
constant and vary the representations of discourse 
and strategies to use the discourse so that their 
different impacts on the final answer extraction can 
be learned.  
As a summary, the experience in the TREC 10 
context task is very valuable. It does not discount the 
importance of context modeling for context 
questions. But rather, it motivates a more in-depth 
investigation of the role of discourse in context 
question answering.  
5 Conclusion 
Questions are not asked in isolation, but rather in a 
cohesive manner that involves a sequence of related 
questions to meet user?s information needs. It is 
important to understand the role of discourse to 
support this cohesive question answering.  
By all means, a QA discourse can be represented 
as coarse as a list of keywords extracted from 
previous questions or as sophisticated as a fine-
grained representation as described in this paper. 
There is a balance between how much we like to 
represent the context and how far we can get there.  
Given recent advances in text-based domain 
independent semantic processing and discourse 
parsing, as well as the availability of rich semantic 
knowledge sources, we believe it is the time to start 
from the other end of the spectrum to examine the 
possibility and impact of semantic-rich discourse 
representation for open-domain question answering.     
References 
Baker, C., Fillmore, C., and Lowe, J. 1998. The Berkeley 
FrameNet project. In Proceedings of COLING/ACL, pp. 
86-90, Montreal, Canada 
Carpenter, R.1992. The Logic of Typed Feature Structures. 
Cambridge University Press.  
Chai, J., Pan, S., and Zhou, M. 2003. MIND: A Context-
based Multimodal Interpretation Framework in Conver-
sational Systems, Natural, Intelligent and Effective In-
teraction in Multimodal Dialogue Systems,  Eds. O. 
Bernsen ,  L. Dybkjaer and  J.  van Kuppevelt, Kluwer 
Academic Publishers. 
Carlson, L., Marcu, D., and Okurowski, M. 2003. Building 
a discourse-tagged corpus in the framework of Rhetori-
cal Structure Theory. In Jan van Kuppevelt and Ronnie 
Smith, editors, Current Directions in Discourse and 
Dialogue, Kluwer Academic publishers.  
Collins, M. 1997. Three Generative, Lexicalized Models 
for Statistical Parsing. In Proceedings of the 35th Annual 
meeting of the Association for Computational Linguis-
tics (ACL 1997): 16-23, Madrid, Spain.  
Fillmore, C. and Atkins, B. 1998. FrameNet and lexico-
graphic relevance, Proceedings of the First Interna-
tional Conference on Language Resources and 
Evaluation, Granada, Spain. 
Grosz, B. J. and Sidner, C. 1986. Attention, intention, and 
the structure of discourse. Computational Linguistics, 
12(3):175-204. 1986. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of 
Semantic Roles. Computational Linguistics, 28(3):245-
288.  
Gildea, D. and Palmer, M. 2002. The Necessity of Parsing 
for Predicate Argument Recgnition. In Proceedings of 
the 40th Meeting of the Association for Computational 
Linguistics (ACL 2002): 239-246, Philadelphia, PA.  
Harabagiu, S., Pasca, M., and Maiorano, S. Experiments 
with Open-domain Textual Question Answering. In 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING-2000), 2000 
Harabagiu, S., et al, Answering Complex, List and Con-
text Questions with LCC?s Question-Answering Server. 
In the Proceedings of TREC 2001, 2001. 
Hobbs, J. 1996. On the relations between the informational 
and Intentional Perspectives on Discourse. In Burning 
Issues in Discourse: An inter-Disciplinary Account  
(eds. E. Hovy and D. Scott), volume 151 of NATO ASI 
Series, Series F: Computer and Systems Sciences, pp 
139-157. Springer-Verlag, Berlin, Germany 
Kingsbury, P. and Palmer, M. 2002. From Treebank to 
Propbank. In Proceedings of the 3rd International Con-
ference on Language Resources and Evaluation (LREC-
2002), Las Palmas, Canary Islands, Spain.  
Roth, D., et al  2002. Question Answering via Enhanced 
Understanding of Questions. Proceedings of 
TREC2002.  
Surdeanu, M., Harabagiu, S., Williams, J., and Aarseth, P. 
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41th Meeting of 
the Association for Computational Linguistics (ACL 
2003), Sapporo, Japan.  
Soricut, R. and Marcu, D. 2003. Sentence Level Discourse 
Parsing using Syntactic and Lexical Information. In 
Proceedings of HLT-NAACL. Edmonton, Canada. 
Waldinger, R., et al, 2003. Deductive Question Answering 
from Multiple Resources, New Directions in Question 
Answering, AAAI, 2003 
Voorhees, E. 2001. Overview of TREC 2001 Question 
Answering Track. Proceedings of TREC2001.  
Voorhees, E. 2002. Overview of TREC 2002 Question 
Answering Track. Proceedings of TREC2002.  
 
