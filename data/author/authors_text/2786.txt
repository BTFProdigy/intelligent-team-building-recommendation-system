Corpus-Based Syntactic Error Detection Using Syntactic Patterns 
Koldo Gojenola, Maite Oronoz 
Informatika Fakultatea, 649 P. K., Euskal Herriko Unibertsitatea, 
20080 Donostia (Euskal Herria) 
jipgogak@si.ehu.es, jiboranm@si.ehu.es 
Abstract 
This paper presents a parsing system for the 
detection of syntactic errors. It combines a 
robust partial parser which obtains the main 
sentence components and a finite-state 
parser used for the description of syntactic 
error patterns. The system has been tested on 
a corpus of real texts, containing both 
correct and incorrect sentences, with 
promising results. 
Introduction 
The problem of syntactic error detection and 
correction has been addressed since the early 
years of natural language processing. Different 
techniques have been proposed for the treatment 
of the significant portion of errors (typographic, 
phonetic, cognitive and grammatical) that result 
m valid words (Weischedel and Sondheimer 
1983; Heidorn et al 1982). However, although 
most currently used word-processors actually 
provide a grammar checking module, little work 
has been done on the evaluation of results. 
There are several reasons for this: 
? Incomplete coverage. Some of the best 
parsers at the moment can analyze only a 
subset of the sentences in real texts. 
Compared to syntactic valid structures, the set 
of syntactically incorrect sentences can be 
considered almost infinite. When a sentence 
cannot be parsed it is difficult to determine 
whether it corresponds to a syntactic error or 
to an uncovered syntactic onstruction. In the 
literature, syntactic errors have been defined 
mostly with respect to their corresponding 
correct constructions. The use of unrestricted 
corpora confronts us with the problem of 
flagging a correct structure as erroneous 
(false alarms). These facts widen the scope of 
the problem, as not only incorrect structures 
but also correct ones must be taken into 
account. 
On the other hand, robust parsing systems 
(e.g., statistical ones) are often unable to 
distinguish ungrammatical structures from 
correct ones. 
24 
? The need for big corpora. Each kind of 
syntactic error occurs with very low 
frequency and, therefore, big corpora are 
needed for testing. Even if such corpora were 
available, the task of recognizing error 
instances for evaluation is a hard task, as there 
are no syntactically annotated treebanks with 
error marks for the purposes of evaluation 
and testing. Thus, to obtain naturally 
occurring test data, hundreds of texts must be 
automatically and manually examined and 
marked. 
The aim of the present work is to examine the 
feasibility of corpus-based syntactic error 
detection, with methods that are sensitive enough 
to obtain high correction rates and 
discriminating enough to maintain low false 
alarm rates. The system will be applied to 
Basque, an agglutinative language with relative 
free order among sentence components. Its 
recent standardization makes it necessary to 
develop a syntactic hecking tool. 
The remainder of this paper is organized as 
follows. After commenting on the literature on 
syntactic error detection in section 2, section 3 
presents a description of the linguistic resources 
we have used. Section 4 describes the error types 
we have treated, while section 5 gives the 
evaluation results. 
1 Background 
Kukich (1992) surveys the state of the art in 
syntactic error detection. She estimates that a 
proportion of all the errors varying between 
25% and over 50%, depending on the 
application, are valid words. Atwell and Elliott 
(1987) made a manual study concluding that 
55% of them are local syntactic errors 
(detectable by an examination of the local 
syntactic context), 18% are due to global 
syntactic errors (involving long-distance 
syntactic dependencies, which need a full parse 
of the sentence), and 27% are semantic errors. 
Regarding their treatment, different approaches 
have been proposed: 
? The relaxation of syntactic constraints 
(Douglas and Dale 1992). This grammar- 
based method allows the analysis of sentences 
\[ . . . . . . . . . . . . . . . . . . . . .  
, Sentence I 
I I 
I Morphological 
analysis and disambiguation 
II 
i 
Chart-parser 
, chart (automaton) I 
J I 
Finite-state parser 
, No Error / Error Type(s) ,J 
I . . . . . . . . . . . . . . . . . . . . .  | 
Figure 1. Overview of the system. 
that do not fulfill some of the constraints of 
the language by identifying a rule that might 
have been violated, determining whether its 
relaxation might lead to a successful parse. Its 
main disadvantage is the need of a full- 
coverage grammar, a problem not solved at 
the moment, except for restricted 
environments (Menzel and SchrOder 1999). 
? Error patterns (Kukich 1992; Golding and 
Schabes 1996; Mangu and Brill 1997), in the 
form of statistical information, hand-coded 
rules or automatically earned ones. 
? Charts have been used in grammar-based 
systems as a source of information; they can 
be resorted to if no complete analysis is 
found, so as to detect a syntactic error 
(Mellish 1989; Min-Wilson 1998). 
2 L ingu is t i c  resources  
We have used a parsing system (Aldezabal et al 
1999, 2000) divided in three main modules (see 
figure 1): 
? Morphological analysis and disambiguation. 
A robust morphological analyzer (Alegria et 
al. 1996) obtains for each word its 
segmentation(s) into component morphemes. 
After that, morphological disambiguation 
(Ezeiza et al 1998) is applied, reducing the 
high word-level ambiguity from 2.65 to 1.19 
interpretations. 
? Unification-based chart-parsing. After 
morphological nalysis and disambiguation, a 
PATR-II unification grammar is applied 
bottom-up to each sentence, giving a chart as 
a result. The grammar is partial but it gives a 
complete coverage of the main sentence 
elements, uch as noun phrases, prepositional 
phrases, sentential complements and simple 
sentences. The result is a shallow parser 
(Abney 1997) that can be used for 
subsequent processing (see figure 2). In this 
figure, dashed lines are used to indicate 
lexical elements (lemmas and morphemes), 
while plain lines define syntactic onstituents. 
Bold circles represent word-boundaries, and 
plain ones delimit morpheme-boundaries. 
The figure has been simplified, as each arc is 
actually represented by its morphological nd 
syntactic information, in the form of a 
sequence of feature-value pairs. 
? Finite-state parsing. A tool is needed that will 
allow the definition of complex linguistic 
error patterns over the chart. For that reason, 
we view the chart as an automaton to which 
finite-state constraints can be applied 
encoded in the form of automata and 
transducers (we use the Xerox Finite State 
Tool, XFST, (Karttunen et al 1997)). Finite- 
state rules provide a modular, declarative and 
flexible workbench to deal with the resulting 
chart. Among the finite-state operators used, 
we apply composition, intersection and union 
of regular expressions and relations. 
PP (in the nice house at the mountain) 
~modi f ie r  (at the mountain) 
~ -  . _ - -  ~ S (I have seen (it)) 
~ PP (in the nice house) kk ~ e s e e n ~  
mend i~o-~'O- -k~O " 0 et ~e"~)~ ~" 0 " 
Figure 2. State of the chart after the analysis ofMendiko etxepolitean ikusi dut nik ('I have seen (it) 
in the nice house at the mountain'). 
25  
Durangon, 1999ko martxoaren 7an 
In Durango, 1999, March the 7th 
(Durango, (1999 (March, (7, 
inessive, genitive) genitive, inessive 
sing) sing) sing) 
Example 1. Format of a valid date expression. 
The full system provides a robust basis, 
necessary for any treatment based on corpora. 
In the case of error detection, a solid base is 
indispensable. 
3 Er ror  detect ion  
As a test, we chose the case of date expressions 
due to several reasons: 
? It was relatively easy to obtain test data 
compared to other kinds of errors. Although 
the data must be obtained mostly manually, 
date expressions contain several cues (month 
names, year numbers) that help in the process 
of finding semiautomatically test sentences. 
In any case, manual marking is needed for all 
the retrieved sentences. 
? The context of application is wide, that is, 
date expressions contain morphologically 
and syntactically rich enough phenomena 
where several types of errors can be found. 
These can be viewed as representative of the 
set of local syntactic errors so that the same 
procedure can be used when dealing with 
other kinds of errors. Example 1 shows one 
of the formats of a date expression. 
Basque being an agglutinative language, most of 
the elements appearing in date expressions (year 
numbers, months and days) must be inflected, 
attaching to them the corresponding number 
and case morphemes. Moreover, each different 
date format requires that the elements involved 
appear in fixed combinations. This is a common 
source of errors, not detectable by a spelling- 
checker, as each isolated word-form is correct. 
For evaluation, we collected 267 essays written 
by students (with a high proportion of errors) 
and texts from newspapers and magazines, 
totaling more than 500,000 words. From them 
we chose 658 sentences, including correct dates, 
incorrect dates, and also structures 'similar' to 
dates (those sentences containing months and 
years, which could be mistaken for a date), in 
order to test false positives (see table 1). As a 
result of the selection procedure, the proportion 
of errors is higher than in normal texts. We 
divided our data into two groups. One of them 
was used for development, leaving the second 
one for the final test. The proportion of correct 
dates is higher in the case of test data with 
respect o those in the development corpus, so 
that the effect of false positives will be evaluated 
with more accuracy. 
Number of sentences 
Correct dates 
i 
\[Structures 'similar' to dates 
Incorrect dates 
Incorrect dates with 1 error 
Deve lopment  
411 
corpus 
247 
65 39 
255 171 
91 37 
Test  corpus 
43 % 47 6 % 16 
Incorrect dates with 2 errors 42 % 46 27 % 73 
Incorrect dates with 3 errors 6 % 7 4 % 11 
Table 1. Test data. 
i 
Error t~,pe 
1. The year number cannot be inflected using a hyphen 
2. The month lmartxoak) must appear in lowercase 
3. The optional locative preceding dates (Frantzia) 
must be followed by a comma 
4. The day number after a month in genitive case 
(martxoaren) must have a case mark 
5. The day number after a month in absolutive case 
(ekainak) cannot have a case mark 
6. The month (martxoan) must be inflected in genitive 
or absolutive case 
Example  
I 
Donostian, 1995-eko martxoaren 14an 
1997ko martxoak 14 
Frantzia 1997ko irailaren 8an 
Donostian, 19995eko martxoaren 22 
1998.eko ekainak 14ean argitaratua 
Donostian, 1995.eko martxoan 28an 
Combination of errors I2, 3 and 4) karrera bukatu nuenean 1997ko Ekainaren 30an 
Table 2. Most frequent error types in dates. 
26  
define NP_Mon th_Absolu t ive or_Ergat ive 
define PP Year_Genitive 
define Error_Type 5 
define Mark_Error Type__5 
NP_Month_Abs olut ive_or_Ergat ive Inflected_Number; 
\[ Error__Type_5 \] @-> BEGINERRORTYPE5 "... " ENDERRORTYPE5 
I I Optional_place_Name Optional_Cor~na PP_Year_Genit ive _ 
Example 2. Regular expressions for an error pattern. 
After examining different instances of errors, we 
chose the six most frequent error types (see table 
2). In a first phase, one or more patterns were 
defined for each error type. However, we soon 
realized that this approach failed because quite 
often two or three errors might appear in the 
same expression. This phenomenon asked for a 
kind of 'gradual relaxation' approach, which 
had to consider that several mistakes could co- 
occur. Instead of treating each error 
independently, we had to design error patterns 
bearing in mind not only the correct expression, 
but its erroneous versions as well. For example, 
the last sentence in table 2 contains three 
different errors, so that the error pattern for the 
second error should consider the possibility of 
also containing errors 3 and 4. This relaxation 
on what could be considered a correct date had 
the risk of increasing the number of false 
positives. As the number of interactions among 
errors grows exponentially with the number of  
errors (there are potentially 2 6 combinations of 
the six error types), we based our error patterns 
on the combinations actually found in the 
corpus, so that in practice that number can be 
considerably reduced (we did not find any 
expression containing more than three errors in 
the corpus). 
The error pattern for the fifth kind of error (see 
example 21 ) is defined in two steps. First, the 
syntactic pattern of the error is defined (an NP 
consisting of a month in ergative or absolutive 
case followed by an inflected number), and 
named Error_Type5. Second, a transducer 
(Mark_Error_Type_5) is defined which 
surrounds the incorrect pattem (represented by 
Number of sentences 
Undetected date errors 
Detected date errors 
False alarms 
"... ") by two error tags (BEGINERRORTYPE5 
and ENDERRORTYPE5). To further restrict the 
application of the rule, left and right contexts for 
the error can be defined (in a notation 
reminiscent of two-level morphology), mostly to 
assure that the rule is only applied to dates, thus 
preventing the possibility of obtaining false 
positives. 
Concerning the definition of error patterns, 
equal care must be taken for correct and 
incorrect dates. In a first phase, we devised rules 
for the errors but, after testing them on correct 
dates from the development corpus, we had to 
extend the rules so as to eliminate false positives. 
As a result, more than 60 morphosyntactic 
patterns (each corresponding to a finite-state 
automata or transducer) were needed for the 
definition of the six basic error patterns. They 
range from small local constraints (45 automata 
with less than 100 states) to the most complex 
patterns (a transducer with 10,000 states and 
475,000 arcs). 
4 Eva luat ion  
Table 3 shows the results? As the development 
corpus could be inspected uring the refinement 
of the parser, the results in the second and third 
columns can be understood as an upper limit of  
the parser in its current state, with 100% 
precision (no false alarms) and 91% recall. 
The system obtains 84% recall over the corpus 
of previously unseen 247 sentences? 31 errors 
out of 37 are detected giving the exact cause of  
the error (in cases with multiple errors almost all 
of them were found)? 
Development corpus 
411 
7 9% 
84 91% 
0 
Table 3. Evaluation results. 
Test corpus 
'247 
6 16% 
31 84% 
5 
i For more information on XFST regular expressions, 
see (Karttunen et al 1997)? 
27 
Example 
atxiloketa 1998ko urtarriletik irailaren 16ra ... 
the imprisonment from January 1998 till the 16th of 
September 
Donostian 1960ko Urtarrilaren jaioa 
born in Donostia in the January of 1960 
etorriko da 1997ko irailaren 26ko 1 : 15etan 
it will come the 26 of Septernber 1997 at 1:15 
atzotik 1999ko abenduaren 31arte 
,from ~esterday until the 31st of December 
Primakovek 1998ko irailaren 1 in hartu zuen ... 
Primakov took it on the 11th o\[ September 1998 
Cause of the error 
Structure similar to a date incorrectly interpreted as a 
date and flagged as erroneous. 
Incorrect Basque construction that is interpreted asa 
date. 
The system takes the hour number (1:15) as the day of 
the month. 
The grammar does not cover the arte (until) particle, so 
a correct date is flagged as ungrammatical. 
The unknown word Primakov is interpreted as a 
locative. 
Table 4. False alarms. 
Regarding precision, there are 5 false alarms, 
that is, correct dates or sentences similar to dates 
flagged as erroneous. If these false positives are 
divided by the number of sentences (247) of the 
test corpus, we can estimate the false alarm rate 
to be 2.02% over the number of dates in real 
texts. Table 4 examines ome of the false alarms, 
two of them due to expressions imilar to dates 
that are mistaken for dates, other two relate to 
constructions not taken into account in the 
design of the partial grammar, and the last one is 
due to insufficient lexical coverage. 
Although the results are promising, more corpus 
data will be needed in order to maximize 
precision. 
Conc lus ions  
This work presents the application of a parsing 
system to syntactic error detection. The reported 
experiment has as its main features: 
? It is corpus-based. If a system is to be useful, 
it must be tested on real examples of both 
correct and incorrect sentences. Although this 
may seem evident, it has not been the case for 
most of the previous work on syntactic errors. 
This implies the existence of big corpora and, 
for most of the errors, manual annotation. 
? The most successful methods for error 
detection, i.e., relaxation of syntactic 
constraints and error patterns over a chart, 
have been combined with good results. On 
the other hand, the relaxation is not applied 
dynamically at parsing time, but it has been 
manually coded. This implies a considerable 
amount of work, as we had to consider the 
formats for valid sentences as well as for all 
their incorrect variants. 
? A partial robust parsing architecture provides 
a powerful way to consider simultaneously 
information at the morphemic and syntactic 
levels. The unification grammar is necessary 
to treat aspects like complex agreement and 
word order variations, currently unsolvable 
using finite-state networks. It constructs all 
the possible syntactic components. On the 
other hand, regular expressions in the form 
of automata nd transducers are suitable for 
the definition of complex error patterns 
based on linguistic units. 
We are currently exploring new extensions to the 
system: 
? Adding new kinds of errors. Our system, as 
well as any system dealing with syntactic 
errors, suffers the problem of scaling up, as 
the addition of new types of errors will 
suppose an increment in the number of error 
patterns that involves a considerable amount 
of work in the process of  hand-coding the 
rules. The possible interaction among rules 
for different error types must be studied, 
although we expect that the rule sets will be 
mostly independent. Another interesting 
aspect is the reusability of the linguistic 
patterns: in the process of treating errors in 
dates some patterns describe general 
linguistic facts that can be reused, while 
others pertain to idiosyncratic facts of dates. 
We plan to extend the system to other 
qualitatively different ypes of errors, such as 
those involving agreement between the main 
components of the sentence, which is very 
rich in Basque, errors due to incorrect use of  
subcategorization and errors in post- 
positions. Although the number of potential 
syntactic errors is huge, we think that the 
treatment of the most frequent kinds of error 
with high recall and precision can result in 
useful grammar-checking tools. 
? Automatic acquisition of error detecting 
patterns. Although manual examination 
seems unavoidable we think that, with a 
corpus of errors big enough, machine 
learning techniques could be applied to the 
28 
problem of writing error patterns (Golding 
and Roth 1996; Mangu and Brill 1997). This 
solution would be even more useful in the 
case of combinations of different errors. In 
any case, it must be examined whether 
automatic methods reach the high precision 
and reliability obtained by hand-coded rules. 
? Using either hand-coded rules or 
automatically learned ones, both methods 
have still the problem .of obtaining and 
marking big test corpora, a process that will 
have to be made mostly manually (except for 
some limited cases like word confusion 
(Golding and Roth 1996)). This is one of the 
major bottlenecks. 
Acknowledgements 
This research is supported by the Basque 
Government, the University of the Basque 
Country and the Interministerial Commission for 
Science and Technology (CICYT). Thanks to 
Gorka Elordieta for his help writing the final 
version of the paper. 
References 
Agirre E., Gojenola K., Sarasola K., Voutilainen A. 
(1998) Towards a Single Proposal in Spelling 
Correction. COLING-ACL'98, Montreal. 
Abney S. (1997) Part-of-Speech Tagging and Partial 
Parsing. In Corpus-Based Methods in Language and 
Speech Processing, Kluwer, Dordrecht, 1997. 
Aldezabal I., Gojenola K., Oronoz M. (1999) 
Combining Chart-Parsing and Finite State Parsing. 
Proceedings of the Student Session of the European 
Summer School in Logic, Language and 
Computation (ESSLLI'99), Utrecht. 
Aldezabal I., Gojenola K., Sarasola K. (2000) A 
Bootstrapping Approach to Parser Development. 
Sixth International Workshop on Parsing 
Technologies, Trento. 
Alegria I., Artola X., Sarasola K., Urkia. M. (1996) 
Automatic morphological nalysis of Basque. Literary 
& Linguistic Computing, Vol. 11. 
Atwell E., Elliott S. (1987) Dealing with Ill-Formed 
English Text. In The Computational Analysis of 
English: a Corpus-Based Approach, De. Longman. 
Douglas, S., Dale R. 1992. Towards Robust PATR. 
COL1NG'92, Nantes. 
Ezeiza N., Alegria I., Arriola J.M., Urizar R., Aduriz I. 
(1998) Combining Stochastic and Rule-Based 
Methods for Disambiguation in Agglutinative 
Languages. COLING-ACL-98, Montreal. 
Golding A. and Schabes. Y. (1996) Combining trigram- 
based and feature-based methods for context-sensitive 
spelling correction. In Proc. of the 34th ACL 
Meeting, Santa Cruz, CA. 
Golding A., Roth. D. (1996) A Winnow-based 
Approach to Spelling Correction. Proceedings of the 
13th International Conference on Machine Learning, 
ICML'96. 
Heidom G. E., Jensen K., Miller L. A., Byrd R. J., 
Chodorow M. S. (1982) The EPISTLE text-critiquing 
system. IBM Systems Journal, Vol. 21, No. 3. 
Karttunen L., Chanod J-P., Grefenstette G., Schiller A. 
(1997) Regular Expressions For Language 
Engineering. Journal of Natural Language 
Engineering. 
Kukich K. (1992) Techniques for automatically 
correcting words in text. In ACM Computing 
Surveys, Vol. 24, N. 4, December, pp. 377-439. 
Mangu L., Brill E. (1997) Automatic Rule Acquisition 
for Spelling Correction. Proceedings of the 14th 
International Conference on Machine Learning, 
ICML'97. 
Mellish C. (1989) Some Chart-Based Techniques for 
Parsing Ill-Formed Input. EACL' 89. 
Menzel W., Schr6der I. (1999) Error Diagnosis for 
Language Learning Systems. RECALL, special 
edition, May 1999. 
Min K., Wilson W. (1998) Integrated Control of Chart 
Items for Error Repair. COLING-ACL'98, Montreal. 
Roche E., Schabes Y. (1997) Finite-State Language 
Processing. MIT Press. 
Weischedel R.M., Sondheimer N.K. (1983) Meta-rules 
as a Basis for Processing Ill-Formed Input. American 
Journal of Computational Linguistics, 9. 
29 
A word-grammar based morl)hoh)gieal nalyzer 
for agglutinative languages 
Aduriz 1.+, Agirre E., Aldezabal I., Alegria I., Arregi X., Arriohl J. M., Artola X., Gojenola K., 
Marilxalar A., Sarasola K., Urkia M.+ 
l)ept, of Colllptiier 1Aulgtlages and Systems, University of lhe Basqtlo Cotlnlry, 64.9 P. K., 
E-20080 1)onostia, Basque Counh'y 
tUZEI, Aldapeta 20, E-20009 1)onostia, Basque Country 
+Universidad de Barcelona, Grin Vfii de Isis Cortes CalaiallaS, 585, E-08007 Flarcelona 
j ipgogak @ si.elm, es. 
Abst rac l  
Agglutinative languages presenl rich 
morphology and for sonic applications 
they lleed deep analysis at word level. 
Tile work here presenled proposes a 
model for designing a full nlorpho- 
logical analyzer. 
The model integrates lhe two-level 
fornlalisnl alld a ullificalion-I)asod 
fornialisni. In contrast to other works, 
we propose to separate the treatment of 
sequential and non-sequetTtial mou)ho- 
lactic constraints. Sequential constraints 
are applied in lhe seglllenlalion phase, 
and non-seqtlontial OlleS ill the filial 
feature-combination phase. Early appli- 
cation of sequential nlorpholactic 
coilsli'aiills during tile segnloillaiioi/ 
process nlakes feasible :,ill officienl 
iinplenleilialion of tile full morpho- 
logical analyzer. 
The result of lhis research has been tile 
design and imi)len~entation of a full 
nlorphosynlactic analysis procedure for 
each word in unrestricted Basque texts. 
I n t roduct ion  
Morphological analysis of woMs is a basic 
tool for automatic language processing, and 
indispensable when dealing willl highly 
agglutinative languages like Basque (Aduriz el 
al., 98b). In lhis conlext, some applications, 
like spelling corfeclion, do ilOI need illOl'e lhan 
the seglllOlltation of each word inlo its 
different COlllponenl nlorphellles alollg with 
their morphological information, ltowever, 
there are oiher applications such as lemnializa- 
tion, lagging, phrase recognition, and 
delernlinaiion of clause boundaries (Aduriz el 
al., 95), which need an additional global 
morphological i)arsing j of the whole word. 
Such a complete nlorphological analyzer has 
lo consider three main aspects (l~,ilchie et al, 
92; Sproal, 92): 
1 Morl)hographenfics (also called morpho- 
phonology). This ternl covers orthographic 
variations that occur when linking 
I l lOfphellleS. 
2) morpholactics. Specil'ication of which 
nlorphenles can or cannot combine with 
each other lo form wflid words. 
3) Feature-combination. Specification of how 
these lnorphemes can be grouped and how 
their nlorphosyntactic features can be 
comlfined. 
The system here presented adopts, oil the one 
hand, tile lwo-level fornlalisnl to deal with 
morphogralfilemics and sequential morl)ho- 
lactics (Alegria el al., 96) and, on the other 
hand, a unification-based woM-grammar 2 to 
combine the grammatical information defined 
in nlorphemes and to  tackle complex 
nlorphotactics. This design allowed us to 
develop a full coverage analyzer that processes 
efl'iciently unrestricted texts in Basque. 
The remainder of tills paper is organized sis 
follows. After a brief' description of Basque 
nlorphology, section 2 describes tile 
architecture for morphological processing, 
where the morphosynlactic omponent is 
included. Section 3 specifies tile plaenomena 
covered by the analyzer, explains its desigi~ 
criteria, alld presents implementation and 
ewthialion details. Section d compares file 
I This has also been called mo*7)hOSh,ntactic 
parsitlg. When we use lhc \[(fill #11017~\]lOSyltl~/X WC 
will always refer to il~c lficrarchical structure at 
woM level, conlbining morphology and synlax. 
2 '\]'\]lt3 \[IDl'll\] WOl'd-gF(lllllllUl" should not be confused 
with the synlaclic lilcory presented in (Hudson, 84). 
system with previous works. Finally, the paper 
ends with some concluding renmrks. 
1 Brief description of Basque 
morphology 
These are the most important features of 
Basque morphology (Alegria et al, 96): 
? As prepositional functions are realized by 
case suffixes inside word-fornls, Basque 
presents a relatively high power to generate 
inflected word-forms. For instance, froth a 
single noun a minimum of 135 inflected 
forms can be generated. Therefore, the 
number of simple word-forms covered by 
the current 70,000 dictionary entries woukl 
not be less than 10 million. 
? 77 of the inflected forms are simple 
combinations of number, determination, 
and case marks, not capable of further 
inflection, but the other 58 word-forms 
ending in one of the two possible genitives 
(possessive and locative) can be further 
inflected with the 135 morphemes. This 
kind of recursive construction reveals a 
noun ellipsis inside a noun phrase and 
could be theoretically exteuded ad 
infinitum; however, in practice it is not 
usual to fiud more than two levels of this 
kind of recursion in a word-form. Taking 
into account a single level of noun ellipsis, 
the number of word-forum coukl be 
estimated over half a billion. 
? Verbs offer a lot of grammatical 
information. A verb tbrln conveys informa- 
tion about the subject, the two objects, as 
well as the tense and aspect. For example: 
diotsut (Eng.: 1 am telling you something). 
o Word-formation is very productive in 
Basque. It is very usual to create new 
compounds as well as derivatives. 
As a result of this wealth of infornmtion 
contained within word-forms, complex struc- 
tures have to be built to represent complete 
morphological information at word level. 
2 An architecture for the full 
morphological ana lyzer  
The framework we propose for the 
morphological treatment is shown in Figure 1. 
The morphological nalyzer is the fiont-end to 
all present applications for the processing of 
Basque texts. It is composed of two modules: 
the segmentation module and the 
morphosyntactic analyzer. 
conformant .................. ~ U~atabas N TEZ-conf~ 
\[Segmentation module 
____~| HorphograDhemics 
Morphotactics I 
TEI-FS .............. ~ ~ ~ ~  ~ - p ~  
conformant Cegmented TexN 
Morphosyntactic 
analyzer 
Feature- combination 
Morphotactics II 
TEI-FS \] .............. ~ actically 
Lermnatization, linguistic Analysis tagging tools 
Figure 1. Architecture 1"o1" morphological processing. 
The segmentation ,nodule was previously 
implemented in (Alegria et al, 96). This 
system applies two-level morphology 
(Koskenniemi, 83) for the morphological 
description and obtains, for each word, its 
possible segmentations (one or many) into 
component morphemes. The two-level system 
has the following components: 
? A set of 24 morphograf~hemic rules, 
compiled into transducers (Karttunen, 94). 
? A lexicon made up of around 70,000 items, 
grouped into 120 sublexicons and stored in 
a general lexical database (Aduriz et al, 
98a). 
This module has full coverage of free-running 
texts in Basque, giving an average number of 
2.63 different analyses per word. The result is 
the set of possible morphological segmenta- 
tions of a word, where each morpheme is 
associated with its corresponding features in 
the lexicon: part of speech (POS), 
subcategory, declension case, number, 
definiteness, as well as syntactic function and 
some semantic features. Therefore, the output 
of the segmeutation phase is very rich, as 
shown in Figure 2 with the word amarengan 
(Eng.: on the mother). 
grammar 
mother) 
POS noun) 
subc~t common 
:count: +) 
(an imate  +) 
(nleasurable "-) 
aren 
(of life) 
(POS decl-suffix) 
(definite +) 
(number sing) 
(case genitive) 
(synt-f @nouncomp) 
J gan \] 
(o.1 / 
(POS decl-suf fix) I 
(case inossivo) \] 
(synt-f @adverbial)I 
=> 
amarengan 
(o. the mother) 
POS noun) 
subcat common) 
number sing) 
definite +) 
case inessive) 
count +) 
animate +) 
measurable -) 
synt-f @adverbial) 
iq:e, ure 2. Morphosynlactic analysis eof (unureugun (l{ng.: (m 
The architecture is a modular envhoument that 
allows different ypes of output depending on 
the desired level of analysis. The foundation of 
the architecture lies in the fact lhat TEI- 
confommnt SGML has been adopted for the 
comnmnication allloIlg modules (Ide and 
VCFOIIiS, 95). l~'eature shucluleS coded 
accoMing TIU are used to represent linguistic 
information, illcluding tile input mM outl)ut of 
the morplaological analyzer. This reprcscnta- 
tion rambles the use of SGML-aware parsers 
and tools, and Call he easily filtered into 
different formats (Artola et ill., 00). 
3 Word level morl)hosyntactic analysis 
This section Hrst presents the l~henomena lhat 
must be covered by the morphosyntactic 
analyzer, then explains ils design criteria, and 
finally shows implementation and ewfluation 
details. 
3.1 Phenomena covered by the analyzer 
There are several features that emphasized the 
need of morphosyntactic almlysis in order to 
build up word level information: 
I) Multiplicity of values for the same feature 
in successive morphemes. In the analysis 
of Figure 2 there are two different values 
for the POS (noun and declension suffix), 
two for the case (genitive and inessive), 
and two for the syntactic function 
(@nouncomp and @adverbial). Multiple 
values at moq~hemc-level will have to be 
merged to obtain the word level infer 
mation. 
2) Words with phrase structure. Although the 
segmentation is done for isolated words, 
independently of context, in several cases 
3 l?calurc wtlues starling with the "@" character 
correspond to syntactic functions, like @noullcomp 
(norm complement) or @adverbial. 
the mother) 
tile resulting structure is oquiwflent o the 
aualysis of a phrase, as can be seen i, 
Figure 2. 111 this case, although there are 
two different cases (genitive and inessive), 
lhe case of the full word-form is simply 
inessive. 
3) Noun ellipsis inside word-lbrms. A noun 
ellipsis can occur withi, the word 
(oceasi(mally more than once). This 
information must be made explicit in the 
resulting analysis. For example, Figure 3 
shows the analysis of a single word-forln 
like diotsudumtrel&z (Eng.: with what I am 
lelling you). The first line shows its 
segmentation into four morphemes 
(die tsut+en+ 0 +arekin). The feature 
compl ill tile final analysis conveys the 
information for the verb (l um lelliHg you), 
that carries information about pc'rson, 
number and case o1' subject, object and 
indirect object. The feature comp2 
represents an elided noun and its 
declension stfffix (with). 
4) l)erivation and composition are productive 
in Basque. There arc more than 80 deri- 
w/tion morphemes (especially suffixes) 
intensively used in word-fornlatioll. 
3.2 Design of the word-grammar 
The need to impose hierarchical structure upon 
sequences of morphemes and to build complex 
constructions from them forced us to choose a 
unil'ication mechanism. This task is currently 
unsolwlble using finite-state techniques, clue to 
the growth in size of the resulting network 
(Beesley, 98). We have developed a unifica- 
tion based word-grammar, where each rule 
combines information flom different 
mot+lJlemes giving as a result a feature 
structure for each interpretation of a word- 
fol'nl, treating the previously mentioned cases. 
3 
diotsut 
I am tellh,g you) 
POS verb) 
(tense present) 
(pers-ergative is)\[ 
(pets-dative 2s) 
(pers-absol 3s) 
en 
(what) 
(POS relation) 
(subcat subord) 
(relator relative 
(synt-f @rel-clause 
0 
() 
(POS ellipsis) 
arekin 
(wire) 
(POS declension-suffix)) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
=> diotsudanarekin (wi~ what lamtel l ingyou) 
(POS verb-noun_ellipsis) 
(case sociative) 
(number sing) 
(definite +) 
(synt-f @adverbial) 
(compl (POS verb) 
(subcat subord) 
(relator relative) 
(synt-f @tel-clause) 
(tense present) 
(pers-ergative is) 
(pets-dative 2s) 
(pers-absol 3s)) 
(comp2 (POS noun) 
(subcat common) 
(number sing) 
(definite+) 
(synt-f @adverbial)) 
Figure 3. Morphosyntactic analysis of diotxudanarekin (Eng.: with what I am tellittg you) 
As a consequence of the rich naorphology of 
Basque we decided to control morphotactic 
phenomena, as much as possible, in the 
morphological segmentation phase. Alterna- 
tively, a model with minimal morphotactic 
treatment (Ritchie et al, 92) would produce 
too many possible analyses after segmentation, 
which should be reiected in a second phase. 
Therefore, we propose to separate sequential 
morphotactics (i.e., which sequences of 
morphemes can or cannot combine with each 
other to form valid words), which will be 
recognized by the two-level system by means 
of continuation classes, and non-sequential 
morphotactics like long-distance dependencies 
that will be controlled by the word-gmnunar. 
The general linguistic principles used to define 
unification equations in the word-grannnar 
rules are the following: 
1) Information risen from the lemma. The 
POS and semantic features are risen flom 
the lemnm. This principle is applied to 
common nouns, adjectives and adverbs. 
The lemma also gives the mnnber in 
proper nouns, pronouns and determiners 
(see Figure 2). 
2) lnfornmtion risen from case suffixes. 
Simple case suffixes provide information 
on declension case, number and syntactic 
function. For example, tile singular 
genitive case is given by the suffix -tell in 
ama+ren (Eng.: of the mother). For 
compound case suffixes the number and 
determination are taken from the first 
suffix and the case from the second one. 
First, both suffixes are joined and after 
that they are attached to the lemma. 
3) Noun ellipsis. When an ellipsis occurs, the 
POS of the whole word-form is expressed 
by a compound, which indicates both the 
presence of the ellipsis (always a noun) 
and the main POS of the word. 
For instance, the resulting POS is 
verb-noun_e l l ips is  when a noun- 
ellipsis occurs after a verb. All the 
information corresponding to both units, 
the explicit lemma and the elided one, is 
stored (see Figure 3). 
4) Subordination morl~hemes. When a 
subordination morpheme is attached to a 
verb, the verb POS and its featm'es are 
risen as well as the subordhmte relation 
and the syntactic fnnction conveyed by the 
naorpheme. 
5) Degree morphemes attached to adjectives, 
past participles and adverbs. The POS and 
diotsudan 
(diotsut + en) 
(POS verb) 
(tense present) 
(relator relative) 
/ \ / 
diotsut 
(POS verb) 
(tense present 
diotsudanarekin 
(diotsut + en -I 0 + arekin) 
(POS verb-noun_ell ipsis) 
(case sociative) 
arekin 
(0 + arekin) 
(POS noun ellipsis) 
(case sociative) 
en 
(pos 
? . . 
o 
(POS e l l ips i s  re la t ion)  
arekin 
(case sociative) 
Figure 4. Parse tree for diotmuhmarekitl (Eng.: with what I am lellittg yott) 
main features arc taken from the lemma 
and the features corresponding to the 
degrees of comparison (comparative, 
supcrhttive) aft taken from the degree 
morphemes. 
6) l)efiwttion. 1)miwttion suffixes select tile 
POS of the base-form to create the deriw> 
tive anti in most cases to change its POS. 
For instance, the suffix -garri (Eng.: -able) 
is applied to verbs and the derived word is 
an adjective. When the derived form is 
obtained by means o1' a prefix, it does not 
change the POS of the base-form. In both 
cases the morphosyntactic rules add a new 
feature representing the structure of tile 
word as a derivative (root and affixes). 
7) Composition. At the moment, we only 
treat the most freqttent kind of 
composition (noun-noun). Since Basque is 
syntactically characterized as a right-head 
hmguage, the main information of the 
compound is taken from the second 
element. 
8) Order of application of the mofphosyn- 
tactic phenomena. When several morpho- 
syntactic phenomena are applied to the 
same leml l la ,  so as to eliminate 
nonsensical readings, the natural order to 
consider them in Basque is the following: 
lemmas, derbation prefixes, deriwltion 
suffixes, composition and inflection (see 
Figure 4). 
9) Morl)hotactic constraints. Elimination of 
illegal sequences of morphemes, such as 
those due to long-distance dependencies, 
which are difficult to restrict by means of 
conti.uation classes. 
The first and second principles are defined lo 
combine information of previously recognized 
mOrl~hemcs, but all the other principles arc 
related to both feature-combination a d non- 
sequential moq~hotactics. 
3.3 Implementation 
We have chosen the PATR formalism 
(Shiebcr, 86) for the definition of the moqflm- 
syntactic rules. There were two main reasons 
for this choice: 
? The formalism is based o.  unification. 
Unification is adequate for the treatment of 
complex phenomena (e.g., agreement of 
conslituents in case, tmmber and definite- 
hess) and complex linguistic structures. 
? Simplicity. The grammar is not linked to a 
linguistic theory, e.g. GPSG in (Ritchie et 
al., 92)? The fact that PATR is simpler than 
more sophisticated formalisms will allow 
that in @e future the grammar could be 
adapted to any of them. 
25 rules have been defined, distributed in the 
following way: 
? 11 rules for the merging of declension 
morphemes and their combination with the 
main categories, 
? 9 rules for the description of verbal 
subordination morphenles, 
? 2 general fulcs for derivation, 
? 1 rule for each of the following 
phenomeml: ellipsis, degree of COlnpavison 
of adjectives (comparative and SUl)erlative) 
and noun composition. 
3.4 Evaluat ion 
As a cousequence of the size of the lexical 
database and tile extensive treatment of 
nlorphosyntax, the resulting analyzer offers 
full coverage when applied to real texts, 
capable of treating unknown words and non- 
standard forms (dialectal wtriants and typical 
errors). 
We performed four experilnents to ewtluate 
tile efficiency of the implemented analyzer 
(see Table 1). A 10,832-word text was 
randomly selected from newspapers. We 
measured tile number of words per second 
analyzed by the morphosyntactic analyzer and 
also by the whole morphological analyzer 
(results taken on a Sun Ultra 10). Ill the first 
experiment all tile word-t'ornls were analyzed 
one-by-one; while ill tile other three experi- 
ments words with more than one occurrence 
were analyzed only once. Ill the last two 
experimeuts a memory with the analysis of tile 
most frequent word-forms (MFW) in Basque 
was used, so that only word-forms not found 
in the MFW were analyzed. 
Test 
description 
All 
word forms 
Diffcrent 
word forms 
MFW 
10,000 words 
(I 5 Mb) 
MFW 
50,000 words 
(75 mb) 
# words/scc 
analyzed Morphosynt. 
words analyzer 
10,832 
3,692 
1,483 
533 
15,13 
44 40 
111 95 
308 270 
words/see 
Full 
morphological 
analyzer 
13,5 
Table 1. Evaluation results. 
Even when our language is agglutinative, and 
its morphological phenomena need more 
computational resources to build complex and 
deep structures, the results prove tile feasibility 
of implementiug efficiently a fifll 
morphological analyzer, although efficiency 
was not the main concern of our 
implementation. The system is currently being 
applied to unrestricted texts in real-time 
applications. 
4 Related work 
(Koskeniemmi, 83) defined the formalism 
named two-level morphology. Its main 
contributiou was the treatment of 
morl)hographemics and morphotactics. The 
formalisnl has been stmcessfully applied to a 
wide wlriety ot' languages. 
(Karttunen, 94) speeds the two-level model 
compiling two-level rules into lexical 
transducers, also increasing the expressiveness 
of the model 
The morphological analyzer created by 
(Ritchie et al, 92) does not adopt finite state 
mechanisms to control morphotactic 
phenomena. Their two-level implementation 
incorporates a straightforward morphotactics, 
reducing tile number of sublexicons to the 
indispensable (prefixes, lemmas and suffixes). 
This approximation would be highly 
inefficient for agglutinative languages, as it 
would create lnany nonsensical interpretatiolas 
that should be rejected by tile unification 
phase. They use the word-grammar for both 
morphotactics and feature-conlbination. 
ill a similar way, (Trost, 90) make a proposal 
to combine two-level morphology and non- 
sequential morphotactics. 
The PC-Kimmo-V2 system (Antworth, 94) 
presents an architecture similar to ours applied 
to English, using a finite-state segmentation 
phase before applying a unification-based 
grammar. 
(Pr6szdky and Kis, 99) describe a morpho- 
syntactic analyzer for Hungarian, an agglu- 
tinative language. The system clots not use the 
two-level model for segmentation, precom- 
piling suffix-sequences to improve efficiency. 
They claim the need of a word-grammar, 
giving a first outline of its design, although 
they do not describe it in detail. 
(Oflazer, 99) presents a different approach for 
the treatment of Turkish, an agglutinative 
language, applying directly a dependency 
parsing scheme to morpheme groups, that is, 
merging morphosyntax and syntax. Although 
we are currently using a similar model to 
Basque, there are several applications that are 
word-based and need full morphological 
parsing of each word-t'orm, like the word- 
oriented Constraint Graminar formalism for 
disambiguation (Karlsson et aI., 95). 
Conc lus ion  
We propose a model for fllll morphological 
analysis iutegrating two different components. 
On tile one hand, the two-level formalism 
deals with morphographenfics and sequential 
morphotactics and, on the other hand, a 
unil\]cation-based word-grammar combines lhe 
granlll-iatical in\['ornlatioli defined in illoi'- 
phelllOS alld also handles COlllplcx illori)ho- 
tactics. 
Early application of sCqtloniial I/lOrl)hotactic 
conslraints dtu-ing the segmentation process 
avoids all excessive laUlllber of nleaningless 
segmentation possibilities before the 
coulputationally lllOlO expensive unification 
process. Unification permits lhe resohition of a 
wide variety of morl)hological phenonlena, 
like ellipsis, thal force the definition of: 
complex and deep structures Io roprosenl the 
output of the analyzer. 
This design allowed us io develop a full 
coverage allalyzor that processes efficiently 
unrestricted loxis in Basque, a strongly 
agglulinafive langttage. 
The anaiyzcl" has bccll integrated ill a gCllOl'al 
franlework for the l)lOCessing of l~asquc, with 
all the linguistic inodulos communicating by 
l l leallS O\[: foattll'C stltlClll l 'eS ill accord  {o the 
principles of ihe Text Encoding Initiative. 
Acknowledgements  
This research was partially supported by the 
Basque Government, the University of the 
\]71aS(lUe Cotlntry {/lid the CICYq' (Cotllisidn 
lntcrministorial de Ciencia y Tecnologfil). 
References 
Aduriz 1., Aldczabal I., Ansa ()., Arlola X., I)faz de 
Ilarraza A., Insau.~li .I.M. (1998a) EI)BL: a 
Mttlli-l~ttrposed Lexica/ Sttl)l)c;rl .lot the 
Treatment of Ba,s'que. Proceedings of the l;irst 
Inlernational Confcncncc on l Auiguagc Resources 
and Ewduation, Granada. 
Aduriz I., Agirre E., Aldczabal 1., Alegria 1., Ansa 
O., Arrcgi X., Arriola J.M., ArtolaX., I)faz de 
lhu'raza A., Ezciza N., Gqicnola K., Maritxahu" 
A., Maritxalar M., Oronoz M., Sarasola K., 
Soroa A., Urizar R., Urkia M. (1998b) A 
Framework .for the Automatic Pmce.vsi#~g (if" 
Basqtte. Proceedings o1 the First Ii~ternational 
Con \[elel i te on Lall.gtlagc Resources turf 
Evaluation, Granada. 
Aduriz I., Alcgria I., Arriohl J.M., Artola X., l)faz 
do Ilarraza A., Ecciza N., Gojcnola K., 
Maritxalar M. (1995) Di\[.ferelt! Issues in the 
Design qf a lemmatizer/Tagger fo Ba,s'qtte. From 
Tcxls to Tags: Issues in Mullilingual Language 
Analysis. ACL SIGI)AT Workshop, l)ublin. 
Alcgria 1., Art(Ha X., Sarasoht K., Urkia M. (1996) 
Automatic moqdzological analysis of Basque. 
IAtcrary and IAnguistic Computing, 11 (4): 193- 
203. Oxford University. 
Aniworlh E. I.. (1994) Morphological Par, ffng with 
a lhl(fication-ba,s'ed Word Grcmmutr. Norlh 
Te, xas Natural l~anguage Processing Workshop, 
Texas. 
Arlola X., Dfaz de \]larraza A., Ezciza N., Oo.icnohi 
K., Marilxahu' A., Soma A. (2000) A proposal 
for the integration of NLP tools using SGML- 
lagged documeHls. Proceedings of ll~e Second 
Cotfforence or1 Language Resources and 
Evaltmfion (IA~,EC 2000). Athens, Greece 2000. 
Bcesl%, K. (1998)AraDic Morphological Analysis 
(m the lnlernet, l'rocccdings of the International 
Conference on Mulii-IAngual Computing (Arabic 
& lhlglish), Cambridge. 
Hudson R. (1990) English Word Grammmar. 
Oxford: Basil Blackwcll. 
ldc N., Vcronis J. K. (1995) Text-Ettcoding hHtia- 
tire, Bac:kgmtmd and Context. Kluwcr Academic 
Publishers. 
Karlsson F., Voulilaincn A., Heikkiht J., Anltila A. 
(1995) Constrai, t Gnmmmr: A lxm,?tmge- 
i#ldcpcndent System Jor Pm:ffng Um'estricled 
Text, Mouton do Gruyicr ed.. 
Kartmnen 1,. (1994) Con,s'tructin~ l,e.vical 
7)'ansdttcers. Proc. of CO13NG'94, 406-411. 
Koskcnniemi, K, (1983) Two-level Mc;qdlo\[ogy: A 
ge,eral Comptttational Model ./br Word-Form 
Recognition and Pmduclioth University of 
Ilclsinki, l)clmrtmcnt of General IAnguisiics. 
l~ublications " 11. 
()flazcr K (1999) l)epetMe/t O' Parsing, with a, 
E.rtended I:inite State Approac\]t. ACL'99, 
Maryland. 
Pr6sz6ky G., Kis B (1999)A Unificati(m-hascd 
Apl~roach to Moqdto-syntactic I'arsitl<~ of 
Agghttinative and Other (Highly) lnjlectional 
Languages. ACtd99, Ma,yhmd. 
Ritchie G., Pulhnan S. G., FJlack A. W., Russcl G. 
J. (1992) Comlmtational Moudu)logy: Practical 
Mechanism,s'.fi)r the l#lglish l,exico,. ACL-MIT 
Series on Natural Language Processing, MIT 
Press. 
Shicbcr S. M. (1986) At/ lntroductiotz to 
Unification-Based Approaches to Grammar. 
CSLI, Slanford. 
Sproat R. (1992) Morphology anU Computcaion. 
ACL-MIT Press series in Natural Language 
Processing. 
Trost It. (1990) The application of two-level 
morldzo/ogy to rzon-concatenative German 
moqgtology. COIANG'90, Hclsinki. 
7 
Coling 2008: Companion volume ? Posters and Demonstrations, pages 31?34
Manchester, August 2008
Detecting Erroneous Uses of Complex Postpositions in an 
Agglutinative Language 
Arantza D?az de Ilarraza Koldo Gojenola Maite Oronoz  
IXA NLP group. University of the Basque Country 
jipdisaa@si.ehu.es koldo.gojenola@ehu.es maite.oronoz@ehu.es  
 
Abstract 
This work presents the development of a 
system that detects incorrect uses of com-
plex postpositions in Basque, an aggluti-
native language. Error detection in com-
plex postpositions is interesting because: 
1) the context of detection is limited to a 
few words; 2) it implies the interaction of 
multiple levels of linguistic processing 
(morphology, syntax and semantics). So, 
the system must deal with problems rang-
ing from tokenization and ambiguity to 
syntactic agreement and examination of 
local contexts. The evaluation was per-
formed in order to test both incorrect uses 
of postpositions and also false alarms.1 
1 Structure of complex postpositions  
Basque postpositions play a role similar to 
English prepositions, with the difference that 
they appear at the end of noun phrases or 
postpositional phrases. They are defined as 
?forms that represent grammatical relations 
among phrases appearing in a sentence? 
(Euskaltzaindia, 1994). There are two main types 
of postpositions in Basque: (1) a suffix appended 
to a lemma and, (2) a suffix followed by a lemma 
(main element) that can also be inflected. 
(1) etxe-tik 
house-(from the)  
from the house 
 (2) etxe-aren gain-etik  
house-(of the)  top-(from the) 
from the top of the house 
The last type of elements has been termed as 
complex postposition. We will use this term to 
name the whole sequence of two words involved, 
and not just to refer to the second element. Com-
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 
Unported license (http://creativecommons.org/ 
licenses/by-nc-sa/3.0/). Some rights reserved. 
plex postpositions can be described as: 
(3) lemma1 + (suffix1 + lemma2 + suffix2) 
In these constructions, the second lemma is fixed 
for each postposition, while the first lemma al-
lows for much more variation, ranging from 
every noun to some specific semantic classes. 
The above description (3) is intended to stress  
(with parentheses) the fact that the combination 
of both suffixes with the second lemma acts as a 
complex case-suffix that is ?appended? to the 
first lemma. Both suffixes present different com-
binations of number and case, which can agree in 
several ways, depending on the lemma, case or 
contextual factors. Table 1 shows the different 
variants of two complex postpositions, derived 
from the lemmas bitarte and aurre. For example, 
the lemma bitarte is polysemous (?means, by 
means of, instrument, while (temporal), be-
tween?). Multiple factors affect the correctness 
of a postposition, including morphological and 
syntactic constraints. We also discovered a num-
ber of relevant contextual factors, which are not 
explicitly accounted for in standard grammars. 
2 The corpus 
The detection of erroneous uses of complex 
postpositions needs first a corpus that can serve 
for both development and evaluation of the sys-
tem. To obtain such a corpus is a labor-intensive 
task, to which it must be added the examination 
and markup of incorrect examples. The use of a 
big ?correct? corpus will allow us to test our sys-
tem negatively, thoroughly testing the system?s 
behavior in respect to false alarms. We used an 
automatic system for detecting complex postpo-
sitions in order to get development and test data. 
There are two text types: Newspaper corpora 
(henceforth NC, 8,207,919 word-forms) that is 
subject to an edition process and style guides, 
and Learner corpora (LC, 994,658 word-forms), 
which come from texts written by learners of 
Basque and University students. These texts are 
more ?susceptible? of containing errors. 
31
We decided to study those types of postpositions 
that appear most frequently in texts, those con-
taining the following lemmas as their second ele-
ment: arte, aurre, bitarte, buruz, and zehar2. We 
selected these postpositions given that they are 
well documented in grammar books, with de-
tailed descriptions of their correct and incorrect 
uses (e.g. see Table 1 for bitarte), and also that 
they are very frequent in both types of texts. 
Each kind of syntactic error occurs with very 
low frequency and, therefore, big corpora are 
needed for evaluation and testing3. Even if such 
corpora are available, to obtain naturally occur-
ring test data, hundreds of texts should be manu-
ally examined and marked. As a result, we de-
cided to only manually mark errors in Learners? 
Corpora (LC), because NC, an order of magni-
tude bigger than LC, is presumed to contain less 
errors. This implies that we will be able to meas-
ure precision4 in both corpora, while recall5 will 
only be evaluated in LC. Table 2 shows the 
number of sentences used for development (60% 
of each corpus) and test (40%). We treated LC 
and NC separately, as they presumably differ in 
the number of errors. 
3 Linguistic Processing Tools 
The corpus was automatically analyzed by means 
of several linguistic processors: a morphosyntac-
tic analyzer (Aduriz et al, 2000), EUSTAGGER, 
the lemmatizer/tagger for Basque, and the Con-
straint Grammar parser (CG, Tapanainen, 1996) 
for morphological disambiguation.  
                                                 
2
 As each lemma has several meanings depending on each 
variant, we will not give their translation equivalence. 
3
 We made an estimate of more than 1% of elements in 
general corpora being complex postpositions. 
4
 Number of errors correctly identified by the system / total 
number of elements identified as erroneous. 
5
 Number of errors correctly identified by the system / total 
number of real errors. 
Added to these, we also used other resources: 
? Grammar books which describe errors in 
postpositions (Zubiri & Zubiri, 1995). 
? Place names. Two of the selected postposi-
tions (arte, aurre) are used in expressions 
that denote temporal and spatial coordinates, 
but their variants impose different restric-
tions and agreement (case, number). In order 
to recognize common nouns that refer to a 
spatial context, we made use of a new lexical 
resource: electronic versions of dictionaries 
(Sarasola, 2007; Elhuyar, 2000). 168 and 242 
words were automatically acquired from 
each dictionary. To this, we added proper 
names corresponding to places. 
? Animate/inanimate distinction. Regarding 
postpositions formed with aurre, Zubiri et al 
(1995) point out that ?typically the previous 
word takes the genitive case, although it can 
also be used without a case mark with inani-
mate nouns?. For this reason, we used a dic-
tionary enriched with semantic features, such 
as animate/inanimate, time or instrument. 
We selected 1,642 animate words. We also 
added person names and pronouns. 
4 Rule design 
The system will assign an error-tag to those 
word-forms that show the presence of an incor-
rect use of a postposition. We use the CG formal-
ism (Tapanainen, 1996) for this task. CG allows 
 
NC LC  
Dev Test Dev Test 
arte 7769 5179 1209 806 
aurre 8129 5420 1157 771 
bitarte 3846 2564 772 514 
buruz 5435 3623 560 373 
zehar 1500 1000 186 126 
Total 26679 17786 3884 2590 
Errors   60 29 
Table 2. Number of sentences in development 
and test sets, including the errors in LC.  
lemma2 suffix1 suffix 2 Examples 
-en (genitive) -z (instrumental) etxearen bitartez  (by means of the house) 
-ra (alative) -n (inessive, sg.) etxera bitartean  (while going to the house) 
-a (absolutive, sg.) -n (inessive, sg.) ordubata bitartean (around one o?clock) 
-? (no case) -n (inessive, sg.) meza bitartean (while attending mass) 
-en (genitive) -n (inessive, sg.) mendeen bitartean (between those centuries) 
-? (no case) -? /ko (no case/genitive) Lau hektarea bitarte  (in a range of four hectares) 
-ak (absolutive, pl.) 
-? /ko (no case/genitive) seiak bitarte (around six o?clock) 
bitarte 
(noun) 
-ra (alative) 
-? /ko (no case/genitive) etxera bitarte (in the way home) 
-? /-en (no 
case/genitive) 
-n/-ra/-tik/-ko (inessive/ ala-
tive/ ablative/ genitive) 
eliza aurrean (in front of the church) aurre 
(noun) 
-tik (ablative) -ra (alative) hemendik aurrera (from here onwards) 
Table 1. Complex postpositions for bitarte and aurre. 
32
the definition of complex contextual rules in or-
der to detect error patterns by means of mapping 
rules and a notation akin to regular expressions. 
Fig. 1 shows a general overview of the system. 
Syntactic constraints are encoded by means of 
CG rules using morphosyntactic categories (part 
of speech, case, number, ?). Semantic restric-
tions are enforced by lists of words belonging to 
a semantic group. All of the five postpositions 
have clear requirements about the combinations 
of case and number in the surrounding context.  
Overall, the CG grammar contains 30 rules for 
the set of 5 postpositions. We found that 
although the study of authoritative grammatical 
descriptions was exhaustive, the grammarians? 
descriptions of correct and incorrect uses refer 
mainly to morphology and syntax. Nevertheless, 
we discovered empirically that most of the rules 
needed to be extended with several classes of 
semantic restrictions. Among others, distinctions 
were needed for animate nouns, place names, or 
several classes of time expressions, depending on 
each different variant of each postposition.  
5 Evaluation 
The rules were applied both to the (presumably) 
correct newspapers texts (NC) and to the learn-
ers? texts (LC). The actual errors in LC were 
marked in advance but not in NC, which means 
that recall can only be evaluated in LC. Table 3 
shows the main results including all the selected 
five postpositions. The LC corpus contains 60 
and 29 error instances in development and test 
corpus, respectively. If we concentrate on preci-
sion, Table 4 shows the overall precision results 
for the total of errors detected in the test corpora. 
When we consider the whole set of postpositions 
precision is 50.5%, giving 42 false alarms out of 
85 detected elements. We performed an analysis 
of false alarms which showed several causes: 
? Morphological ambiguity (43% of alarms). 
? Semantic ambiguity (28%). We included sets 
of context words to identify the correct 
senses, but it still causes many false alarms. 
? Syntactic ambiguity (22%). The false alarms 
are mostly concerned with coordination. 
? Tokenization errors (7%). 
As most of the false alarms came from postpo-
sitions formed with arte, the most ambiguous 
one, we counted the errors when dealing only 
with the other four postpositions, giving a better 
precision (70.4%, second row in Table 4), al-
though detecting less true errors. If the system 
only deals with three postpositions (third row in 
Table 4), then precision reaches 78.3%. Johan-
nessen et al (2002) note that the acceptable num-
ber of false alarms in a grammar checker should 
not exceed 30%, that is, at least 70% of all 
alarms had to report true errors. Our experiments 
show that our system performs within that limit, 
albeit restricting its application to the most ?prof-
itable? postpositions. Although the number of 
rules varies widely (from 15 rules for arte to 2 
rules in the case of zehar) their effectiveness 
greatly depends on the complexity and ambiguity 
of the contextual factors. For that reason, arte 
presents the worst precision results even when it 
contains by far the biggest set of detection rules. 
On the other hand, zehar, with 2 rules, presents 
the best precision, due to its limited ambiguity. 
So, to deal with the full set postpositions (several 
works estimate more than 150), it will be more 
profitable to make a preliminary study on ambi-
guities and variants for each postposition. 
6 Related work 
Kukich (1992) surveys the state of the art in syn-
tactic error detection. She estimates that a pro-
portion of all the errors varying between 25% 
and over 50% are valid words. Atwell and Elliott 
Postpositions Precision 
arte, aurre, bitarte, buruz, zehar 50.5% (43/85)  
aurre, bitarte, buruz, zehar 70.4% (31/44) 
bitarte, buruz, zehar 78.3% (29/37) 
Table 4. Precision for the test sets (NC + LC). 
 NC LC 
 Dev Test Dev Test 
Sentences 26679 17786 3884 2590 
Errors - - 60 29 
Undetected - - 10 10 
Detected 30 24 50 19 
False alarms 45 33 2 9 
Recall - - 83% 65% 
Precision 40% 42% 96% 67% 
Table 3. Evaluation results. 
Sentences 
Morphological  
analysis 
Constraint Grammar 
parser 
No Error / Error Type 
Figure 1. General architecture. 
Error detection  
grammar 
Place 
nouns 
Animate 
nouns 
?
33
(1987) concluded that 55% of them are local syn-
tactic errors (detectable by an examination of the 
local syntactic context), 18% are due to global 
syntactic errors (which need a full parse of the 
sentence), and 27% are semantic errors. Regard-
ing their treatment, there have been proposals 
ranging from error patterns (Kukich 1992; Gold-
ing and Schabes 1996), in the form of hand-
coded rules or automatically learned ones, to sys-
tems that integrate syntactic analysis. 
(Chodorow et al, 2007) present a system for 
detecting errors in English prepositions using 
machine learning. Although both English prepo-
sitions and Basque postpositions have in some 
part relation with semantic features, Basque 
postpositions are, in our opinion, qualitatively 
more complex, as they are distributed across two 
words, and they also show different kinds of syn-
tactic agreement in case and number, together 
with a high number of variants. This is the main 
reason why we chose a knowledge-based method. 
7 Conclusions 
We have presented a system for the detection of 
errors in complex postpositions in Basque. Al-
though at first glance it could seem that postposi-
tions imply the examination of two consecutive 
words, a posterior analysis showed that they of-
fer rich and varied contexts of application, re-
quiring the inspection of several context words, 
albeit not enough to need a full syntactic or se-
mantic analysis of sentences. The system uses a 
varied set of linguistic resources, ranging from 
morphological analysis to specialized lexical re-
sources. As the detection of these errors implies a 
detailed and expert linguistic knowledge, the sys-
tem uses a purely knowledge-based approach. 
A considerable effort has been invested in the 
compilation of a corpus that provides a testbed 
for the system, which should be representative 
enough as to predict the behaviour of the system 
in an environment of a grammar checker. For 
that reason, we have tried to put a real emphasis 
on avoiding false alarms, that is, treating also lots 
of correct instances. The results show that good 
precision can be obtained. Regarding recall, our 
experiments do not allow to make an estimation, 
as the NC test corpora is too big to perform a 
detailed examination. However, the LC corpora 
can give us an upper bound of 65% (see Table 3). 
This work also shows that the use of purely 
morphosyntactic information is not enough for 
the detection of errors in postpositions. For that 
reason we were forced to also include several 
types of semantic features into the system. On 
the other hand, the process of automatic error 
detection has also helped us to explore new sets 
of semantic distinctions. So, the process of error 
detection has helped us to organize concepts into 
sets of semantically related elements, and can 
serve to make explicit types of knowledge that 
can be used to enrich other linguistic resources. 
We can conclude saying that descriptive lin-
guistics could benefit from error diagnosis and 
detection, as this could help to deeply understand 
the linguistic descriptions of postpositions, which 
are done at the moment mainly by means of 
morphosyntactic information, insufficient to give 
an account of the involved phenomena.  
Acknowledgements 
This research is supported by the University of 
the Basque Country (GIU05/52) and the Basque 
Government (ANHITZ project, IE06-185). 
References 
Aduriz I., Agirre E., Aldezabal I., Alegria I., Arregi 
X., Arriola J., Artola X., Gojenola K., Sarasola 
K.  2000. A Word-grammar based morphological 
analyzer for agglutinative languages. COLING-00. 
Atwell E., Elliott S. (1987) Dealing with Ill-Formed 
English Text. In The Computational Analysis of 
English: a Corpus-Based Approach. Longman. 
Chodorow M., Tetreault J. and Han N. 2007. Detec-
tion of Grammatical Errors Involving Prepositions. 
4th ACL-SIGSEM Workshop on Prepositions. 
D?az de Ilarraza A., Gojenola K., Oronoz M.  2008. 
Detecting Erroneous Uses of Complex Postposi-
tions in an Agglutinative Language. Internal report 
(extended version). (https://ixa.si.ehu.es/Ixa/Argitalpenak) 
Elhuyar. 2000. Modern Basque Dictionary. Elkar.  
Euskaltzaindia. 1994. Basque Grammar: First Steps 
(in Basque). Euskaltzaindia. 
Golding A. and Schabes. Y. (1996) Combining tri-
gram-based and feature-based methods for context-
sensitive spelling correction. ACL 1996. 
Johannessen J.B., Hagen K., and Lane P. 2002. The 
performance of a grammar checker with deviant 
language input. Proceedings of COLING, Taiwan. 
Kukich K. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys. 
Tapanainen P. 1996. The Constraint Grammar parser 
CG-2. Publications of the Univ. of Helsinki, 27. 
Sarasola, Ibon. 2007. Basque Dictionary (in Basque). 
Donostia : Elkar, L.G. ISBN 978-84-9783-258-8. 
Zubiri I. and  Zubiri E. 1995. E. Euskal Gramatika 
Osoa (in Basque). Didaktiker, Bilbo. 
34
Learning Argument/Adjunct Distinction for Basque 
Abstract 
This paper presents experiments performed on 
lexical knowledge acquisition in the form of 
verbal argumental information. The system 
obtains the data from raw corpora after the 
application of a partial parser and statistical 
filters. We used two different statistical filters 
to acquire the argumental information: Mutual 
Information, and Fisher?s Exact test. 
Due to the characteristics of agglutinative 
languages like Basque, the usual classification 
of arguments in terms of their syntactic 
category (such as NP or PP) is not suitable. 
For that reason, the arguments will be 
classified in 48 different kinds of case 
markers, which makes the system fine grained 
if compared to equivalent systems that have 
been developed for other languages. 
This work addresses the problem of 
distinguishing arguments from adjuncts, this 
being one of the most significant sources of 
noise in subcategorization frame acquisition. 
Introduction 
In recent years a considerable effort has been done 
on the acquisition of lexical information. As 
several authors point out, this information is useful 
for a wide range of applications. For example, J. 
Carroll et al (1998) show how adding 
subcategorization information improves the 
performance of a parser. 
With this in mind our aim is to obtain a system 
that automatically discriminates between 
subcategorized elements of verbs (arguments) and 
non-subcategorized ones (adjuncts).  
We have evaluated our system in two ways: 
comparing the results to a gold standard and 
estimating the coverage over sentences in the 
corpus. The purpose was to find out which was the 
impact of each approach on this particular task. 
The two methods of evaluation yield significantly 
different results.  
Basque is the subject of this study. A language 
that, in contrast to languages like English, has 
limited resources in the form of digital corpora, 
computational lexicons, grammars or annotated 
treebanks. Therefore, any effort like the one 
presented here, oriented to create lexical resources, 
has to be driven to do as much automatic work as 
possible, minimizing development costs. 
The paper is divided into 4 sections. The first 
section is devoted to explain the theoretical 
motivations underlying the process. The second 
section is a description of the different stages of 
the system. The third section presents the results 
obtained. The fourth section is a review of 
previous work on automatic subcategorization 
acquisition. Finally, we present the main 
conclusions. 
1 The argument/adjunct distinction 
Talking about Subcategorization Frames (SCF), 
means talking about arguments. Many existing 
systems acquire directly a set of possible SCFs 
without any previous filtering of adjuncts. 
However, adjuncts are a substantial source of noise 
and therefore, in order to avoid this problem, our 
approach addresses the problem of the 
argument/adjunct distinction. 
The argument/adjunct distinction is probably 
one of the most unclear issues in linguistics. The 
distinction has being presented, for example, in the 
generativist tradition, in the following way: 
arguments are those elements participating in the 
event and adjuncts are those elements 
contextualizing or locating the event. 
This definition seems to be quite clear, but 
when we deal with concrete examples it is not the 
Izaskun Aldezabal, Maxux Aranzabe, Koldo 
Gojenola , Kepa Sarasola 
Dept. of Computer Languages and Systems, 
University of the Basque Country, 649 P. K., 
E-20080 Donostia,  
Basque Country 
Aitziber Atutxa 
University of Maryland  
College Park 
Maryland, 20740 
jibatsaa@si.ehu.es 
                     July 2002, pp. 42-50.  Association for Computational Linguistics.
                     ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,
                  Unsupervised Lexical Acquisition: Proceedings of the Workshop of the
case. For example, if we take two verbs, talk and 
play.  
a. Yesterday I talked with Mary. 
b. Yesterday I played soccer with Mary. 
Here Mary is a participant of the event in both 
cases, therefore under the given definition both 
would be arguments. But this is contradictory to 
what traditional views consider in practice. The 
PP, with Mary, is considered an argument of talk 
but not an argument of play. It is true that there are 
differences between both of them because playing 
does not require two participants (though it can 
have them), while talking (under the sense of 
communicating) seems to require two participants. 
Finer argument/adjunct distinction have also 
been proposed differentiating between basic 
arguments, pseudo-arguments and adjuncts. Basic 
arguments are those required by the verb. Pseudo-
arguments are those that even if they are not 
required by the verb, when appearing they extend 
the verbal semantics, for example, adding new 
participants. And finally adjuncts, which would be 
contextualizers of the event. The most radical view 
is to consider the argument/adjunct distinction as a 
continuum where the elements belonging to the 
extremes of this continuum can be easily classified 
as arguments or adjuncts. On the contrary, the 
elements belonging to the central part of the 
continuum can be easily misclassified. For further 
reference see C. Schutze (1995), J.M. Gawron 
(1986), C. Verspoor (1997), J. Grimshaw (1990), 
and N. Chomsky (1995). 
From the different diagnostics proposed in the 
literature some are quite consistent among various 
authors (R. Grishman et al 1994, C. Pollard and I. 
Sag 1987, C. Verspoor 1997). 
1) The Obligatoriness condition. When a verb 
demands obligatorily the appearance of an 
element, this element will be an argument. 
a. John put the book on the table 
b. *John put the book 
2) Frequency. Arguments of a verb occur more 
frequently with that verb than with the other 
verbs. 
a. I came from home (argument). 
b. I heard it from you (adjunct). 
3) Iterability: Several instances of the same 
adjunct can appear together with a verb, while 
several instances of an argument cannot appear 
with a verb. 
a. I saw you in Washington, in the 
Kenedy Center. 
b. *I saw Alice John (being John and 
Alice two persons) 
4) Relative order: Arguments tend to appear closer 
to the verb than adjuncts.  
a. I put the book on the table at three 
b. *I put at three the book on the 
table 
5) Implicational test:  Arguments are semantically 
implied, even when they are optional. 
   a. I came to your house (from x) 
  b. I heard that (from x) 
The third and fourth tests were not very useful 
to us. Iterability test is quite weak because it seems 
to rely more on some other semantic notions such 
as part/whole relation than in the argument/adjunct 
distinction. For example, sentence 3.a would be 
grammatical due to semantic plausibility. The 
Kennedy Center is a part of Washington, therefore 
to see somebody in the Kennedy Center and see 
him in Washington are not semantically 
incompatible, so it is plausible to say it. In the case 
of 3.b John is not a part of  Alice and therefore it is 
not plausible to see Alice John. But for example it 
is plausible to say I saw you the hand. The relative 
order test is difficult to apply on a language like 
Basque which is a free word order language.  
The first and fifth tests are robust enough to be 
useful in practice. But only the two first 
diagnostics can be captured statistically by the 
application of association measures like Mutual 
Information. We did not come out with any  
straightforward way to apply the fifth test 
computationally. 
Before talking about the different measures 
applied, we will present step by step the whole 
process we pursued for achieving the 
argument/adjunct distinction. 
2 The acquisition process  
Our starting point was a raw newspaper corpus 
from of 1,337,445 words, where there were 
instances of 1,412 verbs. From them, we selected 
640 verbs as statistically relevant because they 
appear in more than 10 sentences.  
As we said earlier, our goal was to distinguish 
arguments from adjuncts. When starting from raw 
corpus, like in this case, it is necessary to get 
instances of verbs together with their dependents 
(arguments and adjuncts). We obtained this 
information applying a partial parser (section 2.1) 
to the corpus. Once we had the dependents, 
statistical measures helped us deciding which were 
arguments and which were adjuncts (section 2.2). 
2.1 The parsing phase 
Aiming to obtain the data against which statistical 
filters will be applied, we analyzed the corpus 
using several available linguistic resources: 
? First, we performed morphological analysis of 
the corpus, based on two-level morphology (K. 
Koskenniemi 1983; I. Alegria et al 1996) and 
disambiguation using the Constraint Grammar 
formalism (Karlsson et al 1995, Aduriz et al 
1997).  
? Second, a shallow parser was applied (I. 
Aldezabal et al 2000), which recognizes basic 
syntactic units including noun phrases, 
prepositional phrases and several types of 
subordinate sentences. 
? The third step consisted in linking each verb 
and its dependents. Basque lacks a robust 
parser as in (T. Briscoe & J. Carroll 1997, D. 
Kawahara et al 2001) and, therefore, we used a 
finite state grammar to link the dependents 
(both arguments and adjuncts) with the verb (I. 
I. Aldezabal et al 2001). This grammar was 
developed using the Xerox Finite State Tool (L. 
Karttunen et al 1997). Figure 1 shows the 
result of the parsing phase. In this case, both 
commitative and inessive cases (PPs) are 
adjuncts, while the ergative NP is an argument. 
The linking of dependents to a verb is not 
trivial considering that Basque is a language 
with free order of constituents, and any element 
appearing between two verbs could be, in 
principle, dependent on any of them. Many 
problems must be taken into account, such as 
ambiguity and determination of clause 
boundaries, among others. We evaluated the 
accuracy up to this point, obtaining a precision 
over dependents of 87% and a recall of 66%. 
So the input data to the next phase was 
relatively noisy.  
2.2 The argument selection phase 
In the data resulting from the shallow parsing 
phase we counted up to 65 different cases (types of 
arguments, including postpositions and different 
types of suffixes). These are divided in two main 
groups: 
? 43 correspond to postpositions. Some of them 
can be directly mapped to English prepositions, 
but in many cases several Basque postpositions 
correspond to just one English preposition (see 
Table 1a.). This set alo contains postpositions 
1)? (a) [ EEBBetako lehendakariak] (b) [UEko 15 herrialdeetako merkataritza ministroekin] 
(c) [bazkaldu behar zuen] (d) [negoziazioen bilgunean] ? 
 
2) ? the president of the USA had to eat with the ministers of Commerce of 15 countries of the UE in
the negotiation center ? 
 
(a)  [EEBB-etako lehendakari-a-k]       (b)  [UE-ko    15 herrialde-etako    merkataritza ministro-ekin]  
     [USA-of         president-the-erg.]        [UE-of    15 countries-of          Commerce ministers-with]  
      NP-ergative(president, singular)                PP(with)-commitative(minister, plural)  
 The president of the USA  with the ministers of Commerce of 15 countries of the UE 
 
 
(c) [bazkaldu behar zuen]                  (d)   [negoziazio-en     bilgune-an] 
        [to eat        had]                                  [negotiation-of     center-in]   
          verb(eat)                                        PP(in)-inessive(center, singular) 
 had to eat in the negotiation center 
Figure 1. Example of the output of the shallow parsing phase: 1) Input (in Basque), 2) English translation,. 
Below (c) Verb phrase and (a,b,d) verbal dependents (phrases), and also under the case+head 
that map to categories other than English 
prepositions, such as adverbs (Table 1b). 
Table 1. Correspondence between English 
prepositions and Basque postpositions. 
 English Basque 
a. to dative (suffix) 
alative (suffix) 
final ablative (suffix) 
b. like -en gisa (suffix) 
gisa 
bezala 
legez 
 
? 22 types of sentential complements (For 
instance, English that complementizer 
corresponds to several subordination suffixes:  
-la, -n, -na, -nik). 
This shows to which extent the range of 
arguments is fine grained, in contrast to other 
works where the range is at the categorial level, 
such as NP or PP (M. Brent 1993, C. Manning 
1993, P. Merlo & M. Leybold 2001). 
Due to the complexity carried by having such a 
high number of cases, we decided to gather 
postpositions that are semantically equivalent or 
almost equivalent (for example, English between 
and among). Even if there are some semantic 
differences between them they do not seem to be 
relevant at the syntactic level. Some linguists were 
in charge of completing this grouping task. Even 
considering the risk of making mistakes when 
grouping the cases, we concluded that the loss of 
accuracy due to having too sparse data 
(consequence of having many cases) would be 
worse than the noise introduced by any mistake in 
the grouping. The resulting set contained 48 cases. 
The complexity is reduced but it is still 
considerable.  
Most of the work on automatic acquisition of 
subcategorization information (J. Carroll & T. 
Briscoe 1997, A. Sarkar & D. Zeman 2000, A. 
Korhonen 2001) apply statistical methods 
(hypothesis testing). Basically the idea is the 
following: they get "possible subcategorization 
frames" from automatically parsed data (either 
completely or partially parsed) or from a 
syntactically annotated corpus. Afterwards a 
statistical filter is employed to decide whether 
those "possible frames" are or not real 
subcategorization frames. These statistical 
methods can be problematic mostly because they 
perform badly on sparse data. In order to avoid as 
much as possible data sparseness, we decided to 
design a system that learns which are the 
arguments of a given verb instead of learning 
whole frames. Frames are combinations of 
arguments, and considering that our system deals 
with 48 cases, the number of combinations was 
high, resulting in sparse data. So we decided to 
work at the level of the argument/adjunct 
distinction. Working on this distinction is also very 
useful to avoid noise in the subcategorization 
frame, because in this task adjuncts are synonyms 
of noise. A system that tries to get 
subcategorization frames without previously 
making the argument/adjunct distinction suffers of 
having sparse and noisy data.  
To accomplish the argument/adjunct distinction 
we applied two measures: Mutual Information 
(MI), and Fisher's Exact Test (for more 
information on these measures, see C. Manning & 
H. Sch?tze 1999). MI is a measure coming from 
Information Theory, defined as the logarithm of 
the ratio between the probability of the co-
occurrence of the verb and the case, and the 
probability of the verb and the case appearing 
together calculated from their independent 
probability. So higher Mutual Information values 
correspond to higher associated verb and cases 
(see table 2). 
Table 2. Examples from MI values for verb-case 
pairs 
verb case MI 
atera(to take/go out) ablative(from) 1.830 
atera(to take/go out) instrumental(with) -0.955 
erabili(to use) gisa(as) 2.255 
erabili(to use) instrumental(with) -0.783 
Mutual Information shows higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These pairs were manually tagged as 
arguments, therefore Mutual information makes 
the right prediction. On the contrary, atera-
instrumental (to go/take out-with), erabili-
instrumental (to use-with) were manually tagged as 
adjuncts. Mutual information values in table 2 go 
along with the manual tagging for these last pairs 
as well, because the Mutual information values are 
low as should correspond to adjuncts.  
Fisher?s Exact Test is a hypothesis testing 
statistical measure1. We used the left-side version 
of the test (see T. Pederssen, 1996). Under this 
version the test tells us how likely it would be to 
perform the same experiment again and be less 
accurate. That is to say, if you were repeating the 
experiment and there were no relation between the 
verb and the case, you would have a big 
probability of finding a lower co-occurrence 
frequency than the one you observed in your 
experiment. So higher left-side Fisher values tell 
us that there is a correlation between the verb and 
the case (see table 3.) 
Table 3. Examples of Fisher?s Exact Test  values for 
verb-case pairs 
verb Case Fisher 
atera(to take/go out) Ablative(from) 1.0000 
atera(to take/go out) instrumental(with) 0.0003 
erabili(to use) gisa(as) 1.0000 
erabili(to use) instrumental(with) 0.0002 
Fisher?s Exact values show higher values for 
atera-ablative(to go/take out), erabili-gisa (to use-
as). These values predict correctly the association 
between the verbs and cases for these examples. 
The low values for the atera-instrumental (to 
go/take out-with), and erabili-instrumental (to use-
with) pairs, should be interpreted as the non-
association between the verbs and the cases in 
these examples, that is to say, they are adjuncts. 
And again, the prediction would be right according 
to the taggers. 
These tests are broadly used to discover 
associations between words, but they show 
different behaviour depending on the nature of the 
data. We did not want to make any a priori 
decision on the measure employed. On the 
contrary, we aimed to check which test behaved 
better on our data.  
3 Evaluation  
We found in the literature two main approaches to 
evaluate a system like the one proposed in this 
paper (T. Briscoe & J. Carroll 1997, A. Sarkar & 
D. Zeman 2000, A. Korhonen 2001): 
                                                     
1 There are two ways of interpreting Fisher?s test, as one 
or two sided test. In the one sided fashion there is still 
another interpretation, as a right or left sided test. 
 
? Comparing the obtained information with a 
gold standard.  
? Calculating the coverage of the obtained 
information on a corpus. This can give  an 
estimate of how well the information obtained 
could help a parser on that corpus. 
Under the former approach a further distinction 
emerges: using a dictionary as a gold standard, or 
performing manual evaluation, where some 
linguists extract the subcategorization frames 
appearing in a corpus and comparing them with the 
set of subcategorization frames obtained 
automatically.  
We decided to evaluate the system both ways, 
that is to say, using a gold standard and calculating 
the coverage over a corpus. The intention was to 
determine, all things being equal, the impact of 
doing it one way or the other. 
3.1 Evaluation 1: comparison of the results with a 
gold standard 
From the 640 analyzed verbs, we selected 10 for 
evaluation. For each of these verbs we extracted 
from the corpus the list of all their dependents. The 
list was a set of bare verb-case pairs, that is, no 
context was involved and, therefore, as the sense 
of the given verb could not be derived, different 
senses of the verb were taken into account.  We 
provided 4 human annotators/taggers with this list 
and they marked each dependent as either 
argument or adjunct. The taggers accomplished the 
task three times. Once, with the simple guideline 
of the implicational test and obligatoriness test, but 
with no further consensus. The inter-tagger 
agreement was low (57%). The taggers gathered 
and realized that the problem came mostly from 
semantics. While some taggers tagged the verb-
case pairs assuming a concrete semantic domain 
the others took into account a wider rage of senses 
(moreover, in some cases the senses did not even 
match). So the tagging was repeated when all of 
them considered the same semantics to the 
different verbs. The inter-tagger agreement raised 
up to a 80%. The taggers gathered again to discuss, 
deciding over the non clear pairs. 
The list obtained from merging2 the 4 lists in 
one is taken to be our gold standard. Notice that 
                                                     
2 Merging was possible once the annotators agreed on 
the marking of each element. 
when the annotators decided whether a possible 
argument was really an argument or not, no 
context was involved. In other words, they were 
deciding over bare pairs of verbs and cases. 
Therefore different senses of the verb were 
considered because there was no way to 
disambiguate the specific meaning of the verb. So 
the evaluation is an approximation of how well 
would the system perform over any corpus. Table 
4 shows the results in terms of Precision and 
Recall. 
Table 4. Results of Evaluation 1 (context 
independent) 
 Precision Recall F-score 
MI 62% 50% 55% 
Fisher 64% 44% 52% 
 
3.2 Evaluation 2: Calculation of the coverage on a 
corpus 
The initial corpus was divided in two parts, one for 
training the system and another one for evaluating 
it. From the fraction reserved for evaluation we 
extracted 200 sentences corresponding to the same 
10 verbs used in the "gold standard" based 
evaluation. In this case, the task carried out by the 
annotators consisted in extracting, for each of the 
200 sentences, the elements (arguments/adjuncts) 
linked to the corresponding verb. Each element 
was marked as argument or adjunct. Note that in 
this case the annotation takes place inside the 
context of the sentence. In other words, the verb 
shows precise semantics.  
We performed a simple evaluation on the 
sentences (see table 5), calculating precision and 
recall over each argument marked by the 
annotators3. For example, if a verb appeared in a 
sentence with two arguments and the statistical 
filters were recognizing them as arguments, both 
precision and recall would be 100%. If, on the 
contrary, only one was found, then precision 
would be 100%, and recall 50%.  
Table 5. Results of Evaluation 2 (inside context) 
 Precision Recall F-score 
MI 93% 97% 95% 
Fisher 93% 93% 93% 
                                                     
3 The inter-tagger agreement in this case was of  97%.  
3.3 Discussion 
It is obvious that the results attained in the first 
evaluation are different than those in the second 
one. The origin of this difference comes mostly, on 
one hand, from semantics and, on the other hand, 
from the nature of statistics: 
? Semantic source. The former evaluation was 
not contextualized, while the latter used the 
sentence context. Our experience showed us 
that broader semantics (non-contextualized 
evaluation) leads to a situation where the 
number of arguments increases with respect to 
narrower (contextualized evaluation) 
semantics. This happens because in many 
cases different senses of the same verb require 
different arguments. So when the meaning of 
the verb is not specified, different meanings 
have to be taken into account and, therefore, 
the task becomes more difficult. 
? Statistical reason. The disagreement in the 
results comes from the nature of the statistics 
themselves. Any statistical measure performs 
better on the most frequent cases than on the 
less frequent ones. In the first experiment all 
possible arguments are evaluated, including 
the less frequent ones, whereas in the second 
experiment only the possible arguments found 
in the piece of corpus used were evaluated. In 
most of the cases, the possible arguments 
found were the most frequent ones. 
At this point it is important to note that the 
system deals with non-structural cases. In Basque 
there are three structural cases (ergative, absolutive 
and dative) which are special because, when they 
appear, they are always arguments. They 
correspond to the subject, direct object and indirect 
object functions. These cases are not very 
conflictive about argumenthood, mainly because in 
Basque the auxiliary bears information about their 
appearance in the sentence. So they are easily 
recognized and linked to the corresponding verb. 
That is the reason for not including them in this 
work. Precision and recall would improve 
considerably if they were included because they 
are the most frequent cases (as statistics perform 
well over frequent data), and also because the 
shallow parser links them correctly using the 
information carried by the auxiliary. Notice that 
we did not incorporate them because in the future 
we would like to use the subcategorization  
information obtained for helping our parser, and 
the non-structural cases are the most problematic 
ones.    
4 Related work  
Concerning the acquisition of verb 
subcategorization information, there are proposals 
ranging from manual examination of corpora (R. 
Grishman et al 1994) to fully automatic 
approaches.  
Table 3, partially borrowed from A. Korhonen 
(2001), summarizes several systems on 
subcategorization frame acquisition. 
C. Manning (1993) presents the acquisition of 
subcategorization frames from unlabelled text 
corpora. He uses a stochastic tagger and a finite 
state parser to obtain instances of verbs with their 
adjacent elements (either arguments or adjuncts), 
and then a statistical filtering phase produces 
subcategorization frames (from a set of previously 
defined 19 frames) for each verb.  
T. Briscoe and J. Carroll (1997) describe a 
grammar based experiment for the extraction of 
subcategorization frames with their associated 
relative frequencies, obtaining 76.6% precision 
and 43.4% recall. Regarding evaluation, they use 
the ANLT and COMLEX Syntax dictionaries as 
gold standard. They also performed evaluation of 
coverage over a corpus. For our work, we could 
not make use of any previous information on 
subcategorization, because there is nothing like a  
subcategorization dictionary for Basque. 
A. Sarkar and D. Zeman (2000) report results 
on the automatic acquisition of subcategorization 
frames for verbs in Czech, a free word order 
language. The input to the system is a set of 
manually annotated sentences from a treebank, 
where each verb is linked with its dependents 
(without distinguishing arguments and adjuncts). 
The task consists in iteratively eliminating 
elements from the possible frames with the aim of 
removing adjuncts. For evaluation, they give an 
estimate of how many of the obtained frames 
appear in a set of 500 sentences where dependents 
were annotated manually, showing an 
improvement from a baseline of 57% (all elements 
are adjuncts) to 88%. 
Comparing this approach to our work, we must 
point out that Sarkar and Zeman's data does not 
come from raw corpus, and thus they do not deal 
with the problem of noise coming from the parsing 
phase. Their main limitation comes by relying on a 
treebank, which is an expensive resource. 
D. Kawahara et al (2001) use a full syntactic 
parser to obtain a case frame dictionary for 
Japanese, where arguments are distinguished by 
their syntactic case, including their headword 
(selectional restrictions). The resulting case frame 
components are selected by a frequency threshold. 
Table 3. Summary of several systems on subcategorization information. 
Method Number 
of frames 
Number 
of verbs 
Linguistic 
resources 
F-Score 
(evaluation 
based on a 
gold standard) 
Coverage on a 
corpus 
C. Manning (1993) 19 200 POS tagger + simple 
finite state parser 
58  
T. Briscoe & J. 
Carroll (1997) 
161 14 Full parser 55  
A. Sarkar & D. 
Zeman (2000) 
137 914 Annotated treebank - 88 
D. Kawahara et al 
(2001) 
- 23,497 Full parser  82 accuracy 
M. Maragoudakis et 
al. (2001) 
- 47 Simple phrase 
chunker 
77  
This paper - 640 Morph. Analyzer + 
Phrase Chunker + 
Finite State Parser 
55 95 
      
M. Maragoudakis et al (2001) apply a 
morphological analyzer and phrase chunking 
module to acquire subcategorization frames for 
Modern Greek. In contrast to this work, they use 
different machine learning techniques. They claim 
that Bayesian Belief Networks are the best 
learning technique. 
P. Merlo and M. Leybold (2001) present 
learning experiments for automatic distinction of 
arguments and adjuncts, applied to the case of 
prepositional phrases attached to a verb. She uses 
decision trees tested on a set of 400 verb instances 
with a single PP, reaching an accuracy of 86.5% 
over a baseline of 74%. 
Note that both Manning and Merlo and 
Leybold's systems learn from contexts with just 
one PP (maximum) per verb (finite state filter). 
Our system learns from contexts with up to 5 PPs. 
Furthermore, we distinguish 48 different kinds of 
cases, hence the number of combinations is 
considerably bigger.  
Regarding the parsing phase, the systems 
presented so far are heterogeneous. While  
Manning, Merlo and Leybold and Maragoudakis et 
al. use very simple parsing techniques, Briscoe and 
Carroll and Kawahara et al use sophisticated 
parsers. Our system can be placed between these 
two approaches. The result of the shallow parsing 
is not simple in that it relies on a robust 
morphological analysis and disambiguation. 
Remember that Basque is an agglutinative 
language with strong morphology and, therefore, 
this stage is particularly relevant. Moreover, the 
finite state filter we used for parsing is very 
sophisticated (L. Karttunen et al 1997, I. 
Aldezabal et al 2001), compared to Manning's. 
Conclusion  
This work describes an initial effort to obtain 
subcategorization information for Basque. To 
successfully perform this task we had to go deeper 
than mere syntactic categories (NP, PP, ?) 
enriching the set of possible arguments to 48 
different classes. This leads to quite sparse data.  
Together with sparseness, another problem 
common to every subcategorization acquisition 
system is that of noise, coming from adjuncts and 
incorrectly parsed elements. For that reason, we 
defined subcategorization acquisition in terms of 
distinguishing between arguments and adjuncts. 
The system presented was applied to a 
newspaper corpus. Subcategorization acquisition is 
highly associated to semantics in that different 
senses of a verb will most of the times show 
different subcategorization information. Thus, the 
task of learning subcategorization information is 
influenced by the corpus. As for the evaluation of 
this work, we carried out two different kinds of 
evaluation. This way, we verified the relevance of 
semantics in this kind of task. 
For the future, we plan to incorporate the 
information resulting from this work in our parsing 
system. We hope that this will lead to better results 
in parsing. Consequently, we would get better 
subcategorization information, in a bootstrapping 
cycle. We also plan to improve the results by using 
semantic information as proposed in A. Korhonen 
(2001).  
Acknowledgements 
This work has been supported by the Department 
of Economy of the Government of Gipuzkoa, The 
University of the Basque Country, the Department 
of Education of the Basque Government and the 
Commission of Science and Technology of the 
Spanish Government.   
References 
I. Aduriz, J. M. Arriola, X. Artola, A. D?az de 
Ilarraza, K. Gojenola and M. Maritxalar (1997) 
Morphosyntactic disambiguation for Basque based on 
the Constraint Grammar Formalism. Conference on 
Recent Advances in Natural Language Processing 
(RANLP).  
I. Alegria, X. Artola, K. Sarasola and M. Urkia (1996) 
Automatic morphological analysis of Basque. Literary 
and Linguistic Computing. 11 (4), Oxford University. 
I. Aldezabal, K. Gojenola and K. Sarasola (2000) A 
Bootstrapping Approach to Parser Development. 
International Workshop on Parsing Technologies 
(IWPT), Trento. 
I. Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola, 
M. Oronoz M. and Sarasola K. (2001) Application of 
finite-state transducers to the acquisition of verb 
subcategorization information. Finite State Methods 
in Natural Language Processing, ESSLLI Workshop, 
Helsinki. 
M. R. Brent (1993) From Grammar to Lexicon: 
Unsupervised Learning of Lexical Syntax. 
Computational Linguistics, 19:243-262. 
T. Briscoe and J. Carroll  (1997) Automatic Extraction 
of Subcategorization from Corpora. ANLP-97:356-
363. 
J. Carroll, G. Minnen and T. Briscoe (1998) Can 
Subcategorization Probabilities Help a Statistical 
Parser? Proceedings of the 6th ACL/SIGDAT 
Workshop on Very Large Corpora, Montreal. 
N. Chomsky (1995) The Minimalist Program. 
Cambridge MA, MIT Press. 
T. Dunning  (1993) Accurate Methods for the 
Statistics of Surprise and Coincidence. Computational 
Linguistics 19, 1  
J.M. Gawron (1986) Situations and prepositions. 
Linguistics and Philosophy 9(3), 327-382. 
J. Grimshaw (1990) Argument Structure. Cambridge, 
MA, MIT Press. 
R. Grishman, C. Macleod, A. Meyers (1994) Comlex 
Syntax: Building a Computational Lexicon. COLING-
94. 
F. Karlsson, A. Voutilainen, J. Heikkila, A. Anttila 
(1995) Constraint Grammar: A Language-
independent System for Parsing Unrestricted Text. 
Mouton de Gruyter. 
L. Karttunen, J.P. Chanod, G. Grefenstette, A. Schiller 
(1997) Regular Expressions For Language 
Engineering. Natural Language Engineering. 
D. Kawahara, N. Kaji and S. Kurohashi (2000) 
Japanese Case Structure Analysis by Unsupervised 
Construction of a Case Frame Dictionary. COLING-
2000, Saarbrucken. 
A. Korhonen (2001) Subcategorization acquisition. 
Unpublished  PhD Thesis, University of Cambridge. 
K. Koskenniemi (1983) Two-level Morphology: A 
general Computational Model for Word-Form 
Recognition and Production. PhD thesis, University 
of Helsinki. 
J. Kuhn, J. Eckle-Kohlerm and C. Rohrer (1998) 
Lexicon Acquisition with and for Symbolic NLP-
Systems -- a Bootstrapping Approach. First 
International Conference on Language Resources and 
Evaluation (LREC98), Granada. 
C.D. Manning (1993) Automatic Acquisition of a 
Large Subcategorization Dictionary from Corpora. 
Proceedings of the 31th ACL. 
C.D. Manning and H. Sch?tze (1999) Foundations of 
Statistical Natural Language Processing. The MIT 
Press, Cambridge, Massachusetts.  
M. Maragoudakis, K. Kermanidis, N. Fakotakis and 
G. Kokkinakis (2001) Learning Automatic Acquisition 
of Subcategorization Frames using Bayesian 
Inference and Support Vector Machines. The 2001 
IEEE International Conference on Data Mining, 
IMDC'01, San Jos?. 
P. Merlo and M. Leybold (2001) Automatic 
Distinction of Arguments and Modifiers: the Case of 
Prepositional Phrases. EACL-2001, Toulousse. 
T. Pederssen (1996) Fishing for Exactness In the 
Proceeding of the South-Central SAS User Group 
Conference (SCSUG-96). 
C. Pollard and I. Sag (1987) An information based 
Syntax and Semantics, volume 13. CSLI lecture. 
Notes, Standford University. 
A. Sarkar and D. Zeman (2000) Automatic Extraction 
of Subcategorization Frames for Czech. COLING-
2000, Saarbrucken.  
C. Schutze (1995) PP Attachment and Argumenthood. 
MIT Working Papers in Linguistics. 
C. Verspoor (1997) Contextually-Dependent Lexical 
Semantics. PhD thesis, Brandeis University, MA. 
Representation and Treatment of Multiword Expressions in Basque 
I?aki Alegria, Olatz Ansa, Xabier Artola 
Nerea Ezeiza, Koldo Gojenola and Ruben Urizar 
Ixa Group 
University of the Basque Country 
649 pk E-20.080 
Donostia. Basque Country 
rubenu@sc.ehu.es 
Abstract 
This paper describes the representation of 
Basque Multiword Lexical Units and the 
automatic processing of Multiword 
Expressions. After discussing and stating 
which kind of multiword expressions we 
consider to be processed at the current 
stage of the work, we present the 
representation schema of the 
corresponding lexical units in a general-
purpose lexical database. Due to its 
expressive power, the schema can deal 
not only with fixed expressions but also 
with morphosyntactically flexible 
constructions. It also allows us to 
lemmatize word combinations as a unit 
and yet to parse the components 
individually if necessary. Moreover, we 
describe HABIL, a tool for the automatic 
processing of these expressions, and we 
give some evaluation results. This work 
must be placed in a general framework of 
written Basque processing tools, which 
currently ranges from the tokenization 
and segmentation of single words up to 
the syntactic tagging of general texts. 
1 Introduction 
2 
Most texts are rich in multiword expressions, 
which must be necessarily processed if we want 
any NLP tool to perform accurately. Jackendoff 
(1997) estimates that their number in the speakers' 
lexicon ?is of the same order of magnitude as the 
number of single words?. 
There is no agreement among authors about the 
definition of the term Multiword Expression. 
However, in this article, Multiword Expressions 
(hereafter MWE) refer to any word combinations 
ranging from idioms, over proper names, 
compounds, lexical and grammatical 
collocations? to institutionalized phrases. MWEs 
comprise both semantically compositional and 
non-compositional combinations, and both 
syntactically regular and idiosyncratic phrases, 
including complex named entities such as proper 
nouns, dates and number expressions (see section 
2). 
In contrast, Multiword Lexical Units (hereafter 
MWLU) comprise lexicalized phrases ?
semantically non-compositional or syntactically 
idiosyncratic word combinations? which are 
represented and stored in the lexical database of 
Basque (EDBL). 
The remaining sections are organized as 
follows. Section 2 presents the main features of 
MWEs in Basque, and defines which are currently 
considered for automatic processing. Section 3 
describes the representation of MWLUs in the 
lexical database. Section 4 is devoted to the 
description and evaluation of the automatic 
treatment of MWEs by means of HABIL. Section 
5 summarizes future work. And, finally, section 6 
outlines some conclusions. 
Multiword Expressions in the 
processing of real texts in Basque 
The definition of the term Multiword Expression 
and the types of such MWEs to be treated in NLP 
may vary considerably depending on the purposes 
or "the depth of processing being undertaken" 
(Copestake et al, 2002). Multiword itself is a 
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 48-55
vague term. At text level, a word could be defined 
as "any string of characters between two blanks" 
(Fontenelle et al, 1994). This is not applicable to 
languages as Japanese, which are typically written 
without spaces. Besides, a great number of MWEs 
that in uninflected languages would be multiword, 
constitute a single typographic unit in agglutinative 
languages such as Basque (ziurrenik 'most 
probably', aurrerantzean 'from now on', aurretiaz 
'in advance'). Therefore, we consider them single 
words and they are included in the lexical database 
as such (or recognized by means of morphological 
analysis). 
In our case, when deciding which Basque 
MWEs to include in the database, we mostly rely 
on lexicographers' expertise since we consider 
lexicalized phrases have a top priority for both 
lemmatizing and syntactic purposes. So, the 
MWEs dealt with in the database comprise fixed 
expressions, which admit no morphosyntactic or 
internal modification ?including foreign 
expressions such as in situ, a priori, strictu sensu, 
etc.?, idioms, both decomposable and non-
decomposable, and lexicalized compounds. We 
also consider light verb constructions when they 
are syntactically idiosyncratic.  
However, currently we do not treat open 
collocations, proverbs, catch phrases and similes. 
Mostly, we don't include proper names in the 
database either, since complex named entities are 
given a separate treatment. Apart from proper 
nouns, also dates and number expressions are 
treated separately (see 4.1). 
So far we have described 2,270 MWLUs in our 
database. This work has been carried out in two 
phases. For the first phase, we made use of the 
Statistical Corpus of 20th Century Basque 
(http://www.euskaracorpusa.net) that contains 
about 4.7 million words. As a starting point, we 
chose the MWLUs that occurred more than 10 
times in this manually lemmatized corpus. This 
amounted to about 1,300 expressions. For the 
second phase, this list has been enlarged using the 
Hiztegi Batua, a dictionary of standard Basque that 
the Basque Language Academy updates regularly 
(http://www2.euskaltzaindia.net/hiztegibatua). 
2.1 
3 
Main features of lexicalized phrases 
Many of the lexicalized phrases are semantically 
non-compositional (or partially compositional), i.e. 
they can hardly be interpreted in terms of the 
meaning of their constituents (adarra jo 'to pull 
someone's leg', literally 'to play the horn'). 
Often, a component of these sequences hardly 
occurs in any other context and it is difficult to 
assign it a part of speech. For example, the word 
noizik is an archaism of modern noiztik 'from 
when', which occurs just in the expressions noizik 
behin, noizik behinean, noizik noizera, and noizik 
behinka all meaning 'once in a while'. Besides, it is 
not clear which is the part of speech of the words 
laprast in laprast egin 'to slip' or dir-dir in dir-dir 
egin 'to shine'. 
From a syntactic point of view, many of these 
MWEs present an unusual structure. For example, 
many complex verbs in Basque are light verb 
constructions, being the meaning of the compound 
quite compositional, e.g. lo egin 'to sleep' literally 
'to make (a) sleep' or lan egin 'to work' literally 'to 
make (a) work'. However, lo egin and lan egin can 
be considered 'syntactically idiomatic' since the 
nouns in these expressions, lo and lan, take no 
determiner, which would be completely 
ungrammatical for a noun functioning as a regular 
direct object (*arroz jan nuen 'I ate rice'). 
Morphosyntactic flexibility, being significant in 
this type of constructions in Basque, may vary 
considerably. For example in lo egin 'to sleep' the 
noun lo admits modification (lo asko egin zuen 'he 
slept very much') and may take the partitive 
assignment (ez dut lorik egin 'I haven't slept') while 
the verb egin can be subject to focalization (egin 
duzu lorik bart? 'did you sleep at all last night?'); 
besides, the components of the construction may 
change positions and some elements and phrases 
may be placed between them (mendian egin omen 
zuen lasai lo 'it is said that he slept peacefully in 
the mountain'). In contrast, alde egin 'to escape' is 
morphosyntactically quite rigid. In all the cases, 
the verb egin can take any inflection. 
For our database, we have worked out a single 
representation that covers all MWLUs ranging 
from fixed expressions to these of highest 
morphosyntactic flexibility. 
Representation of MWLUs in the lexical 
database 
In this section we explain how MWLUs are 
represented in EDBL (Aldezabal et al, 2001), a 
lexical database oriented to language processing 
that currently contains more than 80,000 entries, 
out of which 2,270 are MWLUs. Among these: 
? ~69% are always unambiguous. The average 
number of Surface Realization Schemas 
(SRS, see section 3.2) is 1.02. 
? ~23% are sometimes unambiguous and have 
3.6 SRSs in average, half of them 
ambiguous. 
? ~8% are always ambiguous and have 1.2 
SRSs in average. 
We want to point out that almost all of the 
unambiguous MWLUs have only one SRS, their 
components appearing in contiguous positions and 
always in the same order. About half of them are 
inflected, so, even if we discard the interpretations 
of the components, there is still some 
morphosyntactic ambiguity left. However, the 
identification of these MWLUs helps in 
disambiguation, as the input of tagging is more 
precise. 
 
The description of MWLUs within a general-
purpose lexical database must include, at least, two 
aspects (see Figure 1): (1) their composition, i.e. 
which the components of the MWLU are, whether 
each of them can be inflected or not, and according 
to which one-word lexical unit (OWLU 1 ) it 
inflects; and (2), what we call the surface 
realization, that is, the order in which the 
components may occur in the text, the mandatory 
or optional contiguousness of components, and the 
inflectional restrictions applicable to each one of 
the components. 
3.1 
                                                          
Composition 
As it has just been said, the description of the 
composition of MWLUs in EDBL gathers two 
aspects: on the one side, it depicts which the 
individual components of a MWLU are; on the 
other side, it links the inflectable components of a 
MWLU to the corresponding OWLU according to 
which each of them inflects. 
In Figure 1, we can see that the composed of 
relationship links every MWLU to up to 9 
individual components (MWLU_Components). 
Each component is characterized by the following 
attributes: 
1 We consider OWLUs lexical units with no spaces within its 
orthographical form; so, we also take hyphenated compounds 
as OWLUs. 
? Component_Position: this indicates 
the position of the component word-form in 
the canonical form of the MWLU. 
? Component_Form: i.e. the word-form 
itself as it appears in the canonical form of 
the MWLU. 
? Conveys_Morph_Info?: this is a 
Boolean value, indicating whether the 
component inflection conveys the 
morphological information corresponding to 
the whole MWLU or not2. 
(0,n)
(1,1)
(1,1)
(1,n)
(2,9)
(1,1)
 
MWLUs
Surface_Realization_Schemas
Order_Contiguousness
Sureness
Inflection_Restrictions
MWLU_Components
Component_Position
Component_Form
Conveys_Morph_Info?
OWLUs
corresp.
SR schemas
inflects
according to
composed of
Figure 1. Composition and surface realization of 
MWLUs. 
Moreover, the components of a MWLU are 
linked to its corresponding OWLU (according to 
which it inflects). This is represented by means of 
the inflects according to relationship 
(see Figure 1). 
                                                          
2 The morphological information that the attribute refers to is 
the set of morphological features the inflection takes in the 
current component instance. 
These two aspects concerning the composition 
of a MWLU are physically stored in a single table 
of the relational database in which EDBL resides.  
The columns of the table are the following: 
Entry, Homograph_Id, Component_ 
Position, Component_Form, Conveys_ 
Morph_Info?, OWLU_Entry, and OWLU_ 
Homograph_Id. In the example below, the 
composition of the MWLU begi bistan egon 'to be 
evident' is described. Note that one row is used per 
component: 
<begi bistan egon, 0, 1, begi, -, begi, 2> 
<begi bistan egon, 0, 2, bistan, -, bista, 1> 
<begi bistan egon, 0, 3, egon, +, egon, 1> 
This expression allows different realizations 
such as begi bistan dago 'it is evident' (literally 'it 
is at reach of the eyes'), begi bistan daude 'they are 
evident', begien bistan egon, 'to be evident', etc. In 
the table rows above, it can be seen that the last 
component egon 1 'to be' conveys the 
morphological information for the whole MWLU 
(+ in the corresponding column). 
3.2 Surface realization 
As for surface realization, we have already 
mentioned that the components of a MWLU can 
occur in a text either contiguously or dispersed. 
Besides, the order of the constituents may be fixed 
or not, and they may either inflect or occur in an 
invariable form. In the case of inflected 
components, some of them may accept any 
inflection according to its corresponding OWLU, 
whilst others may only inflect in a restricted way. 
Moreover, some MWLUs are unambiguous and 
some are not, since it cannot be certainly assured 
that the very same sequence of words in a text 
corresponds undoubtedly to a multiword entry in 
every context. For example, in the sentence Emilek 
buruaz baiezko keinu bat egin zuen 'Emile nodded 
his head' the words bat and egin do not correspond 
to the complex verb bat egin 'to unite' but to two 
separate phrases. 
According to these features, we use a formal 
description where different realization patterns 
may be defined for each MWLU. The corresp. 
SR schemas relationship in Figure 1 links every 
MWLU to one or more Surface_Realiza-
tion_Schemas. Each SRS is characterized by 
the following attributes: 
? Order_Contiguousness: an expression 
that indicates both the order in which the 
components may appear in the different 
instances of the MWLU and the 
contiguousness of these components. In 
these expressions the position of the digits 
indicate the position each component takes 
in a particular SRS, * indicates that 0 or 
more words may occur between two 
components, and ? indicates that at most 
one single word may appear between two 
given components of the MWLU. 
? Unambiguousness: a Boolean value, 
indicating whether the particular SRS 
corresponds to an unambiguous MWLU or 
not. It expresses whether the sequence of 
words matching this SRS must be 
unambiguously analyzed as an instance of 
the MWLU or, on the contrary, may be 
analyzed as separate OWLUs in some 
contexts. 
? Inflection_Restrictions: an 
expression that indicates the inflection 
paradigm according to which the MWLU 
may inflect in this specific SRS. In these 
expressions each component of the MWLU 
is represented by one list component (in the 
same order as the components of the 
MWLU appear in its canonical form): % 
indicates that the whole inflection paradigm 
of the corresponding inflectable component 
may occur; the minus sign (-) is used for 
non-inflectable components (no inflection at 
all may occur); finally, a logical expression 
(and, or, and not are allowed) composed 
of attribute-value pairs is used to express the 
inflectional restrictions and the 
morphotactics the component undergoes in 
this particular SRS of the MWLU (in 
brackets in the examples below). 
In the examples below, it can be seen that one 
row is used per SRS. The columns of the table are 
the following: Entry, Homograph_Id, Or-
der_Contiguousness, Unambiguousness, 
and Inflection_Restrictions: 
<begi bistan egon, 0, 123, +, 
 (((CAS=ABS) and (DEF=-)) or 
  ((CAS=GEN) and (NUM=PL)), -, %)> 
<begi bistan egon, 0, 312, +, 
 (((CAS=ABS) and (DEF=-)) or 
  ((CAS=GEN) and (NUM=PL)), -, %)> 
<begi bistan egon, 0, 3?12, +, 
 (((CAS=ABS) and (DEF=-)), -, %)> 
 
The first SRS matches occurrences such as begi 
bistan dago hau ez dela aski 'it is evident that it is 
not enough' or begien bistan zegoen honela 
bukatuko genuela 'it was evident that we would 
end up this way', where the components are 
contiguous and the analysis as an instance of the 
MWLU would be unambiguous. This SRS allows 
the inflection of the first component as absolutive 
case (non-definite) or as genitive (plural), and the 
whole set of inflection morphemes of the third one. 
The third SRS matches occurrences such as ez 
dago horren begi bistan 'it is not so evident', where 
the components are not contiguous (at most one 
word is allowed between the ?third? component 
and the ?first one?) and they occur in a non-
canonical order: 3?12. In this case, the 
interpretation as an instance of the MWLU would 
also be unambiguous. However, this SRS only 
allows the inflection of the first component as 
absolutive case (non-definite). 
3.3 
4 
Different information requirements in 
lemmatization and syntax processing 
The first prototype for the treatment of MWEs in 
Basque HABIL (Ezeiza et al, 1998; Ezeiza, 2003) 
was built for lemmatization purposes. However, 
we are nowadays involved in the construction of a 
deep syntactic parser (Aduriz et al, 2004) and the 
MWEs seem to need a different treatment. The fact 
that many MWEs may be syntactically regular but, 
above all, that an external element may have a 
dependency relation with one of the constituents, 
forces us to analyze the elements independently. 
For example, in the verb beldur izan 'to be afraid 
(of)' an external noun phrase may have a modifier-
noun dependency relation with beldur 'fear' as in 
sugeen beldur naiz 'I'm afraid of snakes'. In loak 
hartu 'to fall asleep' there is a subject-verb relation 
as in loak hartu nau 'I have fallen asleep', literally 
'sleep has caught me'; therefore subject-auxiliary 
verb agreement would fade if both components 
were analyzed as one. 
The MWLU representation we have adopted 
allows us to lemmatize the word combination as a 
unit and yet to parse the components individually 
whenever necessary. In order to do so, when 
describing each MWLU, we specify whether the 
elements in the MWLU must be analyzed 
separately or not3. 
Treatment of multiword expressions 
MWEs could be treated at different stages of the 
language process. Some approaches treat them at 
tokenization stage, identifying fixed phrases, such 
as prepositional phrases or compounds, included in 
a list (Carmona et al, 1998; Karlsson et al, 1995). 
Other approaches rely on morphological analysis 
to better identify the features of the MWE using 
finite state technology (Breidt et al, 1996). 
Finally, there is another approach that identifies 
them after the tagging process, allowing the 
correction of some tagging errors (Leech et al, 
1994). 
All of these approaches are based on the use of 
a closed set of MWLUs that could be included in a 
list or a database. However, some groups of MWEs 
are not subject to be included in a database, 
because they comprise an open class of 
expressions. That is the case of collocations, 
compounds or named entities. The group of 
collocations and compounds should be delimited 
using statistical approaches, such as Xtract 
(Smadja, 1993) or LocalMax (Silva et al, 1999), 
so that only the most relevant?those of higher 
frequency? are included in the database.  
Named entity recognition task has been solved 
for a large set of languages. Most of these works 
are linked to the Message Understanding 
Conference (Chinchor, 1997). There is a variety of 
methods that have been used in NE recognition, 
such as HMM, Maximum Entropy Models, 
Decision Trees, Boosting and Voted Perceptron 
(Collins, 2002), Syntactic Structure based 
approaches and WordNet-based approaches 
(Magnini et al, 2002; Ar?valo, 2002). Most 
references on NE task might be accessed at 
http://www.muc.saic.com. 
4.1 
                                                          
Processing MWEs with HABIL 
We have implemented HABIL, a tool for the 
treatment of multiword expressions (MWE), based 
3  Currently we are studying the MWLUs in the lexical 
database in order to determine which of them deserve to be 
parsed as separate elements. We have not defined yet how this 
will be formally represented in the database. 
on the features described in the lexical database. 
The most important features of HABIL are the 
following: 
? It deals with both contiguous and split 
MWEs. 
? It takes into account all the possible orders 
of the components (SRS). 
? It checks that inflectional restrictions are 
complied with. 
? It generates morphosyntactic interpretations 
for the MWE. 
This tool has two different components: on the 
one hand, there is a searching engine that identifies 
MWEs along the text, and, on the other hand, there 
is a morphosyntactic processor that assigns the 
corresponding interpretations to the components of 
the MWE. 
The morphosyntactic processor generates the 
interpretations for MWEs using category and 
subcategory information in the lexical database. 
When one of the components adds information to 
the MWE, the processor applies pattern-matching 
techniques to extract the corresponding 
morphological features of the analyses of that 
component, and these features are included in the 
interpretation of the MWE. Then, it replaces all the 
morphosyntactic interpretations of the components 
of unambiguous MWEs with the MWE 
interpretations. When MWEs are ambiguous, the 
new interpretations are added to the existing ones. 
HABIL also identifies and treats dates and 
numerical expressions. As they make up an open 
class, they are not obviously included in the lexical 
database. Furthermore, their components are 
always contiguous, have a very strict structure, and 
use a closed lexicon. Thus, it is quite easy to 
identify them using simple finite state transducers. 
For the morphosyntactic treatment of dates and 
numerical expressions, we use the morphosyntactic 
component of HABIL. These expressions may 
appear inflected and, in this case, the last 
component adds morphosyntactic features to the 
MWE. Finally, as they are unambiguous 
expressions, the processor discards the 
interpretations of the components and assigns them 
all the interpretations of the whole expression. 
4.2 Evaluation 
We performed several experiments using 650 
unambiguous, contiguous and ordered MWEs. We 
treated a reference corpus of around 36,000 tokens 
and there were 386 instances of 149 different 
MWEs. We also applied this process to a small test 
corpus of around 7,100 tokens in which there were 
87 instances of 45 MWEs. Taking both corpora 
into account, there were 473 instances of 167 
different MWEs, which amounted to 25% of the 
expressions considered, and 50% of the instances 
were ambiguous. Besides, only 14 dates and 12 
numerical expressions were found in the reference 
corpus, and 18 dates and 9 numerical expressions 
in the test corpus. 
 
  Ambiguity 
Rate 
Interpretations 
per Token 
Recall
word-
forms: 
before
after 
81.78% 
79.83% 
3.37 
3.30 
99.31%
99.31%
all 
tokens: 
before
after 
67.47% 
65.86% 
2.96 
2.89 
99.43%
99.43%
Table 1. Results of HABIL. 
The ambiguity measures of the test corpus are 
shown in Table 1. The ambiguity rate of word-
forms decreases by 2% and the average ambiguity 
rate by 1.5% after the processing of MWEs. It is 
important to point out that no error is made along 
the process. Furthermore, some important MWEs, 
more specifically, some complex sentence 
connectors that have highly ambiguous 
components, are correctly disambiguated. 
Bearing in mind the proportion of words treated 
by HABIL, these results help significantly in 
improving precision results of tagging and 
avoiding almost 10% of the errors, as shown in 
Table 2.  
 
 Precision Error 
before MWE processing 94.96% 5.04% 
after MWE processing 95.42% 4.58% 
Table 2. Tagging results. 
5 Future work 
After confirming the viability of the system and the 
good results in POS tagging, our main goal is to 
increase the number of MWLUs in the database, 
which will improve the identification of MWEs in 
corpora. 
A remaining difficulty that we are facing is the 
problem of ambiguous split MWEs. At present, we 
are creating a disambiguation grammar that will 
discard or select the multiword interpretations in 
ambiguous MWLUs. We are developing similar 
rules using both the Constraint Grammar 
formalism and finite state transducers (XFST tools, 
Kartunnen et al 1997). The very first rules seem to 
be quite effective. Soon, we will be assessing the 
first results, and then we will be able to choose the 
method that performs best with a lesser effort. 
Once we have chosen the best formalism, we 
intend to develop a comprehensive grammar that 
will disambiguate as many ambiguous MWLUs as 
possible. 
In addition, we are developing new processes 
after POS tagging in order to identify complex 
named entities and terminological units. These 
units constitute an open class and so their 
exhaustive inclusion in a database would not be 
viable. 
6 Conclusion 
7 
In this paper we have described a whole 
framework for the representation and treatment of 
MWEs, which is being currently used at the IXA 
Research Group to process this kind of expressions 
in general texts. Although it has been conceived 
and so far used for Basque, a highly inflected 
language, we think that it is general enough to be 
applied to other languages. 
A general representation schema for MWLUs at 
the lexical level has been proposed. This schema 
allows us to state which components a MWLU has 
and to formally encode all the different surface 
realizations it can adopt in the text. 
The problems that diverse information require-
ments in lemmatization and syntactic processing 
can eventually pose have been explained, and a 
possible solution for the representation of these 
phenomena has also been outlined. 
As for the processing aspects, we have 
described HABIL, the tool for the treatment of 
MWEs. HABIL processes MWEs based on their 
description in the lexical database, dealing also 
with some types of open class MWEs. 
One of the remaining problems when split and 
ambiguous MWEs are to be tagged is related with 
disambiguation procedures using Hidden Markov 
Models, which are not able to manage different 
paths with variable lengths. This problem can be 
solved using rule-based methods or lattice 
structures for tagging. 
Acknowledgements 
This research is being partially funded by the 
European Commission (MEANING project, IST-
2001-34460) and the Basque Government 
(Etortek-Hizking, Saiotek-Ihardetsi). 
References 
Aduriz I., Aranzabe M., Arriola J., D?az de Ilarraza A., 
Gojenola K., Oronoz M., Uria L. 2004. A cascaded 
syntactic analyser for Basque. Fifth International 
Conference on Intelligence Text Processing and 
Computational Linguistics (CICLing2004). Seoul, 
Korea. 
Aldezabal I., Ansa O., Arrieta B., Artola X., Ezeiza N., 
Hern?ndez G., Lersundi M. 2001. EDBL: a General 
Lexical Basis for the Automatic Processing of 
Basque. IRCS Workshop on Linguistic Databases. 
Philadelphia. 
Ar?valo M. 2002. MICE, un recurso para la resoluci?n 
de la an?fora. International Workshop on 
Computational Linguistics. http://www.lsi.upc.es/ 
~nlp/iwcl02. 
Breidt E., Segond F., Valetto G. 1996. Local grammars 
for the description of multi-word lexemes and their 
automatic recognition in texts. Proceedings of 
COMPLEX'96, 19-28. Budapest. 
Carmona J., Cervell S., M?rquez L., Mart? M.A., Padr? 
L., Placer R., Rodr?guez H., Taul? M., Turmo J. 
1998. An environment for morphosyntactic 
processing of unrestricted Spanish text. Proceedings 
of LREC'98. 915-922. 
Chinchor N. 1997. MUC-7 Named Entity Task 
Definition. Version 3.5. http://www.itl.nist.gov/iaui/ 
894.02/related_projects/muc/  
Collins M. 2002. Ranking Algorithms for Named-Entity 
Extraction: Boosting and the Voted Percetron. 
Proceedings of ACL-2002. 
Collins M., Singer Y. 1999. Unsupervised Models for 
Named Entity Classification. Proceedings of the 
Joint Conference on Empirical Methods in Natural 
Language Processing and Workshop on Very Large 
Corpora (EMNLP-VLC-99). 
Copestake A., Lambean F., Villavicencio A., Bond F., 
Baldwin T., Sag I., Flickinger D. 2002. Multiword 
Expressions: linguistic precision and reusability. 
Proceedings of the Third International Conference 
on Language Resources and Evaluation (LREC 
2002), Las Palmas, pp. 1941-7. 
Ezeiza N. 2003. Corpusak ustiatzeko tresna 
linguistikoak. Euskararen etiketatzaile sintaktiko 
sendo eta malgua. PhD thesis, University of the 
Basque Country. 
Ezeiza N., Aduriz I., Alegria I., Arriola J.M., Urizar R.  
1998. Combining Stochastic and Rule-Based 
Methods for Disambiguation in Agglutinative 
Languages. COLING-ACL'98, Montreal (Canada). 
Fontenelle T., Adriaens G., De Braekeleer G. 1994. The 
Lexical Unit in the Metal? MT System, MT. The 
Netherlands. v9. 1-19. 
Jackendoff R. 1997. The Architecture of the Language 
Faculty. Cambridge, MA MIT Press. 
Karlsson F., Voutilainen A., Heikkila J,. Anttila A. 
1995. Constraint Grammar: A Language-
independent System for Parsing Unrestricted Text. 
Mouton de Gruyter. 
Karttunen L., Chanod J-P., Grefenstette G., Schiller A. 
1997. Regular expressions for language engineering. 
Natural Language Engineering, Cambridge 
University Press. 
Leech G., Garside R., Bryan M. 1994. CLAWS4: The 
tagging of the British National Corpus. Proceedings 
of COLING-94, 622-628. 
Magnini B., Negri M., Prevete R., Tanev H. 2002. A 
WordNet Approach to Named Entities Recognition. 
Proceeding of the Workshop SemaNet'02: Binding 
and Using Semantic Networks. 
Silva J., Dias G., Guillor? S., Lopes G. 1999. Using 
localmaxs algorithm for the extraction of contiguous 
and non-contiguous multiword lexical units. 
Proceedings of 9th Portuguese Conference in 
Artificial Inteligence, 21-24. 
Smadja F. 1993. Retrieving Collocations from Text: 
Xtract. Computational Linguistics, 19(1), 143-177. 
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 142?145,
Paris, October 2009. c?2009 Association for Computational Linguistics
Application of feature propagation to dependency parsing 
Kepa Bengoetxea Koldo Gojenola 
IXA NLP Group, Technical School of Engineering, Bilbao 
University of the Basque Country, Plaza La Casilla 3, 48012, Bilbao 
kepa.bengoetxea@ehu.es, koldo.gojenola@ehu.es 
Abstract 
This paper presents a set of experiments per-
formed on parsing the Basque Dependency 
Treebank. We have applied feature propaga-
tion to dependency parsing, experimenting the 
propagation of several morphosyntactic fea-
ture values. In the experiments we have used 
the output of a parser to enrich the input of a 
second parser. Both parsers have been gener-
ated by Maltparser, a freely data-driven de-
pendency parser generator. The transforma-
tions, combined with the pseudoprojective 
graph transformation, obtain a LAS of 77.12% 
improving the best reported results for Basque. 
1 Introduction 
This work presents several experiments per-
formed on dependency parsing of the Basque 
Dependency Treebank (BDT, Aduriz et al 
2003). We have experimented the idea of feature 
propagation through dependency arcs, in order to 
help the parser. Feature propagation has been 
used in classical unification-based grammars as a 
means of propagating linguistic information 
through syntax trees. We apply this idea in the 
context of inductive dependency parsing, com-
bining a reduced set of linguistic principles that 
express feature propagation among linguistic 
elements with Maltparser (Nivre et al 2007a), an 
automatic dependency parser generator. 
We have concentrated on propagating several 
morphosyntactic feature values from: a) auxiliary 
verbs to the main verb, b) the last constituent to 
the head noun, in noun phrases c)  the last con-
junct to the conjunction, in coordination.  
This work was developed in the context of de-
pendency parsing exemplified by the CoNLL 
shared task on dependency parsing in years 2006 
and 2007 (Nivre et al 2007b), where several sys-
tems had to compete analyzing data from a typo-
logically varied range of 11 languages. The tree-
banks for all languages were standardized using 
a previously agreed CoNLL-X format (see Fig-
ure 1). BDT was one of the evaluated treebanks, 
which will allow a direct comparison of results. 
Many works on treebank parsing have dedi-
cated an effort to the task of pre-processing train-
ing trees (Nilsson et al 2007). When these ap-
proaches have been applied to dependency pars-
ing several works (Nilsson et al 2007; Ben-
goetxea and Gojenola 2009) have concentrated 
on modifying the structure of the dependency 
tree, changing the shape of the graph. In contrast, 
rather than modifying the tree structure, we will 
experiment changing the information contained 
in the nodes of the tree. This approach requires 
having an initial dependency tree in order to ap-
ply the feature propagation process, which will 
be obtained by means of a standard trained 
model. This way, the features will be propagated 
through some incorrect dependency arcs, and the 
process will be dependent on the reliability of the 
initial arcs. After enriching the tree, a second 
parsing model will try to use this new informa-
tion to improve the standard model. This process 
can also be seen as an example of stacked learn-
ing (Martins et al 2008, Nivre and McDonald 
2008) where a second parser is used to improve 
the performance of a first one. 
The rest of the paper is organized as follows. 
Section 2 presents the main resources used in this 
work. Section 3 presents three different propos-
als for the propagation of the most important 
morphological features. Next, section 4 will 
evaluate the results of each transformation, and 
the last section outlines the main conclusions. 
2 Resources 
This section will describe the main elements that 
have been used in the experiments. First, subsec-
tion 2.1 will present the Basque Dependency 
Treebank data, while subsection 2.2 will describe 
the main characteristics of Maltparser, a state of 
the art and data-driven dependency parser. 
2.1 The Basque Dependency Treebank 
The BDT can be considered a pure dependency 
treebank, as its initial design considered that all 
the dependency arcs would connect sentence to-
kens. Although this decision had consequences 
on the annotation process, its simplicity is also 
an advantage when applying several of the most 
142
efficient parsing algorithms. The treebank con-
sists of 55,469 tokens forming 3,700 sentences, 
334 of which were used as test data. 
(1) Etorri (come) dela (that-has) eta 
(and) joan  (go) dela (that-has) esan 
(tell) zien (did) mutil (boy) 
txikiak(the-little) 
He told the little boy that he has come 
and he has gone 
 
Figure 1 contains an example of a sentence 
(1), annotated in the CoNLL-X format. The text 
is organized in eight tab-separated columns: 
word-number, form, lemma, category , subcate-
gory, morphological features, and the depend-
ency relation (headword + dependency). Basque 
is an agglutinative language and it presents a 
high power to generate inflected word-forms. 
The information in Figure 1 has been simplified 
due to space reasons, as typically the Features 
column will contain many morphological fea-
tures, which are relevant for parsing. 
2.2 Maltparser 
Maltparser (Nivre et al 2007a) is a state of the 
art dependency parser that has been successfully 
applied to typologically different languages and 
treebanks. While several variants of the base 
parser have been implemented, we will use one 
of its standard versions (Maltparser version 0.4). 
The parser obtains deterministically a depend-
ency tree in linear-time in a single pass over the 
input. To determine which is the best action at 
each parsing step, the parser uses history-based 
feature models and discriminative machine learn-
ing. In all the following experiments, we made 
use of a SVM classifier. The specification of the 
features used for learning can in principle be any 
kind of data in Figure 1 (such as word-form, 
lemma, category or morphological features). 
3 Experiments  
We applied the following steps: 
a) Application of feature propagation to the 
training data, using the gold standard arcs, ob-
taining a ?enriched training data?.  
b) Training Maltparser on the ?enriched train-
ing data? to generate a ?enriched parser?.  
c) Training Maltparser with the training data, 
without any transformation, to generate a 
?standard parser?.  
d) Parse the test data with the ?standard 
parser?, obtaining the ?standard output?.  
e) Apply feature propagation to the ?standard 
output?, using the dependency arcs given by 
the parser (with some incorrect arcs), obtain-
ing the ?standard parser?s enriched output?. 
f) Finally, parsing the ?standard parser?s en-
riched output? with the ?enriched parser?, 
Index Word Lemma Category Subcategory Features  Head Dependency 
1 etorri etorri V  V  _   3 lot 
2 dela izan AUXV  AUXV  SC:CMP|SUBJ:3S 1 auxmod 
3 eta eta CONJ  CONJ  _   6 ccomp_obj 
4 joan joan V  V  _   3 lot 
5 dela izan AUXV  AUXV  SC:CMP|SUBJ:3S 4 auxmod 
6 esan esan V  V  _   0 ROOT 
7 zien *edun AUXV  AUXV  SUBJ:3S|OBJ:3P 6 auxmod 
8 mutil mutil NOUN  NOUN  _   6 ncsubj 
9 txikiak txiki ADJ  ADJ  CASE:ERG|NUM:S 8 ncmod 
10 . . PUNT  PUNT_PUNT _   9 PUNC 
 
Figure 1: Example of a BDT sentence in the CONLL-X format 
(V = main verb, AUXV = auxiliary verb, SC = subordinated clause, CMP = completive, ccomp_obj = clausal 
complement object,  SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl.). 
 
auxmod 
coord 
auxmod auxmod 
coord 
ccomp_obj 
 
 
 
 
Etorri da+la  eta joan da+la  esan zien  mutil txiki+ak  
come has+he+that and go has+he+that tell did+he+them  boy little+the 
V AUXV+3S+CMP CONJ V AUXV+3S+CMP V    AUXV+SUBJ3S+OBJ3P  NOUN ADJ+ERG 
Figure 2: Dependency tree for the sentence in Figure 1. 
(V = main verb; AUXV: auxiliary verb; CMP: completive subordinated mark; CONJ: conjunction; ERG: ergative case). 
 
ncmod 
ncsubj 
143
evaluating the output with the gold test data. 
We have applied three types of feature propa-
gation of the most important morphological fea-
ture values: a) from auxiliary verbs to the main 
verb (verb phrases) b) from post-modifiers to the 
head noun (noun phrases) c) from the last con-
junct to the conjunction (coordination). This was 
done because Basque is a head final language, 
where many relevant features are located at the 
end of constituents. Figure 3 shows (dotted lines) 
the arcs that will propagate features from child to 
parent. The three transformations will be de-
scribed in the following subsections.  
3.1 Verb compounds 
In BDT the verbal elements are organized around 
the main verb, but much syntactically relevant 
verbal information, like subordination type, as-
pect, tense and agreement usually appear at-
tached to the auxiliary verb, which is the de-
pendent. Its main consequence for parsing is that 
the elements bearing the relevant information for 
parsing are situated far in the tree with respect to 
their head. In Figure 2, we can see that the mor-
pheme ?la, indicating a subordinated completive 
sentence, appears down in the tree, and this could 
affect the correct attachment of the two coordi-
nated verbs to the conjunction (eta), as conjunc-
tions should link elements showing similar 
grammatical features (-la in this example). Simi-
larly, it could affect the decision about the de-
pendency type of eta (and) with respect to the 
main verb esan (to say), as the dependency rela-
tion ccomp_obj is defined by means of the ?la 
(completive) morpheme, far down in the tree. 
Figure 3 shows the effect of propagating the 
completive feature value (CMP) from the auxil-
iary verb to the main verb through the auxmod 
(auxiliary modifier) relation. 
3.2 Noun Phrases 
In noun phrases and postpositional phrases, the 
most important morphological feature values 
(case and number) are situated in the last post-
modifier after the noun. Figure 3 shows the ef-
fect of propagating the ergative (ERG) case fea-
ture value from the adjective (the last constituent 
of the noun phrase) to the noun through the rela-
tion ncmod (non-clausal modifier).  
3.3 Coordination 
Coordination in BDT was annotated in the so 
called Prague Style, where the conjunction is 
taken as the head, and the conjuncts depend on it. 
Basque is head final, so usually the last conjunct 
contains syntactically relevant features. We ex-
perimented the promotion of the category, case 
and subordination information from the last con-
junct to the conjunction. In the example in Figure 
3, the conjunction (eta) receives a new feature 
(HV for Head:Verb) from its dependent. This can 
be seen as an alternative to (Nilsson et al 2007) 
who transform dependency arcs. 
4 Evaluation 
Evaluation was performed dividing the treebank 
in three sets: training set (45,000 tokens), devel-
opment and test sets (5,000 tokens each). Train-
ing and testing of the system have been per-
formed on the same datasets presented at the 
CoNLL 2007 shared task, which will allow for a 
direct comparison. Table 1 presents the Labeled 
Attachment Score (LAS) of the different tests on 
development and test data. The first row presents 
the best system score (76.94% LAS) in CoNLL 
2007. This system combined six variants of a 
base parser (Maltparser). The second row shows 
the single Maltparser approach which obtained 
the fifth position. Row 3 presents Bengoetxea 
and Gojenola?s results (76.80% LAS) when ap-
plying graph transformations (pseudo-projective, 
coordination and verb groups) to Basque, in the 
spirit of Nilsson et al (2007). Row 4 shows our 
results after applying several feature optimiza-
tions, which we will use as our baseline. 
auxmod 
coord 
auxmod auxmod 
coord 
ccomp_obj  
 
 
 
 
Etorri  da+la  eta  joan  da+la  esan zien    mutil  txiki+ak  
come  has+he+that and   go  has+he+that tell did+he+them   boy    little+the 
V+CMP   AUXV+3S+CMP CONJ+HV    V+CMP AUXV+3S+CMP      V    AUXV+SUBJ3S+OBJ3P NOUN+ERG ADJ+ERG 
Figure 3: Dependency tree after propagating the morphological features. 
 
ncmod 
ncsubj 
144
Feature propagation in verb groups (PVG) im-
proves LAS in almost 0.5% (row 6 in Table 1). 
While coordination and case propagation do not 
improve significantly the accuracy by themselves 
(rows 7 and 8), their combination with PVG (verb 
groups) significantly increases LAS (+0.86%, 
see row 10). Looking at the accuracy of the de-
pendency arcs used for feature propagation, aux-
liary verbs are the most reliable elements, as 
their arcs (linking it to its head, the main verb) 
have 97% precision and 98% recall. This is in 
accord with PVG giving the biggest increase, 
while arcs related to coordination (63% precision 
and 65% recall) give a more modest contribution. 
BDT contains 2.9% of nonprojective arcs, so 
we experimented the effect of combining the 
pseudoprojective transformation (Nilsson et al 
2007) with feature propagation, obtaining a LAS 
of 77.12%, the best reported results for the BDT. 
5 Conclusions 
We have performed a set of experiments using 
the output of a parser to enrich the input of a 
second parser, propagating the relevant morpho-
logical feature values through dependency arcs. 
The best system, after applying three types of 
feature propagation, obtains a 77.12% LAS 
(2.05% improvement over the baseline) on the 
test set, which is the best reported result for 
Basque dependency parsing, improving the better 
published result for a combined parser (76.94%). 
Acknowledgements 
This research was supported by the Basque Gov-
ernment (EPEC-RS, S-PE08UN48) and the Uni-
versity of the Basque Country (EHU-EJIE, 
EJIE07/05).  
References  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque dependency 
treebank. Treebanks and Linguistic Theories. 
Kepa Bengoetxea and Koldo Gojenola. 2009. Explor-
ing Treebank Transformations in Dependency 
Parsing. Proceedings of RANLP?2009. 
Johan Hall, Jens Nilsson, Joakim Nivre J., Eryigit G., 
Megyesi B., Nilsson M. and Saers M. 2007. Single 
Malt or Blended? A Study in Multilingual 
Parser Optimization. Proceedings of the CoNLL 
Shared Task EMNLP-CoNLL. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, 
Eric P. Xing. 2008. Stacking Dependency Pars-
ing. EMNLP-2008. 
Jens Nilsson, Joakim Nivre and Johan Hall. 2007. 
Tree Transformations for Inductive Depend-
ency Parsing. Proceedings of the 45th ACL. 
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A., 
G?lsen Eryi?it, Sandra K?bler, Marinov S., and 
Edwin Marsi. 2007a. MaltParser: A language-
independent system for data-driven depend-
ency parsing. Natural Language Engineering.  
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared 
Task on Dependency Parsing. EMNLP-CoNLL. 
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graphbased and transition-based depend-
ency parsers. ACL-2008. 
  LAS 
 System Development Test 
1 Nivre et al 2007b (CoNLL 2007) -  76.94%  
2 Hall et al 2007 (CoNLL 2007)   74.99%  
3 Bengoetxea and Gojenola 2009   76.80%  
4 Feature optimization (baseline) 77.46%  75.07%  
5 Proj 78.16%  (+0.70) *75.99%  (+0.92) 
6 PVG 78.14%  (+0.68) 75.54%  (+0.47) 
7 PCOOR 77.36%  (-0.10) 75.22%  (+0.15) 
8 PCAS 77.32%  (-0.14) 74.86%  (-0.21) 
9 PVG + PCAS  78.53%  (+1.09) 75.42%  (+0.35) 
10 PCOOR + PVG + PCAS  78.31%  (+0.85) *75.93%  (+0.86) 
11 PCOOR + PVG 78.25%  (+0.79) *75.93%  (+0.86) 
12 Proj + PVG  78.91%  (+1.45) *76.12%  (+1.05) 
13 Proj + PVG + PCOOR  + PCAS 78.31%  (+0.85) *77.12%  (+2.05) 
Table 1. Evaluation results  
(Proj: Pseudo-projective, PVG, PCAS, PCOOR: Propagation on verb compounds, case (NPs)  and coordination; *: statistically 
significant in McNemar's test with respect to labeled attachment score with p < 0.01) 
 
145
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 699?703,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Improving Dependency Parsing with Semantic Classes 
Eneko Agirre*, Kepa Bengoetxea*, Koldo Gojenola*, Joakim Nivre+ 
* Department of Computer Languages and Systems, University of the Basque Country 
UPV/EHU 
+ Department of Linguistics and Philosophy, Uppsala University 
{e.agirre, kepa.bengoetxea, koldo.gojenola}@ehu.es joakim.nivre@lingfil.uu.se 
 
 
 
 
Abstract  
This paper presents the introduction of 
WordNet semantic classes in a dependency 
parser, obtaining improvements on the full 
Penn Treebank for the first time. We tried 
different combinations of some basic se-
mantic classes and word sense disambigua-
tion algorithms. Our experiments show that 
selecting the adequate combination of se-
mantic features on development data is key 
for success. Given the basic nature of the 
semantic classes and word sense disam-
biguation algorithms used, we think there is 
ample room for future improvements. 
1 Introduction 
Using semantic information to improve parsing 
performance has been an interesting research ave-
nue since the early days of NLP, and several re-
search works have tried to test the intuition that 
semantics should help parsing, as can be exempli-
fied by the classical PP attachment experiments 
(Ratnaparkhi, 1994). Although there have been 
some significant results (see Section 2), this issue 
continues to be elusive. In principle, dependency 
parsing offers good prospects for experimenting 
with word-to-word-semantic relationships. 
We present a set of experiments using semantic 
classes in dependency parsing of the Penn Tree-
bank (PTB). We extend the tests made in Agirre et 
al. (2008), who used different types of semantic 
information, obtaining significant improvements in 
two constituency parsers, showing how semantic 
information helps in constituency parsing.  
As our baseline parser, we use MaltParser 
(Nivre, 2006). We will evaluate the parser on both 
the full PTB (Marcus et al 1993) and on a sense-
annotated subset of the Brown Corpus portion of 
PTB, in order to investigate the upper bound per-
formance of the models given gold-standard sense 
information, as in Agirre et al (2008). 
2 Related Work 
Agirre et al (2008) trained two state-of-the-art sta-
tistical parsers (Charniak, 2000; Bikel, 2004) on 
semantically-enriched input, where content words 
had been substituted with their semantic classes. 
This was done trying to overcome the limitations 
of lexicalized approaches to parsing (Magerman, 
1995; Collins, 1996; Charniak, 1997; Collins, 
2003), where related words, like scissors and knife 
cannot be generalized. This simple method allowed 
incorporating lexical semantic information into the 
parser. They tested the parsers in both a full pars-
ing and a PP attachment context. The experiments 
showed that semantic classes gave significant im-
provement relative to the baseline, demonstrating 
that a simplistic approach to incorporating lexical 
semantics into a parser significantly improves its 
performance. This work presented the first results 
over both WordNet and the Penn Treebank to show 
that semantic processing helps parsing.  
Collins (2000) tested a combined parsing/word 
sense disambiguation model based in WordNet 
which did not obtain improvements in parsing. 
Koo et al (2008) presented a semisupervised 
method for training dependency parsers, using 
word clusters derived from a large unannotated 
corpus as features. They demonstrate the effective-
ness of the approach in a series of dependency 
parsing experiments on PTB and the Prague De-
pendency Treebank, showing that the cluster-based 
features yield substantial gains in performance 
across a wide range of conditions. Suzuki et al 
(2009) also experiment with the same method 
combined with semi-supervised learning. 
699
Ciaramita and Attardi (2007) show that adding 
semantic features extracted by a named entity tag-
ger (such as PERSON or MONEY) improves the 
accuracy of a dependency parser, yielding a 5.8% 
relative error reduction on the full PTB. 
Candito and Seddah (2010) performed experi-
ments in statistical parsing of French, where termi-
nal forms were replaced by more general symbols, 
particularly clusters of words obtained through 
unsupervised clustering. The results showed that 
word clusters had a positive effect. 
Regarding dependency parsing of the English 
PTB, currently Koo and Collins (2010) and Zhang 
and Nivre (2011) hold the best results, with 93.0 
and 92.9 unlabeled attachment score, respectively. 
Both works used the Penn2Malt constituency-to-
dependency converter, while we will make use of 
PennConverter (Johansson and Nugues, 2007). 
Apart from these, there have been other attempts 
to make use of semantic information in different 
frameworks and languages, as in (Hektoen 1997; 
Xiong et al 2005; Fujita et al 2007). 
3 Experimental Framework 
In this section we will briefly describe the data-
driven parser used for the experiments (subsection 
3.1), followed by the PTB-based datasets (subsec-
tion 3.2). Finally, we will describe the types of se-
mantic representation used in the experiments. 
3.1 MaltParser 
MaltParser (Nivre et al 2006) is a trainable de-
pendency parser that has been successfully applied 
to typologically different languages and treebanks. 
We will use one of its standard versions (version 
1.4). The parser obtains deterministically a de-
pendency tree in linear-time in a single pass over 
the input using two main data structures: a stack of 
partially analyzed items and the remaining input 
sequence. To determine the best action at each 
step, the parser uses history-based feature models 
and SVM classifiers. One of the main reasons for 
using MaltParser for our experiments is that it eas-
ily allows the introduction of semantic informa-
tion, adding new features, and incorporating them 
in the training model. 
3.2 Dataset 
We used two different datasets: the full PTB and 
the Semcor/PTB intersection (Agirre et al 2008). 
The full PTB allows for comparison with the state-
of-the-art, and we followed the usual train-test 
split. The Semcor/PTB intersection contains both 
gold-standard sense and parse tree annotations, and 
allows to set an upper bound of the relative impact 
of a given semantic representation on parsing. We 
use the same train-test split of Agirre et al (2008), 
with a total of 8,669 sentences containing 151,928 
words partitioned into 3 sets: 80% training, 10% 
development and 10% test data. This dataset is 
available on request to the research community. 
We will evaluate the parser via Labeled Attach-
ment Score (LAS). We will use Bikel?s random-
ized parsing evaluation comparator to test the 
statistical significance of the results using word 
sense information, relative to the respective base-
line parser using only standard features.  
We used PennConverter (Johansson and 
Nugues, 2007) to convert constituent trees in the 
Penn Treebank annotation style into dependency 
trees. Although in general the results from parsing 
Pennconverter?s output are lower than with other 
conversions, Johansson and Nugues (2007) claim 
that this conversion is better suited for semantic 
processing, with a richer structure and a more fine-
grained set of dependency labels. For the experi-
ments, we used the best configuration for English 
at the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007) as our baseline.  
3.3 Semantic representation and disambigua-
tion methods 
We will experiment with the range of semantic 
representations used in Agirre et al (2008), all of 
which are based on WordNet 2.1. Words in Word-
Net (Fellbaum, 1998) are organized into sets of 
synonyms, called synsets (SS). Each synset in turn 
belongs to a unique semantic file (SF). There are a 
total of 45 SFs (1 for adverbs, 3 for adjectives, 15 
for verbs, and 26 for nouns), based on syntactic 
and semantic categories. For example, noun se-
mantic files (SF_N) differentiate nouns denoting 
acts or actions, and nouns denoting animals, 
among others. We experiment with both full syn-
sets and SFs as instances of fine-grained and 
coarse-grained semantic representation, respec-
tively. As an example of the difference in these 
two representations, knife in its tool sense is in the 
EDGE TOOL USED AS A CUTTING 
INSTRUMENT singleton synset, and also in the 
ARTIFACT SF along with thousands of other 
700
words including cutter. Note that these are the two 
extremes of semantic granularity in WordNet. 
As a hybrid representation, we also tested the ef-
fect of merging words with their corresponding SF 
(e.g. knife+ARTIFACT). This is a form of seman-
tic specialization rather than generalization, and 
allows the parser to discriminate between the dif-
ferent senses of each word, but not generalize 
across words. For each of these three semantic rep-
resentations, we experimented with using each of: 
(1) all open-class POSs (nouns, verbs, adjectives 
and adverbs), (2) nouns only, and (3) verbs only. 
There are thus a total of 9 combinations of repre-
sentation type and target POS: SS (synset), SS_N 
(noun synsets), SS_V (verb synsets), SF (semantic 
file), SF_N (noun semantic files), SF_V (verb se-
mantic files), WSF (wordform+SF), WSF_N 
(wordform+SF for nouns) and WSF_V (for verbs).  
For a given semantic representation, we need 
some form of WSD to determine the semantics of 
each token occurrence of a target word. We ex-
perimented with three options: a) gold-standard 
(GOLD) annotations from SemCor, which gives 
the upper bound performance of the semantic rep-
resentation, b) first Sense (1ST), where all token 
instances of a given word are tagged with their 
most frequent sense in WordNet, and c) automatic 
Sense Ranking (ASR) which uses the sense re-
turned by an unsupervised system based on an in-
dependent corpus (McCarthy et al 2004). For the 
full Penn Treebank experiments, we only had ac-
cess to the first sense, taken from Wordnet 1.7. 
4 Results 
In the following two subsections, we will first pre-
sent the results in the SemCor/PTB intersection, 
with the option of using gold, 1st sense and auto-
matic sense information (subsection 4.1) and the 
next subsection (4.2) will show the results on the 
full PTB, using 1st sense information. All results 
are shown as labelled attachment score (LAS). 
4.1 Semcor/PTB (GOLD/1ST/ASR) 
We conducted a series of experiments testing: 
? Each individual semantic feature, which 
gives 9 possibilities, also testing different 
learning configurations for each one. 
? Combinations of semantic features, for in-
stance, SF+SS_N+WSF would combine the 
semantic file with noun synsets and word-
form+semantic file. 
Although there were hundreds of combinations, 
we took the best combination of semantic features 
on the development set for the final test. For that 
reason, the table only presents 10 results for each 
disambiguation method, 9 for the individual fea-
tures and one for the best combination. 
Table 1 presents the results obtained for each of 
the disambiguation methods (gold standard sense 
information, 1st sense, and automatic sense rank-
ing) and individual semantic feature. In all cases 
except two, the use of semantic classes is benefi-
 System            LAS 
Baseline  81.10  
SS 81.18 +0.08 
SS_N 81.40 +0.30 
SS_V *81.58 +0.48 
SF **82.05 +0.95 
SF_N
 81.51 +0.41 
SF_V 81.51 +0.41 
WSF 81.51 +0.41 
WSF_N 81.43 +0.33 
WSF_V *81.51 +0.41 
 
 
Gold 
SF+SF_N+SF_V+SS+WSF_N *81.74 +0.64 
SS 81.30 +0.20 
SS_N *81.56 +0.46 
SS_V *81.49 +0.39 
SF 81.00 -0.10 
SF_N
 80.97 -0.13 
SF_V **81.66 +0.56 
WSF 81.32 +0.22 
WSF_N *81.62 +0.52 
WSF_V **81.72 +0.62 
 
 
ASR 
SF_V+SS_V 81.41 +0.31 
SS 81.40 +0.30 
SS_N 81.39 +0.29 
SS_V *81.48 +0.38 
SF *81.59 +0.49 
SF_N
 81.38 +0.28 
SF_V *81.52 +0.42 
WSF *81.57 +0.46 
WSF_N 81.40 +0.30 
WSF_V 81.42 +0.32 
 
 
1ST 
SF+SS_V+WSF_N **81.92 +0.81 
Table 1. Evaluation results on the test set for the 
Semcor-Penn intersection. Individual semantic 
features and best combination. 
(**: statistically significant, p < 0.005; *: p < 0.05) 
 
701
cial albeit small. Regarding individual features, the 
SF feature using GOLD senses gives the best im-
provement. However, GOLD does not seem to 
clearly improve over 1ST and ASR on the rest of 
the features. Comparing the automatically obtained 
classes, 1ST and ASR, there is no evident clue 
about one of them being superior to the other. 
Regarding the best combination as selected in 
the training data, each WSD method yields a dif-
ferent combination, with best results for 1ST. The 
improvement is statistically significant for both 
1ST and GOLD. In general, the results in Table 1 
do not show any winning feature across all WSD 
algorithms. The best results are obtained when us-
ing the first sense heuristic, but the difference is 
not statistically significant. This shows that perfect 
WSD is not needed to obtain improvements, but it 
also shows that we reached the upperbound of our 
generalization and learning method. 
4.2 Penn Treebank and 1st sense 
We only had 1st sense information available for 
the full PTB. We tested MaltParser on the best 
configuration obtained for the reduced Sem-
cor/PTB on the full treebank, taking sections 2-21 
for training and section 23 for the final test. Table 
2 presents the results, showing that several of the 
individual features and the best combination give 
significant improvements. To our knowledge, this 
is the first time that WordNet semantic classes help 
to obtain improvements on the full Penn Treebank. 
It is interesting to mention that, although not 
shown on the tables, using lemmatization to assign 
semantic classes to wordforms gave a slight in-
crease for all the tests (0.1 absolute point approxi-
mately), as it helped to avoid data sparseness. We 
applied Schmid?s (1994) TreeTagger. This can be 
seen as an argument in favour of performing mor-
phological analysis, an aspect that is many times 
neglected when processing morphologically poor 
languages as English. 
We also did some preliminary experiments us-
ing Koo et al?s (2008) word clusters, both inde-
pendently and also combined with the WordNet-
based features, without noticeable improvements. 
5 Conclusions 
We tested the inclusion of several types of seman-
tic information, in the form of WordNet semantic 
classes in a dependency parser, showing that: 
? Semantic information gives an improvement 
on a transition-based deterministic depend-
ency parsing. 
? Feature combinations give an improvement 
over using a single feature. Agirre et al 
(2008) used a simple method of substituting 
wordforms with semantic information, 
which only allowed using a single semantic 
feature. MaltParser allows the combination 
of several semantic features together with 
other features such as wordform, lemma or 
part of speech. Although tables 1 and 2 only 
show the best combination for each type of 
semantic information, this can be appreci-
ated on GOLD and 1ST in Table 1. Due to 
space reasons, we only have showed the best 
combination, but we can say that in general 
combining features gives significant in-
creases over using a single semantic feature. 
? The present work presents a statistically sig-
nificant improvement for the full treebank 
using WordNet-based semantic information 
for the first time. Our results extend those of 
Agirre et al (2008), which showed im-
provements on a subset of the PTB. 
Given the basic nature of the semantic classes 
and WSD algorithms, we think there is room for 
future improvements, incorporating new kinds of 
semantic information, such as WordNet base con-
cepts, Wikipedia concepts, or similarity measures. 
 
 System            LAS 
Baseline  86.27  
SS *86.53 +0.26 
SS_N 86.33 +0.06 
SS_V *86.48 +0.21 
SF **86.63 +0.36 
SF_N
 *86.56 +0.29 
SF_V 86.34 +0.07 
WSF *86.50 +0.23 
WSF_N 86.25 -0.02 
WSF_V *86.51 +0.24 
 
 
1ST 
SF+SS_V+WSF_N *86.60 +0.33 
 
Table 1. Evaluation results (LAS) on the test 
set for the full PTB. Individual features and 
best combination. 
(**: statistically, p < 0.005; *: p < 0.05) 
 
702
References  
Eneko Agirre, Timothy Baldwin, and David Martinez. 
2008. Improving parsing and PP attachment perform-
ance with sense information. In Proceedings of ACL-
08: HLT, pages 317?325, Columbus, Ohio. 
Daniel M. Bikel. 2004. Intricacies of Collins? parsing 
model. Computational Linguistics, 30(4):479?511. 
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In Proceedings of the NAACL HLT 2010 First 
Workshop on Statistical Parsing of Morphologically-
Rich Language, Los Angeles, USA. 
M. Ciaramita and G. Attardi. 2007. Dependency Parsing 
with Second-Order Feature Maps and Annotated Se-
mantic Information, In Proceedings of the 10th In-
ternational Conference on Parsing Technology.  
Eugene Charniak. 1997. Statistical parsing with a con-
text-free grammar and word statistics. In Proc. of the 
15th Annual Conference on Artificial Intelligence 
(AAAI-97), pages 598?603, Stanford, USA. 
Eugene Charniak. 2000. A maximum entropy-based 
parser. In Proc. of the 1st Annual Meeting of the 
North American Chapter of Association for Compu-
tational Linguistics (NAACL2000), Seattle, USA. 
Michael J. Collins. 1996. A new statistical parser based 
on lexical dependencies. In Proc. of the 34th Annual 
Meeting of the ACL, pages 184?91, USA. 
Michael Collins. 2000. A Statistical Model for Parsing 
and Word-Sense Disambiguation. Proceedings of the 
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora. 
Michael Collins. 2003. Head-driven statistical models 
for natural language parsing. Computational Linguis-
tics, 29(4):589?637. 
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge. 
Sanae Fujita, Francis Bond, Stephan Oepen, and Taka-
aki Tanaka. 2007. Exploiting semantic information 
for HPSG parse selection. In Proc. of the ACL 2007 
Workshop on Deep Linguistic Processing. 
Richard Johansson and Pierre Nugues. 2007. Extended 
Constituent-to-dependency Conversion for English. 
In Proceedings of NODALIDA 2007, Tartu, Estonia. 
Erik Hektoen. 1997. Probabilistic parse selection based 
on semantic cooccurrences. In Proc. of the 5th Inter-
national Workshop on Parsing Technologies. 
Terry Koo, Xavier Carreras, and Michael Collins. 2008. 
Simple semi-supervised dependency parsing. In Pro-
ceedings of ACL-08, pages 595?603, USA. 
Terry Koo, and Michael Collins. 2008. Efficient Third-
order Dependency Parsers. In Proceedings of ACL-
2010, pages 1?11, Uppsala, Sweden. 
Shari Landes, Claudia Leacock, and Randee I. Tengi. 
1998. Building semantic concordances. In Christiane 
Fellbaum, editor, WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, USA. 
David M. Magerman. 1995. Statistical decision-tree 
models for parsing. In Proc. of the 33rd Annual 
Meeting of the ACL, pages 276?83, USA. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated 
corpus of English: the Penn treebank. Computational 
Linguistics, 19(2):313?30. 
Diana McCarthy, Rob Koeling, Julie Weeds, and John 
Carroll. 2004. Finding predominant senses in 
untagged text. In Proc. of the 42nd Annual Meeting 
of the ACL, pages 280?7, Barcelona, Spain. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Text, Speech and Language Technology series, 
Springer. 2006, XI, ISBN: 978-1-4020-4888-3. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. Prague, Czech Republic. 
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos. 
1994. A maximum entropy model for prepositional 
phrase attachment. In HLT ?94: Proceedings of the 
Workshop on Human Language Technology, USA. 
Helmut Schmid. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. September 1994 
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Mi-
chael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for De-
pendency Parsing. In Proceedings of EMNLP, pages 
551?560. Association for Computational Linguistics. 
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and 
Yueliang Qian. 2005. Parsing the Penn Chinese 
Treebank with semantic knowledge. In Proc. of the 
2nd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05), Korea. 
Yue Zhang, and Joakim Nivre. 2011. Transition-Based 
Parsing with Rich Non-Local Features. In Proceed-
ings of the 49th Annual Meeting of the Association 
for Computational Linguistics. 
703
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 649?655,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
On WordNet Semantic Classes and Dependency Parsing
Kepa Bengoetxea?, Eneko Agirre?, Joakim Nivre?,
Yue Zhang*, Koldo Gojenola?
?University of the Basque Country UPV/EHU / IXA NLP Group
?Uppsala University / Department of Linguistics and Philology
? Singapore University of Technology and Design
kepa.bengoetxea@ehu.es, e.agirre@ehu.es,
joakim.nivre@lingfil.uu.se, yue zhang@sutd.edu.sg,
koldo.gojenola@ehu.es
Abstract
This paper presents experiments with
WordNet semantic classes to improve de-
pendency parsing. We study the effect
of semantic classes in three dependency
parsers, using two types of constituency-
to-dependency conversions of the English
Penn Treebank. Overall, we can say that
the improvements are small and not sig-
nificant using automatic POS tags, con-
trary to previously published results using
gold POS tags (Agirre et al, 2011). In
addition, we explore parser combinations,
showing that the semantically enhanced
parsers yield a small significant gain only
on the more semantically oriented LTH
treebank conversion.
1 Introduction
This work presents a set of experiments to investi-
gate the use of lexical semantic information in de-
pendency parsing of English. Whether semantics
improve parsing is one interesting research topic
both on parsing and lexical semantics. Broadly
speaking, we can classify the methods to incor-
porate semantic information into parsers in two:
systems using static lexical semantic repositories,
such as WordNet or similar ontologies (Agirre et
al., 2008; Agirre et al, 2011; Fujita et al, 2010),
and systems using dynamic semantic clusters au-
tomatically acquired from corpora (Koo et al,
2008; Suzuki et al, 2009).
Our main objective will be to determine
whether static semantic knowledge can help pars-
ing. We will apply different types of semantic in-
formation to three dependency parsers. Specifi-
cally, we will test the following questions:
? Does semantic information in WordNet help
dependency parsing? Agirre et al (2011)
found improvements in dependency parsing
using MaltParser on gold POS tags. In this
work, we will investigate the effect of seman-
tic information using predicted POS tags.
? Is the type of semantic information related
to the type of parser? We will test three
different parsers representative of successful
paradigms in dependency parsing.
? How does the semantic information relate to
the style of dependency annotation? Most ex-
periments for English were evaluated on the
Penn2Malt conversion of the constituency-
based Penn Treebank. We will also examine
the LTH conversion, with richer structure and
an extended set of dependency labels.
? How does WordNet compare to automati-
cally obtained information? For the sake of
comparison, we will also perform the experi-
ments using syntactic/semantic clusters auto-
matically acquired from corpora.
? Does parser combination benefit from seman-
tic information? Different parsers can use se-
mantic information in diverse ways. For ex-
ample, while MaltParser can use the semantic
information in local contexts, MST can in-
corporate them in global contexts. We will
run parser combination experiments with and
without semantic information, to determine
whether it is useful in the combined parsers.
After introducing related work in section 2, sec-
tion 3 describes the treebank conversions, parsers
and semantic features. Section 4 presents the re-
sults and section 5 draws the main conclusions.
2 Related work
Broadly speaking, we can classify the attempts to
add external knowledge to a parser in two sets:
using large semantic repositories such as Word-
Net and approaches that use information automat-
ically acquired from corpora. In the first group,
Agirre et al (2008) trained two state-of-the-art
constituency-based statistical parsers (Charniak,
649
2000; Bikel, 2004) on semantically-enriched in-
put, substituting content words with their seman-
tic classes, trying to overcome the limitations of
lexicalized approaches to parsing (Collins, 2003)
where related words, like scissors and knife, can-
not be generalized. The results showed a signi-
cant improvement, giving the first results over both
WordNet and the Penn Treebank (PTB) to show
that semantics helps parsing. Later, Agirre et al
(2011) successfully introduced WordNet classes in
a dependency parser, obtaining improvements on
the full PTB using gold POS tags, trying different
combinations of semantic classes. MacKinlay et
al. (2012) investigate the addition of semantic an-
notations in the form of word sense hypernyms, in
HPSG parse ranking, reducing error rate in depen-
dency F-score by 1%, while some methods pro-
duce substantial decreases in performance. Fu-
jita et al (2010) showed that fully disambiguated
sense-based features smoothed using ontological
information are effective for parse selection.
On the second group, Koo et al (2008) pre-
sented a semisupervised method for training de-
pendency parsers, introducing features that incor-
porate word clusters automatically acquired from
a large unannotated corpus. The clusters include
strongly semantic associations like {apple, pear}
or {Apple, IBM} and also syntactic clusters like
{of, in}. They demonstrated its effectiveness in
dependency parsing experiments on the PTB and
the Prague Dependency Treebank. Suzuki et al
(2009), Sagae and Gordon (2009) and Candito
and Seddah (2010) also experiment with the same
cluster method. Recently, T?ackstr?om et al (2012)
tested the incorporation of cluster features from
unlabeled corpora in a multilingual setting, giving
an algorithm for inducing cross-lingual clusters.
3 Experimental Framework
In this section we will briefly describe the PTB-
based datasets (subsection 3.1), followed by the
data-driven parsers used for the experiments (sub-
section 3.2). Finally, we will describe the different
types of semantic representation that were used.
3.1 Treebank conversions
Penn2Malt
1
performs a simple and direct conver-
sion from the constituency-based PTB to a depen-
dency treebank. It obtains projective trees and has
been used in several works, which allows us to
1
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
compare our results with related experiments (Koo
et al, 2008; Suzuki et al, 2009; Koo and Collins,
2010). We extracted dependencies using standard
head rules (Yamada and Matsumoto, 2003), and a
reduced set of 12 general dependency tags.
LTH
2
(Johansson and Nugues, 2007) presents
a conversion better suited for semantic process-
ing, with a richer structure and a more fine-grained
set of dependency labels (42 different dependency
labels), including links to handle long-distance
phenomena, giving a 6.17% of nonprojective sen-
tences. The results from parsing the LTH output
are lower than those for Penn2Malt conversions.
3.2 Parsers
We have made use of three parsers representative
of successful paradigms in dependency parsing.
MaltParser (Nivre et al, 2007) is a determinis-
tic transition-based dependency parser that obtains
a dependency tree in linear-time in a single pass
over the input using a stack of partially analyzed
items and the remaining input sequence, by means
of history-based feature models. We added two
features that inspect the semantic feature at the top
of the stack and the next input token.
MST
3
represents global, exhaustive graph-
based parsing (McDonald et al, 2005; McDon-
ald et al, 2006) that finds the highest scoring di-
rected spanning tree in a graph. The learning pro-
cedure is global since model parameters are set
relative to classifying the entire dependency graph,
in contrast to the local but richer contexts used
by transition-based parsers. The system can be
trained using first or second order models. The
second order projective algorithm performed best
on both conversions, and we used it in the rest of
the evaluations. We modified the system in or-
der to add semantic features, combining them with
wordforms and POS tags, on the parent and child
nodes of each arc.
ZPar
4
(Zhang and Clark, 2008; Zhang and
Nivre, 2011) performs transition-based depen-
dency parsing with a stack of partial analysis
and a queue of remaining inputs. In contrast to
MaltParser (local model and greedy deterministic
search) ZPar applies global discriminative learn-
ing and beam search. We extend the feature set of
ZPar to include semantic features. Each set of se-
mantic information is represented by two atomic
2
http://nlp.cs.lth.se/software/treebank converter
3
http://mstparser.sourceforge.net
4
www.sourceforge.net/projects/zpar
650
Base WordNet WordNet Clusters
line SF SS
Malt 88.46 88.49 (+0.03) 88.42 (-0.04) 88.59 (+0.13)
MST 90.55 90.70 (+0.15) 90.47 (-0.08) 90.88 (+0.33)?
ZPar 91.52 91.65 (+0.13) 91.70 (+0.18)? 91.74 (+0.22)
Table 1: LAS results with several parsing algo-
rithms, Penn2Malt conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
feature templates, associated with the top of the
stack and the head of the queue, respectively. ZPar
was directly trained on the Penn2Malt conversion,
while we applied the pseudo-projective transfor-
mation (Nilsson et al, 2008) on LTH, in order to
deal with non-projective arcs.
3.3 Semantic information
Our aim was to experiment with different types of
WordNet-related semantic information. For com-
parison with automatically acquired information,
we will also experiment with bit clusters.
WordNet. We will experiment with the seman-
tic representations used in Agirre et al (2008) and
Agirre et al (2011), based on WordNet 2.1. Word-
Net is organized into sets of synonyms, called
synsets (SS). Each synset in turn belongs to a
unique semantic file (SF). There are a total of 45
SFs (1 for adverbs, 3 for adjectives, 15 for verbs,
and 26 for nouns), based on syntactic and seman-
tic categories. For example, noun SFs differen-
tiate nouns denoting acts or actions, and nouns
denoting animals, among others. We experiment
with both full SSs and SFs as instances of fine-
grained and coarse-grained semantic representa-
tion, respectively. As an example, knife in its
tool sense is in the EDGE TOOL USED AS A
CUTTING INSTRUMENT singleton synset, and
also in the ARTIFACT SF along with thousands
of words including cutter. These are the two ex-
tremes of semantic granularity in WordNet. For
each semantic representation, we need to deter-
mine the semantics of each occurrence of a target
word. Agirre et al (2011) used i) gold-standard
annotations from SemCor, a subset of the PTB, to
give an upper bound performance of the semantic
representation, ii) first sense, where all instances
of a word were tagged with their most frequent
sense, and iii) automatic sense ranking, predicting
the most frequent sense for each word (McCarthy
et al, 2004). As we will make use of the full PTB,
we only have access to the first sense information.
Clusters. Koo et al (2008) describe a semi-
Base WordNet WordNet Clusters
line SF SS
Malt 84.95 85.12 (+0.17) 85.08 (+0.16) 85.13 (+0.18)
MST 85.06 85.35 (+0.29)? 84.99 (-0.07) 86.18 (+1.12)?
ZPar 89.15 89.33 (+0.18) 89.19 (+0.04) 89.17 (+0.02)
Table 2: LAS results with several parsing algo-
rithms in the LTH conversion (?: p <0.05, ?: p
<0.005). In parenthesis, difference with baseline.
supervised approach that makes use of cluster fea-
tures induced from unlabeled data, providing sig-
nificant performance improvements for supervised
dependency parsers on the Penn Treebank for En-
glish and the Prague Dependency Treebank for
Czech. The process defines a hierarchical cluster-
ing of the words, which can be represented as a
binary tree where each node is associated to a bit-
string, from the more general (root of the tree) to
the more specific (leaves). Using prefixes of vari-
ous lengths, it can produce clusterings of different
granularities. It can be seen as a representation of
syntactic-semantic information acquired from cor-
pora. They use short strings of 4-6 bits to represent
parts of speech and the full strings for wordforms.
4 Results
In all the experiments we employed a baseline fea-
ture set using word forms and parts of speech, and
an enriched feature set (WordNet or clusters). We
firstly tested the addition of each individual se-
mantic feature to each parser, evaluating its contri-
bution to the parser?s performance. For the combi-
nations, instead of feature-engineering each parser
with the wide array of different possibilities for
features, as in Agirre et al (2011), we adopted
the simpler approach of combining the outputs of
the individual parsers by voting (Sagae and Lavie,
2006). We will use Labeled Attachment Score
(LAS) as our main evaluation criteria. As in pre-
vious work, we exclude punctuation marks. For
all the tests, we used a perceptron POS-tagger
(Collins, 2002), trained on WSJ sections 2?21, to
assign POS tags automatically to both the training
(using 10-way jackknifing) and test data, obtaining
a POS tagging accuracy of 97.32% on the test data.
We will make use of Bikel?s randomized parsing
evaluation comparator to test the statistical signi-
cance of the results. In all of the experiments the
parsers were trained on sections 2-21 of the PTB
and evaluated on the development set (section 22).
Finally, the best performing system was evaluated
on the test set (section 23).
651
Parsers LAS UAS
Best baseline (ZPar) 91.52 92.57
Best single parser (ZPar + Clusters) 91.74 (+0.22) 92.63
Best combination (3 baseline parsers) 91.90 (+0.38) 93.01
Best combination of 3 parsers:
3 baselines + 3 SF extensions 91.93 (+0.41) 92.95
Best combination of 3 parsers:
3 baselines + 3 SS extensions 91.87 (+0.35) 92.92
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 91.90 (+0.38) 92.90
Table 3: Parser combinations on Penn2Malt.
Parsers LAS UAS
Best baseline (ZPar) 89.15 91.81
Best single parser (ZPar + SF) 89.33 (+0.15) 92.01
Best combination (3 baseline parsers) 89.15 (+0.00) 91.81
Best combination of 3 parsers:
3 baselines + 3 SF extensions 89.56 (+0.41)? 92.23
Best combination of 3 parsers:
3 baselines + 3 SS extensions 89.43 (+0.28) 93.12
Best combination of 3 parsers:
3 baselines + 3 cluster extensions 89.52 (+0.37)? 92.19
Table 4: Parser combinations on LTH (?: p <0.05,
?: p <0.005).
4.1 Single Parsers
We run a series of experiments testing each indi-
vidual semantic feature, also trying different learn-
ing configurations for each one. Regarding the
WordNet information, there were 2 different fea-
tures to experiment with (SF and SS). For the bit
clusters, there are different possibilities, depend-
ing on the number of bits used. For Malt and MST,
all the different lengths of bit strings were used.
Given the computational requirements and the pre-
vious results on Malt and MST, we only tested all
bits in ZPar. Tables 1 and 2 show the results.
Penn2Malt. Table 1 shows that the only signifi-
cant increase over the baseline is for ZPar with SS
and for MST with clusters.
LTH. Looking at table 2, we can say that the dif-
ferences in baseline parser performance are accen-
tuated when using the LTH treebank conversion,
as ZPar clearly outperforms the other two parsers
by more than 4 absolute points. We can see that
SF helps all parsers, although it is only significant
for MST. Bit clusters improve significantly MST,
with the highest increase across the table.
Overall, we see that the small improvements
do not confirm the previous results on Penn2Malt,
MaltParser and gold POS tags. We can also con-
clude that automatically acquired clusters are spe-
cially effective with the MST parser in both tree-
bank conversions, which suggests that the type of
semantic information has a direct relation to the
parsing algorithm. Section 4.3 will look at the de-
tails by each knowledge type.
4.2 Combinations
Subsection 4.1 presented the results of the base al-
gorithms and their extensions based on semantic
features. Sagae and Lavie (2006) report improve-
ments over the best single parser when combining
three transition-based models and one graph-based
model. The same technique was also used by the
winning team of the CoNLL 2007 Shared Task
(Hall et al, 2007), combining six transition-based
parsers. We used MaltBlender
5
, a tool for merging
the output of several dependency parsers, using the
Chu-Liu/Edmonds directed MST algorithm. After
several tests we noticed that weighted voting by
each parser?s labeled accuracy gave good results,
using it in the rest of the experiments. We trained
different types of combination:
? Base algorithms. This set includes the 3 base-
line algorithms, MaltParser, MST, and ZPar.
? Extended parsers, adding semantic informa-
tion to the baselines. We include the three
base algorithms and their semantic exten-
sions (SF, SS, and clusters). It is known (Sur-
deanu and Manning, 2010) that adding more
parsers to an ensemble usually improves ac-
curacy, as long as they add to the diver-
sity (and almost regardless of their accuracy
level). So, for the comparison to be fair, we
will compare ensembles of 3 parsers, taken
from sets of 6 parsers (3 baselines + 3 SF,
SS, and cluster extensions, respectively).
In each experiment, we took the best combina-
tion of individual parsers on the development set
for the final test. Tables 3 and 4 show the results.
Penn2Malt. Table 3 shows that the combina-
tion of the baselines, without any semantic infor-
mation, considerably improves the best baseline.
Adding semantics does not give a noticeable in-
crease with respect to combining the baselines.
LTH (table 4). Combining the 3 baselines does
not give an improvement over the best baseline, as
ZPar clearly outperforms the other parsers. How-
ever, adding the semantic parsers gives an increase
with respect to the best single parser (ZPar + SF),
which is small but significant for SF and clusters.
4.3 Analysis
In this section we analyze the data trying to under-
stand where and how semantic information helps
most. One of the obstacles of automatic parsers
is the presence of incorrect POS tags due to auto-
5
http://w3.msi.vxu.se/users/jni/blend/
652
LAS on sentences LAS on sentences
POS tags Parser LAS test set without POS errors with POS errors
Gold ZPar 90.45 91.68 89.14
Automatic ZPar 89.15 91.62 86.51
Automatic Best combination of 3 parsers: 89.56 (+0.41) 91.90 (+0.28) 87.06 (+0.55)
3 baselines + 3 SF extensions
Automatic Best combination of 3 parsers: 89.43 (+0.28) 91.95 (+0.33) 86.75 (+0.24)
3 baselines + 3 SS extensions
Automatic Best combination of 3 parsers: 89.52 (+0.37) 91.92 (+0.30) 86.96 (+0.45)
3 baselines + 3 cluster extensions
Table 5: Differences in LAS (LTH) for baseline and extended parsers with sentences having cor-
rect/incorrect POS tags (the parentheses show the difference w.r.t ZPar with automatic POS tags).
matic tagging. For example, ZPar?s LAS score on
the LTH conversion drops from 90.45% with gold
POS tags to 89.12% with automatic POS tags. We
will examine the influence of each type of seman-
tic information on sentences that contain or not
POS errors, and this will clarify whether the incre-
ments obtained when using semantic information
are useful for correcting the negative influence of
POS errors or they are orthogonal and constitute
a source of new information independent of POS
tags. With this objective in mind, we analyzed the
performance on the subset of the test corpus con-
taining the sentences which had POS errors (1,025
sentences and 27,300 tokens) and the subset where
the sentences had (automatically assigned) correct
POS tags (1,391 sentences and 29,386 tokens).
Table 5 presents the results of the best single
parser on the LTH conversion (ZPar) with gold
and automatic POS tags in the first two rows. The
LAS scores are particularized for sentences that
contain or not POS errors. The following three
rows present the enhanced (combined) parsers
that make use of semantic information. As the
combination of the three baseline parsers did not
give any improvement over the best single parser
(ZPar), we can hypothesize that the gain coming
from the parser combinations comes mostly from
the addition of semantic information. Table 5 sug-
gests that the improvements coming from Word-
Net?s semantic file (SF) are unevenly distributed
between the sentences that contain POS errors and
those that do not (an increase of 0.28 for sentences
without POS errors and 0.55 for those with er-
rors). This could mean that a big part of the in-
formation contained in SF helps to alleviate the
errors performed by the automatic POS tagger. On
the other hand, the increments are more evenly
distributed for SS and clusters, and this can be
due to the fact that the semantic information is
orthogonal to the POS, giving similar improve-
ments for sentences that contain or not POS errors.
We independently tested this fact for the individ-
ual parsers. For example, with MST and SF the
gains almost doubled for sentences with incorrect
POS tags (+0.37 with respect to +0.21 for sen-
tences with correct POS tags) while the gains of
adding clusters? information for sentences without
and with POS errors were similar (0.91 and 1.33,
repectively). This aspect deserves further inves-
tigation, as the improvements seem to be related
to both the type of semantic information and the
parsing algorithm.We did an initial exploration but
it did not give any clear indication of the types of
improvements that could be expected using each
parser and semantic data.
5 Conclusions
This work has tried to shed light on the contribu-
tion of semantic information to dependency pars-
ing. The experiments were thorough, testing two
treebank conversions and three parsing paradigms
on automatically predicted POS tags. Compared
to (Agirre et al, 2011), which used MaltParser on
the LTH conversion and gold POS tags, our results
can be seen as a negative outcome, as the improve-
ments are very small and non-significant in most
of the cases. For parser combination, WordNet
semantic file information does give a small sig-
nificant increment in the more fine-grained LTH
representation. In addition we show that the im-
provement of automatic clusters is also weak. For
the future, we think tdifferent parsers, eitherhat a
more elaborate scheme is needed for word classes,
requiring to explore different levels of generaliza-
tion in the WordNet (or alternative) hierarchies.
Acknowledgments
This research was supported by the the Basque
Government (IT344- 10, S PE11UN114), the Uni-
versity of the Basque Country (GIU09/19) and
the Spanish Ministry of Science and Innovation
(MICINN, TIN2010-20218).
653
References
Eneko Agirre, Timothy Baldwin, and David Martinez.
2008. Improving parsing and PP attachment per-
formance with sense information. In Proceedings
of ACL-08: HLT, pages 317?325, Columbus, Ohio,
June. Association for Computational Linguistics.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency pars-
ing with semantic classes. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 699?703, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Daniel M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30(4):479?511.
Marie Candito and Djam?e Seddah. 2010. Pars-
ing word clusters. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?84, Los
Angeles, CA, USA, June. Association for Computa-
tional Linguistics.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, NAACL 2000, pages
132?139, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589?637, December.
Sanae Fujita, Francis Bond, Stephan Oepen, and
Takaaki Tanaka. 2010. Exploiting semantic infor-
mation for hpsg parse selection. Research on Lan-
guage and Computation, 8(1):122.
Johan Hall, Jens Nilsson, Joakim Nivre, Glsen Eryigit,
Beta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multi-
lingual parser optimization. In Proceedings of the
CoNLL Shared Task EMNLP-CoNLL.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for
English. In Proceedings of NODALIDA 2007, pages
105?112, Tartu, Estonia, May 25-26.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1?11, Uppsala, Sweden,
July. Association for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy,
and Timothy Baldwin. 2012. The effects of seman-
tic annotations on precision parse ranking. In First
Joint Conference on Lexical and Computational Se-
mantics (*SEM), page 228236, Montreal, Canada,
June. Association for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of ACL.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proceedings of CoNLL 2006.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2008.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th Con-
ference of the ACL.
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,
Glsen Eryiit, Sandra Kbler, Marinov S., and Edwin
Marsi. 2007. Maltparser: A language-independent
system for data-driven dependency parsing. Natural
Language Engineering.
Kenji Sagae and Andrew Gordon. 2009. Clustering
words by syntactic similarity improves dependency
parsing of predicate-argument structures. In Pro-
ceedings of the Eleventh International Conference
on Parsing Technologies.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 551?560, Singapore, August. As-
sociation for Computational Linguistics.
654
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 477?
487, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th IWPT, pages 195?
206. Association for Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 188?193, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
655
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 361?365,
Dublin, Ireland, August 23-24, 2014.
IxaMed: Applying Freeling and a Perceptron Sequential Tagger at the
Shared Task on Analyzing Clinical Texts
Koldo Gojenola, Maite Oronoz, Alicia P
?
erez, Arantza Casillas
IXA Taldea (UPV-EHU)
maite.oronoz@ehu.es
http://ixa.si.ehu.es
Abstract
This paper presents the results of the Ix-
aMed team at the SemEval-2014 Shared
Task 7 on Analyzing Clinical Texts.
We have developed three different sys-
tems based on: a) exact match, b) a
general-purpose morphosyntactic analyzer
enriched with the SNOMED CT termi-
nology content, and c) a perceptron se-
quential tagger based on a Global Linear
Model. The three individual systems re-
sult in similar f-score while they vary in
their precision and recall. We have also
tried direct combinations of the individual
systems, obtaining considerable improve-
ments in performance.
1 Introduction
This paper presents the results of the IxaMed team.
The task is focused on the identification (Task A)
and normalization (Task B) of diseases and disor-
ders in clinical reports.
We have developed three different systems
based on: a) exact match, b) a general-
purpose morphosyntactic analyzer enriched with
the SNOMED CT terminology content, and c) a
perceptron sequential tagger based on a Global
Linear Model. The first system can be seen as
a baseline that can be compared with other ap-
proaches, while the other two represent two alter-
native approaches based on knowledge organized
in dictionaries/ontologies and machine learning,
respectively. We also tried direct combinations of
the individual systems, obtaining considerable im-
provements in performance.
These approaches are representative of different
solutions that have been proposed in the literature
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
(Pradhan et al., 2013), which can be broadly clas-
sified in the following types:
? Knowledge-based. This approach makes use
of large-scale dictionaries and ontologies,
that are sometimes integrated in general tools
adapted to the clinical domain, as MetaMap
(Aronson and Lang, 2010) and cTAKES (Xia
et al., 2013).
? Rule-based. For example, in (Wang and
Akella, 2013) the authors show the use
of a rule-based approach on the output of
MetaMap.
? Statistical techniques. These systems take a
training set as input and apply different vari-
ants of machine learning, such as sequen-
tial taggers based on hidden Markov mod-
els (HMMs) or conditional random fields
(CRFs) (Zuccon et al., 2013; Bodnari et al.,
2013; Gung, 2013; Hervas et al., 2013; Lea-
man et al., 2013).
? Combinations. These approaches try to take
the advantages of different system types, us-
ing methods such as voting or metaclassi-
fiers (Liu et al., 2013).
In the rest of the paper, we will first introduce
the different systems that we have developed in
section 2, presenting the main results in section 3,
and ending with the main conclusions.
2 System Description
The task of detecting diseases and their corre-
sponding concept unique identifiers (CUI) has
been faced using three methods that are described
in the following subsections.
2.1 Exact Match
The system based on Exact Match (EM) simply
obtained a list of terms and their corresponding
361
CUI identifier from the training set and marked
any appearance of those terms in the evaluation
set. This simple method was improved with some
additional extensions:
? Improving precision. In order to reduce the
number of false positives (FP), we applied
first the EM system to the training set it-
self. This process helped to measure FPs,
for example, blood gave 184 FPs and 2 true
positives (TPs). For the sake of not hurting
the recall, we allowed the system to detect
only those terms where TP > FP , that is,
?blood? would not be classified as disorder.
? Treatment of discontinuous terms. For
these terms, our system performed a soft-
matching comparison allowing a limited vari-
ation for the text comprised between the
term elements (for example ?right atrium is
mildly/moderately dilated?). These patterns
were tuned manually.
2.2 Adapting Freeling to the Medical Domain
Freeling is an open-source multilingual language
processing library providing a wide range of ana-
lyzers for several languages (Padr?o et al., 2010),
Spanish and English among others. We had al-
ready adapted Freeling to the medical domain in
Spanish (Oronoz et al., 2013), so we used our pre-
vious experience to adapt the English version to
the same domain. For the sake of clarity, we will
refer to this system as FreeMed henceforth.
The linguistic resources (lexica, grammars,. . . )
in Freeling can be modified, so we took advantage
of this flexibility extending two standard Freel-
ing dictionaries: a basic dictionary of terms con-
sisting of a unique word, and a multiword-term
dictionary. Both of them were enriched with a
dictionary of medical abbreviations
1
and with the
Systematized Nomenclature of Medicine Clinical
Terms (SNOMED CT) version dated 31st of July
of 2013. In addition to the changes in the lexica,
we added regular expressions in the tokenizer to
recognize medical terms as ?Alzheimer?s disease?
as a unique term.
In our approach, the system distinguishes be-
tween morphology and syntax on one side and
semantics on the other side. First, on the mor-
phosyntactic processing, our system only catego-
rizes word-forms using their basic part-of-speech
1
http://www.jdmd.com/abbreviations-glossary.asp
(POS) categories. Next, the semantic distinctions
are applied (the identification of the term as sub-
stance, disorder, procedure,. . . ). Following this
approach, whenever the specific term on the new
domain (biomedicine in this case) was already in
Freeling?s standard dictionaries, the specific en-
tries will not be added to the lexicon. Instead,
medical meanings are added in a later semantic
tagging stage. For example: the widely used term
?fever?, as common noun, was not added to the
lexicon but its semantic class is given in a sec-
ond stage. Only very specific terms not appear-
ing in the lexica as, for instance, ?diskospondyli-
tis? were inserted. This solution helps to avoid
an explosion of ambiguity in the morphosyntactic
analysis and, besides, it enables a clear separation
between morphosyntax and semantics.
In figure 1 the results of both levels of anal-
ysis, morphosyntactic and semantic, are shown.
The linguistic and medical information of medical
texts is stored in the Kyoto Annotation Format or
KAF (Bosma et al., 2009) that is based in the eX-
tended Markup Language (XML). In this example
the term aneurysm is analyzed as NN (meaning
noun) and it is semantically categorized as mor-
phological abnormality and disorder.
SNOMED CT is part of the Metathesaurus,
one of the elements of the Unified Medical Lan-
guage System (UMLS). We used the Metathe-
saurus vocabulary database to extract the map-
ping between SNOMED CT?s concept identifiers
and their corresponding UMLS?s concept unique
identifier (CUI). All the medical terms appearing
in SNOMED CT and analyzed with FreeMed are
tagged with both identifiers. For instance, the term
aneurysm in figure 1 has the 85659009 SNOMED
CT identifier when the term is classified in the
morphological abnormality content hierarchy and
the 432119003 identifier as disorder. Both are
linked to the same concept identifier, C0002940,
in UMLS. This mapping has been used for Task
B, whenever the CUI is the same in all the analy-
sis of the same term.
All the terms from all the 19 content hierarchies
of SNOMED CT were tagged with semantic infor-
mation in the provided texts.
The training corpus was linguistically analyzed
and its format was changed from XML to the for-
mat specified at the shared task. After a manual
inspection of the results and the Gold Standard,
some selection of terms was performed:
362
<term tid=?t241? lemma=?aneurysm? pos=?NN?>
<extRefs>
<extRef resource=?SCT 20130731? reference=?85659009?
reftype=?morphologic abnormality? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/ >
</extRef>
<extRef resource=?SCT 20130731? reference=?432119003?
reftype=?disorder? >
<extRef resource=?UMLS-2010AB? reference=?C0002940?/>
</extRef>
</extRefs>
</term>
Figure 1: Analysis with augmented information.
? Selection and combination of semantic
classes. All the terms from the disor-
der semantic class (for example ?Hypothy-
roidism?) and from the finding class (for in-
stance ?headache?) are chosen, as well as
some tag combinations (see figure 1). After
analyzing the train corpus we decided to join
into a unique term a body structure immedi-
ately followed by a disorder/finding. In this
way, we identify terms as ?MCA aneurysm?
that are composed of the MCA abbreviation
(meaning ?middle cerebral artery?) and the
inmediately following ?aneurysm? disorder.
? Filtering. Not all the terms from the men-
tioned SNOMED CT hierarchies are identi-
fied as disorders in the Gold Standard. Some
terms are discarded following these criteria:
i) findings describing personal situations (e.g.
?alcoholic?), ii) findings describing current
situations (e.g. ?awake?), iii) findings with
words indicating a negation or normal situ-
ation (e.g. ?stable blood pressure?) and iv)
too general terms (e.g. ?problems?).
The medical terms indicating disorders that are
linked to more than one CUI identifier, were
tagged as CUI-less. That is, we did not perform
any CUI disambiguation.
In subsequent iterations and after analyzing our
misses, new terms and term variations (Hina et
al., 2013) are added to the lexica in Freeling with
the restriction that, at least, one synonym should
appear in SNOMED CT. Thus, equivalent forms
were created for all the terms indicating a cancer,
a tumor, a syndrome, or a specific disease. For in-
stance, variants for the term ?cancer of colon? and
with the same SNOMED CT concept identifier
(number 363406005) are created with the forms
?colon cancer?, ?cancer of the colon? and ?can-
cer in colon?. Some abbreviation variations found
in the Gold Standard are added in the lexica too,
following the same criteria.
2.3 Perceptron Sequential Tagger
This system uses a Global Linear Model (GLM),
a sequential tagger using the perceptron algorithm
(Collins, 2002), that relies on Viterbi decoding of
training examples combined with simple additive
updates. The algorithm is competitive to other op-
tions such as maximum-entropy taggers or CRFs.
The original textual files are firstly processed by
FreeMed, and then the tagger uses all the available
information to assign tags to the text. Each token
contains information about the word form, lemma,
part of speech, and SNOMED CT category.
Our GLM system only deals with Task A, and
it will not tackle the problem of concept normal-
ization, due to time constraints. In this respect, for
Task B the GLM system will simply return the first
SNOMED CT category given by FreeMed. This
does not mean that GLM and FreeMed will give
the same result for Task B, as the GLM system
first categorizes each element as a disease, and it
gives a CUI only when that element is identified.
2.4 Combinations
The previous subsections presented three differ-
ent approaches to the problem that obtain com-
parable scores (see table 1). In the area of auto-
matic tagging, there are several works that com-
bine disparate systems, usually getting good re-
sults. For this reason, we tried the simplest ap-
proach of merging the outputs of the three individ-
ual systems into a single file.
3 Results
Table 1 presents the results of the individual and
combined systems on the development set. Look-
ing at the individual systems on Task A, we can see
that all of them obtain a similar f-score, although
there are important differences in terms of preci-
sion and recall. Contrary to our initial intuition,
the FreeMed system, based on dictionaries and on-
tologies, gives the best precision and the lowest re-
call. In principle, having SNOMED CT as a base,
we could expect that the coverage would be more
complete (attaining the highest recall). However,
the results show that there is a gap between the
writing of the standard SNOMED CT terms and
the terms written by doctors in their notes. On the
other hand, the sequential tagger gives the best re-
363
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
INDIVIDUAL SYSTEMS
Exact Match (EM) 0.804 0.505 0.620 0.958 0.604 0.740 0.479 0.948
FreeMed 0.822 0.501 0.622 0.947 0.578 0.718 0.240 0.479
GLM 0.715 0.570 0.634 0.908 0.735 0.813 0.298 0.522
COMBINATIONS
FreeMed + EM 0.766 0.652 0.704 0.936 0.754 0.835 0.556 0.855
FreeMed + GLM 0.689 0.668 0.678 0.903 0.790 0.843 0.345 0.518
EM + GLM 0.680 0.679 0.679 0.907 0.819 0.861 0.398 0.598
FreeMed + EM + GLM 0.659 0.724 0.690 0.899 0.845 0.871 0.421 0.584
Table 1: Results of the different systems on the development set.
Task A Task B
Strict Relaxed Strict Relaxed
System Precision Recall F-Score Precision Recall F-Score Accuracy
FreeMed + EM 0.729 0.701 0.715 0.885 0.808 0.845 0.604 0.862
FreeMed + EM + GLM 0.681 0.786 0.730 0.872 0.890 0.881 0.439 0.558
Best system 0.843 0.786 0.813 0.936 0.866 0.900 0.741 0.873
Table 2: Results on the test set.
call. Since the tagger uses both contextual words
and prefixes and suffixes as features for learning,
this method has proven helpful for the recognition
of terms that do not appear in the training data (see
the difference with the EM approach).
Looking at the different combinations in table 1,
we see that two approaches work best, either com-
bining FreeMed and EM, or combining the three
individual systems. The inclusion of GLM results
in the best coverage, but at the expense of preci-
sion. On the other hand, combining FreeMed and
EM gives a better precision but lower coverage.
As pointed out by Collins (2002), the results of
the perceptron tagger are competitive with respect
to other statistical approaches such as CRFs (Zuc-
con et al., 2013; Bodnari et al., 2013; Gung, 2013;
Hervas et al., 2013; Leaman et al., 2013).
Regarding Task B, we can see that the EM sys-
tem is by far the most accurate, while FreeMed
is well below its a priori potential. The reason of
this low result is mainly due to the high ambiguity
found on the output of the SNOMED CT tagger, as
many terms are associated with more than one CUI
and, consequently, are left untagged. This problem
deserves future work on automatic semantic dis-
ambiguation. On the combinations, FreeMed and
EM together give the best result. However, as we
told before, the GLM system was only trained for
Task A, so it is not surprising to see that its results
deteriorate the accuracy in Task B.
We chose these best two combinations for the
evaluation on the test set (using training and de-
velopment for experimentation or training), which
are presented in table 2. Here we can see that re-
sults on the development also hold on the test set.
Given the unsophisticated approach to combine
the systems, we can figure out more elaborated so-
lutions, such as majority or weighted voting, or
even more, the definition of a machine learning
classifier to select the best system for every pro-
posed term. These ideas are left for future work.
4 Conclusions
We have presented the IxaMed approach, com-
posed of three systems that are based on exact
match, linguistic and knowledge repositories, and
a statistical tagger, respectively. The results of in-
dividual systems are comparable, with differences
in precision and recall. We also tested a sim-
ple combination of the systems, which proved to
give significant improvements over each individ-
ual system. The results are competitive, although
still far from the winning system.
For future work, we plan to further improve the
individual systems. Besides, we hope that the ex-
perimentation with new combination approaches
will offer room for improvement.
Acknowledgements
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
364
References
Alan R Aronson and Francois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17:229?236.
Andreea Bodnari, Louise Deleger, Thomas Lavergne,
Aurelie Neveol, and Pierre Zweigenbaum. 2013.
A Supervised Named-Entity Extraction System for
Medical Text. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Wauter Bosma, Piek Vossen, Aitor Soroa, German
Rigau, Maurizio Tesconi, Andrea Marchetti, Mon-
ica Monachini, and Carlo Aliprandi. 2009. KAF: a
Generic Semantic Annotation Format. In Proceed-
ings of the 5th International Conference on Gener-
ative Approaches to the Lexicon GL, pages 17?19,
Septembre.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. In Proceed-
ings of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 1?8. Asso-
ciation for Computational Linguistics, July.
James Gung. 2013. Using Relations for Identification
and Normalization of Disorders: Team CLEAR in
the ShARe/CLEF 2013 eHealth Evaluation Lab. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Lucia Hervas, Victor Martinez, Irene Sanchez, and Al-
berto Diaz. 2013. UCM at CLEF eHealth 2013
Shared Task1. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Saman Hina, Eric Atwell, and Owen Johnson. 2013.
SnoMedTagger: A semantic tagger for medical nar-
ratives. In Conference on Intelligent Text Processing
and Computational Linguistics (CICLING).
Robert Leaman, Ritu Khare, and Zhiyong Lu. 2013.
NCBI at 2013 ShARe/CLEF eHealth Shared Task:
Disorder Normalization in Clinical Notes with
Dnorm. In Online Working Notes of the CLEF 2013
Evaluation Labs and Workshop, September.
Hongfang Liu, Kavishwar Wagholikar, Siddhartha Jon-
nalagadda, and Sunghwan Sohn. 2013. Integrated
cTAKES for Concept Mention Detection and Nor-
malization. In Online Working Notes of the CLEF
2013 Evaluation Labs and Workshop, September.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic Annotation of
Medical Records in Spanish with Disease, Drug
and Substance Names. In Lecture Notes in Com-
puter Science, 8259. Progress in Pattern Recogni-
tion, ImageAnalysis, ComputerVision, and Applica-
tions 18th Iberoamerican Congress, CIARP 2013,
Havana, Cuba, November 20-23.
Lluis Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic Services in Freeling 2.1:
WordNet and UKB. In Global Wordnet Conference,
Mumbai, India.
Sameer Pradhan, Noemie Elhadad, Brett R. South,
David Martinez, Lee Christensen, Amy Vogel,
Hanna Suominen, Wendy W. Chapman, and Guer-
gana Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Online Working Notes
of the CLEF 2013 Evaluation Labs and Workshop,
September.
Chunye Wang and Ramakrishna Akella. 2013. UCSCs
System for CLEF eHealth 2013 Task 1. In Online
Working Notes of the CLEF 2013 Evaluation Labs
and Workshop, September.
Yunqing Xia, Xiaoshi Zhong, Peng Liu, Cheng Tan,
Sen Na, Qinan Hu, and Yaohai Huang. 2013. Com-
bining MetaMap and cTAKES in Disorder Recogni-
tion: THCIB at CLEF eHealth Lab 2013 Task 1. In
Online Working Notes of the CLEF 2013 Evaluation
Labs and Workshop, September.
Guido Zuccon, Alexander Holloway, Bevan Koop-
man, and Anthony Nguyen. 2013. Identify Disor-
ders in Health Records using Conditional Random
Fields and Metamap AEHRC at ShARe/CLEF 2013
eHealth Evaluation Lab Task 1. In Online Working
Notes of the CLEF 2013 Evaluation Labs and Work-
shop, September.
365
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 31?39,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Application of Different Techniques to Dependency Parsing of Basque 
 
Kepa Bengoetxea Koldo Gojenola 
IXA NLP Group IXA NLP Group 
University of the Basque Country University of the Basque Country 
Technical School of Engineering, Bilbao,  
Plaza La Casilla 3, 48012, Bilbao 
Technical School of Engineering, Bilbao,  
Plaza La Casilla 3, 48012, Bilbao 
kepa.bengoetxea@ehu.es koldo.gojenola@ehu.es 
 
  
 
 
Abstract 
We present a set of experiments on depend-
ency parsing of the Basque Dependency Tree-
bank (BDT). The present work has examined 
several directions that try to explore the rich 
set of morphosyntactic features in the BDT: i) 
experimenting the impact of morphological 
features, ii) application of dependency tree 
transformations, iii) application of a two-stage 
parsing scheme (stacking), and iv) combina-
tions of the individual experiments. All the 
tests were conducted using MaltParser (Nivre 
et al, 2007a), a freely available and state of 
the art dependency parser generator. 
1 Introduction 
This paper presents several experiments performed 
on dependency parsing of the Basque Dependency 
Treebank (BDT, Aduriz et al, 2003). Basque can 
be briefly described as a morphologically rich lan-
guage with free constituent order of the main sen-
tence elements with respect to the main verb. 
This work has been developed in the context of 
dependency parsing exemplified by the CoNLL 
Shared Task on Dependency Parsing in years 2006 
and 2007 (Nivre et al, 2007b), where several sys-
tems competed analyzing data from a typologically 
varied range of 19 languages. The treebanks for all 
languages were standardized using a previously 
agreed CoNLL-X format (see Figure 1). An early 
version of the BDT (BDT I) was one of the evalu-
ated treebanks, which will allow a comparison with 
our results. One of the conclusions of the CoNLL 
2007 workshop (Nivre et al, 2007a) was that there 
is a class of languages, those that combine a rela-
tively free word order with a high degree of inflec-
tion, that obtained the worst scores. This asks for 
the development of new methods and algorithms 
that will help to reach the parsing performance of 
the more studied languages, as English. 
In this work, we will take the opportunity of 
having a new fresh version of the BDT, (BDT II 
henceforth), which is the result of an extension 
(three times bigger than the original one), and its 
redesign (see section 3.2). Using MaltParser, a 
freely available and state of the art dependency 
parser for all the experiments (Nivre et al, 2007a), 
this paper will concentrate on the application of 
different techniques to the task of parsing this new 
treebank, with the objective of giving a snapshot 
that can show the expected gains of each tech-
nique, together with some of their combinations. 
Some of the techniques have already been evalu-
ated with other languages/treebanks or BDT I, 
while others have been adapted or extended to deal 
with specific aspects of the Basque language or the 
Basque Treebank. We will test the following: 
? Impact of rich morphology. Although many 
systems performed feature engineering on the 
BDT at CoNLL 2007, providing a strong 
baseline, we will take a step further to im-
prove parsing accuracy taking into account the 
effect of specific morphosyntactic features. 
? Application of dependency-tree transforma-
tions. Nilsson et al (2007) showed that they 
can increase parsing accuracy across lan-
guages/treebanks. We have performed similar 
experiments adapted to the specific properties 
of Basque and the BDT. 
? Several works have tested the effect of using a 
two-stage parser (Nivre and McDonald, 2008; 
Martins et al, 2008), where the second parser 
takes advantage of features obtained by the 
first one. Similarly, we will experiment the 
31
addition of new features to the input of the 
second-stage parser, in the form of morpho-
syntactic features propagated through the first 
parser?s dependency tree and also as the addi-
tion of contextual features (such as category 
or dependency relation of parent, grandparent, 
and descendants). 
? Combinations of the individual experiments. 
The rest of the paper is organized as follows. 
After presenting related work in section 2, section 
3 describes the main resources used in this work. 
Next, section 4 will examine the details of the dif-
ferent experiments to be performed, while section 
5 will evaluate their results. Finally, the last section 
outlines our main conclusions. 
2 Related work 
Until recently, many works on treebank parsing 
have been mostly dedicated to languages with poor 
morphology, as exemplified by the Penn English 
Treebank. As the availability of treebanks for typo-
logically different languages has increased, there 
has been a growing interest towards research on 
extending the by now standard algorithms and 
methods to the new languages and treebanks (Tsar-
faty et al, 2009). For example, Collins et al (1999) 
adapted Collins? parser to Czech, a highly-
inflected language. Cowan and Collins (2005) ap-
ply the same parser to Spanish, concluding that the 
inclusion of morphological information improves 
the analyzer. Eryi?it et al (2008) experiment the 
use of several types of morphosyntactic informa-
tion in Turkish, showing how the richest the in-
formation improves precision. They also show that 
using morphemes as the unit of analysis (instead of 
words) gets better results, as a result of the aggluti-
native nature of Turkish, where each wordform 
contains several morphemes that can be individu-
ally relevant for parsing. Goldberg and Tsarfaty 
(2008) concluded that an integrated model of mor-
phological disambiguation and syntactic parsing in 
Hebrew Treebank parsing improves the results of a 
pipelined approach. This is in accord with our ex-
periment of dividing words into morphemes and 
transforming the tree accordingly (see section 4.2). 
Since the early times of treebank-based parsing 
systems, a lot of effort has been devoted to aspects 
of preprocessing trees in order to improve the re-
sults (Collins, 1999). When applied to dependency 
parsing, several works (Nilsson et al, 2007; Ben-
goetxea and Gojenola, 2009a) have concentrated 
on modifying the structure of the dependency tree, 
changing its original shape. For example, Nilsson 
et al (2007) present the application of pseudopro-
jective, verbal group and coordination transforma-
tions to several languages/treebanks using 
MaltParser, showing that they improve the results.  
Another interesting research direction has exam-
ined the application of a two-stage parser, where 
the second parser tries to improve upon the result 
of a first parser. For example, Nivre and McDonald 
(2008) present the combination of two state of the 
art dependency parsers feeding each another, 
showing that there is a significant improvement 
over the simple parsers. This experiment can be 
seen as an instance of stacked learning, which was 
also tested on dependency parsing of several lan-
guages in (Martins et al, 2008) with significant 
improvements over the base parser. 
3 Resources 
This section will describe the main resources that 
have been used in the experiments. First, subsec-
Index Word   Lemma   Category Subcategory Features  Head Dependency 
1 etorri   etorri   V  V  _   3 coord 
2 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 1 auxmod 
3 eta   eta     CONJ CONJ  _   6 ccomp_obj 
4 joan   joan     V  V  _   3 coord 
5 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 4 auxmod 
6 esan   esan     V  V  _   0 ROOT 
7 zien   *edun    AUXV AUXV  SUBJ:3S|OBJ:3P 6 auxmod 
8 mutilak  mutil   NOUN NOUN_C  CASE:ERG|NUM:S 6 ncsubj 
9 .   .     PUNT PUNT_PUNT _   8 PUNC 
 
Figure 1: Example of a BDT sentence in the CoNLL-X format 
(V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj = 
clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl, coord = 
coordination, auxmod = auxiliary, ncsubj = non-clausal subject, ncmod = non-clausal modifier). 
 
32
tion 3.1 will describe the Basque Dependency 
Treebank, which has increased its size from 55,469 
tokens in its original version to more than 150,000, 
while subsection 3.2 will present the main charac-
teristics of MaltParser, a state of the art and data-
driven dependency parser. 
3.1 The Basque Dependency Treebank 
Basque can be described as an agglutinative lan-
guage that presents a high power to generate in-
flected word-forms, with free constituent order of 
sentence elements with respect to the main verb. 
The BDT can be considered a pure dependency 
treebank from its original design, due mainly to the 
syntactic characteristics of Basque.  
(1) Etorri  dela  eta joan  dela   esan  zien mutilak 
    come  that-has and go  that-has tell did  boy-the 
  The boy told them that he has come and gone 
 
Figure 1 contains an example of a sentence (1), 
annotated in the CoNLL-X format. The text is or-
ganized in eight tab-separated columns: word-
number, form, lemma, category, subcategory, mor-
phological features, and the dependency relation 
(headword + dependency). The information in Fig-
ure 1 has been simplified due to space reasons, as 
typically the Features column will contain many 
morphosyntactic1 features (case, number, type of 
subordinated sentence, ?), which are relevant for 
parsing. The first version of the Basque Depend-
ency Treebank contained 55,469 tokens forming 
3,700 sentences (Aduriz et al, 2003). This tree-
bank was used as one of the evaluated treebanks in 
the CoNLL 2007 Shared Task on Dependency 
Parsing (Nivre et al, 2007b). Our work will make 
use of the second version of the BDT (BDT II), 
which is the consequence of a process of extension 
and redesign of the original requirements: 
? The new version contains 150,000 tokens 
(11,225 sentences), a three-fold increase. 
? The new design considered that all the de-
pendency arcs would connect sentence tokens. 
In contrast, the original annotation contained 
empty nodes, especially when dealing with el-
lipsis and some kinds of coordination. As a 
result, the number of non-projective arcs di-
                                                          
1 We will use the term morphosyntactic to name the set of 
features attached to each word-form, which by the agglutina-
tive nature of Basque correspond to both morphology and 
syntax. 
minished from 2.9% in the original treebank 
to 1.3% in the new version. 
? The annotation follows a stand-off markup 
approach, inspired on TEI-P4 (Artola et al, 
2005). There was a conversion process from a 
set of interconnected XML files to the 
CoNLL-X format of the present experiments. 
Although the different characteristics and size of 
the two treebank versions do not allow a strict 
comparison, our preliminary experiments showed 
that the results on both treebanks were similar re-
garding our main evaluation criterion (Labeled 
Attachment Score, or LAS). In the rest of the paper 
we will only use the new BDT II. 
3.2 MaltParser 
MaltParser (Nivre et al 2007a) is a state of the art 
dependency parser that has been successfully ap-
plied to typologically different languages and tree-
banks. While several variants of the base parser 
have been implemented, we will use one of its 
standard versions (MaltParser version 1.3). The 
parser obtains deterministically a dependency tree 
in linear-time in a single pass over the input using 
two main data structures: a stack of partially ana-
lyzed items and the remaining input sequence. To 
determine the best action at each parsing step, the 
parser uses history-based feature models and dis-
criminative machine learning. In all the following 
experiments, we will make use of a SVM classi-
fier. The specification of the configuration used for 
learning can in principle include any kind of col-
umn in Figure 1 (such as word-form, lemma, cate-
gory, subcategory or morphological features), 
together with a feature function. This means that a 
learning model can be described as a series of 
(column, function) pairs, where column represents 
the name of a column in Figure 1, and function 
makes reference to the parser?s main data struc-
tures. For example, the two pairs (Word, Stack[0]), 
and (Word, Stack[1]) represent two features that 
correspond to the word-forms on top and next to 
top elements of the stack, respectively, while 
(POSTAG, Input[0]) represents the POS category 
of the first token in the remaining input sequence. 
4 Experiments 
The following subsections will present three types 
of techniques that will be tested with the aim of 
33
improving the results of the syntactic analyzer. 
Subsection 4.1 presents the process of fine-tuning 
the rich set of available morphosyntactic features. 
Then, 4.2 will describe the application of three 
types of tree transformations, while subsection 4.3 
will examine the application of propagating syntac-
tic features through a first-stage dependency tree, a 
process that can also be seen as an application of 
stacked learning, as tested in (Nivre and McDon-
ald, 2008; Martins et al, 2008) 
4.1 Feature engineering 
The original CoNLL-X format uses 10 different 
columns (see Figure 12), grouping the full set of 
morphosyntactic features in a single column. We 
will experiment the effect of individual features, 
following two steps: 
? First, we tested the effect of incorporating 
each individual lexical feature, concluding 
that there were two features that individually 
gave significant performance increases. They 
were syntactic case, which is relevant for 
marking a word?s syntactic function (or, 
equivalently, the type of dependency relation), 
and subordination type (REL henceforth). 
This REL feature appears in verb-ending mor-
phemes that specify a type of subordinated 
sentence, such as in relative, completive, or 
indirect interrogative clauses. The feature is, 
therefore, relevant for establishing the main 
structure of a sentence, helping to delimit 
main and subordinated clauses, and it is also 
crucial for determining the dependency rela-
tion between the subordinated sentence and 
the main verb (head). 
? Then, we separated these features in two in-
dependent columns, grouping the remaining 
features under the Features column. This way, 
Maltparser?s learning specification can be 
more fine-grained, in terms of three morpho-
syntactic feature sets (CASE, REL and the 
rest, see Table 2). 
This will allow us testing learning models with 
different configurations for each column, instead 
of treating the full set of features as a whole. So, 
we will have the possibility of experimenting with 
                                                          
2 As a matter of fact, Figure 1 only shows 8 columns, although 
the CoNLL-X format includes two additional columns for the 
projective head (PHEAD) and projective dependency relation 
(PDEPREL), which have not been used in our work. 
richer contexts (that is, advancing the Stack and/or 
Input3 functions for each feature). 
4.2 Tree transformations  
Tree transformations have long been applied with 
the objective of improving parsing results (Collins, 
1999; Nilsson et al, 2007). The general process 
consists of the following steps: 
? Apply tree transformations to the treebank 
? Train the system on the modified treebank 
? Apply the parser to the test set 
? Apply the inverse transformations 
? Evaluate the result on the original treebank 
We will test three different tree transformations, 
which had already been applied to the Treebank 
(BDT I) (Bengoetxea and Gojenola, 2009a): 
? Projectivization (TP). This is a language inde-
pendent transformation already tested in sev-
eral languages (Nivre and Nilsson, 2005). 
This transformation is totally language inde-
pendent, and can be considered a standard 
transformation. Its performance on the first 
version of BDT had been already tested (Hall 
et al, 2007), giving significant improvements 
This is in accordance with BDT I having a 
2.9% of non-projective arcs. 
? Coordination (TC). The transformation on co-
ordinated sentences can be considered general 
(Nilsson et al, 2007) but it is also language 
dependent, as it depends on the specific con-
figurations present in each language, mainly 
the set of coordination conjunctions and the 
types of elements that can be coordinated, to-
gether with their morphosyntactic properties 
(such as head initial or final). Coordination in 
BDT (both versions) is annotated in the so 
called Prague Style (PS, see Figure 2), where 
the conjunction is taken as the head, and the 
                                                          
3 Maltparser allows a rich set of functions to be specified for 
each column. In our experiments we mainly used the Stack 
and Input functions, which allow the inspection of the contents 
of the top elements of the Stack (Stack[0], Stack[1], ?) or the 
currently unanalyzed input sequence (Input[0], Input [1], ?). 
  C1 C2  S C3   C1 C2 S C3  C1 C2 S C3  
 
(PS) (MS) (MS-sym) 
Figure 2. Dependency structures for coordination. 
 
34
conjuncts depend on it. Nilsson et al (2007) 
advocate the Mel?cuk style (MS) for parsing 
Czech, taking the first conjunct as the head, 
and creating a chain where each element de-
pends on the preceding one. Basque is a head 
final language, where many important syntac-
tic features, like case or subordinating mor-
phemes are located at the end of constituents. 
For that reason, Bengoetxea and Gojenola 
(2009a) proposed MS-sym, a symmetric 
variation of MS in which the coordinated 
elements will be dependents of the last con-
junct (which will be the head, see Figure 2).  
? Transformation of subordinated sentences 
(TS). They are formed in Basque by attaching 
the corresponding morphemes to the auxiliary 
verbs. However, in BDT (I and II) the verbal 
elements are organized around the main verb 
(semantic head) while the syntactic head 
corresponds to the subordination morpheme, 
which appears usually attached to the 
auxiliary. Its main consequence is that the 
elements bearing the relevant information for 
parsing are situated far in the tree with respect 
to their head. In Figure 3, we see that the 
morpheme ?la, indicating a subordinated 
completive sentence, appears down in the tree, 
and this could affect the correct attachment to 
the main verb (esan). Figure 4 shows the 
effect of transforming the original tree in 
Figure 3. The subordination morpheme (-la) is 
separated from the auxiliary verb (da), and is 
?promoted? as the syntactic head of  the 
subordinated sentence. New arcs are created 
from the main verb (etorri) to the morpheme 
(which is now the head), and also a new 
dependency relation (SUB).  
Overall, the projectivization transformation (TP) 
is totally language-independent. TC (coordination) 
can be considered in the middle, as it depends on 
the general characteristics of the language. Finally, 
the transformation of subordinated sentences (TS) 
is specific to the treebank and intrinsecally linked 
to the agglutinative nature of Basque. Bengoetxea 
and Gojenola (2009a) also found that the order of 
transformations can be relevant. Their best system, 
after applying all the transformations, obtained a 
76.80% LAS on BDT I (2.24% improvement over 
a baseline of 74.52%) on the test set. We include 
these already evaluated transformations in the pre-
sent work with two objectives in mind: 
? We want to test its effect on BDT II, 3 times 
larger than BDT I, and also with a lower 
proportion of non-projective arcs (1.3%). 
? We are also interested in testing its 
combination with the rest of the techniques 
(see subsections 4.1 and 4.3).  
4.3 Two-stage parsing (stacking) 
Bengoetxea and Gojenola (2009b) tested the effect 
of propagating several morphosyntactic feature 
values after a first parsing phase, as in classical 
unification-based grammars, as a means of propa-
gating linguistic information through syntax trees. 
They applied three types of feature propagation of 
the morphological feature values: a) from auxiliary 
verbs to the main verb (verb groups) b) propaga-
tion of case and number from post-modifiers to the 
head noun (noun phrases) c) from the last conjunct 
to the conjunction (coordination). This was done 
mainly because Basque is head final, and relevant 
features are located at the end of constituents.  
Nivre and McDonald (2008) present an 
application of stacked learning to dependency 
parsing, in which a second predictor is trained to 
improve the performance of the first. Martins et al 
(2008) specify the following steps: 
? Split training data D into L partitions D1, ... , 
DL. 
? Train L instances of the level 0 parser in the 
following way: the l-th instance, gl, is trained 
auxmod 
ccomp_obj 
 
Figure 4. Effect of applying the transformation on 
subordinated sentences to the tree in Figure 3 
(dotted lines represent the modified arcs). 
 
Etorri   da    +la  esan   du   
 come      has+he  that  told     did+he    
  V       AUXV+3S  COMPL   V      AUXV 
SUB auxmod 
auxmod 
ccomp_obj 
 
auxmod 
Figure 3. Dependency tree for the sentence Etorri 
dela esan du (He told that he would come). 
 
Etorri    da+la     esan   du   
 come      has+he+that   told    did+he    
  V       AUXV+3S+COMPL   V       AUXV 
35
on D?l = D \ Dl. Then use gl to output 
predictions for the (unseen) partition Dl. At 
the end, we have an augmented dataset D* = D 
+ new set of stacked/propagated features. 
? Train the level 0 parser g on the original 
training data D. 
? Train the level 1 parser on the augmented 
training data D*. 
In our tests, it was enough with two partitions (L 
= 2), as experiments with L > 2 did not give any 
significant improvement. Figure 5 shows the types 
of information that can be added to each target 
element. The token X can take several kinds of 
information from its children (A and B) or his par-
ent (H). The information that is propagated can 
vary, including part of speech, morphosyntactic 
features or the dependency relations between X 
and its children/parent. We can roughly classify the 
stacked features in two different sets: 
? Linguistic features (feature passing), such as 
case or number, which are propagated 
applying linguistic principles, such as ?the 
syntactic case is propagated from the 
dependents towards the head of NPs and 
postpositional phrases?. The idea is to 
propagate several morphosyntactic features 
(case, number, ?) from dependents to  heads. 
? Parser features. They will be based solely on 
different dependency tree configurations (see 
Figure 5), similarly to (Nivre and McDonald, 
2008; Martins et al, 2008). Among them, we 
will test the inclusion of several features 
(dependency relation, category and 
morphosyntactic features) from the following: 
parent, grandparent, siblings, and children. 
In the present work, we have devised the follow-
ing experiments: 
? We will test the effect of propagating 
linguistic features on the new BDT II. In 
contrast to (Bengoetxea and Gojenola, 
2009b), who used the enriched gold data as D* 
directly, we will test Martins et al?s proposal, 
in which the level 1 parser will be able to 
learn on the errors of the level 0 parser. 
? We will extend these experiments with the use 
of different parser features (Nivre and 
McDonald, 2008; Martins et al, 2008). 
4.4 Combination 
Finally, we will combine the different techniques. 
An important point is to determine whether the 
techniques are independent (and accumulative) or 
it could also be that they can serve as alternative 
treatments to deal with the same phenomena. 
5 Evaluation 
BDT I was used at the CoNLL 2007 Shared Task, 
where many systems competed on it (Nivre et al, 
2007b). We will use Labeled Attachment Score 
(LAS) as the evaluation measure: the percentage of 
correct arcs (both dependency relation and head) 
over all arcs, with respect to the gold standard. Ta-
ble 1 shows the best CoNLL 2007 results on BDT 
I. The best system obtained a score of 76.94%, 
combining six variants of MaltParser, and compet-
ing with 19 systems. Carreras (2007) and Titov and 
Henderson (2007) obtained the second and third 
positions, respectively. We consider the last two 
lines in Table 1 as our baselines, which consist in 
applying a single MaltParser version (Hall et al, 
2007), that obtained the fifth position at CoNLL 
2007. Although Hall et al (2007) applied the pro-
jectivization transformation (TP), we will not use it 
in our baseline because we want to evaluate the 
effect of multiple techniques over a base parser. 
Although we could not use the subset of BDT II 
corresponding to BDT I, we run4 a test with a set 
of sentences the size of BDT I. As could be ex-
                                                          
4 For space reasons, we do not specify details of the algorithm 
and the parameters. These data can be obtained, together with 
the BDT II data, from any of the authors. 
 System LAS 
Nivre et al 2007b (MaltParser, 
combined) 
76.94%   
Carreras, 2007 75.75%   
Titov and Henderson, 2007 75.49%   
C 
o 
N 
L 
L 
 
07 
Hall et al, 2007 (MaltParser 
(single parser) + pseudoprojec-
tive transformation) 
74.99% 
 
 
 
BDT I 
MaltParser (single parser) 74.52% 
BDT I size 74.83% BDT II  MaltParser (single 
parser)  Baseline 77.08% 
Table 1. Top LAS scores for Basque dependency parsing. 
d2 
d1 
d3 
Figure 5. Stacked features. X can take several 
features from its descendants (dependency arcs 
d2 and d3) or his head (d1). 
 
 A            X           B    H   
36
pected, the three-fold increase in the new treebank 
gives a 2.35% improvement over BDT I. 
For evaluation, we divided the treebank in three 
sets, corresponding to training, development, and 
test (80%, 10%, and 10%, respectively). All the 
experiments were done on the development set, 
leaving the best systems for the final test. 
5.1 Single systems 
Table 3 shows the results for the basic systems 
employing each of the techniques advanced in Sec-
tion 4. As a first result, we see that a new step of 
reengineering MaltParser?s learning configuration 
was rewarding (see row 2 in Table 3), as morpho-
syntactic features were more finely specified with 
respect to the most relevant features. Table 2 pre-
sents the baseline and the best learning model5. We 
see that advancing the input lookahead for CASE 
and REL gives an increase of 0.82 points. 
Looking at the transformations (rows 3 to 7), the 
new Treebank BDT II obtains results similar to 
those described in (Bengoetxea and Gojenola, 
2009a). As could be expected from the reduction 
of non-projective arcs (from 2.9% to 1.3%), the 
gains of TP are proportionally lower than in BDT I. 
Also, we can observe that TS alone worsens the 
baseline, but it gives the best results when com-
bined with the rest (rows 6 and 7). This can be ex-
plained because TS creates new non-projective 
arcs, so it is effective only if TP is applied later. 
The transformation on coordination (TC) alone 
does not get better results, but when combined 
with TP and TS gives the best results. 
Applying feature propagation and stacking (see 
rows 9-17), we can see that most of the individual 
techniques (rows 9-14) give improvements over 
the baseline. When combining what we defined as 
                                                          
5 This experiment was possible due to the fact that Malt-
Parser?s functionality was extended, allowing the specification 
of new columns/features, as the first versions of MaltParser 
only permitted a single column that included all the features. 
linguistic features (those morphosyntactic features 
propagated by the application of three linguistic 
principles), we can see that their combination 
seems accumulative (row 15). The parser features 
also give a significant improvement individually 
(rows 12-14), but, when combined either among 
themselves (row 16) or with the linguistic features 
(row 17), their effect does not seem to be additive. 
5.2 Combined systems 
After getting significant improvements on the indi-
vidual techniques and some of their combinations, 
we took a further step to integrate different tech-
niques. An important aspect that must be taken into 
account is that the combination is not trivial all the 
times. For example, we have seen (section 5.1) that 
combinations of the three kinds of tree transforma-
tions must be defined having in mind the possible 
side-effects of any previous transformation. When 
combining different techniques, care must be taken 
to avoid any incompatibility. For that reason we 
only tested some possibilities. Rows 18-21 show 
some of the combined experiments. Combination 
of feature optimization with the pseudoprojective 
transformation yields an accumulative improve-
ment (row 18). However, the combination of all 
the tree transformations with FO (row 19) does not 
accumulate. This can be due to the fact that feature 
optimization already cancelled the effect of the 
transformation on coordination and subordinated 
sentences, or otherwise it could also need a better 
exploration of their interleaved effect. Finally, row 
21 shows that feature optimization, the pseudopro-
jective transformation and feature propagation are 
also accumulative, giving the best results. The rela-
tions among the rest of the transformations deserve 
future examination, as the results do not allow us 
to extract a precise conclusion.  
6 Conclusions and future work 
We studied several proposals for improving a base-
line system for parsing the Basque Treebank. All 
the results were evaluated on the new version, 
BDT II, three times larger than the previous one. 
We have obtained the following main results: 
? Using rich morphological features. We have 
extended previous works, giving a finer 
grained description of morphosyntactic 
features on the learner?s configuration, 
  Stack[0] Input[0] Input[1] Input[2] 
1 Features + +   
CASE + + +  
REL + + + + 
2 
Features 
(rest) 
+ +   
Table 2. Learning configurations for morphosyntactic fea-
tures (1 = best model for the whole set of features. 
2 = best model when specializing features). 
37
showing that it can significantly improve the 
results. In particular, differentiating case and 
the type of subordinated sentence gives the 
best LAS increase (+0.82%).  
? Tree transformations. We have replicated the 
set of tree transformations that were tested in 
the old treebank (Bengoetxea and Gojenola 
2009a). Two of the transformations 
(projectivization and coordination) can be 
considered language independent, while the 
treatment of subordination morphemes is 
related to the morphological nature of Basque. 
? Feature propagation. We have experimented 
the effect of a stacked learning scheme. Some 
of the stacked features were language-
independent, as in (Nivre and McDonald. 
2008), but we have also applied a 
generalization of the stacking mechanism to a 
morphologically rich language, as some of the 
stacked features are morphosyntactic features 
(such as case and number) which were 
propagated through a first stage dependency 
tree by the application of linguistic principles 
(noun phrases, verb groups and coordination). 
? Combination of techniques. Although several 
of the combined approaches are accumulative 
with respect to the individual systems, some 
others do not give a improvement over the 
basic systems. A careful study must be 
conducted to investigate whether the 
approaches are exclusive or complementary. 
For example, the transformation on 
subordinated sentences and feature 
propagation on verbal groups seem to be 
attacking the same problem, i. e., the relations 
between main and subordinated sentences. In 
this respect, they can be viewed as alternative 
approaches to dealing with these phenomena. 
The results show that the application of these 
techniques can give noticeable results, getting an 
overall improvement of 1.90% (from 77.08% until 
78.98%), which can be roughly comparable to the 
effect of doubling the size of the treebank (see the 
last two lines of Table 1).  
Acknowledgements 
This research was supported by the Department of 
Industry of the Basque Government (IE09-262) 
and the University of the Basque Country 
(GIU09/19). Thanks to Joakim Nivre and his team 
for their support using Maltparser and his fruitful 
suggestion about the use of stacked features. 
  Row System LAS 
Baseline 1  77.08%  
Feature optimization 2 FO *77.90% (+0.82) 
3 TP **77.92% (+0.84) 
4 TS 75.95% (-1.13) 
5 TC 77.05% (-0.03) 
6 TS + TP **78.41% (+1.33) 
 
 
Transformations 
7 TS + TC + TP **78.59% (+1.51) 
9 SVG **77.68%  (+0.60) 
10 SNP 77.17% (+0.09) 
11 SC 77.40% (+0.32) 
12 SP *77.70% (+0.62) 
13 SCH *77.80% (+0.72) 
14 SGP 77.37% (+0.29) 
15 SVG + SNP + SC **78.22% (+1.14) 
16 SP + SCH **77.96% (+0.88) 
 
 
 
 
 
 
 
 
Single  
technique 
 
 
 
 
Stacking 
17 SVG + SNP + SC + SP + SCH **78.44% (+1.36) 
 18 FO + TP **78.78% (+1.70) 
 19 FO + TS + TC + TP **78.47% (+1.39) 
 20 TP + SVG + SNP + SC **78.56% (+1.48) 
Combination 
 21 FO + TP + SVG + SNP + SC **78.98% (+1.90) 
Table 3. Evaluation results. 
(FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations; 
SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs  and Coordination; 
SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent; 
*: statistically significant in McNemar's test, p < 0.005; **: statistically significant, p < 0.001) 
38
References  
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arrio-
la, Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea 
Garmendia and Maite Oronoz. 2003. Construction of 
a Basque dependency treebank. Treebanks and Lin-
guistic Theories. 
Xabier Artola, Arantza  D?az de Ilarraza, Nerea Ezei-
za, Koldo Gojenola, Gorka Labaka, Aitor Sologais-
toa, Aitor Soroa.  2005. A framework for 
representing and managing linguistic annotations 
based on typed feature structures. Proceedings of the 
International Conference on Recent Advances in 
Natural Language Processing, RANLP 2005. 
Kepa Bengoetxea and Koldo Gojenola. 2009a. Explor-
ing Treebank Transformations in Dependency Pars-
ing. Proceedings of the International Conference on 
Recent Advances in Natural Language Processing, 
RANLP?2009. 
Kepa Bengoetxea and Koldo Gojenola. 2009b. Applica-
tion of feature propagation to dependency parsing. 
Proceedings of the International Workshop on Pars-
ing Technologies (IWPT?2009). 
Xavier Carreras.  2007. Experiments with a high-order 
projective dependency parser. In Proceedings of the 
CoNLL 2007 Shared Task (EMNLP-CoNLL). 
Shay B. Cohen and Noah A. Smith. 2007. Joint Mor-
phological and Syntactic Disambiguation. In Pro-
ceedings of the CoNLL 2007 Shared Task. 
Michael Collins, Jan Hajic, Lance Ramshaw and Chris-
toph Tillmann. 1999. A Statistical Parser for Czech. 
Proceedings of ACL. 
Michael Collins. 1999. Head-Driven Statistical Models 
for Natural Language Parsing. PhD Dissertation, 
University of Pennsylvania.. 
Brooke Cowan and Michael Collins. 2005. Morphology 
and Reranking for the Statistical Parsing of Span-
ish. In Proceedings of EMNLP 2005. 
G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer. 2008. 
Dependency Parsing of Turkish. Computational 
Linguistics, Vol. 34 (3). 
Yoav Goldberg and Reut Tsarfaty. 2008. A Single Gen-
erative Model for Joint Morphological Segmenta-
tion and Syntactic Parsing. Proceedings of ACL-
HLT 2008, Colombus, Ohio, USA.  
Johan Hall, Jens Nilsson, Joakim Nivre, G?lsen Eryigit, 
Be?ta Megyesi, Mattias Nilsson and Markus Saers. 
2007. Single Malt or Blended? A Study in Multilin-
gual Parser Optimization. Proceedings of the CoNLL 
Shared Task EMNLP-CoNLL. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith, 
Eric P. Xing. 2008. Stacking Dependency Parsing. 
Proceedings of EMNLP-2008. 
Jens Nilsson, Joakim Nivre and Johan Hall. 2007. Gen-
eralizing Tree Transformations for Inductive De-
pendency Parsing. Proceedings of the 45th 
Conference of the ACL. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Springer. 
Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A., 
G?lsen Eryi?it, Sandra K?bler, Marinov S., and 
Edwin Marsi. 2007a. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering.  
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson,  Sebastian Riedel and 
Deniz Yuret. 2007b. The CoNLL 2007 Shared Task 
on Dependency Parsing. Proceedings of EMNLP-
CoNLL. 
Joakim Nivre and Ryan McDonald. 2008. Integrating 
graph-based and transition-based dependency pars-
ers. Proceedings of ACL-2008. 
Ivan Titov and James Henderson. 2007. Fast and robust 
multilingual dependency parsing with a generative 
latent variable model. In Proceedings of the CoNLL 
2007 Shared Task (EMNLP-CoNLL). 
Reut Tsarfaty, Khalil Sima?an, and Remko Scha. 2009. 
An Alternative to Head-Driven Approaches for 
Parsing a (Relatively) Free Word-Order Language. 
Proceedings of EMNLP. 
 
 
39
Proceedings of BioNLP Shared Task 2011 Workshop, pages 138?142,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Using Kybots for Extracting Events in Biomedical Texts
Arantza Casillas (*)
arantza.casillas@ehu.es
Arantza D??az de Ilarraza (?)
a.diazdeilarraza@ehu.es
Koldo Gojenola (?)
koldo.gojenola@ehu.es
Maite Oronoz (?)
maite.oronoz@ehu.es
German Rigau (?)
german.rigau@ehu.es
IXA Taldea UPV/EHU
(*) Department of Electricity and Electronics
(?) Department of Computer Languages and Systems
Abstract
In this paper we describe a rule-based sys-
tem developed for the BioNLP 2011 GENIA
event detection task. The system applies Ky-
bots (Knowledge Yielding Robots) on anno-
tated texts to extract bio-events involving pro-
teins or genes. The main goal of this work is to
verify the usefulness and portability of the Ky-
bot technology to the domain of biomedicine.
1 Introduction
The aim of the BioNLP?11 Genia Shared Task (Kim
et al, 2011b) concerns the detection of molecular
biology events in biomedical texts using NLP tools
and methods. It requires the identification of events
together with their gene or protein arguments. Nine
event types are considered: localization, binding,
gene expression, transcription, protein catabolism,
phosphorylation, regulation, positive regulation and
negative regulation.
When identifying the events related to the given
proteins, it is mandatory to detect also the event
triggers, together with its associated event-type, and
recognize their primary arguments. There are ?sim-
ple? events, concerning an event together with its
arguments (Theme, Site, ...) and also ?complex?
events, or events that have other events as secundary
arguments. Our system did not participate in the op-
tional tasks of recognizing negation and speculation.
The training dataset contained 909 texts together
with a development dataset of 259 texts. 347 texts
were used for testing the system.
The main objective of the present work was to ver-
ify the applicability of a new Information Extraction
(IE) technology developed in the KYOTO project1
(Vossen et al, 2008), to a new specific domain. The
KYOTO system comprises a general and extensible
multilingual architecture for the extraction of con-
ceptual and factual knowledge from texts, which has
already been applied to the environmental domain.
Currently, our system follows a rule-based ap-
proach (i.e. (Kim et al, 2009), (Kim et al, 2011a),
(Cohen et al, 2011) or (Vlachos, 2009)), using a set
of manually developed rules.
2 System Description
Our system proceeds in two phases. Firstly, text doc-
uments are tokenized and structured using an XML
layered structure called KYOTO Annotation Format
(KAF) (Bosma et al, 2009). Secondly, a set of Ky-
bots (Knowledge Yielding Robots) are applied to de-
tect the biological events of interest occurring in the
KAF documents. Kybots form a collection of gen-
eral morpho-syntactic and semantic patterns on se-
quences of KAF terms. These patterns are defined
in a declarative format using Kybot profiles.
2.1 KAF
Firstly, basic linguistic processors apply segmenta-
tion and tokenization to the text. Additionally, the
offset positions of the proteins given by the task or-
ganizers are also considered. The output of this ba-
sic processing is stored in KAF, where words, terms,
syntactic and semantic information can be stored in
separate layers with references across them.
Currently, our system only considers a minimal
amount of linguistic information. We are only using
1http://www.kyoto-project.eu/
138
the word form and term layers. Figure 1 shows an
example of a KAF document where proteins have
been annotated using a special POS tag (PRT). Note
that our approach did not use any external resource
apart of the basic linguistic processing.
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<KAF xml:lang="en">
<text>
...
<wf wid="w210" sent="10">phosphorylation</wf>
<wf wid="w211" sent="10">of</wf>
<wf wid="w212" sent="10">I</wf>
<wf wid="w213" sent="10">kappaB</wf>
<wf wid="w214" sent="10">alpha<...
</text>
<term tid="t210" type="open" lemma="phosphorylation"
start="1195" end="1210" pos="W">
<span><target id="w210"/></span>
</term>
<term tid="t211" type="open" lemma="of"
start="1211" end="1213" pos="W">
<span><target id="w211"/></span>
</term>
<term tid="T5" type="open" lemma="I kappaB alpha"
start="1214" end="1228" pos="PRT">
<span><target id="w212"/></span>
<target id="w213"/>
<target id="w214"/></span>
</term>...
</terms>
</KAF>
Figure 1: Example of a document in KAF format.
2.2 Kybots
Kybots (Knowledge Yielding Robots) are abstract
patterns that detect actual concept instances and re-
lations in KAF. The extraction of factual knowledge
by the mining module is done by processing these
abstract patterns on the KAF documents. These pat-
terns are defined in a declarative format using Kybot
profiles, which describe general morpho-syntactic
and semantic conditions on sequences of terms. Ky-
bot profiles are compiled to XQueries to efficiently
scan over KAF documents uploaded into an XML
database. These patterns extract and rank the rele-
vant information from each match.
Kybot profiles are described using XML syn-
tax and each one consists of three main declarative
parts:
? Variables: In this part, the entities and its prop-
erties are defined
? Relations: This part specifies the positional re-
lations among the previously defined variables
? Events: describes the output to be produced for
every matching
Variables (see the Kybot section variables in fig-
ure 2) describe the term variables used by the Kybot.
They have been designed with the aim of being flex-
ible enough to deal with many different information
associated with the KAF terms including semantic
and ontological statements.
Relations (see the Kybot section relations in fig-
ure 2) define the sequence of variables the Kybot
is looking for. For example, in the Kybot in fig-
ure 2, the variable named Phosphorylation
is the main pivot, the variable Of must follow
Phosphorylation (immediate is true indi-
cating that it must be the next term in the sequence),
and a variable representing a Proteinmust follow
Of. Proteins and genes are identified with the PRT
tag.
Events (expressions marked as events in figure 2)
describes the output template of the Kybot. For ev-
ery matched pattern, the kybot produces a new event
filling the template structure with the selected pieces
of information. For example, the Kybot in figure 2
selects some features of the event represented with
the variable called Phosphorylation: its term-
identification (@tid), its lemma, part of speech and
offset. The expression also describes that the vari-
able Protein plays the role of being the ?Theme?
of the event. The output obtained when aplying the
Kybot in figure 2 is shown in figure 3. Comparing
the examples in table 1 and in figure 3 we observe
that all the features needed for generating the files
for describing the results are also produced by the
Kybot.
<doc shortname="PMID-9032271.kaf">
<event eid="e1" target="t210" kybot="phosphorylation of P"
type="Phosphorylation"
lemma="phosphorylation" start="1195" end="1210" />
<role target="T5" rtype="Theme"
lemma="I kappaB alpha" start="1214" end="1228" />
</doc>
Figure 3: Output obtained after the application of the Ky-
bot in figure 2.
3 GENIA Event Extraction Task and
Results
We developed a set of basic auxiliary pro-
grams to extract event patterns from the train-
ing corpus. These programs obtain the struc-
139
<?xml version="1.0" encoding="utf-8"?>
<!-- Sentence: phosphorylation of Protein
Event1: phosphorylation
Role: Theme Protein -->
<Kybot id="bionlp">
<variables>
<var name="Phosphorylation" type="term" lemma="phosphorylat*>
<var name="Of" type="term" lemma="of"/>
<var name="Protein" type="term" pos="PRT"/>
</variables>
<relations>
<root span="Phosphorylation"/>
<rel span="Of" pivot="Phosphorylation" direction="following" immediate="true"/>
<rel span="Protein" pivot="Of" direction="following" immediate="true"/>
</relations>
<events>
<event eid="" target="$Phosphorylation/@tid" kybot="phosphorylation of P"
type="Phosphorylation" lemma="$Phosphorylation/@lemma"
pos="$Phosphorylation/@pos" start="$Phosphorylation/@start" end="$Phosphorylation/@end"/>
<role target="$Protein/@tid" rtype="Theme" lemma="$Protein/@lemma" start="$Protein/@start"
end="$Protein/@end"/>
</events>
</Kybot>
Figure 2: Example of a Kybot for the pattern Event of Protein.
.a1 file
T5 Protein 1214 1228 I kappaB alpha
.a2 file
T20 Phosphorylation 1195 1210 phosphorylation
E7 Phosphorylation:T20 Theme:T5
Table 1: Results in the format required in the GENIA
shared task.
ture of the events, their associated trigger words
and their frequency. For example, in the
training corpus, a pattern of the type Event
of Protein appears 35 times, where the
Event is further described as phosporylation,
phosphorylated.... Taking the most fre-
quently occurring patterns in the training data into
account, we manually developed the set of Kybots
used to extract the events from the development and
test corpora. For example, in this way we wrote the
Kybot in figure 2 that fulfils the conditions of the
pattern of interest.
The two phases mentioned in section 2, corre-
sponding to the generation of the KAF documents
and the application of Kybots, have different input
files depending on the type of event we want to
detect: simple or complex events. When extract-
ing simple events (see figure 4), we used the in-
put text and the files containing protein annotations
(?.a1? files in the task) to generate the KAF docu-
ments. These KAF documents and Kybots for sim-
ple events are provided to the mining module. In
the case of complex events (events that have other
KAF generator
.txt .a1
.kaf
Kybot processor
Kybots
(Simple)
.a2
Figure 4: Application of Kybots. Simple events.
events as arguments), the identifiers of the detected
simple events are added to the KAF document in the
first phase. A new set of Kybots describing complex
events and KAF (now with annotations of the simple
events) are used to obtain the final result (see figure
5).
For the evaluation, we also developed some pro-
grams for adapting the output of the Kybots (see fig-
ure 3) to the required format (see table 1).
We used the development corpus to improve the
Kybot performance. We developed 65 Kybots for
detecting simple events. Table 2 shows the number
of Kybots for each event type. Complex events rela-
tive to regulation (also including negative and posi-
tive regulations) were detected using a set of 24 Ky-
bots.
The evaluation of the task was based on the output
140
KAF generator
.a2 .kaf
.kaf
(with simple events)
Kybot processor
Kybots
(Complex)
.a2
Figure 5: Application of Kybots. Complex events.
Event Class Simple Kyb. Complex Kyb.
Transcription 10
Protein Catabolism 5
Binding 5
Regulation 3
Negative Regulation 5 4
Positive Regulation 3 17
Localization 7
Phosphorylation 18
Gene Exrpesion 12
Total 65 24
Table 2: Number of Kybots generated for each event.
of the system when applied to the test dataset of 347
previously unseen texts. Table 3 shows in the Gold
column the number of instances for each event-type
in the test corpus. R, P and F-score columns stand
for the recall, precision and f-score the system ob-
tained for each type of event. As a consequence of
the characteristics of our system, precision is primed
over recall. For example, the system obtains 95%
and 97% precision on Phosphorylation an Localiza-
tion events, respectively, although its recall is con-
siderably lower (41% and 19%).
4 Conclusions and Future work
This work presents the first results of the applica-
tion of the KYOTO text mining system for extracting
events when ported to the biomedical domain. The
KYOTO technology and data formats have shown to
be flexible enough to be easily adapted to a new task
and domain. Although the results are far from satis-
factory, we must take into account the limited effort
we dedicated to adapting the system and designing
the kybots, which can be roughly estimated in two
Event Class Gold R P F-score
Localization 191 19.90 97.44 33.04
Binding 491 5.30 50.00 9.58
Gene Expression 1002 54.19 42.22 47.47
Transcription 174 13.22 62.16 21.80
Protein catabolism 15 26.67 44.44 33.33
Phosphorylation 185 41.62 95.06 57.89
Non-reg total 2058 34.55 47.27 39.92
Regulation 385 7.53 9.63 8.45
Positive regulation 1443 6.38 62.16 11.57
Negative regulation 571 3.15 26.87 5.64
Regulatory total 2399 5.79 26.94 9.54
All total 4457 19.07 42.08 26.25
Table 3: Performance analysis on the test dataset.
person/months.
After the final evaluation, our system obtained the
thirteenth position out of 15 participating systems
in the main task (processing PubMed abstracts and
full paper articles), obtaining 19.07%, 42.08% and
26.25 recall, precision an f-score, respectively, far
from the best competing system (49.41%, 64.75%
and 56.04%). Although they are far from satisfac-
tory, we must take into account the limited time we
dedicated to adapting the system and designing the
kybots. Apart from that, due to time restrictions,
our system did not make use of the ample set of
resources available, such as named entities, corefer-
ence resolution or syntactic parsing of the sentences.
On the other hand, the system, based on manually
developed rules, obtains reasonable accuracy in the
task of processing full paper articles, obtaining 45%
precision and 21% recall, compared to 59% and 47%
for the best system, which means that the rule-based
approach performs more robustly when dealing with
long texts (5 full texts correspond to approximately
150 abstracts). As we have said before, our main
objective was to evaluate the capabilities of the KY-
OTO technology without adding any additional in-
formation. The use of more linguistic information
will probably facilitate our work and will benefit the
system results. In the near future we will study the
application of machine learning techniques for the
automatic generation of Kybots from the training
data. We also plan to include additional linguistic
and semantic processing in the event extraction pro-
cess to exploit the current semantic and ontological
capabilities of the KYOTO technology.
141
Acknowledgments
This research was supported by the the KYOTO
project (STREP European Community ICT-2007-
211423) and the Basque Government (IT344-10).
References
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini and Carlo Aliprandi. KAF: a generic semantic
annotation format Proceedings of the 5th International
Conference on Generative Approaches to the Lexicon
GL 2009 Pisa, Italy, September 17-19, 2009
Kevin Bretonnel Cohen, Karin Verspoor, Helen L. John-
son, Chris Roeder, Philip V. Ogren, Willian A. Baum-
gartner, Elizabeth White, Hannah Tipney, and Lawer-
ence Hunter. High-precision biological event extrac-
tion: Effects of system and data. Computational Intel-
ligence, to appear, 2011.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano and Jun?ichi Tsujii. Overview of
BioNLP?09 Shared Task on Event Extraction. Pro-
ceedings of the BioNLP 2009 Workshop. Association
for Computational Linguistics. Boulder, Colorado, pp.
89?96., 2011
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Junichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. Proceedings of the
BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task. Association for Computational Linguis-
tics. Portland, Oregon, 2011.
Andreas Vlachos. Two Strong Baselines for the BioNLP
2009 Event Extraction Task. Proceedings of the 2010
Workshop on Biomedical Natural Language Process-
ing. Association for Computational Linguistics Upp-
sala, Sweden, pp. 1?9., 2010
Piek Vossen, Eneko Agirre, Nicoletta Calzolari, Chris-
tiane Fellbaum, Shu-kai Hsieh, Chu-Ren Huang, Hi-
toshi Isahara, Kyoko Kanzaki, Andrea Marchetti,
Monica Monachini, Federico Neri, Remo Raffaelli,
German Rigau, Maurizio Tescon, Joop VanGent. KY-
OTO: A System for Mining, Structuring, and Dis-
tributing Knowledge Across Languages and Cultures.
Proceedings of LREC 2008. Marrakech, Morocco,
2008.
142
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 48?54,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Combining Rule-Based and Statistical Syntactic Analyzers 
 
 
Mar?a Jes?s Aranzabe*, Arantza D?az de Ilarraza, Nerea Ezeiza, Kepa Bengoetxea, 
Iakes Goenaga, Koldo Gojenola,  
Department of Computer Languages and Systems / * Department of Basque Philology 
University of the Basque Country UPV/EHU 
{maxux.aranzabe@ehu.es, kepa.bengoetxea, jipdisaa@si.ehu.es, 
n.ezeiza@ehu.es, koldo.gojenola@ehu.es, iakes@gmail.com} 
 
 
 
Abstract 
This paper presents the results of a set of 
preliminary experiments combining two 
knowledge-based partial dependency 
analyzers with two statistical parsers, 
applied to the Basque Dependency 
Treebank. The general idea will be to apply 
a stacked scheme where the output of the 
rule-based partial parsers will be given as 
input to MaltParser and MST, two state of 
the art statistical parsers. The results show 
a modest improvement over the baseline, 
although they also present interesting lines 
for further research. 
1. Introduction 
In this paper we present a set of preliminary 
experiments on the combination of two 
knowledge-based partial syntactic analyzers with 
two state of the art data-driven statistical parsers. 
The experiments have been performed on the 
Basque Dependency Treebank (Aduriz et al, 
2003). 
In the last years, many attempts have been 
performed trying to combine different parsers 
(Surdeanu and Manning, 2010), with significant 
improvements over the best individual parser?s 
baseline. The two most successful approaches have 
been stacking (Martins et al, 2008) and voting 
(Sagae and Lavie, 2006, Nivre and McDonald, 
2008, McDonald and Nivre, 2011). In this paper 
we will experiment the use of the stacking 
technique, giving the tags obtained by the rule-
based syntactic partial parsers as input to the 
statistical parsers. 
Morphologically rich languages present new 
challenges, as the use of state of the art parsers for 
more configurational and non-inflected languages 
like English does not reach similar performance 
levels in languages like Basque, Greek or Turkish 
(Nivre et al, 2007a). As it was successfully done 
on part of speech (POS) tagging, where the use of 
rule-based POS taggers (Tapanainen and 
Voutilainen, 1994) or a combination of a rule-
based POS tagger with a statistical one (Aduriz et 
al., 1997, Ezeiza et al, 1998) outperformed purely 
statistical taggers, we think that exploring the 
combination of knowledge-based and data-driven 
systems in syntactic processing can be an 
interesting line of research. 
Most of the experiments on combined parsers 
have relied on different types of statistical parsers 
(Sagae and Lavie, 2006, Martins et al, 2008, 
McDonald and Nivre, 2011), trained on an 
automatically annotated treebank. Yeh (2000) used 
the output of several baseline diverse parsers to 
increase the performance of a second 
transformation-based parser. In our work we will 
study the use of two partial rule-based syntactic 
analyzers together with two data-driven parsers: 
? A rule-based chunker (Aduriz et al, 2004) 
that marks the beginning and end of noun 
phrases, postpositional phrases and verb 
chains, in the IOB (Inside/ 
Outside/Beginning of a chunk) style. 
? A shallow dependency relation annotator 
(Aranzabe et al, 2004), which tries to 
detect dependency relations by assigning a 
48
set of predefined tags to each word, where 
each tag gives both the name of a 
dependency relation (e.g. subject) together 
with the direction of its head (left or right). 
? We will use two statistical dependency 
parsers, MaltParser (Nivre et al, 2007b) 
and MST (McDonald et al 2005). 
In the rest of this paper, section 2 will first 
present the corpus and the different parsers we will 
combine, followed by the experimental results in 
section 3, and the main conclusions of the work. 
2. Resources 
This section will describe the main resources that 
have been used in the experiments. First, 
subsection 2.1 will describe the Basque 
Dependency Treebank, and then subsection 2.2 
will explain the main details of the analyzers that 
have been employed. The analyzers are a rule-
based chunker, a rule-based shallow dependency 
parser and two state of the art data-driven 
dependency parsers, MaltParser and MST.  
2.1 Corpora 
Our work will make use the second version of the 
Basque dependency Treebank (BDT II, Aduriz et 
al., 2003), containing 150,000 tokens (11,225 
sentences). Figure 1 presents an example of a 
syntactically annotated sentence. Each word 
contains its form, lemma, category or coarse part 
of speech (CPOS), POS, morphosyntactic features 
such as case, number of subordinate relations, and 
the dependency relation (headword + dependency). 
The information in figure 1 has been simplified 
due to space reasons, as typically each word 
contains many morphosyntactic features (case, 
number, type of subordinated sentence, ...), which 
are relevant for parsing. The last two lines of the 
sentence in figure 1 do not properly correspond to 
the treebank, but are the result of the rule-based 
partial syntactic analyzers (see subsection 2.2). For 
evaluation, we divided the treebank in three sets, 
corresponding to training, development, and test 
(80%, 10%, and 10%, respectively). The 
experiments were performed on the development 
set, leaving the best system for the final test. 
2.2 Analyzers 
This subsection will present the four types of 
analyzers that have been used. The rule-based 
analyzers are based on the Contraint Grammar 
(CG) formalism (Karlsson et al, 1995), based on 
the assignment of morphosyntactic tags to words 
using a formalism that has the capabilities of finite 
state automata or regular expressions, by means of 
a set of rules that examine mainly local contexts of 
words to determine the correct tag assignment. 
The rule-based chunker (RBC henceforth, 
Aranzabe et al, 2009) uses 560 rules, where 479 of 
the rules deal with noun phrases and the rest with 
verb phrases. The chunker delimits the chunks with 
three tags, using a standard IOB marking style (see 
figure 1). The first one is to mark the beginning of 
the phrase (B-VP if it is a verb phrase and B-NP 
whether it's a noun phrase) and the other one to 
mark the continuation of the phrase (I-NP or I-VP, 
meaning that the word is inside an NP or VP). The 
last tag marks words that are outside a chunk. The 
evaluation of the chunker on the BDT gave a result 
of 87% precision and 85% recall over all chunks. 
We must take into account that this evaluation was 
auxmod 
ccomp_obj 
 auxmod 
Gizonak    mutil    handia   etorri     dela        esan      du . 
The-man       boy        tall-the    come         has+he+that     tell      he+did+it   
N-ERG-S       N          ADJ-ABS-S   V            AUXV+S+COMPL    V         AUXV 
B-NP          B-NP       I-NP        B-VP         I-NP            B-VP      I-VP 
&NCSUBJ>      &NCSUBJ>   $<NCMOD     $CCOMP_OBJ>  &<AUXMOD        &MAINV    &<AUXMOD 
ncsubj 
ncmod 
ncsubj 
Figure 1. Dependency tree for the sentence Gizonak mutil handia etorri dela esan du (the man told that the tall 
boy has come). The two last lines show the tags assigned by the rule-based chunker and the rule-based 
dependency analyzer, respectively. 
(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG = 
ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, 
&MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). 
 
49
performed on the gold POS tags, rather than on 
automatically assigned POS tasks, as in the present 
experiment. For that reason, the results can serve 
as an upper bound on the real results. 
The rule-based dependency analyzer (RBDA, 
Aranzabe et al, 2004) uses a set of 505 CG rules 
that try to assign dependency relations to 
wordforms. As the CG formalism only allows the 
assignment of tags, the rules only aim at marking 
the name of the dependency relation together with 
the direction of the head (left or right). For 
example, this analyzer assigns tags of the form 
&NCSUBJ> (see figure 1), meaning that the 
corresponding wordform is a non-clausal syntactic 
subject and that its head is situated to its right (the 
?>? or ?<? symbols mark the direction of the 
head). This means that the result of this analysis is 
on the one hand a partial analysis and, on the other 
hand, it does not define a dependency tree, and can 
also be seen as a set of constraints on the shape of 
the tree. The system was evaluated on the BDT, 
obtaining f-scores between 90% for the auxmod 
dependency relation between the auxiliary and the 
main verb and 52% for the subject dependency 
relation, giving a (macro) average of 65%. 
Regarding the data-driven parsers, we have 
made use of MaltParser (Nivre et al, 2007b) and 
MST Parser (McDonald et al, 2006), two state of 
the art dependency parsers representing two 
dominant approaches in data-driven dependency 
parsing, and that have been successfully applied to 
typologically different languages and treebanks 
(McDonald and Nivre, 2007).  
MaltParser (Nivre, 2006) is a representative of 
local, greedy, transition-based dependency parsing 
models, where the parser obtains deterministically 
a dependency tree in a single pass over the input 
using two data structures: a stack of partially 
analyzed items and the remaining input sequence. 
To determine the best action at each step, the 
parser uses history-based feature models and 
discriminative machine learning. The learning 
configuration can include any kind of information 
(such as word-form, lemma, category, subcategory 
or morphological features). Several variants of the 
parser have been implemented, and we will use 
one of its standard versions (MaltParser version 
1.4). In our experiments, we will use the Stack-
Lazy algorithm with the liblinear classifier.  
The MST Parser can be considered a 
representative of global, exhaustive graph-based 
parsing (McDonald et al, 2005, 2006). This 
algorithm finds the highest scoring directed 
spanning tree in a dependency graph forming a 
valid dependency tree. To learn arc scores, it uses 
large-margin structured learning algorithms, which 
optimize the parameters of the model to maximize 
the score margin between the correct dependency 
graph and all incorrect dependency graphs for 
every sentence in a training set. The learning 
procedure is global since model parameters are set 
relative to classifying the entire dependency graph, 
and not just over single arc attachments. This is in 
contrast to the local but richer contexts used by 
transition-based parsers. We use the freely 
available version of MSTParser1. In the following 
experiments we will make use of the second order 
non-projective algorithm.  
3. Experiments  
We will experiment the effect of using the output 
of the knowledge-based analyzers as input to the 
data-driven parsers in a stacked learning scheme. 
Figure 1 shows how the two last lines of the 
example sentence contain the tags assigned by the 
rule-based chunker (B-NP, I-NP, B-VP and I-VP) 
and the rule-based partial dependency analyzer 
(&NCSUBJ, &<NCMOD, &<AUXMOD, 
&CCOMP_OBJ and &MAINV) . 
The first step consisted in applying the complete 
set of text processing tools for Basque, including: 
? Morphological analysis. In Basque, each 
word can receive multiple affixes, as each 
lemma can generate thousands of word-
forms by means of morphological 
properties, such as case, number, tense, or 
different types of subordination for verbs. 
Consequently, the  morphological analyzer 
for Basque (Aduriz et al 2000) gives a 
high ambiguity. If only categorial (POS) 
ambiguity is taken into account, there is an 
average of 1.55 interpretations per word-
form, which rises to 2.65 when the full 
morphosyntactic information is taken into 
account, giving an overall 64% of 
ambiguous word-forms. 
? Morphological disambiguation. 
Disambiguating the output of 
morphological analysis, in order to obtain 
a single interpretation for each word-form, 
                                                           
1 http://mstparser.sourceforge.net 
50
can pose an important problem, as 
determining the correct interpretation for 
each word-form requires in many cases the 
inspection of local contexts, and in some 
others, as the agreement of verbs with 
subject, object or indirect object, it could 
also suppose the examination of elements 
which can be far from each other, added to 
the free constituent order of the main 
sentence elements in Basque. The 
erroneous assignment of incorrect part of 
speech or morphological features can 
difficult the work of the parser. 
? Chunker 
? Partial dependency analyzer 
When performing this task, we found the 
problem of matching the treebank tokens with 
those obtained from the analyzers, as there were 
divergences on the treatment of multiword units, 
mostly coming from Named Entities, verb 
compounds and complex postpositions (formed 
with morphemes appearing at two different words). 
For that reason, we performed a matching process 
trying to link the multiword units given by the 
morphological analysis module and the treebank, 
obtaining a correct match for 99% of the sentences.  
Regarding the data-driven parsers, they are 
trained using two kinds of tags as input: 
? POS and morphosyntactic tags coming 
from the automatic morphological 
processing of the dependency treebank. 
Disambiguation errors, such as an 
incorrect POS category or morphological 
analyses (e.g. the assignment of an 
incorrect case) can harm the parser, as 
tested in Bengoetxea et al (2011). 
? The output of the rule-based partial 
syntactic analyzers (two last lines of the 
example in figure 1). These tags contain 
errors of the CG-based syntactic taggers. 
As the analyzers are applied after 
morphological processing, the errors can 
be propagated and augmented. 
Table 1 shows the results of using the output of 
the knowledge-based analyzers as input to the 
statistical parsers. We have performed three 
experiments for each statistical parser, trying with 
the chunks provided by the chunker, the partial 
dependency parser, and both. The table shows 
modest gains, suggesting that the rule-based 
analyzers help the statistical ones, giving slight 
increases over the baseline, which are statistically 
significant when applying MaltParser to the output 
of the rule-based dependency parser and a 
combination of the chunker and rule-based parsers. 
As table 1 shows, the parser type is relevant, as 
MaltParser seems to be sensitive when using the 
stacked features, while the partial parsers do not 
seem to give any significant improvement to MST. 
3.1 Error analysis 
Looking with more detail at the errors made by the 
different versions of the parsers, we observe 
significant differences in the results for different 
dependency relations, seeing that the statistical 
parsers behave in a different manner regarding to 
each relation, as shown in table 2. The table shows 
the differences in f-score2  corresponding to five 
local dependency relations, (determination of 
verbal modifiers, such as subject, object and 
indirect object).  
McDonald and Nivre (2007) examined the types 
of errors made by the two data-driven parsers used 
in this work, showing how the greedy algorithm of 
MaltParser performed better with local dependency 
relations, while the graph-based algorithm of MST 
was more accurate for global relations. As both the 
chunker and the partial dependency analyzer are 
based on a set of local rules in the CG formalism, 
we could expect that the stacked parsers could 
benefit mostly on the local dependency relations. 
                                                           
2 f-score = 2 * precision * recall / (precision + recall) 
 MaltParser MST Parser 
 LAS UAS LAS UAS 
Baseline 76.77% 82.09%  77.96% 84.04% 
+ RBC 77.10% (+0.33) 82.29% (+0.20)  77.99% (+0.03) 83.99% (-0.05) 
+ RBDA *77.15% (+0.38) 82.27% (+0.18)  78.03% (+0.07) 83.76% (-0.28) 
+ RBC + RBDA  *77.25% (+0.48) 82.18% (+0.09)  78.00% (+0.04) 83.34% (-0.70) 
Table 1. Evaluation results  
(RBC = rule-based chunker, RBDA = rule-based dependency analyzer, LAS: Labeled Attachment Score,  
UAS: Unlabeled Attachment Score, *: statistically significant in McNemar's test, p < 0.05) 
 
51
Table 2 shows how the addition of the rule-based 
parsers? tags performs in accord with this behavior, 
as MaltParser gets f-score improvements for the 
local relations. Although not shown in Table 2, we 
also inspected the results on the long distance 
relations, where we did not observe noticeable 
improvements with respect to the baseline on any 
parser. For that reason, MaltParser, seems to 
mostly benefit of the local nature of the stacked 
features, while MST does not get a significant 
improvement, except for some local dependency 
relations, such as ncobj and ncsubj. 
We performed an additional test using the partial 
dependency analyzer?s gold dependency relations 
as input to MaltParser. As could be expected, the 
gold tags gave a noticeable improvement to the 
parser?s results, reaching 95% LAS. However, 
when examining the scores for the output 
dependency relations, we noticed that the gold 
partial dependency tags are beneficial for some 
relations, although negative for some others. For 
example the non-clausal modifier (ncmod) 
relation?s f-score increases 3.25 points, while the 
dependency relation for clausal subordinate 
sentences functioning as indirect object decreases 
0.46 points, which is surprising in principle. 
For all those reasons, the relation between the 
input dependency tags and the obtained results 
seems to be intricate, and we think that it deserves 
new experiments in order to determine their nature. 
As each type of syntactic information can have an 
important influence on the results on specific 
relations, their study can shed light on novel 
schemes of parser combination. 
4. Conclusions  
We have presented a preliminary effort to integrate 
different syntactic analyzers, with the objective of 
getting the best from each system. Although the 
potential gain is in theory high, the experiments 
have shown very modest improvements, which 
seem to happen in the set of local dependency 
relations. We can point out some avenues for 
further research: 
? Development of the rule-based 
dependency parser using the dependencies 
that give better improvements on the gold 
dependency tags, as this can measure the 
impact of each kind of shallow 
dependency tag on the data-driven parsers. 
? Development of rules that deal with the 
phenomena where the statistical parsers 
perform worse. This requires a careful 
error analysis followed by a redesign of 
the manually developed CG tagging rules. 
? Application of other types of combining 
schemes, such as voting, trying to get the 
best from each type of parser. 
Finally, we must also take into account that the 
rule-based analyzers were developed mainly 
having linguistic principles in mind, such as 
coverage of diverse linguistic phenomena or the 
treatment of specific syntactic constructions 
(Aranzabe et al, 2004), instead of performance-
oriented measures, such as precision and recall. 
This means that there is room for improvement in 
the first-stage knowledge-based parsers, which will 
have, at least in theory, a positive effect on the 
second-phase statistical parsers, allowing us to test 
whether knowledge-based and machine learning-
based systems can be successfully combined. 
Acknowledgements 
This research was supported by the Department of 
Industry of the Basque Government (IT344-10, S-
PE11UN114), the University of the Basque 
Country (GIU09/19) and the Spanish Ministry of 
 MaltParser MST Parser 
Dependency 
relation 
Baseline + RBC + RBDA + RBC  
+ RBDA 
Baseline + RBC + RBDA + RBC  
+ RBDA 
ncmod 75,29 75,90 76,08 76,40 77,15 77,44 76,39 76,92 
ncobj 67,34 68,49 69,67 69,54 64,85 64,86 65,56 66,18 
ncpred 61,37 61,92 61,26 63,50 60,37 57,55 58,44 59,27 
ncsubj 61,92 61,90 63,96 63,91 59,19 59,26 62,23 61,61 
nciobj 75,76 76,53 77,16 76,29 74,23 74,47 72,16 69,08 
Table 2. Comparison of the different parsers? f-score with regard to specific dependency relations 
(ncmod = non-clausal modifier, ncobj = non-clausal object, ncpred = non-clausal predicate, ncsubj = non-clausal subject, 
nciobj = non-clausal indirect object) 
52
Science and Innovation (MICINN, TIN2010- 
20218). 
References  
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, 
Arantza D?az de Ilarraza, Koldo Gojenola and 
Montse Maritxalar. 1997. Morphosyntactic 
disambiguation for Basque based on the Constraint 
Grammar Formalism. Conference on Recent 
Advances in Natural Language Processing 
(RANLP), Bulgaria. 
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki 
Alegria, Xabier Arregi, Jose Mari Arriola, Xabier 
Artola, Koldo Gojenola, Montserrat Maritxalar, Kepa 
Sarasola, and Miriam Urkia. 2000. A word-grammar 
based morphological analyzer for agglutinative 
languages. Coling 2000, Saarbrucken. 
Itziar Aduriz, Jos? Mar?a Arriola, Arantza D?az de 
Ilarraza, Koldo Gojenola, Maite Oronoz and Larraitz 
Uria. 2004. A cascaded syntactic analyser for 
Basque. In Computational Linguistics and Intelligent 
Text Processing, pages 124-135. LNCS Series. 
Springer Verlag. Berlin. 2004 
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria 
Arriola, Aitziber Atutxa, Arantza Diaz de Ilarraza, 
Aitzpea Garmendia and Maite Oronoz. 2003. 
Construction of a Basque dependency treebank. 
Treebanks and Linguistic Theories. 
Mar?a Jes?s Aranzabe, Jos? Mar?a Arriola and Arantza 
D?az de Ilarraza. 2004. Towards a Dependency 
Parser for Basque. In Proceedings of the Workshop 
on Recent Advances in Dependency Grammar, 
Geneva, Switzerland. 
Maria Jesus Aranzabe, Jose Maria Arriola and Arantza 
D?az de Ilarraza. 2009. Theoretical and 
Methodological issues of tagging Noun Phrase 
Structures following Dependency Grammar 
Formalism. In Artiagoitia, X. and Lakarra J.A. (eds) 
Gramatika Jaietan. Patxi Goenagaren omenez. 
Donostia: Gipuzkoako Foru Aldundia-UPV/EHU. 
Kepa Bengoetxea and Koldo Gojenola. 2010. 
Application of Different Techniques to Dependency 
Parsing of Basque. Proceedings of the 1st Workshop 
on Statistical Parsing of Morphologically Rich 
Languages (SPMRL), NAACL-HLT Workshop. 
Kepa Bengoetxea, Arantza Casillas and Koldo 
Gojenola. 2011. Testing the Effect of Morphological 
Disambiguation in Dependency Parsing of Basque. 
Proceedings of the International Conference on 
Parsing Technologies (IWPT). 2nd Workshop on 
Statistical Parsing Morphologically Rich Languages 
(SPMRL), Dublin, Ireland. 
G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer. 2008. 
Dependency Parsing of Turkish. Computational 
Linguistics, Vol. 34 (3).  
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n 
Urizar and Itziar Aduriz. 1998. Combining 
Stochastic and Rule-Based Methods for 
Disambiguation in Agglutinative Languages. 
COLING-ACL?98, Montreal. 
Fred Karlsson, Atro Voutilainen, Juka Heikkila and 
Arto Anttila. 1995. Constraint Grammar: A 
Language-independent System for Parsing 
Unrestricted Text. Mouton de Gruyter. 
Andr? F. T. Martins, Dipanjan Das, Noah A. Smith and 
Eric P. Xing. 2008. Stacking Dependency Parsing. 
Proceedings of EMNLP-2008. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2005. Online large-margin training of dependency 
parsers. In Proceedings of ACL. 
Ryan McDonald, Kevin Lerman and Fernando Pereira. 
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. CoNLL. 
Ryan McDonald and Joakim Nivre. 2007. 
Characterizing the Errors of Data-Driven 
Dependency Parsing Models. Proceedings of the 
2007 Joint Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning, EMNLP/CoNLL. 
Ryan McDonald and Joakim Nivre. 2011. Analyzing 
and Integrating Dependency Parsers. Computational 
Linguistics, Vol. 37(1), 197-230. 
Joakim Nivre. 2006. Inductive Dependency Parsing. 
Springer. 
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan 
McDonald, Jens Nilsson, Sebastian Riedel and Deniz 
Yuret. 2007a. The CoNLL 2007 Shared Task on 
Dependency Parsing. Proceedings of EMNLP-
CoNLL. 
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, 
G?lsen Eryi?it, Sandra K?bler, S. Marinov and 
Edwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency 
parsing. Natural Language Engineering.  
Joakim Nivre and Ryan McDonald. 2008. Integrating 
graph-based and transition-based dependency 
parsers. Proceedings of ACL-2008. 
Kenji Sagae and Alon Lavie. 2006. Parser Combination 
by Reparsing. Proceedings of the Human Language 
53
Technology Conference of the North American 
Chapter of the ACL, pages 129?132, New York. 
Mihai Surdeanu and Christopher D. Manning. 2010. 
Ensemble Models for Dependency Parsing: Cheap 
and Good? Proceedings of the Human Language 
Technology Conference of the North American 
Chapter of the ACL. 
Pasi Tapanainen and Atro Voutilainen. 1994. Tagging 
Accurately-Don?t guess if you know. Proceedings 
of the Conference on Applied Natural Language 
Processing, ANLP?94. 
Alexander Yeh. 2000. Using existing systems to 
supplement small amounts of annotated 
grammatical relations training data. Proceedings of 
ACL 2000. 
54
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 60?64,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
First approaches on Spanish medical record classification using
Diagnostic Term to class transduction
A. Casillas(1), A. D??az de Ilarraza(2), K. Gojenola(2), M. Oronoz(2), A. Pe?rez(2)
(1) Dep. Electricity and Electronics
(2) Dep. Computer Languages and Systems
University of the Basque Country (UPV/EHU)
arantza.casillas@ehu.es
Abstract
This paper presents an application of finite-
state transducers to the domain of medicine.
The objective is to assign disease codes to
each Diagnostic Term in the medical records
generated by the Basque Health Hospital Sys-
tem. As a starting point, a set of manually
coded medical records were collected in order
to code new medical records on the basis of
this set of positive samples. Since the texts
are written in natural language by doctors, the
same Diagnostic Term might show alternative
forms. Hence, trying to code a new medical
record by exact matching the samples in the
set is not always feasible due to sparsity of
data. In an attempt to increase the coverage
of the data, our work centered on applying a
set of finite-state transducers that helped the
matching process between the positive sam-
ples and a set of new entries. That is, these
transducers allowed not only exact matching
but also approximate matching. While there
are related works in languages such as En-
glish, this work presents the first results on au-
tomatic assignment of disease codes to medi-
cal records written in Spanish.
1 Introduction
During the last years an exponential increase in
the number of electronic documents in the medi-
cal domain has occurred. The automatic process-
ing of these documents allows to retrieve informa-
tion, helping the health professionals in their work.
There are different sort of valuable data that help to
exploit medical information. Our framework lays
on the classification of Medical Records (MRs) ac-
cording to a standard. In our context, the MRs pro-
duced in a hospital have to be classified with re-
spect to the World Health Organization?s 9th Revi-
sion of the International Classification of Diseases1
(ICD-9). ICD-9 is designed for the classification of
morbidity and mortality information and for the in-
dexing of hospital records by disease and procedure.
The already classified MRs are stored in a database
that serves for further classification purposes. Each
MR consists of two pieces of information:
Diagnostic Terms (DTs): one or more terms that
describe the diseases corresponding to the MR.
Body-text: a description of the patient?s details,
antecedents, symptoms, adverse effects, meth-
ods of administration of medicines etc.
Even though the DTs are within a limited domain,
their description is not subject to a standard. Doc-
tors express the DTs in natural language with their
own style and different degrees of precision. Usu-
ally, a given concept might be expressed by alterna-
tive DTs with variations due to modifiers, abbrevia-
tions, acronyms, dates, names, misspellings or style.
This is a typical problem that arises in natural lan-
guage processing due to the fact that doctors focus
on the patients and not so much on the writing of the
MR. On account of this, there is ample variability in
the presentation of the DTs. Consequently, it is not
a straightforward task to get the corresponding ICD-
codes. That is, the task is by far more complex than
a standard dictionary lookup.
1http://www.cdc.gov/nchs/icd/icd9.htm
60
The Basque Health Hospital System is concerned
with the automatization of this ICD-code assign-
ment task. So far, the hospital processes the daily
produced documents in the following sequence:
1. Automatic: exact match of the DTs in a set of
manually coded samples.
2. Semi-automatic: through semantic match,
ranking the DTs by means of machine-learning
techniques. This stage requires that experts se-
lect amongst the ranked choices.
3. Manual: the documents that were not matched
in the previous two stages are examined by pro-
fessional coders assigning the codes manually.
The goal of this paper is to bypass the variability
associated to natural language descriptions in an at-
tempt to maximize the proportion of automatically
assigned codes, as the Hospital System aims to ex-
pand the use of the automatic codification of MRs
to more hospitals. According to experts, even an in-
crease of 1% in exact match would represent a sig-
nificant improvement allowing to gain time and re-
sources.
Related work can be found in the literature. For
instance, Pestian et al (2007) reported on a shared
task involving the assignment of ICD-codes to radi-
ology reports written in English from a reduced set
of 45 codes. In general it implied the examination of
the full MR (including body-text). In our case, the
number of ICD-codes is above 1,000, although we
restrict ourselves to exact and approximate match
over the diagnoses.
Farkas and Szarvas (2008) used machine learning
for the automatic assignment of ICD-9 codes. Their
results showed that hand-crafted systems could be
reproduced by replacing several laborious steps in
their construction with machine learning models.
Tsuruoka et al (2008) presented a system that
tried to normalize different variants of the terms con-
tained in a medical dictionary, automatically getting
normalizing rules for genes, proteins, chemicals and
diseases in English.
The contribution of this work is: i) to collect
manually coded MRs in Spanish; ii) to approximate
transduction with finite-state (FS) models for auto-
matic MR coding and, iii) to assess the performance
of the proposed FS transduction approaches.
2 Approximate transduction
As it was previously mentioned, there are variations
regarding the DT descriptions due to style, miss-
spells, etc. Table 1 shows several pairs of DT and
ICD-codes within the collected samples that illus-
trate some of those variations.
DT ICD
1 Adenocarcinoma de prostata 185
2 Adenocarcinomas pro?stata. 185
3 Ca. prostata 185
4 CA?NCER DE PROSTATA 185
5 adenocarcinoma de pulmon estadio IV 1629
6 CA pulmo?n estadio 4 1629
7 ADENOCARCINOMA PANCREAS 1579
Table 1: Examples of DTs and their ICD-codes.
There are differences in the use of uppercase/lower
case; omissions of accents; use of both standard and
non-standard abbreviations (e.g. ca. for both ca?ncer
and adenocarcinoma); punctuation marks (inciden-
tal use of full-stop as commas, etc.); omission of
prepositions (see rows 1 and 2); equivalence be-
tween Roman and Arabic numerals (rows 5 and 6).
Due to these variations, our problem can be defined
as an approximate lookup in a dictionary.
2.1 Finite-state models
Foma toolkit was used to build the FS machines and
code the evaluation sets. Foma (Hulden, 2009) is
a freely available2 toolkit that allows to both build
and parse FS automata and transducers. Foma of-
fers a versatile layout that supports imports/exports
from/to other tools such as: Xerox XFST (Beesley
and Karttunen, 2003), AT&T (Mehryar Mohri
and Riley, 2003), OpenFST (Riley et al, 2009).
There are, as well, outstanding alternatives such as
HFST (Linde?n et al, 2010). Refer to (Yli-Jyra? et al,
2006) for a thorough inventory on FS resources.
The FS models in Figure 1 perform the conver-
sions necessary to carry out a soft match between
the dictionary entries and their variants.
? First, we define the transducer Accents that
takes into account the correspondences be-
tween standard letters and their versions using
accent text marks.
2http://code.google.com/p/foma
61
define Accents [a:a?|e:e?|i:??|o:o?|u:u?|...];
define Case [a:A|b:B|c:C|d:D|e:E|f:F|...];
define Spaces [..] (->) " " || [.#. | "."] , .#.;
define Punctuation ["."|"-"|" "]:["."|"-"|" "];
define Plurals [..] -> ([s|es]) || [.#. | "." | " "];
define PluralsI [s|es] (->) "" || [.#. | "." | ","| " "];
define Preps [..] (->) [de |del |con |por ] || " " ;
define Disease [enf|enf.|enfermedad]:[enf|enf.|enfermedad];
define AltCa [tumor|ca|ca.|carcinoma|adenocarcinoma|ca?ncer];
define TagNormCa AltCa:AltCa;
define AltIzq [izquierdo|izquierda|izq|izq.|izqda|izqda.|
izqdo|izqdo.|izda|izda.|izdo|izdo.];
define TagNormIzq AltIzq:AltIzq;
Figure 1: A simplified version in Foma source code of the regular expressions and transducers used to
bypass several sources of distortion within the DTs in order to parse variations of unseen input DTs.
? The expression Case matches uppercase and
lowercase versions of the DTs.
? There is a set of transducers (Spaces,
Punctuation, Plurals and PluralsI)
that deal with the addition or deletion of spaces
and separators (as full-stop, comma, and hy-
phen) between words or at the end of the DT.
? Prepositions. Many DTs can be differen-
tiated by the use or absence of prepositions, al-
though they correspond to the same ICD-code.
For that reason, we designed a transducer that
inserts or deletes the prepositions from a re-
duced set that were identified by inspection of
the training set. In this way, expressions as
?Adenocarcinoma prostata? and ?Adenocarci-
noma de prostata? can be mapped to each other.
? Tag Normalization of synonyms, vari-
ants and abbreviations. The examination of the
DTs in the training set revealed that there were
several terms used indistinctly, including syn-
onyms and different kinds of variants (mascu-
line and feminine) and abbreviations. For ex-
ample, the words adenocarcinoma, adenoca.,
carcinoma, ca, ca. and cancer serve to name
the same disease. There are also multiple vari-
ants of left/right, indicating the location of an
illness, that do not affect the assignment of the
ICD-code (e.g. izquierdo, izq., izda.).
Finally, all the FS transducers were composed
into a single machine that served to overcome all the
sources of distortion together.
3 Experimental results
To begin with, coded MRs produced in the hospi-
tal throughout 12 months were collected summing
up a total of 8,020 MRs as described in Table 2.
Note that there are ambiguities in our data-set since
there are 3,313 different DTs that have resulted in
3,407 (DT, ICD-code) different pairs (as shown in
Table 2). That is, the same DT was not always as-
signed the same ICD-code.
DT ICD-code
entries 8,020
different entries 3,407
different forms 3,313 1,011
Table 2: The data-set of (DT, ICD-code) pairs.
Next, the data-set was shuffled and divided into 3
disjoint sets for training, development and test pur-
poses as shown in Table 3.
train dev test
entries 6,020 1,000 1,000
different entries 2,825 734 728
Table 3: The data-set shuffled and divided into 3 sets
Using the set of mappings derived from the train-
ing set we performed the experiments on the devel-
opment set. After several rounds of tuning the sys-
tem, the resulting system was applied to the test set.
62
PERCENTAGE OF UNCLASSIFIED DTs
TRAIN EVAL-SET exact-match + case-ins. + punct. + plurals +preps. + tag-norm.
train dev 30.6 27.0 25.2 24.4 23.9 23.2
train test 29.8 26.7 25.1 24.8 24.3 23.2
train+dev test 27.7 24.5 23.0 22.9 22.5 21.4
Table 4: Performance of different FS machines in terms of the percentage of unclassified entries. All the
classified entries were correctly classified, yielding, as a result, a precision of 100%.
Given a DT, the goal is to find its corresponding
ICD-code despite the variations. Different FS ap-
proaches (described in Section 2.1) were proposed
to bypass particular sources of noise in the DT. Their
performance was assessed by means of the percent-
age of unclassified DTs, as summarized in Table 4.
Note that the lower the number of unclassified DTs
the better the performance. In each of the three rows
of Table 4 the results of different experimental se-
tups are shown: in the first two rows the training set
was used to build the models and either the devel-
opment or the test set was evaluated in their turn;
in the third row, both the training and the devel-
opment sets were used to build the model and the
test set was evaluated. The impact of adding pro-
gressively the FS machines built to tackle particular
sources of noise is shown by columns. Thus, the re-
sults of the last column represent the performance
of the transducer allowing exact-match search to-
gether with case-insensitive search, bypassing punc-
tuation marks, allowing plurals, bypassing preposi-
tions and allowing tag-normalization. The compo-
sition of each transducer outperforms the previous
result, yielding an improvement on the test of 6 ab-
solute points over the exact-match baseline, from
27.7% to 21.4%. As it can be derived from the
first column of Table 4 the test set contributed to the
training+development set with %27.7 of new DTs.
Overall, the FSMs progressively improved the re-
sults for the three series of experiments carried out
in more than 6%. As a result, less and less DTs are
left unclassified. In other words, the FS machines
tackling different sources of errors contribute to as-
sign ICD-codes to previously unassigned DTs.
A manual inspection over the results associated
to the evaluation of the development set (focus on
the first row of Table 4) showed that all the DTs
were correctly classified according to the training
data. Overall, the resulting transducer was unable
to classify 232 DTs out of 1,000 (see last column
in first row). Among the unclassified DTs, 10 out
of 232 were due to misspellings: e.g. cic atriz
(instead of cicatriz), desprendimineot (instead of
desprendimiento). In fact, spelling correction re-
ported improvements in related tasks (Patrick et al,
2010). The remaining DTs showed wider variations
in their forms, as unexpected degree of specificity
(e.g. named entities), spurious dates or numbers.
4 Conclusions
Medical records in Spanish were collected yielding
a data set of 8,020 DT and ICD-code pairs. While
there are a number of references dealing with En-
glish medical records, there are few for Spanish.
The goal of this work was to build a system that
given a DT it would find its corresponding ICD-
code as in a standard key-value dictionary. Yet, the
DTs are far from being standard since they contain
a number of variations. We proposed the use of sev-
eral FS models to bypass different variants and al-
low to provide ICD-codes even when the exact DT
was not found. Each source of variations was tack-
led with a specific transducer based on handwritten
rules. The composition of each machine improved
the performance of the system gradually, leading to
an improvement up to 6% in accuracy, from 27.7%
unclassified DTs with the exact-match baseline to
21.4% with the tag-normalization transducer.
Future work will focus on the unclassified DTs.
Together with FS models, other strategies shall be
explored. Machine-learning strategies in the field of
information retrieval might help to make the most of
the piece of information that was here discarded (i.e.
the body-text). All in all, regardless of the approach,
the command in this MR classification context is to
get an accuracy of 100%, possibly through the inter-
active inference framework (Toselli et al, 2011).
63
Acknowledgments
Authors would like to thank the Hospital Galdakao-
Usansolo for their contributions and support, in par-
ticular to Javier Yetano, responsible of the Clinical
Documentation Service.
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S-
PE11UN114, GIC10/158 IT375-10), the University
of the Basque Country (GIU09/19) and the Span-
ish Ministry of Science and Innovation (MICINN,
TIN2010- 20218).
References
[Beesley and Karttunen2003] Kenneth R. Beesley and
Lauri Karttunen. 2003. Finite State Morphology.
CSLI Publications,.
[Farkas and Szarvas2008] Richa?rd Farkas and Gyo?rgy
Szarvas. 2008. Automatic construction of rule-based
ICD-9-CM coding systems. BMC Bioinformatics., 9
(Suppl 3): S10.
[Hulden2009] Mans Hulden. 2009. Foma: a Finite-State
Compiler and Library. In EACL (Demos), pages 29?
32. The Association for Computer Linguistics.
[Linde?n et al2010] Krister Linde?n, Miikka Silfverberg,
and Tommi Pirinen. 2010. HFST tools for morphol-
ogy ? an efficient open-source package for construc-
tion of morphological analyzers.
[Mehryar Mohri and Riley2003] Fernando C. N. Pereira
Mehryar Mohri and Michael D. Riley. 2003. AT&T
FSM LibraryTM ? Finite-State Machine Library.
www.research.att.com/sw/tools/fsm.
[Patrick et al2010] Jon Patrick, Mojtaba Sabbagh, Suvir
Jain, and Haifeng Zheng. 2010. Spelling correction
in clinical notes with emphasis on first suggestion ac-
curacy. In 2nd Workshop on Building and Evaluating
Resources for Biomedical Text Mining (BioTxtM2010)
LREC. ELRA.
[Pestian et al2007] John P. Pestian, Chris Brew, Pawel
Matykiewicz, D. J Hovermale, Neil Johnson, K. Bre-
tonnel Cohen, and Wlodzislaw Duch. 2007. A shared
task involving multi-label classification of clinical free
text. In Biological, translational, and clinical lan-
guage processing, pages 97?104, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Riley et al2009] Michael Riley, Cyril Allauzen, and
Martin Jansche. 2009. OpenFST: An open-source,
weighted finite-state transducer library and its applica-
tions to speech and language. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, Companion Vol-
ume: Tutorial Abstracts, pages 9?10, Boulder, Col-
orado, May. Association for Computational Linguis-
tics.
[Toselli et al2011] Alejandro H. Toselli, Enrique Vi-
dal, and Francisco Casacuberta. 2011. Multi-
modal Interactive Pattern Recognition and Applica-
tions. Springer.
[Tsuruoka et al2008] Yoshimasa Tsuruoka, John Mc-
Naught, and Sophia Ananiadou. 2008. Normalizing
biomedical terms by minimizing ambiguity and vari-
ability. BMC Bioinformatics, 9(Suppl 3):S2.
[Yli-Jyra? et al2006] A. Yli-Jyra?, K. Koskenniemi, and
K.. Linde?n. 2006. Common infrastructure for
finite-state based methods and linguistic descriptions.
In Proceedings of International Workshop Towards
a Research Infrastructure for Language Resources.,
Genoa, May.
64
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 71?77,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Exploiting the Contribution of Morphological Information to Parsing: the
BASQUE TEAM system in the SPRML?2013 Shared Task
Iakes Goenaga, Nerea Ezeiza
IXA NLP Group
Faculty of Computer Science
Univ. of the Basque Country UPV/EHU
iakesg@gmail.com, n.ezeiza@ehu.es
Koldo Gojenola
IXA NLP Group
Technical School of Engineering, Bilbao
Univ. of the Basque Country UPV/EHU
koldo.gojenola@ehu.es
Abstract
This paper presents a dependency parsing
system, presented as BASQUE TEAM at
the SPMRL?2013 Shared Task, based on
the analysis of each morphological feature
of the languages. Once the specific rel-
evance of each morphological feature is
calculated, this system uses the most sig-
nificant of them to create a series of ana-
lyzers using two freely available and state
of the art dependency parsers, MaltParser
and Mate. Finally, the system will com-
bine previously achieved parses using a
voting approach.
1 Introduction
Morphologically rich languages present new chal-
lenges, as the use of state of the art parsers for
more configurational and non-inflected languages
like English does not reach similar performance
levels in languages like Basque, Greek or Turk-
ish (Nivre et al, 2007). Using morphological in-
formation as features in parsing has been a com-
monly used method for parsing MRLs (Tsarfaty et
al., 2010). In some cases the effect of this infor-
mation is positive but in others it does not help or
causes a negative effect.
In most of the work on dependency parsing, the
specific relevance of each morphological feature
in the final result is unknown. The authors include
all the morphological features1 in their systems
with the aim of taking advantage of the diversity
of the used information. This approach commonly
produces very good results but they are not always
the best ones (see table 2).
On the other hand, some authors have made ex-
periments to specify which is the real impact of
1That is, they treat all the morphological features in the
same way in the feature specification, and let the learning
algorithms decide the weight assigned to each one.
the morphological features. Ambati et al (2010)
explore ways of integrating local morphosyntactic
features into Hindi dependency parsing. They ex-
periment with different sets of features on a graph-
based and a transition-based dependency parser.
They show that using some morphological fea-
tures (root, case, and suffix) outperforms a base-
line using POS as the only feature, with both gold
and predicted settings .
Bengoetxea and Gojenola (2010) make use of
MaltParser?s feature configuration file to take ad-
vantage of morphological features in parsing with
gold data. Their experiments show that case and
subordination type considerably increase parsing
accuracy.
Marton et al (2013) also explore which mor-
phological features could be useful in dependency
parsing of Arabic. They observe the effect of fea-
tures by adding them one at a time separately and
comparing the outcomes. Experiments showed
that when gold morphology is provided, case
markers help the most, whereas when the mor-
phology is automatically predicted the outcome
is the opposite: using case harms the results the
most. When features are combined in a greedy
heuristic, using definiteness, person, number, and
gender information improves accuracy.
Similarly, Seeker and Kuhn (2013) also deter-
mine that the use of case is specially relevant for
parsing, demonstrating that morpho-syntactic con-
straints can delimit the search space of a statistical
dependency parser to outperform state-of-the-art
baselines for Czech, German and Hungarian.
Following this line of research, our first step
will be to determine which is the concrete value of
each feature on dependency parsing, adding one of
the morphological features at a time starting with
an empty FEATS column.
C?etinog?lu and Kuhn (2013) have shown that
some parsers tend to improve the results when
swapping or replacing POS by some of the mor-
71
phological features. They have made use of the
METU-Sabanc Turkish Treebank (Oflazer et al,
2003) for training and the ITU validation set
(Eryigit, 2007) for testing. In their work, it is ob-
served that moving CASE to the POS field helps
with a 0.3% LAS absolute increase in the gold
pipeline settings and using CASE instead of nom-
inal POS improves the labelled accuracy by 0.3%
absolute for the training set.
These experiments suggest that in some way
the parser is not making an optimal use of all the
available morpho-syntactic information, and that
the parser algorithm (or the feature specification
for the learning phase) is geared towards POS and
CPOS, giving a lower status to other types of in-
formation. Although this strategy is good in gen-
eral, it seems that, at least for some languages, spe-
cific features (e.g. CASE) are crucial in obtaining
a high parsing performance. Taking these ideas
into consideration, we will work on three different
approaches:
? We will experiment the effect of using only
the best three morphological features in the
FEATS column (see table 1), compared to
working with the full set of morpho-syntactic
features. This can have the effect of speed-
ing the learning and parsing processes, as the
number of features can be smaller. On the
other hand, the elimination of non-relevant
features can also help to improve the parser?s
results, because some features can even be
detrimental for parsing.
? Following C?etinog?lu and Kuhn (2013), once
our system resolves which feature is the most
significant, it will be used to replace the POS
and CPOS fields one by one and we will test
the effect of these variants on the parsers. Fi-
nally, we will also try right-to-left versions
of those 3 variants (baseline, and replacing
POS and CPOS) completing a set of 6 differ-
ent parsers.
? Finally, we will experiment the combination
of the different or parsers with a voting ap-
proach (Hall et al, 2010) using the Malt-
Blender tool2.
All of the experiments will be performed on
automatically predicted POS and morphosyntactic
data, taking the tags given in the Shared Task data,
2http://w3.msi.vxu.se/users/jni/blend/
that is, we will not made use of any specifically
trained morphological tagger.
In the rest of this paper we will first present
the resources we have used to carry out our ex-
periments in section 2, followed by a study of the
contribution of the morphological information to
parsing in section 3 and the effect of this infor-
mation on the individual parsers in subsection 4.1.
The final results of the best parser combinations
are showed in subsection 4.2 and the main conclu-
sions of the work in section 5.
2 Resources
This section will describe the main resources that
have been used in the experiments. Subsection
2.1 will describe the languages we have used in
our experiments, subsection 2.2 will explain the
parsers we use, while subsection 2.3 will present
briefly the MaltBlender tool.
2.1 Selected Languages
Although the SPMRL?2013 Shared Task (Seddah
et al, 2013) offers the opportunity to parse nine
morphologically rich languages, to carry out our
experiments we have selected five of them, due in
part to time constraints, but also taking into ac-
count the relevance of the morpho-syntactic infor-
mation (FEATS column, see table 1) . The selected
five languages are: Basque (Aduriz et al, 2003),
French (Abeille? et al, 2003), German (Seeker and
Kuhn, 2012), Hungarian (Vincze et al, 2010) and
Swedish (Nivre et al, 2006).
2.2 Parsers
We have made use of MaltParser (Nivre et al,
2007b) and Mate (Bohnet and Nivre, 2012), two
state of the art dependency parsers3 representing
the dominant approaches in data-driven depen-
dency parsing, and that have been successfully
applied to typologically different languages and
treebanks.
MaltParser is a representative of local, greedy,
transition-based dependency parsing models,
where the parser obtains deterministically a
dependency tree in a single pass over the input
using two data structures: a stack of partially
analyzed items and the remaining input sequence.
To determine the best action at each step, the
3Due to time constraints, we did not have enough time to
experiment with other options such as the MST parser or the
EasyFirst parser.
72
parser uses history-based feature models and dis-
criminative machine learning. The specification
of the learning configuration can include any
kind of information (such as word-form, lemma,
category, subcategory or morphological features).
We will use one of its latest versions (MaltParser
version 1.7).
To fine-tune Maltparser we have used MaltOp-
timizer (Ballesteros and Nivre, 2012a; Ballesteros
and Nivre, 2012b). This tool is an interactive sys-
tem that first performs an analysis of the training
set in order to select a suitable starting point for
optimization and then guides the user through the
optimization of parsing algorithm, feature model,
and learning algorithm. Empirical evaluation on
data from the CoNLL 2006 and 2007 shared tasks
on dependency parsing shows that MaltOptimizer
consistently improves over the baseline of default
settings and sometimes even surpasses the result
of manual optimization.
The Mate parser (Bohnet and Nivre, 2012) is a
development of the algorithms described in (Car-
reras, 2007; Johansson and Nugues, 2008). It basi-
cally adopts the second order maximum spanning
tree dependency parsing algorithm. In particular,
this parser exploits a hash kernel, a new parallel
parsing and feature extraction algorithm that im-
proves accuracy as well as parsing speed (Bohnet,
2010).
2.3 Parser Combinations
The MaltBlender tool makes a two-stage optimiza-
tion of the result of several parser outcomes, based
on the work of Sagae and Lavie (2006), and it was
used for the first time for the ten languages in the
multilingual track of the CoNLL 2007 shared task
on dependency parsing(Hall et al, 2010). The first
stage consists in tuning several single-parser sys-
tems. The second stage consists in building an
ensemble system that will combine the different
parsers. When this system was evaluated on the
official test sets at the CoNLL 2007 shared task,
the ensemble system significantly outperformed
the single-parser system and achieved the highest
average labelled attachment score of all participat-
ing systems.
3 Contribution of Morphological
Information to Parsing
We examined the effect of each type of morpho-
logical information, contained in the FEATS col-
umn, to investigate their overall contribution to
parsing. This will help us to determine which are
the most relevant features for parsing. To carry out
this task we have used the Mate parser, due to lack
of time for testing, and also taking into consid-
eration that it gives better results than MaltParser
for all the languages?s baselines. Firstly, we will
obtain the baseline for each language parsing the
files with an empty FEATS column. This baseline
will help us to determine the contribution of each
morphological feature to parsing. Next, we trained
the parsers using one feature at a time obtaining as
many results as features for each language. Table
1 shows the effect of each information on the Mate
parser.
In this table we can observe that Basque is one
of the most sensitive languages regarding the influ-
ence of its features. Using case (KAS) as a unique
feature improves the labelled attachment score
over using an empty FEATS column by almost
5.7%. The next two better features are number
(NUM) and type of subordinate sentence (ERL).
They help with a 1.1% and 0.6% increase, respec-
tively. The rest of the features do not contribute
much in isolation, with a maximum of 0.2%. On
the other hand, including all the features results in
an improvement of 6.5%.
If we analyze the results for French we see that,
in contrast to Basque, the influence of the features
on the parser is minimum. The most significant
feature is gender (g), which helps with a 0.1% in-
crease. With respect to the improvement using the
other features, although they do not provide big in-
creases all of them contribute positively. In clos-
ing, including all the features we obtain a 84.6%
labelled attachment score with a 0.4% improve-
ment over not using any features.
As with French, the German morphological fea-
tures provide small increases. The most two sig-
nificant features are case and gender, which obtain
increases of 0.2%, 0.13%, respectively. It is inter-
esting to observe how including all the features we
obtain worse results than using only the case, al-
though the difference is not significant. That could
occur due to the weak influence of its features in
the final result and the negative influence of some
of them.
Hungarian is the language which offers more
features, 14 altogether. This language, in line with
Basque, tends to vary significantly its labelled at-
tachment score depending on the used morpholog-
73
Basque French German Hungarian Swedish
all feats 83.0 all feats 84.6 all feats 91.0 all feats 82.8 all feats 76.7
no feats 76.5 no feats 84.2 no feats 90.9 no feats 75.3 no feats 76.9
KAS 82.2 g 84.3 case 91.0 Cas 80.9 verbform 77.0
NUM 77.7 n 84.3 gender 91.0 PerP 76.3 definiteness 76.8
ERL 77.1 p 84.3 number 90.9 NumP 76.3 degree 76.8
DADUDIO 76.8 c 84.2 person 90.9 SubPOS 75.9 case 76.8
NORK 76.7 m 84.2 tense 90.9 Def 75.7 number 76.3
MDN 76.6 s 84.2 degree 90.8 Num 75.7 perfectform 76.3
NOR 76.6 t 84.2 mood 90.8 PerP 75.7 abbrv 76.3
ASP 76.4 Mood 75.5 mood 76.2
NORI 76.2 NumPd 75.4 pronounform 76.1
ADM 76.5 Coord 75.3 gender 76.0
Form 75.3
Tense 75.3
Type 75.3
Deg 75.0
Table 1: The effect of each feature sorted by language (MATE parser)
ical feature. If we focus on the three most signif-
icant features, the case (Cas) helps with a 5.6%
increase, person of possessor (PerP) with a 1%,
while number of possessor helps with a 0.9%. The
grammatical subcategory within the main part of
speech (SubPOS) improves the baseline in a 0.6%
and the number and person in a 0.4%. The remain-
ing features do not contribute very appreciatively
even obtaining negative results. Including all the
features we obtain a labelled attachment score of
82.83%. That means the real contribution of all
the features is 7.5%, this improvement being the
most important among all the used languages.
In common with French and German, the
Swedish morphological features do not seem to
help the parsers to achieve significant improve-
ments in terms of LAS. However, we can observe
some interesting phenomena. While in the other
languages the case is one of the best features, in
Swedish is does not help, achieving a negative re-
sult. In general, excluding the verb form (verb-
form), all the features obtain negative results with
respect to not using any feature. In this scenario
it is not surprising to verify that including all the
features does not help the Mate parser. Having
said this, the best three features are the verb form
(verbform), definiteness (definiteness) and degree
(degree).
4 Testing the Effect of Different
Morphosyntactic features on parsers
We examined the effect of the most significant
morphological features, examined in the previous
step, to investigate their overall contribution to
parsing. For this task, we created three variants for
each parser, apart from the baseline using all the
morphosyntactic features. We obtain these vari-
ants by: i) using the most 3 relevant features in
the FEATS column (see table 1 in previous sec-
tion), ii) moving the most relevant feature for each
language to the POS column and iii) moving the
most relevant feature to the CPOS column. Next,
we have tested parser combinations including all
the baselines and their variants in subsection 4.2.
4.1 Individual Parsers
Table 2 shows the effect of each information on
both parsers, Maltparser and Mate parser. If we
analyze the results on Basque, the difference be-
tween the two parsers is noticeable, as Mate ob-
tains on average a 3 point improvement with re-
spect to MaltParser. A similar difference occurs
on all the used languages. The best LAS in Basque
is acquired using the 3 best features in the FEATS
column with the Mate parser (83.4%). On a com-
parison with the LAS obtained by the Mate base-
line (All-Feats), that means a 0.4 improvement.
Regarding Maltparser?s results for Basque, we get
the best LAS (81.0%) moving the best feature
(case) to POS in its right-to-left version, increas-
ing the LAS baseline (All-Feats) by 1.0. We no-
tice that Maltparser and Mate tend to improve their
baseline scores using some of the presented vari-
ants.
On the other hand, the best score for French
is obtained using the baseline (All-Feats and
74
Basque French German Hungarian Swedish
Baselines
All ? FeatsMalt 80.0 79.9 87.6 77.3 73.4
All ? FeatsMate 83.0 84.6 91.0 82.3 76.7
Left2right
3? bestMalt 79.9 79.9 87.6 75.9 73.4
CPOS ? bestMalt 80.3 79.7 87.5 76.6 72.9
POS ? bestMalt 78.7 78.7 86.6 77.2 72.8
3? bestMate 83.4 84.3 90.8 82.4 76.6
CPOS ? bestMate 82.7 84.3 91.0 82.7 76.8
POS ? bestMate 82.2 83.4 90.5 82.5 76.5
Right2left
3? bestMalt 80.1 78.9 86.9 75.3 69.3
CPOS ? bestMalt 80.0 79.0 86.7 76.6 69.3
POS ? bestMalt 81.0 77.8 85.4 74.9 70.2
3? bestMate 83.3 84.3 90.9 82.1 76.5
CPOS ? bestMate 83.1 84.6 91.0 82.6 77.0
POS ? bestMate 81.6 83.5 90.6 82.4 76.4
Table 2: Testing the effect of features on MaltParser and Mate
the Mate parser, 84,6%). Contrary to Basque,
in French, although some of the used variants
achieve similar scores with respect to their base-
lines (All-Feats), they do not give noticeable in-
creases. The unique variant that equals its base-
line (79,9%) is 3? bestMalt using the left-to-right
version and the three best features (gender, num-
ber and person) in the FEATS column using Malt-
parser.
With respect to German, the only variant that
equals the baseline is CPOS ? bestMate with
91.0% LAS. . If we focus on Maltparser?s (Mal-
tOptimizer) scores, we get the best result among
the variants with 3 ? bestMalt (87.6%) using the
left-to-right version. The variants do not improve
Maltparser?s baseline.
Although some of the Hungarian variant scores
are very similar to their baselines, they give some
improvements over the baseline. The best two re-
sults on the Mate parser are 82.7% and 82.6%. We
obtain the first score moving the best feature (case)
to CPOS in its left-to-right version, and the second
one using the same configuration in its right-to-left
version. The best two scores on Maltparser with-
out taking the baseline into account are 77.2% and
76.6%, obtained when moving the best feature to
POS and moving the best feature to CPOS in its
right-to-left version, respectively.
The best two results for Swedish on the Mate
parser are 77.0% and 76.8%. We get the first re-
sult moving the best feature (verbform) to CPOS
in its right-to-left version and the second one in its
standard version. These two results are the only
variants that improve the baseline (76.7% LAS)
with a 0.30 and 0.17 increase, respectively. On the
other hand, if we focus on Maltparser, the variants
do not improve the baseline (73.4% LAS) where
the best two results are 73.4% and 72.9% LAS.
For the best result we use the three best features
(verbform, definiteness and degree) in the FEATS
column, while for the second one the best feature
(verbform) has been moved to CPOS.
Despite that only the Basque and Swedish vari-
ants haven been able to significantly improve their
baselines, in the next subsection we present a com-
bination system expecting to take advantage on the
variety of the parsed files (Surdeanu and Manning,
2010).
4.2 Parser Combinations
Although in several cases the use of specific mor-
phosyntactic information does not give noticeable
increases, we also tested the effect on parser com-
binations. Table 3 presents the result of combin-
ing the extended parsers with the baselines (us-
ing all the features) obtained in individual parsers.
The table shows that the Basque language has
achieved the biggest increase. Parser combination
in Basque helps with an improvement of 3.2 with
respect to the Mate baseline. Contrary to Basque,
French is the language that has obtained the small-
est increases in parser combination if we compare
it with the Mate (highest) parser baseline. The
combined system improves the Mate parser base-
75
Basque French German Hungarian Swedish
MaltParser baseline 80.0 79.9 87.6 77.3 73.4
Mate parser baseline 83.0 84.6 91.0 82.8 76.7
Parser combination 86.2 85.1 91.8 84.1 78.1
Table 3: Results of parser combinations
line by 0.5. Parser combination in German gives a
0.8 increase with respect to the best single parser
(Mate, 91.0). Our system achieves a 1.3 increase
for Hungarian with respect to the Mate parser?s
baseline. Finally, if we focus on Swedish, the
parser combination helps with a 1.4 increase with
respect to the Mate parser.
After examining the parsers involved in parser
combinations we noticed that there are always sev-
eral variants included in the best parser combina-
tions, although the only variant that appears in all
the best parser combinations is CPOS?bestMate
in its left-to-right version. Taking into account
that the most relevant feature for Basque, German
and Hungarian is the case, it would be interest-
ing to use the CPOS?caseMate variant for other
languages. Finally, the presented results suggest
that the introduced variants contribute positively
on parsing and they help to improve the scores ob-
tained by the base parsers.
5 Conclusion and Future Work
We have presented a combined system that was
designed after analyzing the relevance of the mor-
phological features in order to take advantage on
the effect of those features on some parsers. In
general the improvements have been noticeable,
specially for Basque. We can point out some in-
teresting avenues for research:
? Use of new parsing algorithms for testing
the effect of different morphological fea-
tures. The results of this work show that the
used techniques are specially useful for lan-
guages where the FEATS column, contain-
ing morpho-syntactic information, gives the
biggest increments with respect to not us-
ing the features, like Basque and Hungar-
ian. We expect that similar improvements
could be obtained for languages like Turkish
or Czech, which share many characteristics
with Basque and Hungarian.
? Experimenting different models for parser
combinations using new parsers. Several of
the parser variants we have used give only
slight modifications over the base algorithms,
even though when combined they give sig-
nificant increases. Widening the spectrum of
parsers and adding new algorithms can imply
an important boost in parser combination.
? Application to the rest of the languages of the
SPMRL 2013 Shared Task: Korean, Hebrew,
Arabic and Polish.
Acknowledgements
This research was supported by the Department of
Industry of the Basque Government (IT344-10, S
PE11UN114), the University of the Basque Coun-
try (GIU09/19) and the Spanish Ministry of Sci-
ence and Innovation (MICINN, TIN2010-20218).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel.
2003. Building a treebank for french. In Anne
Abeille?, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. D??az de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency tree-
bank. pages 201?204.
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010. Two
methods to incorporate local morphosyntactic fea-
tures in hindi dependency parsing. In Proceedings of
the NAACL HLT 2010 First Workshop on Statistical
Parsing of Morphologically-Rich Languages, pages
22?30.
Miguel Ballesteros and Joakim Nivre. 2012a. Maltop-
timizer: A system for maltparser optimization. In
LREC, pages 2757?2763.
Miguel Ballesteros and Joakim Nivre. 2012b. Mal-
toptimizer: an optimization tool for maltparser. In
Proceedings of the Demonstrations at the 13th Con-
ference of the European Chaptr of the Association
for Computational Linguistics, pages 58?62.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency pars-
ing of basque. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 31?39.
76
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1455?1465.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 89?97.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, Prague, Czech Republic, June.
O?zlem C?etinog?lu and Jonas Kuhn. 2013. Towards
joint morphological analysis and dependency pars-
ing of turkish. In Proceedings of the Second In-
ternational Conference on Dependency Linguistics
(DepLing 2013), pages 23?32, Prague, Czech Re-
public, August. Charles University in Prague, Mat-
fyzpress, Prague, Czech Republic.
Gu?lsen Eryigit. 2007. Itu validation set for metu-
sabanc? turkish treebank. URL: http://www3. itu.
edu. tr/ gulsenc/papers/validationset. pdf.
Johan Hall, Jens Nilsson, and Joakim Nivre. 2010.
Single malt or blended? a study in multilingual
parser optimization. In Trends in Parsing Technol-
ogy, pages 19?33. Springer.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis with
propbank and nombank. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, pages 183?187.
Yuval Marton, Nizar Habash, and Owen Rambow.
2013. Dependency parsing of modern standard ara-
bic with lexical and inflectional features. Computa-
tional Linguistics, 39(1):161?194.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
LREC, pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task
on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL
2007, Prague, Czech Republic, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav
Marinov, and Erwin Marsi. 2007b. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-
Tu?r, and Go?khan Tu?r. 2003. Building a turkish
treebank. Building and Exploiting Syntactically-
annotated Corpora.
Kenji Sagae and Alon Lavie. 2006. Parser com-
bination by reparsing. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie
Candito, Jinho Choi, Richa?rd Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Wolin?ski, Alina
Wro?blewska, and Eric Villemonte de la Cle?rgerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morpholog-
ically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing of Morphologically
Rich Languages: Shared Task, Seattle, WA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132?3139, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):23?55.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics Conference (NAACL-2010), Los Ange-
les, CA, June.
Reut Tsarfaty, Djam Seddah, Yoav Goldberg, San-
dra Ku?bler, Marie Candito, Jennifer Foster, Yan-
nick Versley, Ines Rehbein, and Lamia Tounsi.
2010. Statistical parsing of morphologically rich
languages (spmrl) what, how and whither. In In Pro-
ceedings of the NAACL HLT 2010 First Workshop
on Statistical Parsing of Morphologically-Rich Lan-
guages.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy
Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hun-
garian dependency treebank. In LREC.
77
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 85?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Adverse Drug Event prediction combining
shallow analysis and machine learning
Sara Santiso
Alicia P
?
erez
Koldo Gojenola
IXA Taldea (UPV-EHU)
Arantza Casillas
Maite Oronoz
IXA Taldea (UPV-EHU)
http://ixa.si.ehu.es
Abstract
The aim of this work is to infer a model
able to extract cause-effect relations be-
tween drugs and diseases. A two-level
system is proposed. The first level car-
ries out a shallow analysis of Electronic
Health Records (EHRs) in order to iden-
tify medical concepts such as drug brand-
names, substances, diseases, etc. Next,
all the combination pairs formed by a
concept from the group of drugs (drug
and substances) and the group of diseases
(diseases and symptoms) are characterised
through a set of 57 features. A supervised
classifier inferred on those features is in
charge of deciding whether that pair rep-
resents a cause-effect type of event.
One of the challenges of this work is the
fact that the system explores the entire
document. The contributions of this pa-
per stand on the use of real EHRs to dis-
cover adverse drug reaction events even in
different sentences. Besides, the work fo-
cuses on Spanish language.
1 Introduction
This work deals with semantic data mining within
the clinical domain. The aim is to automatically
highlight the Adverse Drug Reactions (ADRs) in
EHRs in order to alleviate the work-load to sev-
eral services within a hospital (pharmacy service,
documentation service,. . . ) that have to read these
reports. Event detection was thoroughly tackled in
the Natural Language Processing for Clinical Data
2010 Challenge. Since then, cause-effect event ex-
traction has emerged as a field of interest in the
Biomedical domain (Bj?orne et al., 2010; Mihaila
et al., 2013). The motivation is, above all, practi-
cal. Electronic Health Records (EHRs) are studied
by several services in the hospital, not only by the
doctor in charge of the patient but also by the phar-
macy and documentation services, amongst oth-
ers. There are some attempts in the literature that
aim to make the reading of the reports in English
easier and less time-consuming by means of an au-
tomatic annotation toolkit (Rink et al., 2011; Bot-
sis et al., 2011; Toldo et al., 2012). This work is
a first approach on automatic learning of relations
between drugs causing diseases in Spanish EHRs.
This work presents a system that entails two
stages in cascade: 1) the first one carries out the
annotation of drugs or substances (from now on-
wards both of them shall be referred to as DRUG)
and diseases or symptoms (referred to as DIS-
EASE); 2) the second one determines whether a
given (DRUG, DISEASE) pair of concepts repre-
sents a cause-effect reaction. Note that we are in-
terested in highlighting events involving (DRUG,
DISEASE) pairs where the drug caused an adverse
reaction or a disease. By contrast, often, (DRUG,
DISEASE) pairs would entail a drug prescribed to
combat a disease, but these correspond to a differ-
ent kind of events (indeed, diametrically opposed).
Besides, (DRUG, DISEASE) pairs might represent
other sort of events or they might even be unre-
lated at all. Finally, the system should present the
ADRs marked in a friendly front-end. To this end,
the aim is to represent the text in the framework
provided by Brat (Stenetorp et al., 2012). Figure 1
shows an example, represented in Brat, of some
cause-effect events manually tagged by experts.
There are related works in this field aiming at
a variety of biomedical event extraction, such as
binary protein-protein interaction (Wong, 2001),
biomolecular event extraction (Kim et al., 2011),
and drug-drug interaction extraction (Segura-
Bedmar et al., 2013). We are focusing on a variety
of interaction extraction: drugs causing diseases.
There are previous works in the literature that try
to warn whether a document contains or not this
type of events. There are more recent works that
85
Figure 1: Some cause-effect events manually annotated in the Brat framework.
cope with event extraction within the same sen-
tence, that is, intra-sentence events. By contrast, in
this work we have realised that around 26% of the
events occur between concepts that are in differ-
ent sentences. Moreover, some of them are at very
long distance. Hence, our method aims at provid-
ing all the (DRUG, DISEASE) concepts within the
document that represent a cause-effect relation.
We cope with real discharge EHRs written by
around 400 different doctors. These records are
not written in a template, that is, the EHRs do not
follow a pre-determined structure, and this, by it-
self entails a challenge. The EHRs we are dealing
with are written in a free structure using natural
language, non-standard abbreviations etc. More-
over, we tackle Spanish language, for which little
work has been carried out. In addition, we do not
only aim at single concept-words but also at con-
cepts based on multi-word terms.
2 System overview
The system, as depicted in Figure 2 entails two
stages.
EHR
Stage 1:
ANNOTATING
CONCEPTS
Stage 2:
EXTRACTING
EVENTS
MARKED 
EHR
Figure 2: The ADR event extraction system.
In the first stage, relevant pairs of concepts have
to be identified within an EHR. Concept annota-
tion is accomplished by means of a shallow anal-
yser system (described in section 2.1). Once the
analyser has detected (DRUG, DISEASE) pairs in
a document, all the pairs will be examined by
an inferred supervised classifier (described in sec-
tion 2.2).
2.1 Annotating concepts by shallow analysis
The first stage of the system has to detect and an-
notate two types of semantic concepts: drugs and
diseases. Each concept, as requested by the phar-
macy service, should gather several sub-concepts
stated as follows:
1. DRUG concept:
(a) Generic names for pharmaceutical
drugs: e.g. corticoids;
(b) Brand-names for pharmaceutical drugs:
e.g. Aspirin;
(c) Active ingredients: e.g. vancomycin;
(d) Substances: e.g. dust, rubber;
2. DISEASE concept:
(a) Diseases
(b) Signs
(c) Symptoms
These concepts were identified by means of a
general purpose analyser available for Spanish,
called FreeLing (Padr?o et al., 2010), that had been
enhanced with medical ontologies and dictionar-
ies, such as SNOMED-CT, BotPLUS, ICD-9-CM,
etc. (Oronoz et al., 2013). This toolkit is able
to identify multi-word context-terms, lemmas and
also POS tags. An example of the morphological,
semantic and syntactic analysis, provided by this
parser is given in Figure 3. In the figure two pieces
of information can be distinguished: for exam-
ple, given the word ?secundarios? (meaning sec-
ondaries) 1) the POS tag provided is AQOM corre-
sponding to Qualificative Adjective Ordinal Mas-
culine Singular; and 2) the provided lemma is ?se-
cundario? (secondary). Besides, in a third layer,
the semantic tag is given, that is, the tag ?ENFER-
MEDAD? (meaning disease) involves the multi-
word concept ?HTP severa? (severe pulmonary
hypertension).
86
Figure 3: Lemmas, POS-tags and semantic tags are identified by the clinic domain analyser (diseases in
yellow and drugs or substances in violet).
2.2 Extracting adverse drug reaction events
using inferred classifiers
The goal of the second stage is to determine if a
given (DRUG, DISEASE) pair represents an ADR
event or not. On account of this, we resorted to
supervised classification models. These models
can be automatically inferred from a set of doc-
uments in which the target concepts had been pre-
viously annotated. Hence, first of all, a set of an-
notated data representative for the task is required.
To this end, our starting point is a manually anno-
tated corpus (presented in section 2.2.1). Besides,
in order to automatically learn the classifier, the
(DRUG, DISEASE) pairs have to be described in an
operative way, that is, in terms of a finite-set of
features (see section 2.2.2). The supervised clas-
sification model selected was a type of ensemble
classifier: Random Forests (for further details turn
to section 2.2.3).
2.2.1 Producing an annotated set
A supervised classifier was inferred from an-
notated real EHRs. The annotation was carried
out by doctors from the same hospital that pro-
duced the EHRs. Given the text with the con-
cepts marked on the first stage (turn to section 2.1)
and represented within the framework provided by
Brat
1
, around 4 doctors from the same hospital an-
notated the events. This annotated set would work
as a source of data to get instances that would
serve to train supervised classification models, as
the one referred in section 2.2.
2.2.2 Operational description of events
As it is well-known, the success of the techniques
based on Machine Learning relies upon the fea-
tures used to describe the instances. Hence, we se-
lected the following features that eventually have
1
Brat is the framework a priori selected as the output
front-end shown in Figure 1
proven useful to capture the semantic relations be-
tween ADRs. The features can be organised in the
following sets:
? Concept-words and context-words: to be
precise, we make use of entire terms
including both single-words and multi-
words.
? DRUG concept-word together with
left and right context words (a con-
text up to 3, yielding, thus, 7 fea-
tures).
? DISEASE concept-word together
with left and right context words (7
features).
? Concept-lemmas and context-lemmas
for both drug and disease (14 features
overall)
? Concept-POS and context-POS for both
drug and disease (14 features)
? Negation and speculation: these are
binary valued features to determine
whether the concept words or their con-
text was either negated or speculated (2
features).
? Presence/absence of other drugs in the
context of the target drug and disease (12
features)
? Distance: the number of characters from
the DRUG concept to the DISEASE con-
cept (1 feature).
2.2.3 Inferring a supervised classifier
Given the operational description of a set of
(DRUG, DISEASE) pairs, this stage has to deter-
87
mine if there exists an ADR event (that is, a cause-
effect relation) or not. To do so, we resorted
to Random Forests (RFs), a variety of ensemble
models. RFs combine a number of decision trees
being each tree built on the basis of the C4.5 algo-
rithm (Quinlan, 1993) but with a distinctive char-
acteristic: some randomness is introduced in the
order in which the nodes are generated. Particu-
larly, each time a node is generated in the tree, in-
stead of chosing the attribute that maximizes the
Information Gain, the attribute is randomly se-
lected amongst the k best options. We made use
of the implementation of this algorithm available
in Weka-6.9 (Hall et al., 2009). Ensemble models
were proved useful on drug-drug interaction ex-
traction tasks (Thomas et al., 2011).
3 Experimental results
We count on data consisting of discharge sum-
maries from Galdakao-Usansolo Hospital. The
records are semi-structured in the sense that there
are two main fields: the first one for personal data
of the patient (age, dates relating to admittance)
that were not provided by the hospital for privacy
issues; and the second one, our target, a single
field that contains the antecedents, treatment, clin-
ical analysis, etc. This second field is an unstruc-
tured section (some hospitals rely upon templates
that divide this field into several subfields, provid-
ing it with further structure). The discharge notes
describe a chronological development of the pa-
tient?s condition, the undergone treatments, and
also the clinical tests that were carried out.
Given the entire set of manually annotated doc-
uments, 34% were randomly selected without re-
placement to produce the evaluation set. The re-
sulting partition is presented in Table 1 (where the
train and evaluation sets are referred to as Train
and Eval respectivelly).
Documents Concepts Relations
Train 144 6,105 4,675
Eval 50 2,206 1,598
Table 1: Quantitative description of the data.
All together, there are 194 EHRs manually
tagged with more than 8,000 concepts (entailing
diseases, symptoms, drugs, substances and proce-
dures). From these EHRs all the (DRUG,DISEASE)
pairs are taken into account as event candidates,
and these are referred to as relations in Table 1.
The system was assessed using per-class aver-
aged precision, recall and f1-measure as presented
in Table 2.
Precision Recall F1-measure
0.932 0.849 0.883
Table 2: Experimental results.
Semantic knowledge and contextual features
have proven very relevant to detect cause-effect re-
lations. Particularly, those used to detect the con-
cepts and also negation or speculation of the con-
text in which the concept appear.
A manual inspection was carried out on both the
false positives and false negative predictions and
the following conclusions were drawn:
? The majority of false positives were caused
by i) pairs of concepts at a very long distance;
ii) pairs where one of the elements is related
to past-events undergone while the other el-
ement is in the current treatment prescribed
(e.g. the disease is in the antecedents and the
drug in the current diagnostics).
? The vast majority of false negatives were
due to concepts in the same sentence where
the context-words are irrelevant (e.g. filler
words, determiners, etc.).
4 Concluding Remarks and Future Work
This work presents a system that first identifies rel-
evant pairs of concepts in EHRs by means of a
shallow analysis and next examines all the pairs
by an inferred supervised classifier to determine if
a given pair represents a cause-effect event. A rel-
evant contribution of this work is that we extract
events occurring between concepts that are in dif-
ferent sentences. In addition, this is one of the first
works on medical event extraction for Spanish.
Our aim for future work is to determine whether
the (DRUG, DISEASE) pair represents either a rela-
tion where 1) the drug is to overcome the disease;
2) the drug causes the disease; 3) there is no rela-
tionship between the drug and the disease.
The aim of context features is to capture charac-
teristics of the text surrounding the relevant con-
cepts that trigger a relation. More features could
also be explored such as trigger words, regular pat-
terns, n-grams, etc.
88
Acknowledgments
The authors would like to thank the Pharmacy
and Pharmacovigilance services of Galdakao-
Usansolo Hospital.
This work was partially supported by the Euro-
pean Commission (325099 and SEP-210087649),
the Spanish Ministry of Science and Innovation
(TIN2012-38584-C06-02) and the Industry of the
Basque Government (IT344-10).
References
Jari Bj?orne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsu-
jii, and Tapio Salakoski. 2010. Complex event ex-
traction at pubmed scale. Bioinformatics [ISMB],
26(12):382?390.
Taxiarchis Botsis, Michael D. Nguyen, Emily Jane
Woo, Marianthi Markatou, and Robert Ball. 2011.
Text mining for the vaccine adverse event reporting
system: medical text classification using informative
feature selection. JAMIA, 18(5):631?638.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1):10?18.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In
Proceedings of the BioNLP Shared Task 2011
Workshop, pages 1?6. Association for Computa-
tional Linguistics.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Maite Oronoz, Arantza Casillas, Koldo Gojenola, and
Alicia Perez. 2013. Automatic annotation of
medical records in Spanish with disease, drug and
substance names. In Lecture Notes in Computer
Science, volume 8259, pages 536?547. Springer-
Verlag.
Lluis Padr?o, S. Reese, Eneko Agirre, and Aitor Soroa.
2010. Semantic Services in Freeling 2.1: WordNet
and UKB. In Global Wordnet Conference, Mumbai,
India.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts.
2011. Automatic extraction of relations between
medical concepts in clinical texts. JAMIA, 18:594?
600.
Isabel Segura-Bedmar, P Mart??nez, and Mar?a Herrero-
Zazo. 2013. Semeval-2013 task 9: Extraction of
drug-drug interactions from biomedical texts (ddiex-
traction 2013). Proceedings of Semeval, pages 341?
350.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: A web-based tool for nlp-
assisted text annotation. In In Proceedings of the
Demonstrations Session at EACL 2012.
Philippe Thomas, Mariana Neves, Ill?es Solt,
Domonkos Tikk, and Ulf Leser. 2011. Relation
extraction for drug-drug interactions using ensem-
ble learning. 1st Challenge task on Drug-Drug
Interaction Extraction (DDIExtraction 2011), pages
11?18.
Luca Toldo, Sanmitra Bhattacharya, and Harsha Gu-
rulingappa. 2012. Automated identification of ad-
verse events from case reports using machine learn-
ing. In Workshop on Computational Methods in
Pharmacovigilance.
Limsoon Wong. 2001. A protein interaction extraction
system. In Pacific Symposium on Biocomputing,
volume 6, pages 520?531. Citeseer.
89
