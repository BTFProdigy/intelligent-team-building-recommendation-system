Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321?328
Manchester, August 2008
Improving Statistical Machine Translation using
Lexicalized Rule Selection
Zhongjun He1,2 and Qun Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{zjhe,liuqun,sxlin}@ict.ac.cn
Abstract
This paper proposes a novel lexicalized ap-
proach for rule selection for syntax-based
statistical machine translation (SMT). We
build maximum entropy (MaxEnt) mod-
els which combine rich context informa-
tion for selecting translation rules dur-
ing decoding. We successfully integrate
the MaxEnt-based rule selection models
into the state-of-the-art syntax-based SMT
model. Experiments show that our lexical-
ized approach for rule selection achieves
statistically significant improvements over
the state-of-the-art SMT system.
1 Introduction
The syntax-based statistical machine translation
(SMT) models (Chiang, 2005; Liu et al, 2006;
Galley et al, 2006; Huang et al, 2006) use rules
with hierarchical structures as translation knowl-
edge, which can capture long-distance reorderings.
Generally, a translation rule consists of a left-hand-
side (LHS) 1and a right-hand-side (RHS). The
LHS and RHS can be words, phrases, or even syn-
tactic trees, depending on SMT models. Transla-
tion rules can be learned automatically from par-
allel corpus. Usually, an LHS may correspond to
multiple RHS?s in multiple rules. Therefore, in sta-
tistical machine translation, the rule selection task
is to select the correct RHS for an LHS during de-
coding.
The conventional approach for rule selection is
to use precomputed translation probabilities which
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1In this paper, we use LHS and source-side interchange-
ably (so are RHS and target-side).
are estimated from the training corpus, as well as a
n-gram language model which is trained on the tar-
get language. The limitation of this method is that
it ignores context information (especially on the
source-side) during decoding. Take the hierarchi-
cal model (Chiang, 2005) as an example. Consider
the following rules for Chinese-to-English transla-
tion 2:
(1) X ? ?? X
1
? X
2
, X
2
in X
1
?
(2) X ? ?? X
1
? X
2
, at X
1
?s X
2
?
(3) X ? ?? X
1
? X
2
, with X
2
of X
1
?
These rules have the same source-side, and all of
them can pattern-match all the following source
phrases:
(a) ?
in
[??
economic
??]
1
field
?
?s
[??]
2
cooperation
[cooperation]
2
in [the economic field]
1
(b) ?
at
[??]
1
today
?
?s
[??
meeting
?]
2
on
at [today]
1
?s [meeting]
2
(c) ?
with
[??]
1
people
?
?s
[??
support
?]
2
under
with [the support]
2
of [the people]
1
Given a source phrase, how does the decoder
know which rule is suitable? In fact, rule (1) and
rule (2) have different syntactic structures (the left
two trees of Figure 1). Thus rule (1) can be used
for translating noun phrase (a), and rule (2) can be
applied to prepositional phrase (b). The weakness
2In this paper, we use Chinese and English as the source
and target language, respectively.
321
NP
DNP
PP
?
X
1
?
X
2
X
2
of
X
1
PP
LCP
NP
?
X
1
?
X
2
at
X
1
?s
X
2
PP
LCP
NP
?
X
1
?
X
2
with
X
2
of
X
1
Figure 1: Syntactic structures of the same source-side in different rules.
of Chiang?s hierarchical model is that it cannot
distinguish different structures on the source-side.
The linguistically syntax-based models (Liu et al,
2006; Huang et al, 2006) can distinguish syntactic
structures by parsing source sentence. However,
as an LHS tree may correspond to different RHS
strings in different rules (the right two rules of Fig-
ure 1), these models also face the rule selection
problem during decoding.
In this paper, we propose a lexicalized approach
for rule selection for syntax-based statistical ma-
chine translation. We use the maximum entropy
approach to combine various context features, e.g.,
context words of rules, boundary words of phrases,
parts-of-speech (POS) information. Therefore, the
decoder can use rich context information to per-
form context-dependent rule selection. We build
a maximum entropy based rule selection (MaxEnt
RS) model for each ambiguous hierarchical LHS,
the LHS which contains nonterminals and corre-
sponds to multiple RHS?s in multiple rules. We
integrate the MaxEnt RS models into the state-of-
the-art hierarchical SMT system (Chiang, 2005).
Experiments show that the lexicalized rule se-
lection approach improves translation quality of
the state-of-the-art SMT system, and the improve-
ments are statistically significant.
2 Previous Work
2.1 The Selection Problem in SMT
Statistical machine translation systems usually
face the selection problem because of the one-to-
many correspondence between the source and tar-
get language. Recent researches showed that rich
context information can help SMT systems per-
form selection and improves translation quality.
The discriminative phrasal reordering models
(Xiong et al, 2006; Zens and Ney, 2006) pro-
vided a lexicalized method for phrase reordering.
In these models, LHS and RHS can be consid-
ered as phrases and reordering types, respectively.
Therefore the selection task is to select a reorder-
ing type for phrases. They use a MaxEnt model
to combine context features and distinguished two
kinds of reorderings between two adjacent phrases:
monotone or swap. However, our method is more
generic, we perform lexicalized rule selection for
syntax-based SMT models. In these models, the
rules with hierarchical structures can handle re-
orderings of non-adjacent phrases. Furthermore,
the rule selection can be considered as a multi-
class classification task, while the phrase reorder-
ing between two adjacent phrases is a two-class
classification task.
Recently, word sense disambiguation (WSD)
techniques improved the performance of SMT sys-
tems by helping the decoder perform lexical selec-
tion. Carpuat and Wu (2007b) integrated a WSD
system into a phrase-based SMT system, Pharaoh
(Koehn, 2004a). Furthermore, they extendedWSD
to phrase sense disambiguation (PSD) (Carpuat
and Wu, 2007a). Either the WSD or PSD system
combines rich context information to solve the am-
biguity problem for words or phrases. Their exper-
iments showed stable improvements of translation
quality. These are different from our work. On
one hand, they focus on solving the lexical am-
biguity problem, and use a WSD or PSD system
to predict translations for phrases which only con-
sist of words. However, we put emphasis on rule
selection, and predict translations for hierarchical
LHS?s which consist of both words and nontermi-
nals. On the other hand, they incorporated a WSD
or PSD system into a phrase-based SMT system
with a weak distortion model for phrase reorder-
ing. While we incorporate MaxEnt RS models
into the state-of-the-art syntax-based SMT system,
which captures phrase reordering by using a hier-
archical model.
322
Chan et al (2007) incorporated a WSD system
into the hierarchical SMT system, Hiero (Chi-
ang, 2005), and reported statistically significant
improvement. But they only focused on solving
ambiguity for terminals of translation rules, and
limited the length of terminals up to 2. Different
from their work, we consider a translation rule as a
whole, which contains both terminals and nonter-
minals. Moreover, they explored features for the
WSD system only on the source-side. While we
define context features for the MaxEnt RS models
on both the source-side and target-side.
2.2 The Hierarchical Model
The hierarchical model (Chiang, 2005; Chiang,
2007) is built on a weighted synchronous context-
free grammar (SCFG) . A SCFG rule has the fol-
lowing form:
X ? ??, ?,??(4)
where X is a nonterminal, ? is an LHS string con-
sists of terminals and nonterminals, ? is the trans-
lation of ?, ? defines a one-one correspondence
between nonterminals in ? and ?. For example,
(5) X ? ?????, economic development?
(6) X ? ? X
1
? X
2
? the X
2
of X
1
?
Rule (5) contains only terminals, which is simi-
lar to phrase-to-phrase translation in phrase-based
SMT models. Rule (6) contains both terminals
and nonterminals, which causes a reordering of
phrases. The hierarchical model uses the max-
imum likelihood method to estimate translation
probabilities for a phrase pair ??, ??, independent
of any other context information.
To perform translation, Chiang uses a log-linear
model (Och and Ney, 2002) to combine various
features. The weight of a derivation D is computed
by:
w(D) =
?
i
?
i
(D)
?
i(7)
where ?
i
(D) is a feature function and ?
i
is the fea-
ture weight of ?
i
(D). During decoding, the de-
coder searches the best derivation with the lowest
cost by applying SCFG rules. However, the rule
selections are independent of context information,
except the left neighboring n ? 1 target words for
computing n-gram language model.
3 Lexicalized Rule Selection
The rule selection task can be considered as a
multi-class classification task. For a source-side,
each corresponding target-side is a label. The max-
imum entropy approach (Berger et al, 1996) is
known to be well suited to solve the classification
problem. Therefore, we build a maximum entropy
based rule selection (MaxEnt RS) model for each
ambiguous hierarchical LHS. In this section, we
will describe how to build the MaxEnt RS mod-
els and how to integrate them into the hierarchical
SMT model.
3.1 The MaxEnt RS Model
Following (Chiang, 2005), we use ??, ?? to repre-
sent a SCFG rule extracted from the training cor-
pus, where ? and ? are source and target strings,
respectively. The nonterminals in ? and ? are rep-
resented by X
k
, where k is an index indicating
one-one correspondence between nonterminals in
source and target sides. Let us use f(X
k
) to rep-
resent the source text covered by X
k
, and e(X
k
)
to represent the translation of f(X
k
). Let C(?) be
the context information of source text matched by
?, and C(?) be the context information of target
text matched by ?. Under the MaxEnt model, we
have:
P
rs
(?|?, f(X
k
), e(X
k
)) =(8)
exp[
?
i
?
i
h
i
(C(?), C(?), f(X
k
), e(X
k
))]
?
?
?
exp[
?
i
?
i
h
i
(C(?
?
), C(?), f(X
k
), e(X
k
))]
where h
i
is a binary feature function, ?
i
is the fea-
ture weight of h
i
. The MaxEnt RS model com-
bines rich context information of grammar rules,
as well as information of the subphrases which
will be reduced to nonterminal X during decoding.
However, these information is ignored by Chiang?s
hierarchical model.
We design three kinds of features for a rule
??, ??:
? Lexical features, which are the words imme-
diately to the left and right of ?, and boundary
words of subphrase f(X
k
) and e(X
k
);
? Parts-of-speech (POS) features, which are
POS tags of the source words defined in lexi-
cal features.
? Length features, which are the length of sub-
phrases f(X
k
) and e(X
k
).
323
Side Type Name Description
W?
?1
The source word immediately to the left of ?
W?
+1
The source word immediately to the right of ?
WL
f(X
k
)
The first word of f(X
k
)
Lexical Features
WR
f(X
k
)
The last word of f(X
k
)
P?
?1
POS of W?
?1
P?
+1
POS of W?
+1
PL
f(X
k
)
POS of WL
f(X
k
)
POS Features
PR
f(X
k
)
POS of WR
f(X
k
)
Source-side
Length Feature LEN
f(X
k
)
Length of source subphrase f(X
k
)
WL
e(X
k
)
The first word of e(X
k
)Lexical Features
WR
e(X
k
)
The last word of e(X
k
)Target-side
Length Feature LEN
e(X
k
)
Length of target subphrase e(X
k
)
Table 1: Feature categories of the MaxEnt RS model.
Type Feature
W?
?1
=?? W?
+1
=b
Lexical Features WL
f(X
1
)
=?? WR
f(X
1
)
=?? WL
f(X
2
)
=?? WR
f(X
1
)
=??
WL
e(X
1
)
=economic WR
e(X
1
)
=field WL
e(X
2
)
=cooperation WR
f(X
1
)
=cooperation
P?
?1
=v W?
+1
=wjPOS Features
PL
f(X
1
)
=n PR
f(X
1
)
=n PL
f(X
2
)
=vn PR
f(X
2
)
=vn
Length Feature LEN
f(X
1
)
=2 LEN
f(X
2
)
=1 LEN
e(X
1
)
=2 LEN
e(X
2
)
=1
Table 2: Features of rule X ? ?? X
1
? X
2
, X
2
in the X
1
?.
??/v ?/p ??/n ??/n ?/ude ??/vn b/wj
strengthen
the
cooperation
in
the
economic
field
.
Figure 2: An training example for rule extraction.
Table 1 shows these features in detail.
These features can be easily gathered accord-
ing to Chinag?s rule extraction method (Chiang,
2005). We use an example for illustration. Fig-
ure 2 is a word-aligned training example with POS
tags on the source side. We can obtain a SCFG
rule:
(9) X ? ?? X
1
? X
2
, X
2
in the X
1
?
Where the source phrases covered by X
1
and X
2
are ??? ??? and ????, respectively. Table
2 shows features of this rule. Note that following
(Chiang, 2005), we limit the number of nontermi-
nals of a rule up to 2. Thus a rule may have 20
features at most.
After extracting features from the training cor-
pus, we use the toolkit implemented by Zhang
(2004) to train a MaxEnt RS model for each am-
biguous hierarchical LHS. We set iteration number
to 100 and Gaussian prior to 1.
3.2 Integrating the MaxEnt RS Models into
the SMT Model
We integrate the MaxEnt RS models into the SMT
model during the translation of each source sen-
tence. Thus the MaxEnt RS models can help the
decoder perform context-dependent rule selection
during decoding.
In (Chiang, 2005), the log-linear model com-
bines 8 features: the translation probabilities
P (?|?) and P (?|?), the lexical weights P
w
(?|?)
and P
w
(?|?), the language model, the word
penalty, the phrase penalty, and the glue rule
penalty. For integration, we add two new features:
? P
rs
(?|?, f(X
k
), e(X
k
)). This feature is
computed by the MaxEnt RS model, which
gives a probability that the model selecting a
target-side ? given an ambiguous source-side
?, considering context information.
? P
rsn
= exp(1). This feature is similar to
phrase penalty feature. In our experiments,
324
we find that some source-sides are not am-
biguous, and correspond to only one target-
side. However, if a source-side ?? is not am-
biguous, the first feature P
rs
will be set to 1.0.
In fact, these rules are not reliable since they
usually occur only once in the training corpus.
Therefore, we use this feature to reward the
ambiguous source-side. During decoding, if
an LHS has multiple translations, this feature
is set to exp(1), otherwise it is set to exp(0).
The advantage of our integration is that we need
not change the main decoding algorithm of a SMT
system. Furthermore, the weights of the new fea-
tures can be trained together with other features of
the translation model.
Chiang (2007) uses the CKY algorithm with a
cube pruning method for decoding. This method
can significantly reduce the search space by effi-
ciently computing the top-n items rather than all
possible items at a node, using the k-best Algo-
rithms of Huang and Chiang (2005) to speed up
the computation. In cube pruning, the translation
model is treated as the monotonic backbone of
the search space, while the language model score
is a non-monotonic cost that distorts the search
space (see (Huang and Chiang, 2005) for defini-
tion of monotonicity). Similarly, in the MaxEnt
RS model, source-side features form a monotonic
score while target-side features constitute a non-
monotonic cost that can be seen as part of the lan-
guage model.
For translating a source sentence F J
I
, the de-
coder adopts a bottom-up strategy. All derivations
are stored in a chart structure. Each cell c[i, j] of
the chart contains all partial derivations which cor-
respond to the source phrase f j
i
. For translating
a source-side span [i, j], we first select all possi-
ble rules from the rule table. Meanwhile, we can
obtain features of the MaxEnt RS models which
are defined on the source-side since they are fixed
before decoding. During decoding, for a source
phrase f j
i
, suppose the rule
X ? ?fk
i
X
1
f
j
t
, e
k
?
i
?
X
1
e
j
?
t
?
?(10)
is selected by the decoder, where i ? k < t ? j
and k + 1 < t, then we can gather features which
are defined on the target-side of the subphrase X
1
from the ancestor chart cell c[k + 1, t ? 1] since
the span [k + 1, t ? 1] has already been covered.
Then the new feature scores P
rs
and P
rsn
can be
computed. Therefore, the cost of the derivation can
be obtained. Finally, the decoding is completed
when the whole sentence is covered, and the best
derivation of the source sentence F J
I
is the item
with the lowest cost in cell c[I, J ].
4 Experiments
4.1 Corpus
We carry out experiments on two translation tasks
with different sizes and domains of the training
corpus.
? IWSLT-05: We use about 40,000 sentence
pairs from the BTEC corpus with 354k Chi-
nese words and 378k English words as our
training data. The English part is used to train
a trigram language model. We use IWSLT-04
test set as the development set and IWSLT-05
test set as the test set.
? NIST-03: We use the FBIS corpus as the
training corpus, which contains 239k sen-
tence pairs with 6.9M Chinese words and
8.9M English words. For this task, we train
two trigram language models on the English
part of the training corpus and the Xinhua
portion of the Gigaword corpus, respectively.
NIST-02 test set is used as the development
set and NIST-03 test set is used as the test set.
4.2 Training
To train the translation model, we first run
GIZA++ (Och and Ney, 2000) to obtain word
alignment in both translation directions. Then the
word alignment is refined by performing ?grow-
diag-final? method (Koehn et al, 2003). We use
the same method suggested in (Chiang, 2005) to
extract SCFG grammar rules. Meanwhile, we
gather context features for training the MaxEnt RS
models. The maximum initial phrase length is set
to 10 and the maximum rule length of the source-
side is set to 5.
We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train language models for both tasks.
We use minimum error rate training (Och, 2003) to
tune the feature weights for the log-linear model.
The translation quality is evaluated by BLEU
metric (Papineni et al, 2002), as calculated by
mteval-v11b.pl with case-insensitive matching of
n-grams, where n = 4.
4.3 Baseline
We reimplement the decoder of Hiero (Chiang,
2007) in C++, which is the state-of-the-art SMT
325
System IWSLT-05 NIST-03
Baseline 56.20 28.05
+ MaxEnt RS
SLex 56.51 28.26
PF 56.95 28.78
SLex+PF 56.99 28.89
SLex+PF+SLen 57.10 28.96
SLex+PF +SLen+TF 57.20 29.02
Table 3: BLEU-4 scores (case-insensitive) on IWSLT-05 task and NIST MT-03 task. SLex = Source-side
Lexical Features, PF = POS Features, SLen = Source-side Length Feature, TF = Target-side features.
system. During decoding, we set b = 100 to prune
grammar rules, ? = 10, b = 30 to prune X cells,
and ? = 10, b = 15 to prune S cells. For cube
pruning, we set the threshold ? = 1.0. See (Chi-
ang, 2007) for meanings of these pruning parame-
ters.
The baseline system uses precomputed phrase
translation probabilities and two trigram language
models to perform rule selection, independent of
any other context information. The results are
shown in the row Baseline of Table 3. For IWSLT-
05 task, the baseline system achieves a BLEU-4
score of 56.20. For NIST MT-03 task, the BLEU-
4 score is 28.05 .
4.4 Baseline + MaxEnt RS
As described in Section 3.2, we add two new fea-
tures to integrate the MaxEnt RS models into the
hierarchical model. To run the decoder, we share
the same pruning settings with the baseline system.
Table 3 shows the results.
Using all features defined in Section 3.1 to train
the MaxEnt RS models, for IWSLT-05 task, the
BLEU-4 score is 57.20, which achieves an abso-
lute improvement of 1.0 over the baseline. For
NIST-03 task, our system obtains a BLEU-4 score
of 29.02, with an absolute improvement of 0.97
over the baseline. Using Zhang?s significance
tester (Zhang et al, 2004) to perform paired boot-
strap sampling (Koehn, 2004b), both improve-
ments on the two tasks are statistically significant
at p < 0.05.
In order to explore the utility of the context fea-
tures, we train the MaxEnt RS models on different
feature sets. We find that POS features are the most
useful features since they can generalize over all
training examples. Moreover, length feature also
yields improvement. However, these features are
never used in the baseline.
NO. of NO. of NO. of
LHS H-LHS AH-LHS
NIST MT-03 163,097 148,671 95,424
Baseline 12,069 7,164 5,745
+MaxEnt RS
(All features) 12,655 10,306 9,259
Table 4: Number of possible source-sides of SCFG
rules for NIST-03 task and number of source-sides
of the best translation. H-LHS = Hierarchical
LHS, AH-LHS = Ambiguous hierarchical LHS.
5 Analysis
Table 4 shows the number of source-sides of
the SCFG rules for NIST-03 task. After extract-
ing grammar rules from the training corpus, there
are 163,097 source-sides match the test corpus,
91.15% are hierarchical LHS?s (H-LHS, the LHS
which contains nonterminals). For the hierarchi-
cal LHS?s, 64.18% are ambiguous (AH-LHS, the
H-LHS which has multiple translations). This in-
dicates that the decoder will face serious rule se-
lection problem during decoding. We also note the
number of the source-sides of the best translation
for the test corpus. For the baseline system, the
number of H-LHS only account for 59.36% of to-
tal LHS?s. However, by incorporating MaxEnt RS
models, that proportion increases to 81.44%, since
the number of AH-LHS increases. The reason is
that, we use the feature P
rsn
to reward ambiguous
hierarchical LHS?s. This has some advantages. On
one hand, H-LHS can capture phrase reorderings.
On the other hand, AH-LHS is more reliable than
non-ambiguous LHS, since most non-ambiguous
LHS?s occur only once in the training corpus.
In order to know how the MaxEnt RS models
improve the performance of the SMT system, we
326
study the best translation of Baseline and Base-
line+MaxEnt RS. We find that the MaxEnt RS
models improve translation quality in 2 ways.
5.1 Better Phrase reordering
Since the SCFG rules which contain nonterminals
can capture reordering of phrases, better rule se-
lection will produce better phrase reordering. For
example, the source sentence ?... [??????
??]
1
? [???? ???]
2
... ? is translated
as follows:
? Reference: ... the five permanent members of
the UN Security Council ...
? Baseline: ... the [United Nations Security
Council]
1
[five permanent members]
2
...
? +MaxEnt RS: ... [the five permanent
members]
2
of [the UN Security Council]
1
...
The source sentence is translated incorrectly by the
baseline system, which selects the rule
(11) X ? ? X
1
? X
2
, the X
1
X
2
?
and produces a monotone translation. In contrast,
by considering information of the subphrases X
1
and X
2
, the MaxEnt RS model chooses the rule
(12) X ? ? X
1
? X
2
, X
2
of X
1
?
and obtains a correct translation by swapping X
1
and X
2
on the target-side.
5.2 Better Lexical Translation
The MaxEnt RS models can also help the decoder
perform better lexical translation than the baseline.
This is because the SCFG rules contain terminals.
When the decoder selects a rule for a source-side,
it also determines the translations of the source ter-
minals. For example, the translations of the source
sentence ??????????? ?b? are
as follows:
? Reference I?m afraid this flight is full.
? Baseline: I?m afraid already booked for this
flight.
? +MaxEnt RS: I?m afraid this flight is full.
Here, the baseline translates the Chinese phrase
???? into ?booked? by using the rule:
(13) X ? ? X
1
??, X
1
booked?
The meaning is not fully expressed since the Chi-
nese word ??? is not translated. However, the
MaxEnt RS model obtains a correct translation by
using the rule:
(14) X ? ? X
1
??, X
1
full ?
However, we also find that some results pro-
duced by the MaxEnt RS models seem to decrease
the BLEU score. An interesting example is the
translation of the source sentence ??????
???:
? Reference1: What is the name of this street?
? Reference2: What is this street called?
? Baseline: What is the name of this street?
? +MaxEnt RS: What?s this street called?
In fact, both translations are correct. But the trans-
lation of the baseline fully matches Reference1.
Although the translation produced by the MaxEnt
RS model is almost the same as Reference2, as
the BLEU metric is based on n-gram matching,
the translation ?What?s? cannot match ?What is?
in Reference2. Therefore, the MaxEnt RS model
achieves a lower BLEU score.
6 Conclusion
In this paper, we propose a generic lexicalized ap-
proach for rule selection. We build maximum en-
tropy based rule selection models for each ambigu-
ous hierarchical source-side of translation rules.
The MaxEnt RS models combine rich context in-
formation, which can help the decoder perform
context-dependent rule selection during decod-
ing. We integrate the MaxEnt RS models into
the hierarchical SMT model by adding two new
features. Experiments show that the lexicalized
approach for rule selection achieves statistically
significant improvements over the state-of-the-art
syntax-based SMT system.
Furthermore, our approach not only can be used
for the formally syntax-based SMT systems, but
also can be applied to the linguistically syntax-
based SMT systems. For future work, we will ex-
plore more sophisticated features for the MaxEnt
RS models and integrate the models into the lin-
guistically syntax-based SMT systems.
327
Acknowledgements
We would like to show our special thanks to Hwee
Tou Ng, Liang Huang, Yajuan Lv and Yang Liu
for their valuable suggestions. We also appreciate
the anonymous reviewers for their detailed com-
ments and recommendations. This work was sup-
ported by the National Natural Science Foundation
of China (NO. 60573188 and 60736014), and the
High Technology Research and Development Pro-
gram of China (NO. 2006AA010108).
References
Berger, A. L., S. A. Della Pietra, and V. J. Della.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, page
22(1):39?72.
Carpuat, Marine and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation, pages 43?52.
Carpuat, Marine and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61?72.
Chan, Yee Seng, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves sta-
tistical machine translation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 33?40.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Chiang, David. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 33(2):201?
228.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL 2006, pages 961?968.
Huang, Liang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas.
Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127?133.
Koehn, Philipp. 2004a. Pharaoh: a beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the Sixth Conference of
the Association for Machine Translation in the Amer-
icas, pages 115?124.
Koehn, Philipp. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388?395.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 609?616.
Och, Franz Josef and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440?447.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 295?302.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Process-
ing, volume 2, pages 901?904.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 521?528.
Zens, Richard and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings of the Workshop on Statistical
Machine Translation, pages 55?63.
Zhang, Ying, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores: How much improve-
ment do we need to have a better system? In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation, pages 2051?
2054.
Zhang, Le. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
328
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385?392
Manchester, August 2008
Word Lattice Reranking for Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? ? Haitao Mi ? ? Qun Liu ?
?Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
?Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{jiangwenbin,htmi,liuqun}@ict.ac.cn
Abstract
In this paper, we describe a new rerank-
ing strategy named word lattice reranking,
for the task of joint Chinese word segmen-
tation and part-of-speech (POS) tagging.
As a derivation of the forest reranking
for parsing (Huang, 2008), this strategy
reranks on the pruned word lattice, which
potentially contains much more candidates
while using less storage, compared with
the traditional n-best list reranking. With a
perceptron classifier trained with local fea-
tures as the baseline, word lattice rerank-
ing performs reranking with non-local fea-
tures that can?t be easily incorporated into
the perceptron baseline. Experimental re-
sults show that, this strategy achieves im-
provement on both segmentation and POS
tagging, above the perceptron baseline and
the n-best list reranking.
1 Introduction
Recent work for Chinese word segmentation and
POS tagging pays much attention to discriminative
methods, such as Maximum Entropy Model (ME)
(Ratnaparkhi and Adwait, 1996), Conditional Ran-
dom Fields (CRFs) (Lafferty et al, 2001), percep-
tron training algorithm (Collins, 2002), etc. Com-
pared to generative ones such as Hidden Markov
Model (HMM) (Rabiner, 1989; Fine et al, 1998),
discriminative models have the advantage of flexi-
bility in representing features, and usually obtains
almost perfect accuracy in two tasks.
Originated by Xue and Shen (2003), the typ-
ical approach of discriminative models conducts
c
? 2008. Licensed to the Coling 2008 Organizing Com-
mittee for publication in Coling 2008 and for re-publishing in
any form or medium.
segmentation in a classification style, by assign-
ing each character a positional tag indicating its
relative position in the word. If we extend these
positional tags to include POS information, seg-
mentation and POS tagging can be performed by a
single pass under a unify classification framework
(Ng and Low, 2004). In the rest of the paper, we
call this operation mode Joint S&T. Experiments
of Ng and Low (2004) shown that, compared with
performing segmentation and POS tagging one at
a time, Joint S&T can achieve higher accuracy not
only on segmentation but also on POS tagging.
Besides the usual local features such as the
character-based ones (Xue and Shen, 2003; Ng
and Low, 2004), many non-local features related
to POSs or words can also be employed to improve
performance. However, as such features are gener-
ated dynamically during the decoding procedure,
incorporating these features directly into the clas-
sifier results in problems. First, the classifier?s fea-
ture space will grow much rapidly, which is apt to
overfit on training corpus. Second, the variance of
non-local features caused by the model evolution
during the training procedure will hurt the param-
eter tuning. Last but not the lest, since the cur-
rent predication relies on the results of prior predi-
cations, exact inference by dynamic programming
can?t be obtained, and then we have to maintain a
n-best candidate list at each considering position,
which also evokes the potential risk of depress-
ing the parameter tuning procedure. As a result,
many theoretically useful features such as higher-
order word- or POS- grams can not be utilized ef-
ficiently.
A widely used approach of using non-local
features is the well-known reranking technique,
which has been proved effective in many NLP
tasks, for instance, syntactic parsing and machine
385
v0
v
1
v
2
v
3
v
4
v
5
v
6
v
7
C
1
:e
C
2
:? C
3
:U
C
4
:/ C
5
:? C
6
:?
C
7
:Y
NN
VV
NN
M
NN
NN
NN
NN
VV
NN
NN
Figure 1: Pruned word lattice as directed graph. The character sequence we choose is ?e-?-U-/-
?-?-Y?. For clarity, we represent each subsequence-POS pair as a single edge, while ignore the
corresponding scores of the edges.
translation (Collins, 2000; Huang, 2008), etc. Es-
pecially, Huang (2008) reranked the packed for-
est, which contains exponentially many parses.
Inspired by his work, we propose word lattice
reranking, a strategy that reranks the pruned word
lattice outputted by a baseline classifier, rather than
only a n-best list. Word lattice, a directed graph as
shown in Figure 1, is a packed structure that can
represent many possibilities of segmentation and
POS tagging. Our experiments on the Penn Chi-
nese Treebank 5.0 show that, reranking on word
lattice gains obvious improvement over the base-
line classifier and the reranking on n-best list.
Compared against the baseline, we obtain an error
reduction of 11.9% on segmentation, and 16.3%
on Joint S&T.
2 Word Lattice
Formally, a word lattice L is a directed graph
?V,E?, where V is the node set, and E is the
edge set. Suppose the word lattice is for sentence
C
1:n
= C
1
..C
n
, node v
i
? V (i = 1..n ? 1) de-
notes the position between C
i
and C
i+1
, while v
0
before C
1
is the source node, and v
n
after C
n
is
the sink node. An edge e ? E departs from v
b
and
arrives at v
e
(0 ? b < e ? n), it covers a subse-
quence of C
1:n
, which is recognized as a possible
word. Considering Joint S&T, we label each edge
a POS tag to represent a word-POS pair. A series
of adjoining edges forms a path, and a path con-
necting the source node and the sink node is called
diameter, which indicates a specific pattern of seg-
mentation and POS tagging. For a diameter d, |d|
denotes the length of d, which is the count of edges
contained in this diameter. In Figure 1, the path
p
?
= v?
0
v
3
? v?
3
v
5
? v?
5
v
7
is a diameter, and
|p
?
| is 3.
2.1 Oracle Diameter in Lattice
Given a sentence s, its reference r and pruned
word lattice L generated by the baseline classi-
fier, the oracle diameter d? of L is define as the
diameter most similar to r. With F-measure as the
scoring function, we can identify d? using the al-
gorithm depicted in Algorithm 1, which is adapted
to lexical analysis from the forest oracle computa-
tion of Huang (2008).
Before describe this algorithm in detail, we de-
pict the key point for finding the oracle diameter.
Given the system?s output y and the reference y?,
using |y| and |y?| to denote word counts of them
respectively, and |y ? y?| to denote matched word
count of |y| and |y?|, F-measure can be computed
by:
F (y, y
?
) =
2PR
P + R
=
2|y ? y
?
|
|y| + |y
?
|
(1)
Here, P = |y?y
?
|
|y|
is precision, and R = |y?y
?
|
|y
?
|
is recall. Notice that F (y, y?) isn?t a linear func-
tion, we need access the largest |y ? y?| for each
possible |y| in order to determine the diameter with
maximum F , or another word, we should know the
maximum matched word count for each possible
diameter length.
The algorithm shown in Algorithm 1 works in
a dynamic programming manner. A table node
T [i, j] is defined for sequence span [i, j], and it has
a structure S to remember the best |y
i:j
? y
?
i:j
| for
each |y
i:j
|, as well as the back pointer for this best
choice. The for-loop in line 2 ? 14 processes for
each node T [i, j] in a shorter-span-first order. Line
3? 7 initialize T [i, j] according to the reference r
and the word lattice?s edge set L ?E. If there exists
an edge e in L ?E covering the span [i, j], then we
386
Algorithm 1 Oracle Diameter, U la Huang (2008,
Sec. 4.1).
1: Input: sentence s, reference r and lattice L
2: for [i, j] ? [1, |s|] in topological order do
3: if ?e ? L ? E s.t. e spans from i to j then
4: if e ? label exists in r then
5: T [i, j] ? S[1]? 1
6: else
7: T [i, j] ? S[1]? 0
8: for k s.t. T [i, k ? 1] and T [k, j] defined do
9: for p s.t. T [i, k ? 1] ? S[p] defined do
10: for q s.t. T [k, j] ? S[q] defined do
11: n? T [i, k ? 1] ? S[p] + T [k, j] ? S[q]
12: if n > T [i, j] ? S[p + q] then
13: T [i, j] ? S[p + q]? n
14: T [i, j] ? S[p + q] ? bp? ?k, p, q?
15: t? argmax
t
2?T [1,|s|]?S[t]
t+|r|
16: d? ? Tr(T [1, |s|] ? S[t].bp)
17: Output: oracle diameter: d?
define T [i, j], otherwise we leave this node unde-
fined. In the first situation, we initialize this node?s
S structure according to whether the word-POS
pair of e is in the reference (line 4?7). Line 8?14
update T [i, j]?s S structure using the S structures
from all possible child-node pair, T [i, k ? 1] and
T [k, j]. Especially, line 9? 10 enumerate all com-
binations of p and q, where p and q each repre-
sent a kind of diameter length in T [i, k ? 1] and
T [k, j]. Line 12 ? 14 refreshes the structure S
of node T [i, j] when necessary, and meanwhile,
a back pointer ?k, p, q? is also recorded. When
the dynamic programming procedure ends, we se-
lect the diameter length t of the top node T [1, |s|],
which maximizes the F-measure formula in line
15, then we use function Tr to find the oracle di-
ameter d? by tracing the back pointer bp.
2.2 Generation of the Word Lattice
We can generate the pruned word lattice using the
baseline classifier, with a slight modification. The
classifier conducts decoding by considering each
character in a left-to-right fashion. At each consid-
ering position i, the classifier enumerates all can-
didate results for subsequence C
1:i
, by attaching
each current candidate word-POS pair p to the tail
of each candidate result at p?s prior position, as
the endmost of the new generated candidate. We
give each p a score, which is the highest, among
all C
1:i
?s candidates that have p as their endmost.
Then we select N word-POS pairs with the high-
est scores, and insert them to the lattice?s edge set.
This approach of selecting edges implies that, for
the lattice?s node set, we generate a node v
i
at each
position i. Because N is the limitation on the count
Algorithm 2 Lattice generation algorithm.
1: Input: character sequence C
1:n
2: E ? ?
3: for i? 1 .. n do
4: cands? ?
5: for l? 1 .. min(i, K) do
6: w ? C
i?l+1:i
7: for t ? POS do
8: p? ?w, t?
9: p ? score? Eval(p)
10: s? p ? score + Best[i? l]
11: Best[i]? max(s,Best[i])
12: insert ?s, p? into cands
13: sort cands according to s
14: E ? E ? cands[1..N ] ? p
15: Output: edge set of lattice: E
of edges that point to the node at position i, we call
this pruning strategy in-degree pruning. The gen-
eration algorithm is shown in Algorithm 2.
Line 3 ? 14 consider each character C
i
in se-
quence, cands is used to keep the edges closing at
position i. Line 5 enumerates the candidate words
ending with C
i
and no longer than K, where K
is 20 in our experiments. Line 5 enumerates all
POS tags for the current candidate word w, where
POS denotes the POS tag set. Function Eval in
line 9 returns the score for word-POS pair p from
the baseline classifier. The array Best preserve the
score for sequence C
1:i
?s best labelling results. Af-
ter all possible word-POS pairs (or edges) consid-
ered, line 13? 14 select the N edges we want, and
add them to edge set E.
Though this pruning strategy seems relative
rough ? simple pruning for edge set while no
pruning for node set, we still achieve a promising
improvement by reranking on such lattices. We be-
lieve more elaborate pruning strategy will results
in more valuable pruned lattice.
3 Reranking
A unified framework can be applied to describing
reranking for both n-best list and pruned word lat-
tices (Collins, 2000; Huang, 2008). Given the can-
didate set cand(s) for sentence s, the reranker se-
lects the best item y? from cand(s):
y? = argmax
y?cand(s)
w ? f(y) (2)
For reranking n-best list, cand(s) is simply the set
of n best results from the baseline classifier. While
for reranking word lattice, cand(s) is the set of
all diameters that are impliedly built in the lattice.
w ? f(y) is the dot product between a feature vec-
tor f and a weight vector w, its value is used to
387
Algorithm 3 Perceptron training for reranking
1: Input: Training examples{cand(s
i
), y
?
i
}
N
i=1
2: w? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: y? ? argmax
y?cand(s
i
)
w ? f(y)
6: if y? 6= y?
i
then
7: w? w + f(y?
i
)? f(y?)
8: Output: Parameters: w
Non-local Template Comment
W
0
T
0
current word-POS pair
W
?1
word 1-gram before W
0
T
0
T
?1
POS 1-gram before W
0
T
0
T
?2
T
?1
POS 2-gram before W
0
T
0
T
?3
T
?2
T
?1
POS 3-gram before W
0
T
0
Table 1: Non-local feature templates used for
reranking
rerank cand(s). Following usual practice in pars-
ing, the first feature f
1
(y) is specified as the score
outputted by the baseline classifier, and its value
is a real number. The other features are non-local
ones such as word- and POS- n-grams extracted
from candidates in n-best list (for n-best rerank-
ing) or diameters (for word lattice reranking), and
they are 0 ? 1 valued.
3.1 Training of the Reranker
We adopt the perceptron algorithm (Collins, 2002)
to train the reranker. as shown in Algorithm 3. We
use a simple refinement strategy of ?averaged pa-
rameters? of Collins (2002) to alleviate overfitting
on the training corpus and obtain more stable per-
formance.
For every training example {cand(s
i
), y
?
i
}, y
?
i
denotes the best candidate in cand(s
i
). For n-
best reranking, the best candidate is easy to find,
whereas for word lattice reranking, we should use
the algorithm in Algorithm 1 to determine the or-
acle diameter, which represents the best candidate
result.
3.2 Non-local Feature Templates
The non-local feature templates we use to train the
reranker are listed in Table 1. Notice that all fea-
tures generated from these templates don?t contain
?future? words or POS tags, it means that we only
use current or history word- or POS- n-grams to
evaluate the current considering word-POS pair.
Although it is possible to use ?future? information
in n-best list reranking, it?s not the same when we
rerank the pruned word lattice. As we have to tra-
verse the lattice topologically, we face difficulty in
Algorithm 4 Cube pruning for non-local features.
1: function CUBE(L)
2: for v ? L ? V in topological order do
3: NBEST(v)
4: return D
v
sink
[1]
5: procedure NBEST(v)
6: heap? ?
7: for v? topologically before v do
8: ?? all edges from v? to v
9: p? ?D
v
?
,??
10: ?p,1??score? Eval(p,1)
11: PUSH(?p,1?, heap)
12: HEAPIFY(heap)
13: buf ? ?
14: while |heap| > 0 and |buf | < N do
15: item? POP-MAX(heap)
16: append item to buf
17: PUSHSUCC(item, heap)
18: sort buf to D
v
19: procedure PUSHSUCC(?p, j?, heap)
20: p is ?vec
1
,vec
2
?
21: for i? 1..2 do
22: j? ? j+ bi
23: if |vec
i
| ? j
?
i
then
24: ?p, j???score? Eval(p, j?)
25: PUSH(?p, j??, heap)
utilizing the information ahead of the current con-
sidering node.
3.3 Reranking by Cube Pruning
Because of the non-local features such as word-
and POS- n-grams, the reranking procedure is sim-
ilar to machine translation decoding with inter-
grated language models, and should maintain a
list of N best candidates at each node of the lat-
tice. To speed up the procedure of obtaining the
N best candidates, following Huang (2008, Sec.
3.3), we adapt the cube pruning method from ma-
chine translation (Chiang, 2007; Huang and Chi-
ang 2007) which is based on efficient k-best pars-
ing algorithms (Huang and Chiang, 2005).
As shown in Algorithm 4, cube pruning works
topologically in the pruned word lattice, and main-
tains a list of N best derivations at each node.
When deducing a new derivation by attaching a
current word-POS pair to the tail of a antecedent
derivation, a function Eval is used to compute the
new derivation?s score (line 10 and 24). We use
a max-heap heap to hold the candidates for the
next-best derivation. Line 7 ? 11 initialize heap
to the set of top derivations along each deducing
source, the vector pair ?D
v
head
,??.Here, ? de-
notes the vector of current word-POS pairs, while
D
v
head
denotes the vector of N best derivations
at ??s antecedent node. Then at each iteration,
388
Non-lexical-target Instances
C
n
(n = ?2..2) C
?2
=e, C
?1
=?, C
0
=U, C
1
=/, C
2
=?
C
n
C
n+1
(n = ?2..1) C
?2
C
?1
=e?, C
?1
C
0
=?U, C
0
C
1
=U/, C
1
C
2
=/?
C
?1
C
1
C
?1
C
1
=?/
Lexical-target Instances
C
0
C
n
(n = ?2..2) C
0
C
?2
=Ue, C
0
C
?1
=U?, C
0
C
0
=UU, C
0
C
1
=U/, C
0
C
2
=U?
C
0
C
n
C
n+1
(n = ?2..1) C
0
C
?2
C
?1
=Ue?, C
0
C
?1
C
0
=U?U, C
0
C
0
C
1
=UU/, C
0
C
1
C
2
=U/?
C
0
C
?1
C
1
C
0
C
?1
C
1
= U?/
Table 2: Feature templates and instances. Suppose we consider the third character ?U? in the sequence
?e?U/??.
we pop the best derivation from heap (line 15),
and push its successors into heap (line 17), until
we get N derivations or heap is empty. In line 22
of function PUSHSUCC, j is a vector composed of
two index numbers, indicating the two candidates?
indexes in the two vectors of the deducing source
p, where the two candidates are selected to deduce
a new derivation. j? is a increment vector, whose
ith dimension is 1, while others are 0. As non-
local features (word- and POS- n-grams) are used
by function Eval to compute derivation?s score,
the derivations extracted from heap may be out of
order. So we use a buffer buf to keep extracted
derivations (line 16), then sort buf and put its first
N items to D
v
(line 18).
4 Baseline Perceptron Classifier
4.1 Joint S&T as Classification
Following Jiang et al (2008), we describe segmen-
tation and Joint S&T as below:
For a given Chinese sentence appearing as a
character sequence:
C
1:n
= C
1
C
2
.. C
n
the goal of segmentation is splitting the sequence
into several subsequences:
C
1:e
1
C
e
1
+1:e
2
.. C
e
m?1
+1:e
m
While in Joint S&T, each of these subsequences is
labelled a POS tag:
C
1:e
1
/t
1
C
e
1
+1:e
2
/t
2
.. C
e
m?1
+1:e
m
/t
m
Where C
i
(i = 1..n) denotes a character, C
l:r
(l ?
r) denotes the subsequence ranging from C
l
to C
r
,
and t
i
(i = 1..m,m ? n) denotes the POS tag of
C
e
i?1
+1:e
i
.
If we label each character a positional tag in-
dicating its relative position in an expected subse-
quence, we can obtain the segmentation result ac-
cordingly. As described in Ng and Low (2004) and
Jiang et al (2008), we use s indicating a single-
character word, while b, m and e indicating the be-
gin, middle and end of a word respectively. With
these positional tags, the segmentation transforms
to a classification problem. For Joint S&T, we
expand positional tags by attaching POS to their
tails as postfix. As each tag now contains both
positional- and POS- information, Joint S&T can
also be resolved in a classification style frame-
work. It means that, a subsequence is a word with
POS t, only if the positional part of the tag se-
quence conforms to s or bm?e pattern, and each
element in the POS part equals to t. For example,
a tag sequence b NN m NN e NN represents a
three-character word with POS tag NN .
4.2 Feature Templates
The features we use to build the classifier are gen-
erated from the templates of Ng and Low (2004).
For convenience of comparing with other, they
didn?t adopt the ones containing external knowl-
edge, such as punctuation information. All their
templates are shown in Table 2. C denotes a char-
acter, while its subscript indicates its position rela-
tive to the current considering character(it has the
subscript 0).
The table?s upper column lists the templates that
immediately from Ng and Low (2004). they
named these templates non-lexical-target because
predications derived from them can predicate with-
out considering the current character C
0
. Tem-
plates called lexical-target in the column below are
introduced by Jiang et al (2008). They are gener-
ated by adding an additional field C
0
to each non-
lexical-target template, so they can carry out pred-
ication not only according to the context, but also
according to the current character itself.
Notice that features derived from the templates
in Table 2 are all local features, which means all
features are determined only by the training in-
stances, and they can be generated before the train-
ing procedure.
389
Algorithm 5 Perceptron training algorithm.
1: Input: Training examples (x
i
, y
i
)
2: ?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: z
i
? argmax
z?GEN(x
i
)
?(x
i
, z) ? ?
6: if z
i
6= y
i
then
7: ?? ? +?(x
i
, y
i
)??(x
i
, z
i
)
8: Output: Parameters: ?
4.3 Training of the Classifier
Collins (2002)?s perceptron training algorithm
were adopted again, to learn a discriminative clas-
sifier, mapping from inputs x ? X to outputs
y ? Y . Here x is a character sequence, and y is
the sequence of classification result of each char-
acter in x. For segmentation, the classification re-
sult is a positional tag, while for Joint S&T, it is
an extended tag with POS information. X denotes
the set of character sequence, while Y denotes the
corresponding set of tag sequence.
According to Collins (2002), the function
GEN(x) generates all candidate tag sequences for
the character sequence x , the representation ?
maps each training example (x, y) ? X ? Y to
a feature vector ?(x, y) ? Rd, and the parameter
vector ? ? Rd is the weight vector corresponding
to the expected perceptron model?s feature space.
For a given input character sequence x, the mission
of the classifier is to find the tag sequence F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ? (3)
The inner product ?(x, y) ? ? is the score of the
result y given x, it represents how much plausibly
we can label character sequence x as tag sequence
y. The training algorithm is depicted in Algorithm
5. We also use the ?averaged parameters? strategy
to alleviate overfitting.
5 Experiments
Our experiments are conducted on the Penn Chi-
nese Treebank 5.0 (CTB 5.0). Following usual
practice of Chinese parsing, we choose chapters
1?260 (18074 sentences) as the training set, chap-
ters 301? 325 (350 sentences) as the development
set, and chapters 271 ? 300 (348 sentences) as
the final test set. We report the performance of
the baseline classifier, and then compare the per-
formance of the word lattice reranking against the
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
ur
e
number of iterations
Perceptron Learning Curves
Segmentation
Joint ST
Figure 2: Baseline averaged perceptron learning
curves for segmentation and Joint S&T.
n-best reranking, based on this baseline classifier.
For each experiment, we give accuracies on seg-
mentation and Joint S&T. Analogous to the situa-
tion in parsing, the accuracy of Joint S&T means
that, a word-POS is recognized only if both the
positional- and POS- tags are correctly labelled for
each character in the word?s span.
5.1 Baseline Perceptron Classifier
The perceptron classifier are trained on the train-
ing set using features generated from the templates
in Table 2, and the development set is used to
determine the best parameter vector. Figure 2
shows the learning curves for segmentation and
Joint S&T on the development set. We choose
the averaged parameter vector after 7 iterations for
the final test, this parameter vector achieves an F-
measure of 0.973 on segmentation, and 0.925 on
Joint S&T. Although the accuracy on segmentation
is quite high, it is obviously lower on Joint S&T.
Experiments of Ng and Low (2004) on CTB 3.0
also shown the similar trend, where they obtained
F-measure 0.952 on segmentation, and 0.919 on
Joint S&T.
5.2 Preparation for Reranking
For n-best reranking, we can easily generate n best
results for every training instance, by a modifica-
tion for the baseline classifier to hold n best can-
didates at each considering point. For word lattice
reranking, we use the algorithm in Algorithm 2 to
generate the pruned word lattice. Given a training
instance s
i
, its n best result list or pruned word
lattice is used as a reranking instance cand(s
i
),
the best candidate result (of the n best list) or or-
acle diameter (of the pruned word lattice) is the
reranking target y?
i
. We find the best result of the
n best results simply by computing each result?s
390
F-measure, and we determine the oracle diame-
ter of the pruned word lattice using the algorithm
depicted in Algorithm 1. All pairs of cand(s
i
)
and y?
i
deduced from the baseline model?s training
instances comprise the training set for reranking.
The development set and test set for reranking are
obtained in the same way. For the reranking train-
ing set {cand(s
i
), y
?
i
}
N
i=1
, {y
?
i
}
N
i=1
is called oracle
set, and the F-measure of {y?
i
}
N
i=1
against the ref-
erence set is called oracle F-measure. We use the
oracle F-measure indicating the utmost improve-
ment that an reranking algorithm can achieve.
5.3 Results and Analysis
The flows of the n-best list reranking and the
pruned word lattice reranking are similar to the
training procedure for the baseline classifier. The
training set for reranking is used to tune the param-
eter vector of the reranker, while the development
set for reranking is used to determine the optimal
number of iterations for the reranker?s training pro-
cedure.
We compare the performance of the word lat-
tice reranking against the n-best list reranking. Ta-
ble 3 shows the experimental results. The up-
per four rows are the experimental results for n-
best list reranking, while the four rows below are
for word lattice reranking. In n-best list rerank-
ing, with list size 20, the oracle F-measure on
Joint S&T is 0.9455, and the reranked F-measure
is 0.9280. When list size grows up to 50, the oracle
F-measure on Joint S&T jumps to 0.9552, while
the reranked F-measure becomes 0.9302. How-
ever, when n grows to 100, it brings tiny improve-
ment over the situation of n = 50. In word lat-
tice reranking, there is a trend similar to that in
n-best reranking, the performance difference be-
tween in degree = 2 and in degree = 5 is ob-
vious, whereas the setting in degree = 10 does
not obtain a notable improvement over the perfor-
mance of in degree = 5. We also notice that even
with a relative small in degree limitation, such as
in degree = 5, the oracle F-measures for seg-
mentation and Joint S&T both reach a quite high
level. This indicates the pruned word lattice con-
tains much more possibilities of segmentation and
tagging, compared to n-best list.
With the setting in degree = 5, the oracle F-
measure on Joint S&T reaches 0.9774, and the
reranked F-measure climbs to 0.9336. It achieves
an error reduction of 16.3% on Joint S&T, and an
error reduction of 11.9% on segmentation, over the
n-best Ora Seg Tst Seg Ora S&T Tst S&T
20 0.9827 0.9749 0.9455 0.9280
50 0.9903 0.9754 0.9552 0.9302
100 0.9907 0.9755 0.9558 0.9305
Degree Ora Seg Rnk Seg Ora S&T Rnk S&T
2 0.9898 0.9753 0.9549 0.9296
5 0.9927 0.9774 0.9768 0.9336
10 0.9934 0.9774 0.9779 0.9337
Table 3: Performance of n-best list reranking and
word lattice reranking. n-best: the size of the n-
best list for n-best list reranking; Degree: the in de-
gree limitation for word lattice reranking; Ora Seg:
oracle F-measure on segmentation of n-best lists or
word lattices; Ora S&T: oracle F-measure on Joint
S&T of n-best lists or word lattices; Rnk Seg: F-
measure on segmentation of reranked result; Rnk
S&T: F-measure on Joint S&T of reranked result
baseline classifier. While for n-best reranking with
setting n = 50, the Joint S&T?s error reduction is
6.9% , and the segmentation?s error reduction is
8.9%. We can see that reranking on pruned word
lattice is a practical method for segmentation and
POS tagging. Even with a much small data rep-
resentation, it obtains obvious advantage over the
n-best list reranking.
Comparing between the baseline and the two
reranking techniques, We find the non-local infor-
mation such as word- or POS- grams do improve
accuracy of segmentation and POS tagging, and
we also find the reranking technique is effective to
utilize these kinds of information. As even a small
scale n-best list or pruned word lattice can achieve
a rather high oracle F-measure, reranking tech-
nique, especially the word lattice reranking would
be a promising refining strategy for segmentation
and POS tagging. This is based on this viewpoint:
On the one hand, compared with the initial input
character sequence, the pruned word lattice has a
quite smaller search space while with a high ora-
cle F-measure, which enables us to conduct more
precise reranking over this search space to find the
best result. On the other hand, as the structure of
the search space is approximately outlined by the
topological directed architecture of pruned word
lattice, we have a much wider choice for feature se-
lection, which means that we would be able to uti-
lize not only features topologically before the cur-
rent considering position, just like those depicted
in Table 2 in section 4, but also information topo-
logically after it, for example the next word W
1
or
the next POS tag T
1
. We believe the pruned word
391
lattice reranking technique will obtain higher im-
provement, if we develop more precise reranking
algorithm and more appropriate features.
6 Conclusion
This paper describes a reranking strategy called
word lattice reranking. As a derivation of the for-
est reranking of Huang (2008), it performs rerank-
ing on pruned word lattice, instead of on n-best
list. Using word- and POS- gram information, this
reranking technique achieves an error reduction of
16.3% on Joint S&T, and 11.9% on segmentation,
over the baseline classifier, and it also outperforms
reranking on n-best list. It confirms that word lat-
tice reranking can effectively use non-local infor-
mation to select the best candidate result, from a
relative small representation structure while with a
quite high oracle F-measure. However, our rerank-
ing implementation is relative coarse, and it must
have many chances for improvement. In future
work, we will develop more precise pruning al-
gorithm for word lattice generation, to further cut
down the search space while maintaining the ora-
cle F-measure. We will also investigate the feature
selection strategy under the word lattice architec-
ture, for effective use of non-local information.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contracts 60736014
and 60573188, and 863 State Key Project No.
2006AA010108. We show our special thanks to
Liang Huang for his valuable suggestions.
References
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
17th International Conference on Machine Learn-
ing, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Fine, Shai, Yoram Singer, and Naftali Tishby. 1998.
The hierarchical hidden markov model: Analysis
and applications. In Machine Learning, pages 32?
41.
Huang, Liang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics.
Jiang, Wenbin, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 23rd International Con-
ference on Machine Learning, pages 282?289, Mas-
sachusetts, USA.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Rabiner, Lawrence. R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?
286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
392
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 343?350, Prague, June 2007. c?2007 Association for Computational Linguistics
Improving Statistical Machine Translation Performance by         
Training Data Selection and Optimization 
Yajuan L?, Jin Huang and Qun Liu 
Key Laboratory of Intelligent Information Processing 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100080, China 
{lvyajuan, huangjin,liuqun}@ict.ac.cn 
 
Abstract 
Parallel corpus is an indispensable resource 
for translation model training in statistical 
machine translation (SMT). Instead of col-
lecting more and more parallel training 
corpora, this paper aims to improve SMT 
performance by exploiting full potential of 
the existing parallel corpora. Two kinds of 
methods are proposed: offline data optimi-
zation and online model optimization. The 
offline method adapts the training data by 
redistributing the weight of each training 
sentence pairs. The online method adapts 
the translation model by redistributing the 
weight of each predefined submodels. In-
formation retrieval model is used for the 
weighting scheme in both  methods. Ex-
perimental results show that without using 
any additional resource, both methods can 
improve SMT performance significantly.   
1 Introduction 
Statistical machine translation relies heavily on the 
available training data.  Typically, the more data is 
used to estimate the parameters of the translation 
model, the better it can approximate the true trans-
lation probabilities, which will obviously lead to a 
higher translation performance. However, large 
corpora are not easily available. The collected cor-
pora are usually from very different areas. For 
example, the parallel corpora provided by LDC 
come from quite different domains, such as 
Hongkong laws, Hangkong Hansards and 
Hongkong news. This results in the problem that a 
translation system trained on data from a particular 
domain(e.g. Hongkong Hansards) will perform 
poorly when translating text from a different 
domain(e.g. news articles). Our experiments also 
show that simply putting all these domain specific 
corpora together will not always improve 
translation quality. From another aspect, larger 
amount of training data also requires larger 
computational resources. With the increasing of 
training data, the improvement of translation 
quality will become smaller and smaller. Therefore, 
while keeping collecting more and more parallel 
corpora, it is also important to seek effective ways 
of making better use of available parallel training 
data.  
There are two cases when we train a SMT 
system. In one case, we know the target test set or 
target test domain, for example, when building a 
specific domain SMT system or when participating 
the NIST MT evaluation1. In the other case, we are 
unaware of any information of the testing data. 
This paper presents two methods to exploit full 
potential of the available parallel corpora in the 
two cases. For the first case, we try to optimize the 
training data offline to make it match the test data 
better in domain, topic and style, thus improving 
the translation performance. For the second case, 
we first divide the training data into several do-
mains and train submodels for each domain. Then, 
in the translation process, we try to optimize the 
predefined models according to the online input 
source sentence. Information retrieval model is 
used for similar sentences retrieval in both meth-
ods. Our preliminary experiments show that both 
methods can improve SMT performance without 
using any additional data.  
                                                 
1 http://www.nist.gov/speech/tests/mt/ 
343
The remainder of this paper is organized as fol-
lows: Section 2 describes the offline data selection 
and optimization method. Section 3 describes the 
online model optimization method. The evaluation 
and discussion are given in section 4. Related work 
is introduced before concluding.  
2 Offline training data optimization 
In offline training data optimization, we assume 
that the target test data or target test domain is 
known before building the translation model. We 
first select sentences similar to the test text using 
information retrieval method to construct a small 
and adapted training data. Then the extracted simi-
lar subset is used to optimize the distribution of the 
whole training data. The adapted and the optimized 
training data will be used to train new translation 
models.  
2.1 Similar data selection using TF-IDF 
We use information retrieval method for similar 
data retrieval. The standard TF-IDF (Term Fre-
quency and Inverse Document Frequency) term 
weighting scheme is used to measure the similarity 
between the test sentence and the training sentence. 
TF-IDF is a similarity measure widely used in in-
formation retrieval. Each document i is represented 
as a vector  ,  is the size of the 
vocabulary.  is calculate as follows: 
D
),...,,( 21 inii www n
ijw
)log( jijij idftfw ?=  
where,  
ij  is the term frequency(TF) of the j-th word 
in the vocabulary in the document , i.e. the 
number of occurrences;  
tf
iD
j  is the inverse document frequency(IDF) 
of the j-th word calculated as below: 
idf
    
th term-j  #
#
containingdocuments
documentsidf j = . 
The similarity between two documents is then 
defined as the cosine of the angle between the two 
vectors. 
We perform information retrieval using the Le-
mur toolkit2. The source language part of the par-
allel training data is used as the document collec-
tion. Each sentence represents one document. Each 
sentence from the test data or test domain is used 
as one separate query. In the sentence retrieval 
                                                 
2 http://www.cs.cmu.edu/~lemur/
process, both the query and the document are con-
verted into vectors by assigning a term weight to 
each word. Then the cosine similarity is calculated 
proportional to the inner product of the two vectors. 
All retrieved sentences are ranked according to 
their similarity with the query. We pair each of the 
retrieved sentences with the corresponding target 
part and the top N most similar sentences pairs are 
put together to form an adapted parallel data. N 
ranges from one to several thousand in our experi-
ments. Since Lemur toolkit gives the similarity 
score for each retrieved sentences, it is also possi-
ble to select the most similar sentences according 
to the similarity score. 
Note that the selected similar data can contain 
duplicate sentences as the top N retrieval results 
for different test sentences can contain the same 
training sentences. The duplicate sentences will 
force the translation probability towards the more 
often seen words. Intuitively, this could help. In 
experiment section, we will compare experimental 
results by keeping or removing duplicates to see 
how the duplicate sentences affect the translations.  
The selected subset contains the similar sen-
tences with the test data or test domain. It matches 
the test data better in domain, topic and style. 
Hopefully, training translation model using this 
adapted parallel data may helpful for improving 
translation performance. In addition, the translation 
model trained using the selected subset is usually 
much smaller than that trained using the whole 
translation data. Limiting the size of translation 
model is very important for some real applications. 
Since SMT systems usually require large computa-
tion resource. The complexity of standard training 
and decoding algorithm depends mainly on the size 
of the parallel training data and the size of the 
translation model. Limiting the size of the training 
data with the similar translation performance 
would also reduce the memories and speed up the 
translations.  
In the information retrieval process, we only use 
the source language part for document indexing 
and query generating. It is easy to get source part 
of the test data. This is different from the common 
language model adaptation methods, which have to 
do at lease one pass machine translation to get the 
candidate English translation as query(Zhao 2004, 
Zhang 2006). So our method has the advantage 
that it is independent from the quality of baseline 
translation system.  
344
2.2 Training data optimization  
There are two factors on training data that influ-
ence the translation performance of SMT system: 
the scale and the quality. In some sense, we im-
prove the quality of the training data by selecting 
the similar sentence to form an adapted training set. 
However, we also reduce the scale of the training 
data at the same time. Although this is helpful for 
some small device applications, it is also possible 
to induce the data sparseness problem.  Here, we 
introduce a method to optimize between the scale 
and the quality of the training data.  
The basic idea is that we still use all the avail-
able training data; by redistributing the weight of 
each sentence pairs we adapt the whole training 
data to the test domain. In our experiments, we 
simply combine the selected small similar subset 
and the whole training data. The weights of each 
sentence pairs are changed accordingly. Figure 1 
shows the procedure of the optimization.  
 
Figure 1. Training data optimization 
As can be seen, through the optimization, the 
weight of the similar sentence pairs are increased, 
while the general sentence pairs still have an ordi-
nary weight. This make the translation model in-
clined to give higher probabilities to the adapted 
words, and at the same time avoid the data sparse-
ness problem. Since we only change the weight of 
the sentence pairs, and no new training data is in-
troduced, the translation model size trained on the 
optimized data will keep as the same as the origi-
nal one. We use GIZA++ toolkit3 for word align-
                                                 
3 http://www.fjoch.com/GIZA++.html
ment training in the training process. The input 
training file formats for GIZA++ is as follows: 
Each training sentence pair is stored in three lines. 
The first line is the number of times this sentence 
pair occurred. The second line is the source sen-
tence where each token is replaced by its unique 
integer id and the third is the target sentence in the 
same format. To deal with our optimized training 
data, we only need to change the number of sen-
tence pairs in the first line accordingly. This will 
not call for extra training time and memory for the 
whole training process.  
It might be beneficial to investigate other so-
phisticated weighting schemes under the similar 
idea, such as to give more precise fractional 
weights to the sentences according the retrieval 
similarity scores. 
3 Online model optimization  
In most circumstances, we don?t know exactly the 
test data or the test domain when we train a ma-
chine translation system. This results in the fact 
that the performance of the translation system 
highly depends on the training data and the test 
data it is used in. To alleviate this blindfold status 
and maximize the potential of the available train-
ing corpora, we propose a novel online model op-
timization method.  
     The basic idea is that: several candidate transla-
tion models are prepared in training stage. In par-
ticularly, a general model is also prepared. Then, in 
the translation process, the similarity between the 
input sentence and the predefined models is calcu-
lated online to get the weights of each model. The 
optimized model is used to translate the input sen-
tence.  
There are two problems in the method: how to 
prepare submodels in training process and how to 
optimize the model weight online in translation 
process.  
3.1 Prepare the submodels  
There are several ways to prepare submodels in 
training process. If the training data comes from 
very different sources, we can divide the data ac-
cording to its origins. Otherwise, we can use clus-
tering method to separate the training corpus into 
several classes. In addition, our offline data adapta-
tion method can also be used for submodel prepa-
ration. For each candidate domain, we can use the 
345
source side of a small corpus as queries to extract a 
domain specific training set. In this case, a sen-
tence pair in the training data may occur in several 
sub training data, but this doesn?t matter. The gen-
eral model is used when the online input is not 
similar to any prepared submodels. We can use all 
available training data to train the general model 
since generally larger data can get better model 
even there are some noises.   
3.2 Online model weighting 
We also use TF-IDF information retrieval method 
for online model weighting. The procedure is as 
follows: 
For each input sentence: 
 1. Do IR on training data collection, using the 
input sentence as query.  
 2. Determine the weights of submodels accord-
ing to the retrieved sentences.  
 3. Use the optimized model to translate the sen-
tence.  
The information retrieval process is the same as 
the offline data selection except that each retrieved 
sentence is attached with the sub-corpus informa-
tion, i.e. it belongs to which sub-models in the 
training process.   
With the sub-corpus information, we can calcu-
late the weights of submodels. We get the top N 
most similar sentences, and then calculate propor-
tions of each submodel?s sentences. The proportion 
can be calculated use the count of the sentences or 
the similarity score of the sentences. The weight of 
each submodel can be determined according to the 
proportions.  
Our optimized model is the log linear interpola-
tion of the sub-models as follows: 
?
=
?=
M
i
i
icepcepcep
1
0 )|()|()|(? 0
??  
?
=
+=
M
i
ii
e
cepcepe
1
00 )))|(log())|(log((maxarg? ??
 
where, 0 is the probability of general model, ip is 
the probability of submodel i. 0
p
? is the weight of 
general model. i? is the weight of submodel i. Each 
model i is also implemented using log linear model in 
our SMT system. So after the log operation, the sub-
models are interpolated linearly.  
In our experiments, the interpolation factor i?  is 
determined using the following four simple weight-
ing schemes:   
Weighting scheme 1:  
  ;0     ;1     ;00 === ?max_modelimax_model ???  
Weighting scheme 2:  
      if  Proportion(max_model) > 0.5 
          Use weighting scheme1; 
     else 
  ;0    ;1     0 == i??  
Weighting scheme 3:  
   
);(Proportion 
;00
ii model=
=
?
?
 
Weighting scheme 4:  
       if  Proportion(max_model) > 0.5 
           Use weighting scheme3; 
       else 
 );( Proportion5.0     
  ;5.0     0
ii model?=
=
?
?  
where, modeli is the i-th submodel, . 
Proportion (model
)...1( Mi =
i) is the proportion of modeli in 
the retrieved results. We use count for proportion 
calculation. max_model is the submodel with the 
max proportion score. 
The training and translation procedure of online 
model optimization is illustrated in Figure 2. 
Figure 2. Online model optimization 
346
      The online model optimization method makes 
it possible to select suitable models for each indi-
vidual test sentence. Since the IR process is done 
on a fixed training data, the size of the index data 
is quite small compared with the web IR. The IR 
process will not take much time in the translation.  
4 Experiments and evaluation 
4.1 Experimental setting 
We conduct our experiments on Chinese-to-
English translation tasks. The baseline system is a 
variant of the phrase-base SMT system, imple-
mented using log-linear translation model (He et al 
2006). The baseline SMT system is used in all ex-
periments. The only difference between them is 
that they are trained on different parallel training 
data.  
In training process, we use GIZA++4 toolkit for 
word alignment in both translation directions, and 
apply ?grow-diag-final? method to refine it (Koehn 
et al, 2003). We change the preprocess part of 
GIZA++ toolkit to make it accept the weighted 
training data. Then we use the same criterion as 
suggested in (Zens et al, 2002) to do phrase ex-
traction. For the log-linear model training, we take 
minimum-error-rate training method as described 
in (Och, 2003). The language model is trained us-
ing Xinhua portion of Gigaword with about 190M 
words. SRI Language Modeling toolkit5 is used to 
train a 4-gram model with modified Kneser-Ney 
smoothing(Chen and Goodman, 1998). All ex-
periments use the same language model. This en-
sures that any differences in performance are 
caused only by differences in the parallel training 
data. 
Our training data are from three LDC corpora as 
shown in Table 1. We random select 200,000 sen-
tence pairs from each corpus and combine them 
together as the baseline corpus, which includes 
16M Chinese words and 19M English words in 
total. This is the usual case when we train a SMT 
system, i.e. we simply combine all corpora from 
different origins to get a larger training corpus. 
We use the 2002 NIST MT evaluation test data 
as our development set, and the 2005 NIST MT 
test data as the test set in offline data optimization 
experiments. In both data, each sentence has four 
                                                 
4 http://www.fjoch.com/GIZA++.html
5 http://www.speech.sri.com/projects/srilm/
human translations as references. The translation 
quality is evaluated by BLEU metric (Papineni et 
al., 2002), as calculated by mteval-v11b.pl6 with 
case-sensitive matching of n-grams. 
Corpus LDC No. Description # sent. pairs
FBIS LDC2003E14 FBIS Multilanguage Texts 200000 
HK_Hansards LDC2004T08 Hong Kong Hansards Text 200000 
HK_News LDC2004T08 Hong Kong News Text 200000 
Baseline - All above data 600000 
Table 1. Training corpora 
4.2 Baseline experiments 
We first train translation models on each sub train-
ing corpus and the baseline corpus. The develop-
ment set is used to tune the feature weights. The 
results on test set are shown in Table 2.   
System BLEU on dev set BLEU on test set 
FBIS 0.2614 0.2331 
HK_Hansards 0.1679 0.1624 
HK_News 0.1748 0.1608  
Baseline 0.2565 0.2363 
Table 2. Baseline results 
From the results we can see that although the 
size of each sub training corpus is similar, the 
translation results from the corresponding system 
are quite different on the same test set. It seems 
that the FBIS corpus is much similar to the test set 
than the other two corpora.  In fact, it is the case. 
The FBIS contains text mainly from China 
mainland news stories, while the 2005 NIST test 
set alo include lots of China news text. The results 
illustrate the importance of selecting suitable train-
ing data.  
When combining all the sub corpora together, 
the baseline system gets a little better result than 
the sub systems. This indicates that larger data is 
useful even it includes some noise data. However, 
compared with the FBIS corpus, the baseline cor-
pus contains three times larger data, while the im-
provement of translation result is not significant. 
This indicates that simply putting different corpora 
together is not a good way to make use of the 
available corpora.  
                                                 
6http://www.nist.gov/speech/tests/mt/resources/scoring.htm  
347
4.3 Offline data optimization experiments 
We use baseline corpus as initial training corpus, 
and take Lemur toolkit to build document index on 
Chinese part of the corpus. The Chinese sentences 
in development set and test set are used as queries. 
For each query, N = 100, 200, 500, 1000, 2000 
similar sentences are retrieved from the indexed 
collection. The extracted similar sentence pairs are 
used to train the new adapted translation models. 
Table 3 illustrates the results. We give the distinct 
pair numbers for each adapted set and compare the 
size of the translation models. To illustrate the ef-
fect of duplicate sentences, we also give the results 
with duplicates and without duplicates (distinct). 
System Distinct pairs 
Size of 
trans model 
BLEU on 
duplicates
BLEU on 
distinct
Baseline 600000 2.41G 0.2363 0.2363 
Top100 91804 0.43G 0.2306 0.2346 
Top200 150619 0.73G 0.2360 0.2345 
Top500 261003 1.28G 0.2415 0.2370 
Top1000 357337 1.74G 0.2463 0.2376 
Top2000 445890 2.11G 0.2351 0.2346 
Table 3. Offline data adaptation results 
The results show that: 
1. By using similar data selection, it is possible 
to use much smaller training data to get compara-
ble or even better results than the baseline system. 
When N=200, using only 1/4 of the training data 
and 1/3 of the model size, the adapted translation 
model achieves comparable result with the baseline 
model. When N=500, the adapted model outper-
forms the baseline model with much less training 
data. The results indicate that relevant data is better 
data. The method is particular useful for SMT ap-
plications on small device.  
2. In general, using duplicate data achieves bet-
ter results than using distinct data. This justifies 
our idea that give a higher weight to more similar 
data will benefit.  
3. With the increase of training data size,   the 
translation performance tends to improve also. 
However, when the size of corpus achieves a cer-
tain scale, the performance may drop. This maybe 
because that with the increase of the data, noisy 
data may also be included. More and more in-
cluded noises may destroy the data. It is necessary 
to use a development set to determine an optimal 
size of N. 
We combine each adapted data with the baseline 
corpus to get the optimized models. The results are 
shown in Table 4. We also compare the adapted 
models (TopN) and the optimized models (TopN+) 
in the table.   
Without using any additional data, the optimized 
models achieve significant better results than the 
baseline model by redistributing the weight of 
training sentences. The optimized models also out-
perform adapted models when the size of the 
adapted data is small since they make use of all the 
available data which decrease the influence of data 
sparseness. However, with the increase of the 
adapted data, the performance of optimized models 
is similar to that of the adapted models.  
System Distinct pairs 
BLEU on 
TopN 
BLEU on 
TopN+ 
Baseline 600000 0.2363 0.2363 
Top100+ 600000 0.2306 0.2387 
Top200+ 600000 0.2360 0.2443 
Top500+ 600000 0.2415 0.2461 
Top1000+ 600000 0.2463 0.2431 
Top2000+ 600000 0.2351 0.2355 
Table 4. Offline data optimization results 
4.4 Online model optimization experiments 
Since 2005 NIST MT test data tends bias to FBIS 
corpus too much, we build a new test set to evalu-
ate the online model optimization method. We ran-
domly select 500 sentences from extra part of FBIS, 
HK_Hansards and HK_News corpus respectively 
(i.e the selected 1500 test sentences are not in-
cluded in any of the training set). The correspond-
ing English part is used as translation reference. 
Note that there is only one reference for each test 
sentence. We also include top 500 sentence and 
their first reference translation of 2005 NIST MT 
test data in the new test set. So in total, the new test 
contains 2000 test sentences with one translation 
reference for each sentence.  The test set is used to 
simulate SMT system?s online inputs which may 
come from various domains.   
The baseline translation results are shown in Ta-
ble 5. We also give results on each sub test set (de-
notes as Xcorpus_part). Please note that the abso-
lute BLEU scores are not comparable to the previ-
ous experiments since there is only one reference 
in this test set.  
348
As expected, using the same domain data for 
training and testing achieves the best results as in-
dicate by bold fonts.  The results demonstrate 
again that relevant data is better data.  
To test our online model optimization method, 
we divide the baseline corpus according to the ori-
gins of sub corpus. That is, the FBIS, HK_ Han-
sards and HK_News models are used as three sub-
models and the baseline model is used as general 
model. The four weighting schemes described in 
section 3.2 are used as online weighting schemes 
individually. The experimental results are shown in 
Table 6. S_i indicates the system using weighting 
scheme i.   
      System 
Test data FBIS 
HK_ 
Hansards 
HK_
News Baseline
FBIS-part 0.1096 0.0687 0.0622 0.1030
HK_Hans_part 0.0726 0.0918 0.0846 0.0897
HK_News_part 0.0664 0.0801 0.0936 0.0870
MT05_part 0.1130 0.0805 0.0776 0.1116
Whole test set 0.0937 0.0799 0.0781 0.0993
Table 5. Baseline results on new test set 
      System 
Test data S_1 S_2 S_3 S_4 
FBIS-part 0.1090 0.1090 0.1089 0.1089
HK_Hans_part 0.0906 0.0903 0.0902 0.0902
HK_News_part 0.0952 0.0950 0.0933 0.0934
MT05_part 0.1119 0.1123 0.1149 0.1151
Whole test set 0.1034 0.1034 0.1038 0.1038
Table 6. Online model optimization results 
Different weighting schemes don?t show signifi-
cant improvements from each other. However, all 
the four weighting schemes achieve better results 
than the baseline system. The improvements are 
shown not only on the whole test set but also on 
each part of the sub test set. The results justify the 
effectiveness of our online model optimization 
method.  
5 Related work 
Most previous research on SMT training data is 
focused on parallel data collection. Some work 
tries to acquire parallel sentences from web (Nie et 
al. 1999; Resnik and Smith 2003; Chen et al 2004). 
Others extract parallel sentences from comparable 
or non-parallel corpora (Munteanu and Marcu 
2005, 2006). These work aims to collect more 
parallel training corpora, while our work aims to 
make better use of existing parallel corpora.  
Some research has been conducted on parallel 
data selection and adaptation. Eck et al (2005) 
propose a method to select more informative sen-
tences based on n-gram coverage. They use n-
grams to estimate the importance of a sentence. 
The more previously unseen n-grams in the sen-
tence the more important the sentence is. TF-IDF 
weighting scheme is also tried in their method, but 
didn?t show improvements over n-grams. This 
method is independent of test data. Their goal is to 
decrease the amount of training data to make SMT 
system adaptable to small devices. Similar to our 
work, Hildebrand et al (2005) also use information 
retrieval method for translation model adaptation.  
They select sentences similar to the test set from 
available in-of-domain and out-of-domain training 
data to form an adapted translation model. Differ-
ent from their work, our method further use the 
small adapted data to optimize the distribution of 
the whole training data. It takes the full advantage 
of larger data and adapted data. In addition, we 
also propose an online translation model optimiza-
tion method, which make it possible to select 
adapted translation model for each individual sen-
tence. 
Since large scale monolingual corpora are easier 
to obtain than parallel corpora. There has some 
research on language model adaptation recent 
years. Zhao et al (2004) and Eck et al(2004) in-
troduce information retrieval method for language 
model adaptation. Zhang et al(2006)  and  Mauser 
et al(2006) use adapted language model for SMT 
re-ranking. Since language model is built for target 
language in SMT, one pass translation is usually 
needed to generate n-best translation candidates in 
language model adaptation. Translation model ad-
aptation doesn?t need a pre-translation procedure. 
Comparatively, it is more direct. Language model 
adaptation and translation model adaptation are 
good complement to each other. It is possible that 
combine these two adaptation approaches could 
further improve machine translation performance. 
6 Conclusion and future work 
This paper presents two new methods to im-
prove statistical machine translation performance 
by making better use of the available parallel train-
ing corpora. The offline data selection method 
349
adapts the training corpora to the test domain by 
retrieving similar sentence pairs and redistributing 
their weight in the training data. Experimental re-
sults show that the selected small subset achieves 
comparable or even better performance than the 
baseline system with much less training data. The 
optimized training data can further improve trans-
lation performance without using any additional 
resource. The online model optimization method 
adapts the translation model to the online test 
sentence by redistributing the weight of each 
predefined submodels. Preliminary results show 
the effectiveness of the method. Our work also 
demonstrates that in addition to larger training data, 
more relevant training data is also important for 
SMT model training. 
In future work, we will improve our methods in 
several aspects. Currently, the similar sentence re-
trieval model and the weighting schemes are very 
simple. It might work better by trying other sophis-
ticated similarity measure models or using some 
optimization algorithms to determine submodel?s 
weights. Introducing language model optimization 
into our system might further improve translation 
performance.  
Acknowledgement 
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095 
and 60573188. 
References 
Jisong Chen, Rowena Chau, Chung-Hsing Yeh 2004. 
Discovering Parallel Text from the World Wide Web. 
ACSW Frontiers 2004: 157-161  
Stanley F. Chen and Joshua Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language 
Modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technol-
ogy. 
Matthias Eck, Stephan Vogel, and Alex Waibel 2004. 
Language Model Adaptation for Statistical Machine 
Translation Based on Information Retrieval. Proceed-
ings of Fourth International Conference on Language 
Resources and Evaluation:327-330 
Matthias Eck, Stephan Vogel,  Alex Waibel 2005. Low 
cost portability for statistical machine translation 
based on n-gram coverage. MT Summit X: 227-234. 
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, and 
Qun Liu 2006. ICT System Description for the 2006 
TC-STAR Run#2 SLT Evaluation. Proceedings of TC-
STAR Workshop on Speech-to-Speech Translation: 
63-68 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. Proceedings of 
HLT-NAACL 2003: 127?133. 
Arne Mauser, Richard Zens, Evgeny Matusov, Sasa 
Hasan, Hermann Ney 2006. The RWTH Statistical 
Machine Translation System for the IWSLT 2006 
Evaluation. Proceedings of International Workshop 
on Spoken Language Translation.:103-110 
Dragos Stefan Munteanu and Daniel Marcu 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Comparable Corpora. Computational Lin-
guistics, 31 (4): 477-504  
Dragos Stefan Munteanu and Daniel Marcu 2006. Ex-
tracting Parallel Sub-Sentential Fragments from 
Comparable Corpora. ACL-2006: 81-88 
Jian-Yun Nie, Michel Simard, Pierre Isabelle, Richard 
Durand 1999. Cross-Language Information Retrieval 
based on Parallel Texts and Automatic Mining of 
Parallel Texts in the Web. SIGIR-1999: 74-81 
Franz Josef Och 2003. Minimum Error Rate Training in 
Statistical Machine Translation. ACL-2003:160-167. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic 
Evaluation of Machine Translation. ACL-2002: 311?
318 
Philip Resnik and Noah A. Smith 2003. The Web as a 
Parallel Corpus. Computational Linguistics 29(3): 
349-380 
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, 
and Alex Waibel 2005. Adaptation of the Translation 
Model for Statistical Machine Translation based on 
Information Retrieval. Proceedings of EAMT 2005: 
133-142. 
Richard Zens, Franz Josef Och, Hermann Ney 2002. 
Phrase-Based Statistical Machine Translation. An-
nual German Conference on AI, KI 2002, Vol. LNAI 
2479: 18-32 
Ying Zhang, Almut Silja Hildebrand, Stephan Vogel 
2006. Distributed Language Modeling for N-best List 
Re-ranking.  EMNLP-2006:216-223 
Bing Zhao, Matthias Eck, Stephan Vogel 2004. Lan-
guage Model Adaptation for Statistical Machine 
Translation with structured query models. COLING- 
2004 
350
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89?97,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Maximum Entropy based Rule Selection Model for
Syntax-based Statistical Machine Translation
Qun Liu1 and Zhongjun He1,2 and Yang Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{liuqun,zjhe,yliu,sxlin}@ict.ac.cn
Abstract
This paper proposes a novel maximum en-
tropy based rule selection (MERS) model
for syntax-based statistical machine transla-
tion (SMT). The MERS model combines lo-
cal contextual information around rules and
information of sub-trees covered by variables
in rules. Therefore, our model allows the de-
coder to perform context-dependent rule se-
lection during decoding. We incorporate the
MERS model into a state-of-the-art linguis-
tically syntax-based SMT model, the tree-
to-string alignment template model. Experi-
ments show that our approach achieves signif-
icant improvements over the baseline system.
1 Introduction
Syntax-based statistical machine translation (SMT)
models (Liu et al, 2006; Galley et al, 2006; Huang
et al, 2006) capture long distance reorderings by us-
ing rules with structural and linguistical information
as translation knowledge. Typically, a translation
rule consists of a source-side and a target-side. How-
ever, the source-side of a rule usually corresponds
to multiple target-sides in multiple rules. Therefore,
during decoding, the decoder should select a correct
target-side for a source-side. We call this rule selec-
tion.
Rule selection is of great importance to syntax-
based SMT systems. Comparing with word selec-
tion in word-based SMT and phrase selection in
phrase-based SMT, rule selection is more generic
and important. This is because that a rule not only
contains terminals (words or phrases), but also con-
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
X 1 X 2 levels X 2 standard of X 1
Figure 1: Example of translation rules
tains nonterminals and structural information. Ter-
minals indicate lexical translations, while nontermi-
nals and structural information can capture short or
long distance reorderings. See rules in Figure 1 for
illustration. These two rules share the same syntactic
tree on the source side. However, on the target side,
either the translations for terminals or the phrase re-
orderings for nonterminals are quite different. Dur-
ing decoding, when a rule is selected and applied to a
source text, both lexical translations (for terminals)
and reorderings (for nonterminals) are determined.
Therefore, rule selection affects both lexical transla-
tion and phrase reordering.
However, most of the current syntax-based sys-
tems ignore contextual information when they se-
lecting rules during decoding, especially the infor-
mation of sub-trees covered by nonterminals. For
example, the information of X 1 and X 2 is not
recorded when the rules in Figure 1 extracted from
the training examples in Figure 2. This makes the
decoder hardly distinguish the two rules. Intuitively,
information of sub-trees covered by nonterminals as
well as contextual information of rules are believed
89
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
industrial products manufacturing levels overall standard of the match
Figure 2: Training examples for rules in Figure 1
to be helpful for rule selection.
Recent research showed that contextual infor-
mation can help perform word or phrase selec-
tion. Carpuat and Wu (2007b) and Chan et
al. (2007) showed improvents by integrating word-
sense-disambiguation (WSD) system into a phrase-
based (Koehn, 2004) and a hierarchical phrase-
based (Chiang, 2005) SMT system, respectively.
Similar to WSD, Carpuat and Wu (2007a) used con-
textual information to solve the ambiguity prob-
lem for phrases. They integrated a phrase-sense-
disambiguation (PSD) model into a phrase-based
SMT system and achieved improvements.
In this paper, we propose a novel solution for
rule selection for syntax-based SMT. We use the
maximum entropy approach to combine rich con-
textual information around a rule and the informa-
tion of sub-trees covered by nonterminals in a rule.
For each ambiguous source-side of translation rules,
a maximum entropy based rule selection (MERS)
model is built. Thus the MERS models can help the
decoder to perform a context-dependent rule selec-
tion.
Comparing with WSD (or PSD), there are some
advantages of our approach:
? Our approach resolves ambiguity for rules with
multi-level syntactic structure, while WSD re-
solves ambiguity for strings that have no struc-
tures;
? Our approach can help the decoder perform
both lexical selection and phrase reorderings,
while WSD can help the decoder only perform
lexical selection;
? Our method takes WSD as a special case, since
a rule may only consists of terminals.
In our previous work (He et al, 2008), we re-
ported improvements by integrating a MERS model
into a formally syntax-based SMT model, the hier-
archical phrase-based model (Chiang, 2005). In this
paper, we incorporate the MERS model into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment template (TAT) model
(Liu et al, 2006). The basic differences are:
? The MERS model here combines rich informa-
tion of source syntactic tree as features since
the translation model is linguistically syntax-
based. He et al (2008) did not use this in-
formation.
? In this paper, we build MERS models for all
ambiguous source-sides, including lexicalized
(source-side which only contains terminals),
partially lexicalized (source-side which con-
tains both terminals and nonterminals), and un-
lexicalized (source-side which only contains
nonterminals). He et al (2008) only built
MERS models for partially lexicalized source-
sides.
In the TAT model, a TAT can be considered as a
translation rule which describes correspondence be-
tween source syntactic tree and target string. TAT
can capture linguistically motivated reorderings at
short or long distance. Experiments show that by
incorporating MERS model, the baseline system
achieves statistically significant improvement.
This paper is organized as follows: Section 2
reviews the TAT model; Section 3 introduces the
MERS model and describes feature definitions; Sec-
tion 4 demonstrates a method to incorporate the
MERS model into the translation model; Section 5
reports and analyzes experimental results; Section 6
gives conclusions.
2 Baseline System
Our baseline system is Lynx (Liu et al, 2006),
which is a linguistically syntax-based SMT system.
For translating a source sentence fJ1 = f1...fj ...fJ ,
Lynx firstly employs a parser to produce a source
syntactic tree T (fJ1 ), and then uses the source
syntactic tree as the input to search translations:
90
e?I1 = argmaxe?I1Pr(e
I
1|f
J
1 )(1)
= argmaxe?I1Pr(T (f
J
1 )|f
J
1 )Pr(e
I
1|T (f
J
1 ))
In doing this, Lynx uses tree-to-string alignment
template to build relationship between source syn-
tactic tree and target string. A TAT is actually a
translation rule: the source-side is a parser tree with
leaves consisting of words and nonterminals, the
target-side is a target string consisting of words and
nonterminals.
TAT can be learned from word-aligned, source-
parsed parallel corpus. Figure 4 shows three types
of TATs extracted from the training example in Fig-
ure 3: lexicalized (the left), partially lexicalized
(the middle), unlexicalized (the right). Lexicalized
TAT contains only terminals, which is similar to
phrase-to-phrase translation in phrase-based model
except that it is constrained by a syntactic tree on the
source-side. Partially lexicalized TAT contains both
terminals and non-terminals, which can be used for
both lexical translation and phrase reordering. Un-
lexicalized TAT contains only nonterminals and can
only be used for phrase reordering.
Lynx builds translation model in a log-linear
framework (Och and Ney, 2002):
P (eI1|T (f
J
1 )) =(2)
exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]?
e? exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]
Following features are used:
? Translation probabilities: P (e?|T? ) and P (T? |e?);
? Lexical weights: Pw(e?|T? ) and Pw(T? |e?);
? TAT penalty: exp(1), which is analogous to
phrase penalty in phrase-based model;
? Language model Plm(eI1);
? Word penalty I .
In Lynx, rule selection mainly depends on trans-
lation probabilities and lexical weights. These four
scores describe how well a source tree links to a tar-
get string, which are estimated on the training cor-
pus according to occurrence times of e? and T? . There
IP
NPB
NN NN NN
VP
VV VPB
VV
The incomes of city and village resident continued to grow
Figure 3: Word-aligned, source-parsed training example.
NN NPB
NN
X 1
NN NN
NPB
NN
X 1
NN
X 2
NN
X 3
city and village incomes of X 1 resident X 3 X 1 X 2
Figure 4: TATs learned from the training example in Fig-
ure 3.
are no features in Lynx that can capture contextual
information during decoding, except for the n-gram
language model which considers the left and right
neighboring n-1 target words. But this information
it very limited.
3 The Maximum Entropy based Rule
Selection Model
3.1 The model
In this paper, we focus on using contextual infor-
mation to help the TAT model perform context-
dependent rule selection. We consider the rule se-
lection task as a multi-class classification task: for
a source syntactic tree T? , each corresponding target
string e? is a label. Thus during decoding, when a
TAT ?T? , e??? is selected, T? is classified into label e??,
actually.
A good way to solve the classification problem is
the maximum entropy approach:
Prs(e?|T? , T (Xk)) =(3)
exp[
?
i ?ihi(e?, C(T? ), T (Xk))]
?
e??
exp[
?
i ?ihi(e??, C(T? ), T (Xk))]
91
where T? and e? are the source tree and target string of
a TAT, respectively. hi is a binary feature functions
and ?i is the feature weight of hi. C(T? ) defines local
contextual information of T? . Xk is a nonterminal in
the source tree T? , where k is an index. T (Xk) is the
source sub-tree covered by Xk.
The advantage of the MERS model is that it uses
rich contextual information to compute posterior
probability for e? given T? . However, the transla-
tion probabilities and lexical weights in Lynx ignore
these information.
Note that for each ambiguous source tree, we
build a MERS model. That means, if there are
N source trees extracted from the training corpus
are ambiguous (the source tree which corresponds
to multiple translations), thus for each ambiguous
source tree Ti (i = 1, ..., N ), a MERS model Mi
(i = 1, ..., N ) is built. Since a source tree may cor-
respond to several hundreds of target translations at
most, the feature space of a MERS model is not pro-
hibitively large. Thus the complexity for training a
MERS model is low.
3.2 Feature Definition
Let ?T? , e?? be a translation rule in the TAT model.
We use f(T? ) to represent the source phrase covered
by T? . To build a MERS model for the source tree T? ,
we explore various features listed below.
1. Lexical Features (LF)
These features are defined on source words.
Specifically, there are two kinds of lexical fea-
tures: external features f?1 and f+1, which
are the source words immediately to the left
and right of f(T? ), respectively; internal fea-
tures fL(T (Xk)) and fR(T (Xk)), which are
the left most and right most boundary words of
the source phrase covered by T (Xk), respec-
tively.
See Figure 5 (a) for illustration. In
this example, f?1=t??ga?o, f+1=zh?`za`o,
fL(T (X1))=go?ngye`, fR(T (X1))=cha?np??n.
2. Parts-of-speech (POS) Features (POSF)
These features are the POS tags of the source
words defined in the lexical features: P?1,
P+1, PL(T (Xk)), PR(T (Xk)) are the POS
tags of f?1, f+1, fL(T (Xk)), fR(T (Xk)), re-
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(a) Lexical Features
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(b) POS Features
DNP
X 1 :NP
2 words
DEG
de
NP
DNP
X 1 :NP DEG
de
(c) Span Feature (d) Parent Feature
NP
DNP
X 1 :NP DEG
de
NPB
(e) Sibling Feature
Figure 5: Illustration of features of theMERSmodel. The
source tree of the TAT is ? DNP(NP X 1 ) (DEG de)?.
Gray nodes denote information included in the feature.
92
spectively. POS tags can generalize over all
training examples.
Figure 5 (b) shows POS features. P?1=VV,
P+1=NN, PL(T (X1))=NN, PR(T (X1))=NN.
3. Span Features (SPF)
These features are the length of the source
phrase f(T (Xk)) covered by T (Xk). In Liu?s
TATmodel, the knowledge learned from a short
span can be used for a larger span. This is not
reliable. Thus we use span features to allow the
MERS model to learn a preference for short or
large span.
In Figure 5 (c), the span of X 1 is 2.
4. Parent Feature (PF)
The parent node of T? in the parser tree of the
source sentence. The same source sub-tree may
have different parent nodes in different training
examples. Therefore, this feature may provide
information for distinguishing source sub-trees.
Figure 5 (d) shows that the parent is a NP node.
5. Sibling Features (SBF)
The siblings of the root of T? . This feature con-
siders neighboring nodes which share the same
parent node.
In Figure 5 (e), the source tree has one sibling
node NPB.
Those features make use of rich information
around a rule, including the contextual information
of a rule and the information of sub-trees covered
by nonterminals. They are never used in Liu?s TAT
model.
Figure 5 shows features for a partially lexicalized
source tree. Furthermore, we also build MERS mod-
els for lexicalized and unlexicalized source trees.
Note that for lexicalized tree, features do not include
the information of sub-trees since there is no nonter-
minals.
The features can be easily obtained by modify-
ing the TAT extraction algorithm described in (Liu
et al, 2006). When a TAT is extracted from a
word-aligned, source-parsed parallel sentence, we
just record the contextual features and the features of
the sub-trees. Then we use the toolkit implemented
by Zhang (2004) to train MERS models for the am-
biguous source syntactic trees separately. We set the
iteration number to 100 and Gaussian prior to 1.
4 Integrating the MERS Models into the
Translation Model
We integrate the MERS models into the TAT model
during the translation of each source sentence. Thus
the MERS models can help the decoder perform
context-dependent rule selection during decoding.
For integration, we add two new features into the
log-linear translation model:
? Prs(e?|T? , T (Xk)). This feature is computed by
the MERS model according to equation (3),
which gives a probability that the model select-
ing a target-side e? given an ambiguous source-
side T? , considering rich contextual informa-
tion.
? Pap = exp(1). During decoding, if a source
tree has multiple translations, this feature is set
to exp(1), otherwise it is set to exp(0). Since
the MERS models are only built for ambiguous
source trees, the first feature Prs(e?|T? , T (Xk))
for non-ambiguous source tree will be set to
1.0. Therefore, the decoder will prefer to
use non-ambiguous TATs. However, non-
ambiguous TATs usually occur only once in the
training corpus, which are not reliable. Thus
we use this feature to reward ambiguous TATs.
The advantage of our integration is that we need
not change the main decoding algorithm of Lynx.
Furthermore, the weights of the new features can be
trained together with other features of the translation
model.
5 Experiments
5.1 Corpus
We carry out experiments on Chinese-to-English
translation. The training corpus is the FBIS cor-
pus, which contains 239k sentence pairs with 6.9M
Chinese words and 8.9M English words. For the
language model, we use SRI Language Modeling
Toolkit (Stolcke, 2002) with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) to train two
tri-gram language models on the English portion of
93
No. of No. of No. of ambiguous
Type
TATs source trees source trees
% ambiguous
Lexicalized 333,077 16,367 14,380 87.86
Partially Lexicalized 342,767 38,497 28,397 73.76
Unlexicalized 83,024 7,384 5,991 81.13
Total 758,868 62,248 48,768 78.34
Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.
System
Features
P (e?|T? ) P (T? |e?) Pw(e?|T? ) Pw(T? |e?) lm1 lm2 TP WP Prs AP
Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - -
+MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207
Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used
by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for
WP and AP indicate a reward.
the training corpus and the Xinhua portion of the Gi-
gaword corpus, respectively. NIST MT 2002 test set
is used as the development set. NIST MT 2003 and
NIST MT 2005 test sets are used as the test sets.
The translation quality is evaluated by BLEU met-
ric (Papineni et al, 2002), as calculated by mteval-
v11b.pl with case-insensitive matching of n-grams,
where n = 4.
5.2 Training
To train the translation model, we first run GIZA++
(Och and Ney, 2000) to obtain word alignment in
both translation directions. Then the word alignment
is refined by performing ?grow-diag-final? method
(Koehn et al, 2003). We use a Chinese parser de-
veloped by Deyi Xiong (Xiong et al, 2005) to parse
the Chinese sentences of the training corpus.
Our TAT extraction algorithm is similar to Liu et
al. (2006), except that we make some tiny modifica-
tions to extract contextual features for MERS mod-
els. To extract TAT, we set the maximum height of
the source sub-tree to h = 3, the maximum number
of direct descendants of a node of sub-tree to c = 5.
See (Liu et al, 2006) for specific definitions of these
parameters.
Table 1 shows statistical information of TATs
which are filtered by the two test sets. For each type
(lexicalized, partially lexicalized, unlexicalized) of
TATs, a great portion of the source trees are am-
biguous. The number of ambiguous source trees ac-
counts for 78.34% of the total source trees. This in-
dicates that the TAT model faces serious rule selec-
tion problem during decoding.
5.3 Results
We use Lynx as the baseline system. Then the
MERS models are incorporated into Lynx, and
the system is called Lynx+MERS. To run the
decoder, Lynx and Lynx+MERS share the same
settings: tatTable-limit=30, tatTable-threshold=0,
stack-limit=100, stack-threshold=0.00001. The
meanings of the pruning parameters are the same to
Liu et al (2006).
We perform minimum error rate training (Och,
2003) to tune the feature weights for the log-linear
model to maximize the systems?s BLEU score on the
development set. The weights are shown in Table 2.
These weights are then used to run Lynx and
Lynx+MERS on the test sets. Table 3 shows the
results. Lynx obtains BLEU scores of 26.15 on
NIST03 and 26.09 on NIST05. Using all features
described in Section 3.2, Lynx+MERS finally ob-
tains BLEU scores of 27.05 on NIST03 and 27.28
on NIST05. The absolute improvements is 0.90
and 1.19, respectively. Using the sign-test described
by Collins et al (2005), both improvements are
statistically significant at p < 0.01. Moreover,
Lynx+MERS also achieves higher n-gram preci-
sions than Lynx.
94
Test Set System BLEU-4
Individual n-gram precisions
1 2 3 4
NIST03
Lynx 26.15 71.62 35.64 18.64 9.82
+MERS 27.05 72.00 36.72 19.51 10.37
NIST05
Lynx 26.09 70.39 35.12 18.53 10.11
+MERS 27.28 71.16 36.19 19.62 10.95
Table 3: BLEU-4 scores (case-insensitive) on the test sets.
5.4 Analysis
The baseline system only uses four features for
rule selection: the translation probabilities P (e?|T? )
and P (T? |e?); and the lexical weights Pw(e?|T? ) and
Pw(T? |e?). These features are estimated on the train-
ing corpus by the maximum likelihood approach,
which does not allow the decoder to perform a con-
text dependent rule selection. Although Lynx uses
language model as feature, the n-gram language
model only considers the left and right n-1 neigh-
boring target words.
The MERS models combines rich contextual in-
formation as features to help the decoder perform
rule selection. Table 4 shows the effect of different
feature sets. We test two classes of feature sets: the
single feature (the top four rows of Table 4) and the
combination of features (the bottom five rows of Ta-
ble 4). For the single feature set, the POS tags are
the most useful and stable features. Using this fea-
ture, Lynx+MERS achieves improvements on both
the test sets. The reason is that POS tags can be gen-
eralized over all training examples, which can alle-
viate the data sparseness problem.
Although we find that some single features may
hurt the BLEU score, they are useful in combina-
tion of features. This is because one of the strengths
of the maximum entropy model is that it can in-
corporate various features to perform classification.
Therefore, using all features defined in Section 3.2,
we obtain statistically significant improvements (the
last row of Table 4). In order to know how the
MERS models improve translation quality, we in-
spect the 1-best outputs of Lynx and Lynx+MERS.
We find that the first way that theMERSmodels help
the decoder is that they can perform better selection
for words or phrases, similar to the effect of WSD
or PSD. This is because that lexicalized and partially
lexicalized TAT contains terminals. Considering the
Feature Sets NIST03 NIST05
LF 26.12 26.32
POSF 26.36 26.21
PF 26.17 25.90
SBF 26.47 26.08
LF+POSF 26.61 26.59
LF+POSF+SPF 26.70 26.44
LF+POSF+PF 26.81 26.56
LF+POSF+SBF 26.68 26.89
LF+POSF+SPF+PF+SBF 27.05 27.28
Table 4: BLEU-4 scores on different feature sets.
following examples:
? Source:
? Reference: Malta is located in southern Eu-
rope
? Lynx: Malta in southern Europe
? Lynx+MERS: Malta is located in southern Eu-
rope
Here the Chinese word ? ? is incor-
rectly translated into ?in? by the baseline system.
Lynx+MERS produces the correct translation ?is lo-
cated in?. That is because, the MERS model consid-
ers more contextual information for rule selection.
In the MERS model, Prs(in| ) = 0.09, which is
smaller than Prs(is located in| ) = 0.14. There-
fore, the MERS model prefers the translation ?is lo-
cated in?. Note that here the source tree (VV )
is lexicalized, and the role of the MERS model is
actually the same as WSD.
The second way that the MERS models help the
decoder is that they can perform better phrase re-
orderings. Considering the following examples:
95
? Source: [ ]1 [ ]2
...
? Reference: According to its [development
strategy]2 [in the Chinese market]1 ...
? Lynx: Accordance with [the Chinese market]1
[development strategy]2 ...
? Lynx+MERS: According to the [development
strategy]2 [in the Chinese market]1
The syntactic tree of the Chinese phrase ?
? is shown in Figure 6. How-
ever, there are two TATs which can be applied to the
source tree, as shown in Figure 7. The baseline sys-
tem selects the left TAT and produces a monotone
translation of the subtrees ?X 1 :PP? and ?X 2 :NPB?.
However, Lynx+MERS uses the right TAT and per-
forms correct phrase reordering by swapping the two
source phrases. Here the source tree is partially lex-
icalized, and both the contextual information and
the information of sub-trees covered by nontermi-
nals are considered by the MERS model.
6 Conclusion
In this paper, we propose a maximum entropy based
rule selection model for syntax-based SMT. We
use two kinds information as features: the local-
contextual information of a rule, the information of
sub-trees matched by nonterminals in a rule. During
decoding, these features allow the decoder to per-
form a context-dependent rule selection. However,
this information is never used in most of the current
syntax-based SMT models.
The advantage of the MERS model is that it can
help the decoder not only perform lexical selection,
but also phrase reorderings. We demonstrate one
way to incorporate the MERS models into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment model. Experiments
show that by incorporating the MERS models, the
baseline system achieves statistically significant im-
provements.
We find that rich contextual information can im-
prove translation quality for a syntax-based SMT
system. In future, we will explore more sophisti-
cated features for the MERS model. Moreover, we
will test the performance of the MERS model on
large scale corpus.
NP
DNP
PP DEG
NPB
in Chinese market of
development strategy
Figure 6: Syntactic tree of the source phrase ?
?.
NP
DNP
PP
X 1
DEG
NPB
X 2
NP
DNP
PP
X 1
DEG
NPB
X 2
X 1 X 2 X 2 X 1
Figure 7: TATs which can be used for the source phrase
? ?.
Acknowledgements
We would like to thank Yajuan Lv for her valuable
suggestions. This work was supported by the Na-
tional Natural Science Foundation of China (NO.
60573188 and 60736014), and the High Technology
Research and Development Program of China (NO.
2006AA010108).
References
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In 11th
Conference on Theoretical and Methodological Issues
in Machine Translation, pages 43?52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
96
Meeting of the Association for Computational Linguis-
tics, pages 33?40.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL05, pages 531?540.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL 2006, pages 961?968.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 321?328.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics, pages
609?616.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901?904.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the penn chinese tree-
bank with semantic knowledge. In Proceedings of
IJCNLP 2005, pages 70?81.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
97
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017?1026,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Weighted Alignment Matrices for Statistical Machine Translation
Yang Liu , Tian Xia , Xinyan Xiao and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,xiatian,xiaoxinyan,liuqun}@ict.ac.cn
Abstract
Current statistical machine translation sys-
tems usually extract rules from bilingual
corpora annotated with 1-best alignments.
They are prone to learn noisy rules due
to alignment mistakes. We propose a new
structure called weighted alignment matrix
to encode all possible alignments for a par-
allel text compactly. The key idea is to as-
sign a probability to each word pair to in-
dicate how well they are aligned. We de-
sign new algorithms for extracting phrase
pairs from weighted alignment matrices
and estimating their probabilities. Our ex-
periments on multiple language pairs show
that using weighted matrices achieves con-
sistent improvements over using n-best
lists in significant less extraction time.
1 Introduction
Statistical machine translation (SMT) relies heav-
ily on annotated bilingual corpora. Word align-
ment, which indicates the correspondence be-
tween the words in a parallel text, is one of the
most important annotations in SMT. Word-aligned
corpora have been found to be an excellent source
for translation-related knowledge, not only for
phrase-based models (Och and Ney, 2004; Koehn
et al, 2003), but also for syntax-based models
(e.g., (Chiang, 2007; Galley et al, 2006; Shen
et al, 2008; Liu et al, 2006)). Och and Ney
(2003) indicate that the quality of machine transla-
tion output depends directly on the quality of ini-
tial word alignment.
Modern alignment methods can be divided into
two major categories: generative methods and dis-
criminative methods. Generative methods (Brown
et al, 1993; Vogel and Ney, 1996) treat word
alignment as a hidden process and maximize the
likelihood of bilingual training corpus using the
expectation maximization (EM) algorithm. In
contrast, discriminative methods (e.g., (Moore et
al., 2006; Taskar et al, 2005; Liu et al, 2005;
Blunsom and Cohn, 2006)) have the freedom to
define arbitrary feature functions that describe var-
ious characteristics of an alignment. They usu-
ally optimize feature weights on manually-aligned
data. While discriminative methods show supe-
rior alignment accuracy in benchmarks, genera-
tive methods are still widely used to produce word
alignments for large sentence-aligned corpora.
However, neither generative nor discriminative
alignment methods are reliable enough to yield
high quality alignments for SMT, especially for
distantly-related language pairs such as Chinese-
English and Arabic-English. The F-measures for
Chinese-English and Arabic-English are usually
around 80% (Liu et al, 2005) and 70% (Fraser
and Marcu, 2007), respectively. As most current
SMT systems only use 1-best alignments for ex-
tracting rules, alignment errors might impair trans-
lation quality.
Recently, several studies have shown that offer-
ing more alternatives of annotations to SMT sys-
tems will result in significant improvements, such
as replacing 1-best trees with packed forests (Mi
et al, 2008) and replacing 1-best word segmenta-
tions with word lattices (Dyer et al, 2008). Sim-
ilarly, Venugopal et al (2008) use n-best align-
ments instead of 1-best alignments for translation
rule extraction. While they achieve significant im-
provements on the IWSLT data, extracting rules
from n-best alignments might be computationally
expensive.
In this paper, we propose a new structure named
weighted alignment matrix to represent the align-
ment distribution for a sentence pair compactly. In
a weighted matrix, each element that corresponds
to a word pair is assigned a probability to measure
the confidence of aligning the two words. There-
fore, a weighted matrix is capable of using a lin-
1017
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
Figure 1: An example of word alignment between
a pair of Chinese and English sentences.
ear space to encode the probabilities of exponen-
tially many alignments. We develop a new algo-
rithm for extracting phrase pairs from weighted
matrices and show how to estimate their relative
frequencies and lexical weights. Experimental re-
sults show that using weighted matrices achieves
consistent improvements in translation quality and
significant reduction in extraction time over using
n-best lists.
2 Background
Figure 1 shows an example of word alignment be-
tween a pair of Chinese and English sentences.
The Chinese and English words are listed horizon-
tally and vertically, respectively. The dark points
indicate the correspondence between the words in
two languages. For example, the first Chinese
word ?zhongguo? is aligned to the fourth English
word ?China?.
Formally, given a source sentence f = fJ
1
=
f
1
, . . . , f
j
, . . . , f
J
and a target sentence e = eI
1
=
e
1
, . . . , e
i
, . . . , e
I
, we define a link l = (j, i) to
exist if f
j
and e
i
are translation (or part of trans-
lation) of one another. Then, an alignment a is a
subset of the Cartesian product of word positions:
a ? {(j, i) : j = 1, . . . , J ; i = 1, . . . , I} (1)
Usually, SMT systems only use the 1-best align-
ments for extracting translation rules. For exam-
ple, given a source phrase ?f and a target phrase
e?, the phrase pair ( ?f , e?) is said to be consistent
(Och and Ney, 2004) with the alignment if and
only if: (1) there must be at least one word in-
side one phrase aligned to a word inside the other
phrase and (2) no words inside one phrase can be
aligned to a word outside the other phrase.
After all phrase pairs are extracted from the
training corpus, their translation probabilities can
be estimated as relative frequencies (Och and Ney,
2004):
?(e?|
?
f) =
count(
?
f, e?)
?
e?
?
count(
?
f , e?
?
)
(2)
where count( ?f , e?) indicates how often the phrase
pair ( ?f, e?) occurs in the training corpus.
Besides relative frequencies, lexical weights
(Koehn et al, 2003) are widely used to estimate
how well the words in ?f translate the words in
e?. To do this, one needs first to estimate a lexi-
cal translation probability distribution w(e|f) by
relative frequency from the same word alignments
in the training corpus:
w(e|f) =
count(f, e)
?
e
?
count(f, e
?
)
(3)
Note that a special source NULL token is added
to each source sentence and aligned to each un-
aligned target word.
As the alignment a? between a phrase pair ( ?f, e?)
is retained during extraction, the lexical weight
can be calculated as
p
w
(e?|
?
f, a?) =
|e?|
?
i=1
1
|{j|(j, i) ? a?}|
?
w(e
i
|f
j
) (4)
If there are multiple alignments a? for a phrase
pair ( ?f , e?), Koehn et al (2003) choose the one
with the highest lexical weight:
p
w
(e?|
?
f) = max
a?
{
p
w
(e?|
?
f, a?)
}
(5)
Simple and effective, relative frequencies and
lexical weights have become the standard features
in modern discriminative SMT systems.
3 Weighted Alignment Matrix
We believe that offering more candidate align-
ments to extracting translation rules might help
improve translation quality. Instead of using n-
best lists (Venugopal et al, 2008), we propose a
new structure called weighted alignment matrix.
We use an example to illustrate our idea. Fig-
ure 2(a) and Figure 2(b) show two alignments of
a Chinese-English sentence pair. We observe that
some links (e.g., (1,4) corresponding to the word
1018
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(a) (b) (c)
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c)
the resulting weighted alignment matrix that takes the two alignments as samples, of which the initial
probabilities are 0.6 and 0.4, respectively.
pair (?zhongguo?, ?China?)) occur in both align-
ments, some links (e.g., (2,3) corresponding to the
word pair (?de?,?of?)) occur only in one align-
ment, and some links (e.g., (1,1) corresponding
to the word pair (?zhongguo?, ?the?)) do not oc-
cur. Intuitively, we can estimate how well two
words are aligned by calculating its relative fre-
quency, which is the probability sum of align-
ments in which the link occurs divided by the
probability sum of all possible alignments. Sup-
pose that the probabilities of the two alignments in
Figures 2(a) and 2(b) are 0.6 and 0.4, respectively.
We can estimate the relative frequencies for every
word pair and obtain a weighted matrix shown in
Figure 2(c). Therefore, each word pair is associ-
ated with a probability to indicate how well they
are aligned. For example, in Figure 2(c), we say
that the word pair (?zhongguo?, ?China?) is def-
initely aligned, (?zhongguo?, ?the?) is definitely
unaligned, and (?de?, ?of?) has a 60% chance to
get algned.
Formally, a weighted alignment matrix m is a
J ? I matrix, in which each element stores a link
probability p
m
(j, i) to indicate how well f
j
and
e
i
are aligned. Currently, we estimate link proba-
bilities from an n-best list by calculating relative
frequencies:
p
m
(j, i) =
?
a?N
p(a)? ?(a, j, i)
?
a?N
p(a)
(6)
=
?
a?N
p(a)? ?(a, j, i) (7)
where
?(a, j, i) =
{
1 (j, i) ? a
0 otherwise (8)
Note that N is an n-best list, p(a) is the probabil-
ity of an alignment a in the n-best list, ?(a, j, i)
indicates whether a link (j, i) occurs in the align-
ment a or not. We assign 0 to any unseen
alignment. As p(a) is usually normalized (i.e.,
?
a?N
p(a) ? 1), we remove the denominator in
Eq. (6).
Accordingly, the probability that the two words
f
j
and e
i
are not aligned is
p?
m
(j, i) = 1.0? p
m
(j, i) (9)
For example, as shown in Figure 2(c), the prob-
ability for the two words ?de? and ?of? being
aligned is 0.6 and the probability that they are not
aligned is 0.4.
Intuitively, the probability of an alignment a is
the product of link probabilities. If a link (j, i)
occurs in a, we use p
m
(j, i); otherwise we use
p?
m
(j, i). Formally, given a weighted alignment
matrix m, the probability of an alignment a can
be calculated as
p
m
(a) =
J
?
j=1
I
?
i=1
(p
m
(j, i) ? ?(a, j, i) +
p?
m
(j, i) ? (1? ?(a, j, i))) (10)
It proves that the sum of all alignment proba-
bilities is always 1:
?
a?A
p
m
(a) ? 1, where A
1019
1: procedure PHRASEEXTRACT(fJ
1
, e
I
1
, m, l)
2: R ? ?
3: for j
1
? 1 . . . J do
4: j
2
? j
1
5: while j
2
< J ? j
2
? j
1
< l do
6: T ? {i|?j : j
1
? j ? j
2
? p
m
(j, i) > 0}
7: i
l
? MIN(T )
8: i
u
? MAX(T )
9: for n? 1 . . . l do
10: for i
1
? i
l
? n + 1 . . . i
u
do
11: i
2
? i
1
+ n? 1
12: R ? R? {(f j2
j
1
, e
i
2
i
1
)}
13: end for
14: end for
15: j
2
? j
2
+ 1
16: end while
17: end for
18: returnR
19: end procedure
Figure 3: Algorithm for extracting phrase pairs
from a sentence pair ?fJ
1
, e
I
1
? annotated with a
weighted alignment matrix m.
is the set of all possible alignments. Therefore, a
weighted alignment matrix is capable of encoding
the probabilities of 2J?I alignments using only a
J ? I space.
Note that p
m
(a) is not necessarily equal to p(a)
because the encoding of a weighted alignment ma-
trix changes the alignment probability distribu-
tion. For example, while the initial probability of
the alignment in Figure 2(a) (i.e., p(a)) is 0.6, the
probability of the same alignment encoded in the
matrix shown in Figure 2(c) (i.e., p
m
(a)) becomes
0.1296 according to Eq. (10). It should be em-
phasized that a weighted matrix encodes all pos-
sible alignments rather than the input n-best list,
although the link probabilities are estimated from
the n-best list.
4 Phrase Pair Extraction
In this section, we describe how to extract phrase
pairs from the training corpus annotated with
weighted alignment matrices (Section 4.1) and
how to estimate their relative frequencies (Section
4.2) and lexical weights (Section 4.3).
4.1 Extraction Algorithm
Och and Ney (2004) describe a ?phrase-extract?
algorithm for extracting phrase pairs from a sen-
tence pair annotated with a 1-best alignment.
Given a source phrase, they first identify the target
phrase that is consistent with the alignment. Then,
they expand the boundaries of the target phrase if
the boundary words are unaligned.
Unfortunately, this algorithm cannot be directly
used to manipulate a weighted alignment matrix,
which is a compact representation of all pos-
sible alignments. The major difference is that
the ?tight? phrase that has both boundary words
aligned is not necessarily the smallest candidate
in a weighted matrix. For example, in Figure
2(a), the ?tight? target phrase corresponding to
the source phrase ?zhongguo de? is ?of China?.
According to Och?s algorithm, the target phrase
?China? breaks the alignment consistency and
therefore is not valid candidate. However, this is
not true for using the weighted matrix shown in
Figure 2(c). The target phrase ?China? is treated
as a ?potential? candidate 1, although it might be
assigned only a small fractional count (see Table
1).
Therefore, we enumerate all potential phrase
pairs and calculate their fractional counts for
eliminating less promising candidates. Figure 3
shows the algorithm for extracting phrases from
a weighted matrix. The input of the algorithm
is a source sentence fJ
1
, a target sentence eI
1
, a
weighted alignment matrix m, and a phrase length
limit l (line 1). After initializing R that stores col-
lected phrase pairs (line 2), we identify the cor-
responding target phrases for all possible source
phrases (lines 3-5). Given a source phrase f j2
j
1
, we
find the lower and upper bounds of target positions
(i.e., i
l
and i
u
) that have positive link probabili-
ties (lines 6-8). For example, the lower bound is
3 and the upper bound is 5 for the source phrase
?zhongguo de? in Figure 2(c). Finally, we enu-
merate all target phrases that allow for unaligned
boundary words with varying phrase lengths (lines
9-14). Note that we need to ensure that 1 ? i
1
? I
and 1 ? i
2
? I in lines 10-11, which are omitted
for simplicity.
4.2 Calculating Relative Frequencies
To estimate the relative frequency of a phrase pair,
we need to estimate how often it occurs in the
training corpus. Given an n-best list, the fractional
count of a phrase pair is the probability sum of
the alignments with which the phrase pair is con-
sistent. Obviously, it is unrealistic for a weighted
alignment matrix to enumerate all possible align-
ments explicitly to calculate fractional counts. In-
stead, we resort to link probabilities to calculate
1By potential, we mean that the fractional count of a
phrase pair is positive. Section 4.2 describes how to calcu-
late fractional counts.
1020
the
development
of
China
?s
economy
z
h
o
n
g
g
u
o
d
e
j
i
n
g
j
i
f
a
z
h
a
n
1.0
0.6
0.40.4
1.0
1.0
0.4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 4: An example of calculating fractional
count. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
counts efficiently. Equivalent to explicit enumera-
tion, we interpret the fractional count of a phrase
pair as the probability that it satisfies the two align-
ment consistency conditions (see Section 2).
Given a phrase pair, we divide the elements of
a weighted alignment matrix into three categories:
(1) inside elements that fall inside the phrase pair,
(2) outside elements that fall outside the phrase
pair while fall in the same row or the same col-
umn, and (3) irrelevant elements that fall outside
the phrase pair while fall in neither the same row
nor the same column. Figure 4 shows an exam-
ple. Given the phrase pair (?zhongguo de?, ?of
China?), we divide the matrix into three areas: in-
side (heavy shading), outside (light shading), and
irrelevant (no shading).
To what extent a phrase pair satisfies the align-
ment consistency is measured by calculating in-
side and outside probabilities. Although there are
the same terms in the parsing literature, they have
different meanings here. The inside probability in-
dicates the chance that there is at least one word
inside one phrase aligned to a word inside the
other phrase. The outside probability indicates the
chance that no words inside one phrase are aligned
to a word outside the other phrase.
Given a phrase pair (f j2
j
1
, e
i
2
i
1
), we denote the in-
side area as in(j
1
, j
2
, i
1
, i
2
) and the outside area
as out(j
1
, j
2
, i
1
, i
2
). Therefore, the inside proba-
bility of a phrase pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) = 1?
?
(j,i)?in(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (11)
target phrase ? ? count
of China 1.0 0.36 0.36
of China ?s 1.0 0.36 0.36
China ?s 1.0 0.24 0.24
China 1.0 0.24 0.24
?s economy 0.4 0 0
Table 1: Some candidate target phrases of the
source phrase ?zhongguo de? in Figure 4, where ?
is inside probability, ? is outside probability, and
count is fractional count.
For example, the inside probability for (?zhong-
guo de?, ?of China?) in Figure 4 is 1.0, which
means that there always exists at least one aligned
word pair inside.
Accordingly, the outside probability of a phrase
pair is calculated as
?(j
1
, j
2
, i
1
, i
2
) =
?
(j,i)?out(j
1
,j
2
,i
1
,i
2
)
p?
m
(j, i) (12)
For example, the outside probability for
(?zhongguo de?, ?of China?) in Figure 4 is 0.36,
which means the probability that there are no
aligned word pairs outside is 0.36.
Finally, we use the product of inside and outside
probabilities as the fractional count of a phrase
pair:
count(f
j
2
j
1
, e
i
2
i
1
) = ?(j
1
, j
2
, i
1
, i
2
)?
?(j
1
, j
2
, i
1
, i
2
) (13)
Table 1 lists some candidate target phrases of
the source phrase ?zhongguo de? in Figure 4. We
also give their inside probabilities, outside proba-
bilities, and fractional counts.
After collecting the fractional counts from the
training corpus, we then use Eq. (2) to calculate
relative frequencies in two translation directions.
Often, our approach extracts a large amount of
phrase pairs from training corpus as we soften
the alignment consistency constraint. To main-
tain a reasonable phrase table size, we discard any
phrase pair that has a fractional count lower than
a threshold t. During extraction, we first obtain
a list of candidate target phrases for each source
phrase, as shown in Table 1. Then, we prune the
list according to the threshold t. For example, we
only retain the top two candidates in Table 1 if
t = 0.3. Note that we perform the pruning locally.
Although it is more reasonable to prune a phrase
table after accumulating all fractional counts from
1021
training corpus, such global pruning strategy usu-
ally leads to very large disk and memory require-
ments.
4.3 Calculating Lexical Weights
Recall that we need to obtain two translation prob-
ability tables w(e|f) and w(f |e) before calculat-
ing lexical weights (see Section 2). Following
Koehn et al (2003), we estimate the two distribu-
tions by relative frequencies from the training cor-
pus annotated with weighted alignment matrices.
In other words, we still use Eq. (3) but the way of
calculating fractional counts is different now.
Given a source word f
j
, a target word e
i
, and
a weighted alignment matrix, the fractional count
count(f
j
, e
i
) is p
m
(j, i). For NULL words, the
fractional counts can be calculated as
count(f
j
, e
0
) =
I
?
i=1
p?
m
(j, i) (14)
count(f
0
, e
i
) =
J
?
j=1
p?
m
(j, i) (15)
For example, in Figure 4, count(de, of) is 0.6,
count(de,NULL) is 0.24, and count(NULL,of) is
0.24.
Then, we adapt Eq. (4) to calculate lexical
weight:
p
w
(e?|
?
f ,m) =
|e?|
?
i=1
(
(
1
{j|p
m
(j, i) > 0}
?
?
?j:p
m
(j,i)>0
p(e
i
|f
j
)? p
m
(j, i)
)
+
p(e
i
|f
0
)?
|
?
f |
?
j=1
p?
m
(j, i)
)
(16)
For example, for the target word ?of? in Figure
4, the sum of aligned and unaligned probabilities
is
1
2
? (p(of|de)? 0.6 + p(of|fazhan)? 0.4) +
p(of|NULL)? 0.24
Note that we take link probabilities into account
and calculate the probability that a target word
translates a source NULL token explicitly.
5 Experiments
5.1 Data Preparation
We evaluated our approach on Chinese-to-English
translation. We used the FBIS corpus (6.9M
+ 8.9M words) as the training data. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT evaluation test
set as our development set, and used the NIST
2005 test set as our test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
To obtain weighted alignment matrices, we fol-
lowed Venugopal et al (2008) to produce n-
best lists via GIZA++. We first ran GIZA++
to produce 50-best lists in two translation direc-
tions. Then, we used the refinement technique
?grow-diag-final-and? (Koehn et al, 2003) to all
50 ? 50 bidirectional alignment pairs. Suppose
that p
s2t
and p
t2s
are the probabilities of an align-
ment pair assigned by GIZA++, respectively. We
used p
s2t
? p
t2s
as the probability of the result-
ing symmetric alignment. As different alignment
pairs might produce the same symmetric align-
ments, we followed Venugopal et al (2008) to
remove duplicate alignments and retain only the
alignment with the highest probability. Therefore,
there were 550 candidate alignments on average
for each sentence pair in the training data. We
obtained n-best lists by selecting the top n align-
ments from the 550-best lists. The probability of
each alignment in the n-best list was re-estimated
by re-normalization (Venugopal et al, 2008). Fi-
nally, these n-best alignments served as samples
for constructing weighted alignment matrices.
After extracting phrase pairs from n-best lists
and weighted alignment matrices, we ran Moses
(Koehn et al, 2007) to translate the development
and test sets. We used the simple distance-based
reordering model to remove the dependency of
lexicalization on word alignments for Moses.
5.2 Effect of Pruning Threshold
Our first experiment investigated the effect of
pruning threshold on translation quality (BLEU
scores on the test set) and the phrase table size (fil-
tered for the test set), as shown in Figure 5. To
save time, we extracted phrase pairs just from the
first 10K sentence pairs of the FBIS corpus. We
used 12 different thresholds: 0.0001, 0.001, 0.01,
0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9. Obvi-
ously, the lower the threshold is, the more phrase
pairs are extracted. When t = 0.0001, the number
of phrase pairs used on the test set was 460,284
1022
0.195
0.196
0.197
0.198
0.199
0.200
0.201
0.202
0.203
0.204
0.205
0.206
0.207
0.208
 150  200  250  300  350  400  450  500
BL
EU
 sc
or
e
phrase table size (103)
t=10-4
t=10-3
t=10-2
t=0.9...0.1
Figure 5: Effect of pruning threshold on transla-
tion quality and phrase table size.
and the BLEU score was 20.55. Generally, both
the number of phrase pairs and the BLEU score
went down with the increase of t. However, this
trend did not hold within the range [0.1, 0.9]. To
achieve a good tradeoff between translation qual-
ity and phrase table size, we set t = 0.01 for the
following experiments.
5.3 N -best lists Vs. Weighted Matrices
Figure 6 shows the BLEU scores and aver-
age extraction time using n-best alignments and
weighted matrices, respectively. We used the en-
tire training data for phrase extraction. When us-
ing 1-best alignments, Moses achieved a BLEU
score of 0.2826 and the average extraction time
was 4.19 milliseconds per sentence pair (see point
n = 1). The BLEU scores rose with the in-
crease of n for using n-best alignments. How-
ever, the score went down slightly when n = 50.
This suggests that including more noisy align-
ments might be harmful. These improvements
over 1-best alignments are not statistically signif-
icant. This finding failed to echo the promising
results reported by Venogopal et al (2008). We
think that there are two possible reasons. First,
they evaluated their approach on the IWSLT data
while we used the NIST data. It might be easier
to obtain significant improvements on the IWSLT
data in which the sentences are shorter. Sec-
ond, they used the hierarchical phrase-based sys-
tem while we used the phrase-based system, which
might be less sensitive to word alignments because
the alignments inside the phrase pairs hardly have
an effect.
When using weighted alignment matrices, we
0.280
0.281
0.282
0.283
0.284
0.285
0.286
0.287
0.288
0.289
0.290
0.291
0.292
0.293
 0  10  20  30  40  50  60  70  80  90
BL
EU
 sc
or
e
average extracting time (milliseconds/sentence pair)
n=1
n=5
n=10
n=50
n=5
n=10
n=50
n-best
m(n)
Figure 6: Comparison of n-best alignments and
weighted alignment matrices. We use m(n) to de-
note the matrices that take n-best lists as samples.
obtained higher BLEU scores than using n-best
lists with much less extraction time. We achieved
a BLEU score of 0.2901 when using the weighted
matrices estimated from 10-best lists. The abso-
lute improvement of 0.75 over using 1-best align-
ments (from 0.2826 to 0.2901) is statistically sig-
nificant at p < 0.05 by using sign-test (Collins
et al, 2005). Although the improvements over n-
best lists are not always statistically significant,
weighted alignment matrices maintain consistent
superiority in both translation quality and extrac-
tion speed.
5.4 Comparison of Parameter Estimation
In theory, the set of phrase pairs extracted from n-
best alignments is the subset of the set extracted
from the corresponding weighted matrices. In
practice, however, this is not true because we use
the pruning threshold t to maintain a reasonable
table size. Even so, the phrase tables produced by
n-best lists and weighted matrices still share many
phrase pairs.
Table 2 gives some statistics. We use m(10)
to represent the weighted matrices estimated from
10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables,
and ?non-shared? denotes the complement. Note
that the probabilities of ?shared? phrase pairs are
different for the two approaches. We obtained
6.13M and 6.34M phrase pairs for the test set by
using 10-best lists and the corresponding matrices,
respectively. There were 4.58M phrase pairs in-
cluded by both tables. Note that the relative fre-
quencies and lexical weights for the same phrase
1023
shared non-shared all
method phrases BLEU phrases BLEU phrases BLEU
10-best 4.58M 28.35 1.55M 12.32 6.13M 28.47
m(10) 4.58M 28.90 1.76M 13.21 6.34M 29.01
Table 2: Comparison of phrase tables learned from n-best lists and weighted matrices. We use m(10)
to represent the weighted matrices estimated from 10-best lists. ?all? denotes the full phrase table,
?shared? denotes the intersection of two tables, and ?non-shared? denotes the complement. Note that the
probabilities of ?shared? phrase pairs are different for the two approaches.
0.200
0.210
0.220
0.230
0.240
0.250
0.260
0.270
0.280
0.290
 0  50  100  150  200  250
BL
EU
 sc
or
e
training corpus size (103)
1-best
10-best
m(10)
Figure 7: Comparison of n-best alignments and
weighted alignment matrices with varying training
corpus sizes.
pairs might be different in two tables. We found
that using matrices outperformed using n-best lists
even with the same phrase pairs. This suggests that
our methods for parameter estimation make better
use of noisy data. Another interesting finding was
that using the shared phrase pairs achieved almost
the same results with using full phrase tables.
5.5 Effect of Training Corpus Size
To investigate the effect of training corpus size on
our approach, we extracted phrase pairs from n-
best lists and weighted matrices trained on five
training corpora with varying sizes: 10K, 50K,
100K, 150K, and 239K sentence pairs. As shown
in Figure 7, our approach outperformed both 1-
best and n-best lists consistently. More impor-
tantly, the gains seem increase when more training
data are used.
5.6 Results on Other Language Pairs
To further examine the efficacy of the proposed ap-
proach, we scaled our experiments to large data
with multiple language pairs. We used the Eu-
roparl training corpus from the WMT07 shared
S?E F?E G?E
Sentences 1.26M 1.29M 1.26M
Foreign words 33.16M 33.18M 29.58M
English words 31.81M 32.62M 31.93M
Table 3: Statistics of the Europarl training data.
?S? denotes Spanish, ?E? denotes English, ?F? de-
notes French, ?G? denotes German.
1-best 10-best m(10)
S?E 30.90 30.97 31.03
E?S 31.16 31.25 31.34
F?E 30.69 30.76 30.82
E?F 26.42 26.65 26.54
G?E 24.46 24.58 24.66
E?G 18.03 18.30 18.20
Table 4: BLEU scores (case-insensitive) on the
Europarl data. ?S? denotes Spanish, ?E? denotes
English, ?F? denotes French, ?G? denotes Ger-
man.
task. 2 Table 3 shows the statistics of the train-
ing data. There are four languages (Spanish,
French, German, and English) and six transla-
tion directions (Foreign-to-English and English-
to-Foreign). We used the ?dev2006? data in the
?dev? directory as the development set and the
?test2006? data in the ?devtest? directory as the
test set. Both the development and test sets contain
2,000 sentences with single reference translations.
We tokenized and lowercased all the training,
development, and test data. We trained a 4-gram
language model using SRI Language Modeling
Toolkit on the target side of the training corpus for
each task. We ran GIZA++ on the entire train-
ing data to obtain n-best alignments and weighted
matrices. To save time, we just used the first 100K
sentences of each aligned training corpus to ex-
tract phrase pairs.
2http://www.statmt.org/wmt07/shared-task.html
1024
Table 4 lists the case-insensitive BLEU scores
of 1-best, 10-best, and m(10) on the Europarl
data. Using weighted packed matrices continued
to show advantage over using 1-best alignments on
multiple language pairs. However, these improve-
ments were very small and not significant. We at-
tribute this to the fact that GIZA++ usually pro-
duces high quality 1-best alignments for closely-
related European language pairs, especially when
trained on millions of sentences.
6 Related Work
Recent studies has shown that SMT systems
can benefit from making the annotation pipeline
wider: using packed forests instead of 1-best trees
(Mi et al, 2008), word lattices instead of 1-best
segmentations (Dyer et al, 2008), and n-best
alignments instead of 1-best alignments (Venu-
gopal et al, 2008). We propose a compact repre-
sentation of multiple word alignments that enables
SMT systems to make a better use of noisy align-
ments.
Matusov et al (2004) propose ?cost matrices?
for producing symmetric alignments. Kumar et al
(2007) describe how to use ?posterior probabil-
ity matrices? to improve alignment accuracy via
a bridge language. Although not using the term
?weighted matrices? directly, they both assign a
probability to each word pair.
We follow Och and Ney (2004) to develop
a new phrase extraction algorithm for weighted
alignment matrices. The methods for calculating
relative frequencies (Och and Ney, 2004) and lex-
ical weights (Koehn et al, 2003) are also adapted
for the weighted matrix case.
Many researchers (e.g., (Venugopal et al, 2003;
Deng et al, 2008)) observe that softening the
alignment consistency constraint help improve
translation quality. For example, Deng et al
(2008) define a feature named ?within phrase pair
consistency ratio? to measure the degree of consis-
tency. As each link is associated with a probability
in a weighted matrix, we use these probabilities to
evaluate the validity of a phrase pair.
We estimate the link probabilities by calculating
relative frequencies over n-best lists. Niehues and
Vogel (2008) propose a discriminative approach to
modeling the alignment matrix directly. The dif-
ference is that they assign a boolean value instead
of a probability to each word pair.
7 Conclusion and Future Work
We have presented a new structure called weighted
alignment matrix that encodes the alignment dis-
tribution for a sentence pair. Accordingly, we de-
velop new methods for extracting phrase pairs and
estimating their probabilities. Our experiments
show that the proposed approach achieves better
translation quality over using n-best lists in less
extraction time. An interesting finding is that our
approach performs better than the baseline even
they use the same phrase pairs.
Although our approach consistently outper-
forms using 1-best alignments for varying lan-
guage pairs, the improvements are comparatively
small. One possible reason is that taking n-best
lists as samples sometimes might change align-
ment probability distributions inappropriately. A
more principled solution is to directly model the
weighted alignment matrices, either in a genera-
tive or a discriminative way. We believe that better
estimation of alignment distributions will result in
more significant improvements.
Another interesting direction is applying our ap-
proach to extracting translation rules with hierar-
chical structures such as hierarchical phrases (Chi-
ang, 2007) and tree-to-string rules (Galley et al,
2006; Liu et al, 2006). We expect that these
syntax-based systems could benefit more from our
approach.
Acknowledgement
The authors were supported by Microsoft Re-
search Asia Natural Language Processing Theme
Program grant (2009-2010), High-Technology
R&D Program (863) Project No. 2006AA010108,
and National Natural Science Foundation of China
Contract 60736014. Part of this work was done
while Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Stephan Vogel, Alon Lavie, Fran-
cisco Guzman, Nguyen Bach, Andreas Zollmann,
Vamshi Ambati, and Kevin Gimpel for their help-
ful feedback.
References
Phil Blunsom and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of COLING/ACL 2006, pages 65?72,
Sydney, Australia, July.
1025
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL 2005, pages
531?540, Ann Arbor, USA, June.
Yonggang Deng, Jia Xu, and Yuqing Gao. 2008.
Phrase table training for precision and recall: What
makes a good phrase and a good phrase pair?
In Proceedings of ACL/HLT 2008, pages 81?88,
Columbus, Ohio, USA, June.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of ACL/HLT 2008, pages
1012?1020, Columbus, Ohio, June.
Alexander Fraser and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Computational Linguistics, Squibs and
Discussions, 33(3):293?303.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING/ACL 2006, pages 961?968,
Sydney, Australia, July.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL 2007 (poster), pages
77?80, Prague, Czech Republic, June.
Shankar Kumar, Franz J. Och, and Wolfgang
Macherey. 2007. Improving word alignment with
bridge languages. In Proceedings of EMNLP 2007,
pages 42?50, Prague, Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of ACL 2005, pages 459?466, Ann Arbor, Michigan,
June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING/ACL 2006,
pages 609?616, Sydney, Australia, July.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING
2004, pages 219?225, Geneva, Switzerland, August.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, June.
Robert C. Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In Proceedings of COLING/ACL 2006,
pages 513?520, Sydney, Australia, July.
Jan Niehues and Stephan Vogel. 2008. Discrimina-
tive word alignment via alignment matrix modeling.
In Proceedings of WMT-3, pages 18?25, Columbus,
Ohio, USA, June.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL 2002, pages 311?318, Philadelphia, Penn-
sylvania, USA, July.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL/HLT 2008, pages 577?585,
Columbus, Ohio, June.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proceedings of ICSLP
2002, pages 901?904, Denver, Colorado, Septem-
ber.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of HLT/EMNLP 2005,
pages 73?80, Vancouver, British Columbia, Canada,
October.
Ashish Venugopal, Stephan Vogel, and Alex Waibel.
2003. Effective phrase translation extraction from
alignment models. In Proceedings of ACL 2003,
pages 319?326, Sapporo, Japan, July.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-
best alignments and parses in mt training. In Pro-
ceedings of AMTA 2008, pages 192?201, Waikiki,
Hawaii, October.
Stephan Vogel and Hermann Ney. 1996. Hmm-based
word alignment in statistical translation. In Pro-
ceedings of COLING 1996, pages 836?841, Copen-
hagen, Danmark, August.
1026
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1105?1113,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Lattice-based System Combination for Statistical Machine Translation
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{fengyang, yliu, htmi, liuqun, lvyajuan}@ict.ac.cn
Abstract
Current system combination methods usu-
ally use confusion networks to find consensus
translations among different systems. Requir-
ing one-to-one mappings between the words
in candidate translations, confusion networks
have difficulty in handling more general situa-
tions in which several words are connected to
another several words. Instead, we propose a
lattice-based system combination model that
allows for such phrase alignments and uses
lattices to encode all candidate translations.
Experiments show that our approach achieves
significant improvements over the state-of-
the-art baseline system on Chinese-to-English
translation test sets.
1 Introduction
System combination aims to find consensus transla-
tions among different machine translation systems.
It has been proven that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
In recent several years, the system combination
methods based on confusion networks developed
rapidly (Bangalore et al, 2001; Matusov et al, 2006;
Sim et al, 2007; Rosti et al, 2007a; Rosti et al,
2007b; Rosti et al, 2008; He et al, 2008), which
show state-of-the-art performance in benchmarks. A
confusion network consists of a sequence of sets of
candidate words. Each candidate word is associated
with a score. The optimal consensus translation can
be obtained by selecting one word from each set to
maximizing the overall score.
To construct a confusion network, one first need
to choose one of the hypotheses (i.e., candidate
translations) as the backbone (also called ?skeleton?
in the literature) and then decide the word align-
ments of other hypotheses to the backbone. Hy-
pothesis alignment plays a crucial role in confusion-
network-based system combination because it has a
direct effect on selecting consensus translations.
However, a confusion network is restricted in
such a way that only 1-to-1 mappings are allowed
in hypothesis alignment. This is not the fact even
for word alignments between the same languages. It
is more common that several words are connected
to another several words. For example, ?be capa-
ble of? and ?be able to? have the same meaning.
Although confusion-network-based approaches re-
sort to inserting null words to alleviate this problem,
they face the risk of producing degenerate transla-
tions such as ?be capable to? and ?be able of?.
In this paper, we propose a new system combina-
tion method based on lattices. As a more general
form of confusion network, a lattice is capable of
describing arbitrary mappings in hypothesis align-
ment. In a lattice, each edge is associated with a
sequence of words rather than a single word. There-
fore, we select phrases instead of words in each
candidate set and minimize the chance to produce
unexpected translations such as ?be capable to?.
We compared our approach with the state-of-the-art
confusion-network-based system (He et al, 2008)
and achieved a significant absolute improvement of
1.23 BLEU points on the NIST 2005 Chinese-to-
English test set and 0.93 BLEU point on the NIST
2008 Chinese-to-English test set.
1105
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(a) unidirectional alignments
He feels like apples
He prefer apples
He feels like apples
He is fond of apples
(b) bidirectional alignments
He feels like ? apples
? prefer of
is fond
(c) confusion network
he feels like apples
? prefer
is fond of
(d) lattice
Figure 1: Comparison of a confusion network and a lat-
tice.
2 Background
2.1 Confusion Network and Lattice
We use an example shown in Figure 1 to illustrate
our idea. Suppose that there are three hypotheses:
He feels like apples
He prefer apples
He is fond of apples
We choose the first sentence as the backbone.
Then, we perform hypothesis alignment to build a
confusion network, as shown in Figure 1(a). Note
that although ?feels like? has the same meaning with
?is fond of?, a confusion network only allows for
one-to-one mappings. In the confusion network
shown in Figure 1(c), several null words ? are in-
serted to ensure that each hypothesis has the same
length. As each edge in the confusion network only
has a single word, it is possible to produce inappro-
priate translations such as ?He is like of apples?.
In contrast, we allow many-to-many mappings
in the hypothesis alignment shown in Figure 2(b).
For example, ?like? is aligned to three words: ?is?,
?fond?, and ?of?. Then, we use a lattice shown in
Figure 1(d) to represent all possible candidate trans-
lations. Note that the phrase ?is fond of? is attached
to an edge. Now, it is unlikely to obtain a translation
like ?He is like of apples?.
A lattice G = ?V,E? is a directed acyclic graph,
formally a weighted finite state automation (FSA),
where V is the set of nodes and E is the set of edges.
The nodes in a lattice are usually labeled according
to an appropriate numbering to reflect how to pro-
duce a translation. Each edge in a lattice is attached
with a sequence of words as well as the associated
probability.
As lattice is a more general form of confusion
network (Dyer et al, 2008), we expect that replac-
ing confusion networks with lattices will further im-
prove system combination.
2.2 IHMM-based Alignment Method
Since the candidate hypotheses are aligned us-
ing Indirect-HMM-based (IHMM-based) alignment
method (He et al, 2008) in both direction, we briefly
review the IHMM-based alignment method first.
Take the direction that the hypothesis is aligned to
the backbone as an example. The conditional prob-
ability that the hypothesis is generated by the back-
bone is given by
p(e
?
1
J
|e
I
1
) =
?
a
J
1
J
?
j=1
[p(a
j
|a
j?1
, I)p(e
?
j
|e
a
j
)]l (1)
Where eI
1
= (e
1
, ..., e
I
) is the backbone, e?J
1
=
(e
?
1
, ..., e
?
J
) is a hypothesis aligned to eI
1
, and aJ
1
=
(a
1
, .., a
J
) is the alignment that specifies the posi-
tion of backbone word that each hypothesis word is
aligned to.
The translation probability p(e?
j
|e
i
) is a linear in-
terpolation of semantic similarity p
sem
(e
?
j
|e
i
) and
surface similarity p
sur
(e
?
j
|e
i
) and ? is the interpo-
lation factor:
p(e
?
j
|e
i
) = ??p
sem
(e
?
j
|e
i
)+(1??)?p
sur
(e
?
j
|e
i
) (2)
The semantic similarity model is derived by using
the source word sequence as a hidden layer, so the
bilingual dictionary is necessary. The semantic sim-
1106
ilarity model is given by
p
sem
(e
?
j
|e
i
) =
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
, e
i
)
?
K
?
k=0
p(f
k
|e
i
)p(e
?
j
|f
k
)
(3)
The surface similarity model is estimated by calcu-
lating the literal matching rate:
p
sur
(e
?
j
|e
i
) = exp{? ? [s(e
?
j
, e
i
)? 1]} (4)
where s(e?
j
, e
i
) is given by
s(e
?
j
, e
i
) =
M(e
?
j
, e
i
)
max(|e
?
j
|, |e
i
|)
(5)
where M(e?
j
, e
i
) is the length of the longest matched
prefix (LMP) and ? is a smoothing factor that speci-
fies the mapping.
The distortion probability p(a
j
= i|a
j?1
= i
?
, I)
is estimated by only considering the jump distance:
p(i|i
?
, I) =
c(i? i
?
)
?
I
i=1
c(l ? i
?
)
(6)
The distortion parameters c(d) are grouped into 11
buckets, c(? ?4), c(?3), ..., c(0), ..., c(5), c(? 6).
Since the alignments are in the same language, the
distortion model favor monotonic alignments and
penalize non-monotonic alignments. It is given in
a intuitive way
c(d) = (1 + |d? 1|)
?K
, d = ?4, ..., 6 (7)
where K is tuned on held-out data.
Also the probability p
0
of jumping to a null word
state is tuned on held-out data. So the overall distor-
tion model becomes
p(i|i
?
, I) =
{
p
0
if i = null state
(1? p
0
) ? p(i|i
?
, I) otherwise
3 Lattice-based System Combination
Model
Lattice-based system combination involves the fol-
lowing steps:
(1) Collect the hypotheses from the candidate sys-
tems.
(2) Choose the backbone from the hypotheses.
This is performed using a sentence-level Minimum
Bayes Risk (MBR) method. The hypothesis with the
minimum cost of edits against all hypotheses is se-
lected. The backbone is significant for it influences
not only the word order, but also the following align-
ments. The backbone is selected as follows:
E
B
= argmin
E
?
?E
?
E?E
TER(E
?
, E) (8)
(3) Get the alignments of the backbone and hy-
pothesis pairs. First, each pair is aligned in both di-
rections using the IHMM-based alignment method.
In the IHMM alignment model, bilingual dictionar-
ies in both directions are indispensable. Then, we
apply a grow-diag-final algorithm which is widely
used in bilingual phrase extraction (Koehn et al,
2003) to monolingual alignments. The bidirec-
tional alignments are combined to one resorting to
the grow-diag-final algorithm, allowing n-to-n map-
pings.
(4)Normalize the alignment pairs. The word or-
der of the backbone determines the word order of
consensus outputs, so the word order of hypotheses
must be consistent with that of the backbone. All
words of a hypotheses are reordered according to
the alignment to the backbone. For a word aligned
to null, an actual null word may be inserted to the
proper position. The alignment units are extracted
first and then the hypothesis words in each unit are
shifted as a whole.
(5) Construct the lattice in the light of phrase
pairs extracted on the normalized alignment pairs.
The expression ability of the lattice depends on the
phrase pairs.
(6) Decode the lattice using a model similar to the
log-linear model.
The confusion-network-based system combina-
tion model goes in a similar way. The first two steps
are the same as the lattice-based model. The differ-
ence is that the hypothesis pairs are aligned just in
one direction due to the expression limit of the con-
fusion network. As a result, the normalized align-
ments only contain 1-to-1 mappings (Actual null
words are also needed in the case of null alignment).
In the following, we will give more details about the
steps which are different in the two models.
1107
4 Lattice Construction
Unlike a confusion network that operates words
only, a lattice allows for phrase pairs. So phrase
pairs must be extracted before constructing a lat-
tice. A major difficulty in extracting phrase pairs
is that the word order of hypotheses is not consistent
with that of the backbone. As a result, hypothesis
words belonging to a phrase pair may be discon-
tinuous. Before phrase pairs are extracted, the hy-
pothesis words should be normalized to make sure
the words in a phrase pair is continuous. We call a
phrase pair before normalization a alignment unit.
The problem mentioned above is shown in Fig-
ure 2. In Figure 2 (a), although (e?
1
e
?
3
, e
2
) should be
a phrase pair, but /e?
1
0 and /e?
3
0 are discontin-
uous, so the phrase pair can not be extracted. Only
after the words of the hypothesis are reordered ac-
cording to the corresponding words in the backbone
as shown in Figure 2 (b), /e?
1
0 and /e?
3
0 be-
come continuous and the phrase pair (e?
1
e
?
3
, e
2
) can
be extracted. The procedure of reordering is called
alignment normalization
E
h
: e?
1
e
?
2
e
?
3
E
B
:
e
1
e
2
e
3
(a)
E
h
: e?
2
e
?
1
e
?
3
E
B
:
e
1
e
2
e
3
(b)
Figure 2: An example of alignment units
4.1 Alignment Normalization
After the final alignments are generated in the grow-
diag-final algorithm, minimum alignment units are
extracted. The hypothesis words of an alignment
unit are packed as a whole in shift operations.
See the example in Figure 2 (a) first. All mini-
mum alignment units are as follows: (e?
2
, e
1
), (e?
1
e
?
3
,
e
2
) and (?, e
3
). (e?
1
e
?
2
e
?
3
, e
1
e
2
) is an alignment unit,
but not a minimum alignment unit.
Let a?
i
= (e?
?
i
, e?
i
) denote a minimum alignment
unit, and assume that the word string e??
i
covers words
e
?
i
1
,..., e
?
i
m
on the hypothesis side, and the word
string e?
i
covers the consecutive words e
i
1
,..., e
i
n
on
the backbone side. In an alignment unit, the word
string on the hypothesis side can be discontinuous.
The minimum unit a?
i
= (e?
?
i
, e?
i
) must observe the
following rules:
E
B
: e
1
e
2
e
3
E
h
:
e
?
1
e
?
2 (a)
e
1
e
2
e
3
e
?
2
?
e
?
1
E
B
: e
1
e
2
E
h
: e
?
1
e
?
2
e
?
3
e
1
e
2
e
?
1
e
?
3
e
?
1
e
?
2
e
?
3
(b)
E
B
: e
1
e
2
E
h
:
e
?
1
e
?
2
e
?
3
e
1
?
e
2
e
?
1
e
?
2
e
?
3
(c)
Figure 3: Different cases of null insertion
? ? e
?
i
k
? e?
?
i
, e
a
?
i
k
? e?
i
? ? e
i
k
? e?
i
, e
?
a
i
k
= null or e?
a
i
k
? e?
?
i
? ? a?
j
= (e?
?
j
, e?
j
), e?
j
= e
i
1
, ..., e
i
k
or e?
j
=
e
i
k
, ..., e
i
n
, k ? [1, n]
Where a?
i
k
denotes the position of the word in the
backbone that e?
i
k
is aligned to, and a
i
k
denotes the
position of the word in the hypothesis that e
i
k
is
aligned to.
An actual null word may be inserted to a proper
position if a word, either from the hypothesis or from
the backbone, is aligned to null. In this way, the
minimum alignment set is extended to an alignment
unit set, which includes not only minimum align-
ment units but also alignment units which are gener-
ated by adding null words to minimum alignment
units. In general, the following three conditions
should be taken into consideration:
? A backbone word is aligned to null. A null
word is inserted to the hypothesis as shown in
Figure 3 (a).
? A hypothesis word is aligned to null and it is
between the span of a minimum alignment unit.
A new alignment unit is generated by insert-
ing the hypothesis word aligned to null to the
minimum alignment unit. The new hypothesis
string must remain the original word order of
the hypothesis. It is illustrated in Figure 3 (b).
? A hypothesis word is aligned to null and it is
not between the hypothesis span of any mini-
mum alignment unit. In this case, a null word
1108
e1
e
2
?
e
3
e?
?
4
e?
?
5
e?
?
6
(a)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
(b)
e
1
?
e
2
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
(c)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
(d)
e
1
?
e
2
?
e
3
e?
?
1
e?
?
2
e?
?
3
e?
?
4
e?
?
5
e?
?
6
(e)
Figure 4: A toy instance of lattice construction
are inserted to the backbone. This is shown in
Figure 3 (c).
4.2 Lattice Construction Algorithm
The lattice is constructed by adding the normalized
alignment pairs incrementally. One backbone arc in
a lattice can only span one backbone word. In con-
trast, all hypothesis words in an alignment unit must
be packed into one hypothesis arc. First the lattice is
initialized with a normalized alignment pair. Then
given all other alignment pairs one by one, the lat-
tice is modified dynamically by adding the hypothe-
sis words of an alignment pair in a left-to-right fash-
ion.
A toy instance is given in Figure 4 to illustrate the
procedure of lattice construction. Assume the cur-
rent inputs are: an alignment pair as in Figure 4 (a),
and a lattice as in Figure 4 (b). The backbone words
of the alignment pair are compared to the backbone
words of the lattice one by one. The procedure is as
follows:
? e
1
is compared with e
1
. Since they are the
same, the hypothesis arc e??
4
, which comes from
the same node with e
1
in the alignment pair,
is compared with the hypothesis arc e??
1
, which
comes from the same node with e
1
in the lat-
tice. The two hypothesis arcs are not the same,
so e??
4
is added to the lattice as shown in Figure
4(c). Both go to the next backbone words.
? e
2
is compared with ?. The lattice remains the
same. The lattice goes to the next backbone
word e
2
.
? e
2
is compared with e
2
. There is no hypothesis
arc coming from the same node with the bone
arc e
2
in the alignment pair, so the lattice re-
mains the same. Both go to the next backbone
words.
? ? is compared with e
3
. A null backbone arc is
inserted into the lattice between e
2
and e
3
. The
hypothesis arc e??
5
is inserted to the lattice, too.
The modified lattice is shown in Figure 4(d).
The alignment pair goes to the next backbone
word e
3
.
? e
3
is compared with e
3
. For they are the same
and there is no hypothesis arc e??
6
in the lattice,
e?
?
6
is inserted to the lattice as in Figure 4(e).
? Both arrive at the end and it is the turn of the
next alignment pair.
When comparing a backbone word of the given
alignment pair with a backbone word of the lattice,
the following three cases should be handled:
? The current backbone word of the given align-
ment pair is a null word while the current back-
bone word of the lattice is not. A null back-
bone word is inserted to the lattice.
? The current backbone word of the lattice is a
null word while the current word of the given
alignment pair is not. The current null back-
bone word of the lattice is skipped with nothing
to do. The next backbone word of the lattice is
compared with the current backbone word of
the given alignment pair.
1109
Algorithm 1 Lattice construction algorithm.
1: Input: alignment pairs {p
n
}
N
n=1
2: L? p
1
3: Unique(L)
4: for n? 2 .. N do
5: pnode = p
n
? first
6: lnode = L ? first
7: while pnode ? barcnext 6= NULL do
8: if lnode ? barcnext = NULL or pnode ?
bword = null and lnode ? bword 6= null then
9: INSERTBARC(lnode, null)
10: pnode = pnode ? barcnext
11: else
12: if pnode ? bword 6= null and lnode ?
bword = null then
13: lnode = lnode ? barcnext
14: else
15: for each harc of pnode do
16: if NotExist(lnode, pnode ? harc)
then
17: INSERTHARC(lnode, pnode ?
harc)
18: pnode = pnode ? barcnext
19: lnode = lnode ? barcnext
20: Output: lattice L
? The current backbone words of the given align-
ment pair and the lattice are the same. Let
{harc
l
} denotes the set of hypothesis arcs,
which come from the same node with the cur-
rent backbone arc in the lattice, and harc
h
de-
notes one of the corresponding hypothesis arcs
in the given alignment pair. In the {harc
l
},
if there is no arc which is the same with the
harc
h
, a hypothesis arc projecting to harc
h
is
added to the lattice.
The algorithm of constructing a lattice is illus-
trated in Algorithm 1. The backbone words of the
alignment pair and the lattice are processed one by
one in a left-to-right manner. Line 2 initializes the
lattice with the first alignment pair, and Line 3 re-
moves the hypothesis arc which contains the same
words with the backbone arc. barc denotes the back-
bone arc, storing one backbone word only, and harc
denotes the hypothesis arc, storing the hypothesis
words. For there may be many alignment units span
the same backbone word range, there may be more
than one harc coming from one node. Line 8 ? 10
consider the condition 1 and function InsertBarc in
Line 9 inserts a null bone arc to the position right
before the current node. Line 12?13 deal with con-
dition 2 and jump to the next backbone word of the
lattice. Line 15?19 handle condition 3 and function
InsertHarc inserts to the lattice a harc with the same
hypothesis words and the same backbone word span
with the current hypothesis arc.
5 Decoding
In confusion network decoding, a translation is gen-
erated by traveling all the nodes from left to right.
So a translation path contains all the nodes. While
in lattice decoding, a translation path may skip some
nodes as some hypothesis arcs may cross more than
one backbone arc.
Similar to the features in Rosti et al (2007a), the
features adopted by lattice-based model are arc pos-
terior probability, language model probability, the
number of null arcs, the number of hypothesis arcs
possessing more than one non-null word and the
number of all non-null words. The features are com-
bined in a log-linear model with the arc posterior
probabilities being processed specially as follows:
log p(e/f) =
N
arc
?
i=1
log (
N
s
?
s=1
?
s
p
s
(arc))
+ ?L(e) + ?N
nullarc
(e)
+ ?N
longarc
(e) + ?N
word
(e)
(9)
where f denotes the source sentence, e denotes a
translation generated by the lattice-based system,
N
arc
is the number of arcs the path of e covers,
N
s
is the number of candidate systems and ?
s
is the
weight of system s. ? is the language model weight
and L(e) is the LM log-probability. N
nullarcs
(e) is
the number of the arcs which only contain a null
word, and N
longarc
(e) is the number of the arcs
which store more than one non-null word. The
above two numbers are gotten by counting both
backbone arcs and hypothesis arcs. ? and ? are the
corresponding weights of the numbers, respectively.
N
word
(e) is the non-null word number and ? is its
weight.
Each arc has different confidences concerned with
different systems, and the confidence of system s
is denoted by p
s
(arc). p
s
(arc) is increased by
1110
1/(k+1) if the hypothesis ranking k in the system s
contains the arc (Rosti et al, 2007a; He et al, 2008).
Cube pruning algorithm with beam search is em-
ployed to search for the consensus output (Huang
and Chiang, 2005). The nodes in the lattice are
searched in a topological order and each node re-
tains a list of N best candidate partial translations.
6 Experiments
The candidate systems participating in the system
combination are as listed in Table 1: System A is a
BTG-based system using a MaxEnt-based reorder-
ing model; System B is a hierarchical phrase-based
system; System C is the Moses decoder (Koehn et
al., 2007); System D is a syntax-based system. 10-
best hypotheses from each candidate system on the
dev and test sets were collected as the input of the
system combination.
In our experiments, the weights were all tuned on
the NIST MT02 Chinese-to-English test set, includ-
ing 878 sentences, and the test data was the NIST
MT05 Chinese-to-English test set, including 1082
sentences, except the experiments in Table 2. A 5-
gram language model was used which was trained
on the XinHua portion of Gigaword corpus. The re-
sults were all reported in case sensitive BLEU score
and the weights were tuned in Powell?s method to
maximum BLEU score. The IHMM-based align-
ment module was implemented according to He et
al. (2008), He (2007) and Vogel et al (1996). In all
experiments, the parameters for IHMM-based align-
ment module were set to: the smoothing factor for
the surface similarity model, ? = 3; the controlling
factor for the distortion model, K = 2.
6.1 Comparison with
Confusion-network-based model
In order to compare the lattice-based system with
the confusion-network-based system fairly, we used
IHMM-based system combination model on behalf
of the confusion-network-based model described in
He et al (2008). In both lattice-based and IHMM-
based systems, the bilingual dictionaries were ex-
tracted on the FBIS data set which included 289K
sentence pairs. The interpolation factor of the simi-
larity model was set to ? = 0.1.
The results are shown in Table 1. IHMM stands
for the IHMM-based model and Lattice stands for
the lattice-based model. On the dev set, the lattice-
based system was 3.92 BLEU points higher than the
best single system and 0.36 BLEU point higher than
the IHMM-based system. On the test set, the lattice-
based system got an absolute improvement by 3.73
BLEU points over the best single system and 1.23
BLEU points over the IHMM-based system.
System MT02 MT05
BLEU% BLEU%
SystemA 31.93 30.68
SystemB 32.16 32.07
SystemC 32.09 31.64
SystemD 33.37 31.26
IHMM 36.93 34.57
Lattice 37.29 35.80
Table 1: Results on the MT02 and MT05 test sets
The results on another test sets are reported in Ta-
ble 2. The parameters were tuned on the newswire
part of NIST MT06 Chinese-to-English test set, in-
cluding 616 sentences, and the test set was NIST
MT08 Chinese-to-English test set, including 1357
sentences. The BLEU score of the lattice-based sys-
tem is 0.93 BLEU point higher than the IHMM-
based system and 3.0 BLEU points higher than the
best single system.
System MT06 MT08
BLEU% BLEU%
SystemA 32.51 25.63
SystemB 31.43 26.32
SystemC 31.50 23.43
SystemD 32.41 26.28
IHMM 36.05 28.39
Lattice 36.53 29.32
Table 2: Results on the MT06 and MT08 test sets
We take a real example from the output of the
two systems (in Table 3) to show that higher BLEU
scores correspond to better alignments and better
translations. The translation of System C is selected
as the backbone. From Table 3, we can see that
because of 1-to-1 mappings, ?Russia? is aligned to
?Russian? and ??s? to ?null? in the IHMM-based
model, which leads to the error translation ?Russian
1111
Source: ?dIE?h?i??dIEd?i?1??
SystemA: Russia merger of state-owned oil company and the state-run gas company in Russia
SystemB: Russia ?s state-owned oil company is working with Russia ?s state-run gas company mergers
SystemC: Russian state-run oil company is combined with the Russian state-run gas company
SystemD: Russia ?s state-owned oil companies are combined with Russia ?s state-run gas company
IHMM: Russian ?s state-owned oil company working with Russia ?s state-run gas company
Lattice: Russia ?s state-owned oil company is combined with the Russian state-run gas company
Table 3: A real translation example
?s?. Instead, ?Russia ?s? is together aligned to ?Rus-
sian? in the lattice-based model. Also due to 1-to-
1 mappings, null word aligned to ?is? is inserted.
As a result, ?is? is missed in the output of IHMM-
based model. In contrast, in the lattice-based sys-
tem, ?is working with? are aligned to ?is combined
with?, forming a phrase pair.
6.2 Effect of Dictionary Scale
The dictionary is important to the semantic similar-
ity model in IHMM-based alignment method. We
evaluated the effect of the dictionary scale by using
dictionaries extracted on different data sets. The dic-
tionaries were respectively extracted on similar data
sets: 30K sentence pairs, 60K sentence pairs, 289K
sentence pairs (FBIS corpus) and 2500K sentence
pairs. The results are illustrated in Table 4. In or-
der to demonstrate the effect of the dictionary size
clearly, the interpolation factor of similarity model
was all set to ? = 0.1.
From Table 4, we can see that when the cor-
pus size rise from 30k to 60k, the improvements
were not obvious both on the dev set and on the
test set. As the corpus was expanded to 289K, al-
though on the dev set, the result was only 0.2 BLEU
point higher, on the test set, it was 0.63 BLEU point
higher. As the corpus size was up to 2500K, the
BLEU scores both on the dev and test sets declined.
The reason is that, on one hand, there are more noise
on the 2500K sentence pairs; on the other hand, the
289K sentence pairs cover most of the words appear-
ing on the test set. So we can conclude that in or-
der to get better results, the dictionary scale must be
up to some certain scale. If the dictionary is much
smaller, the result will be impacted dramatically.
MT02 MT05
BLEU% BLEU%
30k 36.94 35.14
60k 37.09 35.17
289k 37.29 35.80
2500k 37.14 35.62
Table 4: Effect of dictionary scale
6.3 Effect of Semantic Alignments
For the IHMM-based alignment method, the transla-
tion probability of an English word pair is computed
using a linear interpolation of the semantic similar-
ity and the surface similarity. So the two similarity
models decide the translation probability together
and the proportion is controlled by the interpolation
factor. We evaluated the effect of the two similarity
models by varying the interpolation factor ?.
We used the dictionaries extracted on the FBIS
data set. The result is shown in Table 5. We got the
best result with ? = 0.1. When we excluded the
semantic similarity model (? = 0.0) or excluded the
surface similarity model (? = 1.0), the performance
became worse.
7 Conclusion
The alignment model plays an important role in
system combination. Because of the expression
limitation of confusion networks, only 1-to-1 map-
pings are employed in the confusion-network-based
model. This paper proposes a lattice-based system
combination model. As a general form of confusion
networks, lattices can express n-to-n mappings. So
a lattice-based model processes phrase pairs while
1112
MT02 MT05
BLEU% BLEU%
? = 1.0 36.41 34.92
? = 0.7 37.21 35.65
? = 0.5 36.43 35.02
? = 0.4 37.14 35.55
? = 0.3 36.75 35.66
? = 0.2 36.81 35.55
? = 0.1 37.29 35.80
? = 0.0 36.45 35.14
Table 5: Effect of semantic alignments
a confusion-network-based model processes words
only. As a result, phrase pairs must be extracted be-
fore constructing a lattice.
On NIST MT05 test set, the lattice-based sys-
tem gave better results with an absolute improve-
ment of 1.23 BLEU points over the confusion-
network-based system (He et al, 2008) and 3.73
BLEU points over the best single system. On
NIST MT08 test set, the lattice-based system out-
performed the confusion-network-based system by
0.93 BLEU point and outperformed the best single
system by 3.0 BLEU points.
8 Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China Contract 60736014, Na-
tional Natural Science Foundation of China Con-
tract 60873167 and High Technology R&D Program
Project No. 2006AA010108. Thank Wenbin Jiang,
Tian Xia and Shu Cai for their help. We are also
grateful to the anonymous reviewers for their valu-
able comments.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. of
IEEE ASRU, pages 351?354.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL/HLT 2008, pages 1012?1020, Colum-
bus, Ohio, June.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP, pages
95?100.
Xiaodong He, Mei Yang, Jangfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for computing outputs from ma-
chine translation systems. In Proc. of EMNLP, pages
98?107.
Xiaodong He. 2007. Using word-dependent translation
models in hmm based word alignment for statistical
machine translation. In Proc. of COLING-ACL, pages
961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages 53?
64.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstration
Session.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of IEEE EACL, pages 33?40.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007a. Improved word-level system com-
bination for machine translation. In Proc. of ACL,
pages 312?319.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007b. Combining outputs from multiple ma-
chine translation systems. In Proc. of NAACL-HLT,
pages 228?235.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translaiton system combination. In
Proc. of the Third ACL WorkShop on Statistical Ma-
chine Translation, pages 183?186.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Con-
sensus network decoding for statistical machine trans-
lation system combination. In Proc. of ICASSP, pages
105?108.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proc. of COLING, pages 836?841.
1113
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1222?1231,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bilingually-Constrained (Monolingual) Shift-Reduce Parsing
Liang Huang
Google Research
1350 Charleston Rd.
Mountain View, CA 94043, USA
lianghuang@google.com
liang.huang.sh@gmail.com
Wenbin Jiang and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
jiangwenbin@ict.ac.cn
Abstract
Jointly parsing two languages has been
shown to improve accuracies on either or
both sides. However, its search space is
much bigger than the monolingual case,
forcing existing approaches to employ
complicated modeling and crude approxi-
mations. Here we propose a much simpler
alternative, bilingually-constrained mono-
lingual parsing, where a source-language
parser learns to exploit reorderings as ad-
ditional observation, but not bothering to
build the target-side tree as well. We show
specifically how to enhance a shift-reduce
dependency parser with alignment fea-
tures to resolve shift-reduce conflicts. Ex-
periments on the bilingual portion of Chi-
nese Treebank show that, with just 3 bilin-
gual features, we can improve parsing ac-
curacies by 0.6% (absolute) for both En-
glish and Chinese over a state-of-the-art
baseline, with negligible (?6%) efficiency
overhead, thus much faster than biparsing.
1 Introduction
Ambiguity resolution is a central task in Natu-
ral Language Processing. Interestingly, not all lan-
guages are ambiguous in the same way. For exam-
ple, prepositional phrase (PP) attachment is (no-
toriously) ambiguous in English (and related Eu-
ropean languages), but is strictly unambiguous in
Chinese and largely unambiguous Japanese; see
(1a) I [ saw Bill ] [ with a telescope ].
wo [ yong wangyuanjin] [kandao le Bi?er].
?I used a telescope to see Bill.?
(1b) I saw [ Bill [ with a telescope ] ].
wo kandao le [ [ na wangyuanjin ] de Bi?er].
?I saw Bill who had a telescope at hand.?
Figure 1: PP-attachment is unambiguous in Chi-
nese, which can help English parsing.
Figure 1 for an example.1 It is thus intuitive to use
two languages for better disambiguation, which
has been applied not only to this PP-attachment
problem (Fossum and Knight, 2008; Schwartz et
al., 2003), but also to the more fundamental prob-
lem of syntactic parsing which subsumes the for-
mer as a subproblem. For example, Smith and
Smith (2004) and Burkett and Klein (2008) show
that joint parsing (or reranking) on a bitext im-
proves accuracies on either or both sides by lever-
aging bilingual constraints, which is very promis-
ing for syntax-based machine translation which re-
quires (good-quality) parse trees for rule extrac-
tion (Galley et al, 2004; Mi and Huang, 2008).
However, the search space of joint parsing is in-
evitably much bigger than the monolingual case,
1Chinese uses word-order to disambiguate the attachment
(see below). By contrast, Japanese resorts to case-markers
and the unambiguity is limited: it works for the ?V or N?
attachment ambiguities like in Figure 1 (see (Schwartz et al,
2003)) but not for the ?N
1
or N
2
? case (Mitch Marcus, p.c.).
1222
forcing existing approaches to employ compli-
cated modeling and crude approximations. Joint
parsing with a simplest synchronous context-free
grammar (Wu, 1997) is O(n6) as opposed to the
monolingual O(n3) time. To make things worse,
languages are non-isomorphic, i.e., there is no 1-
to-1 mapping between tree nodes, thus in practice
one has to use more expressive formalisms such
as synchronous tree-substitution grammars (Eis-
ner, 2003; Galley et al, 2004). In fact, rather than
joint parsing per se, Burkett and Klein (2008) re-
sort to separate monolingual parsing and bilingual
reranking over k2 tree pairs, which covers a tiny
fraction of the whole space (Huang, 2008).
We instead propose a much simpler alterna-
tive, bilingually-constrained monolingual parsing,
where a source-language parser is extended to ex-
ploit the reorderings between languages as addi-
tional observation, but not bothering to build a tree
for the target side simultaneously. To illustrate the
idea, suppose we are parsing the sentence
(1) I saw Bill [PP with a telescope ].
which has 2 parses based on the attachment of PP:
(1a) I [ saw Bill ] [PP with a telescope ].
(1b) I saw [ Bill [PP with a telescope ]].
Both are possible, but with a Chinese translation
the choice becomes clear (see Figure 1), because
a Chinese PP always immediately precedes the
phrase it is modifying, thus making PP-attachment
strictly unambiguous.2 We can thus use Chinese to
help parse English, i.e., whenever we have a PP-
attachment ambiguity, we will consult the Chinese
translation (from a bitext), and based on the align-
ment information, decide where to attach the En-
glish PP. On the other hand, English can help Chi-
nese parsing as well, for example in deciding the
scope of relative clauses which is unambiguous in
English but ambiguous in Chinese.
This method is much simpler than joint pars-
ing because it remains monolingual in the back-
bone, with alignment information merely as soft
evidence, rather than hard constraints since auto-
matic word alignment is far from perfect. It is thus
2to be precise, in Fig. 1(b), the English PP is translated
into a Chinese relative clause, but nevertheless all phrasal
modifiers attach to the immediate right in Mandarin Chinese.
straightforward to implement within a monolin-
gual parsing algorithm. In this work we choose
shift-reduce dependency parsing for its simplicity
and efficiency. Specifically, we make the following
contributions:
? we develop a baseline shift-reduce depen-
dency parser using the less popular, but clas-
sical, ?arc-standard? style (Section 2), and
achieve similar state-of-the-art performance
with the the dominant but complicated ?arc-
eager? style of Nivre and Scholz (2004);
? we propose bilingual features based on word-
alignment information to prefer ?target-side
contiguity? in resolving shift-reduce conflicts
(Section 3);
? we verify empirically that shift-reduce con-
flicts are the major source of errors, and cor-
rect shift-reduce decisions strongly correlate
with the above bilingual contiguity condi-
tions even with automatic alignments (Sec-
tion 5.3);
? finally, with just three bilingual features,
we improve dependency parsing accuracy
by 0.6% for both English and Chinese over
the state-of-the-art baseline with negligible
(?6%) efficiency overhead (Section 5.4).
2 Simpler Shift-Reduce Dependency
Parsing with Three Actions
The basic idea of classical shift-reduce parsing
from compiler theory (Aho and Ullman, 1972) is
to perform a left-to-right scan of the input sen-
tence, and at each step, choose one of the two ac-
tions: either shift the current word onto the stack,
or reduce the top two (or more) items on the stack,
replacing them with their combination. This idea
has been applied to constituency parsing, for ex-
ample in Sagae and Lavie (2006), and we describe
below a simple variant for dependency parsing
similar to Yamada and Matsumoto (2003) and the
?arc-standard? version of Nivre (2004).
2.1 The Three Actions
Basically, we just need to split the reduce ac-
tion into two symmetric (sub-)actions, reduceL
and reduceR, depending on which one of the two
1223
stack queue arcs
previous S w
i
|Q A
shift S|w
i
Q A
previous S|s
t?1
|s
t
Q A
reduceL S|st Q A ? {(st, st?1)}
reduceR S|st?1 Q A ? {(st?1, st)}
Table 1: Formal description of the three actions.
Note that shift requires non-empty queue while
reduce requires at least two elements on the stack.
items becomes the head after reduction. More for-
mally, we describe a parser configuration by a tu-
ple ?S,Q,A? where S is the stack, Q is the queue
of remaining words of the input, and A is the set
of dependency arcs accumulated so far.3 At each
step, we can choose one of the three actions:
1. shift: move the head of (a non-empty) queue
Q onto stack S;
2. reduceL: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace
them with s
t
(as the head), and add a left arc
(s
t
, s
t?1
) to A;
3. reduceR: combine the top two items on the
stack, s
t
and s
t?1
(t ? 2), and replace them
with s
t?1
(as the head), and add a right arc
(s
t?1
, s
t
) to A.
These actions are summarized in Table 1. The
initial configuration is always ??, w
1
. . . w
n
, ??
with empty stack and no arcs, and the final con-
figuration is ?w
j
, ?, A? where w
j
is recognized as
the root of the whole sentence, and A encodes a
spanning tree rooted at w
j
. For a sentence of n
words, there are exactly 2n ? 1 actions: n shifts
and n ? 1 reductions, since every word must be
pushed onto stack once, and every word except the
root will eventually be popped in a reduction. The
time complexity, as other shift-reduce instances, is
clearly O(n).
2.2 Example of Shift-Reduce Conflict
Figure 2 shows the trace of this paradigm on the
example sentence. For the first two configurations
3a ?configuration? is sometimes called a ?state? (Zhang
and Clark, 2008), but that term is confusing with the states in
shift-reduce LR/LL parsing, which are quite different.
0 - I saw Bill with a ...
1 shift I saw Bill with a ...
2 shift I saw Bill with a ...
3 reduceL saw Bill with a ...
I
4 shift saw Bill with a ...
I
5a reduceR saw with a ...
I Bill
5b shift saw Bill with a ...
I
Figure 2: A trace of 3-action shift-reduce on the
example sentence. Shaded words are on stack,
while gray words have been popped from stack.
After step (4), the process can take either (5a)
or (5b), which correspond to the two attachments
(1a) and (1b) in Figure 1, respectively.
(0) and (1), only shift is possible since there are
not enough items on the stack for reduction. At
step (3), we perform a reduceL, making word ?I?
a modifier of ?saw?; after that the stack contains
a single word and we have to shift the next word
?Bill? (step 4). Now we face a shift-reduce con-
flict: we can either combine ?saw? and ?Bill? in
a reduceR action (5a), or shift ?Bill? (5b). We will
use features extracted from the configuration to re-
solve the conflict. For example, one such feature
could be a bigram s
t
? s
t?1
, capturing how likely
these two words are combined; see Table 2 for the
complete list of feature templates we use in this
baseline parser.
We argue that this kind of shift-reduce conflicts
are the major source of parsing errors, since the
other type of conflict, reduce-reduce conflict (i.e.,
whether left or right) is relatively easier to resolve
given the part-of-speech information. For exam-
ple, between a noun and an adjective, the former
is much more likely to be the head (and so is a
verb vs. a preposition or an adverb). Shift-reduce
resolution, however, is more non-local, and often
involves a triple, for example, (saw, Bill, with) for
a typical PP-attachment. On the other hand, if we
indeed make a wrong decision, a reduce-reduce
mistake just flips the head and the modifier, and
often has a more local effect on the shape of the
tree, whereas a shift-reduce mistake always leads
1224
Type Features
Unigram s
t
T (s
t
) s
t
? T (s
t
)
s
t?1
T (s
t?1
) s
t?1
? T (s
t?1
)
w
i
T (w
i
) w
i
? T (w
i
)
Bigram s
t
? s
t?1
T (s
t
) ? T (s
t?1
) T (s
t
) ? T (w
i
)
T (s
t
) ? s
t?1
? T (s
t?1
) s
t
? s
t?1
? T (s
t?1
) s
t
? T (s
t
) ? T (s
t?1
)
s
t
? T (s
t
) ? s
t?1
s
t
? T (s
t
) ? s
t?1
? T (s
t?1
)
Trigram T (s
t
) ? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? T (s
t
) ? T (w
i
) T (s
t?2
) ? T (s
t?1
) ? T (s
t
)
s
t
? T (w
i
) ? T (w
i+1
) T (s
t?1
) ? s
t
? T (w
i
)
Modifier T (s
t?1
) ? T (lc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (rc(s
t?1
)) ? T (s
t
) T (s
t?1
) ? T (s
t
) ? T (lc(s
t
))
T (s
t?1
) ? T (s
t
) ? T (rc(s
t
)) T (s
t?1
) ? T (lc(s
t?1
)) ? s
t
T (s
t?1
) ? T (rc(s
t?1
)) ? s
t
T (s
t?1
) ? s
t
? T (lc(s
t
))
Table 2: Feature templates of the baseline parser. s
t
, s
t?1
denote the top and next to top words on the
stack; w
i
and w
i+1
denote the current and next words on the queue. T (?) denotes the POS tag of a
given word, and lc(?) and rc(?) represent the leftmost and rightmost child. Symbol ? denotes feature
conjunction. Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.
to vastly incompatible tree shapes with crossing
brackets (for example, [saw Bill] vs. [Bill with a
telescope]). We will see in Section 5.3 that this
is indeed the case in practice, thus suggesting us
to focus on shift-reduce resolution, which we will
return to with the help of bilingual constraints in
Section 3.
2.3 Comparison with Arc-Eager
The three action system was originally described
by Yamada and Matsumoto (2003) (although their
methods require multiple passes over the input),
and then appeared as ?arc-standard? in Nivre
(2004), but was argued against in comparison to
the four-action ?arc-eager? variant. Most subse-
quent works on shift-reduce or ?transition-based?
dependency parsing followed ?arc-eager? (Nivre
and Scholz, 2004; Zhang and Clark, 2008), which
now becomes the dominant style. But we argue
that ?arc-standard? is preferable because:
1. in the three action ?arc-standard? system, the
stack always contains a list of unrelated sub-
trees recognized so far, with no arcs between
any of them, e.g. (I? saw) and (Bill) in step
4 of Figure 2), whereas the four action ?arc-
eager? style can have left or right arrows be-
tween items on the stack;
2. the semantics of the three actions are atomic
and disjoint, whereas the semantics of 4 ac-
tions are not completely disjoint. For exam-
ple, their Left action assumes an implicit Re-
duce of the left item, and their Right ac-
tion assumes an implicit Shift. Furthermore,
these two actions have non-trivial precondi-
tions which also causes the next problem (see
below). We argue that this is rather compli-
cated to implement.
3. the ?arc-standard? scan always succeeds,
since at the end we can always reduce with
empty queue, whereas the ?arc-eager? style
sometimes goes into deadends where no ac-
tion can perform (prevented by precondi-
tions, otherwise the result will not be a well-
formed tree). This becomes parsing failures
in practice (Nivre and Scholz, 2004), leaving
more than one fragments on stack.
As we will see in Section 5.1, this simpler
arc-standard system performs equally well with
a state-of-the-art arc-eager system (Zhang and
Clark, 2008) on standard English Treebank pars-
ing (which is never shown before). We argue
that all things being equal, this simpler paradigm
should be preferred in practice. 4
2.4 Beam Search Extension
We also enhance deterministic shift-reduce pars-
ing with beam search, similar to Zhang and Clark
(2008), where k configurations develop in paral-
lel. Pseudocode 1 illustrates the algorithm, where
we keep an agenda V of the current active con-
figurations, and at each step try to extend them by
applying one of the three actions. We then dump
the best k new configurations from the buffer back
4On the other hand, there are also arguments for ?arc-
eager?, e.g., ?incrementality?; see (Nivre, 2004; Nivre, 2008).
1225
Pseudocode 1 beam-search shift-reduce parsing.
1: Input: POS-tagged word sequence w
1
. . . w
n
2: start ? ??, w
1
. . . w
n
, ?? ? initial config: empty stack,
no arcs
3: V? {start} ? initial agenda
4: for step ? 1 . . . 2n? 1 do
5: BUF? ? ? buffer for new configs
6: for each config in agenda V do
7: for act ? {shift, reduceL, reduceR} do
8: if act is applicable to config then
9: next ? apply act to config
10: insert next into buffer BUF
11: V? top k configurations of BUF
12: Output: the tree of the best config in V
into the agenda for the next step. The complexity
of this algorithm is O(nk), which subsumes the
determinstic mode as a special case (k = 1).
2.5 Online Training
To train the parser we need an ?oracle? or gold-
standard action sequence for gold-standard depen-
dency trees. This oracle turns out to be non-unique
for the three-action system (also non-unique for
the four-action system), because left dependents
of a head can be reduced either before or after all
right dependents are reduced. For example, in Fig-
ure 2, ?I? is a left dependent of ?saw?, and can in
principle wait until ?Bill? and ?with? are reduced,
and then finally combine with ?saw?. We choose
to use the heuristic of ?shortest stack? that always
prefers reduceL over shift, which has the effect that
all left dependents are first recognized inside-out,
followed by all right dependents, also inside-out,
which coincides with the head-driven constituency
parsing model of Collins (1999).
We use the popular online learning algorithm
of structured perceptron with parameter averag-
ing (Collins, 2002). Following Collins and Roark
(2004) we also use the ?early-update? strategy,
where an update happens whenever the gold-
standard action-sequence falls off the beam, with
the rest of the sequence neglected. As a special
case, for the deterministic mode, updates always
co-occur with the first mistake made. The intuition
behind this strategy is that future mistakes are of-
ten caused by previous ones, so with the parser on
the wrong track, future actions become irrelevant
for learning. See Section 5.3 for more discussions.
(a) I
:::::::::
saw Bill with a telescope .
wo yong wangyuanjin kandao le Bi?er.
c(s
t?1
, s
t
) =+; reduce is correct
(b) I
:::::::::
saw Bill with a telescope .
wo kandao le na wangyuanjin de Bi?er.
c(s
t?1
, s
t
) =?; reduce is wrong
(c) I saw
:::::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo kandao le na wangyuanjin de Bi?er.
cR(st, wi) =+; shift is correct
(d) I saw
:::::::::
Bill with
:::
a
::::::::::
telescope
:
.
wo yong wangyuanjin kandao le Bi?er.
cR(st, wi) =?; shift is wrong
Figure 3: Bilingual contiguity features c(s
t?1
, s
t
)
and cR(st, wi) at step (4) in Fig. 2 (facing a shift-
reduce decision). Bold words are currently on
stack while gray ones have been popped. Here the
stack tops are s
t
= Bill, s
t?1
= saw, and the queue
head is w
i
= with; underlined texts mark the source
and target spans being considered, and wavy un-
derlines mark the allowed spans (Tab. 3). Red bold
alignment links violate contiguity constraints.
3 Soft Bilingual Constraints as Features
As suggested in Section 2.2, shift-reduce con-
flicts are the central problem we need to address
here. Our intuition is, whenever we face a deci-
sion whether to combine the stack tops s
t?1
and
s
t
or to shift the current word w
i
, we will consult
the other language, where the word-alignment in-
formation would hopefully provide a preference,
as in the running example of PP-attachment (see
Figure 1). We now develop this idea into bilingual
contiguity features.
1226
3.1 A Pro-Reduce Feature c(s
t?1
, s
t
)
Informally, if the correct decision is a reduction,
then it is likely that the corresponding words of
s
t?1
and s
t
on the target-side should also form a
contiguous span. For example, in Figure 3(a), the
source span of a reduction is [saw .. Bill], which
maps onto [kandao . . . Bi?er] on the Chinese side.
This target span is contiguous, because no word
within this span is aligned to a source word out-
side of the source span. In this case we say feature
c(s
t?1
, s
t
) =+, which encourages ?reduce?.
However, in Figure 3(b), the source span is still
[saw .. Bill], but this time maps onto a much
longer span on the Chinese side. This target span
is discontiguous, since the Chinese words na and
wangyuanjin are alinged to English ?with? and
?telescope?, both of which fall outside of the
source span. In this case we say feature c(s
t?1
, s
t
)
=?, which discourages ?reduce? .
3.2 A Pro-Shift Feature cR(st, wi)
Similarly, we can develop another feature
cR(st, wi) for the shift action. In Figure 3(c),
when considering shifting ?with?, the source
span becomes [Bill .. with] which maps to [na
.. Bi?er] on the Chinese side. This target span
looks like discontiguous in the above definition
with wangyuanjin aligned to ?telescope?, but we
tolerate this case for the following reasons. There
is a crucial difference between shift and reduce:
in a shift, we do not know yet the subtree spans
(unlike in a reduce we are always combining two
well-formed subtrees). The only thing we are
sure of in a shift action is that s
t
and w
i
will be
combined before s
t?1
and s
t
are combined (Aho
and Ullman, 1972), so we can tolerate any target
word aligned to source word still in the queue,
but do not allow any target word aligned to an
already recognized source word. This explains
the notational difference between cR(st, wi) and
c(s
t?1
, s
t
), where subscript ?R? means ?right
contiguity?.
As a final example, in Figure 3(d), Chinese
word kandao aligns to ?saw?, which is already
recognized, and this violates the right contiguity.
So cR(st, wi) =?, suggesting that shift is probably
wrong. To be more precise, Table 3 shows the for-
mal definitions of the two features. We basically
source target alowed
feature f span sp span tp span ap
c(s
t?1
, s
t
) [s
t?1
..s
t
] M(sp) [s
t?1
..s
t
]
cR(st, wi) [st..wi] M(sp) [st..wn]
f = + iff. M?1(M(sp)) ? ap
Table 3: Formal definition of bilingual features.
M(?) is maps a source span to the target language,
and M?1(?) is the reverse operation mapping back
to the source language.
map a source span sp to its target span M(sp),
and check whether its reverse image back onto the
source language M?1(M(sp)) falls inside the al-
lowed span ap. For cR(st, wi), the allowed span
extends to the right end of the sentence.5
3.3 Variations and Implementation
To conclude so far, we have got two alignment-
based features, c(s
t?1
, s
t
) correlating with reduce,
and cR(st, wi) correlating with shift. In fact, the
conjunction of these two features,
c(s
t?1
, s
t
) ? cR(st, wi)
is another feature with even stronger discrimina-
tion power. If
c(s
t?1
, s
t
) ? cR(st, wi) = + ? ?
it is strongly recommending reduce, while
c(s
t?1
, s
t
) ? cR(st, wi) = ? ?+
is a very strong signal for shift. So in total we got
three bilingual feature (templates), which in prac-
tice amounts to 24 instances (after cross-product
with {?,+} and the three actions). We show in
Section 5.3 that these features do correlate with
the correct shift/reduce actions in practice.
The naive implemention of bilingual feature
computation would be of O(kn2) complexity
in the worse case because when combining the
largest spans one has to scan over the whole sen-
tence. We envision the use of a clever datastructure
would reduce the complexity, but leave this to fu-
ture work, as the experiments (Table 8) show that
5Our definition implies that we only consider faithful
spans to be contiguous (Galley et al, 2004). Also note that
source spans include all dependents of s
t
and s
t?1
.
1227
the parser is only marginally (?6%) slower with
the new bilingual features. This is because the ex-
tra work, with just 3 bilingual features, is not the
bottleneck in practice, since the extraction of the
vast amount of other features in Table 2 dominates
the computation.
4 Related Work in Grammar Induction
Besides those cited in Section 1, there are some
other related work on using bilingual constraints
for grammar induction (rather than parsing). For
example, Hwa et al (2005) use simple heuris-
tics to project English trees to Spanish and Chi-
nese, but get discouraging accuracy results learned
from those projected trees. Following this idea,
Ganchev et al (2009) and Smith and Eisner (2009)
use constrained EM and parser adaptation tech-
niques, respectively, to perform more principled
projection, and both achieve encouraging results.
Our work, by constrast, never uses bilingual
tree pairs not tree projections, and only uses word
alignment alone to enhance a monolingual gram-
mar, which learns to prefer target-side contiguity.
5 Experiments
5.1 Baseline Parser
We implement our baseline monolingual parser (in
C++) based on the shift-reduce algorithm in Sec-
tion 2, with feature templates from Table 2. We
evaluate its performance on the standard Penn En-
glish Treebank (PTB) dependency parsing task,
i.e., train on sections 02-21 and test on section 23
with automatically assigned POS tags (at 97.2%
accuracy) using a tagger similar to Collins (2002),
and using the headrules of Yamada and Mat-
sumoto (2003) for conversion into dependency
trees. We use section 22 as dev set to deter-
mine the optimal number of iterations in per-
ceptron training. Table 4 compares our baseline
against the state-of-the-art graph-based (McDon-
ald et al, 2005) and transition-based (Zhang and
Clark, 2008) approaches, and confirms that our
system performs at the same level with those state-
of-the-art, and runs extremely fast in the determin-
istic mode (k=1), and still quite fast in the beam-
search mode (k=16).
parser accuracy secs/sent
McDonald et al (2005) 90.7 0.150
Zhang and Clark (2008) 91.4 0.195
our baseline at k=1 90.2 0.009
our baseline at k=16 91.3 0.125
Table 4: Baseline parser performance on standard
Penn English Treebank dependency parsing task.
The speed numbers are not exactly comparable
since they are reported on different machines.
Training Dev Test
CTB Articles 1-270 301-325 271-300
Bilingual Paris 2745 273 290
Table 5: Training, dev, and test sets from bilingual
Chinese Treebank a` la Burkett and Klein (2008).
5.2 Bilingual Data
The bilingual data we use is the translated por-
tion of the Penn Chinese Treebank (CTB) (Xue
et al, 2002), corresponding to articles 1-325 of
PTB, which have English translations with gold-
standard parse trees (Bies et al, 2007). Table 5
shows the split of this data into training, devel-
opment, and test subsets according to Burkett and
Klein (2008). Note that not all sentence pairs could
be included, since many of them are not one-
to-one aligned at the sentence level. Our word-
alignments are generated from the HMM aligner
of Liang et al (2006) trained on approximately
1.7M sentence pairs (provided to us by David Bur-
kett, p.c.). This aligner outputs ?soft alignments?,
i.e., posterior probabilities for each source-target
word pair. We use a pruning threshold of 0.535 to
remove low-confidence alignment links,6 and use
the remaining links as hard alignments; we leave
the use of alignment probabilities to future work.
For simplicity reasons, in the following exper-
iments we always supply gold-standard POS tags
as part of the input to the parser.
5.3 Testing our Hypotheses
Before evaluating our bilingual approach, we need
to verify empirically the two assumptions we
made about the parser in Sections 2 and 3:
6and also removing notoriously bad links in {the, a, an}?
{de, le} following Fossum and Knight (2008).
1228
sh ? re re ? sh sh-re re-re
# 92 98 190 7
% 46.7% 49.7% 96.4% 3.6%
Table 6: [Hypothesis 1] Error distribution in the
baseline model (k = 1) on English dev set.
?sh ? re? means ?should shift, but reduced?. Shift-
reduce conflicts overwhelmingly dominate.
1. (monolingual) shift-reduce conflict is the ma-
jor source of errors while reduce-reduce con-
flict is a minor issue;
2. (bilingual) the gold-standard decisions of
shift or reduce should correlate with contigu-
ities of c(s
t?1
, s
t
), and of cR(st, wi).
Hypothesis 1 is verified in Table 6, where we
count all the first mistakes the baseline parser
makes (in the deterministic mode) on the En-
glish dev set (273 sentences). In shift-reduce pars-
ing, further mistakes are often caused by previ-
ous ones, so only the first mistake in each sen-
tence (if there is one) is easily identifiable;7 this
is also the argument for ?early update? in apply-
ing perceptron learning to these incremental pars-
ing algorithms (Collins and Roark, 2004) (see also
Section 2). Among the 197 first mistakes (other
76 sentences have perfect output), the vast ma-
jority, 190 of them (96.4%), are shift-reduce er-
rors (equally distributed between shift-becomes-
reduce and reduce-becomes-shift), and only 7
(3.6%) are due to reduce-reduce conflicts.8 These
statistics confirm our intuition that shift-reduce de-
cisions are much harder to make during parsing,
and contribute to the overwhelming majority of er-
rors, which is studied in the next hypothesis.
Hypothesis 2 is verified in Table 7. We take
the gold-standard shift-reduce sequence on the En-
glish dev set, and classify them into the four cat-
egories based on bilingual contiguity features: (a)
c(s
t?1
, s
t
), i.e. whether the top 2 spans on stack
is contiguous, and (b) cR(st, wi), i.e. whether the
7to be really precise one can define ?independent mis-
takes? as those not affected by previous ones, i.e., errors
made after the parser recovers from previous mistakes; but
this is much more involved and we leave it to future work.
8Note that shift-reduce errors include those due to the
non-uniqueness of oracle, i.e., between some reduceL and
shift. Currently we are unable to identify ?genuine? errors
that would result in an incorrect parse. See also Section 2.5.
c(s
t?1
, s
t
) cR(st, wi) shift reduce
+ ? 172 ? 1,209
? + 1,432 > 805
+ + 4,430 ? 3,696
? ? 525 ? 576
total 6,559 = 6,286
Table 7: [Hyp. 2] Correlation of gold-standard
shift/reduce decisions with bilingual contiguity
conditions (on English dev set). Note there is al-
ways one more shift than reduce in each sentence.
stack top is contiguous with the current word w
i
.
According to discussions in Section 3, when (a) is
contiguous and (b) is not, it is a clear signal for
reduce (to combine the top two elements on the
stack) rather than shift, and is strongly supported
by the data (first line: 1209 reduces vs. 172 shifts);
and while when (b) is contiguous and (a) is not,
it should suggest shift (combining s
t
and w
i
be-
fore s
t?1
and s
t
are combined) rather than reduce,
and is mildly supported by the data (second line:
1432 shifts vs. 805 reduces). When (a) and (b) are
both contiguous or both discontiguous, it should
be considered a neutral signal, and is also consis-
tent with the data (next two lines). So to conclude,
this bilingual hypothesis is empirically justified.
On the other hand, we would like to note that
these correlations are done with automatic word
alignments (in our case, from the Berkeley aligner)
which can be quite noisy. We suspect (and will fin-
ish in the future work) that using manual align-
ments would result in a better correlation, though
for the main parsing results (see below) we can
only afford automatic alignments in order for our
approach to be widely applicable to any bitext.
5.4 Results
We incorporate the three bilingual features (again,
with automatic alignments) into the baseline
parser, retrain it, and test its performance on the
English dev set, with varying beam size. Table 8
shows that bilingual constraints help more with
larger beams, from almost no improvement with
the deterministic mode (k=1) to +0.5% better with
the largest beam (k=16). This could be explained
by the fact that beam-search is more robust than
the deterministic mode, where in the latter, if our
1229
baseline +bilingual
k accuracy time (s) accuracy time (s)
1 84.58 0.011 84.67 0.012
2 85.30 0.025 85.62 0.028
4 85.42 0.040 85.81 0.044
8 85.50 0.081 85.95 0.085
16 85.57 0.158 86.07 0.168
Table 8: Effects of beam size k on efficiency and
accuracy (on English dev set). Time is average
per sentence (in secs). Bilingual constraints show
more improvement with larger beams, with a frac-
tional efficiency overhead over the baseline.
English Chinese
monolingual baseline 86.9 85.7
+bilingual features 87.5 86.3
improvement +0.6 +0.6
signficance level p < 0.05 p < 0.08
Berkeley parser 86.1 87.9
Table 9: Final results of dependency accuracy (%)
on the test set (290 sentences, beam size k=16).
bilingual features misled the parser into a mistake,
there is no chance of getting back, while in the
former multiple configurations are being pursued
in parallel. In terms of speed, both parsers run pro-
portionally slower with larger beams, as the time
complexity is linear to the beam-size. Computing
the bilingual features further slows it down, but
only fractionally so (just 1.06 times as slow as the
baseline at k=16), which is appealing in practice.
By contrast, Burkett and Klein (2008) reported
their approach of ?monolingual k-best parsing fol-
lowed by bilingual k2-best reranking? to be ?3.8
times slower? than monolingual parsing.
Our final results on the test set (290 sentences)
are summarized in Table 9. On both English
and Chinese, the addition of bilingual features
improves dependency arc accuracies by +0.6%,
which is mildly significant using the Z-test of
Collins et al (2005). We also compare our results
against the Berkeley parser (Petrov and Klein,
2007) as a reference system, with the exact same
setting (i.e., trained on the bilingual data, and test-
ing using gold-standard POS tags), and the result-
ing trees are converted into dependency via the
same headrules. We use 5 iterations of split-merge
grammar induction as the 6th iteration overfits the
small training set. The result is worse than our
baseline on English, but better than our bilingual
parser on Chinese. The discrepancy between En-
glish and Chinese is probably due to the fact that
our baseline feature templates (Table 2) are engi-
neered on English not Chinese.
6 Conclusion and Future Work
We have presented a novel parsing paradigm,
bilingually-constrained monolingual parsing,
which is much simpler than joint (bi-)parsing, yet
still yields mild improvements in parsing accuracy
in our preliminary experiments. Specifically,
we showed a simple method of incorporating
alignment features as soft evidence on top of a
state-of-the-art shift-reduce dependency parser,
which helped better resolve shift-reduce conflicts
with fractional efficiency overhead.
The fact that we managed to do this with only
three alignment features is on one hand encour-
aging, but on the other hand leaving the bilingual
feature space largely unexplored. So we will en-
gineer more such features, especially with lexical-
ization and soft alignments (Liang et al, 2006),
and study the impact of alignment quality on pars-
ing improvement. From a linguistics point of view,
we would like to see how linguistics distance
affects this approach, e.g., we suspect English-
French would not help each other as much as
English-Chinese do; and it would be very interest-
ing to see what types of syntactic ambiguities can
be resolved across different language pairs. Fur-
thermore, we believe this bilingual-monolingual
approach can easily transfer to shift-reduce con-
stituency parsing (Sagae and Lavie, 2006).
Acknowledgments
We thank the anonymous reviewers for pointing to
us references about ?arc-standard?. We also thank
Aravind Joshi and Mitch Marcus for insights on
PP attachment, Joakim Nivre for discussions on
arc-eager, Yang Liu for suggestion to look at man-
ual alignments, and David A. Smith for sending
us his paper. The second and third authors were
supported by National Natural Science Foundation
of China, Contracts 60603095 and 60736014, and
863 State Key Project No. 2006AA010108.
1230
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume I: Parsing of Series in Automatic Computation.
Prentice Hall, Englewood Cliffs, New Jersey.
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English chinese translation treebank
v1.0. LDC2007T02.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL (poster), pages 205?208.
Victoria Fossum and Kevin Knight. 2008. Using bilin-
gual chinese-english word alignments to resolve pp-
attachment ambiguity in english. In Proceedings of
AMTA Student Workshop.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL, pages 273?280.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP,
Honolulu, Haiwaii.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together. Workshop
at ACL-2004, Barcelona.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of ACL
(poster).
Lee Schwartz, Takako Aikawa, and Chris Quirk. 2003.
Disambiguation of english pp attachment using mul-
tilingual aligned data. In Proceedings of MT Summit
IX.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous features.
In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese cor-
pus. In Proceedings of COLING.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
1231
Parsing the Penn Chinese Treebank
with Semantic Knowledge
Deyi Xiong1,2, Shuanglong Li1,3,
Qun Liu1, Shouxun Lin1, and Yueliang Qian1
1 Institute of Computing Technology, Chinese Academy of Sciences,
P.O. Box 2704, Beijing 100080, China
{dyxiong, liuqun, sxlin}@ict.ac.cn
2 Graduate School of Chinese Academy of Sciences
3 University of Science and Technology Beijing
Abstract. We build a class-based selection preference sub-model to in-
corporate external semantic knowledge from two Chinese electronic se-
mantic dictionaries. This sub-model is combined with modifier-head gen-
eration sub-model. After being optimized on the held out data by the
EM algorithm, our improved parser achieves 79.4% (F1 measure), as well
as a 4.4% relative decrease in error rate on the Penn Chinese Treebank
(CTB). Further analysis of performance improvement indicates that se-
mantic knowledge is helpful for nominal compounds, coordination, and
NV tagging disambiguation, as well as alleviating the sparseness of in-
formation available in treebank.
1 Introduction
In the recent development of full parsing technology, semantic knowledge is sel-
dom used, though it is known to be useful for resolving syntactic ambiguities.
The reasons for this may be twofold. The first one is that it can be very difficult
to add additional features which are not available in treebanks to generative
models like Collins (see [1]), which are very popular for full parsing. For smaller
tasks, like prepositional phrase attachment disambiguation, semantic knowledge
can be incorporated flexibly using different learning algorithms (see [2,3,4,5]).
For full parsing with generative models, however, incorporating semantic knowl-
edge may involve great changes of model structures. The second reason is that
semantic knowledge from external dictionaries seems to be noisy, ambiguous and
not available in explicit forms, compared with the information from treebanks.
Given these two reasons, it seems to be difficult to combine the two different
information sources?treebank and semantic knowledge?into one integrated sta-
tistical parsing model.
One feasible way to solve this problem is to keep the original parsing model
unchanged and build an additional sub-model to incorporate semantic knowledge
from external dictionaries. The modularity afforded by this approach makes
it easier to expand or update semantic knowledge sources with the treebank
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 70?81, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Parsing the Penn Chinese Treebank with Semantic Knowledge 71
unchanged or vice versa. Further, the combination of the semantic sub-model
and the original parsing model can be optimized automatically.
In this paper, we build a class-based selection preference sub-model, which
is embedded in our lexicalized parsing model, to incorporate external seman-
tic knowledge. We use two Chinese electronic dictionaries and their combi-
nation as our semantic information sources. Several experiments are carried
out on the Penn Chinese Treebank to test our hypotheses. The results indi-
cate that a significant improvement in performance is achieved when seman-
tic knowledge is incorporated into parsing model. Further improvement analy-
sis is made. We confirm that semantic knowledge is indeed useful for nominal
compounds and coordination ambiguity resolution. And surprisingly, semantic
knowledge is also helpful to correct Chinese NV mistagging errors mentioned
by Levy and Manning (see [12]). Yet another great benefit to incorporating
semantic knowledge is to alleviate the sparseness of information available in
treebank.
2 The Baseline Parser
Our baseline parsing model is similar to the history-based, generative and lexical-
ized Model 1 of Collins (see [1]). In this model, the right hand side of lexicalized
rules is decomposed into smaller linguistic objects as follows:
P (h) ? #Ln(ln)...L1(l1)H(h)R1(r1)...Rm(rm)# .
The uppercase letters are delexicalized nonterminals, while the lowercase letters
are lexical items, e.g. head word and head tag (part-of-speech tag of the head
word), corresponding to delexicalized nonterminals. H(h) is the head constituent
of the rule from which the head lexical item h is derived according to some head
percolation rules.1 The special termination symbol # indicates that there is no
more symbols to the left/right. Accordingly, the rule probability is factored into
three distributions. The first distribution is the probability of generating the
syntactic label of the head constituent of a parent node with label P , head word
Hhw and head tag Hht:
PrH(H |P, Hht, Hhw) .
Then each left/right modifier of head constituent is generated in two steps: first
its syntactic label Mi and corresponding head tag Miht are chosen given context
features from the parent (P ), head constituent (H, Hht, Hhw), previously gen-
erated modifier (Mi?1, Mi?1ht) and other context information like the direction
(dir) and distance2 (dis) to the head constituent:
1 Here we use the modified head percolation table for Chinese from Xia (see [6]).
2 Our distance definitions are different for termination symbol and non-termination
symbol, which are similar to Klein and Manning (see [7]).
72 D. Xiong et al
PrM (Mi, Miht|HCM ) .
where the history context HCM is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis .
Then the new modifier?s head word Mihw is also generated with the probability:
PrMw(Mihw|HCMw ) .
where the history context HCMw is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis, Mi, Miht .
All the three distributions are smoothed through Witten-Bell interpolation
just like Collins (see [1]). For the distribution PrM , we build back-off struc-
tures with six levels, which are different from Collins? since we find our back-off
structures work better than the three-level back-off structures of Collins. For
the distribution PrMw , the parsing model backs off to the history context with
head word Hhw removed, then to the modifier head tag Miht, just like Collins.
Gildea (see [9]) and Bikel (see [10]) both observed that the effect of bilexical de-
pendencies is greatly impaired due to the sparseness of bilexical statistics. Bikel
even found that the parser only received an estimate that made use of bilexi-
cal statistics a mere 1.49% of the time. However, according to the wisdom of
the parsing community, lexical bigrams, the word pairs (Mihw, Hhw) are very
informative with semantic constraints. Along this line, in this paper, we build
an additional class-based selectional preference sub-model, which is described
in section 3, to make good use of this semantic information through selectional
restrictions between head and modifier words.
Our parser takes segmented but untagged sentences as input. The probability
of unknown words, Pr(uword|tag), is estimated based on the first character of
the word and if the first characters are unseen, the probability is estimated by
absolute discounting.
We do some linguistically motivated re-annotations for the baseline parser.
The first one is marking non-recursive noun phrases from other common noun
phrases without introducing any extra unary levels (see [1,8]). We find this basic
NP re-annotation very helpful for the performance. We think it is because of the
annotation style of the Upenn Chinese Treebank (CTB). According to Xue et al
(see [11]), noun-noun compounds formed by an uninterrupted sequence of words
POS-tagged as NNs are always left flat because of difficulties in determining
which modifies which. The second re-annotation is marking basic VPs, which we
think is beneficial for reducing multilevel VP adjunction ambiguities (see [12]).
To speed up parsing, we use the beam thresholding techniques in Xiong et
al. (see [13]). In all cases, the thresholding for completed edges is set at ct = 9
and incomplete edges at it = 7. The performance of the baseline parser is 78.5%
in terms of F1 measure of labeled parse constituents on the same CTB training
and test sets with Bikel et al (see [14])
Parsing the Penn Chinese Treebank with Semantic Knowledge 73
3 Incorporating Semantic Knowledge
In this section, we describe how to incorporate semantic knowledge from external
semantic dictionaries into parsing model to improve the performance. Firstly, we
extract semantic categories through two Chinese electronic semantic dictionaries
and some heuristic rules. Then we build a selection preference sub-model based
on extracted semantic categories. In section 3.3, we present our experiments
and results in detail. And finally, we compare parses from baseline parser with
those from the new parser incorporated with semantic knowledge. We empirically
confirm that semantic knowledge is helpful for nominal compound, coordination
and POS tagging ambiguity resolution. Additionally, we also find that semantic
knowledge can greatly alleviate problems caused by data sparseness.
3.1 Extracting Semantic Categories
Semantic knowledge is not presented in treebanks and therefore has to be ex-
tracted from external knowledge sources. We have two Chinese electronic se-
mantic dictionaries, both are good knowledge sources for us to extract semantic
categories. One is the HowNet dictionary3, which covers 67,440 words defined
by 2112 different sememes. The other is the ?TongYiCi CiLin? expanded version
(henceforth CiLin)4, which represents 77,343 words in a dendrogram.
HowNet (HN): Each sememe defined by the HowNet is regarded as a semantic
category. And through the hypernym-hyponym relation between different cat-
egories, we can extract semantic categories at various granularity levels. Since
words may have different senses, and therefore different definitions in HowNet,
we just use the first definition of words in HowNet. At the first level HN1, we ex-
tract the first definitions and use them as semantic categories of words. Through
the hypernym ladders, we can get HN2, HN3, by replacing categories at lower
level with their hypernyms at higher level. Table 1 shows information about
words and extracted categories at different levels.
CiLin (CL): CL is a branching diagram, where each node represents a semantic
category. There are three levels in total, and from the top down, 12 categories in
the first level (CL1), 97 categories in the second level (CL2), 1400 categories in
the third level (CL3). We extract semantic categories at level CL1, CL2 and CL3.
HowNet+CiLin: Since the two dictionaries have different ontologies and rep-
resentations of semantic categories, we establish a strategy to combine them:
HowNet is used as a primary dictionary, and CiLin as a secondary dictionary.
If a word is not found in HowNet but found in Cilin, we will look up other
words from its synset defined by CiLin in HowNet. If HowNet query succeeds,
the corresponding semantic category in HowNet will be assigned to this word.
3 http://www.keenage.com/.
4 The dictionary is recorded and expanded by Information Retrieval Laboratory,
Harbin Institute of Technology.
74 D. Xiong et al
Table 1. Sizes and coverage of words and semantic categories from different semantic
knowledge sources
Data HN1 HN2 HN3 CL1 CL2 CL3
words in train 9522 6040 6469
words in test 1824 1538 1581
words in both 1412 1293 1310
classes in train - 1054 381 118 12 92 1033
classes in test - 520 251 93 12 79 569
classes in both - 504 248 93 12 79 552
According to our experimental results, we choose HN2 as the primary semantic
category set and combine it with CL1, CL2 and CL3.
Heuristic Rules (HR): Numbers and time expressions are recognized using
simple heuristic rules. For a better recognition, one can define accurate regular
expressions. However, we just collect suffixes and feature characters to match
strings. For example, Chinese numbers are strings whose characters all come
from a predefined set. These two classes are merged into HowNet and labelled
by semantic categories from HowNet.
In our experiments, we combine HN2, CL1/2/3, and HR as our external
sources. In these combinations {HN2+CL1/2/3+HR}, all semantic classes come
from the primary semantic category set HN2, therefore we get the same class
coverage that we obtain from the single source HN2 but a bigger word coverage.
The number of covered words of these combinations in {train, test, both} is
{7911, 1672, 1372} respectively.
3.2 Building Class-Based Selection Preference Sub-model
There are several ways to incorporate semantic knowledge into parsing model.
Bikel (see [15]) suggested a way to capture semantic preferences by employing
bilexical-class statistics, in other words, dependencies among head-modifier word
classes. Bikel did not carry it out and therefore greater details are not available.
However, the key point, we think, is to use classes extracted from semantic
dictionary, instead of words, to model semantic dependencies between head and
modifier. Accordingly, we build a similar bilexical-class sub-model as follows:
Prclass(CMihw|CHhw, Hht, Miht, dir) .
where CMihw and CHhw represent semantic categories of words Mihw and Hhw,
respectively. This model is combined with sub-model PrMw to form a mixture
model Pmix:
Prmix = ?PrMw + (1 ? ?)Prclass . (1)
? is hand-optimized, and an improvement of about 0.5% in terms of F1 measure is
gained. However, even a very slight change in the value of ?, e.g. 0.001, will have
a great effect on the performance. Besides, it seems that the connection between
Parsing the Penn Chinese Treebank with Semantic Knowledge 75
entropy, i.e. the total negative logarithm of the inside probability of trees, and F1
measure, is lost, while this relation is observed in many experiments. Therefore,
automatic optimization algorithms, like EM, can not work in this mixture model.
The reason, we guess, is that biclass dependencies among head-modifier word
classes seem too coarse-grained to capture semantic preferences between head
and modifier. In most cases, a head word has a strong semantic constraints on
the concept ? of mw, one of its modifier words, but that doesn?t mean other
words in the same class with the head word has the same semantic preferences
on the concept ?. For example, the verb eat impose a selection restriction on
its object modifier5: it has to be solid food. On the other hand, the verb drink
specifies its object modifier to be liquid beverage. At the level HN2, verb eat
and drink have the same semantic category metabolize. However, they impose
different selection preferences on their PATIENT roles.
To sum up, bilexical dependencies are too fine-grained when being used to
capture semantic preferences and therefore lead to serious data sparseness. Bi-
class dependencies, which result in an unstable performance improvement, on the
other hand, seem to be too coarse-grained for semantic preferences. We build a
class-based selection preference model:
Prsel(CMihw|Hhw, P ) .
This model is similar to Resnik (see [2]). We use the parent node label P to
represent the grammatical relation between head and modifier. Besides, in this
model, only modifier word is replaced with its semantic category. The depen-
dencies between head word and modifier word class seem to be just right for
capturing these semantic preferences.
The final mixture model is the combination of the class-based selection pref-
erence sub-model Prsel and modifier-head generation sub-model PrMw :
Prmix = ?PrMw + (1 ? ?)Prsel . (2)
Since the connection between entropy and F1 measure is observed again, EM
algorithm is used to optimize ?. Just like Levy (see [12]), we set aside articles 1-
25 in CTB as held out data for EM algorithm and use articles 26-270 as training
data during ? optimization.
3.3 Experimental Results
We have designed several experiments to check the power of our class-based se-
lection preference model with different semantic data sources. In all experiments,
we first use the EM algorithm to optimize the parameter ?. As mentioned above,
during parameter optimization, articles 1-25 are used as held out data and ar-
ticles 26-270 are used as training data. Then we test our mixture model with
optimized parameter ? using the training data of articles 1-270 and test data of
articles 271-300 of length at most 40 words.
5 According to Thematic Role theory, this modifier has a PATIENT role.
76 D. Xiong et al
Table 2. Results for incorporating different semantic knowledge sources. The baseline
parser is described in Sect. 2. in detail.
Baseline HN1 HN2 HN3 CL1 CL2 CL3
F1(%) 78.5 78.6 79.1 78.9 77.5 78.7 78.8
Table 3. Results for combinations of different semantic knowledge sources
Baseline HN2+CL1+HR HN2+CL2+HR HN2+CL3+HR
F1(%) 78.5 79.2 79.4 79.3
Firstly, we carry out experiments on HowNet and CiLin, separately. Exper-
imental results are presented in Table 2. As can be seen, CiLin has a greater
coverage of words than that of HowNet, however, it works worse than HowNet.
And at the level CL1, coarse-grained classes even yield degraded results. It?s dif-
ficult to explain this, but the main reason may be that HowNet has a fine-grained
and substantial ontology while CiLin is designed only as a synset container.
Since HowNet has a better semantic representation and CiLin better cov-
erage, we want to combine them. The combination is described in Sect. 3.1,
where HN2 is used as the primary semantic category set. Words found by CiLin
and heuristic rules are labelled by semantic categories from HN2. Results are
shown in Table 3. Although external sources HN2+CL1/2/3+HR have the iden-
tical word coverage and yield exactly the same number of classes, the different
word-class distributions in them lead to the different results.
Due to the combination of HN2, CL2 and HR, we see that our new parser
with external semantic knowledge outperforms the baseline parser by 0.9% in
F1 measure. Given we are already at the 78% level of accuracy, an improve-
ment of 0.9% is well worth obtaining and confirms the importance of semantic
dependencies on parsing. Further, we do the significance test using Bikel?s sig-
nificance tester6 which is modified to output p-value for F1. The significance
level for F-score is at most (43376+1)/(1048576+1) = 0.041. A second 1048576
iteration produces the similar result. Therefore the improvement is statistically
significant.
3.4 Performance Improvement Analysis
We manually analyze parsing errors of the baseline parser (BP ) as well as per-
formance improvement of the new parser (IP ) with semantic knowledge from
the combination of HN2, CL2 and HR. Improvement analysis can provide an
additional valuable perspective: how semantic knowledge helps to resolve some
ambiguities. We compare BP and IP on the test data parse by parse. There are
299 sentences of length at most 40 words among the total 348 test sentences. The
two parsers BP and IP found different parses for 102 sentences, among which
6 See http://www.cis.upenn.edu/ dbikel/software.html
Parsing the Penn Chinese Treebank with Semantic Knowledge 77
Table 4. Frequency of parsing improvement types. AR represents ambiguity resolution.
Type Count Percent(%)
Nominal Compound AR 19 38
Coordination AR 9 18
NV AR in NV+noun 6 12
Other AR 16 32
IP yields better parse trees for 47 sentences according to the gold standard trees.
We have concentrated on these 47 sentences and compared parse trees found by
IP with those found by BP . Frequencies of major types of parsing improvement
is presented in Table 4. Levy and Manning (see [12])(henceforth L&M) observed
the top three parsing error types: NP-NP modification, Coordination and NV
mistagging, which are also common in our baseline parser. As can be seen, our
improved parser can address these types of ambiguities to some extent through
semantic knowledge.
Nominal Compounds (NCs) Disambiguation: Nominal compounds are no-
torious ?every way ambiguous? constructions.7 The different semantic interpre-
tations have different dependency structures. According to L&M, this ambiguity
will be addressed by the dependency model when word frequencies are large
enough to be reliable. However, even for the treebank central to a certain topic,
many very plausible dependencies occur only once.8 A good technique for re-
solving this conflict is to generalize the dependencies from word pairs to word-
class pairs. Such generalized dependencies, as noted in section 3.2, can capture
semantic preferences, as well as alleviate the data sparseness associated with
standard bilexical statistics. In our class-based selection preference model, if the
frequency of pair [CMhw, Hhw]9 is large enough, the parser can interpret nominal
compounds correctly, that is, it can tell which modify which.
NCs are always parsed as flatter structures by our baseline parser, just like
the tree a. in Figure 1. This is partly because of the annotation style of CTB,
where there is no NP-internal structure. For these NCs without internal analysis,
we re-annotated them as basic NPs with label NPB, as mentioned in section 2.
This re-annotation really helps. Another reason is that the baseline parser, or
the modifier word generating sub-model PMw , can not capture hierarchical se-
mantic dependencies of internal structures of NCs due to the sparseness of bilex-
ical dependencies. In our new parser, however, the selection preference model is
able to build semantically preferable structures through word-class dependency
statistics. For NCs like (n1, n2, n3), where ni is a noun, dependency structures
7 ?Every way ambiguous? constructions are those for which the number of analy-
ses is the number of binary trees over the terminal elements. Prepositional phrase
attachment, coordination, and nominal compounds are all ?every way ambiguous?
constructions.
8 Just as Klein et al (see [8]) said, one million words of training data just isn?t enough.
9 Henceforth, [s1, s2] denotes a dependency structure, where s1 is a modifier word or
its semantic class (C), and s2 is the head word.
78 D. Xiong et al
a. NPB




NR
??
NN
??
NN
??
b. NP




NP

NPB
NR
??
NPB
NN
??
NPB
NN
??
Fig. 1. Nominal Compounds: The North Korean government?s special envoy. a. is the
incorrect flat parse, b. is the right one in corpus
{[Cn1 , n2], [Cn1 , n3], [Cn2 , n3]} will be checked in terms of semantic acceptability
and semantically preferable structures will be built finally. For more complicated
NCs, similar analysis follows.
In our example (see Fig. 1.), the counts of word dependencies [??/North
Korea, ??/government] and [??/North Korea,??/special envoy] in the
training data both are 0. Therefore, it is impossible for the baseline parser to
have a preference between these two dependency structures. On the other hand,
the counts of word-class dependencies [???,??/government], where ???
is the semantic category of?? in HN2, is much larger than the counts of [??
?,??/special envoy] and [??,??/special envoy], where?? is the semantic
category of ?? in the training data. Therefore, the dependency structure of
[??/North Korea, ??/government] will be built.
Coordination Disambiguation: Coordination is another kind of ?every way
ambiguous? construction. For coordination structures, the head word is meaning-
less. But that doesn?t matter, since semantic dependency between the spurious
head and modifier will be used to measure the meaning similarity of coordinated
structures. Therefore, our selection preference model still works in coordination
constructions. We have also found VP coordination ambiguity, which is similar
to that observed by L&M. The latter VP in coordinated VPs is often parsed as
an IP due to pro-drop by the baseline parser. That is, the coordinated structure
VP is parsed as: V P 0 ? V P 1IP 2. This parse will be penalized by the selection
preference model because the hypothesis that the head word of IP 2 has a sim-
ilar meaning to the head word of V P 1 under the grammatical relation V P 0 is
infrequent.
NV-ambiguous Tagging Disambiguation: The lack of overt morphological
marking for transforming verbal words to nominal words in Chinese results in
ambiguity between these two categories. L&M argued that the way to resolve
this ambiguity is to look at more external context, like some function words,
e.g. adverbial or prenominal modifiers, co-occurring with NV-ambiguous words.
However, in some cases, NV-ambiguous words can be tagged correctly without
external context. Chen et al (see [16]) studied the pattern of NV+noun, which
will be analyzed as a predicate-object structure if NV is a verb and a modifier-
noun structure if NV is a noun. They found that in most cases, this pattern can
Parsing the Penn Chinese Treebank with Semantic Knowledge 79
a. VP

VPB
VV
??
NPB
NN
??
b. NPB

NN
??
NN
??
Fig. 2. NV-ambiguity: a. implement plans (incorrect parse) versus b. implementation
plans (corpus)
Table 5. Previous Results on CTB parsing for sentences of length at most 40 words
LP LR F1
Bikel and Chiang 2000 77.2 76.2 76.7
Levy and Manning 2003 78.4 79.2 78.8
Present work 80.1 78.7 79.4
Bikel Thesis 2004 81.2 78.0 79.6
Chiang and Bikel 2002 81.1 78.8 79.9
be parsed correctly without any external context. Furthermore, they argued that
semantic preferences are helpful for the resolution of ambiguity between these
two different structures. In our selection preference model, semantic preferences
interweave with grammatical relations. These semantic dependencies impose con-
straints on the structure of the pattern NV+noun and therefore on the POS
tag of NV. Figure 2 shows our new parser can correct NV mistagging errors
occurring in the pattern of NV+noun.
Smoothing:Besides the three ambiguity resolution noted above, semantic knowl-
edge indeed helps alleviate the fundamental sparseness of the lexical dependency
information available in the CTB. For many word pairs [mod,head], whose count
information is not available in the training data, the dependency statistics of head
and modifier can still work through the semantic category of mod. During our man-
ual analysis of performance improvement, many other structural ambiguities are
addressed due to the smoothing function of semantic knowledge.
4 Related Work on CTB Parsing
Previous work on CTB parsing and their results are shown in table 5. Bikel and
Chiang (see [14]) used two different models on CTB, one based on the modi-
fied BBN model which is very similar to our baseline model, the other on Tree
Insertion Grammar (TIG). While our baseline model used the same unknown
word threshold with Bikel and Chiang but smaller beam width, our result out-
performs theirs due to other features like distance, basic NP re-annotation used
by our baseline model. Levy and Manning (see [12]) used a factored model with
rich re-annotations guided by error analysis. In the baseline model, we also used
several re-annotations but find most re-annotations they suggested do not fit
80 D. Xiong et al
our model. The three parsing error types expounded above are also found by
L&M. However, we used more efficient measures to keep our improved model
from these errors.
The work of Bikel thesis (see [10]) emulated Collins? model and created a
language package to Chinese parsing. He used subcat frames and an additional
POS tagger for unseen words. Chiang and Bikel (see [17]) used the EM algorithm
on the same TIG-parser to improve the head percolation table for Chinese pars-
ing. Both these two parsers used fine-tuned features recovered from the treebank
that our model does not use. This leads to better results and indicates that there
is still room of improvement for our model.
5 Conclusions
We have shown that how semantic knowledge may be incorporated into a gener-
ative model for full parsing, which reaches 79.4% in CTB. Experimental results
are quite consistent with our intuition. After the manual analysis of performance
improvement, the working mechanism of semantic knowledge in the selection
preference model is quite clear:
1. Using semantic categories extracted from external dictionaries, the class-
based selection preference model first generalizes standard bilexical depen-
dencies, some of which are not available in training data, to word-class de-
pendencies. These dependencies are neither too fine-grained nor too coarse-
grained compared with bilexical and biclass dependencies, and really help to
alleviate fundamental information sparseness in treebank.
2. Based on the generalized word-class pairs, semantic dependencies are cap-
tured and used to address different kinds of ambiguities, like nominal com-
pounds, coordination construction, even NV-ambiguous words tagging.
Our experiments show that generative models have room for improvement
by employing semantic knowledge. And that may be also true for discrimina-
tive models, since these models can easily incorporate richer features in a well-
founded fashion. This is the subject of our future work.
Acknowledgements
This work was supported in part byNational HighTechnologyResearch andDevel-
opment Program under grant #2001AA114010 and #2003AA111010. We would
like to acknowledge anonymous reviewers who provided helpful suggestions.
References
1. Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Pars-
ing. PhD thesis, University of Pennsylvania.
2. Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to
Lexical Relationships. PhD thesis, University of Pennsylvania, Philadelphia, PA,
USA.
Parsing the Penn Chinese Treebank with Semantic Knowledge 81
3. Sanda Harabagiu. 1996. An Application of WordNet to Prepositional Attachement.
In Proceedings of ACL-96, June 1996, Santa Cruz CA, pages 360-363.
4. Yuval Krymolowski and Dan Roth. 1998. Incorporating Knowledge in Natural Lan-
guage Learning: A Case Study. In COLING-ACL?98 Workshop on Usage of Word-
Net in Natural Language Processing Systems,Montreal, Canada.
5. Mark McLauchlan. 2004. Thesauruses for Prepositional Phrase Attachment. In
Proceedings of CoNLL-2004,Boston, MA, USA, 2004, pp. 73-80.
6. Fei Xia. 1999. Automatic Grammar Generation from Two Different Perspectives.
PhD thesis, University of Pennsylvania.
7. Dan Klein and Christopher D. Manning. 2002. Fast Exact Natural Language Pars-
ing with a Factored Model. In Advances in Neural Information Processing Systems
15 (NIPS-2002).
8. Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In
Proceedings of ACL-03.
9. Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of
EMNLP-01, Pittsburgh, Pennsylvania.
10. Daniel M. Bikel. 2004a. On the Parameter Space of Generative Lexicalized Statis-
tical Parsing Models. PhD thesis, University of Pennsylvania.
11. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for Chinese Treebank
Project. Technical Report IRCS 00-08, University of Pennsylvania.
12. Roger Levy and Christopher Manning. 2003. Is it harder to parse Chinese, or the
Chinese Treebank? In Proceedings of ACL-03.
13. Deyi Xiong, Qun Liu and Shouxun Lin. 2005. Lexicalized Beam Thresholding Pars-
ing with Prior and Boundary Estimates. In Proceedings of the 6th Conference on
Intelligent Text Processing and Computational Linguistics (CICLing), Mexico City,
Mexico, 2005.
14. Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied
to the chinese treebank. In Proceedings of the Second Chinese Language Processing
Workshop, pages 1-6.
15. Daniel M. Bikel. 2004b. Intricacies of Collins? Parsing Model. to appear in Com-
putational Linguistics.
16. Kejian Chen and Weimei Hong. 1996. Resolving Ambiguities of Predicate-object
and Modifier-noun Structures for Chinese V-N Patterns. in Chinese. In Communi-
cation of COLIPS, Vol.6, #2, pages 73-79.
17. David Chiang and Daniel M. Bikel. 2002. Recovering Latent Information in Tree-
banks. In proceedings of COLING,2002.
Refinements in BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Haitao Mi, Qun Liu and Shouxun Lin
Key Lab of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing China, 100080
{htmi, liuqun, sxlin}@ict.ac.cn
Abstract
Bracketing Transduction Grammar (BTG)
has been well studied and used in statistical
machine translation (SMT) with promising
results. However, there are two major issues
for BTG-based SMT. First, there is no effec-
tive mechanism available for predicting or-
ders between neighboring blocks in the orig-
inal BTG. Second, the computational cost is
high. In this paper, we introduce two re-
finements for BTG-based SMT to achieve
better reordering and higher-speed decod-
ing, which include (1) reordering heuristics
to prevent incorrect swapping and reduce
search space, and (2) special phrases with
tags to indicate sentence beginning and end-
ing. The two refinements are integrated into
a well-established BTG-based Chinese-to-
English SMT system that is trained on large-
scale parallel data. Experimental results on
the NIST MT-05 task show that the proposed
refinements contribute significant improve-
ment of 2% in BLEU score over the baseline
system.
1 Introduction
Bracket transduction grammar was proposed by Wu
(1995) and firstly employed in statistical machine
translation in (Wu, 1996). Because of its good trade-
off between efficiency and expressiveness, BTG re-
striction is widely used for reordering in SMT (Zens
et al, 2004). However, BTG restriction does not
provide a mechanism to predict final orders between
two neighboring blocks.
To solve this problem, Xiong et al (2006)
proposed an enhanced BTG with a maximum en-
tropy (MaxEnt) based reordering model (MEBTG).
MEBTG uses boundary words of bilingual phrases
as features to predict their orders. Xiong et
al. (2006) reported significant performance im-
provement on Chinese-English translation tasks in
two different domains when compared with both
Pharaoh (Koehn, 2004) and the original BTG us-
ing flat reordering. However, error analysis of the
translation output of Xiong et al (2006) reveals
that boundary words predict wrong swapping, espe-
cially for long phrases although the MaxEnt-based
reordering model shows better performance than
baseline reordering models.
Another big problem with BTG-based SMT is the
high computational cost. Huang et al (2005) re-
ported that the time complexity of BTG decoding
with m-gram language model is O(n3+4(m?1)). If a
4-gram language model is used (common in many
current SMT systems), the time complexity is as
high as O(n15). Therefore with this time complexity
translating long sentences is time-consuming even
with highly stringent pruning strategy.
To speed up BTG decoding, Huang et al (2005)
adapted the hook trick which changes the time
complexity from O(n3+4(m?1)) to O(n3+3(m?1)).
However, the implementation of the hook trick with
pruning is quite complicated. Another method to in-
crease decoding speed is cube pruning proposed by
Chiang (2007) which reduces search space signifi-
cantly.
In this paper, we propose two refinements to ad-
dress the two issues, including (1) reordering heuris-
505
tics to prevent incorrect swapping and reduce search
space using swapping window and punctuation re-
striction, and (2) phrases with special tags to indicate
beginning and ending of sentence. Experimental re-
sults show that both refinements improve the BLEU
score significantly on large-scale data.
The above refinements can be easily implemented
and integrated into a baseline BTG-based SMT sys-
tem. However, they are not specially designed for
BTG-based SMT and can also be easily integrated
into other systems with different underlying trans-
lation strategies, such as the state-of-the-art phrase-
based system (Koehn et al, 2007), syntax-based sys-
tems (Chiang et al, 2005; Marcu et al, 2006; Liu et
al., 2006).
The rest of the paper is organized as follows. In
section 2, we review briefly the core elements of
the baseline system. In section 3 we describe our
proposed refinements in detail. Section 4 presents
the evaluation results on Chinese-to-English trans-
lation based on these refinements as well as results
obtained in the NIST MT-06 evaluation exercise. Fi-
nally, we conclude our work in section 5.
2 The Baseline System
In this paper, we use Xiong et al (2006)?s sys-
tem Bruin as our baseline system. Their system has
three essential elements which are (1) a stochastic
BTG, whose rules are weighted using different fea-
tures in log-linear form, (2) a MaxEnt-based reorder-
ing model with features automatically learned from
bilingual training data, (3) a CKY-style decoder us-
ing beam search similar to that of Wu (1996). We
describe the first two components briefly below.
2.1 Model
The translation process is modeled using BTG rules
which are listed as follows
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
The lexical rule (3) is used to translate source phrase
x into target phrase y and generate a block A. The
two rules (1) and (2) are used to merge two consec-
utive blocks into a single larger block in a straight or
inverted order.
To construct a stochastic BTG, we calculate rule
probabilities using the log-linear model (Och and
Ney, 2002). For the two merging rules (1) and (2),
the assigned probability Prm(A) is defined as fol-
lows
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where ?, the reordering score of block A1 and
A2, is calculated using the MaxEnt-based reordering
model (Xiong et al, 2006) described in the next sec-
tion, ?? is the weight of ?, and 4pLM (A1,A2) is the
increment of language model score of the two blocks
according to their final order, ?LM is its weight.
For the lexical rule (3), it is applied with a proba-
bility Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|y|)?6
?p?LMLM (y) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical translation
probabilities in both directions, exp(1) and exp(|y|)
are the phrase penalty and word penalty, respec-
tively and ?s are weights of features. These features
are commonly used in the state-of-the-art systems
(Koehn et al, 2005; Chiang et al, 2005).
2.2 MaxEnt-based Reordering Model
The MaxEnt-based reordering model is defined on
two consecutive blocks A1 and A2 together with
their order o ? {straight, inverted} according to
the maximum entropy framework.
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(6)
where the functions hi ? {0, 1} are model features
and ?i are weights of the model features trained au-
tomatically (Malouf, 2002).
There are three steps to train a MaxEnt-based re-
ordering model. First, we need to extract reordering
examples from unannotated bilingual data, then gen-
erate features from these examples and finally esti-
mate feature weights.
506
For extracting reordering examples, there are two
points worth mentioning:
1. In the extraction of useful reordering examples,
there is no length limitation over blocks com-
pared with extracting bilingual phrases.
2. When enumerating all combinations of neigh-
boring blocks, a good way to keep the number
of reordering examples acceptable is to extract
smallest blocks with the straight order while
largest blocks with the inverted order .
3 Refinements
In this section we describe two refinements men-
tioned above in detail. First, we present fine-
grained reordering heuristics using swapping win-
dow and punctuation restriction. Secondly, we inte-
grate special bilingual phrases with sentence begin-
ning/ending tags.
3.1 Reordering Heuristics
We conduct error analysis of the translation out-
put of the baseline system and observe that Bruin
sometimes incorrectly swaps two large neighboring
blocks on the target side. This happens frequently
when inverted order successfully challenges straight
order by the incorrect but strong support from the
language model and the MaxEnt-based reordering
model. The reason is that only boundary words
are used as evidences by both language model and
MaxEnt-based reordering model when the decoder
selects which merging rule (straight or inverted) to
be used 1. However, statistics show that bound-
ary words are not reliable for predicting the right
order between two larger neighboring blocks. Al-
Onaizan and Papineni (2006) also proved that lan-
guage model is insufficient to address long-distance
word reordering. If a wrong inverted order is se-
lected for two large consecutive blocks, incorrect
long-distance swapping happens.
Yet another finding is that many incorrect swap-
pings are related to punctuation marks. First, the
source sequence within a pair of balanced punctua-
tion marks (quotes and parentheses) should be kept
1In (Xiong et al, 2006), the language model uses the left-
most/rightmost words on the target side as evidences while the
MaxEnt-based reordering model uses the boundary words on
both sides.
Chinese: ?? : ??????????
?????????????????
????
Bruin: urgent action , he said : ?This is a very
serious situation , we can only hope that there
will be a possibility .?
Bruin+RH: he said : ?This is a very serious sit-
uation , we can only hope that there will be the
possibility to expedite action .?
Ref: He said: ?This is a very serious situa-
tion. We can only hope that it is possible to
speed up the operation.?
Figure 1: An example of incorrect long-distance
swap. The underlined Chinese words are incorrectly
swapped to the beginning of the sentence by the
original Bruin. RH means reordering heuristics.
within the punctuation after translation. However,
it is not always true when reordering is involved.
Sometime the punctuation marks are distorted with
the enclosed words sequences being moved out.
Secondly, it is found that a series of words is fre-
quently reordered from one side of a structural mark,
such as commas, semi-colons and colons, to the
other side of the mark for long sentences contain-
ing such marks. Generally speaking, on Chinese-
to-English translation, source words are translated
monotonously relative to their adjacent punctuation
marks, which means their order relative to punctua-
tion marks will not be changed. In summary, punctu-
ation marks place a strong constraint on word order
around them.
For example, in Figure 1, Chinese words ???
??? are reordered to sentence beginning. That is
an incorrect long-distance swapping, which makes
the reordered words moved out from the balanced
punctuation marks ??? and ???, and incorrectly
precede their previous mark ???.
These incorrect swappings definitely jeopardize
the quality of translation. Here we propose two
straightforward but effective heuristics to control
and adjust the reordering, namely swapping window
and punctuation restriction.
Swapping Window (SW): It constrains block
swapping in the following way
ACTIVATE A ? ?A1, A2? IF |A1s|+ |A2s| < sws
507
where |Ais| denotes the number of words on the
source side Ais of block Ai, sws is a pre-defined
swapping window size. Any inverted reordering be-
yond the pre-defined swapping window size is pro-
hibited.
Punctuation Restriction (PR): If two neighbor-
ing blocks include any of the punctuation marks p ?
{? ? ? ? ? ? ? ? ? ? ? ?}, the two
blocks will be merged with straight order.
Punctuation marks were already used in pars-
ing (Christine Doran, 2000) and statistical machine
translation (Och et al, 2003). In (Och et al,
2003), three kinds of features are defined, all re-
lated to punctuation marks like quotes, parentheses
and commas. Unfortunately, no statistically signifi-
cant improvement on the BLEU score was reported
in (Och et al, 2003). In this paper, we consider
this problem from a different perspective. We em-
phasize that words around punctuation marks are
reordered ungrammatically and therefore we posi-
tively use punctuation marks as a hard decision to
restrict such reordering around punctuations. This
is straightforward but yet results in significant im-
provement on translation quality.
The two heuristics described above can be used
together. If the following conditions are satisfied,
we can activate the inverted rule:
|A1s|+ |A2s| < sws && P
?
(A1s
?
A2s) = ?
where P is the set of punctuation marks mentioned
above.
The two heuristics can also speed up decoding be-
cause decoding will be monotone within those spans
which are not in accordance with both heuristics.
For a sentence with n words, the total number of
spans is O(n2). If we set sws = m (m < n),
then the number of spans with monotone search is
O((n?m)2). With punctuation restriction, the non-
monotone search space will reduce further.
3.2 Phrases with Sentence Beginning/Ending
Tags
We observe that in a sentence some phrases are more
likely to be located at the beginning, while other
phrases are more likely to be at the end. This kind of
location information with regard to the phrase posi-
tion could be used for reordering. A straightforward
way to use this information is to mark the begin-
ning and ending of word-aligned sentences with ?s?
and ?/s? respectively. This idea is borrowed from
language modeling (Stolcke, 2002). The corre-
sponding tags at the source and target sentences are
aligned to each other, i.e, the beginning tag of source
sentences is aligned to the beginning tag of target
sentences, similarly for the ending tag. Figure 2
shows a word-aligned sentence pair annotated with
the sentence beginning and ending tag.
During training, the sentence beginning and end-
ing tags (?s? and ?/s?) are treated as words. There-
fore the phrase extraction and MaxEnt-based re-
ordering training algorithm need not to be modified.
Phrases with the sentence beginning/ending tag will
be extracted and MaxEnt-based reordering features
with such tags will also be generated. For example,
from the word-aligned sentence pair in Figure 2, we
can extract tagged phrases like
?s??? ||| ?s? Tibet ?s
?? ?/s? ||| achievements ?/s?
and generate MaxEnt-based reordering features with
tags like
hi(o, b1, b2) =
{ 1, b2.t1 = ?/s?, o = s
0, otherwise
where b1, b2 are blocks, t1 denotes the last source
word, o = s means the order between two blocks
is straight. To avoid wrong alignments, we remove
tagged phrases where only the beginning/ending tag
is extracted on either side of the phrases, such as
?s? ||| ?s? Those?
?/s? ||| ?/s?
During decoding, we first annotate source sen-
tences with the beiginning/ending tags, then trans-
late them as what Bruin does. Note that phrases
with sentence beginning/ending tags will be used in
the same way as ordinary phrases without such tags
during decoding. With the additional support of lan-
guage model and MaxEnt-based reordering model,
we observe that phrases with such tags are always
moved to the beginning or ending of sentences cor-
rectly.
508
?s? ?? ?? ?? ?? ?? ?? ?/s?
?s? Tibet ?s financial work has gained remarkable achievements ?/s?
Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.
4 Evaluation
In this section, we report the performance of the en-
hanced Bruin on the NIST MT-05 and NIST MT-06
Chinese-to-English translation tasks. We describe
the corpus, model training, and experiments related
to the refinements described above.
4.1 Corpus
The bilingual training data is derived from the fol-
lowing various sources: the FBIS (LDC2003E14),
Hong Kong Parallel Text (Hong Kong News and
Hong Kong Hansards, LDC2004T08), Xinhua News
(LDC2002E18), Chinese News Translation Text
Part1 (LDC2005T06), Translations from the Chi-
nese Treebank (LDC2003E07), Chinese English
News Magazine (LDC2005E47). It contains 2.4M
sentence pairs in total (68.1M Chinese words and
73.8M English words).
For the efficiency of minimum-error-rate training,
we built our development set using sentences not ex-
ceeding 50 characters from the NIST MT-02 evalu-
ation test data (580 sentences).
4.2 Training
We use exactly the same way and configuration de-
scribed in (He et al, 2006) to preprocess the training
data, align words and extract phrases.
We built two four-gram language models using
Xinhua section of the English Gigaword corpus
(181.1M words) and the English side of the bilin-
gual training data described above respectively. We
applied modified Kneser-Ney smoothing as imple-
mented in the SRILM toolkit (Stolcke, 2002).
The MaxEnt-based reordering model is trained
using the way of (Xiong et al, 2006). The difference
is that we only use lexical features generated by tail
words of blocks, instead of head words, removing
features generated by the combination of two bound-
ary words.
Bleu(%) Secs/sent
Bruin 29.96 54.3
sws RH1 RH12 RH1 RH12
5 29.65 29.95 42.6 41.2
10 30.55 31.27 46.2 41.8
15 30.26 31.40 48.0 42.2
20 30.19 31.42 49.1 43.2
Table 1: Effect of reordering heuristics. RH1 de-
notes swapping window while RH12 denotes swap-
ping window with the addition of punctuation re-
striction.
4.3 Translation Results
Table 1 compares the BLEU scores 2 and the speed
in seconds/sentence of the baseline system Bruin
and the enhanced system with reordering heuristics
applied. The second row gives the BLEU score and
the average decoding time of Bruin. The rows be-
low row 3 show the BLEU scores and speed of the
enhanced Bruin with different combinations of re-
ordering heuristics. We can clearly see that the re-
ordering heuristics proposed by us have a two-fold
effect on the performance: improving the BLEU
score and decreasing the average decoding time.
The example in Figure 1 shows how reordering
heuristics prevent incorrect long-distance swapping
which is not in accordance with the punctuation re-
striction.
Table 1 also shows that a 15-word swapping win-
dow is an inflexion point with the best tradeoff be-
tween the decoding time and the BLEU score. We
speculate that in our corpus most reorderings hap-
pen within a 15-word window. We use the FBIS
corpus to testify this hypothesis. In this corpus, we
extract all reordering examples using the algorithm
of Xiong et al (2006). Figure 3 shows the reorder-
ing length distribution curve in this corpus. Accord-
2In this paper, all BLEU scores are case-sensitive and evalu-
ated on the NIST MT-05 Chinese-to-English translation task if
there is no special note.
509
0 10 20 30 40 50 60 70 80
0
5
10
15
20
25
Pe
rce
nt 
(%
)
Reordering Length
Figure 3: Reordering length distribution. The hor-
izontal axis (reordering length) indicates the num-
ber of words on the source side of two neighboring
blocks which are to be swapped. The vertical axis
represents what proportion of reorderings with a cer-
tain length is likely to be in all reordering examples
with an inverted order.
Bleu(%)
Without Special Phrases 31.40
With Special Phrases 32.01
Table 2: Effect of integrating special phrases with
the sentence beginning/ending tag.
ing to our statistics, reorderings within a window
not exceeding 15 words have a very high proportion,
97.29%. Therefore we set sws = 15 for later exper-
iments.
Table 2 shows the effect of integrating special
phrases with sentence beginning/ending tags into
Bruin. As special phrases accounts for only 1.95%
of the total phrases used, an improvement of 0.6%
in BLEU score is well worthwhile. Further, the im-
provement is statistically significant at the 99% con-
fidence level according to Zhang?s significant tester
(Zhang et al, 2004). Figure 4 shows several exam-
ples translated with special phrases integrated. We
can see that phrases with sentence beginning/ending
tags are correctly selected and located at the right
place.
Table 3 shows the performance of two systems on
the NIST MT-05 Chinese test data, which are (1)
System Refine MT-05 MT-06
Bruin - 29.96 -
EBruin RH 31.40 30.22
EBruin RH+SP 32.01 -
Table 3: Results of different systems. The refine-
ments RH, SP represent reordering heuristics and
special phrases with the sentence beginning/ending
tag, respectively.
Bruin, trained on the large data described above; and
(2) enhanced Bruin (EBruin) with different refine-
ments trained on the same data set. This table also
shows the evaluation result of the enhanced Bruin
with reordering heuristics, obtained in the NIST MT-
06 evaluation exercise. 3
5 Conclusions
We have described in detail two refinements for
BTG-based SMT which include reordering heuris-
tics and special phrases with tags. The refinements
were integrated into a well-established BTG-based
system Bruin introduced by Xiong et al (2006). Re-
ordering heuristics proposed here achieve a twofold
improvement: better reordering and higher-speed
decoding. To our best knowledge, we are the first
to integrate special phrases with the sentence be-
ginning/ending tag into SMT. Experimental results
show that the above refinements improve the base-
line system significantly.
For further improvements, we will investigate
possible extensions to the BTG grammars, e.g.
learning useful nonterminals using unsupervised
learning algorithm.
Acknowledgements
We would like to thank the anonymous review-
ers for useful comments on the earlier version of
this paper. The first author was partially sup-
ported by the National Science Foundations of
China (No. 60573188) and the High Technology
Research and Development Program of China (No.
2006AA010108) while he studied in the Institute of
Computing Technology, Chinese Academy of Sci-
ences.
3Full results are available at http://www.nist.gov/
speech/tests/mt/doc/mt06eval official results.html.
510
With Special Phrases Without Special Phrases
?s? Japan had already pledged to provide 30 mil-
lion US dollars of aid due to the tsunami victims of
the country . ?/s?
originally has pledged to provide 30 million US
dollars of aid from Japan tsunami victimized coun-
tries .
?s? the results of the survey is based on the re-
sults of the chiefs of the Ukrainian National 50.96%
cast by chiefs . ?/s?
is based on the survey findings Ukraine 50.96% cast
by the chiefs of the chiefs of the country .
?s? and at the same time , the focus of the world have
been transferred to other areas . ?/s?
and at the same time , the global focus has shifted
he.
Figure 4: Examples translated with special phrases integrated. The bold underlined words are special phrases
with the sentence beginning/ending tag.
References
Yaser Al-Onaizan, Kishore Papineni. 2006. Distortion
Models for Statistical Machine Translation. In Pro-
ceedings of ACL-COLING 2006.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, Michael Subotin. 2005. The
Hiero Machine Translation System: Extensions, Eval-
uation, and Analysis. In Proceedings of HLT/EMNLP,
pages 779?786, Vancouver, October 2005.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. In computational linguistics, 33(2).
Christine Doran. 2000. Punctuation in a Lexicalized
Grammar. In Proceedings of Workshop TAG+5, Paris.
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, Qun
Liu. 2006. ICT System Description for the 2006
TC-STAR Run #2 SLT Evaluation. In Proceedings of
TC-STAR Workshop on Speech-to-Speech Translation,
Barcelona, Spain.
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Ma-
chine Translation as Lexicalized Parsing with Hooks.
In Proceedings of the 9th International Workshop
on Parsing Technologies (IWPT-05), Vancouver, BC,
Canada, October 2005.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, demonstration session, Prague, Czech Republic,
June 2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL-COLING 2006.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phraases. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2003. Final
Report of Johns Hopkins 2003 Summer Workshop on
Syntax for Statistical Machine Translation.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAL 1995, pages 1328-1334, Montreal, August.
511
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL
1996.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006, pages 521?528.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC 2004, pages 2051? 2054.
512
Proceedings of the 43rd Annual Meeting of the ACL, pages 459?466,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Log-linear Models for Word Alignment
Yang Liu , Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No. 6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu, liuqun, sxlin}@ict.ac.cn
Abstract
We present a framework for word align-
ment based on log-linear models. All
knowledge sources are treated as feature
functions, which depend on the source
langauge sentence, the target language
sentence and possible additional vari-
ables. Log-linear models allow statis-
tical alignment models to be easily ex-
tended by incorporating syntactic infor-
mation. In this paper, we use IBM Model
3 alignment probabilities, POS correspon-
dence, and bilingual dictionary cover-
age as features. Our experiments show
that log-linear models significantly out-
perform IBM translation models.
1 Introduction
Word alignment, which can be defined as an object
for indicating the corresponding words in a parallel
text, was first introduced as an intermediate result of
statistical translation models (Brown et al, 1993). In
statistical machine translation, word alignment plays
a crucial role as word-aligned corpora have been
found to be an excellent source of translation-related
knowledge.
Various methods have been proposed for finding
word alignments between parallel texts. There are
generally two categories of alignment approaches:
statistical approaches and heuristic approaches.
Statistical approaches, which depend on a set of
unknown parameters that are learned from training
data, try to describe the relationship between a bilin-
gual sentence pair (Brown et al, 1993; Vogel and
Ney, 1996). Heuristic approaches obtain word align-
ments by using various similarity functions between
the types of the two languages (Smadja et al, 1996;
Ker and Chang, 1997; Melamed, 2000). The cen-
tral distinction between statistical and heuristic ap-
proaches is that statistical approaches are based on
well-founded probabilistic models while heuristic
ones are not. Studies reveal that statistical alignment
models outperform the simple Dice coefficient (Och
and Ney, 2003).
Finding word alignments between parallel texts,
however, is still far from a trivial work due to the di-
versity of natural languages. For example, the align-
ment of words within idiomatic expressions, free
translations, and missing content or function words
is problematic. When two languages widely differ
in word order, finding word alignments is especially
hard. Therefore, it is necessary to incorporate all
useful linguistic information to alleviate these prob-
lems.
Tiedemann (2003) introduced a word alignment
approach based on combination of association clues.
Clues combination is done by disjunction of single
clues, which are defined as probabilities of associa-
tions. The crucial assumption of clue combination
that clues are independent of each other, however,
is not always true. Och and Ney (2003) proposed
Model 6, a log-linear combination of IBM transla-
tion models and HMM model. Although Model 6
yields better results than naive IBM models, it fails
to include dependencies other than IBM models and
HMM model. Cherry and Lin (2003) developed a
459
statistical model to find word alignments, which al-
low easy integration of context-specific features.
Log-linear models, which are very suitable to in-
corporate additional dependencies, have been suc-
cessfully applied to statistical machine translation
(Och and Ney, 2002). In this paper, we present a
framework for word alignment based on log-linear
models, allowing statistical models to be easily ex-
tended by incorporating additional syntactic depen-
dencies. We use IBM Model 3 alignment proba-
bilities, POS correspondence, and bilingual dictio-
nary coverage as features. Our experiments show
that log-linear models significantly outperform IBM
translation models.
We begin by describing log-linear models for
word alignment. The design of feature functions
is discussed then. Next, we present the training
method and the search algorithm for log-linear mod-
els. We will follow with our experimental results
and conclusion and close with a discussion of possi-
ble future directions.
2 Log-linear Models
Formally, we use following definition for alignment.
Given a source (?English?) sentence e = eI1 = e1,
. . . , ei, . . . , eI and a target language (?French?) sen-
tence f = fJ1 = f1, . . . , fj , . . . , fJ . We define a link
l = (i, j) to exist if ei and fj are translation (or part
of a translation) of one another. We define the null
link l = (i, 0) to exist if ei does not correspond to a
translation for any French word in f . The null link
l = (0, j) is defined similarly. An alignment a is
defined as a subset of the Cartesian product of the
word positions:
a ? {(i, j) : i = 0, . . . , I; j = 0, . . . , J} (1)
We define the alignment problem as finding the
alignment a that maximizes Pr(a | e, f ) given e and
f .
We directly model the probability Pr(a | e, f ).
An especially well-founded framework is maximum
entropy (Berger et al, 1996). In this framework, we
have a set of M feature functions hm(a, e, f), m =
1, . . . , M . For each feature function, there exists
a model parameter ?m, m = 1, . . . , M . The direct
alignment probability is given by:
Pr(a|e, f) = exp[
?M
m=1 ?mhm(a, e, f)]?
a? exp[
?M
m=1 ?mhm(a?, e, f)](2)
This approach has been suggested by (Papineni et
al., 1997) for a natural language understanding task
and successfully applied to statistical machine trans-
lation by (Och and Ney, 2002).
We obtain the following decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f)
}
(3)
Typically, the source language sentence e and the
target sentence f are the fundamental knowledge
sources for the task of finding word alignments. Lin-
guistic data, which can be used to identify associ-
ations between lexical items are often ignored by
traditional word alignment approaches. Linguistic
tools such as part-of-speech taggers, parsers, named-
entity recognizers have become more and more ro-
bust and available for many languages by now. It
is important to make use of linguistic information
to improve alignment strategies. Treated as feature
functions, syntactic dependencies can be easily in-
corporated into log-linear models.
In order to incorporate a new dependency which
contains extra information other than the bilingual
sentence pair, we modify Eq.2 by adding a new vari-
able v:
Pr(a|e, f ,v) = exp[
?M
m=1 ?mhm(a, e, f ,v)]?
a? exp[
?M
m=1 ?mhm(a?, e, f ,v)](4)
Accordingly, we get a new decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f ,v)
}
(5)
Note that our log-linear models are different from
Model 6 proposed by Och and Ney (2003), which
defines the alignment problem as finding the align-
ment a that maximizes Pr(f , a | e) given e.
3 Feature Functions
In this paper, we use IBM translation Model 3 as the
base feature of our log-linear models. In addition,
we also make use of syntactic information such as
part-of-speech tags and bilingual dictionaries.
460
3.1 IBM Translation Models
Brown et al (1993) proposed a series of statisti-
cal models of the translation process. IBM trans-
lation models try to model the translation probabil-
ity Pr(fJ1 |eI1), which describes the relationship be-
tween a source language sentence eI1 and a target
language sentence fJ1 . In statistical alignment mod-
els Pr(fJ1 , aJ1 |eI1), a ?hidden? alignment a = aJ1 is
introduced, which describes a mapping from a tar-
get position j to a source position i = aj . The
relationship between the translation model and the
alignment model is given by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (6)
Although IBM models are considered more co-
herent than heuristic models, they have two draw-
backs. First, IBM models are restricted in a way
such that each target word fj is assigned to exactly
one source word eaj . A more general way is to
model alignment as an arbitrary relation between
source and target language positions. Second, IBM
models are typically language-independent and may
fail to tackle problems occurred due to specific lan-
guages.
In this paper, we use Model 3 as our base feature
function, which is given by 1:
h(a, e, f) = Pr(fJ1 , aJ1 |eI1)
=
(
m? ?0
?0
)
p0m?2?0p1?0
l?
i=1
?i!n(?i|ei)?
m?
j=1
t(fj |eaj )d(j|aj , l,m) (7)
We distinguish between two translation directions
to use Model 3 as feature functions: treating English
as source language and French as target language or
vice versa.
3.2 POS Tags Transition Model
The first linguistic information we adopt other than
the source language sentence e and the target lan-
guage sentence f is part-of-speech tags. The use
of POS information for improving statistical align-
ment quality of the HMM-based model is described
1If there is a target word which is assigned to more than one
source words, h(a, e, f) = 0.
in (Toutanova et al, 2002). They introduce addi-
tional lexicon probability for POS tags in both lan-
guages.
In IBM models as well as HMM models, when
one needs the model to take new information into
account, one must create an extended model which
can base its parameters on the previous model. In
log-linear models, however, new information can be
easily incorporated.
We use a POS Tags Transition Model as a fea-
ture function. This feature learns POS Tags tran-
sition probabilities from held-out data (via simple
counting) and then applies the learned distributions
to the ranking of various word alignments. We
define eT = eT I1 = eT1, . . . , eTi, . . . , eTI and
fT = fT J1 = fT1, . . . , fTj , . . . , fTJ as POS tag
sequences of the sentence pair e and f . POS Tags
Transition Model is formally described as:
Pr(fT|a, eT) =
?
a
t(fTa(j)|eTa(i)) (8)
where a is an element of a, a(i) is the corresponding
source position of a and a(j) is the target position.
Hence, the feature function is:
h(a, e, f , eT, fT) =
?
a
t(fTa(j)|eTa(i)) (9)
We still distinguish between two translation direc-
tions to use POS tags Transition Model as feature
functions: treating English as source language and
French as target language or vice versa.
3.3 Bilingual Dictionary
A conventional bilingual dictionary can be consid-
ered an additional knowledge source. We could use
a feature that counts how many entries of a conven-
tional lexicon co-occur in a given alignment between
the source sentence and the target sentence. There-
fore, the weight for the provided conventional dic-
tionary can be learned. The intuition is that the con-
ventional dictionary is expected to be more reliable
than the automatically trained lexicon and therefore
should get a larger weight.
We define a bilingual dictionary as a set of entries:
D = {(e, f, conf)}. e is a source language word,
f is a target langauge word, and conf is a positive
real-valued number (usually, conf = 1.0) assigned
461
by lexicographers to evaluate the validity of the en-
try. Therefore, the feature function using a bilingual
dictionary is:
h(a, e, f ,D) =
?
a
occur(ea(i), fa(j), D) (10)
where
occur(e, f,D) =
{
conf if (e, f) occurs in D
0 else
(11)
4 Training
We use the GIS (Generalized Iterative Scaling) al-
gorithm (Darroch and Ratcliff, 1972) to train the
model parameters ?M1 of the log-linear models ac-
cording to Eq. 4. By applying suitable transforma-
tions, the GIS algorithm is able to handle any type of
real-valued features. In practice, We use YASMET
2 written by Franz J. Och for performing training.
The renormalization needed in Eq. 4 requires a
sum over a large number of possible alignments. If
e has length l and f has length m, there are pos-
sible 2lm alignments between e and f (Brown et
al., 1993). It is unrealistic to enumerate all possi-
ble alignments when lm is very large. Hence, we
approximate this sum by sampling the space of all
possible alignments by a large set of highly proba-
ble alignments. The set of considered alignments are
also called n-best list of alignments.
We train model parameters on a development cor-
pus, which consists of hundreds of manually-aligned
bilingual sentence pairs. Using an n-best approx-
imation may result in the problem that the param-
eters trained with the GIS algorithm yield worse
alignments even on the development corpus. This
can happen because with the modified model scaling
factors the n-best list can change significantly and
can include alignments that have not been taken into
account in training. To avoid this problem, we iter-
atively combine n-best lists to train model parame-
ters until the resulting n-best list does not change,
as suggested by Och (2002). However, as this train-
ing procedure is based on maximum likelihood cri-
terion, there is only a loose relation to the final align-
ment quality on unseen bilingual texts. In practice,
2Available at http://www.fjoch.com/YASMET.html
having a series of model parameters when the itera-
tion ends, we select the model parameters that yield
best alignments on the development corpus.
After the bilingual sentences in the develop-
ment corpus are tokenized (or segmented) and POS
tagged, they can be used to train POS tags transition
probabilities by counting relative frequencies:
p(fT |eT ) = NA(fT, eT )N(eT )
Here, NA(fT, eT ) is the frequency that the POS tag
fT is aligned to POS tag eT and N(eT ) is the fre-
quency of eT in the development corpus.
5 Search
We use a greedy search algorithm to search the
alignment with highest probability in the space of all
possible alignments. A state in this space is a partial
alignment. A transition is defined as the addition of
a single link to the current state. Our start state is
the empty alignment, where all words in e and f are
assigned to null. A terminal state is a state in which
no more links can be added to increase the probabil-
ity of the current alignment. Our task is to find the
terminal state with the highest probability.
We can compute gain, which is a heuristic func-
tion, instead of probability for efficiency. A gain is
defined as follows:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
(12)
where l = (i, j) is a link added to a.
The greedy search algorithm for general log-
linear models is formally described as follows:
Input: e, f , eT, fT, and D
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3. Terminate if ?l, gain(a, l) ? 1.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
462
The above search algorithm, however, is not effi-
cient for our log-linear models. It is time-consuming
for each feature to figure out a probability when
adding a new link, especially when the sentences
are very long. For our models, gain(a, l) can be
obtained in a more efficient way 3:
gain(a, l) =
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
(13)
Note that we restrict that h(a, e, f) ? 0 for all fea-
ture functions.
The original terminational condition for greedy
search algorithm is:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
? 1.0
That is:
M?
m=1
?m[hm(a ? l, e, f)? hm(a, e, f)] ? 0.0
By introducing gain threshold t, we obtain a new
terminational condition:
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
? t
where
t =
M?
m=1
?m
{
log
(hm(a ? l, e, f)
hm(a, e, f)
)
?[hm(a ? l, e, f)? hm(a, e, f)]
}
Note that we restrict h(a, e, f) ? 0 for all feature
functions. Gain threshold t is a real-valued number,
which can be optimized on the development corpus.
Therefore, we have a new search algorithm:
Input: e, f , eT, fT, D and t
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3We still call the new heuristic function gain to reduce no-
tational overhead, although the gain in Eq. 13 is not equivalent
to the one in Eq. 12.
3. Terminate if ?l, gain(a, l) ? t.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
The gain threshold t depends on the added link
l. We remove this dependency for simplicity when
using it in search algorithm by treating it as a fixed
real-valued number.
6 Experimental Results
We present in this section results of experiments on
a parallel corpus of Chinese-English texts. Statis-
tics for the corpus are shown in Table 1. We use a
training corpus, which is used to train IBM transla-
tion models, a bilingual dictionary, a development
corpus, and a test corpus.
Chinese English
Train Sentences 108 925
Words 3 784 106 3 862 637
Vocabulary 49 962 55 698
Dict Entries 415 753
Vocabulary 206 616 203 497
Dev Sentences 435
Words 11 462 14 252
Ave. SentLen 26.35 32.76
Test Sentences 500
Words 13 891 15 291
Ave. SentLen 27.78 30.58
Table 1. Statistics of training corpus (Train), bilin-
gual dictionary (Dict), development corpus (Dev),
and test corpus (Test).
The Chinese sentences in both the development
and test corpus are segmented and POS tagged by
ICTCLAS (Zhang et al, 2003). The English sen-
tences are tokenized by a simple tokenizer of ours
and POS tagged by a rule-based tagger written by
Eric Brill (Brill, 1995). We manually aligned 935
sentences, in which we selected 500 sentences as
test corpus. The remaining 435 sentences are used
as development corpus to train POS tags transition
probabilities and to optimize the model parameters
and gain threshold.
Provided with human-annotated word-level align-
ment, we use precision, recall and AER (Och and
463
Size of Training Corpus
1K 5K 9K 39K 109K
Model 3 E ? C 0.4497 0.4081 0.4009 0.3791 0.3745
Model 3 C ? E 0.4688 0.4261 0.4221 0.3856 0.3469
Intersection 0.4588 0.4106 0.4044 0.3823 0.3687
Union 0.4596 0.4210 0.4157 0.3824 0.3703
Refined Method 0.4154 0.3586 0.3499 0.3153 0.3068
Model 3 E ? C 0.4490 0.3987 0.3834 0.3639 0.3533
+ Model 3 C ? E 0.3970 0.3317 0.3217 0.2949 0.2850
+ POS E ? C 0.3828 0.3182 0.3082 0.2838 0.2739
+ POS C ? E 0.3795 0.3160 0.3032 0.2821 0.2726
+ Dict 0.3650 0.3092 0.2982 0.2738 0.2685
Table 2. Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.
Ney, 2003) for scoring the viterbi alignments of each
model against gold-standard annotated alignments:
precision = |A ? P ||A|
recall = |A ? S||S|
AER = 1? |A ? S|+ |A ? P ||A|+ |S|
where A is the set of word pairs aligned by word
alignment systems, S is the set marked in the gold
standard as ?sure? and P is the set marked as ?pos-
sible? (including the ?sure? pairs). In our Chinese-
English corpus, only one type of alignment was
marked, meaning that S = P .
In the following, we present the results of log-
linear models for word alignment. We used GIZA++
package (Och and Ney, 2003) to train IBM transla-
tion models. The training scheme is 15H535, which
means that Model 1 are trained for five iterations,
HMM model for five iterations and finally Model
3 for five iterations. Except for changing the iter-
ations for each model, we use default configuration
of GIZA++. After that, we used three types of meth-
ods for performing a symmetrization of IBM mod-
els: intersection, union, and refined methods (Och
and Ney , 2003).
The base feature of our log-linear models, IBM
Model 3, takes the parameters generated by GIZA++
as parameters for itself. In other words, our log-
linear models share GIZA++ with the same parame-
ters apart from POS transition probability table and
bilingual dictionary.
Table 2 compares the results of our log-linear
models with IBM Model 3. From row 3 to row 7
are results obtained by IBM Model 3. From row 8
to row 12 are results obtained by log-linear models.
As shown in Table 2, our log-linear models
achieve better results than IBM Model 3 in all train-
ing corpus sizes. Considering Model 3 E ? C of
GIZA++ and ours alone, greedy search algorithm
described in Section 5 yields surprisingly better
alignments than hillclimbing algorithm in GIZA++.
Table 3 compares the results of log-linear mod-
els with IBM Model 5. The training scheme is
15H5354555. Our log-linear models still make use
of the parameters generated by GIZA++.
Comparing Table 3 with Table 2, we notice that
our log-linear models yield slightly better align-
ments by employing parameters generated by the
training scheme 15H5354555 rather than 15H535,
which can be attributed to improvement of param-
eters after further Model 4 and Model 5 training.
For log-linear models, POS information and an
additional dictionary are used, which is not the case
for GIZA++/IBM models. However, treated as a
method for performing symmetrization, log-linear
combination alone yields better results than intersec-
tion, union, and refined methods.
Figure 1 shows how gain threshold has an effect
on precision, recall and AER with fixed model scal-
ing factors.
Figure 2 shows the effect of number of features
464
Size of Training Corpus
1K 5K 9K 39K 109K
Model 5 E ? C 0.4384 0.3934 0.3853 0.3573 0.3429
Model 5 C ? E 0.4564 0.4067 0.3900 0.3423 0.3239
Intersection 0.4432 0.3916 0.3798 0.3466 0.3267
Union 0.4499 0.4051 0.3923 0.3516 0.3375
Refined Method 0.4106 0.3446 0.3262 0.2878 0.2748
Model 3 E ? C 0.4372 0.3873 0.3724 0.3456 0.3334
+ Model 3 C ? E 0.3920 0.3269 0.3167 0.2842 0.2727
+ POS E ? C 0.3807 0.3122 0.3039 0.2732 0.2667
+ POS C ? E 0.3731 0.3091 0.3017 0.2722 0.2657
+ Dict 0.3612 0.3046 0.2943 0.2658 0.2625
Table 3. Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
gain threshold
 Precision
 Recall
 AER
Figure 1. Precision, recall and AER over different
gain thresholds with the same model scaling factors.
and size of training corpus on search efficiency for
log-linear models.
Table 4 shows the resulting normalized model
scaling factors. We see that adding new features also
has an effect on the other model scaling factors.
7 Conclusion
We have presented a framework for word alignment
based on log-linear models between parallel texts. It
allows statistical models easily extended by incor-
porating syntactic information. We take IBM Model
3 as base feature and use syntactic information such
as POS tags and bilingual dictionary. Experimental
1k 5k 9k 39k 109k
200
400
600
800
1000
1200
t
i
m
e
 
c
o
n
s
u
m
e
d
 
f
o
r
 
s
e
a
r
c
h
i
n
g
 
(
s
e
c
o
n
d
)
size of training corpus
 M3EC
 M3EC + M3CE
 M3EC + M3CE + POSEC
 M3EC + M3CE + POSEC + POSCE
 M3EC + M3CE + POSEC + POSCE + Dict
Figure 2. Effect of number of features and size of
training corpus on search efficiency.
MEC +MCE +PEC +PCE +Dict
?1 1.000 0.466 0.291 0.202 0.151
?2 - 0.534 0.312 0.212 0.167
?3 - - 0.397 0.270 0.257
?4 - - - 0.316 0.306
?5 - - - - 0.119
Table 4. Resulting model scaling factors: ?1: Model
3 E ? C (MEC); ?2: Model 3 C ? E (MCE); ?3:
POS E ? C (PEC); ?4: POS C ? E (PCE); ?5: Dict
(normalized such that ?5m=1 ?m = 1).
results show that log-linear models for word align-
ment significantly outperform IBM translation mod-
els. However, the search algorithm we proposed is
465
supervised, relying on a hand-aligned bilingual cor-
pus, while the baseline approach of IBM alignments
is unsupervised.
Currently, we only employ three types of knowl-
edge sources as feature functions. Syntax-based
translation models, such as tree-to-string model (Ya-
mada and Knight, 2001) and tree-to-tree model
(Gildea, 2003), may be very suitable to be added into
log-linear models.
It is promising to optimize the model parameters
directly with respect to AER as suggested in statisti-
cal machine translation (Och, 2003).
Acknowledgement
This work is supported by National High Technol-
ogy Research and Development Program contract
?Generally Technical Research and Basic Database
Establishment of Chinese Platform? (Subject No.
2004AA114010).
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
DellaPietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39-72, March.
Eric Brill. 1995. Transformation-based-error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4), December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470-1480.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), Sapporo, Japan.
Sue J. Ker and Jason S. Chang. 1997. A class-based ap-
proach to word alignment. Computational Linguistics,
23(2):313-343, June.
I. Dan Melamed 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221-249, June.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295-302, Philadelphia, PA,
July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages: 160-167, Sapporo, Japan.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19-51, March.
Kishore A. Papineni, Salim Roukos, and Todd Ward.
1997. Feature-based language understanding. In Eu-
ropean Conf. on Speech Communication and Technol-
ogy, pages 1435-1438, Rhodes, Greece, September.
Frank Smadja, Vasileios Hatzivassiloglou, and Kathleen
R. McKeown 1996. Translating collocations for bilin-
gual lexicons: A statistical approach. Computational
Linguistics, 22(1):1-38, March.
Jo?rg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of the 10th Conference of Euro-
pean Chapter of the ACL (EACL), Budapest, Hungary,
April.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2003. Extensions to HMM-based statistical
word alignment models. In Proceedings of Empirical
Methods in Natural Langauge Processing, Philadel-
phia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on Com-
putational Linguistics, pages 836-841, Copenhagen,
Denmark, August.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages: 523-530,
Toulouse, France, July.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the second SigHan Work-
shop affiliated with 41th ACL, pages: 184-187, Sap-
poro, Japan.
466
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521?528,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Based Phrase Reordering
Model for Statistical Machine Translation
Deyi Xiong
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
Graduate School of Chinese Academy of Sciences
dyxiong@ict.ac.cn
Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{liuqun, sxlin}@ict.ac.cn
Abstract
We propose a novel reordering model for
phrase-based statistical machine transla-
tion (SMT) that uses a maximum entropy
(MaxEnt) model to predicate reorderings
of neighbor blocks (phrase pairs). The
model provides content-dependent, hier-
archical phrasal reordering with general-
ization based on features automatically
learned from a real-world bitext. We
present an algorithm to extract all reorder-
ing events of neighbor blocks from bilin-
gual data. In our experiments on Chinese-
to-English translation, this MaxEnt-based
reordering model obtains significant im-
provements in BLEU score on the NIST
MT-05 and IWSLT-04 tasks.
1 Introduction
Phrase reordering is of great importance for
phrase-based SMT systems and becoming an ac-
tive area of research recently. Compared with
word-based SMT systems, phrase-based systems
can easily address reorderings of words within
phrases. However, at the phrase level, reordering
is still a computationally expensive problem just
like reordering at the word level (Knight, 1999).
Many systems use very simple models to re-
order phrases 1. One is distortion model (Och
and Ney, 2004; Koehn et al, 2003) which penal-
izes translations according to their jump distance
instead of their content. For example, if N words
are skipped, a penalty of N will be paid regard-
less of which words are reordered. This model
takes the risk of penalizing long distance jumps
1In this paper, we focus our discussions on phrases that
are not necessarily aligned to syntactic constituent boundary.
which are common between two languages with
very different orders. Another simple model is flat
reordering model (Wu, 1996; Zens et al, 2004;
Kumar et al, 2005) which is not content depen-
dent either. Flat model assigns constant probabili-
ties for monotone order and non-monotone order.
The two probabilities can be set to prefer mono-
tone or non-monotone orientations depending on
the language pairs.
In view of content-independency of the dis-
tortion and flat reordering models, several re-
searchers (Och et al, 2004; Tillmann, 2004; Ku-
mar et al, 2005; Koehn et al, 2005) proposed a
more powerful model called lexicalized reorder-
ing model that is phrase dependent. Lexicalized
reordering model learns local orientations (mono-
tone or non-monotone) with probabilities for each
bilingual phrase from training data. During de-
coding, the model attempts to finding a Viterbi lo-
cal orientation sequence. Performance gains have
been reported for systems with lexicalized reorder-
ing model. However, since reorderings are re-
lated to concrete phrases, researchers have to de-
sign their systems carefully in order not to cause
other problems, e.g. the data sparseness problem.
Another smart reordering model was proposed
by Chiang (2005). In his approach, phrases are re-
organized into hierarchical ones by reducing sub-
phrases to variables. This template-based scheme
not only captures the reorderings of phrases, but
also integrates some phrasal generalizations into
the global model.
In this paper, we propose a novel solution for
phrasal reordering. Here, under the ITG constraint
(Wu, 1997; Zens et al, 2004), we need to con-
sider just two kinds of reorderings, straight and
inverted between two consecutive blocks. There-
fore reordering can be modelled as a problem of
521
classification with only two labels, straight and
inverted. In this paper, we build a maximum en-
tropy based classification model as the reordering
model. Different from lexicalized reordering, we
do not use the whole block as reordering evidence,
but only features extracted from blocks. This is
more flexible. It makes our model reorder any
blocks, observed in training or not. The whole
maximum entropy based reordering model is em-
bedded inside a log-linear phrase-based model of
translation. Following the Bracketing Transduc-
tion Grammar (BTG) (Wu, 1996), we built a
CKY-style decoder for our system, which makes
it possible to reorder phrases hierarchically.
To create a maximum entropy based reordering
model, the first step is learning reordering exam-
ples from training data, similar to the lexicalized
reordering model. But in our way, any evidences
of reorderings will be extracted, not limited to re-
orderings of bilingual phrases of length less than a
predefined number of words. Secondly, features
will be extracted from reordering examples ac-
cording to feature templates. Finally, a maximum
entropy classifier will be trained on the features.
In this paper we describe our system and the
MaxEnt-based reordering model with the associ-
ated algorithm. We also present experiments that
indicate that the MaxEnt-based reordering model
improves translation significantly compared with
other reordering approaches and a state-of-the-art
distortion-based system (Koehn, 2004).
2 System Overview
2.1 Model
Under the BTG scheme, translation is more
like monolingual parsing through derivations.
Throughout the translation procedure, three rules
are used to derive the translation
A [ ]? (A1, A2) (1)
A ? ?? (A1, A2) (2)
A ? (x, y) (3)
During decoding, the source sentence is seg-
mented into a sequence of phrases as in a standard
phrase-based model. Then the lexical rule (3) 2 is
2Currently, we restrict phrases x and y not to be null.
Therefore neither deletion nor insertion is carried out during
decoding. However, these operations are to be considered in
our future version of model.
used to translate source phrase y into target phrase
x and generate a block A. Later, the straight rule
(1) merges two consecutive blocks into a single
larger block in the straight order; while the in-
verted rule (2) merges them in the inverted order.
These two merging rules will be used continuously
until the whole source sentence is covered. When
the translation is finished, a tree indicating the hi-
erarchical segmentation of the source sentence is
also produced.
In the following, we will define the model in
a straight way, not in the dynamic programming
recursion way used by (Wu, 1996; Zens et al,
2004). We focus on defining the probabilities of
different rules by separating different features (in-
cluding the language model) out from the rule
probabilities and organizing them in a log-linear
form. This straight way makes it clear how rules
are used and what they depend on.
For the two merging rules straight and inverted,
applying them on two consecutive blocks A1 and
A2 is assigned a probability Prm(A)
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where the ? is the reordering score of block A1
and A2, ?? is its weight, and 4pLM (A1,A2) is the
increment of the language model score of the two
blocks according to their final order, ?LM is its
weight.
For the lexical rule, applying it is assigned a
probability Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|x|)?6
?p?LMLM (x) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical transla-
tion probabilities in both directions, and exp(1)
and exp(|x|) are the phrase penalty and word
penalty, respectively. These features are very com-
mon in state-of-the-art systems (Koehn et al,
2005; Chiang, 2005) and ?s are weights of fea-
tures.
For the reordering model ?, we define it on the
two consecutive blocks A1 and A2 and their order
o ? {straight, inverted}
? = f(o,A1, A2) (6)
Under this framework, different reordering mod-
els can be designed. In fact, we defined four re-
ordering models in our experiments. The first one
522
is NONE, meaning no explicit reordering features
at all. We set ? to 1 for all different pairs of
blocks and their orders. So the phrasal reorder-
ing is totally dependent on the language model.
This model is obviously different from the mono-
tone search, which does not use the inverted rule at
all. The second one is a distortion style reordering
model, which is formulated as
? =
{
exp(0), o = straight
exp(|A1|) + (|A2|), o = inverted
where |Ai| denotes the number of words on the
source side of blocks. When ?? < 0, this de-
sign will penalize those non-monotone transla-
tions. The third one is a flat reordering model,
which assigns probabilities for the straight and in-
verted order. It is formulated as
? =
{
pm, o = straight
1? pm, o = inverted
In our experiments on Chinese-English tasks, the
probability for the straight order is set at pm =
0.95. This is because word order in Chinese and
English is usually similar. The last one is the maxi-
mum entropy based reordering model proposed by
us, which will be described in the next section.
We define a derivation D as a sequence of appli-
cations of rules (1) ? (3), and let c(D) and e(D)
be the Chinese and English yields of D. The prob-
ability of a derivation D is
Pr(D) =
?
i
Pr(i) (7)
where Pr(i) is the probability of the ith applica-
tion of rules. Given an input sentence c, the final
translation e? is derived from the best derivation
D?
D? = argmax
c(D)=c
Pr(D)
e? = e(D?) (8)
2.2 Decoder
We developed a CKY style decoder that employs a
beam search algorithm, similar to the one by Chi-
ang (2005). The decoder finds the best derivation
that generates the input sentence and its transla-
tion. From the best derivation, the best English e?
is produced.
Given a source sentence c, firstly we initiate the
chart with phrases from phrase translation table
by applying the lexical rule. Then for each cell
that spans from i to j on the source side, all pos-
sible derivations spanning from i to j are gener-
ated. Our algorithm guarantees that any sub-cells
within (i, j) have been expanded before cell (i, j)
is expanded. Therefore the way to generate deriva-
tions in cell (i, j) is to merge derivations from
any two neighbor sub-cells. This combination is
done by applying the straight and inverted rules.
Each application of these two rules will generate
a new derivation covering cell (i, j). The score of
the new generated derivation is derived from the
scores of its two sub-derivations, reordering model
score and the increment of the language model
score according to the Equation (4). When the
whole input sentence is covered, the decoding is
over.
Pruning of the search space is very important for
the decoder. We use three pruning ways. The first
one is recombination. When two derivations in
the same cell have the same w leftmost/rightmost
words on the English yields, where w depends on
the order of the language model, they will be re-
combined by discarding the derivation with lower
score. The second one is the threshold pruning
which discards derivations that have a score worse
than ? times the best score in the same cell. The
last one is the histogram pruning which only keeps
the top n best derivations for each cell. In all our
experiments, we set n = 40, ? = 0.5 to get a
tradeoff between speed and performance in the de-
velopment set.
Another feature of our decoder is the k-best list
generation. The k-best list is very important for
the minimum error rate training (Och, 2003a)
which is used for tuning the weights ? for our
model. We use a very lazy algorithm for the k-best
list generation, which runs two phases similarly to
the one by Huang et al (2005). In the first phase,
the decoder runs as usual except that it keeps some
information of weaker derivations which are to be
discarded during recombination. This will gener-
ate not only the first-best of final derivation but
also a shared forest. In the second phase, the
lazy algorithm runs recursively on the shared for-
est. It finds the second-best of the final deriva-
tion, which makes its children to find their second-
best, and children?s children?s second-best, until
the leaf node?s second-best. Then it finds the third-
best, forth-best, and so on. In all our experiments,
we set k = 200.
523
The decoder is implemented in C++. Using the
pruning settings described above, without the k-
best list generation, it takes about 6 seconds to
translate a sentence of average length 28.3 words
on a 2GHz Linux system with 4G RAM memory.
3 Maximum Entropy Based Reordering
Model
In this section, we discuss how to create a max-
imum entropy based reordering model. As de-
scribed above, we defined the reordering model ?
on the three factors: order o, block A1 and block
A2. The central problem is, given two neighbor
blocks A1 and A2, how to predicate their order
o ? {straight, inverted}. This is a typical prob-
lem of two-class classification. To be consistent
with the whole model, the conditional probabil-
ity p(o|A1, A2) is calculated. A simple way to
compute this probability is to take counts from the
training data and then to use the maximum likeli-
hood estimate (MLE)
p(o|A1, A2) = Count(o,A
1, A2)
Count(A1, A2) (9)
The similar way is used by lexicalized reordering
model. However, in our model this way can?t work
because blocks become larger and larger due to us-
ing the merging rules, and finally unseen in the
training data. This means we can not use blocks
as direct reordering evidences.
A good way to this problem is to use features of
blocks as reordering evidences. Good features can
not only capture reorderings, avoid sparseness, but
also integrate generalizations. It is very straight
to use maximum entropy model to integrate fea-
tures to predicate reorderings of blocks. Under the
MaxEnt model, we have
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(10)
where the functions hi ? {0, 1} are model features
and the ?i are weights of the model features which
can be trained by different algorithms (Malouf,
2002).
3.1 Reordering Example Extraction
Algorithm
The input for the algorithm is a bilingual corpus
with high-precision word alignments. We obtain
the word alignments using the way of Koehn et al
(2005). After running GIZA++ (Och and Ney,
target
source
b1
b2
b3
b4
c1
c2
Figure 1: The bold dots are corners. The ar-
rows from the corners are their links. Corner c1 is
shared by block b1 and b2, which in turn are linked
by the STRAIGHT links, bottomleft and topright
of c1. Similarly, block b3 and b4 are linked by the
INVERTED links, topleft and bottomright of c2.
2000) in both directions, we apply the ?grow-
diag-final? refinement rule on the intersection
alignments for each sentence pair.
Before we introduce this algorithm, we intro-
duce some formal definitions. The first one is
block which is a pair of source and target contigu-
ous sequences of words
b = (si2i1 , t
j2
j1)
b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2
This definition is similar to that of bilingual phrase
except that there is no length limitation over block.
A reordering example is a triple of (o, b1, b2)
where b1 and b2 are two neighbor blocks and o
is the order between them. We define each vertex
of block as corner. Each corner has four links in
four directions: topright, topleft, bottomright, bot-
tomleft, and each link links a set of blocks which
have the corner as their vertex. The topright and
bottomleft link blocks with the straight order, so
we call them STRAIGHT links. Similarly, we call
the topleft and bottomright INVERTED links since
they link blocks with the inverted order. For con-
venience, we use b ?? L to denote that block b
is linked by the link L. Note that the STRAIGHT
links can not coexist with the INVERTED links.
These definitions are illustrated in Figure 1.
The reordering example extraction algorithm is
shown in Figure 2. The basic idea behind this al-
gorithm is to register all neighbor blocks to the
associated links of corners which are shared by
them. To do this, we keep an array to record link
524
1: Input: sentence pair (s, t) and their alignment M
2: < := ?
3: for each span (i1, i2) ? s do
4: find block b = (si2i1 , t
j2
j1) that is consistent with M
5: Extend block b on the target boundary with one possi-
ble non-aligned word to get blocks E(b)
6: for each block b? ? b?E(b) do
7: Register b? to the links of four corners of it
8: end for
9: end for
10: for each corner C in the matrix M do
11: if STRAIGHT links exist then
12: < := <?{(straight, b1, b2)},
b1 ?? C.bottomleft, b2 ?? C.topright
13: else if INVERTED links exist then
14: < := <?{(inverted, b1, b2)},
b1 ?? C.topleft, b2 ?? C.bottomright
15: end if
16: end for
17: Output: reordering examples <
Figure 2: Reordering Example Extraction Algo-
rithm.
information of corners when extracting blocks.
Line 4 and 5 are similar to the phrase extraction
algorithm by Och (2003b). Different from Och,
we just extend one word which is aligned to null
on the boundary of target side. If we put some
length limitation over the extracted blocks and out-
put them, we get bilingual phrases used in standard
phrase-based SMT systems and also in our sys-
tem. Line 7 updates all links associated with the
current block. You can attach the current block
to each of these links. However this will increase
reordering examples greatly, especially those with
the straight order. In our Experiments, we just at-
tach the smallest blocks to the STRAIGHT links,
and the largest blocks to the INVERTED links.
This will keep the number of reordering examples
acceptable but without performance degradation.
Line 12 and 14 extract reordering examples.
3.2 Features
With the extracted reordering examples, we can
obtain features for our MaxEnt-based reordering
model. We design two kinds of features, lexi-
cal features and collocation features. For a block
b = (s, t), we use s1 to denote the first word of the
source s, t1 to denote the first word of the target t.
Lexical features are defined on the single word
s1 or t1. Collocation features are defined on the
combination s1 or t1 between two blocks b1 and
b2. Three kinds of combinations are used. The first
one is source collocation, b1.s1&b2.s1. The sec-
ond is target collocation, b1.t1&b2.t1. The last one
hi(o, b1, b2) =
{ 1, b1.t1 = E1, o = O
0, otherwise
hj(o, b1, b2) =
{ 1, b1.t1 = E1, b2.t1 = E2, o = O
0, otherwise
Figure 3: MaxEnt-based reordering feature tem-
plates. The first one is a lexical feature, and the
second one is a target collocation feature, where
Ei are English words, O ? {straight, inverted}.
is block collocation, b1.s1&b1.t1 and b2.s1&b2.t1.
The templates for the lexical feature and the collo-
cation feature are shown in Figure 3.
Why do we use the first words as features?
These words are nicely at the boundary of blocks.
One of assumptions of phrase-based SMT is that
phrase cohere across two languages (Fox, 2002),
which means phrases in one language tend to be
moved together during translation. This indicates
that boundary words of blocks may keep informa-
tion for their movements/reorderings. To test this
hypothesis, we calculate the information gain ra-
tio (IGR) for boundary words as well as the whole
blocks against the order on the reordering exam-
ples extracted by the algorithm described above.
The IGR is the measure used in the decision tree
learning to select features (Quinlan, 1993). It
represents how precisely the feature predicate the
class. For feature f and class c, the IGR(f, c)
IGR(f, c) = En(c)? En(c|f)En(f) (11)
where En(?) is the entropy and En(?|?)
is the conditional entropy. To our sur-
prise, the IGR for the four boundary words
(IGR(?b1.s1, b2.s1, b1.t1, b2.t1?, order) =
0.2637) is very close to that for the two blocks
together (IGR(?b1, b2?, order) = 0.2655).
Although our reordering examples do not cover
all reordering events in the training data, this
result shows that boundary words do provide
some clues for predicating reorderings.
4 Experiments
We carried out experiments to compare against
various reordering models and systems to demon-
strate the competitiveness of MaxEnt-based re-
ordering:
1. Monotone search: the inverted rule is not
used.
525
2. Reordering variants: the NONE, distortion
and flat reordering models described in Sec-
tion 2.1.
3. Pharaoh: A state-of-the-art distortion-based
decoder (Koehn, 2004).
4.1 Corpus
Our experiments were made on two Chinese-to-
English translation tasks: NIST MT-05 (news do-
main) and IWSLT-04 (travel dialogue domain).
NIST MT-05. In this task, the bilingual train-
ing data comes from the FBIS corpus with 7.06M
Chinese words and 9.15M English words. The tri-
gram language model training data consists of En-
glish texts mostly derived from the English side
of the UN corpus (catalog number LDC2004E12),
which totally contains 81M English words. For the
efficiency of minimum error rate training, we built
our development set using sentences of length at
most 50 characters from the NIST MT-02 evalua-
tion test data.
IWSLT-04. For this task, our experiments were
carried out on the small data track. Both the
bilingual training data and the trigram language
model training data are restricted to the supplied
corpus, which contains 20k sentences, 179k Chi-
nese words and 157k English words. We used the
CSTAR 2003 test set consisting of 506 sentence
pairs as development set.
4.2 Training
We obtained high-precision word alignments us-
ing the way described in Section 3.1. Then we
ran our reordering example extraction algorithm to
output blocks of length at most 7 words on the Chi-
nese side together with their internal alignments.
We also limited the length ratio between the target
and source language (max(|s|, |t|)/min(|s|, |t|))
to 3. After extracting phrases, we calculated the
phrase translation probabilities and lexical transla-
tion probabilities in both directions for each bilin-
gual phrase.
For the minimum-error-rate training, we re-
implemented Venugopal?s trainer 3 (Venugopal
et al, 2005) in C++. For all experiments, we ran
this trainer with the decoder iteratively to tune the
weights ?s to maximize the BLEU score on the
development set.
3See http://www.cs.cmu.edu/ ashishv/mer.html. This is a
Matlab implementation.
Pharaoh
We shared the same phrase translation tables
between Pharaoh and our system since the two
systems use the same features of phrases. In fact,
we extracted more phrases than Pharaoh?s trainer
with its default settings. And we also used our re-
implemented trainer to tune lambdas of Pharaoh
to maximize its BLEU score. During decoding,
we pruned the phrase table with b = 100 (default
20), pruned the chart with n = 100, ? = 10?5
(default setting), and limited distortions to 4
(default 0).
MaxEnt-based Reordering Model
We firstly ran our reordering example extraction
algorithm on the bilingual training data without
any length limitations to obtain reordering ex-
amples and then extracted features from these
examples. In the task of NIST MT-05, we
obtained about 2.7M reordering examples with
the straight order, and 367K with the inverted
order, from which 112K lexical features and
1.7M collocation features after deleting those
with one occurrence were extracted. In the task
of IWSLT-04, we obtained 79.5k reordering
examples with the straight order, 9.3k with the
inverted order, from which 16.9K lexical features
and 89.6K collocation features after deleting those
with one occurrence were extracted. Finally, we
ran the MaxEnt toolkit by Zhang 4 to tune the
feature weights. We set iteration number to 100
and Gaussian prior to 1 for avoiding overfitting.
4.3 Results
We dropped unknown words (Koehn et al, 2005)
of translations for both tasks before evaluating
their BLEU scores. To be consistent with the
official evaluation criterions of both tasks, case-
sensitive BLEU-4 scores were computed For the
NIST MT-05 task and case-insensitive BLEU-4
scores were computed for the IWSLT-04 task 5.
Experimental results on both tasks are shown in
Table 1. Italic numbers refer to results for which
the difference to the best result (indicated in bold)
is not statistically significant. For all scores, we
also show the 95% confidence intervals computed
using Zhang?s significant tester (Zhang et al,
2004) which was modified to conform to NIST?s
4See http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
5Note that the evaluation criterion of IWSLT-04 is not to-
tally matched since we didn?t remove punctuation marks.
526
definition of the BLEU brevity penalty.
We observe that if phrasal reordering is totally
dependent on the language model (NONE) we
get the worst performance, even worse than the
monotone search. This indicates that our language
models were not strong to discriminate between
straight orders and inverted orders. The flat and
distortion reordering models (Row 3 and 4) show
similar performance with Pharaoh. Although they
are not dependent on phrases, they really reorder
phrases with penalties to wrong orders supported
by the language model and therefore outperform
the monotone search. In row 6, only lexical fea-
tures are used for the MaxEnt-based reordering
model; while row 7 uses lexical features and col-
location features. On both tasks, we observe that
various reordering approaches show similar and
stable performance ranks in different domains and
the MaxEnt-based reordering models achieve the
best performance among them. Using all features
for the MaxEnt model (lex + col) is marginally
better than using only lex features (lex).
4.4 Scaling to Large Bitexts
In the experiments described above, collocation
features do not make great contributions to the per-
formance improvement but make the total num-
ber of features increase greatly. This is a prob-
lem for MaxEnt parameter estimation if it is scaled
to large bitexts. Therefore, for the integration of
MaxEnt-based phrase reordering model in the sys-
tem trained on large bitexts, we remove colloca-
tion features and only use lexical features from
the last words of blocks (similar to those from the
first words of blocks with similar performance).
This time the bilingual training data contain 2.4M
sentence pairs (68.1M Chinese words and 73.8M
English words) and two trigram language models
are used. One is trained on the English side of
the bilingual training data. The other is trained on
the Xinhua portion of the Gigaword corpus with
181.1M words. We also use some rules to trans-
late numbers, time expressions and Chinese per-
son names. The new Bleu score on NIST MT-05
is 0.291 which is very promising.
5 Discussion and Future Work
In this paper we presented a MaxEnt-based phrase
reordering model for SMT. We used lexical fea-
tures and collocation features from boundary
words of blocks to predicate reorderings of neigh-
Systems NIST MT-05 IWSLT-04
monotone 20.1 ? 0.8 37.8 ? 3.2
NONE 19.6 ? 0.8 36.3 ? 2.9
Distortion 20.9 ? 0.8 38.8 ? 3.0
Flat 20.5 ? 0.8 38.7 ? 2.8
Pharaoh 20.8 ? 0.8 38.9 ? 3.3
MaxEnt (lex) 22.0 ? 0.8 42.4 ? 3.3
MaxEnt (lex + col) 22.2 ? 0.8 42.8 ? 3.3
Table 1: BLEU-4 scores (%) with the 95% confi-
dence intervals. Italic numbers refer to results for
which the difference to the best result (indicated in
bold) is not statistically significant.
bor blocks. Experiments on standard Chinese-
English translation tasks from two different do-
mains showed that our method achieves a signif-
icant improvement over the distortion/flat reorder-
ing models.
Traditional distortion/flat-based SMT transla-
tion systems are good for learning phrase transla-
tion pairs, but learn nothing for phrasal reorder-
ings from real-world data. This is our original
motivation for designing a new reordering model,
which can learn reorderings from training data just
like learning phrasal translations. Lexicalized re-
ordering model learns reorderings from training
data, but it binds reorderings to individual concrete
phrases, which restricts the model to reorderings
of phrases seen in training data. On the contrary,
the MaxEnt-based reordering model is not limited
by this constraint since it is based on features of
phrase, not phrase itself. It can be easily general-
ized to reorder unseen phrases provided that some
features are fired on these phrases.
Another advantage of the MaxEnt-based re-
ordering model is that it can take more fea-
tures into reordering, even though they are non-
independent. Tillmann et. al (2005) also use a
MaxEnt model to integrate various features. The
difference is that they use the MaxEnt model to
predict not only orders but also blocks. To do that,
it is necessary for the MaxEnt model to incorpo-
rate real-valued features such as the block trans-
lation probability and the language model proba-
bility. Due to the expensive computation, a local
model is built. However, our MaxEnt model is just
a module of the whole log-linear model of transla-
tion which uses its score as a real-valued feature.
The modularity afforded by this design does not
incur any computation problems, and make it eas-
527
ier to update one sub-model with other modules
unchanged.
Beyond the MaxEnt-based reordering model,
another feature deserving attention in our system
is the CKY style decoder which observes the ITG.
This is different from the work of Zens et. al.
(2004). In their approach, translation is generated
linearly, word by word and phrase by phrase in a
traditional way with respect to the incorporation
of the language model. It can be said that their de-
coder did not violate the ITG constraints but not
that it observed the ITG. The ITG not only de-
creases reorderings greatly but also makes reorder-
ing hierarchical. Hierarchical reordering is more
meaningful for languages which are organized hi-
erarchically. From this point, our decoder is simi-
lar to the work by Chiang (2005).
The future work is to investigate other valuable
features, e.g. binary features that explain blocks
from the syntactical view. We think that there is
still room for improvement if more contributing
features are used.
Acknowledgements
This work was supported in part by National High
Technology Research and Development Program
under grant #2005AA114140 and National Nat-
ural Science Foundation of China under grant
#60573188. Special thanks to Yajuan Lu? for
discussions of the manuscript of this paper and
three anonymous reviewers who provided valuable
comments.
References
Ashish Venugopal, Stephan Vogel. 2005. Considerations in
Maximum Mutual Information and Minimum Classifica-
tion Error training for Statistical Machine Translation. In
the Proceedings of EAMT-05, Budapest, Hungary May 30-
31.
Christoph Tillmann. 2004. A block orientation model for
statistical machine translation. In HLT-NAACL, Boston,
MA, USA.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for statistical machine translation. In
Proceedings of ACL 2005, pages 557?564.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005, pages 263?270.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statis-
tical Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000, pages
440?447.
Franz Josef Och. 2003a. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL 2003,
pages 160?167.
Franz Josef Och. 2003b. Statistical Machine Translation:
From Single-Word Models to Alignment Templates The-
sis.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417?449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel
Galley, and Mark Hopkins. 2004. Arabic and Chinese MT
at USC/ISI. Presentation given at NIST Machine Transla-
tion Evaluation Workshop.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP 2002.
J. R. Quinlan. 1993. C4.5: progarms for machine learning.
Morgan Kaufmann Publishers.
Kevin Knight. 1999. Decoding complexity in wordreplace-
ment translation models. Computational Linguistics,
Squibs & Discussion, 25(4).
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proceedings of the Ninth International Workshop on
Parsing Technology, Vancouver, October, pages 53?64.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the Sixth Conference of the Association for
Machine Translation in the Americas, pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
Chris Callison-Burch, Miles Osborne and David Talbot.
2005. Edinburgh System Description for the 2005 IWSLT
Speech Translation Evaluation. In International Work-
shop on Spoken Language Translation.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Machine
Translation. In Proceedings of CoLing 2004, Geneva,
Switzerland, pp. 205-211.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002).
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proceedings of HLT-EMNLP.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do
we need to have a better system? In Proceedings of LREC
2004, pages 2051? 2054.
528
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609?616,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Tree-to-String Alignment Template for Statistical Machine Translation
Yang Liu , Qun Liu , and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No.6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu,liuqun,sxlin}@ict.ac.cn
Abstract
We present a novel translation model
based on tree-to-string alignment template
(TAT) which describes the alignment be-
tween a source parse tree and a target
string. A TAT is capable of generating
both terminals and non-terminals and per-
forming reordering at both low and high
levels. The model is linguistically syntax-
based because TATs are extracted auto-
matically from word-aligned, source side
parsed parallel texts. To translate a source
sentence, we first employ a parser to pro-
duce a source parse tree and then ap-
ply TATs to transform the tree into a tar-
get string. Our experiments show that
the TAT-based model significantly outper-
forms Pharaoh, a state-of-the-art decoder
for phrase-based models.
1 Introduction
Phrase-based translation models (Marcu and
Wong, 2002; Koehn et al, 2003; Och and Ney,
2004), which go beyond the original IBM trans-
lation models (Brown et al, 1993) 1 by model-
ing translations of phrases rather than individual
words, have been suggested to be the state-of-the-
art in statistical machine translation by empirical
evaluations.
In phrase-based models, phrases are usually
strings of adjacent words instead of syntactic con-
stituents, excelling at capturing local reordering
and performing translations that are localized to
1The mathematical notation we use in this paper is taken
from that paper: a source string fJ1 = f1, . . . , fj , . . . , fJ is
to be translated into a target string eI1 = e1, . . . , ei, . . . , eI .
Here, I is the length of the target string, and J is the length
of the source string.
substrings that are common enough to be observed
on training data. However, a key limitation of
phrase-based models is that they fail to model re-
ordering at the phrase level robustly. Typically,
phrase reordering is modeled in terms of offset po-
sitions at the word level (Koehn, 2004; Och and
Ney, 2004), making little or no direct use of syn-
tactic information.
Recent research on statistical machine transla-
tion has lead to the development of syntax-based
models. Wu (1997) proposes Inversion Trans-
duction Grammars, treating translation as a pro-
cess of parallel parsing of the source and tar-
get language via a synchronized grammar. Al-
shawi et al (2000) represent each production in
parallel dependency tree as a finite transducer.
Melamed (2004) formalizes machine translation
problem as synchronous parsing based on multi-
text grammars. Graehl and Knight (2004) describe
training and decoding algorithms for both gen-
eralized tree-to-tree and tree-to-string transduc-
ers. Chiang (2005) presents a hierarchical phrase-
based model that uses hierarchical phrase pairs,
which are formally productions of a synchronous
context-free grammar. Ding and Palmer (2005)
propose a syntax-based translation model based
on a probabilistic synchronous dependency in-
sert grammar, a version of synchronous gram-
mars defined on dependency trees. All these ap-
proaches, though different in formalism, make use
of synchronous grammars or tree-based transduc-
tion rules to model both source and target lan-
guages.
Another class of approaches make use of syn-
tactic information in the target language alone,
treating the translation problem as a parsing prob-
lem. Yamada and Knight (2001) use a parser in
the target language to train probabilities on a set of
609
operations that transform a target parse tree into a
source string.
Paying more attention to source language anal-
ysis, Quirk et al (2005) employ a source language
dependency parser, a target language word seg-
mentation component, and an unsupervised word
alignment component to learn treelet translations
from parallel corpus.
In this paper, we propose a statistical translation
model based on tree-to-string alignment template
which describes the alignment between a source
parse tree and a target string. A TAT is capa-
ble of generating both terminals and non-terminals
and performing reordering at both low and high
levels. The model is linguistically syntax-based
because TATs are extracted automatically from
word-aligned, source side parsed parallel texts.
To translate a source sentence, we first employ a
parser to produce a source parse tree and then ap-
ply TATs to transform the tree into a target string.
One advantage of our model is that TATs can
be automatically acquired to capture linguistically
motivated reordering at both low (word) and high
(phrase, clause) levels. In addition, the training of
TAT-based model is less computationally expen-
sive than tree-to-tree models. Similarly to (Galley
et al, 2004), the tree-to-string alignment templates
discussed in this paper are actually transformation
rules. The major difference is that we model the
syntax of the source language instead of the target
side. As a result, the task of our decoder is to find
the best target string while Galley?s is to seek the
most likely target tree.
2 Tree-to-String Alignment Template
A tree-to-string alignment template z is a triple
?T? , S?, A??, which describes the alignment A? be-
tween a source parse tree T? = T (F J ?1 ) 2 and
a target string S? = EI?1 . A source string F J
?
1 ,
which is the sequence of leaf nodes of T (F J ?1 ),
consists of both terminals (source words) and non-
terminals (phrasal categories). A target string EI?1
is also composed of both terminals (target words)
and non-terminals (placeholders). An alignment
A? is defined as a subset of the Cartesian product
of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?} (1)
2We use T (?) to denote a parse tree. To reduce notational
overhead, we use T (z) to represent the parse tree in z. Simi-
larly, S(z) denotes the string in z.
Figure 1 shows three TATs automatically
learned from training data. Note that when
demonstrating a TAT graphically, we represent
non-terminals in the target strings by blanks.
NP
NR
??
NN
??
LCP
NP
NR
??
CC
?
NR
LC
?
NP
DNP
NP DEG
NP
President Bush
between United States and
Figure 1: Examples of tree-to-string alignment
templates obtained in training
In the following, we formally describe how to
introduce tree-to-string alignment templates into
probabilistic dependencies to model Pr(eI1|fJ1 ) 3.
In a first step, we introduce the hidden variable
T (fJ1 ) that denotes a parse tree of the source sen-
tence fJ1 :
Pr(eI1|fJ1 ) =
?
T (fJ1 )
Pr(eI1, T (fJ1 )|fJ1 ) (2)
=
?
T (fJ1 )
Pr(T (fJ1 )|fJ1 )Pr(eI1|T (fJ1 ), fJ1 ) (3)
Next, another hidden variable D is introduced
to detach the source parse tree T (fJ1 ) into a se-
quence of K subtrees T?K1 with a preorder transver-
sal. We assume that each subtree T?k produces
a target string S?k. As a result, the sequence
of subtrees T?K1 produces a sequence of target
strings S?K1 , which can be combined serially to
generate the target sentence eI1. We assume that
Pr(eI1|D,T (fJ1 ), fJ1 ) ? Pr(S?K1 |T?K1 ) because eI1
is actually generated by the derivation of S?K1 .
Note that we omit an explicit dependence on the
detachment D to avoid notational overhead.
Pr(eI1|T (fJ1 ), fJ1 ) =
?
D
Pr(eI1, D|T (fJ1 ), fJ1 ) (4)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(eI1|D,T (fJ1 ), fJ1 ) (5)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(S?K1 |T?K1 ) (6)
=
?
D
Pr(D|T (fJ1 ), fJ1 )
K?
k=1
Pr(S?k|T?k) (7)
3The notational convention will be as follows. We use
the symbol Pr(?) to denote general probability distribution
with no specific assumptions. In contrast, for model-based
probability distributions, we use generic symbol p(?).
610
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
NP
DNP
NP DEG
?
NP
NP
NR
??
NP
NN NN
NN
??
NN
??
?? ? ?? ??
parsing
detachment production
of
China
economic development
combination
economic development of China
Figure 2: Graphic illustration for translation pro-
cess
To further decompose Pr(S?|T? ), the tree-to-
string alignment template, denoted by the variable
z, is introduced as a hidden variable.
Pr(S?|T? ) =
?
z
Pr(S?, z|T? ) (8)
=
?
z
Pr(z|T? )Pr(S?|z, T? ) (9)
Therefore, the TAT-based translation model can
be decomposed into four sub-models:
1. parse model: Pr(T (fJ1 )|fJ1 )
2. detachment model: Pr(D|T (fJ1 ), fJ1 )
3. TAT selection model: Pr(z|T? )
4. TAT application model: Pr(S?|z, T? )
Figure 2 shows how TATs work to perform
translation. First, the input source sentence is
parsed. Next, the parse tree is detached into five
subtrees with a preorder transversal. For each sub-
tree, a TAT is selected and applied to produce a
string. Finally, these strings are combined serially
to generate the translation (we use X to denote the
non-terminal):
X1 ? X2 of X3
? X2 of China
? X3 X4 of China
? economic X4 of China
? economic development of China
Following Och and Ney (2002), we base our
model on log-linear framework. Hence, all knowl-
edge sources are described as feature functions
that include the given source string fJ1 , the target
string eI1, and hidden variables. The hidden vari-
able T (fJ1 ) is omitted because we usually make
use of only single best output of a parser. As we
assume that all detachment have the same proba-
bility, the hidden variable D is also omitted. As
a result, the model we actually adopt for exper-
iments is limited because the parse, detachment,
and TAT application sub-models are simplified.
Pr(eI1, zK1 |fJ1 )
= exp[
?M
m=1 ?mhm(eI1, fJ1 , zK1 )]?
e?I1,z?K1 exp[
?M
m=1 ?mhm(e?I1, fJ1 , z?K1 )]
e?I1 = argmax
eI1,zK1
{ M?
m=1
?mhm(eI1, fJ1 , zK1 )
}
For our experiments we use the following seven
feature functions 4 that are analogous to default
feature set of Pharaoh (Koehn, 2004). To simplify
the notation, we omit the dependence on the hid-
den variables of the model.
h1(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(T (z))
h2(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(S(z))
h3(eI1, fJ1 ) = log
K?
k=1
lex(T (z)|S(z)) ? ?(T (z), T?k)
h4(eI1, fJ1 ) = log
K?
k=1
lex(S(z)|T (z)) ? ?(T (z), T?k)
h5(eI1, fJ1 ) = K
h6(eI1, fJ1 ) = log
I?
i=1
p(ei|ei?2, ei?1)
h7(eI1, fJ1 ) = I
4When computing lexical weighting features (Koehn et
al., 2003), we take only terminals into account. If there are
no terminals, we set the feature value to 1. We use lex(?)
to denote lexical weighting. We denote the number of TATs
used for decoding by K and the length of target string by I .
611
Tree String Alignment
( NR?? ) Bush 1:1
( NN?? ) President 1:1
( VV?? ) made 1:1
( NN?? ) speech 1:1
( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1
( NP ( NR?? ) ( NN ) ) X | Bush 1:2 2:1
( NP ( NR ) ( NN?? ) ) President | X 1:2 2:1
( NP ( NR?? ) ( NN?? ) ) President | Bush 1:2 2:1
( VP ( VV ) ( NN ) ) X1 | a | X2 1:1 2:3
( VP ( VV?? ) ( NN ) ) made | a | X 1:1 2:3
( VP ( VV ) ( NN?? ) ) X | a | speech 1:1 2:3
( VP ( VV?? ) ( NN?? ) ) made | a | speech 1:1 2:3
( IP ( NP ) ( VP ) ) X1 | X2 1:1 2:2
Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2
3 Training
To extract tree-to-string alignment templates from
a word-aligned, source side parsed sentence pair
?T (fJ1 ), eI1, A?, we need first identify TSAs (Tree-
String-Alignment) using similar criterion as sug-
gested in (Och and Ney, 2004). A TSA is a triple
?T (f j2j1 ), ei2i1 , A?)? that is in accordance with the
following constraints:
1. ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
2. T (f j2j1 ) is a subtree of T (fJ1 )
Given a TSA ?T (f j2j1 ), ei2i1 , A??, a triple
?T (f j4j3 ), ei4i3 , A?? is its sub TSA if and only
if:
1. T (f j4j3 ), ei4i3 , A?? is a TSA
2. T (f j4j3 ) is rooted at the direct descendant of
the root node of T (f j1j2 )
3. i1 ? i3 ? i4 ? i2
4. ?(i, j) ? A? : i3 ? i ? i4 ? j3 ? j ? j4
Basically, we extract TATs from a TSA
?T (f j2j1 ), ei2i1 , A?? using the following two rules:
1. If T (f j2j1 ) contains only one node,
then ?T (f j2j1 ), ei2i1 , A?? is a TAT
2. If the height of T (f j2j1 ) is greater than one,
then build TATs using those extracted from
sub TSAs of ?T (f j2j1 ), ei2i1 , A??.
IP
NP
NR
??
NN
??
VP
VV
??
NN
??
President Bush made a speech
Figure 3: An example of TSA
Usually, we can extract a very large amount of
TATs from training data using the above rules,
making both training and decoding very slow.
Therefore, we impose three restrictions to reduce
the magnitude of extracted TATs:
1. A third constraint is added to the definition of
TSA:
?j?, j?? : j1 ? j? ? j2 and j1 ? j?? ? j2
and (i1, j?) ? A? and (i2, j??) ? A?
This constraint requires that both the first
and last symbols in the target string must be
aligned to some source symbols.
2. The height of T (z) is limited to no greater
than h.
3. The number of direct descendants of a node
of T (z) is limited to no greater than c.
Table 1 shows the TATs extracted from the TSA
in Figure 3 with h = 2 and c = 2.
As we restrict that T (f j2j1 ) must be a subtree of
T (fJ1 ), TATs may be treated as syntactic hierar-
612
chical phrase pairs (Chiang, 2005) with tree struc-
ture on the source side. At the same time, we face
the risk of losing some useful non-syntactic phrase
pairs. For example, the phrase pair
???????? President Bush made
can never be obtained in form of TAT from the
TSA in Figure 3 because there is no subtree for
that source string.
4 Decoding
We approach the decoding problem as a bottom-up
beam search.
To translate a source sentence, we employ a
parser to produce a parse tree. Moving bottom-
up through the source parse tree, we compute a
list of candidate translations for the input subtree
rooted at each node with a postorder transversal.
Candidate translations of subtrees are placed in
stacks. Figure 4 shows the organization of can-
didate translation stacks.
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
8
4 7
2 3 5 6
1
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 4: Candidate translations of subtrees are
placed in stacks according to the root index set by
postorder transversal
A candidate translation contains the following
information:
1. the partial translation
2. the accumulated feature values
3. the accumulated probability
A TAT z is usable to a parse tree T if and only
if T (z) is rooted at the root of T and covers part
of nodes of T . Given a parse tree T , we find all
usable TATs. Given a usable TAT z, if T (z) is
equal to T , then S(z) is a candidate translation of
T . If T (z) covers only a portion of T , we have
to compute a list of candidate translations for T
by replacing the non-terminals of S(z) with can-
didate translations of the corresponding uncovered
subtrees.
NP
DNP
NP DEG
?
NP
8
4 7
2 3
of
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 5: Candidate translation construction
For example, when computing the candidate
translations for the tree rooted at node 8, the TAT
used in Figure 5 covers only a portion of the parse
tree in Figure 4. There are two uncovered sub-
trees that are rooted at node 2 and node 7 respec-
tively. Hence, we replace the third symbol with
the candidate translations in stack 2 and the first
symbol with the candidate translations in stack 7.
At the same time, the feature values and probabil-
ities are also accumulated for the new candidate
translations.
To speed up the decoder, we limit the search
space by reducing the number of TATs used for
each input node. There are two ways to limit the
TAT table size: by a fixed limit (tatTable-limit) of
how many TATs are retrieved for each input node,
and by a probability threshold (tatTable-threshold)
that specify that the TAT probability has to be
above some value. On the other hand, instead of
keeping the full list of candidates for a given node,
we keep a top-scoring subset of the candidates.
This can also be done by a fixed limit (stack-limit)
or a threshold (stack-threshold). To perform re-
combination, we combine candidate translations
that share the same leading and trailing bigrams
in each stack.
5 Experiments
Our experiments were on Chinese-to-English
translation. The training corpus consists of 31, 149
sentence pairs with 843, 256 Chinese words and
613
System Features BLEU4
d + ?(e|f) 0.0573 ? 0.0033
Pharaoh d + lm + ?(e|f) + wp 0.2019 ? 0.0083
d + lm + ?(f |e) + lex(f |e) + ?(e|f) + lex(e|f) + pp + wp 0.2089 ? 0.0089
h1 0.1639 ? 0.0077
Lynx h1 + h6 + h7 0.2100 ? 0.0089
h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ? 0.0080
Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus
949, 583 English words. For the language model,
we used SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a trigram model with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) on the 31, 149 English sentences. We se-
lected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development cor-
pus, and used the 2005 NIST MT Evaluation test
set as our test corpus. We evaluated the transla-
tion quality using the BLEU metric (Papineni et
al., 2002), as calculated by mteval-v11b.pl with its
default setting except that we used case-sensitive
matching of n-grams.
5.1 Pharaoh
The baseline system we used for comparison was
Pharaoh (Koehn et al, 2003; Koehn, 2004), a
freely available decoder for phrase-based transla-
tion models:
p(e|f) = p?(f |e)?? ? pLM(e)?LM ?
pD(e, f)?D ? ?
length(e)?W(e) (10)
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain
a single many-to-many word alignment for each
sentence pair. After that, we used some heuristics,
which including rule-based translation of num-
bers, dates, and person names, to further improve
the alignment accuracy.
Given the word-aligned bilingual corpus, we
obtained 1, 231, 959 bilingual phrases (221, 453
used on test corpus) using the training toolkits
publicly released by Philipp Koehn with its default
setting.
To perform minimum error rate training (Och,
2003) to tune the feature weights to maximize the
system?s BLEU score on development set, we used
optimizeV5IBMBLEU.m (Venugopal and Vogel,
2005). We used default pruning settings for
Pharaoh except that we set the distortion limit to
4.
5.2 Lynx
On the same word-aligned training data, it took
us about one month to parse all the 31, 149 Chi-
nese sentences using a Chinese parser written by
Deyi Xiong (Xiong et al, 2005). The parser was
trained on articles 1 ? 270 of Penn Chinese Tree-
bank version 1.0 and achieved 79.4% (F1 mea-
sure) as well as a 4.4% relative decrease in er-
ror rate. Then, we performed TAT extraction de-
scribed in section 3 with h = 3 and c = 5
and obtained 350, 575 TATs (88, 066 used on test
corpus). To run our decoder Lynx on develop-
ment and test corpus, we set tatTable-limit = 20,
tatTable-threshold = 0, stack-limit = 100, and
stack-threshold = 0.00001.
5.3 Results
Table 2 shows the results on test set using Pharaoh
and Lynx with different feature settings. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition
of the BLEU brevity penalty. For Pharaoh, eight
features were used: distortion model d, a trigram
language model lm, phrase translation probabili-
ties ?(f |e) and ?(e|f), lexical weightings lex(f |e)
and lex(e|f), phrase penalty pp, and word penalty
wp. For Lynx, seven features described in sec-
tion 2 were used. We find that Lynx outperforms
Pharaoh with all feature settings. With full fea-
tures, Lynx achieves an absolute improvement of
0.006 over Pharaoh (3.1% relative). This differ-
ence is statistically significant (p < 0.01). Note
that Lynx made use of only 88, 066 TATs on test
corpus while 221, 453 bilingual phrases were used
for Pharaoh.
The feature weights obtained by minimum er-
614
FeaturesSystem d lm ?(f |e) lex(f |e) ?(e|f) lex(e|f) pp wp
Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000
Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620
Table 3: Feature weights obtained by minimum error rate training on the development corpus
BLEU4
tat 0.2178 ? 0.0080
tat + bp 0.2240 ? 0.0083
Table 4: Effect of using bilingual phrases for Lynx
ror rate training for both Pharaoh and Lynx are
shown in Table 3. We find that ?(f |e) (i.e. h2) is
not a helpful feature for Lynx. The reason is that
we use only a single non-terminal symbol instead
of assigning phrasal categories to the target string.
In addition, we allow the target string consists of
only non-terminals, making translation decisions
not always based on lexical evidence.
5.4 Using bilingual phrases
It is interesting to use bilingual phrases to
strengthen the TAT-based model. As we men-
tioned before, some useful non-syntactic phrase
pairs can never be obtained in form of TAT be-
cause we restrict that there must be a correspond-
ing parse tree for the source phrase. Moreover,
it takes more time to obtain TATs than bilingual
phrases on the same training data because parsing
is usually very time-consuming.
Given an input subtree T (F j2j1 ), if F
j2
j1 is a string
of terminals, we find all bilingual phrases that the
source phrase is equal to F j2j1 . Then we build a
TAT for each bilingual phrase ?fJ ?1 , eI
?
1 , A??: the
tree of the TAT is T (F j2j1 ), the string is eI
?
1 , and
the alignment is A?. If a TAT built from a bilingual
phrase is the same with a TAT in the TAT table, we
prefer to the greater translation probabilities.
Table 4 shows the effect of using bilingual
phrases for Lynx. Note that these bilingual phrases
are the same with those used for Pharaoh.
5.5 Results on large data
We also conducted an experiment on large data to
further examine our design philosophy. The train-
ing corpus contains 2.6 million sentence pairs. We
used all the data to extract bilingual phrases and
a portion of 800K pairs to obtain TATs. Two tri-
gram language models were used for Lynx. One
was trained on the 2.6 million English sentences
and another was trained on the first 1/3 of the Xin-
hua portion of Gigaword corpus. We also included
rule-based translations of named entities, dates,
and numbers. By making use of these data, Lynx
achieves a BLEU score of 0.2830 on the 2005
NIST Chinese-to-English MT evaluation test set,
which is a very promising result for linguistically
syntax-based models.
6 Conclusion
In this paper, we introduce tree-to-string align-
ment templates, which can be automatically
learned from syntactically-annotated training data.
The TAT-based translation model improves trans-
lation quality significantly compared with a state-
of-the-art phrase-based decoder. Treated as spe-
cial TATs without tree on the source side, bilingual
phrases can be utilized for the TAT-based model to
get further improvement.
It should be emphasized that the restrictions
we impose on TAT extraction limit the expressive
power of TAT. Preliminary experiments reveal that
removing these restrictions does improve transla-
tion quality, but leads to large memory require-
ments. We feel that both parsing and word align-
ment qualities have important effects on the TAT-
based model. We will retrain the Chinese parser
on Penn Chinese Treebank version 5.0 and try to
improve word alignment quality using log-linear
models as suggested in (Liu et al, 2005).
Acknowledgement
This work is supported by National High Tech-
nology Research and Development Program con-
tract ?Generally Technical Research and Ba-
sic Database Establishment of Chinese Plat-
form?(Subject No. 2004AA114010). We are
grateful to Deyi Xiong for providing the parser and
Haitao Mi for making the parser more efficient and
robust. Thanks to Dr. Yajuan Lv for many helpful
comments on an earlier draft of this paper.
615
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite-state head transducers.
Computational Linguistics, 26(1):45-60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen and Joshua Goodman. 1998. Am
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of 43rd Annual Meeting of the ACL, pages
263-270.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insert grammars. In Proceedings of 43rd Annual
Meeting of the ACL, pages 541-548.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-HLT 2004, pages 273-
280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-HLT
2004, pages 105-112.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trnasla-
tion models. In Proceedings of the Sixth Confer-
ence of the Association for Machine Translation in
the Americas, pages 115-124.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of 43rd Annual Meeting of the ACL, pages 459-466.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 133-139.
Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of 42nd Annual Meeting
of the ACL, pages 653-660.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of 38th
Annual Meeting of the ACL, pages 440-447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the ACL, pages 295-302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
41st Annual Meeting of the ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the ACL, pages 311-318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of 43rd An-
nual Meeting of the ACL, pages 271-279.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and min-
imum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Con-
ference of the European Association for Machine
Translation (EAMT-05).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
treebank with semantic knowledge. In Proceedings
of IJCNLP 2005, pages 70-81.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the ACL, pages 523-530.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC),
pages 2051-2054.
616
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704?711,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Forest-to-String Statistical Translation Rules
Yang Liu , Yun Huang , Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100080, China
{yliu,huangyun,liuqun,sxlin}@ict.ac.cn
Abstract
In this paper, we propose forest-to-string
rules to enhance the expressive power of
tree-to-string translation models. A forest-
to-string rule is capable of capturing non-
syntactic phrase pairs by describing the cor-
respondence between multiple parse trees
and one string. To integrate these rules
into tree-to-string translation models, auxil-
iary rules are introduced to provide a gen-
eralization level. Experimental results show
that, on the NIST 2005 Chinese-English test
set, the tree-to-string model augmented with
forest-to-string rules achieves a relative im-
provement of 4.3% in terms of BLEU score
over the original model which allows tree-
to-string rules only.
1 Introduction
The past two years have witnessed the rapid de-
velopment of linguistically syntax-based translation
models (Quirk et al, 2005; Galley et al, 2006;
Marcu et al, 2006; Liu et al, 2006), which induce
tree-to-string translation rules from parallel texts
with linguistic annotations. They demonstrated very
promising results when compared with the state of
the art phrase-based system (Och and Ney, 2004)
in the NIST 2006 machine translation evaluation 1.
While Galley et al (2006) and Marcu et al (2006)
put emphasis on target language analysis, Quirk et
al. (2005) and Liu et al (2006) show benefits from
modeling the syntax of source language.
1See http://www.nist.gov/speech/tests/mt/
One major problem with linguistically syntax-
based models, however, is that tree-to-string rules
fail to syntactify non-syntactic phrase pairs because
they require a syntax tree fragment over the phrase
to be syntactified. Here, we distinguish between syn-
tactic and non-syntactic phrase pairs. By ?syntactic?
we mean that the phrase pair is subsumed by some
syntax tree fragment. The phrase pairs without trees
over them are non-syntactic. Marcu et al (2006)
report that approximately 28% of bilingual phrases
are non-syntactic on their English-Chinese corpus.
We believe that it is important to make available
to syntax-based models all the bilingual phrases that
are typically available to phrase-based models. On
one hand, phrases have been proven to be a simple
and powerful mechanism for machine translation.
They excel at capturing translations of short idioms,
providing local re-ordering decisions, and incorpo-
rating context information straightforwardly. Chi-
ang (2005) shows significant improvement by keep-
ing the strengths of phrases while incorporating syn-
tax into statistical translation. On the other hand,
the performance of linguistically syntax-based mod-
els can be hindered by making use of only syntac-
tic phrase pairs. Studies reveal that linguistically
syntax-based models are sensitive to syntactic anal-
ysis (Quirk and Corston-Oliver, 2006), which is still
not reliable enough to handle real-world texts due to
limited size and domain of training data.
Various solutions are proposed to tackle the prob-
lem. Galley et al (2004) handle non-constituent
phrasal translation by traversing the tree upwards
until reaches a node that subsumes the phrase.
Marcu et al (2006) argue that this choice is inap-
704
propriate because large applicability contexts are re-
quired.
For a non-syntactic phrase pair, Marcu et al
(2006) create a xRS rule headed by a pseudo, non-
syntactic nonterminal symbol that subsumes the
phrase and corresponding multi-headed syntactic
structure; and one sibling xRS rule that explains how
the non-syntactic nonterminal symbol can be com-
bined with other genuine nonterminals so as to ob-
tain genuine parse trees. The name of the pseudo
nonterminal is designed to reflect how the corre-
sponding rule can be fully realized. However, they
neglect alignment consistency when creating sibling
rules. In addition, it is hard for the naming mecha-
nism to deal with more complex phenomena.
Liu et al (2006) treat bilingual phrases as lexi-
calized TATs (Tree-to-string Alignment Template).
A bilingual phrase can be used in decoding if the
source phrase is subsumed by the input parse tree.
Although this solution does help, only syntactic
bilingual phrases are available to the TAT-based
model. Moreover, it is problematic to combine
the translation probabilities of bilingual phrases and
TATs, which are estimated independently.
In this paper, we propose forest-to-string rules
which describe the correspondence between multi-
ple parse trees and a string. They can not only cap-
ture non-syntactic phrase pairs but also have the ca-
pability of generalization. To integrate these rules
into tree-to-string translation models, auxiliary rules
are introduced to provide a generalization level. As
there is no pseudo node or naming mechanism, the
integration of forest-to-string rules is flexible, rely-
ing only on their root nodes. The forest-to-string and
auxiliary rules enable tree-to-string models to derive
in a more general way, while the strengths of con-
ventional tree-to-string rules still remain.
2 Forest-to-String Translation Rules
We define a tree-to-string rule r as a triple ?T? , S?, A??,
which describes the alignment A? between a source
parse tree T? = T (fJ ?
1
) and a target string S? = eI?
1
.
A source string fJ ?
1
, which is the sequence of leaf
nodes of T (fJ ?
1
), consists of both terminals (source
words) and nonterminals (phrasal categories). A tar-
get string eI?
1
is also composed of both terminals
(target words) and nonterminals (placeholders). An
IP
NP
NN
  
VP
SB
 
VP
NP
NN
  
VV
 
PU
 
The gunman was killed by police .
Figure 1: An English sentence aligned with a Chi-
nese parse tree.
alignment A? is defined as a subset of the Cartesian
product of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?}
A derivation ? = r
1
? r
2
? . . . ? rn is a left-
most composition of translation rules that explains
how a source parse tree T = T (fJ
1
), a target sen-
tence S = eI
1
, and the word alignment A are syn-
chronously generated. For example, Table 1 demon-
strates a derivation composed of only tree-to-string
rules for the ?T, S,A? tuple in Figure 1 2.
As we mentioned before, tree-to-string rules can
not syntactify phrase pairs that are not subsumed
by any syntax tree fragments. For example, for the
phrase pair ??   ?, ?The gunman was?? in Fig-
ure 1, it is impossible to extract an equivalent tree-
to-string rule that subsumes the same phrase pair
because valid tree-to-string rules can not be multi-
headed.
To address this problem, we propose forest-to-
string rules3 to subsume the non-syntactic phrase
pairs. A forest-to-string rule r 4 is a triple ?F? , S?, A??,
which describes the alignment A? between K source
parse trees F? = T?K
1
and a target string S?. The
source string fJ ?
1
is therefore the sequence of leaf
nodes of F? .
Auxiliary rules are introduced to integrate forest-
to-string rules into tree-to-string translation models.
An auxiliary rule is a special unlexicalized tree-to-
string rule that allows multiple source nonterminals
2We use ?X? to denote a nonterminal in the target string. If
there are more than one nonterminals, they are indexed.
3The term ?forest? refers to an ordered and finite set of trees.
4We still use ?r? to represent a forest-to-string rule to reduce
notational overhead.
705
No. Rule
(1) ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3
(2) ( NP ( NN   ) ) The gunman 1:1 1:2
(3) ( VP ( SB  ) ( VP ( NP ( NN ) ) ( VV  ) ) ) was killed by X 1:1 2:4 3:2
(4) ( NN   ) police 1:1
(5) ( PU  ) . 1:1
Table 1: A derivation composed of only tree-to-string rules for Figure 1.
No. Rule
(1) ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:2 4:2
(2) ( NP ( NN   ) ) ( SB  ) The gunman was 1:1 1:2 2:3
(3) ( VP ( NP ) ( VV  ) ) ( PU  ) killed by X . 1:3 2:1 3:4
(4) ( NP ( NN   ) ) police 1:1
Table 2: A derivation composed of tree-to-string, forest-to-string, and auxiliary rules for Figure 1.
to correspond to one target nonterminal, suggesting
that the forest-to-string rules that are rooted at such
source nonterminals can be integrated.
For example, Table 2 shows a derivation com-
posed of tree-to-string, forest-to-string, and auxil-
iary rules for the ?T, S,A? tuple in Figure 1. r
1
is
an auxiliary rule, r
2
and r
3
are forest-to-string rules,
and r
4
is a conventional tree-to-string rule.
Following Marcu et al (2006), we define the
probability of a tuple ?T, S,A? as the sum over all
derivations ?i ? ? that are consistent with the tuple,
c(?) = ?T, S,A?. The probability of each deriva-
tion ?i is given by the product of the probabilities of
all the rules p(rj) in the derivation.
Pr(T, S,A) =
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj) (1)
3 Training
We obtain tree-to-string and forest-to-string rules
from word-aligned, source side parsed bilingual cor-
pus. The extraction algorithm is shown in Figure 2.
Note that T ? denotes either a tree or a forest.
For each span, the ?tree/forest, string, alignment?
triples are identified first. If a triple is consistent with
the alignment, the skeleton of the triple is computed
then. A skeleton s is a rule satisfying the following:
1. s ? R(t), s is induced from t.
2. node(T (s)) ? 2, the tree/forest of s contains
two or more nodes.
3. ?r ? R(t) ? node(T (r)) ? 2, T (s) ? T (r),
the tree/forest of s is the subgraph of that of any
r containing two or more nodes.
1: Input: a source tree T = T (fJ
1
), a target string
S = eI
1
, and word alignment A between them
2: R := ?
3: for u := 0 to J ? 1 do
4: for v := 1 to J ? u do
5: identify the triple set T corresponding to
span (v, v + u)
6: for each triple t = ?T ?, S?, A?? ? T do
7: if ?T ?, S?? is not consistent with A then
8: continue
9: end if
10: if u = 0 ? node(T ?) = 1 then
11: add t to R
12: add ?root(T ?), ?X?, 1:1? to R
13: else
14: compute the skeleton s of the triple t
15: register rules that are built on s using rules
extracted from the sub-triples of t:
R := R? build(s,R)
16: end if
17: end for
18: end for
19: end for
20: Output: rule set R
Figure 2: Rule extraction algorithm.
Given the skeleton and rules extracted from the
sub-triples, the rules for the triple can be acquired.
For example, the algorithm identifies the follow-
ing triple for span (1, 2) in Figure 1:
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
The skeleton of the triple is:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
As the algorithm proceeds bottom-up, five rules
have already been extracted from the sub-triples,
rooted at ?NP? and ?SB? respectively:
?( NP ),?X?, 1:1?
?( NP ( NN ) ),?X?, 1:1?
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
706
?( SB ),?X?, 1:1?
?( SB  ),?was?, 1:1?
Hence, we can obtain new rules by replacing the
source and target symbols of the skeleton with corre-
sponding rules and also by modifying the alignment
information. For the above triple, the combination
of the five rules produces 2 ? 3 = 6 new rules:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN ) ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ( NN ) ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN   ) ) ( SB ),?The gunman X?, 1:1 1:2?
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
Since we need only to check the alignment con-
sistency, in principle all phrase pairs can be captured
by tree-to-string and forest-to-string rules. To lower
the complexity for both training and decoding, we
impose four restrictions:
1. Both the first and the last symbols in the target
string must be aligned to some source symbols.
2. The height of a tree or forest is no greater than
h.
3. The number of direct descendants of a node is
no greater than c.
4. The number of leaf nodes is no greater than l.
Although possible, it is infeasible to learn aux-
iliary rules from training data. To extract an auxil-
iary rule which integrates at least one forest-to-string
rule, one need traverse the parse tree upwards until
one reaches a node that subsumes the entire forest
without violating the alignment consistency. This
usually results in very complex auxiliary rules, es-
pecially on real-world training data, making both
training and decoding very slow. As a result, we
construct auxiliary rules in decoding instead.
4 Decoding
Given a source parse tree T (fJ
1
), our decoder finds
the target yield of the single best derivation that has
source yield of T (fJ
1
):
S? = argmax
S,A
Pr(T, S,A)
= argmax
S,A
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj)
1: Input: a source parse tree T = T (fJ
1
)
2: for u := 0 to J ? 1 do
3: for v := 1 to J ? u do
4: for each T ? spanning from v to v + u do
5: if T ? is a tree then
6: for each usable tree-to-string rule r do
7: for each derivation ? inferred from r
and derivations in matrix do
8: add ? to matrix[v, v + u, root(T ?)]
9: end for
10: end for
11: search subcell divisions D[v, v + u]
12: for each subcell division d ? D[v, v + u] do
13: if d contains at least one forest cell then
14: construct auxiliary rule r
a
15: for each derivation ? inferred from r
a
and derivations in matrix do
16: add ? to matrix[v, v + u, root(T ?)]
17: end for
18: end if
19: end for
20: else
21: for each usable forest-to-string rule r do
22: for each derivation ? inferred from r
and derivations in matrix do
23: add ? to matrix[v, v + u, ??]
24: end for
25: end for
26: search subcell divisions D[v, v + u]
27: end if
28: end for
29: end for
30: end for
31: find the best derivation ?? in matrix[1, J, root(T )] and
get the best translation ?S = e(??)
32: Output: a target string ?S
Figure 3: Decoding algorithm.
? argmax
S,A,?
?
r
j
??,c(?)=?T,S,A?
p(rj) (2)
Figure 3 demonstrates the decoding algorithm.
It organizes the derivations into an array matrix
whose cells matrix[j
1
, j
2
,X] are sets of derivations.
[j
1
, j
2
,X] represents a tree/forest rooted at X span-
ning from j
1
to j
2
. We use the empty string ?? to
denote the pseudo root of a forest.
Next, we will explain how to infer derivations for
a tree/forest provided a usable rule. If T (r) = T?,
there is only one derivation which contains only the
rule r. This usually happens for leaf nodes. If
T (r) ? T ?, the rule r resorts to derivations from
subcells to infer new derivations. Suppose that the
decoder is to translate the source tree in Figure 1
and finds a usable rule for [1, 5, ?IP?]:
?( IP ( NP ) ( VP ) ( PU ) ),?X
1
X
2
X
3
?, 1:1 2:2 3:3?
707
Subcell Division Auxiliary Rule
[1, 1][2, 2][3, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3 4:3
[1, 2][3, 4][5, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:1 3:2 4:3
[1, 3][4, 5] ( IP ( NP ) ( VP ( SB ) ( VP ( NP ) ( VV ) ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:1 4:2 5:2
[1, 1][2, 5] ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
1:1 2:2 3:2
Table 3: Subcell divisions and corresponding auxiliary rules for the source tree in Figure 1
Since the decoding algorithm proceeds in a
bottom-up fashion, the uncovered portions have al-
ready been translated.
For [1, 1, ?NP?], suppose that we can find a
derivation in matrix:
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
For [2, 4, ?VP?], we find a derivation in matrix:
?( VP ( SB  ) ( VP ( NP ( NN )) (VV ) ) ),
?was killed by X?, 1:1 2:4 3:2?
?( NN   ),?police?, 1:1?
For [5, 5, ?PU?], we find a derivation in matrix:
?( PU  ),?.?, 1:1?
Henceforth, we get a derivation for [1, 5, ?IP?],
shown in Table 1.
A translation rule r is said to be usable to an input
tree/forest T ? if and only if:
1. T (r) ? T ?, the tree/forest of r is the subgraph
of T ?.
2. root(T (r)) = root(T ?), the root sequence of
T (r) is identical to that of T ?.
For example, the following rules are usable to the
tree ?( NP ( NR   ) ( NN   ) )?:
?( NP ( NR ) ( NN ) ),?X
1
X
2
?, 1:2 2:1?
?( NP ( NR   ) ( NN ) ),?China X?, 1:1 2:2?
?( NP ( NR   ) ( NN  ) ),?China economy?, 1:1 2:2?
Similarly, the forest-to-string rule
?( ( NP ( NR ) ( NN ) ) ( VP ) ),?X
1
X
2
X
3
?, 1:2 2:1 3:3?
is usable to the forest
( NP ( NR ) ( NN   ) ) ( VP (VV )( NN  ) )
As we mentioned before, auxiliary rules are spe-
cial unlexicalized tree-to-string rules that are built in
decoding rather than learnt from real-world data. To
get an auxiliary rule for a cell, we need first identify
its subcell division.
A cell sequence c
1
, c
2
, . . . , cn is referred to as a
subcell division of a cell c if and only if:
1. c
1
.begin = c.begin
1: Input: a cell [j
1
, j
2
], the derivation array matrix,
the subcell division array D
2: if j
1
= j
2
then
3: p? := 0
4: for each derivation ? in matrix[j
1
, j
2
, ?] do
5: p? := max(p(?), p?)
6: end for
7: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
8: else
9: if [j
1
, j
2
] is a forest cell then
10: p? := 0
11: for each derivation ? in matrix[j
1
, j
2
, ?] do
12: p? := max(p(?), p?)
13: end for
14: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
15: end if
16: for j := j
1
to j
2
? 1 do
17: for each division d
1
? D[j
1
, j] do
18: for each division d
2
? D[j + 1, j
2
] do
19: create a new division: d := d
1
? d
2
20: add d to D[j
1
, j
2
]
21: end for
22: end for
23: end for
24: end if
25: Output: subcell divisions D[j
1
, j
2
]
Figure 4: Subcell division search algorithm.
2. cn.end = c.end
3. cj .end + 1 = cj+1.begin, 1 ? j < n
Given a subcell division, it is easy to construct the
auxiliary rule for a cell. For each subcell, one need
transverse the parse tree upwards until one reaches
nodes that subsume it. All descendants of these
nodes are dropped. The target string consists of only
nonterminals, the number of which is identical to
that of subcells. To limit the search space, we as-
sume that the alignment between the source tree and
the target string is monotone.
Table 3 shows some subcell divisions and corre-
sponding auxiliary rules constructed for the source
tree in Figure 1. For simplicity, we ignore the root
node label.
There are 2n?1 subcell divisions for a cell which
has a length of n. We need only consider the sub-
708
cell divisions which contain at least one forest cell
because tree-to-string rules have already explored
those contain only tree cells.
The actual search algorithm for subcell divisions
is shown in Figure 4. We use matrix[j
1
, j
2
, ?] to de-
note all trees or forests spanning from j
1
to j
2
. The
subcell divisions and their associated probabilities
are stored in an array D. We define an operator ?
between two divisions: their cell sequences are con-
catenated and the probabilities are accumulated.
As sometimes there are no usable rules available,
we introduce default rules to ensure that we can al-
ways get a translation for any input parse tree. A de-
fault rule is a tree-to-string rule 5, built in two ways:
1. If the input tree contains only one node, the
target string of the default rule is equal to the
source string.
2. If the height of the input tree is greater than
one, the tree of the default rule contains only
the root node and its direct descendants of the
input tree, the string contains only nontermi-
nals, and the alignment is monotone.
To speed up the decoder, we limit the search space
by reducing the number of rules used for each cell.
There are two ways to limit the rule table size: by
a fixed limit a of how many rules are retrieved for
each cell, and by a probability threshold ? that spec-
ify that the rule probability has to be above some
value. Also, instead of keeping the full list of deriva-
tions for a cell, we store a top-scoring subset of the
derivations. This can also be done by a fixed limit
b or a threshold ?. The subcell division array D, in
which divisions containing forest cells have priority
over those composed of only tree cells, is pruned by
keeping only a-best divisions.
Following Och and Ney (2002), we base our
model on log-linear framework and adopt the seven
feature functions described in (Liu et al, 2006). It
is very important to balance the preference between
conventional tree-to-string rules and the newly-
introduced forest-to-string and auxiliary rules. As
the probabilities of auxiliary rules are not learnt
from training data, we add a feature that sums up the
5There are no default rules for forests because only tree-to-
string rules are essential to tree-to-string translation models.
node count of auxiliary rules of a derivation to pe-
nalize the use of forest-to-string and auxiliary rules.
5 Experiments
In this section, we report on experiments with
Chinese-to-English translation. The training corpus
consists of 31, 149 sentence pairs with 843, 256 Chi-
nese words and 949, 583 English words. For the
language model, we used SRI Language Modeling
Toolkit (Stolcke, 2002) to train a trigram model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the 31, 149 English sentences. We
selected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development corpus,
and used the 2005 NIST MT Evaluation test set as
our test corpus. Our evaluation metric is BLEU-4
(Papineni et al, 2002), as calculated by the script
mteval-v11b.pl with its default setting except that
we used case-sensitive matching of n-grams. To
perform minimum error rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem?s BLEU score on development set, we used the
script optimizeV5IBMBLEU.m (Venugopal and Vo-
gel, 2005).
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain a
single many-to-many word alignment for each sen-
tence pair. Next, we employed a Chinese parser
written by Deyi Xiong (Xiong et al, 2005) to parse
all the 31, 149 Chinese sentences. The parser was
trained on articles 1-270 of Penn Chinese Treebank
version 1.0 and achieved 79.4% in terms of F1 mea-
sure.
Given the word-aligned, source side parsed bilin-
gual corpus, we obtained bilingual phrases using the
training toolkits publicly released by Philipp Koehn
with its default setting. Then, we applied extrac-
tion algorithm described in Figure 2 to extract both
tree-to-string and forest-to-string rules by restricting
h = 3, c = 5, and l = 7. All the rules, including
bilingual phrases, tree-to-string rules, and forest-to-
string rules, are filtered for the development and test
sets.
According to different levels of lexicalization, we
divide translation rules into three categories:
709
Rule L P U Total
BP 251, 173 0 0 251, 173
TR 56, 983 41, 027 3, 529 101, 539
FR 16, 609 254, 346 25, 051 296, 006
Table 4: Number of rules used in experiments (BP:
bilingual phrase, TR: tree-to-string rule, FR: forest-
to-string rule; L: lexicalized, P: partial lexicalized,
U: unlexicalized).
System Rule Set BLEU4
Pharaoh BP 0.2182 ? 0.0089
BP 0.2059 ? 0.0083
TR 0.2302 ? 0.0089Lynx TR + BP 0.2346 ? 0.0088
TR + FR + AR 0.2402 ? 0.0087
Table 5: Comparison of Pharaoh and Lynx with dif-
ferent rule sets.
1. lexicalized: all symbols in both the source and
target strings are terminals
2. unlexicalized: all symbols in both the source
and target strings are nonterminals
3. partial lexicalized: otherwise
Table 4 shows the statistics of rules used in our ex-
periments. We find that even though forest-to-string
rules are introduced the total number (i.e. 73, 592)
of lexicalized tree-to-string and forest-to-string rules
is still far less than that (i.e. 251, 173) of bilingual
phrases. This difference results from the restriction
we impose in training that both the first and last sym-
bols in the target string must be aligned to some
source symbols. For the forest-to-string rules, par-
tial lexicalized ones are in the majority.
We compared our system Lynx against a freely
available phrase-based decoder Pharaoh (Koehn et
al., 2003). For Pharaoh, we set a = 20, ? = 0,
b = 100, ? = 10?5, and distortion limit dl = 4. For
Lynx, we set a = 20, ? = 0, b = 100, and ? = 0.
Two postprocessing procedures ran to improve the
outputs of both systems: OOVs removal and recapi-
talization.
Table 5 shows results on test set using Pharaoh
and Lynx with different rule sets. Note that Lynx
is capable of using only bilingual phrases plus de-
Forest-to-String Rule Set BLEU4
None 0.2225 ? 0.0085
L 0.2297 ? 0.0081
P 0.2279 ? 0.0083
U 0.2270 ? 0.0087
L + P + U 0.2312 ? 0.0082
Table 6: Effect of lexicalized, partial lexicalized,
and unlexicalized forest-to-string rules.
fault rules to perform monotone search. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition of
the BLEU brevity penalty. We find that Lynx out-
performs Pharaoh significantly. The integration of
forest-to-string rules achieves an absolute improve-
ment of 1.0% (4.3% relative) over using tree-to-
string rules only. This difference is statistically sig-
nificant (p < 0.01). It also achieves better result
than treating bilingual phrases as lexicalized tree-to-
string rules. To produce the best result of 0.2402,
Lynx made use of 26, 082 tree-to-string rules, 9, 219
default rules, 5, 432 forest-to-string rules, and 2, 919
auxiliary rules. This suggests that tree-to-string
rules still play a central role, although the integra-
tion of forest-to-string and auxiliary rules is really
beneficial.
Table 6 demonstrates the effect of forest-to-string
rules with different lexicalization levels. We set
a = 3, ? = 0, b = 10, and ? = 0. The second row
?None? shows the result of using only tree-to-string
rules. ?L? denotes using tree-to-string rules and lex-
icalized forest-to-string rules. Similarly, ?L+P+U?
denotes using tree-to-string rules and all forest-to-
string rules. We find that lexicalized forest-to-string
rules are more useful.
6 Conclusion
In this paper, we introduce forest-to-string rules to
capture non-syntactic phrase pairs that are usually
unaccessible to traditional tree-to-string translation
models. With the help of auxiliary rules, forest-to-
string rules can be integrated into tree-to-string mod-
els to offer more general derivations. Experiment re-
sults show that the tree-to-string model augmented
with forest-to-string rules significantly outperforms
710
the original model which allows tree-to-string rules
only.
Our current rule extraction algorithm attaches the
unaligned target words to the nearest ascendants that
subsume them. This constraint hampers the expres-
sive power of our model. We will try a more general
way as suggested in (Galley et al, 2006), making
no a priori assumption about assignment and using
EM training to learn the probability distribution. We
will also conduct experiments on large scale training
data to further examine our design philosophy.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University Center for
Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proceedings of HLT/NAACL 2004, pages 273?280,
Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961?968, Sydney,
Australia, July.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. InProceed-
ings of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL 2006, pages
609?616, Sydney, Australia, July.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52, Sydney,
Australia, July.
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of ACL 2000,
pages 440?447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadephia, USA, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP
2006, pages 62?69, Sydney, Australia, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages
271?279, Ann Arbor, Michigan, June.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 30, pages 901?904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and mini-
mum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Confer-
ence of the European Association for Machine Trans-
lation, pages 271?279.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings of IJCNLP 2005, pages
70?81.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores how much improvement do
we need to have a better system? In Proceedings
of Fourth International Conference on Language Re-
sources and Evaluation, pages 2051?2054.
711
Proceedings of ACL-08: HLT, pages 192?199,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Forest-Based Translation
Haitao Mi? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
{htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
Among syntax-based translation models, the
tree-based approach, which takes as input a
parse tree of the source sentence, is a promis-
ing direction being faster and simpler than
its string-based counterpart. However, current
tree-based systems suffer from a major draw-
back: they only use the 1-best parse to direct
the translation, which potentially introduces
translation mistakes due to parsing errors. We
propose a forest-based approach that trans-
lates a packed forest of exponentially many
parses, which encodes many more alternatives
than standard n-best lists. Large-scale exper-
iments show an absolute improvement of 1.7
BLEU points over the 1-best baseline. This
result is also 0.8 points higher than decoding
with 30-best parses, and takes even less time.
1 Introduction
Syntax-based machine translation has witnessed
promising improvements in recent years. Depend-
ing on the type of input, these efforts can be di-
vided into two broad categories: the string-based
systems whose input is a string to be simultane-
ously parsed and translated by a synchronous gram-
mar (Wu, 1997; Chiang, 2005; Galley et al, 2006),
and the tree-based systems whose input is already a
parse tree to be directly converted into a target tree
or string (Lin, 2004; Ding and Palmer, 2005; Quirk
et al, 2005; Liu et al, 2006; Huang et al, 2006).
Compared with their string-based counterparts, tree-
based systems offer some attractive features: they
are much faster in decoding (linear time vs. cubic
time, see (Huang et al, 2006)), do not require a
binary-branching grammar as in string-based mod-
els (Zhang et al, 2006), and can have separate gram-
mars for parsing and translation, say, a context-free
grammar for the former and a tree substitution gram-
mar for the latter (Huang et al, 2006). However, de-
spite these advantages, current tree-based systems
suffer from a major drawback: they only use the 1-
best parse tree to direct the translation, which po-
tentially introduces translation mistakes due to pars-
ing errors (Quirk and Corston-Oliver, 2006). This
situation becomes worse with resource-poor source
languages without enough Treebank data to train a
high-accuracy parser.
One obvious solution to this problem is to take as
input k-best parses, instead of a single tree. This k-
best list postpones some disambiguation to the de-
coder, which may recover from parsing errors by
getting a better translation from a non 1-best parse.
However, a k-best list, with its limited scope, of-
ten has too few variations and too many redundan-
cies; for example, a 50-best list typically encodes
a combination of 5 or 6 binary ambiguities (since
25 < 50 < 26), and many subtrees are repeated
across different parses (Huang, 2008). It is thus inef-
ficient either to decode separately with each of these
very similar trees. Longer sentences will also aggra-
vate this situation as the number of parses grows ex-
ponentially with the sentence length.
We instead propose a new approach, forest-based
translation (Section 3), where the decoder trans-
lates a packed forest of exponentially many parses,1
1There has been some confusion in the MT literature regard-
ing the term forest: the word ?forest? in ?forest-to-string rules?
192
VP
PP
P
yu?
x1:NPB
VPB
VV
ju?x??ng
AS
le
x2:NPB
? held x2 with x1
Figure 1: An example translation rule (r3 in Fig. 2).
which compactly encodes many more alternatives
than k-best parses. This scheme can be seen as
a compromise between the string-based and tree-
based methods, while combining the advantages of
both: decoding is still fast, yet does not commit to
a single parse. Large-scale experiments (Section 4)
show an improvement of 1.7 BLEU points over the
1-best baseline, which is also 0.8 points higher than
decoding with 30-best trees, and takes even less time
thanks to the sharing of common subtrees.
2 Tree-based systems
Current tree-based systems perform translation in
two separate steps: parsing and decoding. A parser
first parses the source language input into a 1-best
tree T , and the decoder then searches for the best
derivation (a sequence of translation steps) d? that
converts source tree T into a target-language string
among all possible derivations D:
d? = argmax
d?D
P(d|T ). (1)
We will now proceed with a running example
translating from Chinese to English:
(2) ?
Bu`sh??
Bush
?
yu?
with/and
??
Sha?lo?ng
Sharon1
>L
ju?x??ng
hold
?
le
pass.
Proceedings of ACL-08: HLT, pages 897?904,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Cascaded Linear Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Wenbin Jiang ? Liang Huang ? Qun Liu ? Yajuan Lu? ?
?Key Lab. of Intelligent Information Processing ?Department of Computer & Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
jiangwenbin@ict.ac.cn lhuang3@cis.upenn.edu
Abstract
We propose a cascaded linear model for
joint Chinese word segmentation and part-
of-speech tagging. With a character-based
perceptron as the core, combined with real-
valued features such as language models, the
cascaded model is able to efficiently uti-
lize knowledge sources that are inconvenient
to incorporate into the perceptron directly.
Experiments show that the cascaded model
achieves improved accuracies on both seg-
mentation only and joint segmentation and
part-of-speech tagging. On the Penn Chinese
Treebank 5.0, we obtain an error reduction of
18.5% on segmentation and 12% on joint seg-
mentation and part-of-speech tagging over the
perceptron-only baseline.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are important tasks in computer processing of
Chinese and other Asian languages. Several mod-
els were introduced for these problems, for example,
the Hidden Markov Model (HMM) (Rabiner, 1989),
Maximum Entropy Model (ME) (Ratnaparkhi and
Adwait, 1996), and Conditional Random Fields
(CRFs) (Lafferty et al, 2001). CRFs have the ad-
vantage of flexibility in representing features com-
pared to generative ones such as HMM, and usually
behaves the best in the two tasks. Another widely
used discriminative method is the perceptron algo-
rithm (Collins, 2002), which achieves comparable
performance to CRFs with much faster training, so
we base this work on the perceptron.
To segment and tag a character sequence, there
are two strategies to choose: performing POS tag-
ging following segmentation; or joint segmentation
and POS tagging (Joint S&T). Since the typical ap-
proach of discriminative models treats segmentation
as a labelling problem by assigning each character
a boundary tag (Xue and Shen, 2003), Joint S&T
can be conducted in a labelling fashion by expand-
ing boundary tags to include POS information (Ng
and Low, 2004). Compared to performing segmen-
tation and POS tagging one at a time, Joint S&T can
achieve higher accuracy not only on segmentation
but also on POS tagging (Ng and Low, 2004). Be-
sides the usual character-based features, additional
features dependent on POS?s or words can also be
employed to improve the performance. However, as
such features are generated dynamically during the
decoding procedure, two limitation arise: on the one
hand, the amount of parameters increases rapidly,
which is apt to overfit on training corpus; on the
other hand, exact inference by dynamic program-
ming is intractable because the current predication
relies on the results of prior predications. As a result,
many theoretically useful features such as higher-
order word or POS n-grams are difficult to be in-
corporated in the model efficiently.
To cope with this problem, we propose a cascaded
linear model inspired by the log-linear model (Och
and Ney, 2004) widely used in statistical machine
translation to incorporate different kinds of knowl-
edge sources. Shown in Figure 1, the cascaded
model has a two-layer architecture, with a character-
based perceptron as the core combined with other
real-valued features such as language models. We
897
Core
Linear Model
(Perceptron)
g1 =
?
i ?i ? fi
~?
Outside-layer
Linear Model
S = ?j wj ? gj
~w
f1
f2
f|R|
g1
Word LM: g2 = Pwlm(W ) g2
POS LM: g3 = Ptlm(T ) g3
Labelling: g4 = P (T |W ) g4
Generating: g5 = P (W |T ) g5
Length: g6 = |W | g6
S
Figure 1: Structure of Cascaded Linear Model. |R| denotes the scale of the feature space of the core perceptron.
will describe it in detail in Section 4. In this ar-
chitecture, knowledge sources that are intractable to
incorporate into the perceptron, can be easily incor-
porated into the outside linear model. In addition,
as these knowledge sources are regarded as separate
features, we can train their corresponding models in-
dependently with each other. This is an interesting
approach when the training corpus is large as it re-
duces the time and space consumption. Experiments
show that our cascaded model can utilize different
knowledge sources effectively and obtain accuracy
improvements on both segmentation and Joint S&T.
2 Segmentation and POS Tagging
Given a Chinese character sequence:
C1:n = C1 C2 .. Cn
the segmentation result can be depicted as:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
while the segmentation and POS tagging result can
be depicted as:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
Here, Ci (i = 1..n) denotes Chinese character,
ti (i = 1..m) denotes POS tag, and Cl:r (l ? r)
denotes character sequence ranges from Cl to Cr.
We can see that segmentation and POS tagging task
is to divide a character sequence into several subse-
quences and label each of them a POS tag.
It is a better idea to perform segmentation and
POS tagging jointly in a uniform framework. Ac-
cording to Ng and Low (2004), the segmentation
task can be transformed to a tagging problem by as-
signing each character a boundary tag of the follow-
ing four types:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
We can extract segmentation result by splitting
the labelled result into subsequences of pattern s or
bm?e which denote single-character word and multi-
character word respectively. In order to perform
POS tagging at the same time, we expand boundary
tags to include POS information by attaching a POS
to the tail of a boundary tag as a postfix following
Ng and Low (2004). As each tag is now composed
of a boundary part and a POS part, the joint S&T
problem is transformed to a uniform boundary-POS
labelling problem. A subsequence of boundary-POS
labelling result indicates a word with POS t only if
the boundary tag sequence composed of its bound-
ary part conforms to s or bm?e style, and all POS
tags in its POS part equal to t. For example, a tag
sequence b NN m NN e NN represents a three-
character word with POS tag NN .
3 The Perceptron
The perceptron algorithm introduced into NLP by
Collins (2002), is a simple but effective discrimina-
tive training method. It has comparable performance
898
Non-lexical-target Instances
Cn (n = ?2..2) C?2=e, C?1=?, C0=U, C1=/, C2=?
CnCn+1 (n = ?2..1) C?2C?1=e?, C?1C0=?U, C0C1=U/, C1C2=/?
C?1C1 C?1C1=?/
Lexical-target Instances
C0Cn (n = ?2..2) C0C?2=Ue, C0C?1=U?, C0C0=UU, C0C1=U/, C0C2=U?
C0CnCn+1 (n = ?2..1) C0C?2C?1=Ue?, C0C?1C0=U?U, C0C0C1=UU/, C0C1C2=U/?
C0C?1C1 C0C?1C1 =U?/
Table 1: Feature templates and instances. Suppose we are considering the third character ?U? in ?e? U /??.
to CRFs, while with much faster training. The per-
ceptron has been used in many NLP tasks, such as
POS tagging (Collins, 2002), Chinese word seg-
mentation (Ng and Low, 2004; Zhang and Clark,
2007) and so on. We trained a character-based per-
ceptron for Chinese Joint S&T, and found that the
perceptron itself could achieve considerably high ac-
curacy on segmentation and Joint S&T. In following
subsections, we describe the feature templates and
the perceptron training algorithm.
3.1 Feature Templates
The feature templates we adopted are selected from
those of Ng and Low (2004). To compare with oth-
ers conveniently, we excluded the ones forbidden by
the close test regulation of SIGHAN, for example,
Pu(C0), indicating whether character C0 is a punc-
tuation.
All feature templates and their instances are
shown in Table 1. C represents a Chinese char-
acter while the subscript of C indicates its posi-
tion in the sentence relative to the current charac-
ter (it has the subscript 0). Templates immediately
borrowed from Ng and Low (2004) are listed in
the upper column named non-lexical-target. We
called them non-lexical-target because predications
derived from them can predicate without consider-
ing the current character C0. Templates in the col-
umn below are expanded from the upper ones. We
add a field C0 to each template in the upper col-
umn, so that it can carry out predication according
to not only the context but also the current char-
acter itself. As predications generated from such
templates depend on the current character, we name
these templates lexical-target. Note that the tem-
plates of Ng and Low (2004) have already con-
tained some lexical-target ones. With the two kinds
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi)?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? +?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
of predications, the perceptron model will do exact
predicating to the best of its ability, and can back
off to approximately predicating if exact predicating
fails.
3.2 Training Algorithm
We adopt the perceptron training algorithm of
Collins (2002) to learn a discriminative model map-
ping from inputs x ? X to outputs y ? Y , where X
is the set of sentences in the training corpus and Y
is the set of corresponding labelled results. Follow-
ing Collins, we use a function GEN(x) generating
all candidate results of an input x , a representation
? mapping each training example (x, y) ? X ? Y
to a feature vector ?(x, y) ? Rd, and a parameter
vector ~? ? Rd corresponding to the feature vector.
d means the dimension of the vector space, it equals
to the amount of features in the model. For an input
character sequence x, we aim to find an output F (x)
satisfying:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
?(x, y) ? ~? represents the inner product of feature
vector ?(x, y) and the parameter vector ~?. We used
the algorithm depicted in Algorithm 1 to tune the
parameter vector ~?.
899
To alleviate overfitting on the training examples,
we use the refinement strategy called ?averaged pa-
rameters? (Collins, 2002) to the algorithm in Algo-
rithm 1.
4 Cascaded Linear Model
In theory, any useful knowledge can be incorporated
into the perceptron directly, besides the character-
based features already adopted. Additional features
most widely used are related to word or POS n-
grams. However, such features are generated dy-
namically during the decoding procedure so that
the feature space enlarges much more rapidly. Fig-
ure 2 shows the growing tendency of feature space
with the introduction of these features as well as the
character-based ones. We noticed that the templates
related to word unigrams and bigrams bring to the
feature space an enlargement much rapider than the
character-base ones, not to mention the higher-order
grams such as trigrams or 4-grams. In addition, even
though these higher grams were managed to be used,
there still remains another problem: as the current
predication relies on the results of prior ones, the
decoding procedure has to resort to approximate in-
ference by maintaining a list of N -best candidates at
each predication position, which evokes a potential
risk to depress the training.
To alleviate the drawbacks, we propose a cas-
caded linear model. It has a two-layer architec-
ture, with a perceptron as the core and another linear
model as the outside-layer. Instead of incorporat-
ing all features into the perceptron directly, we first
trained the perceptron using character-based fea-
tures, and several other sub-models using additional
ones such as word or POS n-grams, then trained the
outside-layer linear model using the outputs of these
sub-models, including the perceptron. Since the per-
ceptron is fixed during the second training step, the
whole training procedure need relative small time
and memory cost.
The outside-layer linear model, similar to those
in SMT, can synthetically utilize different knowl-
edge sources to conduct more accurate comparison
between candidates. In this layer, each knowledge
source is treated as a feature with a corresponding
weight denoting its relative importance. Suppose we
have n features gj (j = 1..n) coupled with n corre-
 0
 300000
 600000
 900000
 1.2e+006
 1.5e+006
 1.8e+006
 2.1e+006
 2.4e+006
 2.7e+006
 3e+006
 3.3e+006
 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22
Fe
atu
re 
sp
ac
e
Introduction of features
growing curve
Figure 2: Feature space growing curve. The horizontal
scope X[i:j] denotes the introduction of different tem-
plates. X[0:5]: Cn (n = ?2..2); X[5:9]: CnCn+1 (n =
?2..1); X[9:10]: C?1C1; X[10:15]: C0Cn (n =
?2..2); X[15:19]: C0CnCn+1 (n = ?2..1); X[19:20]:
C0C?1C1; X[20:21]: W0; X[21:22]: W?1W0. W0 de-
notes the current considering word, while W?1 denotes
the word in front of W0. All the data are collected from
the training procedure on MSR corpus of SIGHAN bake-
off 2.
sponding weights wj (j = 1..n), each feature gj
gives a score gj(r) to a candidate r, then the total
score of r is given by:
S(r) =
?
j=1..n
wj ? gj(r) (2)
The decoding procedure aims to find the candidate
r? with the highest score:
r? = argmax
r
S(r) (3)
While the mission of the training procedure is to
tune the weights wj(j = 1..n) to guarantee that the
candidate r with the highest score happens to be the
best result with a high probability.
As all the sub-models, including the perceptron,
are regarded as separate features of the outside-layer
linear model, we can train them respectively with
special algorithms. In our experiments we trained
a 3-gram word language model measuring the flu-
ency of the segmentation result, a 4-gram POS lan-
guage model functioning as the product of state-
transition probabilities in HMM, and a word-POS
co-occurrence model describing how much probably
a word sequence coexists with a POS sequence. As
shown in Figure 1, the character-based perceptron is
used as the inside-layer linear model and sends its
output to the outside-layer. Besides the output of the
perceptron, the outside-layer also receive the outputs
900
of the word LM, the POS LM, the co-occurrence
model and a word count penalty which is similar to
the translation length penalty in SMT.
4.1 Language Model
Language model (LM) provides linguistic probabil-
ities of a word sequence. It is an important measure
of fluency of the translation in SMT. Formally, an
n-gram word LM approximates the probability of a
word sequence W = w1:m with the following prod-
uct:
Pwlm(W ) =
m
?
i=1
Pr(wi|wmax(0,i?n+1):i?1) (4)
Similarly, the n-gram POS LM of a POS sequence
T = t1:m is:
Ptlm(T ) =
m
?
i=1
Pr(ti|tmax(0,i?n+1):i?1) (5)
Notice that a bi-gram POS LM functions as the prod-
uct of transition probabilities in HMM.
4.2 Word-POS Co-occurrence Model
Given a training corpus with POS tags, we can train
a word-POS co-occurrence model to approximate
the probability that the word sequence of the la-
belled result co-exists with its corresponding POS
sequence. Using W = w1:m to denote the word se-
quence, T = t1:m to denote the corresponding POS
sequence, P (T |W ) to denote the probability that W
is labelled as T , and P (W |T ) to denote the prob-
ability that T generates W , we can define the co-
occurrence model as follows:
Co(W,T ) = P (T |W )?wt ? P (W |T )?tw (6)
?wt and ?tw denote the corresponding weights of the
two components.
Suppose the conditional probability Pr(t|w) de-
scribes the probability that the word w is labelled as
the POS t, while Pr(w|t) describes the probability
that the POS t generates the word w, then P (T |W )
can be approximated by:
P (T |W ) ?
m
?
k=1
Pr(tk|wk) (7)
And P (W |T ) can be approximated by:
P (W |T ) ?
m
?
k=1
Pr(wk|tk) (8)
Pr(w|t) and Pr(t|w) can be easily acquired by
Maximum Likelihood Estimates (MLE) over the
corpus. For instance, if the word w appears N times
in training corpus and is labelled as POS t for n
times, the probability Pr(t|w) can be estimated by
the formula below:
Pr(t|w) ? nN (9)
The probability Pr(w|t) could be estimated through
the same approach.
To facilitate tuning the weights, we use two com-
ponents of the co-occurrence model Co(W,T ) to
represent the co-occurrence probability of W and T ,
rather than use Co(W,T ) itself. In the rest of the
paper, we will call them labelling model and gener-
ating model respectively.
5 Decoder
Sequence segmentation and labelling problem can
be solved through a viterbi style decoding proce-
dure. In Chinese Joint S&T, the mission of the de-
coder is to find the boundary-POS labelled sequence
with the highest score. Given a Chinese character
sequence C1:n, the decoding procedure can proceed
in a left-right fashion with a dynamic programming
approach. By maintaining a stack of size N at each
position i of the sequence, we can preserve the top N
best candidate labelled results of subsequence C1:i
during decoding. At each position i, we enumer-
ate all possible word-POS pairs by assigning each
POS to each possible word formed from the charac-
ter subsequence spanning length l = 1..min(i,K)
(K is assigned 20 in all our experiments) and ending
at position i, then we derive all candidate results by
attaching each word-POS pair p (of length l) to the
tail of each candidate result at the prior position of p
(position i? l), and select for position i a N -best list
of candidate results from all these candidates. When
we derive a candidate result from a word-POS pair
p and a candidate q at prior position of p, we cal-
culate the scores of the word LM, the POS LM, the
labelling probability and the generating probability,
901
Algorithm 2 Decoding algorithm.
1: Input: character sequence C1:n
2: for i? 1 .. n do
3: L ? ?
4: for l? 1 .. min(i, K) do
5: w ? Ci?l+1:i
6: for t ? POS do
7: p? label w as t
8: for q ? V[i? l] do
9: append D(q, p) to L
10: sort L
11: V[i]? L[1 : N ]
12: Output: n-best results V[n]
as well as the score of the perceptron model. In ad-
dition, we add the score of the word count penalty as
another feature to alleviate the tendency of LMs to
favor shorter candidates. By equation 2, we can syn-
thetically evaluate all these scores to perform more
accurately comparing between candidates.
Algorithm 2 shows the decoding algorithm.
Lines 3 ? 11 generate a N -best list for each char-
acter position i. Line 4 scans words of all possible
lengths l (l = 1..min(i,K), where i points to the
current considering character). Line 6 enumerates
all POS?s for the word w spanning length l and end-
ing at position i. Line 8 considers each candidate
result in N -best list at prior position of the current
word. Function D derives the candidate result from
the word-POS pair p and the candidate q at prior po-
sition of p.
6 Experiments
We reported results from two set of experiments.
The first was conducted to test the performance of
the perceptron on segmentation on the corpus from
SIGHAN Bakeoff 2, including the Academia Sinica
Corpus (AS), the Hong Kong City University Cor-
pus (CityU), the Peking University Corpus (PKU)
and the Microsoft Research Corpus (MSR). The sec-
ond was conducted on the Penn Chinese Treebank
5.0 (CTB5.0) to test the performance of the cascaded
model on segmentation and Joint S&T. In all ex-
periments, we use the averaged parameters for the
perceptrons, and F-measure as the accuracy mea-
sure. With precision P and recall R, the balance
F-measure is defined as: F = 2PR/(P + R).
 0.966
 0.968
 0.97
 0.972
 0.974
 0.976
 0.978
 0.98
 0.982
 0.984
 0  1  2  3  4  5  6  7  8  9  10
F-
me
as
su
re
number of iterations
Perceptron Learning Curve
Non-lex + avg
Lex + avg
Figure 3: Averaged perceptron learning curves with Non-
lexical-target and Lexical-target feature templates.
AS CityU PKU MSR
SIGHAN best 0.952 0.943 0.950 0.964
Zhang & Clark 0.946 0.951 0.945 0.972
our model 0.954 0.958 0.940 0.975
Table 2: F-measure on SIGHAN bakeoff 2. SIGHAN
best: best scores SIGHAN reported on the four corpus,
cited from Zhang and Clark (2007).
6.1 Experiments on SIGHAN Bakeoff
For convenience of comparing with others, we focus
only on the close test, which means that any extra
resource is forbidden except the designated train-
ing corpus. In order to test the performance of the
lexical-target templates and meanwhile determine
the best iterations over the training corpus, we ran-
domly chosen 2, 000 shorter sentences (less than 50
words) as the development set and the rest as the
training set (84, 294 sentences), then trained a per-
ceptron model named NON-LEX using only non-
lexical-target features and another named LEX us-
ing both the two kinds of features. Figure 3 shows
their learning curves depicting the F-measure on the
development set after 1 to 10 training iterations. We
found that LEX outperforms NON-LEX with a mar-
gin of about 0.002 at each iteration, and its learn-
ing curve reaches a tableland at iteration 7. Then
we trained LEX on each of the four corpora for 7
iterations. Test results listed in Table 2 shows that
this model obtains higher accuracy than the best of
SIGHAN Bakeoff 2 in three corpora (AS, CityU
and MSR). On the three corpora, it also outper-
formed the word-based perceptron model of Zhang
and Clark (2007). However, the accuracy on PKU
corpus is obvious lower than the best score SIGHAN
902
Training setting Test task F-measure
POS- Segmentation 0.971
POS+ Segmentation 0.973
POS+ Joint S&T 0.925
Table 3: F-measure on segmentation and Joint S&T of
perceptrons. POS-: perceptron trained without POS,
POS+: perceptron trained with POS.
reported, we need to conduct further research on this
problem.
6.2 Experiments on CTB5.0
We turned to experiments on CTB 5.0 to test the per-
formance of the cascaded model. According to the
usual practice in syntactic analysis, we choose chap-
ters 1? 260 (18074 sentences) as training set, chap-
ter 271? 300 (348 sentences) as test set and chapter
301? 325 (350 sentences) as development set.
At the first step, we conducted a group of contrast-
ing experiments on the core perceptron, the first con-
centrated on the segmentation regardless of the POS
information and reported the F-measure on segmen-
tation only, while the second performed Joint S&T
using POS information and reported the F-measure
both on segmentation and on Joint S&T. Note that
the accuracy of Joint S&T means that a word-POS
pair is recognized only if both the boundary tags and
the POS?s are correctly labelled.
The evaluation results are shown in Table 3. We
find that Joint S&T can also improve the segmen-
tation accuracy. However, the F-measure on Joint
S&T is obvious lower, about a rate of 95% to the
F-measure on segmentation. Similar trend appeared
in experiments of Ng and Low (2004), where they
conducted experiments on CTB 3.0 and achieved F-
measure 0.919 on Joint S&T, a ratio of 96% to the
F-measure 0.952 on segmentation.
As the next step, a group of experiments were
conducted to investigate how well the cascaded lin-
ear model performs. Here the core perceptron was
just the POS+ model in experiments above. Be-
sides this perceptron, other sub-models are trained
and used as additional features of the outside-layer
linear model. We used SRI Language Modelling
Toolkit (Stolcke and Andreas, 2002) to train a 3-
gram word LM with modified Kneser-Ney smooth-
ing (Chen and Goodman, 1998), and a 4-gram POS
Features Segmentation F1 Joint S&T F1
All 0.9785 0.9341
All - PER 0.9049 0.8432
All - WLM 0.9785 0.9340
All - PLM 0.9752 0.9270
All - GPR 0.9774 0.9329
All - LPR 0.9765 0.9321
All - LEN 0.9772 0.9325
Table 4: Contribution of each feture. ALL: all features,
PER: perceptron model, WLM: word language model,
PLM: POS language model, GPR: generating model,
LPR: labelling model, LEN: word count penalty.
LM with Witten-Bell smoothing, and we trained
a word-POS co-occurrence model simply by MLE
without smoothing. To obtain their corresponding
weights, we adapted the minimum-error-rate train-
ing algorithm (Och, 2003) to train the outside-layer
model. In order to inspect how much improvement
each feature brings into the cascaded model, every
time we removed a feature while retaining others,
then retrained the model and tested its performance
on the test set.
Table 4 shows experiments results. We find that
the cascaded model achieves a F-measure increment
of about 0.5 points on segmentation and about 0.9
points on Joint S&T, over the perceptron-only model
POS+. We also find that the perceptron model func-
tions as the kernel of the outside-layer linear model.
Without the perceptron, the cascaded model (if we
can still call it ?cascaded?) performs poorly on both
segmentation and Joint S&T. Among other features,
the 4-gram POS LM plays the most important role,
removing this feature causes F-measure decrement
of 0.33 points on segmentation and 0.71 points on
Joint S&T. Another important feature is the labelling
model. Without it, the F-measure on segmentation
and Joint S&T both suffer a decrement of 0.2 points.
The generating model, which functions as that in
HMM, brings an improvement of about 0.1 points
to each test item. However unlike the three fea-
tures, the word LM brings very tiny improvement.
We suppose that the character-based features used
in the perceptron play a similar role as the lower-
order word LM, and it would be helpful if we train
a higher-order word LM on a larger scale corpus.
Finally, the word count penalty gives improvement
to the cascaded model, 0.13 points on segmentation
903
and 0.16 points on Joint S&T.
In summary, the cascaded model can utilize these
knowledge sources effectively, without causing the
feature space of the percptron becoming even larger.
Experimental results show that, it achieves obvious
improvement over the perceptron-only model, about
from 0.973 to 0.978 on segmentation, and from
0.925 to 0.934 on Joint S&T, with error reductions
of 18.5% and 12% respectively.
7 Conclusions
We proposed a cascaded linear model for Chinese
Joint S&T. Under this model, many knowledge
sources that may be intractable to be incorporated
into the perceptron directly, can be utilized effec-
tively in the outside-layer linear model. This is a
substitute method to use both local and non-local
features, and it would be especially useful when the
training corpus is very large.
However, can the perceptron incorporate all the
knowledge used in the outside-layer linear model?
If this cascaded linear model were chosen, could
more accurate generative models (LMs, word-POS
co-occurrence model) be obtained by training on
large scale corpus even if the corpus is not correctly
labelled entirely, or by self-training on raw corpus in
a similar approach to that of McClosky (2006)? In
addition, all knowledge sources we used in the core
perceptron and the outside-layer linear model come
from the training corpus, whereas many open knowl-
edge sources (lexicon etc.) can be used to improve
performance (Ng and Low, 2004). How can we uti-
lize these knowledge sources effectively? We will
investigate these problems in the following work.
Acknowledgement
This work was done while L. H. was visiting
CAS/ICT. The authors were supported by National
Natural Science Foundation of China, Contracts
60736014 and 60573188, and 863 State Key Project
No. 2006AA010108 (W. J., Q. L., and Y. L.), and by
NSF ITR EIA-0205456 (L. H.). We would also like
to Hwee-Tou Ng for sharing his code, and Yang Liu
and Yun Huang for suggestions.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th ICML, pages 282?289, Mas-
sachusetts, USA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of ACL 2006.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Lawrence. R. Rabiner. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of IEEE, pages 257?286.
Ratnaparkhi and Adwait. 1996. A maximum entropy
part-of-speech tagger. In Proceedings of the Empirical
Methods in Natural Language Processing Conference.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 311?318.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
904
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 161?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Partial Matching Strategy for Phrase-based Statistical Machine Translation
Zhongjun He1,2 and Qun Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{zjhe,liuqun,sxlin}@ict.ac.cn
Abstract
This paper presents a partial matching strat-
egy for phrase-based statistical machine trans-
lation (PBSMT). Source phrases which do not
appear in the training corpus can be trans-
lated by word substitution according to par-
tially matched phrases. The advantage of this
method is that it can alleviate the data sparse-
ness problem if the amount of bilingual corpus
is limited. We incorporate our approach into
the state-of-the-art PBSMT system Moses and
achieve statistically significant improvements
on both small and large corpora.
1 Introduction
Currently, most of the phrase-based statistical ma-
chine translation (PBSMT) models (Marcu and
Wong, 2002; Koehn et al, 2003) adopt full matching
strategy for phrase translation, which means that a
phrase pair (f? , e?) can be used for translating a source
phrase f? , only if f? = f? . Due to lack of generaliza-
tion ability, the full matching strategy has some lim-
itations. On one hand, the data sparseness problem
is serious, especially when the amount of the bilin-
gual data is limited. On the other hand, for a certain
source text, the phrase table is redundant since most
of the bilingual phrases cannot be fully matched.
In this paper, we address the problem of trans-
lation of unseen phrases, the source phrases that
are not observed in the training corpus. The
alignment template model (Och and Ney, 2004)
enhanced phrasal generalizations by using words
classes rather than the words themselves. But the
phrases are overly generalized. The hierarchical
phrase-based model (Chiang, 2005) used hierar-
chical phrase pairs to strengthen the generalization
ability of phrases and allow long distance reorder-
ings. However, the huge grammar table greatly in-
creases computational complexity. Callison-Burch
et al (2006) used paraphrases of the trainig corpus
for translating unseen phrases. But they only found
and used the semantically similar phrases. Another
method is to use multi-parallel corpora (Cohn and
Lapata, 2007; Utiyama and Isahara, 2007) to im-
prove phrase coverage and translation quality.
This paper presents a partial matching strategy for
translating unseen phrases. When encountering un-
seen phrases in a source sentence, we search par-
tially matched phrase pairs from the phrase table.
Then we keep the translations of the matched part
and translate the unmatched part by word substitu-
tion. The advantage of our approach is that we alle-
viate the data sparseness problem without increasing
the amount of bilingual corpus. Moreover, the par-
tially matched phrases are not necessarily synony-
mous. We incorporate the partial matching method
into the state-of-the-art PBSMT system, Moses. Ex-
periments show that, our approach achieves statis-
tically significant improvements not only on small
corpus, but also on large corpus.
2 Partial Matching for PBSMT
2.1 Partial Matching
We use matching similarity to measure how well the
source phrases match each other. Given two source
phrases f?J1 and f? ?
J
1 , the matching similarity is com-
puted as:
161
?/P {I/N <?/N u?/V ??/N
issued warning to the American people
?/P /N <?/N ?5/V ?/N
bring advantage to the Taiwan people
Figure 1: An example of partially matched phrases with
the same POS sequence and word alignment.
SIM(f?J1 , f? ?
J
1 ) =
?J
j=1 ?(fj , f ?j)
J (1)
where,
?(f, f ?) =
{
1 if f = f ?
0 otherwise (2)
Therefore, partial matching takes full matching
(SIM(f? , f?) = 1.0) as a special case. Note that in
order to improve search efficiency, we only consider
the partially matched phrases with the same length.
In our experiments, we use a matching threshold
? to tune the precision of partial matching. Low
threshold indicates high coverage of unseen phrases,
but will suffer from much noise. In order to alleviate
this problem, we search partially matched phrases
under the constraint that they must have the same
parts-of-speech (POS) sequence. See Figure 1 for
illustration. Although the matching similarity of the
two phrases is only 0.2, as they have the same POS
sequence, the word alignments are the same. There-
fore, the lower source phrase can be translated ac-
cording to the upper phrase pair with correct word
reordering. Furthermore, this constraint can sharply
decrease the computational complexity since there
is no need to search the whole phrase table.
2.2 Translating Unseen Phrases
We translate an unseen phrase fJ1 according to the
partially matched phrase pair (f ?J1 , e?I1, a?) as follows:
1. Compare each word between fJ1 and f ?J1 to get
the position set of the different words: P =
{j|fj 6= f ?j , j = 1, 2, . . . , J};
2. Remove f ?j from f ?J1 and e?aj from e?I1, where
j ? P ;
3. Find the translation e for fj(j ? P ) from the
phrase table and put it into the position aj in
e?I1 according to the word alignment a?.
u
?U
-?
I
u
?
-?
?.?
arrived in Prague last evening
u
-?
arrived in
arrived in Thailand yesterday
Figure 2: An example of phrase translation.
Figure 2 shows an example. In fact, we create a
translation template dynamically in step 2:
?u X1 -? X2, arrived in X2 X1? (3)
Here, on the source side, each of the non-terminal
X corresponds to a single source word. In addition,
the removed sub-phrase pairs should be consistent
with the word alignment matrix.
Following conventional PBSMT models, we use
4 features to measure phrase translation quality: the
translation weights p(f? |e?) and p(e?|f?), the lexical
weights pw(f? |e?) and pw(e?|f?). The new constructed
phrase pairs keep the translation weights of their
?parent? phrase pair. The lexical weights are com-
puted by word substitution. Suppose S{(f ?, e?)} is
the pair set in (f? ?,e??,a?) which replaced by S{(f, e)}
to create the new phrase pair (f? ,e?,a?), the lexical
weight is computed as:
pw(f? |e?, a?)
=
pw(f? ?|e??, a?) ?
?
(f,e)?S{(f,e)} pw(f |e)?
(f ?,e?)?S{(f ?,e?)} pw(f ?|e?)
(4)
Therefore, the newly constructed phrase pairs can be
used for decoding as they have already existed in the
phrase table.
2.3 Incorporating Partial Matching into the
PBSMT Model
In this paper, we incorporate the partial matching
strategy into the state-of-the-art PBSMT system,
Moses1. Given a source sentence, Moses firstly
uses the full matching strategy to search all possi-
ble translation options from the phrase table, and
then uses a beam-search algorithm for decoding.
1http://www.statmt.org/moses/
162
Therefore, we do incorporation by performing par-
tial matching for phrase translation before decod-
ing. The advantage is that the main search algorithm
need not be changed.
For a source phrase f? , we search partially
matched phrase pair (f? ?, e??, a?) from the phrase table.
If SIM(f? , f? ?)=1.0, which means f? is observed in
the training corpus, thus e?? can be directly stored as a
translation option. However, if ? ? SIM(f? , f? ?) <
1.0, we construct translations for f? according to Sec-
tion 2.2. Then the newly constructed translations are
stored as translation options.
Moses uses translation weights and lexical
weights to measure the quality of a phrase transla-
tion pair. For partial matching, besides these fea-
tures, we add matching similarity SIM(f? , f? ?) as a
new feature. For a source phrase, we select top N
translations for decoding. In Moses, N is set by the
pruning parameter ttable-limit.
3 Experiments
We carry out experiments on Chinese-to-English
translation on two tasks: Small-scale task, the train-
ing corpus consists of 30k sentence pairs (840K +
950K words); Large-scale task, the training cor-
pus consists of 2.54M sentence pairs (68M + 74M
words). The 2002 NIST MT evaluation test data is
used as the development set and the 2005 NIST MT
test data is the test set. The baseline system we used
for comparison is the state-of-the-art PBSMT sys-
tem, Moses.
We use the ICTCLAS toolkit2 to perform Chinese
word segmentation and POS tagging. The training
script of Moses is used to train the bilingual corpus.
We set the maximum length of the source phrase
to 7, and record word alignment information in the
phrase table. For the language model, we use the
SRI Language Modeling Toolkit (Stolcke, 2002) to
train a 4-gram model on the Xinhua portion of the
Gigaword corpus.
To run the decoder, we set ttable-limit=20,
distortion-limit=6, stack=100. The translation qual-
ity is evaluated by BLEU-4 (case-sensitive). We per-
form minimum-error-rate training (Och, 2003) to
tune the feature weights of the translation model to
maximize the BLEU score on development set.
2http://www.nlp.org.cn/project/project.php?proj id=6
? 1.0 0.7 0.5 0.3 0.1
BLEU 24.44 24.43 24.86 25.31 25.13
Table 1: Effect of matching threshold on BLEU score.
3.1 Small-scale Task
Table 1 shows the effect of matching threshold on
translation quality. The baseline uses full matching
(?=1.0) for phrase translation and achieves a BLEU
score of 24.44. With the decrease of the matching
threshold, the BLEU scores increase. when ?=0.3,
the system obtains the highest BLEU score of 25.31,
which achieves an absolute improvement of 0.87
over the baseline. However, if the threshold con-
tinue decreasing, the BLEU score decreases. The
reason is that low threshold increases noise for par-
tial matching.
The effect of matching threshold on the coverage
of n-gram phrases is shown in Figure 3. When us-
ing full matching (?=1.0), long phrases (length?3)
face a serious data sparseness problem. With the de-
crease of the threshold, the coverage increases.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 1  2  3  4  5  6  7
co
ve
ra
ge
 ra
tio
 on
 th
e t
es
t s
et
phrase length
?=1.0
?=0.7
?=0.5
?=0.3
?=0.1
Figure 3: Effect of matching threshold on the coverage of
n-gram phrases.
Table 2 shows the phrase number of 1-best out-
put under ?=1.0 and ?=0.3. When ?=1.0, the long
phrases (length?3) only account for 2.9% of the to-
tal phrases. When ?=0.3, the number increases to
10.7%. Moreover, the total phrase of ?=0.3 is less
than that of ?=1.0, since source text is segmented
into more long phrases under partial matching, and
most of the long phrases are translated from partially
matched phrases (the row 0.3? SIM <1.0).
3.2 Large-scale Task
For this task, the BLEU score of the baseline is
30.45. However, for partial matching method with
163
Phrase Length 1 2 3 4 5 6 7 total
?=1.0 19485 4416 615 87 12 2 1 24618
SIM=1.0 14750 2977 387 48 10 1 0?=0.3 0.3? SIM <1.0 0 1196 1398 306 93 17 12 21195
Table 2: Phrase number of 1-best output. ?=1.0 means full matching. For ?=0.3, SIM=1.0 means full matching,
0.3 ? SIM < 1.0 means partial matching.
?=0.53, the BLEU score is 30.96, achieving an ab-
solute improvement of 0.51. Using Zhang?s signif-
icant tester (Zhang et al, 2004), both the improve-
ments on the two tasks are statistically significant at
p < 0.05.
The improvement on large-scale task is less than
that on small-scale task since larger corpus relieves
data sparseness. However, the partial matching ap-
proach can also improve translation quality by using
long phrases. For example, the segmentation and
translation for the Chinese sentence ???L
?????? are as follows:
Full matching:
? | ?L? |? | |?? |?
long term | economic output | , but | the | trend | will
Partial matching:
? | ?L???? |?
but | the long-term trend of economic output | will
Here the source phrase ??L ?  ? ?
?? cannot be fully matched. Thus the decoder
breaks it into 4 short phrases, but performs an in-
correct reordering. Using partial matching, the long
phrase is translated correctly since it can partially
matched the phrase pair ??Lu7,???
the inevitable trend of economic development?.
3.3 Conclusion
This paper presents a partial matching strategy for
phrase-based statistical machine translation. Phrases
which are not observed in the training corpus can
be translated according to partially matched phrases
by word substitution. Our method can relieve data
sparseness problem without increasing the amount
of the corpus. Experiments show that our approach
achieves statistically significant improvements over
the state-of-the-art PBSMT system Moses.
In future, we will study sophisticated partial
matching methods, since current constraints are ex-
cessively strict. Moreover, we will study the effect
3Due to time limit, we do not tune the threshold for large-
scale task.
of word alignment on partial matching, which may
affect word substitution and reordering.
Acknowledgments
We would like to thank Yajuan Lv and Yang Liu
for their valuable suggestions. This work was sup-
ported by the National Natural Science Foundation
of China (NO. 60573188 and 60736014), and the
High Technology Research and Development Pro-
gram of China (NO. 2006AA010108).
References
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of NAACL06, pages 17?24.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL05,
pages 263?270.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: Making effective use of multi-parallel
corpora. In Proc. of ACL07, pages 728?735.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL03,
pages 127?133.
D. Marcu and W. Wong. 2002. A phrasebased joint
probabilitymodel for statistical machine translation. In
Proc. of EMNLP02, pages 133?139.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL03, pages 160?
167.
A. Stolcke. 2002. Srilm ? an extensible language model-
ing toolkit. In Proc. of ICSLP02, pages 901?904.
M. Utiyama and H. Isahara. 2007. A comparison of pivot
methods for phrase-based statistical machine transla-
tion. In Proc. of NAACL-HLT07, pages 484?491.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system? In Proc. of LREC04, pages
2051?2054.
164
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 522?530,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatic Adaptation of Annotation Standards:
Chinese Word Segmentation and POS Tagging ? A Case Study
Wenbin Jiang ? Liang Huang ? Qun Liu ?
?Key Lab. of Intelligent Information Processing ?Google Research
Institute of Computing Technology 1350 Charleston Rd.
Chinese Academy of Sciences Mountain View, CA 94043, USA
P.O. Box 2704, Beijing 100190, China lianghuang@google.com
{jiangwenbin, liuqun}@ict.ac.cn liang.huang.sh@gmail.com
Abstract
Manually annotated corpora are valuable
but scarce resources, yet for many anno-
tation tasks such as treebanking and se-
quence labeling there exist multiple cor-
pora with different and incompatible anno-
tation guidelines or standards. This seems
to be a great waste of human efforts, and
it would be nice to automatically adapt
one annotation standard to another. We
present a simple yet effective strategy that
transfers knowledge from a differently an-
notated corpus to the corpus with desired
annotation. We test the efficacy of this
method in the context of Chinese word
segmentation and part-of-speech tagging,
where no segmentation and POS tagging
standards are widely accepted due to the
lack of morphology in Chinese. Experi-
ments show that adaptation from the much
larger People?s Daily corpus to the smaller
but more popular Penn Chinese Treebank
results in significant improvements in both
segmentation and tagging accuracies (with
error reductions of 30.2% and 14%, re-
spectively), which in turn helps improve
Chinese parsing accuracy.
1 Introduction
Much of statistical NLP research relies on some
sort of manually annotated corpora to train their
models, but these resources are extremely expen-
sive to build, especially at a large scale, for ex-
ample in treebanking (Marcus et al, 1993). How-
ever the linguistic theories underlying these anno-
tation efforts are often heavily debated, and as a re-
sult there often exist multiple corpora for the same
task with vastly different and incompatible anno-
tation philosophies. For example just for English
treebanking there have been the Chomskian-style
{1 B2 o3 ?4 ?5 u6
NR NN VV NR
U.S. Vice-President visited China
{1 B2 o3 ?4 ?5 u6
ns b n v
U.S. Vice President visited-China
Figure 1: Incompatible word segmentation and
POS tagging standards between CTB (upper) and
People?s Daily (below).
Penn Treebank (Marcus et al, 1993) the HPSG
LinGo Redwoods Treebank (Oepen et al, 2002),
and a smaller dependency treebank (Buchholz and
Marsi, 2006). A second, related problem is that
the raw texts are also drawn from different do-
mains, which for the above example range from
financial news (PTB/WSJ) to transcribed dialog
(LinGo). These two problems seem be a great
waste in human efforts, and it would be nice if
one could automatically adapt from one annota-
tion standard and/or domain to another in order
to exploit much larger datasets for better train-
ing. The second problem, domain adaptation, is
very well-studied, e.g. by Blitzer et al (2006)
and Daume? III (2007) (and see below for discus-
sions), so in this paper we focus on the less stud-
ied, but equally important problem of annotation-
style adaptation.
We present a very simple yet effective strategy
that enables us to utilize knowledge from a differ-
ently annotated corpora for the training of a model
on a corpus with desired annotation. The basic
idea is very simple: we first train on a source cor-
pus, resulting in a source classifier, which is used
to label the target corpus and results in a ?source-
style? annotation of the target corpus. We then
522
train a second model on the target corpus with the
first classifier?s prediction as additional features
for guided learning.
This method is very similar to some ideas in
domain adaptation (Daume? III and Marcu, 2006;
Daume? III, 2007), but we argue that the underly-
ing problems are quite different. Domain adapta-
tion assumes the labeling guidelines are preserved
between the two domains, e.g., an adjective is al-
ways labeled as JJ regardless of from Wall Street
Journal (WSJ) or Biomedical texts, and only the
distributions are different, e.g., the word ?control?
is most likely a verb in WSJ but often a noun
in Biomedical texts (as in ?control experiment?).
Annotation-style adaptation, however, tackles the
problem where the guideline itself is changed, for
example, one treebank might distinguish between
transitive and intransitive verbs, while merging the
different noun types (NN, NNS, etc.), and for ex-
ample one treebank (PTB) might be much flatter
than the other (LinGo), not to mention the fun-
damental disparities between their underlying lin-
guistic representations (CFG vs. HPSG). In this
sense, the problem we study in this paper seems
much harder and more motivated from a linguistic
(rather than statistical) point of view. More inter-
estingly, our method, without any assumption on
the distributions, can be simultaneously applied to
both domain and annotation standards adaptation
problems, which is very appealing in practice be-
cause the latter problem often implies the former,
as in our case study.
To test the efficacy of our method we choose
Chinese word segmentation and part-of-speech
tagging, where the problem of incompatible an-
notation standards is one of the most evident: so
far no segmentation standard is widely accepted
due to the lack of a clear definition of Chinese
words, and the (almost complete) lack of mor-
phology results in much bigger ambiguities and
heavy debates in tagging philosophies for Chi-
nese parts-of-speech. The two corpora used in
this study are the much larger People?s Daily (PD)
(5.86M words) corpus (Yu et al, 2001) and the
smaller but more popular Penn Chinese Treebank
(CTB) (0.47M words) (Xue et al, 2005). They
used very different segmentation standards as well
as different POS tagsets and tagging guidelines.
For example, in Figure 1, People?s Daily breaks
?Vice-President? into two words while combines
the phrase ?visited-China? as a compound. Also
CTB has four verbal categories (VV for normal
verbs, and VC for copulas, etc.) while PD has only
one verbal tag (v) (Xia, 2000). It is preferable to
transfer knowledge from PD to CTB because the
latter also annotates tree structures which is very
useful for downstream applications like parsing,
summarization, and machine translation, yet it is
much smaller in size. Indeed, many recent efforts
on Chinese-English translation and Chinese pars-
ing use the CTB as the de facto segmentation and
tagging standards, but suffers from the limited size
of training data (Chiang, 2007; Bikel and Chiang,
2000). We believe this is also a reason why state-
of-the-art accuracy for Chinese parsing is much
lower than that of English (CTB is only half the
size of PTB).
Our experiments show that adaptation from PD
to CTB results in a significant improvement in seg-
mentation and POS tagging, with error reductions
of 30.2% and 14%, respectively. In addition, the
improved accuracies from segmentation and tag-
ging also lead to an improved parsing accuracy on
CTB, reducing 38% of the error propagation from
word segmentation to parsing. We envision this
technique to be general and widely applicable to
many other sequence labeling tasks.
In the rest of the paper we first briefly review
the popular classification-based method for word
segmentation and tagging (Section 2), and then
describe our idea of annotation adaptation (Sec-
tion 3). We then discuss other relevant previous
work including co-training and classifier combina-
tion (Section 4) before presenting our experimen-
tal results (Section 5).
2 Segmentation and Tagging as
Character Classification
Before describing the adaptation algorithm, we
give a brief introduction of the baseline character
classification strategy for segmentation, as well as
joint segmenation and tagging (henceforth ?Joint
S&T?). following our previous work (Jiang et al,
2008). Given a Chinese sentence as sequence of n
characters:
C1 C2 .. Cn
where Ci is a character, word segmentation aims
to split the sequence into m(? n) words:
C1:e1 Ce1+1:e2 .. Cem?1+1:em
where each subsequence Ci:j indicates a Chinese
word spanning from characters Ci to Cj (both in-
523
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
clusive). While in Joint S&T, each word is further
annotated with a POS tag:
C1:e1/t1 Ce1+1:e2/t2 .. Cem?1+1:em/tm
where tk(k = 1..m) denotes the POS tag for the
word Cek?1+1:ek .
2.1 Character Classification Method
Xue and Shen (2003) describe for the first time
the character classification approach for Chinese
word segmentation, where each character is given
a boundary tag denoting its relative position in a
word. In Ng and Low (2004), Joint S&T can also
be treated as a character classification problem,
where a boundary tag is combined with a POS tag
in order to give the POS information of the word
containing these characters. In addition, Ng and
Low (2004) find that, compared with POS tagging
after word segmentation, Joint S&T can achieve
higher accuracy on both segmentation and POS
tagging. This paper adopts the tag representation
of Ng and Low (2004). For word segmentation
only, there are four boundary tags:
? b: the begin of the word
? m: the middle of the word
? e: the end of the word
? s: a single-character word
while for Joint S&T, a POS tag is attached to the
tail of a boundary tag, to incorporate the word
boundary information and POS information to-
gether. For example, b-NN indicates that the char-
acter is the begin of a noun. After all charac-
ters of a sentence are assigned boundary tags (or
with POS postfix) by a classifier, the correspond-
ing word sequence (or with POS) can be directly
derived. Take segmentation for example, a char-
acter assigned a tag s or a subsequence of words
assigned a tag sequence bm?e indicates a word.
2.2 Training Algorithm and Features
Now we will show the training algorithm of the
classifier and the features used. Several classi-
fication models can be adopted here, however,
we choose the averaged perceptron algorithm
(Collins, 2002) because of its simplicity and high
accuracy. It is an online training algorithm and
has been successfully used in many NLP tasks,
such as POS tagging (Collins, 2002), parsing
(Collins and Roark, 2004), Chinese word segmen-
tation (Zhang and Clark, 2007; Jiang et al, 2008),
and so on.
Similar to the situation in other sequence label-
ing problems, the training procedure is to learn a
discriminative model mapping from inputs x ? X
to outputs y ? Y , where X is the set of sentences
in the training corpus and Y is the set of corre-
sponding labelled results. Following Collins, we
use a function GEN(x) enumerating the candi-
date results of an input x , a representation?map-
ping each training example (x, y) ? X ? Y to a
feature vector?(x, y) ? Rd, and a parameter vec-
tor ~? ? Rd corresponding to the feature vector.
For an input character sequence x, we aim to find
an output F (x) that satisfies:
F (x) = argmax
y?GEN(x)
?(x, y) ? ~? (1)
where?(x, y) ?~? denotes the inner product of fea-
ture vector ?(x, y) and the parameter vector ~?.
Algorithm 1 depicts the pseudo code to tune the
parameter vector ~?. In addition, the ?averaged pa-
rameters? technology (Collins, 2002) is used to al-
leviate overfitting and achieve stable performance.
Table 1 lists the feature template and correspond-
ing instances. Following Ng and Low (2004),
the current considering character is denoted as C0,
while the ith character to the left of C0 as C?i,
and to the right as Ci. There are additional two
functions of which each returns some property of a
character. Pu(?) is a boolean function that checks
whether a character is a punctuation symbol (re-
turns 1 for a punctuation, 0 for not). T (?) is a
multi-valued function, it classifies a character into
four classifications: number, date, English letter
and others (returns 1, 2, 3 and 4, respectively).
3 Automatic Annotation Adaptation
From this section, several shortened forms are
adopted for representation inconvenience. We use
source corpus to denote the corpus with the anno-
tation standard that we don?t require, which is of
524
Feature Template Instances
Ci (i = ?2..2) C?2 =?, C?1 =, C0 =c, C1 =?, C2 = R
CiCi+1 (i = ?2..1) C?2C?1 =?, C?1C0 =c, C0C1 =c?, C1C2 =?R
C?1C1 C?1C1 =?
Pu(C0) Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) T (C?2)T (C?1)T (C0)T (C1)T (C2) = 11243
Table 1: Feature templates and instances from Ng and Low (Ng and Low, 2004). Suppose we are
considering the third character ?c? in ?? c ?R?.
course the source of the adaptation, while target
corpus denoting the corpus with the desired stan-
dard. And correspondingly, the two annotation
standards are naturally denoted as source standard
and target standard, while the classifiers follow-
ing the two annotation standards are respectively
named as source classifier and target classifier, if
needed.
Considering that word segmentation and Joint
S&T can be conducted in the same character clas-
sification manner, we can design an unified stan-
dard adaptation framework for the two tasks, by
taking the source classifier?s classification result
as the guide information for the target classifier?s
classification decision. The following section de-
picts this adaptation strategy in detail.
3.1 General Adaptation Strategy
In detail, in order to adapt knowledge from the
source corpus, first, a source classifier is trained
on it and therefore captures the knowledge it con-
tains; then, the source classifier is used to clas-
sify the characters in the target corpus, although
the classification result follows a standard that we
don?t desire; finally, a target classifier is trained
on the target corpus, with the source classifier?s
classification result as additional guide informa-
tion. The training procedure of the target clas-
sifier automatically learns the regularity to trans-
fer the source classifier?s predication result from
source standard to target standard. This regular-
ity is incorporated together with the knowledge
learnt from the target corpus itself, so as to ob-
tain enhanced predication accuracy. For a given
un-classified character sequence, the decoding is
analogous to the training. First, the character se-
quence is input into the source classifier to ob-
tain an source standard annotated classification
result, then it is input into the target classifier
with this classification result as additional infor-
mation to get the final result. This coincides with
the stacking method for combining dependency
parsers (Martins et al, 2008; Nivre and McDon-
source corpus
train with
normal features
source classifier
train with
additional features
target classifier
target corpus source annotation
classification result
Figure 2: The pipeline for training.
raw sentence source classifier source annotation
classification result
target classifier
target annotation
classification result
Figure 3: The pipeline for decoding.
ald, 2008), and is also similar to the Pred baseline
for domain adaptation in (Daume? III and Marcu,
2006; Daume? III, 2007). Figures 2 and 3 show
the flow charts for training and decoding.
The utilization of the source classifier?s classi-
fication result as additional guide information re-
sorts to the introduction of new features. For the
current considering character waiting for classi-
fication, the most intuitive guide features is the
source classifier?s classification result itself. How-
ever, our effort isn?t limited to this, and more spe-
cial features are introduced: the source classifier?s
classification result is attached to every feature
listed in Table 1 to get combined guide features.
This is similar to feature design in discriminative
dependency parsing (McDonald et al, 2005; Mc-
525
Donald and Pereira, 2006), where the basic fea-
tures, composed of words and POSs in the context,
are also conjoined with link direction and distance
in order to obtain more special features. Table 2
shows an example of guide features and basic fea-
tures, where ?? = b ? represents that the source
classifier classifies the current character as b, the
beginning of a word.
Such combination method derives a series of
specific features, which helps the target classifier
to make more precise classifications. The parame-
ter tuning procedure of the target classifier will au-
tomatically learn the regularity of using the source
classifier?s classification result to guide its deci-
sion making. For example, if a current consid-
ering character shares some basic features in Ta-
ble 2 and it is classified as b, then the target clas-
sifier will probably classify it as m. In addition,
the training procedure of the target classifier also
learns the relative weights between the guide fea-
tures and the basic features, so that the knowledge
from both the source corpus and the target corpus
are automatically integrated together.
In fact, more complicated features can be
adopted as guide information. For error tolerance,
guide features can be extracted from n-best re-
sults or compacted lattices of the source classifier;
while for the best use of the source classifier?s out-
put, guide features can also be the classification
results of several successive characters. We leave
them as future research.
4 Related Works
Co-training (Sarkar, 2001) and classifier com-
bination (Nivre and McDonald, 2008) are two
technologies for training improved dependency
parsers. The co-training technology lets two dif-
ferent parsing models learn from each other dur-
ing parsing an unlabelled corpus: one model
selects some unlabelled sentences it can confi-
dently parse, and provide them to the other model
as additional training corpus in order to train
more powerful parsers. The classifier combina-
tion lets graph-based and transition-based depen-
dency parsers to utilize the features extracted from
each other?s parsing results, to obtain combined,
enhanced parsers. The two technologies aim to
let two models learn from each other on the same
corpora with the same distribution and annota-
tion standard, while our strategy aims to integrate
the knowledge in multiple corpora with different
Baseline Features
C?2 ={
C?1 =B
C0 =o
C1 =?
C2 =?
C?2C?1 ={B
C?1C0 =Bo
C0C1 =o?
C1C2 =??
C?1C1 =B?
Pu(C0) = 0
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444
Guide Features
? = b
C?2 ={ ? ? = b
C?1 =B ? ? = b
C0 =o ? ? = b
C1 =? ? ? = b
C2 =? ? ? = b
C?2C?1 ={B ? ? = b
C?1C0 =Bo ? ? = b
C0C1 =o? ? ? = b
C1C2 =?? ? ? = b
C?1C1 =B? ? ? = b
Pu(C0) = 0 ? ? = b
T (C?2)T (C?1)T (C0)T (C1)T (C2) = 44444 ? ? = b
Table 2: An example of basic features and guide
features of standard-adaptation for word segmen-
tation. Suppose we are considering the third char-
acter ?o? in ?{B o ??u?.
annotation-styles.
Gao et al (2004) described a transformation-
based converter to transfer a certain annotation-
style word segmentation result to another style.
They design some class-type transformation tem-
plates and use the transformation-based error-
driven learning method of Brill (1995) to learn
what word delimiters should be modified. How-
ever, this converter need human designed transfor-
mation templates, and is hard to be generalized to
POS tagging, not to mention other structure label-
ing tasks. Moreover, the processing procedure is
divided into two isolated steps, conversion after
segmentation, which suffers from error propaga-
tion and wastes the knowledge in the corpora. On
the contrary, our strategy is automatic, generaliz-
able and effective.
In addition, many efforts have been devoted
to manual treebank adaptation, where they adapt
PTB to other grammar formalisms, such as such
as CCG and LFG (Hockenmaier and Steedman,
2008; Cahill and Mccarthy, 2007). However, they
are heuristics-based and involve heavy human en-
gineering.
526
5 Experiments
Our adaptation experiments are conducted from
People?s Daily (PD) to Penn Chinese Treebank 5.0
(CTB). These two corpora are segmented follow-
ing different segmentation standards and labeled
with different POS sets (see for example Figure 1).
PD is much bigger in size, with about 100K sen-
tences, while CTB is much smaller, with only
about 18K sentences. Thus a classifier trained on
CTB usually falls behind that trained on PD, but
CTB is preferable because it also annotates tree
structures, which is very useful for downstream
applications like parsing and translation. For ex-
ample, currently, most Chinese constituency and
dependency parsers are trained on some version
of CTB, using its segmentation and POS tagging
as the de facto standards. Therefore, we expect the
knowledge adapted from PD will lead to more pre-
cise CTB-style segmenter and POS tagger, which
would in turn reduce the error propagation to pars-
ing (and translation).
Experiments adapting from PD to CTB are con-
ducted for two tasks: word segmentation alone,
and joint segmentation and POS tagging (Joint
S&T). The performance measurement indicators
for word segmentation and Joint S&T are bal-
anced F-measure, F = 2PR/(P +R), a function
of Precision P and Recall R. For word segmen-
tation, P indicates the percentage of words in seg-
mentation result that are segmented correctly, and
R indicates the percentage of correctly segmented
words in gold standard words. For Joint S&T, P
and R mean nearly the same except that a word
is correctly segmented only if its POS is also cor-
rectly labelled.
5.1 Baseline Perceptron Classifier
We first report experimental results of the single
perceptron classifier on CTB 5.0. The original
corpus is split according to former works: chap-
ters 271 ? 300 for testing, chapters 301 ? 325 for
development, and others for training. Figure 4
shows the learning curves for segmentation only
and Joint S&T, we find all curves tend to moder-
ate after 7 iterations. The data splitting conven-
tion of other two corpora, People?s Daily doesn?t
reserve the development sets, so in the following
experiments, we simply choose the model after 7
iterations when training on this corpus.
The first 3 rows in each sub-table of Table 3
show the performance of the single perceptron
0.880
0.890
0.900
0.910
0.920
0.930
0.940
0.950
0.960
0.970
0.980
 1  2  3  4  5  6  7  8  9  10
F 
m
ea
su
re
number of iterations
segmentation only
segmentation in Joint S&T
Joint S&T
Figure 4: Averaged perceptron learning curves for
segmentation and Joint S&T.
Train on Test on Seg F1% JST F1%
Word Segmentation
PD PD 97.45 ?
PD CTB 91.71 ?
CTB CTB 97.35 ?
PD ? CTB CTB 98.15 ?
Joint S&T
PD PD 97.57 94.54
PD CTB 91.68 ?
CTB CTB 97.58 93.06
PD ? CTB CTB 98.23 94.03
Table 3: Experimental results for both baseline
models and final systems with annotation adap-
tation. PD ? CTB means annotation adaptation
from PD to CTB. For the upper sub-table, items of
JST F1 are undefined since only segmentation is
performs. While in the sub-table below, JST F1
is also undefined since the model trained on PD
gives a POS set different from that of CTB.
models. Comparing row 1 and 3 in the sub-table
below with the corresponding rows in the upper
sub-table, we validate that when word segmenta-
tion and POS tagging are conducted jointly, the
performance for segmentation improves since the
POS tags provide additional information to word
segmentation (Ng and Low, 2004). We also see
that for both segmentation and Joint S&T, the per-
formance sharply declines when a model trained
on PD is tested on CTB (row 2 in each sub-table).
In each task, only about 92% F1 is achieved. This
obviously fall behind those of the models trained
on CTB itself (row 3 in each sub-table), about 97%
F1, which are used as the baselines of the follow-
ing annotation adaptation experiments.
527
POS #Word #BaseErr #AdaErr ErrDec%
AD 305 30 19 36.67 ?
AS 76 0 0
BA 4 1 1
CC 135 8 8
CD 356 21 14 33.33 ?
CS 6 0 0
DEC 137 31 23 25.81 ?
DEG 197 32 37 ?
DEV 10 0 0
DT 94 3 1 66.67 ?
ETC 12 0 0
FW 1 1 1
JJ 127 41 44 ?
LB 2 1 1
LC 106 3 2 33.33 ?
M 349 18 4 77.78 ?
MSP 8 2 1 50.00 ?
NN 1715 151 126 16.56 ?
NR 713 59 50 15.25 ?
NT 178 1 2 ?
OD 84 0 0
P 251 10 6 40.00 ?
PN 81 1 1
PU 997 0 1 ?
SB 2 0 0
SP 2 2 2
VA 98 23 21 08.70 ?
VC 61 0 0
VE 25 1 0 100.00 ?
VV 689 64 40 37.50 ?
SUM 6821 213 169 20.66 ?
Table 4: Error analysis for Joint S&T on the devel-
oping set of CTB. #BaseErr and #AdaErr denote
the count of words that can?t be recalled by the
baseline model and adapted model, respectively.
ErrDec denotes the error reduction of Recall.
5.2 Adaptation for Segmentation and
Tagging
Table 3 also lists the results of annotation adap-
tation experiments. For word segmentation, the
model after annotation adaptation (row 4 in upper
sub-table) achieves an F-measure increment of 0.8
points over the baseline model, corresponding to
an error reduction of 30.2%; while for Joint S&T,
the F-measure increment of the adapted model
(row 4 in sub-table below) is 1 point, which cor-
responds to an error reduction of 14%. In addi-
tion, the performance of the adapted model for
Joint S&T obviously surpass that of (Jiang et al,
2008), which achieves an F1 of 93.41% for Joint
S&T, although with more complicated models and
features.
Due to the obvious improvement brought by an-
notation adaptation to both word segmentation and
Joint S&T, we can safely conclude that the knowl-
edge can be effectively transferred from on an-
Input Type Parsing F1%
gold-standard segmentation 82.35
baseline segmentation 80.28
adapted segmentation 81.07
Table 5: Chinese parsing results with different
word segmentation results as input.
notation standard to another, although using such
a simple strategy. To obtain further information
about what kind of errors be alleviated by annota-
tion adaptation, we conduct an initial error analy-
sis for Joint S&T on the developing set of CTB. It
is reasonable to investigate the error reduction of
Recall for each word cluster grouped together ac-
cording to their POS tags. From Table 4 we find
that out of 30 word clusters appeared in the devel-
oping set of CTB, 13 clusters benefit from the an-
notation adaptation strategy, while 4 clusters suf-
fer from it. However, the compositive error rate of
Recall for all word clusters is reduced by 20.66%,
such a fact invalidates the effectivity of annotation
adaptation.
5.3 Contribution to Chinese Parsing
We adopt the Chinese parser of Xiong et al
(2005), and train it on the training set of CTB 5.0
as described before. To sketch the error propaga-
tion to parsing from word segmentation, we rede-
fine the constituent span as a constituent subtree
from a start character to a end character, rather
than from a start word to a end word. Note that if
we input the gold-standard segmented test set into
the parser, the F-measure under the two definitions
are the same.
Table 5 shows the parsing accuracies with dif-
ferent word segmentation results as the parser?s
input. The parsing F-measure corresponding to
the gold-standard segmentation, 82.35, represents
the ?oracle? accuracy (i.e., upperbound) of pars-
ing on top of automatic word segmention. After
integrating the knowledge from PD, the enhanced
word segmenter gains an F-measure increment of
0.8 points, which indicates that 38% of the error
propagation from word segmentation to parsing is
reduced by our annotation adaptation strategy.
6 Conclusion and Future Works
This paper presents an automatic annotation adap-
tation strategy, and conducts experiments on a
classic problem: word segmentation and Joint
528
S&T. To adapt knowledge from a corpus with an
annotation standard that we don?t require, a clas-
sifier trained on this corpus is used to pre-process
the corpus with the desired annotated standard, on
which a second classifier is trained with the first
classifier?s predication results as additional guide
information. Experiments of annotation adapta-
tion from PD to CTB 5.0 for word segmentation
and POS tagging show that, this strategy can make
effective use of the knowledge from the corpus
with different annotations. It obtains considerable
F-measure increment, about 0.8 point for word
segmentation and 1 point for Joint S&T, with cor-
responding error reductions of 30.2% and 14%.
The final result outperforms the latest work on the
same corpus which uses more complicated tech-
nologies, and achieves the state-of-the-art. More-
over, such improvement further brings striking F-
measure increment for Chinese parsing, about 0.8
points, corresponding to an error propagation re-
duction of 38%.
In the future, we will continue to research on
annotation adaptation for other NLP tasks which
have different annotation-style corpora. Espe-
cially, we will pay efforts to the annotation stan-
dard adaptation between different treebanks, for
example, from HPSG LinGo Redwoods Treebank
to PTB, or even from a dependency treebank
to PTB, in order to obtain more powerful PTB
annotation-style parsers.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are especially grateful to
Fernando Pereira and the anonymous reviewers
for pointing us to relevant domain adaption refer-
ences. We also thank Yang Liu and Haitao Mi for
helpful discussions.
References
Daniel M. Bikel and David Chiang. 2000. Two statis-
tical parsing models applied to the chinese treebank.
In Proceedings of the second workshop on Chinese
language processing.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. In Computational
Linguistics.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Aoife Cahill and Mairead Mccarthy. 2007. Auto-
matic annotation of the penn treebank with lfg f-
structure information. In in Proceedings of the
LREC Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated
Language Data.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 201?228.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42th Annual Meeting of the Association
for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8, Philadelphia, USA.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. In Journal of Artifi-
cial Intelligence Research.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Julia Hockenmaier and Mark Steedman. 2008. Ccg-
bank: a corpus of ccg derivations and dependency
structures extracted from the penn treebank. In
Computational Linguistics, volume 33(3), pages
355?396.
Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
529
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics (COLING 2002).
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0). In Technical
Reports.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics.
530
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558?566,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Improving Tree-to-Tree Translation with Packed Forests
Yang Liu and Yajuan Lu? and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,lvyajuan,liuqun}@ict.ac.cn
Abstract
Current tree-to-tree models suffer from
parsing errors as they usually use only 1-
best parses for rule extraction and decod-
ing. We instead propose a forest-based
tree-to-tree model that uses packed forests.
The model is based on a probabilis-
tic synchronous tree substitution gram-
mar (STSG), which can be learned from
aligned forest pairs automatically. The de-
coder finds ways of decomposing trees in
the source forest into elementary trees us-
ing the source projection of STSG while
building target forest in parallel. Compa-
rable to the state-of-the-art phrase-based
system Moses, using packed forests in
tree-to-tree translation results in a signif-
icant absolute improvement of 3.6 BLEU
points over using 1-best trees.
1 Introduction
Approaches to syntax-based statistical machine
translation make use of parallel data with syntactic
annotations, either in the form of phrase structure
trees or dependency trees. They can be roughly
divided into three categories: string-to-tree mod-
els (e.g., (Galley et al, 2006; Marcu et al, 2006;
Shen et al, 2008)), tree-to-string models (e.g.,
(Liu et al, 2006; Huang et al, 2006)), and tree-to-
tree models (e.g., (Eisner, 2003; Ding and Palmer,
2005; Cowan et al, 2006; Zhang et al, 2008)).
By modeling the syntax of both source and tar-
get languages, tree-to-tree approaches have the po-
tential benefit of providing rules linguistically bet-
ter motivated. However, while string-to-tree and
tree-to-string models demonstrate promising re-
sults in empirical evaluations, tree-to-tree models
have still been underachieving.
We believe that tree-to-tree models face two
major challenges. First, tree-to-tree models are
more vulnerable to parsing errors. Obtaining
syntactic annotations in quantity usually entails
running automatic parsers on a parallel corpus.
As the amount and domain of the data used to
train parsers are relatively limited, parsers will
inevitably output ill-formed trees when handling
real-world text. Guided by such noisy syntactic in-
formation, syntax-based models that rely on 1-best
parses are prone to learn noisy translation rules
in training phase and produce degenerate trans-
lations in decoding phase (Quirk and Corston-
Oliver, 2006). This situation aggravates for tree-
to-tree models that use syntax on both sides.
Second, tree-to-tree rules provide poorer rule
coverage. As a tree-to-tree rule requires that there
must be trees on both sides, tree-to-tree mod-
els lose a larger amount of linguistically unmoti-
vated mappings. Studies reveal that the absence of
such non-syntactic mappings will impair transla-
tion quality dramatically (Marcu et al, 2006; Liu
et al, 2007; DeNeefe et al, 2007; Zhang et al,
2008).
Compactly encoding exponentially many
parses, packed forests prove to be an excellent
fit for alleviating the above two problems (Mi et
al., 2008; Mi and Huang, 2008). In this paper,
we propose a forest-based tree-to-tree model. To
learn STSG rules from aligned forest pairs, we in-
troduce a series of notions for identifying minimal
tree-to-tree rules. Our decoder first converts the
source forest to a translation forest and then finds
the best derivation that has the source yield of one
source tree in the forest. Comparable to Moses,
our forest-based tree-to-tree model achieves an
absolute improvement of 3.6 BLEU points over
conventional tree-based model.
558
IP1
NP2 VP3
PP4 VP-B5
NP-B6 NP-B7 NP-B8
NR9 CC10P11 NR12 VV13 AS14 NN15
bushi yu shalong juxing le huitan
Bush held a talk with Sharon
NNP16 VBD17 DT18 NN19 IN20 NNP21
NP22 NP23 NP24
NP25 PP26
NP27
VP28
S 29
Figure 1: An aligned packed forest pair. Each
node is assigned a unique identity for reference.
The solid lines denote hyperedges and the dashed
lines denote word alignments. Shaded nodes are
frontier nodes.
2 Model
Figure 1 shows an aligned forest pair for a Chinese
sentence and an English sentence. The solid lines
denote hyperedges and the dashed lines denote
word alignments between the two forests. Each
node is assigned a unique identity for reference.
Each hyperedge is associated with a probability,
which we omit in Figure 1 for clarity. In a forest,
a node usually has multiple incoming hyperedges.
We use IN(v) to denote the set of incoming hy-
peredges of node v. For example, the source node
?IP1? has following two incoming hyperedges: 1
e1 = ?(NP-B6,VP3), IP1?
e2 = ?(NP2,VP-B5), IP1?
1As there are both source and target forests, it might be
confusing by just using a span to refer to a node. In addition,
some nodes will often have the same labels and spans. There-
fore, it is more convenient to use an identity for referring to a
node. The notation ?IP1? denotes the node that has a label of
?IP? and has an identity of ?1?.
Formally, a packed parse forest is a compact
representation of all the derivations (i.e., parse
trees) for a given sentence under a context-free
grammar. Huang and Chiang (2005) define a for-
est as a tuple ?V,E, v?,R?, where V is a finite set
of nodes, E is a finite set of hyperedges, v? ? V is
a distinguished node that denotes the goal item in
parsing, and R is the set of weights. For a given
sentence w1:l = w1 . . . wl, each node v ? V is in
the form of Xi,j , which denotes the recognition of
non-terminal X spanning the substring from posi-
tions i through j (that is, wi+1 . . . wj). Each hy-
peredge e ? E is a triple e = ?T (e), h(e), f(e)?,
where h(e) ? V is its head, T (e) ? V ? is a vector
of tail nodes, and f(e) is a weight function from
R|T (e)| to R.
Our forest-based tree-to-tree model is based on
a probabilistic STSG (Eisner, 2003). Formally,
an STSG can be defined as a quintuple G =
?Fs,Ft,Ss,St, P ?, where
? Fs andFt are the source and target alhabets,
respectively,
? Ss and St are the source and target start sym-
bols, and
? P is a set of production rules. A rule r is a
triple ?ts, tt,?? that describes the correspon-
dence ? between a source tree ts and a target
tree tt.
To integrate packed forests into tree-to-tree
translation, we model the process of synchronous
generation of a source forest Fs and a target forest
Ft using a probabilistic STSG grammar:
Pr(Fs, Ft) =
?
Ts?Fs
?
Tt?Ft
Pr(Ts, Tt)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
Pr(d)
=
?
Ts?Fs
?
Tt?Ft
?
d?D
?
r?d
p(r) (1)
where Ts is a source tree, Tt is a target tree, D is
the set of all possible derivations that transform Ts
into Tt, d is one such derivation, and r is a tree-to-
tree rule.
Table 1 shows a derivation of the forest pair in
Figure 1. A derivation is a sequence of tree-to-tree
rules. Note that we use x to represent a nontermi-
nal.
559
(1) IP(x1:NP-B, x2:VP)? S(x1:NP, x2:VP)
(2) NP-B(x1:NR)? NP(x1:NNP)
(3) NR(bushi)? NNP(Bush)
(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))? VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)
(5) PP(x1:P, x2:NP-B)? PP(x1:IN, x2:NP)
(6) P(yu)? IN(with)
(7) NP-B(x1:NR)? NP(x1:NP)
(8) NR(shalong) ? NNP(Sharon)
(9) VV(juxing) ? VBD(held)
(10) NP-B(x1:NN)? NP(x1:NN)
(11) NN(huitan) ? NN(talk)
Table 1: A minimal derivation of the forest pair in Figure 1.
id span cspan complement consistent frontier counterparts
1 1-6 1-2, 4-6 1 1 29
2 1-3 1, 5-6 2, 4 0 0
3 2-6 2, 4-6 1 1 1 28
4 2-3 5-6 1-2, 4 1 1 25, 26
5 4-6 2, 4 1, 5-6 1 0
6 1-1 1 2, 4-6 1 1 16, 22
7 3-3 6 1-2, 4-5 1 1 21, 24
8 6-6 4 1-2, 5-6 1 1 19, 23
9 1-1 1 2, 4-6 1 1 16, 22
10 2-2 5 1-2, 4, 6 1 1 20
11 2-2 5 1-2, 4, 6 1 1 20
12 3-3 6 1-2, 4-5 1 1 21, 24
13 4-4 2 1, 4-6 1 1 17
14 5-5 1-2, 4-6 1 0
15 6-6 4 1-2, 5-6 1 1 19, 23
16 1-1 1 2-4, 6 1 1 6, 9
17 2-2 4 1-3, 6 1 1 13
18 3-3 1-4, 6 1 0
19 4-4 6 1-4 1 1 8, 15
20 5-5 2 1, 3-4, 6 1 1 10, 11
21 6-6 3 1-2, 4, 6 1 1 7, 12
22 1-1 1 2-4, 6 1 1 6, 9
23 3-4 6 1-4 1 1 8, 15
24 6-6 3 1-2, 4, 6 1 1 7, 12
25 5-6 2-3 1, 4, 6 1 1 4
26 5-6 2-3 1, 4, 6 1 1 4
27 3-6 2-3, 6 1, 4 0 0
28 2-6 2-4, 6 1 1 1 3
29 1-6 1-4, 6 1 1 1
Table 2: Node attributes of the example forest pair.
3 Rule Extraction
Given an aligned forest pair as shown in Figure
1, how to extract all valid tree-to-tree rules that
explain its synchronous generation process? By
constructing a theory that gives formal seman-
tics to word alignments, Galley et al (2004)
give principled answers to these questions for ex-
tracting tree-to-string rules. Their GHKM proce-
dure draws connections among word alignments,
derivations, and rules. They first identify the
tree nodes that subsume tree-string pairs consis-
tent with word alignments and then extract rules
from these nodes. By this means, GHKM proves
to be able to extract all valid tree-to-string rules
from training instances. Although originally de-
veloped for the tree-to-string case, it is possible to
extend GHKM to extract all valid tree-to-tree rules
from aligned packed forests.
In this section, we introduce our tree-to-tree rule
extraction method adapted from GHKM, which
involves four steps: (1) identifying the correspon-
dence between the nodes in forest pairs, (2) iden-
tifying minimum rules, (3) inferring composed
rules, and (4) estimating rule probabilities.
3.1 Identifying Correspondence Between
Nodes
To learn tree-to-tree rules, we need to find aligned
tree pairs in the forest pairs. To do this, the start-
ing point is to identify the correspondence be-
tween nodes. We propose a number of attributes
for nodes, most of which derive from GHKM, to
facilitate the identification.
Definition 1 Given a node v, its span ?(v) is an
index set of the words it covers.
For example, the span of the source node
?VP-B5? is {4, 5, 6} as it covers three source
words: ?juxing?, ?le?, and ?huitan?. For conve-
nience, we use {4-6} to denotes a contiguous span
{4, 5, 6}.
Definition 2 Given a node v, its corresponding
span ?(v) is the index set of aligned words on an-
other side.
For example, the corresponding span of the
source node ?VP-B5? is {2, 4}, corresponding to
the target words ?held? and ?talk?.
Definition 3 Given a node v, its complement span
?(v) is the union of corresponding spans of nodes
that are neither antecedents nor descendants of v.
For example, the complement span of the source
node ?VP-B5? is {1, 5-6}, corresponding to target
words ?Bush?, ?with?, and ?Sharon?.
Definition 4 A node v is said to be consistent with
alignment if and only if closure(?(v))??(v) = ?.
For example, the closure of the corresponding
span of the source node ?VP-B5? is {2-4} and
its complement span is {1, 5-6}. As the intersec-
tion of the closure and the complement span is an
empty set, the source node ?VP-B5? is consistent
with the alignment.
560
PP4
NP-B7
P11 NR12
PP4
P11 NP-B7
PP4
NP-B7
P11 NR12
PP26
IN20
NP24
NNP21
PP4
P11 NP-B7
PP26
IN 20 NP24
(a) (b) (c) (d)
Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontier
tree pair. All trees are taken from the example forest pair in Figure 1. Shaded nodes are frontier nodes.
Each node is assigned an identity for reference.
Definition 5 A node v is said to be a frontier node
if and only if:
1. v is consistent;
2. There exists at least one consistent node v? on
another side satisfying:
? closure(?(v?)) ? ?(v);
? closure(?(v)) ? ?(v?).
v? is said to be a counterpart of v. We use ?(v) to
denote the set of counterparts of v.
A frontier node often has multiple counter-
parts on another side due to the usage of unary
rules in parsers. For example, the source node
?NP-B6? has two counterparts on the target side:
?NNP16? and ?NP22?. Conversely, the target node
?NNP16? also has two counterparts counterparts
on the source side: ?NR9? and ?NP-B6?.
The node attributes of the example forest pair
are listed in Table 2. We use identities to refer to
nodes. ?cspan? denotes corresponding span and
?complement? denotes complement span. In Fig-
ure 1, there are 12 frontier nodes (highlighted by
shading) on the source side and 12 frontier nodes
on the target side. Note that while a consistent
node is equal to a frontier node in GHKM, this is
not the case in our method because we have a tree
on the target side. Frontier nodes play a critical
role in forest-based rule extraction because they
indicate where to cut the forest pairs to obtain tree-
to-tree rules.
3.2 Identifying Minimum Rules
Given the frontier nodes, the next step is to iden-
tify aligned tree pairs, from which tree-to-tree
rules derive. Following Galley et al (2006), we
distinguish between minimal and composed rules.
As a composed rule can be decomposed as a se-
quence of minimal rules, we are particularly inter-
ested in how to extract minimal rules. Also, we in-
troduce a number of notions to help identify mini-
mal rules.
Definition 6 A frontier tree is a subtree in a forest
satisfying:
1. Its root is a frontier node;
2. If the tree contains only one node, it must be
a lexicalized frontier node;
3. If the tree contains more than one nodes,
its leaves are either non-lexicalized frontier
nodes or lexicalized non-frontier nodes.
For example, Figure 2(a) shows a frontier tree
in which all nodes are frontier nodes.
Definition 7 A minimal frontier tree is a frontier
tree such that all nodes other than the root and
leaves are non-frontier nodes.
For example, Figure 2(b) shows a minimal fron-
tier tree.
Definition 8 A frontier tree pair is a triple
?ts, tt,?? satisfying:
1. ts is a source frontier tree;
561
2. tt is a target frontier tree;
3. The root of ts is a counterpart of that of tt;
4. There is a one-to-one correspondence ? be-
tween the frontier leaves of ts and tt.
For example, Figure 2(c) shows a frontier tree
pair.
Definition 9 A frontier tree pair ?ts, tt,?? is said
to be a subgraph of another frontier tree pair
?ts?, tt?,??? if and only if:
1. root(ts) = root(ts?);
2. root(tt) = root(tt?);
3. ts is a subgraph of ts?;
4. tt is a subgraph of tt?.
For example, the frontier tree pair shown in Fig-
ure 2(d) is a subgraph of that in Figure 2(c).
Definition 10 A frontier tree pair is said to be
minimal if and only if it is not a subgraph of any
other frontier tree pair that shares with the same
root.
For example, Figure 2(d) shows a minimal fron-
tier tree pair.
Our goal is to find the minimal frontier tree
pairs, which correspond to minimal tree-to-tree
rules. For example, the tree pair shown in Figure
2(d) denotes a minimal rule as follows:
PP(x1:P,x2:NP-B)? PP(x1:IN, x2:NP)
Figure 3 shows the algorithm for identifying
minimal frontier tree pairs. The input is a source
forest Fs, a target forest Ft, and a source frontier
node v (line 1). We use a set P to store collected
minimal frontier tree pairs (line 2). We first call
the procedure FINDTREES(Fs , v) to identify a set
of frontier trees rooted at v in Fs (line 3). For ex-
ample, for the source frontier node ?PP4? in Figure
1, we obtain two frontier trees:
(PP4(P11)(NP-B7))
(PP4(P11)(NP-B7(NR12)))
Then, we try to find the set of corresponding
target frontier trees (i.e., Tt). For each counter-
part v? of v (line 5), we call the procedure FIND-
TREES(Ft, v?) to identify a set of frontier trees
rooted at v? in Ft (line 6). For example, the source
1: procedure FINDTREEPAIRS(Fs , Ft, v)
2: P = ?
3: Ts ? FINDTREES(Fs , v)
4: Tt ? ?
5: for v? ? ?(v) do
6: Tt ? Tt? FINDTREES(Ft , v?)
7: end for
8: for ?ts, tt? ? Ts ? Tt do
9: if ts ? tt then
10: P ? P ? {?ts, tt,??}
11: end if
12: end for
13: for ?ts, tt,?? ? P do
14: if ??ts?, tt?,??? ? P : ?ts?, tt?,??? ?
?ts, tt,?? then
15: P ? P ? {?ts, tt,??}
16: end if
17: end for
18: end procedure
Figure 3: Algorithm for identifying minimal fron-
tier tree pairs.
frontier node ?PP4? has two counterparts on the
target side: ?NP25? and ?PP26?. There are four
target frontier trees rooted at the two nodes:
(NP25(IN20)(NP24))
(NP25(IN20)(NP24(NNP21)))
(PP26(IN20)(NP24))
(PP26(IN20)(NP24(NNP21)))
Therefore, there are 2 ? 4 = 8 pairs of trees.
We examine each tree pair ?ts, tt? (line 8) to see
whether it is a frontier tree pair (line 9) and then
update P (line 10). In the above example, all the
eight tree pairs are frontier tree pairs.
Finally, we keep only minimal frontier tree pairs
in P (lines 13-15). As a result, we obtain the
following two minimal frontier tree pairs for the
source frontier node ?PP4?:
(PP4(P11)(NP-B7))? (NP25(IN20)(NP24))
(PP4(P11)(NP-B7))? (PP26(IN20)(NP24))
To maintain a reasonable rule table size, we re-
strict that the number of nodes in a tree of an STSG
rule is no greater than n, which we refer to as max-
imal node count.
It seems more efficient to let the procedure
FINDTREES(F, v) to search for minimal frontier
562
trees rather than frontier trees. However, a min-
imal frontier tree pair is not necessarily a pair of
minimal frontier trees. On our Chinese-English
corpus, we find that 38% of minimal frontier tree
pairs are not pairs of minimal frontier trees. As a
result, we have to first collect all frontier tree pairs
and then decide on the minimal ones.
Table 1 shows some minimal rules extracted
from the forest pair shown in Figure 1.
3.3 Inferring Composed Rules
After minimal rules are learned, composed rules
can be obtained by composing two or more min-
imal rules. For example, the composition of the
second rule and the third rule in Table 1 produces
a new rule:
NP-B(NR(shalong))? NP(NNP(Sharon))
While minimal rules derive from minimal fron-
tier tree pairs, composed rules correspond to non-
minimal frontier tree pairs.
3.4 Estimating Rule Probabilities
We follow Mi and Huang (2008) to estimate the
fractional count of a rule extracted from an aligned
forest pair. Intuitively, the relative frequency of a
subtree that occurs in a forest is the sum of all the
trees that traverse the subtree divided by the sum
of all trees in the forest. Instead of enumerating
all trees explicitly and computing the sum of tree
probabilities, we resort to inside and outside prob-
abilities for efficient calculation:
c(r) =
p(ts)? ?(root(ts))?
?
v?leaves(ts) ?(v)
?(v?s)
?
p(tt)? ?(root(tt))?
?
v?leaves(tt) ?(v)
?(v?t)
where c(r) is the fractional count of a rule, ts is the
source tree in r, tt is the target tree in r, root(?) a
function that gets tree root, leaves(?) is a function
that gets tree leaves, and ?(v) and ?(v) are outside
and inside probabilities, respectively.
4 Decoding
Given a source packed forest Fs, our decoder finds
the target yield of the single best derivation d that
has source yield of Ts(d) ? Fs:
e? = e
(
argmax
d s.t. Ts(d)?Fs
p(d)
)
(2)
We extend the model in Eq. 1 to a log-linear
model (Och and Ney, 2002) that uses the follow-
ing eight features: relative frequencies in two di-
rections, lexical weights in two directions, num-
ber of rules used, language model score, number
of target words produced, and the probability of
matched source tree (Mi et al, 2008).
Given a source parse forest and an STSG gram-
mar G, we first apply the conversion algorithm
proposed by Mi et al (2008) to produce a trans-
lation forest. The translation forest has a simi-
lar hypergraph structure. While the nodes are the
same as those of the parse forest, each hyperedge
is associated with an STSG rule. Then, the de-
coder runs on the translation forest. We use the
cube pruning method (Chiang, 2007) to approxi-
mately intersect the translation forest with the lan-
guage model. Traversing the translation forest in
a bottom-up order, the decoder tries to build tar-
get parses at each node. After the first pass, we
use lazy Algorithm 3 (Huang and Chiang, 2005)
to generate k-best translations for minimum error
rate training.
5 Experiments
5.1 Data Preparation
We evaluated our model on Chinese-to-English
translation. The training corpus contains 840K
Chinese words and 950K English words. A tri-
gram language model was trained on the English
sentences of the training corpus. We used the 2002
NIST MT Evaluation test set as our development
set, and used the 2005 NIST MT Evaluation test
set as our test set. We evaluated the translation
quality using the BLEU metric, as calculated by
mteval-v11b.pl with its default setting except that
we used case-insensitive matching of n-grams.
To obtain packed forests, we used the Chinese
parser (Xiong et al, 2005) modified by Haitao
Mi and the English parser (Charniak and Johnson,
2005) modified by Liang Huang to produce en-
tire parse forests. Then, we ran the Python scripts
(Huang, 2008) provided by Liang Huang to out-
put packed forests. To prune the packed forests,
Huang (2008) uses inside and outside probabili-
ties to compute the distance of the best derivation
that traverses a hyperedge away from the glob-
ally best derivation. A hyperedge will be pruned
away if the difference is greater than a threshold
p. Nodes with all incoming hyperedges pruned
are also pruned. The greater the threshold p is,
563
p avg trees # of rules BLEU
0 1 73, 614 0.2021 ? 0.0089
2 238.94 105, 214 0.2165 ? 0.0081
5 5.78 ? 106 347, 526 0.2336 ? 0.0078
8 6.59 ? 107 573, 738 0.2373 ? 0.0082
10 1.05 ? 108 743, 211 0.2385 ? 0.0084
Table 3: Comparison of BLEU scores for tree-
based and forest-based tree-to-tree models.
0.04
0.05
0.06
0.07
0.08
0.09
0.10
 0  1  2  3  4  5  6  7  8  9  10  11
co
ve
ra
ge
maximal node count
p=0
p=2
p=5
p=8
p=10
Figure 4: Coverage of lexicalized STSG rules on
bilingual phrases.
the more parses are encoded in a packed forest.
We obtained word alignments of the training
data by first running GIZA++ (Och and Ney, 2003)
and then applying the refinement rule ?grow-diag-
final-and? (Koehn et al, 2003).
5.2 Forests Vs. 1-best Trees
Table 3 shows the BLEU scores of tree-based and
forest-based tree-to-tree models achieved on the
test set over different pruning thresholds. p is the
threshold for pruning packed forests, ?avg trees?
is the average number of trees encoded in one for-
est on the test set, and ?# of rules? is the number
of STSG rules used on the test set. We restrict that
both source and target trees in a tree-to-tree rule
can contain at most 10 nodes (i.e., the maximal
node count n = 10). The 95% confidence inter-
vals were computed using Zhang ?s significance
tester (Zhang et al, 2004).
We chose five different pruning thresholds in
our experiments: p = 0, 2, 5, 8, 10. The forests
pruned by p = 0 contained only 1-best tree per
sentence. With the increase of p, the average num-
ber of trees encoded in one forest rose dramati-
cally. When p was set to 10, there were over 100M
parses encoded in one forest on average.
p extraction decoding
0 1.26 6.76
2 2.35 8.52
5 6.34 14.87
8 8.51 19.78
10 10.21 25.81
Table 4: Comparison of rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence)
Moreover, the more trees are encoded in packed
forests, the more rules are made available to
forest-based models. The number of rules when
p = 10 was almost 10 times of p = 0. With the
increase of the number of rules used, the BLEU
score increased accordingly. This suggests that
packed forests enable tree-to-tree model to learn
more useful rules on the training data. However,
when a pack forest encodes over 1M parses per
sentence, the improvements are less significant,
which echoes the results in (Mi et al, 2008).
The forest-based tree-to-tree model outper-
forms the original model that uses 1-best trees
dramatically. The absolute improvement of 3.6
BLEU points (from 0.2021 to 0.2385) is statis-
tically significant at p < 0.01 using the sign-
test as described by Collins et al (2005), with
700(+1), 360(-1), and 15(0). We also ran Moses
(Koehn et al, 2007) with its default setting us-
ing the same data and obtained a BLEU score of
0.2366, slightly lower than our best result (i.e.,
0.2385). But this difference is not statistically sig-
nificant.
5.3 Effect on Rule Coverage
Figure 4 demonstrates the effect of pruning thresh-
old and maximal node count on rule coverage.
We extracted phrase pairs from the training data
to investigate how many phrase pairs can be cap-
tured by lexicalized tree-to-tree rules that con-
tain only terminals. We set the maximal length
of phrase pairs to 10. For tree-based tree-to-tree
model, the coverage was below 8% even the max-
imal node count was set to 10. This suggests that
conventional tree-to-tree models lose over 92%
linguistically unmotivated mappings due to hard
syntactic constraints. The absence of such non-
syntactic mappings prevents tree-based tree-to-
tree models from achieving comparable results to
phrase-based models. With more parses included
564
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
0.17
0.18
0.19
0.20
 0  1  2  3  4  5  6  7  8  9  10  11
BL
EU
maximal node count
Figure 5: Effect of maximal node count on BLEU
scores.
in packed forests, the rule coverage increased ac-
cordingly. When p = 10 and n = 10, the cov-
erage was 9.7%, higher than that of p = 0. As
a result, packed forests enable tree-to-tree models
to capture more useful source-target mappings and
therefore improve translation quality. 2
5.4 Training and Decoding Time
Table 4 gives the rule extraction time (sec-
onds/1000 sentence pairs) and decoding time (sec-
ond/sentence) with varying pruning thresholds.
We found that the extraction time grew faster than
decoding time with the increase of p. One possi-
ble reason is that the number of frontier tree pairs
(see Figure 3) rose dramatically when more parses
were included in packed forests.
5.5 Effect of Maximal Node Count
Figure 5 shows the effect of maximal node count
on BLEU scores. With the increase of maximal
node count, the BLEU score increased dramati-
cally. This implies that allowing tree-to-tree rules
to capture larger contexts will strengthen the ex-
pressive power of tree-to-tree model.
5.6 Results on Larger Data
We also conducted an experiment on larger data
to further examine the effectiveness of our ap-
proach. We concatenated the small corpus we
used above and the FBIS corpus. After remov-
ing the sentences that we failed to obtain forests,
2Note that even we used packed forests, the rule coverage
was still very low. One reason is that we set the maximal
phrase length to 10 words, while an STSG rule with 10 nodes
in each tree usually cannot subsume 10 words.
the new training corpus contained about 260K sen-
tence pairs with 7.39M Chinese words and 9.41M
English words. We set the forest pruning threshold
p = 5. Moses obtained a BLEU score of 0.3043
and our forest-based tree-to-tree system achieved
a BLEU score of 0.3059. The difference is still not
significant statistically.
6 Related Work
In machine translation, the concept of packed for-
est is first used by Huang and Chiang (2007) to
characterize the search space of decoding with lan-
guage models. The first direct use of packed for-
est is proposed by Mi et al (2008). They replace
1-best trees with packed forests both in training
and decoding and show superior translation qual-
ity over the state-of-the-art hierarchical phrase-
based system. We follow the same direction and
apply packed forests to tree-to-tree translation.
Zhang et al (2008) present a tree-to-tree model
that uses STSG. To capture non-syntactic phrases,
they apply tree-sequence rules (Liu et al, 2007)
to tree-to-tree models. Their extraction algorithm
first identifies initial rules and then obtains abstract
rules. While this method works for 1-best tree
pairs, it cannot be applied to packed forest pairs
because it is impractical to enumerate all tree pairs
over a phrase pair.
While Galley (2004) describes extracting tree-
to-string rules from 1-best trees, Mi and Huang et
al. (2008) go further by proposing a method for
extracting tree-to-string rules from aligned forest-
string pairs. We follow their work and focus on
identifying tree-tree pairs in a forest pair, which is
more difficult than the tree-to-string case.
7 Conclusion
We have shown how to improve tree-to-tree trans-
lation with packed forests, which compactly en-
code exponentially many parses. To learn STSG
rules from aligned forest pairs, we first identify
minimal rules and then get composed rules. The
decoder finds the best derivation that have the
source yield of one source tree in the forest. Ex-
periments show that using packed forests in tree-
to-tree translation results in dramatic improve-
ments over using 1-best trees. Our system also
achieves comparable performance with the state-
of-the-art phrase-based system Moses.
565
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done
while Yang Liu was visiting the SMT group led
by Stephan Vogel at CMU. We thank the anony-
mous reviewers for their insightful comments.
Many thanks go to Liang Huang, Haitao Mi, and
Hao Xiong for their invaluable help in producing
packed forests. We are also grateful to Andreas
Zollmann, Vamshi Ambati, and Kevin Gimpel for
their helpful feedback.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL 2005.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proc. of EMNLP 2006.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP 2007.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proc. of ACL 2005.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003 (Companion Volume).
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of NAACL/HLT 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of COLING/ACL 2006.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT 2005.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL 2007.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proc. of
ACL/HLT 2008.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL 2007 (demonstration session).
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of COLING/ACL 2006.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL 2007.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proc. of EMNLP 2006.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL/HLT 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Chris Quirk and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL/HLT 2008.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores how much improve-
ment do we need to have a better system? In Proc.
of LREC 2004.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL/HLT 2008.
566
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576?584,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Joint Decoding with Multiple Translation Models
Yang Liu and Haitao Mi and Yang Feng and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,htmi,fengyang,liuqun}@ict.ac.cn
Abstract
Current SMT systems usually decode with
single translation models and cannot ben-
efit from the strengths of other models in
decoding phase. We instead propose joint
decoding, a method that combines multi-
ple translation models in one decoder. Our
joint decoder draws connections among
multiple models by integrating the trans-
lation hypergraphs they produce individu-
ally. Therefore, one model can share trans-
lations and even derivations with other
models. Comparable to the state-of-the-art
system combination technique, joint de-
coding achieves an absolute improvement
of 1.5 BLEU points over individual decod-
ing.
1 Introduction
System combination aims to find consensus trans-
lations among different machine translation sys-
tems. It proves that such consensus translations
are usually better than the output of individual sys-
tems (Frederking and Nirenburg, 1994).
Recent several years have witnessed the rapid
development of system combination methods
based on confusion networks (e.g., (Rosti et al,
2007; He et al, 2008)), which show state-of-the-
art performance in MT benchmarks. A confusion
network consists of a sequence of sets of candidate
words. Each candidate word is associated with a
score. The optimal consensus translation can be
obtained by selecting one word from each set of
candidates to maximizing the overall score. While
it is easy and efficient to manipulate strings, cur-
rent methods usually have no access to most infor-
mation available in decoding phase, which might
be useful for obtaining further improvements.
In this paper, we propose a framework for com-
bining multiple translation models directly in de-
coding phase. 1 Based on max-translation decod-
ing and max-derivation decoding used in conven-
tional individual decoders (Section 2), we go fur-
ther to develop a joint decoder that integrates mul-
tiple models on a firm basis:
? Structuring the search space of each model
as a translation hypergraph (Section 3.1),
our joint decoder packs individual translation
hypergraphs together by merging nodes that
have identical partial translations (Section
3.2). Although such translation-level combi-
nation will not produce new translations, it
does change the way of selecting promising
candidates.
? Two models could even share derivations
with each other if they produce the same
structures on the target side (Section 3.3),
which we refer to as derivation-level com-
bination. This method enlarges the search
space by allowing for mixing different types
of translation rules within one derivation.
? As multiple derivations are used for finding
optimal translations, we extend the minimum
error rate training (MERT) algorithm (Och,
2003) to tune feature weights with respect
to BLEU score for max-translation decoding
(Section 4).
We evaluated our joint decoder that integrated
a hierarchical phrase-based model (Chiang, 2005;
Chiang, 2007) and a tree-to-string model (Liu et
al., 2006) on the NIST 2005 Chinese-English test-
set. Experimental results show that joint decod-
1It might be controversial to use the term ?model?, which
usually has a very precise definition in the field. Some
researchers prefer to saying ?phrase-based approaches? or
?phrase-based systems?. On the other hand, other authors
(e.g., (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2007))
do use the expression ?phrase-based models?. In this paper,
we use the term ?model? to emphasize that we integrate dif-
ferent approaches directly in decoding phase rather than post-
processing system outputs.
576
S ? ?X1,X1?
X ? ?fabiao X1, give a X1?
X ? ?yanjiang, talk?
Figure 1: A derivation composed of SCFG rules
that translates a Chinese sentence ?fabiao yan-
jiang? into an English sentence ?give a talk?.
ing with multiple models achieves an absolute im-
provement of 1.5 BLEU points over individual de-
coding with single models (Section 5).
2 Background
Statistical machine translation is a decision prob-
lem where we need decide on the best of target
sentence matching a source sentence. The process
of searching for the best translation is convention-
ally called decoding, which usually involves se-
quences of decisions that translate a source sen-
tence into a target sentence step by step.
For example, Figure 1 shows a sequence of
SCFG rules (Chiang, 2005; Chiang, 2007) that
translates a Chinese sentence ?fabiao yanjiang?
into an English sentence ?give a talk?. Such se-
quence of decisions is called a derivation. In
phrase-based models, a decision can be translating
a source phrase into a target phrase or reordering
the target phrases. In syntax-based models, deci-
sions usually correspond to transduction rules. Of-
ten, there are many derivations that are distinct yet
produce the same translation.
Blunsom et al (2008) present a latent vari-
able model that describes the relationship between
translation and derivation clearly. Given a source
sentence f , the probability of a target sentence e
being its translation is the sum over all possible
derivations:
Pr(e|f) =
?
d??(e,f)
Pr(d, e|f) (1)
where ?(e, f) is the set of all possible derivations
that translate f into e and d is one such derivation.
They use a log-linear model to define the con-
ditional probability of a derivation d and corre-
sponding translation e conditioned on a source
sentence f :
Pr(d, e|f) = exp
?
m ?mhm(d, e, f)
Z(f) (2)
where hm is a feature function, ?m is the asso-
ciated feature weight, and Z(f) is a constant for
normalization:
Z(f) =
?
e
?
d??(e,f)
exp
?
m
?mhm(d, e, f) (3)
A feature value is usually decomposed as the
product of decision probabilities: 2
h(d, e, f) =
?
d?d
p(d) (4)
where d is a decision in the derivation d.
Although originally proposed for supporting
large sets of non-independent and overlapping fea-
tures, the latent variable model is actually a more
general form of conventional linear model (Och
and Ney, 2002).
Accordingly, decoding for the latent variable
model can be formalized as
e? = argmax
e
{
?
d??(e,f)
exp
?
m
?mhm(d, e, f)
}
(5)
where Z(f) is not needed in decoding because it
is independent of e.
Most SMT systems approximate the summa-
tion over all possible derivations by using 1-best
derivation for efficiency. They search for the 1-
best derivation and take its target yield as the best
translation:
e? ? argmax
e,d
{
?
m
?mhm(d, e, f)
}
(6)
We refer to Eq. (5) as max-translation decoding
and Eq. (6) as max-derivation decoding, which are
first termed by Blunsom et al (2008).
By now, most current SMT systems, adopting
either max-derivation decoding or max-translation
decoding, have only used single models in decod-
ing phase. We refer to them as individual de-
coders. In the following section, we will present
a new method called joint decoding that includes
multiple models in one decoder.
3 Joint Decoding
There are two major challenges for combining
multiple models directly in decoding phase. First,
they rely on different kinds of knowledge sources
2There are also features independent of derivations, such
as language model and word penalty.
577
Sgive
0-1
talk
1-2
give a talk
0-2
give talks
0-2
S
give
0-1
speech
1-2
give a talk
0-2
make a speech
0-2
S
give
0-1
talk
1-2
speech
1-2
give a talk
0-2
give talks
0-2
make a speech
0-2
packing(a) (b)
(c)
Figure 2: (a) A translation hypergraph produced by one model; (b) a translation hypergraph produced by
another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote
the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating
that the two models produce the same translations.
and thus need to collect different information dur-
ing decoding. For example, taking a source parse
as input, a tree-to-string decoder (e.g., (Liu et al,
2006)) pattern-matches the source parse with tree-
to-string rules and produces a string on the tar-
get side. On the contrary, a string-to-tree decoder
(e.g., (Galley et al, 2006; Shen et al, 2008)) is a
parser that applies string-to-tree rules to obtain a
target parse for the source string. As a result, the
hypothesis structures of the two models are funda-
mentally different.
Second, translation models differ in decoding
algorithms. Depending on the generating order
of a target sentence, we distinguish between two
major categories: left-to-right and bottom-up. De-
coders that use rules with flat structures (e.g.,
phrase pairs) usually generate target sentences
from left to right while those using rules with hier-
archical structures (e.g., SCFG rules) often run in
a bottom-up style.
In response to the two challenges, we first ar-
gue that the search space of an arbitrary model can
be structured as a translation hypergraph, which
makes each model connectable to others (Section
3.1). Then, we show that a packed translation hy-
pergraph that integrates the hypergraphs of indi-
vidual models can be generated in a bottom-up
topological order, either integrated at the transla-
tion level (Section 3.2) or the derivation level (Sec-
tion 3.3).
3.1 Translation Hypergraph
Despite the diversity of translation models, they all
have to produce partial translations for substrings
of input sentences. Therefore, we represent the
search space of a translation model as a structure
called translation hypergraph.
Figure 2(a) demonstrates a translation hyper-
graph for one model, for example, a hierarchical
phrase-based model. A node in a hypergraph de-
notes a partial translation for a source substring,
except for the starting node ?S?. For example,
given the example source sentence
0 fabiao 1 yanjiang 2
the node ??give talks?, [0, 2]? in Figure 2(a) de-
notes that ?give talks? is one translation of the
source string f21 = ?fabiao yanjiang?.
The hyperedges between nodes denote the deci-
sion steps that produce head nodes from tail nodes.
For example, the incoming hyperedge of the node
??give talks?, [0, 2]? could correspond to an SCFG
rule:
X ? ?X1 yanjiang,X1 talks?
Each hyperedge is associated with a number of
weights, which are the feature values of the corre-
sponding translation rules. A path of hyperedges
constitutes a derivation.
578
Hypergraph Decoding
node translation
hyperedge rule
path derivation
Table 1: Correspondence between translation hy-
pergraph and decoding.
More formally, a hypergraph (Klein and Man-
ning., 2001; Huang and Chiang, 2005) is a tuple
?V,E,R?, where V is a set of nodes, E is a set
of hyperedges, and R is a set of weights. For a
given source sentence f = fn1 = f1 . . . fn, each
node v ? V is in the form of ?t, [i, j]?, which de-
notes the recognition of t as one translation of the
source substring spanning from i through j (that
is, fi+1 . . . fj). Each hyperedge e ? E is a tuple
e = ?tails(e), head(e), w(e)?, where head(e) ?
V is the consequent node in the deductive step,
tails(e) ? V ? is the list of antecedent nodes, and
w(e) is a weight function from R|tails(e)| to R.
As a general representation, a translation hyper-
graph is capable of characterizing the search space
of an arbitrary translation model. Furthermore,
it offers a graphic interpretation of decoding pro-
cess. A node in a hypergraph denotes a translation,
a hyperedge denotes a decision step, and a path
of hyperedges denotes a derivation. A translation
hypergraph is formally a semiring as the weight
of a path is the product of hyperedge weights and
the weight of a node is the sum of path weights.
While max-derivation decoding only retains the
single best path at each node, max-translation de-
coding sums up all incoming paths. Table 1 sum-
marizes the relationship between translation hy-
pergraph and decoding.
3.2 Translation-Level Combination
The conventional interpretation of Eq. (1) is that
the probability of a translation is the sum over all
possible derivations coming from the same model.
Alternatively, we interpret Eq. (1) as that the
derivations could come from different models.3
This forms the theoretical basis of joint decoding.
Although the information inside a derivation
differs widely among translation models, the be-
ginning and end points (i.e., f and e, respectively)
must be identical. For example, a tree-to-string
3The same for all d occurrences in Section 2. For exam-
ple, ?(e, f) might include derivations from various models
now. Note that we still use Z for normalization.
model first parses f to obtain a source tree T (f)
and then transforms T (f) to the target sentence
e. Conversely, a string-to-tree model first parses
f into a target tree T (e) and then takes the surface
string e as the translation. Despite different inside,
their derivations must begin with f and end with e.
This situation remains the same for derivations
between a source substring f ji and its partial trans-
lation t during joint decoding:
Pr(t|f ji ) =
?
d??(t,fji )
Pr(d, t|f ji ) (7)
where d might come from multiple models. In
other words, derivations from multiple models
could be brought together for computing the prob-
ability of one partial translation.
Graphically speaking, joint decoding creates a
packed translation hypergraph that combines in-
dividual hypergraphs by merging nodes that have
identical translations. For example, Figure 2 (a)
and (b) demonstrate two translation hypergraphs
generated by two models respectively and Fig-
ure 2 (c) is the resulting packed hypergraph. The
solid lines denote the hyperedges of the first model
and the dashed lines denote those of the second
model. The shaded nodes are shared by both mod-
els. Therefore, the two models are combined at the
translation level. Intuitively, shared nodes should
be favored in decoding because they offer consen-
sus translations among different models.
Now the question is how to decode with multi-
ple models jointly in just one decoder. We believe
that both left-to-right and bottom-up strategies can
be used for joint decoding. Although phrase-based
decoders usually produce translations from left to
right, they can adopt bottom-up decoding in prin-
ciple. Xiong et al (2006) develop a bottom-up de-
coder for BTG (Wu, 1997) that uses only phrase
pairs. They treat reordering of phrases as a binary
classification problem. On the other hand, it is
possible for syntax-based models to decode from
left to right. Watanabe et al (2006) propose left-
to-right target generation for hierarchical phrase-
based translation. Although left-to-right decod-
ing might enable a more efficient use of language
models and hopefully produce better translations,
we adopt bottom-up decoding in this paper just for
convenience.
Figure 3 demonstrates the search algorithm of
our joint decoder. The input is a source language
sentence fn1 , and a set of translation models M
579
1: procedure JOINTDECODING(fn1 , M )
2: G? ?
3: for l ? 1 . . . n do
4: for all i, j s.t. j ? i = l do
5: for all m ?M do
6: ADD(G, i, j,m)
7: end for
8: PRUNE(G, i, j)
9: end for
10: end for
11: end procedure
Figure 3: Search algorithm for joint decoding.
(line 1). After initializing the translation hyper-
graph G (line 2), the decoder runs in a bottom-
up style, adding nodes for each span [i, j] and for
each model m. For each span [i, j] (lines 3-5),
the procedure ADD(G, i, j,m) add nodes gener-
ated by the model m to the hypergraph G (line 6).
Each model searches for partial translations inde-
pendently: it uses its own knowledge sources and
visits its own antecedent nodes, just running like
a bottom-up individual decoder. After all mod-
els finishes adding nodes for span [i, j], the pro-
cedure PRUNE(G, i, j) merges identical nodes and
removes less promising nodes to control the search
space (line 8). The pruning strategy is similar to
that of individual decoders, except that we require
there must exist at least one node for each model
to ensure further inference.
Although translation-level combination will not
offer new translations as compared to single mod-
els, it changes the way of selecting promising can-
didates in a combined search space and might po-
tentially produce better translations than individ-
ual decoding.
3.3 Derivation-Level Combination
In translation-level combination, different models
interact with each other only at the nodes. The
derivations of one model are unaccessible to other
models. However, if two models produce the same
structures on the target side, it is possible to com-
bine two models within one derivation, which we
refer to as derivation-level combination.
For example, although different on the source
side, both hierarchical phrase-based and tree-to-
string models produce strings of terminals and
nonterminals on the target side. Figure 4 shows
a derivation composed of both hierarchical phrase
IP(x1:VV, x2:NN) ? x1 x2
X ? ?fabiao, give?
X ? ?yanjiang, a talk?
Figure 4: A derivation composed of both SCFG
and tree-to-string rules.
pairs and tree-to-string rules. Hierarchical phrase
pairs are used for translating smaller units and
tree-to-string rules for bigger ones. It is appealing
to combine them in such a way because the hierar-
chical phrase-based model provides excellent rule
coverage while the tree-to-string model offers lin-
guistically motivated non-local reordering. Sim-
ilarly, Blunsom and Osborne (2008) use both hi-
erarchical phrase pairs and tree-to-string rules in
decoding, where source parse trees serve as condi-
tioning context rather than hard constraints.
Depending on the target side output, we dis-
tinguish between string-targeted and tree-targeted
models. String-targeted models include phrase-
based, hierarchical phrase-based, and tree-to-
string models. Tree-targeted models include
string-to-tree and tree-to-tree models. All models
can be combined at the translation level. Models
that share with same target output structure can be
further combined at the derivation level.
The joint decoder usually runs as max-
translation decoding because multiple derivations
from various models are used. However, if all
models involved belong to the same category, a
joint decoder can also adopt the max-derivation
fashion because all nodes and hyperedges are ac-
cessible now (Section 5.2).
Allowing derivations for comprising rules from
different models and integrating their strengths,
derivation-level combination could hopefully pro-
duce new and better translations as compared with
single models.
4 Extended Minimum Error Rate
Training
Minimum error rate training (Och, 2003) is widely
used to optimize feature weights for a linear model
(Och and Ney, 2002). The key idea of MERT is
to tune one feature weight to minimize error rate
each time while keep others fixed. Therefore, each
580
xf(x)
t1
t2
t3
(0, 0) x1 x2
Figure 5: Calculation of critical intersections.
candidate translation can be represented as a line:
f(x) = a? x + b (8)
where a is the feature value of current dimension,
x is the feature weight being tuned, and b is the
dotproduct of other dimensions. The intersection
of two lines is where the candidate translation will
change. Instead of computing all intersections,
Och (2003) only computes critical intersections
where highest-score translations will change. This
method reduces the computational overhead sig-
nificantly.
Unfortunately, minimum error rate training can-
not be directly used to optimize feature weights of
max-translation decoding because Eq. (5) is not a
linear model. However, if we also tune one dimen-
sion each time and keep other dimensions fixed,
we obtain a monotonic curve as follows:
f(x) =
K
?
k=1
eak?x+bk (9)
where K is the number of derivations for a can-
didate translation, ak is the feature value of cur-
rent dimension on the kth derivation and bk is the
dotproduct of other dimensions on the kth deriva-
tion. If we restrict that ak is always non-negative,
the curve shown in Eq. (9) will be a monotoni-
cally increasing function. Therefore, it is possible
to extend the MERT algorithm to handle situations
where multiple derivations are taken into account
for decoding.
The key difference is the calculation of criti-
cal intersections. The major challenge is that two
curves might have multiple intersections while
two lines have at most one intersection. Fortu-
nately, as the curve is monotonically increasing,
we need only to find the leftmost intersection of
a curve with other curves that have greater values
after the intersection as a candidate critical inter-
section.
Figure 5 demonstrates three curves: t1, t2, and
t3. Suppose that the left bound of x is 0, we com-
pute the function values for t1, t2, and t3 at x = 0
and find that t3 has the greatest value. As a result,
we choose x = 0 as the first critical intersection.
Then, we compute the leftmost intersections of t3
with t1 and t2 and choose the intersection closest
to x = 0, that is x1, as our new critical intersec-
tion. Similarly, we start from x1 and find x2 as the
next critical intersection. This iteration continues
until it reaches the right bound. The bold curve de-
notes the translations we will choose over different
ranges. For example, we will always choose t2 for
the range [x1, x2].
To compute the leftmost intersection of two
curves, we divide the range from current critical
intersection to the right bound into many bins (i.e.,
smaller ranges) and search the bins one by one
from left to right. We assume that there is at most
one intersection in each bin. As a result, we can
use the Bisection method for finding the intersec-
tion in each bin. The search process ends immedi-
ately once an intersection is found.
We divide max-translation decoding into three
phases: (1) build the translation hypergraphs, (2)
generate n-best translations, and (3) generate n?-
best derivations. We apply Algorithm 3 of Huang
and Chiang (2005) for n-best list generation. Ex-
tended MERT runs on n-best translations plus n?-
best derivations to optimize the feature weights.
Note that feature weights of various models are
tuned jointly in extended MERT.
5 Experiments
5.1 Data Preparation
Our experiments were on Chinese-to-English
translation. We used the FBIS corpus (6.9M +
8.9M words) as the training corpus. For lan-
guage model, we used the SRI Language Mod-
eling Toolkit (Stolcke, 2002) to train a 4-gram
model on the Xinhua portion of GIGAWORD cor-
pus. We used the NIST 2002 MT Evaluation test
set as our development set, and used the NIST
2005 test set as test set. We evaluated the trans-
lation quality using case-insensitive BLEU metric
(Papineni et al, 2002).
Our joint decoder included two models. The
581
Max-derivation Max-translationModel Combination Time BLEU Time BLEU
hierarchical N/A 40.53 30.11 44.87 29.82
tree-to-string N/A 6.13 27.23 6.69 27.11
translation N/A N/A 55.89 30.79both derivation 48.45 31.63 54.91 31.49
Table 2: Comparison of individual decoding and joint decoding on average decoding time (sec-
onds/sentence) and BLEU score (case-insensitive).
first model was the hierarchical phrase-based
model (Chiang, 2005; Chiang, 2007). We obtained
word alignments of training data by first running
GIZA++ (Och and Ney, 2003) and then applying
the refinement rule ?grow-diag-final-and? (Koehn
et al, 2003). About 2.6M hierarchical phrase pairs
extracted from the training corpus were used on
the test set.
Another model was the tree-to-string model
(Liu et al, 2006; Liu et al, 2007). Based on
the same word-aligned training corpus, we ran a
Chinese parser on the source side to obtain 1-best
parses. For 15,157 sentences we failed to obtain
1-best parses. Therefore, only 93.7% of the train-
ing corpus were used by the tree-to-string model.
About 578K tree-to-string rules extracted from the
training corpus were used on the test set.
5.2 Individual Decoding Vs. Joint Decoding
Table 2 shows the results of comparing individ-
ual decoding and joint decoding on the test set.
With conventional max-derivation decoding, the
hierarchical phrase-based model achieved a BLEU
score of 30.11 on the test set, with an average de-
coding time of 40.53 seconds/sentence. We found
that accounting for all possible derivations in max-
translation decoding resulted in a small negative
effect on BLEU score (from 30.11 to 29.82), even
though the feature weights were tuned with respect
to BLEU score. One possible reason is that we
only used n-best derivations instead of all possi-
ble derivations for minimum error rate training.
Max-derivation decoding with the tree-to-string
model yielded much lower BLEU score (i.e.,
27.23) than the hierarchical phrase-based model.
One reason is that the tree-to-string model fails
to capture a large amount of linguistically unmo-
tivated mappings due to syntactic constraints. An-
other reason is that the tree-to-string model only
used part of the training data because of pars-
ing failure. Similarly, accounting for all possible
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
 0  1  2  3  4  5  6  7  8  9  10  11
pe
rc
en
ta
ge
span width
Figure 6: Node sharing in max-translation de-
coding with varying span widths. We retain at
most 100 nodes for each source substring for each
model.
derivations in max-translation decoding failed to
bring benefits for the tree-to-string model (from
27.23 to 27.11).
When combining the two models at the trans-
lation level, the joint decoder achieved a BLEU
score of 30.79 that outperformed the best result
(i.e., 30.11) of individual decoding significantly
(p < 0.05). This suggests that accounting for
all possible derivations from multiple models will
help discriminate among candidate translations.
Figure 6 demonstrates the percentages of nodes
shared by the two models over various span widths
in packed translation hypergraphs during max-
translation decoding. For one-word source strings,
89.33% nodes in the hypergrpah were shared by
both models. With the increase of span width, the
percentage decreased dramatically due to the di-
versity of the two models. However, there still ex-
ist nodes shared by two models even for source
substrings that contain 33 words.
When combining the two models at the deriva-
tion level using max-derivation decoding, the joint
decoder achieved a BLEU score of 31.63 that out-
performed the best result (i.e., 30.11) of individ-
582
Method Model BLEU
hierarchical 30.11individual decoding
tree-to-string 27.23
system combination both 31.50
joint decoding both 31.63
Table 3: Comparison of individual decoding, sys-
tem combination, and joint decoding.
ual decoding significantly (p < 0.01). This im-
provement resulted from the mixture of hierarchi-
cal phrase pairs and tree-to-string rules. To pro-
duce the result, the joint decoder made use of
8,114 hierarchical phrase pairs learned from train-
ing data, 6,800 glue rules connecting partial trans-
lations monotonically, and 16,554 tree-to-string
rules. While tree-to-string rules offer linguistically
motivated non-local reordering during decoding,
hierarchical phrase pairs ensure good rule cover-
age. Max-translation decoding still failed to sur-
pass max-derivation decoding in this case.
5.3 Comparison with System Combination
We re-implemented a state-of-the-art system com-
bination method (Rosti et al, 2007). As shown
in Table 3, taking the translations of the two indi-
vidual decoders as input, the system combination
method achieved a BLEU score of 31.50, slightly
lower than that of joint decoding. But this differ-
ence is not significant statistically.
5.4 Individual Training Vs. Joint Training
Table 4 shows the effects of individual training and
joint training. By individual, we mean that the two
models are trained independently. We concatenate
and normalize their feature weights for the joint
decoder. By joint, we mean that they are trained
together by the extended MERT algorithm. We
found that joint training outperformed individual
training significantly for both max-derivation de-
coding and max-translation decoding.
6 Related Work
System combination has benefited various NLP
tasks in recent years, such as products-of-experts
(e.g., (Smith and Eisner, 2005)) and ensemble-
based parsing (e.g., (Henderson and Brill, 1999)).
In machine translation, confusion-network based
combination techniques (e.g., (Rosti et al, 2007;
He et al, 2008)) have achieved the state-of-the-
art performance in MT evaluations. From a dif-
Training Max-derivation Max-translation
individual 30.70 29.95
joint 31.63 30.79
Table 4: Comparison of individual training and
joint training.
ferent perspective, we try to combine different ap-
proaches directly in decoding phase by using hy-
pergraphs. While system combination techniques
manipulate only the final translations of each sys-
tem, our method opens the possibility of exploit-
ing much more information.
Blunsom et al (2008) first distinguish between
max-derivation decoding and max-translation de-
coding explicitly. They show that max-translation
decoding outperforms max-derivation decoding
for the latent variable model. While they train the
parameters using a maximum a posteriori estima-
tor, we extend the MERT algorithm (Och, 2003)
to take the evaluation metric into account.
Hypergraphs have been successfully used in
parsing (Klein and Manning., 2001; Huang and
Chiang, 2005; Huang, 2008) and machine trans-
lation (Huang and Chiang, 2007; Mi et al, 2008;
Mi and Huang, 2008). Both Mi et al (2008) and
Blunsom et al (2008) use a translation hyper-
graph to represent search space. The difference is
that their hypergraphs are specifically designed for
the forest-based tree-to-string model and the hier-
archical phrase-based model, respectively, while
ours is more general and can be applied to arbi-
trary models.
7 Conclusion
We have presented a framework for including mul-
tiple translation models in one decoder. Repre-
senting search space as a translation hypergraph,
individual models are accessible to others via shar-
ing nodes and even hyperedges. As our decoder
accounts for multiple derivations, we extend the
MERT algorithm to tune feature weights with re-
spect to BLEU score for max-translation decod-
ing. In the future, we plan to optimize feature
weights for max-translation decoding directly on
the entire packed translation hypergraph rather
than on n-best derivations, following the lattice-
based MERT (Macherey et al, 2008).
583
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60873167
and 60736014, and 863 State Key Project No.
2006AA010108. Part of this work was done while
Yang Liu was visiting the SMT group led by
Stephan Vogel at CMU. We thank the anonymous
reviewers for their insightful comments. We are
also grateful to Yajuan Lu?, Liang Huang, Nguyen
Bach, Andreas Zollmann, Vamshi Ambati, and
Kevin Gimpel for their helpful feedback.
References
Phil Blunsom and Mile Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP08.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL08.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of ACL05.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. of ANLP94.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL06.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-HMM-
based hypothesis alignment for combining outputs
from machine translation systems. In Proc. of
EMNLP08.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combining
parsers. In Proc. of EMNLP99.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT05.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proc. of ACL07.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL08.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proc. of ACL08.
Phillip Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of
NAACL03.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL06.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proc. of ACL07.
Wolfgang Macherey, Franz J. Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proc. of EMNLP08.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proc. of EMNLP08.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL08.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL02.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1).
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL03.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL02.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proc. of ACL07.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL08.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proc. of ACL05.
Andreas Stolcke. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP02.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of ACL06.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL06.
584
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 121?124,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Reducing SMT Rule Table with Monolingual Key Phrase
Zhongjun He? Yao Meng? Yajuan Lj ? Hao Yu? Qun Liu?
? Fujitsu R&D Center CO., LTD, Beijing, China
{hezhongjun, mengyao, yu}@cn.fujitsu.com
? Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
{lvyajuan, liuqun}@ict.ac.cn
Abstract
This paper presents an effective approach
to discard most entries of the rule table for
statistical machine translation. The rule ta-
ble is filtered by monolingual key phrases,
which are extracted from source text us-
ing a technique based on term extraction.
Experiments show that 78% of the rule ta-
ble is reduced without worsening trans-
lation performance. In most cases, our
approach results in measurable improve-
ments in BLEU score.
1 Introduction
In statistical machine translation (SMT) commu-
nity, the state-of-the-art method is to use rules that
contain hierarchical structures to model transla-
tion, such as the hierarchical phrase-based model
(Chiang, 2005). Rules are more powerful than
conventional phrase pairs because they contain
structural information for capturing long distance
reorderings. However, hierarchical translation
systems often suffer from a large rule table (the
collection of rules), which makes decoding slow
and memory-consuming.
In the training procedure of SMT systems, nu-
merous rules are extracted from the bilingual cor-
pus. During decoding, however, many of them are
rarely used. One of the reasons is that these rules
have low quality. The rule quality are usually eval-
uated by the conditional translation probabilities,
which focus on the correspondence between the
source and target phrases, while ignore the quality
of phrases in a monolingual corpus.
In this paper, we address the problem of reduc-
ing the rule table with the information of mono-
lingual corpus. We use C-value, a measurement
of automatic term recognition, to score source
phrases. A source phrase is regarded as a key
phrase if its score greater than a threshold. Note
that a source phrase is either a flat phrase consists
of words, or a hierarchical phrase consists of both
words and variables. For rule table reduction, the
rule whose source-side is not key phrase is dis-
carded.
Our approach is different from the previous re-
search. Johnson et al (2007) reduced the phrase
table based on the significance testing of phrase
pair co-occurrence in bilingual corpus. The ba-
sic difference is that they used statistical infor-
mation of bilingual corpus while we use that of
monolingual corpus. Shen et al (2008) pro-
posed a string-to-dependency model, which re-
stricted the target-side of a rule by dependency
structures. Their approach greatly reduced the rule
table, however, caused a slight decrease of trans-
lation quality. They obtained improvements by
incorporating an additional dependency language
model. Different from their research, we restrict
rules on the source-side. Furthermore, the system
complexity is not increased because no additional
model is introduced.
The hierarchical phrase-based model (Chiang,
2005) is used to build a translation system. Exper-
iments show that our approach discards 78% of the
rule table without worsening the translation qual-
ity.
2 Monolingual Phrase Scoring
2.1 Frequency
The basic metrics for phrase scoring is the fre-
quency that a phrase appears in a monolingual cor-
pus. The more frequent a source phrase appears in
a corpus, the greater possibility the rule that con-
tains the source phrase may be used.
However, one limitation of this metrics is that if
we filter the rule table by the source phrase with
lower frequency, most long phrase pairs will be
discarded. Because the longer the phrase is, the
less possibility it appears. However, long phrases
121
are very helpful for reducing ambiguity since they
contains more information than short phrases.
Another limitation is that the frequency metrics
focuses on a phrase appearing by itself while ig-
nores it appears as a substring of longer phrases.
It is therefore inadequate for hierarchical phrases.
We use an example for illustration. Considering
the following three rules (the subscripts indicate
word alignments):
R
1
:
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 137?140,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Sub-Sentence Division for Tree-Based Machine Translation 
 
Hao Xiong*, Wenwen Xu+, Haitao Mi*, Yang Liu* and Qun Liu* 
*Key Lab. of Intelligent Information Processing 
+Key Lab. of Computer System and Architecture 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{xionghao,xuwenwen,htmi,yliu,liuqun}@ict.ac.cn
 
Abstract 
Tree-based statistical machine translation 
models have made significant progress in re-
cent years, especially when replacing 1-best 
trees with packed forests. However, as the 
parsing accuracy usually goes down dramati-
cally with the increase of sentence length, 
translating long sentences often takes long 
time and only produces degenerate transla-
tions. We propose a new method named sub-
sentence division that reduces the decoding 
time and improves the translation quality for 
tree-based translation. Our approach divides 
long sentences into several sub-sentences by 
exploiting tree structures. Large-scale ex-
periments on the NIST 2008 Chinese-to-
English test set show that our approach 
achieves an absolute improvement of 1.1 
BLEU points over the baseline system in 
50% less time. 
1 Introduction 
Tree-based statistical machine translation 
models in days have witness promising progress 
in recent years, such as tree-to-string models (Liu 
et al, 2006; Huang et al, 2006), tree-to-tree 
models (Quirk et al,2005;Zhang et al, 2008). 
Especially, when incorporated with forest, the 
correspondent forest-based tree-to-string models 
(Mi et al, 2008; Zhang et al, 2009), tree-to-tree 
models (Liu et al, 2009) have achieved a prom-
ising improvements over correspondent tree-
based systems. However, when we translate long 
sentences, we argue that two major issues will be 
raised. On one hand, parsing accuracy will be 
lower as the length of sentence grows. It will in-
evitably hurt the translation quality (Quirk and 
Corston-Oliver, 2006; Mi and Huang, 2008). On 
the other hand, decoding on long sentences will 
be time consuming, especially for forest ap-
proaches. So splitting long sentences into sub- 
 
Figure 1. Main framework of our method 
 
sentences becomes a natural way in MT litera-
ture.  
A simple way is to split long sentences by 
punctuations. However, without concerning 
about the original whole tree structures, this ap-
proach will result in ill-formed sub-trees which 
don?t respect to original structures. In this paper, 
we present a new approach, which pays more 
attention to parse trees on the long sentences. We 
firstly parse the long sentences into trees, and 
then divide them accordingly into sub-sentences, 
which will be translated independently (Section 
3). Finally, we combine sub translations into a 
full translation (Section 4). Large-scale experi-
ments (Section 5) show that the BLEU score 
achieved by our approach is 1.1 higher than di-
rect decoding and 0.3 higher than always split-
ting on commas on the 2008 NIST MT Chinese-
English test set. Moreover, our approach has re-
duced decoding time significantly. 
2 Framework  
Our approach works in following steps. 
(1) Split a long sentence into sub-sentences.  
(2) Translate all the sub-sentences respectively. 
(3) Combine the sub-translations.   
Figure 1 illustrates the main idea of our ap-
proach. The crucial issues of our method are how 
to divide long sentences and how to combine the 
sub-translations.  
3 Sub Sentence Division  
Long sentences could be very complicated in 
grammar and sentence structure, thereby creating 
an obstacle for translation. Consequently, we 
need to break them into shorter and easier 
clauses. To divide sentences by punctuation is 
137
 
 
Figure 2. An undividable parse tree 
 
 
Figure 3. A dividable parse tree 
 
one of the most commonly used methods. How-
ever, simply applying this method might damage 
the accuracy of parsing. As a result, the strategy 
we proposed is to operate division while con-
cerning the structure of parse tree. 
As sentence division should not influence the 
accuracy of parsing, we have to be very cautious 
about sentences whose division might decrease 
the accuracy of parsing. Figure 2(a) shows an 
example of the parse tree of an undividable sen-
tence. 
As can be seen in Figure 2, when we divide 
the sentence by comma, it would break the struc-
ture of ?VP? sub-tree and result in a ill-formed 
sub-tree ?VP? (right sub-tree), which don?t have 
a subject and don?t respect to original tree struc-
tures. 
Consequently, the key issue of sentence divi-
sion is finding the sentences that can be divided 
without loosing parsing accuracy. Figure 2(b) 
shows the parse tree of a sentence that can be 
divided by punctuation, as sub-sentences divided 
by comma are independent. The reference trans-
lation of the sentence in figure 3 is 
 
Less than two hours earlier, a Palestinian took 
on a shooting spree on passengers in the town of 
Kfar Saba in northern Israel. 
Pseudocode 1 Check Sub Sentence Divi-
sion Algorithm 
1: procedure CheckSubSentence(sent) 
2: for each word i in sent 
3:    if(i is a comma) 
4:       left={words in left side of i}; 
          //words between last comma and cur-
rent comma i 
5:       right={words in right side of i}; 
         //words between i and next comma or
 semicolon, period, question mark 
6:       isDividePunct[i]=true; 
7:       for each j in left 
8:          if(( LCA(j, i)!=parent[i]) 
9:             isDividePunct[i]=false; 
10:           break; 
11:     for each j in right 
12:        if(( LCA(j, i)!=parent[i]) 
13:           isDividePunct[i]=false; 
14:           break; 
15: function LCA(i, j) 
16:    return lowest common ancestor(i, j);
 
It demonstrates that this long sentence can be 
divided into two sub-sentences, providing a good 
support to our division. 
In addition to dividable sentences and non-
dividable sentences, there are sentences contain-
ing more than one comma, some of which are 
dividable and some are not. However, this does 
not prove to be a problem, as we process each 
comma independently. In other words, we only 
split the dividable part of this kind of sentences, 
leaving the non-dividable part unchanged.  
To find the sentences that can be divided, we 
present a new method and provide its pseudo 
code. Firstly, we divide a sentence by its commas. 
For each word in the sub-sentence on the left 
side of a comma, we compute its lowest common 
ancestor (LCA) with the comma. And we process 
the words in the sub-sentence on the right side of 
the comma in the same way. Finally, we check if 
all the LCA we have computed are comma?s par-
ent node.  If all the LCA are the comma?s parent 
node, the sub-sentences are independent.  
As shown in figure 3, the LCA (AD ?? , 
PU ?),  is ?IP? ,which is the parent node of 
?PU ??; and the LCA (NR ??? , PU ?) is 
also ?IP?.  Till we have checked all the LCA of 
each word and comma, we finally find that all 
the LCA are ?IP?. As a result, this sentence can 
be divided without loosing parsing accuracy. 
LCA can be computed by using union-set (Tar-
jan, 1971) in lineal time. Concerning the  
138
sub-sentence 1: ???? 
Translation 1: Johndroe said                   A1
Translation 2: Johndroe pointed out       A2
Translation 3: Qiang Zhuo said              A3
comma 1: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 2: ???????????
??????????????? 
Translation 1: the two presidents also wel-
comed the US-South Korea free trade 
agreement that was signed yesterday       B1
Translation 2: the two presidents also ex-
pressed welcome to the US ? South Korea 
free trade agreement signed yesterday     B2
comma 2: , 
Translation: punctuation translation (white 
space, that ? ) 
sub-sentence 3:???????????
?????? 
Translation 1: and would work to ensure 
that the congresses of both countries ap-
prove this agreement.                               C1
Translation 2: and will make efforts to en-
sure the Congress to approve this agreement 
of the two countries.                                C2
 
Table 1. Sub translation example 
 
implementation complexity, we have reduced the 
problem to range minimum query problem 
(Bender et al, 2005) with a time complexity of  
(1)?  for querying.  
Above all, our approach for sub sentence 
works as follows: 
(1)Split a sentence by semi-colon if there is 
one. 
(2)Parse a sentence if it contains a comma, 
generating k-best parses (Huang Chiang, 2005) 
with k=10.  
 (3)Use the algorithm in pseudocode 1 to 
check the sentence and divide it if there are 
more than 5 parse trees indicates that the sen-
tence is dividable.  
4 Sub Translation Combining  
For sub translation combining, we mainly use the 
best-first expansion idea from cube pruning 
(Huang and Chiang, 2007) to combine sub- 
translations and generate the whole k-best trans-
lations. We first select the best translation from 
sub translation sets, and then use an interpolation 
 
Test Set 02 05 08 
No Sent Division 34.56 31.26 24.53 
Split by Comma 34.59 31.23 25.39 
Our Approach 34.86 31.23 25.69 
 
Table 2. BLEU results (case sensitive) 
 
Test Set 02 05 08 
No Sent Division 28 h 36 h 52 h 
Split by Comma 18h 23h 29h 
Our Approach 18 h 22 h 26 h 
 
Table 3. Decoding time of our experiments 
(h means hours) 
 
language model for rescoring (Huang and Chiang, 
2007).  
For example, we split the following sentence ??
???,??????????????????
????????,?????????????
????? into three sub-sentences and generate 
some translations, and the results are displayed in 
Table 1.  
As seen in Table 1, for each sub-sentence, 
there are one or more versions of translation. For 
convenience, we label the three translation ver-
sions of sub-sentence 1 as A1, A2, and A3, re-
spectively. Similarly, B1, B2, C1, C2 are also 
labels of translation. We push the A1, white 
space, B1, white space, C1 into the cube, and 
then generate the final translation. 
According to cube pruning algorithm, we will 
generate other translations until we get the best 
list we need. Finally, we rescore the k-best list 
using interpolation language model and find the 
best translation which is A1 that B1 white space 
C1. 
5 Experiments  
5.1 Data preparation 
We conduct our experiments on Chinese-English 
translation, and use the Chinese parser of Xiong 
et al (2005) to parse the source sentences. And 
our decoder is based on forest-based tree-to-
string translation model (Mi et al 2008). 
Our training corpus consists of 2.56 million 
sentence pairs. Forest-based rule extractor (Mi 
and Huang 2008) is used with a pruning thresh-
old p=3. And we use SRI Language Modeling 
Toolkit (Stolcke, 2002) to train two 5-gram lan-
guage models with Kneser-Ney smoothing on the 
English side of the training corpus and the Xin-
hua portion of Gigaword corpora respectively. 
139
We use 2006 NIST MT Evaluation test set as 
development set, and 2002, 2005 and 2008 NIST 
MT Evaluation test sets as test sets. We also use 
minimum error-rate training (Och, 2003) to tune 
our feature weights. We evaluate our results with 
case-sensitive BLEU-4 metric (Papineni et al, 
2002). The pruning threshold p for parse forest in 
decoding time is 12. 
5.2 Results 
The final BLEU results are shown in Table 2, our 
approach has achieved a BLEU score that is 1.1 
higher than direct decoding and 0.3 higher than 
always splitting on commas. 
The decoding time results are presented in Ta-
ble 3. The search space of our experiment is ex-
tremely large due to the large pruning threshold 
(p=12), thus resulting in a long decoding time. 
However, our approach has reduced the decoding 
time by 50% over direct decoding, and 10% over 
always splitting on commas. 
6 Conclusion & Future Work  
We have presented a new sub-sentence division 
method and achieved some good results. In the 
future, we will extend our work from decoding to 
training time, where we divide the bilingual sen-
tences accordingly.  
Acknowledgement 
The authors were supported by National Natural 
Science Foundation of China, Contracts 0873167 
and 60736014, and 863 State Key Project 
No.2006AA010108. We thank Liang Huang for 
his insightful suggestions.  
References  
Bender, Farach-Colton, Pemmasani, Skiena, Sumazin, 
Lowest common ancestors in trees and di- 
rected acyclic graphs. J. Algorithms 57(2), 75?
94 (2005) 
Liang Huang and David Chiang. 2005. Better kbest 
Parsing. In Proceedings of IWPT-2005. 
Liang Huang and David Chiang. 2007. Forest res-
coring: Fast decoding with integrated lan-
guage models. In Proceedings of ACL. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. 
Statistical syntax-directed translation with ex-
tended domain of locality. In Proceedings of 
AMTA 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 127-133. 
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String alignments template for statistical ma-
chine translation. In Proceedings of ACL. 
Yang Liu, Yajuan Lv and Qun Liu.2009. Improving 
Tree-to-Tree Translation with Packed Forests.To 
appear in Proceedings of ACL/IJCNLP.. 
Daniel Marcu, Wei Wang, AbdessamadEchihabi, and 
Kevin Knight. 2006. Statistical Machine Trans-
lation with syntactifiedtarget language 
phrases. In Proceedings of EMNLP. 
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL: HLT. 
Haitao Mi and Liang Huang. 2008. Forest-based 
translation rule extraction. In Proceedings of 
EMNLP. 
Franz J. Och. 2003. Minimum error rate training 
in statistical machine translation. In Proceed-
ings of ACL, pages 160?167. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In 
Proceedings of ACL, pages 311?318,. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically 
informed phrasal SMT. In Proceedings of ACL. 
Chris Quirk and Simon Corston-Oliver. 2006. The 
impact of parse quality on syntactically-
informed statistical machine translation. In 
Proceedings of EMNLP. 
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of 
ICSLP, volume 30, pages 901?904. 
Georgianna Tarjan, Depth First Search and Linear 
Graph Algorithms. SIAM J. Comp. 1:2, pp. 146?
160, 1972. 
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun 
Lin.2005. Parsing the Penn Chinese Treebank 
with semantic knowledge. In Proceedings of 
IJCNLP. 
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, 
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree transla-
tion model. In Proceedings of ACL. 
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and 
Chew Lim Tan. 2009. Forest-based Tree Sequence 
to String Translation Model. To appear in Proceed-
ings of ACL/IJCNLP 
140
A Character-net Based Chinese Text Segmentation Method 
Lixin Zhou 
zhoulx@ict.ac.cn 
Qun Liu  
Liuqun@ ict.ac.cn 
Institute of Computing Technology, Chinese Academy of Science. 
 NO. 6 Kexueyuan South Road, Beijing, China, P.O. BOX 2704, 100080 
 
Abstract 
The segmentation of Chinese texts is a key 
process in Chinese information processing. 
The difficulties in segmentation are the 
process of ambiguous character string and 
unknown Chinese words. In order to obtain 
the correct result, the first is identification of 
all possible candidates of Chinese words in a 
text. In this paper, a data structure 
Chinese-character-net is put forward, then, 
based on this character-net, a new algorithm 
is presented to obtain all possible candidate 
of Chinese words in a text. This paper gives 
the experiment result. Finally the 
characteristics of the algorithm are analysed. 
Keywords: segmentation, connection, 
character-net, ambiguity, unknown words.
 
1 Introduction 
The segmentation of Chinese texts is a key 
problem in Chinese information processing. In the 
process of segmentation, the ambiguity processing, 
unknown Chinese words (not included in the 
lexicon) recognition (such as person names, 
organization names etc) are very difficult. As for 
those problems, many algorithms are put forward 
[Liu 2000]. But the existing algorithms haven?t a 
universal data structure, each algorithm can 
resolve a problem, and correspond to a concrete 
data structure specifically. In process of the 
difficulties, the first step is identification of all 
possible candidates of Chinese words 
segmentation. For examples:
 	

	
these words should be obtained: 
	

		Automatic Recognition of Chinese Unknown Words1 Based on Roles Tagging2 
 
Kevin Zhang (Hua-Ping ZHANG)  Qun LIU   Hao ZHANG    Xue-Qi CHENG 
Software Division, Institute of Computing Technology, Chinese Academy of Sciences 
NO. 6, South Road, Kexueyuan, Zhongguancun, Haidian Dist. P.O. BOX 2704, Beijing, P.R. China, 100080 
Email: {zhanghp,liuqun, zhanghao,cxq}@software.ict.ac.cn 
 
Abstract 
This paper presents a unified solution, which 
is based on the idea of ?roles tagging?, to the 
complicated problems of Chinese unknown words 
recognition. In our approach, an unknown word is 
identified according to its component tokens and 
context tokens. In order to capture the functions of 
tokens, we use the concept of roles. Roles are 
tagged through applying the Viterbi algorithm in 
the fashion of a POS tagger. In the resulted most 
probable roles sequence, all the eligible unknown 
words are recognized through a maximum patterns 
matching. We have got excellent precision and 
recalling rates, especially for person names and 
transliterations. The result and experiments in our 
system ICTCLAS shows that our approach based 
on roles tagging is simple yet effective. 
Keywords: Chinese unknown words recognition, 
roles tagging, word segmentation, Viterbi 
algorithm. 
Introduction 
It is well known that word segmentation is a 
prerequisite to Chinese information processing. 
Previous research and work in word segmentation 
have made great progresses. However, cases with 
unknown words are not satisfactory. In general, 
any lexicon is limited and unable to cover all the 
words in real texts or speeches. According to our 
statistics on a 2,305,896-character news corpus 
from the People's Daily, there are about 1.19% 
unknown words. But they are difficult to be 
recalled and often greatly reduce the recognition 
rate of known words close to them. For example, 
the sentence ? ? ? ? ? ? ? ? ? ? ? 
(Pronunciation: ?Bu Zhang Sun Jia Zheng Zai 
Gong Zuo.?)  has two valid segmentations:  ??
?/???/?/??? (The minister Sun Jiazheng is 
at work) and ??? /?? /?? /?? ? (The 
minister Sun Jia now is at work). ????? is a 
person name in the first, while ???? is another 
name in the latter. Meanwhile, the string ????
?? will lead to overlapping ambiguity and bring a 
collision between the unknown word ???
?? (Sun Jiazheng) and ????(zheng zai; now). 
What?s more, the recognizing precision rates of 
person names, place names, and transliterations are 
91.26%, 69.12%, and 82.83%, respectively, while 
the recalling rates of them are just 68.77%, 60.47%, 
and 78.29%, respectively. (Data from official 
testing in 1999) [Liu (1999)] In a word, unknown 
words recognition has become one of the biggest 
stumbling blocks on the way of Chinese lexical 
analysis. A proper solution is important and 
urgent.  
Various approaches are taken in Chinese 
unknown words recognition. They can be broadly 
categorized into ?one-for-one?, ?one-for-several? 
and ?one-for-all? based on the number of 
categories of unknown words, which can be 
recognized. One-for-one solutions solve a 
particular problem, such as person name 
recognition [Song (1993); Ji (2001)], place name 
recognition [Tan (1999)] and transliteration 
recognition [Sun (1993)]. Similarly, 
one-for-several approaches provide one solution 
for several specific categories of unknown words 
[Lv (2001); Luo (2001)]. One-for-all solutions, as 
far as we know, have not been applicable yet 
[Chen (1999); He (2001)].  
Although currently practicable methods could 
achieve great precision or recalling rates in some 
special cases, they have their inherent deficiencies. 
First of all, rules applied are mostly summarized 
by linguists through painful study of all kinds of 
huge ?special name libraries? [Luo (2001)]. It?s 
time-consuming, expensive and inflexible. The 
categories of unknown words are diverse and the 
amount of such words is huge. With the rapid 
development of the Internet, this situation is 
becoming more and more serious. Therefore, it?s 
very difficult to summarize simple yet thorough 
rules about their compositions and contexts. 
Secondly, the recognition process cannot be 
activated until some ?indicator? tokens are 
scanned in. For instance, possible surnames or 
titles often trigger person name recognition on the 
following 2 or more characters. In the case of 
place name recognition, the postfixes such as 
? ? ?(county), ? ? ?(city) will activate the 
recognition on the previous characters. What?s 
more, these methods tend to work only on the 
monosyllabic tokens, which are obvious fragments 
after tokenization [Luo (2001); Lv (2001)]. It takes 
the risk of losing lots of unknown words without 
any explicit features. Furthermore, this trigger 
mechanism cannot resolve the ambiguity. For 
example, unknown word ????? (Fang Lin Shan) 
maybe a person name ??/???(Fang Linshan) or 
a place name ???/??(Fanglin Mountain). 
This paper presents a one-for-all approach 
based on roles tagging to avoid such problems. 
The process is: tagging tokens after word 
segmentation with the most probable roles and 
making unknown words recognition based on roles 
sequence. The mechanism of roles tagging is just 
like that of a small and simple Part-Of-Speech 
tagger.  
The paper is organized as follows: In section 
2, we will describe the approach in general. 
Following that, we will present the solution in 
practice. In the final part, we provide recognition 
experiments using roles-tagging methods. The 
result and possible problems are discussed as well. 
1 Unknown words recognition based on roles 
tagging  
1.1 Lexical roles of unknown words 
Unknown words are often made up of 
distinctive components, most of which are 
monosyllabic characters or short words; in 
addition, there are some regular relations between 
unknown words and their locality, especially with 
their left and right context. As we often write or 
speak, a Chinese person name is usually comprised 
of a one-or-two-character surname and a following 
given name of one or two characters, like ???
??(Xiao Jianqun) and ?????(Zhu-Ge Liang). 
The  previous words are mostly titles, 
occupations or some conjunctive words, such as 
????(Manager), ????(Driver) and ???(To). 
The following words tend to be verbs such as ??? 
(to say) , ????(to express). Similar components, 
contexts and relations can be discovered in place 
name, transliteration, organization name, or other 
types of unknown words.   
We define unknown word roles with respect 
to varied internal components, previous and 
succeeding contexts and other tokens in a 
particular sentence. Various roles are extracted 
according to their functions in the forming of 
different unknown words. Person names roles and 
transliterations roles set are shown in table 1a and 
1b respectively. Using the roles set of person name, 
the tokens sequence ??/?/??/?/?/?/?/?/
?/??/?/??/?/?/??/? (What Zhou Enlai 
and Deng Yunchao used before death are 
presented in the museum) will be tagged as ??/A 
?/A ??/K ?/B ?/C ?/D ?/M ?/B ?/C 
??/V ?/A ??/A ?/A ?/A??/A?. 
Role Significance Examples 
B Surname or family 
name. 
?/?/?/???
??/? 
C First Chinese char in 
the 2-char given name 
?/?/?/?? 
D Last Chinese char in 
the 2-char given name. 
?/?/?/?? 
E Given name with a 
single Chinese char. 
?/? 
F Prefix in the name. ?/???/? 
G Postfix in the name. ?/???/???/? 
K Previous context before 
person name. 
?/??/?/?/
?/?/? 
L Succeeding context 
following person name. 
???/???
?/? 
M Parts between two 
person names. 
??/?/?/?/
?/?/?/?/?
U Known words 
generated by previous 
context and the first 
component of name. 
?? /?? /? /
?/?/??/??
? /?? /?? /
?/?/?/ 
V Known words 
generated by the last 
component and next 
context. 
? /? /?? /?
? /, ? /? /?
?/? 
..... 
A Others tokens not 
mentioned above. 
??/??/??/
??/?/?/?/ 
Table 1a: Roles set of Chinese person names 
 
Role Significance Examples 
B The first 
component of 
transliteration 
?/?/? 
C Middle component ?/?/?/?/?/?
/?/?/? 
D Last component  ?/?/? 
..... 
  
Table 1b: Roles set of transliterations 
1.2 Roles tagging and unknown words recognition 
On the one hand, the sentence include words 
with different roles for a particular category of 
unknown words, on the other hand, such words 
can be recognized after identifying their roles 
sequence. That is: tagging tokens after word 
segmentation with the most probable roles 
sequence, then recognizing unknown words by 
maximum patterns matching on the final roles 
sequence. 
Roles tagging is similar to Part-Of-Speech 
tagging. Our tagging process is based on Viterbi 
Algorithm [Rabiner and Juang (1989)], which is to 
select the optimum with maximum probability 
from all possible tag sequences. The methodology 
and its deduction is given as below: 
Suppose that T is the tokens sequence after 
word segmentation and R is the roles sequence for 
T. We take the role sequence R# with the 
maximum probability as the best choice. That is: 
T=(t1, t 2, ? , t m), 
R=(r1, r2, ? , rm), m>0, 
R#= arg P(R|T)...................?........E1    
R
max
According to the Bayes equation, we can get: 
P(R|T)= P(R)P(T|R)/P(T) ...................E2 
For a particular token sequence, P(T) is a 
constant. So, We can get E3 based on E1 and E2: 
R#= arg P(R)P(T|R) ......................E3 
R
max
We may consider T as the observation value 
sequence while R as the state sequence hidden 
behind the observation. Now we introduce Hidden 
Markov Model [Rabiner and Juang (1986)] to 
resolve such a typical problem:  
P(R) P(T|R)?  ?
=
?
m
i
iiii rrprtp
0
1 )|()|(
?R#? .......E4 
R
maxarg ?
=
?
m
i
iiii rrprtp
0
1 )|()|(
?R#? 
? ......E5 
R
minarg ?
=
?+
m
i
iiii rrprtp
0
1)}|(ln)|({ln
E5 is simpler for computation than E4. 
Now, we can find the most possible token 
sequence with equation E5. It?s a simple 
application of Viterbi Algorithm. 
The final recognition through maximum pattern 
matching is not performed on the original texts but 
performed on roles sequence. The person patterns 
are {BBCD, BBE, BBZ, BCD, BE, BG, BXD, BZ, CD, FB, 
Y, XD}. Before matching, we should split the 
tokens whose roles are like ?U? or ?V?(which 
indicate that the related token is generated by 
internal components and the outside contexts of 
unknown words) into two proper parts. Such a 
processing can recall more unknown words and 
reduce the overlapping collision. As for the above 
sample sentence, the final roles sequence after 
splitting is ?AAKBCDMBCDLAAAAAA?. Therefore, 
we can identify the possible person names ???
?? and ????? according to the recognition 
pattern ?BCD?. 
1.3  Automatic acquisition of roles knowledge 
 As described in E5, the tag sequence R#  is 
decided by two kinds of factors: and 
.  is the probability of a 
token t
)|( ii rtp
)|( 1?ii rrp )|( ii rtp
( irp
i given the condition of being tagged with 
role ri, while  is the transitive 
probability from role r
)| 1?ir
i-1 to role ri. Both factors are 
useful lexical knowledge for tagging and final 
recognition. According to laws of large numbers, if 
the training corpus is large enough, we can acquire 
the roles knowledge as following: 
)|( ii rtp ?C(ti,ri)/C(ri) ................??......... E6 
Where C(ti, ri) is the count of token ti  being role ri; 
and C(ri) is the count of role ri. 
)|( 1?ii rrp ?C(ri-1,ri)/C(ri-1) ........?.?.....?E7 
Where C(ri-1,ri) is the count of role ri-1  followed 
by role ri. 
 C(ti,ri), C(ri) and C(ri-1,ri) are extracted from 
corpus through a training process. The training 
corpus came from one-month news from the 
People?s Daily with 2,305,896 Chinese characters, 
which are manually checked after word 
segmentation and POS tagging (It can be 
downloaded at icl.pku.edu.cn, the homepage of the 
Institute of Computational Linguistics, Peking 
University).  
However, the corpus is tagged with the 
Part-Of-Speech set. Before training, the original 
POS tags should be converted to the proper roles 
by analysing every token in the sentence.  
2 Algorithm and implementation 
The unknown words recognition based on 
roles tagging has three main steps: automatic 
acquisition of roles knowledge from the corpus; 
roles tagging with Viterbi algorithm and unknown 
words recognition through maximum pattern 
matching. 
  Viterbi algorithm is a classic approach in 
statistics. It aims to select the optimum roles 
sequence with maximum possibility from all 
possible results. Our evaluation function for 
decision-making is E5 given in sub-section 1.2. 
Considering the length limitation of this paper, we 
skip the details.  
Therefore, we only provide algorithms for 
roles knowledge learning. In the last part, the 
entire process of unknown words recognition will 
be listed. 
2.1 Roles knowledge learning 
Input: Corpus which is segmented and POS 
tagged 
T: the type of unknown words; 
R: Roles set of T 
Output: C(ti,ri), C(ri) and C(ri-1,ri) 
Algorithm: 
(1) Get one sentence S from corpus C;  
(2) Extract all tokens and POS tags from S; 
(3) Convert all POS tags to roles in T after role 
analysis.  
(4) Store the tokens whose role is not ?A? into the 
recognition lexicons of unknown words T, where 
?A? is not internal components nor context role.  
(5) Calculate the total number C(ti,ri) of token ti  
being role ri. At the same time, count C(ri), which is 
the number of role ri appearances. 
(6) Sum C(ri-1,ri) which is the times of role ri-1 
followed by role ri. 
(7) If no more sentences in the corpus C, exit; else 
go to (1)  
   First of all, we must explain step (3). Our 
corpus is tagged with POS and person, place or 
organization name are tagged with ?nr?, ?ns? or ?nt? 
respectively; Such POS are unique and different 
from noun. Transliterations can be extracted from 
words tagged with ?nr? or ?ns? and through 
analysing its component chars. So we can easily 
locate such kinds of words. Meanwhile, we can 
judge whether a word is unknown by looking it up 
in the core lexicon. Then we can identify roles of 
words according to their locality, which are before 
or following a particular unknown word. 
Here we provide a sample sentence from our 
corpus like ???/r  ??/ns  ??/t  ??/t  
?/n  ??/n  ?/nr  ??/nr  ?/w  ?/nr  ?
?/nr  ??/v?. In step (2), we can extract tokens 
and tags like ????/ ?r?; ????/ ?ns? and so on. 
When we train person recognition roles, firstly, we 
locate person name ??/nr  ??/nr? and ??/nr  
??/nr? just by searching POS ?nr?; Secondly, 
judge whether they are unknown after looking 
them up in the core lexicon; At last we can tag 
unknown words component and their context near 
their locality. So the final roles after conversion 
are ???/A  ??/A ??/A 1?/A ?/A ??
/K ?/B  ?/C?/D ?/M ?/B ?/C?/D ??
/L?. Then we can train the parameters based on 
new segmentation and person recognition roles 
sequence.  
In addition, we train every different kind of 
unknown word on the same corpus individually. 
That is: person roles, place roles and other roles 
are acquired respectively. Therefore, the unknown 
place recognition roles sequence of the above 
sentence may like ???/K  ?/B?/D  ??/L  
??/A  ?/A  ??/K  ?/A ??/A  ?/A  
?/A ??/A  ??/A?. Such a mechanism can 
greatly reduce the problem of sparse data. 
2.2 The entire process of Unknown words 
recognition 
Input: Original sentence S; 
R: the roles set of unknown words; 
P: pattern sets for recognition. 
Output: Possible unknown words of type T. 
Algorithm: 
(1) Word segmentation (we segment words on 
sentence S with N-shortest paths method 
[Hua-Ping ZHANG, Qun LIU (2002)]); 
(2) Tag tokens sequence with roles in set R using 
Viterbi algorithm. Get the roles sequence R# 
with maximum possibility. 
(3) Split tokens whose role is like ?U? or ?V? in the 
person roles. These roles indicate that the 
internal components glue together with their 
context. 
(4) Maximum match final roles sequence to the 
recognition patterns P and record their 
position. 
(5) Generate the candidate unknown words 
according to the result of pattern matching. 
(6) Exclude those candidates which are fit for the 
exclusive rules.(For example, Chinese person 
name can not include non-Chinese chars. ) 
(7) Output the possible unknown words. 
Now, we take person recognition on the 
sentence ?????????????????
????? as exemplification. In the first place, 
we can get the sequence ???/??/??/??/?
/??/?/?/?/?/?/?/?/??/? after rough 
word segmentation; Then we tag it with Viterbi 
algorithm using person recognition roles lexicon 
and transitive array. So, the most probable roles 
sequence is ?AAAAAKBCDMBCDL?.Therefore, 
candidate perosn names ????? and ????? 
can be recognized after maximum string matching. 
3 Experiments and Discussions 
Both close and open recognition test were 
conducted. In the close test, we tested our system 
within the training corpus, which is the knowledge 
base for recognition. Open test, however, is more 
realistic, because it is performed on arbitrary real 
texts outside the training corpus. The corpus in our 
experiments is from 2-months news in 1998 from 
the People?s Daily. 
In this paper, we only provide the recognition 
results of Chinese person and transliterations. The 
recognition of place names and other kind of 
unknown words can get similar performance. 
3.1 Recognition experiment of Chinese person name  
Test Type Close Open 
Corpus (news date) 1.1-2.20 2.20-2.28
Corpus Size  14,446K 2,605K 
Num of Chinese 
person names 
21,256 3,149 
Num of recognized 
person names 
27,813 4,130 
Num of correctly 20,865 2,886 
recognized names 
Precision rate 75.02% 69.88% 
Recalling rate 98.17% 91.65% 
F-measurement  85.05% 79.30% 
Table 2 Experiment results of Chinese person 
names recognition 
In Tables 2, precision rate and recalling rate are 
defined as equations E6 and E7 respectively. In 
addition, F-measurement is a uniformly weighted 
harmonic mean of precision rate and recalling rate 
as shown in E8. 
Precision rate=  
 wordsrecognized of num
 wordsrecognizedcorrectly  of num
??..E6 
Recalling rate=  
wordsunknown   totalofnum
 wordsrecognizedcorrectly  of num
??..E7 
F-measurement = 
ratePrecision rate Recalling
2ratePrecision rate Recalling
+
??
...?.E8 
3.2 Recognition Experiments of transliterations 
Test Type Close Open 
Corpus (news date) 1.1-2.20 2.20-2.28 
Corpus Size  14,446K 2,605K 
Num of 
transliterations  
9,059 1,592 
Num of recognized 
transliterations 
10,013 1,930 
Num of correctly 
recognized 
transliterations 
8,946 1,496 
Precision rate 89.35% 77.52% 
Recalling rate 98.75% 93.97% 
F-measurement  93.85% 84.96% 
Table 3 Results of transliterations recognition 
3.3 Discussions 
The traditional ways to test unknown words 
recognition is to collect sentences including 
unknown words and to make recognition 
experiments. Those sentences that haven?t the type 
of unknown words will be excluded from 
experiments in the pre-processing. In our 
experiments, we just take the realistic corpus and 
make no filtering. Therefore, the precision rates 
may be lower but closer to the realistic linguistic 
environment than previous tests. We have made 
experiments in the traditional way and the 
precision rate can be improved by less than 15%. 
In a word, there is no comparable with precision 
rates of previous unknown words recognition 
experiment. 
  In addition, our experiments show that the 
unknown words recognition based on role tagging 
can achieve very high recalling rates. For such a 
problem, recalling is more essential than precision. 
Low recalling rate means that we have no chance 
to recognize many unknown words through any 
efforts in the following steps, although words 
recognized are mostly valid; However, precision 
rate can be greatly improved in other processes, 
such as POS tagging or sentence simple parsing. In 
our system ICTCLAS (Institute of Computing 
Technology, Chinese Lexical System), we can 
exclude most invalid unknown words during POS 
tagging. The precision rate of Chinese person 
names recognition can achieve over 95% after 
POS tagging while the recalling rate is not 
reduced. 
  Our approach is purely corpus-based. We all 
know that, in any usual corpus, unknown words 
are sparsely distributed. If we depend totally on the 
corpus, the problem of sparse data is inevitable. 
But in the fine-tuning of our system, we found 
some countermeasures and successfully solved the 
problem. 
Lexical knowledge from linguists can be 
incorporated into the system. This does not mean 
that we fall back to the old ways. We just demand 
for those general rules about name formation to 
avoid apparent mistakes. As to person name 
recognition, there are several strict restrictions, 
such as the length of name, the order between 
surname and given name. 
Except for enlarging the training corpus, we 
provide two more counteractions: 
Firstly, a ?best n? approach [Hua-Ping ZHANG, 
Qun LIU (2002)], which provides n (n>1) possible 
tag sequences with leading probabilities, is feasible. 
Usually the desired tag sequence could be 
re-targeted or constructed from the best n 
sequences. In this way, we improved the recalling 
rate at the cost of precision rate. But given a better 
recalling, we could remedy in latter stages of 
language processing. When 3 most probable 
sequences are employed, the recalling and 
precision of unknown words in ICTCLAS can be 
enhanced obviously. 
The second resolution is training on a name 
library in addition to training on a corpus. As we 
all know, it?s easier and cheaper to get a person 
name library or other special name libraries than to 
segment and tag a corpus. We could extract the 
inner components relations from the unknown 
words libraries, and then merge these data into the 
roles information from the original corpus. When 
the special name libraries were introduced, both 
precision and recalling rates can be improved. 
Conclusion 
The paper presents a one-for-all approach for 
Chinese unknown words recognition based on 
roles tagging. At first, we define roles set for every 
category of unknown words according to the 
function of tokens, such as internal component or 
contexts. Unknown words are recognized on roles 
sequence, tagged with the roles set using Viterbi 
algorithm. The knowledge about roles is extracted 
from the learning on corpus. Experiments on large 
size corpus verify that the approach based on role 
tagging is simple and applicable. 
Acknowledgements 
First of all, our thanks go to the Institute of 
Computational Linguistics, Peking University for 
providing the corpus. And we owe lots of thanks to 
our colleagues: Zougang, Li Jifeng, Li Sujian,Li 
Shengtao, Zhu Hailong, Zhao Hongchao, Wang 
Shuxi and Dr. Zhou Lixin. We would especially 
express gratitude to the chief scientist Bai Shuo. 
References  
K.Y. Liu (1999)  The Assessment to Automatic Word 
Segmentation and POS Tagging Software. 
Proceedings of the 4th Conference on Chinese 
Computer Intelligent Interface and Application, 
Beijing. 
Z. Luo and R. Song (2001)  Integrated and Fast 
Recognition of Proper Noun in Modern Chinese Word 
Segmentation.  Proceedings of International 
Conference on Chinese Computing 2001, Singapore, 
pp. 323-328. 
H. Luo and Z. Ji (2001)  Inverse Name Frequency 
Model and Rules Based on Chinese Name Identifying.  
In "Natural Language Understanding and Machine 
Translation", C. N. Huang & P. Zhang, ed., Tsinghua 
Univ. Press, Beijing, China, pp. 123-128. 
R. Song (1993)  Person Name Recognition Method 
Based on Corpus and Rule.  In ?Computational 
Language Research and Development", L. W. Chen 
& Q. Yuan, ed., Beijing Institute of Linguistic Press. 
H. Y. Tan (1999)  Chinese Place Automatic 
Recognition Research.  In "Proceedings of 
Computational Language ", C. N. Huang & Z.D. 
Dong, ed., Tsinghua Univ. Press, Beijing, China. 
M.S. Sun (1993)  English Transliteration Automatic 
Recognition.  In "Computational Language 
Research and Development", L. W. Chen & Q. Yuan, 
ed., Beijing Institute of Linguistic Press. 
Y.J. Lv, T. J. Zhao (2001)  Levelled Unknown Chinese 
Words Resolution by Dynamic Programming.  
Journal of Chinese Information Processing. 15, 1, pp. 
28-33. 
X. H. Chen (1999)  One-for-all Solution for Unknown 
Word in Chinese Segmentation. Application of 
Language and Character, 3. 
Y. He (2001)  Identification of Unlisted Words on 
Transitive Probability of Monosyllabic Words.  In 
"Natural Language Understanding and Machine 
Translation", C. N. Huang & P. Zhang, ed., Tsinghua 
Univ. Press, Beijing, China, pp. 123-128. 
Hua-Ping  ZHANG, Qun LIU (2002)  Model of 
Chinese Words Rough Segmentation Based on 
N-Shortest-Paths Method.  Journal of Chinese 
Information Processing. 16, 5, pp. 77-83. 
L. R.Rabiner (1989)  A Tutorial on Hidden Markov 
Models and Selected Applications in Speech 
Recognition.  Proceedings of IEEE 77(2): 
pp.257-286. 
L.R. Rabiner and B.H. Juang, (Jun. 1986) An 
Introduction to Hidden Markov Models.  IEEE 
ASSP Mag., Pp.4-166. 
???????????????????3 
???   ? ?   ? ?   ??? 
Email : zhanghp@software.ict.ac.cn 
??????????????? 
?? 2704????????????? 6?, ??  
??: ?????????????????
?????????????????????
?????????????????????
?????????????????????
?? Viterbi??????? Token?????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
????????????????????
????? ICTCLAS????????????
?????????????????????
????????????? 
???:?????????????????
Viterbi?? 
                                                        
1 We define unknown words to be those not included in 
the core lexicon and unable to be generated by FSA, 
such as person name, place name. But numeric or 
common time word is not unknown because they can 
generate by a simple FSA. 
2 Related research in this paper is supported by 
Foundation of National Key Basic Research Project (ID: 
G1998030507-4 and G1998030510). 
3 ??????????????????(???
G1998030507-4?G1998030510)??? 
Proceedings of the Second Workshop on Statistical Machine Translation, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Dependency Treelet String Correspondence
Model for Statistical Machine Translation
Deyi Xiong, Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{dyxiong, liuqun, sxlin}@ict.ac.cn
Abstract
This paper describes a novel model using
dependency structures on the source side
for syntax-based statistical machine transla-
tion: Dependency Treelet String Correspon-
dence Model (DTSC). The DTSC model
maps source dependency structures to tar-
get strings. In this model translation pairs of
source treelets and target strings with their
word alignments are learned automatically
from the parsed and aligned corpus. The
DTSC model allows source treelets and tar-
get strings with variables so that the model
can generalize to handle dependency struc-
tures with the same head word but with dif-
ferent modifiers and arguments. Addition-
ally, target strings can be also discontinuous
by using gaps which are corresponding to
the uncovered nodes which are not included
in the source treelets. A chart-style decod-
ing algorithm with two basic operations?
substituting and attaching?is designed for
the DTSC model. We argue that the DTSC
model proposed here is capable of lexical-
ization, generalization, and handling discon-
tinuous phrases which are very desirable for
machine translation. We finally evaluate our
current implementation of a simplified ver-
sion of DTSC for statistical machine trans-
lation.
1 Introduction
Over the last several years, various statistical syntax-
based models were proposed to extend traditional
word/phrase based models in statistical machine
translation (SMT) (Lin, 2004; Chiang, 2005; Ding
et al, 2005; Quirk et al, 2005; Marcu et al, 2006;
Liu et al, 2006). It is believed that these models
can improve the quality of SMT significantly. Com-
pared with phrase-based models, syntax-based mod-
els lead to better reordering and higher flexibility
by introducing hierarchical structures and variables
which make syntax-based models capable of hierar-
chical reordering and generalization. Due to these
advantages, syntax-based approaches are becoming
an active area of research in machine translation.
In this paper, we propose a novel model based on
dependency structures: Dependency Treelet String
Correspondence Model (DTSC). The DTSC model
maps source dependency structures to target strings.
It just needs a source language parser. In contrast to
the work by Lin (2004) and by Quirk et al (2005),
the DTSC model does not need to generate target
language dependency structures using source struc-
tures and word alignments. On the source side, we
extract treelets which are any connected subgraphs
and consistent with word alignments. While on the
target side, we allow the aligned target sequences
to be generalized and discontinuous by introducing
variables and gaps. The variables on the target side
are aligned to the corresponding variables of treelets,
while gaps between words or variables are corre-
sponding to the uncovered nodes which are not in-
cluded by treelets. To complete the translation pro-
cess, we design two basic operations for the decod-
ing: substituting and attaching. Substituting is used
to replace variable nodes which have been already
translated, while attaching is used to attach uncov-
40
ered nodes to treelets.
In the remainder of the paper, we first define de-
pendency treelet string correspondence in section
2 and describe an algorithm for extracting DTSCs
from the parsed and word-aligned corpus in section
3. Then we build our model based on DTSC in sec-
tion 4. The decoding algorithm and related pruning
strategies are introduced in section 5. We also spec-
ify the strategy to integrate phrases into our model
in section 6. In section 7 we evaluate our current
implementation of a simplified version of DTSC for
statistical machine translation. And finally, we dis-
cuss related work and conclude.
2 Dependency Treelet String
Correspondence
A dependency treelet string correspondence pi is a
triple < D,S,A > which describes a translation
pair < D,S > and their alignment A, where D is
the dependency treelet on the source side and S is
the translation string on the target side. < D,S >
must be consistent with the word alignment M of
the corresponding sentence pair
?(i, j) ? M, i ? D ? j ? S
A treelet is defined to be any connected subgraph,
which is similar to the definition in (Quirk et al,
2005). Treelet is more representatively flexible than
subtree which is widely used in models based on
phrase structures (Marcu et al, 2006; Liu et al,
2006). The most important distinction between the
treelet in (Quirk et al, 2005) and ours is that we al-
low variables at positions of subnodes. In our defini-
tion, the root node must be lexicalized but the subn-
odes can be replaced with a wild card. The target
counterpart of a wildcard node in S is also replaced
with a wild card. The wildcards introduced in this
way generalize DTSC to match dependency struc-
tures with the same head word but with different
modifiers or arguments.
Another unique feature of our DTSC is that we al-
low target strings with gaps between words or wild-
cards. Since source treelets may not cover all subn-
odes, the uncovered subnodes will generate a gap as
its counterpart on the target side. A sequence of con-
tinuous gaps will be merged to be one gap and gaps
at the beginning and the end of S will be removed
automatically.
??
eeeeeee
?
?
?
? ??eeeeeee
s s
s s
s s
s s??
??
? ?eeeeeee
?
?
?
?
]]]]]]]]]]
the conference cooperation of the ?
??
eeeeeeebbbbbbbbbbbbb
s s
s s
s s
s s?1
w w
w w
? YYYYYYY
SSS
SSS
S ?
?2 ]]]]]]]]]]?1 keep a G with the ?2
Figure 1: DTSC examples. Note that ? represents
variable and G represents gap.
Gap can be considered as a special kind of vari-
able whose counterpart on the source side is not
present. This makes the model more flexible to
match more partial dependency structures on the
source side. If only variables can be used, the model
has to match subtrees rather than treelets on the
source side. Furthermore, the positions of variables
on the target side are fixed so that some reorderings
related with them can be recorded in DTSC. The po-
sitions of gaps on the target side, however, are not
fixed until decoding. The presence of one gap and
its position can not be finalized until attaching op-
eration is performed. The introduction of gaps and
the related attaching operation in decoding is the
most important distinction between our model and
the previous syntax-based models.
Figure 1 shows several different DTSCs automat-
ically extracted from our training corpus. The top
left DTSC is totally lexicalized, while the top right
DTSC has one variable and the bottom has two vari-
ables and one gap. In the bottom DTSC, note that
the node ? which is aligned to the gap G of the
target string is an uncovered node and therefore not
included in the treelet actually. Here we just want
to show there is an uncovered node aligned with the
gap G.
Each node at the source treelet has three attributes
1. The head word
2. The category, i.e. the part of speech of the head
word
3. The node order which specifies the local order
of the current node relative to its parent node.
41
??/VV
eeeeeeebbbbbbbbbbbbbb ]]]]]]]]]]]]]]]]]]]]]
?
?
?
?
?
??/VV
?
?
?
DD
DD
D ?/P YYYYYYY
XXXXX
XXXXX
XXXX
??/NN
eeeeeee
z z
z z
z
????/NR
\\\\\\\\\\
\\\\ ??/NN
j j j
j??
go on providingfinancial aid to Palestine
1 2 3 4 5 6 7
Figure 2: An example dependency tree and its align-
ments
Note that the node order is defined at the context of
the extracted treelets but not the context of the orig-
inal tree. For example, the attributes for the node?
in the bottom DTSC of Figure 1 are {?, P, -1}. For
two treelets, if and only if their structures are iden-
tical and each corresponding nodes share the same
attributes, we say they are matched.
3 Extracting DTSCs
To extract DTSCs from the training corpus, firstly
the corpus must be parsed on the source side and
aligned at the word level. The source structures pro-
duced by the parser are unlabelled, ordered depen-
dency trees with each word annotated with a part-of-
speech. Figure 2 shows an example of dependency
tree really used in our extractor.
When the source language dependency trees and
word alignments between source and target lan-
guages are obtained, the DTSC extraction algorithm
runs in two phases along the dependency trees and
alignments. In the first step, the extractor annotates
each node with specific attributes defined in section
3.1. These attributes are used in the second step
which extracts all possible DTSCs rooted at each
node recursively.
3.1 Node annotation
For each source dependency node n, we define three
attributes: word span, node span and crossed.
Word span is defined to be the target word sequence
aligned with the head word of n, while node span is
defined to be the closure of the union of node spans
of all subnodes of n and its word span. These two at-
tributes are similar to those introduced by Lin (Lin,
2004). The third attribute crossed is an indicator that
has binary values. If the node span of n overlaps
the word span of its parent node or the node span
of its siblings, the crossed indicator of n is 1 and
n is therefore a crossed node, otherwise the crossed
indicator is 0 and n is a non-crossed node. Only
non-crossed nodes can generate DTSCs because the
target word sequence aligned with the whole subtree
rooted at it does not overlap any other sequences and
therefore can be extracted independently.
For the dependency tree and its alignments shown
in Figure 2, only the node ?? is a crossed node
since its node span ([4,5]) overlaps the word span
([5,5]) of its parent node??.
3.2 DTSCs extraction
The DTSC extraction algorithm (shown in Figure 3)
runs recursively. For each non-crossed node, the al-
gorithm generates all possible DTSCs rooted at it by
combining DTSCs from some subsets of its direct
subnodes. If one subnode n selected in the com-
bination is a crossed node, all other nodes whose
word/node spans overlap the node span of n must be
also selected in this combination. This kind of com-
bination is defined to be consistent with the word
alignment because the DTSC generated by this com-
bination is consistent with the word alignment. All
DTSCs generated in this way will be returned to the
last call and outputted. For each crossed node, the
algorithm generates pseudo DTSCs1 using DTSCs
from all of its subnodes. These pseudo DTSCs will
be returned to the last call but not outputted.
During the combination of DTSCs from subnodes
into larger DTSCs, there are two major tasks. One
task is to generate the treelet using treelets from
subnodes and the current node. This is a basic tree
generation operation. It is worth mentioning that
some non-crossed nodes are to be replaced with a
wild card so the algorithm can learn generalized
DTSCs described in section 2. Currently, we re-
place any non-crossed node alone or together with
their sibling non-crossed nodes. The second task
is to combine target strings. The word sequences
aligned with uncovered nodes will be replaced with
a gap. The word sequences aligned with wildcard
nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes,
all 2m combinations will be considered. This will
generate a very large number of DTSCs, which is
1Some words in the target string are aligned with nodes
which are not included in the source treelet.
42
DTSCExtractor(Dnode n)
< := ? (DTSC container of n)
for each subnode k of n do
R := DTSCExtractor(k)
L := L?R
end for
if n.crossed! = 1 and there are no subnodes whose span
overlaps the word span of n then
Create a DTSC pi =< D,S,A > where the dependency
treelet D only contains the node n (not including any chil-
dren of it)
output pi
for each combination c of n?s subnodes do
if c is consistent with the word alignment then
Generate all DTSCs R by combining DTSCs (L)
from the selected subnodes with the current node n
< := <?R
end if
end for
output <
return <
else if n.crossed == 1 then
Create pseudo DTSCs P by combining all DTSCs from
n?s all subnodes.
< := <?P
return <
end if
Figure 3: DTSC Extraction Algorithm.
undesirable for training and decoding. Therefore we
filter DTSCs according to the following restrictions
1. If the number of direct subnodes of node n is
larger than 6, we only consider combining one
single subnode with n each time because in this
case reorderings of subnodes are always mono-
tone.
2. On the source side, the number of direct subn-
odes of each node is limited to be no greater
than ary-limit; the height of treelet D is limited
to be no greater than depth-limit.
3. On the target side, the length of S (including
gaps and variables) is limited to be no greater
than len-limit; the number of gaps in S is lim-
ited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from
each subnode are sorted by size (in descending
order). Only the top comb-limit DTSCs will be
selected to generate larger DTSCs.
As an example, for the dependency tree and its
alignments in Figure 2, all DTSCs extracted by the
Treelet String
(??/VV/0) go on
(????/NR/0) Palestine
(?/P/0) to
(?/P/0 (????/NR/1)) to Palestine
(?/P/0 (?/1)) to ?
(??/NN/0 (??/NN/-1)) financial aid
(??/VV/0) providing
(??/VV/0 (?/1)) providing ?
(??/VV/0 (?/-1)) providing G ?
(??/VV/0 (??/VV/-1)) go on providing
(??/VV/0 (?/-1)) ? providing
(??/VV/0 (?1/-1) (?2/1)) providing ?2 ?1
(??/VV/0 (?1/-1 ) (?2/1)) ?1 providing ?2
Table 1: Examples of DTSCs extracted from Figure
2. Alignments are not shown here because they are
self-evident.
algorithm with parameters { ary-limit = 2, depth-
limit = 2, len-limit = 3, gap-limit = 1, comb-limit
= 20 } are shown in the table 1.
4 The Model
Given an input dependency tree, the decoder gen-
erates translations for each dependency node in
bottom-up order. For each node, our algorithm will
search all matched DTSCs automatically learned
from the training corpus by the way mentioned in
section 3. When the root node is traversed, the trans-
lating is finished. This complicated procedure in-
volves a large number of sequences of applications
of DTSC rules. Each sequence of applications of
DTSC rules can derive a translation.
We define a derivation ? as a sequence of appli-
cations of DTSC rules, and let c(?) and e(?) be the
source dependency tree and the target yield of ?, re-
spectively. The score of ? is defined to be the prod-
uct of the score of the DTSC rules used in the trans-
lation, and timed by other feature functions:
?(?) =
?
i
?(i) ? plm(e)?lm ? exp(??apA(?)) (1)
where ?(i) is the score of the ith application of
DTSC rules, plm(e) is the language model score,
and exp(??apA(?)) is the attachment penalty,
where A(?) calculates the total number of attach-
ments occurring in the derivation ?. The attach-
ment penalty gives some control over the selection
of DTSC rules which makes the model prefer rules
43
with more nodes covered and therefore less attach-
ing operations involved.
For the score of DTSC rule pi, we define it as fol-
lows:
?(pi) =
?
j
fj(pi)?j (2)
where the fj are feature functions defined on DTSC
rules. Currently, we used features proved to be ef-
fective in phrase-based SMT, which are:
1. The translation probability p(D|S).
2. The inverse translation probability p(S|D).
3. The lexical translation probability plex(D|S)
which is computed over the words that occur
on the source and target sides of a DTSC rule
by the IBM model 1.
4. The inverse lexical translation probability
plex(S|D) which is computed over the words
that occur on the source and target sides of a
DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model
to favor longer or shorter derivations.
It is worth mentioning how to integrate the N-
gram language mode into our DTSC model. During
decoding, we have to encounter many partial transla-
tions with gaps and variables. For these translations,
firstly we only calculate the language model scores
for word sequences in the translations. Later we up-
date the scores when gaps are removed or specified
by attachments or variables are substituted. Each up-
dating involves merging two neighbor substrings sl
(left) and sr (right) into one bigger string s. Let the
sequence of n ? 1 (n is the order of N-gram lan-
guage model used) rightmost words of sl be srl and
the sequence of n?1 leftmost words of sr be slr. we
have:
LM(s) = LM(sl) + LM(sr) + LM(srl slr)
?LM(srl )? LM(slr) (3)
where LM is the logarithm of the language model
probability. We only need to compute the increment
of the language model score:
4LM = LM(srl slr)? LM(srl )? LM(slr) (4)
for each node n of the input tree T , in bottom-up order do
Get al matched DTSCs rooted at n
for each matched DTSC pi do
for each wildcard node n? in pi do
Substitute the corresponding wildcard on the target
side with translations from the stack of n?
end for
for each uncovered node n@ by pi do
Attach the translations from the stack of n@ to the
target side at the attaching point
end for
end for
end for
Figure 4: Chart-style Decoding Algorithm for the
DTSC Model.
Melamed (2004) also used a similar way to integrate
the language model.
5 Decoding
Our decoding algorithm is similar to the bottom-up
chart parsing. The distinction is that the input is a
tree rather than a string and therefore the chart is in-
dexed by nodes of the tree rather than spans of the
string. Also, several other tree-based decoding al-
gorithms introduced by Eisner (2003), Quirk et al
(2005) and Liu et al (2006) can be classified as the
chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate
the bottom-up order by postorder transversal. This
order guarantees that any subnodes of node n have
been translated before node n is done. For each
node n in the bottom-up order, all matched DTSCs
rooted at n are found, and a stack is also built for it to
store the candidate translations. A DTSC pi is said to
match the input dependency subtree T rooted at n if
and only if there is a treelet rooted at n that matches
2 the treelet of pi on the source side.
For each matched DTSC pi, two operations will
be performed on it. The first one is substituting
which replaces a wildcard node with the correspond-
ing translated node. The second one is attaching
which attaches an uncovered node to pi. The two op-
erations are shown in Figure 5. For each wildcard
node n?, translations from the stack of it will be se-
lected to replace the corresponding wildcard on the
2The words, categories and orders of each corresponding
nodes are matched. Please refer to the definition of matched
in section 2.
44
(a) A
eeeeeee YYYYYYYB
eeeeeee
? + D
C ? De
?e Ae Be Ce
Substitute ?
(b) A
eeeeeee YYYYYYYB
eeeeeee
D + E
C ? Ee
De Ae Be Ce
Attach ?
(c) A
eeeeeee YYYYYYYB
eeeeeee YYYYYYY
D
C E
De Ae Be Ee Ce
Figure 5: Substituting and attaching operations for
decoding. Xe is the translation of X . Node that ? is
a wildcard node to be substituted and node ? is an
uncovered node to be attached.
target side and the scores of new translations will be
calculated according to our model. For each uncov-
ered node n@, firstly we determine where transla-
tions from the stack of n@ should be attached on the
target side. There are several different mechanisms
for choosing attaching points. Currently, we imple-
ment a heuristic way: on the source side, we find the
node n@p which is the nearest neighbor of n@ from
its parent and sibling nodes, then the attaching point
is the left/right of the counterpart of n@p on the target
side according to their relative order. As an example,
see the uncovered node ? in Figure 5. The nearest
node to it is node B. Since node ? is at the right
of node B, the attaching point is the right of Be.
One can search all possible points using an ordering
model. And this ordering model can also use infor-
mation from gaps on the target side. We believe this
ordering model can improve the performance and let
it be one of directions for our future research.
Note that the gaps on the target side are not neces-
sarily attaching points in our current attaching mech-
anism. If they are not attaching point, they will be
removed automatically.
The search space of the decoding algorithm is
very large, therefore some pruning techniques have
to be used. To speed up the decoder, the following
pruning strategies are adopted.
1. Stack pruning. We use three pruning ways.
The first one is recombination which converts
the search to dynamic programming. When
two translations in the same stack have the
same w leftmost/rightmost words, where w de-
pends on the order of the language model, they
will be recombined by discarding the transla-
tion with lower score. The second one is the
threshold pruning which discards translations
that have a score worse than stack-threshold
times the best score in the same stack. The
last one is the histogram pruning which only
keeps the top stack-limit best translations for
each stack.
2. Node pruning. For each node, we only keep
the top node-limit matched DTSCs rooted at
that node, as ranked by the size of source
treelets.
3. Operation pruning. For each operation, sub-
stituting and attaching, the decoding will gen-
erate a large number of partial translations3
for the current node. We only keep the top
operation-limit partial translations each time
according to their scores.
6 Integrating Phrases
Although syntax-based models are good at dealing
with hierarchical reordering, but at the local level,
translating idioms and similar complicated expres-
sions can be a problem. However, phrase-based
models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based
models can improve the performance (Marcu et al,
2006; Liu et al, 2006). Since our DTSC model is
based on dependency structures and lexicalized nat-
urally, DTSCs are more similar to phrases than other
translation units based on phrase structures. This
means that phrases will be easier to be integrated
into our model.
The way to integrate phrases is quite straightfor-
ward: if there is a treelet rooted at the current node,
3There are wildcard nodes or uncovered nodes to be han-
dled.
45
of which the word sequence is continuous and iden-
tical to the source of some phrase, then a phrase-
style DTSC will be generated which uses the target
string of the phrase as its own target. The procedure
is finished during decoding. In our experiments, in-
tegrating phrases improves the performance greatly.
7 Current Implementation
To test our idea, we implemented the dependency
treelet string correspondence model in a Chinese-
English machine translation system. The current im-
plementation in this system is actually a simplified
version of the DTSC model introduced above. In
this version, we used a simple heuristic way for the
operation of attaching rather than a sophisticated sta-
tistical model which can learn ordering information
from the training corpus. Since dependency struc-
tures are more?flattened? compared with phrasal
structures, there are many subnodes which will not
be covered even by generalized matched DTSCs.
This means the attaching operation is very common
during decoding. Therefore better attaching model
which calculates the best point for attaching , we be-
lieve, will improve the performance greatly and is a
major goal for our future research.
To obtain the dependency structures of the source
side, one can parse the source sentences with a de-
pendency parser or parse them with a phrasal struc-
ture parser and then convert the phrasal structures
into dependency structures. In our experiments we
used a Chinese parser implemented by Xiong et
al. (2005) which generates phrasal structures. The
parser was trained on articles 1-270 of Penn Chinese
Treebank version 1.0 and achieved 79.4% (F1 mea-
sure). We then converted the phrasal structure trees
into dependency trees using the way introduced by
Xia (1999).
To obtain the word alignments, we use the way
of Koehn et al (2005). After running GIZA++
(Och and Ney, 2000) in both directions, we apply
the ?grow-diag-final? refinement rule on the in-
tersection alignments for each sentence pair.
The training corpus consists of 31, 149 sentence
pairs with 823K Chinese words and 927K English
words. For the language model, we used SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram model with modified Kneser-Ney smooth-
Systems BLEU-4
PB 20.88 ? 0.87
DTSC 20.20 ? 0.81
DTSC + phrases 21.46 ? 0.83
Table 2: BLEU-4 scores for our system and a
phrase-based system.
ing on the 31, 149 English sentences. We selected
580 short sentences of length at most 50 characters
from the 2002 NIST MT Evaluation test set as our
development corpus and used it to tune ?s by max-
imizing the BLEU score (Och, 2003), and used the
2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729,
964 distinct DTSCs with the configuration { ary-
limit = 4, depth-limit = 4, len-limit = 15, gap-limit
= 2, comb-limit = 20 }. Among them, 160,694
DTSCs are used for the test set. To run our de-
coder on the development and test set, we set stack-
thrshold = 0.0001, stack-limit = 100, node-limit =
100, operation-limit = 20.
We also ran a phrase-based system (PB) with a
distortion reordering model (Xiong et al, 2006) on
the same corpus. The results are shown in table 2.
For all BLEU scores, we also show the 95% confi-
dence intervals computed using Zhang?s significant
tester (Zhang et al, 2004) which was modified to
conform to NIST?s definition of the BLEU brevity
penalty. The BLEU score of our current system with
the DTSC model is lower than that of the phrase-
based system. However, with phrases integrated, the
performance is improved greatly, and the new BLEU
score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang?s
tester. This result can be improved further using a
better parser (Quirk et al, 2006) or using a statisti-
cal attaching model.
8 Related Work
The DTSC model is different from previous work
based on dependency grammars by Eisner (2003),
Lin (2004), Quirk et al (2005), Ding et al (2005)
since they all deduce dependency structures on the
target side. Among them, the most similar work is
(Quirk et al, 2005). But there are still several major
differences beyond the one mentioned above. Our
46
treelets allow variables at any non-crossed nodes and
target strings allow gaps, which are not available in
(Quirk et al, 2005). Our language model is calcu-
lated during decoding while Quirk?s language model
is computed after decoding because of the complex-
ity of their decoding.
The DTSC model is also quite distinct from pre-
vious tree-string models by Marcu et al (2006)
and Liu et al (2006). Firstly, their models are
based on phrase structure grammars. Secondly, sub-
trees instead of treelets are extracted in their mod-
els. Thirdly, it seems to be more difficult to integrate
phrases into their models. And finally, our model al-
low gaps on the target side, which is an advantage
shared by (Melamed, 2004) and (Simard, 2005).
9 Conclusions and Future Work
We presented a novel syntax-based model using
dependency trees on the source side?dependency
treelet string correspondence model?for statistical
machine translation. We described an algorithm to
learn DTSCs automatically from the training corpus
and a chart-style algorithm for decoding.
Currently, we implemented a simple version of
the DTSC model. We believe that our performance
can be improved greatly using a more sophisticated
mechanism for determining attaching points. There-
fore the most important future work should be to de-
sign a better attaching model. Furthermore, we plan
to use larger corpora for training and n-best depen-
dency trees for decoding, which both are helpful for
the improvement of translation quality.
Acknowledgements
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Translation Us-
ing Probabilistic Synchronous Dependency Insertion Gram-
mars. In Proceedings of ACL.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot. 2005.
Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In International Workshop on Spo-
ken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phraases. In Proceedings of
EMNLP.
I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statisti-
cal Machine Translation. In Proceedings of the Conference
on Theoretical and Methodological Issues in Machine Trans-
lation (TMI), Baltimore, MD.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation. In
Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical machine
translation. In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Fei Xia. 1999. Automatic Grammar Generation from Two Dif-
ferent Perspectives. PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of COLING-ACL, Sydney,
Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC,
pages 2051? 2054.
47
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 47?54,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Improving Statistical Machine Translation Using
Domain Bilingual Multiword Expressions
Zhixiang Ren1 Yajuan Lu?1 Jie Cao1 Qun Liu1 Yun Huang2
1Key Lab. of Intelligent Info. Processing 2Department of Computer Science
Institute of Computing Technology School of Computing
Chinese Academy of Sciences National University of Singapore
P.O. Box 2704, Beijing 100190, China Computing 1, Law Link, Singapore 117590
{renzhixiang,lvyajuan huangyun@comp.nus.edu.sg
caojie,liuqun}@ict.ac.cn
Abstract
Multiword expressions (MWEs) have
been proved useful for many natural lan-
guage processing tasks. However, how to
use them to improve performance of statis-
tical machine translation (SMT) is not well
studied. This paper presents a simple yet
effective strategy to extract domain bilin-
gual multiword expressions. In addition,
we implement three methods to integrate
bilingual MWEs to Moses, the state-of-
the-art phrase-based machine translation
system. Experiments show that bilingual
MWEs could improve translation perfor-
mance significantly.
1 Introduction
Phrase-based machine translation model has been
proved a great improvement over the initial word-
based approaches (Brown et al, 1993). Recent
syntax-based models perform even better than
phrase-based models. However, when syntax-
based models are applied to new domain with few
syntax-annotated corpus, the translation perfor-
mance would decrease. To utilize the robustness
of phrases and make up the lack of syntax or se-
mantic information in phrase-based model for do-
main translation, we study domain bilingual mul-
tiword expressions and integrate them to the exist-
ing phrase-based model.
A multiword expression (MWE) can be consid-
ered as word sequence with relatively fixed struc-
ture representing special meanings. There is no
uniform definition of MWE, and many researchers
give different properties of MWE. Sag et al (2002)
roughly defined MWE as ?idiosyncratic interpre-
tations that cross word boundaries (or spaces)?.
Cruys and Moiro?n (2007) focused on the non-
compositional property of MWE, i.e. the property
that whole expression cannot be derived from their
component words. Stanford university launched
a MWE project1, in which different qualities of
MWE were presented. For bilingual multiword
expression (BiMWE), we define a bilingual phrase
as a bilingual MWE if (1) the source phrase is a
MWE in source language; (2) the source phrase
and the target phrase must be translated to each
other exactly, i.e. there is no additional (boundary)
word in target phrase which cannot find the corre-
sponding word in source phrase, and vice versa.
In recent years, many useful methods have been
proposed to extract MWEs or BiMWEs automati-
cally (Piao et al, 2005; Bannard, 2007; Fazly and
Stevenson, 2006). Since MWE usually constrains
possible senses of a polysemous word in context,
they can be used in many NLP applications such
as information retrieval, question answering, word
sense disambiguation and so on.
For machine translation, Piao et al (2005) have
noted that the issue of MWE identification and
accurate interpretation from source to target lan-
guage remained an unsolved problem for existing
MT systems. This problem is more severe when
MT systems are used to translate domain-specific
texts, since they may include technical terminol-
ogy as well as more general fixed expressions and
idioms. Although some MT systems may employ
a machine-readable bilingual dictionary of MWE,
it is time-consuming and inefficient to obtain this
resource manually. Therefore, some researchers
have tried to use automatically extracted bilingual
MWEs in SMT. Tanaka and Baldwin (2003) de-
scribed an approach of noun-noun compound ma-
chine translation, but no significant comparison
was presented. Lambert and Banchs (2005) pre-
sented a method in which bilingual MWEs were
used to modify the word alignment so as to im-
prove the SMT quality. In their work, a bilin-
gual MWE in training corpus was grouped as
1http://mwe.stanford.edu/
47
one unique token before training alignment mod-
els. They reported that both alignment quality
and translation accuracy were improved on a small
corpus. However, in their further study, they re-
ported even lower BLEU scores after grouping
MWEs according to part-of-speech on a large cor-
pus (Lambert and Banchs, 2006). Nonetheless,
since MWE represents liguistic knowledge, the
role and usefulness of MWE in full-scale SMT
is intuitively positive. The difficulty lies in how
to integrate bilingual MWEs into existing SMT
system to improve SMT performance, especially
when translating domain texts.
In this paper, we implement three methods that
integrate domain bilingual MWEs into a phrase-
based SMT system, and show that these ap-
proaches improve translation quality significantly.
The main difference between our methods and
Lambert and Banchs? work is that we directly aim
at improving the SMT performance rather than im-
proving the word alignment quality. In detail, dif-
ferences are listed as follows:
? Instead of using the bilingual n-gram trans-
lation model, we choose the phrase-based
SMT system, Moses2, which achieves sig-
nificantly better translation performance than
many other SMT systems and is a state-of-
the-art SMT system.
? Instead of improving translation indirectly
by improving the word alignment quality,
we directly target at the quality of transla-
tion. Some researchers have argued that large
gains of alignment performance under many
metrics only led to small gains in translation
performance (Ayan and Dorr, 2006; Fraser
and Marcu, 2007).
Besides the above differences, there are some
advantages of our approaches:
? In our method, automatically extracted
MWEs are used as additional resources rather
than as phrase-table filter. Since bilingual
MWEs are extracted according to noisy au-
tomatic word alignment, errors in word align-
ment would further propagate to the SMT and
hurt SMT performance.
? We conduct experiments on domain-specific
corpus. For one thing, domain-specific
2http://www.statmt.org/moses/
corpus potentially includes a large number
of technical terminologies as well as more
general fixed expressions and idioms, i.e.
domain-specific corpus has high MWE cov-
erage. For another, after the investigation,
current SMT system could not effectively
deal with these domain-specific MWEs es-
pecially for Chinese, since these MWEs are
more flexible and concise. Take the Chi-
nese term ?^ j ? (? for example. The
meaning of this term is ?soften hard mass
and dispel pathogenic accumulation?. Ev-
ery word of this term represents a special
meaning and cannot be understood literally
or without this context. These terms are dif-
ficult to be translated even for humans, let
alone machine translation. So, treating these
terms as MWEs and applying them in SMT
system have practical significance.
? In our approach, no additional corpus is intro-
duced. We attempt to extract useful MWEs
from the training corpus and adopt suitable
methods to apply them. Thus, it benefits
for the full exploitation of available resources
without increasing great time and space com-
plexities of SMT system.
The remainder of the paper is organized as fol-
lows. Section 2 describes the bilingual MWE ex-
traction technique. Section 3 proposes three meth-
ods to apply bilingual MWEs in SMT system.
Section 4 presents the experimental results. Sec-
tion 5 draws conclusions and describes the future
work. Since this paper mainly focuses on the ap-
plication of BiMWE in SMT, we only give a brief
introduction on monolingual and bilingual MWE
extraction.
2 Bilingual Multiword Expression
Extraction
In this section we describe our approach of bilin-
gual MWE extraction. In the first step, we obtain
monolingual MWEs from the Chinese part of par-
allel corpus. After that, we look for the translation
of the extracted MWEs from parallel corpus.
2.1 Automatic Extraction of MWEs
In the past two decades, many different ap-
proaches on automatic MWE identification were
reported. In general, those approaches can be
classified into three main trends: (1) statisti-
cal approaches (Pantel and Lin, 2001; Piao et
48
al., 2005), (2) syntactic approaches (Fazly and
Stevenson, 2006; Bannard, 2007), and (3) seman-
tic approaches (Baldwin et al, 2003; Cruys and
Moiro?n, 2007). Syntax-based and semantic-based
methods achieve high precision, but syntax or se-
mantic analysis has to be introduced as preparing
step, so it is difficult to apply them to domains with
few syntactical or semantic annotation. Statistical
approaches only consider frequency information,
so they can be used to obtain MWEs from bilin-
gual corpora without deeper syntactic or semantic
analysis. Most statistical measures only take two
words into account, so it not easy to extract MWEs
containing three or more than three words.
Log Likelihood Ratio (LLR) has been proved a
good statistical measurement of the association of
two random variables (Chang et al, 2002). We
adopt the idea of statistical approaches, and pro-
pose a new algorithm named LLR-based Hierar-
chical Reducing Algorithm (HRA for short) to ex-
tract MWEs with arbitrary lengths. To illustrate
our algorithm, firstly we define some useful items.
In the following definitions, we assume the given
sentence is ?A B C D E?.
Definition 1 Unit: A unit is any sub-string of the
given sentence. For example, ?A B?, ?C?, ?C D E?
are all units, but ?A B D? is not a unit.
Definition 2 List: A list is an ordered sequence of
units which exactly cover the given sentence. For
example, {?A?,?B C D?,?E?} forms a list.
Definition 3 Score: The score function only de-
fines on two adjacent units and return the LLR
between the last word of first unit and the first
word of the second unit3. For example, the score
of adjacent unit ?B C? and ?D E? is defined as
LLR(?C?,?D?).
Definition 4 Select: The selecting operator is to
find the two adjacent units with maximum score
in a list.
Definition 5 Reduce: The reducing operator is to
remove two specific adjacent units, concatenate
them, and put back the result unit to the removed
position. For example, if we want to reduce unit
?B C? and unit ?D? in list {?A?,?B C?,?D?,?E?},
we will get the list {?A?,?B C D?,?E?}.
Initially, every word in the sentence is consid-
ered as one unit and all these units form a initial
list L. If the sentence is of length N , then the
3we use a stoplist to eliminate the units containing func-
tion words by setting their score to 0
list contains N units, of course. The final set of
MWEs, S, is initialized to empty set. After initial-
ization, the algorithm will enter an iterating loop
with two steps: (1) select the two adjacent units
with maximum score in L, naming U1 and U2; and
(2) reduce U1 and U2 in L, and insert the reducing
result into the final set S. Our algorithm termi-
nates on two conditions: (1) if the maximum score
after selection is less than a given threshold; or (2)
if L contains only one unit.
c1(?? p ?? w  ? )
c1(?? p ?? w )
c1(?? p ?? w)
c1(??)
c2(p ?? w)
c2(p ??)
c2(p) c3(??) c4(w) c5()
c6(? )
c6(?) c7()
?? p ?? w  ? 147.1 6755.2 1059.6 0 0 809.6
Figure 1: Example of Hierarchical Reducing Al-
gorithm
Let us make the algorithm clearer with an ex-
ample. Assume the threshold of score is 20, the
given sentence is ??? p ?? w  ? ?4.
Figure 1 shows the hierarchical structure of given
sentence (based on LLR of adjacent words). In
this example, four MWEs (?p ???, ?p ??
w?, ?? ?, ??? p ?? w?) are extracted
in the order, and sub-strings over dotted line in fig-
ure 1 are not extracted.
From the above example, we can see that the
extracted MWEs correspond to human intuition.
In general, the basic idea of HRA is to reflect
the hierarchical structure pattern of natural lan-
guage. Furthermore, in the HRA, MWEs are mea-
sured with the minimum LLR of adjacent words
in them, which gives lexical confidence of ex-
tracted MWEs. Finally, suppose given sentence
has length N , HRA would definitely terminate
within N ? 1 iterations, which is very efficient.
However, HRA has a problem that it would ex-
tract substrings before extracting the whole string,
even if the substrings only appear in the particu-
lar whole string, which we consider useless. To
solve this problem, we use contextual features,
4The whole sentence means ?healthy tea for preventing
hyperlipidemia?, and we give the meaning for each Chi-
nese word: ??(preventing), p(hyper-), ??(-lipid-), w(-
emia), (for),?(healthy),(tea).
49
contextual entropy (Luo and Sun, 2003) and C-
value (Frantzi and Ananiadou, 1996), to filter out
those substrings which exist only in few MWEs.
2.2 Automatic Extraction of MWE?s
Translation
In subsection 2.1, we described the algorithm to
obtain MWEs, and we would like to introduce the
procedure to find their translations from parallel
corpus in this subsection.
For mining the English translations of Chinese
MWEs, we first obtain the candidate translations
of a given MWE from the parallel corpus. Steps
are listed as follows:
1. Run GIZA++5 to align words in the training
parallel corpus.
2. For a given MWE, find the bilingual sentence
pairs where the source language sentences in-
clude the MWE.
3. Extract the candidate translations of the
MWE from the above sentence pairs accord-
ing to the algorithm described by Och (2002).
After the above procedure, we have already
extracted all possible candidate translations of a
given MWE. The next step is to distinguish right
candidates from wrong candidates. We construct
perceptron-based classification model (Collins,
2002) to solve the problem. We design two
groups of features: translation features, which
describe the mutual translating chance between
source phrase and target phrase, and the language
features, which refer to how well a candidate
is a reasonable translation. The translation fea-
tures include: (1) the logarithm of source-target
translation probability; (2) the logarithm of target-
source translation probability; (3) the logarithm
of source-target lexical weighting; (4) the loga-
rithm of target-source lexical weighting; and (5)
the logarithm of the phrase pair?s LLR (Dunning,
1993). The first four features are exactly the same
as the four translation probabilities used in tradi-
tional phrase-based system (Koehn et al, 2003).
The language features include: (1) the left entropy
of the target phrase (Luo and Sun, 2003); (2) the
right entropy of the target phrase; (3) the first word
of the target phrase; (4) the last word of the target
phrase; and (5) all words in the target phrase.
5http://www.fjoch.com/GIZA++.html
We select and annotate 33000 phrase pairs ran-
domly, of which 30000 pairs are used as training
set and 3000 pairs are used as test set. We use the
perceptron training algorithm to train the model.
As the experiments reveal, the classification preci-
sion of this model is 91.67%.
3 Application of Bilingual MWEs
Intuitively, bilingual MWE is useful to improve
the performance of SMT. However, as we de-
scribed in section 1, it still needs further research
on how to integrate bilingual MWEs into SMT
system. In this section, we propose three methods
to utilize bilingual MWEs, and we will compare
their performance in section 4.
3.1 Model Retraining with Bilingual MWEs
Bilingual phrase table is very important for
phrase-based MT system. However, due to the er-
rors in automatic word alignment and unaligned
word extension in phrase extraction (Och, 2002),
many meaningless phrases would be extracted,
which results in inaccuracy of phrase probability
estimation. To alleviate this problem, we take the
automatically extracted bilingual MWEs as paral-
lel sentence pairs, add them into the training cor-
pus, and retrain the model using GIZA++. By
increasing the occurrences of bilingual MWEs,
which are good phrases, we expect that the align-
ment would be modified and the probability es-
timation would be more reasonable. Wu et al
(2008) also used this method to perform domain
adaption for SMT. Different from their approach,
in which bilingual MWEs are extracted from ad-
ditional corpus, we extract bilingual MWEs from
the original training set. The fact that additional
resources can improve the domain-specific SMT
performance was proved by many researchers
(Wu et al, 2008; Eck et al, 2004). However,
our method shows that making better use of the
resources in hand could also enhance the quality
of SMT system. We use ?Baseline+BiMWE? to
represent this method.
3.2 New Feature for Bilingual MWEs
Lopez and Resnik (2006) once pointed out that
better feature mining can lead to substantial gain
in translation quality. Inspired by this idea, we
append one feature into bilingual phrase table to
indicate that whether a bilingual phrase contains
bilingual MWEs. In other words, if the source lan-
guage phrase contains a MWE (as substring) and
50
the target language phrase contains the translation
of the MWE (as substring), the feature value is 1,
otherwise the feature value is set to 0. Due to the
high reliability of bilingual MWEs, we expect that
this feature could help SMT system to select bet-
ter and reasonable phrase pairs during translation.
We use ?Baseline+Feat? to represent this method.
3.3 Additional Phrase Table of bilingual
MWEs
Wu et al (2008) proposed a method to construct a
phrase table by a manually-made translation dic-
tionary. Instead of manually constructing transla-
tion dictionary, we construct an additional phrase
table containing automatically extracted bilingual
MWEs. As to probability assignment, we just as-
sign 1 to the four translation probabilities for sim-
plicity. Since Moses supports multiple bilingual
phrase tables, we combine the original phrase ta-
ble and new constructed bilingual MWE table. For
each phrase in input sentence during translation,
the decoder would search all candidate transla-
tion phrases in both phrase tables. We use ?Base-
line+NewBP? to represent this method.
4 Experiments
4.1 Data
We run experiments on two domain-specific patent
corpora: one is for traditional medicine domain,
and the other is for chemical industry domain. Our
translation tasks are Chinese-to-English.
In the traditional medicine domain, table 1
shows the data statistics. For language model, we
use SRI Language Modeling Toolkit6 to train a tri-
gram model with modified Kneser-Ney smoothing
(Chen and Goodman, 1998) on the target side of
training corpus. Using our bilingual MWE ex-
tracting algorithm, 80287 bilingual MWEs are ex-
tracted from the training set.
Chinese English
Training Sentences 120,355
Words 4,688,873 4,737,843
Dev Sentences 1,000
Words 31,722 32,390
Test Sentences 1,000
Words 41,643 40,551
Table 1: Traditional medicine corpus
6http://www.speech.sri.com/projects/srilm/
In the chemical industry domain, table 2 gives
the detail information of the data. In this experi-
ment, 59466 bilingual MWEs are extracted.
Chinese English
Training Sentences 120,856
Words 4,532,503 4,311,682
Dev Sentences 1,099
Words 42,122 40,521
Test Sentences 1,099
Words 41,069 39,210
Table 2: Chemical industry corpus
We test translation quality on test set and use the
open source tool mteval-vllb.pl7 to calculate case-
sensitive BLEU 4 score (Papineni et al, 2002) as
our evaluation criteria. For this evaluation, there is
only one reference per test sentence. We also per-
form statistical significant test between two trans-
lation results (Collins et al, 2005). The mean of
all scores and relative standard deviation are calcu-
lated with a 99% confidence interval of the mean.
4.2 MT Systems
We use the state-of-the-art phrase-based SMT sys-
tem, Moses, as our baseline system. The features
used in baseline system include: (1) four transla-
tion probability features; (2) one language model
feature; (3) distance-based and lexicalized distor-
tion model feature; (4) word penalty; (5) phrase
penalty. For ?Baseline+BiMWE? method, bilin-
gual MWEs are added into training corpus, as a
result, new alignment and new phrase table are
obtained. For ?Baseline+Feat? method, one ad-
ditional 0/1 feature are introduced to each entry in
phrase table. For ?Baseline+NewBP?, additional
phrase table constructed by bilingual MWEs is
used.
Features are combined in the log-linear model.
To obtain the best translation e? of the source sen-
tence f , log-linear model uses following equation:
e? = arg max
e
p(e|f)
= arg max
e
M
?
m=1
?mhm(e, f) (1)
in which hm and ?m denote the mth feature and
weight. The weights are automatically turned by
minimum error rate training (Och, 2002) on devel-
opment set.
7http://www.nist.gov/speech/tests/mt/resources/scoring.htm
51
4.3 Results
Methods BLEU
Baseline 0.2658
Baseline+BiMWE 0.2661
Baseline+Feat 0.2675
Baseline+NewBP 0.2719
Table 3: Translation results of using bilingual
MWEs in traditional medicine domain
Table 3 gives our experiment results. From
this table, we can see that, bilingual MWEs
improve translation quality in all cases. The
Baseline+NewBP method achieves the most im-
provement of 0.61% BLEU score compared
with the baseline system. The Baseline+Feat
method comes next with 0.17% BLEU score im-
provement. And the Baseline+BiMWE achieves
slightly higher translation quality than the baseline
system.
To our disappointment, however, none of these
improvements are statistical significant. We
manually examine the extracted bilingual MWEs
which are labeled positive by perceptron algorithm
and find that although the classification precision
is high (91.67%), the proportion of positive exam-
ple is relatively lower (76.69%). The low positive
proportion means that many negative instances
have been wrongly classified to positive, which in-
troduce noises. To remove noisy bilingual MWEs,
we use the length ratio x of the source phrase over
the target phrase to rank the bilingual MWEs la-
beled positive. Assume x follows Gaussian distri-
butions, then the ranking score of phrase pair (s, t)
is defined as the following formula:
Score(s, t) = log(LLR(s, t))? 1?
2pi?
?e?
(x??)2
2?2
(2)
Here the mean ? and variance ?2 are estimated
from the training set. After ranking by score, we
select the top 50000, 60000 and 70000 bilingual
MWEs to perform the three methods mentioned in
section 3. The results are showed in table 4.
From this table, we can conclude that: (1) All
the three methods on all settings improve BLEU
score; (2) Except the Baseline+BiMWE method,
the other two methods obtain significant improve-
ment of BLEU score (0.2728, 0.2734, 0.2724)
over baseline system (0.2658); (3) When the scale
of bilingual MWEs is relatively small (50000,
60000), the Baseline+Feat method performs better
Methods 50000 60000 70000
Baseline 0.2658
Baseline+BiMWE 0.2671 0.2686 0.2715
Baseline+Feat 0.2728 0.2734 0.2712
Baseline+NewBP 0.2662 0.2706 0.2724
Table 4: Translation results of using bilingual
MWEs in traditional medicine domain
than others; (4) As the number of bilingual MWEs
increasing, the Baseline+NewBP method outper-
forms the Baseline+Feat method; (5) Comparing
table 4 and 3, we can see it is not true that the
more bilingual MWEs, the better performance of
phrase-based SMT. This conclusion is the same as
(Lambert and Banchs, 2005).
To verify the assumption that bilingual MWEs
do indeed improve the SMT performance not only
on particular domain, we also perform some ex-
periments on chemical industry domain. Table 5
shows the results. From this table, we can see that
these three methods can improve the translation
performance on chemical industry domain as well
as on the traditional medicine domain.
Methods BLEU
Baseline 0.1882
Baseline+BiMWE 0.1928
Baseline+Feat 0.1917
Baseline+Newbp 0.1914
Table 5: Translation results of using bilingual
MWEs in chemical industry domain
4.4 Discussion
In order to know in what respects our methods im-
prove performance of translation, we manually an-
alyze some test sentences and gives some exam-
ples in this subsection.
(1) For the first example in table 6, ?? ??
is aligned to other words and not correctly trans-
lated in baseline system, while it is aligned to cor-
rect target phrase ?dredging meridians? in Base-
line+BiMWE, since the bilingual MWE (?? ??,
?dredging meridians?) has been added into train-
ing corpus and then aligned by GIZA++.
(2) For the second example in table 6, ??
? has two candidate translation in phrase table:
?tea? and ?medicated tea?. The baseline system
chooses the ?tea? as the translation of ?? ?,
while the Baseline+Feat system chooses the ?med-
52
Src T ?? ? k ?? ! ? ? ! ? ? ! ) 9 ! | Y ! S  _? ?
 , ? ?  ?E 8 "
Ref the obtained product is effective in tonifying blood , expelling cold , dredging meridians
, promoting production of body fluid , promoting urination , and tranquilizing mind ;
and can be used for supplementing nutrition and protecting health .
Baseline the food has effects in tonifying blood , dispelling cold , promoting salivation and water
, and tranquilizing , and tonic effects , and making nutritious health .
+Bimwe the food has effects in tonifying blood , dispelling cold , dredging meridians , promoting
salivation , promoting urination , and tranquilizing tonic , nutritious pulverizing .
Src ? ? ???J ! J ! ?J !?! 5J "
Ref the product can also be made into tablet , pill , powder , medicated tea , or injection .
Baseline may also be made into tablet , pill , powder , tea , or injection .
+Feat may also be made into tablet , pill , powder , medicated tea , or injection .
Table 6: Translation example
icated tea? because the additional feature gives
high probability of the correct translation ?medi-
cated tea?.
5 Conclusion and Future Works
This paper presents the LLR-based hierarchical
reducing algorithm to automatically extract bilin-
gual MWEs and investigates the performance of
three different application strategies in applying
bilingual MWEs for SMT system. The translation
results show that using an additional feature to rep-
resent whether a bilingual phrase contains bilin-
gual MWEs performs the best in most cases. The
other two strategies can also improve the quality
of SMT system, although not as much as the first
one. These results are encouraging and motivated
to do further research in this area.
The strategies of bilingual MWE application is
roughly simply and coarse in this paper. Com-
plicated approaches should be taken into account
during applying bilingual MWEs. For example,
we may consider other features of the bilingual
MWEs and examine their effect on the SMT per-
formance. Besides application in phrase-based
SMT system, bilingual MWEs may also be inte-
grated into other MT models such as hierarchical
phrase-based models or syntax-based translation
models. We will do further studies on improving
statistical machine translation using domain bilin-
gual MWEs.
Acknowledgments
This work is supported by National Natural Sci-
ence Foundation of China, Contracts 60603095
and 60873167. We would like to thank the anony-
mous reviewers for their insightful comments on
an earlier draft of this paper.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond aer: an extensive analysis of word align-
ments and their impact on mt. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 9?16.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions: Analysis, Acquisiton and Treatment,
pages 89?96.
Colin Bannard. 2007. A measure of syntactic flex-
ibility for automatically identifying multiword ex-
pressions in corpora. In Proceedings of the ACL
Workshop on A Broader Perspective on Multiword
Expressions, pages 1?8.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Baobao Chang, Pernilla Danielsson, and Wolfgang
Teubert. 2002. Extraction of translation unit from
chinese-english parallel corpora. In Proceedings of
the first SIGHAN workshop on Chinese language
processing, pages 1?5.
Stanley F. Chen and Joshua Goodman. 1998. Am em-
pirical study of smoothing techniques for language
modeling. Technical report.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
53
Meeting on Association for Computational Linguis-
tics, pages 531?540.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Empirical Methods in Natural Language Pro-
cessing Conference, pages 1?8.
Tim Van de Cruys and Begon?a Villada Moiro?n. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on A Broader Per-
spective on Multiword Expressions, pages 25?32.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Improving statistical machine translation in the med-
ical domain using the unified medical language sys-
tem. In Proceedings of the 20th international con-
ference on Computational Linguistics table of con-
tents, pages 792?798.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In Proceedings of the EACL,
pages 337?344.
Katerina T. Frantzi and Sophia Ananiadou. 1996. Ex-
tracting nested collocations. In Proceedings of the
16th conference on Computational linguistics, pages
41?46.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293?303.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54.
Patrik Lambert and Rafael Banchs. 2005. Data in-
ferred multi-word expressions for statistical machine
translation. In Proceedings of Machine Translation
Summit X, pages 396?403.
Patrik Lambert and Rafael Banchs. 2006. Grouping
multi-word expressions according to part-of-speech
in statistical machine translation. In Proceedings of
the Workshop on Multi-word-expressions in a multi-
lingual context, pages 9?16.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the
link? In proceedings of the 7th conference of the as-
sociation for machine translation in the Americas:
visions for the future of machine translation, pages
90?99.
Shengfen Luo and Maosong Sun. 2003. Two-character
chinese word extraction based on hybrid of internal
and contextual measures. In Proceedings of the sec-
ond SIGHAN workshop on Chinese language pro-
cessing, pages 24?30.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.d. thesis, Computer Science Department,
RWTH Aachen, Germany.
Patrick Pantel and Dekang Lin. 2001. A statistical cor-
pus based term extractor. In AI ?01: Proceedings of
the 14th Biennial Conference of the Canadian Soci-
ety on Computational Studies of Intelligence, pages
36?46.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Conference of the Association for Com-
putational Linguistics, pages 311?318.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining a
semantic tagger and a statistical tool for mwe extrac-
tion. Computer Speech and Language, 19(4):378?
397.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for nlp. In
Proceedings of the 3th International Conference
on Intelligent Text Processing and Computational
Linguistics(CICLing-2002), pages 1?15.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of
the ACL-2003 Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment, pages 17?24.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of Conference on Computa-
tional Linguistics (COLING), pages 993?1000.
54
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 25?28,
Paris, October 2009. c?2009 Association for Computational Linguistics
Automatic Adaptation of Annotation Standards for Dependency Parsing
? Using Projected Treebank as Source Corpus
Wenbin Jiang and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jiangwenbin, liuqun}@ict.ac.cn
Abstract
We describe for dependency parsing an an-
notation adaptation strategy, which can au-
tomatically transfer the knowledge from
a source corpus with a different annota-
tion standard to the desired target parser,
with the supervision by a target corpus an-
notated in the desired standard. Further-
more, instead of a hand-annotated one, a
projected treebank derived from a bilin-
gual corpus is used as the source cor-
pus. This benefits the resource-scarce
languages which haven?t different hand-
annotated treebanks. Experiments show
that the target parser gains significant im-
provement over the baseline parser trained
on the target corpus only, when the target
corpus is smaller.
1 Introduction
Automatic annotation adaptation for sequence la-
beling (Jiang et al, 2009) aims to enhance a
tagger with one annotation standard by transfer-
ring knowledge from a source corpus annotated in
another standard. It would be valuable to adapt
this strategy to parsing, since for some languages
there are also several treebanks with different an-
notation standards, such as Chomskian-style Penn
Treebank (Marcus et al, 1993) and HPSG LinGo
Redwoods Treebank (Oepen et al, 2002) for En-
glish. However, we are not content with conduct-
ing annotation adaptation between existing differ-
ent treebanks, because it would be more valuable
to boost the parsers also for the resource-scarce
languages, rather than only for the resource-rich
ones that already have several treebanks.
Although hand-annotated treebanks are costly
and scarce, it is not difficult for many languages to
collect large numbers of bilingual sentence-pairs
aligned to English. According to the word align-
ment, the English parses can be projected across
to their translations, and the projected trees can be
leveraged to boost parsing. Many efforts are de-
voted to the research on projected treebanks, such
as (Lu? et al, 2002), (Hwa et al, 2005) and
(Ganchev et al, 2009), etc. Considering the fact
that a projected treebank partially inherits the En-
glish annotation standard, some hand-written rules
are designed to deal with the divergence between
languages such as in (Hwa et al, 2002). How-
ever, it will be more valuable and interesting to
adapt this divergence automatically and boost the
existing parsers with this projected treebank.
In this paper, we investigate the automatic anno-
tation adaptation strategy for Chinese dependency
parsing, where the source corpus for adaptation is
a projected treebank derived from a bilingual cor-
pus aligned to English with word alignment and
English trees. We also propose a novel, error-
tolerant tree-projecting algorithm, which dynam-
ically searches the project Chinese tree that has
the largest consistency with the corresponding En-
glish tree, according to an alignment matrix rather
than a single alignment. Experiments show that
when the target corpus is smaller, the projected
Chinese treebank, although with inevitable noise
caused by non-literal translation and word align-
ment error, can be successfully utilized and re-
sult in significant improvement over the baseline
model trained on the target corpus only.
In the rest of the paper, we first present the tree-
projecting algorithm (section 2), and then the an-
notation adaptation strategy (section 3). After dis-
cussing the related work (section 4) we show the
experiments (section 5).
2 Error-Tolerant Tree-Projecting
Algorithm
Previous works making use of projected cor-
pus usually adopt the direct-mapping method for
structure projection (Yarowsky and Ngai, 2001;
Hwa et al, 2005; Ganchev et al, 2009), where
25
some filtering is needed to eliminate the inaccurate
or conflicting labels or dependency edges. Here
we propose a more robust algorithm for depen-
dency tree projection. According to the align-
ment matrix, this algorithm dynamically searches
the projected Chinese dependency tree which has
the largest consistency with the corresponding En-
glish tree.
We briefly introduce the alignment matrix be-
fore describing our projecting algorithm. Given
a Chinese sentence C1:M and its English transla-
tion E1:N , the alignment matrix A is an M ? N
matrix with each element Ai,j denoting the proba-
bility of Chinese word Ci aligned to English word
Ej . Such structure potentially encodes many more
possible alignments.
Using C(TC |TE , A) to denote the degree of Chi-
nese tree TC being consistent with English tree TE
according to alignment matrix A, the projecting al-
gorithm aims to find
T?C = argmax
TC
C(TC |TE , A) (1)
C(TC |TE , A) can be factorized into each depen-
dency edge x ? y in TC , that is to say
C(TC |TE , A) =
?
x?y?TC
Ce(x ? y|TE , A) (2)
We can obtain Ce by simple accumulation across
all possible alignments
Ce(x ? y|TE, A)
= ?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|TE) (3)
where ?(x?, y?|TE) is a 0-1 function that equals 1
only if x? ? y? exists in TE .
The searching procedure, argmax operation in
equation 1, can be effectively solved by a simple,
bottom-up dynamic algorithm with cube-pruning
speed-up (Huang and Chiang, 2005). We omit the
detailed algorithm here due to space restrictions.
3 Annotation Adaptation for
Dependency Parsing
The automatic annotation adaptation strategy for
sequence labeling (Jiang et al, 2009) aims to
strengthen a tagger trained on a corpus annotated
in one annotation standard with a larger assistant
corpus annotated in another standard. We can de-
fine the purpose of the automatic annotation adap-
tation for dependency parsing in the same way.
Similar to that in sequence labeling, the train-
ing corpus with the desired annotation standard is
called the target corpus while the assistant cor-
pus annotated in a different standard is called
the source corpus. For training, an intermediate
parser, called the source parser, is trained directly
on the source corpus and then used to parse the tar-
get corpus. After that a second parser, called the
target parser, is trained on the target corpus with
guide features extracted from the source parser?s
parsing results. For testing, a token sequence is
first parsed by the source parser to obtain an inter-
mediate parsing result with the source annotation
standard, and then parsed by the target parser with
the guide features extracted from the intermediate
parsing result to obtain the final result.
The design of the guide features is the most im-
portant, and is specific to the parsing algorithm of
the target parser. In this work we adopt the max-
imum spanning tree (MST) algorithm (McDon-
ald et al, 2005; McDonald and Pereira, 2006) for
both the source and the target parser, so the guide
features should be defined on dependency edges
in accordance with the edge-factored property of
MST models. In the decoding procedure of the
target parser, the degree of a dependency edge be-
ing supported can be adjusted by the relationship
between this edge?s head and modifier in the in-
termediate parsing result of the source parser. The
most intuitionistic relationship is whether the de-
pendency between head and modifier exists in this
intermediate result. Such a bi-valued relationship
is similar to that in the stacking method for com-
bining dependency parsers (Martins et al, 2008;
Nivre and McDonald, 2008). The guide features
are then defined as this relationship itself as well as
its combinations with the lexical features of MST
models.
Furthermore, in order to explore more de-
tailed knowledge from the source parser, we re-
define the relationship as a four-valued variable
which covers the following situations: parent-
child, child-parent, siblings and else. With the
guide features, the parameter tuning procedure of
the target parser will automatically learn the regu-
larity of using the source parser?s intermediate re-
sult to guide its decision making.
4 Related Works
Many works have been devoted to obtain pars-
ing knowledge from word aligned bilingual cor-
26
pora. (Lu? et al, 2002) learns Chinese bracket-
ing knowledge via ITG alignment; (Hwa et al,
2005) and (Ganchev et al, 2009) induces depen-
dency grammar via projection from aligned En-
glish, where some filtering is used to reduce the
noise and some hand-designed rules to handle lan-
guage heterogeneity.
Just recently, Smith and Eisner (2009) gave
an idea similar to ours. They perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar (QG) Features. Al-
though both related to projection and annotation,
there are still important differences between these
two works. First, we design an error-tolerant
alignment-matrix-based tree-projecting algorithm
to perform whole-tree projection, while they re-
sort to QG features to score local configurations
of aligned source and target trees. Second, their
adaptation emphasizes to transform a tree from
one annotation standard to another, while our
adaptation emphasizes to strengthen the parser us-
ing a treebank annotated in a different standard.
5 Experiments
The source corpus for annotation adaptation, that
is, the projected Chinese treebank, is derived from
5.6 millions LDC Chinese-English sentence pairs.
The Chinese side of the bilingual corpus is word-
segmented and POS-tagged by an implementation
of (Jiang et al, 2008), and the English sentences
are parsed by an implementation of (McDonald
and Pereira, 2006) which is instead trained on WSJ
section of Penn English Treebank (Marcus et al,
1993). The alignment matrixes for sentence pairs
are obtained according to (Liu et al, 2009). The
English trees are then projected across to Chinese
using the algorithm in section 2. Out of these pro-
jected trees, we only select 500 thousands with
word count l s.t. 6 ? l ? 100 and with project-
ing confidence c = C(TC |TE , A)1/l s.t. c ? 0.35.
While for the target corpus, we take Penn Chinese
Treebank (CTB) 1.0 and CTB 5.0 (Xue et al,
2005) respectively, and follow the traditional cor-
pus splitting: chapters 271-300 for testing, chap-
ters 301-325 for development, and else for train-
ing.
We adopt the 2nd-order MST model (McDon-
ald et al, 2005) as the target parser for better
performance, and the 1st-order MST model as
the source parser for fast training. Both the two
parsers are trained with averaged perceptron algo-
Model P% on CTB 1 P% on CTB 5
source parser 53.28 53.28
target parser 83.56 87.34
baseline parser 82.23 87.15
Table 1: Performances of annotation adaptation
with CTB 1.0 and CTB 5.0 as the target corpus re-
spectively, as well as of the baseline parsers (2nd-
order MST parsers trained on the target corpora).
 0.7
 0.75
 0.8
 0.85
 100  1000  10000
de
pe
nd
en
cy
 a
cc
ur
ac
y
sentence count of target corpus
baseline
target parser
Figure 1: Performance of the target parsers with
target corpora of different scales.
rithm (Collins, 2002). The development set of
CTB is also used to determine the best model for
the source parser, conditioned on the hypothesis
of larger isomorphisme between Chinese and En-
glish.
Table 1 shows that the experimental results of
annotation adaptation, with CTB 1.0 and CTB 5.0
as the target corpus respectively. We can see that
the source parsers, directly trained on the source
corpora of projected trees, performs poorly on
both CTB test sets (which are in fact the same).
This is partly due to the noise in the projected tree-
bank, and partly due to the heterogeneous between
the CTB trees and the projected trees. On the
contrary, automatic annotation adaptation effec-
tively transfers the knowledge to the target parsers,
achieving improvement on both target corpora.
Especially on CTB 1.0, an accuracy increment of
1.3 points is obtained over the baseline parser.
We observe that for the much larger CTB 5.0,
the performance of annotation adaptation is much
lower. To further investigate the adaptation perfor-
mances with target corpora of different scales, we
conduct annotation adaptation on a series of tar-
get corpora which consist of different amount of
dependency trees from CTB 5.0. Curves in Fig-
ure 1 shows the experimental results. We see that
the smaller the training corpus is, the more signif-
icant improvement can be obtained. For example,
27
with a target corpus composed of 2K trees, nearly
2 points of accuracy increment is achieved. This
is a good news to the resource-scarce languages.
6 Conclusion and Future Works
This paper describes for dependency parsing an
automatic annotation adaptation strategy. What
is more important, we use a projected treebank,
rather than a hand-annotated one, as the source
corpus for adaptation. This is quite different from
previous works on projected trees (Hwa et al,
2005; Ganchev et al, 2009), and is also more valu-
able than previous works of annotation adaptation
(Jiang et al, 2009). Experiments show that this
strategy gains improvement over baseline parsers
with target corpora of different scales, especially
the smaller ones. This provides a new strategy for
resource-scarce languages to train high-precision
dependency parsers. In the future, we will adapt
this strategy to constituent parsing, which is more
challenging and interesting due to the complexity
of projection between constituent trees, and due
to the obscurity of annotation adaptation for con-
stituent parsing.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contracts 60603095
and 60736014, and 863 State Key Project No.
2006AA010108. We are grateful to the anony-
mous reviewers for their valuable suggestions. We
also thank Yang Liu for sharing his codes of align-
ment matrix generation, and Liang Huang and
Haitao Mi for helpful discussions.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311?325.
Wenbin Jiang, Liang Huang, Yajuan Lu?, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of the ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Andre? F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers.
In Proceedings of EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning Dan Flickinger, and Thorsten
Brants. 2002. The lingo redwoods treebank: Moti-
vation and preliminary applications. In In Proceed-
ings of COLING.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the NAACL.
28
	



	
			


Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 707?715,
Beijing, August 2010
Joint Parsing and Translation
Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{yliu,liuqun} @ict.ac.cn
Abstract
Tree-based translation models, which ex-
ploit the linguistic syntax of source lan-
guage, usually separate decoding into two
steps: parsing and translation. Although
this separation makes tree-based decoding
simple and efficient, its translation perfor-
mance is usually limited by the number
of parse trees offered by parser. Alter-
natively, we propose to parse and trans-
late jointly by casting tree-based transla-
tion as parsing. Given a source-language
sentence, our joint decoder produces a
parse tree on the source side and a transla-
tion on the target side simultaneously. By
combining translation and parsing mod-
els in a discriminative framework, our ap-
proach significantly outperforms a forest-
based tree-to-string system by 1.1 ab-
solute BLEU points on the NIST 2005
Chinese-English test set. As a parser,
our joint decoder achieves an F1 score of
80.6% on the Penn Chinese Treebank.
1 Introduction
Recent several years have witnessed the rapid
development of syntax-based translation models
(Chiang, 2007; Galley et al, 2006; Shen et al,
2008; Quirk et al, 2005; Liu et al, 2006; Huang
et al, 2006; Eisner, 2003; Zhang et al, 2008; Chi-
ang, 2010), which incorporate formal or linguis-
tic syntax into translation process. Depending on
whether modeling the linguistic syntax of source
language or not, we divide them into two cate-
gories: string-based and tree-based models. 1
1Mi et al (2008) also distinguish between string-based
and tree-based models but depending on the type of input.
source
target
parse+translate
string tree
string
source
target
string
parse
tree
translate
string
(a)
(b)
Figure 1: Tree-based decoding: (a) separate pars-
ing and translation versus (b) joint parsing and
translation.
String-based models include string-to-string
(Chiang, 2007) and string-to-tree (Galley et al,
2006; Shen et al, 2008). Regardless of the syn-
tactic information on the source side, they treat
decoding as a parsing problem: the decoder parses
a source-language sentence using the source pro-
jection of a synchronous grammar while building
the target sub-translations in parallel.
Tree-based models include tree-to-string (Liu
et al, 2006; Huang et al, 2006) and tree-to-tree
(Quirk et al, 2005; Eisner, 2003; Zhang et al,
2008; Chiang, 2010). These models explicitly
use source parse trees and divide decoding into
two separate steps: parsing and translation. A
parser first parses a source-language sentence into
a parse tree, and then a decoder converts the tree
to a translation on the target side (see Figure 1(a)).
Figure 2 gives a training example for tree-to-
string translation, which consists of a Chinese
tree, an English sentence, and the word align-
ment between them. Romanized Chinese words
are given to facilitate identification. Table 1 shows
707
?? ? ?9 ?1


?!
NR P NR VV AS NN
NPB NPB NPB
PP VPB
VP
IP
bushi yu shalong juxing le huitan
Bush held a meeting with Sharon
Figure 2: A training example that consists of a
Chinese parse, an English sentence, and the word
alignment between them.
a set of tree-to-string rules obtained from Figure
2. The source side of a rule is a tree fragment
and the target side is a string. We use x to denote
non-terminals and the associated subscripts indi-
cate the correspondence between non-terminals
on both sides.
Conventionally, decoding for tree-to-string
translation is cast as a tree parsing problem (Eis-
ner, 2003). The tree parsing algorithm visits each
node in the input source tree in a top-down order
and tries to match each translation rule against the
local sub-tree rooted at the node. For example, the
first rule in Table 1 matches a sub-tree rooted at
IP0,6 in Figure 2. The descendent nodes of IP0,6
(i.e., NPB0,1, PP1,3, and VPB3,6) can be further
matched by other rules in Table 1. The matching
procedure runs recursively until the entire tree is
covered. Finally, the output on the target side can
be taken as a translation.
Compared with its string-based counterparts,
tree-based decoding is simpler and faster: there
is no need for synchronous binarization (Huang
et al, 2009b; Zhang et al, 2006) and tree parsing
generally runs in linear time (Huang et al, 2006).
While separating parsing and translation makes
tree-based decoding simple and efficient, its
search space is limited by the number of parse
trees offered by parser. Studies reveal that tree-
based systems are prone to produce degenerate
translations due to the propagation of parsing mis-
takes (Quirk and Corston-Oliver, 2006). This
problem can be alleviated by offering more alter-
(1) IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2
(2) NPB(NR(bushi))?Bush
(3) PP(P(yu) x1:NPB)?with x1
(4) NPB(NR(shalong))?Sharon
(5) VPB(VV(juxing) AS(le) x1:NPB)?held a x1
(6) NPB(NN(huitan))?meeting
Table 1: Tree-to-string rules extracted from Figure
2.
natives to the pipeline. An elegant solution is to
replace 1-best trees with packed forests that en-
code exponentially many trees (Mi et al, 2008;
Liu et al, 2009). Mi et al (2008) present an
efficient algorithm to match tree-to-string rules
against packed forests that encode millions of
trees. They prove that offering more alternatives
to tree parsing improves translation performance
substantially.
In this paper, we take a further step towards the
direction of offering multiple parses to translation
by proposing joint parsing and translation. As
shown in Figure 1(b), our approach parses and
translates jointly as it finds a parse tree and a
translation of a source-language sentence simul-
taneously. We integrate the tree-to-string model
(Liu et al, 2006; Huang et al, 2006), n-gram lan-
guage model, probabilistic context-free grammar
(PCFG), and Collins? Model 1 (Collins, 2003) in a
discriminative framework (Och, 2003). Allowing
parsing and translation to interact with each other,
our approach obtains an absolute improvement of
1.1 BLEU points over a forest-based tree-to-string
translation system (Mi et al, 2008) on the 2005
NIST Chinese-English test set. As a parser, our
joint decoder achieves an F1 score of 80.6% on
the Penn Chinese Treebank.
2 Joint Parsing and Translation
2.1 Decoding as Parsing
We propose to integrate parsing and translation
into a single step. To achieve joint parsing and
translation, we cast tree-to-string decoding as a
monolingual parsing problem (Melamed, 2004;
Chiang, 2007; Galley et al, 2006): the de-
coder takes a source-language string as input and
parses it using the source-projection of SCFG
while building the corresponding sub-translations
simultaneously.
708
For example, given the Chinese sentence bushi
yu sha long juxing le huitan in Figure 2, the
derivation in Table 1 explains how a Chinese tree,
an English string, and the word alignment be-
tween them are generated synchronously. Unlike
the string-based systems as described in (Chiang,
2007; Galley et al, 2006; Shen et al, 2008), we
exploit the linguistic syntax on the source side
explicitly. Therefore, the source parse trees pro-
duced by our decoder are meaningful from a lin-
guistic point of view.
As tree-to-string rules usually have multiple
non-terminals that make decoding complexity
generally exponential, synchronous binarization
(Huang et al, 2009b; Zhang et al, 2006) is a
key technique for applying the CKY algorithm
to parsing with tree-to-string rules. 2 Huang et
al. (2009b) factor each tree-to-string rule into two
SCFG rules: one from the root nonterminal to
the subtree, and the other from the subtree to the
leaves. In this way, one can uniquely reconstruct
the original tree using a two-step SCFG deriva-
tion.
For example, consider the first rule in Table 1:
IP(x1:NPB VP(x2:PP x3:VPB))?x1 x3 x2
We use a specific non-terminal, say, T, to
uniquely identify the left-hand side subtree and
produce two SCFG rules: 3
IP ? ?T 1 ,T 1 ? (1)
T ? ?NPB 1 PP 2 VPB 3 ,NPB 1 VPB 3 PP 2 ? (2)
where the boxed numbers indicate the correspon-
dence between nonterminals.
Then, the rule (2) can be further binarized into
two rules that have at most two non-terminals:
T ? ?NPB 1 PP-VPB 2 ,NPB 1 PP-VPB 2 ? (3)
PP-VPB ? ?PP 1 VPB 2 ,VPB 2 PP 1 ? (4)
where PP-VPB is an intermediate virtual non-
terminal.
2But CKY is not the only choice. The Earley algorithm
can also be used to parse with tree-to-string rules (Zhao and
Al-Onaizan, 2008). As the Earley algorithm binarizes multi-
nonterminal rules implicitly, there is no need for synchronous
binarization.
3It might look strange that the node VP disappears. This
node is actually stored in the monolithic node T. Please refer
to page 573 of (Huang et al, 2009b) for more details about
how to convert tree-to-string rules to SCFG rules.
We call rules the tree roots of which are vir-
tual non-terminals virtual rules and others natural
rules. For example, the rule (1) is a natural rule
and the rules (3) and (4) are virtual rules. We fol-
low Huang et al (2009b) to keep the probabilities
of a natural rule unchanged and set those of a vir-
tual rule to 1. 4
After binarizing tree-to-string rules into SCFG
rules that have at most two non-terminals, we can
use the CKY algorithm to parse a source sentence
and produce its translation simultaneously as de-
scribed in (Chiang, 2007; Galley et al, 2006).
2.2 Adding Parsing Models
As our decoder produces ?genuine? parse trees
during decoding, we can integrate parsing mod-
els as features together with translation features
such as the tree-to-string model, n-gram language
model, and word penalty into a discriminative
framework (Och, 2003). We expect that pars-
ing and translation could interact with each other:
parsing offers linguistically motivated reordering
to translation and translation helps parsing resolve
ambiguity.
2.2.1 PCFG
We use the probabilistic context-free grammar
(PCFG) as the first parsing feature in our decoder.
Given a PCFG, the probability for a tree is the
product of probabilities for the rules that it con-
tains. That is, if a tree pi is a context-free deriva-
tion that involves K rules of the form ?k ? ?k ,
its probability is given by
P(pi) =
?
k=1...K
Ppcfg(?k ? ?k) (5)
For example, the probability for the tree in Fig-
ure 2 is
P(pi) = Ppcfg(IP ? NPB VP)?
Ppcfg(NPB ? NR)?
Ppcfg(NR ? bushi)?
. . . (6)
4This makes the scores of hypotheses in the same chart
cell hardly comparable because some hypotheses are cov-
ered by a natural non-terminal and others covered by a virtual
non-terminal. To alleviate this problem, we follow Huang et
al. (2009b) to separate natural and virtual hypotheses in dif-
ferent beams.
709
IP
T
NPB PP-VP
PP VPB
IP
NPB VP
PP VPB
Figure 3: Reconstructing original tree from virtual
rules. We first construct the tree on the left by
substituting the trees of the rules (1), (3), and (4)
and then restore the original tree on the right via
the monolithic node T.
There are 13 PCFG rules involved. We omit the
remaining 10 rules.
We formalize the decoding process as a deduc-
tive system to illustrate how to include a PCFG.
Given a natural rule
VP ? ?PP 1 VPB 2 ,VPB 2 PP 1 ? (7)
the following deductive step grows an item in the
chart by the rule
(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)
(VP1,6) : (w, e2e1)
(8)
where PP1,3 denotes the recognition of the non-
terminal PP spanning from the substring from po-
sition 1 through 3 (i.e., yu shalong in Figure 2), w1
and e1 are the score and translation of the first an-
tecedent item, respectively, and the resulting item
score is calculated as: 5
w = w1 + w2 + logPpcfg(VP ? PP VPB) (9)
As the PCFG probabilities of natural rules are
fixed during decoding, they can be pre-computed
and stored in the rule table. Therefore, including
PCFG for natural rules hardly increases decoding
complexity.
However, calculating the PCFG probabilities
for virtual rules is quite different due to the pres-
ence of virtual non-terminals. For instance, using
the rule (4) in Section 2.1 to generate an item leads
to the following deductive step:
(PP1,3) : (w1, e1) (VPB3,6) : (w2, e2)
(PP-VPB1,6) : (w, e2e1)
(10)
5The logarithmic form of probability is used to avoid ma-
nipulating very small numbers for practical reasons. w1 and
w2 take the PCFG probabilities of the two antecedent items
into consideration.
As PP-VPB is a virtual non-terminal, the sub-
tree it dominates is a virtual tree, for which we
cannot figure out its PCFG probability. There-
fore, we have to postpone the calculation of PCFG
probabilities until reaching a natural non-terminal
such as IP. In other words, only when using the
rule (1) to produce an item, the decoding algo-
rithm can update PCFG probabilities because the
original tree can be restored from the special node
T now. Figure 3 shows how to reconstruct the
original tree from virtual rules. We first construct
the tree on the left by substituting the trees of the
rules (1), (3), and (4) and then restore the origi-
nal tree on the right via T. Now, we can calculate
the PCFG probability of the original tree. 6 In
practice, we pre-compute this PCFG probability
and store it in the rule (1) to reduce computational
overhead.
2.2.2 Lexicalized PCFG
Although widely used in natural language pro-
cessing, PCFGs are often criticized for the lack of
lexicalization, which is very important to capture
the lexical dependencies between words. There-
fore, we use Collins? Model 1 (Collins, 2003), a
simple and effective lexicalized parsing model, as
the second parsing feature in our decoder.
Following Collins (2003), we first lexicalize a
tree by associating a headword h with each non-
terminal. Figure 4 gives the lexicalized tree corre-
sponding to Figure 2. The left-hand side of a rule
in a lexicalized PCFG is P (h) and the right-hand
side has the form:
Ln(ln) . . . L1(l1)H(h)R1(?1) . . . Rm(?m) (11)
where H is the head-child that inherits the
headword h from its parent P , L1 . . . Ln and
R1 . . . Rm are left and right modifiers of H , and
l1 . . . ln and ?1 . . . ?m are the corresponding head-
words. Either n or m may be zero, and n =
m = 0 for unary rules. Collins (2003) extends the
left and right sequences to include a terminating
STOP symbol. Thus, Ln+1 = Rm+1 = STOP.
6Postponing the calculation of PCFG probabilities also
leads to the ?hard-to-compare? problem mentioned in foot-
note 4 due to the presence of virtual non-terminals. We still
maintain multiple beams for natural and virtual hypotheses
(i.e., items) to alleviate this prblem.
710
?? ? ?9 ?1


?!
NR P NR VV AS NN
NPB NPB NPB
PP VPB
VP
IP
bushi yu shalong juxing le huitan
bushi
bushi
yu shalong
shalong
yu
juxing le huitan
huitan
juxing
juxing
juxing
Figure 4: The lexicalized tree corresponding to
Figure 2.
Collins (2003) breaks down the generation of
the right-hand side of a rule into a sequence of
smaller steps. The probability of a rule is decom-
posed as:
Ph(H|P (h)) ??
i=1...n+1
Pl(Li(li)|P (h),H, t,?) ?
?
j=1...m+1
Pr(Rj(?j)|P (h),H, t,?) (12)
where t is the POS tag of of the headword h and ?
is the distance between words that captures head-
modifier relationship.
For example, the probability of the lexicalized
rule IP(juxing) ? NPB(bushi) VP(juxing) can
be computed as 7
Ph(VP|IP, juxing)?
Pl(NPB(bushi)|IP,VP, juxing)?
Pl(STOP|IP,VP, juxing)?
Pr(STOP|IP,VP, juxing) (13)
We still use the deductive system to explain
how to integrate the lexicalized PCFG into the de-
coding process. Now, Eq. (8) can be rewritten as:
(PPyu1,3) : (w1, e1) (VPB
juxing
3,6 ) : (w2, e2)
(VPjuxing1,6 ) : (w, e2e1)
(14)
where yu and juxing are the headwords attached
to PP1,3, VPB3,6, and VP1,6. The resulting item
7For simplicity, we omit POS tag and distance in the pre-
sentation. In practice, we implemented the Collins? Model 1
exactly as described in (Collins, 2003).
score is given by
w = w1 + w2 + logPh(VPB|VP, juxing) +
logPl(PP(yu)|VP,VPB, juxing) +
logPl(STOP|VP,VPB, juxing) +
logPr(STOP|VP,VPB, juxing) (15)
Unfortunately, the lexicalized PCFG probabili-
ties of most natural rules cannot be pre-computed
because the headword of a non-terminal must be
determined on the fly during decoding. Consider
the third rule in Table 1
PP(P(yu) x1:NPB) ? with x1
It is impossible to know what the headword of
NPB is in advance, which depends on the ac-
tual sentence being translated. However, we could
safely say that the headword attached to PP is al-
ways yu because PP should have the same head-
word with its child P.
Similar to the PCFG scenario, calculating lex-
icalized PCFG for virtual rules is different from
natural rules. Consider the rule (4) in Section 2.1,
the corresponding deductive step is
(PPyu1,3) : (w1, e1) (VPB
juxing
3,6 ) : (w2, e2)
(PP-VPB?1,6) : (w, e2e1)
(16)
where ??? denotes that the headword of
PP-VPB1,6 is undefined.
We still need to postpone the calculation of lex-
icalized PCFG probabilities until reaching a nat-
ural non-terminal such as IP. In other words,
only when using the rule (1) to produce an item,
the decoding algorithm can update the lexicalized
PCFG probabilities. After restoring the original
tree from T, we need to visit backwards to fron-
tier nodes of the tree to find headwords and calcu-
late lexicalized PCFG probabilities. More specifi-
cally, updating lexicalized PCFG probabilities for
the rule the rule (1) involves the following steps:
1. Reconstruct the original tree from the rules
(1), (3), and (4) as shown in Figure 3;
2. Attach headwords to all nodes;
3. Calculate the lexicalized PCFG probabilities
according to Eq. (12).
711
Back-off Pl(Li(li)| . . . )
level Ph(H| . . . ) Pr(Rj(?j)| . . . )
1 P , h, t P , H , h, t, ?
2 P , t P , H , t, ?
3 P P , H , ?
Table 2: The conditioning variables for each level
of back-off.
As suggested by Collins (2003), we use back-
off smoothing for sub-model probabilities during
decoding. Table 2 shows the various levels of
back-off for each type of parameter in the lexi-
calized parsing model we use. For example, Ph
estimation p interpolates maximum-likelihood es-
timates p1 = Ph(H|P, h, t), p2 = Ph(H|P, t),
and p3 = Ph(H|P ) as follows:
p1 = ?1p1 + (1? ?1)(?2p2 + (1? ?2)p3) (17)
where ?1, ?2, and ?3 are smoothing parameters.
3 Experiments
In this section, we try to answer two questions:
1. Does tree-based translation by parsing out-
perform the conventional tree parsing algo-
rithm? (Section 3.1)
2. How about the parsing performance of the
joint decoder? (Section 3.2)
3.1 Translation Evaluation
We used a bilingual corpus consisting of 251K
sentences with 7.3M Chinese words and 9.2M En-
glish words to extract tree-to-string rules. The
Chinese sentences in the bilingual corpus were
parsed by an in-house parser (Xiong et al, 2005),
which obtains an F1 score of 84.4% on the Penn
Chinese Treebank. After running GIZA++ (Och
and Ney, 2003) to obtain word alignments, we
used the GHKM algorithm (Galley et al, 2004)
and extracted 11.4M tree-to-string rules from the
source-side parsed, word-aligned bilingual cor-
pus. Note that the bilingual corpus does not con-
tain the bilingual version of Penn Chinese Tree-
bank. In other words, all tree-to-string rules were
learned from noisy parse trees and alignments. We
used the SRILM toolkit (Stolcke, 2002) to train a
4-gram language model on the Xinhua portion of
the GIGAWORD corpus, which contains 238M
English words. We trained PCFG and Collins?
Model 1 on the Penn Chinese Treebank.
We used the 2002 NIST MT Chinese-English
test set as the development set and the 2005 NIST
test set as the test set. Following Huang (2008),
we modified our in-house parser to produce and
prune packed forests on the development and test
sets. There are about 105M parse trees encoded
in a forest of a sentence on average. We also ex-
tracted 1-best trees from the forests.
As the development and test sets have many
long sentences (? 100 words) that make our de-
coder prohibitively slow, we divided long sen-
tences into short sub-sentences simply based on
punctuation marks such as comma and period.
The source trees and target translations of sub-
sentences were concatenated to form the tree and
translation of the original sentence.
We compared our parsing-based decoder with
the tree-to-string translation systems based on the
tree parsing algorithm, which match rules against
either 1-best trees (Liu et al, 2006; Huang et al,
2006) or packed forests (Mi et al, 2008). All the
three systems used the same rule set containing
11.4M tree-to-string rules. Given the 1-best trees
of the test set, there are 1.2M tree-to-string rules
that match fragments of the 1-best trees. For the
forest-based system (Mi et al, 2008), the num-
ber of filtered rules increases to 1.9M after replac-
ing 1-best trees with packed forests, which con-
tain 105M trees on average. As our decoder takes
a string as input, 7.7M tree-to-string rules can be
used to parse and translate the test set. We bi-
narized 99.6% of tree-to-string rules into 16.2M
SCFG rules and discarded non-binarizable rules.
As a result, the search space of our decoder is
much larger than those of the tree parsing coun-
terparts.
Table 3 shows the results. All the three sys-
tems used the conventional translation features
such as relative frequencies, lexical weights, rule
count, n-gram language model, and word count.
Without any parsing models, the tree-based sys-
tem achieves a BLEU score of 29.8. The forest-
based system outperforms the tree-based system
by +1.8 BLEU points. Note that each hyperedge
712
Algorithm Input Parsing model # of rules BLEU (%) Time (s)
tree - 1.2M 29.8 0.56tree parsing forest PCFG 1.9M 31.6 9.49
- 32.0 51.41
PCFG 32.4 55.52parsing string
Lex
7.7M 32.6 89.35
PCFG + Lex 32.7 91.72
Table 3: Comparison of tree parsing and parsing for tree-to-string translation in terms of case-insensitive
BLEU score and average decoding time (second per sentence). The column ?parsing model? indicates
which parsing models were used in decoding. We use ?-? to denote using only translation features.
?Lex? represents the Collins? Model 1. We excluded the extra parsing time for producing 1-best trees
and packed forests.
Forest size Exact match (%) Precision (%)
1 0.55 41.5
390 0.74 47.7
5.8M 0.92 54.1
66M 1.48 62.0
105M 2.22 65.9
Table 4: Comparison of 1-best trees produced by
our decoder and the parse forests produced by the
monolingual Chinese parser. Forest size repre-
sents the average number of trees stored in a for-
est.
in a parse forest is assigned a PCFG probabil-
ity. Therefore, the forest-based system actually in-
cludes PCFG as a feature (Mi et al, 2008). With-
out incorporating any parsing models as features,
our joint decoder achieves a BLEU score of 32.0.
Adding PCFG and Collins? Model 1 (i.e., ?Lex? in
Table 2) increases translation performance. When
both PCFG and Collins? Model 1 are used, our
joint decoder outperforms the tree parsing systems
based on 1-best trees (+2.9) and packed forests
(+1.1) significantly (p < 0.01). This result is also
better than that of using only translation features
significantly (from 32.0 to 32.7, p < 0.05).
Not surprisingly, our decoder is much slower
than pattern matching on 1-best trees and packed
forests (with the same beam size). In particu-
lar, including Collins? Model 1 increases decoding
time significantly because its sub-model probabil-
ities requires back-off smoothing on the fly.
How many 1-best trees produced by our de-
coder are included in the parse forest produced by
a standard parser? We used the Chinese parser
to generate five pruned packed forests with dif-
ferent sizes (average number of trees stored in a
forest). As shown in Table 4, only 2.22% of the
trees produced by our decoder were included in
the biggest forest. One possible reason is that
we used sub-sentence division to reduce decoding
complexity. To further investigate the matching
rate, we also calculated labeled precision, which
indicates how many brackets in the parse match
those in the packed forest. The labeled precision
on the biggest forest is 65.9%, suggesting that the
1-best trees produced by our decoder are signifi-
cantly different from those in the packed forests
produced by a standard parser. 8
3.2 Parsing Evaluation
We followed Petrov and Klein (2007) to divide the
Penn Chinese Treebank (CTB) version 5 as fol-
lows: Articles 1-270 and 400-1151 as the training
set, Articles 301-325 as the development set, and
Articles 271-300 as the test set. We used max-F1
training (Och, 2003) to train the feature weights.
We did not use sub-sentence division as the sen-
tences in the test set have no more than 40 words.
8The packed forest produced by our decoder (?rule?
forest) might be different from the forest produced by a
monolingual parser (?parser? forest). While tree-based and
forest-based decoders search in the intersection of the two
forests (i.e., matched forest), our decoder directly explores
the ?rule? forest, which represents the true search space of
tree-to-string translation. This might be the key difference of
our approach from forest-based translation (Mi et al, 2008).
As sub-sentence division makes direct comparison of the two
forests quite difficult, we leave this to future work.
713
Parsing model F1 (%) Time (s)
- 62.7 23.9
PCFG 65.4 24.7
Lex 79.8 48.8
PCFG + Lex 80.6 50.4
Table 5: Effect of parsing models on parsing per-
formance (? 40 words) and average decoding
time (second per sentence). We use ?-? to denote
only using translation features.
Table 5 shows the results. Translation features
were used for all configurations. Without pars-
ing models, the F1 score is 62.7. Adding Collins?
Model 1 results in much larger gains than adding
PCFG. With all parsing models integrated, our
joint decoder achieves an F1 score of 80.6 on the
test set. Although lower than the F1 score of the
in-house parser that produces the noisy training
data, this result is still very promising because
the tree-to-string rules that construct trees in the
decoding process are learned from noisy training
data.
4 Related Work
Charniak et al (2003) firstly associate lexical-
ized parsing model with syntax-based translation.
They first run a string-to-tree decoder (Yamada
and Knight, 2001) to produce an English parse
forest and then use a lexicalized parsing model to
select the best translation from the forest. As the
parsing model operates on the target side, it actu-
ally serves as a syntax-based language model for
machine translation. Recently, Shen et al (2008)
have shown that dependency language model is
beneficial for capturing long-distance relations
between target words. As our approach adds pars-
ing models to the source side where the source
sentence is fixed during decoding, our decoder
does parse the source sentence like a monolingual
parser instead of a syntax-based language model.
More importantly, we integrate translation models
and parsing models in a discriminative framework
where they can interact with each other directly.
Our work also has connections to joint parsing
(Smith and Smith, 2004; Burkett and Klein, 2008)
and bilingually-constrained monolingual parsing
(Huang et al, 2009a) because we use another
language to resolve ambiguity for one language.
However, while both joint parsing and bilingually-
constrained monolingual parsing rely on the target
sentence, our approach only takes a source sen-
tence as input.
Blunsom and Osborne (2008) incorporate the
source-side parse trees into their probabilistic
SCFG framework and treat every source-parse
PCFG rule as an individual feature. The differ-
ence is that they parse the test set before decoding
so as to exploit the source syntactic information to
guide translation.
More recently, Chiang (2010) has shown
that (?exact?) tree-to-tree translation as pars-
ing achieves comparable performance with Hiero
(Chiang, 2007) using much fewer rules. Xiao et
al. (2010) integrate tokenization and translation
into a single step and improve the performance of
tokenization and translation significantly.
5 Conclusion
We have presented a framework for joint parsing
and translation by casting tree-to-string transla-
tion as a parsing problem. While tree-to-string
rules construct parse trees on the source side
and translations on the target side simultaneously,
parsing models can be integrated to improve both
translation and parsing quality.
This work can be considered as a final step to-
wards the continuum of tree-to-string translation:
from single tree to forest and finally to the inte-
gration of parsing and translation. In the future,
we plan to develop more efficient decoding al-
gorithms, analyze forest matching systematically,
and use more sophisticated parsing models.
Acknowledgement
The authors were supported by National Nat-
ural Science Foundation of China, Contracts
60736014 and 60903138, and 863 State Key
Project No. 2006AA010108. We are grateful to
the anonymous reviewers for their insightful com-
ments. We thank Liang Huang, Hao Zhang, and
Tong Xiao for discussions on synchronous bina-
rization, Haitao Mi and Hao Xiong for provid-
ing and running the baseline systems, and Wenbin
Jiang for helping us train parsing models.
714
References
Blunsom, Phil and Miles Osborne. 2008. Probabilis-
tic inference for machine translation. In Proc. of
EMNLP 2008.
Burkett, David and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Proc.
of EMNLP 2008.
Charniak, Eugene, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proc. of MT Summit IX.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chiang, David. 2010. Learning to translate with
source and target syntax. In Proc. of ACL 2010.
Collins, Michael. 2003. Head-driven statistical mod-
els for natural language parsing. Computational
Linguistics, 29(4):589?637.
Eisner, Jason. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proc. of ACL
2003.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of NAACL 2004.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL 2006.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009a.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proc. of EMNLP 2009.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009b. Binarization of synchronous
context-free grammars. Computational Linguistics,
35(4):559?595.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL
2008.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL 2006.
Liu, Yang, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
of ACL 2009.
Melamed, I. Dan. 2004. Statistical machine transla-
tion by parsing. In Proc. of ACL 2004.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Och, Franz J. and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL 2003.
Petrov, Slav and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of NAACL 2007.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proc. of EMNLP
2006.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proc. of ACL 2005.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. of ACL 2008.
Smith, David and Noah Smith. 2004. Bilingual pars-
ing with factored estimation: using english to parse
korean. In Proc. of EMNLP 2004.
Stolcke, Andreas. 2002. Srilm - an extension language
model modeling toolkit. In Proc. of ICSLP 2002.
Xiao, Xinyan, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Joint tokenization
and translation. In Proc. of COLING 2010.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proc. of IJCNLP 2005.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of ACL
2001.
Zhang, Hao, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translatio. In Proc. of NAACL 2007.
Zhang, Min, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree
sequence alignment-based tree-to-tree translation
model. In Proc. of ACL 2008.
Zhao, Bing and Yaser Al-Onaizan. 2008. General-
izing local and non-local word-reordering patterns
for syntax-based machine translation. In Proc. of
EMNLP 2008.
715
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,
Beijing, August 2010
Joint Tokenization and Translation
Xinyan Xiao ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
As tokenization is usually ambiguous for
many natural languages such as Chinese
and Korean, tokenization errors might po-
tentially introduce translation mistakes for
translation systems that rely on 1-best to-
kenizations. While using lattices to of-
fer more alternatives to translation sys-
tems have elegantly alleviated this prob-
lem, we take a further step to tokenize
and translate jointly. Taking a sequence
of atomic units that can be combined to
form words in different ways as input, our
joint decoder produces a tokenization on
the source side and a translation on the
target side simultaneously. By integrat-
ing tokenization and translation features
in a discriminative framework, our joint
decoder outperforms the baseline trans-
lation systems using 1-best tokenizations
and lattices significantly on both Chinese-
English and Korean-Chinese tasks. In-
terestingly, as a tokenizer, our joint de-
coder achieves significant improvements
over monolingual Chinese tokenizers.
1 Introduction
Tokenization plays an important role in statistical
machine translation (SMT) because tokenizing a
source-language sentence is always the first step
in SMT systems. Based on the type of input, Mi
and Huang (2008) distinguish between two cat-
egories of SMT systems : string-based systems
(Koehn et al, 2003; Chiang, 2007; Galley et al,
source
target
tokenize+translate
string tokenization
translation
source
target
string
tokenize
tokenization
translate
translation
(a)
(b)
Figure 1: (a) Separate tokenization and translation and (b)
joint tokenization and translation.
2006; Shen et al, 2008) that take a string as input
and tree-based systems (Liu et al, 2006; Mi et al,
2008) that take a tree as input. Note that a tree-
based system still needs to first tokenize the input
sentence and then obtain a parse tree or forest of
the sentence. As shown in Figure 1(a), we refer to
this pipeline as separate tokenization and transla-
tion because they are divided into single steps.
As tokenization for many languages is usually
ambiguous, SMT systems that separate tokeniza-
tion and translation suffer from a major drawback:
tokenization errors potentially introduce transla-
tion mistakes. As some languages such as Chi-
nese have no spaces in their writing systems, how
to segment sentences into appropriate words has
a direct impact on translation performance (Xu et
al., 2005; Chang et al, 2008; Zhang et al, 2008).
In addition, although agglutinative languages such
as Korean incorporate spaces between ?words?,
which consist of multiple morphemes, the gran-
ularity is too coarse and makes the training data
1200
considerably sparse. Studies reveal that seg-
menting ?words? into morphemes effectively im-
proves translating morphologically rich languages
(Oflazer, 2008). More importantly, a tokenization
close to a gold standard does not necessarily leads
to better translation quality (Chang et al, 2008;
Zhang et al, 2008). Therefore, it is necessary
to offer more tokenizations to SMT systems to
alleviate the tokenization error propagation prob-
lem. Recently, many researchers have shown that
replacing 1-best tokenizations with lattices im-
proves translation performance significantly (Xu
et al, 2005; Dyer et al, 2008; Dyer, 2009).
We take a next step towards the direction of
offering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), our approach tokenizes
and translates jointly to find a tokenization and
a translation for a source-language string simul-
taneously. We integrate translation and tokeniza-
tion models into a discriminative framework (Och
and Ney, 2002), within which tokenization and
translation models interact with each other. Ex-
periments show that joint tokenization and trans-
lation outperforms its separate counterparts (1-
best tokenizations and lattices) significantly on
the NIST 2004 and 2005 Chinese-English test
sets. Our joint decoder also reports positive results
on Korean-Chinese translation. As a tokenizer,
our joint decoder achieves significantly better to-
kenization accuracy than three monolingual Chi-
nese tokenizers.
2 Separate Tokenization and Translation
Tokenization is to split a string of characters into
meaningful elements, which are often referred to
as words. Typically, machine translation sepa-
rates tokenization from decoding as a preprocess-
ing step. An input string is first preprocessed by a
tokenizer, and then is translated based on the tok-
enized result. Take the SCFG-based model (Chi-
ang, 2007) as an example. Given the character
sequence of Figure 2(a), a tokenizer first splits it
into the word sequence as shown in Figure 2(b),
then the decoder translates the word sequence us-
ing the rules in Table 1.
This approach makes the translation process
simple and efficient. However, it may not be
? ? ? ? ? ? 0? 1 2 3 4 5 6 7
Figure 2: Chinese tokenization: (a) character sequence; (b)
and (c) tokenization instances; (d) lattice created from (b)
and (c). We insert ?-? between characters in a word just for
clarity.
r1 tao-fei-ke ?Taufik
r2 duo fen ? gain a point
r3 x1 you-wang x2 ? x1 will have the chance to x2
Table 1: An SCFG derivation given the tokenization of Fig-
ure 2(b).
optimal for machine translation. Firstly, optimal
granularity is unclear for machine translation. We
might face severe data sparseness problem by us-
ing large granularity, while losing much useful in-
formation with small one. Consider the example
in Figure 2. It is reasonable to split duo fen into
two words as duo and fen, since they have one-
to-one alignments to the target side. Nevertheless,
while you and wang also have one-to-one align-
ments, it is risky to segment them into two words.
Because the decoder is prone to translate wang as
a verb look without the context you. Secondly,
there may be tokenization errors. In Figure2(c),
tao fei ke is recognized as a Chinese person name
with the second name tao and the first name fei-ke,
but the whole string tao fei ke should be a name of
the Indonesian badminton player.
Therefore, it is necessary to offer more tok-
enizations to SMT systems to alleviate the tok-
enization error propagation problem. Recently,
many researchers have shown that replacing 1-
best tokenizations with lattices improves transla-
tion performance significantly. In this approach, a
lattice compactly encodes many tokenizations and
is fixed before decoding.
1201
0 1 2 3 4 5 6 7
1 2
3
Figure 3: A derivation of the joint model for the tokenization
in Figure 2(b) and the translation in Figure 2 by using the
rules in Table 1. N means tokenization while  represents
translation.
3 Joint Tokenization and Translation
3.1 Model
We take a next step towards the direction of of-
fering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), the decoder takes an un-
tokenized string as input, and then tokenizes the
source side string while building the correspond-
ing translation of the target side. Since the tradi-
tional rules like those in Table 1 natively include
tokenization information, we can directly apply
them for simultaneous construction of tokeniza-
tion and translation by the source side and target
side of rules respectively. In Figure 3, our joint
model takes the character sequence in Figure 2(a)
as input, and synchronously conducts both trans-
lation and tokenization using the rules in Table 1.
As our model conducts tokenization during de-
coding, we can integrate tokenization models as
features together with translation features under
the discriminative framework. We expect tok-
enization and translation could collaborate with
each other. Tokenization offers translation with
good tokenized results, while translation helps to-
kenization to eliminate ambiguity. Formally, the
probability of a derivation D is represented as
P (D) ?
?
i
?i(D)?i (1)
where ?i are features defined on derivations in-
cluding translation and tokenization, and ?i are
feature weights. We totally use 16 features:
? 8 traditional translation features (Chiang,
2007): 4 rule scores (direct and reverse trans-
lation scores; direct and reverse lexical trans-
lation scores); language model of the target
side; 3 penalties for word count, extracted
rule and glue rule.
? 8 tokenization features: maximum entropy
model, language model and word count of
the source side (Section 3.2). To handle
the Out Of Vocabulary (OOV) problem (Sec-
tion 3.3), we also introduce 5 OOV features:
OOV character count and 4 OOV discount
features.
Since our model is still a string-based model, the
CKY algorithm and cube pruning are still applica-
ble for our model to find the derivation with max
score.
3.2 Adding Tokenization Features
Maximum Entropy model (ME). We first intro-
duce ME model feature for tokenization by cast-
ing it as a labeling problem (Xue and Shen, 2003;
Ng and Low, 2004). We label a character with the
following 4 types:
? b: the begin of a word
? m: the middle of a word
? e: the end of a word
? s: a single-character word
Taking the tokenization you-wang of the string
you wang for example, we first create a label se-
quence b e for the tokenization you-wang and then
calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
? P (e | wang, you wang)
Given a tokenization wL1 with L words for a
character sequence cn1 , we firstly create labels ln1
for every characters and then calculate the proba-
bility by
P (wL1 |cn1 ) = P (ln1 |cn1 ) =
n?
i=1
P (li|ci, cn1 ) (2)
1202
Under the ME framework, the probability of as-
signing the character c with the label l is repre-
sented as:
P (l|c, cn1 ) =
exp[?i ?ihi(l, c, cn1 )]?
l? exp[
?
i ?ihi(l?, c, cn1 )]
(3)
where hi is feature function, ?i is the feature
weight of hi. We use the feature templates the
same as Jiang et al, (2008) to extract features for
ME model. Since we directly construct tokeniza-
tion when decoding, it is straight to calculate the
ME model score of a tokenization according to
formula (2) and (3).
Language Model (LM). We also use the n-
gram language model to calculate the probability
of a tokenization wL1 :
P (wL1 ) =
L?
i=1
P (wi|wi?1i?n+1) (4)
For instance, we compute the probability of the
tokenization shown in Figure 2(b) under a 3-gram
model by
P (tao-fei-ke)
?P (you-wang | tao-fei-ke)
?P (duo | tao-fei-ke, you-wang)
?P (fen | you-wang, duo)
Word Count (WC). This feature counts the
number of words in a tokenization. Language
model is prone to assign higher probabilities to
short sentences in a biased way. This feature can
compensate this bias by encouraging long sen-
tences. Furthermore, using this feature, we can
optimize the granularity of tokenization for trans-
lation. If larger granularity is preferable for trans-
lation, then we can use this feature to punish the
tokenization containing more words.
3.3 Considering All Tokenizations
Obviously, we can construct the potential tok-
enizations and translations by only using the ex-
tracted rules, in line with traditional translation
decoding. However, it may limits the potential to-
kenization space. Consider a string you wang. If
you-wang is not reachable by the extracted rules,
the tokenization you-wang will never be consid-
ered under this way. However, the decoder may
still create a derivation by splitting the string as
small as possible with tokenization you wang and
translating you with a and wang with look, which
may hurt the translation performance. This case
happens frequently for named entity especially.
Overall, it is necessary to assure that the de-
coder can derive all potential tokenizations (Sec-
tion 4.1.3).
To assure that, when a span is not tokenized into
a single word by the extracted rules, we will add
an operation, which is considering the entire span
as an OOV. That is, we tokenize the entire span
into a single word with a translation that is the
copy of source side. We can define the set of all
potential tokenizations ?(cn1 ) for the character se-
quence cn1 in a recursive way by
?(cn1 ) =
n?1?
i
{?(ci1)
?
{w(cni+1)}} (5)
here w(cni+1) means a word contains characters
cni+1 and
?
means the times of two sets. Ac-
cording to this recursive definition, it is easy to
prove that all tokenizations is reachable by using
the glue rule (S ? SX,SX) and the added op-
eration. Here, glue rule is used to concatenate the
translation and tokenization of the two variables S
and X, which acts the role of the operator ? in
equation (5).
Consequently, this introduces a large number
of OOVs. In order to control the generation of
OOVs, we introduce the following OOV features:
OOV Character Count (OCC). This feature
counts the number of characters covered by OOV.
We can control the number of OOV characters by
this feature. It counts 3 when tao-fei-ke is an OOV,
since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs
vary for words with different counts of characters.
We can directly attack this problem by adding
features ODi that reward or punish OOV words
which contains with i characters, or ODi,j for
OOVs contains with i to j characters. 4 OD fea-
tures are used in this paper: 1, 2, 3 and 4+. For
example, OD3 counts 1 when the word tao-fei-ke
is an OOV.
1203
Method Train #Rule Test TFs MT04 MT05 Speed
Separate
ICT 151M ICT ? 34.82 33.06 2.48
SF 148M SF ? 35.29 33.22 2.55
ME 141M ME ? 33.71 30.91 2.34
All 219M Lattice ? 35.79 33.95 3.83? 35.85 33.76 6.79
Joint
ICT 151M
Character
?
36.92 34.69 17.66
SF 148M 37.02 34.56 17.37
ME 141M 36.78 34.17 17.23
All 219M 37.25** 34.88** 17.52
Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train
and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8
tokenization features is used (?) or not (?). ICT, SF and ME are segmenter names for preprocessing. All means combined
corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al, (2008). ** means
significantly (Koehn, 2004) better than Lattice (p < 0.01).
4 Experiments
In this section, we try to answer the following
questions:
1. Does the joint method outperform conven-
tional methods that separate tokenization
from decoding. (Section 4.1)
2. How about the tokenization performance of
the joint decoder? (Section 4.2)
4.1 Translation Evaluation
We use the SCFG model (Chiang, 2007) for our
experiments. We firstly work on the Chinese-
English translation task. The bilingual training
data contains 1.5M sentence pairs coming from
LDC data.1 The monolingual data for training
English language model includes Xinhua portion
of the GIGAWORD corpus, which contains 238M
English words. We use the NIST evaluation sets
of 2002 (MT02) as our development data set, and
sets of 2004(MT04) and 2005(MT05) as test sets.
We use the corpus derived from the People?s Daily
(Renmin Ribao) in Feb. to Jun. 1998 containing
6M words for training LM and ME tokenization
models.
Translation Part. We used GIZA++ (Och and
Ney, 2003) to perform word alignment in both di-
rections, and grow-diag-final-and (Koehn et al,
2003) to generate symmetric word alignment. We
extracted the SCFG rules as describing in Chiang
(2007). The language model were trained by the
1including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06
SRILM toolkit (Stolcke, 2002).2 Case insensitive
NIST BLEU (Papineni et al, 2002) was used to
measure translation performance.
Tokenization Part. We used the toolkit imple-
mented by Zhang (2004) to train the ME model.
Three Chinese word segmenters were used for
comparing: ICTCLAS (ICT) developed by insti-
tute of Computing Technology Chinese Academy
of Sciences (Zhang et al, 2003); SF developed at
Stanford University (Huihsin et al, 2005) and ME
which exploits the ME model described in section
(3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and trans-
lation with the conventional separate methods.
The input of separate tokenization and translation
can either be a single segmentation or a lattice.
The lattice combines the 1-best segmentations of
segmenters. Same as Dyer et al, (2008), we also
extracted rules from a combined bilingual corpus
which contains three copies from different seg-
menters. We refer to this version of rules as All.
Table 2 shows the result.3 Using all rule ta-
ble, our joint method significantly outperforms the
best single system SF by +1.96 and +1.66 points
on MT04 and MT05 respectively, and also out-
performs the lattice-based system by +1.46 and
+0.93 points. However, the 8 tokenization fea-
tures have small impact on the lattice system,
probably because the tokenization space limited
2The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself.
3The weights are retrained for different test conditions, so
do the experiments in other sections.
1204
ME LM WC OCC OD MT05
? ? ? ? ? 24.97? ? ? ? ? 25.30
? ? ? ? ? 24.70
? ? ? ? ? 24.84
? ? ? ? ? 25.51
? ? ? ? ? 25.34
? ? ? ? ? 25.74? ? ? ? ?
26.37
Table 3: Effect of tokenization features on Chinese-English
translation task. ?
?
? denotes using a tokenization feature
while ??? denotes that it is inactive.
by lattice has been created from good tokeniza-
tion. Not surprisingly, our decoding method is
about 2.6 times slower than lattice method with
tokenization features, since the joint decoder takes
character sequences as input, which is about 1.7
times longer than the corresponding word se-
quences tokenized by segmenters. (Section 4.1.4).
The number of extracted rules with different
segment methods are quite close, while the All
version contains about 45% more rules than the
single systems. With the same rule table, our joint
method improves the performance over separate
method up to +3.03 and +3.26 points (ME). In-
terestingly, comparing with the separate method,
the tokenization of training data has smaller effect
on joint method. The BLEU scores of MT04 and
MT05 fluctuate about 0.5 and 0.7 points when ap-
plying the joint method, while the difference of
separate method is up to 2 and 3 points respec-
tively. It shows that the joint method is more ro-
bust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization
features on translation. In order to reduce the time
for tuning weights and decoding, we extracted
rules from the FBIS part of the bilingual corpus,
and trained a 4-gram English language model on
the English side of FBIS.
Table 3 shows the result. Only using the 8 trans-
lation features, our system achieves a BLEU score
of 24.97. By activating all tokenization features,
the joint decoder obtains an absolute improve-
ment by 1.4 BLEU points. When only adding
one single tokenization feature, the LM and WC
fail to show improvement, which may result from
their bias to short or long tokenizations. How-
Method BLEU #Word Grau #OOV
ICT 33.06 30,602 1.65 644
SF 33.22 30,119 1.68 882
ME 30.91 29,717 1.70 1,614
Lattice 33.95 30,315 1.66 494
JointICT 34.69 29,723 1.70 996
JointSF 34.56 29,839 1.69 972
JointME 34.17 29,771 1.70 1,062
JointAll 34.88 29,644 1.70 883
Table 4: Granularity (Grau, counts of character per word)
and counts of OOV words of different methods on MT05.
The subscript of joint means the type of rule table.
ever, these two features have complementary ad-
vantages and collaborate well when using them to-
gether (line 8). The OCC and OD features also
contribute improvements which reflects the fact
that handling the generation of OOV is important
for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering
all potential tokenizations, we compare the perfor-
mances of whether to tokenize a span as a single
word or not as illustrated in section 3.3. When
only tokenizing by the extracted rules, we obtain
34.37 BLEU on MT05, which is about 0.5 points
lower than considering all tokenizations shown in
Table 2. This indicates that spuriously limitation
of the tokenization space may degenerate transla-
tion performance.
4.1.4 Results Analysis
To better understand why the joint method can
improve the translation quality, this section shows
some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word
counts of different configurations. The lattice
method reduces the OOV words quite a lot which
is 23% and 70% comparing with ICT and ME. In
contrast, the joint method gain an absolute im-
provement even thought the OOV count do not
decrease. It seems the lattice method prefers to
translate more characters (since smaller granular-
ity and less OOVs), while our method is inclined
to maintain integrity of words (since larger granu-
larity and more OOVs). This also explains the dif-
ficulty of deciding optimal tokenization for trans-
lation before decoding.
There are some named entities or idioms that
1205
Method Type F1 Time
Monolingual
ICT 97.47 0.010
SF 97.48 0.007
ME 95.53 0.008
Joint
ICT 97.68 9.382
SF 97.68 10.454
ME 97.60 10.451
All 97.70 9.248
Table 5: Comparison of segmentation performance in terms
of F1 score and speed (second per sentence). Type column
means the segmenter for monolingual method, while repre-
sents the rule tables used by joint method.
are split into smaller granularity by the seg-
menters. For example:???? which is an English
name ?Stone? or ??-g -u? which means
?teenage?. Although the separate method is possi-
ble to translate them using smaller granularity, the
translation results are in fact wrong. In contrast,
the joint method tokenizes them as entire OOV
words, however, it may result a better translation
for the whole sentence.
We also count the overlap of the segments
used by the JointAll system towards the single
segmentation systems. The tokenization result
of JointAll contains 29, 644 words, and shares
28, 159 , 27, 772 and 27, 407 words with ICT ,
SF and ME respectively. And 46 unique words
appear only in the joint method, where most of
them are named entity.
4.2 Chinese Word Segmentation Evaluation
We also test the tokenization performance of our
model on Chinese word segmentation task. We
randomly selected 3k sentences from the corpus
of People?s Daily in Jan. 1998. 1k sentences
were used for tuning weights, while the other 2k
sentences were for testing. We use MERT (Och,
2003) to tune the weights by minimizing the error
measured by F1 score.
As shown in Table 5, with all features activated,
our joint decoder achieves an F1 score of 97.70
which reduces the tokenization error comparing
with the best single segmenter ICT by 8.7%. Sim-
ilar to the translation performance evaluation, our
joint decoder outperforms the best segmenter with
any version of rule tables.
Feature F1
TFs 97.37
TFs + RS 97.65
TFs + LM 97.67
TFs + RS + LM 97.62
All 97.70
Table 6: Effect of the target side information on Chinese
word segmentation. TFs stands for the 8 tokenization fea-
tures. All represents all the 16 features.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores
(RS), target side Language Model (LM) on tok-
enization. Table 6 shows the effect on Chinese
word segmentation. When only use tokenization
features, our joint decoder achieves an F1 score
of 97.37. Only integrating language model or rule
scores, the joint decoder achieves an absolute im-
provement of 0.3 point in F1 score, which reduces
the error rate by 11.4%. However, when combin-
ing them together, the F1 score deduces slightly,
which may result from the weight tuning. Us-
ing all feature, the performance comes to 97.70.
Overall, our experiment shows that the target side
information can improve the source side tokeniza-
tion under a supervised way, and outperform state-
of-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al, 2008; Chang et
al., 2008) have shown that preprocessing the in-
put string for decoder by better segmenters do
not always improve the translation quality, we re-
verify this by testing whether the joint decoder
produces good tokenization and good translation
at the same time. To answer the question, we
used the feature weights optimized by maximiz-
ing BLEU for tokenization and used the weights
optimized by maximizing F1 for translation. We
test BLEU on MT05 and F1 score on the test data
used in segmentation evaluation experiments. By
tuning weights regarding to BLEU (the configura-
tion for JointAll in table 2), our decoder achieves
a BLEU score of 34.88 and an F1 score of 92.49.
Similarly, maximizing F1 (the configuration for
the last line in table 6) leads to a much lower
BLEU of 27.43, although the F1 is up to 97.70.
This suggests that better tokenization may not al-
ways lead to better translations and vice versa
1206
Rule #Rule Method Test Time
Morph 46M Separate 21.61 4.12Refined 55M 21.21 4.63
All 74M Joint 21.93* 5.10
Table 7: Comparison of Separate and Joint method in terms
of BLEU score and decoding speed (second per sentence) on
Korean-Chinese translation task.
even by the joint decoding. This also indicates the
hard of artificially defining the best tokenization
for translation.
4.3 Korean-Chinese Translation
We also test our model on a quite different task:
Korean-Chinese. Korean is an agglutinative lan-
guage, which comes from different language fam-
ily comparing with Chinese.
We used a newswire corpus containing 256k
sentence pairs as training data. The development
and test data set contain 1K sentence each with
one single reference. We used the target side of
training set for language model training. The Ko-
rean part of these data were tokenized into mor-
pheme sequence as atomic unit for our experi-
ments.
We compared three methods. First is directly
use morpheme sequence (Morph). The second
one is refined data (Refined), where we use selec-
tive morphological segmentation (Oflazer, 2008)
for combining morpheme together on the training
data. Since the selective method needs alignment
information which is unavailable in the decod-
ing, the test data is still of morpheme sequence.
These two methods still used traditional decoding
method. The third one extracting rules from com-
bined (All) data of methods 1 and 2, and using
joint decoder to exploit the different granularity
of rules.
Table 7 shows the result. Since there is no gold
standard data for tokenization, we do not use ME
and LM tokenization features here. However, our
joint method can still significantly (p < 0.05) im-
prove the performance by about +0.3 points. This
also reflects the importance of optimizing granu-
larity for morphological complex languages.
5 Related Work
Methods have been proposed to optimize tok-
enization for word alignment. For example, word
alignment can be simplified by packing (Ma et al,
2007) several consecutive words together. Word
alignment and tokenization can also be optimized
by maximizing the likelihood of bilingual corpus
(Chung and Gildea, 2009; Xu et al, 2008). In fact,
these work are orthogonal to our joint method,
since they focus on training step while we are con-
cerned of decoding. We believe we can further
the performance by combining these two kinds of
work.
Our work also has connections to multilingual
tokenization (Snyder and Barzilay, 2008). While
they have verified that tokenization can be im-
proved by multilingual learning, our work shows
that we can also improve tokenization by collabo-
rating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows
the effect of joint method. They integrate parsing
and translation into a single step and improve the
performance of translation significantly.
6 Conclusion
We have presented a novel method for joint tok-
enization and translation which directly combines
the tokenization model into the decoding phase.
Allowing tokenization and translation to collab-
orate with each other, tokenization can be opti-
mized for translation, while translation also makes
contribution to tokenization performance under a
supervised way. We believe that our approach can
be applied to other string-based model such as
phrase-based model (Koehn et al, 2003), string-
to-tree model (Galley et al, 2006) and string-to-
dependency model (Shen et al, 2008).
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang, Zhiyang Wang and Zongcheng Ji for
their helpful feedback.
1207
References
Chang, Pi-Chuan, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In the
Third Workshop on SMT.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chung, Tagyoung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP 2009.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proc. ACL 2008.
Dyer, Chris. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc.
NAACL 2009.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL 2006.
Huihsin, Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In
Fourth SIGHAN Workshop.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proc. ACL 2008.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Liu, Yang and Qun Liu. 2010. Joint parsing and trans-
lation. In Proc. Coling 2010.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL 2006.
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proc. ACL 2007.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proc. EMNLP
2004.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. ACL
2003.
Oflazer, Kemal. 2008. Statistical machine translation
into a morphologically complex language. In Proc.
CICL 2008.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
ACL 2002.
Shen, Libin, Xu Jinxi, and Weischedel Ralph. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL 2008.
Snyder, Benjamin and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proc. ACL 2008.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit.
Xu, Jia, Evgeny Matusov, Richard Zens, and Her-
mann Ney. 2005. Integrated chinese word segmen-
tation in statistical machine translation. In Proc.
IWSLT2005.
Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
chinese word segmentation for statistical machine
translation. In Proc. Coling 2008.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In SIGHAN Work-
shop.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexical an-
alyzer ictclas. In the Second SIGHAN Workshop.
Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In the Third
Workshop on SMT.
Zhang, Le. 2004. Maximum entropy modeling toolkit
for python and c++.
1208
Coling 2010: Poster Volume, pages 285?293,
Beijing, August 2010
An Efficient Shift-Reduce Decoding Algorithm for Phrased-Based
Machine Translation
Yang Feng, Haitao Mi, Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{fengyang,htmi,yliu,liuqun}@ict.ac.cn
Abstract
In statistical machine translation, decod-
ing without any reordering constraint is
an NP-hard problem. Inversion Transduc-
tion Grammars (ITGs) exploit linguistic
structure and can well balance the needed
flexibility against complexity constraints.
Currently, translation models with ITG
constraints usually employs the cube-time
CYK algorithm. In this paper, we present
a shift-reduce decoding algorithm that can
generate ITG-legal translation from left to
right in linear time. This algorithm runs
in a reduce-eager style and is suited to
phrase-based models. Using the state-of-
the-art decoder Moses as the baseline, ex-
periment results show that the shift-reduce
algorithm can significantly improve both
the accuracy and the speed on different
test sets.
1 Introduction
In statistical machine translation, for the diver-
sity of natural languages, the word order of
source and target language may differ and search-
ing through all possible translations is NP-hard
(Knight, 1999). So some measures have to be
taken to reduce search space: either using a search
algorithm with pruning technique or restricting
possible reorderings.
Currently, beam search is widely used (Till-
mann and Ney, 2003; Koehn, 2004) to reduce
search space. However, the pruning technique
adopted by this algorithm is not risk-free. As a
result, the best partial translation may be ruled out
during pruning. The more aggressive the prun-
ing is, the more likely the best translation escapes.
There should be a tradeoff between the speed and
the accuracy. If some heuristic knowledge is em-
ployed to guide the search, the search algorithm
can discard some implausible hypotheses in ad-
vance and focus on more possible ones.
Inversion Transduction Grammars (ITGs) per-
mit a minimal extra degree of ordering flexibility
and are particularly well suited to modeling or-
dering shifts between languages (Wu, 1996; Wu,
1997). They can well balance the needed flex-
ibility against complexity constraints. Recently,
ITG has been successfully applied to statistical
machine translation (Zens and Ney, 2003; Zens
et al, 2004; Xiong et al, 2006). However, ITG
generally employs the expensive CYK parsing al-
gorithm which runs in cube time. In addition, the
CYK algorithm can not calculate language model
exactly in the process of decoding, as it can not
catch the full history context of the left words in a
hypothesis.
In this paper, we introduce a shift-reduce de-
coding algorithm with ITG constraints which runs
in a left-to-right manner. This algorithm parses
source words in the order of their corresponding
translations on the target side. In the meantime,
it gives all candidate ITG-legal reorderings. The
shift-reduce algorithm is different from the CYK
algorithm, in particular:
? It produces translation in a left-to-right man-
ner. As a result, language model probability
can be calculated more precisely in the light
of full history context.
? It decodes much faster. Applied with distor-
285
target side target side target side
(a) straight (b) inverted (c) discontinuous
Figure 1: Orientation of two blocks.
tion limit, shift-reduce decoding algorithm
can run in linear time, while the CYK runs
in cube time.
? It holds ITG structures generated during de-
coding. That is to say, it can directly give
ITG-legal spans, which leads to faster de-
coding. Furthermore, it can be extended to
syntax-based models.
We evaluated the performance of the shift-
reduce decoding algorithm by adding ITG con-
straints to the state-of-the-art decoder Moses. We
did experiments on three data sets: NIST MT08
data set, NIST MT05 data set and China Work-
shop on Machine Translation 2007 data set. Com-
pared to Moses, the improvements of the accuracy
are 1.59, 0.62, 0.8 BLEU score, respectively, and
the speed improvements are 15%, 24%, 30%, re-
spectively.
2 Decoding with ITG constraints
In this paper, we employ the shift-reduce algo-
rithm to add ITG constraints to phrase-based ma-
chine translation model. It is different from the
traditional shift-reduce algorithm used in natural
language parsing. On one hand, as natural lan-
guage parsing has to cope with a high degree of
ambiguity, it need take ambiguity into considera-
tion. As a result, the traditional one often suffers
shift-reduce divergence. Nonetheless, the shift-
reduce algorithm in this paper does not pay atten-
tion to ambiguity and acts in a reduce-eager man-
ner. On the other hand, the traditional algorithm
can not ensure that all reorderings observe ITG
constraints, so we have to modify the traditional
algorithm to import ITG constraints.
We will introduce the shift-reduce decoding al-
gorithm in the following two steps: First, we
1\1
zairu1
??2
shijian2
N3
diaocha3
]4
ziliaode4
>M5
diannao5
;?6
zaoqie6
The laptop with inquiry data on the event was stolen
(a)
A1
The laptop
diannao5
with
A2
zairu1
inquiry
A3
diaocha3
data
A4
ziliaode4
A5
on the event
shijian2
A6
was stolen
zaoqie6
A7
A8
A9
A10
A11
(b)
Figure 2: A Chinese-to-English sentence pair and
its corresponding ITG tree.
will deduce how to integrate the shift-reduce al-
gorithm and ITG constraints and show its correct-
ness (Section 2.1). Second, we will describe the
shift-reduce decoding algorithm in details (Sec-
tion 2.2).
2.1 Adding ITG constraints
In the process of decoding, a source phrase is re-
garded as a block and a source sentence is seen
as a sequence of blocks. The orientation of two
blocks whose translations are adjacent on the tar-
get side can be straight, inverted or discontinu-
ous, as shown in Figure 1. According to ITG,
two blocks which are straight or inverted can be
merged into a single block. For parsing, differ-
ent mergence order of a sequence of continuous
blocks may yield different derivations. In con-
trast, the phrase-based machine translation does
not compute reordering probabilities hierarchi-
cally, so the mergence order will not impact the
computation of reordering probabilities. As a
result, the shift-reduce decoding algorithm need
not take into consideration the shift-reduce diver-
gence. It merges two continuous blocks as soon
as possible, acting in a reduce-eager style.
Every ITG-legal sentence pair has a corre-
286
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry
(a) (b) (c)
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(d) (e) (f)
 
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
S zairu1 shijian2 diaocha3 ziliaode4 diannao5 zaoqie6
The laptop with inquiry data on the event
(g) (h) (i)
Figure 3: The partial translation procedure of the sentence in Figure 2.
sponding ITG tree, and source words covered
by every node (eg. A1, ..., A11 in Figure 2(b))
in the ITG tree can be seen as a block. By
watching the tree in Figure 2, we can find that
a block must be adjacent to the block either on
its left or on its right, then they can be merged
into a larger block. For example, A2 matches
the block [zairu1] and A8 matches the block
[shijian2 diaocha3 ziliaode4].1 The two blocks
are adjacent and they are merged into a larger
block [zairu1 shijian2 diaocha3 ziliaode4],
covered by A9. The procedure of translating
zairu1 shijian2 diaocha3 ziliaode4 diannao5
is illustrated in Figure 3.
For a hypothesis during decoding, we assign it
three factors: the current block, the left neigh-
boring uncovered span and the right neighbor-
ing uncovered span. For example, in Figure
3(c), the current block is [diaocha3] and the left
neighboring uncovered span is [shijian2] and the
right neighboring uncovered span is [ziliaode4].
[zaoqie6] is not thought of as the right neighbor-
ing block, for it is not adjacent to [diaocha3]. The
next covered block is [ziliaode4] (as shown in
Figure 3(d)). For [diaocha3] and [ziliaode4] are
adjacent, they are merged. In Figure 3(e), the cur-
rent block is [diaocha3 ziliaode4].
A sentence is translated with ITG constraints iff
1The words within a block are sorted by their order in the
source sentence.
its source side can be covered by an ITG tree. That
is to say, for every hypothesis during decoding, the
next block to cover must be selected from the left
or right neighboring uncovered span.
First, we show that if the next block to cover is
selected in this way, the translation must observe
ITG constraints. For every hypothesis during de-
coding, the immediate left and right words of the
current block face the following three conditions:
(1) The immediately left word is not covered
and the immediately right word is covered, then
the next block to cover must be selected from the
left neighboring uncovered span, eg. for the cur-
rent block [diaocha3 ziliaode4] in Figure 3(e). In
this condition, the ITG tree can be constructed in
the following two ways: either all words in the left
neighboring uncovered span are translated first,
then this span is merged with the current span
(taking three nodes as an example, this case is
shown in Figure 4(a)), or the right part of the left
neighboring uncovered span is merged with the
current block first, then the new block is merged
with the rest part of the left neighboring uncov-
ered span (shown in Figure 4(b)). In a word, only
after all words in the left neighboring uncovered
span are covered, other words can be covered.
(2) The immediately right word is not covered
and the immediately left word is covered. Simi-
larly, only after all words in the right neighboring
uncovered span are covered, other words can be
287
(a) (b)
Figure 4: The two ways that the current block is
merged with its left neighboring uncovered span.
The third node in the first row denotes the current
block, the first and second nodes in the first row
denote left and right parts of the left neighboring
uncovered span, respectively.
covered.
(3) The immediately left and right words are
neither covered. The next block can be selected
from either the left or the right neighboring uncov-
ered span until the immediate left or right word is
covered.
The above operations can be performed recur-
sively until the whole source sentence is merged
into a single block, so the reordering observes ITG
constraints.
Now, we show that translation which is not gen-
erated in the above way must violate ITG con-
straints.
If the next block is selected out of the neighbor-
ing uncovered spans, the current block can be nei-
ther adjacent to the last covered block nor adjacent
to the selected next block, so the current block can
not be merged with any block and the whole sen-
tence can not be covered by an ITG tree. As in
Figure 3(b), if the next block to cover is [zaoqie6],
then [zairu1] is neither adjacent to [diannao5]
nor adjacent to [zaoqie6].
We can conclude that if we select the next block
from the left or right neighboring uncovered span
of the current block, then the translation must ob-
serve ITG constraints.
2.2 Shift-Reduce Decoding Algorithm
In order to generate the translation with ITG con-
straints, the shift-reduce algorithm have to keep
trace of covered blocks, left and right neighboring
uncovered spans. Formally, the shift-reduce de-
coding algorithm uses the following three stacks:
? St: the stack for covered blocks. The blocks
are pushed in the order that they are covered,
not the order that they are in the source sen-
tence.
? Sl : the stack for the left uncovered spans of
the current block. When a block is pushed
into St, its corresponding left neighboring
uncovered span is pushed into Sl.
? Sr :the stack for the right uncovered spans of
the current block. When a block is pushed
into St, its corresponding right neighboring
uncovered span is pushed into Sr.
A translation configuration is a triple c =
?St, Sl, Sr?. Given a source sentence f =
f1, f2, ..., fm, we import a virtual start word and
the whole translation procedure can be seen as
a sequence of transitions from cs to ct, where
cs = ?[0], ?, [1,m]? is the initial configura-
tion, ct = ?[0,m], ?, ?? is the terminal con-
figuration. The configuration for Figure 3 (e) is
?[0][5][1][3, 4], [2], [6]?.
We define three types of transitions from
a configuration to another . Assume the cur-
rent configuration c = ? [ft11, ft12]...[ftk1, ftk2],
[fl11, fl12]...[flu1, flu2], [frv1, frv2]...[fr11, fr12] ?,
then :
? Transitions LShift pop the top element
[flu1, flu2] from Sl and select a block [i, j]
from [flu1, flu2] to translate. In addition,
they push [i, j] into St, and if i 6= flu1, they
push [flu1, i ? 1] into Sl, and if j 6= flu2,
they push [j+1, flu2] into Sr. The precondi-
tion to operate the transition is that Sl is not
null and the top span of Sl is adjacent to the
top block of St. Formally, the precondition
is flu2 + 1 = ftk1.
? Transitions RShift pop the top element
[frv1, frv2] of Sr and select a block [i, j]
from [frv1, frv2] to translate. In addition,
they push [i, j] into St, and if i 6= frv1, they
push [frv1, i?1] into Sl, and if j 6= frv2, they
push [j + 1, frv2] into Sr. The precondition
is that Sr is not null and the top span of Sr is
288
adjacent to the top block of St. Formally, the
precondition is ftk2 + 1 = frv1.
? Transitions Reduce pop the top two blocks
[ftk?11, ftk?12], [ftk1, ftk2] from St and push
the merged span [ftk?11, ftk2] into St. The
precondition is that the top two blocks are ad-
jacent. Formally, the precondition is ftk?12+
1 = ftk1
The transition sequence of the example in Fig-
ure 2 is listed in Figure 5. For the purpose of
efficiency, transitions Reduce are integrated with
transitions LShift and RShift in practical imple-
mentation. Before transitions LShift and RShift
push [i, j] into St, they check whether [i, j] is ad-
jacent to the top block of St. If so, they change
the top block into the merged block directly.
In practical implementation, in order to further
restrict search space, distortion limit is applied be-
sides ITG constraints: a source phrase can be cov-
ered next only when it is ITG-legal and its distor-
tion does not exceed distortion limit. The distor-
tion d is calculated by d = |starti ? endi?1 ? 1|,
where starti is the start position of the current
phrase and endi?1 is the last position of the last
translated phrase.
3 Related Work
Galley and Manning (2008) present a hierarchi-
cal phrase reordering model aimed at improving
non-local reorderings. Via the hierarchical mer-
gence of two blocks, the orientation of long dis-
tance words can be computed. Their shift-reduce
algorithm does not import ITG constraints and ad-
mits the translation violating ITG constraints.
Zens et al (2004) introduce a left-to-
right decoding algorithm with ITG constraints
on the alignment template system (Och et al,
1999). Their algorithm processes candidate
source phrases one by one through the whole
search space and checks if the candidate phrase
complies with ITG constraints. Besides, their al-
gorithm checks validity via cover vector and does
not formalize ITG structure. The shift-reduce de-
coding algorithm holds ITG structure via three
stacks. As a result, it can offer ITG-legal spans
directly and decode faster. Furthermore, with
Transition St Sl Sr
[0] ? [1, 6]
RShift [0][5] [1, 4] [6]
LShift [0][5][1] ? [2, 4][6]
RShift [0][5][1][3] [2] [4][6]
RShift [0][5][1][3][4] [2] [6]
Reduce [0][5][1][3, 4] [2] [6]
LShift [0][5][1][3, 4][2] ? [6]
Reduce [0][5][1][2, 4] ? [6]
Reduce [0][5][1, 4] ? [6]
Reduce [0][1, 5] ? [6]
Reduce [0, 5] ? [6]
RShift [0, 5][6] ? ?
Reduce [0, 6] ? ?
Figure 5: Transition sequence for the example in
Figure 2. The top nine transitions correspond to
Figure 3 (a), ... , Figure 3 (i), respectively.
the help of ITG structure, it can be extended to
syntax-based models easily.
Xiong et al (2006) propose a BTG-based
model, which uses the context to determine the
orientation of two adjacent spans. It employs the
cube-time CYK algorithm.
4 Experiments
We compare the shift-reduce decoder with the
state-of-the-art decoder Moses (Koehn et al,
2007). The shift-reduce decoder was imple-
mented by modifying the normal search algo-
rithm of Moses to our shift-reduce algorithm,
without cube pruning (Huang and Chiang, 2005).
We retained the features of Moses: four trans-
lation features, three lexical reordering features
(straight, inverted and discontinuous), linear dis-
tortion, phrase penalty, word penalty and language
model, without importing any new feature. The
decoding configurations used by all the decoders,
including beam size, phrase table limit and so on,
were the same, so the performance was compared
fairly.
First, we will show the performance of shift-
reduce algorithm on three data sets with large
training data sets (Section 4.1). Then, we will
analyze the performance elaborately in terms of
accuracy, speed and search ability with a smaller
289
training data set (Section 4.2). All experiments
were done on Chinese-to-English translation tasks
and all results are reported with case insensitive
BLEU score. Statistical significance were com-
puted using the sign-test described in Collins et
al. (Collins et al, 2005).
4.1 Performance Evaluation
We did three experiments to compare the perfor-
mance of the shift-reduce decoder, Moses and the
decoder with ITG constraints using cover vector
(denoted as CV). 2 The shift-reduce decoder de-
coded with two sets of parameters: one was tuned
by itself (denoted as SR) and the other was tuned
by Moses (denoted as SR-same), using MERT
(Och, 2003). Two searching algorithms of Moses
are considered: one is the normal search algorithm
without cubing pruning (denoted as Moses), the
other is the search algorithm with cube pruning
(denoted as Moses-cb). For all the decoders, the
distortion limit was set to 6, the nbest size was set
to 100 and the phrase table limit was 50.
In the first experiment, the development set is
part of NIST MT06 data set including 862 sen-
tences, the test set is NIST MT08 data set and
the training data set contains 5 million sentence
pairs. We used a 5-gram language model which
were trained on the Xinhua and AFP portion of
the Gigaword corpus. The results are shown in
Table 1(a).
In the second experiment, the development data
set is NIST MT02 data set and the test set is NIST
MT05 data set. Language model and the training
data set are the same to that of the first experiment.
The result is shown in Table 1(b).
In the third experiment, the development set
is China Workshop on Machine Translation 2008
data set (denoted as CWMT08) and the test set
is China Workshop on Machine Translation 2007
data set (denoted as CWMT07). The training set
contains 2 Million sentence pairs and the language
model are a 6-gram language model trained on
the Reuter corpus and English corpus. Table 1(c)
gives the results.
In the above three experiments, SR decoder
2The decoder CV is implemented by adding the ITG con-
straints to Moses using the algorithm described in (Zens et
al., 2004).
NIST06 NIST08 speed
Moses 30.24 25.08 4.827
Moses-cb 30.27 23.80 1.501
CV 30.35 26.23** 4.335
SR-same ?? 25.09 3.856
SR 30.47 26.67** 4.126
(a)
NIST02 NIST05 speed
Moses 35.68 35.80 7.142
Moses-cb 35.42 35.03 1.811
CV 35.45 36.56** 6.276
SR-same ?? 35.84 5.008
SR 35.99* 36.42** 5.432
(b)
CWMT08 CWMT07 speed
Moses 27.75 25.91 3.061
Moses-cb 27.82 25.16 0.548
CV 27.71 26.58** 2.331
SR-same ?? 25.97 1.988
SR 28.14* 26.71** 2.106
(c)
Table 1: Performance comparison. Moses: Moses
without cube pruning, Moses-cb: Moses with
cube pruning, CV: the decoder using cover vector,
SR-same: the shift-reduce decoder decoding with
parameters tunes by Moses, SR: the shift-reduce
decoder with parameters tuned by itself. The sec-
ond column stands for develop set, the third col-
umn stands for test set and speed column shows
the average time (seconds) of translating one sen-
tence in the test set. **: significance at the .01
level.
improves the accuracy by 1.59, 0.62, 0.8 BLEU
score (p < .01), respectively, and improves the
speed by 15%, 24%, 30%, respectively. we can
see that SR can improve both the accuracy and
the speed while SR-same can increase the speed
significantly with a slight improvement on the ac-
curacy. As both SR and CV decode with ITG
constraints, they match each other on the accu-
290
27.00
27.50
28.00
28.50
29.00
29.50
30.00
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17
B
LE
U
average decoding speed (s)
d=-1
d=-1
d=-1
SR
SR-same
Moses
Figure 6: Performance comparison on NIST05.
For a curve, the dots correspond to distortion limit
4, 6, 8, 10, 14 and no distortion from left to right.
d = ?1 stands for no distortion limit.
racy. However, the speed of SR is faster than CV.
Cube pruning can improve decoding speed dra-
matically, but it is not risk-free pruning technol-
ogy, so the BLEU score declines obviously.
4.2 Performance Analysis
We make performance analysis with the same ex-
periment configuration as the second experiment
in Section 4.1, except that the training set in
the analysis experiment is FBIS corpus, includ-
ing 289k sentence pairs. In the following exper-
iments, Moses employs the normal search algo-
rithm without cube pruning.
For the decoders employ the linear distortion
feature, the distortion limit will influence the
translation accuracy. Besides, with different dis-
tortion limit, the proportion of ITG-legal transla-
tion generated by Moses will differ. The smaller
the distortion limit is, the greater the proportion is.
So we first compare the performance with differ-
ent distortion limit.
We compare the shift-reduce decoder with
Moses using different distortion limit. The re-
sults are shown in Figure 6. When distortion limit
is set to 6, every decoder gets a peak value and
SR has an improvement of 0.66 BLEU score over
Moses. From the curves, we can see that the
BLEU score of SR-same with distortion limit 8
28.00
28.50
29.00
29.50
30.00
30.50
31.00
31.50
32.00
32.50
33.00
33.50
34.00
34.50
35.00
35.50
36.00
36.50
37.00
 4  6  8  10  12  14  16
B
LE
U
distortion limit
SR
SR-same
Moses
(a) ITG set
25.00
25.50
26.00
26.50
27.00
27.50
28.00
 4  6  8  10  12  14  16
BL
EU
distortion limit
SR
SR-same
Moses
(b) rest set
Figure 7: Accuracy comparison on the ITG set
and rest set of NIST05. The ITG set includes the
sentences the translations of which generated by
Moses are ITG-legal, and the rest set contains the
rest sentences. distortion limit = 16 denotes no
distortion limit.
is lower than that of Mose with distortion limit
6. This is because the decoding speed of SR-
same with distortion limit 8 is not faster than that
of Moses with distortion limit 6. On the whole,
compared to Moses, SR-same can improve the ac-
curacy slightly with much faster decoding speed,
and SR can obtain improvements on both the ac-
curacy and the speed.
We split the test set into two sets: one contains
the sentences, the translations of which generated
by Moses are ITG-legal (denoted as ITG set) and
the other contains the rest (denoted as rest set).
From Figure 7, we can see that no matter on the
ITG set or on the rest set, SR decoder can gain ob-
vious accuracy improvements with all distortion
291
ITG restd
Moses SR-same total < = > Moses SR-same total < = >
4 28.67 28.68 1050 8 1042 0 25.61 25.82 32 0 0 32
6 31.34 31.42 758 51 705 2 25.78 25.72 324 32 2 290
8 32.59 32.93* 594 72 516 6 25.68 25.65 488 82 3 403
10 34.36 34.99** 456 80 365 11 26.04 26.50* 626 147 3 476
12 33.16 33.61** 454 63 380 11 27.01 27.13 628 165 1 462
14 35.98 36.25* 383 60 316 7 26.35 26.67* 699 203 1 495
-1 34.13 34.96** 351 39 308 4 26.17 26.78** 731 154 0 577
Table 2: Search ability comparison. The ITG set and the rest set of NIST05 were tested, respectively.
On the ITG set, the following six factors are reported from left to right: BLEU score of Moses, BLEU
score of SR-same, the number of sentences in the ITG set, the number of sentences the translation
probabilities of which computed by Moses, compared to that computed by SR, is lower, equal and
greater. The rest set goes similarly. *: significance at the .05 level, **: significance at the .01 level.
limit. While SR-same decoder only gets better re-
sults on the ITG set with all distortion limit. This
may result from the use of the linear distortion
feature. Moses may generate hypotheses the dis-
tortion of which is forbidden in the shift-reduce
decoder. This especially sharpens on the rest set.
So SR-same may suffer from an improper linear
distortion parameter.
The search ability of Moses and the shift-
reduce decoder are evaluated, too. The translation
must be produced with the same set of parameters.
In our experiments, we employed the parameters
tuned by Moses. The test was done on the ITG and
the rest set, respectively. The results are shown in
Table 2. As the distortion limit becomes greater,
the number of the ITG-legal translation generated
by Moses becomes smaller. On the ITG set, trans-
lation probabilities from the shift-reduce decoder
is either greater or equal to that from Moses on
most sentences, and BLEU scores of shift-reduce
decoder is greater than that of Moses with all
distortion limit. Although the search space of
shift-reduce decoder is smaller than that of Moses,
shift-reduce decoder can give the translation that
Moses can not reach. On the rest set, for most sen-
tences, the translation probabilities from Moses is
greater than that from shift-reduce decoder. But
only when distortion limit is 6 and 8, the BLEU
score of Moses is greater than that of the shift-
reduce decoder. We may conclude that greater
score does not certainly lead to greater BLEU
score.
5 Conclusions and Future Work
In this paper, we present a shift-reduce decod-
ing algorithm for phrase-based translation model
that can generate the ITG-legal translation in lin-
ear time. The algorithm need not consider shift-
reduce divergence and performs reduce operation
as soon as possible. We compare the performance
of the shift-reduce decoder with the state-of-the-
art decoder Moses. Experiment results show that
the shift-reduce algorithm can improve both the
accuracy and the speed significantly on different
test sets. We further analyze the performance and
find that on the ITG set, the shift-reduce decoder
is superior over Moses in terms of accuracy, speed
and search ability, while on the rest set, it does
not display advantage, suffering from improper
parameters.
Next, we will extend the shift-reduce algorithm
to syntax-based translation models, to see whether
it works.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China Contract 60736014,
National Natural Science Foundation of China
Contract 60873167 and High Technology R&D
Program Project No. 2006AA010108. We are
grateful to the anonymous reviewers for their
valuable comments.
292
References
Collins, Michael, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Galley, Michel and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the Ninth International
Workshop on Parsing Technologies (IWPT), pages
53?64.
Knight, Kevin. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25:607?615.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of the 45th ACL, Demonstra-
tion Session.
Koehn, Philipp. 2004. Pharaoh: A beam search de-
coder for phrased-based statistical machine transla-
tion. In Proc. of AMTA, pages 115?124.
Och, Frans J., Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical
machine translation. In Proc. of EMNLP, pages 20?
28.
Och, Frans J. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL,
pages 160?167.
Tillmann, Chirstoph and Hermann Ney. 2003.
Word reordering and a dynamic programming beam
search algorithm for statistical machine translation.
Computational Linguistics, 29:97?133.
Wu, Dekai. 1996. A polynomial-time algorithm for
statistical machine translation. In Proc. of ACL,
pages 152?158.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL, pages
521?528.
Zens, Richard and Hermann Ney. 2003. A compara-
tive study on reordering constraints in statistical ma-
chine translation. In Proc. of ACL, pages 144?151.
Zens, Richard, Hermann Ney, Taro Watanable, and
Eiichiro Sumita. 2004. Reordering constraints
for phrase-based statistical machine translation. In
Proc. of COLING, pages 205?211.
293
Coling 2010: Poster Volume, pages 516?524,
Beijing, August 2010
Effective Constituent Projection across Languages
Wenbin Jiang and Yajuan Lu? and Yang Liu and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, lvyajuan, yliu, liuqun}@ict.ac.cn
Abstract
We describe an effective constituent pro-
jection strategy, where constituent pro-
jection is performed on the basis of de-
pendency projection. Especially, a novel
measurement is proposed to evaluate the
candidate projected constituents for a tar-
get language sentence, and a PCFG-style
parsing procedure is then used to search
for the most probable projected con-
stituent tree. Experiments show that, the
parser trained on the projected treebank
can significantly boost a state-of-the-art
supervised parser. When integrated into a
tree-based machine translation system, the
projected parser leads to translation per-
formance comparable with using a super-
vised parser trained on thousands of anno-
tated trees.
1 Introduction
In recent years, supervised constituent parsing has
been well studied and achieves the state-of-the-art
for many resource-rich languages (Collins, 1999;
Charniak, 2000; Petrov et al, 2006). Because
of the cost and difficulty in treebank construc-
tion, researchers have also investigated the utiliza-
tion of unannotated text, including the unsuper-
vised parsing which totally uses unannotated data
(Klein and Manning, 2002; Klein and Manning,
2004; Bod, 2006; Seginer, 2007), and the semi-
supervised parsing which uses both annotated and
unannotated data (Sarkar, 2001; Steedman et al,
2003; McClosky et al, 2006).
Because of the higher complexity and lower
performance of unsupervised methods, as well as
the need of reliable priori knowledge in semi-
supervised methods, it seems promising to project
the syntax structures from a resource-rich lan-
guage to a resource-scarce one across a bilingual
corpus. Lots of researches have so far been de-
voted to dependency projection (Hwa et al, 2002;
Hwa et al, 2005; Ganchev et al, 2009; Smith
and Eisner, 2009). While for constituent projec-
tion there is few progress. This is due to the fact
that the constituent syntax describes the language
structure in a more detailed way, and the degree of
isomorphism between constituent structures ap-
pears much lower.
In this paper we propose for constituent pro-
jection a stepwise but totally automatic strategy,
which performs constituent projection on the ba-
sis of dependency projection, and then use a con-
straint EM optimization algorithm to optimized
the initially projected trees. Given a word-aligned
bilingual corpus with source sentences parsed, we
first project the dependency structures of these
constituent trees to the target sentences using a
dynamic programming algorithm, then we gener-
ate a set of candidate constituents for each target
sentence and design a novel evaluation function
to calculate the probability of each candidate con-
stituent, finally, we develop a PCFG-style parsing
procedure to search for the most probable pro-
jected constituent tree in the evaluated candidate
constituent set. In addition, we design a constraint
EM optimization procedure to decrease the noise
in the initially projected constituent treebank.
Experimental results validate the effectiveness
of our approach. On the Chinese-English FBIS
corpus, we project the English parses produced
by the Charniak parser across to the Chinese sen-
516
tences. A berkeley parser trained on this pro-
jected treebank can effectively boost the super-
vised parsers trained on bunches of CTB trees.
Especially, the supervised parser trained on the
smaller CTB 1.0 benefits a significant F-measure
increment of more than 1 point from the projected
parser. When using the projected parser in a tree-
based translation model (Liu et al, 2006), we
achieve translation performance comparable with
using a state-of-the-art supervised parser trained
on thousands of CTB trees. This surprising re-
sult gives us an inspiration that better translation
would be achieved by combining both projected
parsing and supervised parsing into a hybrid pars-
ing schema.
2 Stepwise Constituent Projection
We first introduce the dynamic programming pro-
cedure for dependency projection, then describe
the PCFG-style algorithm for constituent projec-
tion which is conducted on projected dependent
structures, and finally show the constraint EM
procedure for constituent optimization.
2.1 Dependency Projection
For dependency projection we adopt a dynamic
programming algorithm, which searches the most
probable projected target dependency structure
according to the source dependency structure and
the word alignment.
In order to mitigate the effect of word alignment
errors, multiple GIZA++ (Och and Ney, 2000) re-
sults are combined into a compact representation
called alignment matrix. Given a source sentence
with m words, represented as E1:m, and a target
sentence with n words, represented as F1:n, their
word alignment matrix A is an m ? n matrix,
where each element Ai,j denotes the probability
of the source word Ei aligned to the target word
Fj .
Using P (DF |DE , A) to denote the probability
of the projected target dependency structure DF
conditioned on the source dependency structure
DE and the alignment matrix A, the projection al-
gorithm aims to find
D?F = argmax
DF
P (DF |DE , A) (1)
Algorithm 1 Dependency projection.
1: Input: F , and Pe for all word pairs in F
2: for ?i, j? ? ?1, |F |? in topological order do
3: buf ? ?
4: for k? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r, Pe) into buf
7: insert DERIV(r, l, Pe) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(p, c, Pe)
11: d? p ? c ? {p ? rooty c ? root} ? new derivation
12: d ? evl ? EVAL(d, Pe) ? evaluation function
13: return d
P (DF |DE , A) can be factorized into each depen-
dency edge xy y in DF
P (DF |DE , A) =
?
xyy?DF
Pe(xy y|DE , A)
Pe can then be obtained by simple accumulation
across all possible situations of correspondence
Pe(xy y|DE , A)
=
?
1?x?,y??|E|
Ax,x? ?Ay,y? ? ?(x?, y?|DE)
where ?(x?, y?|DE) is a 0-1 function that equals
1 only if the dependent relation x? y y? holds in
DE .
The search procedure needed by the argmax op-
eration in equation 1 can be effectively solved
by the Chu-Liu-Edmonds algorithm used in (Mc-
Donald et al, 2005). In this work, however, we
adopt a more general and simple dynamic pro-
gramming algorithm as shown in Algorithm 1,
in order to facilitate the possible expansions. In
practice, the cube-pruning strategy (Huang and
Chiang, 2005) is used to speed up the enumera-
tion of derivations (loops started by line 4 and 5).
2.2 Constituent Projection
The PCFG-style parsing procedure searches for
the most probable projected constituent tree in
a shrunken search space determined by the pro-
jected dependency structure and the target con-
stituent tree. The shrunken search space can be
built as following. First, we generates the candi-
date constituents of the source tree and the can-
didate spans of the target sentence, so as to enu-
merate the candidate constituents of the target sen-
tence. Then we compute the consistent degree for
517
each pair of candidate constituent and span, and
further estimate the probability of each candidate
constituent for the target sentence.
2.2.1 Candidate Constituents and Spans
For the candidate constituents of the source
tree, using only the original constituents imposes
a strong hypothesis of isomorphism on the con-
stituent projection between two languages, since
it requires that each couple of constituent and span
must be strictly matched. While for the candi-
date spans of the target sentences, using all sub-
sequences makes the search procedure suffer from
more perplexity. Therefore, we expand the candi-
date constituent set and restrict the candidate span
set:
? Candidate Constituent: Suppose a produc-
tion in the source constituent tree, denoted as
p ? c1c2..ch..c|p|, and ch is the head child
of the parent p. Each constituent, p or c, is a
triple ?lb, rb, nt?, where nt denotes its non-
terminal, while lb and rb represent its left-
and right bounds of the sub-sequence that the
constituent covers. The candidate constituent
set of this production consists the head of
the production itself, and a set of incomplete
constituents,
{?l, r, p ? nt??|c1 ? lb ? l ? ch ? lb?
ch ? rb ? r ? c|p| ? rb?
(l < ch ? lb ? r > ch ? rb)}
where the symbol ? indicates an incomplete
non-terminal. The candidate constituent set
of the entire source tree is the unification of
the sets extracted from all productions of the
tree.
? Candidate Span: A candidate span of the tar-
get sentence is a tuple ?lb, rb?, where lb and
rb indicate the same as in a constituent. We
define the candidate span set as the spans of
all regular dependent segments in the corre-
sponding projected dependency structure. A
regular dependency segment is a dependent
segment that every modifier of the root is a
complete dependency structure. Suppose a
dependency structure rooted at word p, de-
noted as clL..cl2cl1 x p y cr1cr2..crR, it
has L (L ? 0) modifiers on its left and R
(R ? 0) modifiers on its right, each of them
is a smaller complete dependency structure.
Then the word p itself is a regular depen-
dency segment without any modifier, and
{cli..cl1 x py cr1..crj |0 ? i ? L?
0 ? j ? R?
(i > 0 ? j > 0)}
is a set of regular dependency structures with
at least one modifier. The regular depen-
dency segments of the entire projected de-
pendency structure can simply be accumu-
lated across all dependency nodes.
2.2.2 Span-to-Constituent Correspondence
After determining the candidate constituent set
of the source tree, denoted as ?E , and the can-
didate span set of the target sentence, denoted as
?F , we then calculate the consistent degree for
each pair of candidate constituent and candidate
span.
Given a candidate constituent ? ? ?E and a
candidate span ? ? ?F , their consistent degree
C(?, ?|A) is the probability that they are aligned
to each other according to A.
We display the derivations from bottom to up.
First, we define the alignment probability from a
word i in the span ? to the constituent ? as
P (i 7? ?|A) =
?
??lb?j???rbAi,j?
j Ai,j
Then we define the alignment probability from the
span ? to the constituent ? as
P (? 7? ?|A) =
?
??lb?i???rb
P (i 7? ?|A)
Note that we use i to denote both a word and its in-
dex for simplicity without causing confusion. Fi-
nally, we define C(?,?|A) as
C(?, ?|A) = P (? 7? ?|A)? P (? 7? ?|AT ) (2)
Where P (? 7? ?|AT ) denotes the alignment
probability from the constituent ? to the span ?, it
can be calculated in the same manner.
518
2.2.3 Constituent Projection Algorithm
The purpose of constituent projection is to find
the most probable projected constituent tree for
the target sentence conditioned on the source con-
stituent tree and the word alignment
T?F = argmax
TF??F
P (TF |TE, A) (3)
Here, we use ?F to denote the set of candidate
constituents of the target sentence
?F = ?F ?NT (?E)
= {?F |?(?F ) ? ?F ? nt(?F ) ? NT (?E)}
where ?(?) and nt(?) represent the span and the
non-terminal of a constituent respectively, and
NT (?) represents the set of non-terminals ex-
tracted from a constituent set. Note that TF is a
subset of ?F if we treat a tree as a set of con-
stituents.
The probability of the projected tree TF can be
factorized into the probabilities of the projected
constituents that composes the tree
P (TF |TE , A) =
?
?F?TF
P?(?F |TE , A)
while the probability of the projected source con-
stituent can be defined as a statistics of span-to-
constituent- and constituent-to-constituent consis-
tent degrees
P?(?F |TE , A) =
?
?E??E C(?F , ?E |A)?
?E??E C(?(?F ), ?E |A)
where C(?F , ?E |A) in the numerator denotes the
consistent degree for each pair of constituents,
which can be calculated based on that of span and
constituent described in Formula 2
C(?F , ?E) =
{
0 if ?F ? nt 6= ?E ? nt
C(?(?F ), ?E) else
Algorithm 2 shows the pseudocode for con-
stituent projection. A PCFG-style parsing pro-
cedure searches for the best projected constituent
tree in the constrained space determined by ?F .
Note that the projected trees are binarized, and can
be easily recovered according to the asterisks at
the tails of non-terminals.
Algorithm 2 Constituent projection.
1: Input: ?F , ?F , and P? for all spans in ?F
2: for ?i, j? ? ? in topological order do
3: buf ? ?
4: for p ? ?F s.t. ?(p) = ?i, j? do
5: for k? i..j ? 1 do ? all partitions
6: for l ? V[i, k] and r ? V[k + 1, j] do
7: insert DERIV(l, r, p, P?) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |F |]
10: function DERIV(l, r, p, P?)
11: d? l ? r ? {p} ? new derivation
12: d ? evl ? EVAL(d, P?) ? evaluation function
13: return d
2.3 EM Optimization
Since the constituent projection is conducted on
each sentence pair separately, the projected tree-
bank is apt to suffer from more noise caused by
free translation and word alignment error. It can
be expected that an EM iteration over the whole
projected treebank will lead to trees with higher
consistence.
We adopt the inside-outside algorithm to im-
prove the quality of the initially projected tree-
bank. Different from previous works, all expecta-
tion and maximization operations for a single tree
are performed in a constrained space determined
by the candidate span set of the projected target
dependency structure. That is to say, all the sum-
mation operations, both for calculating ?/? values
and for re-estimating the rule probabilities, only
consider the spans in the candidate span set. This
means that the projected dependency structures
are supposed believable, and the noise is mainly
introduced in the following constituent projection
procedure.
Here we give an overall description of the tree-
bank optimization procedure. First, an initial
PCFG grammar G0F is estimated from the original
projected treebank. Then several iterations of ?/?
calculation and rule probability re-estimation are
performed. For example in the i-the iteration, ?/?
values are calculated based on the current gram-
mar Gi?1F , afterwards the optimized grammar GiF
is obtained based on these ?/? values. The itera-
tive procedure terminates when the likelihood of
whole treebank increases slowly. Finally, with the
optimized grammar, a constrained PCFG parsing
procedure is conducted on each of the initial pro-
519
jected trees, so as to obtain an optimized treebank.
3 Applications of Constituent Projection
The most direct contribution of constituent pro-
jection is pushing an initial step for the statis-
tical constituent parsing of resource-scarce lan-
guages. It also has some meaningful applica-
tions even for the resource-rich languages. For
instances, the projected treebank, due to its large
scale and high coverage, can used to boost an tra-
ditional supervised-trained parser. And, the parser
trained on the projected treebank can adopted to
conduct tree-to-string machine translation, since
it give parsing results with larger isomorphism
with the target language than a supervised-trained
parser dose.
3.1 Boost an Traditional Parser
We first establish a unified framework for the en-
hanced parser where a projected parser is adopted
to guide the parsing procedure of the baseline
parser.
For a given target sentence S, the enhanced
parser selected the best parse T? among the set
of candidates ?(S) according to two evaluation
functions, given by the baseline parser B and the
projected guide parser G, respectively.
T? = argmax
T??(S)
P (T |B)? P (T |G)? (4)
These two evaluation functions can be integrated
deeply into the decoding procedure (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated at a shallow level in a rerank-
ing manner (Collins, 2000; Charniak and John-
son, 2005). For simplicity and generability, we
adopt the reranking strategy. In k-best reranking,
?(S) is simply a set of candidate parses, denoted
as {T1, T2, ..., Tk}, and we use the single parse of
the guide parser, TG, to re-evaluate these candi-
dates. Formula 4 can be redefined as
T? (TG) = argmax
T??(S)
w ? f(T, TG) (5)
Here, f(T, TG) and w represent a high dimen-
sional feature representation and a correspond-
ing weight vector, respectively. The first feature
f1(T, TG) = logP (T |B) is the log probability
of the baseline parser, while the remaining fea-
tures are integer-valued guide features, and each
of them represents the guider parser?s predication
result for a particular configuration in candidate
parse T , so as to utilize the projected parser?s
knowledge to guide the parsing procedure of the
traditional parser.
In our work a guide feature is composed of two
parts, the non-terminal of a certain constituent ?
in the candidate parse T ,1 and the non-terminal
at the corresponding span ?(?) in the projected
parse TG. Note that in the projected parse this
span does not necessarily correspond to a con-
stituent. In such situations, we simply use the
non-terminal of the constituent that just be able
to cover this span, and attach a asterisk at the tail
of this non-terminal. Here is an example of the
guide features
f100(T, TG) = V P ? T ? PP? ? TG
It represents that a V P in the candidate parse cor-
responds to a segment of a PP in the projected
parse. The quantity of its weight w100 indicates
how probably a span can be predicated as V P if
the span corresponds to a partial PP in the pro-
jected parse.
We adopt the perceptron algorithm to train
the reranker. To reduce overfitting and pro-
duce a more stable weight vector, we also use
a refinement strategy called averaged parameters
(Collins, 2002).
3.2 Using in Machine Translation
Researchers have achieved promising improve-
ments in tree-based machine translation (Liu et
al., 2006; Huang et al, 2006). Such models use
a parsed tree as input and converts it into a target
tree or string. Given a source language sentence,
first we use a traditional source language parser
to parse the sentence to obtain the syntax tree T ,
and then use the translation decoder to search for
the best derivation d?, where a derivation d is a se-
quence of transformations that converts the source
tree into the target language string
d? = argmax
d?D
P (d|T ) (6)
1Using non-terminals as features brings no improvement
in the reranking experiments, so as to examine the impact of
the projected parser.
520
Here D is the candidate set of d, and it is deter-
mined by the source tree T and the transformation
rules.
Since the tree-based models are based on
the synchronous transformational grammars, they
suffer much from the isomerism between the
source syntax and the target sentence structure.
Considering that the parsed tree produced by a
projected parser may have larger isomorphism
with the target language, it would be a promis-
ing idea to adopt the projected parser to parse the
input sentence for the subsequent translation de-
coding procedure.
4 Experiments
In this section, we first invalidate the effect of con-
stituent projection by evaluating a parser trained
on the projected treebank. Then we investigate
two applications of the projected parser: boosting
an traditional supervised-trained parser, and inte-
gration in a tree-based machine translation sys-
tem. Following the previous works, we depict the
parsing performance by F-score on sentences with
no more than 40 words, and evaluate the transla-
tion quality by the case-sensitive BLEU-4 metric
(Papineni et al, 2002) with 4 references.
4.1 Constituent Projection
We perform constituent projection from English
to Chinese on the FBIS corpus, which contains
239K sentence pairs with about 6.9M/8.9M words
in Chinese/English. The English sentences are
parsed by the Charniak Parser and the dependency
structures are extracted from these parses accord-
ing to the head-finding rules of (Yamada and
Matsumoto, 2003). The word alignment matrixes
are obtained by combining the 10-best results of
GIZA++ according to (Liu et al, 2009).
We first project the dependency structures from
English to Chinese according to section 2.1, and
then project the constituent structures according
to section 2.2. We define an assessment criteria
to evaluate the confidence of the final projected
constituent tree
c = n
?
P (DF |DE , A) ? P (TF |TE , A)
where n is the word count of a Chinese sentence
in our experiments. A series of projected Chi-
Thres c #Resrv Cons-F1 Span-F1
0.5 12.6K 23.9 32.7
0.4 17.8K 23.9 33.4
0.3 27.2K 25.4 35.7
0.2 45.1K 26.6 38.0
0.1 87.0K 27.8 40.4
Table 1: Performances of the projected parsers
on the CTB test set. #Resrv denotes the amount
of reserved trees within threshold c. Cons-F1 is
the traditional F-measure, while Span-F1 is the F-
measure without consideration of non-terminals.
nese treebanks with different scales are obtained
by specifying different c as the filtering threshold.
The state-of-the-art Berkeley Parser is adopted to
train on these treebanks because of its high per-
formance and independence of head word infor-
mation.
Table 1 shows the performances of these pro-
jected parsers on the standard CTB test set, which
is composed of sentences in chapters 271-300.
We find that along with the decrease of the filter-
ing threshold c, more projected trees are reserved
and the performance of the projected parser con-
stantly increases. We also find that the traditional
F-value, Cons-F1, is obviously lower than the one
without considering non-terminals, Span-F1. This
indicates that the constituent projection procedure
introduces more noise because of the higher com-
plexity of constituent correspondence. In all the
rest experiments, however, we simply use the pro-
jected treebank filtered by threshold c = 0.1 and
do not try any smaller thresholds, since it already
takes more than one weak to train the Berkeley
Parser on the 87 thousands trees resulted by this
threshold.
The constrained EM optimization procedure
described in section 2.3 is used to alleviate the
noise in the projected treebank, which may be
caused by free translation, word alignment errors,
and projection on each single sentence pair. Fig-
ure 1 shows the log-likelihood on the projected
treebank after each EM iteration. It is obvious that
the log-likelihood increases very slowly after 10
iterations. We terminate the EM procedure after
40 iterations.
Finally we train the Berkeley Parser on the op-
timized projected treebank, and test its perfor-
521
-65
-64
-63
-62
-61
-60
-59
-58
 0  5  10  15  20  25  30  35  40
Lo
g-
lik
eli
ho
od
EM iteration
Figure 1: Log-likelihood of the 87K-projected
treebank after each EM interation.
Train Set Cons-F1 Span-F1
Original 87K 27.8 40.4
Optimized 87K 22.8 40.2
Table 2: Performance of the parser trained on the
optimized projected treebank, compared with that
of the original projected parser.
Train Set Baseline Bst-Ini Bst-Opt
CTB 1.0 75.6 76.4 76.9
CTB 5.0 85.2 85.5 85.7
Table 3: Performance improvement brought by
the projected parser to the baseline parsers trained
on CTB 1.0 and CTB 5.0, respectively. Bst-
Ini/Bst-Opt: boosted by the parser trained on the
initial/optimized projected treebank.
mance on the standard CTB test set. Table 2
shows the performance of the parser trained on
the optimized projected treebank. Unexpectedly,
we find that the constituent F1-value of the parser
trained on the optimized treebank drops sharply
from the baseline, although the span F1-value re-
mains nearly the same. We assume that the EM
procedure gives the original projected treebank
more consistency between each single tree while
the revised treebank deviates from the CTB anno-
tation standard, but it needs to be validated by the
following experiments.
4.2 Boost an Traditional Parser
The projected parser is used to help the reranking
of the k-best parses produced by another state-of-
the-art parser, which is called the baseline parser
for convenience. In our experiments we choose
the revised Chinese parser (Xiong et al, 2005)
 70
 72
 74
 76
 78
 80
 82
 84
 86
 88
 1000  10000
Pa
rs
ev
al 
F-
sc
or
e 
(%
)
Scale of treebank (log)
CTB 1.0
CTB 5.0
baseline
boosted parser
Figure 2: Boosting performance of the projected
parser on a series of baseline parsers that are
trained on treebanks of different scales.
based on Collins model 2 (Collins, 1999) as the
baseline parser.2
The baseline parser is respectively trained on
CTB 1.0 and CTB 5.0. For both corpora we
follow the traditional corpus splitting: chapters
271-300 for testing, chapters 301-325 for devel-
opment, and else for training. Experimental re-
sults are shown in Table 3. We find that both
projected parsers bring significant improvement to
the baseline parsers. Especially the later, although
performs worse on CTB standard test set, gives a
larger improvement than the former. This to some
degree confirms the previous assumption. How-
ever, more investigation must be conducted in the
future.
We also observe that for the baseline parser
trained on the much larger CTB 5.0, the boost-
ing performance of the projected parser is rela-
tively lower. To further investigate the regularity
that the boosting performance changes according
to the scale of training treebank of the baseline
parser, we train a series of baseline parsers with
different amounts of trees, then use the projected
parser trained on the optimized treebank to en-
hance these baseline parsers. Figure 2 shows the
experimental results. From the curves we can see
that the smaller the training corpus of the baseline
parser, the more significant improvement can be
obtained. This is a good news for the resource-
scarce languages that have no large treebanks.
2The Berkeley Parser fails to give k-best parses for some
sentences when trained on small treebanks, and these sen-
tences have to be deleted in the k-best reranking experiments.
522
4.3 Using in Machine Translation
We investigate the effect of the projected parser
in the tree-based translation model on Chinese-to-
English translation. A series of contrast transla-
tion systems are built, each of which uses a super-
vised Chinese parser (Xiong et al, 2005) trained
on a particular amount of CTB trees.
We use the FBIS Chinese-English bitext as the
training corpus, the 2002 NIST MT Evaluation
test set as our development set, and the 2005 NIST
MT Evaluation test set as our test set. We first ex-
tract the tree-to-string translation rules from the
training corpus by the algorithm of (Liu et al,
2006), and train a 4-gram language model on
the Xinhua portion of GIGAWORD corpus with
Kneser-Ney smoothing using the SRI Language
Modeling Toolkit (Stolcke and Andreas, 2002).
Then we use the standard minimum error-rate
training (Och, 2003) to tune the feature weights
to maximize the system.s BLEU score.
Figure 3 shows the experimental results. We
find that the translation system using the projected
parser achieves the performance comparable with
the one using the supervised parser trained on
CTB 1.0. Considering that the F-score of the pro-
jected parser is only 22.8%, which is far below of
the 75.6% F-score of the supervised parser trained
on CTB 1.0, we can give more confidence to the
assumption that the projected parser is apt to de-
scribe the syntax structure of the counterpart lan-
guage. This surprising result also gives us an in-
spiration that better translation would be achieved
by combining projected parsing and supervised
parsing into hybrid parsing schema.
5 Conclusion
This paper describes an effective strategy for con-
stituent projection, where dependency projection
and constituent projection are consequently con-
ducted to obtain the initial projected treebank,
and an constraint EM procedure is then per-
formed to optimized the projected trees. The
projected parser, trained on the projected tree-
bank, significantly boosts an existed state-of-the-
art supervised-trained parser, especially trained on
a smaller treebank. When using the projected
parser in tree-based translation, we achieve the
0.220
0.230
0.240
0.250
0.260
0.270
 1000  10000
BL
EU
 sc
or
e
Scale of treebank (log)
use projected parser
CTB 1.0
CTB 5.0
use supervised parsers
Figure 3: Performances of the translation systems,
which use the projected parser and a series of su-
pervised parsers trained CTB trees.
translation performance comparable with using a
supervised parser trained on thousands of human-
annotated trees.
As far as we know, this is the first time that
the experimental results are systematically re-
ported about the constituent projection and its ap-
plications. However, many future works need
to do. For example, more energy needs to be
devoted to the treebank optimization, and hy-
brid parsing schema that integrates the strengths
of both supervised-trained parser and projected
parser would be valuable to be investigated for
better translation.
Acknowledgments
The authors were supported by 863 State Key
Project No. 2006AA010108, National Natural
Science Foundation of China Contract 60873167,
Microsoft Research Asia Natural Language Pro-
cessing Theme Program grant (2009-2010), and
National Natural Science Foundation of China
Contract 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions.
References
Bod, Rens. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proceedings of the COLING-
ACL.
Carreras, Xavier, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
523
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the NAACL.
Collins, Michael. 1999. Head-driven statistical mod-
els for natural language parsing. In Ph.D. Thesis.
Collins, Michael. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Collins, Michael. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Ganchev, Kuzman, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the AMTA.
Huang, Liang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrap-
ping parsers via syntactic projection across paral-
lel texts. In Natural Language Engineering, vol-
ume 11, pages 311?325.
Klein, Dan and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL.
Klein, Dan and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models
of dependency and constituency. In Proceedings of
the ACL.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the ACL.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of the ACL.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
Och, Franz J. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Och, Franz Joseph. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41th Annual Meeting of the Association
for Computational Linguistics, pages 160?167.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Weijing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of the ACL.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
ACL.
Sarkar, Anoop. 2001. Applying co-training methods
to statistical parsing. In Proceedings of NAACL.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the ACL.
Smith, David and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Steedman, Mark, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of the EACL.
Stolcke and Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 311?318.
Xiong, Deyi, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the penn chinese treebank with
semantic knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Yamada, H and Y Matsumoto. 2003. Statistical de-
pendency analysis using support vector machines.
In Proceedings of IWPT.
Zhang, Yue and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
524
Coling 2010: Poster Volume, pages 837?845,
Beijing, August 2010
Machine Translation with Lattices and Forests
Haitao Mi?? Liang Huang? Qun Liu?
?Key Lab. of Intelligent Information Processing ?Information Sciences Institute
Institute of Computing Technology Viterbi School of Engineering
Chinese Academy of Sciences University of Southern California
{htmi,liuqun}@ict.ac.cn {lhuang,haitaomi}@isi.edu
Abstract
Traditional 1-best translation pipelines
suffer a major drawback: the errors of 1-
best outputs, inevitably introduced by each
module, will propagate and accumulate
along the pipeline. In order to alleviate
this problem, we use compact structures,
lattice and forest, in each module instead
of 1-best results. We integrate both lat-
tice and forest into a single tree-to-string
system, and explore the algorithms of lat-
tice parsing, lattice-forest-based rule ex-
traction and decoding. More importantly,
our model takes into account all the proba-
bilities of different steps, such as segmen-
tation, parsing, and translation. The main
advantage of our model is that we can
make global decision to search for the best
segmentation, parse-tree and translation in
one step. Medium-scale experiments show
an improvement of +0.9 BLEU points over
a state-of-the-art forest-based baseline.
1 Introduction
Statistical machine translation (SMT) has wit-
nessed promising progress in recent years. Typi-
cally, conventional SMT is characterized as a 1-
best pipeline system (Figure 1(a)), whose mod-
ules are independent of each other and only take
as input 1-best results from the previous module.
Though this assumption is convenient to reduce
the complexity of SMT systems. It also bring a
major drawback of error propagation. The errors
of 1-best outputs, introduced inevitably in each
phase, will propagate and accumulate along the
pipeline. Not recoverable in the final decoding
(b)source segmentation lattice
parse forest target
source 1-best segmentation
1-best tree target
(a)
Figure 1: The pipeline of tree-based system: (a) 1-
best (b) lattice-forest.
step. These errors will severely hurt the translation
quality. For example, if the accuracy of each mod-
ule is 90%, the final accuracy will drop to 73%
after three separate phases.
To alleviate this problem, an obvious solution
is to widen the pipeline with k-best lists rather
than 1-best results. For example Venugopal et
al. (2008) use k-best alignments and parses in the
training phase. However, with limited scope and
too many redundancies, it is inefficient to search
separately on each of these similar lists (Huang,
2008).
Another efficient method is to use compact data
structures instead of k-best lists. A lattice or forest,
compactly encoded exponentially many deriva-
tions, have proven to be a promising technique.
For example, Mi and Huang (2008), Mi et al
(2008), Liu et al (2009) and Zhang et al (2009)
use forests in rule extraction and decoding phases
to extract more general rules and weaken the influ-
ence of parsing errors; Dyer et al (2008) use word
lattice in Chinese word segmentation and Arabic
morphological variation phases to weaken the in-
fluence of segmentation errors; Huang (2008) and
837
0 1 2 3 4 5 6 7 8 9c0:Bu` c1:sh?? c2:yu? c3:Sha? c4:lo?ng c5:ju? c6:x??ng c7:ta?o c8:lu`n
(0, 2, NR) (2, 3, CC) (3, 5, NR) (5, 6, VV) (6, 8, NN) (8, 9, NN)
(5, 7, VV) (7, 9, NN)(2, 3, P)
Figure 2: The lattice of the example:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n.? The solid lines show the 1-best
result, which is wrong.
Jiang et al (2008b) stress the problems in re-
ranking phase. Both lattices and forests have be-
come popular in machine translation literature.
However, to the best of our knowledge, previous
work only focused on one module at a time. In this
paper, we investigate the combination of lattice
and forest (Section 2), as shown in Figure 1(b).
We explore the algorithms of lattice parsing (Sec-
tion 3.2), rule extraction (Section 4) and decod-
ing (Section 5). More importantly, in the decoding
step, our model can search among not only more
parse-trees but also more segmentations encoded
in the lattice-forests and can take into account all
the probabilities of segmentations and parse-trees.
In other words, our model postpones the disambi-
guition of segmentation and parsing into the final
translation step, so that we can do global search
for the best segmentation, parse-tree and transla-
tion in one step. When we integrate a lattice into
a forest system, medium-scale experiments (Sec-
tion 6) show another improvement of +0.9 BLEU
points over a state-of-the-art forest-based system.
2 Compact Structures
A word lattice (Figure 2) is a compact representa-
tion of all the possible of segmentations and POS
tags, while a parse forest (Figure 5) is a compact
representation of all parse trees.
2.1 Word Lattice
For a given input sentence C = c0..cn?1, where
ci denotes a character at position i, and n is the
length of the sentence.
A word lattice (Figure 2), or lattice in short, is
a set of edges L, where each edge is in the form
of (i, j, X), which denotes a word of tag X , cov-
ering characters ci through cj?1. For example, in
Figure 2, (7, 9, NN) is a noun ?ta?olu`n? of two char-
acters.
The lattice in Figure 2 shows result of the ex-
ample:? Bu` sh?? yu? Sha? lo?ng ju? x??ng ta?o lu`n ?.
One ambiguity comes from the POS tag of word
?yu?? (preposition (P) or conjunction (CC)). The
other one is the segmentation ambiguity of the last
four characters, we can segment into either ?ju?
x??ngta?o lu`n? (solid lines), which means lift, beg-
ging and argument separately for each word or
?ju?x??ng ta?olu`n? (dashed lines), which means hold
a discussion.
lift begging argument
5 ju? 6 x??ng 7 ta?o 8 lu`n 9
hold a discussion
The solid lines above (and also in Figure 2)
show the 1-best result, which is obviously wrong.
If we feed it into the next modules in the SMT
pipeline, parsing and translation will be become
much more difficult, since the segmentation is not
recoverable. So it is necessary to postpone er-
ror segmentation decisions to the final translation
step.
2.2 Parse Forest
In parsing scenario, a parse forest (Figrure 5), or
forest for short, can be formalized as a hyper-
graph H , a pair ?V, E?, where node v ? V is in
the form of Xi,j , which denotes the recognition of
nonterminal X spanning the substring ci:j?1 from
positions ci through cj?1. Each hyperedge e ? E
is a pair ?tails(e), head(e)?, where head(e) ? V
is the consequent node in an instantiated deduc-
tive step, and tails(e) ? (V )? is the list of an-
tecedent nodes.
For the following deduction:
NR0,2 CC2,3 NR3,5
NP0,5 (*)
838
its hyperedge e? is notated:
?(NR0,2, CC2,3, NR3,5), NP0,5?.
where
head(e?) = {NP0,5}, and
tails(e?) = {NR0,2,CC2,3,NR3,5}.
We also denote IN (v) to be the set of incoming
hyperedges of node v, which represents the dif-
ferent ways of deriving v. For simplicity, we only
show a tree in Figure 5(a) over 1-best segmenta-
tion and POS tagging result in Figure 2. So the
IN (NP0,5) is {e?}.
3 Lattice Parsing
In this section, we first briefly review the con-
ventional CYK parsing, and then extend to lattice
parsing. More importantly, we propose a more ef-
ficient parsing paradigm in Section 3.3.
3.1 Conventional Parsing
The conventional CYK parsing algorithm in Fig-
ure 3(a) usually takes as input a single sequence of
words, so the CYK cells are organized over words.
This algorithm consists of two steps: initialization
and parsing. The first step is to initialize the CYK
cells, whose span size is one, with POS tags pro-
duced by a POS tagger or defined by the input
string1. For example, the top line in Figure 3(a)
is initialized with a series of POS tags in 1-best
segmentation. The second step is to search for the
best syntactic tree under a context-free grammar.
For example, the tree composed by the solid lines
in Figure 5(a) shows the parsing tree for the 1-best
segmentation and POS tagging results.
3.2 Lattice Parsing
The main differences of our lattice parsing in Fig-
ure 3(b) from conventional approach are listed in
following: First, the CYK cells are organized over
characters rather than words. Second, in the ini-
tialization step, we only initialize the cells with
all edges L in the lattice. Take the edge (7, 9,
NN) in Figure 2 for example, the corresponding
cell should be (7, 9), then we add a leaf node
v = NN7,9 with a word ta?olu`n. The final initial-
ization is shown in Figure 3(b), which shows that
1For simplicity, we assume the input of a parser is a seg-
mentation and POS tagging result
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC NR VV NN NN
NP VPB
IP
O(n3w)
(a): Parsing over 1-best segmentation
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR
CC,P
NR
VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3)
(b): Parsing over characters
0 Bu` 1 sh?? 2 yu? 3Sha? 4lo?ng 5 ju? 6x??ng 7ta?o 8 lu`n 9
NR CC,P NR VV
VV NN NN
NN
NP VPB
IP
PP
VP
O(n3r)
(c): Parsing over most-refined segmentation
Figure 3: CKY parsing charts (a): Conventional
parsing over 1-best segmentation. (b): Lattice
parsing over characters of input sentence. (c): Lat-
tice parsing over most-refined segmentation of lat-
tice. nw and nr denotes the number of tokens over
the 1-best segmentation and the most-refined seg-
menation respectively, and nw ? nr ? n.
lattice parsing can initialize the cells, whose span
size is larger than one. Third, in the deduction step
of the parsing algorithm i, j, k are the indexes be-
tween characters rather than words.
We formalize our lattice parser as a deductive
proof system (Shieber et al, 1994) in Figure 4.
Following the definitions of the previous Sec-
839
tion, given a set of edges L of a lattice for an in-
put sentence C = c0..cn?1 and a PCFG grammar:
a 4-tuple ?N, ?, P, S?, where N is a set of non-
terminals, ? is a set of terminal symbols, P is a
set of inference rules, each of which is in the form
of X ? ? : p for X ? N , ? ? (N ? ?)? and p is
the probability, and S ? N is the start symbol. The
deductive proof system (Figure 4) consists of ax-
ioms, goals and inference rules. The axioms are
converted by edges in L. Take the (5, 7, NN) as-
sociated with a weight p1 for example, the corre-
sponding axiom is NN ? ta?olu`n : p1. All axioms
converted from the lattice are shown in Figure 3(b)
exclude the italic non-terminals. Please note that
all the probabilities of the edges L in a lattice are
taken into account in the parsing step. The goals
are the recognition X0,n ? S of the whole sen-
tence. The inference rules are the deductions in
parsing. Take the deduction (*) for example, it will
prove a new item NP0,5 (italic NP in Figure 3(b))
and generate a new hyper-edge e? (in Figure 5(b)).
So the parsing algorithm starts with the axioms,
and then applies the inference rules to prove new
items until a goal item is proved. The final whole
forest for the input lattice (Figure 2) is shown in
Figure 5(b). The extra hyper-edges of lattice-forest
are highlighted with dashed lines, which can in-
ference the input sentence correctly. For example:
?yu?? is tagged into P rather than CC.
3.3 Faster Parsing with Most-refined Lattice
However, our statistics show that the average num-
ber of characters n in a sentence is 1.6 times than
the number of words nw in its 1-best segmenta-
tion. As a result, the parsing time over the charac-
ters will grow more than 4 times than parsing over
the 1-best segmentation, since the time complexity
is O(n3). In order to alleviate this problem, we re-
duce the parsing time by using most-refined seg-
mentation for a lattice, whose number of tokens
is nr and has the property nw ? nr ? n.
Given a lattice with its edges L over indexes
(0, .., n), a index i is a split point, if and only if
there exists some edge (i, j, X) ? L or (k, i, X) ?
L. The most-refined segmentation, or ms for
short, is the segmentation result by using all split
points in a lattice. For example, the corresponding
ms of the example is ?Bu`sh?? yu? Sha?lo?ng ju? x??ng
ta?o lu`n? since points 1 and 4 are not split points.
Item form: Xi,j
Axioms: Xi,j : p(i, j, X)
(i, j, X) ? L
Infer. rules:
Xi,k : p1 Yk,j : p2
Zi,j : pp1p2
Z ? XY : p ? P
Goals: X0,n
Figure 4: Lattice parsing as deductive proof sys-
tem. The i, j, k are the indexes between characters.
Figure 3(c) shows the CKY parsing cells over
most-refined segmentation, the average number
of tokens nr is reduced by combining columns,
which are shown with red dashed boxes. As a re-
sult, the search space is reduced without losing any
derivations. Theoretically, the parsing over fs will
speed up in O((n/nr)3). And our experiments in
Section 6 show the efficiency of our new approach.
It turns out that the parsing algorithm developed
in lattice-parsing Section 3.2 can be used here
without any change. The non-terminals inducted
are also shown in Figure 3(c) in italic style.
4 Rule Extraction with Lattice & Forest
We now explore the extraction algorithm from
aligned source lattice-forest and target string2,
which is a tuple ?F, ?, a? in Figure 5(b). Following
Mi and Huang (2008), we extract minimal rules
from a lattice-forest also in two steps:
(1) frontier set computation
(2) fragmentation
Following the algorithms developed by Mi and
Huang (2008) in Algorithm 1, all the nodes in
frontier set (fs) are highlighted with gray in Fig-
ure 5(b).
Our process of fragmentation (lines 1- 13) is
to visit each frontier node v and initial a queue
(open) of growing fragments with a pair of empty
fragment and node v (line 3). Each fragment is as-
sociated with a list of expansion sites (front) being
2For simplicity and consistency, we use character-based
lattice-forest for the running example. The ?Bu`? and ?sh???
are aligned to the same word ?Bush?. In our experiment,
we use most-refined segmentation to run lattice-parsing and
word alignment.
840
IP0,9
NP0,5 VPB5,9
(a)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
.NR0,2 .CC2,3 .NR3,5 .VV5,6 .NN6,8 .NN8,9
e?
IP0,9
NP0,5 VP2,9
PP2,5 VPB5,9
(b)
0 1 2 3 4 5 6 7 8 9.Bu` .sh?? .yu? .Sha? .lo?ng .ju? .x??ng .ta?o .lu`n
. NR0,2 . CC2,3 . NR3,5 .VV5,6 .NN6,8 .NN8,9. VV5,7 . NN7,9. P2,3
e?
Bush held a discussion with Sharon
Forest only (Minimal rules) Lattice & forest (Extra minimal rules)
(c)
IP(NP(x1:NR x2:CC x3:NR) x4:VPB) IP(x1:NR x2:VP) ? x1 x2
? x1 x4 x2 x3 VP(x1:PP x2:VPB) ? x2 x1
CC(yu?) ?with PP(x1:P x2:NR) ? x1 x2
NR(Sha?lo?ng) ?Sharon P(yu?) ?with
NR(Bu`sh??) ?Bush VPB(x1:VV x2:NN) ? x1 x2
VPB(VV(ju?) NN(x??ngta?o) NN(lu`n)) VV(ju?x??ng) ?held
?held a discussion NN(ta?olu`n) ?a discussion
Figure 5: (a): The parse forest over the 1-best segmentation and POS tagging result. (b): Word-aligned
tuple ?F, ?, a?: the lattice-forest F , the target string ? and the word alingment a. The solid hyperedges
form the forest in (a). The dashed hyperedges are the extra hyperedges introduced by the lattice-forest.
(c): The minimal rules extracted on forest-only (left column), and the extra minimal rules extracted on
lattice-forest (right column).
the subset of leaf nodes of the current fragment
that are not in the fs except for the initial node
v. Then we keep expanding fragments in open in
following way. If current fragment is complete,
whose expansion sites is empty, we extract rule
corresponding to the fragment and its target string
841
Code 1 Rule Extraction (Mi and Huang, 2008).
Input: lattice-forest F , target sentence ? , and
alignment a
Output: minimal rule set R
1: fs ? FROSET(F, ?, a)  frontier set
2: for each v ? fs do
3: open ? {??, {v}?}  initial queue
4: while open 6= ? do
5: ?frag , front? ? open.pop()
6: if front = ? then  finished?
7: generate a rule r using frag
8: R.append(r)
9: else  incomplete: further expand
10: u ? front .pop()  expand frontier
11: for each e ? IN (u) do
12: f ? front ? (tails(e) \ fs)
13: open .append(?frag ? {e}, f ?)
(line 7) . Otherwise we pop one expansion node
u to grow and spin-off new fragments by IN (u),
adding new expansion sites (lines 11- 13), until all
active fragments are complete and open queue is
empty.
The extra minimal rules extracted on lattice-
forest are listed at the right bottom of Figure 5(c).
Compared with the forest-only approach, we can
extract smaller and more general rules.
After we get al the minimal rules, we com-
pose two or more minimal rules into composed
rules (Galley et al, 2006), which will be used in
our experiments.
For each rule r extracted, we also assign a frac-
tional count which is computed by using inside-
outside probabilities:
c(r) =
?(root(r)) ? P(lhs(r)) ? Qv?yield(root(r)) ?(v)
?(TOP) ,
(1)
where root(r) is the root of the rule, lhs(r) is
the left-hand-side of rule, rhs(r) is the right-
hand-side of rule, P(lhs(r)) is the product of
all probabilities of hyperedges involved in lhs(r),
yield(root(r)) is the leave nodes, TOP is the root
node of the forest, ?(v) and ?(v) are outside and
inside probabilities, respectively.
Then we compute three conditional probabili-
ties for each rule:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(2)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(3)
P(r | root(r)) = c(r)?
r?:root(r?)=root(r) c(r?)
. (4)
All these probabilities are used in decoding step
(Section 5). For more detail, we refer to the algo-
rithms of Mi and Huang (2008).
5 Decoding with Lattice & Forest
Given a source-side lattice-forest F , our decoder
searches for the best derivation d? among the set of
all possible derivation D, each of which converts
a tree in lattice-forest into a target string ? :
d? = argmax
d?D,T?F
P (d|T )?0 ? e?1|d|
? LM(?(d))?2 ? e?3|?(d)|,
(5)
where |d| is the penalty term on the number of
rules in a derivation, LM(?(d)) is the language
model and e?3|?(d)| is the length penalty term on
target translation. The P (d|T ) decomposes into
the product of rule probabilities P (r), each of
which is decomposed further into
P (d|T ) =
?
r?d
P (r). (6)
Each P (r) in Equation 6 is decomposed further
into the production of five probabilities:
P(r) = P(r|lhs(r))?4
? P(r|rhs(r))?5
? P(r|root(lhs(r))?6
? Plex(lhs(r)|rhs(r))?7
? Plex(rhs(r)|lhs(r))?8 ,
(7)
where the last two are the lexical probabilities be-
tween the terminals of lhs(r) and rhs(r). All the
weights of those features are tuned by using Min-
imal Error Rate Training (Och, 2003).
Following Mi et al (2008), we first convert the
lattice-forest into lattice translation forest with the
conversion algorithm proposed byMi et al (2008),
842
and then the decoder finds the best derivation on
the lattice translation forest. For 1-best search, we
use the cube pruning technique (Chiang, 2007;
Huang and Chiang, 2007) which approximately
intersects the translation forest with the LM. For
k-best search after getting 1-best derivation, we
use the lazy Algorithm 3 of Huang and Chiang
(2005) to incrementally compute the second, third,
through the kth best alternatives.
For more detail, we refer to the algorithms of
Mi et al (2008).
6 Experiments
6.1 Data Preparation
Our experiments are on Chinese-to-English trans-
lation. Our training corpus is FBIS corpus with
about 6.9M/8.9M words in Chinese/English re-
spectively.
We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
Kneser-Ney smoothing on the first 1/3 of the Xin-
hua portion of Gigaword corpus.
We use the 2002 NIST MT Evaluation test set
as development set and the 2005 NIST MT Eval-
uation test set as test set. We evaluate the trans-
lation quality using the case-insensitive BLEU-4
metric (Papineni et al, 2002). We use the standard
MERT (Och, 2003) to tune the weights.
6.1.1 Baseline Forest-based System
We first segment the Chinese sentences into the
1-best segmentations using a state-of-the-art sys-
tem (Jiang et al, 2008a), since it is not necessary
for a conventional parser to take as input the POS
tagging results. Then we parse the segmentation
results into forest by using the parser of Xiong et
al. (2005). Actually, the parser will assign multiple
POS tags to each word rather than one. As a result,
our baseline system has already postponed the
POS tagging disambiguition to the decoding step.
Forest is pruned by using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold are pf = 5 and pf = 10 at
rule extraction and decoding steps respectively.
We word-align the strings of 1-best segmenta-
tions and target strings with GIZA++ (Och and
Ney, 2000) and apply the refinement method
?grow-diag-final-and? (Koehn et al, 2003) to get
the final alignments. Following Mi and Huang
(2008) and Mi et al (2008), we also extract rules
from forest-string pairs and translate forest to
string.
6.1.2 Lattice-forest System
We first segment and POS tag the Chinese sen-
tences into word lattices using the same sys-
tem (Jiang et al, 2008a), and prune each lat-
tice into a reasonable size using the marginal
probability-based pruning algorithm.
Then, as current GIZA++ (Och and Ney, 2000)
can only handle alignment between string-string
pairs, and word-alingment with the pairs of Chi-
nese characters and target-string will obviously re-
sult in worse alignment quality. So a much better
way to utilize GIZA++ is to use the most-refined
segmentation for each lattice instead of the char-
acter sequence. This approach can be viewed as a
compromise between character-string and lattice-
string word-alignment paradigms. In our exper-
iments, we construct the most-refined segmen-
tations for lattices and word-align them against
the English sentences. We again apply the refine-
ment method ?grow-diag-final-and? (Koehn et al,
2003) to get the final alignments.
In order to get the lattice-forests, we modi-
fied Xiong et al (2005)?s parser into a lattice
parser, which produces the pruned lattice forests
for both training, dev and test sentences. Finally,
we apply the rule extraction algorithm proposed in
this paper to obtain the rule set. Both lattices and
forests are pruned using a marginal probability-
based pruning algorithm similar to Huang (2008).
The pruning threshold of lattice is pl = 20 at both
the rule extraction and decoding steps, the thresh-
olds for the latice-forests are pf = 5 and pf = 10
at rule extraction and decoding steps respectively.
6.2 Results and Analysis
Table 1 shows results of two systems. Our lattice-
forest (LF) system achieves a BLEU score of
29.65, which is an absolute improvement of 0.9
points over the forest (F) baseline system, and the
improvement is statistically significant at p < 0.01
using the sign-test of Collins et al (2005).
The average number of tokens for the 1-best
and most-refined segmentations are shown in sec-
ond column. The average number of characters
is 46.7, which is not shown in Table 1. Com-
843
Sys Avg # of Rules BLEU
tokens links All dev&tst
F 28.7 35.1 29.6M 3.3M 28.75
LF 37.1 37.1 23.5M 3.4M 29.65
Table 1: Results of forest (F) and lattice-forest
(LF) systems. Please note that lattice-forest system
only extracts 23.5M rules, which is only 79.4% of
the rules extracted by forest system. However, in
decoding step, lattice-forest system can use more
rules after filtered on dev and test sets.
pared with the characters-based lattice parsing, our
most-refined lattice parsing speeds up parsing by
(37.1/46.7)3 ? 2 times, since parsing complexity
is O(n3).
More interestingly, our lattice-forest model only
extracts 23.5M rules, which is 79.4% percent of
the rules extracted by the baseline system. The
main reason lies in the larger average number
of words for most-refined segmentations over lat-
tices being 37.1 words vs 28.7 words over 1-best
segmentations. With much finer granularity, more
word aligned links and restrictions are introduced
during the rule extraction step by GIZA++. How-
ever, more rules can be used in the decoding step
for the lattice-forest system, since the lattice-forest
is larger than the forest over 1-best segmentation.
We also investigate the question of how often
the non 1-best segmentations are picked in the fi-
nal translation. The statistic on our dev set sug-
gests 33% of sentences choose non 1-best segmen-
tations. So our lattice-forest model can do global
search for the best segmentation and parse-tree to
direct the final translation. More importantly, we
can use more translation rules in the translation
step.
7 Related Works
Compactly encoding exponentially many deriva-
tions, lattice and forest have been used in some
previous works on SMT. To alleviate the prob-
lem of parsing error in 1-best tree-to-string trans-
lation model, Mi et al (2008) first use forest to
direct translation. Then Mi and Huang (2008) use
forest in rule extraction step. Following the same
direction, Liu et al (2009) use forest in tree-
to-tree model, and improve 1-best system by 3
BLEU points. Zhang et al (2009) use forest in
tree-sequence-to-string model and also achieve a
promising improvement. Dyer et al (2008) com-
bine multiple segmentations into word lattice and
then use lattice to direct a phrase-based transla-
tion decoder. Then Dyer (2009) employ a single
Maximum Entropy segmentation model to gen-
erate more diverse lattice, they test their model
on the hierarchical phrase-based system. Lattices
and forests can also be used in Minimal Error
Rate Training and Minimum Bayes Risk Decod-
ing phases (Macherey et al, 2008; Tromble et al,
2008; DeNero et al, 2009; Kumar et al, 2009; Li
and Eisner, 2009). Different from the works listed
above, we mainly focus on how to combine lattice
and forest into a single tree-to-string system.
8 Conclusion and Future Work
In this paper, we have proposed a lattice-forest
based model to alleviate the problem of error prop-
agation in traditional single-best pipeline frame-
work. Unlike previous works, which only focus on
one module at a time, our model successfully in-
tegrates lattice into a state-of-the-art forest tree-to-
string system. We have explored the algorithms of
lattice parsing, rule extraction and decoding. Our
model postpones the disambiguition of segmenta-
tion and parsing into the final translation step, so
that we can make a more global decision to search
for the best segmentation, parse-tree and transla-
tion in one step. The experimental results show
that our lattice-forest approach achieves an abso-
lute improvement of +0.9 points in term of BLEU
score over a state-of-the-art forest-based model.
For future work, we would like to pay more
attention to word alignment between lattice pairs
and forest pairs, which would be more principled
than our current method of word alignment be-
tween most-refined segmentation and string.
Acknowledgement
We thank Steve DeNeefe and the three anony-
mous reviewers for comments. The work is sup-
ported by National Natural Science Foundation
of China, Contracts 90920004 and 60736014,
and 863 State Key Project No. 2006AA010108
(H. M and Q. L.), and in part by DARPA GALE
Contract No. HR0011-06-C-0022, and DARPA
under DOI-NBC Grant N10AP20031 (L. H and
H. M).
844
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan, June.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of ACL/IJCNLP.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, June.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, Sydney,
Australia, July.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL-08: HLT.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling
2008.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133, Edmon-
ton, Canada, May.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the ACL/IJCNLP 2009.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of EMNLP, pages 40?51, Singapore,
August. Association for Computational Linguistics.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation.
In Proceedings of EMNLP 2008.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206?214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
ACL, pages 311?318, Philadephia, USA, July.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1994. Principles and implementation of de-
ductive parsing.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP 2008.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: N-best
alignments and parses in MT training. In Proceed-
ings of AMTA.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
845
Coling 2010: Poster Volume, pages 1185?1193,
Beijing, August 2010
Dependency-Based Bracketing Transduction Grammar
for Statistical Machine Translation
Jinsong Su, Yang Liu, Haitao Mi, Hongmei Zhao, Yajuan Lu?, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{sujinsong,yliu,htmi,zhaohongmei,lvyajuan,liuqun}@ict.ac.cn
Abstract
In this paper, we propose a novel
dependency-based bracketing transduc-
tion grammar for statistical machine
translation, which converts a source sen-
tence into a target dependency tree. Dif-
ferent from conventional bracketing trans-
duction grammar models, we encode tar-
get dependency information into our lex-
ical rules directly, and then we employ
two different maximum entropy models
to determine the reordering and combi-
nation of partial dependency structures,
when we merge two neighboring blocks.
By incorporating dependency language
model further, large-scale experiments on
Chinese-English task show that our sys-
tem achieves significant improvements
over the baseline system on various test
sets even with fewer phrases.
1 Introduction
Bracketing transduction grammar (BTG) (Wu,
1995) is an important subclass of synchronous
context free grammar, which employs a special
synchronous rewriting mechanism to parse paral-
lel sentence of both languages.
Due to the prominent advantages such as the
simplicity of grammar and the good coverage of
syntactic diversities in different language pairs,
BTG has attracted increasing attention in statis-
tical machine translation (SMT). In flat reorder-
ing model (Wu, 1996; Zens et al, 2004), which
assigns constant reordering probabilities depend-
ing on the language pairs, BTG constraint proves
to be very effective for reducing the search space
of phrase reordering. To pursue a better method
to predict the order between two neighboring
blocks1, Xiong et al (2006) present an enhanced
BTG with a maximum entropy (ME) based re-
ordering model. Along this line, source-side syn-
tactic knowledge is introduced into the reorder-
ing model to improve BTG-based translation (Se-
tiawan et al, 2007; Zhang et al, 2007; Xiong et
al., 2008; Zhang and Li, 2009). However, these
methods mainly focus on the utilization of source
syntactic knowledge, while ignoring the modeling
of the target-side syntax that directly influences
the translation quality. As a result, how to ob-
tain better translation by exploiting target syntac-
tic knowledge is somehow neglected. Thus, we
argue that it is important to model the target-side
syntax in BTG-based translation.
Recently, modeling syntactic information on
the target side has progressed significantly. De-
pending on the type of output, these models can
be divided into two categories: the constituent-
output systems (Galley et al, 2006; Zhang et
al., 2008; Liu et al, 2009) and dependency-
output systems (Eisner, 2003; Lin, 2004; Ding
and Palmer, 2005; Quirk et al, 2005; Shen et
al., 2008). Compared with the constituent-output
systems, the dependency-output systems provide a
simpler platform to capture the target-side syntac-
tic information, while also having the best inter-
lingual phrasal cohesion properties (Fox, 2002).
Typically, Shen et al (2008) propose a string-to-
dependency model, which integrates the target-
side well-formed dependency structure into trans-
lation rules. With the dependency structure, this
system employs a dependency language model
(LM) to exploit long distance word relations, and
achieves a significant improvement over the hier-
archical phrase-based system (Chiang, 2007). So
1A block is a bilingual phrase without maximum length
limitation.
1185
we think it will be a promising way to integrate the
target-side dependency structure into BTG-based
translation.
In this paper, we propose a novel dependency-
based BTG (DepBTG) for SMT, which represents
translation in the form of dependency tree. Ex-
tended from BTG, our grammars operate on two
neighboring blocks with target dependency struc-
ture. We integrate target syntax into bilingual
phrases and restrict target phrases to the well-
formed structures inspired by (Shen et al, 2008).
Then, we adopt two ME models to predict how to
reorder and combine partial structures into a target
dependency tree, which gives us access to captur-
ing the target-side syntactic information. To the
best of our knowledge, this is the first effort to
combine the translation generation with the mod-
eling of target syntactic structure in BTG-based
translation.
The remainder of this paper is structured as fol-
lows: In Section 2, we give brief introductions to
the bases of our research: BTG and dependency
tree. In Section 3, we introduce DepBTG in detail.
In Section 4, we further illustrate how to create
two ME models to predict the reordering and de-
pendency combination between two neighboring
blocks. Section 5 describes the implementation
of our decoder. Section 6 shows our experiments
on Chinese-English task. Finally, we end with a
summary and future research in Section 7.
2 Background
2.1 BTG
BTG is a special case of synchronous context free
grammar. There are three rules utilized in BTG:
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
where the reordering rules (1) and (2) are used
to merge two neighboring blocks A1 and A2 in
a straight or inverted order, respectively. The lex-
ical rule (3) is used to translate the source phrase
x into the target phrase y.
 



	




	
 
		

	
 


	


 








Figure 1: The dependency tree for sentence The
UN will provide abundant financial aid to Haiti
next week.
2.2 Dependency Tree
In a given sentence, each word depends on a par-
ent word, except for the root word. The depen-
dency tree for a given sentence reflects the long
distance dependency and grammar relations be-
tween words. Figure 1 shows an example of a de-
pendency tree, where a black arrow points from a
child word to its parent word.
Compared with constituent tree, dependency
tree directly models semantic structure of a sen-
tence in a simpler form. Thus, it provides a desir-
able platform for us to utilize the target-side syn-
tactic knowledge.
3 Dependency-based BTG
3.1 Grammars
In this section, we extend the original BTG into
DepBTG. The rules of DepBTG, which derive
from that of BTG, merge blocks with target de-
pendency structure into a larger one. These rules
take the following forms:
Ad ? [A1d, A2d]CC (4)
Ad ? [A1d, A2d]LA (5)
Ad ? [A1d, A2d]RA (6)
Ad ? ?A1d, A2d?CC (7)
Ad ? ?A1d, A2d?LA (8)
Ad ? ?A1d, A2d?RA (9)
Ad ? x/y (10)
where A1d and A2d represent two neighboring
blocks with target dependency structure. Rules
(4)?(9) are used to determine the reordering and
combination of two dependency structures, when
1186

Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1103?1113, Dublin, Ireland, August 23-29 2014.
A Dependency Edge-based Transfer Model for Statistical Machine
Translation
Hongshen Chen
?
? Jun Xie
?
Fandong Meng
?
? Wenbin Jiang
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{chenhongshen,xiejun,mengfandong,jiangwenbin}@ict.ac.cn
?CNGL, School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Previous models in syntax-based statistical machine translation usually resort to some kinds
of synchronous procedures, few of these works are based on the analysis-transfer-generation
methodology. In this paper, we present a statistical implementation of the analysis-transfer-
generation methodology in rule-based translation. The procedures of syntax analysis, syntax
transfer and language generation are modeled independently in order to break the synchronous
constraint, resorting to dependency structures with dependency edges as atomic manipulating
units. Large-scale experiments on Chinese to English translation show that our model exhibits
state-of-the-art performance by significantly outperforming the phrase-based model. The statis-
tical transfer-generation method results in significantly better performance with much smaller
models.
1 Introduction
Researches in statistical machine translation have been flourishing in recent years. Statistical translation
methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002;
Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004;
Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004;
Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and
phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher
generalization capability by leveraging the hierarchical structures in natural languages, and achieve the
state-of-the-art performance in these years.
Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation
procedures which directly model the structural correspondence between two languages. In contrast,
the analysis-transfer-generation methodology in rule-based translation solves the machine translation
problem in a more divided scheme, where the processing procedures of analysis, structural transfer and
language generation are modeled separately. The analysis-transfer-generation strategy can tolerate higher
non-isomorphism between languages if with a more general transformation unit and it can facilitate
elaborating engineering of each processing procedure, however, there isn?t a statistical transfer model
that shows the comparable performance with the current state-of-the-art SMT model so far.
In this paper, we propose a novel statistical analysis-transfer-generation model for machine transla-
tion, to integrate the advantages of the transfer-generation scheme and the statistical modeling. The
procedures of transfer and generation are modeled on dependency structures with dependency edges
as atomic manipulating units. First, the source sentence is parsed by a dependency parser. Then, the
source dependency structure is transferred into a target structure by translation rules, which composed
of the source and target edges. Last, the target sentence is finally generated from the target edges which
are used as intermediate syntactic structures. By directly modeling the edge, the most basic unit in the
dependency tree, which definitely describe the modifying relationship and positional relation between
words, our model alleviates the non-isomorphic problem and shows the flexibility of reordering.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1103
E
?ob?m?j?nti?nji?n?f?b? ?nqu?nzh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
*:sh?n?m?n?
nsubj advmod
dobj
f?b?
f?b? sh?n?m?n?
?nqu?n
nn
obama 
issue
will
issue
issue
*
sh?n?m?n?
zh?nlu?
nn
a statement of 
security
a statement of 
strategy
f?b?/VV
??
ji?n?/AD
?
sh?n?m?n?/NN
??
?nqu?n/NN
??
zh?nlu?/NN
??
nsubj
advmod
dobj
nn nn
?ob?m?/NN
???
obama today will issue a statement of security strategy
D ? ? ?
? ?
j?nti?n/NT
??
tmod
j?nti?n
tomod
f?b?
today
issue
?
??????????????
Figure 1: (a)An example of labeled Chinese dependency tree aligned with the corresponding English
sentence. (b) Examples of the transfer rules extracted from the tree. ?*? denotes a variable. All the inner
nodes are treated as variables. The label on the target side of a rule denotes whether the head and the
dependent are adjacent or not.
The rest of the paper is organized as follows, we first describe the dependency edge-based transfer
model (Section 2). Then, we present our rule acquisition algorithm (Section 3), the decoding and target
sentence generation process (Section 4). Finally, large-scale experiments (Section 5) on Chinese-to-
English translation show that our edge-based transfer model gains state-of-the-art performance by sig-
nificantly outperforming the phrase-based model (Koehn et al., 2003) by averaged +1.34 BLEU points on
three test sets. To the best of our knowledge, this is the first transfer-generation-based statistical machine
translation model that achieves the state-of-the-art performance.
2 Dependency Edge-based Transfer Model
2.1 Edges in Dependency Trees
Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes.
An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency
relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a
subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), ?`aob?am?a?
modifies ?f?ab`u?. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a
noun phrase acts as the subject of a clause. ?`aob?am?a? is on the left of ?f?ab`u?.
Based on the above observations, we take the edge as the elementary structure of a dependency tree
and regard a dependency tree to be a set of edges.
Definition 1. An source side edge is a 4-tuple e = ?H,D,P,R?, where H is the head, D is the depen-
dent, P denotes the relative position between H and D, left or right, R is the grammatical relation label
.
In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency
tree.
1104
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
nsubj
advmod
dobj
?ob?m?/NN j?nti?n/NT
tmod
Transfer
select the left
adjacent edge
? ? ? ?
extend to the left
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
? ? ?
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
?? ?
H1:obama today will issue
H2:today obama will issue
extend to the right
*
adjacent
? ? ? ?
a statement of security strategy
ad
jac
en
t
issue
?
will
will issue
will
ad
jac
en
t
obama
issue
non-
adjac
ent
today
non
-ad
jac
ent
? ? ?
will
ad
jac
en
t
obama
issue
non-
adja
cent
today
non
-ad
jac
ent
?? ?
H1:obama today will issue a statement of security strategy
H2:today obama will issue a statement of security strategy
?
adjacent
?
*
Generation
Analysis
?ob?m? j?nti?n ji?n? f?b? ?nqu?n zh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
nsubj advmod
f?b?
obama
issue
will
non
-ad
jace
nt
issue
adj
ace
nt
j?nti?n
tomod
f?b?
today
issue
non
-ad
jace
nt
*:sh?n?m?n?
dobj
f?b?
issue
*
adjacent
Figure 2: An example partial generation of translation. The same set of rules generate two target hy-
potheses with the same words and different word order. Assume the sub-tree rooted at ?sh?engm??ng? has
been translated to the corresponding target sentence fragment.
2.2 Transfer Rules
A transfer rule of our model represents the reordering and relative positions of edges between language
pairs. For example, in Figure 1 (b), the first rule shows that when a nominal subject modifies a verb, the
target side keeps the same position relations. ?obama? is also on the left of ?issue?, the same with the
source side relative position. The 5-th and 6-th rules show the inversion relations between the source and
the target. Formally, a transfer rule can be defined as a triple ?e, f,??, where e is an edge extracted from
the source dependency tree, f is a target edge. ? denotes one-to-one correspondence between variables
in e and f .
Figure 1 (b) are part of transfer rules extracted from the word aligned sentence in Figure 1 (a). The
target edge denotes whether the target dependent is on the left or the right side of the target head, the
1105
label on the edge indicates whether the target head and the target dependent are adjacent or not. If
the dependent is an internal node(contrast with the leaf nodes in the dependency tree), then it will be
regarded as a substitution node. The dependent in the 4-th transfer rule is an internal node and the its
corresponding target side is a substitution variable.
Figure 2 shows a partial transfer-generation of our model which involves three phases. First, analysis.
Given a source language sentence, we obtain its dependency tree using a dependency parser. We assume
that the sub-tree of the substitution node has been translated. Second, transfer. For each internal node,
we transfer the source side edges between the head and all its dependents into the target sides. In the
second block of Figure 2, we transfer four edges into the target sides. Third, generation, corresponding
to the third block of Figure 2. We generate the target sentence with the target side edges starting from the
target head, ?issue?. We first try to concatenate the edges to the left. First, we select a target side edge
that is on the left side of ?issue? and adjacent to it to form a consecutive phrase. Edge 3 is selected and ?to
issue? is generated. Then, we enumerate all possible left concatenations of the other edges that are not
adjacent to ?issue?. The two sequences(1,2,3 and 2,1,3) of the edges are generated, corresponding to the
two hypotheses. After that, we extend the two hypotheses to the right. The internal node ?sh?engm??ng? is
a substitution node, so the candidate translation of the sub-tree rooted at ?sh?engm??ng? is concatenated to
the two hypotheses. Finally, we generate the two candidate translations of the input sentence.
3 Acquisition of Transfer Rules
Transfer rules can be extracted automatically from a word-aligned corpus, which is a set of triples
?T, S,A?, where T is a source dependency tree, S is a target side sentence and A is an alignment relation
between T and S. Following the dependency-to-string model (Xie et al., 2011), we extract transfer rules
from each triple ?T, S,A? by three steps:
1. Tree Annotation: Label each node in the dependency tree with the alignment information
2. Edges Identification: Identify acceptable edges from the annotated dependency tree
3. Rule induction: Induce a set of lexicalized and un-lexicalized transfer rules from the acceptable
edges.
3.1 Tree Annotation
Given a triple ?T, S,A? as Figure 3 shows, we define two attributes for every node in T: node span and
sub-tree span:
Definition 2. Given a node n, its node span nsp(n) is a set of consecutive indexes of the target words
aligned with the node n.
For example, nsp(?anqu?an)={7-8}, which corresponds to the target word ?of? and ?security?.
Definition 3. A node span nsp(n) is consistent if for any other node n
?
in the dependency tree, nsp(n)
and nsp(n
?
) are not overlapping.
For example, nsp(zh`anlu`e) is consistent, while nsp(?anqu?an) is not consistent for it corresponds to the
same word ?of? with nsp(sh?engm??ng).
Definition 4. Given a sub-tree T
?
rooted at n, the sub-tree span tsp(n) of n is a consecutive target word
indexes from the lower bound of the nsp of all the nodes in T
?
to the upper bound of those spans.
For example, tsp(sh?engm??ng)={5-9},which corresponds to the target phrase ?a statement of security
strategy?.
Definition 5. A sub-tree span tsp(n) is consistent if for any other node n
?
that is not in the sub-tree
rooted at n in the dependency tree, tsp(n) and nsp(n
?
) are not overlapping.
For example, tsp(sh?engm??ng) is consistent, even though nsp(sh?engm??ng) is not consistent, while
tsp(?anqu?an) is not consistent for ?sh?engm??ng? is not a node in sub-tree rooted at ??anqu?an? and ??anqu?an?
corresponds to the same word ?of ? with nsp(sh?engm??ng) .
1106
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
?nqu?n/NN zh?nlu?/NN
nsubj advmod dobj
nn nn
?ob?m?/NN
obama today will issue a statement of security strategy
j?nti?n/NT
tmod
1 2 3 4 5 6 7 8 9
{1-1}/{1-1} {2-2}/{2-2} {3-3}/{3-3}
{4-4}/{1-9}
{5-7}/{5-9}
{7-8}/{7-8} {9-9}/{9-9}
Figure 3: An example of annotated dependency tree. Each node is annotated with two spans, the former
is node span and the latter is sub-tree span. The gray edge is not acceptable. It is different from Figure
1, because ??anqu?an? aligned with two words in Figure 3. ?of? in the target side is aligned with both
??anqu?an? and ?sh?engm??ng? which makes the gray edge un-acceptable.
3.2 Acceptable Edges Identification
We identify the edges from the annotated dependency tree that are acceptable for rule induction.
For an acceptable edge, its node span of the head nsp(head) and the sub-tree span of the dependent
tsp(dependent) satisfy the following properties:
1. nsp(head) and tsp(dependent) are consistent.
2. nsp(head) and tsp(dependent) are non-overlapping.
For example, tsp(?anqu?an) and nsp(sh?engm??ng) are neither consistent nor non-overlapping. So the
gray edge between head ?sh?engm??ng? and dependent ??anqu?an? is not an acceptable edge. nsp(f?ab`u)
and tsp(sh?engm??ng) are consistent and the two spans are non-overlapping. Thus, the edge between head
?f?ab`u? and dependent ?sh?engm??ng? is an acceptable edge.
3.3 Transfer Rule Induction
From each acceptable source side edge, we induce a set of lexicalized and un-lexicalized transfer rules.
We induce a lexicalized transfer rule from an acceptable edge by the following procedures:
1. extract the source side edge and mark the internal nodes as substitution sites. This form the input of
a transfer rule.
2. extract the position information according to nsp(head) and tsp(dependent), whether they are adja-
cent or not and whether tsp(dependent) is on the left side or the right side of nsp(head).
In Figure 4, the first transfer rule is lexicalized rule, it is induced from the edge between ?f?ab`u? and
?`aob?am?a?.
In addition to the lexicalized rules described above, we also generalized the rules by replacing the
word in an source side edge with a wild card and the part of speech of the word. For example, the rule
in Figure 4 can be generalized in two ways. The generalized versions of the rule apply to ?`aob?am?a?
modifying any verb and ?f?ab`u? modifying any noun, respectively. The generalized rules are also called
1107
Generalize head
?ob?m?
nsubj
f?b?
obama
issue
non
-adj
acen
t
?ob?m?
nsubj
*:VV
obama
*
non-
adja
cent
Generalize
dependent
*:NN
nsubj
f?b?
*
issue
non-a
djace
nt
?
?
?
Figure 4: Generalization of transfer rule.
un-lexicalized rules for the loss of word information. The single node translations of the generalized
words are also extracted.
The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both
left and right directions. We do this process similar with the method of Och and Ney (2004). We might
obtain m(m ? 1) extended rules from an acceptable edge. The frequency of each rule is divided by m.
We take the extracted rule set as observed data and make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
4 Decoding and Generation
We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each
concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate
the target sentence e. The probability of e is defined as?
P (c) ?
?
i
?
i
(c)
?
i
(1)
where ?
i
(c) are features defined on concatenations and ?
i
are feature weights. In our experiments of
this paper, thirteen features are used as follows:
? Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities
P
lex
(t|s) and P
lex
(s|t);
? Bilingual phrases probabilities P
bp
(t|s) and P
bp
(s|t), and bilingual phrases lexical translation prob-
abilities P
bplex
(t|s) and P
bplex
(s|t);
? Transfer rule penalty exp(?1);
? Bilingual phrase penalty exp(?1);
? Pseudo translation rule penalty exp(?1);
? Target word penalty exp(|e|);
? Language model P
lm
(e).
Our decoder is based on a bottom-up chart-based beam-search algorithm. We regard the decoding
process as the composition of the target side edges. For a given source language sentence, we obtain its
1108
f?b?
j?nti?n?ob?m?
obama today to issue
f?b?
sh?n?m?n?
?nqu?n zh?nlu?
issue a statement of security strategy
ji?n?
Figure 5: Two examples of the phrases incorporated in our model.
dependency tree T with an external dependency parser. Each node in T is traversed in post-order. For
each internal node and root node n, we do the transfer-generation translation as the following procedures:
1. Extract all the source side edges including the lexicalized and generalized edges between n and all
its dependents using the same way we extract the source side edges of the transfer rules.
2. Transfer the source side edges into target side edges. For a generalized rule, we restore it to a lex-
icalized rule by combining it with the single word translation. For no matched edges, we construct
the pseudo translation rule according to the word order of the source head-dependent relation.
3. Generate the target sentence by bi-directional extension from an adjacent target edge. We first
group all the target edges by their heads. For each group, we generate translation hypotheses with
the following procedures:
(a) Select an adjacent target edge as the starting position;
(b) Extend to the left side and enumerate all possible permutations of the target edges directing
left;
(c) Extend to the right side and enumerate all possible permutations of the target edges directing
right.
Considering that in dependency trees, a head may relate to more than 4 edges which results in
massive search space. We reduce the time complexity by using the maximum distortion limit. The
distortion is defined as (a
i
? b
i?1
? 1), where a
i
denotes the start position of the source side edge
that is translated into the ith target side edge and b
i?1
denotes the end position of the source side
edge translated into the (i ? 1)th target side edge.
When we reach the root node, the candidate translations of the input sentence are generated.
In our model, only the adjacent target edge of a transfer rule can be regarded as a consecutive phrase
and its corresponding source side length is only 2. As we start extending the target sentence from
the target head, it is quite natural to incorporate the bilingual phrases to make the target sentences be
extended from the phrases as well as the single target head word. Due to the flexibility of our model,
we can incorporate not only the syntactic phrases which are phrases covering a whole sub-tree, but also
the non-syntactic phrases as the fixed dependency structures in Shen et al. (2008) which are consecutive
phrases covering the head. Figure 5 shows two examples of the phrases incorporated in our model.
We prune the search space in several ways. First, beam threshold ?, items with a score worse than ?
times of the best score in the same span will be discarded; second, beam size b, items with a score worse
than the bth best item will be discarded. For our experiments, we set ? = 10
?3
and b = 300; Third,
we also prune rules for the same edge with a fixed rule limit (r = 200), which denotes the maximum
number of rules we keep.
5 Experiments
In this section, the performance of our model is evaluated by comparing with phrase-based model (Koehn
et al., 2003), on the NIST Chinese-to-English translation tasks. We also present the influence of the
1109
mt02-tune mt03 mt04 mt05
0 30.81 30.03 32.44 30.09
1 33.4 32.07 34.55 31.77
2 34.39 32.7 35.4 32.59
3 34.04 32.69 35.46 32.54
4 33.59 31.75 35.15 32.17
30
31
32
33
34
35
36
0 1 2 3 4
maximum distortion limit
BL
EU
(%
)
mt02-tune
mt03
mt04
mt05
Figure 6: Effect of different maximum distortion limits on development
set (mt02) and three tests(mt03,04,05). The performance of all the sets
are consistent.
maximum distortion limit to our model. We take open source phrase-based system Moses (with default
configuration)
1
as our baseline system.
5.1 Experimental Setting
Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LD-
C2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser
(Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and
edges by dependency labels.
To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions
and apply ?grow-diag-and? refinement (Koehn et al., 2003). We extract the phrases covering no more
than 10 nodes of the fixed structures.
We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the Gigaword corpus.
We use NISTMTEvaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets.
The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric
2
.
We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the
BLEU score of the development set.
The statistical significance test is performed by sign-test (Collins et al., 2005).
5.2 Influence of Maximum Distortion Limit
Figure 6 gives the performance of our system with different maximum distortion limits in terms of
uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on
both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low
distortion limit may cause the target sentence been translated more close to the sequence of the source,
especially when the distortion limit equals to 0, none of the reordering is allowed, while a high distortion
limit may lead the good translations be flooded by too many ambiguities when enumerating the possible
sequences of the target non-adjacent dependents. We choose 2 as the maximum distortion limit in the
next experiments.
1
http://www.statmt.org/moses/
2
ftp://jaguar.ncsl.nist.glv/mt/resources/mteval-v11b.pl.
1110
System Rule # MT03 MT04 MT05 Average
Moses 44.49M 32.03 32.83 31.81 32.22
DEBT 30.7M 32.7* 35.4* 32.59* 33.56
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets.
?DEBT? denotes our edge-based transfer model. The ?*? denotes that the results are significantly better
than the baseline system (p<0.01).
5.3 Performance of Our Model
Tabel 1 illustrates the translation results of our experiments. We (DEBT) surpass the baseline over +1.34
BLEU points on average. Our model significant outperforms the baseline phrase-based model, with
p < 0.01 on statistical significance test sign-test (Collins et al., 2005).
We also list the statistical number of rules extracted from the training corpus. The number of our
transfer rules is only 69.0% of the rules extracted by Moses, thus, the total rules in our model is 31%
smaller than Moses.
6 Related Work
Transfer-based MT systems usually take a parse tree in the source language and translate it into a parse
tree in the target language with transfer rules. Both our model and some of those previous works ac-
quired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al.,
2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used
quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of
dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing.
As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation
problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model
the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source depen-
dency structure into target side by word alignment and faced the problem of non-isomorphism between
languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate
the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that
represent the source side as head-dependents relations and the target side as string.
Differently, our model uses a much simpler elementary structure, edge, which consist of only a head
and a dependent. As a transfer-generation model, we transfer an edge in the source dependency tree into
target side and incorporate the position information on the target edge , which alleviate non-isomorphism
problem and incorporate ordering among different target edges simultaneously. Moreover, our decoding
method is quite different from previous dependency tree-based works. After parsing a given source
language sentence, we transfer and generate the target sentence fragments recursively on each internal
node of the dependency tree bottom-up.
7 Conclusions and Future Work
In this paper, we present a novel dependency edge-based transfer model using dependency trees on the
source side for machine translation. We directly transfer the edges in source dependency tree into the
target sides and then generate the target sentences by beam-search. With the concise transfer rules,
our model is compatible with both the syntactic and non-syntactic phrases. Although the generation
process of our model seems relatively simple, it still exhibits a good performance and outperforms the
phrase-based model on large scale experiments. For the first time, a statistical transfer model shows a
comparable performance with the state-of-the-art translation models.
Since the translation procedure is divided into three phases and each phase can be modeled indepen-
dently, we would like to take further steps focusing on modeling the target language generation process
specifically to ensure a better grammatical translation with the help of natural language generation meth-
ods.
1111
Acknowledgments
The authors were supported by National Key Technology R&D Program (No. 2012BAH39B03), CAS
Action Plan for the Development of Western China (No. KGZD-EW-501), and Sino-Thai Scientific and
Technical Cooperation (No. 60-625J). Sincere thanks to the anonymous reviewers for their thorough
reviewing and valuable suggestions.
References
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Jaime Carbonell, Katharina Probst, Erik Peterson, Christian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt. In Machine Translation: From Research to Real Users,
pages 1?10. Springer.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263?270. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531?540.
Association for Computational Linguistics.
Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for
syntax based statistical mt. In Workshop on Recent Advances in Dependency Grammars (COLING), pages
90?97.
Kevin Gimpel and Noah A Smith. 2009. Feature-rich translation by quasi-synchronous lattice parsing. In Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1,
pages 219?228. Association for Computational Linguistics.
Kevin Gimpel and Noah A Smith. 2014. Phrase dependency machine translation with quasi-synchronous tree-to-
tree features. Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 105?112, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2002. Learning domain-specific transfer rules: an experiment
with korean to english translation. In Proceedings of the 2002 COLING workshop on Machine translation in
Asia-Volume 16, pages 1?7. Association for Computational Linguistics.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 609?616. Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine transla-
tion. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume
10, pages 133?139. Association for Computational Linguistics.
1112
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency
and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1066?1076, Seattle, Washington, USA, October. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL-08: HLT, pages
192?199, Columbus, Ohio, June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 295?302, Stroudsburg, PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Stephen Richardson, William Dolan, Arul Menezes, and Jessie Pinkham. 2001. Achieving commercial-quality
translation with example-based methods. In Proceedings of MT Summit VIII, pages 293?298. Santiago De
Compostela, Spain.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
216?226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical
machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07,
pages 40?47, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th
Annual Meeting on Association for Computational Linguistics, pages 523?530. Association for Computational
Linguistics.
1113
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1133?1143, Dublin, Ireland, August 23-29 2014.
A Structured Language Model for Incremental Tree-to-String Translation
Heng Yu1
1Institute of Computing Technology. CAS
University of Chinese Academy of Sciences
yuheng@ict.ac.cn
Haitao Mi
T.J. Watson Research Center
IBM
hmi@us.ibm.com
Liang Huang
Queens College & Grad. Center
City University of New York
huang@cs.qc.cuny.edu
Qun Liu1,2
2Centre for Next Generation Localisation.
Faculty of Engineering and Computing
Dublin City University
qliu@computing.dcu.ie
Abstract
Tree-to-string systems have gained significant popularity thanks to their simplicity and efficien-
cy by exploring the source syntax information, but they lack in the target syntax to guarantee
the grammaticality of the output. Instead of using complex tree-to-tree models, we integrate
a structured language model, a left-to-right shift-reduce parser in specific, into an incremental
tree-to-string model, and introduce an efficient grouping and pruning mechanism for this integra-
tion. Large-scale experiments on various Chinese-English test sets show that with a reasonable
speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
1 Introduction
Tree-to-string models (Liu et al., 2006; Huang et al., 2006) have made promising progress and gained
significant popularity in recent years, as they run faster than string-to-tree counterparts (e.g. (Galley et
al., 2006)), and do not need binarized grammars. Especially, Huang and Mi (2010) make it much faster
by proposing an incremental tree-to-string model, which generates the target translation exactly in a left-
to-right manner. Although, tree-to-string models have made those progresses, they can not utilize the
target syntax information to guarantee the grammaticality of the output, as they only generate strings on
the target side.
One direct approach to handle this problem is to extend tree-to-string models into complex tree-to-tree
models (e.g. (Quirk et al., 2005; Liu et al., 2009; Mi and Liu, 2010)). However, tree-to-tree approaches
still significantly under-perform than tree-to-string systems due to the poor rule coverage (Liu et al.,
2009) and bi-parsing failures (Liu et al., 2009; Mi and Liu, 2010).
Another potential solution is to use structured language models (Slm) (Chelba and Jelinek, 2000; Char-
niak et al., 2003; Post and Gildea, 2008; Post and Gildea, 2009), as the monolingual Slm has achieved
better perplexity than the traditional n-gram word sequence model. More importantly, the Slm is inde-
pendent of any translation model. Thus, integrating a Slm into a tree-to-string model will not face the
problems that tree-to-tree models have. However, integration is not easy, as the following two questions
arise. First, the search space grows significantly, as a partial translation has a lot of syntax structures.
Second, hypotheses in the same bin may not be comparable, since their syntactic structures may not be
comparable, and the future costs are hard to estimate. Hassan et al. (2009) skip those problems by only
keeping the best parsing structure for each hypothesis.
In this paper, we integrate a shift-reduce parser into an incremental tree-to-string model, and intro-
duce an efficient grouping and pruning method to handle the growing search space and incomparable
hypotheses problems. Large-scale experiments on various Chinese-English test sets show that with a rea-
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1133
sonable speed our method gains an average improvement of 0.7 points in terms of (Ter-Bleu)/2 than a
state-of-the-art tree-to-string system.
2 Linear-time Shift-reduce Parsing
parsing
action signature dependency structure
s1 s0 q0
Bush S 0
sh Bush held S 1: Bush
sh Bush held a S 2: Bush held
re
x
held
Bush
a S 3: Bush held
sh held
Bush
a meeting S 4: Bush held a
sh a meeting with S 5: Bush held a meeting
re
x
held
Bush
meeting
a
with S 6: Bush held a meeting
re
y
held
Bush meeting
with S 7: Bush held a meeting
sh held
Bush meeting
with Sharon S 8: Bush held a meeting with
sh with Sharon S 9: Bush held a meeting with Sharon
re
y
held
Bush meeting
with
Sharon
S 10: Bush held a meeting with Sharon
re
y
held
Bush meeting with
S 11: Bush held a meeting with Sharon
Figure 1: Linear-time left-to-right dependency parsing.
A shift-reduce parser performs a left-to-right scan of the input sentence, and at each parsing step,
chooses one of two parsing actions: either shift (sh) the current word onto the stack, or reduce (re)
the top two (or more) items at the end of the stack (Aho and Ullman, 1972). In the dependency parsing
scenario, the reduce action is further divided into two cases: left-reduce (re
x
) and right-reduce (re
y
),
depending on which one of the two items becomes the head after reduction. Each parsing derivation can
be represented by a sequence of parsing actions.
1134
2.1 Shift-reduce Dependency Parsing
We will use the following sentence as the running example:
Bush held a meeting with Sharon
Given an input sentence e, where ei is the ith token, ei...e j is the substring of e from i to j, a shift-reduce
parser searches for a dependency tree with a sequence of shift-reduce moves (see Figure 1). Starting
from an initial structure S 0, we first shift (sh) a word e1, ?Bush?, onto the parsing stack s0, and form a
structure S 1 with a singleton tree. Then e2, ?held?, is shifted, and there are two or more structures in the
parsing stack, we can use re
x
or re
y
step to combine the top two trees on the stack, replace them with
dependency structure e1 x e0 or e1 y e0 (shown as S 3), and add one more dependency edge between
e0 and e1.
Note that the shade nodes are exposed heads on which re
x
or re
y
parsing actions can be performed.
The middle columns in Figure 1 are the parsing signatures: q0 (parsing queue), s0 and s1 (parsing stack),
where s0 and s1 only have one level dependency. Take the line of S 11 for example, ?a? is not in the
signature. As each action results in an update of cost, we can pick the best one (or few, with beam) after
each action. Costs are accumulated in each step by extracting contextual features from the structure and
the action. As the sentence gets longer, the number of partial structures generated at each steps grows
exponentially, which makes it impossible to search all of the hypothesis. In practice, we usually use beam
search instead.
(a) atomic features
s0.w s0.t
s1.w s1.t
s0.lc.t s0.rc.t
q0.w q0.t
(b) feature templates
unigram
s0.w s0.t s0.w ? s0.t
s1.w s1.t s1.w ? s1.t
q0.w q0.t q0.w ? q0.t
bigram
s0.w ? s1.w s0.t ? s1.t
s0.t ? q0.t s0.w ? s0.t ? s1.t
s0.w ? s1.w ? s1.t s0.t ? s1.w ? s1.t
s0.w ? s0.t ? s1.w
trigram s0.t ? s1.t ? q0.t s1.t ? s0.t ? s0.lc.t
s1.t ? s0.t ? q0.t s1.t ? s0.t ? s0.rc.t
(c) ?? parsing stack parsing queue ??
. . . s1 s0
s0.lc ? ? ? s0.rc
q0
Table 1: (a) atomic features, used for parsing signatures. (b): parsing feature templates, adapted from
Huang and Sagae (2010). x.w and x.t denotes the root word and POS tag of the partial dependency tree,
x.lc and x.rc denote x?s leftmost and rightmost child respectively. (c) the feature window.
2.2 Features
We view features as ?abstractions? or (partial) observations of the current structure. Feature templates f
are functions that draw information from the feature window, consisting of current partial tree and first
word to be processed. All Feature functions are listed in Table 1(b), which is a conjunction of atomic
1135
IP
NP
Bu`sh??
VP
PP
P
yu?
NP
Sha?lo?ng
VP
VV
ju?x??ng
AS
le
NP
hu?`ta?n
Figure 2: A parse tree
features in Table 1(a). To decide which action is the best of the current structure, we perform a three-way
classification based on f, and conjoin these feature instances with each action:
[f ? (action=sh/re
x
/re
y
)]
We extract all the feature templates from training data, and use the average perceptron algorithm and
early-update strategy (Collins and Roark, 2004; Huang et al., 2012) to train the model.
3 Incremental Tree-to-string Translation with Slm
The incremental tree-to-string decoding (Huang and Mi, 2010) performs translation in two separate steps:
parsing and decoding. A parser first parses the source language input into a 1-best tree in Figure 2, and
the linear incremental decoder then searches for the best derivation that generates a target-language string
in strictly left-to-right manner. Figure 3 works out the full running example, and we describe it in the
following section.
3.1 Decoding with Slm
Since the incremental tree-to-string model generates translation in strictly left-to-right fashion, and the
shift-reduce dependency parser also processes an input sentence in left-to-right order, it is intuitive to
combine them together. The last two columns in Figure 3 show the dependency structures for the corre-
sponding hypotheses. Start at the root translation stack with a dot  before the root node IP:
[ IP ],
we first predict (pr) with rule r1,
(r1) IP (x1:NP x2:VP)? x1 x2,
and push its English-side to the translation stack, with variables replaced by matched tree nodes, here
x1 for NP and x2 for VP. Since this translation action does not generate any translation string, we don?t
perform any dependency parsing actions. So we have the following translation stack
[ IP ][ NP VP],
where the dot  indicates the next symbol to process in the English word-order. Since node NP is the next
symbol, we then predict with rule r2,
(r2) NP(Bu`sh??)? Bush,
and add it to the translation stack:
[ IP ] [ NP VP ] [ Bush]
Since the symbol right after the dot in the top rule is a word, we scan (sc) it, and append it to the current
translation, which results in the new translation stack
[ IP ] [ NP VP ] [Bush  ]
1136
translation parsing
stack string dependency structure Slm
[  IP ] S 0
1 pr [  IP ] [  NP VP] S 0
2 pr [  IP ] [ NP VP ] [  Bush ] S 0
3 sc [  IP ] [ NP VP] [Bush  ] Bush S 1: Bush P(Bush | S 0)
4 co [  IP ] [NP  VP] S 1:
5 pr [  IP ] [NP  VP] [ held NP with NP] S 1:
6 sc [  IP ] [NP  VP] [held  NP with NP] held S 3: Bush held P(held | S 1)
7 pr [ IP] [NP VP] [held NP with NP] [ a meeting] S 3
8 sc [ IP] [NP VP] [held  NP with NP] [a meeting  ] a meeting S 7: Bush held a meeting P(a meeting | S 3)
9 co [ IP ] [NP VP] [held NP  with NP] S 7
10 sc [ IP] [NP VP] [held NP with  NP] with S 8: Bush held a meeting with P(with | S 7)
S ?8: Bush held a meeting with P
? (with | S 7)
11 pr [ IP] [NP VP] [held NP with  NP] [ Sharon] S 8
S 8?
12 sc [ IP ] [NP  VP] [held NP with  NP] [Sharon ] Sharon S 11: Bush held a meeting with Sharon P(Sharon | S 8)
S ?11? : Bush held a meeting with Sharon P
? (Sharon | S ?8)
13 co [  IP ] [NP  VP] [held NP with NP ] S 11
14 co [  IP ] [NP VP ] S 11
15 co [ IP  ] S 11
Figure 3: Simulation of the integraton of an Slm into an incremental tree-to-string decoding. The first
column is the line number. The second column shows the translation actions: predict (pr), scan (sc), and
complete (co). S i denotes a dependency parsing structure. The shaded nodes are exposed roots of S i.
Immediately after each sc translation action, our shift-reduce parser is triggered. Here, our parser applies
the parsing action sh, and shift ?Bush? into a partial dependency structure S 1 as a root ?Bush? (shaded
node) in Figure 3. Now the top rule on the translation stack has finished (dot is at the end), so we complete
(co) it, pop the top rule and advance the dot in the second-to-top rule, denoting that NP is completed:
[ IP ] [NP  VP].
Following this procedure, we have a dependency structure S 3 after we scan (sc) the word ?held? and
take a shift (sh) and a left reduce (re
x
) parsing actions. The shaded node ?held? means exposed roots,
that the shift-reduce parser takes actions on.
Following Huang and Mi (2010), the hypotheses with same translation step1 fall into the same bin.
Thus, only the prediction (pr) actions actually make a jump from a bin to another. Here line 2 to 4 fall
into one bin (translation step = 4, as there are 4 nodes, IP, NP, VP and Bu`sh??, in the source tree are
covered). Similarly, lines from 7 to 10 fall into another bin (translation step = 15).
1The step number is defined by the number of tree nodes covered in the source tree, and it is not equal to the number of
translation actions taken so far.
1137
Noted that as we number the bins by the translation step, only pr actions make progress, the sc and
co actions are treated as ?closure? operators in practice. Thus we always do as many sc/co actions as
possible immediately after a pr step until the symbol after the dot is another non-terminal. The total
number of bins is equal to the size of the parse tree, and each hypothesis has a constant number of
outgoing hyper-edges to predict, so the time complexity is linear in the sentence length.
After adding our Slm to this translation, an interesting branch occurs after we scan the word ?with?,
we have two different partial dependency structures S 8 and S
?
8 for the same translation. If we denote
N(S i) as the number of re actions that S i takes, N(S 8) is 3, while N(S ?8) is 4. Here N(S i) does not take
into account the number of sh parsing actions, since all partial structures with same translations should
shift the same number of translations. Thus, N(S i) determines the score of dependency structures, and
only the hypotheses with same N(S i) are comparable to each other. In this case, we should distinguish
S 8 with S
?
8, and if we make a prediction over the hypothesis of S 8, we can reach the correct parsing state
S 11 (shown in the red dashed line in Figure 3).
So the key problem of our integration is that, after each translation step, we will apply different se-
quences of parsing actions, which result in different and incomparable dependency structures with the
same translation. In the following two Sections, we introduce three ways for this integration.
3.2 Na??ve: Adding Parsing Signatures into Translation Signatures
One straightforward approach is to add the parsing signatures (in Figure 1) of each dependency structure
(in Figure 1 and Figure 3) to translation signatures. Here, we only take into account of the s0 and s1 in
the parsing stack, as the q0 is the future word that is not available in translation strings. For example, the
dependency structure S 8 has parsing signatures:
held
Bush meeting
with
We add those information to its translation signature, and only the hypothesis that have same translation
and parsing signatures can be recombined.
So, in each translation bin, different dependency structures with same translation strings are treated as
different hypothesis, and all the hypothesis are sorted and ranked in the same way. For example, S 8 and
S ?8 are compared in the bin, and we only keep top b (the beam size) hypothesis for each bin.
Obviously, this simple approach suffers from the incomparable problem for those hypothesis that have
different number of parsing actions (e.g. S 8 and S ?8). Moreover, it may result in very low translation
variance in each beam.
3.3 Best-parse: Keeping the Best Dependency Structure for Each Translation
Following Hassan et al. (2009), we only keep the best parsing tree for each translation. That means after
a consecutive translation sc actions, our shift-reduce parser applies all the possible parsing actions, and
generates a set of new partial dependency structures. Then we only choose the best one with the highest
Slm score, and only use this dependency structure for future predictions.
For example, for the translation in line 10 in Figure 3, we only keep S 8, if the parsing score of S 8 is
higher than S ?8, although they are not comparable. Another complicate example is shown in Figure 4,
within the translation step 15, there are many alternatives with different parsing structures for the same
translation (?a meeting with?) in the third column, but we can only choose the top one in the final.
3.4 Grouping: Regrouping Hypothesis by N(S ) in Each Bin
In order to do comparable sorting and pruning, our basic idea is to regroup those hypotheses in a same
bin into small groups by N(S ). For each translation, we first apply all the possible parsing actions,
and generate all dependency structures. Then we regroup all the hypothesis with different dependency
structures based on the size of N(S ).
1138
Bush held al Bush held a meetingl i
sh
Bush held al
re
Bush held a meetingl i
Bush held a meetingl i
re
sh
Bush held a meetingl i
re
Bush held a meeting withl i i
sh
Bush held a meeting withl i i
sh
sh
Bush held a meeting withl i i
Bush held a meeting withl i i
sh
re
re
Bush held a meeting withl i i
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
Bush held a meeting with Sharonl i i
re
sh
Bush held a meeting with Sharonl i i
sh
......
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i i
re
Bush held a meeting with Sharonl i i
sh
Bush held a meeting with Sharonl i ish
......
Bush held a meeting with Sharonl i i
re
Step 15 Step 16
G1: N(S)=1
......
Bush held a meeting withl i i
G2: N(S)=2
G3: N(S)=3
G4: N(S)=4
Figure 4: Multi-beam structures of two bins with different translation steps (15 and 16). The first three
columns show the parsing movements in bin 15. Each dashed box is a group based on the number of
reduce actions over the new translation strings (?a meeting with? for bin 15, and ?Sharon? for bin 16).
G2 means two reduce actions have been applied. After this regrouping, we perform the pruning in two
phases: 1) keep top b states in each group, and labeled each group with the state with the highest parsing
score in this group; 2) sort the different groups, and keep top g groups.
For example, Figure 4 shows two bins with two different translation steps (15 and 16). In bin 15, the
graph shows the parsing movements after we scan three new words (?a?, ?meeting?, and ?with?). The
parsing sh action happens from a parsing state in one column to another state in the next column, while
re happens from a state to another state in the same column. The third column in bin 15 lists some partial
dependency structures that have all new words parsed. Here each dashed box is a group of hypothesis
with a same N(S ), e.g. the G2 contains all the dependency structures that have two reduce actions after
parsed all the new words. Then, we sort and prune each group by the beam size b, and each group labeled
as the highest hypothesis in this group. Finally, we sort those groups and only keep top g groups for the
future predictions. Again, in Figure 4, we can keep the whole group G3 and partial group of G2 if b = 2.
In our experiments, we set the group size g to 5.
3.5 Log-linear Model
We integrate our dependency parser into the log-linear model as an additional feature. So the decoder
searches for the best translation e? with a latent tree structure (evaluated by our Slm) according to the
following equation:
e? = argmax
e?E
exp(Slm(e) ? ws +
?
i
fi ? wi) (1)
where Slm(e) is the dependency parsing score calculated by our parser, ws is the weight of Slm(e), fi are
the features in the baseline model and wi are the weights.
1139
4 Experiments
4.1 Data Preparation
The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respec-
tively. We use the NIST evaluation sets of MT06 as our development set, and MT03, 04, 05, and 08
(newswire portion) as our test sets. We word-aligned the training data using GIZA++ with refinement
option ?grow-diag-and? (Koehn et al., 2003), and then parsed the Chinese sentences using the Berkeley
parser (Petrov and Klein, 2007). we applied the algorithm of Galley et al. (2004) to extract tree-to-string
translation rules. Our trigram word language model was trained on the target side of the training corpus
using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. At decoding time, we
again parse the input sentences using the Berkeley parser, and convert them into translation forests using
rule pattern-matching (Mi et al., 2008).
Our baseline system is the incremental tree-to-string decoder of Huang and Mi (2010). We use the
same feature set shown in Huang and Mi (2010), and tune all the weights using minimum error-rate
training (Och, 2003) to maximize the Bleu score on the development set.
Our dependency parser is an implementation of the ?arc-standard? shift-reduce parser (Nivre, 2004),
and it is trained on the standard split of English Penn Tree-bank (PTB): Sections 02-21 as the training
set, Section 22 as the held-out set, and Section 23 as the test set. Using the same features as Huang and
Sagae (2010), our dependency parser achieves a similar performance as Huang and Sagae (2010). We
add the structured language model as an additional feature into the baseline system.
We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the scrip-
t mteval-v13a.pl. We also report the Ter scores.
4.2 Complete Comparisons on MT08
To explore the soundness of our approach, we carry out some experiments in Table 2. With a beam size
100, the baseline decoder achieves a Bleu score of 21.06 with a speed of 1.7 seconds per sentence.
Since our dependency parser is trained on the English PTB, which is not included in the MT training
set, there is a chance that the gain of Bleu score is due to the increase of new n-grams in the PTB data.
In order to rule out this possibility, we use the tool SRILM to train another tri-gram language model on
English PTB and use it as a secondary language model for the decoder. The Bleu score is 21.10, which
is similar to the baseline result. Thus we can conclude that any gain of the following +Slm experiments
is not because of the using of the additional English PTB.
Our second experiment re-ranks the 100-best translations of the baseline with our structured language
model trained on PTB. The improvement is less than 0.2 Bleu, which is not statistically significant, as
the search space for re-ranking is relatively small compared with the decoding space.
As shown in Section 3, we have three different ways to integrate an Slm to the baseline system:
? na??ve: adding the parsing signature to the translation signature;
? best-parse: keeping the best dependency structure for each translation;
? grouping: regrouping the hypothesis by N(S ) in each bin.
The na??ve approach achieves a Bleu score of 19.12, which is significantly lower than the baseline. The
main reason is that adding parsing signatures leads to very restricted translation variance in each beam.
We also tried to increase the beam size to 1000, but we do not see any improvement.
The fourth line in Table 2 shows the result of the best-parse (Hassan et al., 2009). This approach only
slows the speed by a factor of two, but the improvement is not statistically significant. We manually
looked into some dependency trees this approach generates, and found this approach always introduce
local parsing errors.
The last line shows our efficient beam grouping scheme with a grouping size 5, it achieves a significant
improvement with an acceptable speed, which is about 6 times slower than the baseline system.
1140
System Bleu Speed
baseline 21.06 1.7
+Slm
re-ranking 21.23 1.73
na??ve 19.12 2.6
best-parse 21.30 3.4
grouping (g=5) 21.64 10.6
Table 2: Results on MT08. The bold score is significantly better than the baseline result at level p < 0.05.
System MT03 MT04 MT05 MT08 Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2
baseline 19.94 10.73 22.03 18.63 19.92 11.45 21.06 10.37 12.80
+Slm 21.49 9.44 22.33 18.38 20.51 10.71 21.64 9.88 12.10
Table 3: Results on all test sets. Bold scores are significantly better than the baseline system (p < 0.5).
4.3 Final Results on All Test Sets
Table 3 shows our main results on all test sets. Our method gains an average improvement of 0.7 points
in terms of (T-B)/2. Results on NIST MT 03, 05, and 08 are statistically significant with p < 0.05, using
bootstrap re-sampling with 1000 samples (Koehn, 2004). The average decoding speed is about 10 times
slower than the baseline.
5 Related Work
The work of Schwartz et al. (2011) is similar in spirit to ours. We are different in the following ways.
First, they integrate an Slm into a phrase-based system (Koehn et al., 2003), we pay more attention to
a syntax-based system. Second, their approach slowdowns the speed at near 2000 times, thus, they can
only tune their system on short sentences less than 20 words. Furthermore, their results are from a much
bigger beam (10 times larger than their baseline), so it is not clear which factor contributes more, the
larger beam size or the Slm. In contrast, our approach gains significant improvements over a state-of-the-
art tree-to-string baseline at a reasonable speed, about 6 times slower. And we answer some questions
beyond their work.
Hassan et al. (2009) incorporate a linear-time CCG parser into a DTM system, and achieve a significant
improvement. Different from their work, we pay more attention to the dependency parser, and we also
test this approach in our experiments. As they only keep 1-best parsing states during the decoding, they
are suffering from the local parsing errors.
Galley and Manning (2009) adapt the maximum spanning tree (MST) parser of McDonald et al. (2005)
to an incremental dependency parsing, and incorporate it into a phrase-based system. But this incremental
parser remains in quadratic time.
Besides, there are also some other efforts that are less closely related to ours. Shen et al. (2008)
and Mi and Liu (2010) develop a generative dependency language model for string-to-dependency and
tree-to-tree models. But they need parse the target side first, and encode target syntactic structures in
translation rules. Both papers integrate dependency structures into translation model, we instead model
the dependency structures with a monolingual parsing model over translation strings.
6 Conclusion
In this paper, we presented an efficient algorithm to integrate a structured language model (an incremen-
tal shift-reduce parser in specific) into an incremental tree-to-string system. We calculate the structured
language model scores incrementally at the decoding step, rather than re-scoring a complete transla-
tion. Our experiments suggest that it is important to design efficient pruning strategies, which have been
1141
overlooked in previous work. Experimental results on large-scale data set show that our approach signif-
icantly improves the translation quality at a reasonable slower speed than a state-of-the-art tree-to-string
system.
The structured language model introduced in our work only takes into account the target string, and
ignores the reordering information in the source side. Thus, our future work seeks to incorporate more
source side syntax information to guide the parsing of the target side, and tune a structured language
model for both Bleu and paring accuracy. Another potential work lies in the more efficient searching and
pruning algorithms for integration.
Acknowledgments
We thank the three anonymous reviewers for helpful suggestions, and Dan Gildea and Licheng Fang for
discussions. Yu and Liu were supported in part by CAS Action Plan for the Development of Western
China (No. KGZD-EW-501) and a grant from Huawei Noah?s Ark Lab, Hong Kong. Liu was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin C-
ity University. Huang was supported by DARPA FA8750-13-2-0041 (DEFT), a Google Faculty Research
Award, and a PSC-CUNY Award, and Mi by DARPA HR0011-12-C-0015. The views and findings in
this paper are those of the authors and are not endorsed by the US or Chinese governments.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. Parsing of series in automatic computation. In The Theory of Parsing,
Translation, and Compiling, page Volume I.
Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine
translation. In Proceedings of MT Summit IX. Intl. Assoc. for Machine Translation.
Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. volume 14, pages 283 ? 332.
Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of
ACL.
Michel Galley and Christopher D. Manning. 2009. Quadratic-time dependency parsing for machine translation.
In Proceedings of the Joint Conference of ACL 2009 and AFNLP, pages 773?781, Suntec, Singapore, August.
Association for Computational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In Proceed-
ings of HLT-NAACL, pages 273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-
ACL, pages 961?968.
Hany Hassan, Khalil Sima?an, and Andy Way. 2009. A syntactified direct translation model with linear-time de-
coding. In Proceedings of EMNLP 2009, pages 1182?1191, Singapore, August. Association for Computational
Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental decoding for tree-to-string translation. In Proceedings
of EMNLP, pages 273?283.
Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings
of ACL 2010, pages 1077?1086, Uppsala, Sweden, July. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proceedings
of NAACL 2012, Montreal, Quebec.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of NAACL, pages 127?133.
1142
Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP,
pages 388?395.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of COLING-ACL, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving tree-to-tree translation with packed forests. In Proceedings
of ACL/IJCNLP, pages 558?566, Suntec, Singapore, August.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing
using spanning tree algorithms. In Proceedings of HLT-EMNLP, pages 523?530, Vancouver, British Columbia,
Canada, October.
Haitao Mi and Qun Liu. 2010. Constituency to dependency translation with forests. In Proceedings of ACL, pages
1433?1442, Uppsala, Sweden, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL: HLT, pages
192?199.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engi-
neering and Cognition Together, pages 50?57, Barcelona, Spain, July. Association for Computational Linguis-
tics.
Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL,
pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411.
Matt Post and Daniel Gildea. 2008. Language modeling with tree substitution grammars. In Proceedings of
AMTA.
Matt Post and Daniel Gildea. 2009. Language modeling with tree substitution grammars. In Proceedings of NIPS
workshop on Grammar Induction, Representation of Language, and Language Learning.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL, Ann Arbor, MI, June.
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language
models for phrase-based translation. In Proceedings of ACL 2011, pages 620?631, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
1143
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
page 1543, Dublin, Ireland, August 23-29 2014.
Annotation Adaptation and Language Adaptation in NLP
Qun Liu
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
qliu@computing.dcu.ie
Invited Speaker Abstract
Adaptation technologies are always useful in NLP when there is discrepancy between the training sce-
nario and use scenario. They are also effective in alleviating the data scarcity problem. Domain adapta-
tion is the most popular kind of adaptation technologies and is intensively researched. In this talk we will
introduce two other kinds of adaptation technologies: annotation adaptation and language adaptation.
Annotation adaptation is used to improve the performance of an automatic annotation task by leveraging
corpora with different annotation schemas, while language adaptation is used to solve an NLP problem
in one language by utilizing the linguistic knowledge which is learnt from solving the same problem
in another language. We investigate these technologies mainly for the tasks of word segmentation and
parsing, however similar technologies may be developed for other NLP tasks also.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1543
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2217?2226, Dublin, Ireland, August 23-29 2014.
Augment Dependency-to-String Translation with Fixed and Floating
Structures
Jun Xie
?
Jinan Xu
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology,Chinese Academy of Sciences
xiejun@ict.ac.cn
?School of Computer and Information Technology, Beijing Jiaotong University
xja2010@gmail.com
?School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
In this paper, we propose an augmented dependency-to-string model to combine the merits of
both the head-dependents relations at handling long distance reordering and the fixed and floating
structures at handling local reordering. For this purpose, we first compactly represent both the
head-dependent relation and the fixed and floating structures into translation rules; second, in
decoding we build ?on-the-fly? new translation rules from the compact translation rules that
can incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrase
coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments
on Chinese-to-English translation show that our augmented dependency-to-string model gains
significant improvement of averaged +0.85 BLEU scores on three test sets over the dependency-
to-string model.
1 Introduction
As a representation holding both syntactic and semantic information, dependency grammar has been
attracting more and more attention in statistical machine translation. Lin (2004) took paths as the el-
ementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to
treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding
and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008)
employed the fixed and floating structures as elementary structures and proposed a string-to-dependency
model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elemen-
tary structures and proposed a dependency-to-string model with good long distance reordering property.
A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed
as an instance of a sentence pattern or phrase pattern.
However, since dependency trees are much flatter than constituency trees, the dependency-to-string
model suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituency-
based models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006). Non-syntactic phrases are those
phrases that can not be covered by whole subtrees. To address this problem, Meng et al. (2013) proposed
to translate with both constituency and dependency trees, which can incorporate non-syntactic phrases
covered by the constituents of the constituency trees. This model requires both constituency and depen-
dency trees, thus may suffer from both constituency and dependency parse errors. Additionally, there are
only few languages that have both constituency and dependency parsers, which limits its practical use.
In this paper, we propose to address non-syntactic phrase coverage problem of the dependency-to-
string model without resort to extra resources (Section 3). To this end, we augment the dependency-to-
string model at two aspects. First, we combine the merits of both the head-dependent relations and the
fixed and floating structures (Shen et al., 2008), and compactly represent these two kinds of knowlege
into augmented HDR rules (Section 3.1). We acquire the augmented HDR rules automatically from the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2217
X4:VV
X5:NNX3:!*
(a)
X3 X2X5
X2:NTX1:PN
X4X1
X4:VV
X5:NNX3:!*
X3 X2X5
X2:NTX1:PN
X4X1
(b)
Figure 1: Examples of an HDR rule (a) and an augmented HDR rule (b). Where each ?*? denotes
a substitute site which is a compact representation of a whole subtree. The shadow with line border
indicates a fixed structure and the shadow with dash line border indicates a floating structure.
word-aligned source dependency tree and target string paris (Section 3.2). In decoding we propose an
?on-the-fly? rule building strategy, which builds new translation rules from the augmented HDR rules
and incorporates non-syntactic phrases into translations (Section 3.4). Large-scale experiments (Section
4) on Chinese-to-English translation show that our augmented model gains significant improvement of
averaged +0.85 BLEU points on three test sets over the dependency-to-string model.
2 Background
For convenience of the description of our augmented dependency-to-string model, we first briefly re-
view the dependency-to-string model and the fixed and floating structures of string-to-dependency model
(Shen et al., 2008).
2.1 Dependency-to-String Translation
The dependency-to-string model (Xie et al., 2011) takes head-dependents relations as the elementary
structures of dependency trees, and represents the translation rules with the source side as HDRs and
the target side as string. Since the HDRs in essence relate to phrase patterns and sentence patterns,
the HDR rules specify the reordering of these patterns. For example, Figure 1 (a) is an example HDR
rule, which represents a reordering manner of a sentence pattern composed of a proper noun (X1:PN),
a temporal noun (X2:NT), an prepositional phrase relate to ?? (give)? (X3:?), a verb (X4:VV) and a
noun (X5:NN).
With the HDR rules, the dependency-to-string model gets rid of the extra reordering heuristics and
reordering models of the previous models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). More
importantly, the model shows state-of-the-art performance and exhibits good long distance reordering
property.
2.2 Fixed and Floating Structures
The fixed structures and floating structures are fundamental structures of the string-to-dependency model
(Shen et al., 2008), which are introduced to handle the coverage of non-constituent rules. Given the
dependency tree d
1
d
2
...d
n
of a sentence f
1
f
2
...f
n
, where d
i
indicates the parent word index of word f
i
.
Definition 1. A dependency structure d
i...j
is fixed on the head h, where h ? [i, j], if and only if it meets
the following conditions:
- d
h
/? [i, j]
- ?k ? [i, j] and k 6= h, d
k
? [i, j]
- ?k /? [i, j], d
k
= h or d
k
/? [i, j]
A fixed structure describes a fragment with a sub-root, where all the children of the sub-root are
complete.
2218
Definition 2. A dependency tree d
i...j
is floating with children C, for a non-empty set C ? i, ..., j, if and
only if it meets the following conditions:
- ?h /? [i, j], s.t.?k ? C, d
k
= h
- ?k ? [i, j] and k /? C, d
k
? [i, j]
- ?k /? [i, j], d
k
/? [i, j]
A floating structure consists of sibling nodes of a common head, but the head itself is unspecified.
In nature, the fixed and floating structures represent the phrases under the structural constraint of
dependency trees, most of them are non-syntactic phrases.
The HDRs are good at handling long distance dependencies, while the fixed and floating structures
excels at handling local reordering. This encourages us to address the non-syntactic phrase coverage
problem of dependency-to-string model by exploiting these two kinds of structures.
3 Augmented Dependency-to-String Translation
In the following, we will describe our augmented dependency-to-string model in detail, including the
augmented HDR rules (Section 3.1), rule acquisition (Section 3.2) and ?on-the-fly? rule building in
decoding (Section 3.4).
3.1 Augmented HDR rules
Our augmented HDR rules aim at combining the merits of both the HDRs at handling long distance re-
ordering and the fixed and floating structures at handling local reordering. For this purpose, we augment
the HDR rules (Xie et al., 2011) by labelling the HDRs with the fixed and floating structures.
Figure 1 (b) shows an example augmented HDR rule. Which is an augmented version of the HDR
rule Figure 1 (a) by labelling it with a fixed structure (shadow with line border) and a floating structure
(shadow with dash line border). The labeled fixed and floating structures indicate the bilingual phrases
that we can incorporate in this sentence pattern.
3.2 Rule Acquisition
Given a word-aligned parallel corpus defined as a set of triples ?T, e, A?, where T is a dependency tree
of source sentence f
J
1
, e
I
1
is the target sentence and A is an alignment relation between f
J
1
and e
I
1
, we
acquire the augmented HDR rules by three steps: tree annotation, acceptable HDR identification and rule
induction. The process is similar with that of Xie et al. (2011). However, we make some extensions so
that we can take the fixed and floating structures into account.
3.2.1 Tree Annotation
Besides annotating each node of T with head span and dependency span as Xie et al. (2011), we also
label the tree with consistent fixed and floating structures.
Definition 3. The head span hsp(n) of a node n is the closure of the set taking the index of the target
words aligned to n as its elements.
The closure of a set contains all the elements between the minimum and the maximum of the set and
each element has only one copy. For example, the closure of set {1, 3} is {1, 2, 3}.
We say a head span is consistent with alignment if the bilingual phrase it covers is consistent with the
alignment (Koehn et al., 2003).
Definition 4. Given a subtree T
?
rooted at n, the dependency span dsp(n) of n is the closure of the union
of the consistent head spans of all the nodes of T
?
.
dsp(n) = closure(
?
n
?
?T
?
hsp(n
?
) is consistent
hsp(n
?
))
If no head spans of all the nodes of T
?
are consistent, dsp(n) = ?.
2219
!"##$%&%'$(&)'
*+",,$-&-'$-&-'
."/$0&0'$0&1'
2"/,$1&1'$1&1'
3445 647 849:;<8I
=*",>$)&)'$)&)'?"/,$(&('$(&('
@:AA B4CD:99E7
. !? =* *+2
% 0 )1 F 1-
!
"#X3:.*
X3 tonightdinner
$"%
cookI will
!"# !$#
%&'()*
+()'()*
Figure 2: An example annotated dependency tree (a) and an example lexicalized augmented HDR rule
(b) induced from the top-level HDR of (a). Each node of the dependency tree is annotated with two
spans: head span (the former) and dependency span (the latter). The shadows denote a consistent fixed
structure (shadow with line border) and a floating structure (shadow with dash line border). The ?*?
denotes a substitute site.
Definition 5. A fixed or floating structure is consistent with alignment if the phrase it covers is consistent
with alignment.
Tree annotation can be readily accomplished by a single post-order traversal of dependency tree T .
For each accessed node n, annotate it with head span and dependency span according to A. If n is an
internal node, enumerate all the fixed and floating structures relate to n, and label those consistent ones
on T . Repeat the above process till the root is accessed.
Figure 2 (a) shows an example annotated dependency tree. Where each node is annotated with two
spans: head span (the former) and dependency span (the latter). Moreover, the dependency tree is also
labeled with two consistent fixed and floating structures that cover phrases ?? ?? and ??? ? ??
respectively.
3.2.2 Acceptable HDR Identification
From the annotated dependency tree, we identify the HDRs that are suitable for rule induction. These
HDRs are called as acceptable HDRs. To this end, we traverse the annotated dependency tree in post-
order and identify the HDRs with the following properties:
- for the head, its head span is consistent;
- for the dependents, the dependency span of each dependent should not be ? unless the dependent
is a leaf node;
- the intersection of the head span of the head and the dependency spans of the dependents is ? (or
do not overlap).
Different from those acceptable HDRs of Xie et al. (2011), the acceptable HDRs here may be labeled
with fixed and floating structures. For example, the top level of Figure 2 (a) is an acceptable HDR, which
is labeled with a fixed structure and a floating structures. Typically an acceptable HDR has three types
of nodes: leaf node (of the dependency tree), internal node (of the dependency tree) and head node (an
internal node function as the head of the HDR).
3.2.3 Rule Induction
From each acceptable HDR, we induce a set of lexicalized and unlexicalized augmented HDR rules. This
process is similar with that of Xie et al. (2011) except that here we have to consider the consistent fixed
2220
!"#X3:!*
X3 tonightdinner
$"%
cookI will
!
&'())X3:!*
X3 X2X5
&*()+&,(-)
cookX1 will
!
"#X3:"*
X3 tonightdinner
$"%
cookI will
!
&'())X3:"*
X3 X2&'
&*()+&,(-)
cookX1 will
&.(//
"#X3:!*
X3 tonightdinner
$"%
X4I will
&.(//
&'())X3:!*
X3 X2X5
&*()+&,(-)
X4X1 will
&.(//
"#X3:"*
X3 tonightdinner
$"%
X4I will
&.(//
&'())X3:"*
X3 X2&'
&*()+&,(-)
X4X1 will
!"# !$#
!%# !&#
!'# !(#
!)# !*#
+,
+, +,
+,
+-+-
UH
Figure 3: Lexicalized augmented HDR rule (a) and unlexicalized augmented HDR rules (b)?(h) induced
from the top level HDR of the annotated dependency tree in Figure 2. Where ?UH?, ?UI? and ?UL?
denotes ?unlexicalize head?, ?unlexicalize internal? and ?unlexicalize leaf? , respectively. The shadows
with line border denote fixed structures and the shadows with dash line border denotes floating structures.
and floating structures.
First, we induce a lexicalized augmented HDR rule with the following principles:
1. extract the HDR, mark each internal node as a variable, and label the HDR with the floating struc-
tures that cover only variables. This forms the input of a lexicalized rule.
2. generate the target string according to head span of the head and the dependency spans of the related
dependents, and turn the word sequences covered by the dependency spans of the internal nodes into
variables. This forms the output of a lexicalized rule.
Figure 2 (b) illustrates a lexicalized augmented HDR rule induced from the top-level HDR of the
annotated dependency tree Figure 2 (a).
From each lexicalized augmented HDR rule (along with the acceptable HDR), we then induce a set of
unlexicalized augmented HDR rules with the following principles:
1. turn each type (leaf, internal or head) of nodes simultaneously into variables;
2. when turning a head or leaf node into a variable, change the counterpart of the target side into the
variable; label the unlexicalized HDR with the fixed and floating structures that cover only variables.
3. when turing an internal node into a variable, keep the counterpart of the target side unchanged.
Totally, we will obtain eight types of augmented HDR rules from an acceptable HDR. In this paper,
we call the lexicalized and unlexicalized HDRs generated by the above process as instances of the HDR.
Figure 3 illustrates the rule induction of seven unlexicalized augmented HDR rules (b)-(h) from lexi-
calized augmented HDR rule (a). Where ?UH?, ?UI? and ?UL? on the dash arrows indicate ?unlexicalize
head?, ?unlexicalize internal? and ?unlexicalized leaf?, respectively.
3.2.4 Probability Estimation
We take the augmented HDR rules acquired from word-aligned parallel corpus as the observed data, and
employ relative frequency estimation to calculate the translation probabilities of the rules. Note that, here
we take the labeled fixed and floating structures of the augmented HDR rules as indicators of bilingual
phrases that can be incorporated in the sentence patterns and phrases patterns represented by the HDRs.
So we consider only the HDRs when counting the augmented HDR rules.
2221
X2:NT
!  "# callcalled
...
$%  &  '
him yesterday
!"#$%&'$()*+)#,-(
...
X45:VV_NN
X3:&*
X3 X2
X2:NTX1:PN
X45X1
X4:VV
X5:NN
X23X5
X23:NT_P*X1:PN
X4X1
X45:VV_NN
X23
X23:NT_P*X1:PN
X45X1
(relate to X4 X5)(relate to X2 X3)
(d) (e) (f)
X4:VV
X5:NNX3:&*
X3 X2X5
X1:PN
X4X1
(a) (b) (c)
!{ }!
X4:VV
X5:NN
X4:VV
X5:NN
X2:NT X3:&*X2:NT X3:&*
Figure 4: Illustration of ?on-the-fly? translation rule building.
3.3 The model
Following Och and Ney (2002), we adopt a general log-linear model for our augmented dependency-to-
string model. Let d be a derivation that converts a source dependency tree T into a target string e. The
probability of derivation d is defined as:
P (d) ?
?
i
?
i
(d)
?
i
(1)
where ?
i
are features defined on derivation and ?
i
are feature weights.
In our implementation, we make use of eleven features, including seven features inherited from the
dependency-to-string model:
- translation probabilities P (f |e) and P (e|f) and lexical translation probabilities P
lex
(f |e) and
P
lex
(e|f) of augmented HDR rules
- rule penalty exp(?1)
- language model P
lm
(e)
- word penalty exp(|e|), where |e| is the length of the generated target string
and four extra features for bilingual phrases relate to fixed and floating structures:
- translation probabilities P
bp
(f |e) and P
bp
(e|f) and lexical translation probabilities P
bp lex
(f |e) and
P
bp lex
(e|f) of bilingual phrases
3.4 ?On-the-Fly? Decoding
The task of the decoder is to find the best derivation from all possible derivations. Our decoder is based
on bottom-up chart parsing, which characterizes at ?on-the-fly? translation rule building.
Given an input dependency tree T , the decoder traverses it in post-order. For each accessed node n,
the decoder first enumerates all instances of the HDR rooted at n as we do in rule induction, and checks
for matched augmented HDR rules. If a matched rule is labeled with fixed and floating structures, the
decoder builds new translation rules ?on the fly? with the following principles:
2222
1. check the phrases covered by the labeled fixed and floating structures for matched bilingual phrases;
2. if there are no matched bilingual phrases for all labeled fixed and floating structures, take the aug-
mented HDR rule as a HDR rule of dependency-to-string model; otherwise,
- enumerate all combinations of the fixed and floating structures with matched bilingual phrases;
- for each combination, build a new translation rule by turning the variable sequences covered
by the fixed and floating structures into new variables;
- the new-built rule inherits the translation probabilities of the deriving augmented HDR rule,
and the new variables take the matched bilingual phrases as their translation hypothesis.
Figure 4 illustrates the ?on-the-fly? rule building process. Suppose augmented HDR rule (a) is the
matched rule, and bilingual phrases (b) and (c) match the phrases covered by the labeled fixed and
floating structures of (a). There will be three combinations of the labeled fixed and floating structures
as shown in the middle of Figure 4. For each combination, the decoder builds a new translation rule by
turning variable sequences ?X2:NT X3:?*? and/or ?X4:VV X5:NN? into new variables ?X23:NT P*?
and/or ?X45:VV NN?. And we will obtain three new translation rules (d)-(f) that can incorporate non-
syntactic phrases into translations.
If there are no matched rules, the decoder builds a pseudo translation rule with monotonic reordering.
The decoder then employs cube pruning (Chiang, 2007; Huang and Chiang, 2007) to generate k-best
hypothesis with integrated language model for node n.
Repeat the above process till the root of T is accessed. The hypothesis with the highest score is output
as translation.
4 Experiments
We evaluated our augmented model by comparison with dependency-to-string model and hierarchical
phrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002).
4.1 Experimental Setup
The parallel training corpus include 1.25M Chinese-English sentence pairs.
1
We parse the Chinese
sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency trees, obtain word
alignment by running GIZA++ (Och and Ney, 2003) in both directions and applying ?grow-diag-final?
refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit
(Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus.
We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04)
and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU-
4 metric
2
, tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the
statistical difference between the systems with significance test (Collins et al., 2005).
4.2 Systems
We take ?Moses-Chart? of Moses
3
(Koehn et al., 2007) as hierarchical phrase-based model baseline. In
our experiments, we use the default settings.
Both the dependency-to-string baseline and our augmented model employ the same settings as those
of Xie et al. (2011), with the beam threshold, beam size and rule size are set to 10
?3
, 200 and 100
respectively. And both systems employ bilingual phrases with length ? 7 extracted by Moses.
4.3 Experiment results
Table 1 shows the results of the BLEU scores of the three systems. Where ?dep2str? and ?dep2str-aug?
denote dependency-to-string model baseline and our augmented dependency-to-string model, respec-
tively. As we can see, ?dep2str? shows better performance (+0.31 BLEU on average) than ?Moses-Chart?
1
From LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
2
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3
http://www.statmt.org/moses/
2223
System Rule# MT03 MT04 MT05 Average
Moses-Chart 116.4M 34.65 36.47 34.39 35.17
dep2str 37M+32.5M 34.92 36.82 34.71 35.48
dep2str-aug 37M+32.5M 35.66
*
(+0.74) 37.61
*
(+0.79) 35.74
*
(+1.03) 36.33 (+0.85)
Table 1: Statistics of the extracted rules and BLEU scores (%) on the test sets of the three systems.
Where ?37M+32.5M? denotes 37M rules and 32.5M bilingual phrases. And ?*? indicates dep2str-aug
are statistically better than dep2str with p < 0.01.
Source: !"# $ % & ' ( ) *+, -. /0 1 23 45 67 8
Reference 1: Sampaio has placed  high hopes on the Portuguese-Sino 
cooperation in the World Expo. 
Moses-Chart: Sampaio on cooperation between the two countries in the 
world expo affairs Portugal and China places great . 
Dep2Str: President placed great  cooperation between Portugal and China , 
the two countries in the World Expo affairs .
Dep2Str-aug: Sampaio placed high expectations of the Portuguese - Chinese 
cooperation in World Expo affairs .
!"#
9:;
$
9<
%
& '
( )
*+,
-=
/0 1
23
45
9>>
67
9::
8
9<?
Reference 2: Sampaio expressed his high expectations on the Sino-
Portuguese cooperation in the work of the world exposition.
Figure 5: Translation examples of ?Moses-Chart?, ?dep2str? and ?dep2str-aug?. The line border shadow
denotes the phrases successfully captured by ?dep2str-aug?.
and is a strong baseline.?Dep2str-aug? gains significant improvements of +0.74, +0.79 and +1.03 BLEU
points over ?dep2str? on the test sets, respectively.
Additionally, we compare the actual translations generated by ?Moses-Chart?, ?dep2str? and ?dep2str-
aug?. Figure 5 shows the translations of these three systems on a sentence of MT05. The source sentence
holds a common sentence pattern in Chinese, which is composed of a proper noun, a verb, a noun and
a prepositional phrases (corresponding to the top level of the dependency tree on the right). However,
the preposition phrase related to ??? holds nine words, thus the simple pattern becomes a long distance
dependency that challenges SMT systems. Limited by the phrase-based rules, ?Moses-Chart? fails to
capture the sentence pattern and outputs a messy translation with little sense. ?Dep2str?, resorting to
HDR rules, successfully captures the pattern and outputs a translation with correct reordering, but it is
still hard to understand. With the help of augmented HDR rules, ?dep2str-aug? captures both the sentence
pattern and non-syntactic phrase ?????? and gives an translation with good adequacy and fluency.
These results reveal the merits of our augmented dependency-to-string model at handling both long
distance reordering (with HDR) and local reordering (with fixed and floating structures), which is promis-
ing for translating language pairs that are syntactically divergent.
5 Conclusion and Future Work
In this paper, we propose an augmented dependency-to-string model to address the non-syntactic phrase
coverage problem for dependency-to-string model. To this purpose, we make two important augmen-
tations to the dependency-to-string model. First, we propose an compact representation to combine
both head-dependent relation and the fixed and floating structures into translation rules. Second, in de-
coding we build ?on the fly? new translation rules from the compact translation rules and incorporate
non-syntactic phrases into translations. By this way, we can combine the merits of both head-dependents
relation at handling long distance reordering and bilingual phrases at handling local reordering. Large-
2224
scale experiments show that our augmented dependency-to-string model gains significant improvements
over the dependency-to-string model.
In the future work, we would like to incorporate semantic knowledge such as typed dependencies and
WordNet
4
(Miller, 1995) so as to better direct the process of translation.
Acknowledgments
The authors were supported by National Nature Science Foundation of China ( Contract 61370130 and
61379086). Liu was partially supported by the Science Foundation Ireland (Grant No. 07/CE/I1142)
as part of the CNGL at Dublin City University. We sincerely thank the anonymous reviewers for their
careful review and insightful suggestions.
References
David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the ACL 2005, pages 531?540, Ann Arbor, Michigan, June.
Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion
grammars. In Proceedings of ACL 2005, pages 271?279.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In HLT-
NAACL 2004: Main Proceedings, pages 273?280, Boston, Massachusetts, USA, May 2 - May 7.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of ACL 2007, pages 144?151, Prague, Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language
parsing. In In Advances in Neural Information Processing Systems 15 (NIPS), pages 3?10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of ACL 2005: Interactive Poster and Demonstration Sessions, pages 177?
180.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of ACL 2006, pages 609?616, Sydney, Australia, July.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency and
dependency trees. In Proceedings of EMNLP 2013, pages 1066?1076, Seattle, Washington, USA, October.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002, pages 295?302, Philadelphia, Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL-
2003, pages 160?167, Sapporo, Japan, July.
4
http://wordnet.princeton.edu
2225
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of ACL 2002, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL 2008: HLT, pages 577?585, Columbus, Ohio,
June.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of EMNLP 2011, pages 216?226, Edinburgh, Scotland, UK., July.
2226
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 216?226,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Novel Dependency-to-String Model for Statistical Machine Translation
Jun Xie, Haitao Mi and Qun Liu
Key Laboratory of Intelligent Information Processiong
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{junxie,htmi,liuqun}@ict.ac.cn
Abstract
Dependency structure, as a first step towards
semantics, is believed to be helpful to improve
translation quality. However, previous works
on dependency structure based models typi-
cally resort to insertion operations to complete
translations, which make it difficult to spec-
ify ordering information in translation rules.
In our model of this paper, we handle this
problem by directly specifying the ordering
information in head-dependents rules which
represent the source side as head-dependents
relations and the target side as strings. The
head-dependents rules require only substitu-
tion operation, thus our model requires no
heuristics or separate ordering models of the
previous works to control the word order of
translations. Large-scale experiments show
that our model performs well on long dis-
tance reordering, and outperforms the state-
of-the-art constituency-to-string model (+1.47
BLEU on average) and hierarchical phrase-
based model (+0.46 BLEU on average) on two
Chinese-English NIST test sets without resort
to phrases or parse forest. For the first time,
a source dependency structure based model
catches up with and surpasses the state-of-the-
art translation models.
1 Introduction
Dependency structure represents the grammatical
relations that hold between the words in a sentence.
It encodes semantic relations directly, and has the
best inter-lingual phrasal cohesion properties (Fox,
2002). Those attractive characteristics make it pos-
sible to improve translation quality by using depen-
dency structures.
Some researchers pay more attention to use de-
pendency structure on the target side. (Shen et al,
2008) presents a string-to-dependency model, which
restricts the target side of each hierarchical rule to be
a well-formed dependency tree fragment, and em-
ploys a dependency language model to make the out-
put more grammatically. This model significantly
outperforms the state-of-the-art hierarchical phrase-
based model (Chiang, 2005). However, those string-
to-tree systems run slowly in cubic time (Huang et
al., 2006).
Using dependency structure on the source side
is also a promising way, as tree-based systems run
much faster (linear time vs. cubic time, see (Huang
et al, 2006)). Conventional dependency structure
based models (Lin, 2004; Quirk et al, 2005; Ding
and Palmer, 2005; Xiong et al, 2007) typically
employ both substitution and insertion operation to
complete translations, which make it difficult to
specify ordering information directly in the transla-
tion rules. As a result, they have to resort to either
heuristics (Lin, 2004; Xiong et al, 2007) or sepa-
rate ordering models (Quirk et al, 2005; Ding and
Palmer, 2005) to control the word order of transla-
tions.
In this paper, we handle this problem by di-
rectly specifying the ordering information in head-
dependents rules that represent the source side as
head-dependents relations and the target side as
string. The head-dependents rules have only one
substitution operation, thus we don?t face the prob-
lems appeared in previous work and get rid of the
216
heuristics and ordering model. To alleviate data
sparseness problem, we generalize the lexicalized
words in head-dependents relations with their cor-
responding categories.
In the following parts, we first describe the moti-
vation of using head-dependents relations (Section
2). Then we formalize our grammar (Section 3),
present our rule acquisition algorithm (Section 4),
our model (Section 5) and decoding algorithm (Sec-
tion 6). Finally, large-scale experiments (Section 7)
show that our model exhibits good performance on
long distance reordering, and outperforms the state-
of-the-art tree-to-string model (+1.47 BLEU on av-
erage) and hierarchical phrase-based model (+0.46
BLEU on average) on two Chinese-English NIST
test sets. For the first time, a source dependency tree
based model catches up with and surpasses the state-
of-the-art translation models.
2 Dependency Structure and
Head-Dependents Relation
2.1 Dependency Sturcture
A dependency structure for a sentence is a directed
acyclic graph with words as nodes and modification
relations as edges. Each edge direct from a head to
a dependent. Figure 1 (a) shows an example depen-
dency structure of a Chinese sentence.
2010? FIFA??????????
2010 FIFA [World Cup] in/at [South Africa]
successfully hold
Each node is annotated with the part-of-speech
(POS) of the related word.
For convenience, we use the lexicon dependency
grammar (Hellwig, 2006) which adopts a bracket
representation to express a projective dependency
structure. The dependency structure of Figure 1 (a)
can be expressed as:
((2010?) (FIFA)???) (?(??)) (??)??
where the lexicon in brackets represents the depen-
dents, while the lexicon out the brackets is the head.
To construct the dependency structure of a sen-
tence, the most important thing is to establish de-
pendency relations and distinguish the head from the
dependent. Here are some criteria (Zwicky, 1985;
x2:?2:x1:???1: x3:AD3:
??
x1 was held x3 x21  s l  3 2
?/P/???/NR/
??/NR/
??/AD/
2010?/NT/ FIFA/NRI /
??/VV/
/P?/???/NR/ ??/AD/
??/VV/
(a)
(b)
(c)
?? successfullys ssf ll(d)
Figure 1: Examples of dependency structure (a), head-
dependents relation (b), head-dependents rule (r1 of Fig-
ure 2) and head rule (d). Where ?x1:???? and
?x2:?? indicate substitution sites which can be replaced
by a subtree rooted at ????? and ??? respectively.
?x3:AD?indicates a substitution site that can be replaced
by a subtree whose root has part-of-speech ?AD?. The
underline denotes a leaf node.
Hudson, 1990) for identifying a syntactic relation
between a head and a dependent between a head-
dependent pair:
1. head determines the syntactic category of C,
and can often replace C;
2. head determines the semantic category of C;
dependent gives semantic specification.
2.2 Head-Dependents Relation
A head-dependents relation is composed of a head
and all its dependents as shown in Figure 1(b).
Since all the head-dependent pairs satisfy crite-
ria 1 and 2, we can deduce that a head-dependents
relation L holds the property that the head deter-
mines the syntactic and semantic categories of L,
and can often replace L. Therefore, we can recur-
217
sively replace the bottom level head-dependent re-
lations of a dependency structure with their heads
until the root. This implies an representation of the
generation of a dependency structure on the basis of
head-dependents relation.
Inspired by this, we represent the translation rules
of our dependency-to-string model on the founda-
tion of head-dependents relations.
3 Dependency-to-String Grammar
Figure 1 (c) and (d) show two examples of the trans-
lation rules used in our dependency-to-string model.
The former is an example of head-dependent rules
that represent the source side as head-dependents re-
lations and act as both translation rules and reorder-
ing rules. The latter is an example of head rules
which are used for translating words.
Formally, a dependency-to-string grammar is de-
fined as a tuple ??, N,?, R?, where ? is a set of
source language terminals, N is a set of categories
for the terminals in ? , ? is a set of target language
terminals, and R is a set of translation rules. A rule
r in R is a tuple ?t, s, ??, where:
- t is a node labeled by terminal from ?; or a
head-dependents relation of the source depen-
dency structures, with each node labeled by a
terminal from ? or a variable from a set X =
{x1, x2, ...} constrained by a terminal from ?
or a category from N ;
- s ? (X ??)? is the target side string;
- ? is a one-to-one mapping from nonterminals
in t to variables in s.
For example, the head-dependents rule shown in
Figure 1 (c) can be formalized as:
t = ((x1:???) (x2:?) (x3:AD)??)
s = x1 was held x3 x2
? = {x1:???? x1, x2:?? x2, x3:AD? x3}
where the underline indicates a leaf node, and
xi:letters indicates a pair of variable and its con-
straint.
A derivation is informally defined as a sequence
of steps converting a source dependency structure
into a target language string, with each step apply-
ing one translation rule. As an example, Figure 2
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
??/VV
2010? FIFAI ??? ? ?? ?? ??
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
was held l
?/P???/NR
??/NR2010?/NT FIFA/NRI
was held successfully l  f ll
?/P
??/NR
2010 FIFA [World Cup] was held successfully  I   [ rl  ]    l   f ll   
??2010 FIFA World Cup was held successfully in  I   rl       l   f ll   i
2010 FIFA World Cup was held successfully in [South Africa] I  rl    l  f ll  i  [ t  fri ]
parser
(a)
(b)
(c)
(d)
(e)
(f)
(g)
r3: (2010?) (FIFA) ??? 
 ?2010 FIFA World Cup
r2: ???successfully
r1: (x1:???)(x2 :?)(x3:AD)??
  ?x1 was held x3 x2
r4: ? (x2:NR)?in x2
r5: ???South Africa
Figure 2: An example derivation of dependency-to-string
translation. The dash lines indicate the reordering when
employing a head-dependents rule.
shows the derivation for translating a Chinese (CH)
sentence into an English (EN) string.
CH 2010? FIFA??????????
EN 2010 FIFA World Cup was held successfully in
South Africa
218
The Chinese sentence (a) is first parsed into a de-
pendency structure (b), which is converted into an
English string in five steps. First, at the root node,
we apply head-dependents rule r1 shown in Figure
1(c) to translate the top level head-dependents rela-
tion and result in three unfinished substructures and
target string in (c). The rule is particular interesting
since it captures the fact: in Chinese prepositional
phrases and adverbs typically modify verbs on the
left, whereas in English prepositional phrases and
adverbs typically modify verbs on the right. Second,
we use head rule r2 translating ???? into ?success-
fully? and reach situation (d). Third, we apply head-
dependents rule r3 translating the head-dependents
relation rooted at ????? and yield (e). Fourth,
head-dependents rules r5 partially translate the sub-
tree rooted at ??? and arrive situation in (f). Finally,
we apply head rule r5 translating the residual node
???? and obtain the final translation in (g).
4 Rule Acquisition
The rule acquisition begins with a word-aligned cor-
pus: a set of triples ?T, S,A?, where T is a source
dependency structure, S is a target side sentence,
and A is an alignment relation between T and S.
We extract from each triple ?T, S,A? head rules that
are consistent with the word alignments and head-
dependents rules that satisfy the intuition that syn-
tactically close items tend to stay close across lan-
guages. We accomplish the rule acquisition through
three steps: tree annotation, head-dependents frag-
ments identification and rule induction.
4.1 Tree Annotation
Given a triple ?T, S,A? as shown in Figure 3, we
first annotate each node n of T with two attributes:
head span and dependency span, which are defined
as follows.
Definition 1. Given a node n, its head span hsp(n)
is a set of index of the target words aligned to n.
For example, hsp(2010?)={1, 5}, which corre-
sponds to the target words ?2010? and ?was?.
Definition 2. A head span hsp(n) is consistent if it
satisfies the following property:
?n? ?=nhsp(n?) ? hsp(n) = ?.
?/P
{5,8}{9,10}{ , }{ , }
???/NR
{3,4}{2-4}
??/NR
{9,10}{9,10}
??/AD
{7}{7}
2010?/NT
{1,5}{}{ , }{}
FIFA/NR
{2,2}{2,2}
??/VV
{6}{2-10}
2010
1
FIFA
2
I World
3
rl held
6
l successfully
7
f ll in
8
i South
9
tCup
4
was
5
Africa
10
fri
Figure 3: An annotated dependency structure. Each node
is annotated with two spans, the former is head span and
the latter dependency span. The nodes in acceptable head
set are displayed in gray, and the nodes in acceptable de-
pendent set are denoted by boxes. The triangle denotes
the only acceptable head-dependents fragment.
For example, hsp(??) is consistent, while
hsp(2010?) is not consistent since hsp(2010?) ?
hsp(?) = 5.
Definition 3. Given a head span hsp(n), its closure
cloz(hsp(n)) is the smallest contiguous head span
that is a superset of hsp(n).
For example, cloz(hsp(2010?)) = {1, 2, 3, 4, 5},
which corresponds to the target side word sequence
?2010 FIFA World Cup was?. For simplicity, we use
{1-5} to denotes the contiguous span {1, 2, 3, 4, 5}.
Definition 4. Given a subtree T ? rooted at n, the
dependency span dsp(n) of n is defined as:
dsp(n) = cloz(
?
n??T ?
hsp(n?) is consistent
hsp(n?)).
If the head spans of all the nodes of T ? is not consis-
tent, dsp(n) = ?.
For example, since hsp(?) is not consistent,
dsp(?)=dsp(??)={9, 10}, which corresponds to
the target words ?South? and ?Africa?.
The tree annotation can be accomplished by a sin-
gle postorder transversal of T . The extraction of
head rules from each node can be readily achieved
with the same criteria as (Och and Ney, 2004). In
219
the following, we focus on head-dependents rules
acquisition.
4.2 Head-Dependents Fragments Identification
We then identify the head-dependents fragments that
are suitable for rule induction from the annotated de-
pendency structure.
To facilitate the identification process, we first de-
fine two sets of dependency structure related to head
spans and dependency spans.
Definition 5. A acceptable head set ahs(T) of a de-
pendency structure T is a set of nodes, each of which
has a consistent head span.
For example, the elements of the acceptable head
set of the dependency structure in Figure 3 are dis-
played in gray.
Definition 6. A acceptable dependent set adt(T) of
a dependency structure T is a set of nodes, each of
which satisfies: dep(n) ?= ?.
For example, the elements of the acceptable de-
pendent set of the dependency structure in Figure 3
are denoted by boxes.
Definition 7. We say a head-dependents fragments
is acceptable if it satisfies the following properties:
1. the root falls into acceptable head set;
2. all the sinks fall into acceptable dependent set.
An acceptable head-dependents fragment holds
the property that the head span of the root and the de-
pendency spans of the sinks do not overlap with each
other, which enables us to determine the reordering
in the target side.
The identification of acceptable head-dependents
fragments can be achieved by a single preorder
transversal of the annotated dependency structure.
For each accessed internal node n, we check
whether the head-dependents fragment f rooted at
n is acceptable. If f is acceptable, we output an
acceptable head-dependents fragment; otherwise we
access the next node.
Typically, each acceptable head-dependents frag-
ment has three types of nodes: internal nodes, inter-
nal nodes of the dependency structure; leaf nodes,
leaf nodes of the dependency structure; head node, a
special internal node acting as the head of the related
head-dependents relation.
?/P
{5,8}{9,10}
/
{ , }{ , }
???/NR
{3,4}{2-4}
??/AD
{7}{7}
??/VV
{6}{2-10}
heldl successfullys ssf ll[FIFA World Cup][ I  rl  ] South Africa][ t  fri ]
Input:
Output:
x2:?2:x1:???1: ??
??
x1 held successfully x21  l  s ssf ll  2
(x1:???)(x2:?)(??) ??
      ?  x1  held successfully x2
(a)
(b)
Figure 4: A lexicalized head-dependents rule (b) induced
from the only acceptable head-dependents fragment (a)
of Figure 3.
4.3 Rule Induction
From each acceptable head-dependents fragment,
we induce a set of lexicalized and unlexicalized
head-dependents rules.
4.3.1 Lexicalized Rule
We induce a lexicalized head-dependents rule
from an acceptable head-dependents fragment by
the following procedure:
1. extract the head-dependents relation and mark
the internal nodes as substitution sites. This
forms the input of a head-dependents rule;
2. place the nodes in order according to the head
span of the root and the dependency spans of
the sinks, then replace the internal nodes with
variables and the other nodes with the target
words covered by their head spans. This forms
the output of a head-dependents rule.
Figure 4 shows an acceptable head-dependents
fragment and a lexicalized head-dependents rule in-
220
duced from it.
4.3.2 Unlexicalized Rules
Since head-dependents relations with verbs as
heads typically consist of more than four nodes, em-
ploying only lexicalized head-dependents rules will
result in severe sparseness problem. To alleviate
this problem, we generalize the lexicalized head-
dependents rules and induce rules with unlexicalized
nodes.
As we know, the modification relation of a head-
dependents relation is determined by the edges.
Therefore, we can replace the lexical word of each
node with its categories (i.e. POS) and obtain new
head-dependents relations with unlexicalized nodes
holding the same modification relation. Here we call
the lexicalized and unlexicalized head-dependents
relations as instances of the modification relation.
For a head-dependents relation with m node, we can
produce 2m ? 1 instances with unlexicalized nodes.
Each instance represents the modification relation
with a different specification.
Based on this observation, from each lexical-
ized head-dependent rule, we generate new head-
dependents rules with unlexicalized nodes according
to the following principles:
1. change the aligned part of the target string into
a new variable when turning a head node or a
leaf node into its category;
2. keep the target side unchanged when turning a
internal node into its category.
Restrictions: Since head-dependents relations
with verbs as heads typically consists of more than
four nodes, enumerating all the instances will re-
sult in a massive grammar with too many kinds of
rules and inflexibility in decoding. To alleviate these
problems, we filter the grammar with the following
principles:
1. nodes of the same type turn into their categories
simultaneously.
2. as for leaf nodes, only those with open class
words can be turned into their categories.
In our experiments of this paper, we only
turn those dependents with POS tag in the
set of {CD,DT,OD,JJ,NN,NR,NT,AD,FW,PN}
into their categories.
x2:?2:x1:???1: ??
heldl successfullyf ll
??
x11 x22
(x1:???)(x2:?)(??) ??
 ? x1  held successfully x2
x2:?2:x1:???1: x3:AD:
heldl x33
??
x11 x22
(x1:???)(x2:?)(x3:AD) ??
 ? x1  held x3 x2
x2:P2:x1:NR1: ??
heldl successfullyf ll
??
x11 x22
(x1:NR)(x2:P)(??) ??
? x1  held successfully x2
x2:P2:x1:NR1: x3:AD3:
heldl x33
??
x11 x22
(x1:NR)(x2:P)(x3:AD) ??
 ? x1  held x3 x2
x2:?2:x1:???1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:???)(x2:?)(??) x4:VV
 ? x1  x4 successfully x2
x2:?2:x1:???1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:???)(x2:?)(x3:AD) x4:VV
 ?   x1  x4  x3 x2
x2:P2:x1:NR1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:NR)(x2:P)(??) x4:VV
 ? x1 x4 successfully x2
x2:P2:x1:NR1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:NR)(x2:P)(x3:AD) x4:VV
 ? x1  x4 x3 x2
generalize leaf generalize leaf
generalize internalgeneralize internal
generalize leaf generalize leaf
generalize
head
Figure 5: An illustration of rule generalization. Where
?x1:???? and ?x2:?? indicate substitution sites
which can be replaced by a subtree rooted at ?????
and ??? respectively. ?x3:AD?indicates a substitution
site that can be replaced by a subtree whose root has part-
of-speech ?AD?. The underline denotes a leaf node. The
box indicates the starting lexicalized head-dependents
rule.
Figure 5 illustrates the rule generalization process
under these restrictions.
4.3.3 Unaligned Words
We handle the unaligned words of the target side
by extending the head spans of the lexicalized head
and leaf nodes on both left and right directions.
This procedure is similar with the method of (Och
and Ney, 2004) except that we might extend several
221
Algorithm 1: Algorithm for Rule Acquisition
Input: Source dependency structure T , target string S, alignment A
Output: Translation rule set R
1 HSet? ACCEPTABLE HEAD(T ,S,A)
2 DSet? ACCEPTABLE DEPENDENT(T ,S,A)
3 for each node n ? HSet do
4 extract head rules
5 append the extracted rules to R
6 if ?n? ? child(n) n? ? DSet
7 then
8 obtain a head-dependent fragment f
9 induce lexicalized and unlexicalized head-dependents rules from f
10 append the induced rules to R
11 end
12 end
spans simultaneously. In this process, we might ob-
tain m(m ? 1) head-dependents rules from a head-
dependent fragment in handling unaligned words.
Each of these rules is assigned with a fractional
count 1/m.
4.4 Algorithm for Rule Acquisition
The rule acquisition is a three-step process, which is
summarized in Algorithm 1.
We take the extracted rule set as observed data and
make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
5 The model
Following (Och and Ney, 2002), we adopt a general
log-linear model. Let d be a derivation that convert
a source dependency structure T into a target string
e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
we used seven features as follows:
- translation probabilities P (t|s) and P (s|t);
- lexical translation probabilities Plex(t|s) and
Plex(s|t);
- rule penalty exp(?1);
- language model Plm(e);
- word penalty exp(|e|).
6 Decoding
Our decoder is based on bottom up chart parsing.
It finds the best derivation d? that convert the input
dependency structure into a target string among all
possible derivations D:
d? = argmaxd?DP (D) (2)
Given a source dependency structure T , the decoder
transverses T in post-order. For each accessed in-
ternal node n, it enumerates all instances of the re-
lated modification relation of the head-dependents
relation rooted at n, and checks the rule set for
matched translation rules. If there is no matched
rule, we construct a pseudo translation rule accord-
ing to the word order of the head-dependents rela-
tion. For example, suppose that we can not find
any translation rule about to ?(2010?) (FIFA) ?
???, we will construct a pseudo translation rule
?(x1:2010?) (x2:FIFA) x3:??? ? x1 x2 x3?.
A larger translation is generated by substituting the
variables in the target side of a translation rule with
the translations of the corresponding dependents.
We make use of cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to find the k-best items with inte-
grated language model for each node.
To balance performance and speed, we prune the
search space in several ways. First, beam thresh-
222
old ? , items with a score worse than ? times of the
best score in the same cell will be discarded; sec-
ond, beam size b, items with a score worse than the
bth best item in the same cell will be discarded. The
item consist of the necessary information used in de-
coding. Each cell contains all the items standing for
the subtree rooted at it. For our experiments, we set
? = 10?3 and b = 300. Additionally, we also prune
rules that have the same source side (b = 100).
7 Experiments
We evaluated the performance of our dependency-
to-string model by comparison with replications of
the hierarchical phrase-based model and the tree-to-
string models on Chinese-English translation.
7.1 Data preparation
Our training corpus consists of 1.5M sentence
pairs from LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, Hansards portion of
LDC2004T07, LDC2004T08 and LDC2005T06.
We parse the source sentences with Stanford
Parser (Klein and Manning, 2003) into projective
dependency structures, whose nodes are annotated
by POS tags and edges by typed dependencies. In
our implementation of this paper, we make use of
the POS tags only.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) on the corpus in
both directions and applying ?grow-diag-and? re-
finement(Koehn et al, 2003).
We apply SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
modified Kneser-Ney smoothing on the Xinhua por-
tion of the Gigaword corpus.
We use NIST MT Evaluation test set 2002 as our
development set, NIST MT Evaluation test set 2004
(MT04) and 2005 (MT05) as our test set. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric (Papineni et al, 2002).1
We make use of the standard MERT (Och, 2003)
to tune the feature weights in order to maximize the
system?s BLEU score on the development set.
1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
System Rule # MT04(%) MT05(%)
cons2str 30M 34.55 31.94
hiero-re 148M 35.29 33.22
dep2str 56M 35.82+ 33.62+
Table 1: Statistics of the extracted rules on training cor-
pus and the BLEU scores on the test sets. Where ?+?
means dep2str significantly better than cons2str with p <
0.01.
7.2 The baseline models
We take a replication of Hiero (Chiang, 2007) as
the hierarchical phrase-based model baseline. In
our experiments of this paper, we set the beam size
b = 200 and the beam threshold ? = 0. The maxi-
mum initial phrase length is 10.
We use constituency-to-string model (Liu et al,
2006) as the syntax-based model baseline which
make use of composed rules (Galley et al, 2006)
without handling the unaligned words. In our exper-
iments of this paper, we set the tatTable-limit=20,
tatTable-threshold=10?1, stack-limit=100, stack-
threshold=10?1,hight-limit=3, and length-limit=7.
7.3 Results
We display the results of our experiments in Table
1. Our dependency-to-string model dep2str signif-
icantly outperforms its constituency structure-based
counterpart (cons2str) with +1.27 and +1.68 BLEU
on MT04 and MT05 respectively. Moreover, with-
out resort to phrases or parse forest, dep2str sur-
passes the hierarchical phrase-based model (hiero-
re) over +0.53 and +0.4 BLEU on MT04 and MT05
respectively on the basis of a 62% smaller rule set.
Furthermore, We compare some actual transla-
tions generated by cons2str, hiero-re and dep2str.
Figure 6 shows two translations of our test sets
MT04 and MT05, which are selected because each
holds a long distance dependency commonly used in
Chinese.
In the first example, the Chinese input holds
a complex long distance dependencies ?? ?
? ?... ?...? ???. This dependency cor-
responds to sentence pattern ?noun+prepostional
phrase+prepositional phrase+verb?, where the for-
mer prepositional phrase specifies the position and
the latter specifies the time. Both cons2str and
hiero-re are confused by this sentence and mistak-
223
??? ? ?? ??? ? ?? ?? ?? ? ?? ?
Afterft r briefri f talkst l withit Powellll ,, thet US State Departmentt t  rt t Barnierr i r ,,saidi
MT05----Segment 163
Reference: After a brief talk with 
Powell at the US State 
Department, Barnier said:
Cons2str: Barnier after brief 
talks in US State Department 
and Powell  said:
Hiero-re: After a short meeting 
with Barnier on the US State 
Department, Powell said:
Dep2str: After brief talks with 
Powell, the US State 
Department Barnier said,
?? ?? ?? ??? ? ?? ?? 1373 2001 ?
Chinai appreciatesr i t efforts
 
ff rt off Anti -ti - Terrorismrr ri Committee itt tot promoter t allll inicountriestri
MT04----Segment 1096
Reference: China appreciates the 
efforts of the Counter-Terrorism 
Committee to promote the 
implementation of the resolution 
1373(2001) in all states and to 
help enhance the anti-terrorist 
capabilities of developing 
countries.
Cons2str: China appreciates 
Anti - Terrorist Committee for 
promoting implementation of 
the resolution No. 1373(2001) 
and help developing countries 
strength counter-terrorism  
capability building for the 
efforts,
Hiero-re: China appreciates 
Anti - Terrorism Committee to 
promote countries implement 
resolution No . 1373 ( 2001 ) 
and help developing countries 
strengthen anti-terrorism 
capacity building support for 
efforts
Dep2str: China appreciates 
efforts of Anti - Terrorism 
Committee to promote all 
countries in the implementation 
of resolution  1373 ( 2001 )  , to 
help strengthen the anti-
terrorism capability building of 
developing countries
?? ? ? ? ?? ??? ????? ? ? ?
nsubj             
prep prep
????? ? ?? ?
thet implementationi l t ti off ......
nsubj dobj
Figure 6: Actual translations produced by the baselines and our system. For our system, we also display the long
distance dependencies correspondence in Chinese and English. Here we omit the edges irrelevant to the long distance
dependencies.
enly treat ???(Powell)? as the subjective, thus
result in translations with different meaning from
the source sentence. Conversely, although ??? is
falsely translated into a comma, dep2str captures
this complex dependency and translates it into ?Af-
ter ... ,(should be at) Barnier said?, which accords
with the reordering of the reference.
In the second example, the Chinese input holds
a long distance dependency ??? ?? ... ?
?? which corresponds to a simple pattern ?noun
phrase+verb+noun phrase?. However, due to the
modifiers of ???? which contains two sub-
sentences including 24 words, the sentence looks
rather complicated. Cons2str and hiero-re fail to
capture this long distance dependency and provide
monotonic translations which do not reflect the
meaning of the source sentence. In contrast, dep2str
successfully captures this long distance dependency
and translates it into ?China appreciates efforts of
...?, which is almost the same with the reference
?China appreciates the efforts of ...?.
All these results prove the effectiveness of our
dependency-to-string model in both translation and
long distance reordering. We believe that the ad-
vantage of dep2str comes from the characteristics of
dependency structures tending to bring semantically
related elements together (e.g., verbs become adja-
cent to all their arguments) and are better suited to
lexicalized models (Quirk et al, 2005). And the in-
capability of cons2str and hiero-re in handling long
distance reordering of these sentences does not lie in
the representation of translation rules but the com-
promises in rule extraction or decoding so as to bal-
ance the speed or grammar size and performance.
The hierarchical phrase-based model prohibits any
nonterminal X from spanning a substring longer
than 10 on the source side to make the decoding al-
gorithm asymptotically linear-time (Chiang, 2005).
224
While constituency structure-based models typically
constrain the number of internal nodes (Galley et
al., 2006) and/or the height (Liu et al, 2006) of
translation rules so as to balance the grammar size
and performance. Both strategies limit the ability of
the models in processing long distance reordering of
sentences with long and complex modification rela-
tions.
8 Related Works
As a first step towards semantics, dependency struc-
tures are attractive to machine translation. And
many efforts have been made to incorporating this
desirable knowledge into machine translation.
(Lin, 2004; Quirk et al, 2005; Ding and Palmer,
2005; Xiong et al, 2007) make use of source depen-
dency structures. (Lin, 2004) employs linear paths
as phrases and view translation as minimal path cov-
ering. (Quirk et al, 2005) extends paths to treelets,
arbitrary connected subgraphs of dependency struc-
tures, and propose a model based on treelet pairs.
Both models require projection of the source depen-
dency structure to the target side via word alignment,
and thus can not handle non-isomorphism between
languages. To alleviate this problem, (Xiong et al,
2007) presents a dependency treelet string corre-
spondence model which directly map a dependency
structure to a target string. (Ding and Palmer, 2005)
presents a translation model based on Synchronous
Dependency Insertion Grammar(SDIG), which han-
dles some of the non-isomorphism but requires both
source and target dependency structures. Most im-
portant, all these works do not specify the ordering
information directly in translation rules, and resort
to either heuristics (Lin, 2004; Xiong et al, 2007) or
separate ordering models(Quirk et al, 2005; Ding
and Palmer, 2005) to control the word order of
translations. By comparison, our model requires
only source dependency structure, and handles non-
isomorphism and ordering problems simultaneously
by directly specifying the ordering information in
the head-dependents rules that represent the source
side as head-dependents relations and the target side
as strings.
(Shen et al, 2008) exploits target dependency
structures as dependency language models to ensure
the grammaticality of the target string. (Shen et al,
2008) extends the hierarchical phrase-based model
and present a string-to-dependency model, which
employs string-to-dependency rules whose source
side are string and the target as well-formed depen-
dency structures. In contrast, our model exploits
source dependency structures, as a tree-based sys-
tem, it run much faster (linear time vs. cubic time,
see (Huang et al, 2006)).
9 Conclusions and future work
In this paper, we present a novel dependency-to-
string model, which employs head-dependents rules
that represent the source side as head-dependents
relations and the target side as string. The head-
dependents rules specify the ordering information
directly and require only substitution operation.
Thus, our model does not need heuristics or order-
ing model of the previous works to control the word
order of translations. Large scale experiments show
that our model exhibits good performance in long
distance reordering and outperforms the state-of-
the-art constituency-to-string model and hierarchi-
cal phrase-based model without resort to phrases and
parse forest. For the first time, a source dependency-
based model shows improvement over the state-of-
the-art translation models.
In our future works, we will exploit the semantic
information encoded in the dependency structures
which is expected to further improve the transla-
tions, and replace 1-best dependency structures with
dependency forests so as to alleviate the influence
caused by parse errors.
Acknowledgments
This work was supported by National Natural Sci-
ence Foundation of China, Contract 60736014,
60873167, 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions. We appreciate Yajuan Lv,
Wenbin Jiang, Hao Xiong, Yang Liu, Xinyan Xiao,
Tian Xia and Yun Huang for the insightful advices in
both experiments and writing. Special thanks goes
to Qian Chen for supporting my pursuit all through.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
225
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of ACL 2005.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of EMNLP 2002,
pages 304?311.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL 2006, pages 961?968, Sydney, Australia,
July. Association for Computational Linguistics.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. In Dependenz und Valenz / Dependency and Va-
lency, volume 2, pages 1081?1109. Berlin, New York.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proceedings of ACL 2007, pages 144?151, Prague,
Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing, pages 1?8, New York City,
New York, June. Association for Computational Lin-
guistics.
Richard Hudson. 1990. English Word Grammar. Black-
ell.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of Coling 2004,
pages 625?630, Geneva, Switzerland, Aug 23?Aug
27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL 2006, pages 609?616,
Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL-
2003, pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
ACL 2002, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271?
279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL 2008: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computational
Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A depen-
dency treelet string correspondence model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
40?47, Prague, Czech Republic, June.
Arnold M. Zwicky. 1985. Heads. Journal of Linguistics,
21:1?29.
226
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880?888,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast Generation of Translation Forest
for Large-Scale SMT Discriminative Training
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn
Abstract
Although discriminative training guarantees to
improve statistical machine translation by in-
corporating a large amount of overlapping fea-
tures, it is hard to scale up to large data due to
decoding complexity. We propose a new al-
gorithm to generate translation forest of train-
ing data in linear time with the help of word
alignment. Our algorithm also alleviates the
oracle selection problem by ensuring that a
forest always contains derivations that exactly
yield the reference translation. With millions
of features trained on 519K sentences in 0.03
second per sentence, our system achieves sig-
nificant improvement by 0.84 BLEU over the
baseline system on the NIST Chinese-English
test sets.
1 Introduction
Discriminative model (Och and Ney, 2002) can
easily incorporate non-independent and overlapping
features, and has been dominating the research field
of statistical machine translation (SMT) in the last
decade. Recent work have shown that SMT benefits
a lot from exploiting large amount of features (Liang
et al, 2006; Tillmann and Zhang, 2006; Watanabe
et al, 2007; Blunsom et al, 2008; Chiang et al,
2009). However, the training of the large number
of features was always restricted in fairly small data
sets. Some systems limit the number of training ex-
amples, while others use short sentences to maintain
efficiency.
Overfitting problem often comes when training
many features on a small data (Watanabe et al,
2007; Chiang et al, 2009). Obviously, using much
more data can alleviate such problem. Furthermore,
large data also enables us to globally train millions
of sparse lexical features which offer accurate clues
for SMT. Despite these advantages, to the best of
our knowledge, no previous discriminative training
paradigms scale up to use a large amount of training
data. The main obstacle comes from the complexity
of packed forests or n-best lists generation which
requires to search through all possible translations
of each training example, which is computationally
prohibitive in practice for SMT.
To make normalization efficient, contrastive esti-
mation (Smith and Eisner, 2005; Poon et al, 2009)
introduce neighborhood for unsupervised log-linear
model, and has presented positive results in various
tasks. Motivated by these work, we use a translation
forest (Section 3) which contains both ?reference?
derivations that potentially yield the reference trans-
lation and also neighboring ?non-reference? deriva-
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to O(n6), because we still need bi-
parsing to create the reference derivations.
Consequently, we propose a method to fast gener-
ate a subset of the forest. The key idea (Section 4)
is to initialize a reference derivation tree with maxi-
mum score by the help of word alignment, and then
traverse the tree to generate the subset forest in lin-
ear time. Besides the efficiency improvement, such
a forest allows us to train the model without resort-
1Exactly, there are no reference derivations, since derivation
is a latent variable in SMT. We call them reference derivation
just for convenience.
880
0,4
0,1
2,4
3,4
1 30 4
21
3
5
4
6
2
hyper-
edge rule
e1 r1 X ? ?X1 bei X2, X1 was X2?
e2 r2 X ? ?qiangshou bei X1,
the gunman was X1?
e3 r3 X ? ?jingfang X1, X1 by the police?
e4 r4 X ? ?jingfang X1, police X1 ?
e5 r5 X ? ?qiangshou, the gunman?
e6 r6 X ? ?jibi, shot dead?
Figure 1: A translation forest which is the running example throughout this paper. The reference translation is ?the
gunman was killed by the police?. (1) Solid hyperedges denote a ?reference? derivation tree t1 which exactly yields
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to
swap the order ofX3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally,
this is done by deleting a node X0,1.
ing to constructing the oracle reference (Liang et al,
2006; Watanabe et al, 2007; Chiang et al, 2009),
which is non-trivial for SMT and needs to be deter-
mined experimentally. Given such forests, we glob-
ally learn a log-linear model using stochastic gradi-
ent descend (Section 5). Overall, both the generation
of forests and the training algorithm are scalable, en-
abling us to train millions of features on large-scale
data.
To show the effect of our framework, we globally
train millions of word level context features moti-
vated by word sense disambiguation (Chan et al,
2007) together with the features used in traditional
SMT system (Section 6). Training on 519K sentence
pairs in 0.03 seconds per sentence, we achieve sig-
nificantly improvement over the traditional pipeline
by 0.84 BLEU.
2 Synchronous Context Free Grammar
We work on synchronous context free grammar
(SCFG) (Chiang, 2007) based translation. The el-
ementary structures in an SCFG are rewrite rules of
the form:
X ? ??, ??
where ? and ? are strings of terminals and nonter-
minals. We call ? and ? as the source side and the
target side of rule respectively. Here a rule means a
phrase translation (Koehn et al, 2003) or a transla-
tion pair that contains nonterminals.
We call a sequence of translation steps as a
derivation. In context of SCFG, a derivation is a se-
quence of SCFG rules {ri}. Translation forest (Mi
et al, 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given sentence
under an SCFG (see Figure 1). A tree t in the forest
corresponds to a derivation. In our paper, tree means
the same as derivation.
More formally, a forest is a pair ?V,E?, where V
is the set of nodes, E is the set of hyperedge. For
a given source sentence f = fn1 , Each node v ? V
is in the form Xi,j , which denotes the recognition
of nonterminal X spanning the substring from the i
through j (that is fi+1...fj). Each hyperedge e ? E
connects a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).
3 Our Translation Forest
We use a translation forest that contains both ?ref-
erence? derivations that potentially yield the refer-
ence translation and also some neighboring ?non-
reference? derivations that fail to produce the ref-
erence translation. Therefore, our forest only repre-
sents some of the derivations for a sentence given an
SCFG rule table. The motivation of using such a for-
est is efficiency. However, since this space contains
both ?good? and ?bad? translations, it still provides
evidences for discriminative training.
First see the example in Figure 1. The derivation
tree t1 represented by solid hyperedges is a reference
derivation. We can construct a non-reference deriva-
tion by making small change to t1. By replacing the
e3 of t1 with e4, we obtain a non-reference deriva-
881
tion tree t2. Considering the rules in each derivation,
the difference between t1 and t2 lies in r3 and r4. Al-
though r3 has a same source side with r4, it produces
a different translation. While r3 provides a swap-
ping translation, r4 generates a monotone transla-
tion. Thus, the derivation t2 fails to move the sub-
ject ?police? to the behind of verb ?shot dead?, re-
sulting a wrong translation ?the gunman was police
shot dead?. Given such derivations, we hope that
the discriminative model is capable to explain why
should use a reordering rule in this context.
Generally, our forest contains all the reference
derivationsRT for a sentence given a rule table, and
some neighboring non-reference derivations NT ,
which can be defined fromRT .
More formally, we call two hyperedges e1 and e2
are competing hyperedges, if their corresponding
rules r(e1) = ??1, ?1? and r(e2) = ??2, ?2? :
?1 = ?2 ? ?1 ?= ?2 (1)
This means they give different translations for a
same source side. We use C(e) to represent the set
of competing hyperedges of e.
Two derivations t1 = ?V 1, E1? and t2 =
?V 2, E2? are competing derivations if there exists
e1 ? E1 and e2 ? E2: 2
V 1 = V 2 ? E1 ? e1 = E2 ? e2
? e2 ? C(e1) (2)
In other words, derivations t1 and t2 only differ in
e1 and e2, and these two hyperedges are competing
hyperedges. We useC(t) to represent the set of com-
peting derivations of tree t, and C(t,e) to represent
the set of competing derivations of t if the competi-
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT for a sentence is determined. Then, the set of
non-reference derivations NT can be defined from
RT :
?t?RT C(t) (3)
Overall, our forest is the compact representation of
RT and NT .
2The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex-
ponentially trees. In tree, the hyperedge degrades to edge.
Algorithm 1 Forest Generation
1: procedure GENERATE(t)
2: list? t
3: for v ? t in post order do
4: e? incoming edge of v
5: append C(t, e) to list;
6: for u ? child(v) from left to right do
7: tn? OPERATE(t, u)
8: if tn ?= t then
9: append tn to list
10: for e? ? tn ? e? /? t do
11: append C(tn,e?) to list
12: if SCORE(t) < SCORE(tn) then
13: t? tn
14: return t,list
4 Fast Generation
It is still slow to calculate the entire forest defined
in Section 3, therefore we use a greedy decoding for
fast generating a subset of the forest. Starting form
a reference derivation, we try to slightly change the
derivation into a new reference derivation. During
this process, we collect the competing derivations
of reference derivations. We describe the details of
local operators for changing a derivation in section
4.1, and then introduce the creation of initial refer-
ence derivation with max score in Section 4.2.
For example, given derivation t1, we delete the
node X0,1 and the related hyperedge e1 and e5. Fix-
ing the other nodes and edges, we try to add a new
edge e2 to create a new reference translation. In this
case, if rule r2 really exists in our rule table, we get
a new reference derivation t3. After constructing t3,
we first collect the new tree and C(t3, e2). Then, we
will move to t3, if the score of t3 is higher than t2.
Notably, if r2 does not exist in the rule table, we fail
to create a new reference derivation. In such case,
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation t, and the out-
put is a new derivation and the generated derivations.
3For simplicity, we list all the trees, and do not compress
them into a forest in practice. It is straight to extent the algo-
rithm to get a compact forest for those generated derivations.
Actually, instead of storing the derivations, we call the generate
function twice to calculate gradient of log-linear model.
882
0,4
0,1 2,4
0,4
2,4
0,4
2,40,2
Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also
need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c).
The list used for storing forest is initialized with the
input tree (line 2). We visit the nodes in t in post-
order (line 3). For each node v, we first append the
competing derivations C(t,e) to list, where e is in-
coming edge of v (lines 4-5). Then, we apply oper-
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn
(line 9), and also the competing derivationsC(tn, e?)
of the new derivation on those edges e? which only
occur in the new derivation (lines 10-11). Finally, if
the new derivation has a larger score, we will replace
the origin derivation with new one (lines 12-13).
Although there is a two-level loop for visiting
nodes (line 3 and 6), each node is visited only one
time in the inner loops. Thus, the complexity is
linear with the number of nodes #node. Consid-
ering that the number of source word (also leaf node
here) is less than the total number of nodes and is
more than ?(#node+1)/2?, the time complexity of
the process is also linear with the number of source
word.
4.1 Lexicalize and Generalize
The function OPERATE in Algorithm 1 uses two op-
erators to change a node: lexicalize and generalize.
Figure 2 shows the effects of the two operators. The
lexicalize operator works on nonterminal nodes. It
moves away a nonterminal node and attaches the
children of current node to its parent. In Figure 2(b),
the node X0,1 is deleted, requiring a more lexical-
ized rule to be applied to the parent node X0,4 (one
more terminal in the source side). We constrain the
lexicalize operator to apply on pre-terminal nodes
whose children are all terminal nodes. In contrast,
the generalize operator works on terminal nodes and
inserts a nonterminal node between current node and
its parent node. This operator generalizes over the
continuous terminal sibling nodes left to the current
node (including the current node). Generalizing the
node bei in Figure 2(b) results Figure 2(c). A new
node X0,2 is inserted as the parent of node qiang-
shou and node bei.
Notably, there are two steps when apply an oper-
ator. Suppose we want to lexicalize the node X0,1
in t1 of Figure 1, we first delete the node X0,1 and
related edge e1 and e5, then we try to add the new
edge e2. Since rule table is fixed, the second step
is a process of decoding. Therefore, sometimes we
may fail to create a new reference derivation (like
r2 may not exist in the rule table). In such case, we
keep the origin derivation unchanged.
The changes made by the two operators are local.
Considering the change of rules, the lexicalize oper-
ator deletes two rules and adds one new rule, while
the generalize operator deletes one rule and adds two
new rules. Such local changes provide us with a way
to incrementally calculate the scores of new deriva-
tions. We use this method motivated by Gibbs Sam-
pler (Blunsom et al, 2009) which has been used for
efficiently learning rules. The different lies in that
we use the operator for decoding where the rule ta-
ble is fixing.
4.2 Initialize a Reference Derivation
The generation starts from an initial reference
derivation with max score. This requires bi-parsing
(Dyer, 2010) over the source sentence f and the ref-
erence translation e. In practice, we may face three
problems.
First is efficiency problem. Exhaustive search
over the space under SCFG requires O(|f |3|e|3).
883
To parse quickly, we only visit the tight consistent
(Zhang et al, 2008) bi-spans with the help of word
alignment a. Only visiting tight consistent spans
greatly speeds up bi-parsing. Besides efficiency,
adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts
tight consistent initial phrases (Chiang, 2007).
Second is degenerate problem. If we only use
the features as traditional SCFG systems, the bi-
parsing may end with a derivation consists of some
giant rules or rules with rare source/target sides,
which is called degenerate solution (DeNero et al,
2006). That is because the translation rules with rare
source/target sides always receive a very high trans-
lation probability. We add a prior score log(#rule)
for each rule, where #rule is the number of occur-
rence of a rule, to reward frequent reusable rules and
derivations with more rules.
Finally, we may fail to create reference deriva-
tions due to the limitation in rule extraction. We
create minimum trees for (f , e,a) using shift-reduce
(Zhang et al, 2008). Some minimum rules in the
trees may be illegal according to the definition of
Chiang (2007). We also add these rules to the rule
table, so as to make sure every sentence is reachable
given the rule table. A source sentence is reachable
given a rule table if reference derivations exists. We
refer these rules as added rules. However, this may
introduce rules with more than two variables and in-
crease the complexity of bi-parsing. To tackle this
problem, we initialize the chart with minimum par-
allel tree from the Zhang et al (2008) algorithm,
ensuring that the bi-parsing has at least one path to
create a reference derivation. Then we only need to
consider the traditional rules during bi-parsing.
5 Training
We use the forest to train a log-linear model with a
latent variable as describe in Blunsom et al(2008).
The probability p(e|f) is the sum over all possible
derivations:
p(e|f) =
?
t??(e,f)
p(t, e|f) (4)
where ?(e, f) is the set of all possible derivations
that translate f into e and t is one such derivation.4
4Although the derivation is typically represent as d, we de-
notes it by t since our paper use tree to represent derivation.
Algorithm 2 Training
1: procedure TRAIN(S)
2: Training Data S = {fn, en,an}Nn=1
3: Derivations T = {}Nn=1
4: for n = 1 to N do
5: tn ? INITIAL(fn, en,an)
6: i? 0
7: for m = 0 to M do
8: for n = 0 to N do
9: ? ? LEARNRATE(i)
10: (?L(wi, tn), tn)?GENERATE(tn)
11: wi ? wi + ? ??L(wi, tn)
12: i? i + 1
13: return
?MN
i=1 wi
MN
This model defines the conditional probability of
a derivation t and the corresponding translation e
given a source sentence f as:
p(t, e|f) = exp
?
i ?ihi(t, e, f)
Z(f) (5)
where the partition function is
Z(f) =
?
e
?
t??(e,f)
exp
?
i
?ihi(t, e, f) (6)
The partition function is approximated by our for-
est, which is labeled as Z?(f), and the derivations
that produce reference translation is approximated
by reference derivations in Z?(f).
We estimate the parameters in log-linear model
using maximum a posteriori (MAP) estimator. It
maximizes the likelihood of the bilingual corpus
S = {fn, en}Nn=1, penalized using a gaussian prior
(L2 norm) with the probability density function
p0(?i) ? exp(??2i /2?2). We set ?2 to 1.0 in our
experiments. This results in the following gradient:
?L
??i
= Ep(t|e,f)[hi]? Ep(e|f)[hi]?
?i
?2 (7)
We use an online learning algorithm to train the
parameters. We implement stochastic gradient de-
scent (SGD) recommended by Bottou.5 The dy-
namic learning rate we use is N(i+i0) , where N is the
5http://leon.bottou.org/projects/sgd
884
number of training example, i is the training itera-
tion, and i0 is a constant number used to get a initial
learning rate, which is determined by calibration.
Algorithm 2 shows the entire process. We first
create an initial reference derivation for every train-
ing examples using bi-parsing (lines 4-5), and then
online learn the parameters using SGD (lines 6-12).
We use the GENERATE function to calculate the gra-
dient. In practice, instead of storing all the deriva-
tions in a list, we traverse the tree twice. The first
time is calculating the partition function, and the
second time calculates the gradient normalized by
partition function. During training, we also change
the derivations (line 10). When training is finished
after M epochs, the algorithm returns an averaged
weight vector (Collins, 2002) to avoid overfitting
(line 13). We use a development set to select total
epoch m, which is set as M = 5 in our experiments.
6 Experiments
Our method is able to train a large number of fea-
tures on large data. We use a set of word context
features motivated by word sense disambiguation
(Chan et al, 2007) to test scalability. A word level
context feature is a triple (f, e, f+1), which counts
the number of time that f is aligned to e and f+1 oc-
curs to the right of f . Triple (f, e, f?1) is similar ex-
cept that f?1 locates to the left of f . We retain word
alignment information in the extracted rules to ex-
ploit such features. To demonstrate the importance
of scaling up the size of training data and the effect
of our method, we compare three types of training
configurations which differ in the size of features
and data.
MERT. We use MERT (Och, 2003) to training 8
features on a small data. The 8 features is the same
as Chiang (2007) including 4 rule scores (direct and
reverse translation scores; direct and reverse lexi-
cal translation scores); 1 target side language model
score; 3 penalties for word counts, extracted rules
and glue rule. Actually, traditional pipeline often
uses such configuration.
Perceptron. We also learn thousands of context
word features together with the 8 traditional features
on a small data using perceptron. Following (Chiang
et al, 2009), we only use 100 most frequent words
for word context feature. This setting use CKY de-
TRAIN RTRAIN DEV TEST
#Sent. 519,359 186,810 878 3,789
#Word 8.6M 1.3M 23K 105K
Avg. Len. 16.5 7.3 26.4 28.0
Lon. Len. 99 95 77 116
Table 1: Corpus statistics of Chinese side, where Sent.,
Avg., Lon., and Len. are short for sentence, longest,
average, and length respectively. RTRAIN denotes the
reachable (given rule table without added rules) subset of
TRAIN data.
coder to generate n-best lists for training. The com-
plexity of CKY decoding limits the training data into
a small size. We fix the 8 traditional feature weights
as MERT to get a comparable results as MERT.
Our Method. Finally, we use our method to train
millions of features on large data. The use of large
data promises us to use full vocabulary of training
data for the context word features, which results mil-
lions of fully lexicalized context features. During
decoding, when a context feature does not exit, we
simply ignore it. The weights of 8 traditional fea-
tures are fixed the same as MERT also. We fix these
weights because the translation feature weights fluc-
tuate intensely during online learning. The main rea-
son may come from the degeneration solution men-
tioned in Section 4.2, where rare rules with very high
translation probability are selected as the reference
derivations. Another reason could be the fact that
translation features are dense intensify the fluctua-
tion. We leave learning without fixing the 8 feature
weights to future work.
6.1 Data
We focus on the Chinese-to-English translation task
in this paper. The bilingual corpus we use con-
tains 519, 359 sentence pairs, with an average length
of 16.5 in source side and 20.3 in target side,
where 186, 810 sentence pairs (36%) are reach-
able (without added rules in Section 4.2). The
monolingual data includes the Xinhua portion of
the GIGAWORD corpus, which contains 238M En-
glish words. We use the NIST evaluation sets of
2002 (MT02) as our development set, and sets of
MT03/MT04/MT05 as test sets. Table 2 shows the
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform
885
System #DATA #FEAT MT03 MT04 MT05 ALL
MERT 878 8 33.03 35.12 32.32 33.85
Perceptron 878 2.4K 32.89 34.88 32.55 33.76
Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69*
Improvement over MERT +1.16 +0.60 +0.77 +0.84
Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast
generation method with different data (only reachable or full data). #Data is the size of data for training the feature
weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01).
word alignment in both directions, and grow-diag-
final-and (Koehn et al, 2003) to generate symmet-
ric word alignment. We extract SCFG rules as de-
scribed in Chiang (2007) and also added rules (Sec-
tion 4.2). Our algorithm runs on the entire training
data, which requires to load all the rules into the
memory. To fit within memory, we cut off those
composed rules which only happen once in the train-
ing data. Here a composed rule is a rule that can be
produced by any other extracted rules. A 4-grams
language model is trained by the SRILM toolkit
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa-
pineni et al, 2002) is used to measure translation
performance.
The training data comes from a subset of the
LDC data including LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06. Since the rule ta-
ble of the entire data is too large to be loaded to
the memory (even drop one-count rules), we remove
many sentence pairs to create a much smaller data
yet having a comparable performance with the entire
data. The intuition lies in that if most of the source
words of a sentence need to be translated by the
added rules, then the word alignment may be highly
crossed and the sentence may be useless. We cre-
ate minimum rules from a sentence pair, and count
the number of source words in those minimum rules
that are added rules. For example, suppose the result
minimum rules of a sentence contain r3 which is an
added rule, then we count 1 time for the sentence. If
the number of such source word is more than 10%
of the total number, we will drop the sentence pair.
We compare the performances of MERT setting
on three bilingual data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K
data that contains 8.6M Chinese and 10.6M En-
glish words; FBIS (LDC2003E14) parts that con-
tains 6.9M Chinese and 9.1M English words. They
produce 33.11/32.32/30.47 BLEU tested on MT05
respectively. The performance of 519K data is com-
parable with that of entire data, and much higher
than that of FBIS data.
6.2 Result
Table 3 shows the performance of the three different
training configurations. The training of MERT and
perceptron run on MT02. For our method, we com-
pare two different training sets: one is trained on
all 519K sentence pairs, the other only uses 186K
reachable sentences.
Although the perceptron system exploits 2.4K
features, it fails to produce stable improvements
over MERT. The reason may come from overfitting,
since the training data for perceptron contains only
878 sentences. However, when use our method to
learn the word context feature on the 519K data,
we significantly improve the performance by 0.84
points on the entire test sets (ALL). The improve-
ments range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num-
ber of features increased into 13.9 millions, which is
impractical to be trained on the small development
set. These results confirm the necessity of exploiting
more features and learning the parameters on large
data. Meanwhile, such results also demonstrate that
we can benefits from the forest generated by our fast
method instead of traditional CKY algorithm.
Not surprisingly, the improvements are smaller
when only use 186K reachable sentences. Some-
times we even fail to gain significant improvement.
This verifies our motivation to guarantee all sentence
886
 0
 30
 60
 90
 120
 150
 180
 0  10  20  30  40  50  60  70  80  90
Tr
ain
ing
 Ti
me
(M
illis
eco
nds
)
Sentence Length
Figure 3: Plot of training times (including forest genera-
tion and SGD training) versus sentence length. We ran-
domly select 1000 sentence from the 519K data for plot-
ting.
are reachable, so as to use all training data.
6.3 Speed
How about the speed of our framework? Our method
learns in 32 mlliseconds/sentence. Figure 3 shows
training times (including forest generation and SGD
training) versus sentence length. The plot confirms
that our training algorithm scales linearly. If we
use n-best lists which generated by CKY decoder
as MERT, it takes about 3105 milliseconds/sentence
for producing 100-best lists. Our method accelerates
the speed about 97 times (even though we search
twice to calculate the gradient). This shows the effi-
ciency of our method.
The procedure of training includes two steps. (1)
Bi-parsing to initialize a reference derivation with
max score. (2) Training procedure which generates
a set of derivations to calculate the gradient and up-
date parameters. Step (1) only runs once. The av-
erage time of processing a sentence for each step
is about 9.5 milliseconds and 30.2 milliseconds re-
spectively.
For simplicity we do not compress the generated
derivations into forests, therefore the size of result-
ing derivations is fairly small, which is about 265.8
for each sentence on average, where 6.1 of them are
reference derivations. Furthermore, we use lexical-
ize operator more often than generalize operator (the
ration between them is 1.5 to 1). Lexicalize operator
is used more frequently mainly dues to that the ref-
erence derivations are initialized with reusable (thus
small) rules.
7 Related Work
Minimum error rate training (Och, 2003) is perhaps
the most popular discriminative training for SMT.
However, it fails to scale to large number of features.
Researchers have propose many learning algorithms
to train many features: perceptron (Shen et al, 2004;
Liang et al, 2006), minimum risk (Smith and Eisner,
2006; Li et al, 2009), MIRA (Watanabe et al, 2007;
Chiang et al, 2009), gradient descent (Blunsom et
al., 2008; Blunsom and Osborne, 2008). The com-
plexity of n-best lists or packed forests generation
hamper these algorithms to scale to a large amount
of data.
For efficiency, we only use neighboring deriva-
tions for training. Such motivation is same as con-
trastive estimation (Smith and Eisner, 2005; Poon et
al., 2009). The difference lies in that the previous
work actually care about their latent variables (pos
tags, segmentation, dependency trees, etc), while
we are only interested in their marginal distribution.
Furthermore, we focus on how to fast generate trans-
lation forest for training.
The local operators lexicalize/generalize are use
for greedy decoding. The idea is related to ?peg-
ging? algorithm (Brown et al, 1993) and greedy de-
coding (Germann et al, 2001). Such types of local
operators are also used in Gibbs sampler for syn-
chronous grammar induction (Blunsom et al, 2009;
Cohn and Blunsom, 2009).
8 Conclusion and Future Work
We have presented a fast generation algorithm for
translation forest which contains both reference
derivations and neighboring non-reference deriva-
tions for large-scale SMT discriminative training.
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
In this paper, we define the forest based on com-
peting derivations which only differ in one rule.
There may be better classes of forest that can pro-
duce a better performance. It?s interesting to modify
the definition of forest, and use more local operators
to increase the size of forest. Furthermore, since the
generation of forests is quite general, it?s straight to
887
apply our forest on other learning algorithms. Fi-
nally, we hope to exploit more features such as re-
ordering features and syntactic features so as to fur-
ther improve the performance.
Acknowledgement
We would like to thank Yifan He, Xianhua Li, Daqi
Zheng, and the anonymous reviewers for their in-
sightful comments. The authors were supported by
National Natural Science Foundation of China Con-
tracts 60736014, 60873167, and 60903138.
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathemat-
ics of statistical machine translation. Computational
Linguistics, 19:263?311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33?40.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP 2002.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on SMT.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc. of
ACL 2001.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING/ACL 2006.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proc. of Coling 2008.
888
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1192?1201,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Relaxed Cross-lingual Projection of Constituent Syntax
Wenbin Jiang and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, liuqun, lvyajuan}@ict.ac.cn
Abstract
We propose a relaxed correspondence as-
sumption for cross-lingual projection of con-
stituent syntax, which allows a supposed
constituent of the target sentence to corre-
spond to an unrestricted treelet in the source
parse. Such a relaxed assumption fundamen-
tally tolerates the syntactic non-isomorphism
between languages, and enables us to learn
the target-language-specific syntactic idiosyn-
crasy rather than a strained grammar di-
rectly projected from the source language syn-
tax. Based on this assumption, a novel con-
stituency projection method is also proposed
in order to induce a projected constituent tree-
bank from the source-parsed bilingual cor-
pus. Experiments show that, the parser trained
on the projected treebank dramatically out-
performs previous projected and unsupervised
parsers.
1 Introduction
For languages with treebanks, supervised models
give the state-of-the-art performance in dependency
parsing (McDonald and Pereira, 2006; Nivre et al,
2006; Koo and Collins, 2010; Martins et al, 2010)
and constituent parsing (Collins, 2003; Charniak
and Johnson, 2005; Petrov et al, 2006). To break the
restriction of the treebank scale, lots of works have
been devoted to the unsupervised methods (Klein
and Manning, 2004; Bod, 2006; Seginer, 2007; Co-
hen and Smith, 2009) and the semi-supervised meth-
ods (Sarkar, 2001; Steedman et al, 2003; McClosky
et al, 2006; Koo et al, 2008) to utilize the unan-
notated text. In recent years, researchers have also
conducted many investigations on syntax projection
(Hwa et al, 2005; Ganchev et al, 2009; Smith and
Eisner, 2009; Jiang et al, 2010), in order to borrow
syntactic knowledge from another language.
Different from the bilingual parsing (Smith and
Smith, 2004; Burkett and Klein, 2008; Zhao et al,
2009; Huang et al, 2009; Chen et al, 2010) that
improves parsing performance with bilingual con-
straints, and the bilingual grammar induction (Wu,
1997; Kuhn, 2004; Blunsom et al, 2008; Snyder et
al., 2009) that induces grammar from parallel text,
the syntax projection aims to project the syntac-
tic knowledge from one language to another. This
seems especially promising for the languages that
have bilingual corpora parallel to resource-rich lan-
guages with large treebanks. Previous works mainly
focus on dependency projection. The dependency
relationship between words in the parsed source sen-
tences can be directly projected across the word
alignment to words in the target sentences, follow-
ing the direct correspondence assumption (DCA)
(Hwa et al, 2005). Due to the syntactic non-
isomorphism between languages, DCA assumption
usually leads to conflicting or incomplete projection.
Researchers have to adopt strategies to tackle this
problem, such as designing rules to handle language
non-isomorphism (Hwa et al, 2005), and resorting
to the quasi-synchronous grammar (Smith and Eis-
ner, 2009).
For constituency projection, however, the lack of
isomorphism becomes much more serious, since a
constituent grammar describes a language in a more
detailed way. In this paper we propose a relaxed
correspondence assumption (RCA) for constituency
1192
Through
a series of
experiments
he verified
the previous hypothesis
.
?
?? ??
?? ?
?? ?
??
d
IN
DT NN IN
NNS
NP
PPNP
NP
PP
PRP
NP
DT JJ NN
.
VBD NP
VP
S
PN
P NN
VV AS
LC DEG
NN
PU
[VBD]
[NP-DT-JJ-*]
[NP]
[VP][PP]
[S-PP-*-VP-*]
[S-PP-NP-VP-*]
[S]
TOP TOP
11
2
2 3
3
4
4
5
5
Figure 1: An example for constituency projection based on the RCA assumption. The projection is from English
to Chinese. A dash dot line links a projected constituent to its corresponding treelet, which is marked with gray
background; An Arabic numeral relates a directly-projected constituent to its counter-part in the source parse.
projection. It allows a supposed constituent of
the target sentence to correspond to an unrestricted
treelet in the source parse. Such a relaxed as-
sumption fundamentally tolerates the syntactic non-
isomorphism between languages, and enables us to
learn the target-language-specific syntactic idiosyn-
crasy, rather than induce a strained grammar directly
projected from the source language syntax. We also
propose a novel cross-lingual projection method for
constituent syntax based on the RCA assumption.
Given a word-aligned source-parsed bilingual cor-
pus, a PCFG grammar can be induced for the target
language by maximum likelihood estimation on the
exhaustive enumeration of candidate projected pro-
ductions, where each nonterminal in a production
is an unrestricted treelet extracted from the source
parse. The projected PCFG grammar is then used
to parse each target sentence under the guidance of
the corresponding source tree, so as to produce an
optimized projected constituent tree.
Experiments validate the effectiveness of the
RCA assumption and the constituency projection
method. We induce a projected Chinese constituent
treebank from the FBIS Chinese-English parallel
corpus with English sentences parsed by the Char-
niak parser. The Berkeley Parser trained on the pro-
jected treebank dramatically outperforms the previ-
ous projected and unsupervised parsers. This pro-
vides an promising substitute for unsupervised pars-
ing methods, to the resource-scarce languages that
have bilingual corpora parallel to resource-rich lan-
guages with human-annotated treebanks.
In the rest of this paper we first presents the RCA
assumption, and the algorithm used to determine the
corresponding treelet in the source parse for a can-
didate constituent in the target sentence. Then we
describe the induction of the projected PCFG gram-
mar and the projected constituent treebank from the
word-aligned source-parsed parallel corpus. After
giving experimental results and the comparison with
previous unsupervised and projected parsers, we fi-
nally conclude our work and point out several as-
pects to be improved in the future work.
2 Relaxed Correspondence Assumption
The DCA assumption (Hwa et al, 2005) works well
in dependency projection. A dependency grammar
describes a sentence in a compact manner where the
syntactic information is carried by the dependency
relationships between pairs of words. It is reason-
able to audaciously assume that the relationship of
1193
Algorithm 1 Treelet Extraction Algorithm.
1: Input: Tf : parse tree of source sentence f
2: e: target sentence
3: A: word alignment of e and f
4: for i, j s.t. 1 ? i < j ? |e| do ? all spans
5: t? EXTTREELET(e, i, j,Tf ,A)
6: T?i,j? ? PRUNETREE(t)
7: Output: treelet set T for all spans of e
8: function EXTTREELET(e, i, j, T, A)
9: if T aligns totally outside ei:j then
10: return ?
11: if T aligns totally inside ei:j then
12: return {T ? root}
13: t? {T ? root} ? partly aligned inside ei:j
14: for each subtree s of T do
15: t? t ? EXTTREELET(e, i, j, s,A)
16: return t
17: function PRUNETREE(T)
18: for each node n in T do
19: merge n?s successive empty children
20: t? T
21: while t has only one non-empty subtree do
22: t? the non-empty subtree of t
23: return t
a word pair in the source sentence also holds for
the corresponding word pair in the target sentence.
Compared with dependency grammar, constituent
grammar depicts syntax in a more complex way that
gives a sentence a hierarchically branched structure.
Therefore the lack of syntactic isomorphism for con-
stituency projection becomes much more serious, it
will be hard and inappropriate to directly project the
complex constituent structure from one language to
another.
For constituency projection, we propose a relaxed
corresponding assumption (RCA) to eliminate the
influence of syntactic non-isomorphism between the
source- and target languages. This assumption al-
lows a supposed constituent of the target sentence to
correspond to an unrestricted treelet in the source
parse. A treelet is a connected subgraph in the
source constituent tree, which covers a discontigu-
ous sequence of words of the source sentence. This
property enables a supposed constituent of the tar-
get sentence not necessarily to correspond to exactly
a constituent of the source parse, so as to funda-
mentally tolerate the syntactic non-isomorphism be-
tween languages. Figure 1 gives an example of re-
* *
DT JJ *
*
* NP
VP
S
[NP-DT-JJ-*]
TOP
DT JJ *
NP
[TOP-[S-*-*-[VP-*-[NP-DT-JJ-*]]-*]](a)
(b)
* NP
DT JJ *
*
NP *
*
S
[NP-DT-JJ-*]
TOP
DT JJ *
NP
[TOP-[S-*-[NP-[NP-DT-JJ-*]-*]-*-*]]
Figure 2: Two examples for treelet pruning. Asterisks
indicate eliminated subtrees, which are represented as
empty children of their parent nodes.
laxed correspondence.
2.1 Corresponding Treelet Extraction
According to the word alignment between the source
and target sentences, we can extract the treelet out of
the source parse for any possible constituent span of
the target sentence. Algorithm 1 shows the treelet
extraction algorithm.
Given the target sentence e, the parse tree Tf of
the source sentence f , and the word alignment A
between e and f , the algorithm extracts the corre-
sponding treelet out of Tf for each candidate span
of e (line 4-6). For a given span ?i, j?, its corre-
sponding treelet in Tf can be extracted by a recur-
sive top-down traversal in the tree. If all nodes in
the current subtree T align outside of source subse-
quence ei:j , the recursion stops and returns an empty
tree ?, indicating that the subtree is eliminated from
the final treelet (line 9-10). And, if all nodes in T
align inside ei:j , the root of T is returned as the con-
cise representation of the whole subtree (line 11-12).
For the third situation, that is to say T aligns partly
inside ei:j , the recursion has to continue to investi-
gate the subtrees of T (line 14-15). The recursive
traversal finally returns a treelet t that exactly corre-
1194
sponds to the candidate constituent span ?i, j? of the
source sentence.
We can find that even for a smaller span, the recur-
sive extraction procedure still starts from the root of
the source tree. This leads to a expatiatory treelet
with some redundant nodes on the top. Function
PRUNETREE takes charge of the treelet pruning (line
6). It traverses the treelet to merge the successive
empty sibling nodes (marked with asterisks) into one
(line 18-19), then conducts a top-down pruning to
delete the redundant branches until meeting a branch
with more than one non-empty subtrees (line 20-22).
Figure 2 shows the effect of the pruning operation
with two examples. The pruning operation maps the
two original treelets into the same simplified ver-
sion, that is, the pruned treelet. The branches pruned
out of the original treelet serve as the context of the
pruned treelet. The bracketed representations of the
pruned treelets, as shown above the treelet graphs,
are used as the nonterminals of the projected target
parses.
Since the overall complexity of the algorithm is
O(|e|3), it seems inefficient to collect the treelets
for all spans in the target sentence. But in fact it
runs fast on the realistic corpus in our experiments,
we assume that the function EXTTREELET doesn?t
always consume O(|e|) because of the more or less
isomorphism between two languages.
3 Projected Grammar and Treebank
This section describes how to build a projected con-
stituent treebank based on the RCA assumption. Ac-
cording to the last section, each span of the target
sentence could correspond to a treelet in the source
parse. If a span ?i, j? has a corresponding treelet t,
a candidate projected constituent can be defined as a
triple ?i, j, t?. For an n-way partition of this span,
?i, k1?, ?k1 + 1, k2?, .., ?kn?1 + 1, j?
if each sub-span ?kp?1+1, kp? corresponds to a can-
didate constituent ?kp?1+1, kp, tp?, a candidate pro-
jected production can then be defined, denoted as
?i, j, t? ? ?i, k1, t1??k1+1, k2, t2?..?kn?1+1, j, tn?
There may be many candidate projected constituents
because of arbitrary combination, the tree projec-
tion procedure aims to find the optimum tree from
the parse forest determined by these candidate con-
stituents. Each production in the optimum tree
should satisfy this principle: the rule used in this
production appears in the whole corpus as frequently
as possible.
However, due to translation diversity and word
alignment error, the real constituent tree of the target
sentence may not be contained in the candidate pro-
jected constituents. We propose a relaxed and fault-
tolerant tree projection strategy to tackle this prob-
lem. First, based on the distribution of candidate
projected constituents over each single sentence, we
estimate the distribution over the whole corpus for
the rules used in these constituents, so as to obtain
a projected PCFG grammar. Then, using a PCFG
parser and this grammar, we parse each target sen-
tence under the guidance of the candidate projected
constituent set of the target sentence, so as to ob-
tain the optimum projected tree as far as possible.
In the following, we first describe the estimation of
the projected PCFG grammar and then show the tree
projection procedure.
3.1 Projected PCFG Grammar
From a human-annotated treebank, we can induce a
PCFG grammar by estimating the frequency of the
production rules, which are contained in the produc-
tions of the trees. But for each target sentence we
don?t know which candidate productions consist the
correct constituent tree, so we can?t estimate the fre-
quency of the production rules directly.
A reasonable hypothesis is, if a candidate pro-
jected production for a target sentence happens to be
in the correct parse of the sentence, the rule used in
this production will appear frequently in the whole
corpus. We assume that each candidate projected
production may be a part of the correct parse, but
with different probabilities. If we give each candi-
date projected production an appropriate probabil-
ity and use this probability as the appearance fre-
quency of this production in the correct parse, we
can achieve an approximation of the PCFG gram-
mar hidden in the target sentences. In this work,
we restrict the productions to be binarized to reduce
the computational complexity. It results in a bina-
rized PCFG grammar, similar to previous unsuper-
vised works.
To estimate the frequencies of the candidate pro-
1195
ductions in the correct parse of the target sentence,
we need first estimate the frequencies of the candi-
date spans, which are described as follows:
p(?i, j?|e) = # of trees including ?i, j?# of all trees (1)
The count of all binary trees of a target sentence e
can be calculated similar to the ? value calculation
in the inside-outside algorithm. Without confusion,
we adopt the symbol ?(i, j) to denote the count of
binary tree for span ?i, j?:
?(i, j) =
?
????
????
1 i = j
j?1?
k=i
?(i, k) ? ?(k + 1, j) i < j
(2)
?(1, |e|) is the count of binary trees of target sen-
tence e. We also need to calculate the count of bi-
nary tree fragments that cover the nodes outside span
?i, j?. This is similar to the calculation of the ? value
in the inside-outside algorithm. We also adopt the
symbol ?(i, j) here:
?(i, j) =
?
??????????
??????????
1 i = 1, j = |e|
|e|?
k=j+1
?(i, k) ? ?(k + 1, |e|)
+
i?1?
k=1
?(k, j) ? ?(k, j ? 1) else
(3)
For simplicity we omit some conditions in above for-
mulas. The count of trees containing span ?i, j? is
?(i, j) ? ?(i, j). Equation 1 can be rewritten as
p(?i, j?|e) = ?(i, j) ? ?(i, j)?(1, |e|) (4)
On condition that ?i, j? is a span in the parse of e,
the probability that ?i, j? has two children ?i, k? and
?k + 1, j? is
p(?i, k??k + 1, j?|?i, j?) = ?(i, k) ? ?(k + 1, j)?(i, j) (5)
Therefore, the probability that ?i, j? is a span in the
parse of e and has two children ?i, k? and ?k + 1, j?
can be calculated as follows:
p(?i,j? ? ?i, k??k + 1, j?|e)
= p(?i, j?|e) ? p(?i, k??k + 1, j?|?i, j?)
= ?(i, j) ? ?(i, k) ? ?(k + 1, j)?(1, |e|)
(6)
Since each candidate projected span aligns to one
treelet at most, this probability is also the frequency
of the candidate projected production related to the
three spans.
The counting approach above is based on the as-
sumption that there is a uniform distribution over the
projected trees for every target sentence. The inside
and outside algorithms and the other counting for-
mulae are used to calculate the expected counts un-
der this assumption. This looks like a single iteration
of EM.
A binarized projected PCFG grammar can then be
easily induced by maximum likelihood estimation.
Due to word alignment errors, free translation, and
exhaustive enumeration of possible projected pro-
ductions, such a PCFG grammar may contain too
much noisy nonterminals and production rules. We
introduce a threshold bRULE to filter the grammar. A
production rule can be reserved only if its frequency
is larger than bRULE .
3.2 Relaxed Tree Projection
The projected PCFG grammar is used in the pro-
cedure of constituency projection. Such a gram-
mar, as a kind of global syntactic knowledge, can
attenuate the negative effect of word alignment er-
ror, free translation and syntactic non-isomorphism
for the constituency projection between each sin-
gle sentence pair. To obtain as optimal a projected
constituency tree as possible, we have to integrate
two kinds of knowledge: the local knowledge in
the candidate projected production set of the target
sentence, and the global knowledge in the projected
PCFG grammar.
The integrated projection strategy can be con-
ducted as follows. We parse each target sentence
with the projected PCFG grammar G, and use the
candidate projected production set D to guide the
PCFG parsing. The parsing procedure aims to find
an optimum projected tree, which maximizes both
the PCFG tree probability and the count of produc-
tions that also appear in the candidate projected pro-
1196
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 1  2  4  8  16  32  64  128  256  512 1024
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
# 
re
se
rv
ed
 ru
le
s
Pe
rc
en
ta
ge
 in
 a
ll r
ul
es
# selected NTs
# reserved rules
Percentage in all rules
Figure 3: Rule counts corresponding to selected nonter-
minal sets, and their frequency summation proportions to
the whole rule set.
duction set. The two optimization objectives can be
coordinated as follows:
y? = argmax
y
?
d?y
(p(d|G) ? e???(d,D)) (7)
Here, d represents a production; ? is a boolean func-
tion that returns 1 if d appears in D and returns 0
otherwise; ? is a weight coefficient that needs to be
tuned to maximize the quality of the projected tree-
bank.
4 Experiments
Our work focuses on the constituency projection
from English to Chinese. The FBIS Chinese-English
parallel corpus is used to obtain a projected con-
stituent treebank. It contains 239 thousand sentence
pairs, with about 6.9/8.9 million Chinese/English
words. We parse the English sentences with the
Charniak Parser (Charniak and Johnson, 2005), and
tag the Chinese sentences with a POS tagger imple-
mented faithfully according to (Collins, 2002) and
trained on the Penn Chinese Treebank 5.0 (Xue et
al., 2005). We perform word alignment by runing
GIZA++ (Och and Ney, 2000), and then use the
alignment results for constituency projection.
Following the previous works of unsupervised
constituent parsing, we evaluate the projected parser
on the subsets of CTB 1.0 and CTB 5.0, which con-
tain no more than 10 or 40 words after the removal
of punctuation. The gold-standard POS tags are di-
rectly used for testing. The evaluation for unsu-
pervised parsing differs slightly from the standard
 10
 15
 20
 25
 30
 35
 1  2  4  8  16  32  64  128  256  512  1024
Un
la
be
le
d 
F1
 (%
)
# selected NTs
Figure 4: Performance curve of the projected PCFG
grammars corresponding to different sizes of nontermi-
nal sets.
PARSEVAL metrics, it ignores the multiplicity of
brackets, brackets of span one, and the bracket la-
bels. In all experiments we report the unlabeled F1
value which is the harmonic mean of the unlabeled
precision and recall.
4.1 Projected PCFG Grammar
An initial projected PCFG grammar can be induced
from the word-aligned and source-parsed parallel
corpus according to section 3.1. Such an initial
grammar is huge and contains a large amount of
projected nonterminals and production rules, where
many of them come from free translation and word
alignment errors. We conservatively set the filtra-
tion threshold bRULE as 1.0 to discard the rules with
frequency less than one, the rule count falls dramat-
ically from 3.3 millions to 92 thousands.
Figure 3 shows the statistics of the remained pro-
duction rules. We sort the projected nonterminals
according to their frequencies and select the top 2N
(1 ? N ? 10) best ones, and then discard the rules
that fall out of the selected nonterminal set. The fre-
quency summation of the rule set corresponding to
32 best nonterminals accounts for nearly 90% of the
frequency summation of the whole rule set.
We use the developing set of CTB 1.0 (chapter
301-325) to evaluate the performance of a series of
filtered grammars. Figure 4 gives the unlabeled F1
value of each grammar on all trees in the developing
set. The filtered grammar corresponding to the set
of top 32 nonterminals achieves the highest perfor-
mance. We denote this grammar as G32 and use it
1197
 36
 37
 38
 39
 40
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5  5
Un
la
be
le
d 
F1
 (%
)
Weight coefficient
Figure 5: Performance curve of the Berkeley Parser
trained on 5 thousand projected trees. The weight co-
efficient ? ranges from 0 to 5.
in the following tree projection procedure.
4.2 Projected Treebank and Parser
The projected grammar G32 provides global syn-
tactic knowledge for constituency projection. Such
global knowledge and the local knowledge carried
by the candidate projected production set are inte-
grated in a linear weighted manner as in Formula
7. The weight coefficient ? is tuned to maximize
the quality of the projected treebank, which is in-
directly measured by evaluating the performance of
the parser trained on it.
We select the first 5 thousand sentence pairs from
the Chinese-English FBIS corpus, and induce a se-
ries of projected treebanks using different ?, ranging
from 0 to 5. Then we train the Berkeley Parser on
each projected treebank, and test it on the develop-
ing set of CTB 1.0. Figure 5 gives the performance
curve, which reports the unlabeled F1 values of the
projected parsers on all sentences of the developing
set. We find that the best performance is achieved
with ? between 1 and 2.5, with slight fluctuation
in this range. It can be concluded that, the pro-
jected PCFG grammar and the candidate projected
production set do represent two different kinds of
constraints, and we can effectively coordinate them
by tuning the weight coefficient. Since different ?
values in this range result in slight performance fluc-
tuation of the projected parser, we simply set it to 1
for the constituency projection on the whole FBIS
corpus.
There are more than 200 thousand projected trees
 45
 45.5
 46
 46.5
 47
 47.5
 48
 48.5
 49
 49.5
Un
la
be
le
d 
F1
 (%
)
Scale of treebank
5000 10000 20000 40000 80000 160000
Figure 6: Performance curve of the Berkeley Parser
trained on different amounts of best project trees. The
scale of the selected treebank ranges from 5000 to
160000.
induced from the Chinese-English FBIS corpus. It
is a heavy burden for a parser to train on so large a
treebank. And on the other hand, the free translation
and word alignment errors result in many projected
trees of poor-quality. We design a criteria to approx-
imate the quality of the projected tree y for the target
sentence x:
Q?(y) = |x|?1
??
d?y
(p(d|G) ? e???(d,D)) (8)
and use an amount of best projected trees instead of
the whole projected treebank to train the parser. Fig-
ure 6 shows the performance of the Berkeley Parser
trained on different amounts of selected trees. The
performance of the Berkeley Parser constantly im-
proves along with the increment of selected trees.
However, treebanks containing more than 40 thou-
sand projected trees can not brings significant im-
provement. The parser trained on 160 thousand trees
only achieves an F1 increment of 0.4 points over the
one trained on 40 thousand trees. This indicates that
the newly added trees do not give the parser more
information due to their projection quality, and a
larger parallel corpus may lead to better parsing per-
formance.
The Berkeley Parser trained on 160 thousand best
projected trees is used in the final test. Table 1
gives the experimental results and the comparison
with related works. This is a sparse table since the
experiments of previous researchers focused on dif-
ferent data sets. Our projected parser significantly
1198
System CTB-TEST-40 CTB1-ALL-10 CTB5-ALL-10 CTB5-ALL-40
(Klein and Manning, 2004) ? 46.7 ? ?
(Bod, 2006) ? 47.2 ? ?
(Seginer, 2007) ? ? 54.6 38.0
(Jiang et al, 2010) 40.4 ? ? ?
our work 52.1 54.4 54.5 49.2
Table 1: The performance of the Berkeley Parser trained on 160 thousand best projected trees, compared with previous
works on constituency projection and unsupervised parsing. CTB-TEST-40: sentences? 40 words from CTB standard
test set (chapter 271-300); CTB1-ALL-10/CTB5-ALL-10: sentences ? 10 words from CTB 1.0/CTB 5.0 after the
removal of punctuation; CTB5-ALL-40: sentences ? 40 words from CTB 5.0 after the removal of punctuation.
outperforms the parser of Jiang et al (2010), where
they directly adapt the DCA assumption of (Hwa
et al, 2005) from dependency projection to con-
stituency projection and resort to a better word align-
ment and a more complicated tree projection algo-
rithm. This indicates that the RCA assumption is
more suitable for constituency projection than the
DCA assumption, and can induce a better grammar
that much more reflects the language-specific syn-
tactic idiosyncrasy of the target language.
Our projected parser also obviously surpasses ex-
isting unsupervised parsers. The parser of Seginer
(2007) performs slightly better on CTB 5.0 sen-
tences no more than 10 words, but obviously falls
behind on sentences no more than 40 words. Fig-
ure 7 shows the unlabeled F1 of our parser on
a series of subsets of CTB 5.0 with different sen-
tence length upper limits. We find that even on the
whole treebank, our parser still gives a promising
result. Compared with unsupervised parsing, con-
stituency projection can make use of the syntactic
information of another language, so that it proba-
bly induce a better grammar. Although compar-
ing a syntax projection technique to supervised or
semi-supervised techniques seems unfair, it still sug-
gests that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is a promising sub-
stitute for unsupervised parsing.
5 Conclusion and Future Works
This paper describes a relaxed correspondence as-
sumption (RCA) for constituency projection. Un-
der this assumption a supposed constituent in the
target sentence can correspond to an unrestricted
 45
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 10  20  30  40  50  60  70  80  90  100
Un
la
be
le
d 
F1
 (%
)
Upper limit of sentence length
+
Figure 7: Performance of the Berkeley Parser on subsets
of CTB 5.0 with different sentence length upper limits.
100+ indicates the whole treebank.
treelet in the parse of the source sentence. Different
from the direct correspondence assumption (DCA)
widely used in dependency projection, the RCA as-
sumption is more suitable for constituency projec-
tion, since it fundamentally tolerates the syntactic
non-isomorphism between the source and target lan-
guages. According to the RCA assumption we pro-
pose a novel constituency projection method. First, a
projected PCFG grammar is induced from the word-
aligned source-parsed parallel corpus. Then, the tree
projection is conducted on each sentence pair by a
PCFG parsing procedure, which integrates both the
global knowledge in the projected PCFG grammar
and the local knowledge in the set of candidate pro-
jected productions.
Experiments show that the parser trained on
the projected treebank significantly outperforms the
projected parsers based on the DCA assumption.
This validates the effectiveness of the RCA assump-
tion and the constituency projection method, and
indicates that the RCA assumption is more suit-
1199
able for constituency projection than the DCA as-
sumption. The projected parser also obviously sur-
passes the unsupervised parsers. This suggests
that if a resource-poor language has a bilingual
corpus parallel to a resource-rich language with a
human-annotated treebank, the constituency projec-
tion based on RCA assumption is an promising sub-
stitute for unsupervised methods.
Although achieving appealing results, our current
work is quite coarse and has many aspects to be im-
proved. First, the word alignment is the fundamental
precondition for projected grammar induction and
the following constituency projection, we can adopt
the better word alignment strategies to improve the
word alignment quality. Second, the PCFG grammar
is too weak due to its context free assumption, we
can adopt more complicated grammars such as TAG
(Joshi et al, 1975), in order to provide a more pow-
erful global syntactic constraints for the tree projec-
tion procedure. Third, the current tree projection
algorithm is too simple, more bilingual constraints
could lead to better projected trees. Last but not
least, the constituency projection and the unsuper-
vised parsing make use of different kinds of knowl-
edge, therefore the unsupervised methods can be in-
tegrated into the constituency projection framework
to achieve better projected grammars, treebanks, and
parsers.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China Contract 90920004,
60736014 and 60873167. We are grateful to the
anonymous reviewers for their thorough reviewing
and valuable suggestions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of the NIPS.
Rens Bod. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of the COLING-ACL.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of the EMNLP.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the ACL.
Wenliang Chen, Jun.ichi Kazama, and Kentaro Tori-
sawa. 2010. Bitext dependency parsing with bilingual
subtree constraints. In Proceedings of the ACL.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
the NAACL-HLT.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the EMNLP.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of the 47th ACL.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the EMNLP.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering.
Wenbin Jiang, Yajuan Lu?, Yang Liu, and Qun Liu. 2010.
Effective constituent projection across languages. In
Proceedings of the COLING.
A. K. Joshi, L. S. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal Computer Systems Sci-
ence.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings of the ACL.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of EMNLP.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proceedings of the ACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL, pages 81?88.
1200
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudoprojec-
tive dependency parsing with support vector machines.
In Proceedings of CoNLL, pages 221?225.
Franz J. Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of the ACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the ACL.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the ACL.
David Smith and Jason Eisner. 2009. Parser adaptation
and projection with quasi-synchronous grammar fea-
tures. In Proceedings of EMNLP.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the EMNLP.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of the ACL.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of the EACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the ACL-IJCNLP.
1201
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 412?420, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Iterative Annotation Transformation with Predict-Self Reestimation
for Chinese Word Segmentation
Wenbin Jiang and Fandong Meng and Qun Liu and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cn
Abstract
In this paper we first describe the technol-
ogy of automatic annotation transformation,
which is based on the annotation adaptation
algorithm (Jiang et al2009). It can auto-
matically transform a human-annotated cor-
pus from one annotation guideline to another.
We then propose two optimization strategies,
iterative training and predict-self reestimation,
to further improve the accuracy of annota-
tion guideline transformation. Experiments on
Chinese word segmentation show that, the it-
erative training strategy together with predict-
self reestimation brings significant improve-
ment over the simple annotation transforma-
tion baseline, and leads to classifiers with sig-
nificantly higher accuracy and several times
faster processing than annotation adaptation
does. On the Penn Chinese Treebank 5.0,
it achieves an F-measure of 98.43%, signif-
icantly outperforms previous works although
using a single classifier with only local fea-
tures.
1 Introduction
Annotation guideline adaptation depicts a general
pipeline to integrate the knowledge of corpora with
different underling annotation guidelines (Jiang et
al., 2009). In annotation adaptation two classifiers
are cascaded together, where the classification re-
sults of the lower classifier are used as guiding fea-
tures of the upper classifier, in order to achieve more
accurate classification. This method can automat-
ically adapt the divergence between different an-
notation guidelines and bring improvement to Chi-
nese word segmentation. However, the need of cas-
caded classification decisions makes it less practical
for tasks of high computational complexity such as
parsing, and less efficient to incorporate more than
two annotated corpora.
In this paper, we first describe the algorithm of
automatic annotation transformation. It is based on
the annotation adaptation algorithm, and it focuses
on the automatic transformation (rather than adapta-
tion) of a human-annotated corpus from one annota-
tion guideline to another. First, a classifier is trained
on the corpus with an annotation guideline not de-
sired, it is used to classify the corpus with the an-
notation guideline we want, so as to obtain a corpus
with parallel annotation guidelines. Then a second
classifier is trained on the parallelly annotated cor-
pus to learn the statistical regularity of annotation
transformation, and it is used to process the previous
corpus to transform its annotation guideline to that
of the target corpus. Instead of the online knowl-
edge integration methodology of annotation adapta-
tion, annotation transformation can lead to improved
classification accuracy in an offline manner by using
the transformed corpora as additional training data
for the classifier. This method leads to an enhanced
classifier with much faster processing than the cas-
caded classifiers in annotation adaptation.
We then propose two optimization strategies, iter-
ative training and predict-self reestimation, to fur-
ther improve the accuracy of annotation transfor-
mation. Although the transformation classifiers
can only be trained on corpora with autogenerated
(rather than gold) parallel annotations, an iterative
training procedure can gradually improve the trans-
412
formation accuracy by iteratively optimizing the par-
allelly annotated corpora. Both source-to-target and
target-to-source annotation transformations are per-
formed in each training iteration, and the trans-
formed corpora are used to provide better annota-
tions for the parallelly annotated corpora of the next
iteration; then the better parallelly annotated corpora
will result in more accurate transformation classi-
fiers, which will generate better transformed corpora
in the new iteration. The predict-self reestimation
is based on the following hypothesis, a better trans-
formation result should be easier to be transformed
back to the original form. The predict-self heuristic
is also validated by Daume? III (2009) in unsuper-
vised dependency parsing.
Experiments in Chinese word segmentation show
that, the iterative training strategy together with
predict-self reestimation brings significant improve-
ment over the simple annotation transformation
baseline. We perform optimized annotation trans-
formation from the People?s Daily (Yu et al2001)
to the Penn Chinese Treebank 5.0 (CTB) (Xue et
al., 2005), in order to improve the word segmenter
with CTB annotation guideline. Compared to anno-
tation adaptation, the optimized annotation transfor-
mation strategy leads to classifiers with significantly
higher accuracy and several times faster processing
on the same data sets. On CTB 5.0, it achieves an F-
measure of 98.43%, significantly outperforms pre-
vious works although using a single classifier with
only local features.
The rest of the paper is organized as follows.
Section 2 describes the classification-based Chinese
word segmentation method. Section 3 details the
simple annotation transformation algorithm and the
two optimization methods. After the introduction of
related works in section 4, we give the experimental
results on Chinese word segmentation in section 5.
2 Classification-Based Chinese Word
Segmentation
Chinese word segmentation can be formalized as
the problem of sequence labeling (Xue and Shen,
2003), where each character in the sentence is given
a boundary tag denoting its position in a word. Fol-
lowing Ng and Low (2004), joint word segmenta-
tion and part-of-speech (POS) tagging can also be
Algorithm 1 Perceptron training algorithm.
1: Input: Training examples (xi, yi)
2: ~?? 0
3: for t? 1 .. T do
4: for i? 1 .. N do
5: zi ? argmaxz?GEN(xi) ?(xi, z) ? ~?
6: if zi 6= yi then
7: ~?? ~? + ?(xi, yi)??(xi, zi)
8: Output: Parameters ~?
solved in a character classification approach by ex-
tending the boundary tags to include POS informa-
tion. For word segmentation we adopt the 4 bound-
ary tags of Ng and Low (2004), b, m, e and s, where
b, m and e mean the beginning, the middle and the
end of a word, and s indicates a single-character
word. The word segmentation result can be gen-
erated by splitting the labeled character sequence
into subsequences of pattern s or bm?e, indicating
single-character words or multi-character words, re-
spectively.
We choose the perceptron algorithm (Collins,
2002) to train the character classifier. It is an online
training algorithm and has been successfully used in
many NLP tasks, including POS tagging (Collins,
2002), parsing (Collins and Roark, 2004) and word
segmentation (Zhang and Clark, 2007; Jiang et al
2008; Zhang and Clark, 2010).
The training procedure learns a discriminative
model mapping from the inputs x ? X to the outputs
y ? Y , where X is the set of sentences in the train-
ing corpus and Y is the set of corresponding labeled
results. We use the function GEN(x) to enumerate
the candidate results of an input x, and the function
? to map a training example (x, y) ? X ? Y to a
feature vector ?(x, y) ? Rd. Given the character
sequence x, the decoder finds the output F (x) that
maximizes the score function:
F (x) = argmax
y?GEN(x)
S(y|~?,?, x)
= argmax
y?GEN(x)
?(x, y) ? ~?
(1)
Where ~? ? Rd is the parameter vector (that is, the
discriminative model) and ?(x, y) ? ~? is the inner
product of ?(x, y) and ~?.
Algorithm 1 shows the perceptron algorithm for
tuning the parameter ~?. The ?averaged parameters?
413
Type Feature Templates
Unigram C?2 C?1 C0
C1 C2
Bigram C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Property Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Table 1: Feature templates for classification-based Chi-
nese segmentation model.
technology (Collins, 2002) is used for better per-
formance. The feature templates for the classifier
is shown in Table 1. C0 denotes the current char-
acter, while C?i/Ci denote the ith character to the
left/right of C0. The function Pu(?) returns true
for a punctuation character and false for others, the
function T (?) classifies a character into four types:
number, date, English letter and others.
3 Iterative and Predict-Self Annotation
Transformation
This section first describes the technology of au-
tomatic annotation transformation, then introduces
the two optimization strategies, iterative training and
predict-self reestimation. Iterative training takes
a global view, it conducts several rounds of bidi-
rectional annotation transformations, and improve
the transformation performance round by round.
Predict-self reestimation takes a local view instead,
it considers each training sentence, and improves the
transformation performance by taking into account
the predication result of the reverse transformation.
The two strategies can be adopted jointly to obtain
better transformation performance.
3.1 Automatic Annotation Transformation
Annotation adaptation can integrate the knowledge
from two corpora with different underling annota-
tion guidelines. First, a classifier (source classi-
fier) is trained on the corpus (source corpus) with
an annotation standard (source annotation) not de-
sired, it is then used to classify the corpus (target
corpus) with the annotation standard (target annota-
tion) we want. Then a second classifier (transforma-
tion classifier 1) is trained on the target corpus with
1It is called target classifier in (Jiang et al2009). We
think that transformation classifier better reflects its role, the
Type Feature Templates
Baseline C?2 C?1 C0
C1 C2
C?2C?1 C?1C0 C0C1
C1C2 C?1C1
Pu(C0)
T (C?2)T (C?1)T (C0)T (C1)T (C2)
Guiding ?
C?2 ? ? C?1 ? ? C0 ? ?
C1 ? ? C2 ? ?
C?2C?1 ? ? C?1C0 ? ? C0C1 ? ?
C1C2 ? ? C?1C1 ? ?
Pu(C0) ? ?
T (C?2)T (C?1)T (C0)T (C1)T (C2) ? ?
Table 2: Feature templates for annotation transformation,
where ? is short for ?(C0), representing the source an-
notation of C0.
the source classifier?s classification result as guid-
ing features. In decoding, a raw sentence is first de-
coded by the source classifier, and then inputted into
the transformation classifier together with the anno-
tations given by the source classifier, so as to obtain
an improved classification result.
However, annotation adaptation has a drawback,
it has to cascade two classifiers in decoding to inte-
grate the knowledge in two corpora, thus seriously
degrades the processing speed. This paper describes
a variant of annotation adaptation, name annotation
transformation, aiming at automatic transformation
(rather than adaptation) between annotation stan-
dards of human-annotated corpora. In annotation
transformation, a source classifier and a transforma-
tion classifier are trained in the same way as in an-
notation adaptation. The transformation classifier is
used to process the source corpus, with the classi-
fication label derived from the segmented sentences
as the guiding features, so as to relabel the source
corpus with the target annotation guideline. By inte-
grating the target corpus and the transformed source
corpus for the training of the character classifier, im-
proved classification accuracy can be achieved.
Both the source classifier and the transforma-
tion classifier are trained with the perceptron algo-
rithm. The feature templates used for the source
classifier are the same with those for the baseline
renaming also avoids name confusion in the optimized annota-
tion transformation.
414
Algorithm 2 Baseline annotation transformation.
1: function ANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Ms?t ? TRANSTRAIN(Cst , Ct)
5: Cts ? TRANSANNOTATE(Ms?t, Cs)
6: Ct? ? Cts ? Ct
7: return Ct?
8: function DECODE(M, ?, x)
9: return argmaxy?GEN(x) S(y|M,?, x)
character classifier. The feature templates for the
transformation classifier are the same with those in
annotation adaptation, as listed in Table 2. Al-
gorithm 2 shows the overall training algorithm
for annotation transformation. Cs and Ct denote
the source corpus and the target corpus; Ms and
Ms?t denote the source classifier and the trans-
formation classifier; Cqp denotes the p corpus re-
labeled in q annotation guideline, for example Cts
is the source corpus transformed to target annota-
tion guideline; Functions TRAIN and TRANSTRAIN
both invoke the perceptron algorithm, yet with
different feature sets; Functions ANNOTATE and
TRANSANNOTATE call the function DECODE with
different models (source/transformation classifiers),
feature functions (without/with guiding features),
and inputs (raw/source-annotated sentences).
The best training iterations for the functions
TRAIN and TRANSTRAIN are determined on the de-
veloping sets of the source corpus and the target
corpus, respectively. In the algorithm the param-
eters corresponding to developing sets are omitted
for simplicity. Compared to the online knowledge
integration methodology of annotation adaptation,
annotation transformation leads to improved perfor-
mance in an offline manner by integrating corpora
before the training procedure. This manner could
achieve processing several times as fast as the cas-
caded classifiers in annotation adaptation. In the fol-
lowing we will describe the two optimization strate-
gies in details.
3.2 Iterative Training for Annotation
Transformation
The training of annotation transformation is based
on an auto-generated (rather than gold) parallelly an-
notated corpus, where the source annotation is pro-
Algorithm 3 Iterative annotation transformation.
1: function ITERANNOTRANS(Cs, Ct)
2: Ms ? TRAIN(Cs)
3: Cst ? ANNOTATE(Ms, Ct)
4: Mt ? TRAIN(Ct)
5: Cts ? ANNOTATE(Mt, Cs)
6: repeat
7: Ms?t ? TRANSTRAIN(Cst , Ct)
8: Mt?s ? TRANSTRAIN(Cts, Cs)
9: Cts ? TRANSANNOTATE(Ms?t, Cs)
10: Cst ? TRANSANNOTATE(Mt?s, Ct)
11: Ct? ? Cts ? Ct
12: M? ? TRAIN(Ct?)
13: until EVAL(M?) converges
14: return Ct?
15: function DECODE(M, ?, x)
16: return argmaxy?GEN(x) S(y|M,?, x)
vided by the source classifier. Therefore, the perfor-
mance of transformation training is correspondingly
determined by the accuracy of the source classifier.
We propose an iterative training procedure to
gradually improve the transformation accuracy by
iteratively optimizing the parallelly annotated cor-
pora. In each training iteration, both source-to-target
and target-to-source annotation transformations are
performed, and the transformed corpora are used to
provide better annotations for the parallelly anno-
tated corpora of the next iteration. Then in the new
iteration, the better parallelly annotated corpora will
result in more accurate transformation classifiers, so
as to generate better transformed corpora.
Algorithm 3 shows the overall procedure of the
iterative training method. The loop of lines 6-13
iteratively performs source-to-target and target-to-
source annotation transformations. The source an-
notations of the parallelly annotated corpora, Cst and
Cts, are initialized by applying the source and tar-
get classifiers respectively on the target and source
corpora (lines 2-5). In each training iteration, the
transformation classifiers are trained on the current
parallelly annotated corpora (lines 7-8), they are
used to produce the transformed corpora (lines 9-10)
which provide better annotations for the parallelly
annotated corpora of the next iteration. The itera-
tive training terminates when the performance of the
classifier trained on the merged corpus Cts ? Ct con-
verges.
415
The discriminative training of TRANSTRAIN pre-
dicts the target annotations with the guidance of
source annotations. In the first iteration, the trans-
formed corpora generated by the transformation
classifiers are better than the initialized ones gener-
ated by the source and target classifiers, due to the
assistance of the guiding features. In the follow-
ing iterations, the transformed corpora provide bet-
ter annotations for the parallelly annotated corpora
of the subsequent iteration, the transformation ac-
curacy will improve gradually along with optimiza-
tion of the parallelly annotated corpora until conver-
gence.
3.3 Predict-Self Reestimation for Annotation
Transformation
The predict-self hypothesis is implicit in many unsu-
pervised learning approaches, such as Markov ran-
dom field. This methodology has also been success-
fully used by Daume? III (2009) in unsupervised de-
pendency parsing. The basic idea of predict-self is
that, if a prediction is a better candidate for an input,
it can be easier converted back to the original input
by a reverse procedure. If applied to the task of an-
notation transformation, predict-self indicates that a
better transformation candidate following the target
annotation guideline can be easier transformed back
to the original form following the source annotation
guideline.
The most intuitionistic strategy to introduce the
predict-self methodology into annotation transfor-
mation is using a reversed annotation transforma-
tion procedure to filter out unreliable predictions of
the previous transformation. In detail, a source-to-
target annotation transformation is performed on the
source annotated sentence to obtain a prediction that
follows the target annotation guideline, then a sec-
ond, target-to-source transformation is performed
on this prediction result to check whether it can
be transformed back to the previous source annota-
tion. Transformation results failing in this reversal
verification are discarded, so this strategy is named
predict-self filtration.
A more precious strategy can be called predict-
self reestimation. Instead of using the reversed
transformation procedure for filtration, the rees-
timation strategy integrates the scores given by
the source-to-target and target-to-source annotation
transformation models when evaluating the transfor-
mation candidates. By properly tuning the relative
weights of the two transformation directions, bet-
ter transformation performance would be achieved.
The scores of the two transformation models are
weighted integrated in a log-linear manner:
S+(y|Ms?t,Mt?s,?, x)
= (1? ?)? S(y|Ms?t,?, x)
+ ?? S(x|Mt?s,?, y)
(2)
The weight parameter ? is tuned on the develop-
ing set. To integrating the predict-self reestima-
tion into the iterative transformation training, a re-
versed transformation model is introduced and the
enhanced scoring function above is used when the
function TRANSANNOTATE invokes the function
DECODE.
4 Related Works
Researches focused on the automatic adaptation
between different corpora can be roughly clas-
sified into two kinds, adaptation between differ-
ent domains (with different statistical distribution)
(Blitzer et al2006; Daume? III, 2007), and adapta-
tion between different annotation guidelines (Jiang
et al2009; Zhu et al2011). There are also
some efforts that totally or partially resort to man-
ual transformation rules, to conduct treebank con-
version (Cahill and Mccarthy, 2002; Hockenmaier
and Steedman, 2007; Clark and Curran, 2009), and
word segmentation guideline transformation (Gao
et al2004; Mi et al2008). This work focuses
on the automatic transformation between annotation
guidelines, and proposes better annotation transfor-
mation technologies to improve the transformation
accuracy and the utilization rate of human-annotated
knowledge.
The iterative training procedure proposed in this
work shares some similarity with the co-training al-
gorithm in parsing (Sarkar, 2001), where the train-
ing procedure lets two different models learn from
each other during parsing the raw text. The key
idea of co-training is utilize the complementarity of
different parsing models to mine additional training
data from raw text, while iterative training for an-
notation transformation emphasizes the iterative op-
timization of the parellelly annotated corpora used
416
Partition Sections # of word
CTB
Training 1? 270 0.47M
400? 931
1001? 1151
Developing 301? 325 6.66K
Test 271? 300 7.82K
PD
Training 02? 06 5.86M
Test 01 1.07M
Table 3: Data partitioning for CTB and PD.
to train the transformation models. The predict-
self methodology is implicit in many unsupervised
learning approaches, it has been successfully used
by (Daume? III, 2009) in unsupervised dependency
parsing. We adapt this idea to the scenario of anno-
tation transformation to improve transformation ac-
curacy.
In recent years many works have been devoted to
the word segmentation task. For example, the in-
troduction of global training or complicated features
(Zhang and Clark, 2007; Zhang and Clark, 2010);
the investigation of word structures (Li, 2011);
the strategies of hybrid, joint or stacked modeling
(Nakagawa and Uchimoto, 2007; Kruengkrai et al
2009; Wang et al2010; Sun, 2011), and the semi-
supervised and unsupervised technologies utilizing
raw text (Zhao and Kit, 2008; Johnson and Gold-
water, 2009; Mochihashi et al2009; Hewlett and
Cohen, 2011). We estimate that the annotation trans-
formation technologies can be adopted jointly with
complicated features, system combination and semi-
supervised/unsupervised technologies to further im-
prove segmentation performance.
5 Experiments and Analysis
We perform annotation transformation from Peo-
ple?s Daily (PD) (Yu et al2001) to Penn Chi-
nese Treebank 5.0 (CTB) (Xue et al2005), follow-
ing the same experimental setting as the annotation
adaptation work (Jiang et al2009) for convenience
of comparison. The two corpora are segmented fol-
lowing different segmentation guidelines and differ
largely in quantity of data. CTB is smaller in size
with about 0.5M words, while PD is much larger,
containing nearly 6M words.
Test on (F1%)
Train on CTB SPD
CTB 97.35 86.65(? 10.70)
SPD 91.23(? 3.02) 94.25
Table 4: Performance of the perceptron classifiers for
Chinese word segmentation.
Model Time (s) Accuracy (F1%)
Merging 1.33 93.79
Anno. Adapt. 4.39 97.67
Anno. Trans. 1.33 97.69
Baseline 1.21 97.35
Table 5: Comparison of the baseline annotation transfor-
mation, annotation adaptation and a simple corpus merg-
ing strategy.
To approximate more general scenarios of anno-
tation adaptation problems, we extract from PD a
subset which is comparable to CTB in size. We ran-
domly select 20, 000 sentences (0.45M words) from
the PD training data as the new training set, and
1000/1000 sentences from the PD test data as the
new test/developing set. 2 We name the smaller ver-
sion of PD as SPD. The balanced source corpus and
target corpus also facilitate the investigation of an-
notation transformation.
5.1 Baseline Classifiers for Word Segmentation
We train the baseline perceptron classifiers de-
scribed in section 2 on the training sets of SPD
and CTB, using the developing sets to determine the
best training iterations. The performance measure-
ment indicators for word segmentation is balanced
F-measure, F = 2PR/(P + R), a function of Pre-
cision P and Recall R. where P is the percentage
of words in segmentation result that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Accuracies of the baseline classifiers are listed in
Table 4. We also report the performance of the clas-
sifiers on the test sets of the opposite corpora. Ex-
perimental results are in line with our expectations.
A classifier performs better in its corresponding test
set, and performs significantly worse on a test set
following a different annotation guideline.
2There are many extremely long sentences in original PD
corpus, we split them into normal sentences according to period
punctuations.
417
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training
Baseline annotation transformation
Figure 1: Learning curve of iterative training for annota-
tion transformation.
5.2 Annotation Transformation vs. Annotation
Adaptation
Experiments of annotation transformation are con-
ducted on the direction of SPD-to-CTB. The trans-
formed corpus can be merged into the regular cor-
pus, so as to train an enhanced classifier. As com-
parison, the cascaded model of annotation adapta-
tion (Jiang et al2009) is faithfully implemented
(yet using our feature representation) and tested on
the same adaptation direction.
Table 5 shows the performances of the classi-
fiers resulted by the baseline annotation transforma-
tion and annotation adaptation, as well as the clas-
sifier trained on the directly merged corpus. The
time costs for decoding are also listed to facilitate
the comparison of practicality. We find that the sim-
ple corpus merging strategy leads to dramatic de-
crease in accuracy, due to the different and incom-
patible annotation guidelines. The baseline annota-
tion transformation method leads to a classifier with
accuracy increment comparable to that of the anno-
tation adaptation strategy, while consuming only one
third of the decoding time.
5.3 Iterative Training with Predict-Self
Reestimation
We adopt the iterative training strategy to the base-
line annotation transformation model. The CTB de-
veloping set is used to determine the best training
iteration for annotation transformation from SPD to
CTB. After each iteration, we test the performance
of the classifier trained on the merged corpus. Fig-
ure 1 shows the performance curve, with iterations
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (F
%)
Predict-self ratio
Predict-self reestimation
Predict-self filtration
Baseline annotation transformation
Figure 2: Performance of predict-self filtration and
predict-self reestimation.
 95.4
 95.6
 95.8
 96
 96.2
 96.4
 0  1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
%)
Training iterations
Iterative training with predict-self reestimation
Iterative training
Figure 3: Learning curve of iterative training with
predict-self reestimation for annotation transformation.
ranging from 1 to 10. The performance of the base-
line annotation transformation model is naturally in-
cluded in the curve (located at iteration 1). The
curve shows that the performance of the classifier
trained on the merged corpus consistently improves
from iteration 2 to iteration 5.
Experimental results of predict-self filtration and
predict-self reestimation are shown in Figure 2.
The curve shows the performance of the predict-self
reestimation according to a series of weight param-
eters, ranging from 0 to 1 with step 0.05. The point
at ? = 0 shows the performance of the baseline
annotation transformation strategy. The upper hor-
izontal line shows the performance of predict-self
filtration. We find that predict-self filtration brings
slight improvement over the baseline, and predict-
self reestimation outperforms the filtration strategy
when ? falls in a proper range. An initial analysis
on the experimental results of predict-self filtration
418
Model Time (s) Accuracy (F1%)
SPD? CTB
Anno. Adapt. 4.39 97.67
Opt. Trans. 1.33 97.97
PD? CTB
Anno. Adapt. 4.76 98.15
Opt. Trans. 1.37 98.43
Previous Works
(Jiang et al2008) 97.85
(Kruengkrai et al2009) 97.87
(Zhang and Clark, 2010) 97.79
(Sun, 2011) 98.17
Table 6: The performance of the iterative annotation
transformation with predict-self reestimation compared
with annotation adaptation.
shows that, the filtration discards 5% of the train-
ing sentences and these discarded sentences contain
nearly 10% of training words. It can be confirmed
that the sentences discarded by predict-self filtra-
tion are much longer and more complicated. With a
properly tuned weight, predict-self reestimation can
make better use of the training data. The best F-
measure improvement achieved over the annotation
transformation baseline is 0.3 points, a little worse
than that brought by iterative training.
Figure 3 shows the performance curve of iterative
annotation transformation with predict-self reesti-
mation. We find that the predict-self reestimation
brings improvement to the iterative training at each
iteration. The maximum performance is achieved
at iteration 4. The corresponding model is evalu-
ated on the test set of CTB, table 6 shows the ex-
perimental results. Compared to annotation adapta-
tion, the optimized annotation transformation strat-
egy leads to a classifier with significantly higher ac-
curacy and several times faster processing. When
using the whole PD as the source corpus, the final
classifier 3 achieves an F-measure of 98.43%, sig-
nificantly outperforms previous works although us-
ing a single classifier with only local features. Of
course, the comparison between our system and pre-
vious works without using additional training data
is unfair. This work aim to find another way to im-
prove Chinese word segmentation, which focuses on
the collection of more training data instead of mak-
3The predict-self reestimation ratio ? is fixed after the first
training iteration for efficiency.
ing full use of a certain corpus. We believe that the
performance can be further improved by adopting
the advanced technologies of previous works, such
as complicated features and model combination.
Considering the fact that today some corpora for
word segmentation are really large (usually tens
of thousands of sentences), it is necessary to ob-
tain the latest CTB and investigate whether and
how much does annotation transformation bring im-
provement to a much higher baseline. On the other
hand, it is valuable to conduct experiments with
more source-annotated training data, such as the
SIGHAN dataset, to investigate the trend of im-
provement along with the increment of the addi-
tional annotated sentences. It is also valuable to
evaluate the improved word segmenter on the out-
of-domain datasets. However, currently most cor-
pora for Chinese word segmentation do not explic-
itly distinguish the domains of their data sections, it
makes such evaluations difficult to conduct.
6 Conclusion and Future Works
In this paper, we first describe an annotation trans-
formation algorithm to automatically transform a
human-annotated corpus from one annotation guide-
line to another. Then we propose two optimization
strategies, iterative training and predict-self reesti-
mation, to further improve the accuracy of anno-
tation guideline transformation. On Chinese word
segmentation, the optimized annotation transforma-
tion strategy leads to classifiers with obviously bet-
ter performance and several times faster processing
on the same datasets, compared to annotation adap-
tation. When adopting the whole PD as the source
corpus, the final classifier significantly outperforms
previous works on CTB 5.0, although using a single
classifier with only local features.
As future works, we will investigate the accel-
eration of the iterative training and the weight pa-
rameter tuning, and extend the optimized annotation
transformation strategy to joint Chinese word seg-
mentation and POS tagging, parsing and other NLP
tasks.
Acknowledgments
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004
419
and 61100082, and 863 State Key Project No.
2011AA01A207. We are grateful to the anonymous
reviewers for their thorough reviewing and valuable
suggestions.
References
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
Aoife Cahill and Mairead Mccarthy. 2002. Automatic
annotation of the penn treebank with lfg f-structure in-
formation. In in Proceedings of the LREC Workshop.
Stephen Clark and James R. Curran. 2009. Comparing
the accuracy of ccg and penn treebank parsers. In Pro-
ceedings of ACL-IJCNLP.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL 2004.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP, pages 1?8, Philadelphia, USA.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of ACL.
Hal Daume? III. 2009. Unsupervised search-based struc-
tured prediction. In Proceedings of ICML.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceedings
of ACL.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with bve and mdl. In Pro-
ceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
a corpus of ccg derivations and dependency structures
extracted from the penn treebank. In Computational
Linguistics, volume 33(3), pages 355?396.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint chinese word segmentation and pos
tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chineseword segmenta-
tion. In Proceedings of ACL.
Haitao Mi, Deyi Xiong, and Qun Liu. 2008. Research
on strategy of integrating chinese lexical analysis and
parser. In Journal of Chinese Information Processing.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested pitman-yor language modeling. In Proceedings
of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A hy-
brid approach to word segmentation and pos tagging.
In Proceedings of ACL.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of NAACL.
Weiwei Sun. 2011. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A
character-based joint model for chinese word segmen-
tation. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In Proceedings of SIGHAN
Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural Lan-
guage Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,
Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,
and Weidong Zhan. 2001. Processing norms of mod-
ern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and pos-tagging using a sin-
gle discriminative model. In Proceedings of EMNLP.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
for word segmentation and named entity recognition.
In Proceedings of SIGHAN Workshop.
Muhua Zhu, Jingbo Zhu, and Minghan Hu. 2011. Better
automatic treebank conversion using a feature-based
approach. In Proceedings of ACL.
420
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1191?1200, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Left-to-Right Tree-to-String Decoding with Prediction
Yang Feng? Yang Liu? Qun Liu? Trevor Cohn?
? Department of Computer Science
The University of Sheffield, Sheffield, UK
{y.feng, t.cohn}@sheffield.ac.uk
? State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing, China
liuyang2011@tsinghua.edu.cn
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
liuqun@ict.ac.cn
Abstract
Decoding algorithms for syntax based ma-
chine translation suffer from high compu-
tational complexity, a consequence of in-
tersecting a language model with a con-
text free grammar. Left-to-right decoding,
which generates the target string in order,
can improve decoding efficiency by simpli-
fying the language model evaluation. This
paper presents a novel left to right decod-
ing algorithm for tree-to-string translation, us-
ing a bottom-up parsing strategy and dynamic
future cost estimation for each partial trans-
lation. Our method outperforms previously
published tree-to-string decoders, including a
competing left-to-right method.
1 Introduction
In recent years there has been rapid progress in the
development of tree-to-string models for statistical
machine translation. These models use the syntac-
tic parse tree of the source language to inform its
translation, which allows the models to capture con-
sistent syntactic transformations between the source
and target languages, e.g., from subject-verb-object
to subject-object-verb word orderings. Decoding al-
gorithms for grammar-based translation seek to find
the best string in the intersection between a weighted
context free grammar (the translation mode, given a
source string/tree) and a weighted finite state accep-
tor (an n-gram language model). This intersection
is problematic, as it results in an intractably large
grammar, and makes exact search impossible.
Most researchers have resorted to approximate
search, typically beam search (Chiang, 2007). The
decoder parses the source sentence, recording the
target translations for each span.1 As the partial
translation hypothesis grows, its component ngrams
are scored and the hypothesis score is updated. This
decoding method though is inefficient as it requires
recording the language model context (n? 1 words)
on the left and right edges of each chart cell. These
contexts allow for boundary ngrams to be evaluated
when the cell is used in another grammar produc-
tion. In contrast, if the target string is generated
in left-to-right order, then only one language model
context is required, and the problem of language
model evaluation is vastly simplified.
In this paper, we develop a novel method of left-
to-right decoding for tree-to-string translation using
a shift-reduce parsing strategy. A central issue in
any decoding algorithm is the technique used for
pruning the search space. Our left-to-right decod-
ing algorithm groups hypotheses, which cover the
same number of source words, into a bin. Pruning
requires the evaluation of different hypotheses in the
same bin, and elimating the least promising options.
As each hypotheses may cover different sets of tree
1The process is analogous for tree-to-string models, except
that only rules and spans matching those in the source trees are
considered. Typically nodes are visited according to a post-
order traversal.
1191
nodes, it is necessary to consider the cost of uncov-
ered nodes, i.e., the future cost. We show that a good
future cost estimate is essential for accurate and effi-
cient search, leading to high quality translation out-
put.
Other researchers have also considered the left-
to-right decoding algorithm for tree-to-string mod-
els. Huang and Mi (2010) developed an Earley-
style parsing algorithm (Earley, 1970). In their ap-
proach, hypotheses covering the same number of
tree nodes were binned together. Their method uses
a top-down depth-first search, with a mechanism for
early elimation of some rules which lead to dead-
ends in the search. Huang and Mi (2010)?s method
was shown to outperform the traditional post-order-
traversal decoding algorithm, considering fewer hy-
potheses and thus decoding much faster at the same
level of performance. However their algorithm used
a very rough estimate of future cost, resulting in
more search errors than our approach.
Our experiments show that compared with the
Earley-style left-to-right decoding (Huang and Mi,
2010) and the traditional post-order-traversal de-
coding (Liu et al 2006) algorithms, our algorithm
achieves a significant improvement on search capac-
ity and better translation performance at the same
level of speed.
2 Background
A typical tree-to-string system (Liu et al 2006;
Huang et al 2006) searches through a 1-best source
parse tree for the best derivation. It transduces the
source tree into a target-language string using a Syn-
chronous Tree Substitution Grammar (STSG). The
grammar rules are extracted from bilingual word
alignments using the GHKM algorithm (Galley et
al., 2004).
We will briefly review the traditional decoding al-
gorithm (Liu et al 2006) and the Earley-style top-
down decoding algorithm (Huang and Mi, 2010) for
the tree-to-string model.
2.1 Traditional Decoding
The traditional decoding algorithm processes source
tree nodes one by one according to a post-order
traversal. For each node, it applies matched STSG
rules by substituting each non-terminal with its cor-
in theory beam search
traditional O(nc|?V |4(g?1)) O(ncb2)
top-down O(c(cr)d|V |g?1) O(ncb)
bottom-up O((cr)d|V |g?1) O(nub)
Table 1: Time complexity of different algorithms. tra-
ditional : Liu et al(2006), top-down : Huang and Mi
(2010). n is the source sentence length, b is the beam
width, c is the number of rules used for each node, V
is the target word vocabulary, g is the order of the lan-
guage model, d is the depth of the source parse tree, u is
the number of viable prefixes for each node and r is the
maximum arity of each rule.
responding translation. For the derivation in Figure
1 (b), the traditional algorithm applies r2 at node
NN2
r2 : NN2 (jieguo) ? the result,
to obtain ?the result? as the translation of NN2. Next
it applies r4 at node NP,
r4 : NP ( NN1 (toupiao), x1 : NN2 )
? x1 of the vote
and replaces NN2 with its translation ?the result?,
then it gets the translation of NP as ?the result of the
vote?.
This algorithm needs to contain boundary words
at both left and right extremities of the target string
for the purpose of LM evaluation, which leads to a
high time complexity. The time complexity in the-
ory and with beam search (Huang and Mi, 2010) is
shown in Table 1.
2.2 Earley-style Top-down Decoding
The Earley-style decoding algorithm performs a top-
down depth-first parsing and generates the target
translation left to right. It applies Context-Free
Grammar (CFG) rules and employs three actions:
predict, scan and complete (Section 3.1 describes
how to convert STSG rules into CFG rules). We can
simulate its translation process using a stack with a
dot  indicating which symbol to process next. For
the derivation in Figure 1(b) and CFG rules in Fig-
ure 1(c), Figure 2 illustrates the whole translation
process.
The time complexity is shown in Table 1 .
1192
3 Bottom-Up Left-to-Right Decoding
We propose a novel method of left-to-right decoding
for tree-to-string translation using a bottom-up pars-
ing strategy. We use viable prefixes (Aho and John-
son, 1974) to indicate all possible target strings the
translations of each node should starts with. There-
fore, given a tree node to expand, our algorithm
can drop immediately to target terminals no matter
whether there is a gap or not. We say that there is a
gap between two symbols in a derivation when there
are many rules separating them, e.g. IP r6? ... r4?
NN2. For the derivation in Figure 1(b), our algo-
rithm starts from the root node IP and applies r2
first although there is a gap between IP and NN2.
Then it applies r4, r5 and r6 in sequence to generate
the translation ?the result of the vote was released
at night?. Our algorithm takes the gap as a black-
box and does not need to fix which partial deriva-
tion should be used for the gap at the moment. So it
can get target strings as soon as possible and thereby
perform more accurate pruning. A valid derivation
is generated only when the source tree is completely
matched by rules.
Our bottom-up decoding algorithm involves the
following steps:
1. Match STSG rules against the source tree.
2. Convert STSG rules to CFG rules.
3. Collect the viable prefix set for each node in a
post-order transversal.
4. Search bottom-up for the best derivation.
3.1 From STSG to CFG
After rule matching, each tree node has its applica-
ble STSG rule set. Given a matched STSG rule, our
decoding algorithm only needs to consider the tree
node the rule can be applied to and the target side,
so we follow Huang and Mi (2010) to convert STSG
rules to CFG rules. For example, an STSG rule
NP ( NN1 (toupiao), x1 : NN2 ) ? x1 of the vote
can be converted to a CFG rule
NP ? NN2 of the vote
The target non-terminals are replaced with corre-
sponding source non-terminals. Figure 1 (c) shows
all converted CFG rules for the toy example. Note
IP
NP
NN1
to?up?`ao
NN2
j??eguo?
VP
NT
wa?nsha`ng
VV
go?ngbu`
(a) Source parse tree
r6: IP
NP VP
? ?
r4: NP
NN1
to?up?`ao
NN2
r5: VP
NT
wa?nsha`ng
VV
go?ngbu`
?
r2: NN2
j??eguo?
the result of the vote was released at night
(b) A derivation
r1: NN1 ? the vote
r2: NN2 ? the result
r3: NP ? NN2 of NN1
r4: NP ? NN2 of the vote
r5: VP ? was released at night
r6: IP ? NP VP
r7: IP ? NN2 of the vote VP
r8: IP ? VP NP
(c) Target-side CFG rule set
Figure 1: A toy example.
that different STSG rules might be converted to the
same CFG rule despite having different source tree
structures.
3.2 Viable Prefix
During decoding, how do we decide which rules
should be used next given a partial derivation, es-
pecially when there is a gap? A key observation is
that some rules should be excluded. For example,
any derivation for Figure 1(a) will never begin with
r1 as there is no translation starting with ?the vote?.
In order to know which rules can be excluded for
each node, we can recursively calculate the start-
ing terminal strings for each node. For example,
1193
NN1: {the vote} NN2: {the result}
NT: ? VV: ?
NP: {the result}
VP: {was released at night}
IP: {the result, was released at night}
Table 2: The Viable prefix sets for Figure 1 (c)
according to r1, the starting terminal string of the
translation for NN1 is ?the vote?. According to r2,
the starting terminal string for NN2 is ?the result?.
According to r3, the starting terminal string of NP
must include that of NN2. Table 2 lists the starting
terminal strings of all nodes in Figure 1(a). As the
translations of node IP should begin with either ?the
result? or ?was released at night?, the first rule must
be either r2 or r5. Therefore, r1 will never be used
as the first rule in any derivation.
We refer to starting terminal strings of a node as
a viable prefixes, a term borrowed from LR pars-
ing (Aho and Johnson, 1974). Viable prefixes are
used to decide which rule should be used to ensure
efficient left-to-right target generation. Formally, as-
sume that VN denotes the set of non-terminals (i.e.,
source tree node labels), VT denotes the set of ter-
minals (i.e., target words), v1, v2 ? VN , w ? VT ,
pi ? {VT ? VN}?, we say that w is a viable prefix of
v1 if and only if:
? v1 ? w, or
? v1 ? wv2pi, or
? v1 ? v2pi, and w is a viable prefix of v2.
Note that we bundle all successive terminals in one
symbol.
3.3 Shift-Reduce Parsing
We use a shift-reduce algorithm to search for the
best deviation. The algorithm maintains a stack of
dotted rules (Earley, 1970). Given the source tree in
Figure 1(a), the stack is initialized with a dotted rule
for the root node IP:
[ IP].
Then, the algorithm selects one viable prefix of IP
and appends it to the stack with the dot at the begin-
ning (predict):
[ IP] [ the result]2.
Then, a scan action is performed to produce a partial
translation ?the result?:
[ IP] [the result ].
Next, the algorithm searches for the CFG rules start-
ing with ?the result? and gets r2. Then, it pops the
rightmost dotted rule and append the left-hand side
(LHS) of r2 to the stack (complete):
[ IP] [NN2 ].
Next, the algorithm chooses r4 whose right-hand
side ?NN2 of the vote? matches the rightmost dot-
ted rule in the stack3 and grows the rightmost dotted
rule:
[ IP] [NN2  of the vote].
Figure 3 shows the whole process of derivation
generation.
Formally, we define four actions on the rightmost
rule in the stack:
? Predict. If the symbol after the dot in the right-
most dotted rule is a non-terminal v, this action
chooses a viable prefix w of v and generates a
new dotted rule for w with the dot at the begin-
ning. For example:
[ IP] predict?? [ IP] [ the result]
? Scan. If the symbol after the dot in the right-
most dotted rule is a terminal string w, this ac-
tion advances the dot to update the current par-
tial translation. For example:
[ IP] [ the result] scan?? [ IP] [the result ]
? Complete. If the rightmost dotted rule ends
with a dot and it happens to be the right-hand
side of a rule, then this action removes the
right-most dotted rule. Besides, if the symbol
after the dot in the new rightmost rule corre-
sponds to the same tree node as the LHS non-
terminal of the rule, this action advance the dot.
For example,
[ IP] [NP  VP] [was released at night ]
complete?? [ IP] [NP VP ]
2There are another option: ?was released at night?
3Here there is an alternative: r3 or r7
1194
step action rule used stack hypothesis
0 [ IP]
1 p r6 [ IP] [ NP VP]
2 p r4 [ IP] [ NP VP] [ NN2 of the vote]
3 p r2 [ IP] [ NP VP] [ NN2 of the vote] [ the result]
4 s [ IP] [ NP VP] [ NN2 of the vote] [the result ] the result
5 c [ IP] [ NP VP] [NN2  of the vote] the result
6 s [ IP] [ NP VP] [NN2 of the vote ] the result of the vote
7 c [ IP] [NP  VP] the result of the vote
8 p r5 [ IP] [NP  VP] [ was released at night] the result of the vote
9 s [ IP] [NP  VP] [was released at night ] the ... vote was ... night
10 c [ IP] [NP VP ] the ... vote was ... night
11 c [IP ] the ... vote was ... night
Figure 2: Simulation of top-down translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete. ?the ... vote? and ?was ... released? are the abbreviated form of ?the result of the vote? and ?was released at
night?, respectively.
step action rule used stack number hypothesis
0 [ IP] 0
1 p [ IP] [ the result] 0
2 s [ IP] [the result ] 1 the result
3 c r2 [ IP] [NN2 ] 1 the result
4 g r4 or r7 [ IP] [NN2  of the vote] 1 the result
5 s [ IP] [NN2 of the vote ] 2 the result of the vote
6 c r4 [ IP] [NP ] 2 the result of the vote
7 g r6 [ IP] [NP  VP] 2 the result of the vote
8 p [ IP] [NP  VP] [ was released at night] 2 the result of the vote
9 s [ IP] [NP  VP] [was released at night ] 4 the ... vote was ... night
10 c r5 [ IP] [NP VP ] 4 the ... vote was ... night
11 c r6 [IP ] 4 the ... vote was ... night
Figure 3: Simulation of bottom-up translation process for the derivation in Figure 1(b). Actions: p, predict; s, scan; c,
complete; g, grow. The column of number gives the number of source words the hypothesis covers.
If the string cannot rewrite on the frontier non-
terminal, then we add the LHS to the stack with
the dot after it. For example:
[ IP] [the result ] complete?? [ IP] [NN2 ]
? Grow. If the right-most dotted rule ends with
a dot and it happens to be the starting part of
a CFG rule, this action appends one symbol of
the remainder of that rule to the stack 4. For
example:
4We bundle the successive terminals in one rule into a sym-
bol
[ IP] [NN2 ]
grow?? [ IP] [NN2  of the vote]
From the above definition, we can find that there
may be an ambiguity about whether to use a com-
plete action or a grow action. Similarly, predict ac-
tions must select a viable prefix form the set for a
node. For example in step 5, although we select
to perform complete with r4 in the example, r7 is
applicable, too. In our implementation, if both r4
and r7 are applicable, we apply them both to gener-
ate two seperate hypotheses. To limit the exponen-
tial explosion of hypotheses (Knight, 1999), we use
beam search over bins of similar partial hypotheses
(Koehn, 2004).
1195
IP
NP
NN2 of NN1
of the vote
VP
was released at night
r7
r4 r5
r6
r3
Figure 4: The translation forest composed of applicable
CFG rules for the partial derivation of step 3 in Figure 3.
3.4 Future Cost
Partial derivations covering different tree nodes may
be grouped in the same bin for beam pruning5. In
order to performmore accurate pruning, we take into
consideration future cost, the cost of the uncovered
part. The merit of a derivation is the covered cost
(the cost of the covered part) plus the future cost.
We borrow ideas from the Inside-Outside algorithm
(Charniak and Johnson, 2005; Huang, 2008; Mi et
al., 2008) to compute the merit. In our algorithm,
the merit of a derivation is just the Viterbi inside cost
? of the root node calculated with the derivations
continuing from the current derivation.
Given a partial derivation, we calculate its future
cost by searching through the translation forest de-
fined by all applicable CFG rules. Figure 4 shows
the translation forest for the derivation of step 3. We
calculate the future cost for each node as follows:
given a node v, we define its cost function f(v) as
f(v) =
?
?
?
?
?
1 v is completed
lm(v) v is a terminal string
maxr?Rv f(r)
?
pi?rhs(r) f(pi) otherwise
where VN is the non-terminal set, VT is the terminal
set, v, pi ? VN ? VT+, Rv is the set of currently ap-
plicable rules for v, rhs(r) is the right-hand symbol
set of r, lm is the local language model probability,
f(r) is calculated using a linear model whose fea-
tures are bidirectional translation probabilities and
lexical probabilities of r. For the translation forest
in Figure 4, if we calculate the future cost of NP with
5Section 3.7 will describe the binning scheme
r4, then
f(NP ) = f(r4) ? f(NN2) ? lm(of the vote)
= f(r4) ? 1 ? lm(of the vote)
Note that we calculate lm(of the vote) locally and do
not take ?the result? derived from NN2 as the con-
text. The lm probability of ?the result? has been in-
cluded in the covered cost.
As a partial derivation grows, some CFG rules
will conflict with the derivation (i.e. inapplicable)
and the translation forest will change accordingly.
For example, when we reach step 5 from step 3 (see
Figure 4 for its translation forest), r3 is inapplica-
ble and thereby should be ruled out. Then the nodes
on the path from the last covered node (it is ?of the
vote? in step 5) to the root node should update their
future cost, as they may employ r3 to produce the
future cost. In step 5, NP and IP should be updated.
In this sense, we say that the future cost is dynamic.
3.5 Comparison with Top-Down Decoding
In order to generate the translation ?the result? based
on the derivation in Figure 1(b), Huang and Mi?s
top-down algorithm needs to specify which rules to
apply starting from the root node until it yields ?the
result?. In this derivation, rule r6 is applied to IP, r4
to NP, r2 to NN2. That is to say, it needs to repre-
sent the partial derivation from IP to NN2 explicitly.
This can be a problem when combined with beam
pruning. If the beam size is small, it may discard the
intermediate hypotheses and thus never consider the
string. In our example with a beam of 1, we must
select a rule for IP among r6, r7 and r8 although we
do not get any information for NP and VP.
Instead, our bottom-up algorithm allows top-
down and bottom-up information to be used together
with the help of viable prefixes. This allows us to
encode more candidate derivations than the purely
top-down method. In the above example, our al-
gorithm does not specify the derivation for the gap
from IP and ?the result?. In fact, all derivations
composed of currently applicable rules are allowed.
When needed, our algorithm derives the derivation
dynamically using applicable rules. So when our
algorithm performs pruning at the root node, it has
got much more information and consequently intro-
duces fewer pruning errors.
1196
3.6 Time Complexity
Assume the depth of the source tree is d, the max-
imum number of matched rules for each node is c,
the maximum arity of each rule is r, the language
model order is g and the target-language vocabulary
is V, then the time complexity of our algorithm is
O((cr)d|V |g?1). Analysis is as follows:
Our algorithm expands partial paths with termi-
nal strings to generate new hypotheses, so the time
complexity depends on the number of partial paths
used. We split a path which is from the root node to a
leaf node with a node on it (called the end node) and
get the segment from the root node to the end node
as a partial path, so the length of the partial path is
not definite with a maximum of d. If the length is
d?(d? ? d), then the number of partial paths is (cr)d? .
Besides, we use the rightest g ? 1 words to signa-
ture each partial path, so we can get (cr)d? |V |g?1
states. For each state, the number of viable prefixes
produced by predict operation is cd?d? , so the total
time complexity is f = O((cr)d? |V |g?1cd?d?) =
O(cdrd? |V |g?1) = O((cr)d|V |g?1).
3.7 Beam Search
Tomake decoding tractable, we employ beam search
(Koehn, 2004) and choose ?binning? as follows: hy-
potheses covering the same number of source words
are grouped in a bin. When expanding a hypothe-
sis in a beam (bin), we take series of actions until
new terminals are appended to the hypothesis, then
add the new hypothesis to the corresponding beam.
Figure 3 shows the number of source words each hy-
pothesis covers.
Among the actions, only the scan action changes
the number of source words each hypothesis cov-
ers. Although the complete action does not change
source word number, it changes the covered cost of
hypotheses. So in our implementation, we take scan
and complete as ?closure? actions. That is to say,
once there are some complete actions after a scan ac-
tion, we finish all the compete actions until the next
action is grow. The predict and grow actions decide
which rules can be used to expand hypotheses next,
so we update the applicable rule set during these two
actions.
Given a source sentence with n words, we main-
tain n beams, and let each beam hold b hypotheses
at most. Besides, we prune viable prefixes of each
node up to u, so each hypothesis can expand to u
new hypotheses at most, so the time complexity of
beam search is O(nub).
4 Related Work
Watanabe et al(2006) present a novel Earley-
style top-down decoding algorithm for hierarchical
phrase-based model (Chiang, 2005). Their frame-
work extracts Greibach Normal Form rules only,
which always has at least one terminal on the left
of each rule, and discards other rules.
Dyer and Resnik (2010) describe a translation
model that combines the merits of syntax-based
models and phrase-based models. Their decoder
works in two passes: for first pass, the decoder col-
lects a context-free forest and performs tree-based
source reordering without a LM. For the second
pass, the decoder adds a LM and performs bottom-
up CKY decoding.
Feng et al(2010) proposed a shift-reduce algo-
rithm to add BTG constraints to phrase-based mod-
els. This algorithm constructs a BTG tree in a
reduce-eager manner while the algorithm in this pa-
per searches for a best derivation which must be de-
rived from the source tree.
Galley and Manning (2008) use the shift-reduce
algorithm to conduct hierarchical phrase reordering
so as to capture long-distance reordering. This al-
gorithm shows good performance on phrase-based
models, but can not be applied to syntax-based mod-
els directly.
5 Experiments
In the experiments, we use two baseline systems:
our in-house tree-to-string decoder implemented ac-
cording to Liu et al(2006) (denoted as traditional)
and the Earley-style top-down decoder implemented
according to Huang and Mi (2010) (denoted as top-
down), respectively. We compare our bottom-up
left-to-right decoder (denoted as bottom-up) with
the baseline in terms of performance, translation
quality and decoding speed with different beam
sizes, and search capacity. Lastly, we show the in-
fluence of future cost. All systems are implemented
in C++.
1197
5.1 Data Setup
We used the FBIS corpus consisting of about 250K
Chinese-English sentence pairs as the training set.
We aligned the sentence pairs using the GIZA++
toolkit (Och and Ney, 2003) and extracted tree-to-
string rules according to the GHKM algorithm (Gal-
ley et al 2004). We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus.
We used the 2002 NIST MT Chinese-English test
set (571 sentences) as the development set and the
2005 NIST MT Chinese-English test set (1082 sen-
tences) as the test set. We evaluated translation qual-
ity using BLEU-metric (Papineni et al 2002) with
case-insensitive n-gram matching up to n = 4. We
used the standard minimum error rate training (Och,
2003) to tune feature weights to maximize BLEU
score on the development set.
5.2 Performance Comparison
Our bottom-up left-to-right decoder employs the
same features as the traditional decoder: rule proba-
bility, lexical probability, language model probabil-
ity, rule count and word count. In order to compare
them fairly, we used the same beam size which is 20
and employed cube pruning technique (Huang and
Chiang, 2005).
We show the results in Table 3. From the re-
sults, we can see that the bottom-up decoder out-
performs top-down decoder and traditional decoder
by 1.1 and 0.8 BLEU points respectively and the
improvements are statistically significant using the
sign-test of Collins et al(2005) (p < 0.01). The
improvement may result from dynamically search-
ing for a whole derivation which leads to more ac-
curate estimation of a partial derivation. The addi-
tional time consumption of the bottom-up decoder
against the top-down decoder comes from dynamic
future cost computation.
Next we compare decoding speed versus transla-
tion quality using various beam sizes. The results
are shown in Figure 5. We can see that our bottom-
up decoder can produce better BLEU score at the
same decoding speed. At small beams (decoding
time around 0.5 second), the improvement of trans-
lation quality is much bigger.
System BLEU(%) Time (s)
Traditional 29.8 0.84
Top-down 29.5 0.41
Bottom-up 30.6 0.81
Table 3: Performance comparison.
29.4
29.6
29.8
30.0
30.2
30.4
30.6
30.8
 0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
BL
EU
 S
co
re
Avg Decoding Time (secs per sentence)
bottom-up
top-down
traditional
Figure 5: BLEU score against decoding time with various
beam size.
5.3 Search Capacity Comparison
We also compare the search capacity of the bottom-
up decoder and the traditional decoder. We do this
in the following way: we let both decoders use the
same weights tuned on the traditional decoder, then
we compare their translation scores of the same test
sentence.
From the results in Table 4, we can see that for
many test sentences, the bottom-up decoder finds
target translations with higher score, which have
been ruled out by the traditional decoder. This may
result from more accurate pruning method. Yet for
some sentences, the traditional decoder can attain
higher translation score. The reason may be that the
traditional decoder can hold more than two nonter-
minals when cube pruning, while the bottom-up de-
coder always performs dual-arity pruning.
Next, we check whether higher translation scores
bring higher BLEU scores. We compute the BLEU
score of both decoders on the test sentence set on
which bottom-up decoder gets higher translation
scores than the traditional decoder does. We record
the results in Figure 6. The result shows that higher
score indeed bring higher BLEU score, but the im-
provement of BLEU score is not large. This is be-
cause the features we use don?t reflect the real statis-
1198
28.0
29.0
30.0
31.0
32.0
33.0
34.0
35.0
 10  20  30  40
BL
EU
 S
co
re
Beam Size
bottom-up
traditional
Figure 6: BLEU score with various beam sizes on the sub
test set consisting of sentences on which the bottom-up
decoder gets higher translation score than the traditional
decoder does.
b > = <
10 728 67% 347 32% 7 1%
20 657 61% 412 38% 13 1%
30 615 57% 446 41% 21 2%
40 526 49% 523 48% 33 3%
50 315 29% 705 65% 62 6%
Table 4: Search capacity comparison. The first column is
beam size, the following three columns denote the num-
ber of test sentences, on which the translation scores of
the bottom-up decoder are greater, equal to, lower than
that of the traditional decoder.
System BLEU(%) Time (s)
with 30.6 0.81
without 28.8 0.39
Table 5: Influence of future cost. The results of the
bottom-up decoder with and without future cost are given
in the second and three rows, respectively.
tical distribution of hypotheses well. In addition, the
weights are tuned on the traditional decoder, not on
the bottom-up decoder. The bottom-up decoder can
perform better with weights tuned by itself.
5.4 Influence of Future Cost
Next, we will show the impact of future cost via ex-
periments. We give the results of the bottom-up de-
coder with and without future cost in Table 5. From
the result, we can conclude that future cost plays a
significant role in decoding. If the bottom-up de-
coder does not employ future cost, its performance
will be influenced dramatically. Furthermore, cal-
culating dynamic future cost is time consuming. If
the bottom-up decoder does not use future cost, it
decodes faster than the top-down decoder. This is
because the top-down decoder has |T | beams, while
the bottom-up decoder has n beams, where T is the
source parse tree and n is the length of the source
sentence.
6 Conclusions
In this paper, we describe a bottom-up left-to-right
decoding algorithm for tree-to-string model. With
the help of viable prefixes, the algorithm generates
a translation by constructing a target-side CFG tree
according to a post-order traversal. In addition, it
takes into consideration a dynamic future cost to es-
timate hypotheses.
On the 2005 NIST Chinese-English MT transla-
tion test set, our decoder outperforms the top-down
decoder and the traditional decoder by 1.1 and 0.8
BLEU points respectively and shows more powerful
search ability. Experiments also prove that future
cost is important for more accurate pruning.
7 Acknowledgements
We would like to thank Haitao Mi and Douwe
Gelling for their feedback, and anonymous review-
ers for their valuable comments and suggestions.
This work was supported in part by EPSRC grant
EP/I034750/1 and in part by High Technology R&D
Program Project No. 2011AA01A207.
References
A. V. Aho and S. C. Johnson. 1974. Lr parsing. Com-
puting Surveys, 6:99?124.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL, pages 173?180.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
1199
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proc. of NAACL,
pages 858?866, June.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13:94?102.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling, pages
285?293.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP, pages 848?856.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT, pages 53?64.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP, pages 273?283.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL,
pages 586?594.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25:607?615.
Philipp Koehn. 2004. Pharaoh: A beam search decoder
for phrased-based statistical machine translation. In
Proc. of AMTA, pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609?
616, July.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19?51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proc. of COLING, pages
777?784.
1200
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535?544,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Alignment of System Combination by Using
Multi-objective Optimization
Tian Xia+, Zongcheng Ji?, Shaodan Zhai+, Yidong Chen++, Qun Liu?, Shaojun Wang+
++ Xiamen University, Xiamen 361005, P.R. China
+ Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA
? Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn
{xia.7, zhai.6, shaojun.wang}@wright.edu
Abstract
This paper proposes a multi-objective opti-
mization framework which supports heteroge-
neous information sources to improve align-
ment in machine translation system combi-
nation techniques. In this area, most of
techniques usually utilize confusion networks
(CN) as their central data structure to com-
pact an exponential number of an potential hy-
potheses, and because better hypothesis align-
ment may benefit constructing better quality
confusion networks, it is natural to add more
useful information to improve alignment re-
sults. However, these information may be het-
erogeneous, so the widely-used Viterbi algo-
rithm for searching the best alignment may
not apply here. In the multi-objective opti-
mization framework, each information source
is viewed as an independent objective, and
a new goal of improving all objectives can
be searched by mature algorithms. The so-
lutions from this framework, termed Pareto
optimal solutions, are then combined to con-
struct confusion networks. Experiments on
two Chinese-to-English translation datasets
show significant improvements, 0.97 and 1.06
BLEU points over a strong Indirected Hidden
Markov Model-based (IHMM) system, and
4.75 and 3.53 points over the best single ma-
chine translation systems.
1 Introduction
System combination (SC) techniques have the
power of boosting translation quality in BLEU by
several percent over the best among all input ma-
chine translation systems (Bangalore et al, 2001;
Matusov et al, 2006; Sim et al, 2007; Rosti et al,
2007b; Rosti et al, 2007a; Huang and Papineni,
2007; He et al, 2008; Rosti et al, 2008; He and
Toutanova, 2009; Li et al, 2009; Feng et al, 2009;
Pauls et al, 2009). A central data structure in the
SC is the confusion network, and its quality greatly
affects the final performance. He et al (2008) pro-
posed a new hypothesis alignment algorithm for
constructing high-quality confusion networks called
Indirect Hidden Markov Model (IHMM), which
does better in synonym matching compared with
the classic translation edit rate (TER) based algo-
rithm (Rosti et al, 2007b; Rosti et al, 2008; Sim et
al., 2007). Now, current state-of-the-art SC systems
have been using IHMM or variants in their align-
ment algorithms more or less (Li et al, 2009; Feng
et al, 2009).
Our motivation derives from an observation that
in an ideal alignment of a pair of sentences, many-to-
many alignments often exist. For instance, ?be about
to? has the same meaning with ?be on the point
of?. Because Hidden Markov Model based align-
ment algorithms, e.g. IHMM for system combina-
tion, HMM in GIZA++ software for statistical ma-
chine translation (SMT) (Och and Ney, 2000; Koehn
et al, 2003), are designed for one-to-many align-
ment, and running GIZA++ from two directions to
gain better performance turns into a standard opera-
tion in SMT, therefore we are seeking a way to em-
power IHMM by introducing bi-directional informa-
tion.
However, it appears to be intractable in an IHMM
model to search the optimal solution by simply
defining a new goal as a product of probabilities
535
from two directions. To bypass this problem, Liang
et al (2006) adopts a simple and effective variational
inference algorithm.
Further, different alignment algorithms capture
different information and linguistic phenomena for
a pair of sentences, hence more information would
be expected to benefit the final alignment. Liang?s
method may not be suitable for this expected out-
come.
We propose to adopt multi-objective optimiza-
tion framework to support heterogeneous informa-
tion sources which may induce difficulties in a
conventional search algorithm. In this framework,
there exist a variety of matured multi-objective op-
timization algorithms, e.g. evolutionary algorithm
(Deb et al, 2000; Deb et al, 2002), Tabu search
(Hansen, 1997), ants colony (Engelbrecht, 2005),
and simulated annealing (Serafini, 1994). In this
work, we select the multi-objective evolutionary al-
gorithm because of its public open source software
(http://www.iitk.ac.in/kangal/codes.shtml). On the
other hand, this framework is also totally unsuper-
vised. It prevents weights of a linearly combined
goal from training even if all information is homoge-
neous and applicable in a Viterbi search (Forney Jr,
1973). This framework views any useful informa-
tion benefiting alignment as an independent objec-
tive, and researchers just need to write short codes
for objective definitions. The search algorithm seeks
for potentially better solutions which are no worse
than the current solution set. The output from multi-
objective optimization algorithms includes a set of
solutions, called Pareto optimal solutions, each one
being a many-to-many alignment. We then com-
bine and normalize them into a unique one-to-one
alignment to perform confusion network construc-
tion (Section 3.3).
Our work is conducted on the classic pipeline
which has three modules, pair-wise hypothesis
alignment, confusion network construction, and
training. Now many work integrates neighboring
modules to avoid propagated errors to gain improved
performance. For example, Rosti et al (2008), and
Li et al (2009) combine the first and the second
module, and He and Toutanova (2009) combine all
modules into one directly. Nevertheless, the classic
structure also owns its merits. Because of the in-
dependence between modules, a system is relatively
simple to maintain, and improvements on each mod-
ule might contribute to final performance additively.
Based on our work, lattice-based minimum error
rate training (lattice-MERT) and minimum bayes
risk training techniques (Kumar et al, 2009) could
be adopted on the third module. And Feng et al
(2009) in the second module adopts a different data
structure called lattice which could directly use our
better many-to-many alignment for construction.
Experiments on the Chinese-to-English task on
two datasets use four objectives, IHMM probabil-
ity (Section 3.2.1), and alignment probability from
GIZA++ (Section 3.2.2) from two directions. Re-
sults show multi-objective optimization framework
efficiently integrates different information to gain
approximately 1 BLEU point improvement over a
strong baseline.
2 Background
We briefly give an introduction to confusion net-
works, and because the IHMM based alignment is
an important objective in our multi-objective frame-
work, here we also provide detailed definition of for-
mulas for completeness of content.
2.1 Confusion Network
Table 1 shows hypotheses h1 and h2 are aligned to
selected backbone h0. When alignment algorithm
obtains good enough results, the expected output
?he prefers apples? is included in its corresponding
confusion network in Figure 1. This suggests de-
veloping better alignment algorithm may help creat-
ing high-quality confusion networks. This also mo-
tivates us to use the BLEU of oracle hypotheses to
approximately measure the quality of a set of CNs.
We hereafter call it an oracle BLEU of a CN. See
more in Section 5.1.
h0 :he feels like apples
h1 :he prefer ? apples
h2 :him prefers to apples
Table 1: A toy example of hypothesis alignment, where
h0 is the backbone hypothesis. h1and h2 are aligned to
the backbone separately. The resulting confusion net-
work is in Figure 1.
A confusion network G = (V,E) is a directed
acyclic graph with a unique source and sink vertex,
536
b b b b
him
he
prefers
prefer
feel
?
like
like
b
apples
Figure 1: A classic confusion network, and the bold path
the expected output.
formally a weighted finite state automation (FSA),
where V is the set of nodes andE is the set of edges.
Each edge is restricted to attach to a single word as
well as an associated probability. A special mark ?
is a place-holder denoting no word here.
2.2 IHMM-based Alignment
Indirected Hidden Markov Model (IHMM) was
firstly proposed by He et. al (2008). Compared with
TER-based alignment performing literal matching,
IHMM supports synonym comparison in redefining
emission probabilities in an IHMM model.
Let f I = (f1, . . . fI) be a backbone hypothesis,
and eJ = (e1, . . . eJ) be a hypothesis aligned to the
backbone, both being English sentences in our ex-
periments. Let aJ = {a1, . . . aj} be an alignment.
Suppose the aj th word in f I is aligned to jth word
in eJ , and the conditional probability that the hy-
pothesis is generated by the backbone, shown in the
upper graph of Figure 3, is given by
p(f I , eJ) =
?
aJ
J?
j=1
{pt(aj |aj?1, I)po(ej |faj )}
(1)
The distortion probability pt(aj |aj?1, I) from po-
sition aj?1 to aj , relies on jumped distance, which
is computed as follows:
pt(i? |i, I) = c(i
?
? i)
?I
t=1 c(t? i)
(2)
The distortion parameters c(d) are grouped into
11 buckets, c(? ?4),c(?3),c(?2). . .c(5),c(? 6).
Because all the hypotheses in system combina-
tion are in the same language, the IHMM model
would support more monotonic alignments, and
non-monotonic alignments will be penalized.
c(d) = (1 + |d? 1|)?K , d = ?4 . . . 6 (3)
where K is tuned on held-out data.
Let p0 be the probability of jumping to a null
word state, which is also tuned on held-out data, and
the accurate transition probability becomes:
pt(i? |i, I) =
{
p0 if i? = null
(1? p0)pt(i
?
|i, I) otherwise
(4)
The output probability po(e|f) from the state
word f to the observation word e, also called trans-
lation probability, is a linear interpolation of se-
mantic similarity psem(e|f) and surface similarity
psur(e|f), and ? is the interpolation factor:
po(e|f) = ?psem(e|f) + (1? ?)psur(e|f) (5)
When calculating semantic similarity psem(e|f),
source sentence src is needed, and a bilingual prob-
abilistic dictionary pdic(w1|w2) is necessary.
psem(e|f) ?
?
c?src
pdic(c|f) ? pdic(e|c) (6)
Note that psem(e|f) has been updated with differ-
ent source sentences.
The surface similarity psur(e|f) is measured by
the literal matching rate:
psur(e, f) = exp{?[ LMP(f, e)max(|f |, |e|) ? 1]} (7)
where LMP(f, e) is the length of the longest
matched prefix, and ? is a smoothing parameter.
3 Multi-objective Optimization
Many decision making problems in the real world
consider more than one objective. One natural way
is to scalarize multiple objectives into one by assign-
ing it with a weight vector. This method allows a
simple optimization algorithm in many cases, while
in system combination, it would cause problems.
In the first module, in order to train suitable
weights of objectives, extra labeled data is needed,
besides that, the efficient Viterbi algorithm for
searching the optimal alignment would not work for
537
the alignment objectives in this work. More, the pa-
rameter training in the third module relies on the
CNs constructed from the output of the first mod-
ule, which increases the instability of the whole sys-
tem. Therefore, an unsupervised multi-objective al-
gorithm may be a good choice allowing for more
alignment information.
There exist other alternative optimization algo-
rithms in the multi-objective optimization frame-
work, though the evolutionary algorithm is adopted
here, we only introduce some general concepts.
3.1 Pareto Optimal Solutions
A general multi-objective optimization problem
consists of a number of objectives and is associated
with a number of constraints. Mathematically, the
problem can be written as follows (Deb, 2001)
Maximize fi(x) i = 1 . . .M
s.t. gj(x) ? 0 j = 1 . . . N
hk(x) = 0 k = 1 . . .K
where x denotes a potential solution, its structure re-
lying on different problems, and the number of con-
straints M,N,K depend on different problems. All
the functions fi, gj , hk map a solution x into a scalar.
We will explain them in terms of system combina-
tion.
In this work, we refer to x = {xi,j |xi,j ? {0, 1}}
as a potential alignment of a pair of hypotheses,
where xi,j is a boolean value to denote whether the
ith word in the first hypothesis is aligned to the jth
word in the second hypothesis. Here the definition of
x seems different from that of a in Formula 1, and
they could convert to each other. Using a line-based
access style, a matrix can be unfolded as a vector.
We refer to f as IHMM alignment probability (He et
al., 2008) and GIZA++ alignment probability (Chen
et al, 2009), total four objectives from two direc-
tions, and the larger the objectives, the better. The
gjs and hks serve as the role of checking if x repre-
sents a legal alignment. For instance, the subscripts
of xi,j are not in bounds.
Definition 1. Let x, x? be two potential align-
ments. If fi(x) ? fi(x?) holds for all i, we call
the alignment x dominates the alignment x?. If there
0
1
2
3
4
5
0 1 2 3 4 5
b
p3
b
p5
b
p7
b
p1
?p2
? p6
?p4
X: Reversed IHMM Probability (1e-8)
Y:D
irec
tIH
MM
Pro
bab
ility
(1e
-8)
Figure 2: Sample solutions with only two objectives.
Pareto Optimal Solutions p1, p3, p5, p7. Other points
p2, p4, p6 are dominated by at least one point in the Pareto
optimal solutions.
does not exist any alignment x?? to dominate x, we
call the alignment x to be non-dominated.
Definition 2. A alignment x is said to be Pareto
optimal if there is no other alignment x? found to
dominate x.
In Figure 2, p1 dominates p2, and p2 dominates
p4. To summarize, a point is dominated by the ones
on its upper and right side with ties. In this example,
p1, p3, p5, p7 are Pareto optimal.
In some cases, Pareto optimal solutions can be
used for good candidate solutions. Considering
the IHMM model, maximizing Y axis, the top-4
best alignments are p1, p2, p3, p4. But from the
view of Pareto optimal, the top-4 alignments would
be p1, p3, p5, p7 without order, which considers a
greater range than a single optimization model. In
our method, we just combine these Pareto optimal
solutions equally into a unique alignment (Section
3.3).
Our adopted multi-objective optimization search-
ing algorithm is the non-dominated sorting ge-
netic algorithm II (NSGA-II) (Deb et al, 2000;
Deb et al, 2002) with an open source software
(http://www.iitk.ac.in/kangal/codes.shtml). NSGA-
II has a complexity of O(mn2), wherem is the num-
ber of objectives and n is the population size in an
evolutionary algorithm.
3.2 Objectives in Evolutionary Algorithm
The optimization objectives in our experiments can
be categorized as an IHMM alignment probability
(He et al, 2008) and GIZA++ alignment probability
538
b b b
b b bO:
S: f1 f2 f3
e1 e2 e3
b b b
b b b
S:
O:
e1 e2 e3
f1 f2 f3
Backbone
Backbone
Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in
two IHMM models. The upper one is a typical example
in IHMM, and in the bottom one, because any word in the
observation is required not to correspond to two statuses,
it has a minor trouble. S: status sequence, O: observation
sequence.
(Chen et al, 2009), total four from two directions.
3.2.1 IHMM Probability
A typical IHMM alignment is demonstrated
in the upper graph of Figure 3, where a
backbone is acting the role of a status se-
quence. The unnormalized conditional align-
ment probability is [pt(1|null)] ? [pt(1|1)pt(2|1)] ?
[po(e1|f1)po(e2|f1)po(e3|f2)]. However, the same
alignment (f1, e1)(f1, e2)(f2, e3), if we change the
alignment direction, the backbone being observa-
tions, would be a bit different. We offer a minor
modification to Formula 1.
Look at the bottom graph of Figure 3, the obser-
vation f1 has two statuses, e1 and e2 at the same
time, it becomes ambiguous to compute the tran-
sitional probability between pt(3|1) and pt(3|2).
This is because IHMM algorithm deals with one-
to-many alignments, and MOEA permits many-to-
many alignments.
We hence empirically modify the IHMM model
to support many-to-many alignments. A new status
is defined, rather than a single position pt(j|i), but
as a set of positions pt({j}|{i}). The positions in
one status need not to be adjacent to each other.
The redefined transitional probability
pt({j}|{i}) =
1
|{j}| ? |{i}|
?
i,j
pt(j|i)
The redefined emission probability
po(j|{i}) =
?
i
po(j|i)
We need to note that there is no guarantee on
the closed property of probabilities, though these
approximations prove to be effective in a practical
sense. Straightforwardly, when there is only one po-
sition in a new status, the expanded IHMM degener-
ates to the standard IHMM.
Let us return to the second IHMM ex-
ample. The new probability becomes
[pt(1|null)pt(2|null)] ? [12pt(3|1)pt(3|2) ?pt(null|3)] ?
[po(f1|e1)po(f1|e2)po(f2|e3)po(f3|null)].
3.2.2 Alignment Probability
GIZA++ considers very different and more in-
formation in alignment, we attempt to utilize them.
All probabilities appearing in below formulas can be
looked up in GIZA++.
Given a pair of hypotheses f I = (f1, . . . fI),
eJ = (e1, . . . eJ), and their alignment a, the align-
ment probability could be calculated as follows
pGiza(eJ |f I ,a) =
?
ei
T (ei|f
I ,a)
T (ei|f
I ,a) =
{
n(?i|ei)
?
(j,i)?a t(ei|fj)a(j|i)/?i if?i 6= 0
n(0|ei)t(ei|null)a(0|i) otherwise
?i = |{j|(i, j) ? a}|
where ?i is the fertility number, t(e|c) the transla-
tion probability for the word pair, z(j|i) alignment
probability to show how likely a target word at posi-
tion i could be translated into a source word at posi-
tion j, and n(?|e) is the fertility probability to show
how likely a given target word e is translated into ?
source words.
In order to increase the coverage of words, we col-
lect all the hypothesis pairs in both the tuning set
and the test set and feed them into GIZA++. This
is an off-line operation, which makes it not suitable
for an online translation system. In some circum-
stances, users submit a pile of documents in the hope
of high-quality translations, thus more useful knowl-
edge sources would be helpful. In our experiments,
a pure GIZA++ based system combination does not
perform as well as IHMM based, but does benefit
the final translation quality if combined in our multi-
objective optimization framework.
539
3.3 Configuration of Evolutionary Algorithm
3.3.1 Encoding
Given a sentence pair <f I , eJ>, we define a two-
dimensional matrix x = {zi,j |zij ? {0, 1}} to en-
code a set of possible alignments. Using a line-based
access style, the matrix could be unfolded as a vector
with |I| ? |J | bits of length.
3.3.2 Initialization
Because in NSGA-II software the initial popu-
lation are generated at random. In order to make
NSGA-II more consistent and flexible, better initial
seeds should be fed with, thus we combine an ex-
isting word alignment results as input. Here we use
together two N-best lists generated from directional
HMM and reversed HMM respectively for initializa-
tion.
3.3.3 Normalization of Pareto Optimal
Solutions
Multi-objective optimization algorithms do not
pose weights on objectives, thus they output a set
of so-called Pareto optimal solutions, each of which
is a many-to-many alignment. We can understand
them as an N-best alignment list without explicit
preferences. We also empirically compare it with the
idea that directly cuts an N-best list from the IHMM
based alignment.
We describe a two-stage strategy for normaliza-
tion. Firstly, we use a simple and effective voting
strategy to combine a set of many-to-many align-
ments into a single many-to-many alignment, and
Secondly we normalize it into a one-to-one align-
ment for confusion network construction. In the first
stage, we count the number of word-to-word align-
ments on each position pair (i, j). If there is more
than a half number of alignments, then we output 1,
otherwise 0. In the second stage, if any word relates
to more than one word alignment, the one with the
highest posterior probability is selected (He et al,
2008; Feng et al, 2009). The posterior probabili-
ties can be computed in a classic forward-backward
procedure in IHMM (He et al, 2008).
4 Training and Decoding
Our work does not change the classic pipeline, thus
the model and features are nearly identical to the
ones in (Rosti et al, 2007b; He et al, 2008), which
are modeled in a log-linear fashion in Eq. 8. Trans-
lation on a CN is just a concatenation of edges tra-
versed, on which 4 categories of features are defined.
1. word posterior probabilities. In Eq. 8,
p(w|sys, span) are word confidence scores. If
the word w comes from the kth hypothesis of
thesys-th system, the raw score should be 1k+1 ,and then it would be normalized by the same
sys and span. The same word coming from
different systems owns a different score, so
there are sys system weights ?sys.
2. logarithm of language model score, L(h).
3. number of null edge, Numnull.
4. number of words, Numw.
log(h) =
?
span log(
?
sys ?sysp(w|sys, span))
+ w0L(h) + w1Numnull + w2Numw
(8)
Decoding a confusion network is straightforward,
traversing each node from left to right, and the beam
search algorithm will retain for each node an N-
best list. The final N-best can be acquired following
(Huang and Chiang, 2005).
The training process follows minimum error rate
training (MERT) described in (Och, 2003; Koehn et
al., 2003). In each iteration, the Powell algorithm
would attempt to predict the optimal parameters on
the cumulative N-best list.
5 Experiments
We evaluate our method in two datasets in the
Chinese-to-English task. In the first one, NIST MT
2002 and 2005 are used for tuning and testing re-
spectively, and in the second, the newswire part of
MT 2006 and 2008 are for tuning and testing. A 5-
gram language model is trained on the Xinhua por-
tion of the Gigaword corpus. We report the case-
sensitive NIST-BLEU score.
Four single machine translation systems partici-
pating in the system combination consist of a BTG-
based system using a Max-Entropy based reordering
model, a hierarchical phrase-based system, a Moses
decoder and a syntax-based system. 10-best unique
hypotheses from a single system on the development
540
SYSTEM MT 2005 MT 2008(news)
best single 0.3207 0.3016
IHMM* 0.3585(+3.78%) 0.3263(+2.47%)
IncIHMM 0.3639(+4.32%) 0.3320(+3.04%)
GIZA++ 0.3438(+2.31%) 0.3166(+1.50%)
PPBD 0.3619(+4.10%) 0.3306(+2.90%)
N-best IHMM 0.3590(+3.83%) 0.3270(+2.54%)
dH+rH 0.3604 0.3284
dH+dT 0.3610 0.3290
dH+rH+dT 0.3609 0.3289
dH+rH+rT 0.3630?(+4.27%) 0.3320?(+3.04%)
dH+rH+dT+rT 0.3682??(+4.75%) 0.3369??(+3.53%)
Table 2: PPBD is a posterior probabilistic-based decod-
ing (section 5.3). N-best IHMM simulates the Pareto op-
timal solutions in our method (section 5.3). The last five
systems adopt different objective combinations. The im-
provement percents in parentheses are compared to the
best single. dH: directed IHMM, rH: reversed IHMM,
dT: directed translation probability, rT: reversed transla-
tion probability. ?? significance at 0.01 level, and ? sig-
nificance at 0.05 level over the IHMM model.
and test sets are collected as the input of the system
combination.
Our baseline systems are described as follows.
Two main baseline systems are IHMM based and in-
cremental IHMM (Li et al, 2009). The first system
differs from our method just in hypothesis alignment
algorithm, and the second combines the first and sec-
ond module of the system combination pipeline.
Because our method utilizes bidirectional infor-
mation, we also provide another two alternative
systems for comparison, which are GIZA++ based
alignment and the posterior probability based align-
ment (Liang et al, 2006). Finally, we also provide
an N-best alignment IHMM system, which com-
bines an N-best alignment list to simulate the Pareto
optimal solutions in our method.
The method that linearly combines all objectives
is not listed as our baseline like (Duh et al, 2012)
does, because their algorithm finds the best weighted
solution in a fixed and small solution set, while
in our problem, the solution space is a trellis-style
structure consisting of an exponential number of so-
lutions, and no efficient algorithms apply here.
The IHMM based alignment utilizes typical set-
tings (He et al, 2008; Feng et al, 2009). The
smoothing factor for the surface similarity model,
and ? = 3 the controlling factor for the distor-
tion model, K = 2. The bilingual probabilistic
dictionary is trained in the FBIS corpus which in-
cludes about 230k parallel sentence pairs. GIZA++
based system is to run GIZA++ from two directions
to align all the hypotheses, and make the intersec-
tion using grow-diag-final heuristics (Koehn et al,
2003). The many-to-many alignments are normal-
ized with the same method with ours. Our system
employs NSGA-II software to realize the MOEA al-
gorithm. The main parameters, generation number,
cross probability and mutation probability, and pop-
ulation size, are empirically set as 100, 0.9, 0.001
and 40, and we examine the influence of difference
populations sizes in the full system combination.
5.1 The Quality of Confusion Networks
This experiment shows the relationship between hy-
pothesis alignment and confusion network. Intu-
itively, we expect a better hypothesis alignment
would reduce the error in constructing confusion
networks, and then improve the final translation
quality.
We first use the alignment error rate (AER) (Och
and Ney, 2000), which is widely used to measure
the quality of hypothesis alignment. The smaller,
the better. For convenience, we only examine exact
literal matching. IHMM based alignment reaches
around 0.15 in AER, and our method 0.145.
As the AER may not vividly reflect the relations
between alignment and the final BLEU of systems,
and the quality of confusion network is hard to mea-
sure directly, we assume that the quality of confu-
sion networks could be measured by the oracle hy-
potheses that could be generated from them. We test
the BLEU of the oracle hypotheses.
From this angle, we demonstrate several oracle
BLEU of CNs generated from some conventional
alignment algorithms. The results are shown in Ta-
ble 3.
We find the confusion network from IHMM based
alignment (He et al, 2008) is better than that from
TER based alignment (Rosti et al, 2007b) by about
1 point in both two datasets. These quantities agree
with the final improvements in the BLEU score in
(He et al, 2008). As confusion networks from
MOEA based alignment also show superiority over
541
alignment MT02 MT05
GIZA++ 0.5690 0.5228
TER 0.5720 0.5270
IHMM 0.5883 0.5382
IncIHMM 0.5931 0.5453
MOEA 0.6017 0.5526
Table 3: Oracle BLEUs of CNs. GIZA++: invoking
GIZA++ software. TER: minimum translation edit rate.
IHMM: indirect hidden markov model. IncIHMM: in-
cremental indirect hidden markov model. MOEA: multi-
objective evolution algorithm.
that from IHMM based in the oracle BLEU, we ex-
pect our final translation quality would be improved.
In Table 3, GIZA++ and TER perform simi-
larly, because the former is more capable of tackling
many-to-many alignments over the latter, while lat-
ter based might obtain relatively more precise align-
ment information. Both of the two do not consider
synonym matching compared to IHMM.
Our method and IncIHMM overpass IHMM on
this metric due to different strategies. Obtaining bet-
ter hypothesis alignment or better construction of
confusion networks benefit the quality of CNs.
5.2 Different Objective Combinations
As our framework is convenient to support different
alignment information, we test the influence of dif-
ferent objective combinations to the final translation
quality. We adopt four objectives to depict the can-
didate alignment, directed IHMM probability (dH),
reversed IHMM probability (rH), directed alignment
probability (dT), and reversed alignment probability
(rT). Table 2 demonstrates all the results.
We can see that the IHMM based system out-
performs the GIZA++ based system by about 1-1.5
points in BLEU, which agrees with the difference of
oracle BLEU in Table 1. From (He et al, 2008), the
IHMM based system outperforms the TER based by
1 point, which also agrees with our results in Table
1. Our system, using dH + rH + dT + rT, improves
BLEU score by about 1 points over the IHMM based
system. This comparison verifies our assumption,
improving the quality of the confusion network does
improve system performance.
The different feature combinations exhibit inter-
esting results. The system with dH + rH + dT is
0.05 point better than the system with dH + rH, and
the system dH + rH + rT is 0.3 point better than sys-
tem with dH + rH, so the contributions of feature
dT and rT are 0.05 and 0.3 respectively. While the
two features are used together in the fourth system,
the contribution is about 0.8 point, rather than 0.35.
This phenomenon also proves the correlations be-
tween different features.
Our method explores a way to integrate GIZA++
and IHMM, and is supportive of useful features.
Compared to the classic and powerful IHMM based
system, we obtained an improvement of 0.97 points
on MT 05 and 1.06 points on news of MT 2008,
and equivalently over the best single system by 4.75
points and 3.53 points respectively. More, compared
with the incremental IHMM, our system also shows
moderate improvement, though not much. We hope
these two ideas could be effectively combined in the
future work.
5.3 Comparison with Other Bi-directional
Alignment Methods
Our method introduces multiple alignment infor-
mation into system combination to obtain improve-
ments, thus it would be interesting to explore other
alternative methods for utilizing this information.
We provide three alternative methods similar to our
motivations, and they fall into two categories.
The first category is from the angle of bi-
directional alignment. We use GiZA++ alignment
and the posterior probability decoding-based align-
ment for comparison. The basic idea for the lat-
ter is setting a word-to-word alignment xi,j as 1,
if its approximate posterior marginal probability
q(xi,j , x) = pd(xi,j |x, ?d) ? pr(xi,j |x, ?r) is greater
than a threshold ?, where pd and pr are posterior
marginal probabilities from directed and reversed
IHMM models, which could be conveniently com-
puted with a forward-backward algorithm, and the ?
is tuned on a validation-set optimized data. We just
list some ? values to examine its best performance
shown in Table 4.
The second class is because our method combines
the Pareto optimal solutions that consist of several
candidate alignments, thus for fairness we also use
a 100-best outputs from the directed IHMM model
and conduct the same normalization technique.
The general results are shown in Table 2. We can
542
? MT 2005 MT 2008
IHMM 0.3585 0.3263
0.15 0.3556 0.3391
0.2 0.3619 0.3306
0.25 0.3575 0.3278
0.3 0.3608 0.3259
Table 4: Posterior decoding. When threshold ? are set
to suitable values, simple bi-directional alignment could
overpass the baseline.
see that, GIZA++ leads to the worst performance,
which can be explained as GIZA++ does not support
synonym matching like IHMM. The N-best IHMM
has a minor improvement over the IHMM method.
We found differences in the N-best list are not obvi-
ous enough. In comparison, the posterior decoding
method brings relatively significant improvements
on both datasets. However, the threshold ? must
be selected suitably. Table 4 lists the ideal results,
which will be hampered when tuning on a validation
set.
All of the three candidate methods can not conve-
niently support extra alignment information, and a
linear model poses restrictions on features to get an
efficient decoding, the multi-objective optimization
may be a good selection as an inference algorithm in
many circumstances.
5.4 Population Size
We test the influence of final translation quality and
time consumed by different population size.
population BLEU
size MT 2005
20 0.3597
40 0.3682
60 0.3655
Table 5: Big population size consumes more CPU time.
In our experiments, we use a multi-thread technique to
speed up the alignment, and choose 40 as the parameter
to leverage the time and BLEU.
We expect enlarging the population size would
improve the translation quality, but the BLEU in
population size set as 60 does not overpass when set
as 40. We conjecture that, in our code, if the N-best
size from IHMM (we set as 50-best) does not reach
the population size, we would use randomly gener-
ated seeds, which may hamper the performance of
MOEA. We also tried a larger population in MOEA,
but did not receive obvious improvement on perfor-
mance.
We exerted a hard restriction on the genes in evo-
lutionary algorithm, that is many-to-many discon-
tiguous alignment is forbidden. This trick speeds up
running by about 20 times, and does not harm sys-
tem performance. Now our method runs about 0.9
seconds to align a pair of hypotheses. In practice,
we utilize multi-thread to speed up.
6 Conclusion
In this paper, we explore a multi-objective frame-
work to conveniently support more useful alignment
objectives to improve the hypothesis alignment. By
a minor modification of the first module in the
classic pipeline, we successfully combine GIZA++
and IHMM to obtain significant improvement over
a powerful and state-of-the-art IHMM based sys-
tem. In comparison with another genre of improving
system combination by combing adjacent modules
of the pipeline, more powerful incremental IHMM
here, our system also show moderate improvement.
Though, our best system may not overpass He and
Toutanova (2009) who combine all the modules into
a unified training procedure, we believe our method
could boost many work on the higher modules of the
pipeline to obtain a further improvement to match
their work.
7 Acknowledgement
This research is partially supported by Air Force
Office of Scientific Research under grant FA9550-
10-1-0335, the National Science Foundation under
grant IIS RI-small 1218863 and a Google research
award. We thank the anonymous reviewers for their
insightful comments.
References
B Bangalore, German Bordel, and Giuseppe Riccardi.
2001. Computing consensus translation from multi-
ple machine translation systems. In Automatic Speech
Recognition and Understanding.
Yidong Chen, Xiaodong Shi, Changle Zhou, and
Qingyang Hong. 2009. A word alignment
543
model based on multiobjective evolutionary algo-
rithms. Computers and Mathematics with Applica-
tions, 57.
Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and
Tanaka Meyarivan. 2000. A fast elitist non-dominated
sorting genetic algorithm for multi-objective optimiza-
tion: Nsga-ii. Lecture notes in computer science,
1917:849?858.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
TAMT Meyarivan. 2002. A fast and elitist multiob-
jective genetic algorithm: Nsga-ii. Evolutionary Com-
putation, IEEE Transactions on, 6(2):182?197.
Kalyanmoy Deb. 2001. Multi-objective optimization.
Multi-objective optimization using evolutionary algo-
rithms, pages 13?46.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Proc. of NAACL, pages 975?983.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proc. of ACL,
pages 1?10.
Andries P Engelbrecht. 2005. Fundamentals of compu-
tational swarm intelligence, volume 1. Wiley Chich-
ester.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan L?. 2009. Lattice-based system combination for
statistical machine translation. In Proc. of EMNLP,
EMNLP ?09.
G David Forney Jr. 1973. The viterbi algorithm. Proc.
of the IEEE, 61(3):268?278.
Michael Pilegaard Hansen. 1997. Tabu search for mul-
tiobjective optimization: Mots. In Proc. of Multiple
Criteria Decision Making, pages 574?586.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. of EMNLP.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. of EMNLP.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Fei Huang and Kishore Papineni. 2007. Hierarchical
system combination for machine translation. In Proc.
of EMNLP-CoNLL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proc. of ACL: Poster,
pages 177?180.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proc. of Joint ACL and
AFNLP.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest rerank-
ing for machine translation with the perceptron algo-
rithm. GALE book chapter on MT From Text.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. of Joint ACL and AFNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of NAACL.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of EACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. Proc. of ACL-08: HLT, pages 192?
199.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, October.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, pages
160?167.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proc. of EMNLP.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a. Combining outputs from multiple machine
translation systems. In Proc. of NAACL-HLT.
Antti-Veikko I Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. of ACL, vol-
ume 45.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proc. of WSMT.
Paolo Serafini. 1994. Simulated annealing for multi ob-
jective optimization problems. In Proc. of Multiple
Criteria Decision Making, pages 283?292. Springer.
Khe Chai Sim, William J Byrne, Mark JF Gales, Hichem
Sahbi, and Phil C Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proc. of ICASSP, volume 4.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proc. of NAACL: Short Papers, pages 205?208.
544
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066?1076,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translation with Source Constituency and Dependency Trees
Fandong Meng?? Jun Xie? Linfeng Song?? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
We present a novel translation model, which
simultaneously exploits the constituency and
dependency trees on the source side, to com-
bine the advantages of two types of trees. We
take head-dependents relations of dependency
trees as backbone and incorporate phrasal n-
odes of constituency trees as the source side
of our translation rules, and the target side as
strings. Our rules hold the property of long
distance reorderings and the compatibility
with phrases. Large-scale experimental result-
s show that our model achieves significantly
improvements over the constituency-to-string
(+2.45 BLEU on average) and dependency-
to-string (+0.91 BLEU on average) model-
s, which only employ single type of trees,
and significantly outperforms the state-of-the-
art hierarchical phrase-based model (+1.12
BLEU on average), on three Chinese-English
NIST test sets.
1 Introduction
In recent years, syntax-based models have become a
hot topic in statistical machine translation. Accord-
ing to the linguistic structures, these models can be
broadly divided into two categories: constituency-
based models (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al, 2006; Huang et al,
2006), and dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong
et al, 2007; Shen et al, 2008; Xie et al, 2011).
These two kinds of models have their own advan-
tages, as they capture different linguistic phenome-
na. Constituency trees describe how words and se-
quences of words combine to form constituents, and
constituency-based models show better compatibil-
ity with phrases. However, dependency trees de-
scribe the grammatical relation between words of
the sentence, and represent long distance dependen-
cies in a concise manner. Dependency-based mod-
els, such as dependency-to-string model (Xie et al,
2011), exhibit better capability of long distance re-
orderings.
In this paper, we propose to combine the advan-
tages of source side constituency and dependency
trees. Since the dependency tree is structurally sim-
pler and directly represents long distance depen-
dencies, we take dependency trees as the backbone
and incorporate constituents to them. Our mod-
el employs rules that represent the source side as
head-dependents relations which are incorporated
with constituency phrasal nodes, and the target side
as strings. A head-dependents relation (Xie et al,
2011) is composed of a head and all its dependents in
dependency trees, and it encodes phrase pattern and
sentence pattern (typically long distance reordering
relations). With the advantages of head-dependents
relations, the translation rules of our model hold the
property of long distance reorderings and the com-
patibility with phrases.
Our new model (Section 2) extracts rules from
word-aligned pairs of source trees (constituency
and dependency) and target strings (Section 3), and
translate source trees into target strings by employ-
ing a bottom-up chart-based algorithm (Section 4).
Compared with the constituency-to-string (Liu et al,
2006) and dependency-to-string (Xie et al, 2011)
models that only employ a single type of trees, our
1066
??/VV
???/NR ?/AD ???/NN
??/NR ?/M ??/JJ
??/OD
NP1
VP2
VP3
????? ? ????? ? ????
NR AD VV NR OD M JJ NN
NP1
CLP
QP
NP
NP
VP2
ADVP
VP3
NP
IP
(a)
(c)
Intel         will   launch  Asia     first              super     laptop
Chinese: ??? ? ?? ?? ?? ? ?? ???
English:  Intel will launch the first Ultrabook in Asia
ADVP NP
(b)
Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree
(a), where the bold phrasal nodes NP1,VP2,VP3 indicate the phrases which can not be captured by dependency syn-
tactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing
the nodes with same phrasal categories.
approach yields encouraging results by exploiting t-
wo types of trees. Large-scale experiments (Sec-
tion 5) on Chinese-English translation show that
our model significantly outperforms the state-of-
the-art single constituency-to-string model by av-
eraged +2.45 BLEU points, dependency-to-string
model by averaged +0.91 BLEU points, and hierar-
chical phrase-based model (Chiang, 2005) by aver-
aged +1.12 BLEU points, on three Chinese-English
NIST test sets.
2 Grammar
We take head-dependents relations of dependency
trees as backbone and incorporate phrasal nodes of
constituency trees as the source side of our transla-
tion rules, and the target side as strings. A head-
dependents relation consists of a head and all its de-
pendents in dependency trees, and it can represent
long distance dependencies. Incorporating phrasal
nodes of constituency trees into head-dependents
relations further enhances the compatibility with
phrases of our rules. Figure 1 shows an example of
phrases which can not be captured by a dependen-
cy tree while captured by a constituency tree, such
as the bold phrasal nodes NP1,VP2 and VP3. The
phrasal node NP1 in the constituency tree indicates
that ??? )P? is a noun phrase and it should
be translated as a basic unit, while in the depen-
dency tree it is a non-syntactic phrase. The head-
dependents relation in the top level of the dependen-
cy tree presents long distance dependencies of the
words ?=A?, ???, ????, and ?)P? in a
concise manner, which is useful for long distance re-
ordering. We adopt this kind of rule representation
to hold the property of long distance reorderings and
the compatibility with phrases.
Figure 2 shows two examples of our translation
rules corresponding to the top level of Figure 1-(b).
We can see that r1 captures a head-dependents rela-
tion, while r2 extends r1 by incorporating a phrasal
node VP2 to replace the two nodes ???/VV? and
?)P/NN?. As shown in Figure 1-(b), VP2 con-
sists of two parts, a head node ???/VV? and a
subtree rooted at the dependent node ?)P/NN?.
Therefore, we use VP2 and the POS tags of the t-
wo nodes VV and NN to denote the part covered
by VP2 in r2, to indicate that the source sequence
covered by VP2 can be translated by a bilingual
phrase. Since VP2 covers a head node ???/VV?,
we represent r2 by constructing a new head node
1067
1
??
??? ? 1
1
2
1 2
??? ? 1
Figure 2: Two examples of our translation rules corre-
sponding to the top level of Figure 1-(b). r1 captures a
head-dependents relation, and r2 extends r1 by incorpo-
rating a phrasal node VP2. ?x1:NN? indicates a substitu-
tion site which can be replaced by a subtree whose root
has POS tag ?NN?. ?x1:VP2|||VV NN? indicates a sub-
stitution site which can be replaced by a source phrase
covered by a phrasal node VP (the phrasal node consist-
s of two dependency nodes with POS tag VV and NN,
respectively). The underline denotes a leaf node.
VP2|||VV NN. For simplicity, we use a shorten for-
m CHDR to represent the head-dependents relations
with/without constituency phrasal nodes.
Formally, our grammar G is defined as a 5-tuple
G = ??, Nc, Nd,?, R?, where ? is a set of source
language terminals, Nc is a set of constituency
phrasal categories, Nd is a set of categories (POS
tags) for the terminals in ?, ? is a set of target lan-
guage terminals, and R is a set of translation rules
that include bilingual phrases for translating source
language terminals and CHDR rules for translation
and reordering. A CHDR rule is represented as a
triple ?t, s,??, where:
? t is CHDR with each node labeled by a ter-
minal from ? or a variable from a set X =
{x1, x2, ? ? ? } constrained by a terminal from ?
or a category from Nd or a joint category (con-
structed by the categories from Nc and Nd);
? s ? (X ??) denotes the target side string;
? ? denotes one-to-one links between nontermi-
nals in t and variables in s.
We use the lexicon dependency grammar (Hellwig,
2006) which adopts a bracket representation to ex-
press the head-dependents relation and CHDR. For
example, the left-hand sides of r1 and r2 in Figure 2
can be respectively represented as follows:
(=A) (?)?? (x1:NN)
(=A) (?) x1:VP2|||VV NN
??/VV
???/NR ?/AD ???/NN
??/NR?/M ??/JJ
??/OD
NP1
VP2
VP3
??? ? ?? ?? ?? ? ?? ???
Parseing      Labelling
???/NR ?/AD launch
Intel will launch ????? in Asia
Intel will launch in Asia
(a)
(b)
(c)
(d)
(e)
NP1
?/M
??/OD
Intel will launch in Asiathe    first(f)
Ultrabook
Ultrabook
???/NN
??/NR?/M ??/JJ
??/OD
NP1
r3
r4 r5
r6
r7
?/M
??/OD
r8
(x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
Intel???
? will
(??)(x1:M)x2:NP1|||JJ_NN x1 x2 in Aisa 
????? Ultrabook
?? (?) the first
Translation Rules
r3
r4
r5
r6
r7
r8
(g)
Figure 3: An example derivation of translation. (g) lists
all the translation rules. r3, r6 and r8 are CHDR rules,
while r4, r5 and r7 are bilingual phrases, which are used
for translating source terminals. The dash lines indicate
the reordering when employing a translation rule.
The formalized presentation of r2 in Figure 2-(b):
t = (=A) (?) x1:VP2|||VV NN
s = Intel will x1
?= x1:VP2|||VV NN ? x1
where the underline indicates a leaf node.
Figure 3 gives an example of the translation
derivation in our model, with the translation rules
1068
listed in (g). r3, r6 and r8 are CHDR rules, while
r4, r5 and r7 are bilingual phrases, which are used
for translating source language terminals. Given a
sentence to translate in (a), we first parse it into a
constituency tree and a dependency tree, then label
the phrasal nodes from the constituency tree to the
dependency tree, and yield (b). Then, we translate
it into a target string by the following steps. At the
root node, we apply rule r3 to translate the top level
head-dependents relation and results in four unfin-
ished substructures and target strings in (c). From
(c) to (d), there are three steps (one rule for one step).
We use r4 to translate ?=A? to ?Intel?, r5 to
translate ??? to ?will?, and r6 to translate the right-
most unfinished part. Then, we apply r7 to translate
the phrase ???)P? to ?Ultrabook?, and yield
(e). Finally, we apply r8 to translate the last frag-
ment to ?the first?, and get the final result (f).
3 Rule Extraction
In this section, we describe how to extract rules from
a set of 4-tuples ?C, T, S,A?, where C is a source
constituency tree, T is a source dependency tree, S
is a target side sentence, and A is an word alignmen-
t relation between T /C and S. We extract CHDR
rules from each 4-tuple ?C, T, S,A? based on GHK-
M algorithm (Galley et al, 2004) with three steps:
1. Label the dependency tree with phrasal nodes
from the constituency tree, and annotate align-
ment information to the phrasal nodes labeled
dependency tree (Section 3.1).
2. Identify acceptable CHDR fragments from the
annotated dependency tree for rule induction
(Section 3.2).
3. Induce a set of lexicalized and generalized
CHDR rules from the acceptable fragments
(Section 3.3).
3.1 Annotation
Given a 4-tuple ?C, T, S,A?, we first label phrasal
nodes from the constituency tree C to the depen-
dency tree T , which can be easily accomplished by
phrases mapping according to the common covered
source sequences. As dependency trees can capture
some phrasal information by dependency syntactic
??/VV
{3-3}{1-8}
???/NR
{1-1}{1-1}
?/AD
{2-2}{2-2}
???/NN
{6-6}{4-8}
??/NR
{7-8}{7-8}
?/M
{null}{4-5}
??/JJ
{6-6}{6-6}
??/OD
{4-5}{4-5}
NP1
<6-6>
VP2
<3-8>
VP3
<2-8>
Figure 4: An annotated dependency tree. Each node is
annotated with two spans, the former is node span and
the latter subtree span. The fragments covered by phrasal
nodes are annotated with phrasal spans. The nodes de-
noted by the solid line box are not nsp consistent.
phrases, in order to complement the information that
dependency trees can not capture, we only label the
phrasal nodes that cover dependency non-syntactic
phrases.
Then, we annotate alignment information to the
phrasal nodes labeled dependency tree T , as shown
in Figure 4. For description convenience, we make
use of the notion of spans (Fox, 2002; Lin, 2004).
Given a node n in the source phrasal nodes labeled
T with word alignment information, the spans of n
induced by the word alignment are consecutive se-
quences of words in the target sentence. As shown
in Figure 4, we annotate each node n of phrasal n-
odes labeled T with two attributes: node span and
subtree span; besides, we annotate phrasal span to
the parts covered by phrasal nodes in each subtree
rooted at n. The three types of spans are defined as
follows:
Definition 1 Given a node n, its node span nsp(n)
is the consecutive target word sequence aligned with
the node n.
Take the node ???/NR? in Figure 4 for example,
nsp(??/NR)={7-8}, which corresponds to the tar-
get words ?in? and ?Asia?.
Definition 2 Given a subtree T ? rooted at n, the
subtree span tsp(n) of n is the consecutive target
word sequence from the lower bound of the nsp of
1069
all nodes in T ? to the upper bound of the same set of
spans.
For instance, tsp()P/NN)={4-8}, which corre-
sponds to the target words ?the first Ultrabook in A-
sia?, whose indexes are from 4 to 8.
Definition 3 Given a fragment f covered by a
phrasal node, the phrasal span psp(f) of f is
the consecutive target word sequence aligned with
source string covered by f .
For example, psp(VP2)=?3-8?, which corresponds
to the target word sequence ?launch the first Ultra-
book in Asia?.
We say nsp, tsp and psp are consistent according
to the notion in the phrase-based model (Koehn et
al., 2003). For example, nsp(??/NR), tsp()P
/NN) and psp(NP1) are consistent while nsp(?
?/JJ) and nsp()P/NN) are not consistent.
The annotation can be achieved by a single pos-
torder transversal of the phrasal nodes labeled de-
pendency tree. For simplicity, we call the annotat-
ed phrasal nodes labeled dependency tree annotated
dependency tree. The extraction of bilingual phrases
(including the translation of head node, dependen-
cy syntactic phrases and the fragment covered by a
phrasal node) can be readily achieved by the algo-
rithm described in Koehn et al, (2003). In the fol-
lowing, we focus on CHDR rules extraction.
3.2 Acceptable Fragments Identification
Before present the method of acceptable fragments
identification, we give a brief description of CHDR
fragments. A CHDR fragment is an annotated frag-
ment that consists of a source head-dependents rela-
tion with/without constituency phrasal nodes, a tar-
get string and the word alignment information be-
tween the source and target side. We identify the ac-
ceptable CHDR fragments that are suitable for rule
induction from the annotated dependency tree. We
divide the acceptable CHDR fragments into two cat-
egories depending on whether the fragments con-
tain phrasal nodes. If an acceptable CHDR frag-
ment does not contain phrasal nodes, we call it
CHDR-normal fragment, otherwise CHDR-phrasal
fragment. Given a CHDR fragment F rooted at n,
we say F is acceptable if it satisfies any one of the
following properties:
CHDR-phrasal Rules
r9: (???)(?)x1:VP2|||VV_NN Intel will x1
r10: (x1:NR)(x2:AD)x3:VP2|||VV_NN x1 x2 x3
r11: (???)x1:VP3|||AD_VV_NN Intel x1
r12: (x1:NR)x2:VP3|||AD_VV_NN x1 x2
CHDR-normal Rules
r4: (x1:NR) (x2:AD) ?? (x3:NN) x1 x2 launch x3
Intel will launch x1r3: (???) (?)?? (x1:NN)
r2: (x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
r1: (???) (?)?? (x1:???) Intel will launch x1
r5: (???) (?) x1:VV (x2:???) Intel will x1 x2
r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4
r6: (x1:NR) (x2:AD) x3:VV (x4:???) x1 x2 x3 x4
Intel will x1 x2r7: (???) (?) x1:VV (x2:NN)
(d)
??/VV
???/NR ?/AD ???/NN
Intel
1
will
2
launch
3
the first Ultrabook in Asia
4-8
(a)
Intel
1
will
2
launch the first Ultrabook in Asia
3-8
VP2
??/VV
???/NR ?/AD ???/NN(b)
(c)
Intel
1
will launch the first Ultrabook in Asia
2-8
VP3
??/VV
???/NR ?/AD ???/NN
VP2|||VV_NN
VP3|||AD_VV_NN
Figure 5: Examples of a CHDR-normal fragment (a), two
CHDR-phrasal fragments (b) and (c) that are identified
from the top level of the annotated dependency tree in
Figure 4, and the corresponding CHDR rules (d) induced
from (a), (b) and (c). The underline denotes a leaf node.
1. Without phrasal nodes, the node span of the
root n is consistent and the subtree spans of
n?s all dependents are consistent. For example,
Figure 5-(a) shows a CHDR-normal fragmen-
t that identified from the top level of the an-
notated dependency tree in Figure 4, since the
nsp(??/VV), tsp(=A/NR), tsp(?/AD)
and tsp()P/NN) are consistent.
1070
2. With phrasal nodes, the phrasal spans of
phrasal nodes are consistent; and for the other
nodes, the node span of head (if it is not cov-
ered by any phrasal node) is consistent, and the
subtree spans of dependents are consistent. For
instance, Figure 5-(b) and (c) show two CHDR-
phrasal fragments identified from the top level
of Figure 4. In Figure 5-(b), psp(VP2), tsp(=
A/NR) and tsp(?/AD) are consistent. In
Figure 5-(c), psp(VP3) and tsp(=A/NR)
are consistent.
The identification of acceptable fragments can be
achieved by a single postorder transversal of the an-
notated dependency tree. Typically, each acceptable
fragment contains at most three types of nodes: head
node, head of the related CHDR; internal nodes, in-
ternal nodes of the related CHDR except head node;
leaf nodes, leaf nodes of the related CHDR.
3.3 Rule Induction
From each acceptable CHDR fragment, we induce
a set of lexicalized and generalized CHDR rules.
We induce CHDR-normal rules and CHDR-phrasal
rules from CHDR-normal fragments and CHDR-
phrasal fragments, respectively.
We first induce a lexicalized form of CHDR rule
from an acceptable CHDR fragment:
1. For a CHDR-normal fragment, we first mark
the internal nodes as substitution sites. This
forms the input of a CHDR-normal rule. Then
we generate the target string according to the
node span of the head and the subtree spans of
the dependents, and turn the word sequences
covered by the internal nodes into variables.
This forms the output of a lexicalized CHDR-
normal rule.
2. For a CHDR-phrasal fragment, we first mark
the internal nodes and the phrasal nodes as sub-
stitution sites. This forms the input of a CHDR-
phrasal rule. Then we construct the output of
the CHDR-phrasal rule in almost the same way
with constructing CHDR-normal rules, except
that we replace the target sequences covered by
the internal nodes and the phrasal nodes with
variables.
For example, rule r1 in Figure 5-(d) is a lexicalized
CHDR-normal rule induced from the CHDR-normal
fragment in Figure 5-(a). r9 and r11 are CHDR-
phrasal rules induced from the CHDR-phrasal frag-
ment in Figure 5-(b) and Figure 5-(c) respectively.
As we can see, these CHDR-phrasal rules are par-
tially unlexicalized.
To alleviate the sparseness problem, we gener-
alize the lexicalized CHDR-normal rules and par-
tially unlexicalized CHDR-phrasal rules with un-
lexicalized nodes by the method proposed in Xie
et al, (2011). As the modification relations be-
tween head and dependents are determined by the
edges, we can replace the lexical word of each n-
ode with its category (POS tag) and obtain new
head-dependents relations with unlexicalized nodes
keeping the same modification relations. We gen-
eralize the rule by simultaneously turn the nodes of
the same type (head, internal, leaf) into their cate-
gories. For example, CHDR-normal rules r2 ? r7
are generalized from r1 in Figure 5-(d). Besides, r10
and r12 are the corresponding generalized CHDR-
phrasal rules. Actually, our CHDR rules are the su-
perset of head-dependents relation rules in Xie et
al., (2011). CHDR-normal rules are equivalent with
the head-dependents relation rules and the CHDR-
phrasal rules are the extension of these rules. For
convenience of description, we use the subscript to
distinguish the phrasal nodes with the same catego-
ry, such as VP2 and VP3. In actual operation, we use
VP instead of VP2 and VP3.
We handle the unaligned words of the target side
by extending the node spans of the lexicalized head
and leaf nodes, and the subtree spans of the lexical-
ized dependents, on both left and right directions.
This procedure is similar with the method of Och
and Ney, (2004). During this process, we might ob-
tain m(m ? 1) CHDR rules from an acceptable
fragment. Each of these rules is assigned with a frac-
tional count 1/m. We take the extracted rule set as
observed data and make use of relative frequency es-
timator to obtain the translation probabilities P (t|s)
and P (s|t).
4 Decoding and the Model
Following Och and Ney, (2002), we adopt a general
loglinear model. Let d be a derivation that convert a
1071
source phrasal nodes labeled dependency tree into a
target string e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
the features are used as follows:
? CHDR rules translation probabilities P (t|s)
and P (s|t), and CHDR rules lexical translation
probabilities Plex(t|s) and Plex(s|t);
? bilingual phrases translation probabilities
Pbp(t|s) and Pbp(s|t), and bilingual phrases
lexical translation probabilities Pbplex(t|s) and
Pbplex(s|t);
? rule penalty exp(?1);
? pseudo translation rule penalty exp(?1);
? target word penalty exp(|e|);
? language model Plm(e).
We have twelve features in our model. The values of
the first four features are accumulated on the CHDR
rules and the next four features are accumulated on
the bilingual phrases. We also use a pseudo transla-
tion rule (constructed according to the word order of
head-dependents relation) as a feature to guarantee
the complete translation when no matched rules can
be found during decoding.
Our decoder is based on bottom-up chart-based
algorithm. It finds the best derivation that convert
the input phrasal nodes labeled dependency tree into
a target string among all possible derivations. Giv-
en the source constituency tree and dependency tree,
we first generate phrasal nodes labeled dependency
tree T as described in Section 3.1, then the decoder
transverses each node in T by postorder. For each
node n, it enumerates all instances of CHDR rooted
at n, and checks the rule set for matched translation
rules. A larger translation is generated by substitut-
ing the variables in the target side of a translation
rule with the translations of the corresponding de-
pendents. Cube pruning (Chiang, 2007; Huang and
Chiang, 2007) is used to find the k-best items with
integrated language model for each node.
To balance the performance and speed of the de-
coder, we limit the search space by reducing the
number of translation rules used for each node.
There are two ways to limit the rule table size: by
a fixed limit (rule-limit) of how many rules are re-
trieved for each input node, and by a threshold (rule-
threshold) to specify that the rule with a score low-
er than ? times of the best score should be discard-
ed. On the other hand, instead of keeping the full
list of candidates for a given node, we keep a top-
scoring subset of the candidates. This can also be
done by a fixed limit (stack-limit) and a threshold
(stack-threshold).
5 Experiments
We evaluated the performance of our model by com-
paring with hierarchical phrase-based model (Chi-
ang, 2007), constituency-to-string model (Liu et al,
2006) and dependency-to-string model (Xie et al,
2011) on Chinese-English translation. First, we de-
scribe data preparation (Section 5.1) and systems
(Section 5.2). Then, we validate that our model sig-
nificantly outperforms all the other baseline models
(Section 5.3). Finally, we give detail analysis (Sec-
tion 5.4).
5.1 Data Preparation
Our training data consists of 1.25M sentence pairs
extracted from LDC 1 data. We choose NIST MT
Evaluation test set 2002 as our development set,
NIST MT Evaluation test sets 2003 (MT03), 2004
(MT04) and 2005 (MT05) as our test sets. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric 2.
We parse the source sentences to constituency
trees (without binarization) and projective depen-
dency trees with Stanford Parser (Klein and Man-
ning, 2002). The word alignments are obtained by
running GIZA++ (Och and Ney, 2003) on the corpus
in both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al, 2003). We get
bilingual phrases from word-aligned data with algo-
rithm described in Koehn et al (2003) by running
Moses Toolkit 3. We apply SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train a 4-gram
1Including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and LD-
C2005T06.
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3http://www.statmt.org/moses/
1072
System Rule # MT03 MT04 MT05 Average
Moses-chart 116.4M 34.65 36.47 34.39 35.17
cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84
dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38
consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.
The ?+? denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The ?*?
denotes that the results are significantly better than all the other systems (p<0.01).
language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the English Gigaword
corpus. We make use of the standard MERT (Och,
2003) to tune the feature weights in order to maxi-
mize the system?s BLEU score on the development
set. The statistical significance test is performed by
sign-test (Collins et al, 2005).
5.2 Systems
We take the open source hierarchical phrase-based
system Moses-chart (with default configuration),
our in-house constituency-to-string system cons2str
and dependency-to-string system dep2str as our
baseline systems.
For cons2str, we follow Liu et al, (Liu et al,
2006) to strict that the height of a rule tree is no
greater than 3 and phrase length is no greater than
7. To keep consistent with our proposed model,
we implement the dependency-to-string model (X-
ie et al, 2011) with GHKM (Galley et al, 2004)
rule extraction algorithm and utilize bilingual phras-
es to translate source head node and dependency
syntactic phrases. Our dep2str shows comparable
performance with Xie et al, (2011), which can be
seen by comparing with the results of hierarchical
phrase-based model in our experiments. For dep2str
and our proposed model consdep2str, we set rule-
threshold and stack-threshold to 10?3, rule-limit to
100, stack-limit to 300, and phrase length limit to 7.
5.3 Experimental Results
Table 1 illustrates the translation results of our ex-
periments. As we can see, our consdep2str sys-
tem has gained the best results on all test sets, with
+1.12 BLEU points higher than Moses-chart, +2.45
BLEU points higher than cons2str, and +0.91 BLEU
points higher than dep2str, averagely on MT03,
MT04 and MT05. Our model significantly outper-
forms all the other baseline models, with p<0.01
on statistical significance test sign-test (Collins et
al., 2005). By exploiting two types of trees on
source side, our model gains significant improve-
ments over constituency-to-string and dependency-
to-string models, which employ single type of trees.
Table 1 also lists the statistical results of rules ex-
tracted from training data by different systems. Ac-
cording to our statistics, the number of rules extract-
ed by our consdep2str system is about 18.88% larger
than dep2str, without regard to the 32.5M bilingual
phrases. The extra rules are CHDR-phrasal rules,
which can bring in BLEU improvements by enhanc-
ing the compatibility with phrases. We will conduct
a deep analysis in the next sub-section.
5.4 Analysis
In this section, we first illustrate the influence of
CHDR-phrasal rules in our consdep2str model. We
calculate the proportion of 1-best translations in test
sets that employ CHDR-phrasal rules, and we cal-
l this proportion ?CHDR-phrasal Sent.?. Besides,
the proportion of CHDR-phrasal rules in all CHDR
rules is calculated in these translations, and we cal-
l this proportion ?CHDR-phrasal Rule?. Table 2
lists the using of CHDR-phrasal rules on test sets,
showing that CHDR-phrasal Sent. on all test sets
are higher than 50%, and CHDR-phrasal Rule on al-
l three test sets are higher than 10%. These results
indicate that CHDR-phrasal rules do play a role in
decoding.
Furthermore, we compare some actual transla-
tions of our test sets generated by cons2str, de-
p2str and consdep2str systems, as shown in Fig-
ure 6. In the first example, the Chinese input hold-
s long distance dependencies ???I ?? ?
... \u ... L? '??, which correspond
to the sentence pattern ?noun+adverb+prepositional
1073
System MT03 MT04 MT05
CHDR-phrasal Sent. 50.71 61.80 56.19
CHDR-phrasal Rule 10.53 13.55 10.83
Table 2: The proportion (%) of 1-best translations that
employs CHDR-phrasal rules (CHDR-phrasal Sent.) and
the proportion (%) of CHDR-phrasal rules in all CHDR
rules in these translations (CHDR-phrasal Rule).
phrase+verb+noun?. Cons2str gives a bad result
with wrong global reordering, while our consdep2str
system gains an almost correct result since we cap-
ture this pattern by CHDR-normal rules. In the sec-
ond example, we can see that the Chinese phrase
?2g?y? is a non-syntactic phrase in the depen-
dency tree, and this phrase can not be captured by
head-dependents relation rules in Xie et al, (2011),
thus can not be translated as one unit. Since we en-
code constituency phrasal nodes to the dependency
tree, ?2g?y? is labeled by a phrasal node ?VP?
(means verb phrase), which can be captured by our
CHDR-phrasal rules and translated into the correct
result ?reemergence? with bilingual phrases.
By combining the merits of constituency and
dependency trees, our consdep2str model learns
CHDR-normal rules to acquire the property of long
distance reorderings and CHDR-phrasal rules to ob-
tain good compatibility with phrases.
6 Related Work
In recent years, syntax-based models have witnessed
promising improvements. Some researchers make
efforts on constituency-based models (Graehl and
Knight, 2004; Liu et al, 2006; Huang et al, 2006;
Zhang et al, 2007; Mi et al, 2008; Liu et al, 2009;
Liu et al, 2011; Zhai et al, 2012). Some works pay
attention to dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong et
al., 2007; Shen et al, 2008; Xie et al, 2011). These
models are based on single type of trees.
There are also some approaches combining mer-
its of different structures. Marton and Resnik (2008)
took the source constituency tree into account and
added soft constraints to the hierarchical phrase-
based model (Chiang, 2005). Cherry (2008) u-
tilized dependency tree to add syntactic cohesion
to the phrased-based model. Mi and Liu, (2010)
proposed a constituency-to-dependency translation
model, which utilizes constituency forests on the
source side to direct the translation, and depen-
dency trees on the target side to ensure grammati-
cality. Feng et al (2012) presented a hierarchical
chunk-to-string translation model, which is a com-
promise between the hierarchical phrase-based mod-
el and the constituency-to-string model. Most work-
s make effort to introduce linguistic knowledge in-
to the phrase-based model and hierarchical phrase-
based model with constituency trees. Only the work
proposed by Mi and Liu, (2010) utilized constituen-
cy and dependency trees, while their work applied
two types of trees on two sides.
Instead, our model simultaneously utilizes con-
stituency and dependency trees on the source side to
direct the translation, which is concerned with com-
bining the advantages of two types of trees in trans-
lation rules to advance the state-of-the-art machine
translation.
7 Conclusion
In this paper, we present a novel model that si-
multaneously utilizes constituency and dependency
trees on the source side to direct the translation. To
combine the merits of constituency and dependen-
cy trees, our model employs head-dependents rela-
tions incorporating with constituency phrasal nodes.
Experimental results show that our model exhibits
good performance and significantly outperforms the
state-of-the-art constituency-to-string, dependency-
to-string and hierarchical phrase-based models. For
the first time, source side constituency and depen-
dency trees are simultaneously utilized to direct the
translation, and the model surpasses the state-of-the-
art translation models.
Since constituency tree binarization can lead
to more constituency-to-string rules and syntactic
phrases in rule extraction and decoding, which im-
prove the performance of constituency-to-string sys-
tems, for future work, we would like to do research
on encoding binarized constituency trees to depen-
dency trees to improve translation performance.
Acknowledgments
The authors were supported by National Natural Sci-
ence Foundation of China (Contracts 61202216),
1074
MT05 ---- segment 448
??? ?? ? ?? ?? ??? ?? ?? ? ?? ?? ???
cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .
consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .
reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.
??? ?? ?? ?? ??? ?? ?? ? ??? ?? ?? ?
dobjpobj
prep
advmod
nsubj
pnuct
the united nations has the deadline of the indonesian government on foreign troopsexpressed concern over .
?? ?? ?? ? ?? ?? ??? ??? 6$56?? ??
dep2srt: ?? again severe acute respiratory syndrome ( SARS ) case ??
consdep2srt: ?? reemergence of a severe acute respiratory syndrome ( SARS ) case??
reference: ?? the reemergence of a severe acute respiratory syndrome (SARS) case ??
MT04 ---- segment 194
dep cons & dep
??/VV
??/AD ?/DEG
VP
reemergence
???/NN
??/JJ ??/JJ??/VV
??/AD ?/DEG
again
???/NN
??/JJ ??/JJ
Figure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.
863 State Key Project (No. 2011AA01A207),
and National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge Inno-
vation Program of Chinese Academy of Sciences
(No. KGZD-EW-501). Qun Liu.s work was
partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL
at Dublin City University. Sincere thanks to the
anonymous reviewers for their thorough reviewing
and valuable suggestions. We appreciate Haitao Mi,
Zhaopeng Tu and Anbang Zhao for insightful ad-
vices in writing.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In ACL, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 541?548.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and
Qun Liu. 2012. Hierarchical chunk-to-string transla-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1, pages 950?958.
Heidi J Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 304?3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule. In Pro-
1075
ceedings of HLT/NAACL, volume 4, pages 273?280.
Boston.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. An International Handbook of Contemporary
Research, 2:1081?1109.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Annual Meeting-Association For Computational Lin-
guistics, volume 45, pages 144?151.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. S-
tatistical syntax-directed translation with extended do-
main of locality. In Proceedings of AMTA, pages 66?
73.
Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, volume 15, pages 3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
625?630.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Join-
t Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 558?566.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoining
tree-to-string translation. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1278?1287.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1433?1442.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for s-
tatistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Andreas Stolcke et al 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A nov-
el dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 216?226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A de-
pendency treelet string correspondence model for s-
tatistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 40?47.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting on Association for Computation-
al Linguistics, pages 523?530.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proceedings of COLING 2012, pages
3037?3054.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07, pages 535?542.
1076
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Syntactic SMT Using a Discriminative Text Generation Model
Yue Zhang Kai Song? Linfeng Song?
SUTD, Singapore NEU, China ICT/CAS, China
yue zhang@sutd.edu.sg songkai.sk@alibaba-inc.com songlinfeng@ict.ac.cn
Jingbo Zhu Qun Liu
NEU, China CNGL, Ireland and ICT/CAS, China
zhujingbo@mail.neu.edu.cn qliu@computing.dcu.ie
Abstract
We study a novel architecture for syntactic
SMT. In contrast to the dominant approach
in the literature, the system does not rely
on translation rules, but treat translation
as an unconstrained target sentence gen-
eration task, using soft features to cap-
ture lexical and syntactic correspondences
between the source and target languages.
Target syntax features and bilingual trans-
lation features are trained consistently in
a discriminative model. Experiments us-
ing the IWSLT 2010 dataset show that the
system achieves BLEU comparable to the
state-of-the-art syntactic SMT systems.
1 Introduction
Translation rules have been central to hierarchi-
cal phrase-based and syntactic statistical machine
translation (SMT) (Galley et al., 2004; Chiang,
2005; Liu et al., 2006; Quirk et al., 2005; Marcu et
al., 2006; Shen and Joshi, 2008; Xie et al., 2011).
They are attractive by capturing the recursiveness
of languages and syntactic correspondences be-
tween them. One important advantage of trans-
lation rules is that they allow efficient decoding
by treating MT as a statistical parsing task, trans-
forming a source sentence to its translation via re-
cursive rule application.
The efficiency takes root in the fact that target
word orders are encoded in translation rules. This
fact, however, also leads to rule explosion, noise
and coverage problems (Auli et al., 2009), which
can hurt translation quality. Flexibility of function
word usage, rich morphology and paraphrasing all
add to the difficulty of rule extraction. In addition,
restricting target word orders by hard translation
rules can also hurt output fluency.
?
* Work done while visiting Singapore University of
Technology and Design (SUTD)
Figure 1: Overall system architecture.
A potential solution to the problems above is to
treat translation as a generation task, represent-
ing syntactic correspondences using soft features.
Both adequacy and fluency can potentially be im-
proved by giving full flexibility to target synthe-
sis, and leaving all options to the statistical model.
The main challenge to this method is a signifi-
cant increase in the search space (Knight, 1999).
To this end, recent advances in tackling complex
search tasks for text generation offer some so-
lutions (White and Rajkumar, 2009; Zhang and
Clark, 2011).
In this short paper, we present a preliminary in-
vestigation on the possibility of building a syn-
tactic SMT system that does not use hard transla-
tion rules, by utilizing recent advances in statisti-
cal natural language generation (NLG). The over-
all architecture is shown in Figure 1. Translation
is performed by first parsing the source sentence,
then transferring source words and phrases to their
target equivalences, and finally synthesizing the
target output.
We choose dependency grammar for both the
source and the target syntax, and adapt the syntac-
tic text synthesis system of Zhang (2013), which
performs dependency-based linearization. The
linearization task for MT is different from the
monolingual task in that not all translation options
are used to build the output, and that bilingual cor-
respondences need to be taken into account dur-
177
ing synthesis. The algorithms of Zhang (2013) are
modified to perform word selection as well as or-
dering, using two sets of features to control trans-
lation adequacy and fluency, respectively.
Preliminary experiments on the IWSLT1 2010
data show that the system gives BLEU compara-
ble to traditional tree-to-string and string-to-tree
translation systems. It demonstrates the feasibility
of leveraging statistical NLG techniques for SMT,
and the possibility of building a statistical transfer-
based MT system.
2 Approach
The main goal being proof of concept, we keep
the system simple by utilizing existing methods
for the main components, minimizing engineer-
ing efforts. Shown in Figure 1, the end-to-end
system consists of two main components: lexical
transfer and synthesis. The former provides can-
didate translations for (overlapping) source words
and phrases. Although lexicons and rules can
be used for this step, we take a simple statisti-
cal alignment-based approach. The latter searches
for a target translation by constructing dependency
trees bottom-up. The process can be viewed as
a syntax-based generation process from a bag of
overlapping translation options.
2.1 Lexical transfer
We perform word alignment using IBM model 4
(Brown et al., 1993), and then extract phrase pairs
according to the alignment and automatically-
annotated target syntax. In particular, consistent
(Och et al., 1999) and cohesive (Fox, 2002) phrase
pairs are extracted from intersected alignments in
both directions: the target side must form a pro-
jective span, with a single root, and the source side
must be contiguous. A resulting phrase pair con-
sists of the source phrase, its target translation, as
well as the head position and head part-of-speech
(POS) of the target span, which are useful for tar-
get synthesis. We further restrict that neither the
source nor the target side of a valid phrase pair
contains over s words.
Given an input source sentence, the lexical
transfer unit finds all valid target translation op-
tions for overlapping source phrases up to size s,
and feeds them as inputs to the target synthesis de-
coder. The translation options with a probability
1International Workshop on Spoken Language Transla-
tion, http://iwslt2010.fbk.eu
below ? ? P
max
are filtered out, where P
max
is the
probability of the most probable translation. Here
the probability of a target translation is calculated
as the count of the translation divided by the count
of all translations of the source phrase.
2.2 Synthesis
The synthesis module is based on the monolingual
text synthesis algorithm of Zhang (2013), which
constructs an ordered dependency tree given a bag
of words. In the bilingual setting, inputs to the al-
gorithm are translation options, which can be over-
lapping and mutually exclusive, and not necessar-
ily all of which are included in the output. As a
result, the decoder needs to perform word selec-
tion in addition to word ordering. Another differ-
ence between the bilingual and monolingual set-
tings is that the former requires translation ade-
quacy in addition to output fluency.
We largely rely on the monolingual system for
MT decoding. To deal with overlapping transla-
tion options, a source coverage vector is used to
impose mutual exclusiveness on input words and
phrases. Each element in the coverage vector is
a binary value that indicates whether a particular
source word has been translated in the correspond-
ing target hypothesis. For translation adequacy,
we use a set of bilingual features on top of the set
of monolingual features for text synthesis.
2.2.1 Search
The search algorithm is the best-first algorithm of
Zhang (2013). Each search hypothesis is a par-
tial or full target-language dependency tree, and
hypotheses are constructed bottom-up from leaf
nodes, which are translation options. An agenda
is used to maintain a list of search hypothesis to
be expanded, and a chart is used to record a set
of accepted hypotheses. Initially empty, the chart
is a beam of size k ? n, where n is the number
of source words and k is a positive integer. The
agenda is a priority queue, initialized with all leaf
hypotheses (i.e. translation options). At each step,
the highest-scored hypothesis e is popped off the
agenda, and expanded by combination with all hy-
potheses on the chart in all possible ways, with
the set of newly generated hypotheses e
1
, e
2
, ...e
N
being put onto the agenda, and e being put onto
the chart. When two hypotheses are combined,
they can be put in two different orders, and in each
case different dependencies can be constructed be-
tween their head words, leading to different new
178
dependency syntax
WORD(h) ? POS(h) ? NORM(size) ,
WORD(h) ? NORM(size), POS(h) ? NORM(size)
POS(h) ? POS(m) ? POS(b) ? dir
POS(h) ? POS(h
l
) ? POS(m) ? POS(m
r
) ? dir (h > m),
POS(h) ? POS(h
r
) ? POS(m) ? POS(m
l
) ? dir (h < m)
WORD(h) ? POS(m) ? POS(m
l
) ? dir ,
WORD(h) ? POS(m) ? POS(m
r
) ? dir
POS(h) ? POS(m) ? POS(m
1
) ? dir ,
POS(h) ? POS(m
1
) ? dir , POS(m) ? POS(m
1
) ? dir
WORD(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
POS(h) ? POS(m) ? POS(m
1
) ? POS(m
2
) ? dir ,
...
dependency syntax for completed words
WORD(h) ? POS(h) ? WORD(h
l
) ? POS(h
l
),
POS(h) ? POS(h
l
),
WORD(h) ? POS(h) ? POS(h
l
),
POS(h) ? WORD(h
l
) ? POS(h
l
) ,
WORD(h) ? POS(h) ? WORD(h
r
) ? POS(h
r
),
POS(h) ? POS(h
r
),
...
surface string patterns (B?bordering index)
WORD(B ? 1) ? WORD(B), POS(B ? 1) ? POS(B),
WORD(B ? 1) ? POS(B), POS(B ? 1) ? WORD(B),
WORD(B ? 1) ? WORD(B) ? WORD(B + 1),
WORD(B ? 2) ? WORD(B ? 1) ? WORD(B),
POS(B ? 1) ? POS(B) ? POS(B + 1),
...
surface string patterns for complete sentences
WORD(0), WORD(0) ? WORD(1),
WORD(size ? 1),
WORD(size ? 1) ? WORD(size ? 2),
POS(0), POS(0) ? POS(1),
POS(0) ? POS(1) ? POS(2),
...
Table 1: Monolingual feature templates.
hypotheses. The decoder expands a fixed number
L hypotheses, and then takes the highest-scored
chart hypothesis that contains over ? ? n words as
the output, where ? is a real number near 1.0.
2.2.2 Model and training
A scaled linear model is used by the decoder to
score search hypotheses:
Score(e) =
~
? ? ?(e)
|e|
,
where ?(e) is the global feature vector of the hy-
pothesis e, ~? is the parameter vector of the model,
and |e| is the number of leaf nodes in e. The
scaling factor |e| is necessary because hypothe-
ses with different numbers of words are compared
with each other in the search process to capture
translation equivalence.
While the monolingual features of Zhang
(2013) are applied (example feature templates
from the system are shown in Table 1), an addi-
tional set of bilingual features is defined, shown
phrase translation features
PHRASE(m) ? PHRASE(t), P (trans),
bilingual syntactic features
POS(th) ? POS(tm) ? dir ? LEN(path),
WORD(th) ? POS(tm) ? dir ? LEN(path),
POS(th) ? WORD(tm) ? dir ? LEN(path),
WORD(th) ? WORD(tm) ? dir ? LEN(path),
WORD(sh) ? WORD(sm) ? dir ? LEN(path),
WORD(sh) ? WORD(th) ? dir ? LEN(path),
WORD(sm) ? WORD(tm) ? dir ? LEN(path),
bilingual syntactic features (LEN(path) ? 3)
POS(th) ? POS(tm) ? dir ? LABELS(path),
WORD(th) ? POS(tm) ? dir ? LABELS(path),
POS(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(th) ? WORD(tm) ? dir ? LABELS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELS(path),
WORD(sh) ? WORD(th) ? dir ? LABELS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELS(path),
POS(th) ? POS(tm) ? dir ? LABELSPOS(path),
WORD(th) ? POS(tm) ? dir ? LABELSPOS(path),
POS(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(th) ? WORD(tm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(sm) ? dir ? LABELSPOS(path),
WORD(sh) ? WORD(th) ? dir ? LABELSPOS(path),
WORD(sm) ? WORD(tm) ? dir ? LABELSPOS(path),
Table 2: Bilingual feature templates.
in Table 2. In the tables, s and t represent the
source and target, respectively; h and m repre-
sent the head and modifier in a dependency arc,
respectively; h
l
and h
r
represent the neighboring
words on the left and right of h, respectively; m
l
and m
r
represent the neighboring words on the left
and right of m, respectively; m
1
and m
2
repre-
sent the closest and second closest sibling of m on
the side of h, respectively. dir represents the arc
direction (i.e. left or right); PHRASE represents
a lexical phrase; P(trans) represents the source-
to-target translation probability from the phrase-
table, used as a real-valued feature; path repre-
sents the shortest path in the source dependency
tree between the two nodes that correspond to the
target head and modifier, respectively; LEN(path)
represents the number of arcs on path, normalized
to bins of [5, 10, 20, 40+]; LABELS(path) repre-
sents the array of dependency arc labels on path;
LABELSPOS(path) represents the array of depen-
dency arc labels and source POS on path. In addi-
tion, a real-valued four-gram language model fea-
ture is also used, with four-grams extracted from
the surface boundary when two hypothesis are
combined.
We apply the discriminative learning algorithm
of Zhang (2013) to train the parameters ~?. The al-
gorithm requires training examples that consist of
full target derivations, with leaf nodes being input
translation options. However, the readily available
179
training examples are automatically-parsed target
derivations, with leaf nodes being the reference
translation. As a result, we apply a search pro-
cedure to find a derivation process, through which
the target dependency tree is constructed from a
subset of input translation options. The search
procedure can be treated as a constrained decod-
ing process, where only the oracle tree and its sub
trees can be constructed. In case the set of transla-
tion options cannot lead to the oracle tree, we ig-
nore the training instance.2 Although the ignored
training sentence pairs cannot be utilized for train-
ing the discriminative synthesizer, they are never-
theless used for building the phrase table and train-
ing the language model.
3 Experiments
We perform experiments on the IWSLT 2010
Chinese-English dataset, which consists of train-
ing sentence pairs from the dialog task (dialog)
and Basic Travel and Expression Corpus (BTEC).
The union of dialog and BTEC are taken as our
training set, which contains 30,033 sentence pairs.
For system tuning, we use the IWSLT 2004 test set
(also released as the second development test set
of IWSLT 2010), which contains 500 sentences.
For final test, we use the IWSLT 2003 test set (also
released as the first development test set of IWSLT
2010), which contains 506 sentences.
The Chinese sentences in the datasets are seg-
mented using NiuTrans3 (Xiao et al., 2012), while
POS-tagging of both English and Chinese is per-
formed using ZPar4 version 0.5 (Zhang and Clark,
2011). We train the English POS-tagger using the
WSJ sections of the Penn Treebank (Marcus et al.,
1993), turned into lower-case. For syntactic pars-
ing of both English and Chinese, we use the de-
fault models of ZPar 0.5.
We choose three baseline systems: a string-to-
tree (S2T) system, a tree-to-string (T2S) system
and a tree-to-tree (T2T) system (Koehn, 2010).
The Moses release 1.0 implementations of all
three systems are used, with default parameter set-
tings. IRSTLM5 release 5.80.03 (Federico et al.,
2008) is used to train a four-gram language models
2This led to the ignoring of over 40% of the training sen-
tence pairs. For future work, we will consider substitute or-
acles from reachable target derivations by using maximum
sentence level BLEU approximation (Nakov et al., 2012) or
METEOR (Denkowski and Lavie, 2011) as selection criteria.
3http://www.nlplab.com/NiuPlan/NiuTrans.ch.html
4http://sourceforge.net/projects/zpar/
5http://sourceforge.net/apps/mediawiki/irstlm
System T2S S2T T2T OURS
BLEU 32.65 36.07 28.46 34.24
Table 3: Final results.
SOURCE:?????????
REF: I have a terrible headache .
OURS: now , I have a headache .
SOURCE:??????????
REF: I ?d like a twin room with a bath please .
OURS: a twin room , I ?ll find a room with a bath .
SOURCE:??????????
REF: can you change yen into dollars ?
OURS: please change yen into dollars .
SOURCE:????? ?
REF: roast chicken , please .
OURS: please have roast chicken .
SOURCE:?????????
REF: take two tablets after every meal .
OURS: please eat after each meal .
SOURCE:????
REF: check , please .
OURS: I have to check - out , please .
SOURCE:?????????????
REF: yes , well , that ?s our specialty .
OURS: ah , the food that ?s right .
SOURCE:?????
REF: my air conditioner is n?t working .
OURS: the air - conditioner does n?t work .
Table 4: Sample output sentences.
over the English training data, which is applied to
the baseline systems and our system. Kneser-Ney
smoothing is used to train the language model.
We use the tuning set to determine the optimal
number of training iterations. The translation op-
tion filter ? is set to 0.1; the phrase size limit s is
set to 5 in order to verify the effectiveness of syn-
thesis; the number of expanded nodes L is set to
200; the chart factor k is set to 16 for a balance be-
tween efficiency and accuracy; the goal parameter
? is set to 0.8.
The final scores of our system and the baselines
are shown in Table 3. Our system gives a BLEU
of 34.24, which is comparable to the baseline sys-
tems. Some example outputs are shown in Table 4.
Manual comparison does not show significant dif-
ferences in overall translation adequacy or fluency
between the outputs of the four systems. However,
an observation is that, while our system can pro-
duce more fluent outputs, the choice of translation
options can be more frequently incorrect. This
suggests that while the target synthesis component
is effective under the bilingual setting, a stronger
lexical selection component may be necessary for
better translation quality.
180
4 Related work
As discussed in the introduction, our work is
closely related to previous studies on syntactic
MT, with the salient difference that we do not rely
on hard translation rules, but allow free target syn-
thesis. The contrast can be summarized as ?trans-
lation by parsing? vs ?translation by generation?.
There has been a line of research on genera-
tion for translation. Soricut and Marcu (2006) use
a form of weighted IDL-expressions (Nederhof
and Satta, 2004) for generation. Bangalore et al.
(2007) treats MT as a combination of global lex-
ical transfer and word ordering; their generation
component does not perform lexical selection, re-
lying on an n-gram language model to order target
words. Goto et al. (2012) use a monotonic phrase-
based system to perform target word selection, and
treats target ordering as a post-processing step.
More recently, Chen et al. (2014) translate source
dependencies arc-by-arc to generate pseudo target
dependencies, and generate the translation by re-
ordering of arcs. In contrast with these systems,
our system relies more heavily on a syntax-based
synthesis component, in order to study the useful-
ness of statistical NLG on SMT.
With respect to syntax-based word ordering,
Chang and Toutanova (2007) and He et al. (2009)
study a simplified word ordering problem by as-
suming that the un-ordered target dependency tree
is given. Wan et al. (2009) and Zhang and Clark
(2011) study the ordering of a bag of words, with-
out input syntax. Zhang et al. (2012), Zhang
(2013) and Song et al. (2014) further extended this
line of research by adding input syntax and allow-
ing joint inflection and ordering. de Gispert et al.
(2014) use a phrase-structure grammer for word
ordering. Our generation system is based on the
work of Zhang (2013), but further allows lexical
selection.
Our work is also in line with the work of Liang
et al. (2006), Blunsom et al. (2008), Flanigan et
al. (2013) and Yu et al. (2013) in that we build a
discriminative model for SMT.
5 Conclusion
We investigated a novel system for syntactic ma-
chine translation, treating MT as an unconstrained
generation task, solved by using a single discrim-
inative model with both monolingual syntax and
bilingual translation features. Syntactic corre-
spondence is captured by using soft features rather
than hard translation rules, which are used by most
syntax-based statistical methods in the literature.
Our results are preliminary in the sense that
the experiments were performed using a relatively
small dataset, and little engineering effort was
made on fine-tuning of parameters for the base-
line and proposed models. Our Python imple-
mentation gives the same level of BLEU scores
compared with baseline syntactic SMT systems,
but is an order of magnitude slower than Moses.
However, the results demonstrate the feasibility of
leveraging text generation techniques for machine
translation, directly connecting the two currently
rather separated research fields. The system is not
strongly dependent on the specific generation al-
gorithm, and one potential of the SMT architec-
ture is that it can directly benefit from advances in
statistical NLG technology.
Acknowledgement
The work has been supported by the Singa-
pore Ministration of Education Tier 2 project
T2MOE201301 and the startup grant SRG ISTD
2012 038 from SUTD. We thank the anonymous
reviewers for their constructive comments.
References
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT, pages 224?
232.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proc. ACL, pages 152?159.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL, pages 200?208.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263?
311.
Pi-Chuan Chang and Kristina Toutanova. 2007. A dis-
criminative syntactic word order model for machine
translation. In Proc. ACL, pages 9?16.
Hongshen Chen, Jun Xie, Fandong Meng, Wenbin
Jiang, and Qun Liu. 2014. A dependency edge-
based transfer model for statistical machine transla-
tion. In Proc. COLING 2014, pages 1103?1113.
181
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
ACL, pages 263?270.
Adria` de Gispert, Marcus Tomalin, and Bill Byrne.
2014. Word ordering with phrase-based grammars.
In Proc. EACL, pages 259?268.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Proc.
WMT, pages 85?91.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proc. In-
terspeech, pages 1618?1621.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proc. NAACL, pages 248?258.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. EMNLP, pages 304?311.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. HLT-NAACL, pages 273?280.
Isao Goto, Masao Utiyama, and Eiichiro Sumita. 2012.
Post-ordering by parsing for Japanese-English sta-
tistical machine translation. In Proc. ACL, pages
311?316.
Wei He, Haifeng Wang, Yuqing Guo, and Ting Liu.
2009. Dependency based Chinese sentence realiza-
tion. In Proc. ACL/AFNLP, pages 809?816.
Kevin Knight. 1999. Squibs and Discussions: Decod-
ing Complexity in Word-Replacement Translation
Models. Computational Linguistics, 25(4):607?
615.
Phillip Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In Proc. COLING/ACL, pages
761?768.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. COLING/ACL, pages 609?616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proc. EMNLP, pages 44?52.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The penn treebank. Com-
putational linguistics, 19(2):313?330.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for sentence-level BLEU+1
yields short translations. In Proc. Coling, pages
1979?1994.
Mark-Jan Nederhof and Giorgio Satta. 2004. Idl-
expressions: a formalism for representing and pars-
ing finite languages in natural language processing.
J. Artif. Intell. Res.(JAIR), 21:287?317.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statis-
tical machine translation. In Proc. EMNLP, pages
20?28.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proc. ACL, pages 271?279.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proc. EMNLP, pages 495?504.
Linfeng Song, Yue Zhang, Kai Song, and Qun Liu.
2014. Joint morphological generation and syntactic
linearization. In Proc. AAAI, pages 1522?1528.
Radu Soricut and Daniel Marcu. 2006. Stochastic lan-
guage generation using widl-expressions and its ap-
plication in machine translation and summarization.
In Proc. ACL, pages 1105?1112.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proc. EACL, pages 852?860.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Proc.
the EMNLP, pages 410?419.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li.
2012. NiuTrans: An open source toolkit for phrase-
based and syntax-based machine translation. In
Proc. ACL Demos, pages 19?24.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proc. EMNLP, pages 216?226.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proc. EMNLP,
pages 1112?1123.
Yue Zhang and Stephen Clark. 2011. Syntax-based
grammaticality improvement using CCG and guided
search. In Proc. EMNLP, pages 1147?1157.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating a
large-scale language model. In Proc. EACL, pages
736?746.
Yue Zhang. 2013. Partial-tree linearization: General-
ized word ordering for text synthesis. In Proc. IJ-
CAI, pages 2232?2238.
182
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 546?556,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Modeling Term Translation for Document-informed Machine Translation
Fandong Meng
1, 2
Deyi Xiong
3
Wenbin Jiang
1, 2
Qun Liu
4, 1
1
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
2
University of Chinese Academy of Sciences
{mengfandong,jiangwenbin,liuqun}@ict.ac.cn
3
School of Computer Science and Technology, Soochow University
dyxiong@suda.edu.cn
4
Centre for Next Generation Localisation, Dublin City University
Abstract
Term translation is of great importance for
statistical machine translation (SMT), es-
pecially document-informed SMT. In this
paper, we investigate three issues of term
translation in the context of document-
informed SMT and propose three cor-
responding models: (a) a term trans-
lation disambiguation model which se-
lects desirable translations for terms in the
source language with domain information,
(b) a term translation consistency model
that encourages consistent translations for
terms with a high strength of translation
consistency throughout a document, and
(c) a term bracketing model that rewards
translation hypotheses where bracketable
source terms are translated as a whole
unit. We integrate the three models into
hierarchical phrase-based SMT and eval-
uate their effectiveness on NIST Chinese-
English translation tasks with large-scale
training data. Experiment results show
that all three models can achieve sig-
nificant improvements over the baseline.
Additionally, we can obtain a further
improvement when combining the three
models.
1 Introduction
A term is a linguistic expression that is used as
the designation of a defined concept in a language
(ISO 1087). As terms convey concepts of a text,
term translation becomes crucial when the text is
translated from its original language to another
language. The translations of terms are often af-
fected by the domain in which terms are used and
the context that surrounds terms (Vasconcellos et
al., 2001). In this paper, we study domain-specific
and context-sensitive term translation for SMT.
In order to achieve this goal, we focus on three
issues of term translation: 1) translation ambigu-
ity, 2) translation consistency and 3) bracketing.
First, term translation ambiguity is related to trans-
lations of the same term in different domains. A
source language term may have different transla-
tions when it occurs in different domains. Second,
translation consistency is about consistent trans-
lations for terms that occur in the same document.
Usually, it is undesirable to translate the same term
in different ways as it occurs in different parts of
a document. Finally, bracketing concerns whether
a multi-word term is bracketable during transla-
tion. Normally, a multi-word term is translated as
a whole unit into a contiguous target string.
We study these three issues in the context
of document-informed SMT. We use document-
informed information to disambiguate term trans-
lations in different documents and maintain con-
sistent translations for terms that occur in the same
document. We propose three different models for
term translation that attempt to address the three
issues mentioned above. In particular,
? Term Translation Disambiguation Model: In
this model, we condition the translations of
terms in different documents on correspond-
ing per-document topic distributions. In do-
ing so, we enable the decoder to favor trans-
lation hypotheses with domain-specific term
translations.
? Term Translation Consistency Model: This
model encourages the same terms with a high
strength of translation consistency that occur
in different parts of a document to be trans-
lated in a consistent fashion. We calculate
the translation consistency strength of a term
based on the topic distribution of the docu-
ments where the term occurs in this model.
? Term Bracketing Model: We use the brack-
eting model to reward translation hypothe-
546
ses where bracketable multi-word terms are
translated as a whole unit.
We integrate the three models into hierarchical
phrase-based SMT (Chiang, 2007). Large-scale
experiment results show that they are all able to
achieve significant improvements of up to 0.89
BLEU points over the baseline. When simulta-
neously integrating the three models into SMT,
we can gain a further improvement, which outper-
forms the baseline by up to 1.16 BLEU points.
In the remainder of this paper, we begin with
a brief overview of related work in Section 2,
and bilingual term extraction in Section 3. We
then elaborate the proposed three models for term
translation in Section 4. Next, we conduct experi-
ments to validate the effectiveness of the proposed
models in Section 5. Finally, we conclude and pro-
vide directions for future work in Section 6.
2 Related Work
In this section, we briefly introduce related work
and highlight the differences between our work
and previous studies.
As we approach term translation disambigua-
tion and consistency via topic modeling, our mod-
els are related to previous work that explores the
topic model (Blei et al., 2003) for machine trans-
lation (Zhao and Xing, 2006; Su et al., 2012;
Xiao et al., 2012; Eidelman et al., 2012). Zhao
and Xing (2006) employ three models that enable
word alignment process to leverage topical con-
tents of document-pairs with topic model. Su et al.
(2012) establish the relationship between out-of-
domain bilingual corpus and in-domain monolin-
gual corpora via topic mapping and phrase-topic
distribution probability estimation for translation
model adaptation. Xiao et al. (2012) propose a
topic similarity model for rule selection. Eidel-
man et al. (2012) use topic models to adapt lexical
weighting probabilities dynamically during trans-
lation. In these studies, the topic model is not used
to address the issues of term translation mentioned
in Section 1.
Our work is also related to document-level
SMT in that we use document-informed informa-
tion for term translation. Tiedemann (2010) pro-
pose cache-based language and translation mod-
els, which are built on recently translated sen-
tences. Gong et al. (2011) extend this by further
introducing two additional caches. They employ
a static cache to store bilingual phrases extracted
from documents in training data that are similar to
the document being translated and a topic cache
with target language topic words. Recently we
have also witnessed efforts that model lexical co-
hesion (Hardmeier et al., 2012; Wong and Kit,
2012; Xiong et al., 2013a; Xiong et al., 2013b)
as well as coherence (Xiong and Zhang, 2013)
for document-level SMT. Hasler et al. (2014a)
use topic models to learn document-level transla-
tion probabilities. Hasler et al. (2014b) use topic-
adapted model to improve lexical selection. The
significant difference between our work and these
studies is that term translation has not been inves-
tigated in these document-level SMT models.
Itagaki and Aikawa (2008) employ bilingual
term bank as a dictionary for machine-aided trans-
lation. Ren et al. (2009) propose a binary feature
to indicate whether a bilingual phrase contains a
term pair. Pinis and Skadins (2012) investigate that
bilingual terms are important for domain adapta-
tion of machine translation. These studies do not
focus on the three issues of term translation as
discussed in Section 1. Furthermore, domain and
document-informed information is not used to as-
sist term translation.
Itagaki et al. (2007) propose a statistical method
to calculate translation consistency for terms with
explicit domain information. Partially inspired
by their study, we introduce a term translation
consistency metric with document-informed infor-
mation. Furthermore, we integrate the proposed
term translation consistency model into an actual
SMT system, which has not been done by Itagaki
et al. (2007). Ture et al. (2012) use IR-inspired
tf-idf scores to encourage consistent translation
choice. Guillou (2013) investigates what kind of
words should be translated consistently. Term
translation consistency has not been investigated
in these studies.
Our term bracketing model is also related
to Xiong et al. (2009)?s syntax-driven bracket-
ing model for phrase-based translation, which pre-
dicts whether a phrase is bracketable or not using
rich syntactic constraints. The difference is that
we construct the model with automatically created
bilingual term bank and do not depend on any syn-
tactic knowledge.
3 Bilingual Term Extraction
Bilingual term extraction is to extract terms from
two languages with the purpose of creating or ex-
547
tending a bilingual term bank, which in turn can
be used to improve other tasks such as information
retrieval and machine translation. In this paper, we
want to automatically build a bilingual term bank
so that we can model term translation to improve
translation quality of SMT. Our interest is to ex-
tract multi-word terms.
Currently, there are mainly two strategies to
conduct bilingual term extraction from parallel
corpora. One of them is to extract term candi-
dates separately for each language according to
monolingual term metrics, such as C-value/NC-
value (Frantzi et al., 1998; Vu et al., 2008), or
other common cooccurrence measures such as
Log-Likelihood Ratio, Dice coefficient and Point-
wise Mutual Information (Daille, 1996; Piao et
al., 2006). The extracted monolingual terms are
then paired together (Hjelm, 2007; Fan et al.,
2009; Ren et al., 2009). The other strategy is to
align words and word sequences that are transla-
tion equivalents in parallel corpora and then clas-
sify them into terms and non-terms (Merkel and
Foo, 2007; Lefever et al., 2009; Bouamor et al.,
2012). In this paper, we adopt the first strategy.
In particular, for each sentence pair, we collect all
source phrases which are terms and find aligned
target phrases for them via word alignments. If
the target side is also a term, we store the source
and target term as a term pair.
We conduct monolingual term extraction using
the C-value/NC-value metric and Log-Likelihood
Ratio (LLR) measure respectively. We then com-
bine terms extracted according to the two metrics
mentioned above. For the C-value/NC-value met-
ric based term extraction, we implement it in the
same way as described in Frantzi et al. (1998).
This extraction method recognizes linguistic pat-
terns (mainly noun phrases) listed as follows.
((Adj|Noun)
+
|((Adj|Noun)
?
(NounPrep)
?
)(Adj|Noun)
?
)Noun
It captures the linguistic structures of terms. For
the LLR metric based term extraction, we imple-
ment it according to Daille (1996), who estimate
the propensity of two words to appear together as a
multi-word expression. We then adopt LLR-based
hierarchical reducing algorithm proposed by Ren
et al. (2009) to extract terms with arbitrary lengths.
Since the C-value/NC-value metric based extrac-
tion method can obtain terms in strict linguistic
patterns while the LLR measure based method ex-
tracts more flexible terms, these two methods are
complementary to each other. Therefore, we use
these two methods to extract monolingual multi-
word terms and then combine the extracted terms.
4 Models
This section presents the three models of term
translation. They are the term translation dis-
ambiguation model, term translation consistency
model and term bracketing model respectively.
4.1 Term Translation Disambiguation Model
The most straightforward way to disambiguate
term translations in different domains is to cal-
culate the conditional translation probability of
a term given domain information. We use the
topic distribution of a document obtained by a
topic model to represent the domain information
of the document. Since Latent Dirichlet Alloca-
tion (LDA) (Blei et al., 2003) is the most widely-
used topic model, we exploit it for inferring topic
distributions of documents. Xiao et al. (2012)
proposed a topic similarity model for rule selec-
tion. Different from their work, we take an eas-
ier strategy that estimates topic-conditioned term
translation probabilities rather than rule-topic dis-
tributions. This makes our model easily scalable
on large training data.
With the bilingual term bank created from the
training data, we calculate the source-to-target
term translation probability for each term pair con-
ditioned on the topic distribution of the source
document where the source term occurs. We main-
tain a K-dimension (K is the number of topics)
vector for each term pair. The k-th component
p(t
e
|t
f
, z = k) measures the conditional transla-
tion probability from source term t
f
to target term
t
e
given the topic k.
We calculate p(t
e
|t
f
, z = k) via maximum
likelihood estimation with counts from training
data. When the source part of a bilingual term
pair occurs in a document D with topic distribu-
tion p(z|D) estimated via LDA tool, we collect
an instance (t
f
, t
e
, p(z|D), c), where c is the frac-
tion count of the instance as described in Chiang
(2007). After collection, we get a set of instances
I = {(t
f
, t
e
, p(z|D), c)}with different document-
topic distributions for each bilingual term pair. Us-
ing these instances, we calculate the probability
548
p(t
e
|t
f
, z = k) as follows:
p(t
e
|t
f
, z = k)
=
?
i?I,i.t
f
=t
f
,i.t
e
=t
e
i.c ? p(z = k|D)
?
i?I,i.t
f
=t
f
i.c ? p(z = k|D)
(1)
We associate each extracted term pair in our
bilingual term bank with its corresponding topic-
conditioned translation probabilities estimated in
the Eq. (1). When translating sentences of docu-
ment D
?
, we first get the topic distribution of D
?
using LDA tool. Given a sentence which contains
T terms {t
f
i
}
T
1
in D
?
, our term translation disam-
biguation model TermDis can be denoted as
TermDis =
T
?
i=1
P
d
(t
e
i
|t
f
i
, D
?
) (2)
where the conditional source-to-target term trans-
lation probability P
d
(t
e
i
|t
f
i
, D
?
) given the docu-
ment D
?
is formulated as follows:
P
d
(t
e
i
|t
f
i
, D
?
)
=
K
?
k=1
p(t
e
i
|t
f
i
, z = k) ? p(z = k|D
?
) (3)
Whenever a source term t
f
i
is translated into t
e
i
,
we check whether the pair of t
f
i
and its translation
t
e
i
can be found in our bilingual term bank. If it
can be found, we calculate the conditional transla-
tion probability from t
f
i
to t
e
i
given the document
D
?
according to Eq. (3).
The term translation disambiguation model is
integrated into the log-linear model of SMT as a
feature. Its weight is tuned via minimum error rate
training (MERT) (Och, 2003). Through the fea-
ture, we can enable the decoder to favor translation
hypotheses that contain target term translations ap-
propriate for the domain represented by the topic
distribution of the corresponding document.
4.2 Term Translation Consistency Model
The term translation disambiguation model helps
the decoder select appropriate translations for
terms that are in accord with their domains. Yet
another translation issue related to the domain-
specific term translation is to what extent a term
should be translated consistently given the domain
where it occurs. Term translation consistency in-
dicates the translation stability that a source term
is translated into the same target term (Itagaki et
al., 2007). When translating a source term, if the
translation consistency strength of the source term
is high, we should take the corresponding target
term as the translation for it. Otherwise, we may
need to create a new translation for it according to
its context. In particular, we want to enable the
decoder to choose between: 1) translating a given
source term into the extracted corresponding tar-
get term or 2) translating it in another way accord-
ing to the strength of its translation consistency.
In doing so, we can encourage consistent transla-
tions for terms with a high translation consistency
strength throughout a document.
Our term translation consistency model can ex-
actly measure the strength of term translation con-
sistency in a document. Since the essential com-
ponent of our term translation consistency model
is the translation consistency strength of the source
term estimated under the topic distribution, we de-
scribe how to calculate it before introducing the
whole model.
With the bilingual term bank created from
training data, we first group each source term
and all its corresponding target terms into a 2-
tuple G?t
f
, Set(t
e
)?, where t
f
is the source term
and Set(t
e
) is the set of t
f
?s corresponding tar-
get terms. We maintain a K-dimension (K is
the number of topics) vector for each 2-tuple
G?t
f
, Set(t
e
)?. The k-th component measures the
translation consistency strength cons(t
f
, k) of the
source term t
f
given the topic k.
We calculate cons(t
f
, k) for each
G?t
f
, Set(t
e
)? with counts from training data as
follows:
cons(t
f
, k) =
M
?
m=1
N
m
?
n=1
(
q
mn
? p(k|m)
Q
k
)
2
(4)
Q
k
=
M
?
m=1
N
m
?
n=1
q
mn
? p(k|m) (5)
where M is the number of documents in which
the source term t
f
occurs, N
m
is the number of
unique corresponding term translations of t
f
in the
mth document, q
mn
is the frequency of the nth
translation of t
f
in the mth document, p(k|m) is
the conditional probability of the mth document
over topic k, and Q
k
is the normalization factor.
All translations of t
f
are from Set(t
e
). We adapt
Itagaki et al. (2007)?s translation consistency met-
ric for terms to our topic-based translation consis-
tency measure in the Eq. (4). This equation cal-
culates the translation consistency strength of the
source term t
f
given the topic k according to the
distribution of t
f
?s translations in each document
549
where they occur. According to Eq. (4), the trans-
lation consistency strength is a score between 0
and 1. If a source term only occurs in a document
and all its translations are the same, the translation
consistency strength of this term is 1.
We reorganize our bilingual term bank into a
list of 2-tuples G?t
f
, Set(t
e
)?s, each of which is
associated with a K-dimension vector storing the
topic-conditioned translation consistency strength
calculated in the Eq. (4). When translating sen-
tences of document D, we first get the topic dis-
tribution of D via LDA tool. Given a sentence
which contains T terms {t
f
i
}
T
1
in D, our term
translation consistency model TermCons can be
denoted as
TermCons =
T
?
i=1
exp(S
c
(t
f
i
|D)) (6)
where the strength of translation consistency for
t
f
i
given the document D is formulated as fol-
lows:
S
c
(t
f
i
|D) = log(
K
?
k=1
cons(t
f
i
, k) ? p(k|D)) (7)
During decoding, whenever a hypothesis just
translates a source term t
f
i
into t
e
, we check
whether the translation t
e
can be found in Set(t
e
)
of t
f
i
from the reorganized bilingual term bank. If
it can be found, we calculate the strength of trans-
lation consistency for t
f
i
given the document D
according to Eq. (7) and take it as a soft con-
straint. If the S
c
(t
f
i
|D) of t
f
i
is high, the decoder
should translate t
f
i
into the extracted correspond-
ing target terms. Otherwise, the decoder will se-
lect translations from outside of Set(t
e
) for t
f
i
. In
doing so, we encourage terms to be translated in
a topic-dependent consistency pattern in the test
data similar to that in the training data so that we
can control the translation consistency of terms in
the test data.
The term translation consistency model is also
integrated into the log-linear model of SMT as a
feature. Through the feature, we can enable the
decoder to translate terms with a high translation
consistency in a document into corresponding tar-
get terms from our bilingual term bank rather than
other translations in a consistent fashion.
4.3 Term Bracketing Model
The term translation disambiguation model and
consistency model concern the term translation ac-
curacy with domain information. We further pro-
pose a term bracketing model to guarantee the in-
tegrality of term translation. Xiong et al. (2009)
proposed a syntax-driven bracketing model for
phrase-based translation, which predicts whether
a phrase is bracketable or not using rich syntac-
tic constraints. If a source phrase remains con-
tiguous after translation, they refer to this type of
phrase as bracketable phrase, otherwise unbrack-
etable phrase. For multi-word terms, it is also
desirable to be bracketable since a source term
should be translated as a whole unit and its trans-
lation should be contiguous.
In this paper, we adapt Xiong et al. (2009)?s
bracketing approach to term translation and build
a classifier to measure the probability that a source
term should be translated in a bracketable man-
ner. For all source parts of the extracted bilingual
term bank, we find their target counterparts in the
word-aligned training data. If the corresponding
target counterpart remains contiguous, we take the
source term as a bracketable instance, otherwise
an unbracketable instance. With these bracketable
and unbracketable instances, we train a maximum
entropy binary classifier to predict bracketable (b)
probability of a given source term t
f
within par-
ticular contexts c(t
f
). The binary classifier is for-
mulated as follows:
P
b
(b|c(t
f
)) =
exp(
?
j
?
j
h
j
(b, c(t
f
)))
?
b
?
exp(
?
j
?
j
h
j
(b
?
, c(t
f
)))
(8)
where h
j
? {0, 1} is a binary feature function and
?
j
is the weight of h
j
. We use the following fea-
tures: 1) the word sequence of the source term, 2)
the first word of the source term, 3) the last word
of the source term, 4) the preceding word of the
first word of the source term, 5) the succeeding
word of the last word of the source term, and 6)
the number of words in the source term.
Given a source sentence which contains T terms
{t
f
i
}
T
1
, our term bracketing model TermBrack
can be denoted as
TermBrack =
T
?
i=1
P
b
(b|c(t
f
i
)) (9)
Whenever a hypothesis just covers a source term
t
f
i
, we calculate the bracketable probability of t
f
i
according to Eq. (8).
The term bracketing model is integrated into the
log-linear model of SMT as a feature. Through the
feature, we want the decoder to translate source
terms with a high bracketable probability as a
whole unit.
550
Source Target D M
F?angy`u X`?t?ong defence mechanisms
F?angy`u X`?t?ong defence systems
F?angy`u X`?t?ong defense programmes 470 56
F?angy`u X`?t?ong prevention systems
... ...
Zh`anlu`e D?aod`an F?angy`u X`?t?ong strategic missile defense system 7 0
Table 1: Examples of bilingual terms extracted from the training data. ?D? means the total number of
documents in which the corresponding source term occurs and ?M? denotes the number of documents in
which the corresponding source term is translated into different target terms. The source side is Chinese
Pinyin. To save space, we do not list all the 23 different translations of the source term ?F?angy`u X`?t?ong?.
5 Experiments
In this section, we conducted experiments to an-
swer the following three questions.
1. Are our term translation disambiguation,
consistency and bracketing models able to
improve translation quality in BLEU?
2. Does the combination of the three models
provide further improvements?
3. To what extent do the proposed models affect
the translations of test sets?
5.1 Setup
Our training data consist of 4.28M sentence pairs
extracted from LDC
1
data with document bound-
aries explicitly provided. The bilingual training
data contain 67,752 documents, 124.8M Chinese
words and 140.3M English words. We chose
NIST MT05 as the MERT (Och, 2003) tuning set,
NIST MT06 as the development test set, and NIST
MT08 as the final test set. The numbers of docu-
ments/sentences in NIST MT05, MT06 and MT08
are 100/1082, 79/1664 and 109/1357 respectively.
The word alignments were obtained by running
GIZA++ (Och and Ney, 2003) on the corpora in
both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al., 2003). We
adopted SRI Language Modeling Toolkit (Stol-
cke and others, 2002) to train a 4-gram language
model with modified Kneser-Ney smoothing on
the Xinhua portion of the English Gigaword cor-
pus. For the topic model, we used the open source
1
The corpora include LDC2003E07, LDC2003E14,
LDC2004T07, LDC2004E12, LDC2005E83, LDC2005T06,
LDC2005T10, LDC2006E24, LDC2006E34, LDC2006E85,
LDC2006E92, LDC2007E87, LDC2007E101,
LDC2008E40, LDC2008E56, LDC2009E16 and
LDC2009E95.
LDA tool GibbsLDA++
2
with the default setting
for training and inference. We performed 100 it-
erations of the L-BFGS algorithm implemented in
the MaxEnt toolkit
3
with both Gaussian prior and
event cutoff set to 1 to train the term bracketing
prediction model (Section 4.3).
We performed part-of-speech tagging for mono-
lingual term extraction (C-value/NC-vaule method
in Section 3) of the source and target languages
with the Stanford NLP toolkit
4
. The bilingual term
bank was extracted based on the following param-
eter settings of term extraction methods. Empiri-
cally, we set the maximum length of a term to 6
words
5
. For both the C-value/NC-value and LLR-
based extraction methods, we set the context win-
dow size to 5 words, which is a widely-used set-
ting in previous work. And we set C-value/NC-
value score threshold to 0 and LLR score threshold
to 10 according to the training corpora.
We used the case-insensitive 4-gram BLEU
6
as
our evaluation metric. In order to alleviate the im-
pact of the instability of MERT (Och, 2003), we
ran it three times for all our experiments and pre-
sented the average BLEU scores on the three runs
following the suggestion by Clark et al. (2011).
We used an in-house hierarchical phrase-based
decoder to verify our proposed models. Although
the decoder translates a document in a sentence-
by-sentence fashion, it incorporates document-
informed information for sentence translation via
the proposed term translation models trained on
documents.
2
http://sourceforge.net/projects/gibbslda/
3
http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html
4
http://nlp.stanford.edu/software/tagger.shtml
5
We determine the maximum length of a term by testing
{5, 6, 7, 8} in our preliminary experiments. We find that
length 6 produces a slightly better performance than other
values.
6
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
551
Zh?y?u W?iyu?nhu? Ch?ngyu?n C?i  K? C?nji? W?iyu?nhu? Sh?ny? ?
Only members of the commission shall take part  in the commission deliberations .
?
He these proposals
T? Ji?ng Zh?xi? Ji?ny? Ji?o Y?u Y? G? B?zh?ngj? W?iyu?nhu? Sh?ny?
submit for approval to a committee of ministers .
(a)
(b)
Figure 1: An example of unbracketable source term in the training data. In (a), ?W?eiyu?anhu`? Sh?eny`?? is
bracketable while in (b) it is unbracketable. The solid lines connect bilingual phrases. The source side is
Chinese Pinyin.
5.2 Bilingual Term Bank
Before reporting the results of the proposed mod-
els, we provide some statistics of the bilingual
term bank extracted from the training data.
According to our statistics, about 1.29M bilin-
gual terms are extracted from the training data.
65.07% of the sentence pairs contain bilingual
terms in the training data. And on average, a
source term has about 1.70 different translations.
These statistics indicate that terms are frequently
used in real-world data and that a source term can
be translated into different target terms.
We also present some examples of bilingual
terms extracted from the training data in Table 1.
Accordingly, we show the total number of doc-
uments in which the corresponding source term
occurs and the number of documents in which
the corresponding source term is translated into
different target terms. The source term ?F?angy`u
X`?t?ong? has 23 different translations in total. They
are distributed in 470 documents in the training
data. In 414 documents, ?F?angy`u X`?t?ong? has
only one single translation. However, in the other
56 documents it has different translations. This
indicates that ?F?angy`u X`?t?ong? is not consistently
translated in these 56 documents. Different from
this, the source term ?Zh`anlu`e D?aod`an F?angy`u
X`?t?ong? only has one translation. And it is trans-
lated consistently in all 7 documents where it oc-
curs. In fact, according to our statistics, there are
about 5.19% source terms whose translations are
not consistent even in the same document.
These examples and statistics suggest 1) that
source terms have domain-specific translations
and 2) that source terms are not necessarily trans-
lated in a consistent manner even in the same doc-
ument. These are exactly the reasons why we pro-
pose the term translation disambiguation and con-
sistency model based on domain information rep-
resented by topic distributions.
Actually, 36.13% of the source terms are not
necessarily translated into target strings as a whole
unit. We show an example of such terms in Fig-
ure 1. In Figure 1-(a), ?W?eiyu?anhu`? Sh?eny`?? is a
term, and is translated into ?commission deliber-
ations? as a whole unit. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is bracketable in this sentence. How-
ever, in Figure 1-(b), ?W?eiyu?anhu`?? and ?Sh?eny`??
are translated separately. Therefore ?W?eiyu?anhu`?
Sh?eny`?? is an unbracketable term in this sentence.
This is the reason why we propose a bracketing
model to predict whether a source term is brack-
etable or not.
5.3 Effect of the Proposed Models
In this section, we validate the effectiveness of the
proposed term translation disambiguation model,
consistency model and bracketing model respec-
tively. In addition to the traditional hiero (Chi-
ang, 2007) system, we also compare against the
?CountFeat? method in Ren et al. (2009) who use
a binary feature to indicate whether a bilingual
phrase contains a term pair. Although Ren et al.
(2009)?s experiments are conducted in a phrase-
based system, the idea can be easily applied to a
hierarchical phrase-based system.
We carried out experiments to investigate the ef-
fect of the term translation disambiguation model
(Dis-Model) and report the results in Table 2. In
order to find the topic number setting with which
our model has the best performance, we ran exper-
iments using the MT06 as the development test set.
From Table 2, we observe that the Dis-Model ob-
tains steady improvements over the baseline and
?CountFeat? method with the topic number K
552
Models MT06 MT08 Avg
Baseline 32.43 24.14 28.29
CountFeat 32.77 24.29 28.53
Dis-Model
K = 50 32.94* 24.53 28.74
K = 100 33.10* 24.57 28.84
K = 150 33.16* 24.67* 28.92
K = 200 33.08* 24.55 28.81
Cons-Model
K = 50 33.09* 24.59 28.84
K = 100 33.13* 24.74* 28.94
K = 150 33.32*+ 24.84*+ 29.08
K = 200 33.02* 24.73* 28.88
Brack-Model 33.09* 24.66* 28.88
Combined-Model 33.59*+ 24.99*+ 29.29
Table 2: BLEU-4 scores (%) of the term translation disambiguation model (Dis-Model), the term transla-
tion consistency model (Cons-Model), the term bracketing model (Brack-Model), and the combination of
the three models, on the development test set MT06 and the final test set MT08. K ? {50, 100, 150, 200}
which is the number of topics for the Dis-Model and the Cons-Model. ?Combined-Model? is the combi-
nation of the three single modes with topic number 150 for the Dis-Model and the Cons-Model. ?Base-
line? is the traditional hierarchical phrase-based system. ?CountFeat? is the method that adds a counting
feature to reward translation hypotheses containing bilingual term pairs. The ?*? and ?+? denote that the
results are significantly (Clark et al., 2011) better than those of the baseline system and the CountFeat
method respectively (p<0.01).
ranging from 50 to 150. However, when we set K
to 200, the performance drops. The highest BLEU
scores 33.16 and 24.67 are obtained at the topic
setting K = 150. In fact, our Dis-Model gains
higher performance in BLEU than both the tradi-
tional hiero baseline and the ?CountFeat? method
with all topic settings. The ?CountFeat? method
rewards translation hypotheses containing bilin-
gual term pairs. However it does not explore any
domain information. Our Dis-Model incorporates
domain information to conduct translation disam-
biguation and achieves higher performance. When
the topic number is set to 150, we gain the high-
est BLEU score, which is higher than that of the
baseline by 0.73 and 0.53 BLEU points on MT06
and MT08, respectively. The final gain over the
baseline is on average 0.63 BLEU points.
We conducted the second group of experiments
to study whether the term translation consistency
model (Cons-Model) is able to improve the per-
formance in BLEU, as well as to investigate the
impact of different topic numbers on the Cons-
Model. Results are shown in Table 2, from which
we observe the similar phenomena to what we
have found in the Dis-Model. Our Cons-Model
gains higher BLEU scores than the baseline sys-
tem and the ?CountFeat? method with all topic
settings. Setting topic number to 150 achieves the
highest BLEU score, which is higher than base-
line by 0.89 BLEU points and 0.70 BLEU points
on MT06 and MT08 respectively, and on average
0.79 BLEU points.
We also conducted experiments to verify the ef-
fectiveness of the term bracketing model (Brack-
Model), which conducts bracketing prediction for
source terms. Results in Table 2 show that
our Brack-Model gains higher BLEU scores than
those of the baseline system and the ?CountFeat?
method. The final gain of Brack-Model over the
baseline is 0.66 BLEU points and 0.52 points on
MT06 and MT08 respectively, and on average
0.59 BLEU points.
5.4 Combination of the Three Models
As shown in the previous subsection, the term
translation disambiguation model, consistency
model and bracketing model substantially outper-
form the baseline. Now, we investigate whether
using these three models simultaneously can lead
to further improvements. The last row in Table 2
shows that the combination of the three models
(Combined-Model) achieves higher BLEU score
than all single models, when we set the topic num-
ber to 150 for the term translation disambigua-
tion model and consistency model. The final gain
553
Models MT06 MT08
Best-Dis-Model 30.89 30.14
Best-Cons-Model 38.04 36.70
Brack-Model 60.46 55.78
Combined-Model 54.39 50.85
Table 3: Percentage (%) of 1-best translations
which are generated by the Combined-Model and
the three single models with best settings on the
development test set MT06 and the final test set
MT08. The topic number is 150 for Best-Dis-
Model and Best-Cons-Model.
of the Combined-Model over the baseline is 1.16
BLEU points and 0.85 points on MT06 and MT08
respectively, and on average 1.00 BLEU points.
5.5 Analysis
In this section, we investigate to what extent the
proposed models affect the translations of test sets.
In Table 3, we show the percentage of 1-best trans-
lations affected by the Combined-Model and the
three single models with best settings on test sets
MT06 and MT08. For single models, if the corre-
sponding feature (disambiguation, consistency or
bracketing) is activated in the 1-best derivation,
the corresponding model has impact on the 1-best
translation. For the Combined-Model, if any of
the corresponding features is activated in the 1-
best derivation, the Combined-Model affects the
1-best translation.
From Table 3, we can see that 1-best transla-
tions of source sentences affected by any of the
proposed models account for a high proportion
(30%?60%) on both MT06 and MT08. This in-
dicates that all proposed models play an important
role in the translation of both test sets. Among
the three proposed models, the Brack-Model is the
one that affects the largest number of 1-best trans-
lations in both test sets. And the percentage is
60.46% and 55.78% on MT06 and MT08 respec-
tively. The Brack-Model only considers source
terms during decoding, while the Dis-Model and
Cons-Model need to match both source and target
terms. The Brack-Model is more likely to be acti-
vated. Hence the percentage of 1-best translations
affected by this model is higher than those of the
other two models. Since we only investigate the
1-best translations generated by the Combined-
Model and single models, the translations gener-
ated by some single models (e.g., Brack-Model)
may not be generated by the Combined-Model.
Therefore it is hard to say that the numbers of 1-
best translations affected by the Combined-Model
must be greater than those of single models.
6 Conclusion and Future Work
We have studied the three issues of term trans-
lation and proposed three different term trans-
lation models for document-informed SMT. The
term translation disambiguation model enables
the decoder to favor the most suitable domain-
specific translations with domain information for
source terms. The term translation consistency
model encourages the decoder to translate source
terms with a high domain translation consistency
strength into target terms rather than other new
strings. Finally, the term bracketing model re-
wards hypotheses that translate bracketable terms
into continuous target strings as a whole unit.
We integrate the three models into a hierarchical
phrase-based SMT system
7
and evaluate their ef-
fectiveness on the NIST Chinese-English transla-
tion task with large-scale training data. Experi-
ment results show that all three models achieve
significant improvements over the baseline. Ad-
ditionally, combining the three models achieves a
further improvement. For future work, we would
like to evaluate our models on term translation
across a range of different domains.
Acknowledgments
This work was supported by National Key Tech-
nology R&D Program (No. 2012BAH39B03) and
CAS Action Plan for the Development of Western
China (No. KGZD-EW-501). Deyi Xiong?s work
was supported by Natural Science Foundation of
Jiangsu Province (Grant No. BK20140355). Qun
Liu?s work was partially supported by Science
Foundation Ireland (Grant No. 07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the anonymous reviewers for their
thorough reviewing and valuable suggestions. The
corresponding author of this paper, according to
the meaning given to this role by University of
Chinese Academy of Sciences and Soochow Uni-
versity, is Deyi Xiong.
7
Our models are not limited to hierarchical phrase-based
SMT. They can be easily applied to other SMT formalisms,
such as phrase- and syntax-based SMT.
554
References
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Houda Bouamor, Aur?elien Max, and Anne Vilnat.
2012. Validation of sub-sentential paraphrases ac-
quired from parallel monolingual corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 716?725. Association for Computa-
tional Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: short papers-Volume
2, pages 176?181.
B?eatrice Daille. 1996. Study and implementation of
combined techniques for automatic extraction of ter-
minology. Journal of The balancing act: Combin-
ing symbolic and statistical approaches to language,
1:49?66.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers-Volume 2, pages 115?119.
Association for Computational Linguistics.
Xiaorong Fan, Nobuyuki Shimizu, and Hiroshi Nak-
agawa. 2009. Automatic extraction of bilin-
gual terms from a chinese-japanese parallel corpus.
In Proceedings of the 3rd International Universal
Communication Symposium, pages 41?45. ACM.
Katerina T Frantzi, Sophia Ananiadou, and Junichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Re-
search and Advanced Technology for Digital Li-
braries, pages 585?604. Springer.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 909?919.
Liane Guillou. 2013. Analysing lexical consistency
in translation. In Proceedings of the Workshop on
Discourse in Machine Translation, pages 10?18.
Christian Hardmeier, Joakim Nivre, and J?org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014a. Dynamic topic adaptation for
phrase-based mt. In Proceedings of the 14th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, Gothenburg, Sweden.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014b.
Dynamic topic adaptation for smt using distribu-
tional profiles. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, pages 445?
456, Baltimore, Maryland, USA, June. Association
for Computational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of 16th Nordic Conference of Computational Lin-
guistics Nodalida, pages 97?104.
Masaki Itagaki and Takako Aikawa. 2008. Post-mt
term swapper: Supplementing a statistical machine
translation system with a user dictionary. In LREC.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology trans-
lation consistency with statistical method. Proceed-
ings of MT summit XI, pages 269?274.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54.
Els Lefever, Lieve Macken, and Veronique Hoste.
2009. Language-independent bilingual terminology
extraction from a multilingual parallel corpus. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 496?504.
Magnus Merkel and Jody Foo. 2007. Terminology
extraction and term ranking for standardizing term
banks. In Proceedings of 16th Nordic Conference
of Computational Linguistics Nodalida, pages 349?
354.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167.
Scott SL Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic extraction of chi-
nese multiword expressions with a statistical tool.
In Workshop on Multi-word-expressions in a Mul-
tilingual Context held in conjunction with the 11th
EACL, Trento, Italy, pages 17?24.
555
Pinis and Skadins. 2012. Mt adaptation for under-
resourced domains?what works and what not. In
Human Language Technologies?The Baltic Perspec-
tive: Proceedings of the Fifth International Confer-
ence Baltic HLT 2012, volume 247, page 176. IOS
Press.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47?54.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 459?468.
J?org Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the 2010
Workshop on Domain Adaptation for Natural Lan-
guage Processing, pages 8?15.
Ferhan Ture, Douglas W Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426. Association for Computational Lin-
guistics.
Muriel Vasconcellos, Brian Avey, Claudia Gdaniec,
Laurie Gerber, Marjorie Le?on, and Teruko Mita-
mura. 2001. Terminology and machine translation.
Handbook of Terminology Management, 2:697?723.
Thuy Vu, Ai Ti Aw, and Min Zhang. 2008. Term ex-
traction through unithood and termhood unification.
In Proceedings of the third international joint con-
ference on natural language processing.
Billy Wong and Chunyu Kit. 2012. Extending ma-
chine translation evaluation metrics with lexical co-
hesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for hi-
erarchical phrase-based translation. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Long Papers-Volume 1,
pages 750?758.
Deyi Xiong and Min Zhang. 2013. A topic-based
coherence model for statistical machine translation.
In Proceedings of the Twenty-Seventh AAAI Confer-
ence on Artificial Intelligence (AAAI-13), Bellevue,
Washington, USA, July.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan L?u,
and Qun Liu. 2013a. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, pages 2183?2189. AAAI
Press.
Deyi Xiong, Yang Ding, Min Zhang, and Chew Lim
Tan. 2013b. Lexical chain based cohesion mod-
els for document-level statistical machine transla-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1563??1573.
Bing Zhao and Eric P Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 969?976.
556
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 185?189,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Active Learning for Post-Editing Based Incrementally Retrained MT
Aswarth Dara Josef van Genabith Qun Liu John Judge Antonio Toral
School of Computing
Dublin City University
Dublin, Ireland
{adara,josef,qliu,jjudge,atoral}@computing.dcu.ie
Abstract
Machine translation, in particular statis-
tical machine translation (SMT), is mak-
ing big inroads into the localisation and
translation industry. In typical work-
flows (S)MT output is checked and (where
required) manually post-edited by hu-
man translators. Recently, a significant
amount of research has concentrated on
capturing human post-editing outputs as
early as possible to incrementally up-
date/modify SMT models to avoid repeat
mistakes. Typically in these approaches,
MT and post-edits happen sequentially
and chronologically, following the way
unseen data (the translation job) is pre-
sented. In this paper, we add to the ex-
isting literature addressing the question
whether and if so, to what extent, this
process can be improved upon by Active
Learning, where input is not presented
chronologically but dynamically selected
according to criteria that maximise perfor-
mance with respect to (whatever is) the re-
maining data. We explore novel (source
side-only) selection criteria and show per-
formance increases of 0.67-2.65 points
TER absolute on average on typical indus-
try data sets compared to sequential PE-
based incrementally retrained SMT.
1 Introduction and Related Research
Machine Translation (MT) has evolved dramati-
cally over the last two decades, especially since
the appearance of statistical approaches (Brown et
al., 1993). In fact, MT is nowadays succesfully
used in the localisation and translation industry,
as for many relevant domains such as technical
documentation, post-editing (PE) of MT output by
human translators (compared to human translation
from scratch) results in notable productivity gains,
as a number of industry studies have shown con-
vincingly, e.g. (Plitt and Masselot, 2010). Fur-
thermore, incremental retraining and update tech-
niques (Bertoldi et al., 2013; Levenberg et al.,
2010; Mathur et al., 2013; Simard and Foster,
2013) allow these PEs to be fed back into the MT
model, resulting in an MT system that is contin-
uously updated to perform better on forthcoming
sentences, which should lead to a further increase
in productivity.
Typically, post-editors are presented with MT
output units (sentences) in the order in which input
sentences appear one after the other in the trans-
lation job. Because of this, incremental MT re-
training and update models based on PE outputs
also proceed in the same chronological order de-
termined by the input data. This may be sub-
optimal. In this paper we study the application of
Active Learning (AL) to the scenario of PE MT
and subsequent PE-based incremental retraining.
AL selects data (here translation inputs and their
MT outputs for PE) according to criteria that max-
imise performance with respect to the remaining
data and may diverge from processing data items
in chronological order. This may allow incremen-
tally PE-based retrained MT to (i) improve more
rapidly than chronologically PE-based retrained
MT and (ii) result in overall productivity gains.
The main contributions of this paper include:
? Previous work (Haffari et al., 2009; Blood-
good and Callison-Burch, 2010) shows that,
given a (static) training set, AL can im-
prove the quality of MT. By contrast, here
we show that AL-based data selection for hu-
man PE improves incrementally and dynami-
cally retrained MT, reducing overall PE time
of translation jobs in the localisation industry
application scenarios.
? We propose novel selection criteria for AL-
based PE: we adapt cross-entropy difference
(Moore and Lewis, 2010; Axelrod et al.,
2011), originally used for domain adaptation,
and propose an extension to cross entropy
difference with a vocabulary saturation filter
(Lewis and Eetemadi, 2013).
? While much of previous work concentrates
on research datasets (e.g. Europarl, News
Commentary), we use industry data (techni-
185
cal manuals). (Bertoldi et al., 2013) shows
that the repetition rate of news is consider-
ably lower than that of technical documenta-
tion, which impacts on the results obtained
with incremental retraining.
? Unlike in previous research, our AL-based
selection criteria take into account only the
source side of the data. This supports se-
lection before translation, keeping costs to a
minimum, a priority in commercial PE MT
applications.
? Our experiments show that AL-based selec-
tion works for PE-based incrementally re-
trained MT with overall performance gains
around 0.67 to 2.65 TER absolute on average.
AL has been successfully applied to many tasks
in natural language processing, including pars-
ing (Tang et al., 2002), named entity recogni-
tion (Miller et al., 2004), to mention just a few. See
(Olsson, 2009) for a comprehensie overview of
the application of AL to natural language process-
ing. (Haffari et al., 2009; Bloodgood and Callison-
Burch, 2010) apply AL to MT where the aim is to
build an optimal MT model from a given, static
dataset. To the best of our knowledge, the most
relevant previous research is (Gonz?alez-Rubio et
al., 2012), which applies AL to interactive MT. In
addition to differences in the AL selection criteria
and data sets, our goals are fundamentally differ-
ent: while the previous work aimed at reducing
human effort in interactive MT, we aim at reduc-
ing the overall PE time in PE-based incremental
MT update applications in the localisation indus-
try.
In our experiments reported in Section 3 below
we want to explore a space consisting of a con-
siderable number of selection strategies and incre-
mental retraining batch sizes. In order to be able to
do this, we use the target side of our industry trans-
lation memory data to approximate human PE out-
put and automatic TER (Snover et al., 2006) scores
as a proxy for human PE times (O?Brien, 2011).
2 Methodology
Given a translation job, our goal is to reduce the
overall PE time. At each stage, we select sen-
tences that are given to the post editor in such a
way that uncertain sentences (with respect to the
MT system at hand)
1
are post-edited first. We then
translate the n top-ranked sentences using the MT
system and use the human PEs of the MT outputs
to retrain the system. Algorithm 1 describes our
1
The uncertainty of a sentence with respect to the model
can be measured according to different criteria, e.g. percent-
age of unknown n-grams, perplexity etc.
method, where s and t stand for source and target,
respectively.
Algorithm 1 Sentence Selection Algorithm
Input:
L?? Initial training data
M?? Initial MT model
for C ? (Random,Sequential,Ngram,CED,CEDN) do
U?? Translation job
while size(U) > 0 do
U1.s?? SelectTopSentences(C, U.s)
U1
1
.t?? Translate(M, U1.s)
U1.t?? PostEdit(U1
1
.t)
U?? U - U1
L?? L ? U1
M?? TrainModel (L)
end while
end for
We use two baselines, i.e. random and sequen-
tial. In the random baseline, the batch of sentences
at each iteration are selected randomly. In the se-
quential baseline, the batches of sentences follow
the same order as the data.
Aside from the Random and Sequential base-
lines we use the following selection criteria:
? N-gram Overlap. An SMT system will en-
counter problems translating sentences con-
taining n-grams not seen in the training data.
Thus, PEs of sentences with high number of
unseen n-grams are considered to be more in-
formative for updating the current MT sys-
tem. However, for the MT system to trans-
late unseen n-grams accurately, they need to
be seen a minimum number V times.
2
We
use an n-gram overlap function similar to
the one described in (Gonz?alez-Rubio et al.,
2012) given in Equation 1 where N(T
(i)
) and
N(S
(i)
) return i-grams in training data and
the sentence S, respectively.
unseen(S) =
n
?
i=1
{|N(T
(i)
) ?N(S
(i)
)|>V }
n
?
i=1
N(S
(i)
)
(1)
? Cross Entropy Difference (CED). This met-
ric is originally used in data selection (Moore
and Lewis, 2010; Axelrod et al., 2011).
Given an in-domain corpus I and a general
corpus O, language models are built from
both,
3
and each sentence in O is scored ac-
cording to the entropy H difference (Equation
2
Following (Gonz?alez-Rubio et al., 2012) we use V =
10.
3
In order to make the LMs comparable they have the same
size. As commonly the size of O is considerable bigger than
I, this means that the LM for O is built from a subset of the
same size as I.
186
2). The lower the score given to a sentence,
the more useful it is to train a system for the
specific domain I .
score(s) = H
I
(s)?H
O
(s) (2)
In our AL scenario, we have the current train-
ing corpus L and an untranslated corpus U.
CED is applied to select sentences from U
that are (i) different from L (as we would like
to add sentences that add new information to
the model) and (ii) similar to the overall cor-
pus U (as we would like to add sentences that
are common in the untranslated data). Hence
we apply CED and select sentences from U
that have high entropy with respect to L and
low entropy with respect to U (Equation 3).
score(s) = H
U
(s)?H
L
(s) (3)
? CED + n-gram (CEDN). This is an exten-
sion of the CED criterion inspired by the con-
cept of the vocabulary saturation filter (Lewis
and Eetemadi, 2013). CED may select many
very similar sentences, and thus it may be the
case that some of them are redundant. By
post-processing the selection made by CED
with vocabulary saturation we aim to spot
and remove redudant sentences. This works
in two steps. In the first step, all the sentences
from U are scored using the CED metric. In
the second step, we down-rank sentences that
are considered redundant. The top sentence is
selected, and its n-grams are stored in local-
ngrams. For the remaining sentences, one by
one, their n-grams are matched against local-
ngrams. If the intersection between them is
lower than a predefined threshold, the current
sentence is added and localngrams is updated
with the n-grams from the current sentence.
Otherwise the sentence is down-ranked to the
bottom. In our experiments, the value n = 1
produces best results.
3 Experiments and Results
We use technical documentation data taken from
Symantec translation memories for the English?
French (EN?FR) and English?German (EN?DE)
language pairs (both directions) for our experi-
ments. The statistics of the data (training and in-
cremental splits) are shown in Table 1.
All the systems are trained using the
Moses (Koehn et al., 2007) phrase-based sta-
tistical MT system, with IRSTLM (Federico et
al., 2008) for language modelling (n-grams up
to order five) and with the alignment heuristic
grow-diag-final-and.
For the experiments, we considered two settings
for each language pair in each direction. In the
first setting, the initial MT system is trained using
the training set (39,679 and 54,907 sentence pairs
for EN?FR and EN?DE, respectively). Then, a
batch of 500 source sentences is selected from the
incremental dataset according to each of the se-
lection criteria, and translations are obtained with
the initial MT system. These translations are post-
edited and the corrected translations are added to
the training data.
4
We then train a new MT sys-
tem using the updated training data (initial training
data plus PEs of the first batch of sentences). The
updated model will be used to translate the next
batch. The same process is repeated until the in-
cremental dataset is finished (16 and 20 iterations
for English?French and English?German, respec-
tively). For each batch we compute the TER score
between the MT output and the refererence trans-
lations for the sentences of that batch. We then
compute the average TER score for all the batches.
These average scores, for each selection criterion,
are reported in Table 2.
In the second setting, instead of using the whole
training data, we used a subset of (randomly se-
lected) 5,000 sentence pairs for training the initial
MT system and a subset of 20,000 sentences from
the remaining data as the incremental dataset.
Here we take batches of 1,000 sentences (thus 20
batches). The results are shown in Table 3.
The first setting aims to reflect the situation
where a translation job is to be completed for a do-
main for which we have a considerable amount of
data available. Conversely, the second setting re-
flects the situation where a translation job is to be
carried out for a domain with little (if any) avail-
able data.
Dir Random Seq. Ngram CED CEDN
EN?FR 29.64 29.81 28.97 29.25 29.05
FR?EN 27.08 27.04 26.15 26.63 26.39
EN?DE 24.00 24.08 22.34 22.60 22.32
DE?EN 19.36 19.34 17.70 17.97 17.48
Table 2: TER average scores for Setting 1
Dir Random Seq. Ngram CED CEDN
EN?FR 36.23 36.26 35.20 35.48 35.17
FR?EN 33.26 33.34 32.26 32.69 32.17
EN?DE 32.23 32.19 30.58 31.96 29.98
DE?EN 27.24 27.29 26.10 26.73 24.94
Table 3: TER average scores for Setting 2
For Setting 1 (Table 2), the best result is ob-
tained by the CEDN criterion for two out of the
four directions. For EN?FR, n-gram overlap
4
As this study simulates the post-editing, we use the ref-
erences of the translated segments instead of the PEs.
187
Type
EN?FR EN?DE
Sentences Avg. EN SL Avg. FR SL Sentences Avg. EN SL Avg. DE SL
Training 39,679 13.55 15.28 54,907 12.66 12.90
Incremental 8,000 13.74 15.50 10,000 12.38 12.61
Table 1: Data Statistics for English?French and English?German Symantec Translation Memory Data.
SL stands for sentence length, EN stands for English, FR stands for French and DE stands for German
performs slightly better than CEDN (0.08 points
lower) with a decrease of 0.67 and 0.84 points
when compared to the baselines (random and se-
quential, respectively). For FR?EN, n-gram
overlap results in a decrease of 0.93 and 0.89
points compared to the baselines. The decrease in
average TER score is higher for the EN?DE and
for DE?EN directions, i.e. 1.68 and 1.88 points
respectively for CEDN compared to the random
baseline.
In the scenario with limited data available be-
forehand (Table 3), CEDN is the best performing
criterion for all the language directions. For the
EN?FR and FR?EN language pairs, CEDN results
in a decrease of 1.06 and 1.09 points compared to
the random baseline. Again, the decrease is higher
for the EN?DE and DE?EN language pairs, i.e.
2.25 and 2.30 absolute points on average.
Figure 1 shows the TER scores per iteration for
each of the criteria, for the scenario DE?EN Set-
ting 2 (the trends are similar for the other scenar-
ios). The two baselines exhibit slight improve-
ment over the iterations, both starting at around
.35 TER points and finishing at around .25 points.
Conversely, all the three criteria start at very high
scores (in the range [.5,.6]) and then improve con-
siderably to arrive at scores below .1 for the last
iterations. Compared to Ngram and CED, CEDN
reaches better scores earlier on, being the criterion
with the lowest score up to iteration 13.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7 RandomSeqNgramCEDCEDN
Iteration
TER
 sco
re
Figure 1: Results per iteration, DE?EN Setting 2
Figure 1 together with Tables 2 and 3 show
that AL for PE-based incremental MT retrain-
ing really works: all AL based methods (Ngram,
CED, CEDN) show strong improvements over
both baselines after the initial 8-9 iterations (Fig-
ure 1) and best performance on the complete incre-
mental data sets, resulting in a noticeable decrease
of the overall TER score (Tables 2 and 3). In six
out of eight scenarios, our novel metric CEDN ob-
tains the best result.
4 Conclusions and Future Work
This paper has presented an application of AL to
MT for dynamically selecting automatic transla-
tions of sentences for human PE, with the aim of
reducing overall PE time in a PE-based incremen-
tal MT retraining scenario in a typical industrial
localisation workflow that aims to capitalise on
human PE as early as possible to avoid repeat mis-
takes.
Our approach makes use of source side informa-
tion only, uses two novel selection criteria based
on cross entropy difference and is tested on indus-
trial data for two language pairs. Our best per-
forming criteria allow the incrementally retrained
MT systems to improve their performance earlier
and reduce the overall TER score by around one
and two absolute points for English?French and
English?German, respectively.
In order to be able to explore a space of selec-
tion criteria and batch sizes, our experiments sim-
ulate PE, in the sense that we use the target ref-
erence (instead of PEs) and approximate PE time
with TER. Given that TER correlates well with PE
time (O?Brien, 2011), we expect AL-based selec-
tion of sentences for human PE to lead to overall
reduction of PE time. In the future work, we plan
to do the experiments using PEs to retrain the sys-
tem and measuring PE time.
In this work, we have taken batches of sentences
(size 500 to 1,000) and do full retraining. As fu-
ture work, we plan to use fully incremental retrain-
ing and perform the selection on a sentence-by-
sentence basis (instead of taking batches).
Finally and importantly, a potential drawback of
our approach is that by dynamically selecting in-
dividual sentences for PE, the human post-editor
looses context, which they may use if processing
sentences sequentially. We will explore the trade
off between the context lost and the productivity
gain achieved, and ways of supplying context (e.g.
previous and following sentence) for real PE.
188
Acknowledgements
This work is supported by Science Foundation
Ireland (Grants 12/TIDA/I2438, 07/CE/I1142 and
12/CE/I2267) as part of the Centre for Next Gen-
eration Localisation (www.cngl.ie) at Dublin City
University. We would like to thank Symantec for
the provision of data sets used in our experiments.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42, Nice, France.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the trend: Large-scale cost-focused active
learning for statistical machine translation. In Jan
Hajic, Sandra Carberry, and Stephen Clark, editors,
ACL, pages 854?864. The Association for Computer
Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Comput. Linguist., 19(2):263?
311, June.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2012. Active learning for
interactive machine translation. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?12, pages 245?254, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In HLT-NAACL, pages 415?
423. The Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL 2007, pages 177?180, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models
for statistical machine translation. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ?10, pages 394?
402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
William Lewis and Sauleh Eetemadi. 2013. Dramati-
cally reducing training data size through vocabulary
saturation. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 281?291,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Prashant Mathur, Mauro Cettolo, and Marcello Fed-
erico. 2013. Online learning approaches in com-
puter assisted translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, ACL, pages 301?308, Sofia, Bulgaria.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings of HLT, pages
337?342.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Sharon O?Brien. 2011. Towards predicting
post-editing productivity. Machine Translation,
25(3):197?215, September.
Fredrik Olsson. 2009. A literature survey of active
machine learning in the context of natural language
processing. Technical Report T2009:06.
Mirko Plitt and Franc?ois Masselot. 2010. A productiv-
ity test of statistical machine translation post-editing
in a typical localisation context. Prague Bull. Math.
Linguistics, 93:7?16.
Michel Simard and George Foster. 2013. Pepr: Post-
edit propagation using phrase-based statistical ma-
chine translation. In Proceedings of the XIV Ma-
chine Translation Summit, pages 191?198, Nice,
France.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231, Cambridge,
MA.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 120?127, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
189
Discriminative Word Alignment by
Linear Modeling
Yang Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Qun Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Shouxun Lin?
Institute of Computing Technology
Chinese Academy of Sciences
Word alignment plays an important role in many NLP tasks as it indicates the correspondence
between words in a parallel text. Although widely used to align large bilingual corpora, gen-
erative models are hard to extend to incorporate arbitrary useful linguistic information. This
article presents a discriminative framework for word alignment based on a linear model. Within
this framework, all knowledge sources are treated as feature functions, which depend on a source
language sentence, a target language sentence, and the alignment between them. We describe a
number of features that could produce symmetric alignments. Our model is easy to extend and
can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art
alignment quality on three word alignment shared tasks for five language pairs with varying
divergence and richness of resources. We further show that our approach improves translation
performance for various statistical machine translation systems.
1. Introduction
Word alignment, which can be defined as an object for indicating the corresponding
words in a parallel text, was first introduced as an intermediate result of statistical
machine translation (Brown et al 1993).
Consider the following Chinese sentence and its English translation:
Published under Creative Commons Attribution-NonCommercial 3.0 Unported license
     
Zhongguo jianzhuye duiwaikaifang chengxian xin geju
The opening of China?s construction industry to the outside presents a new structure
? Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese
Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190,
China. E-mail: {yliu, liuqun, sxlin}@ict.ac.cn.
Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted for
publication: 21 February 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
The Chinese word Zhongguo is aligned to the English word China because they are
translations of one another. Similarly, the Chinese word xin is aligned to the English
word new. These connections are not necessarily one-to-one. For example, one Chinese
word jianzhuye corresponds to two English words construction industry. In addition, the
English words (e.g., opening ... to the outside) connected to a Chinese word (e.g., dui-
waikaifang) could be discontinuous. Figure 1 shows an alignment for this sentence pair.
The Chinese and English words are listed horizontally and vertically, respectively. They
are numbered to facilitate identification. The dark points indicate the correspondence
between the words in two languages. The goal of word alignment is to identify such
correspondences in a parallel text.
Word alignment plays an important role in many NLP tasks. In statistical machine
translation, word-aligned corpora serve as an excellent source for translation-related
knowledge. The estimation of translation model parameters usually relies heavily on
word-aligned corpora, not only for phrase-based and hierarchical phrase-based models
(Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for
syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al 2006; Liu, Liu,
and Lin 2006; Marcu et al 2006). Besides machine translation, many applications for
word-aligned corpora have been suggested, including machine-assisted translation,
Figure 1
Example of a word alignment between a Chinese?English sentence pair. The Chinese and
English words are listed horizontally and vertically, respectively. They are numbered to facilitate
identification. The dark points indicate the correspondences between the words in the two
languages.
304
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
translation assessment and critiquing tools, text generation, bilingual lexigraphy, and
word sense disambiguation.
Various methods have been proposed for finding word alignments between parallel
texts. Among them, generative alignment models (Brown et al 1993; Vogel and Ney
1996) have been widely used to produce word alignments for large bilingual corpora.
Describing the relationship of a bilingual sentence pair, a generative model treats word
alignment as a hidden process and maximizes the likelihood of a training corpus using
the expectation maximization (EM) algorithm. After the maximization process is com-
plete, the unknown model parameters are determined and the word alignments are set
to the maximum posterior predictions of the model.
However, one drawback of generative models is that they are hard to extend. Gen-
erative models usually impose strong independence assumptions between sub-models,
making it very difficult to incorporate arbitrary features explicitly. For example, when
considering whether to align two words, generative models cannot include information
about lexical and syntactic features such as part of speech and orthographic similarity in
an easy way. Such features would allow for more effective use of sparse data and result
in a model that is more robust in the presence of unseen words. Extending a generative
model requires that the interdependence of information sources be modeled explicitly,
which often makes the resulting system quite complex.
In this article, we introduce a discriminative framework for word alignment based
on the linear modeling approach. Within this framework, we treat all knowledge
sources as feature functions that depend on a source sentence, a target sentence,
and the alignment between them. Each feature function is associated with a feature
weight. The linear combination of features gives an overall score to each candidate
alignment. The best alignment is the one with the highest overall score. A linear
model not only allows for easy integration of new features, but also admits optimiz-
ing feature weights directly with respect to evaluation metrics. Experimental results
show that our approach improves both alignment quality and translation performance
significantly.
This article is organized as follows. Section 2 gives a formal description of our
model. We show how to train feature weights by taking evaluation metrics into account
and how to find the most probable alignment in an exponential search space efficiently.
Section 3 describes a number of features used in our experiments, focusing on the
features that produce symmetric alignments. In Section 4, we evaluate our model in
both alignment and translation tasks. Section 5 reviews previous work related to our
approach and the article closes with a conclusion in Section 6.
2. Approach
2.1 The Model
Given a source language sentence f = f1, . . . , fj, . . . , fJ and a target language sentence e =
e1, . . . , ei, . . . , eI, we define a link l = ( j, i) to exist if fj and ei are translations (or part of a
translation) of one another. Then, an alignment is defined as a subset of the Cartesian
product of the word positions:
a ? {( j, i) : j = 1, . . . , J; i = 1, . . . , I} (1)
305
Computational Linguistics Volume 36, Number 3
We propose a linear alignment model:
score(f, e, a) =
M
?
m=1
?mhm(f, e, a) (2)
where hm(f, e, a) is a feature function and ?m is its associated feature weight. The linear
combination of features gives an overall score score(f, e, a) to each candidate alignment
a for a given sentence pair ?f, e?.
2.2 Training
To achieve good alignment quality, it is essential to find a good set of feature weights
?M1 . Before discussing how to train ?
M
1 , we first describe two evaluation metrics that
measure alignment quality, because we will optimize ?M1 with respect to them directly.
2.2.1 Evaluation Metrics. The first metric is alignment error rate (AER), proposed by
Och and Ney (2003). AER has been used as official evaluation criterion in most word
alignment shared tasks. Och and Ney define two kinds of links in hand-aligned align-
ments: sure links for alignments that are unambiguous and possible links for ambiguous
alignments. Sure links usually connect content words such as Zhongguo and China.
In contrast, possible links often align words within idiomatic expressions and free
translations.
An AER score is given by
AER(S, P, A) = 1 ? |A ? S| + |A ? P||A| + |S| (3)
where S is a set of sure links in a reference alignment that is hand-aligned by human
experts, P is a set of possible links in the reference alignment, and A is a candidate
alignment. Note that S is a subset of P: S ? P. The lower the AER score is, the better the
alignment quality is.
Although widely used, AER has been criticized for correlating poorly with transla-
tion quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER
scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b)
argue that reference alignments should consist of only sure links. They propose a new
measure called the balanced F-measure:
precision(S, A) =
|A ? S|
|S| (4)
recall(S, A) =
|A ? S|
|A| (5)
F-measure(S, ?, A) = 1
?
precision(S,A) +
1??
recall(S,A)
(6)
1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and
Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine
translation.
306
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
where ? is a parameter that sets the trade-off between precision and recall. Higher
F-measure means better alignment quality. Obviously, ? less than 0.5 weights recall
higher, whereas ? greater than 0.5 weights precision higher.
We use both AER and F-measure in our experiments. AER is used in experiments
evaluating alignment quality (Section 4.1) and F-measure is used in experiments evalu-
ating translation performance (Section 4.2).
2.2.2 Minimum Error Rate Training. Suppose we have three candidate alignments: a1, a2,
and a3. Their AER scores are 0.21, 0.20, and 0.22, respectively. Therefore, a2 is the best
candidate alignment, a1 is the second best, and a3 is the third best. We use three features
to score each candidate. Table 1 lists the feature values for each candidate.
If the set of feature weights is {1.0, 1.0, 1.0}, the model scores (see Equation (2)) of
the three candidates are ?71, ?74, and ?76, respectively. Whereas reference alignment
considers a2 as the best candidate, a1 has the maximal model score. This is undesirable
because the model fails to agree with the reference. If we change the feature weights
to {1.0,?2.0, 2.0}, the model scores become ?73, ?71, and ?83, respectively. Now, the
model chooses a2 as the best candidate correctly.
If a set of feature weights manages to make model predictions agree with refer-
ence alignments in training examples, we would expect the model to achieve good
alignment quality on unseen data as well. To do this, we adopt the minimum er-
ror rate training (MERT) algorithm proposed by Och (2003) to find feature weights
that minimize AER or maximize F-measure on a representative hand-aligned training
corpus.
Given a reference alignment r and a candidate alignment a, we use a loss func-
tion E(r, a) to measure alignment performance. Note that E(r, a) can be either AER or
1 ? F-measure. Given a bilingual corpus ?fS1, eS1? with a reference alignment rs and a set
of K different candidate alignments Cs = {as,1 . . . as,K} for each sentence pair ?fs, es?, our
goal is to find a set of feature weights ??M1 that minimizes the overall loss on the training
corpus:
??M1 = argmin
?M1
{ S
?
s=1
E(rs, a?(fs, es; ?
M
1 ))
}
(7)
= argmin
?M1
{ S
?
s=1
K
?
k=1
E(rs, as,k)?(a?(fs, es; ?
M
1 ), as,k)
}
(8)
Table 1
Example feature values and alignment error rates.
feature values
candidate h1 h2 h3 AER
a1 ?85 4 10 0.21
a2 ?89 3 12 0.20
a3 ?93 6 11 0.22
307
Computational Linguistics Volume 36, Number 3
where a?(fs, es; ?M1 ) is the best candidate alignment produced by the linear model:
a?(fs, es; ?
M
1 ) = argmax
a
{ M
?
m=1
?mhm(fs, es, a)
}
(9)
The basic idea of MERT is to optimize only one parameter (i.e., feature weight)
each time and keep all other parameters fixed. This process runs iteratively over M
parameters until the overall loss on the training corpus does not decrease.
Formally, suppose we tune a parameter and keep the other M ? 1 parameters fixed;
each candidate alignment corresponds to a line in the plane with ? as the independent
variable:
? ? ?(f, e, a) + ?(f, e, a) (10)
where ? denotes the parameter being tuned (i.e., ?m) and ?(f, e, a) and ?(f, e, a) are
constants with respect to ?:
?(f, e, a) = hm(f, e, a) (11)
?(f, e, a) =
M
?
m?=1,m? =m
?m?hm? (f, e, a) (12)
The set of candidates in Cs defines a set of lines. For example, given the candidate
alignments in Table 1, suppose we only tune ?2 and keep ?1 and ?3 fixed with an initial
set of parameters {1.0, 1.0, 1.0}. According to Equation (10), a1 corresponds to a line
4?? 75, a2 corresponds to a line 3?? 77, and a3 corresponds to a line 6?? 82.
The decision rule in Equation (9) states that a? is the line with the highest model
score for a given ?. The selection of ? for each sentence pair ultimately determines the
loss at ?. How do we find values of ? that could generate different loss values?
As the loss can only change if we move to a ? where the highest line is different
than before, Och (2003) suggests only evaluating the loss at values in between the
intersections that line the top surface of the cluster of lines. Figure 2 demonstrates eight
Figure 2
Candidate alignments in dimension ? and the critical intersections. Each candidate alignment is
represented as a line. ?1,?2, and ?3 are critical intersections where the best candidate a?
(highlighted in bold) will change: a? is a1 in (??, ?1], a2 in (?1, ?2], a7 in (?2, ?3], and a5 in
(?3, +?).
308
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
candidate alignments. The sequence of the topmost line segments highlighted in bold
constitutes an upper envelope, which indicates the best candidate alignments the model
predicts with various values of ?. Instead of computing all possible K2 intersections
between the lines in Cs, we just need to find the critical intersections where the topmost
line changes. In Figure 2, ?1, ?2, and ?3 are critical intersections. In the interval (??, ?1],
a1 has the highest score. Similarly, the best candidates are a2 for (?1, ?2], a7 for (?2, ?3],
and a5 for (?3, +?), respectively. The optimal ?? can be found by collecting all critical
intersections on the training corpus and choosing one ? that results in the minimal loss
value. Please refer to Och (2003) for more details.
2.3 Search
Given a source language sentence f and a target language sentence e, we try to find the
best candidate alignment with the highest model score:
a? = argmax
a
{
score(f, e, a)
}
(13)
= argmax
a
{ M
?
m=1
?mhm(f, e, a)
}
(14)
To do this, we begin with an empty alignment and keep adding new links until
the model score of the current alignment does not increase. Figure 3 illustrates this
search process. Given a source language sentence f1f2 and a target language sentence
e1e2, the initial alignment a1 is empty (i.e., all words are unaligned). Then, we obtain a
new alignment a2 by adding a link (1, 1) to a1. Similarly, the addition of (1, 2) to a1 leads
to a3. a2 and a3 can be further extended to produce more alignments.
Graphically speaking, the search space of a sentence pair can be organized as a
directed acyclic graph. Each node in the graph is a candidate alignment and each edge
corresponds to a link. We say that alignments that have the same number of links
constitute a level. There are 2J?I possible nodes and J ? I + 1 levels in a graph. In
Figure 3, a2, a3, a4, and a5 belong to the same level because they all contain one link.
The maximum level width is given by
( J?I
 J?I2 
)
. In Figure 3, the maximal level width is
(
4
2
)
= 6. Our goal is to find the node with the highest model score in a search graph.
As the search space of word alignment is exponential (although enumerable), it is
computationally prohibitive to explore all the graph. Instead, we can search efficiently
in a greedy way. In Figure 3, starting from a1, we add single links to a1 and obtain four
new alignments: a2, a3, a4, and a5. We retain the best new alignment that has a higher
score than a1, say a3, and discard the others. Then, we add single links to a3 and obtain
three new alignments: a7, a9, and a11. After choosing a9 as the current best alignment, the
next candidates are a12 and a14. Suppose the model scores of both a12 and a14 are lower
than that of a9. We terminate the search process and choose a9 as the best candidate
alignment.
During this search process, we expect that the addition of a single link l to the
current best alignment a will result in a new alignment a ? {l} with a higher score:
score(f, e, a ? {l}) > score(f, e, a) (15)
309
Computational Linguistics Volume 36, Number 3
Figure 3
Search space of a sentence pair: f1 f2 and e1e2. Each node in the directed graph is a candidate
alignment and each edge denotes a transition between two nodes by adding a link.
that is
M
?
m=1
?m
(
hm(f, e, a ? {l}) ? hm(f, e, a)
)
> 0 (16)
As a result, we can remove most of the computational overhead by calculating only
the difference of scores instead of the scores themselves. The difference of alignment
scores with the addition of a link, which we refer to as a link gain, is defined as
G(f, e, a, l) =
M
?
m=1
?mgm(f, e, a, l) (17)
where gm(f, e, a, l) is a feature gain, which is the incremental feature value after adding
a link l to the current alignment a:
gm(f, e, a, l) = hm(f, e, a ? {l}) ? hm(f, e, a) (18)
In our experiments, we use a beam search algorithm that is more general than the
above greedy algorithm. In the greedy algorithm, we retain at most one candidate in
each level of the space graph while traversing top-down. In the beam search algorithm,
we retain at most b candidates at each level.
Algorithm 1 shows the beam search algorithm. The input is a source language
sentence f and a target language sentence e (line 1). The algorithm maintains a list of
310
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 1 A beam search algorithm for word alignment
1: procedure ALIGN(f, e)
2: open ? ?  a list of active alignments
3: N ? ?  n-best list
4: a ? ?  begin with an empty alignment
5: ADD(open, a, ?, b)  initialize the list
6: while open = ? do
7: closed ? ?  a list of promising alignments
8: for all a ? open do
9: for all l ? J ? I ? a do  enumerate all possible new links
10: a? ? a ? {l}  produce a new alignment
11: g ? GAIN(f, e, a, l)  compute the link gain
12: if g > 0 then  ensure that the score will increase
13: ADD(closed, a?, ?, b)  update promising alignments
14: end if
15: ADD(N , a?, 0, n)  update n-best list
16: end for
17: end for
18: open ? closed  update active alignments
19: end while
20: return N  return n-best list
21: end procedure
active alignments open (line 2) and an n-best list N (line 3). The aligning process begins
with an empty alignment a (line 4) and the procedure ADD(open, a, ?, b) adds a to open.
The procedure prunes the search space by discarding any alignment that has a score
worse than:
1. ? multiplied with the best score in the list, or
2. the score of b-th best alignment in the list.
For each iteration (line 6), we use a list closed to store promising alignments that
have higher scores than the current alignment. For every possible link l (line 9), we
produce a new alignment a? (line 10) and calculate the link gain G by calling the
procedure GAIN(f, e, a, l). If a? has a higher score (line 12), it is added to closed (line 13).
We also update N to keep the top n alignments explored during the search (line 15). The
n-best list will be used in training feature weights by MERT. This process iterates
until there are no promising alignments. The theoretical running time of this algorithm
is O(bJ2I2).
3. Feature Functions
The primary art in discriminative modeling is to define useful features that capture var-
ious characteristics of word alignments. Intuitively, we can include generative models
such as the IBM Models 1?5 (Brown et al 1993) as features in a discriminative model.
A straightforward way is to use a generative model itself as a feature directly (Liu, Liu,
311
Computational Linguistics Volume 36, Number 3
and Lin 2005). Another way is to treat each sub-model of a generative model as a feature
(Fraser and Marcu 2006). In either case, a generative model can be regarded as a special
case of a discriminative model where all feature weights are one. A detailed discussion
of the treatment of the IBM models as features can be found in Appendix B.
One major drawback of the IBM models is asymmetry. They are restricted such that
each source word is assigned to exactly one target word. This is not the case for many
language pairs. For example, in our running example, one Chinese word jianzhuye cor-
responds to two English words construction industry. As a result, our linear model will
produce only one-to-one alignments if the IBM models in two translation directions (i.e.,
source-to-target and target-to-source) are both used. Although some authors would use
the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,
Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and
the recall cannot reach 100% in principle.
A more general way is to model alignment as an arbitrary relation between source
and target language word positions. As our linear model is capable of including many
overlapping features regardless of their interdependencies, it is easy to add features
that characterize symmetric alignments. In the following subsections, we will introduce
a number of symmetric features used in our experiments.
3.1 Translation Probability Product
To determine the correspondence of words in two languages, word-to-word translation
probabilities are always the most important knowledge source. To model a symmetric
alignment, a straightforward way is to compute the product of the translation probabil-
ities of each link in two directions.
For example, suppose that there is an alignment {(1, 2)} for a source language
sentence f1 f2 and a target language sentence e1e2; the translation probability prod-
uct is
t(e2| f1) ? t( f1|e2)
where t(e| f ) is the probability that f is translated to e and t( f |e) is the probability that e
is translated to f , respectively.
Unfortunately, the underlying model is biased: The more links added, the smaller
the product will be. For example, if we add a link (2, 2) to the current alignment and
obtain a new alignment {(1, 2), (2, 2)}, the resulting product will decrease after being
multiplied with t(e2| f2) ? t( f2|e2):
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2)
The problem results from the absence of empty cepts. Following Brown et al (1993),
a cept in an alignment is either a single source word or it is empty. They assign cepts
to positions in the source sentence and reserve position zero for the empty cept. All
unaligned target words are assumed to be ?aligned? to the empty cept. For example,
in the current example alignment {(1, 2)}, the unaligned target word e1 is said to be
?aligned? to the empty cept f0. As our model is symmetric, we use f0 to denote the
empty cept on the source side and use e0 to denote the empty cept on the target side,
respectively.
312
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
If we take empty cepts into account, the product for {(1, 2)} can be rewritten as
t(e2| f1) ? t( f1|e2) ? t(e1| f0) ? t( f2|e0)
Similarly, the product for {(1, 2), (2, 2)} now becomes
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2) ? t(e1| f0)
Note that after adding the link (2, 2), the new product still has more factors than the old
product. However, the new product is not necessarily always smaller than the old one.
In this case, the new product divided by the old product is
t(e2| f2) ? t( f2|e2)
t( f2|e0)
Whether a new product increases or not depends on actual translation probabilities.2
Depending on whether they are aligned or not, we divide the words in a sentence
pair into two categories: aligned and unaligned. For each aligned word, we use trans-
lation probabilities conditioned on its counterpart in two directions (i.e., t(ei| fj) and
t( fj|ei)). For each unaligned word, we use translation probabilities conditioned on empty
cepts on the other side in two directions (i.e., t(ei| f0) and t( fj|e0)).
Formally, the feature function for translation probability product is given by3
htpp(f, e, a) =
?
( j,i)?a
(
log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
))
+
J
?
j=1
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
+
I
?
i=1
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(19)
where ?(x, y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We define
the fertility of a source word fj as the number of aligned target words:
?j =
?
( j?,i)?a
?( j?, j) (20)
2 Even though we take empty cepts into account, the bias problem still exists because the product will
decrease by adding new links if there are no unaligned words. For example, the product will go down if
we further add a link (1, 1) to {(1, 2), (2, 2)} as all source words are aligned. This might not be a bad bias
because reference alignments usually do not have all words aligned and contain too many links.
Although translation probability product is degenerate as a generative model, the bias problem can be
alleviated when this feature is combined with other features such as link count (see Section 3.8).
3 We use the logarithmic form of translation probability product to avoid manipulating very small
numbers (e.g., 4.3 ? e?100) just for practical reasons.
313
Computational Linguistics Volume 36, Number 3
Table 2
Calculating feature values of translation probability product for a source sentence f1 f2 and a
target sentence e1e2.
alignment feature value
{} log
(
t(e1| f0) ? t(e2| f0) ? t( f1|e0) ? t( f2|e0)
)
{(1, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t( f1|e2) ? t( f2|e0)
)
{(1, 2), (2, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t(e2| f2) ? t( f1|e2) ? t( f2|e2)
)
Similarly, the fertility of a target word ei is the number of aligned source words:
?i =
?
( j,i? )?a
?(i?, i) (21)
For example, as only one English word China is aligned to the first Chinese word
Zhongguo in Figure 1, the fertility of Zhongguo is ?1 = 1. Similarly, the fertility of the
third Chinese word duiwaikaifang is ?3 = 4 because there are four aligned English
words. The fertility of the first English word The is ?1 = 0. Obviously, the words with
zero fertilities (e.g., The, ?s, and a in Figure 1) are unaligned.
In Equation (19), the first term calculates the product of aligned words, the second
term deals with unaligned source words, and the third term deals with unaligned target
words. Table 2 shows the feature values for some word alignments.
For efficiency, we need to calculate the difference of feature values instead of the
values themselves, which we call feature gain (see Equation (18)). The feature gain for
translation probability product is4
gtpp(f, e, a, j, i) = log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
)
?
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
?
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(22)
where ?j and ?i are the fertilities before adding the link ( j, i).
Although this feature is symmetric, we obtain the translation probabilities t( f |e) and
t(e| f ) by training the IBM models using GIZA++ (Och and Ney 2003).
3.2 Exact Match
Motivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) are
often the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a feature
that sums up the number of words linked to identical words. We adopt this exact match
feature in our model:
hem(f, e, a) =
?
( j,i)?a
?( fj, ei) (23)
4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a, l) because j and i appear in the equation.
314
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
gem(f, e, a, j, i) = ?( fj, ei) (24)
3.3 Cross Count
Due to the diversity of natural languages, word orders between two languages are usu-
ally different. For example, subject-verb-object (SVO) languages such as Chinese and
English often put an object after a verb while subject-object-verb (SOV) languages such
as Japanese and Turkish often put an object before a verb. Even between SVO languages
such as Chinese and English, word orders could be quite different too. In Figure 1,
while Zhongguo is the first Chinese word, its counterpart China is the fourth English
word. Meanwhile, the third Chinese word duiwaikaifang after Zhongguo is aligned to the
second English word opening before China. We say that there is a cross between the two
links (1, 4) and (3, 2) because (1 ? 3) ? (4 ? 2) < 0. In Figure 1, there is only one cross.
As a result, we could use the number of crosses in alignments to capture the divergence
of word orders between two languages.
Formally, the cross count feature function is given by
hcc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (25)
gcc(f, e, a, j, i) =
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (26)
where expr is an indicator function that takes a boolean expression expr as the
argument:
expr =
{
1 if expr is true
0 otherwise
(27)
3.4 Neighbor Count
Moore (2005) finds that word alignments between closely related languages tend to be
approximately monotonic. Even for distantly related languages, the number of crossing
links is far less than chance since phrases tend to be translated as contiguous chunks.
In Figure 1, the dark points are positioned approximately in parallel with the diagonal
line, indicating that the alignment is approximately monotonic.
To capture such monotonicity, we follow Lacoste-Julien et al (2006) to encourage
strictly monotonic alignments by adding a bonus for any pair of links ( j, i) and ( j?, i?)
such that
j ? j? = 1 ? i ? i? = 1
In Figure 1, there is one such link pair: (3, 10) and (4, 11). We call these links
neighbors. Similarly, (5, 13) and (6, 14) are also neighbors.
Formally, the neighbor count feature function is given by
hnc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (28)
315
Computational Linguistics Volume 36, Number 3
gnc(f, e, a, j, i) =
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (29)
3.5 Fertility Probability Product
Casual inspection of some word alignments quickly establishes that some Chinese
words such as Zhongguo and chengxian are often aligned to one English word whereas
other Chinese words such as duiwaikaifang tend to be translated into multiple English
words. Brown et al (1993) call the number of target words to which a source word f is
connected the fertility of f . Recall that we have given the formal definition of fertility in
the symmetric scenario in Equation (20) and Equation (21).
Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3
and 3.4), fertility also proves to be very important in modeling alignment because
sophisticated generative models such as the IBM Models 3?5 parameterize fertilities
directly. As our goal is to produce symmetric alignments, we calculate the product of
fertility probabilities in two directions.
Given an alignment {(1,2)} for a source sentence f1 f2 and a target sentence e1e2, the
fertility probability product is
n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2)
where n(?j| fj) is the probability that fj has a fertility of ?j and n(?i|ei) is the probability
that ei has a fertility of ?i, respectively.5 For example, n(1| f0) denotes the probability
that one target word is ?aligned? to the source empty cept f0 and n(1|e2) denotes the
probability that one source word is aligned to e2.
If we add a link (2, 2) to the current alignment and obtain a new alignment
{(1, 2), (2, 2)}, the resulting product will be
n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2)
The new product divided by the old product is
n(1| f2) ? n(0|e0) ? n(2|e2)
n(0| f2) ? n(1|e0) ? n(1|e2)
Formally, the feature function for fertility probability product is given by
hfpp(f, e, a) =
J
?
j=0
log(n(?j| fj)) +
I
?
i=0
log(n(?i|ei)) (30)
5 Brown et al (1993) treat the empty cept in a different way. They assume that at most half of the source
words in an alignment are not aligned (i.e., ?0 ? J/2) and define a binomial distribution relying on an
auxiliary parameter p0. Here, we use n(?0|e0) instead of the original form n0(?0|
?I
i=1 ?i ) just for
simplicity. See Appendix B for more details.
316
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
The corresponding feature gain is
gfpp(f, e, a, j, i) = log(n(?0 ? ?(?i, 0)| f0)) ? log(n(?0| f0)) +
log(n(?j + 1| fj) ? log(n(?j| fj)) +
log(n(?0 ? ?(?j, 0)|e0)) ? log(n(?0|e0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) (31)
where ?j and ?i are the fertilities before adding the link ( j, i).
Table 3 gives the feature values for some word alignments. In practice, we also
obtain all fertility probabilities n(?j| fj) and n(?i|ei) by using the output of GIZA++
directly.
3.6 Linked Word Count
We observe that there should not be too many unaligned words in good alignments.
For example, there are only three unaligned words on the target side in Figure 1: The,
?s, and a. Unaligned words are usually function words that have little lexical meaning
but instead serve to express grammatical relationships with other words or specify the
attitude or mood of the speaker. To control the number of unaligned words, we follow
Moore, Yih, and Bode (2006) to introduce a linked word count feature that simply counts
the number of aligned words:
hlwc(f, e, a) =
J
?
j=1
?j > 0 +
I
?
i=1
?i > 0 (32)
glwc(f, e, a, j, i) = ?(?j, 0) + ?(?i, 0) (33)
In Equation (33), ?j and ?i are the fertilities before adding l.
3.7 Sibling Distance
In word alignments, there are usually several words connected to the same word on the
other side. For example, in Figure 1, two English words construction and industry are
aligned to one Chinese word jianzhuye. We call the words aligned to the same word on
the other side siblings. In Figure 1, opening, to, the, and outside are also siblings because
they are aligned to duiwaikaifang. A word (e.g., jianzhuye) often tends to produce a series
of words in another language that belong together, whereas others (e.g., duiwaikaifang)
Table 3
Calculating feature values of fertility probability product for a source sentence f1 f2 and a target
sentence e1e2.
alignment feature value
{} log(n(2| f0) ? n(0| f1) ? n(0| f2) ? n(2|e0) ? n(0|e1) ? n(0|e2))
{(1, 2)} log(n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2))
{(1, 2), (2, 2)} log(n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2))
317
Computational Linguistics Volume 36, Number 3
tend to produce a series of words that should be separate. To model this tendency, we
introduce a feature that sums up the distances between siblings.
Formally, we use ?j,k to denote the position of the k-th target word aligned to a
source word fj and use ?i,k to denote the position of the k-th source word aligned to a
target word ei. For example, jianzhuye is the second source word (i.e., f2) in Figure 1.
As the first target word aligned to f2 is construction (i.e., e6), therefore we say that
?2,1 = 6. Similarly, ?2,2 = 7 because industry (i.e., e7) is the second target word aligned
to jianzhuye. Obviously, ?j,k+1 is always greater than ?j,k by definition.
As construction and industry are siblings, we define the distance between them
as ?2,2 ? ?2,1 ? 1 = 0. Note that we give no penalty to siblings that belong closely
together. In Figure 1, there are four siblings opening, to, the, and outside aligned to the
source word duiwaikaifang. The sum of distances between them is calculated as
?3,2 ? ?3,1 ? 1 + ?3,3 ? ?3,2 ? 1 + ?3,4 ? ?3,3 ? 1
= ?3,4 ? ?3,1 ? 3
= 10 ? 2 ? 3
= 5
Therefore, the distance sum of fj can be efficiently calculated as
?( j, ?j) =
{
?j,?j ? ?j,1 ? ?j + 1 if ?j > 1
0 otherwise
(34)
Accordingly, the distance sum of ei is
?(i, ?i) =
{
?i,?i ? ?i,1 ? ?i + 1 if ?i > 1
0 otherwise
(35)
Formally, the feature function for sibling distance is given by
hsd(f, e, a) =
J
?
j=1
?( j, ?j) +
I
?
i=1
?(i, ?i) (36)
The corresponding feature gain is
gsd(f, e, a, j, i) = ?( j, ?j + 1) ? ?( j, ?j) +
?(i, ?i + 1) ??(i, ?i) (37)
where ?j and ?i are the fertilities before adding the link ( j, i).
3.8 Link Count
Given a source sentence with J words and a target sentence with I words, there are
J ? I possible links. However, the actual number of links in a reference alignment is
usually far less. For example, there are only 10 links in Figure 1 although the maximum
is 6 ? 14 = 84. The number of links has an important effect on alignment quality because
318
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
more links result in higher recall while fewer links result in higher precision. A good
trade-off between recall and precision usually results from a reasonable number of links.
Using the number of links as a feature could also alleviate the bias problem posed by
the translation probability product feature (see Section 3.1). A negative weight of the
link count feature often leads to fewer links while a positive weight favors more links.
Formally, the feature function for link count is
hlc(f, e, a) = |a| (38)
glc(f, e, a, l) = 1 (39)
where |a| is the cardinality of a (i.e., the number of links in a).
3.9 Link Type Count
Due to the different fertilities of words, there are different types of links. For instance,
one-to-one links indicate that one source word (e.g., Zhongguo) is translated into ex-
actly one target word (e.g., China) while many-to-many links exist for phrase-to-phrase
translation. The distribution of link types differs for different language pairs. For ex-
ample, one-to-one links occur more frequently in closely related language pairs (e.g.,
French?English) and one-to-many links are more common in distantly related language
pairs (e.g., Chinese?English). To capture the distribution of link types independent of
languages, we use features to count different types of links.
Following Moore (2005), we divide links in an alignment into four categories:
1. one-to-one links, in which neither the source nor the target word
participates in other links;
2. one-to-many links, in which only the source word participates in other
links;
3. many-to-one links, in which only the target word participates in other
links;
4. many-to-many links, in which both the source and target words
participate in other links.
In Figure 1, (1, 4), (4, 11), (5, 13), and (6, 14) are one-to-one links and the others are
one-to-many links.
As a result, we introduce four features:
ho2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i = 1 (40)
ho2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i = 1 (41)
hm2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i > 1 (42)
hm2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i > 1 (43)
319
Computational Linguistics Volume 36, Number 3
Their feature gains cannot be calculated in a straightforward way because the
addition of a link might change the link types of its siblings on both the source and
target sides. For example, if we align the Chinese word chengxian and the English word
industry, the newly added link (4, 7) is a many-to-many link. Its source sibling (2, 7),
which was a one-to-many link, now becomes a many-to-many link. Meanwhile, its
target sibling (4, 11), which was a one-to-one link, now becomes a one-to-many link.
Algorithm 2 shows how to calculate the four feature gains. After initialization
(line 2), we first decide the type of l (lines 3?11). Then, we consider the siblings of l
on the target side (lines 12?24) and those on the source side (lines 25?38), respectively.
Note that the feature gains of siblings will not change if ?i = 1 or ?j = 1.
3.10 Bilingual Dictionary
A conventional bilingual dictionary can be considered an additional knowledge source.
The intuition is that a dictionary is expected to be more reliable than an automatically
trained lexicon. For example, if Zhongguo and China appear in an entry of a dictionary,
they should be more likely to be aligned. Thus, we use a single indicator feature to
encourage linking word pairs that occur in a dictionary D:
hbd(f, e, a, D) =
?
( j,i)?a
( fj, ei) ? D (44)
gbd(f, e, a, D, j, i) = ( fj, ei) ? D (45)
3.11 Link Co-Occurrence Count
The system combination technique that integrates predictions from multiple systems
proves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007;
He et al 2008). In word alignment, a link should be aligned if it appears in most
system predictions. Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4
predictions as features and obtain substantial improvements.
To enable system combination, we design a feature to favor links voted by most
systems. Given an alignment a? produced by another system, we use the number of
links of the intersection of a and a? as a feature:
hlcc(f, e, a, a
?) = |a ? a?| (46)
glcc(f, e, a, a
?, j, i) = l ? a ? a? (47)
4. Experiments
In this section, we try to answer two questions:
1. Does the proposed approach achieve higher alignment quality than
generative alignment models?
2. Do statistical machine translation systems produce better translations if
we replace generative alignment models with the proposed approach?
In Section 4.1, we evaluate our approach on three word alignment shared tasks for
five language pairs with varying divergence and richness of resources. Experimental
320
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 2 Calculating gains for the link type count features
1: procedure GAINLINKTYPECOUNT(f, e, a, j, i)
2: {go2o, go2m, gm2o, gm2m} ? {0, 0, 0, 0}  initialize the feature gains
3: if ?j = 0 ? ?i = 0 then  consider ( j, i) first
4: go2o ? go2o + 1
5: else if ?j > 0 ? ?i = 0 then
6: go2m ? go2m + 1
7: else if ?j = 0 ? ?i > 0 then
8: gm2o ? gm2o + 1
9: else
10: gm2m ? gm2m + 1
11: end if
12: if ?j = 1 then  consider the siblings of ( j, i) on the target side
13: for i? = 1 . . . I do
14: if ( j, i?) ? a ? i? = i then  ( j, i?) is a sibling of ( j, i) on the target side
15: if ?i? = 1 then  ( j, i?) was a one-to-one link
16: go2o ? go2o ? 1
17: go2m ? go2m + 1  ( j, i?) now becomes a one-to-many link
18: else  ( j, i?) was a many-to-one link
19: gm2o ? gm2o ? 1
20: gm2m ? gm2m + 1  ( j, i?) now becomes a many-to-many link
21: end if
22: end if
23: end for
24: end if
25: if ?i = 1 then  consider the siblings of ( j, i) on the source side
26: for j? = 1 . . . J do
27: if ( j?, i) ? a ? j? = j then  ( j?, i) is a sibling of ( j, i) on the source side
28: if ?j? = 1 then  ( j?, i) was a one-to-one link
29: go2o ? go2o ? 1
30: gm2o ? gm2o + 1  ( j?, i) now becomes a many-to-one link
31: else  ( j?, i) was a one-to-many link
32: go2m ? go2m ? 1
33: gm2m ? gm2m + 1  ( j?, i) now becomes a many-to-many link
34: end if
35: end if
36: end for
37: end if
38: return {go2o, go2m, gm2o, gm2m}  return the four feature gains
39: end procedure
results show that our system outperforms systems participating in the three shared
tasks significantly and achieves comparable results with other state-of-the-art discrimi-
native alignment models.
In Section 4.2, we investigate the effect of our model on translation quality. By
training feature weights with respect to F-measure instead of AER, our model results
in superior translation quality over generative methods for phrase-based, hierarchical
phrase-based, and tree-to-string SMT systems.
321
Computational Linguistics Volume 36, Number 3
4.1 Evaluation of Alignment Quality
In this section, we present results of experiments on three word alignment shared tasks:
1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of
the HLT/NAACL 2003 workshop on ?Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,? this shared task includes
two language pairs: English?French and Romanian?English. Participants
can use both limited and unlimited resources.
2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of
the ACL 2005 workshop on ?Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,? this shared task includes three
language pairs to cover different language and data characteristics:
English?Inuktitut, Romanian?English, and English?Hindi. Participants
can use both limited and unlimited resources.
3. HTRDP 2005 shared task. As part of the 2005 HTRDP (National High
Technology Research and Development Program of China, also called
?863? Program) Evaluation on Chinese Information Processing and
Intelligent Human-Machine Interface Technology, this shared task
included only one language pair: Chinese?English. Participants can use
unlimited resources.
Among these, we choose two tasks, English?French and Chinese?English, to report
detailed experimental results. Results for the other tasks can also be found in Table 11.
Corpus statistics for the English?French and Chinese?English tasks are shown in
Tables 4 and 5. The English?French data from the HLT/NAACL 2003 shared task consist
of a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentence
pairs, and a test corpus of 447 sentence pairs. The development and test sets are manu-
ally aligned and marked with both sure and possible labels. Although the Canadian
Hansard bilingual corpus is widely used in the community, direct comparisons are
difficult due to the differences in splitting of training data, development data, and test
data. To make our results more comparable to previous work, we followed Lacoste-
Table 4
Corpus characteristics of the English?French task.
English French
Training corpus Sentences 1,130,104
Words 20.01M 23.61M
Vocabulary 68, 019 86, 591
Development corpus Sentences 37
Words 661 721
Vocabulary 322 344
Test corpus Sentences 447
Words 7, 020 7, 761
Vocabulary 1, 732 1, 943
322
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 5
Corpus characteristics of the Chinese?English task.
Chinese English
Training corpus Sentences 837,594
Words 10.32M 10.71M
Vocabulary 93, 532 134, 143
Development corpus Sentences 502
Words 9, 338 9, 364
Vocabulary 2, 608 2, 587
Test corpus Sentences 505
Words 9, 088 10, 224
Vocabulary 2, 319 2, 651
Julien et al (2006) splitting the original test set into two parts: the first 200 sentences
as the development set and the remaining 247 sentences as the test set. To compare
with systems participating in the 2003 NAACL shared task, we also used the small
development set of 37 sentences to optimize feature weights, and ran our system on the
original test set of 447 sentences. The results are shown in Table 11.
The Chinese?English data from the HTRDP 2005 shared task contains a develop-
ment corpus of 502 sentence pairs and a test corpus of 505 sentence pairs. We use
a training corpus of 837, 594 sentence pairs available from Chinese Linguistic Data
Consortium and a bilingual dictionary containing 415, 753 entries.
4.1.1 Comparison of the Search Algorithm with GIZA++. We develop a word alignment
system named Vigne based on the linear modeling approach. As we mentioned before,
our model can include the IBM models as features (see Appendix B). To investigate the
effectiveness of our search algorithm, we compare Vigne with GIZA++ by using the
same models.
Table 6 shows the alignment error rate percentages for various IBM models in
GIZA++ and Vigne. To make the results comparable, we ensured that Vigne shared
Table 6
Comparison of AER scores for various IBM models in GIZA++ and Vigne. These models are
trained only on development and test sets. The pruning setting for Vigne is ? = 0 and b = 1. All
differences are not statistically significant.
English?French Chinese?English
Model Training Scheme Direction GIZA++ Vigne GIZA++ Vigne
Model 1 15 S ? T 50.6 50.6 58.0 58.0
T ? S 46.2 46.2 56.1 56.1
Model 2 1525 S ? T 47.8 47.8 59.3 59.3
T ? S 43.6 43.6 57.4 57.4
Model 3 15H533 S ? T 31.6 31.4 45.0 44.5
T ? S 27.9 27.9 47.4 46.5
Model 4 15H53343 S ? T 34.5 34.2 44.9 44.6
T ? S 30.8 30.6 46.7 46.4
323
Computational Linguistics Volume 36, Number 3
the same parameters with GIZA++.6 Table 6 also gives the training schemes used for
GIZA++. For example, the training scheme for Model 4 is 15H53343. This notation
indicates that five iterations of Model 1, five iterations of HMM, three iterations of
Model 3, and three iterations of Model 4 are performed. As the two systems use the same
model parameters, the amount of training data will have no effect on the comparison.
Therefore, we trained the IBM Models only on the development and test sets. As a re-
sult, the AER scores in Table 6 look quite high.
In GIZA++, there exist simple polynomial algorithms to find the Viterbi alignments
for Models 1 and 2. We observe that the greedy search algorithm (? = 0 and b = 1)
used by Vigne can also find the optimal alignments. Note that the two systems achieve
identical AER scores because there are no search errors.
For Models 3 and 4, maximization over all alignments cannot be efficiently carried
out as the corresponding search problem is NP-complete. To alleviate the problem,
GIZA++ resorts to a greedy search algorithm. The basic idea is to compute a Viterbi
alignment of a simple model such as Model 2 or HMM. This alignment (an intermediate
node in the search space) is then iteratively improved with respect to the alignment
probability of the refined model by moving or swapping links. In contrast, our search
algorithm starts from an empty alignment and has only one operation: adding a link.
In addition, we treat the fertility probability of an empty cept in a different way (see
Equation B.7). Interestingly, Vigne achieves slightly better results than GIZA++ for both
models. All differences are not statistically significant.
4.1.2 Comparison to Generative Models Using Asymmetric Features. Table 7 compares the
AER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne.
On both tasks, we lowercased all English words in the training, development, and
test sets as a preprocessing step. For GIZA++, we used the default training scheme
of 15H53545. We used the three symmetrization heuristics proposed by Och and Ney
(2003): intersection, union, and refined method. For Cross-EM, we also used the default
configuration and jointly trained Model 1 and HMM for five iterations. For Vigne, we
used a greedy search strategy by setting ? = 0 and b = 1. Note that both GIZA++ and
Cross-EM are unsupervised alignment methods.
On the English?French task, the refined combination of Model 4 alignments pro-
duced by GIZA++ in both translation directions yields an AER of 5.9%. Cross-EM
outperforms GIZA++ significantly by achieving 5.1%. For Vigne, we use Model 4 as
the primary feature. The linear combination of Model 4 in both directions achieves
a lower AER than either one separately. The link count feature controls the number
of links in the resulting alignments and leads to an absolute improvement of 0.1%.
With the addition of cross count and neighbor count features, the AER score drops to
5.4%. We attribute this to the fact that the two features are capable of capturing the
locality and monotonicity properties of natural languages, especially for closely related
language pairs such as English?French. After adding the linked word count feature, our
model achieves an AER of 5.2%. Finally, Vigne achieves an AER of 4.0% by combining
predictions from refined Model 4 and jointly trained HMM.
On the Chinese?English task, one-to-many and many-to-one relationships occur
more frequently in the reference alignments than the English?French task. As Cross-EM
6 In GIZA++ training, the final parameters are estimated on the final alignments, which are computed
using the parameters obtained in the previous iteration. As a result, Vigne made use of the parameters
generated by the iteration before the final iteration. In other experiments, Vigne used the final parameters.
324
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 7
Comparison of GIZA++, Cross-EM, and Vigne on both tasks. Note that Vigne yields only
one-to-one alignments if both ?Model 4 s2t? and ?Model 4 t2s? features are used. The pruning
setting for Vigne is ? = 0 and b = 1. While the final results of our system are better than the best
baseline generative models significantly at p < 0.01, adding a single feature will not always
produce a significant improvement, especially for English?French.
System Setting English?French Chinese?English
Model 4 s2t 7.7 20.9
Model 4 t2s 9.2 30.3
GIZA++ Intersection 6.8 21.8
Union 9.6 28.1
Refined method 5.9 18.4
Cross-EM HMM, joint 5.1 18.9
Model 4 s2t 7.8 20.5
+Model 4 t2s 5.6 18.3
+link count 5.5 17.7
+cross count 5.4 17.6
+neighbor count 5.2 17.4Vigne
+exact match 5.3 -
+linked word count 5.2 17.3
+bilingual dictionary - 17.1
+link co-occurrence count (GIZA++) 5.1 16.3
+link co-occurrence count (Cross-EM) 4.0 15.7
is prone to produce one-to-one alignments by encouraging agreement, symmetrizing
Model 4 by refined method yields better results than Cross-EM. We observe that the ad-
vantages of adding features such as link count, cross count, neighbor count, and linked
word count to our linear model continue to hold, resulting in a much lower AER than
both GIZA++ and Cross-EM. The addition of the bilingual dictionary is beneficial and
yields an AER of 17.1%. Further improvements were obtained by including predictions
from GIZA++ and Cross-EM.
As the IBM models do not allow a source word to be aligned with more than one
target word, the activation of the IBM models in both directions always yields one-
to-one alignments and thus has a loss in recall. To alleviate this problem, we use a
heuristic postprocessing step to produce many-to-one or one-to-many alignments. First,
we collect links that have higher translation probabilities than corresponding null links
in both directions. Then, these candidate links are sorted according to their translation
probabilities. Finally, they are added to the alignments under structural constraints
similar to those of Och and Ney (2003).
On the English?French task, this symmetrization method achieves relatively small
but very consistent improvements ranging from 0.1% to 0.2%. On the Chinese?English
task, the improvements are more significant, ranging from 0.1% to 0.8%. This differ-
ence also results from the fact that the reference alignments of the Chinese?English
task contain more one-to-many and many-to-one relationships than the English?French
task. After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%,
respectively.
4.1.3 Resulting Feature Weights. Table 8 shows the resulting feature weights of minimum
error rate training. We observe that adding new features has an effect on the weights
325
Computational Linguistics Volume 36, Number 3
Table 8
Resulting feature weights of minimum error rate training on the Chinese?English task (M4ST:
Model 4 s2t; M4TS: Model 4 t2s; LC: link count; CC: cross count; NC: neighbor count; LWC:
linked word count; BD: bilingual dictionary; LCCG: link co-occurrence count (GIZA++); LCCC:
link co-occurrence count (Cross-EM)).
M4ST M4TS LC CC NC LWC BD LCCG LCCC
M4ST 1.00 - - - - - - - -
+M4TS 0.63 0.37 - - - - - - -
+LC 0.18 0.07 ?0.75 - - - - - -
+CC 0.19 0.07 ?0.56 ?0.18 - - - - -
+NC 0.12 0.06 ?0.55 ?0.08 0.17 - - - -
+LWC 0.14 0.08 ?0.22 ?0.08 0.25 ?0.26 - - -
+BD 0.07 0.02 ?0.35 ?0.05 0.16 0.01 0.34 - -
+LCCG 0.03 0.04 ?0.13 ?0.05 0.20 ?0.16 0.28 0.11 -
+LCCC 0.02 0.02 0.14 ?0.03 0.10 ?0.26 0.30 0.04 0.09
of other features. The weights of the cross count feature are consistently negative,
suggesting that crossing links are always discouraged for Chinese?English. Also, the
positive weights of the neighbor count feature indicate that monotonic alignments
are encouraged. When the bilingual dictionary was included, the weights of Model 4
features in both directions dramatically decreased.
4.1.4 Results of the Symmetric Alignment Model. As we mentioned before, the linear model
can model many-to-many alignments directly without any postprocessing symmetriza-
tion heuristics.
Table 9 demonstrates the results of the symmetric alignment model on both tasks.
As the activation of translation and fertility probability products allows for arbitrary
relationships, the addition of the link count feature excludes most loosely related links
Table 9
AER scores achieved by the symmetric alignment model on both tasks. The pruning setting for
Vigne is ? = 0 and b = 1. Although the final model obviously outperforms the initial model
significantly at p < 0.01, adding a single feature will not always result in a significant
improvement, especially for English?French.
Features English?French Chinese?English
translation probability product 17.3 23.6
+fertility probability product 14.6 22.6
+link count 14.5 21.6
+cross count 5.8 18.5
+neighbor count 5.2 17.2
+exact match 5.1 -
+linked word count 5.2 17.0
+link types 5.0 16.9
+sibling distance 4.9 16.2
+bilingual dictionary - 15.9
+link co-occurrence count (GIZA++) 4.5 15.1
+link co-occurrence count (Cross-EM) 3.7 14.5
326
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
and results in more significant improvements than for asymmetric IBM models. One
interesting finding is that the cross count feature is very useful, leading to dramatic
absolute reduction of 8.7% on the English?French task and 3.1% on the Chinese?English
task, respectively. We find that the advantages of adding neighbor count and linked
word count still hold. By further including predictions from GIZA++ and Cross-EM,
our linear model achieves the best result: 3.7% on the English?French task and 14.5%
on the Chinese?English task.
We find that the symmetric linear model outperforms the asymmetric one, espe-
cially on the Chinese?English task. This suggests that although the asymmetric model
can produce symmetric alignments via symmetrization heuristics, the ?genuine? sym-
metric model produces many-to-many alignment in a more natural way.
4.1.5 Effect of Beam Search. Table 10 shows the effect of varying beam widths. The aligning
speed (words per second) decreases almost linearly with the increase of beam width b.
For simple alignment models such as using only the translation probability product
feature, enlarging the beam size fails to bring improvements due to modeling errors.
When more features are added, the model becomes more expressive. Therefore, our
system benefits from larger beam size consistently, although some benefits are not
significant statistically. When we set b = 10, the final AER scores for the English?French
and Chinese?English tasks are 3.6% and 14.3%, respectively.
4.1.6 Effect of Training Corpus Size. One disadvantage of our approach is that we need
a hand-aligned training corpus for training feature weights. However, compared with
building a treebank, manual alignment is far less expensive because one annotator only
needs to answer yes?no questions: Should this pair of words be aligned or not? If well
trained, even a non-linguist who is familiar with both source and target languages could
Table 10
Comparison of aligning speed (words per second) and AER score with varying beam widths for
the Chinese?English task. We fix ? = 0.01. Bold numbers refer to the results that are better than
the baseline but not significantly so. We use ?+? to denote the results that outperform the best
baseline (b = 1) and are statistically significant at p < 0.05. Similarly, we use ?++? to denote
significantly better than baseline at p < 0.01.
b=1 b=5 b=10
Features w/sec AER w/sec AER w/sec AER
translation probability product 3, 941 23.6 843 23.6 426 23.7
+fertility probability product 1, 418 22.6 300 22.7 150 22.9
+link count 1, 557 21.6 330 21.7 166 21.9
+cross count 1, 696 18.5 359 18.6 180 18.6
+neighbor count 1, 648 17.2 355 16.8+ 178 16.7+
+linked word count 1, 627 17.0 351 16.4+ 176 16.5+
+sibling distance 1, 531 16.9 326 16.5+ 165 16.4+
+link types 899 16.2 187 15.6+ 96 15.5++
+bilingual dictionary 890 15.9 187 15.6 94 15.5+
+link co-occurrence count (GIZA++) 877 15.1 182 15.0 92 14.9
+link co-occurrence count (Cross-EM) 867 14.5 183 14.4 92 14.3
327
Computational Linguistics Volume 36, Number 3
produce high-quality alignments. We estimate that aligning a sentence pair usually
takes only two minutes on average.
An interesting question is: How many training examples are needed to train a
good discriminative model? Figure 4 shows the learning curves with different numbers
of features on the Chinese?English task. We choose four feature groups with varying
numbers of features: 3, 6, 10, and 14. There are eight fractions of the training corpus: 10,
20, 50, 100, 200, 300, 400, and 502. Generally, the more features a model uses, the more
training examples are needed to train feature weights. Surprisingly, even when we use
14 features, 50 sentences seem to be good enough for minimum error rate training. This
finding suggests that our approach could work well even with a quite small training
corpus.
4.1.7 Summary of Results. Table 11 summarizes the results on all three shared tasks.
Vigne used the same configuration for all tasks. We used the symmetric linear model
and activated all features. The pruning setting is ? = 0.01 and b = 10. Our system
outperforms the systems participating in all the three shared tasks significantly.
Note that for the English?French task we used the small development set of 37
sentences to optimize feature weights, and ran our system on the original test set of
447 sentences. For the Romanian?English language pair, we follow Fraser and Marcu
(2006) in reducing the vocabulary by stemming Romanian and English words down to
their first four characters. For the other language pairs, English?Inuktitut and English?
Hindi, the symmetric linear model maintains its superiority over the asymmetric linear
model and yields better results than the other participants.
Figure 4
Effect of training corpus size on the Chinese?English task. We choose four feature groups with
varying numbers of features: 3, 6, 10, and 14. There are eight training corpora with varying
numbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502.
328
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 11
Comparison with the systems participating in the three shared tasks. ?non-null? denotes that the
reference alignments have no null links, ?null? denotes that the reference alignments have null
links, ?limited? denotes only limited resources can be used, and ?unlimited? denotes that there
are no restrictions on resources used.
Shared Task Task Participants Vigne
Romanian?English, non-null, limited 28.9?52.7 23.5
Romanian?English, null, limited 37.4?59.8 26.9HLT-NAACL 2003
English?French, non-null, limited 8.5?29.4 4.0
English?French, null, limited 18.5?51.7 4.6
English?Inuktitut, limited 9.5?71.3 8.9
ACL 2005 Romanian?English, limited 26.6?44.5 24.7
English?Hindi, limited 51.4 44.8
HTRDP 2005 Chinese?English, unlimited 23.5?49.2 14.3
4.1.8 Comparison to Other Work. In the word alignment literature, the Canadian Hansard
bilingual corpus is the most widely used data set. Table 12 lists alignment error rates
achieved by previous work and our system. Note that direct comparisons are problem-
atic due to the different configurations of training data, development data, and test data.
Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien
et al 2006; Moore, Yih, and Bode 2006).
4.2 Evaluation of Translation Quality
In this section, we report on experiments with Chinese-to-English translation. To inves-
tigate the effect of our discriminative model on translation performance, we used three
translation systems:
1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT
system;
2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system;
3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that
makes use of tree-to-string rules.
Table 12
Comparison of some word alignment systems on the Canadian Hansard data.
System Training Test AER
Och and Ney (2003) 1.5M 500 5.2
Moore (2005) 500K 223 7.5
Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4
Liang, Taskar, and Klein (2006) 1.1M 347 4.9
Lacoste-Julien et al (2006) 1.1M 247 3.8
Blunsom and Cohn (2006) 1.1M 347 5.2
Moore, Yih, and Bode (2006) 1.1M 223 3.7
This work 1.1M 247 3.6
329
Computational Linguistics Volume 36, Number 3
For all three systems we trained the translation models on the FBIS corpus
(7.2M+9.2M words). For the language model, we used the SRI Language Modeling
Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing
on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation
test set as the development set for training feature weights of translation systems, the
2005 test set as the devtest set for choosing optimal values of ? for different translation
systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive
BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference
length for brevity penalty.
We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines
(Melamed 1998). All links are sure ones. These hand-aligned sentences served as the
training corpus for Vigne. To train the feature weights in our discriminative model
using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser
and Marcu 2007b) as the optimization criterion.
The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We
used seven generative alignment methods based on IBM Model 4 and HMM as baseline
systems: (1) C?E, (2) E?C, (3) intersection, (4) union, (5) refined method (Och and
Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang,
Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model
only searches within the union of baseline predictions, which enables our system to
align large bilingual corpus at a very fast speed of 3, 000 words per second. In other
words, our system is able to annotate the FBIS corpus in about 1.5 hours. Then, we train
the feature weights of the linear model on the training corpus with respect to F-measure
under different settings of ?. After that, our system runs on the FBIS corpus to produce
word alignments using the optimized weights. Finally, the three SMT systems train their
models on the word-aligned FBIS corpus.
Can our approach achieve higher F-measure scores than generative methods with
different values of ? (the weighting factor in F-measure)? Table 13 shows the results of
all the systems on the development set. To estimate the loss from restricting the search
Table 13
Maximization of F-measure with different settings of ? (the weighting factor in the balanced
F-measure). We use IBM Model 4 and HMM as baseline systems. Our system restricts the search
space by exploring only the union of baseline predictions. We compute the ?oracle? alignments
by intersecting the union with reference alignments. We use ?+? to denote the result that
outperforms the best baseline result with statistical significance at p < 0.05. Similarly, we use
?++? to denote significantly better than baseline at p < 0.01.
? = 0.1 ? = 0.3 ? = 0.5 ? = 0.7 ? = 0.9
IBM Model 4 C?E 82.6 81.3 80.1 79.0 77.8
IBM Model 4 E?C 68.2 70.5 73.0 75.7 78.5
IBM Model 4 intersection 63.6 68.9 75.2 82.8 92.1
IBM Model 4 union 86.6 82.0 77.9 74.2 70.8
IBM Model 4 refined method 75.4 78.5 81.8 85.4 89.4
IBM Model 4 grow-diag-final 82.4 82.1 81.7 81.4 81.1
Cross-EM HMM 70.4 73.7 77.3 81.2 85.5
oracle 91.9 93.6 95.3 97.1 99.0
Vigne 87.8++ 85.8++ 86.4++ 88.6++ 93.3++
330
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
space, we compute oracle alignments by intersecting the union of baseline predictions
with reference alignments. The F-measures achieved by oracle alignments range from
91.9 to 99.0, indicating that the union of baseline predictions is good enough to approx-
imate the true search space. We observe that C?E, union, and grow-diag-final weight
recall higher because F-measure decreases when ? increases. On the other hand, E?C,
intersection, refined method, and Cross-EM weight precision higher. In particular, ?
has a weak effect on grow-diag-final as its F-measure always keeps above 0.8 when ?
is varied. For each ?, we trained a set of feature weights to maximize the F-measure on
the development set. We find that our discriminative model outperforms the baseline
systems significantly at all values of ?.
Table 14 shows the BLEU scores of the three systems on the devtest set. For Moses
and Hiero, we used the default setting. For Lynx, we used the phrase pairs learned by
Moses to improve rule coverage (Liu, Liu, and Lin 2006). The best generative alignment
method is grow-diag-final, which is widely used in SMT. For all the three SMT systems,
our system outperforms the baseline systems statistically significantly. For Moses, the
best value of ? is 0.5. For Hiero and Lynx, the best ? is 0.3, suggesting that recall-
oriented alignments yield better translation performance.
Table 15 gives the BLEU scores of the three systems on the final test set. We used the
parameters optimized on the dev and devtest sets. More specifically, Moses used grow-
diag-final and ? = 0.5, Hiero used grow-diag-final and ? = 0.3, and Lynx used union
and ? = 0.3. We find that our discriminative alignment model improves the three
systems significantly.
5. Related Work
The first generative alignment models were the IBM Models 1?5 proposed by Brown
et al (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM)
for word alignment. They show that it is beneficial to make the alignment probabilities
dependent on differences in position rather than on the absolute positions. Och and
Table 14
BLEU scores on the devtest set. We use ?+? to denote the result that outperforms the best
baseline result (highlighted in bold) statistically significantly at p < 0.05. Similarly, we use ?++?
to denote significantly better than baseline at p < 0.01.
Moses Hiero Lynx
IBM Model 4 C?E 24.7 25.7 24.8
IBM Model 4 E?C 20.6 23.5 21.6
IBM Model 4 intersection 20.1 23.2 21.2
IBM Model 4 union 24.3 24.1 25.1
IBM Model 4 refined method 24.2 24.0 24.2
IBM Model 4 grow-diag-final 25.0 25.8 24.3
Cross-EM HMM 23.6 24.9 24.8
? = 0.1 tuned 23.9 25.3 26.0++
? = 0.3 tuned 24.9 26.8++ 26.1++
Vigne ? = 0.5 tuned 25.7+ 26.6++ 24.3
? = 0.7 tuned 23.7 25.4 24.7
? = 0.9 tuned 21.9 24.7 23.9
331
Computational Linguistics Volume 36, Number 3
Table 15
BLEU scores on the final test set. We use the parameters optimized on the dev and devtest sets.
We use ?+? to denote the result that outperforms the best baseline result (indicated in bold)
statistically significantly at p < 0.05. Similarly, we use ?++? to denote significantly better than
baseline at p < 0.01.
Moses Hiero Lynx
generative 20.1 20.7 19.9
discriminative 20.8+ 21.6+ 21.0++
Ney (2003) re-implement the IBM models and the HMM model and compare them with
heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz
J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006)
present an unsupervised way to produce symmetric alignments by training two simple
asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a
combination of data likelihood and agreement between the models. Fraser and Marcu
(2007a) introduce a new generative model called LEAF that directly models many-
to-many non-consecutive word alignments. Their model can be trained using both
unsupervised and semi-supervised training methods.
Recent years have witnessed the rapid development of discriminative alignment
methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a
log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003)
develop a statistical model to find word alignments, which allows for easy integration
of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used
in SMT (Och and Ney 2002) to word alignment and report significant improvements
over the IBM models. Moore (2005) presents a discriminative framework for word
alignment and uses averaged perceptron for parameter optimization. Taskar, Lacoste-
Julien, and Klein (2005) treat the alignment prediction task as a maximum weight
bipartite matching problem and use the large-margin method to train feature weights.
Neural networks and transformation-based learning have also been introduced to word
alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a
new discriminative model based on conditional random fields (CRF). Fraser and Marcu
(2006) use sub-models of IBM Model 4 as features and train feature weights using a
semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to
combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic
constraints through discriminative training can improve alignment quality. Lacoste-
Julien et al (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and
Klein (2005) by including fertility and first-order interactions. Recently, max-product
belief propagation has been successfully applied to discriminative word alignment
(Niehues and Vogel 2008; Cromiere`s and Kurohashi 2009). Haghighi et al (2009) investi-
gate supervised word alignment methods that exploit inversion transduction grammar
(ITG) constraints.
Our work can be seen as an application of the linear model (Och 2003) in word
alignment. While aiming at producing symmetric word alignments in a discriminative
way, our approach uses asymmetric generative models (Brown et al 1993) as the major
information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006).
They train two linear models called stage 1 and stage 2. The feature values are extracted
from word-aligned sentence pairs. After the stage 1 model aligns the entire training
corpus automatically, the stage 2 model uses features based not only on the parallel
332
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
sentences themselves but also on statistics of the alignments produced by the stage 1
model. They use average perceptron and support vector machine (SVM) to train feature
weights and use a beam search algorithm to find the most probable alignments. Table 12
shows that the two methods achieve comparable results on the Hansard data, confirm-
ing Moore, Yih, and Bode?s (2006) claim that model structure and feature selection are
more important than discriminative training method.
6. Conclusions and Future Work
We have presented a discriminative framework for word alignment based on the linear
modeling approach. This framework is easy to extend by including features that char-
acterize the aligning process. In addition, our approach supports symmetric alignment
modeling that allows for an arbitrary relationship between source and target language
positions. As the linear model offers excellent flexibility in using a large variety of
features and in combining information from various sources, it is able to produce good
predictions on language pairs that are either closely related (e.g., English?French) or dis-
tantly related (e.g., English?Inuktitut), either with rich resources (e.g., Chinese?English)
or with scarce resources (e.g., English?Hindi). We further show that our approach can
benefit different types of SMT systems: phrase-based, hierarchical phrase-based, and
syntax-based.
The real benefit of our model does not stem from the use of the linear model, but
rather from the discriminative training that optimizes feature weights with respect to
evaluation metrics on the gold-standard word alignments. One disadvantage of our
approach is the need for annotated training data. Although we have shown that a
very small number of training examples would be enough for parameter estimation
(Section 4.1.6), it is difficult to select such a representative training corpus to ensure that
the model will work well on unseen data, especially when the bilingual corpus to be
aligned consists of parallel texts from different domains.
Another problem is that it is hard to find an evaluation metric for word alignment
that correlates well with translation quality because the relationship between alignment
and translation is still not quite clear. Without a good loss function, discriminative
models cannot outperform generative models in large-scale applications. Therefore, it is
important to investigate how to select training examples and how to choose optimiza-
tion criterion.
The design of feature functions is most important for a discriminative alignment
model. Often, we need to try various feature groups manually on the development set
to determine the optimal feature group. Furthermore, a feature group optimized for one
language pair may not have the same effect on another one. In the future, we plan to
investigate an algorithm for automatic feature selection.
Appendix A: Table of Notation
f source sentence
fS1 sequence of source sentences: f1, . . . , fs, . . . , fS
f source word
J length of f
j position in f, j = 1, 2, . . . , J
fj the j-th word in f
f0 empty cept on the source side
333
Computational Linguistics Volume 36, Number 3
e target sentence
eS1 sequence of target sentences: e1, . . . , es, . . . , eS
e target word
I length of e
i position in e, i = 1, 2, . . . , I
ei the i-th word in e
e0 empty cept on the target side
a alignment
l a link ( j, i) in a
?j number of positions of e connected to position j of f
?i number of positions of f connected to position i of e
?j,k position of the k-th target word aligned to fj
?i,k position of the k-th source word aligned to ei
?( j, ?j) sum of sibling distances for fj
?(i, ?i) sum of sibling distances for ei
score(f, e, a) a score that indicates how well a is the alignment between f and e
a? the best candidate alignment
? feature weight
? the feature weight being optimized
h(f, e, a) feature function
G(f, e, a, l) link gain after adding l to a
g(f, e, a, l) feature gain after adding l to a
?(f, e, a) value of the feature being optimized
?(f, e, a) dot-product of fixed features
t(e| f ) the probability that f is translated to e
t( f |e) the probability that e is translated to f
n(?| f ) the probability that f has a fertility of ?
n(?|e) the probability that e has a fertility of ?
r reference alignment
Cs set of candidate alignments for the s-th training example
as,k the k-th candidate alignment for the s-th training example
E(r, a) loss function that measures alignment quality
? the precision/recall weighting factor in balanced F-measure
? pruning threshold in the beam search algorithm
b beam size in the beam search algorithm
?(x, y) the Kronecker function, which is 1 if x = y and 0 otherwise
expr an indicator function taking a boolean expression expr as the argument
Appendix B: Using the IBM Models as Feature Functions
In this article, we use IBM Models 1?4 as feature functions by taking the logarithm of the
models themselves rather than the sub-models just for simplicity. It is easy to separate
each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish
334
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
between two translation directions (i.e., source-to-target and target-to-source) to use the
IBM models as feature functions. All model parameters are estimated by GIZA++ (Och
and Ney 2003).
The feature function for the IBM Model 1 is
hm1 (f, e, a) = log
(
( J|I)
(I + 1)J
J
?
j=1
t( fj|eaj )
)
(B.1)
where ( J|I) predicts the length of the source sentence conditioned on that of the target
sentence, (I + 1)?J defines a uniform distribution of the alignment between source and
target words, and t( fj|ei) is a translation sub-model. Note that aj = i, which means that
fj is connected to ei.
The corresponding feature gain is
gm1 (f, e, a, l) = log
(
t( fj|ei)) ? log(t( fj|e0)
)
(B.2)
where fj and ei are linked by l and e0 is the empty cept to which all unaligned source
words are ?aligned.?
Based on a similar generative story to Model 1, Model 2 replaces the uniform
alignment probability distribution with an alignment sub-model a(i| j, I, J). This sub-
model assumes that the position of ei depends on the position of its translation fj and
sentence lengths I and J.
The feature function for Model 2 is
hm2 (f, e, a) = log
(
( J|I)
J
?
j=1
t( fj|eaj )a(aj| j, I, J)
)
(B.3)
The corresponding feature gain is
gm2 (f, e, a, l) = log(t( fj|ei)) ? log(t( fj|e0)) +
log(a(i| j, I, J)) ? log(a(0| j, I, J)) (B.4)
where fj and ei are linked by l and 0 is the index of the empty cept e0.
Model 3 is a fertility-based model that parameterizes fertility of words. Unlike
Model 2, Model 3 uses a fertility sub-model n(?i|ei) and a distortion sub-model d( j|i, I, J).
Formally, the feature function of Model 3 is given by
hm3 (a, f, e) = log
(
n0
(
?0|
I
?
i=1
?i
)
I
?
i=1
n(?i|ei)?i!
J
?
j=1
t( fj|eaj )
?
?
j:aj =0
d( j|i, I, J)
)
(B.5)
Brown et al (1993) treat n0(?0|
?I
i=1 ?i), the fertility probability of e0, in a differ-
ent way. They assume that at most half of the source words in an alignment are not
335
Computational Linguistics Volume 36, Number 3
aligned (i.e., ?0 ? J2 ) and define a binomial distribution relying on an auxiliary parame-
ter p0:
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0 if ?0 ?
J
2
0 otherwise
(B.6)
Note that we follow Brown et al (1993) in replacing
?I
i=1 ?i with J ? ?0 for simplicity.
The original form should be (
?I
i=1?i
?0
)
p
?I
i=1?i??0
0 (1 ? p0 )?0 .
However, this assumption results in a problem for our search algorithm that begins
with an empty alignment (see Algorithm 1), for which ?0 is J and the feature value
hm3 (f, e, a) is negative infinity. To alleviate this problem, we modify Equation B.6 slightly
by adding a smoothing parameter pn ? (0, 1):
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0pn if ?0 ?
J
2
1?pn
 J2 
otherwise
(B.7)
Therefore, the feature gain for Model 3 is
gm3 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(?i + 1) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(d( j|i, I, J)) (B.8)
where fj and ei are linked by l, ?i is the fertility before adding l, and gn0 ( J, ?0) is the gain
for n0(?0|
?I
i=1 ?i):
gn0 ( J, ?0) =
?
?
?
?
?
?
?
?0?( J??0+1)
( J?2?0+1)?( J?2?0+2) if ?0 ?
J
2
1?pn
(J??0+1?0?1 )?p
J?2?0+2
0 ?(1?p0 )?0?1?pn?
J
2 
J
2 < ?0 ?
J
2 + 1
1 otherwise
(B.9)
Model 4 defines a new distortion sub-model D(a) that relies on word classes A and
B to capture movement of phrases. The feature function for Model 4 is
hm4 (a, f, e) = log
(
n0(?0|
I
?
i=1
?i)
I
?
i=1
n(?i|ei)
J
?
j=1
t( fj|eaj )
1
?0!
D(a)
)
(B.10)
where
D(a) =
I
?
i=1
?i
?
k=1
pik(?ik) (B.11)
336
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
pik( j) =
{
d1( j ? c?i |A(e?i ),B(?i1)) if k = 1
d>1( j? ?i,k?1|B(?ik)) otherwise
(B.12)
Brown et al (1993) propose two distortion models for Model 4: d1(?) for the first
word of a tablet ? and d>1(?) for the other words of the tablet. In Equation B.12, ?i is
the first position to the left of i for which ?i > 0, c?i is the ceiling of the average position
of the words in ??, ?ik denotes the k-th French word aligned to ek, ?i,k?1 denotes the
position of the k ? 1-th French word aligned to ei, and A(?) and B(?) are word classes
for the source and target languages, respectively. Please refer to Brown et al (1993) for
more details.
The corresponding feature gain is
gm4 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(?0) +
log(D(a ? {l})) ? log(D(a)) (B.13)
where fj and ei are linked by l and ?i is the fertility before adding l.
In Model 4, the addition of a single link might change the distortion probabilities
pik( j) of other links. As a result, we have to compute the overall distortion probabilities
D(a) every time.
Acknowledgments
This work was supported by National
Natural Science Foundation of China,
Contract No. 60603095 and 60573188.
Thanks to the three anonymous reviewers
for their insightful and constructive
comments and suggestions. We are
grateful to Rada Mihalcea for giving us
the Romanian?English training data and
David Chiang for allowing us to use Hiero.
Stephan Vogel, Vamshi Ambati, and
Kelly Widmaier offered valuable feedback
on an earlier version of this article.
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006a.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of COLING-ACL 2006,
pages 9?16, Sydney.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006b.
A maximum entropy approach to
combining word alignments. In Proceedings
of HLT-NAACL 2006, pages 96?103, New
York, NY.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005a. Alignment link
projection using transformation-based
learning. In Proceedings of HLT-EMNLP
2005, pages 185?192, Vancouver.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005b. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP
2005, pages 65?72, Vancouver.
Blunsom, Phil and Trevor Cohn. 2006.
Discriminative word alignment with
conditional random fields. In Proceedings
of COLING-ACL 2006, pages 65?72,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL 2003,
pages 88?95, Sapporo.
Cherry, Colin and Dekang Lin. 2006. Soft
syntactic constraints for word alignment
through discriminative training. In
Proceedings of COLING-ACL 2006 (poster),
pages 105?112, Sydney.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
337
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of ACL 2005,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cromiere`s, Fabien and Sadao Kurohashi.
2009. An alignment algorithm using belief
propagation and a structure-based
distortion model. In Proceedings of EACL
2009, pages 166?174, Athens.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for statistical
word alignment. In Proceedings of
COLING-ACL 2006, pages 769?776,
Sydney.
Fraser, Alexander and Daniel Marcu. 2007a.
Getting the structure right for word
alignment: LEAF. In Proceedings of
EMNLP-CoNLL 2007, pages 51?60, Prague.
Fraser, Alexander and Daniel Marcu. 2007b.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe, Wei
Wang, and Ignacio Thayer. 2006. Scalable
inference and training of context-rich
syntactic translation models. In Proceedings
of COLING-ACL 2006, pages 961?968,
Sydney.
Haghighi, Aria, John Blitzer, John DeNero,
and Dan Klein. 2009. Better word
alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP 2009,
pages 923?931, Suntec.
He, Xiaodong, Mei Yang, Jianfeng Gao,
Patrick Nguyen, and Robert Moore. 2008.
Indirect-HMM-based hypothesis
alignment for combining outputs from
machine translation systems. In
Proceedings of EMNLP 2008, pages 98?107,
Honolulu, HI.
Koehn, Philipp and Hieu Hoang. 2007.
Factored translation models. In Proceedings
of EMNLP-CoNLL 2007, pages 868?876,
Prague.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL
2003, pages 127?133, Edmonton.
Lacoste-Julien, Simon, Ben Taskar, Dan Klein,
and Michael I. Jordan. 2006. Word
alignment via quadratic assignment.
In Proceedings of HLT-NAACL 2007,
pages 112?119, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of HLT-NAACL 2006,
pages 104?111, New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005.
Log-linear models for word alignment. In
Proceedings of ACL 2005, pages 459?466,
Ann Arbor, MI.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of COLING-ACL 2006,
pages 609?616, Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52,
Sydney.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL 2005 Workshop on
Building and Using Parallel Texts,
pages 65?74, Ann Arbor, MI.
Melamed, I. Dan. 1998. Annotation style
guide for the blinker project. Technical
report No. 98-06, University of
Pennsylvania, Philadelphia.
Melamed, I. Dan. 2000. Models for
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Mihalcea, Rada and Ted Pedersen. 2003.
An evaluation exercise for word
alignment. In Proceedings of HLT-NAACL
2003 Workshop on Building and Using
Parallel Texts, pages 1?10, Edmonton.
Moore, Robert C. 2005. A discriminative
framework for bilingual word alignment.
In Proceedings of HLT-EMNLP 2005,
pages 81?88, Vancouver.
Moore, Robert C., Wen-tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL 2006, pages 513?520,
Sydney.
Niehues, Jan and Stephan Vogel. 2008.
Discriminative word alignment via
alignment matrix modeling. In
Proceedings of the Third Workshop on
Statistical Machine Translation, pages 18?25,
Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL 2003, pages 160?167,
Sapporo.
Och, Franz J. and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of ACL 2002,
pages 295?302, Philadephia, PA.
Och, Franz J. and Hermann Ney. 2003. A
systematic comparison of various
338
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz J. and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics,
30(4):417?449.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005,
pages 271?279, Ann Arbor, MI.
Rosti, Antti-Veikko, Spyros Matsoukas, and
Richard Schwartz. 2007. Improved
word-level system combination for
machine translation. In Proceedings of ACL
2007, pages 312?319, Prague.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of ICSLP 2002, pages 901?904,
Denver, CO.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of HLT-EMNLP 2005,
pages 73?80, Vancouver.
Vogel, Stephan and Hermann Ney. 1996.
HMM-based word alignment in statistical
translation. In Proceedings of COLING 1996,
pages 836?841, Copenhagen.
339

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 12?20,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Dependency Parsing and Projection Based on Word-Pair Classification
Wenbin Jiang and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jiangwenbin, liuqun}@ict.ac.cn
Abstract
In this paper we describe an intuitionistic
method for dependency parsing, where a
classifier is used to determine whether a
pair of words forms a dependency edge.
And we also propose an effective strategy
for dependency projection, where the de-
pendency relationships of the word pairs
in the source language are projected to the
word pairs of the target language, leading
to a set of classification instances rather
than a complete tree. Experiments show
that, the classifier trained on the projected
classification instances significantly out-
performs previous projected dependency
parsers. More importantly, when this clas-
sifier is integrated into a maximum span-
ning tree (MST) dependency parser, ob-
vious improvement is obtained over the
MST baseline.
1 Introduction
Supervised dependency parsing achieves the state-
of-the-art in recent years (McDonald et al, 2005a;
McDonald and Pereira, 2006; Nivre et al, 2006).
Since it is costly and difficult to build human-
annotated treebanks, a lot of works have also been
devoted to the utilization of unannotated text. For
example, the unsupervised dependency parsing
(Klein and Manning, 2004) which is totally based
on unannotated data, and the semisupervised de-
pendency parsing (Koo et al, 2008) which is
based on both annotated and unannotated data.
Considering the higher complexity and lower per-
formance in unsupervised parsing, and the need of
reliable priori knowledge in semisupervised pars-
ing, it is a promising strategy to project the de-
pendency structures from a resource-rich language
to a resource-scarce one across a bilingual corpus
(Hwa et al, 2002; Hwa et al, 2005; Ganchev et al,
2009; Smith and Eisner, 2009; Jiang et al, 2009).
For dependency projection, the relationship be-
tween words in the parsed sentences can be sim-
ply projected across the word alignment to words
in the unparsed sentences, according to the DCA
assumption (Hwa et al, 2005). Such a projec-
tion procedure suffers much from the word align-
ment errors and syntactic isomerism between lan-
guages, which usually lead to relationship projec-
tion conflict and incomplete projected dependency
structures. To tackle this problem, Hwa et al
(2005) use some filtering rules to reduce noise,
and some hand-designed rules to handle language
heterogeneity. Smith and Eisner (2009) perform
dependency projection and annotation adaptation
with quasi-synchronous grammar features. Jiang
and Liu (2009) resort to a dynamic programming
procedure to search for a completed projected tree.
However, these strategies are all confined to the
same category that dependency projection must
produce completed projected trees. Because of the
free translation, the syntactic isomerism between
languages and word alignment errors, it would
be strained to completely project the dependency
structure from one language to another.
We propose an effective method for depen-
dency projection, which does not have to pro-
duce complete projected trees. Given a word-
aligned bilingual corpus with source language sen-
tences parsed, the dependency relationships of the
word pairs in the source language are projected to
the word pairs of the target language. A depen-
dency relationship is a boolean value that repre-
sents whether this word pair forms a dependency
edge. Thus a set of classification instances are ob-
tained. Meanwhile, we propose an intuitionistic
model for dependency parsing, which uses a clas-
sifier to determine whether a pair of words form
a dependency edge. The classifier can then be
trained on the projected classification instance set,
so as to build a projected dependency parser with-
out the need of complete projected trees.
12
ij j
i
Figure 1: Illegal (a) and incomplete (b) dependency tree produced by the simple-collection method.
Experimental results show that, the classifier
trained on the projected classification instances
significantly outperforms the projected depen-
dency parsers in previous works. The classifier
trained on the Chinese projected classification in-
stances achieves a precision of 58.59% on the CTB
standard test set. More importantly, when this
classifier is integrated into a 2nd-ordered max-
imum spanning tree (MST) dependency parser
(McDonald and Pereira, 2006) in a weighted aver-
age manner, significant improvement is obtained
over the MST baselines. For the 2nd-order MST
parser trained on Penn Chinese Treebank (CTB)
5.0, the classifier give an precision increment of
0.5 points. Especially for the parser trained on the
smaller CTB 1.0, more than 1 points precision in-
crement is obtained.
In the rest of this paper, we first describe
the word-pair classification model for dependency
parsing (section 2) and the generation method
of projected classification instances (section 3).
Then we describe an application of the projected
parser: boosting a state-of-the-art 2nd-ordered
MST parser (section 4). After the comparisons
with previous works on dependency parsing and
projection, we finally five the experimental results.
2 Word-Pair Classification Model
2.1 Model Definition
Following (McDonald et al, 2005a), x is used to
denote the sentence to be parsed, and xi to denote
the i-th word in the sentence. y denotes the de-
pendency tree for sentence x, and (i, j) ? y rep-
resents a dependency edge from word xi to word
xj , where xi is the parent of xj .
The task of the word-pair classification model
is to determine whether any candidate word pair,
xi and xj s.t. 1 ? i, j ? |x| and i 6= j, forms a
dependency edge. The classification result C(i, j)
can be a boolean value:
C(i, j) = p p ? {0, 1} (1)
as produced by a support vector machine (SVM)
classifier (Vapnik, 1998). p = 1 indicates that the
classifier supports the candidate edge (i, j), and
p = 0 the contrary. C(i, j) can also be a real-
valued probability:
C(i, j) = p 0 ? p ? 1 (2)
as produced by an maximum entropy (ME) classi-
fier (Berger et al, 1996). p is a probability which
indicates the degree the classifier support the can-
didate edge (i, j). Ideally, given the classifica-
tion results for all candidate word pairs, the depen-
dency parse tree can be composed of the candidate
edges with higher score (1 for the boolean-valued
classifier, and large p for the real-valued classi-
fier). However, more robust strategies should be
investigated since the ambiguity of the language
syntax and the classification errors usually lead to
illegal or incomplete parsing result, as shown in
Figure 1.
Follow the edge based factorization method
(Eisner, 1996), we factorize the score of a de-
pendency tree s(x,y) into its dependency edges,
and design a dynamic programming algorithm
to search for the candidate parse with maximum
score. This strategy alleviate the classification er-
rors to some degree and ensure a valid, complete
dependency parsing tree. If a boolean-valued clas-
sifier is used, the search algorithm can be formal-
ized as:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j) (3)
And if a probability-valued classifier is used in-
stead, we replace the accumulation with cumula-
13
Type Features
Unigram wordi ? posi wordi posi
wordj ? posj wordj posj
Bigram wordi ? posi ? wordj ? posj posi ? wordj ? posj wordi ? wordj ? posj
wordi ? posi ? posj wordi ? posi ? wordj wordi ? wordj
posi ? posj wordi ? posj posi ? wordj
Surrounding posi ? posi+1 ? posj?1 ? posj posi?1 ? posi ? posj?1 ? posj posi ? posi+1 ? posj ? posj+1
posi?1 ? posi ? posj ? posj+1 posi?1 ? posi ? posj?1 posi?1 ? posi ? posj+1
posi ? posi+1 ? posj?1 posi ? posi+1 ? posj+1 posi?1 ? posj?1 ? posj
posi?1 ? posj ? posj+1 posi+1 ? posj?1 ? posj posi+1 ? posj ? posj+1
posi ? posj?1 ? posj posi ? posj ? posj+1 posi?1 ? posi ? posj
posi ? posi+1 ? posj
Table 1: Feature templates for the word-pair classification model.
tive product:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j) (4)
Where y is searched from the set of well-formed
dependency trees.
In our work we choose a real-valued ME clas-
sifier. Here we give the calculation of dependency
probability C(i, j). We use w to denote the param-
eter vector of the ME model, and f(i, j, r) to de-
note the feature vector for the assumption that the
word pair i and j has a dependency relationship r.
The symbol r indicates the supposed classification
result, where r = + means we suppose it as a de-
pendency edge and r = ? means the contrary. A
feature fk(i, j, r) ? f(i, j, r) equals 1 if it is ac-
tivated by the assumption and equals 0 otherwise.
The dependency probability can then be defined
as:
C(i, j) = exp(w ? f(i, j,+))?
r exp(w ? f(i, j, r))
=
exp(
?
k wk ? fk(i, j,+))
?
r exp(
?
k wk ? fk(i, j, r))
(5)
2.2 Features for Classification
The feature templates for the classifier are simi-
lar to those of 1st-ordered MST model (McDon-
ald et al, 2005a). 1 Each feature is composed
of some words and POS tags surrounded word i
and/or word j, as well as an optional distance rep-
resentations between this two words. Table shows
the feature templates we use.
Previous graph-based dependency models usu-
ally use the index distance of word i and word j
1We exclude the in between features of McDonald et al
(2005a) since preliminary experiments show that these fea-
tures bring no improvement to the word-pair classification
model.
to enrich the features with word distance infor-
mation. However, in order to utilize some syntax
information between the pair of words, we adopt
the syntactic distance representation of (Collins,
1996), named Collins distance for convenience. A
Collins distance comprises the answers of 6 ques-
tions:
? Does word i precede or follow word j?
? Are word i and word j adjacent?
? Is there a verb between word i and word j?
? Are there 0, 1, 2 or more than 2 commas be-
tween word i and word j?
? Is there a comma immediately following the
first of word i and word j?
? Is there a comma immediately preceding the
second of word i and word j?
Besides the original features generated according
to the templates in Table 1, the enhanced features
with Collins distance as postfixes are also used in
training and decoding of the word-pair classifier.
2.3 Parsing Algorithm
We adopt logarithmic dependency probabilities
in decoding, therefore the cumulative product of
probabilities in formula 6 can be replaced by ac-
cumulation of logarithmic probabilities:
y? = argmax
y
s(x,y)
= argmax
y
?
(i,j)?y
C(i, j)
= argmax
y
?
(i,j)?y
log(C(i, j))
(6)
Thus, the decoding algorithm for 1st-ordered MST
model, such as the Chu-Liu-Edmonds algorithm
14
Algorithm 1 Dependency Parsing Algorithm.
1: Input: sentence x to be parsed
2: for ?i, j? ? ?1, |x|? in topological order do
3: buf ? ?
4: for k ? i..j ? 1 do ? all partitions
5: for l ? V[i, k] and r ? V[k + 1, j] do
6: insert DERIV(l, r) into buf
7: insert DERIV(r, l) into buf
8: V[i, j]? top K derivations of buf
9: Output: the best derivation of V[1, |x|]
10: function DERIV(p, c)
11: d? p ? c ? {(p ? root, c ? root)} ? new derivation
12: d ? evl? EVAL(d) ? evaluation function
13: return d
used in McDonald et al (2005b), is also appli-
cable here. In this work, however, we still adopt
the more general, bottom-up dynamic program-
ming algorithm Algorithm 1 in order to facilitate
the possible expansions. Here, V[i, j] contains the
candidate parsing segments of the span [i, j], and
the function EVAL(d) accumulates the scores of
all the edges in dependency segment d. In prac-
tice, the cube-pruning strategy (Huang and Chi-
ang, 2005) is used to speed up the enumeration of
derivations (loops started by line 4 and 5).
3 Projected Classification Instance
After the introduction of the word-pair classifica-
tion model, we now describe the extraction of pro-
jected dependency instances. In order to allevi-
ate the effect of word alignment errors, we base
the projection on the alignment matrix, a compact
representation of multiple GIZA++ (Och and Ney,
2000) results, rather than a single word alignment
in previous dependency projection works. Figure
2 shows an example.
Suppose a bilingual sentence pair, composed of
a source sentence e and its target translation f . ye
is the parse tree of the source sentence. A is the
alignment matrix between them, and each element
Ai,j denotes the degree of the alignment between
word ei and word fj . We define a boolean-valued
function ?(y, i, j, r) to investigate the dependency
relationship of word i and word j in parse tree y:
?(y, i, j, r) =
?
?
?
?
?
?
?
?
?
?
?
1
(i, j) ? y and r = +
or
(i, j) /? y and r = ?
0 otherwise
(7)
Then the score that word i and word j in the target
sentence y forms a projected dependency edge,
Figure 2: The word alignment matrix between a
Chinese sentence and its English translation. Note
that probabilities need not to be normalized across
rows or columns.
s+(i, j), can be defined as:
s+(i, j) =
?
i?,j?
Ai,i? ? Aj,j? ? ?(ye, i?, j?,+) (8)
The score that they do not form a projected depen-
dency edge can be defined similarly:
s?(i, j) =
?
i?,j?
Ai,i? ? Aj,j? ? ?(ye, i?, j?,?) (9)
Note that for simplicity, the condition factors ye
and A are omitted from these two formulas. We
finally define the probability of the supposed pro-
jected dependency edge as:
Cp(i, j) =
exp(s+(i, j))
exp(s+(i, j)) + exp(s?(i, j))
(10)
The probability Cp(i, j) is a real value between
0 and 1. Obviously, Cp(i, j) = 0.5 indicates the
most ambiguous case, where we can not distin-
guish between positive and negative at all. On the
other hand, there are as many as 2|f |(|f |?1) candi-
date projected dependency instances for the target
sentence f . Therefore, we need choose a threshold
b for Cp(i, j) to filter out the ambiguous instances:
the instances with Cp(i, j) > b are selected as the
positive, and the instances with Cp(i, j) < 1 ? b
are selected as the negative.
4 Boosting an MST Parser
The classifier can be used to boost a existing parser
trained on human-annotated trees. We first estab-
lish a unified framework for the enhanced parser.
For a sentence to be parsed, x, the enhanced parser
selects the best parse y? according to both the base-
line model B and the projected classifier C.
y? = argmax
y
[sB(x,y) + ?sC(x,y)] (11)
15
Here, sB and sC denote the evaluation functions
of the baseline model and the projected classi-
fier, respectively. The parameter ? is the relative
weight of the projected classifier against the base-
line model.
There are several strategies to integrate the two
evaluation functions. For example, they can be in-
tegrated deeply at each decoding step (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated shallowly in a reranking man-
ner (Collins, 2000; Charniak and Johnson, 2005).
As described previously, the score of a depen-
dency tree given by a word-pair classifier can be
factored into each candidate dependency edge in
this tree. Therefore, the projected classifier can
be integrated with a baseline model deeply at each
dependency edge, if the evaluation score given by
the baseline model can also be factored into de-
pendency edges.
We choose the 2nd-ordered MST model (Mc-
Donald and Pereira, 2006) as the baseline. Es-
pecially, the effect of the Collins distance in the
baseline model is also investigated. The relative
weight ? is adjusted to maximize the performance
on the development set, using an algorithm similar
to minimum error-rate training (Och, 2003).
5 Related Works
5.1 Dependency Parsing
Both the graph-based (McDonald et al, 2005a;
McDonald and Pereira, 2006; Carreras et al,
2006) and the transition-based (Yamada and Mat-
sumoto, 2003; Nivre et al, 2006) parsing algo-
rithms are related to our word-pair classification
model.
Similar to the graph-based method, our model
is factored on dependency edges, and its decod-
ing procedure also aims to find a maximum span-
ning tree in a fully connected directed graph. From
this point, our model can be classified into the
graph-based category. On the training method,
however, our model obviously differs from other
graph-based models, that we only need a set of
word-pair dependency instances rather than a reg-
ular dependency treebank. Therefore, our model is
more suitable for the partially bracketed or noisy
training corpus.
The most apparent similarity between our
model and the transition-based category is that
they all need a classifier to perform classification
conditioned on a certain configuration. However,
they differ from each other in the classification re-
sults. The classifier in our model predicates a de-
pendency probability for each pair of words, while
the classifier in a transition-based model gives a
possible next transition operation such as shift or
reduce. Another difference lies in the factoriza-
tion strategy. For our method, the evaluation score
of a candidate parse is factorized into each depen-
dency edge, while for the transition-based models,
the score is factorized into each transition opera-
tion.
Thanks to the reminding of the third reviewer
of our paper, we find that the pairwise classifica-
tion schema has also been used in Japanese de-
pendency parsing (Uchimoto et al, 1999; Kudo
and Matsumoto, 2000). However, our work shows
more advantage in feature engineering, model
training and decoding algorithm.
5.2 Dependency Projection
Many works try to learn parsing knowledge from
bilingual corpora. Lu? et al (2002) aims to
obtain Chinese bracketing knowledge via ITG
(Wu, 1997) alignment. Hwa et al (2005) and
Ganchev et al (2009) induce dependency gram-
mar via projection from aligned bilingual cor-
pora, and use some thresholds to filter out noise
and some hand-written rules to handle heterogene-
ity. Smith and Eisner (2009) perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar features. Jiang and
Liu (2009) refer to alignment matrix and a dy-
namic programming search algorithm to obtain
better projected dependency trees.
All previous works for dependency projection
(Hwa et al, 2005; Ganchev et al, 2009; Smith and
Eisner, 2009; Jiang and Liu, 2009) need complete
projected trees to train the projected parsers. Be-
cause of the free translation, the word alignment
errors, and the heterogeneity between two lan-
guages, it is reluctant and less effective to project
the dependency tree completely to the target lan-
guage sentence. On the contrary, our dependency
projection strategy prefer to extract a set of depen-
dency instances, which coincides our model?s de-
mand for training corpus. An obvious advantage
of this strategy is that, we can select an appropriate
filtering threshold to obtain dependency instances
of good quality.
In addition, our word-pair classification model
can be integrated deeply into a state-of-the-art
MST dependency model. Since both of them are
16
Corpus Train Dev Test
WSJ (section) 2-21 22 23
CTB 5.0 (chapter) others 301-325 271-300
Table 2: The corpus partition for WSJ and CTB
5.0.
factorized into dependency edges, the integration
can be conducted at each dependency edge, by
weightedly averaging their evaluation scores for
this dependency edge. This strategy makes better
use of the projected parser while with faster de-
coding, compared with the cascaded approach of
Jiang and Liu (2009).
6 Experiments
In this section, we first validate the word-pair
classification model by experimenting on human-
annotated treebanks. Then we investigate the ef-
fectiveness of the dependency projection by eval-
uating the projected classifiers trained on the pro-
jected classification instances. Finally, we re-
port the performance of the integrated dependency
parser which integrates the projected classifier and
the 2nd-ordered MST dependency parser. We
evaluate the parsing accuracy by the precision of
lexical heads, which is the percentage of the words
that have found their correct parents.
6.1 Word-Pair Classification Model
We experiment on two popular treebanks, the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al, 1993), and the Penn Chi-
nese Treebank (CTB) 5.0 (Xue et al, 2005). The
constituent trees in the two treebanks are trans-
formed to dependency trees according to the head-
finding rules of Yamada and Matsumoto (2003).
For English, we use the automatically-assigned
POS tags produced by an implementation of the
POS tagger of Collins (2002). While for Chinese,
we just use the gold-standard POS tags following
the tradition. Each treebank is splitted into three
partitions, for training, development and testing,
respectively, as shown in Table 2.
For a dependency tree with n words, only n ?
1 positive dependency instances can be extracted.
They account for only a small proportion of all the
dependency instances. As we know, it is important
to balance the proportions of the positive and the
negative instances for a batched-trained classifier.
We define a new parameter r to denote the ratio of
the negative instances relative to the positive ones.
 84
 84.5
 85
 85.5
 86
 86.5
 87
 1  1.5  2  2.5  3
De
pe
nd
en
cy
 P
re
cis
ion
 (%
)
Ratio r (#negative/#positive)
WSJ
CTB 5.0
Figure 3: Performance curves of the word-pair
classification model on the development sets of
WSJ and CTB 5.0, with respect to a series of ratio
r.
Corpus System P %
WSJ Yamada and Matsumoto (2003) 90.3
Nivre and Scholz (2004) 87.3
1st-ordered MST 90.7
2nd-ordered MST 91.5
our model 86.8
CTB 5.0 1st-ordered MST 86.53
2nd-ordered MST 87.15
our model 82.06
Table 3: Performance of the word-pair classifica-
tion model on WSJ and CTB 5.0, compared with
the current state-of-the-art models.
For example, r = 2 means we reserve negative
instances two times as many as the positive ones.
The MaxEnt toolkit by Zhang 2 is adopted to
train the ME classifier on extracted instances. We
set the gaussian prior as 1.0 and the iteration limit
as 100, leaving other parameters as default values.
We first investigate the impact of the ratio r on
the performance of the classifier. Curves in Fig-
ure 3 show the performance of the English and
Chinese parsers, each of which is trained on an in-
stance set corresponding to a certain r. We find
that for both English and Chinese, maximum per-
formance is achieved at about r = 2.5. 3 The
English and Chinese classifiers trained on the in-
stance sets with r = 2.5 are used in the final eval-
uation phase. Table 3 shows the performances on
the test sets of WSJ and CTB 5.0.
We also compare them with previous works on
the same test sets. On both English and Chinese,
the word-pair classification model falls behind of
the state-of-the-art. We think that it is probably
2http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
3We did not investigate more fine-grained ratios, since the
performance curves show no dramatic fluctuation along with
the alteration of r.
17
 54
 54.5
 55
 55.5
 56
 0.65  0.7  0.75  0.8  0.85  0.9  0.95
De
pe
nd
en
cy
 P
re
cis
ion
 (%
)
Threshold b
Figure 4: The performance curve of the word-
pair classification model on the development set
of CTB 5.0, with respect to a series of threshold b.
due to the local optimization of the training pro-
cedure. Given complete trees as training data, it
is easy for previous models to utilize structural,
global and linguistical information in order to ob-
tain more powerful parameters. The main advan-
tage of our model is that it doesn?t need complete
trees to tune its parameters. Therefore, if trained
on instances extracted from human-annotated tree-
banks, the word-pair classification model would
not demonstrate its advantage over existed state-
of-the-art dependency parsing methods.
6.2 Dependency Projection
In this work we focus on the dependency projec-
tion from English to Chinese. We use the FBIS
Chinese-English bitext as the bilingual corpus for
dependency projection. It contains 239K sen-
tence pairs with about 6.9M/8.9M words in Chi-
nese/English. Both English and Chinese sentences
are tagged by the implementations of the POS tag-
ger of Collins (2002), which trained on WSJ and
CTB 5.0 respectively. The English sentences are
then parsed by an implementation of 2nd-ordered
MST model of McDonald and Pereira (2006),
which is trained on dependency trees extracted
from WSJ. The alignment matrixes for sentence
pairs are generated according to (Liu et al, 2009).
Similar to the ratio r, the threshold b need also
be assigned an appropriate value to achieve a bet-
ter performance. Larger thresholds result in better
but less classification instances, the lower cover-
age of the instances would hurt the performance of
the classifier. On the other hand, smaller thresh-
olds lead to worse but more instances, and too
much noisy instances will bring down the classi-
fier?s discriminating power.
We extract a series of classification instance sets
Corpus System P %
CTB 2.0 Hwa et al (2005) 53.9
our model 56.9
CTB 5.0 Jiang and Liu (2009) 53.28
our model 58.59
Table 4: The performance of the projected classi-
fier on the test sets of CTB 2.0 and CTB 5.0, com-
pared with the performance of previous works on
the corresponding test sets.
Corpus Baseline P% Integrated P%
CTB 1.0 82.23 83.70
CTB 5.0 87.15 87.65
Table 5: Performance improvement brought by
the projected classifier to the baseline 2nd-ordered
MST parsers trained on CTB 1.0 and CTB 5.0, re-
spectively.
with different thresholds. Then, on each instance
set we train a classifier and test it on the develop-
ment set of CTB 5.0. Figure 4 presents the ex-
perimental results. The curve shows that the max-
imum performance is achieved at the threshold of
about 0.85. The classifier corresponding to this
threshold is evaluated on the test set of CTB 5.0,
and the test set of CTB 2.0 determined by Hwa et
al. (2005). Table 4 shows the performance of the
projected classifier, as well as the performance of
previous works on the corresponding test sets. The
projected classifier significantly outperforms pre-
vious works on both test sets, which demonstrates
that the word-pair classification model, although
falling behind of the state-of-the-art on human-
annotated treebanks, performs well in projected
dependency parsing. We give the credit to its good
collaboration with the word-pair classification in-
stance extraction for dependency projection.
6.3 Integrated Dependency Parser
We integrate the word-pair classification model
into the state-of-the-art 2nd-ordered MST model.
First, we implement a chart-based dynamic pro-
gramming parser for the 2nd-ordered MST model,
and develop a training procedure based on the
perceptron algorithm with averaged parameters
(Collins, 2002). On the WSJ corpus, this parser
achieves the same performance as that of McDon-
ald and Pereira (2006). Then, at each derivation
step of this 2nd-ordered MST parser, we weight-
edly add the evaluation score given by the pro-
jected classifier to the original MST evaluation
score. Such a weighted summation of two eval-
18
uation scores provides better evaluation for can-
didate parses. The weight parameter ? is tuned
by a minimum error-rate training algorithm (Och,
2003).
Given a 2nd-ordered MST parser trained on
CTB 5.0 as the baseline, the projected classi-
fier brings an accuracy improvement of about 0.5
points. For the baseline trained on the smaller
CTB 1.0, whose training set is chapters 1-270 of
CTB 5.0, the accuracy improvement is much sig-
nificant, about 1.5 points over the baseline. It
indicates that, the smaller the human-annotated
treebank we have, the more significant improve-
ment we can achieve by integrating the project-
ing classifier. This provides a promising strategy
for boosting the parsing performance of resource-
scarce languages. Table 5 summarizes the experi-
mental results.
7 Conclusion and Future Works
In this paper, we first describe an intuitionis-
tic method for dependency parsing, which re-
sorts to a classifier to determine whether a word
pair forms a dependency edge, and then propose
an effective strategy for dependency projection,
which produces a set of projected classification in-
stances rather than complete projected trees. Al-
though this parsing method falls behind of pre-
vious models, it can collaborate well with the
word-pair classification instance extraction strat-
egy for dependency projection, and achieves the
state-of-the-art in projected dependency parsing.
In addition, when integrated into a 2nd-ordered
MST parser, the projected parser brings signifi-
cant improvement to the baseline, especially for
the baseline trained on smaller treebanks. This
provides a new strategy for resource-scarce lan-
guages to train high-precision dependency parsers.
However, considering its lower performance on
human-annotated treebanks, the dependency pars-
ing method itself still need a lot of investigations,
especially on the training method of the classifier.
Acknowledgement
This project was supported by National Natural
Science Foundation of China, Contract 60736014,
and 863 State Key Project No. 2006AA010108.
We are grateful to the anonymous reviewers for
their thorough reviewing and valuable sugges-
tions. We show special thanks to Dr. Rebecca
Hwa for generous help of sharing the experimen-
tal data. We also thank Dr. Yang Liu for sharing
the codes of alignment matrix generation, and Dr.
Liang Huang for helpful discussions.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics.
Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.
2006. Projective dependency parsing with percep-
tron. In Proceedings of the CoNLL.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175?182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 1?8, Philadelphia, USA.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340?345.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311?325.
19
Wenbin Jiang and Qun Liu. 2009. Automatic adapta-
tion of annotation standards for dependency parsing
using projected treebank as source corpus. In Pro-
ceedings of IWPT.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the ACL.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the EMNLP.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan Lu?, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91?
98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of english text. In Proceedings of the
COLING.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen
Eryigit, and Svetoslav Marinov. 2006. Labeled
pseudoprojective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221?225.
Franz J. Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL, pages 160?167.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analysis
based on maximum entropy models. In Proceedings
of the EACL.
Vladimir N. Vapnik. 1998. Statistical learning theory.
In A Wiley-Interscience Publication.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of the ACL.
20
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1433?1442,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Constituency to Dependency Translation with Forests
Haitao Mi and Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{htmi,liuqun}@ict.ac.cn
Abstract
Tree-to-string systems (and their forest-
based extensions) have gained steady pop-
ularity thanks to their simplicity and effi-
ciency, but there is a major limitation: they
are unable to guarantee the grammatical-
ity of the output, which is explicitly mod-
eled in string-to-tree systems via target-
side syntax. We thus propose to com-
bine the advantages of both, and present
a novel constituency-to-dependency trans-
lation model, which uses constituency
forests on the source side to direct the
translation, and dependency trees on the
target side (as a language model) to en-
sure grammaticality. Medium-scale exper-
iments show an absolute and statistically
significant improvement of +0.7 BLEU
points over a state-of-the-art forest-based
tree-to-string system even with fewer
rules. This is also the first time that a tree-
to-tree model can surpass tree-to-string
counterparts.
1 Introduction
Linguistically syntax-based statistical machine
translation models have made promising progress
in recent years. By incorporating the syntactic an-
notations of parse trees from both or either side(s)
of the bitext, they are believed better than phrase-
based counterparts in reorderings. Depending on
the type of input, these models can be broadly di-
vided into two categories (see Table 1): the string-
based systems whose input is a string to be simul-
taneously parsed and translated by a synchronous
grammar, and the tree-based systems whose input
is already a parse tree to be directly converted into
a target tree or string. When we also take into ac-
count the type of output (tree or string), the tree-
based systems can be divided into tree-to-string
and tree-to-tree efforts.
tree on examples (partial) fast gram. BLEU
source Liu06, Huang06 + - +
target Galley06, Shen08 - + +
both Ding05, Liu09 + + -
both our work + + +
Table 1: A classification and comparison of lin-
guistically syntax-based SMT systems, where
gram. denotes grammaticality of the output.
On one hand, tree-to-string systems (Liu et al,
2006; Huang et al, 2006) have gained significant
popularity, especially after incorporating packed
forests (Mi et al, 2008; Mi and Huang, 2008; Liu
et al, 2009; Zhang et al, 2009). Compared with
their string-based counterparts, tree-based systems
are much faster in decoding (linear time vs. cu-
bic time, see (Huang et al, 2006)), do not re-
quire a binary-branching grammar as in string-
based models (Zhang et al, 2006; Huang et al,
2009), and can have separate grammars for pars-
ing and translation (Huang et al, 2006). However,
they have a major limitation that they do not have a
principled mechanism to guarantee grammatical-
ity on the target side, since there is no linguistic
tree structure of the output.
On the other hand, string-to-tree systems ex-
plicitly model the grammaticality of the output
by using target syntactic trees. Both string-to-
constituency system (e.g., (Galley et al, 2006;
Marcu et al, 2006)) and string-to-dependency
model (Shen et al, 2008) have achieved signif-
icant improvements over the state-of-the-art for-
mally syntax-based system Hiero (Chiang, 2007).
However, those systems also have some limita-
tions that they run slowly (in cubic time) (Huang
et al, 2006), and do not utilize the useful syntactic
information on the source side.
We thus combine the advantages of both tree-to-
string and string-to-tree approaches, and propose
1433
a novel constituency-to-dependency model, which
uses constituency forests on the source side to di-
rect translation, and dependency trees on the tar-
get side to guarantee grammaticality of the out-
put. In contrast to conventional tree-to-tree ap-
proaches (Ding and Palmer, 2005; Quirk et al,
2005; Xiong et al, 2007; Zhang et al, 2007;
Liu et al, 2009), which only make use of a sin-
gle type of trees, our model is able to combine
two types of trees, outperforming both phrase-
based and tree-to-string systems. Current tree-to-
tree models (Xiong et al, 2007; Zhang et al, 2007;
Liu et al, 2009) still have not outperformed the
phrase-based system Moses (Koehn et al, 2007)
significantly even with the help of forests.1
Our new constituency-to-dependency model
(Section 2) extracts rules from word-aligned pairs
of source constituency forests and target depen-
dency trees (Section 3), and translates source con-
stituency forests into target dependency trees with
a set of features (Section 4). Medium data exper-
iments (Section 5) show a statistically significant
improvement of +0.7 BLEU points over a state-
of-the-art forest-based tree-to-string system even
with less translation rules, this is also the first time
that a tree-to-tree model can surpass tree-to-string
counterparts.
2 Model
Figure 1 shows a word-aligned source con-
stituency forest Fc and target dependency tree De,
our constituency to dependency translation model
can be formalized as:
P(Fc, De) =
?
Cc?Fc
P(Cc, De)
=
?
Cc?Fc
?
o?O
P(O)
=
?
Cc?Fc
?
o?O
?
r?o
P(r),
(1)
where Cc is a constituency tree in Fc, o is a deriva-
tion that translates Cc to De, O is the set of deriva-
tion, r is a constituency to dependency translation
rule.
1According to the reports of Liu et al (2009), their forest-
based constituency-to-constituency system achieves a com-
parable performance against Moses (Koehn et al, 2007), but
a significant improvement of +3.6 BLEU points over the 1-
best tree-based constituency-to-constituency system.
2.1 Constituency Forests on the Source Side
A constituency forest (in Figure 1 left) is a com-
pact representation of all the derivations (i.e.,
parse trees) for a given sentence under a context-
free grammar (Billot and Lang, 1989).
More formally, following Huang (2008), such
a constituency forest is a pair Fc = Gf =
?V f , Hf ?, where V f is the set of nodes, and Hf
the set of hyperedges. For a given source sen-
tence c1:m = c1 . . . cm, each node vf ? V f is
in the form of X i,j , which denotes the recognition
of nonterminal X spanning the substring from po-
sitions i through j (that is, ci+1 . . . cj). Each hy-
peredge hf ? Hf is a pair ?tails(hf ), head(hf )?,
where head(hf ) ? V f is the consequent node in
the deductive step, and tails(hf ) ? (V f )? is the
list of antecedent nodes. For example, the hyper-
edge hf0 in Figure 1 for deduction (*)
NPB0,1 CC1,2 NPB2,3
NP0,3 , (*)
is notated:
?(NPB0,1, CC1,2, NPB2,3), NP0,3?.
where
head(hf0) = {NP0,3},
and
tails(hf0) = {NPB0,1,CC1,2,NPB2,3}.
The solid line in Figure 1 shows the best parse
tree, while the dashed one shows the second best
tree. Note that common sub-derivations like those
for the verb VPB3,5 are shared, which allows the
forest to represent exponentially many parses in a
compact structure.
We also denote IN (vf ) to be the set of in-
coming hyperedges of node vf , which represents
the different ways of deriving vf . Take node IP0,5
in Figure 1 for example, IN (IP0,5) = {hf1 , h
f
2}.
There is also a distinguished root node TOP in
each forest, denoting the goal item in parsing,
which is simply S0,m where S is the start symbol
and m is the sentence length.
2.2 Dependency Trees on the Target Side
A dependency tree for a sentence represents each
word and its syntactic dependents through directed
arcs, as shown in the following examples. The
main advantage of a dependency tree is that it can
explore the long distance dependency.
1434
1: talk
blank a blan blan
2: held
Bush bla blk talk
a bl
with
b Sharon
We use the lexicon dependency grammar (Hell-
wig, 2006) to express a projective dependency
tree. Take the dependency trees above for exam-
ple, they will be expressed:
1: ( a ) talk
2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )
where the lexicons in brackets represent the de-
pendencies, while the lexicon out the brackets is
the head.
More formally, a dependency tree is also a pair
De = Gd = ?V d, Hd?. For a given target sen-
tence e1:n = e1 . . . en, each node vd ? V d is
a word ei (1 6 i 6 n), each hyperedge hd ?
Hd is a directed arc ?vdi , vdj ? from node vdi to
its head node vdj . Following the formalization of
the constituency forest scenario, we denote a pair
?tails(hd), head(hd)? to be a hyperedge hd, where
head(hd) is the head node, tails(hd) is the node
where hd leaves from.
We also denote Ll(vd) and Lr(vd) to be the left
and right children sequence of node vd from the
nearest to the farthest respectively. Take the node
vd2 = ?held? for example:
Ll(vd2) ={Bush},
Lr(vd2) ={talk, with}.
2.3 Hypergraph
Actually, both the constituency forest and the de-
pendency tree can be formalized as a hypergraph
G, a pair ?V,H?. We use Gf and Gd to distinguish
them. For simplicity, we also use Fc and De to de-
note a constituency forest and a dependency tree
respectively. Specifically, the size of tails(hd) of
a hyperedge hd in a dependency tree is a constant
one.
IP
NP
x1:NPB CC
yu?
x2:NPB
x3:VPB? (x1) x3 (with (x2))
Figure 2: Example of the rule r1. The Chinese con-
junction yu? ?and? is translated into English prepo-
sition ?with?.
3 Rule Extraction
We extract constituency to dependency rules from
word-aligned source constituency forest and target
dependency tree pairs (Figure 1). We mainly ex-
tend the tree-to-string rule extraction algorithm of
Mi and Huang (2008) to our scenario. In this sec-
tion, we first formalize the constituency to string
translation rule (Section 3.1). Then we present
the restrictions for dependency structures as well
formed fragments (Section 3.2). Finally, we de-
scribe our rule extraction algorithm (Section 3.3),
fractional counts computation and probabilities es-
timation (Section 3.4).
3.1 Constituency to Dependency Rule
More formally, a constituency to de-
pendency translation rule r is a tuple
?lhs(r), rhs(r), ?(r)?, where lhs(r) is the
source side tree fragment, whose internal nodes
are labeled by nonterminal symbols (like NP and
VP), and whose frontier nodes are labeled by
source language words ci (like ?yu??) or variables
from a set X = {x1, x2, . . .}; rhs(r) is expressed
in the target language dependency structure with
words ej (like ?with?) and variables from the set
X ; and ?(r) is a mapping from X to nontermi-
nals. Each variable xi ? X occurs exactly once in
lhs(r) and exactly once in rhs(r). For example,
the rule r1 in Figure 2,
lhs(r1) = IP(NP(x1 CC(yu?) x2) x3),
rhs(r1) = (x1) x3 (with (x2)),
?(r1) = {x1 7? NPB, x2 7? NPB, x3 7? VPB}.
3.2 Well Formed Dependency Fragment
Following Shen et al (2008), we also restrict
rhs(r) to be well formed dependency fragment.
The main difference between us is that we use
more flexible restrictions. Given a dependency
1435
IP0,5
?(Bush) .. Sharon))?
hf1
NP0,3
?(Bush) unionsq (with (Sharon))?
NPB0,1
?Bush?
Bu`sh??
hf0
CC1,2
?with?
yu?
VP1,5
?held .. Sharon))?
PP1,3
?with (Sharon)?
P1,2
?with?
NPB2,3
?Sharon?
Sha?lo?ng
VPB3,5
?held ((a) talk)?
VV3,4
?held ((a)*)?
ju?x??ngle
NPB4,5
?talk?
hu?`ta?n
hf2
Minimal rules extracted
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB)
? (x1) x4 (x2 (x3) )
IP (x1:NPB x2:VP)? (x1) x2
VP (x1:PP x2:VPB)? x2 (x1)
PP (x1:P x2:NPB)? x1 (x2)
VPB (VV(ju?x??ngle)) x1:NPB)
? held ((a) x1)
NPB (Bu`sh??)? Bush
NPB (hu?`ta?n)? talk
CC (yu?)? with
P (yu?)? with
NPB (Sha?lo?ng)? Sharon
( Bush ) held ( ( a ) talk ) ( with ( Sharon ) )
Figure 1: Forest-based constituency to dependency rule extraction.
fragment di:j composed by the words from i to j,
two kinds of well formed structures are defined as
follows:
Fixed on one node vdone, fixed for short, if it
meets the following conditions:
? the head of vdone is out of [i, j], i.e.: ?hd, if
tails(hd) = vdone ? head(hd) /? ei:j .
? the heads of other nodes except vdone are in
[i, j], i.e.: ?k ? [i, j] and vdk 6= vdone,?hd if
tails(hd) = vdk ? head(hd) ? ei:j .
Floating with multi nodes M , floating for
short, if it meets the following conditions:
? all nodes in M have a same head node,
i.e.: ?x /? [i, j],?hd if tails(hd) ? M ?
head(hd) = vhx .
? the heads of other nodes not in M are in
[i, j], i.e.: ?k ? [i, j] and vdk /? M, ?hd if
tails(hd) = vdk ? head(hd) ? ei:j .
Take the ? (Bush) held ((a) talk))(with (Sharon))
? for example: partial fixed examples are ? (Bush)
held ? and ? held ((a) talk)?; while the partial float-
ing examples are ? (talk) (with (Sharon)) ? and ?
((a) talk) (with (Sharon)) ?. Please note that the
floating structure ? (talk) (with (Sharon)) ? can not
be allowed in Shen et al (2008)?s model.
The dependency structure ? held ((a))? is not a
well formed structure, since the head of word ?a?
is out of scope of this structure.
3.3 Rule Extraction Algorithm
The algorithm shown in this Section is mainly ex-
tended from the forest-based tree-to-string extrac-
tion algorithm (Mi and Huang, 2008). We extract
rules from word-aligned source constituency for-
est and target dependency tree pairs (see Figure 1)
in three steps:
(1) frontier set computation,
(2) fragmentation,
(3) composition.
The frontier set (Galley et al, 2004) is the po-
tential points to ?cut? the forest and dependency
tree pair into fragments, each of which will form a
minimal rule (Galley et al, 2006).
However, not every fragment can be used for
rule extraction, since it may or may not respect
to the restrictions, such as word alignments and
well formed dependency structures. So we say a
fragment is extractable if it respects to all re-
strictions. The root node of every extractable tree
fragment corresponds to a faithful structure on
the target side, in which case there is a ?transla-
tional equivalence? between the subtree rooted at
the node and the corresponding target structure.
For example, in Figure 1, every node in the forest
is annotated with its corresponding English struc-
ture. The NP0,3 node maps to a non-contiguous
structure ?(Bush) unionsq (with (Sharon))?, the VV3,4
node maps to a contiguous but non-faithful struc-
ture ?held ((a) *)?.
1436
Algorithm 1 Forest-based constituency to dependency rule extraction.
Input: Source constituency forest Fc, target dependency tree De, and alignment a
Output: Minimal rule setR
1: fs ? FRONTIER(Fc, De, a) . compute frontier set
2: for each vf ? fs do
3: open ? {??, {vf}?} . initial queue of growing fragments
4: while open 6= ? do
5: ?hs, exps? ? open.pop() . extract a fragment
6: if exps = ? then . nothing to expand?
7: generate a rule r using fragment hs . generate a rule
8: R.append(r)
9: else . incomplete: further expand
10: v? ? exps .pop() . a non-frontier node
11: for each hf ? IN (v?) do
12: newexps ? exps ? (tails(hf ) \ fs) . expand
13: open .append(?hs ? {hf},newexps?)
Following Mi and Huang (2008), given a source
target sentence pair ?c1:m, e1:n? with an alignment
a, the span of node vf on source forest is the set
of target words aligned to leaf nodes under vf :
span(vf ) , {ei ? e1:n | ?cj ? yield(vf ), (cj , ei) ? a}.
where the yield(vf ) is all the leaf nodes un-
der vf . For each span(vf ), we also denote
dep(vf ) to be its corresponding dependency struc-
ture, which represents the dependency struc-
ture of all the words in span(vf ). Take the
span(PP1,3) ={with, Sharon} for example, the
corresponding dep(PP1,3) is ?with (Sharon)?. A
dep(vf ) is faithful structure to node vf if it meets
the following restrictions:
? all words in span(vf ) form a continuous sub-
string ei:j ,
? every word in span(vf ) is only aligned to leaf
nodes of vf , i.e.: ?ei ? span(vf ), (cj , ei) ?
a? cj ? yield(vf ),
? dep(vf ) is a well formed dependency struc-
ture.
For example, node VV3,4 has a non-faithful
structure (crossed out in Figure 1), since its
dep(VV3,4 = ? held ((a) *)? is not a well formed
structure, where the head of word ?a? lies in the
outside of its words covered. Nodes with faithful
structure form the frontier set (shaded nodes in
Figure 1) which serve as potential cut points for
rule extraction.
Given the frontier set, fragmentation step is to
?cut? the forest at all frontier nodes and form
tree fragments, each of which forms a rule with
variables matching the frontier descendant nodes.
For example, the forest in Figure 1 is cut into 10
pieces, each of which corresponds to a minimal
rule listed on the right.
Our rule extraction algorithm is formalized in
Algorithm 1. After we compute the frontier set
fs (line 1). We visit each frontier node vf ? fs
on the source constituency forest Fc, and keep a
queue open of growing fragments rooted at vf . We
keep expanding incomplete fragments from open ,
and extract a rule if a complete fragment is found
(line 7). Each fragment hs in open is associated
with a list of expansion sites (exps in line 5) being
the subset of leaf nodes of the current fragment
that are not in the frontier set. So each fragment
along hyperedge h is associated with
exps = tails(hf ) \ fs.
A fragment is complete if its expansion sites is
empty (line 6), otherwise we pop one expansion
node v? to grow and spin-off new fragments by
following hyperedges of v?, adding new expansion
sites (lines 11-13), until all active fragments are
complete and open queue is empty (line 4).
After we get al the minimal rules, we glue them
together to form composed rules following Galley
et al (2006). For example, the composed rule r1
in Figure 2 is glued by the following two minimal
rules:
1437
IP (NP(x1:NPB x2:CC x3:NPB) x4:VPB) r2? (x1) x4 (x2 (x3) )
CC (yu?)? with r3
where x2:CC in r2 is replaced with r3 accordingly.
3.4 Fractional Counts and Rule Probabilities
Following Mi and Huang (2008), we penalize a
rule r by the posterior probability of the corre-
sponding constituent tree fragment lhs(r), which
can be computed in an Inside-Outside fashion, be-
ing the product of the outside probability of its
root node, the inside probabilities of its leaf nodes,
and the probabilities of hyperedges involved in the
fragment.
??(lhs(r)) =?(root(r))
?
?
hf ? lhs(r)
P(hf )
?
?
vf ? leaves(lhs(r))
?(vf )
(2)
where root(r) is the root of the rule r, ?(v) and
?(v) are the outside and inside probabilities of
node v, and leaves(lhs(r)) returns the leaf nodes
of a tree fragment lhs(r).
We use fractional counts to compute three con-
ditional probabilities for each rule, which will be
used in the next section:
P(r | lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
, (3)
P(r | rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
, (4)
P(r | root(r)) = c(r)?
r?:root(r?)=root(r) c(r?)
. (5)
4 Decoding
Given a source forest Fc, the decoder searches for
the best derivation o? among the set of all possible
derivations O, each of which forms a source side
constituent tree Tc(o), a target side string e(o), and
a target side dependency tree De(o):
o? = arg max
Tc?Fc,o?O
?1 log P(o | Tc)
+?2 log Plm(e(o))
+?3 log PDLMw(De(o))
+?4 log PDLMp(De(o))
+?5 log P(Tc(o))
+?6ill(o) + ?7|o|+ ?8|e(o)|,
(6)
where the first two terms are translation and lan-
guage model probabilities, e(o) is the target string
(English sentence) for derivation o, the third and
forth items are the dependency language model
probabilities on the target side computed with
words and POS tags separately, De(o) is the target
dependency tree of o, the fifth one is the parsing
probability of the source side tree Tc(o) ? Fc, the
ill(o) is the penalty for the number of ill-formed
dependency structures in o, and the last two terms
are derivation and translation length penalties, re-
spectively. The conditional probability P(o | Tc)
is decomposes into the product of rule probabili-
ties:
P(o | Tc) =
?
r?o
P(r), (7)
where each P(r) is the product of five probabili-
ties:
P(r) =P(r | lhs(r))?9 ? P(r | rhs(r))?10
? P(r | root(lhs(r)))?11
? Plex(lhs(r) | rhs(r))?12
? Plex(rhs(r) | lhs(r))?13 ,
(8)
where the first three are conditional probabilities
based on fractional counts of rules defined in Sec-
tion 3.4, and the last two are lexical probabilities.
When computing the lexical translation probabili-
ties described in (Koehn et al, 2003), we only take
into accout the terminals in a rule. If there is no
terminal, we set the lexical probability to 1.
The decoding algorithm works in a bottom-up
search fashion by traversing each node in forest
Fc. We first use pattern-matching algorithm of Mi
et al (2008) to convert Fc into a translation for-
est, each hyperedge of which is associated with a
constituency to dependency translation rule. How-
ever, pattern-matching failure2 at a node vf will
2Pattern-matching failure at a node vf means there is no
translation rule can be matched at vf or no translation hyper-
edge can be constructed at vf .
1438
cut the derivation path and lead to translation fail-
ure. To tackle this problem, we construct a pseudo
translation rule for each parse hyperedge hf ?
IN (vf ) by mapping the CFG rule into a target de-
pendency tree using the head rules of Magerman
(1995). Take the hyperedge hf0 in Figure1 for ex-
ample, the corresponding pseudo translation rule
is:
NP(x1:NPB x2:CC x3:NPB)? (x1) (x2) x3,
since the x3:NPB is the head word of the CFG
rule: NP? NPB CC NPB.
After the translation forest is constructed, we
traverse each node in translation forest also in
bottom-up fashion. For each node, we use the
cube pruning technique (Chiang, 2007; Huang
and Chiang, 2007) to produce partial hypotheses
and compute all the feature scores including the
dependency language model score (Section 4.1).
If all the nodes are visited, we trace back along
the 1-best derivation at goal item S0,m and build
a target side dependency tree. For k-best search
after getting 1-best derivation, we use the lazy Al-
gorithm 3 of Huang and Chiang (2005) that works
backwards from the root node, incrementally com-
puting the second, third, through the kth best alter-
natives.
4.1 Dependency Language Model Computing
We compute the score of a dependency language
model for a dependency tree De in the same way
proposed by Shen et al (2008). For each nonter-
minal node vdh = eh in De and its children se-
quences Ll = el1 , el2 ...eli and Lr = er1 , er2 ...erj ,
the probability of a trigram is computed as fol-
lows:
P(Ll, Lr | eh?) = P(Ll | eh?) ?P(Lr | eh?), (9)
where the P(Ll | eh?) is decomposed to be:
P(Ll | eh?) =P(ell | eh?)
? P(el2 | el1 , eh?)
...
? P(eln | eln?1 , eln?2).
(10)
We use the suffix ??? to distinguish the head
word and child words in the dependency language
model.
In order to alleviate the problem of data sparse,
we also compute a dependency language model
for POS tages over a dependency tree. We store
the POS tag information on the target side for each
constituency-to-dependency rule. So we will also
generate a POS taged dependency tree simulta-
neously at the decoding time. We calculate this
dependency language model by simply replacing
each ei in equation 9 with its tag t(ei).
5 Experiments
5.1 Data Preparation
Our training corpus consists of 239K sentence
pairs with about 6.9M/8.9M words in Chi-
nese/English, respectively. We first word-align
them by GIZA++ (Och and Ney, 2000) with re-
finement option ?grow-diag-and? (Koehn et al,
2003), and then parse the Chinese sentences using
the parser of Xiong et al (2005) into parse forests,
which are pruned into relatively small forests with
a pruning threshold 3. We also parse the English
sentences using the parser of Charniak (2000) into
1-best constituency trees, which will be converted
into dependency trees using Magerman (1995)?s
head rules. We also store the POS tag informa-
tion for each word in dependency trees, and com-
pute two different dependency language models
for words and POS tags in dependency tree sepa-
rately. Finally, we apply translation rule extraction
algorithm described in Section 3. We use SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
4-gram language model with Kneser-Ney smooth-
ing on the first 1/3 of the Xinhua portion of Giga-
word corpus. At the decoding step, we again parse
the input sentences into forests and prune them
with a threshold 10, which will direct the trans-
lation (Section 4).
We use the 2002 NIST MT Evaluation test set
as our development set and the 2005 NIST MT
Evaluation test set as our test set. We evaluate the
translation quality using the BLEU-4 metric (Pap-
ineni et al, 2002), which is calculated by the script
mteval-v11b.pl with its default setting which is
case-insensitive matching of n-grams. We use the
standard minimum error-rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem?s BLEU score on development set.
5.2 Results
Table 2 shows the results on the test set. Our
baseline system is a state-of-the-art forest-based
constituency-to-string model (Mi et al, 2008), or
forest c2s for short, which translates a source for-
est into a target string by pattern-matching the
1439
constituency-to-string (c2s) rules and the bilin-
gual phrases (s2s). The baseline system extracts
31.9M c2s rules, 77.9M s2s rules respectively and
achieves a BLEU score of 34.17 on the test set3.
At first, we investigate the influence of differ-
ent rule sets on the performance of baseline sys-
tem. We first restrict the target side of transla-
tion rules to be well-formed structures, and we
extract 13.8M constituency-to-dependency (c2d)
rules, which is 43% of c2s rules. We also extract
9.0M string-to-dependency (s2d) rules, which is
only 11.6% of s2s rules. Then we convert c2d and
s2d rules to c2s and s2s rules separately by re-
moving the target-dependency structures and feed
them into the baseline system. As shown in the
third line in the column of BLEU score, the per-
formance drops 1.7 BLEU points over baseline
system due to the poorer rule coverage. However,
when we further use all s2s rules instead of s2d
rules in our next experiment, it achieves a BLEU
score of 34.03, which is very similar to the base-
line system. Those results suggest that restrictions
on c2s rules won?t hurt the performance, but re-
strictions on s2s will hurt the translation quality
badly. So we should utilize all the s2s rules in or-
der to preserve a good coverage of translation rule
set.
The last two lines in Table 2 show the results of
our new forest-based constituency-to-dependency
model (forest c2d for short). When we only use
c2d and s2d rules, our system achieves a BLEU
score of 33.25, which is lower than the baseline
system in the first line. But, with the same rule set,
our model still outperform the result in the sec-
ond line. This suggests that using dependency lan-
guage model really improves the translation qual-
ity by less than 1 BLEU point.
In order to utilize all the s2s rules and increase
the rule coverage, we parse the target strings of
the s2s rules into dependency fragments, and con-
struct the pseudo s2d rules (s2s-dep). Then we
use c2d and s2s-dep rules to direct the translation.
With the help of the dependency language model,
our new model achieves a significant improvement
of +0.7 BLEU points over the forest c2s baseline
system (p < 0.05, using the sign-test suggested by
3According to the reports of Liu et al (2009), with a more
larger training corpus (FBIS plus 30K) but no name entity
translations (+1 BLEU points if it is used), their forest-based
constituency-to-constituency model achieves a BLEU score
of 30.6, which is similar to Moses (Koehn et al, 2007). So our
baseline system is much better than the BLEU score (30.6+1)
of the constituency-to-constituency system and Moses.
System Rule Set BLEUType #
forest c2s
c2s 31.9M 34.17
s2s 77.9M
c2d 13.8M 32.48(?1.7)
s2d 9.0M
c2d 13.8M 34.03(?0.1)
s2s 77.9M
forest c2d
c2d 13.8M 33.25(?0.9)
s2d 9.0M
c2d 13.8M 34.88(?0.7)
s2s-dep 77.9M
Table 2: Statistics of different types of rules ex-
tracted on training corpus and the BLEU scores
on the test set.
Collins et al (2005)). For the first time, a tree-to-
tree model can surpass tree-to-string counterparts
significantly even with fewer rules.
6 Related Work
The concept of packed forest has been used in
machine translation for several years. For exam-
ple, Huang and Chiang (2007) use forest to char-
acterize the search space of decoding with in-
tegrated language models. Mi et al (2008) and
Mi and Huang (2008) use forest to direct trans-
lation and extract rules rather than 1-best tree in
order to weaken the influence of parsing errors,
this is also the first time to use forest directly
in machine translation. Following this direction,
Liu et al (2009) and Zhang et al (2009) apply
forest into tree-to-tree (Zhang et al, 2007) and
tree-sequence-to-string models(Liu et al, 2007)
respectively. Different from Liu et al (2009), we
apply forest into a new constituency tree to de-
pendency tree translation model rather than con-
stituency tree-to-tree model.
Shen et al (2008) present a string-to-
dependency model. They define the well-formed
dependency structures to reduce the size of
translation rule set, and integrate a dependency
language model in decoding step to exploit long
distance word relations. This model shows a
significant improvement over the state-of-the-art
hierarchical phrase-based system (Chiang, 2005).
Compared with this work, we put fewer restric-
tions on the definition of well-formed dependency
structures in order to extract more rules; the
1440
other difference is that we can also extract more
expressive constituency to dependency rules,
since the source side of our rule can encode
multi-level reordering and contain more variables
being larger than two; furthermore, our rules can
be pattern-matched at high level, which is more
reasonable than using glue rules in Shen et al
(2008)?s scenario; finally, the most important one
is that our model runs very faster.
Liu et al (2009) propose a forest-based
constituency-to-constituency model, they put
more emphasize on how to utilize parse forest
to increase the tree-to-tree rule coverage. By
contrast, we only use 1-best dependency trees
on the target side to explore long distance rela-
tions and extract translation rules. Theoretically,
we can extract more rules since dependency
tree has the best inter-lingual phrasal cohesion
properties (Fox, 2002).
7 Conclusion and Future Work
In this paper, we presented a novel forest-based
constituency-to-dependency translation model,
which combines the advantages of both tree-to-
string and string-to-tree systems, runs fast and
guarantees grammaticality of the output. To learn
the constituency-to-dependency translation rules,
we first identify the frontier set for all the
nodes in the constituency forest on the source
side. Then we fragment them and extract mini-
mal rules. Finally, we glue them together to be
composed rules. At the decoding step, we first
parse the input sentence into a constituency for-
est. Then we convert it into a translation for-
est by patter-matching the constituency to string
rules. Finally, we traverse the translation forest
in a bottom-up fashion and translate it into a tar-
get dependency tree by incorporating string-based
and dependency-based language models. Using all
constituency-to-dependency translation rules and
bilingual phrases, our model achieves +0.7 points
improvement in BLEU score significantly over a
state-of-the-art forest-based tree-to-string system.
This is also the first time that a tree-to-tree model
can surpass tree-to-string counterparts.
In the future, we will do more experiments
on rule coverage to compare the constituency-to-
constituency model with our model. Furthermore,
we will replace 1-best dependency trees on the
target side with dependency forests to further in-
crease the rule coverage.
Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 60736014
and 90920004, and 863 State Key Project No.
2006AA010108. We thank the anonymous review-
ers for their insightful comments. We are also
grateful to Liang Huang for his valuable sugges-
tions.
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
of ACL ?89, pages 143?151.
Eugene Charniak. 2000. A maximum-entropy inspired
parser. In Proceedings of NAACL, pages 132?139.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, pages 263?270, Ann Arbor, Michi-
gan, June.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL, pages
541?548, June.
Heidi J. Fox. 2002. Phrasal cohesion and statistical
machine translation. In In Proceedings of EMNLP-
02.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL, pages 961?968, July.
Peter Hellwig. 2006. Parsing with Dependency Gram-
mars, volume II. An International Handbook of
Contemporary Research.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of ACL, pages 144?151, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
1441
Liang Huang, Hao Zhang, Daniel Gildea, , and Kevin
Knight. 2009. Binarization of synchronous context-
free grammars. Comput. Linguist.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133, Edmon-
ton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, pages 177?180, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609?616, Sydney, Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules. In
Proceedings of ACL, pages 704?711, June.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of ACL/IJCNLP, August.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL, pages
276?283, June.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of EMNLP, pages 44?52, July.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of EMNLP
2008, pages 206?214, Honolulu, Hawaii, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08:HLT,
pages 192?199, Columbus, Ohio, June.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440?447.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadephia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL, pages
271?279, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, June.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
volume 30, pages 901?904.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun
Lin. 2005. Parsing the Penn Chinese Treebank with
Semantic Knowledge. In Proceedings of IJCNLP
2005, pages 70?81.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A
dependency treelet string correspondence model for
statistical machine translation. In Proceedings of
SMT, pages 40?47.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. of HLT-NAACL.
Min Zhang, Hongfei Jiang, Aiti Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. In
Proceedings of MT-Summit.
Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw, and
Chew Lim Tan. 2009. Forest-based tree sequence
to string translation model. In Proceedings of the
ACL/IJCNLP 2009.
1442
Proceedings of the ACL 2010 Conference Short Papers, pages 12?16,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Lexicalized Reordering Models from Reordering Graphs
Jinsong Su, Yang Liu, Yajuan Lu?, Haitao Mi, Qun Liu
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{sujinsong,yliu,lvyajuan,htmi,liuqun}@ict.ac.cn
Abstract
Lexicalized reordering models play a crucial
role in phrase-based translation systems. They
are usually learned from the word-aligned
bilingual corpus by examining the reordering
relations of adjacent phrases. Instead of just
checking whether there is one phrase adjacent
to a given phrase, we argue that it is important
to take the number of adjacent phrases into
account for better estimations of reordering
models. We propose to use a structure named
reordering graph, which represents all phrase
segmentations of a sentence pair, to learn lex-
icalized reordering models efficiently. Exper-
imental results on the NIST Chinese-English
test sets show that our approach significantly
outperforms the baseline method.
1 Introduction
Phrase-based translation systems (Koehn et al,
2003; Och and Ney, 2004) prove to be the state-
of-the-art as they have delivered translation perfor-
mance in recent machine translation evaluations.
While excelling at memorizing local translation and
reordering, phrase-based systems have difficulties in
modeling permutations among phrases. As a result,
it is important to develop effective reordering mod-
els to capture such non-local reordering.
The early phrase-based paradigm (Koehn et al,
2003) applies a simple distance-based distortion
penalty to model the phrase movements. More re-
cently, many researchers have presented lexicalized
reordering models that take advantage of lexical
information to predict reordering (Tillmann, 2004;
Xiong et al, 2006; Zens and Ney, 2006; Koehn et
Figure 1: Occurrence of a swap with different numbers
of adjacent bilingual phrases: only one phrase in (a) and
three phrases in (b). Black squares denote word align-
ments and gray rectangles denote bilingual phrases. [s,t]
indicates the target-side span of bilingual phrase bp and
[u,v] represents the source-side span of bilingual phrase
bp.
al., 2007; Galley and Manning, 2008). These mod-
els are learned from a word-aligned corpus to pre-
dict three orientations of a phrase pair with respect
to the previous bilingual phrase: monotone (M ),
swap (S), and discontinuous (D). Take the bilingual
phrase bp in Figure 1(a) for example. The word-
based reordering model (Koehn et al, 2007) ana-
lyzes the word alignments at positions (s?1, u?1)
and (s ? 1, v + 1). The orientation of bp is set
to D because the position (s ? 1, v + 1) contains
no word alignment. The phrase-based reordering
model (Tillmann, 2004) determines the presence
of the adjacent bilingual phrase located in position
(s? 1, v+1) and then treats the orientation of bp as
S. Given no constraint on maximum phrase length,
the hierarchical phrase reordering model (Galley and
Manning, 2008) also analyzes the adjacent bilingual
phrases for bp and identifies its orientation as S.
However, given a bilingual phrase, the above-
mentioned models just consider the presence of an
adjacent bilingual phrase rather than the number of
adjacent bilingual phrases. See the examples in Fig-
12
Figure 2: (a) A parallel Chinese-English sentence pair and (b) its corresponding reordering graph. In (b), we denote
each bilingual phrase with a rectangle, where the upper and bottom numbers in the brackets represent the source
and target spans of this bilingual phrase respectively. M = monotone (solid lines), S = swap (dotted line), and D =
discontinuous (segmented lines). The bilingual phrases marked in the gray constitute a reordering example.
ure 1 for illustration. In Figure 1(a), bp is in a swap
order with only one bilingual phrase. In Figure 1(b),
bp swaps with three bilingual phrases. Lexicalized
reordering models do not distinguish different num-
bers of adjacent phrase pairs, and just give bp the
same count in the swap orientation.
In this paper, we propose a novel method to better
estimate the reordering probabilities with the con-
sideration of varying numbers of adjacent bilingual
phrases. Our method uses reordering graphs to rep-
resent all phrase segmentations of parallel sentence
pairs, and then gets the fractional counts of bilin-
gual phrases for orientations from reordering graphs
in an inside-outside fashion. Experimental results
indicate that our method achieves significant im-
provements over the traditional lexicalized reorder-
ing model (Koehn et al, 2007).
This paper is organized as follows: in Section 2,
we first give a brief introduction to the traditional
lexicalized reordering model. Then we introduce
our method to estimate the reordering probabilities
from reordering graphs. The experimental results
are reported in Section 3. Finally, we end with a
conclusion and future work in Section 4.
2 Estimation of Reordering Probabilities
Based on Reordering Graph
In this section, we first describe the traditional lexi-
calized reordering model, and then illustrate how to
construct reordering graphs to estimate the reorder-
ing probabilities.
2.1 Lexicalized Reordering Model
Given a phrase pair bp = (ei, fai), where ai de-
fines that the source phrase fai is aligned to the
target phrase ei, the traditional lexicalized reorder-
ing model computes the reordering count of bp in
the orientation o based on the word alignments of
boundary words. Specifically, the model collects
bilingual phrases and distinguishes their orientations
with respect to the previous bilingual phrase into
three categories:
o =
?
??
??
M ai ? ai?1 = 1
S ai ? ai?1 = ?1
D |ai ? ai?1| 6= 1
(1)
Using the relative-frequency approach, the re-
ordering probability regarding bp is
p(o|bp) = Count(o, bp)?
o? Count(o?, bp)
(2)
2.2 Reordering Graph
For a parallel sentence pair, its reordering graph in-
dicates all possible translation derivations consisting
of the extracted bilingual phrases. To construct a
reordering graph, we first extract bilingual phrases
using the way of (Och, 2003). Then, the adjacent
13
bilingual phrases are linked according to the target-
side order. Some bilingual phrases, which have
no adjacent bilingual phrases because of maximum
length limitation, are linked to the nearest bilingual
phrases in the target-side order.
Shown in Figure 2(b), the reordering graph for
the parallel sentence pair (Figure 2(a)) can be rep-
resented as an undirected graph, where each rect-
angle corresponds to a phrase pair, each link is the
orientation relationship between adjacent bilingual
phrases, and two distinguished rectangles bs and be
indicate the beginning and ending of the parallel sen-
tence pair, respectively. With the reordering graph,
we can obtain all reordering examples containing
the given bilingual phrase. For example, the bilin-
gual phrase ?zhengshi huitan, formal meetings? (see
Figure 2(a)), corresponding to the rectangle labeled
with the source span [6,7] and the target span [4,5],
is in a monotone order with one previous phrase
and in a discontinuous order with two subsequent
phrases (see Figure 2(b)).
2.3 Estimation of Reordering Probabilities
We estimate the reordering probabilities from re-
ordering graphs. Given a parallel sentence pair,
there are many translation derivations correspond-
ing to different paths in its reordering graph. As-
suming all derivations have a uniform probability,
the fractional counts of bilingual phrases for orien-
tations can be calculated by utilizing an algorithm in
the inside-outside fashion.
Given a phrase pair bp in the reordering graph,
we denote the number of paths from bs to bp with
?(bp). It can be computed in an iterative way
?(bp) = ?bp? ?(bp?), where bp? is one of the pre-
vious bilingual phrases of bp and ?(bs)=1. In a sim-
ilar way, the number of paths from be to bp, notated
as ?(bp), is simply ?(bp) = ?bp?? ?(bp??), where
bp?? is one of the subsequent bilingual phrases of bp
and ?(be)=1. Here, we show the ? and ? values of
all bilingual phrases of Figure 2 in Table 1. Espe-
cially, for the reordering example consisting of the
bilingual phrases bp1=?jiang juxing, will hold? and
bp2=?zhengshi huitan, formal meetings?, marked in
the gray color in Figure 2, the ? and ? values can be
calculated: ?(bp1) = 1, ?(bp2) = 1+1 = 2, ?(bs) =
8+1 = 9.
Inspired by the parsing literature on pruning
src span trg span ? ?
[0, 0] [0, 0] 1 9
[1, 1] [1, 1] 1 8
[1, 7] [1, 7] 1 1
[4, 4] [2, 2] 1 1
[4, 5] [2, 3] 1 3
[4, 6] [2, 4] 1 1
[4, 7] [2, 5] 1 2
[2, 7] [2, 7] 1 1
[5, 5] [3, 3] 1 1
[6, 6] [4, 4] 2 1
[6, 7] [4, 5] 1 2
[7, 7] [5, 5] 3 1
[2, 2] [6, 6] 5 1
[2, 3] [6, 7] 2 1
[3, 3] [7, 7] 5 1
[8, 8] [8, 8] 9 1
Table 1: The ? and ? values of the bilingual phrases
shown in Figure 2.
(Charniak and Johnson, 2005; Huang, 2008), the
fractional count of (o, bp?, bp) is
Count(o, bp?, bp) = ?(bp
?) ? ?(bp)
?(bs) (3)
where the numerator indicates the number of paths
containing the reordering example (o, bp?, bp) and
the denominator is the total number of paths in the
reordering graph. Continuing with the reordering
example described above, we obtain its fractional
count using the formula (3): Count(M, bp1, bp2) =
(1? 2)/9 = 2/9.
Then, the fractional count of bp in the orientation
o is calculated as described below:
Count(o, bp) =
?
bp?
Count(o, bp?, bp) (4)
For example, we compute the fractional count of
bp2 in the monotone orientation by the formula (4):
Count(M, bp2) = 2/9.
As described in the lexicalized reordering model
(Section 2.1), we apply the formula (2) to calculate
the final reordering probabilities.
3 Experiments
We conduct experiments to investigate the effec-
tiveness of our method on the msd-fe reorder-
ing model and the msd-bidirectional-fe reordering
model. These two models are widely applied in
14
phrase-based system (Koehn et al, 2007). The msd-
fe reordering model has three features, which rep-
resent the probabilities of bilingual phrases in three
orientations: monotone, swap, or discontinuous. If a
msd-bidirectional-fe model is used, then the number
of features doubles: one for each direction.
3.1 Experiment Setup
Two different sizes of training corpora are used in
our experiments: one is a small-scale corpus that
comes from FBIS corpus consisting of 239K bilin-
gual sentence pairs, the other is a large-scale corpus
that includes 1.55M bilingual sentence pairs from
LDC. The 2002 NIST MT evaluation test data is
used as the development set and the 2003, 2004,
2005 NIST MT test data are the test sets. We
choose the MOSES1 (Koehn et al, 2007) as the ex-
perimental decoder. GIZA++ (Och and Ney, 2003)
and the heuristics ?grow-diag-final-and? are used to
generate a word-aligned corpus, where we extract
bilingual phrases with maximum length 7. We use
SRILM Toolkits (Stolcke, 2002) to train a 4-gram
language model on the Xinhua portion of Gigaword
corpus.
In exception to the reordering probabilities, we
use the same features in the comparative experi-
ments. During decoding, we set ttable-limit = 20,
stack = 100, and perform minimum-error-rate train-
ing (Och, 2003) to tune various feature weights. The
translation quality is evaluated by case-insensitive
BLEU-4 metric (Papineni et al, 2002). Finally, we
conduct paired bootstrap sampling (Koehn, 2004) to
test the significance in BLEU scores differences.
3.2 Experimental Results
Table 2 shows the results of experiments with the
small training corpus. For the msd-fe model, the
BLEU scores by our method are 30.51 32.78 and
29.50, achieving absolute improvements of 0.89,
0.66 and 0.62 on the three test sets, respectively. For
the msd-bidirectional-fe model, our method obtains
BLEU scores of 30.49 32.73 and 29.24, with abso-
lute improvements of 1.11, 0.73 and 0.60 over the
baseline method.
1The phrase-based lexical reordering model (Tillmann,
2004) is also closely related to our model. However, due to
the limit of time and space, we only use Moses-style reordering
model (Koehn et al, 2007) as our baseline.
model method MT-03 MT-04 MT-05
baseline 29.62 32.12 28.88m-f RG 30.51?? 32.78?? 29.50?
baseline 29.38 32.00 28.64m-b-f RG 30.49?? 32.73?? 29.24?
Table 2: Experimental results with the small-scale cor-
pus. m-f: msd-fe reordering model. m-b-f: msd-
bidirectional-fe reordering model. RG: probabilities esti-
mation based on Reordering Graph. * or **: significantly
better than baseline (p < 0 .05 or p < 0 .01 ).
model method MT-03 MT-04 MT-05
baseline 31.58 32.39 31.49m-f RG 32.44?? 33.24?? 31.64
baseline 32.43 33.07 31.69m-b-f RG 33.29?? 34.49?? 32.79??
Table 3: Experimental results with the large-scale cor-
pus.
Table 3 shows the results of experiments with
the large training corpus. In the experiments of
the msd-fe model, in exception to the MT-05 test
set, our method is superior to the baseline method.
The BLEU scores by our method are 32.44, 33.24
and 31.64, which obtain 0.86, 0.85 and 0.15 gains
on three test set, respectively. For the msd-
bidirectional-fe model, the BLEU scores produced
by our approach are 33.29, 34.49 and 32.79 on the
three test sets, with 0.86, 1.42 and 1.1 points higher
than the baseline method, respectively.
4 Conclusion and Future Work
In this paper, we propose a method to improve the
reordering model by considering the effect of the
number of adjacent bilingual phrases on the reorder-
ing probabilities estimation. Experimental results on
NIST Chinese-to-English tasks demonstrate the ef-
fectiveness of our method.
Our method is also general to other lexicalized
reordering models. We plan to apply our method
to the complex lexicalized reordering models, for
example, the hierarchical reordering model (Galley
and Manning, 2008) and the MEBTG reordering
model (Xiong et al, 2006). In addition, how to fur-
ther improve the reordering model by distinguishing
the derivations with different probabilities will be-
come another study emphasis in further research.
15
Acknowledgement
The authors were supported by National Natural Sci-
ence Foundation of China, Contracts 60873167 and
60903138. We thank the anonymous reviewers for
their insightful comments. We are also grateful to
Hongmei Zhao and Shu Cai for their helpful feed-
back.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proc. of ACL 2005, pages 173?180.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proc. of EMNLP 2008, pages 848?856.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL 2008,
pages 586?594.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004, pages 388?395.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, pages 417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311?318.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. of ICSLP 2002, pages 901?
904.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
ACL 2004, Short Papers, pages 101?104.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for statis-
tical machine translation. In Proc. of ACL 2006, pages
521?528.
Richard Zens and Hermann Ney. 2006. Discriminvative
reordering models for statistical machine translation.
In Proc. of Workshop on Statistical Machine Transla-
tion 2006, pages 521?528.
16
Proceedings of the ACL 2010 Conference Short Papers, pages 142?146,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Better Filtration and Augmentation for Hierarchical Phrase-Based
Translation Rules
Zhiyang Wang ? Yajuan Lu? ? Qun Liu ? Young-Sook Hwang ?
?Key Lab. of Intelligent Information Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
P.O. Box 2704, Beijing 100190, China 11, Euljiro2-ga, Jung-gu, Seoul 100-999, Korea
wangzhiyang@ict.ac.cn yshwang@sktelecom.com
Abstract
This paper presents a novel filtration cri-
terion to restrict the rule extraction for
the hierarchical phrase-based translation
model, where a bilingual but relaxed well-
formed dependency restriction is used to
filter out bad rules. Furthermore, a new
feature which describes the regularity that
the source/target dependency edge trig-
gers the target/source word is also pro-
posed. Experimental results show that, the
new criteria weeds out about 40% rules
while with translation performance im-
provement, and the new feature brings an-
other improvement to the baseline system,
especially on larger corpus.
1 Introduction
Hierarchical phrase-based (HPB) model (Chiang,
2005) is the state-of-the-art statistical machine
translation (SMT) model. By looking for phrases
that contain other phrases and replacing the sub-
phrases with nonterminal symbols, it gets hierar-
chical rules. Hierarchical rules are more powerful
than conventional phrases since they have better
generalization capability and could capture long
distance reordering. However, when the train-
ing corpus becomes larger, the number of rules
will grow exponentially, which inevitably results
in slow and memory-consuming decoding.
In this paper, we address the problem of reduc-
ing the hierarchical translation rule table resorting
to the dependency information of bilingual lan-
guages. We only keep rules that both sides are
relaxed-well-formed (RWF) dependency structure
(see the definition in Section 3), and discard others
which do not satisfy this constraint. In this way,
about 40% bad rules are weeded out from the orig-
inal rule table. However, the performance is even
better than the traditional HPB translation system.
Source
Target 
f f? 
e
Figure 1: Solid wire reveals the dependency rela-
tion pointing from the child to the parent. Target
word e is triggered by the source word f and it?s
head word f ?, p(e|f ? f ?).
Based on the relaxed-well-formed dependency
structure, we also introduce a new linguistic fea-
ture to enhance translation performance. In the
traditional phrase-based SMT model, there are
always lexical translation probabilities based on
IBM model 1 (Brown et al, 1993), i.e. p(e|f),
namely, the target word e is triggered by the source
word f . Intuitively, however, the generation of e
is not only involved with f , sometimes may also
be triggered by other context words in the source
side. Here we assume that the dependency edge
(f ? f ?) of word f generates target word e (we
call it head word trigger in Section 4). Therefore,
two words in one language trigger one word in
another, which provides a more sophisticated and
better choice for the target word, i.e. Figure 1.
Similarly, the dependency feature works well in
Chinese-to-English translation task, especially on
large corpus.
2 Related Work
In the past, a significant number of techniques
have been presented to reduce the hierarchical rule
table. He et al (2009) just used the key phrases
of source side to filter the rule table without taking
advantage of any linguistic information. Iglesias
et al (2009) put rules into syntactic classes based
on the number of non-terminals and patterns, and
applied various filtration strategies to improve the
rule table quality. Shen et al (2008) discarded
142
found
The
girl 
lovely 
house
a beautiful
Figure 2: An example of dependency tree. The
corresponding plain sentence is The lovely girl
found a beautiful house.
most entries of the rule table by using the con-
straint that rules of the target-side are well-formed
(WF) dependency structure, but this filtering led to
degradation in translation performance. They ob-
tained improvements by adding an additional de-
pendency language model. The basic difference
of our method from (Shen et al, 2008) is that we
keep rules that both sides should be relaxed-well-
formed dependency structure, not just the target
side. Besides, our system complexity is not in-
creased because no additional language model is
introduced.
The feature of head word trigger which we ap-
ply to the log-linear model is motivated by the
trigger-based approach (Hasan and Ney, 2009).
Hasan and Ney (2009) introduced a second word
to trigger the target word without considering any
linguistic information. Furthermore, since the sec-
ond word can come from any part of the sentence,
there may be a prohibitively large number of pa-
rameters involved. Besides, He et al (2008) built
a maximum entropy model which combines rich
context information for selecting translation rules
during decoding. However, as the size of the cor-
pus increases, the maximum entropy model will
become larger. Similarly, In (Shen et al, 2009),
context language model is proposed for better rule
selection. Taking the dependency edge as condi-
tion, our approach is very different from previous
approaches of exploring context information.
3 Relaxed-well-formed Dependency
Structure
Dependency models have recently gained consid-
erable interest in SMT (Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008). Depen-
dency tree can represent richer structural infor-
mation. It reveals long-distance relation between
words and directly models the semantic structure
of a sentence without any constituent labels. Fig-
ure 2 shows an example of a dependency tree. In
this example, the word found is the root of the tree.
Shen et al (2008) propose the well-formed de-
pendency structure to filter the hierarchical rule ta-
ble. A well-formed dependency structure could be
either a single-rooted dependency tree or a set of
sibling trees. Although most rules are discarded
with the constraint that the target side should be
well-formed, this filtration leads to degradation in
translation performance.
As an extension of the work of (Shen et
al., 2008), we introduce the so-called relaxed-
well-formed dependency structure to filter the hi-
erarchical rule table. Given a sentence S =
w1w2...wn. Let d1d2...dn represent the position of
parent word for each word. For example, d3 = 4
means that w3 depends on w4. If wi is a root, we
define di = ?1.
Definition A dependency structure wi...wj is
a relaxed-well-formed structure, where there is
h /? [i, j], all the words wi...wj are directly or
indirectly depended on wh or -1 (here we define
h = ?1). If and only if it satisfies the following
conditions
? dh /? [i, j]
? ?k ? [i, j], dk ? [i, j] or dk = h
From the definition above, we can see that
the relaxed-well-formed structure obviously cov-
ers the well-formed one. In this structure, we
don?t constrain that all the children of the sub-root
should be complete. Let?s review the dependency
tree in Figure 2 as an example. Except for the well-
formed structure, we could also extract girl found
a beautiful house. Therefore, if the modifier The
lovely changes to The cute, this rule also works.
4 Head Word Trigger
(Koehn et al, 2003) introduced the concept of
lexical weighting to check how well words of
the phrase translate to each other. Source word
f aligns with target word e, according to the
IBM model 1, the lexical translation probability
is p(e|f). However, in the sense of dependency
relationship, we believe that the generation of the
target word e, is not only triggered by the aligned
source word f , but also associated with f ?s head
word f ?. Therefore, the lexical translation prob-
ability becomes p(e|f ? f ?), which of course
allows for a more fine-grained lexical choice of
143
the target word. More specifically, the probabil-
ity could be estimated by the maximum likelihood
(MLE) approach,
p(e|f ? f ?) = count(e, f ? f
?)
?
e? count(e?, f ? f ?)
(1)
Given a phrase pair f , e and word alignment
a, and the dependent relation of the source sen-
tence dJ1 (J is the length of the source sentence,
I is the length of the target sentence). Therefore,
given the lexical translation probability distribu-
tion p(e|f ? f ?), we compute the feature score of
a phrase pair (f , e) as
p(e|f, dJ1 , a)
= ?|e|i=1
1
|{j|(j, i) ? a}|
?
?(j,i)?a
p(ei|fj ? fdj) (2)
Now we get p(e|f, dJ1 , a), we could obtain
p(f |e, dI1, a) (dI1 represents dependent relation of
the target side) in the similar way. This new fea-
ture can be easily integrated into the log-linear
model as lexical weighting does.
5 Experiments
In this section, we describe the experimental set-
ting used in this work, and verify the effect of
the relaxed-well-formed structure filtering and the
new feature, head word trigger.
5.1 Experimental Setup
Experiments are carried out on the NIST1
Chinese-English translation task with two differ-
ent size of training corpora.
? FBIS: We use the FBIS corpus as the first
training corpus, which contains 239K sen-
tence pairs with 6.9M Chinese words and
8.9M English words.
? GQ: This is manually selected from the
LDC2 corpora. GQ contains 1.5M sentence
pairs with 41M Chinese words and 48M En-
glish words. In fact, FBIS is the subset of
GQ.
1www.nist.gov/speech/tests/mt
2It consists of six LDC corpora:
LDC2002E18, LDC2003E07, LDC2003E14, Hansards part
of LDC2004T07, LDC2004T08, LDC2005T06.
For language model, we use the SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram model on the first 1/3 of the Xinhua portion
of GIGAWORD corpus. And we use the NIST
2002 MT evaluation test set as our development
set, and NIST 2004, 2005 test sets as our blind
test sets. We evaluate the translation quality us-
ing case-insensitive BLEU metric (Papineni et
al., 2002) without dropping OOV words, and the
feature weights are tuned by minimum error rate
training (Och, 2003).
In order to get the dependency relation of the
training corpus, we re-implement a beam-search
style monolingual dependency parser according
to (Nivre and Scholz, 2004). Then we use the
same method suggested in (Chiang, 2005) to
extract SCFG grammar rules within dependency
constraint on both sides except that unaligned
words are allowed at the edge of phrases. Pa-
rameters of head word trigger are estimated as de-
scribed in Section 4. As a default, the maximum
initial phrase length is set to 10 and the maximum
rule length of the source side is set to 5. Besides,
we also re-implement the decoder of Hiero (Chi-
ang, 2007) as our baseline. In fact, we just exploit
the dependency structure during the rule extrac-
tion phase. Therefore, we don?t need to change
the main decoding algorithm of the SMT system.
5.2 Results on FBIS Corpus
A series of experiments was done on the FBIS cor-
pus. We first parse the bilingual languages with
monolingual dependency parser respectively, and
then only retain the rules that both sides are in line
with the constraint of dependency structure. In
Table 1, the relaxed-well-formed structure filtered
out 35% of the rule table and the well-formed dis-
carded 74%. RWF extracts additional 39% com-
pared to WF, which can be seen as some kind
of evidence that the rules we additional get seem
common in the sense of linguistics. Compared to
(Shen et al, 2008), we just use the dependency
structure to constrain rules, not to maintain the tree
structures to guide decoding.
Table 2 shows the translation result on FBIS.
We can see that the RWF structure constraint can
improve translation quality substantially both at
development set and different test sets. On the
Test04 task, it gains +0.86% BLEU, and +0.84%
on Test05. Besides, we also used Shen et al
(2008)?s WF structure to filter both sides. Al-
though it discard about 74% of the rule table, the
144
System Rule table size
HPB 30,152,090
RWF 19,610,255
WF 7,742,031
Table 1: Rule table size with different con-
straint on FBIS. Here HPB refers to the base-
line hierarchal phrase-based system, RWF means
relaxed-well-formed constraint and WF represents
the well-formed structure.
System Dev02 Test04 Test05
HPB 0.3285 0.3284 0.2965
WF 0.3125 0.3218 0.2887
RWF 0.3326 0.3370** 0.3050
RWF+Tri 0.3281 / 0.2965
Table 2: Results of FBIS corpus. Here Tri means
the feature of head word trigger on both sides. And
we don?t test the new feature on Test04 because of
the bad performance on development set. * or **
= significantly better than baseline (p < 0.05 or
0.01, respectively).
over-all BLEU is decreased by 0.66%-0.78% on
the test sets.
As for the feature of head word trigger, it seems
not work on the FBIS corpus. On Test05, it gets
the same score with the baseline, but lower than
RWF filtering. This may be caused by the data
sparseness problem, which results in inaccurate
parameter estimation of the new feature.
5.3 Result on GQ Corpus
In this part, we increased the size of the training
corpus to check whether the feature of head word
trigger works on large corpus.
We get 152M rule entries from the GQ corpus
according to (Chiang, 2007)?s extraction method.
If we use the RWF structure to constrain both
sides, the number of rules is 87M, about 43% of
rule entries are discarded. From Table 3, the new
System Dev02 Test04 Test05
HPB 0.3473 0.3386 0.3206
RWF 0.3539 0.3485** 0.3228
RWF+Tri 0.3540 0.3607** 0.3339*
Table 3: Results of GQ corpus. * or ** = sig-
nificantly better than baseline (p < 0.05 or 0.01,
respectively).
feature works well on two different test sets. The
gain is +2.21% BLEU on Test04, and +1.33% on
Test05. Compared to the result of the baseline,
only using the RWF structure to filter performs the
same as the baseline on Test05, and +0.99% gains
on Test04.
6 Conclusions
This paper proposes a simple strategy to filter the
hierarchal rule table, and introduces a new feature
to enhance the translation performance. We em-
ploy the relaxed-well-formed dependency struc-
ture to constrain both sides of the rule, and about
40% of rules are discarded with improvement of
the translation performance. In order to make full
use of the dependency information, we assume
that the target word e is triggered by dependency
edge of the corresponding source word f . And
this feature works well on large parallel training
corpus.
How to estimate the probability of head word
trigger is very important. Here we only get the pa-
rameters in a generative way. In the future, we we
are plan to exploit some discriminative approach
to train parameters of this feature, such as EM al-
gorithm (Hasan et al, 2008) or maximum entropy
(He et al, 2008).
Besides, the quality of the parser is another ef-
fect for this method. As the next step, we will
try to exploit bilingual knowledge to improve the
monolingual parser, i.e. (Huang et al, 2009).
Acknowledgments
This work was partly supported by National
Natural Science Foundation of China Contract
60873167. It was also funded by SK Telecom,
Korea under the contract 4360002953. We show
our special thanks to Wenbin Jiang and Shu Cai
for their valuable suggestions. We also thank
the anonymous reviewers for their insightful com-
ments.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In ACL
145
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 263?
270.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In ACL ?05: Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 541?548.
Sas?a Hasan and Hermann Ney. 2009. Comparison of
extended lexicon models in search and rescoring for
smt. In NAACL ?09: Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 17?20.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, and
Jesu?s Andre?s-Ferrer. 2008. Triplet lexicon models
for statistical machine translation. In EMNLP ?08:
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 372?
381.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In COLING ?08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 321?328.
Zhongjun He, Yao Meng, Yajuan Lu?, Hao Yu, and Qun
Liu. 2009. Reducing smt rule table with monolin-
gual key phrase. In ACL-IJCNLP ?09: Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 121?124.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1222?1231.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In EACL ?09:
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 380?388.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Joakim Nivre and Mario Scholz. 2004. Determinis-
tic dependency parsing of english text. In COLING
?04: Proceedings of the 20th international confer-
ence on Computational Linguistics, pages 64?70.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL ?02: Proceed-
ings of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In ACL ?05: Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of lin-
guistic and contextual information for statistical ma-
chine translation. In EMNLP ?09: Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 72?80.
Andreas Stolcke. 2002. Srilman extensible language
modeling toolkit. In In Proceedings of the 7th Inter-
national Conference on Spoken Language Process-
ing (ICSLP 2002), pages 901?904.
146
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1278?1287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Adjoining Tree-to-String Translation
Yang Liu, Qun Liu, and Yajuan Lu?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
We introduce synchronous tree adjoining
grammars (TAG) into tree-to-string transla-
tion, which converts a source tree to a target
string. Without reconstructing TAG deriva-
tions explicitly, our rule extraction algo-
rithm directly learns tree-to-string rules from
aligned Treebank-style trees. As tree-to-string
translation casts decoding as a tree parsing
problem rather than parsing, the decoder still
runs fast when adjoining is included. Less
than 2 times slower, the adjoining tree-to-
string system improves translation quality by
+0.7 BLEU over the baseline system only al-
lowing for tree substitution on NIST Chinese-
English test sets.
1 Introduction
Syntax-based translation models, which exploit hi-
erarchical structures of natural languages to guide
machine translation, have become increasingly pop-
ular in recent years. So far, most of them have
been based on synchronous context-free grammars
(CFG) (Chiang, 2007), tree substitution grammars
(TSG) (Eisner, 2003; Galley et al, 2006; Liu et
al., 2006; Huang et al, 2006; Zhang et al, 2008),
and inversion transduction grammars (ITG) (Wu,
1997; Xiong et al, 2006). Although these for-
malisms present simple and precise mechanisms for
describing the basic recursive structure of sentences,
they are not powerful enough to model some impor-
tant features of natural language syntax. For ex-
ample, Chiang (2006) points out that the transla-
tion of languages that can stack an unbounded num-
ber of clauses in an ?inside-out? way (Wu, 1997)
provably goes beyond the expressive power of syn-
chronous CFG and TSG. Therefore, it is necessary
to find ways to take advantage of more powerful syn-
chronous grammars to improve machine translation.
Synchronous tree adjoining grammars (TAG)
(Shieber and Schabes, 1990) are a good candidate.
As a formal tree rewriting system, TAG (Joshi et al,
1975; Joshi, 1985) provides a larger domain of lo-
cality than CFG to state linguistic dependencies that
are far apart since the formalism treats trees as basic
building blocks. As a mildly context-sensitive gram-
mar, TAG is conjectured to be powerful enough to
model natural languages. Synchronous TAG gener-
alizes TAG by allowing the construction of a pair
of trees using the TAG operations of substitution
and adjoining on tree pairs. The idea of using syn-
chronous TAG in machine translation has been pur-
sued by several researchers (Abeille et al, 1990;
Prigent, 1994; Dras, 1999), but only recently in
its probabilistic form (Nesson et al, 2006; De-
Neefe and Knight, 2009). Shieber (2007) argues that
probabilistic synchronous TAG possesses appealing
properties such as expressivity and trainability for
building a machine translation system.
However, one major challenge for applying syn-
chronous TAG to machine translation is computa-
tional complexity. While TAG requires O(n6) time
for monolingual parsing, synchronous TAG requires
O(n12) for bilingual parsing. One solution is to use
tree insertion grammars (TIG) introduced by Sch-
abes and Waters (1995). As a restricted form of
TAG, TIG still allows for adjoining of unbounded
trees but only requires O(n3) time for monolingual
parsing. Nesson et al (2006) firstly demonstrate
1278
o?
zo?ngto?ng
NN
NP
President
X,?1
{I
me?iguo?
NR
NP
US
X,?2
NP? NP?
NP
X? X?
X
,?1
NP
NP? NP
NN
o?
zo?ngto?ng
X
X? X
President
,?2
NP
NP
NR
{I
me?iguo?
NP
NN
o?
zo?ngto?ng
X
X
US
X
President
,?3
Figure 1: Initial and auxiliary tree pairs. The source side (Chinese) is a Treebank-style linguistic tree. The target side
(English) is a purely structural tree using a single non-terminal (X). By convention, substitution and foot nodes are
marked with a down arrow (?) and an asterisk (?), respectively. The dashed lines link substitution sites (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2) in tree pairs. Substituting the initial tree pair ?1 at the NP?-X?
node pair in the auxiliary tree pair ?1 yields a derived tree pair ?2, which can be adjoined at NN-X in ?2 to generate
?3.
the use of synchronous TIG for machine translation
and report promising results. DeNeefe and Knight
(2009) prove that adjoining can improve translation
quality significantly over a state-of-the-art string-
to-tree system (Galley et al, 2006) that uses syn-
chronous TSG with tractable computational com-
plexity.
In this paper, we introduce synchronous TAG into
tree-to-string translation (Liu et al, 2006; Huang et
al., 2006), which is the simplest and fastest among
syntax-based approaches (Section 2). We propose
a new rule extraction algorithm based on GHKM
(Galley et al, 2004) that directly induces a syn-
chronous TAG from an aligned and parsed bilingual
corpus without converting Treebank-style trees to
TAG derivations explicitly (Section 3). As tree-to-
string translation takes a source parse tree as input,
the decoding can be cast as a tree parsing problem
(Eisner, 2003): reconstructing TAG derivations from
a derived tree using tree-to-string rules that allow for
both substitution and adjoining. We describe how to
convert TAG derivations to translation forest (Sec-
tion 4). We evaluated the new tree-to-string system
on NIST Chinese-English tests and obtained con-
sistent improvements (+0.7 BLEU) over the STSG-
based baseline system without significant loss in ef-
ficiency (1.6 times slower) (Section 5).
2 Model
A synchronous TAG consists of a set of linked ele-
mentary tree pairs: initial and auxiliary. An initial
tree is a tree of which the interior nodes are all la-
beled with non-terminal symbols, and the nodes on
the frontier are either words or non-terminal sym-
bols marked with a down arrow (?). An auxiliary
tree is defined as an initial tree, except that exactly
one of its frontier nodes must be marked as foot
node (?). The foot node must be labeled with a non-
terminal symbol that is the same as the label of the
root node.
Synchronous TAG defines two operations to build
derived tree pairs from elementary tree pairs: substi-
tution and adjoining. Nodes in initial and auxiliary
tree pairs are linked to indicate the correspondence
between substitution and adjoining sites. Figure 1
shows three initial tree pairs (i.e., ?1, ?2, and ?3)
and two auxiliary tree pairs (i.e., ?1 and ?2). The
dashed lines link substitution nodes (e.g., NP? and
X? in ?1) and adjoining sites (e.g., NP and X in ?2)
in tree pairs. Substituting the initial tree pair ?1 at
1279
{I
me?iguo?
o?
zo?ngto?ng
n?
a`oba?ma?
?
du`?
l?
qia?ngj??
??
sh`?jia`n
??
yu?y??
gI
qia?nze?
0 1 2 3 4 5 6 7 8
NR NN NR P NN NN VV NN
NP NP NP NP NP
NP PP VP
NP VP
IP
US President Obama has condemned the shooting incident
Figure 2: A training example. Tree-to-string rules can be extracted from shaded nodes.
node minimal initial rule minimal auxiliary rule
NR0,1 [1] ( NR me?iguo? ) ? US
NP0,1 [2] ( NP ( x1:NR? ) ) ? x1
NN1,2 [3] ( NN zo?ngto?ng ) ? President
NP1,2 [4] ( NP ( x1:NN? ) ) ? x1
[5] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[6] ( NP0:1 ( x1:NR? ) ) ? x1 [7] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,2 [8] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[9] ( NP0:1 ( x1:NN? ) ) ? x1 [10] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[11] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NR2,3 [12] ( NR a`oba?ma? ) ? Obama
NP2,3 [13] ( NP ( x1:NR? ) ) ? x1
[14] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[15] ( NP0:2 ( x1:NP? ) ( x2:NP? ) ) ? x1 x2 [16] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
NP0,3 [17] ( NP0:1 ( x1:NR? ) ) ? x1 [18] ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
[19] ( NP0:1 ( x1:NN? ) ) ? x1
[20] ( NP0:1 ( x1:NR? ) ) ? x1
NN4,5 [21] ( NN qia?ngj?? ) ? shooting
NN5,6 [22] ( NN sh?`jia`n ) ? incident
NP4,6 [23] ( NP ( x1:NN? ) ( x2:NN? ) ) ? x1 x2
PP3,6 [24] ( PP ( du?` ) ( x1:NP? ) ) ? x1
NN7,8 [25] ( NN qia?nze? ) ? condemned
NP7,8 [26] ( NP ( x1:NN? ) ) ? x1
VP6,8 [27] ( VP ( VV yu?y?? ) ( x1:NP? ) ) ? x1
[28] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1VP3,8 [29] ( VP0:1 ( VV yu?y?? ) ( x1:NP? ) ) ? x1 [30] ( VP ( x1:PP? ) ( x2:VP? ) ) ? x2 the x1
IP0,8 [31] ( IP ( x1:NP? ) ( x2:VP? ) ) ? x1 has x2
Table 1: Minimal initial and auxiliary rules extracted from Figure 2. Note that an adjoining site has a span as subscript.
For example, NP0:1 in rule 6 indicates that the node is an adjoining site linked to a target node dominating the target
string spanning from position 0 to position 1 (i.e., x1). The target tree is hidden because tree-to-string translation only
considers the target surface string.
1280
the NP?-X? node pair in the auxiliary tree pair ?1
yields a derived tree pair ?2, which can be adjoined
at NN-X in ?2 to generate ?3.
For simplicity, we represent ?2 as a tree-to-string
rule:
( NP0:1 ( NR me?iguo? ) ) ? US
where NP0:1 indicates that the node is an adjoin-
ing site linked to a target node dominating the tar-
get string spanning from position 0 to position 1
(i.e., ?US?). The target tree is hidden because tree-
to-string translation only considers the target surface
string. Similarly, ?1 can be written as
( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
where x denotes a non-terminal and the subscripts
indicate the correspondence between source and tar-
get non-terminals.
The parameters of a probabilistic synchronous
TAG are
?
?
Pi(?) = 1 (1)
?
?
Ps(?|?) = 1 (2)
?
?
Pa(?|?) + Pa(NONE|?) = 1 (3)
where ? ranges over initial tree pairs, ? over aux-
iliary tree pairs, and ? over node pairs. Pi(?) is
the probability of beginning a derivation with ?;
Ps(?|?) is the probability of substituting ? at ?;
Pa(?|?) is the probability of adjoining ? at ?; fi-
nally, Pa(NONE|?) is the probability of nothing ad-
joining at ?.
For tree-to-string translation, these parameters
can be treated as feature functions of a discrimi-
native framework (Och, 2003) combined with other
conventional features such as relative frequency, lex-
ical weight, rule count, language model, and word
count (Liu et al, 2006).
3 Rule Extraction
Inducing a synchronous TAG from training data
often begins with converting Treebank-style parse
trees to TAG derivations (Xia, 1999; Chen and
Vijay-Shanker, 2000; Chiang, 2003). DeNeefe and
Knight (2009) propose an algorithm to extract syn-
chronous TIG rules from an aligned and parsed
bilingual corpus. They first classify tree nodes
into heads, arguments, and adjuncts using heuristics
(Collins, 2003), then transform a Treebank-style tree
into a TIG derivation, and finally extract minimally-
sized rules from the derivation tree and the string on
the other side, constrained by the alignments. Proba-
bilistic models can be estimated by collecting counts
over the derivation trees.
However, one challenge is that there are many
TAG derivations that can yield the same derived tree,
even with respect to a single grammar. It is difficult
to choose appropriate single derivations that enable
the resulting grammar to translate unseen data well.
DeNeefe and Knight (2009) indicate that the way to
reconstruct TIG derivations has a direct effect on fi-
nal translation quality. They suggest that one possi-
ble solution is to use derivation forest rather than a
single derivation tree for rule extraction.
Alternatively, we extend the GHKM algorithm
(Galley et al, 2004) to directly extract tree-to-string
rules that allow for both substitution and adjoining
from aligned and parsed data. There is no need for
transforming a parse tree into a TAG derivation ex-
plicitly before rule extraction and all derivations can
be easily reconstructed using extracted rules. 1 Our
rule extraction algorithm involves two steps: (1) ex-
tracting minimal rules and (2) composition.
3.1 Extracting Minimal Rules
Figure 2 shows a training example, which consists of
a Chinese parse tree, an English string, and the word
alignment between them. By convention, shaded
nodes are called frontier nodes from which tree-to-
string rules can be extracted. Note that the source
phrase dominated by a frontier node and its corre-
sponding target phrase are consistent with the word
alignment: all words in the source phrase are aligned
to all words in the corresponding target phrase and
vice versa.
We distinguish between three categories of tree-
1Note that our algorithm does not take heads, complements,
and adjuncts into consideration and extracts all possible rules
with respect to word alignment. Our hope is that this treatment
would make our system more robust in the presence of noisy
data. It is possible to use the linguistic preferences as features.
We leave this for future work.
1281
to-string rules:
1. substitution rules, in which the source tree is
an initial tree without adjoining sites.
2. adjoining rules, in which the source tree is an
initial tree with at least one adjoining site.
3. auxiliary rules, in which the source tree is an
auxiliary tree.
For example, in Figure 1, ?1 is a substitution rule,
?2 is an adjoining rule, and ?1 is an auxiliary rule.
Minimal substitution rules are the same with those
in STSG (Galley et al, 2004; Liu et al, 2006) and
therefore can be extracted directly using GHKM. By
minimal, we mean that the interior nodes are not
frontier and cannot be decomposed. For example,
in Table 2, rule 1 (for short r1) is a minimal substi-
tution rule extracted from NR0,1.
Minimal adjoining rules are defined as minimal
substitution rules, except that each root node must
be an adjoining site. In Table 2, r2 is a minimal
substitution rule extracted from NP0,1. As NP0,1 is
a descendant of NP0,2 with the same label, NP0,1
is a possible adjoining site. Therefore, r6 can be
derived from r2 and licensed as a minimal adjoining
rule extracted from NP0,2. Similarly, four minimal
adjoining rules are extracted from NP0,3 because it
has four frontier descendants labeled with NP.
Minimal auxiliary rules are derived from minimal
substitution and adjoining rules. For example, in Ta-
ble 2, r7 and r10 are derived from the minimal sub-
stitution rule r5 while r8 and r11 are derived from
r15. Note that a minimal auxiliary rule can have ad-
joining sites (e.g., r8).
Table 1 lists 17 minimal substitution rules, 7 min-
imal adjoining rules, and 7 minimal auxiliary rules
extracted from Figure 2.
3.2 Composition
We can obtain composed rules that capture rich con-
texts by substituting and adjoining minimal initial
and auxiliary rules. For example, the composition
of r12, r17, r25, r26, r29, and r31 yields an initial
rule with two adjoining sites:
( IP ( NP0:1 ( NR a`oba?ma? ) ) ( VP2:3 ( VV yu?y?? )
( NP ( NN qia?nze? ) ) ) ) ? Obama has condemned
Note that the source phrase ?a`oba?ma? . . . yu?y?? qia?nze??
is discontinuous. Our model allows both the source
and target phrases of an initial rule with adjoining
sites to be discontinuous, which goes beyond the ex-
pressive power of synchronous CFG and TSG.
Similarly, the composition of two auxiliary rules
r8 and r16 yields a new auxiliary rule:
( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( x3:NP? ) ) ? x1x2x3
We first compose initial rules and then com-
pose auxiliary rules, both in a bottom-up way. To
maintain a reasonable grammar size, we follow Liu
(2006) to restrict that the tree height of a rule is no
greater than 3 and the source surface string is no
longer than 7.
To learn the probability models Pi(?), Ps(?|?),
Pa(?|?), and Pa(NONE|?), we collect and normal-
ize counts over these extracted rules following De-
Neefe and Knight (2009).
4 Decoding
Given a synchronous TAG and a derived source tree
pi, a tree-to-string decoder finds the English yield
of the best derivation of which the Chinese yield
matches pi:
e? = e
(
arg max
D s.t. f(D)=pi
P (D)
)
(4)
This is called tree parsing (Eisner, 2003) as the de-
coder finds ways of decomposing pi into elementary
trees.
Tree-to-string decoding with STSG is usually
treated as forest rescoring (Huang and Chiang,
2007) that involves two steps. The decoder first con-
verts the input tree into a translation forest using a
translation rule set by pattern matching. Huang et
al. (2006) show that this step is a depth-first search
with memorization in O(n) time. Then, the decoder
searches for the best derivation in the translation for-
est intersected with n-gram language models and
outputs the target string. 2
Decoding with STAG, however, poses one major
challenge to forest rescoring. As translation forest
only supports substitution, it is difficult to construct
a translation forest for STAG derivations because of
2Mi et al (2008) give a detailed description of the two-step
decoding process. Huang and Mi (2010) systematically analyze
the decoding complexity of tree-to-string translation.
1282
?1
IP0,8
NP2,3 VP3,8?
NR2,3?
?2
NR2,3
n?
a`oba?ma?
?1
NP0,3
NP1,2 NP2,3?
NN1,2?
?2
NP0,3
NP0,2? NP
2,3
?
?3
NP0,2
NP0,1 NP1,2?
NR0,1?
?3
NN2,3
o?
zo?ngto?ng
elementary tree translation rule
?1 r1 ( IP ( NP0:1 ( x1:NR? ) ) ( x2:VP? ) ) ? x1 x2
?2 r2 ( NR a`oba?ma? ) ? Obama
?1 r3 ( NP ( NP0:1 ( x1:NN? ) ) ( x2:NP? ) ) ? x1 x2
?2 r4 ( NP ( x1:NP? ) ( x2:NP? ) ) ? x1 x2
?3 r5 ( NP ( NP ( x1:NR? ) ) ( x2:NP? ) ) ? x1 x2
?3 r6 ( NN zo?ngto?ng ) ? President
Figure 3: Matched trees and corresponding rules. Each node in a matched tree is annotated with a span as superscript
to facilitate identification. For example, IP0,8 in ?1 indicates that IP0,8 in Figure 2 is matched. Note that its left child
NP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site.
?1
?2(1.1) ?1(1) ?2(1)
?3(1) ?3(1.1)
IP0,8
NP0,2 VP3,8
NR0,1 NN1,2 NR2,3
e1 e2
e3 e4
hyperedge translation rule
e1 r1 + r4 ( IP ( NP ( x1:NP? ) ( NP ( x2:NR? ) ) ) ( x3:VP? ) ? x1 x2 x3
e2 r1 + r3 + r5 ( IP ( NP ( NP ( x1:NP? ) ( x2:NP? ) ) ( NP ( x3:NR? ) ) ) ( x4:VP? ) ) ? x1 x2 x3 x4
e3 r6 ( NN zo?ngto?ng ) ? President
e4 r2 ( NR a`oba?ma? ) ? Obama
Figure 4: Converting a derivation forest to a translation forest. In a derivation forest, a node in a derivation forest is a
matched elementary tree. A hyperedge corresponds to operations on related trees: substitution (dashed) or adjoining
(solid). We use Gorn addresses as tree addresses. ?2(1.1) denotes that ?2 is substituted in the tree ?1 at the node NR2,3?
of address 1.1 (i.e., the first child of the first child of the root node). As translation forest only supports substitution, we
combine trees with adjoining sites to form an equivalent tree without adjoining sites. Rules are composed accordingly
(e.g., r1 + r4).
1283
adjoining. Therefore, we divide forest rescoring for
STAG into three steps:
1. matching, matching STAG rules against the in-
put tree to obtain a TAG derivation forest;
2. conversion, converting the TAG derivation for-
est into a translation forest;
3. intersection, intersecting the translation forest
with an n-gram language model.
Given a tree-to-string rule, rule matching is to find
a subtree of the input tree that is identical to the
source side of the rule. While matching STSG rules
against a derived tree is straightforward, it is some-
what non-trivial for STAG rules that move beyond
nodes of a local tree. We follow Liu et al (2006) to
enumerate all elementary subtrees and match STAG
rules against these subtrees. This can be done by first
enumerating all minimal initial and auxiliary trees
and then combining them to obtain composed trees,
assuming that every node in the input tree is fron-
tier (see Section 3). We impose the same restrictions
on the tree height and length as in rule extraction.
Figure 3 shows some matched trees and correspond-
ing rules. Each node in a matched tree is annotated
with a span as superscript to facilitate identification.
For example, IP0,8 in ?1 means that IP0,8 in Figure
2 is matched. Note that its left child NP2,3 is not
its direct descendant in Figure 2, suggesting that ad-
joining is required at this site.
A TAG derivation tree specifies uniquely how
a derived tree is constructed using elementary trees
(Joshi, 1985). A node in a derivation tree is an ele-
mentary tree and an edge corresponds to operations
on related elementary trees: substitution or adjoin-
ing. We introduce TAG derivation forest, a com-
pact representation of multiple TAG derivation trees,
to encodes all matched TAG derivation trees of the
input derived tree.
Figure 4 shows part of a TAG derivation forest.
The six matched elementary trees are nodes in the
derivation forest. Dashed and solid lines represent
substitution and adjoining, respectively. We use
Gorn addresses as tree addresses: 0 is the address
of the root node, p is the address of the pth child of
the root node, and p ? q is the address of the qth child
of the node at the address p. The derivation forest
should be interpreted as follows: ?2 is substituted in
the tree ?1 at the node NR2,3? of address 1.1 (i.e., the
first child of the first child of the root node) and ?1 is
adjoined in the tree ?1 at the node NP2,3 of address
1.
To take advantage of existing decoding tech-
niques, it is necessary to convert a derivation forest
to a translation forest. A hyperedge in a transla-
tion forest corresponds to a translation rule. Mi et
al. (2008) describe how to convert a derived tree
to a translation forest using tree-to-string rules only
allowing for substitution. Unfortunately, it is not
straightforward to convert a derivation forest includ-
ing adjoining to a translation forest. To alleviate this
problem, we combine initial rules with adjoining
sites and associated auxiliary rules to form equiv-
alent initial rules without adjoining sites on the fly
during decoding.
Consider ?1 in Figure 3. It has an adjoining site
NP2,3. Adjoining ?2 in ?1 at the node NP2,3 pro-
duces an equivalent initial tree with only substitution
sites:
( IP0,8 ( NP0,3 ( NP0,2? ) ( NP2,3 ( NR2,3? ) ) ) ( VP3,8? ) )
The corresponding composed rule r1 + r4 has no
adjoining sites and can be added to translation forest.
We define that the elementary trees needed to be
composed (e.g., ?1 and ?2) form a composition tree
in a derivation forest. A node in a composition tree is
a matched elementary tree and an edge corresponds
to adjoining operations. The root node must be an
initial tree with at least one adjoining site. The de-
scendants of the root node must all be auxiliary trees.
For example, ( ?1 ( ?2 ) ) and ( ?1 ( ?1 ( ?3 ) ) ) are
two composition trees in Figure 4. The number of
children of a node in a composition tree depends on
the number of adjoining sites in the node. We use
composition forest to encode all possible composi-
tion trees.
Often, a node in a composition tree may have mul-
tiple matched rules. As a large amount of composi-
tion trees and composed rules can be identified and
constructed on the fly during forest conversion, we
used cube pruning (Chiang, 2007; Huang and Chi-
ang, 2007) to achieve a balance between translation
quality and decoding efficiency.
1284
category description number
VP verb phrase 12.40
NP noun phrase 7.69
IP simple clause 7.26
QP quantifier phrase 0.14
CP clause headed by C 0.10
PP preposition phrase 0.09
CLP classifier phrase 0.02
ADJP adjective phrase 0.02
LCP phrase formed by ?XP+LC? 0.02
DNP phrase formed by ?XP+DEG? 0.01
Table 2: Top-10 phrase categories of foot nodes and their
average occurrences in training corpus.
5 Evaluation
We evaluated our adjoining tree-to-string translation
system on Chinese-English translation. The bilin-
gual corpus consists of 1.5M sentences with 42.1M
Chinese words and 48.3M English words. The Chi-
nese sentences in the bilingual corpus were parsed
by an in-house parser. To maintain a reasonable
grammar size, we follow Liu et al (2006) to re-
strict that the height of a rule tree is no greater than
3 and the surface string?s length is no greater than 7.
After running GIZA++ (Och and Ney, 2003) to ob-
tain word alignment, our rule extraction algorithm
extracted 23.0M initial rules without adjoining sites,
6.6M initial rules with adjoining sites, and 5.3M
auxiliary rules. We used the SRILM toolkit (Stol-
cke, 2002) to train a 4-gram language model on the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the 2002
NIST MT Chinese-English test set as the develop-
ment set and the 2003-2005 NIST test sets as the
test sets. We evaluated translation quality using the
BLEU metric, as calculated by mteval-v11b.pl with
case-insensitive matching of n-grams.
Table 2 shows top-10 phrase categories of foot
nodes and their average occurrences in training cor-
pus. We find that VP (verb phrase) is most likely
to be the label of a foot node in an auxiliary rule.
On average, there are 12.4 nodes labeled with VP
are identical to one of its ancestors per tree. NP and
IP are also found to be foot node labels frequently.
Figure 4 shows the average occurrences of foot node
labels VP, NP, and IP over various distances. A dis-
tance is the difference of levels between a foot node
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
 0  1  2  3  4  5  6  7  8  9  10  11
av
er
ag
e 
oc
cu
rre
nc
e
distance
VP
IP
NP
Figure 5: Average occurrences of foot node labels VP,
NP, and IP over various distances.
system grammar MT03 MT04 MT05
Moses - 33.10 33.96 32.17
hierarchical SCFG 33.40 34.65 32.88
STSG 33.13 34.55 31.94
tree-to-string STAG 33.64 35.28 32.71
Table 3: BLEU scores on NIST Chinese-English test sets.
Scores marked in bold are significantly better that those
of STSG at pl.01 level.
and the root node. For example, in Figure 2, the dis-
tance between NP0,1 and NP0,3 is 2 and the distance
between VP6,8 and VP3,8 is 1. As most foot nodes
are usually very close to the root nodes, we restrict
that a foot node must be the direct descendant of the
root node in our experiments.
Table 3 shows the BLEU scores on the NIST
Chinese-English test sets. Our baseline system is the
tree-to-string system using STSG (Liu et al, 2006;
Huang et al, 2006). The STAG system outper-
forms the STSG system significantly on the MT04
and MT05 test sets at pl.01 level. Table 3 also
gives the results of Moses (Koehn et al, 2007) and
an in-house hierarchical phrase-based system (Chi-
ang, 2007). Our STAG system achieves compara-
ble performance with the hierarchical system. The
absolute improvement of +0.7 BLEU over STSG is
close to the finding of DeNeefe and Knight (2009)
on string-to-tree translation. We feel that one major
obstacle for achieving further improvement is that
composed rules generated on the fly during decod-
ing (e.g., r1 + r3 + r5 in Figure 4) usually have too
many non-terminals, making cube pruning in the in-
1285
STSG STAG
matching 0.086 0.109
conversion 0.000 0.562
intersection 0.946 1.064
other 0.012 0.028
total 1.044 1.763
Table 4: Comparison of average decoding time.
tersection phase suffering from severe search errors
(only a tiny fraction of the search space can be ex-
plored). To produce the 1-best translations on the
MT05 test set that contains 1,082 sentences, while
the STSG system used 40,169 initial rules without
adjoining sites, the STAG system used 28,046 initial
rules without adjoining sites, 1,057 initial rules with
adjoining sites, and 1,527 auxiliary rules.
Table 4 shows the average decoding time on the
MT05 test set. While rule matching for STSG needs
0.086 second per sentence, the matching time for
STAG only increases to 0.109 second. For STAG,
the conversion of derivation forests to translation
forests takes 0.562 second when we restrict that at
most 200 rules can be generated on the fly for each
node. As we use cube pruning, although the trans-
lation forest of STAG is bigger than that of STSG,
the intersection time barely increases. In total, the
STAG system runs in 1.763 seconds per sentence,
only 1.6 times slower than the baseline system.
6 Conclusion
We have presented a new tree-to-string translation
system based on synchronous TAG. With translation
rules learned from Treebank-style trees, the adjoin-
ing tree-to-string system outperforms the baseline
system using STSG without significant loss in effi-
ciency. We plan to introduce left-to-right target gen-
eration (Huang and Mi, 2010) into the STAG tree-
to-string system. Our work can also be extended to
forest-based rule extraction and decoding (Mi et al,
2008; Mi and Huang, 2008). It is also interesting to
introduce STAG into tree-to-tree translation (Zhang
et al, 2008; Liu et al, 2009; Chiang, 2010).
Acknowledgements
The authors were supported by National Natural
Science Foundation of China Contracts 60736014,
60873167, and 60903138. We thank the anonymous
reviewers for their insightful comments.
References
Anne Abeille, Yves Schabes, and Aravind Joshi. 1990.
Using lexicalized tags for machine translation. In
Proc. of COLING 1990.
John Chen and K. Vijay-Shanker. 2000. Automated ex-
traction of tags from the penn treebank. In Proc. of
IWPT 2000.
David Chiang. 2003. Statistical parsing with an au-
tomatically extracted tree adjoining grammar. Data-
Oriented Parsing.
David Chiang. 2006. An introduction to synchronous
grammars. ACL Tutorial.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. of ACL 2010.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Steve DeNeefe and Kevin Knight. 2009. Synchronous
tree adjoining machine translation. In Proc. of
EMNLP 2009.
Mark Dras. 1999. A meta-level grammar: Redefining
synchronous tag for translation and paraphrase. In
Proc. of ACL 1999.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL 2003.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of NAACL 2004.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc. of
ACL 2006.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL 2007.
Liang Huang and Haitao Mi. 2010. Efficient incremen-
tal decoding for tree-to-string translation. In Proc. of
EMNLP 2010.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006.
Aravind Joshi, L. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal of Computer and System
Sciences, 10(1).
Aravind Joshi. 1985. How much contextsensitiv-
ity is necessary for characterizing structural descrip-
tions)tree adjoining grammars. Natural Language
1286
Processing)Theoretical, Computational, and Psy-
chological Perspectives.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of ACL 2007 (poster), pages 77?80, Prague,
Czech Republic, June.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of ACL 2006.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc. of
ACL 2009.
Haitao Mi and Liang Huang. 2008. Forest-based transla-
tion rule extraction. In Proceedings of EMNLP 2008.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL/HLT 2008,
pages 192?199, Columbus, Ohio, USA, June.
Rebecca Nesson, Stuart Shieber, and Alexander Rush.
2006. Induction of probabilistic synchronous tree-
insertion grammars for machine translation. In Proc.
of AMTA 2006.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of ACL 2003.
Gilles Prigent. 1994. Synchronous tags and machine
translation. In Proc. of TAG+3.
Yves Schabes and Richard Waters. 1995. A cubic-time,
parsable formalism that lexicalizes context-free gram-
mar without changing the trees produced. Computa-
tional Linguistics, 21(4).
Stuart M. Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING 1990.
Stuart M. Shieber. 2007. Probabilistic synchronous tree-
adjoining grammars for machine translation: The ar-
gument from bilingual dictionaries. In Proc. of SSST
2007.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP 2002,
pages 901?904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proc. of the Fifth Natural Lan-
guage Processing Pacific Rim Symposium.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proc. of ACL 2006.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. of ACL 2008.
1287
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information?
Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,
Huailin Dong1, and Qun Liu2
Xiamen University, Xiamen, China1
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2
Baidu Inc., Beijing, China3
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
Abstract
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
1 Introduction
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al, 2003; Och and Ney, 2004;
Galley et al, 2006; Liu et al, 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
?Part of this work was done during the first author?s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al, 2008; Wu et al, 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al, 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
459
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al, 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, ?bank? often occurs in the sentences
related to the economy topic when translated into
?y?inha?ng?, and occurs in the sentences related to the
geography topic when translated to ?he?a`n?. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if ?bank? occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that ?bank?
is translated to ?y?inha?ng? than to ?he?a`n?. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
2 Background
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al, 2003), given a source sentence f = fJ1 =
f1, . . . , fj , . . . , fJ , a target sentence e = eI1 =
e1, . . . , ei, . . . , eI , and its word alignment a which
is a subset of the Cartesian product of word position-
s: a ? (j, i) : j = 1, . . . , J ; i = 1, . . . , I , the phrase
pair (f? , e?) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
?(e?|f?) =
count(f? , e?)
?
e??
count(f? , e??)
(1)
Here count(f? , e?) indicates how often the phrase pair
(f? , e?) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(e|f) by relative frequency from the train-
ing corpus:
w(e|f) =
count(f, e)
?
e?
count(f, e?)
(2)
Retaining the alignment a? between the phrase pair
(f? , e?), the corresponding lexical weight is calculated
as
pw(e?|f? , a?) =
|e?|?
i=1
1
|{j|(j, i) ? a?}|
?
?(j,i)?a?
w(ei|fj) (3)
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
460
3 Translation Model Adaptation via
Monolingual Topic Information
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
3.1 Hidden Topic Markov Model
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the ?bag-of-words? assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the ?bag-of-words? assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P (word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P (topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
3.2 Adapted Phrase Probability Estimation
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
? Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
? Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
? Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability ?(e?|f?)1 as
follows:
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|f? , tf ) ? P (tf |f?) (4)
where ?(e?|f? , tf ) indicates the probability of trans-
lating f? into e? given the source-side topic tf ,
P (tf |f?) denotes the phrase-topic distribution of f? .
To compute ?(e?|f?), we first apply HTMM to re-
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
?(e?|f? , tf ) and P (tf |f?) from these two corpora. To
avoid confusion, we further refine ?(e?|f? , tf ) and
P (tf |f?) with ?(e?|f? , tf out) and P (tf in|f?), respec-
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability ?(f? |e?), which can be
adjusted in a similar way to ?(e?|f?) with the help of in-domain
monolingual corpus in the target language.
461
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P (tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P (tf out|f?) as follows:
P (tf out|f?) =
?
tf in
P (tf out|tf in) ? P (tf in|f?) (5)
Thus formula (4) can be further refined as the fol-
lowing formula:
?(e?|f?) =
?
tf out
?
tf in
?(e?|f? , tf out)
?P (tf out|tf in) ? P (tf in|f?) (6)
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
3.2.1 Topic-Specific Phrase Translation
Probability ?(e?|f? , tf out)
We follow the common practice (Koehn et al,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
P (tf out|f) from the relevant topic model of Cf out,
the conditional probability ?(e?|f? , tf out) can be eas-
ily obtained by MLE method:
?(e?|f? , tf out)
=
?
?f ,e??Cout
count?f ,e?(f? , e?) ? P (tf out|f)
?
e??
?
?f ,e??Cout
count?f ,e?(f? , e??) ? P (tf out|f)
(7)
where Cout is the out-of-domain bilingual training
corpus, and count?f ,e?(f? , e?) denotes the number of
the phrase pair (f? , e?) in sentence pair ?f , e?.
3.2.2 Topic Mapping Probability P (tf out|tf in)
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
P (tf out|tf in) as follows:
P (tf out|tf in)
=
?
f?Cf out
?
Cf in
P (tf out|f) ? P (f |tf in) (8)
Here we first get P (f |tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P (tf out|f) as:
P (tf out|f)
=
?
f?Cf out
countf (f) ? P (tf out|f)
?
tf out
?
f?Cf out
countf (f) ? P (tf out|f)
(9)
where countf (f) denotes the number of the word f
in sentence f .
3.2.3 Phrase-Topic Distribution P (tf in|f? )
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
P (tf in|f?) = ? ? Pmle(tf in|f?) +
(1? ?) ? Pword(tf in|f?) (10)
where Pmle(tf in|f?) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in|f?) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
? is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f? in sentence f
denoted as countf (f?), we compute the in-domain
phrase-topic distribution in the following way:
Pmle(tf in|f?)
=
?
f?Cf in
countf (f?) ? P (tf in|f)
?
tf in
?
f?Cf in
countf (f?) ? P (tf in|f)
(11)
462
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in|f?). One is
a ?Noisy-OR? combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in|f?) is
defined as:
Pword(tf in|f?)
= 1? Pword(t?f in|f?)
? 1?
?
fj?f?
P (t?f in|fj)
= 1?
?
fj?f?
(1? P (tf in|fj)) (12)
where Pword(t?f in|f?) represents the probability that
tf in is not the topic of the phrase f? . Similarly,
P (t?f in|fj) indicates the probability that tf in is not
the topic of the word fj .
The other method is an ?Averaging? combination
one. With the assumption that tf in is the topic of f?
if at least one of the words in f? belongs to this topic,
we derive Pword(tf in|f?) as follows:
Pword(tf in|f?) ?
?
fj?f?
P (tf in|fj)/|f? | (13)
where |f? | denotes the number of words in phrase f? .
3.3 Adapted Lexical Probability Estimation
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
word, so
w(e|f) =
?
tf out
?
tf in
w(e|f, tf out)
?P (tf out|tf in) ? P (tf in|f) (14)
Here we obtain w(e|f, tf out) with a simi-
lar approach to ?(e?|f? , tf out), and calculate
P (tf out|tf in) and P (tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair (f? , e?).
4 Experiment
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
4.1 Experimental setup
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al, 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, ? = 1.5, ? = 1.01,
iters = 100. See (Gruber et al, 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight ? in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
463
phrase-based machine translation system (Koehn
et al, 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
?grow-diag-final-and? are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: ?Noisy-OR? and ?Averaging?.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
Adaptation
Method
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
Table 1: Experimental results using different smoothing
methods.
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both ?Noisy-OR? and ?Averaging? combination
methods improve the performance over the base-
line, and ?Averaging? method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the ?Noisy-OR? method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than ?Averaging?
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
?Averaging?method.
4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with ?OutBP? and ?AdapBP?, respec-
tively.
Used Phrase
Table
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both ?AdapBP? and ?OutBP + AdapBP?
consistently outperform the baseline, and the lat-
464
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
ter produces further improvements over the former.
Specifically, the BLEU scores of the ?OutBP +
AdapBP? method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
?AdapBP? method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the ?AdapBP?
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by ?AdapBP?,
therefore further improving the translation quality.
4.2.3 Effect of In-domain Monolingual Corpus
Size
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the ?OutBP + AdapBP? method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P < 0.01 using the significance test tool
developed by Zhang et al(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
5 Related work
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al, 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al, 2009) or the phrase pairs of phrase ta-
ble (Foster et al, 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
465
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al, 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al, 2007; Carpuat and Wu, 2007; He
et al, 2008; Liu et al, 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
? We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
? In addition to the utilization of in-domain
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al, 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model ? HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
? Different from trigger-based lexicon model and
context-dependent translation selection both of
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
6 Conclusion and future work
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
466
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
Acknowledgement
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. of ACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?s
Andre?s-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
467
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Sas?a
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Sas?a Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
468
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750?758,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Topic Similarity Model
for Hierarchical Phrase-based Translation
Xinyan Xiao? Deyi Xiong? Min Zhang?? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Human Language Technology
Institute of Computing Technology Institute for Infocomm Research
Chinese Academy of Sciences
{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang?}@i2r.a-star.edu.sg
Abstract
Previous work using topic model for statis-
tical machine translation (SMT) explore top-
ic information at the word level. Howev-
er, SMT has been advanced from word-based
paradigm to phrase/rule-based paradigm. We
therefore propose a topic similarity model to
exploit topic information at the synchronous
rule level for hierarchical phrase-based trans-
lation. We associate each synchronous rule
with a topic distribution, and select desirable
rules according to the similarity of their top-
ic distributions with given documents. We
show that our model significantly improves
the translation performance over the baseline
on NIST Chinese-to-English translation ex-
periments. Our model also achieves a better
performance and a faster speed than previous
approaches that work at the word level.
1 Introduction
Topic model (Hofmann, 1999; Blei et al, 2003) is
a popular technique for discovering the underlying
topic structure of documents. To exploit topic infor-
mation for statistical machine translation (SMT), re-
searchers have proposed various topic-specific lexi-
con translation models (Zhao and Xing, 2006; Zhao
and Xing, 2007; Tam et al, 2007) to improve trans-
lation quality.
Topic-specific lexicon translation models focus
on word-level translations. Such models first esti-
mate word translation probabilities conditioned on
topics, and then adapt lexical weights of phrases
?Corresponding author
by these probabilities. However, the state-of-the-
art SMT systems translate sentences by using se-
quences of synchronous rules or phrases, instead of
translating word by word. Since a synchronous rule
is rarely factorized into individual words, we believe
that it is more reasonable to incorporate the topic
model directly at the rule level rather than the word
level.
Consequently, we propose a topic similari-
ty model for hierarchical phrase-based translation
(Chiang, 2007), where each synchronous rule is as-
sociated with a topic distribution. In particular,
? Given a document to be translated, we cal-
culate the topic similarity between a rule and
the document based on their topic distributions.
We augment the hierarchical phrase-based sys-
tem by integrating the proposed topic similarity
model as a new feature (Section 3.1).
? As we will discuss in Section 3.2, the similarity
between a generic rule and a given source docu-
ment computed by our topic similarity model is
often very low. We don?t want to penalize these
generic rules. Therefore we further propose a
topic sensitivity model which rewards generic
rules so as to complement the topic similarity
model.
? We estimate the topic distribution for a rule
based on both the source and target side topic
models (Section 4.1). In order to calculate sim-
ilarities between target-side topic distributions
of rules and source-side topic distributions of
given documents during decoding, we project
750
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(a) ?? ?? ? opera-
tional capability
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(b) ??X1 ? grandsX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(c) ??X1 ? giveX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(d) X1 ?? ?? X2 ?
held talksX1 X2
Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution,
where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares
the same source Chinese string, but they have different topic distributions due to the different English translations.
the target-side topic distributions of rules into
the space of source-side topic model by one-to-
many projection (Section 4.2).
Experiments on Chinese-English translation tasks
(Section 6) show that, our method outperforms the
baseline hierarchial phrase-based system by +0.9
BLEU points. This result is also +0.5 points high-
er and 3 times faster than the previous topic-specific
lexicon translation method. We further show that
both the source-side and target-side topic distribu-
tions improve translation quality and their improve-
ments are complementary to each other.
2 Background: Topic Model
A topic model is used for discovering the topics
that occur in a collection of documents. Both La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
and Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999) are types of topic models. LDA
is the most common topic model currently in use,
therefore we exploit it for mining topics in this pa-
per. Here, we first give a brief description of LDA.
LDA views each document as a mixture pro-
portion of various topics, and generates each word
by multinomial distribution conditioned on a topic.
More specifically, as a generative process, LDA first
samples a document-topic distribution for each doc-
ument. Then, for each word in the document, it sam-
ples a topic index from the document-topic distribu-
tion and samples the word conditioned on the topic
index according the topic-word distribution.
Generally speaking, LDA contains two types of
parameters. The first one relates to the document-
topic distribution, which records the topic distribu-
tion of each document. The second one is used for
topic-word distribution, which represents each topic
as a distribution over words. Based on these param-
eters (and some hyper-parameters), LDA can infer a
topic assignment for each word in the documents. In
the following sections, we will use these parameters
and the topic assignments of words to estimate the
parameters in our method.
3 Topic Similarity Model
Sentences should be translated in consistence with
their topics (Zhao and Xing, 2006; Zhao and Xing,
2007; Tam et al, 2007). In the hierarchical phrase
based system, a synchronous rule may be related to
some topics and unrelated to others. In terms of
probability, a rule often has an uneven probability
distribution over topics. The probability over a topic
is high if the rule is highly related to the topic, other-
wise the probability will be low. Therefore, we use
topic distribution to describe the relatedness of rules
to topics.
Figure 1 shows four synchronous rules (Chiang,
2007) with topic distributions, some of which con-
tain nonterminals. We can see that, although the
source part of rule (b) and (c) are identical, their top-
ic distributions are quite different. Rule (b) contains
a highest probability on the topic about ?China-U.S.
relationship?, which means rule (b) is much more
related to this topic. In contrast, rule (c) contains
an even distribution over various topics. Thus, giv-
en a document about ?China-U.S. relationship?, we
hope to encourage the system to apply rule (b) but
penalize the application of rule (c). We achieve this
by calculating similarity between the topic distribu-
tions of a rule and a document to be translated.
More formally, we associate each rule with a rule-
topic distribution P (z|r), where r is a rule, and z is
a topic. Suppose there are K topics, this distribution
751
can be represented by a K-dimension vector. The
k-th component P (z = k|r) means the probability
of topic k given the rule r. The estimation of such
distribution will be described in Section 4.
Analogously, we represent the topic information
of a document d to be translated by a document-
topic distribution P (z|d), which is also a K-
dimension vector. The k-th dimension P (z = k|d)
means the probability of topic k given document d.
Different from rule-topic distribution, the document-
topic distribution can be directly inferred by an off-
the-shelf LDA tool.
Consequently, based on these two distribution-
s, we select a rule for a document to be translat-
ed according to their topic similarity (Section 3.1),
which measures the relatedness of the rule to the
document. In order to encourage the application
of generic rules which are often penalized by our
similarity model, we also propose a topic sensitivity
model (Section 3.2).
3.1 Topic Similarity
By comparing the similarity of their topic distribu-
tions, we are able to decide whether a rule is suitable
for a given source document. The topic similarity
computes the distance of two topic distributions. We
calculate the topic similarity by Hellinger function:
Similarity(P (z|d), P (z|r))
=
K
?
k=1
(
?
P (z = k|d) ?
?
P (z = k|r)
)2
(1)
Hellinger function is used to calculate distribution
distance and is popular in topic model (Blei and Laf-
ferty, 2007).1 By topic similarity, we aim to encour-
age or penalize the application of a rule for a giv-
en document according to their topic distributions,
which then helps the SMT system make better trans-
lation decisions.
3.2 Topic Sensitivity
Domain adaptation (Wu et al, 2008; Bertoldi and
Federico, 2009) often distinguishes general-domain
data from in-domain data. Similarly, we divide the
rules into topic-insensitive rules and topic-sensitive
1We also try other distance functions, including Euclidean
distance, Kullback-Leibler divergence and cosine function.
They produce similar results in our preliminary experiments.
rules according to their topic distributions. Let?s
revisit Figure 1. We can easily find that the topic
distribution of rule (c) distribute evenly. This in-
dicates that it is insensitive to topics, and can be
applied in any topics. We call such a rule a topic-
insensitive rule. In contrast, the distributions of the
rest rules peak on a few topics. Such rules are called
topic-sensitive rules. Generally speaking, a topic-
insensitive rule has a fairly flat distribution, while a
topic-sensitive rule has a sharp distribution.
A document typically focuses on a few topics, and
has a sharp topic distribution. In contrast, the distri-
bution of topic-insensitive rule is fairly flat. Hence,
a topic-insensitive rule is always less similar to doc-
uments and is punished by the similarity function.
However, topic-insensitive rules may be more
preferable than topic-sensitive rules if neither of
them are similar to given documents. For a doc-
ument about the ?military? topic, the rule (b) and
(c) in Figure 1 are both dissimilar to the document,
because rule (b) relates to the ?China-U.S. relation-
ship? topic and rule (c) is topic-insensitive. Never-
theless, since rule (c) occurs more frequently across
various topics, it may be better to apply rule (c).
To address such issue of the topic similarity mod-
el, we further introduce a topic sensitivity model to
describe the topic sensitivity of a rule using entropy
as a metric:
Sensitivity(P (z|r))
= ?
K
?
k=1
P (z = k|r) ? log (P (z = k|r)) (2)
According to the Eq. (2), a topic-insensitive rule has
a large entropy, while a topic-sensitive rule has a s-
maller entropy. By incorporating the topic sensitivi-
ty model with the topic similarity model, we enable
our SMT system to balance the selection of these t-
wo types of rules. Given rules with approximately
equal values of Eq. (1), we prefer topic-insensitive
rules.
4 Estimation
Unlike document-topic distribution that can be di-
rectly learned by LDA tools, we need to estimate the
rule-topic distribution according to our requirement.
In this paper, we try to exploit the topic information
752
of both source and target language. To achieve this
goal, we use both source-side and target-side mono-
lingual topic models, and learn the correspondence
between the two topic models from word-aligned
bilingual corpus.
Specifically, we use two types of rule-topic dis-
tributions: one is source-side rule-topic distribution
and the other is target-side rule-topic distribution.
These two rule-topic distributions are estimated by
corresponding topic models in the same way (Sec-
tion 4.1). Notably, only source language documents
are available during decoding. In order to compute
the similarity between the target-side topic distribu-
tion of a rule and the source-side topic distribution
of a given document?we need to project the target-
side topic distribution of a synchronous rule into the
space of the source-side topic model (Section 4.2).
A more principle way is to learn a bilingual topic
model from bilingual corpus (Mimno et al, 2009).
However, we may face difficulty during decoding,
where only source language documents are avail-
able. It requires a marginalization to infer the mono-
lingual topic distribution using the bilingual topic
model. The high complexity of marginalization pro-
hibits such a summation in practice. Previous work
on bilingual topic model avoid this problem by some
monolingual assumptions. Zhao and Xing (2007)
assume that the topic model is generated in a mono-
lingual manner, while Tam et al, (2007) construct
their bilingual topic model by enforcing a one-to-
one correspondence between two monolingual topic
models. We also estimate our rule-topic distribution
by two monolingual topic models, but use a differ-
ent way to project target-side topics onto source-side
topics.
4.1 Monolingual Topic Distribution Estimation
We estimate rule-topic distribution from word-
aligned bilingual training corpus with documen-
t boundaries explicitly given. The source and tar-
get side distributions are estimated in the same way.
For simplicity, we only describe the estimation of
source-side distribution in this section.
The process of rule-topic distribution estimation
is analogous to the traditional estimation of rule
translation probability (Chiang, 2007). In addition
to the word-aligned corpus, the input for estimation
also contains the source-side topic-document distri-
bution of every documents inferred by LDA tool.
We first extract synchronous rules from training
data in a traditional way. When a rule r is extracted
from a document d with topic distribution P (z|d),
we collect an instance (r, P (z|d), c), where c is the
fraction count of an instance as described in Chiang,
(2007). After extraction, we get a set of instances
I = {(r, P (z|d), c)} with different document-topic
distributions for each rule. Using these instances,
we calculate the topic probability P (z = k|r) as
follows:
P (z = k|r) =
?
I?I c? P (z = k|d)
?K
k?=1
?
I?I c? P (z = k?|d)
(3)
By using both source-side and target-side
document-topic distribution, we obtain two rule-
topic distributions for each rule in total.
4.2 Target-side Topic Distribution Projection
As described in the previous section, we also esti-
mate the target-side rule-topic distribution. How-
ever, only source document-topic distributions are
available during decoding. In order to calculate
the similarity between the target-side rule-topic dis-
tribution of a rule and the source-side document-
topic distribution of a source document, we need to
project target-side topics into the source-side topic
space. The projection contains two steps:
? In the first step, we learn the topic-to-topic cor-
respondence probability p(zf |ze) from target-
side topic ze to source-side topic zf .
? In the second step, we project the target-side
topic distribution of a rule into source-side top-
ic space using the correspondence probability.
In the first step, we estimate the correspondence
probability by the co-occurrence of the source-side
and the target-side topic assignment of the word-
aligned corpus. The topic assignments are output
by LDA tool. Thus, we denotes each sentence pair
by (zf , ze,a), where zf and ze are the topic as-
signments of source-side and target-side sentences
respectively, and a is a set of links {(i, j)}. A
link (i, j) means a source-side position i aligns to
a target-side position j. Thus, the co-occurrence of
a source-side topic with index kf and a target-side
753
e-topic f-topic 1 f-topic 2 f-topic 3
enterprises ??(agricultural) ??(enterprise) ??(develop)
rural ??(rural) ??(market) ??(economic)
state ??(peasant) ??(state) ??(technology )
agricultural ??(reform) ??(company) ??(China)
market ??(finance) ??(finance) ??(technique)
reform ??(social) ??(bank) ??(industry)
production ??(safety) ??(investment) ??(structure)
peasants ??(adjust) ??(manage) ??(innovation)
owned ??(policy) ??(reform) ??(accelerate)
enterprise ??(income) ??(operation) ??(reform)
p(zf |ze) 0.38 0.28 0.16
Table 1: Example of topic-to-topic correspondence. The
last line shows the correspondence probability. Each col-
umnmeans a topic represented by its top-10 topical word-
s. The first column is a target-side topic, while the rest
three columns are source-side topics.
topic ke is calculated by:
?
(zf ,ze,a)
?
(i,j)?a
?(zfi , kf ) ? ?(zej , ke) (4)
where ?(x, y) is the Kronecker function, which is 1
if x = y and 0 otherwise. We then compute the
probability of P (z = kf |z = ke) by normalizing
the co-occurrence count. Overall, after the first step,
we obtain an correspondence matrix MKe?Kf from
target-side topic to source-side topic, where the item
Mi,j represents the probability P (zf = i|ze = j).
In the second step, given the correspondence ma-
trix MKe?Kf , we project the target-side rule-topic
distribution P (ze|r) to the source-side topic space
by multiplication as follows:
T (P (ze|r)) = P (ze|r) ?MKe?Kf (5)
In this way, we get a second distribution for a rule
in the source-side topic space, which we called pro-
jected target-side topic distribution T (P (ze|r)).
Obviously, our projection method allows one
target-side topic to align to multiple source-side top-
ics. This is different from the one-to-one correspon-
dence used by Tam et al, (2007). From the training
result of the correspondence matrix MKe?Kf , we
find that the topic correspondence between source
and target language is not necessarily one-to-one.
Typically, the probability P (z = kf |z = ke) of a
target-side topic mainly distributes on two or three
source-side topics. Table 1 shows an example of
a target-side topic with its three mainly aligned
source-side topics.
5 Decoding
We incorporate our topic similarity model as a
new feature into a traditional hiero system (Chi-
ang, 2007) under discriminative framework (Och
and Ney, 2002). Considering there are a source-
side rule-topic distribution and a projected target-
side rule-topic distribution, we add four features in
total:
? Similarity (P (zf |d), P (zf |r))
? Similarity(P (zf |d), T (P (ze|r)))
? Sensitivity(P (zf |r))
? Sensitivity(T (P (ze|r))
To calculate the total score of a derivation on each
feature listed above during decoding, we sum up the
correspondent feature score of each applied rule.2
The source-side and projected target-side rule-
topic distribution are calculated before decoding.
During decoding, we first infer the topic distribution
P (zf |d) for a given document on source language.
When applying a rule, it is straightforward to calcu-
late these topic features. Obviously, the computa-
tional cost of these features is rather small.
In the topic-specific lexicon translation model,
given a source document, it first calculates the topic-
specific translation probability by normalizing the
entire lexicon translation table, and then adapts the
lexical weights of rules correspondingly. This makes
the decoding slower. Therefore, comparing with the
previous topic-specific lexicon translation method,
our method provides a more efficient way for incor-
porating topic model into SMT.
6 Experiments
We try to answer the following questions by experi-
ments:
1. Is our topic similarity model able to improve
translation quality in terms of BLEU? Further-
more, are source-side and target-side rule-topic
distributions complementary to each other?
2Since glue rule and rules of unknown words are not extract-
ed from training data, here, we just ignore the calculation of the
four features for them.
754
System MT06 MT08 Avg Speed
Baseline 30.20 21.93 26.07 12.6
TopicLex 30.65 22.29 26.47 3.3
SimSrc 30.41 22.69 26.55 11.5
SimTgt 30.51 22.39 26.45 11.7
SimSrc+SimTgt 30.73 22.69 26.71 11.2
Sim+Sen 30.95 22.92 26.94 10.2
Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the
traditional hierarchical system (?Baseline?) and the topic-specific lexicon translation method (?TopicLex?). ?SimSrc?
and ?SimTgt? denote similarity by source-side and target-side rule-distribution respectively, while ?Sim+Sen? acti-
vates the two similarity and two sensitivity features. ?Avg? is the average BLEU score on the two test sets. Scores
marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01).
2. Is it helpful to introduce the topic sensitivi-
ty model to distinguish topic-insensitive and
topic-sensitive rules?
3. Is it necessary to project topics by one-to-many
correspondence instead of one-to-one corre-
spondence?
4. What is the effect of our method on various
types of rules, such as phrase rules and rules
with non-terminals?
6.1 Data
We present our experiments on the NIST Chinese-
English translation tasks. The bilingual training da-
ta contains 239K sentence pairs with 6.9M Chinese
words and 9.14M English words, which comes from
the FBIS portion of LDC data. There are 10,947
documents in the FBIS corpus. The monolingual da-
ta for training English language model includes the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the NIST
evaluation set of 2005 (MT05) as our development
set, and sets of MT06/MT08 as test sets. The num-
bers of documents in MT05, MT06, MT08 are 100,
79, and 109 respectively.
We obtained symmetric word alignments of train-
ing data by first running GIZA++ (Och and Ney,
2003) in both directions and then applying re-
finement rule ?grow-diag-final-and? (Koehn et al,
2003). The SCFG rules are extracted from this
word-aligned training data. A 4-gram language
model was trained on the monolingual data by the
SRILM toolkit (Stolcke, 2002). Case-insensitive
NIST BLEU (Papineni et al, 2002) was used to mea-
sure translation performance. We used minimum er-
ror rate training (Och, 2003) for optimizing the fea-
ture weights.
For the topic model, we used the open source L-
DA tool GibbsLDA++ for estimation and inference.3
GibssLDA++ is an implementation of LDA using
gibbs sampling for parameter estimation and infer-
ence. The source-side and target-side topic models
are estimated from the Chinese part and English part
of FBIS corpus respectively. We set the number of
topic K = 30 for both source-side and target-side,
and use the default setting of the tool for training and
inference.4 During decoding, we first infer the top-
ic distribution of given documents before translation
according to the topic model trained on Chinese part
of FBIS corpus.
6.2 Effect of Topic Similarity Model
We compare our method with two baselines. In addi-
tion to the traditional hiero system, we also compare
with the topic-specific lexicon translation method in
Zhao and Xing (2007). The lexicon translation prob-
ability is adapted by:
p(f |e,DF ) ? p(e|f,DF )P (f |DF ) (6)
=
?
k
p(e|f, z = k)p(f |z = k)p(z = k|DF ) (7)
However, we simplify the estimation of p(e|f, z =
k) by directly using the word alignment corpus with
3http://gibbslda.sourceforge.net/
4We determine K by testing {15, 30, 50, 100, 200} in our
preliminary experiments. We find that K = 30 produces a s-
lightly better performance than other values.
755
Type Count Src% Tgt%
Phrase-rule 3.9M 83.4 84.4
Monotone-rule 19.2M 85.3 86.1
Reordering-rule 5.7M 85.9 86.8
All-rule 28.8M 85.1 86.0
Table 3: Percentage of topic-sensitive rules of various
types of rule according to source-side (?Src?) and target-
side (?Tgt?) topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain
nonterminals (Section 6.5).
topic assignment that is inferred by the GibbsL-
DA++. Despite the simplification of estimation, the
improvement of our implementation is comparable
with the improvement in Zhao et al,(2007). Given a
new document, we need to adapt the lexical transla-
tion weights of the rules based on topic model. The
adapted lexicon translation model is added as a new
feature under the discriminative framework.
Table 2 shows the result of our method compar-
ing with the traditional system and the topic-lexicon
specific translation method described as above. By
using all the features (last line in the table), we im-
prove the translation performance over the baseline
system by 0.87 BLEU point on average. Our method
also outperforms the topic-lexicon specific transla-
tion method by 0.47 points. This verifies that topic
similarity model can improve the translation quality
significantly.
In order to gain insights into why our model is
helpful, we further investigate how many rules are
topic-sensitive. As described in Section 3.2, we use
entropy to measure the topic sensitivity. If the en-
tropy of a rule is smaller than a certain threshold,
then the rule is topic sensitive. Since documents of-
ten focus on some topics, we use the average entropy
of document-topic distribution of all training docu-
ments as the threshold. We compare both source-
side and target-side distribution shown in Table 3.
We find that more than 80 percents of the rules are
topic-sensitive, thus provides us a large space to im-
prove the translation by exploiting topics.
We also compare these methods in terms of the
decoding speed (words/second). The baseline trans-
lates 12.6 words per second, while the topic-specific
lexicon translation method only translates 3.3 word-
s in one second. The overhead of the topic-specific
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
One-to-One 30.27 22.12 26.20
One-to-Many 30.51 22.39 26.45
Table 4: Effects of one-to-one and one-to-many topic pro-
jection.
lexicon translation method mainly comes from the
adaptation of lexical weights. It takes 72.8% of
the time to do the adaptation, despite only lexical
weights of the used rules are adapted. In contrast,
our method has a speed of 10.2 words per second for
each sentence on average, which is three times faster
than the topic-specific lexicon translation method.
Meanwhile, we try to separate the effects of
source-side topic distribution from the target-side
topic distribution. From lines 4-6 of Table 2. We
clearly find that the two rule-topic distributions im-
prove the performance by 0.48 and 0.38 BLEU
points over the baseline respectively. It seems that
the source-side topic model is more helpful. Fur-
thermore, when combine these two distributions, the
improvement is increased to 0.64 points. This indi-
cates that the effects of source-side and target-side
distributions are complementary.
6.3 Effect of Topic Sensitivity Model
As described in Section 3.2, because the similari-
ty features always punish topic-insensitive rules, we
introduce topic sensitivity features as a complemen-
t. In the last line of Table 2, we obtain a fur-
ther improvement of 0.23 points, when incorporat-
ing topic sensitivity features with topic similarity
features. This suggests that it is necessary to dis-
tinguish topic-insensitive and topic-sensitive rules.
6.4 One-to-One Vs. One-to-Many Topic
Projection
In Section 4.2, we find that source-side topic and
target-side topics may not exactly match, hence we
use one-to-many topic correspondence. Yet anoth-
er method is to enforce one-to-one topic projection
(Tam et al, 2007). We achieve one-to-one projection
by aligning a target topic to the source topic with the
largest correspondence probability as calculated in
Section 4.2.
Table 4 compares the effects of these two method-
756
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
Phrase-rule 30.53 22.29 26.41
Monotone-rule 30.72 22.62 26.67
Reordering-rule 30.31 22.40 26.36
All-rule 30.95 22.92 26.94
Table 5: Effect of our topic model on three types of rules.
Phrase rules are fully lexicalized, while monotone and
reordering rules contain nonterminals.
s. We find that the enforced one-to-one topic method
obtains a slight improvement over the baseline sys-
tem, while one-to-many projection achieves a larger
improvement. This confirms our observation of the
non-one-to-one mapping between source-side and
target-side topics.
6.5 Effect on Various Types of Rules
To get a more detailed analysis of the result, we
further compare the effect of our method on differ-
ent types of rules. We divide the rules into three
types: phrase rules, which only contain terminal-
s and are the same as the phrase pairs in phrase-
based system; monotone rules, which contain non-
terminals and produce monotone translations; re-
ordering rules, which also contain non-terminals but
change the order of translations. We define the
monotone and reordering rules according to Chiang
et al, (2008).
Table 5 show the results. We can see that our
method achieves improvements on all the three type-
s of rules. Our topic similarity method on mono-
tone rule achieves the most improvement which is
0.6 BLEU points, while the improvement on reorder-
ing rules is the smallest among the three types. This
shows that topic information also helps the selec-
tions of rules with non-terminals.
7 Related Work
In addition to the topic-specific lexicon transla-
tion method mentioned in the previous sections,
researchers also explore topic model for machine
translation in other ways.
Foster and Kunh (2007) describe a mixture-model
approach for SMT adaptation. They first split a
training corpus into different domains. Then, they
train separate models on each domain. Finally, they
combine a specific domain translation model with a
general domain translation model depending on var-
ious text distances. One way to calculate the dis-
tance is using topic model.
Gong et al (2010) introduce topic model for fil-
tering topic-mismatched phrase pairs. They first as-
sign a specific topic for the document to be translat-
ed. Similarly, each phrase pair is also assigned with
one specific topic. A phrase pair will be discarded if
its topic mismatches the document topic.
Researchers also introduce topic model for cross-
lingual language model adaptation (Tam et al, 2007;
Ruiz and Federico, 2011). They use bilingual topic
model to project latent topic distribution across lan-
guages. Based on the bilingual topic model, they ap-
ply the source-side topic weights into the target-side
topic model, and adapt the n-gram language model
of target side.
Our topic similarity model uses the document top-
ic information. From this point, our work is related
to context-dependent translation (Carpuat and Wu,
2007; He et al, 2008; Shen et al, 2009). Previous
work typically use neighboring words and sentence
level information, while our work extents the con-
text into the document level.
8 Conclusion and Future Work
We have presented a topic similarity model which
incorporates the rule-topic distributions on both the
source and target side into traditional hierarchical
phrase-based system. Our experimental results show
that our model achieves a better performance with
faster decoding speed than previous work on topic-
specific lexicon translation. This verifies the advan-
tage of exploiting topic model at the rule level over
the word level. Further improvement is achieved by
distinguishing topic-sensitive and topic-insensitive
rules using the topic sensitivity model.
In the future, we are interesting to find ways to
exploit topic model on bilingual data without docu-
ment boundaries, thus to enlarge the size of training
data. Furthermore, our training corpus mainly focus
on news, it is also interesting to apply our method on
corpus with more diverse topics. Finally, we hope to
apply our method to other translation models, espe-
cially syntax-based models.
757
Acknowledgement
The authors were supported by High-Technology
R&D Program (863) Project No 2011AA01A207
and 2012BAH39B03. This work was done dur-
ing Xinyan Xiao?s internship at I2R. We would like
to thank Yun Huang, Zhengxian Gong, Wenliang
Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong
Su and the anonymous reviewers for their insightful
comments.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proc of WMT 2009.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. AAS, 1(1):17?35.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. In Proceedings of the MT Sum-
mit XI.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010.
Statistical machine translation based on lda. In Proc.
IUCS 2010, page 286?290, Oct.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. EMNLP 2008.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of UAI 1999, pages 289?296.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proc. of EMNLP 2009.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Nick Ruiz and Marcello Federico. 2011. Topic adapta-
tion for lecture translation through bilingual latent se-
mantic models. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, July.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proc. Coling 2008.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Proc.
ACL 2006.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
Proc. NIPS 2007.
758
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950?958,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Chunk-to-String Translation?
Yang Feng? Dongdong Zhang? Mu Li? Ming Zhou? Qun Liu?
? Department of Computer Science ? Microsoft Research Asia
University of Sheffield dozhang@microsoft.com
Sheffield, UK muli@microsoft.com
y.feng@shef.ac.uk mingzhou@microsoft.com
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
liuqun@ict.ac.cn
Abstract
We present a hierarchical chunk-to-string
translation model, which can be seen as a
compromise between the hierarchical phrase-
based model and the tree-to-string model,
to combine the merits of the two models.
With the help of shallow parsing, our model
learns rules consisting of words and chunks
and meanwhile introduce syntax cohesion.
Under the weighed synchronous context-free
grammar defined by these rules, our model
searches for the best translation derivation
and yields target translation simultaneously.
Our experiments show that our model signif-
icantly outperforms the hierarchical phrase-
based model and the tree-to-string model on
English-Chinese Translation tasks.
1 Introduction
The hierarchical phrase-based model (Chiang, 2007)
makes an advance of statistical machine translation
by employing hierarchical phrases, which not only
uses phrases to learn local translations but also uses
hierarchical phrases to capture reorderings of words
and subphrases which can cover a large scope. Be-
sides, this model is formal syntax-based and does
not need to specify the syntactic constituents of
subphrases, so it can directly learn synchronous
context-free grammars (SCFG) from a parallel text
without relying on any linguistic annotations or as-
sumptions, which makes it used conveniently and
widely.
?This work was done when the first author visited Microsoft
Research Asia as an intern.
However, it is often desirable to consider syntac-
tic constituents of subphrases, e.g. the hierarchical
phrase
X ? ?X 1 for X 2 , X 2 de X 1 ?
can be applied to both of the following strings in
Figure 1
?A request for a purchase of shares?
?filed for bankruptcy?,
and get the following translation, respectively
?goumai gufen de shenqing?
?pochan de shenqing?.
In the former, ?A request? is a NP and this rule acts
correctly while in the latter ?filed? is a VP and this
rule gives a wrong reordering. If we specify the first
X on the right-hand side to NP, this kind of errors
can be avoided.
The tree-to-string model (Liu et al, 2006; Huang
et al, 2006) introduces linguistic syntax via source
parse to direct word reordering, especially long-
distance reordering. Furthermore, this model is for-
malised as Tree Substitution Grammars, so it ob-
serves syntactic cohesion. Syntactic cohesion means
that the translation of a string covered by a subtree
in a source parse tends to be continuous. Fox (2002)
shows that translation between English and French
satisfies cohesion in the majority cases. Many pre-
vious works show promising results with an as-
sumption that syntactic cohesion explains almost
all translation movement for some language pairs
(Wu, 1997; Yamada and Knight, 2001; Eisner, 2003;
Graehl and Knight, 2004; Quirk et al, 2005; Cherry,
2008; Feng et al, 2010).
950
But unfortunately, the tree-to-string model re-
quires each node must be strictly matched during
rule matching, which makes it strongly dependent
on the relationship of tree nodes and their roles in
the whole sentence. This will lead to data sparse-
ness and being vulnerable to parse errors.
In this paper, we present a hierarchical chunk-to-
string translation model to combine the merits of the
two models. Instead of parse trees, our model intro-
duces linguistic information in the form of chunks,
so it does not need to care the internal structures and
the roles in the main sentence of chunks. Based on
shallow parsing results, it learns rules consisting of
either words (terminals) or chunks (nonterminals),
where adjacent chunks are packed into one nonter-
minal. It searches for the best derivation through the
SCFG-motivated space defined by these rules and
get target translation simultaneously. In some sense,
our model can be seen as a compromise between
the hierarchical phrase-based model and the tree-to-
string model, specifically
? Compared with the hierarchical phrase-based
model, it integrates linguistic syntax and sat-
isfies syntactic cohesion.
? Compared with the tree-to-string model, it only
needs to perform shallow parsing which intro-
duces less parsing errors. Besides, our model
allows a nonterminal in a rule to cover several
chunks, which can alleviate data sparseness and
the influence of parsing errors.
? we refine our hierarchical chunk-to-string
model into two models: a loose model (Section
2.1) which is more similar to the hierarchical
phrase-based model and a tight model (Section
2.2) which is more similar to the tree-to-string
model.
The experiments show that on the 2008 NIST
English-Chinese MT translation test set, both the
loose model and the tight model outperform the hi-
erarchical phrase-based model and the tree-to-string
model, where the loose model has a better perfor-
mance. While in terms of speed, the tight model
runs faster and its speed ranking is between the tree-
to-string model and the hierarchical phrase-based
model.
NP IN NP IN NP VBD VP
A request for a purchase of shares was made
goumai gufen de shenqing bei dijiao
?? ?? ? ?? ? ??
(a)
NP VBZ VBN IN NP
The bank has filed for bankruptcy
gai yinhang yijing shenqing pochan
? ?? ?? ?? ??
(b)
Figure 1: A running example of two sentences. For each
sentence, the first row gives the chunk sequence.
S
NP
DT
The
NN
bank
VP
VBZ
has
VP
VBN
filed
PP
IN
for
NP
NN
bankruptcy
(a) A parse tree
B-NP I-NP B-VBZ B-VBN B-IN B-NP
The bank has filed for bankruptcy
(b) A chunk sequence got from the parse tree
Figure 2: An example of shallow parsing.
2 Modeling
Shallow parsing (also chunking) is an analysis of
a sentence which identifies the constituents (noun
groups, verbs, verb groups, etc), but neither spec-
ifies their internal structures, nor their roles in the
main sentence. In Figure 1, we give the chunk se-
quence in the first row for each sentence. We treat
shallow parsing as a sequence label task, and a sen-
tence f can have many possible different chunk la-
bel sequences. Therefore, in theory, the conditional
probability of a target translation e conditioned on
the source sentence f is given by taking the chunk
label sequences as a latent variable c:
p(e|f) =
?
c
p(c|f)p(e|f , c) (1)
951
In practice, we only take the best chunk label se-
quence c? got by
c? = argmax
c
p(c|f) (2)
Then we can ignore the conditional probability
p(c?|f) as it holds the same value for each transla-
tion, and get:
p(e|f) = p(c?|f)p(e|f , c?)
= p(e|f , c?) (3)
We formalize our model as a weighted SCFG.
In a SCFG, each rule (usually called production in
SCFGs) has an aligned pair of right-hand sides ?
the source side and the target side, just as follows:
X ? ??, ?,??
where X is a nonterminal, ? and ? are both strings of
terminals and nonterminals, and ? denotes one-to-
one links between nonterminal occurrences in ? and
nonterminal occurrences in ?. A SCFG produces a
derivation by starting with a pair of start symbols
and recursively rewrites every two coindexed non-
terminals with the corresponding components of a
matched rule. A derivation yields a pair of strings
on the right-hand side which are translation of each
other.
In a weighted SCFG, each rule has a weight and
the total weight of a derivation is the production
of the weights of the rules used by the derivation.
A translation may be produced by many different
derivations and we only use the best derivation to
evaluate its probability. With d denoting a deriva-
tion and r denoting a rule, we have
p(e|f) = max
d
p(d,e|f , c?)
= max
d
?
r?d
p(r,e|f , c?) (4)
Following Och and Ney (2002), we frame our model
as a log-linear model:
p(e|f) = exp
?
k ?kHk(d,e, c?,f)
exp
?
d?,e?,k ?kHk(d?,e?, c?,f)
(5)
where Hk(d,e, c?,f) =
?
r
hk(f , c?, r)
So the best translation is given by
e? = argmax
e
?
k
?kHk(d,e, c?,f) (6)
We employ the same set of features for the log-
linear model as the hierarchical phrase-based model
does(Chiang, 2005).
We further refine our hierarchical chunk-to-string
model into two models: a loose model which is more
similar to the hierarchical phrase-based model and
a tight model which is more similar to the tree-to-
string model. The two models differ in the form of
rules and the way of estimating rule probabilities.
While for decoding, we employ the same decoding
algorithm for the two models: given a test sentence,
the decoders first perform shallow parsing to get the
best chunk sequence, then apply a CYK parsing al-
gorithm with beam search.
2.1 A Loose Model
In our model, we employ rules containing non-
terminals to handle long-distance reordering where
boundary words play an important role. So for the
subphrases which cover more than one chunk, we
just maintain boundary chunks: we bundle adjacent
chunks into one nonterminal and denote it as the first
chunk tag immediately followed by ?-? and next fol-
lowed by the last chunk tag. Then, for the string pair
<filed for bankruptcy, shenqing pochan>, we can
get the rule
r1 : X ? ?VBN 1 for NP 2 , VBN 1 NP 2 ?
while for the string pair <A request for a purchase
of shares, goumai gufen de shenqing>, we can get
r2 : X ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?.
The rule matching ?A request for a purchase of
shares was? will be
r3 : X ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
We can see that in contrast to the method of rep-
resenting each chunk separately, this representation
form can alleviate data sparseness and the influence
of parsing errors.
952
?S 1 , S 1 ? ? ?S 2 X 3 , S 2 X 3 ?
? ?X 4 X 3 , X 4 X 3 ?
? ?NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ?
? ?NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ?
? ?A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(a) The loose model
?NP-VP 1 , NP-VP 1 ? ? ?NP-VBD 2 VP 3 , NP-VBD 2 VP 3 ?
? ?NP-NP 4 VBD 5 VP 3 , NP-NP 4 VBD 5 VP 3 ?
? ?NP 6 for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de NP 6 VBD 5 VP 3 ?
? ?A request for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares VBD 5 VP 3 , goumai gufen de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares was VP 3 , goumai gufen de shenqing bei VP 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(b) The tight model
Figure 3: The derivations of the sentence in Figure 1(a).
In these rules, the left-hand nonterminal symbol X
can not match any nonterminal symbol on the right-
hand side. So we need a set of rules such as
NP ? ?X 1 , X 1 ?
NP-NP ? ?X 1 , X 1 ?
and so on, and set the probabilities of these rules to
1. To simplify the derivation, we discard this kind of
rules and assume that X can match any nonterminal
on the right-hand side.
Only with r2 and r3, we cannot produce any
derivation of the whole sentence in Figure 1 (a). In
this case we need two special glue rules:
r4 : S ? ?S 1 X 2 , S 1 X 2 ?
r5 : S ? ?X 1 , X 1 ?
Together with the following four lexical rules,
r6 : X ? ?a request, shenqing?
r7 : X ? ?a purchase of shares, goumai gufen?
r8 : X ? ?was, bei?
r9 : X ? ?made, dijiao?
Figure 3(a) shows the derivation of the sentence in
Figure 1(a).
2.2 A Tight Model
In the tight model, the right-hand side of each rule
remains the same as the loose model, but the left-
hand side nonterminal is not X but the correspond-
ing chunk labels. If a rule covers more than one
chunk, we just use the first and the last chunk la-
bels to denote the left-hand side nonterminal. The
rule set used in the tight model for the example in
Figure 1(a) corresponding to that in the loose model
becomes:
r2 : NP-NP ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?
r3 : NP-VBD ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
r6 : NP ? ?a request, shenqing?
r7 : NP-NP ? ?a purchase of shares, goumai gufen?
r8 : VBD ? ?was, bei?
r9 : VP ? ?made, dijiao?
During decoding, we first collect rules for each
span. For a span which does not have any matching
rule, if we do not construct default rules for it, there
will be no derivation for the whole sentence, then we
need to construct default rules for this kind of span
by enumerating all possible binary segmentation of
the chunks in this span. For the example in Figure
1(a), there is no rule matching the whole sentence,
953
so we need to construct default rules for it, which
should be
NP-VP ? ?NP-VBD 1 VP 2 , NP-VBD 1 VP 2 ?.
NP-VP ? ?NP-NP 1 VBD-VP 2 , NP-NP 1 VBD-VP 2 ?.
and so on.
Figure 3(b) shows the derivation of the sentence
in Figure 1(a).
3 Shallow Parsing
In a parse tree, a chunk is defined by a leaf node or
an inner node whose children are all leaf nodes (See
Figure 2 (a)). In our model, we identify chunks by
traversing a parse tree in a breadth-first order. Once
a node is recognized as a chunk, we skip its children.
In this way, we can get a sole chunk sequence given
a parse tree. Then we label each word with a label
indicating whether the word starts a chunk (B-) or
continues a chunk (I-). Figure 2(a) gives an example.
In this method, we get the training data for shallow
parsing from Penn Tree Bank.
We take shallow Parsing (chunking) as a sequence
label task and employ Conditional Random Field
(CRF)1 to train a chunker. CRF is a good choice for
label tasks as it can avoid label bias and use more
statistical correlated features. We employ the fea-
tures described in Sha and Pereira (2003) for CRF.
We do not introduce CRF-based chunkier in this pa-
per and more details can be got from Hammersley
and Clifford (1971), Lafferty et al (2001), Taskar et
al. (2002), Sha and Pereira (2003).
4 Rule Extraction
In what follows, we introduce how to get the rule
set. We learn rules from a corpus that first is bi-
directionally word-aligned by the GIZA++ toolkit
(Och and Ney, 2000) and then is refined using a
?final-and? strategy. We generate the rule set in two
steps: first, we extract two sets of phrases, basic
phrases and chunk-based phrases. Basic phrases are
defined using the same heuristic as previous systems
(Koehn et al, 2003; Och and Ney, 2004; Chiang,
2005). A chunk-based phrase is such a basic phrase
that covers one or more chunks on the source side.
1We use the open source toolkit CRF++ got in
http://code.google.com/p/crfpp/ .
We identity chunk-based phrases ?cj2j1 ,f
j2
j1 ,e
i2
i1? as
follows:
1. A chunk-based phrase is a basic phrase;
2. cj1 begins with ?B-?;
3. fj2 is the end word on the source side or cj2+1
does not begins with ?I-?.
Given a sentence pair ?f ,e,??, we extract rules for
the loose model as follows
1. If ?f j2j1 ,e
i2
i1? is a basic phrase, then we can have
a rule
X ? ?f j2j1 ,e
i2
i1?
2. Assume X ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
X ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of these rules in the
same way as Chiang (2007).
We extract rules for the tight model as follows
1. If ?f j2j1 ,e
i2
i1? is a chunk-based phrase with a
chunk sequence Ys ? ? ?Yt, then we can have a
rule
Ys-Yt ? ?f j2j1 ,e
i2
i1?
2. Assume Ys-Yt ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
Ys-Yt ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of rules in the same way
as Liu et al (2006).
For the loose model, the nonterminals must be co-
hesive, while the whole rule can be noncohesive: if
both ends of a rule are nonterminals, the whole rule
is cohesive, otherwise, it may be noncohesive. In
contrast, for the tight model, both the whole rule and
the nonterminal are cohesive.
Even with the cohesion constraints, our model
still generates a large number of rules, but not all
954
of the rules are useful for translation. So we follow
the method described in Chiang (2007) to filter the
rule set except that we allow two nonterminals to be
adjacent.
5 Related Works
Watanabe et al (2003) presented a chunk-to-string
translation model where the decoder generates a
translation by first translating the words in each
chunk, then reordering the translation of chunks.
Our model distinguishes from their model mainly
in reordering model. Their model reorders chunks
resorting to a distortion model while our model re-
orders chunks according to SCFG rules which retain
the relative positions of chunks.
Nguyen et al (2008) presented a tree-to-string
phrase-based method which is based on SCFGs.
This method generates SCFGs through syntac-
tic transformation including a word-to-phrase tree
transformation model and a phrase reordering model
while our model learns SCFG-based rules from
word-aligned bilingual corpus directly
There are also some works aiming to introduce
linguistic knowledge into the hierarchical phrase-
based model. Marton and Resnik (2008) took the
source parse tree into account and added soft con-
straints to hierarchical phrase-based model. Cherry
(2008) used dependency tree to add syntactic cohe-
sion. These methods work with the original SCFG
defined by hierarchical phrase-based model and use
linguistic knowledge to assist translation. Instead,
our model works under the new defined SCFG with
chunks.
Besides, some other researchers make efforts on
the tree-to-string model by employing exponentially
alternative parses to alleviate the drawback of 1-best
parse. Mi et al (2008) presented forest-based trans-
lation where the decoder translates a packed forest
of exponentially many parses instead of i-best parse.
Liu and Liu (2010) proposed to parse and to trans-
late jointly by taking tree-based translation as pars-
ing. Given a source sentence, this decoder produces
a parse tree on the source side and a translation on
the target side simultaneously. Both the models per-
form in the unit of tree nodes rather than chunks.
6 Experiments
6.1 Data Preparation
Data for shallow parsing We got training data and
test data for shallow parsing from the standard Penn
Tree Bank (PTB) English parsing task by splitting
the sections 02-21 on the Wall Street Journal Portion
(Marcus et al, 1993) into two sets: the last 1000
sentences as the test set and the rest as the training
set. We filtered the features whose frequency was
lower than 3 and substituted ?? and ?? with ? to
keep consistent with translation data. We used L2
algorithm to train CRF.
Data for Translation We used the NIST training
set for Chinese-English translation tasks excluding
the Hong Kong Law and Hong Kong Hansard2 as the
training data, which contains 470K sentence pairs.
For the training data set, we first performed word
alignment in both directions using GIZA++ toolkit
(Och and Ney, 2000) then refined the alignments
using ?final-and?. We trained a 5-gram language
model with modified Kneser-Ney smoothing on the
Xinhua portion of LDC Chinese Gigaword corpus.
For the tree-to-string model, we parsed English sen-
tences using Stanford parser and extracted rules us-
ing the GHKM algorithm (Galley et al, 2004).
We used our in-house English-Chinese data set
as the development set and used the 2008 NIST
English-Chinese MT test set (1859 sentences) as the
test set. Our evaluation metric was BLEU-4 (Pap-
ineni et al, 2002) based on characters (as the tar-
get language is Chinese), which performed case-
insensitive matching of n-grams up to n = 4 and
used the shortest reference for the brevity penalty.
We used the standard minimum error-rate training
(Och, 2003) to tune the feature weights to maximize
the BLEU score on the development set.
6.2 Shallow Parsing
The standard evaluation metrics for shallow parsing
are precision P, recall R, and their harmonic mean
F1 score, given by:
P = number of exactly recognized chunks
number of output chunks
R = number of exactly recognized chunks
number of reference chunks
2The source side and target side are reversed.
955
Word number Chunk number Accuracy %
23861 12258 94.48
Chunk type P % R % F1 % Found
All 91.14 91.35 91.25 12286
One 90.32 90.99 90.65 5236
NP 93.97 94.47 94.22 5523
ADVP 82.53 84.30 83.40 475
VP 93.66 92.04 92.84 284
ADJP 65.68 69.20 67.39 236
WHNP 96.30 95.79 96.04 189
QP 83.06 80.00 81.50 183
Table 1: Shallow parsing result. The collum Found gives
the number of chunks recognized by CRF, the row All
represents all types of chunks, and the row One represents
the chunks that consist of one word.
F1 =
2 ? P ? R
P +R
Besides, we need another metric, accuracy A, to
evaluate the accurate rate of individual labeling de-
cisions of every word as
A = number of exactly labeled words
number of words
For example, given a reference sequence
B-NP I-NP I-NP B-VP I-VP B-VP, CRF out-
puts a sequence O-NP I-NP I-NP B-VP I-VP I-NP,
then P = 33.33%, A = 66.67%.
Table 1 summaries the results of shallow parsing.
For ?? and ?? were substituted with ? , the perfor-
mance was slightly influenced.
The F1 score of all chunks is 91.25% and the F1
score of One and NP, which in number account for
about 90% of chunks, is 90.65% and 94.22% respec-
tively. F score of NP chunking approaches 94.38%
given in Sha and Pereira (2003).
6.3 Performance Comparison
We compared our loose decoder and tight decoder
with our in-house hierarchical phrase-based decoder
(Chiang, 2007) and the tree-to-string decoder (Liu et
al., 2006). We set the same configuration for all the
decoders as follows: stack size = 30, nbest size = 30.
For the hierarchical chunk-based and phrase-based
decoders, we set max rule length to 5. For the tree-
to-string decoder, we set the configuration of rule
System Dev NIST08 Speed
phrase 0.2843 0.3921 1.163
tree 0.2786 0.3817 1.107
tight 0.2914 0.3987 1.208
loose 0.2936 0.4023 1.429
Table 2: Performance comparison. Phrase represents
the hierarchical phrase-based decoder, tree represents the
tree-to-string decoder, tight represents our tight decoder
and loose represents our loose decoder. The speed is re-
ported by seconds per sentence. The speed for the tree-to-
string decoder includes the parsing time (0.23s) and the
speed for the tight and loose models includes the shallow
parsing time, too.
extraction as: the height up to 3 and the number of
leaf nodes up to 5.
We give the results in Table 2. From the results,
we can see that both the loose and tight decoders
outperform the baseline decoders and the improve-
ment is significant using the sign-test of Collins et
al. (2005) (p < 0.01). Specifically, the loose model
has a better performance while the tight model has a
faster speed.
Compared with the hierarchical phrase-based
model, the loose model only imposes syntactic cohe-
sion cohesion to nonterminals while the tight model
imposes syntax cohesion to both rules and nonter-
minals which reduces search space, so it decoders
faster. We can conclude that linguistic syntax can
indeed improve the translation performance; syntac-
tic cohesion for nonterminals can explain linguis-
tic phenomena well; noncohesive rules are useful,
too. The extra time consumption against hierarchi-
cal phrase-based system comes from shallow pars-
ing.
By investigating the translation result, we find that
our decoder does well in rule selection. For exam-
ple, in the hierarchical phrase-based model, this kind
of rules, such as
X ? ?X of X, ??, X ? ?X for X, ??
and so on, where ? stands for the target component,
are used with a loose restriction as long as the ter-
minals are matched, while our models employ more
stringent constraints on these rules by specifying the
syntactic constituent of ?X?. With chunk labels, our
models can make different treatment for different
situations.
956
System Dev NIST08 Speed
cohesive 0.2936 0.4023 1.429
noncohesive 0.2937 0.3964 1.734
Table 3: Influence of cohesion. The row cohesive rep-
resents the loose system where nonterminals satisfy co-
hesion, and the row noncohesive represents the modified
version of the loose system where nonterminals can be
noncohesive.
Compared with the tree-to-string model, the re-
sult indicates that the change of the source-side lin-
guistic syntax from parses to chunks can improve
translation performance. The reasons should be our
model can reduce parse errors and it is enough to use
chunks as the basic unit for machine translation. Al-
though our decoders and tree-to-string decoder all
run in linear-time with beam search, tree-to-string
model runs faster for it searches through a smaller
SCFG-motivated space.
6.4 Influence of Cohesion
We verify the influence of syntax cohesion via the
loose model. The cohesive model imposes syntax
cohesion on nonterminals to ensure the chunk is re-
ordered as a whole. In this experiment, we introduce
a noncohesive model by allowing a nonterminal to
match part of a chunk. For example, in the nonco-
hesive model, it is legal for a rule with the source
side
?NP for NP-NP?
to match
?request for a purchase of shares?
in Figure 1 (a), where ?request? is part of NP. As
well, the rule with the source side
?NP for a NP-NP?
can match
?request for a purchase of shares?.
In this way, we can ensure all the rules used in the
cohesive system can be used in the noncohesive sys-
tem. Besides cohesive rules, the noncohesive system
can use noncohesive rules, too.
We give the results in Table 3. From the results,
we can see that cohesion helps to reduce search
space, so the cohesive system decodes faster. The
noncohesive system decoder slower, as it employs
System Number Dev NIST08 Speed
loose two 0.2936 0.4023 1.429
loose three 0.2978 0.4037 2.056
tight two 0.2914 0.3987 1.208
tight three 0.2954 0.4026 1.780
Table 4: The influence of the number of nonterminals.
The column number lists the number of nonterminals
used at most in a rule.
more rules, but this does not bring any improvement
of translation performance. As other researches said
in their papers, syntax cohesion can explain linguis-
tic phenomena well.
6.5 Influence of the number of nonterminals
We also tried to allow a rule to hold three nonter-
minals at most. We give the result in Table 4. The
result shows that using three nonterminals does not
bring a significant improvement of translation per-
formance but quite more time consumption. So we
only retain two nonterminals at most in a rule.
7 Conclusion
In this paper, we present a hierarchical chunk-
to-string model for statistical machine translation
which can be seen as a compromise of the hierarchi-
cal phrase-based model and the tree-to-string model.
With the help of shallow parsing, our model learns
rules consisting of either words or chunks and com-
presses adjacent chunks in a rule to a nonterminal,
then it searches for the best derivation under the
SCFG defined by these rules. Our model can com-
bine the merits of both the models: employing lin-
guistic syntax to direct decoding, being syntax co-
hesive and robust to parsing errors. We refine the hi-
erarchical chunk-to-string model into two models: a
loose model (more similar to the hierarchical phrase-
based model) and a tight model (more similar to the
tree-to-string model).
Our experiments show that our decoder can im-
prove translation performance significantly over the
hierarchical phrase-based decoder and the tree-to-
string decoder. Besides, the loose model gives a bet-
ter performance while the tight model gives a faster
speed.
957
8 Acknowledgements
We would like to thank Trevor Cohn, Shujie Liu,
Nan Duan, Lei Cui and Mo Yu for their help,
and anonymous reviewers for their valuable com-
ments and suggestions. This work was supported
in part by EPSRC grant EP/I034750/1 and in part
by High Technology R&D Program Project No.
2011AA01A207.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. of ACL, pages
72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL, pages
205?208.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling:Posters,
pages 285?293.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304?
3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. of HLT-NAACL, pages 105?112.
J Hammersley and P Clifford. 1971. Markov fields on
finite graphs and lattices. In Unpublished manuscript.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML, pages 282?289.
Yang Liu and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of COLING, pages 707?715.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of COLING-ACL, pages 609?616.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19:313?330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. of ACL, pages 1003?1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho,
Minh Le Nguyen, and Vinh Van Nguyen. 2008. A
tree-to-string phrase-based model for statistical ma-
chine translation. In Proc. of CoNLL, pages 143?150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL, pages 295?
302.
Frans J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417?449.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of HLT-
NAACL, pages 134?141.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Eighteenth Conference on Uncertainty in Artificial
Intelligence.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno.
2003. Chunk-based statistical translation. In Proc. of
ACL, pages 303?310.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523?530.
958
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 761?769,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative Learning with Natural Annotations:
Word Segmentation as a Case Study
Wenbin Jiang 1 Meng Sun 1 Yajuan Lu? 1 Yating Yang 2 Qun Liu 3, 1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{jiangwenbin, sunmeng, lvyajuan}@ict.ac.cn
2Multilingual Information Technology Research Center
The Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences
yangyt@ms.xjb.ac.cn
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Structural information in web text pro-
vides natural annotations for NLP prob-
lems such as word segmentation and pars-
ing. In this paper we propose a discrim-
inative learning algorithm to take advan-
tage of the linguistic knowledge in large
amounts of natural annotations on the In-
ternet. It utilizes the Internet as an external
corpus with massive (although slight and
sparse) natural annotations, and enables a
classifier to evolve on the large-scaled and
real-time updated web text. With Chinese
word segmentation as a case study, exper-
iments show that the segmenter enhanced
with the Chinese wikipedia achieves sig-
nificant improvement on a series of testing
sets from different domains, even with a
single classifier and local features.
1 Introduction
Problems related to information retrieval, machine
translation and social computing need fast and ac-
curate text processing, for example, word segmen-
tation and parsing. Taking Chinese word seg-
mentation for example, the state-of-the-art mod-
els (Xue and Shen, 2003; Ng and Low, 2004;
Gao et al, 2005; Nakagawa and Uchimoto, 2007;
Zhao and Kit, 2008; Jiang et al, 2009; Zhang and
Clark, 2010; Sun, 2011b; Li, 2011) are usually
trained on human-annotated corpora such as the
Penn Chinese Treebank (CTB) (Xue et al, 2005),
and perform quite well on corresponding test sets.
Since the text used for corpus annotating are usu-
ally drawn from specific fields (e.g. newswire or
finance), and the annotated corpora are limited in
 think that NLP                  has already ...
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Natural annotation by hyperlink
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Knowledge for word segmentation
(c) Knowledge for dependency parsing
Figure 1: Natural annotations for word segmenta-
tion and dependency parsing.
size (e.g. tens of thousands), the performance of
word segmentation tends to degrade sharply when
applied to new domains.
Internet provides large amounts of raw text, and
statistics collected from it have been used to im-
prove parsing performance (Nakov and Hearst,
2005; Pitler et al, 2010; Bansal and Klein, 2011;
Zhou et al, 2011). The Internet alo gives mas-
sive (although slight and sparse) natural annota-
tions in the forms of structural information includ-
ing hyperlinks, fonts, colors and layouts (Sun,
2011a). These annotations usually imply valuable
knowledge for problems such as word segmen-
tation and parsing, based on the hypothesis that
the subsequences marked by structural informa-
tion are meaningful fragments in sentences. Fig-
ure 1 shows an example. The hyperlink indicates
761
a Chinese phrase (meaning NLP), and it probably
corresponds to a connected sub-graph for depen-
dency parsing. Creators of web text give valuable
annotations during editing, the whole Internet can
be treated as a wide-coveraged and real-time up-
dated corpus.
Different from the dense and accurate annota-
tions in human-annotated corpora, natural annota-
tions in web text are sparse and slight, it makes
direct training of NLP models impracticable. In
this work we take for example a most important
problem, word segmentation, and propose a novel
discriminative learning algorithm to leverage the
knowledge in massive natural annotations of web
text. Character classification models for word seg-
mentation usually factorize the whole prediction
into atomic predictions on characters (Xue and
Shen, 2003; Ng and Low, 2004). Natural anno-
tations in web text can be used to get rid of im-
plausible predication candidates for related char-
acters, knowledge in the natural annotations is
therefore introduced in the manner of searching
space pruning. Since constraint decoding in the
pruned searching space integrates the knowledge
of the baseline model and natural annotations, it
gives predictions not worse than the normal decod-
ing does. Annotation differences between the out-
puts of constraint decoding and normal decoding
are used to train the enhanced classifier. This strat-
egy makes the usage of natural annotations simple
and universal, which facilitates the utilization of
massive web text and the extension to other NLP
problems.
Although there are lots of choices, we choose
the Chinese wikipedia as the knowledge source
due to its high quality. Structural information, in-
cluding hyperlinks, fonts and colors are used to de-
termine the boundaries of meaningful fragments.
Experimental results show that, the knowledge im-
plied in the natural annotations can significantly
improve the performance of a baseline segmenter
trained on CTB 5.0, an F-measure increment of
0.93 points on CTB test set, and an average incre-
ment of 1.53 points on 7 other domains. It is an ef-
fective and inexpensive strategy to build word seg-
menters adaptive to different domains. We hope to
extend this strategy to other NLP problems such
as named entity recognition and parsing.
In the rest of the paper, we first briefly intro-
duce the problems of Chinese word segmentation
and the character classification model in section
Type Templates Instances
n-gram C?2 C?2=@
C?1 C?1=?
C0 C0=g
C1 C1=,
C2 C2=?
C?2C?1 C?2C?1=@?
C?1C0 C?1C0=?g
C0C1 C0C1=g,
C1C2 C1C2=,?
C?1C1 C?1C1=?,
function Pu(C0) Pu(C0)=false
T (C?2:2) T (C?2:2)= 44444
Table 1: Feature templates and instances for
character classification-based word segmentation
model. Suppose we are considering the i-th char-
acter ?g? in ?...@? g ,???n??...?.
2, then describe the representation of the knowl-
edge in natural annotations of web text in section
3, and finally detail the strategy of discriminative
learning on natural annotations in section 4. Af-
ter giving the experimental results and analysis in
section 5, we briefly introduce the previous related
work and then give the conclusion and the expec-
tation of future research.
2 Character Classification Model
Character classification models for word segmen-
tation factorize the whole prediction into atomic
predictions on single characters (Xue and Shen,
2003; Ng and Low, 2004). Although natural anno-
tations in web text do not directly support the dis-
criminative training of segmentation models, they
do get rid of the implausible candidates for predic-
tions of related characters.
Given a sentence as a sequence of n charac-
ters, word segmentation splits the sequence into
m(? n) subsequences, each of which indicates a
meaningful word. Word segmentation can be for-
malized as a character classification problem (Xue
and Shen, 2003), where each character in the sen-
tence is given a boundary tag representing its posi-
tion in a word. We adopt the boundary tags of Ng
and Low (2004), b, m, e and s, where b, m and
e mean the beginning, the middle and the end of a
word, and s indicates a single-character word. the
decoding procedure searches for the labeled char-
acter sequence y that maximizes the score func-
762
Algorithm 1 Perceptron training algorithm.
1: Input: Training corpus C
2: ~?? 0
3: for t? 1 .. T do ? T iterations
4: for (x, y?) ? C do
5: y ? argmaxy ?(x, y) ? ~?
6: if y 6= y? then
7: ~?? ~?+?(x, y?)? ?(x, y)
8: Output: Parameters ~?
tion:
f(x) = argmax
y
S(y|~?,?, x)
= argmax
y
?(x, y) ? ~?
= argmax
y
?
(i,t)?y
?(i, t, x, y) ? ~?
(1)
The score of the whole sequence y is accumulated
across all its character-label pairs, (i, t) ? y (s.t.
1 ? i ? n and t ? {b,m, e, s}). The feature
function ? maps a labeled sequence or a character-
label pair into a feature vector, ~? is the parame-
ter vector and ?(x, y) ? ~? is the inner product of
?(x, y) and ~?.
Analogous to other sequence labeling prob-
lems, word segmentation can be solved through a
viterbi-style decoding procedure. We omit the de-
coding algorithm in this paper due to its simplicity
and popularity.
The feature templates for the classifier is shown
in Table 1. C0 denotes the current character, while
C?k/Ck denote the kth character to the left/right
of C0. The function Pu(?) returns true for a punc-
tuation character and false for others, the function
T (?) classifies a character into four types, 1, 2, 3
and 4, representing number, date, English letter
and others, respectively.
The classifier can be trained with online learn-
ing algorithms such as perceptron, or offline learn-
ing models such as support vector machines.
We choose the perceptron algorithm (Collins,
2002) to train the classifier for the character
classification-based word segmentation model. It
learns a discriminative model mapping from the
inputs x ? X to the outputs y? ? Y , where X is the
set of sentences in the training corpus and Y is the
set of corresponding labeled results. Algorithm 1
shows the perceptron algorithm for tuning the pa-
rameter ~?. The ?averaged parameters? technology
(Collins, 2002) is used for better performance.
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(a) Original searching space
n
n
n
n
n
n
n
n
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
n ? ? ? ? ? ? ? ? ? ? n
i-1 i j j+1
(b) Shrinked searching space
n
n
n
n
n
n
n
n
b
m
e
s
e
s
b
s
b
m
e
s
b
m
e
s
b
m
e
s
b
m
e
s
e
s
b
s
b
m
e
s
Figure 2: Shrink of searching space for the charac-
ter classification-based word segmentation model.
3 Knowledge in Natural Annotations
Web text gives massive natural annotations in the
form of structural informations, including hyper-
links, fonts, colors and layouts (Sun, 2011a). Al-
though slight and sparse, these annotations imply
valuable knowledge for problems such as word
segmentation and parsing.
As shown in Figure 1, the subsequence P =
i..j of sentence S is composed of bolded charac-
ters determined by a hyperlink. Such natural anno-
tations do not clearly give each character a bound-
ary tag, or define the head-modifier relationship
between two words. However, they do help to
shrink the set of plausible predication candidates
for each character or word. For word segmenta-
tion, it implies that characters i ? 1 and j are the
rightmost characters of words, while characters i
and j + 1 are the leftmost characters of words.
For i ? 1 or j, the plausible predication set ? be-
comes {e, s}; For i and j + 1, it becomes {b, s};
For other characters c except the two at sentence
boundaries, ?(c) is still {b,m, e, s}. For depen-
dency parsing, the subsequence P tends to form
a connected dependency graph if it contains more
than one word. Here we use ? to denote the set of
plausible head of a word (modifier). There must
be a single word w ? P as the root of subse-
quence P , whose plausible heads fall out of P ,
that is, ?(w) = {x|x ? S ? P}. For the words
in P except the root, the plausible heads for each
763
Algorithm 2 Perceptron learning with natural an-
notations.
1: ~?? TRAIN(C)
2: for x ? F do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: C? ? C? ? {y?}
7: ~?? TRAIN(C ? C?)
word w are the words in P except w itself, that is,
?(w) = {x|x ? P ? {w}}.
Creators of web text give valuable structural
annotations during editing, these annotations re-
duce the predication uncertainty for atomic char-
acters or words, although not exactly defining
which predication is. Figure 2 shows an exam-
ple for word segmentation, depicting the shrink
of searching space for the character classification-
based model. Since the decrement of uncertainty
indicates the increment of knowledge, the whole
Internet can be treated as a wide-coveraged and
real-time updated corpus. We choose the Chinese
wikipedia as the external knowledge source, and
structural information including hyperlinks, fonts
and colors are used in the current work due to their
explicitness of representation.
4 Learning with Natural Annotations
Different from the dense and accurate annotations
in human-annotated corpora, natural annotations
are sparse and slight, which makes direct training
of NLP models impracticable. Annotations im-
plied by structural information do not give an ex-
act predication to a character, however, they help
to get rid of the implausible predication candidates
for related characters, as described in the previous
section.
Previous work on constituency parsing or ma-
chine translation usually resort to some kinds of
heuristic tricks, such as punctuation restrictions,
to eliminate some implausible candidates during
decoding. Here the natural annotations also bring
knowledge in the manner of searching space prun-
ing. Conditioned on the completeness of the de-
coding algorithm, a model trained on an exist-
ing corpus probably gives better or at least not
worse predications, by constraint decoding in the
pruned searching space. The constraint decoding
procedure integrates the knowledge of the baseline
Algorithm 3 Online version of perceptron learn-
ing with natural annotations.
1: ~?? TRAIN(C)
2: for x with natural annotations do
3: y ? DECODE(x, ~?)
4: y? ? CONSTRAINTDECODE(x, ~?,?)
5: if y 6= y? then
6: ~?? ~? +?(x, y?)??(x, y)
7: output ~? at regular time
model and natural annotations, the predication dif-
ferences between the outputs of constraint decod-
ing and normal decoding can be used to train the
enhanced classifier.
Restrictions of the searching space according to
natural annotations can be easily incorporated into
the decoder. If the completeness of the searching
algorithm can be guaranteed, the constraint decod-
ing in the pruned searching space will give predi-
cations not worse than those given by the normal
decoding. If a predication of constraint decoding
differs from that of normal decoding, it indicates
that the annotation precision is higher than the lat-
ter. Furthermore, the degree of difference between
the two predications represents the amount of new
knowledge introduced by the natural annotations
over the baseline.
The baseline model ~? is trained on an exist-
ing human-annotated corpus. A set of sentences
F with natural annotations are extracted from the
Chinese wikipedia, and we reserve the ones for
which constraint decoding and normal decoding
give different predications. The predictions of re-
served sentences by constraint decoding are used
as additional training data for the enhanced classi-
fier. The overall training pipeline is analogous to
self-training (McClosky et al, 2006), Algorithm
2 shows the pseudo-codes. Considering the online
characteristic of the perceptron algorithm, if we
are able to leverage much more (than the Chinese
wikipedia) data with natural annotations, an online
version of learning procedure shown in Algorithm
3 would be a better choice. The technology of ?av-
eraged parameters? (Collins, 2002) is easily to be
adapted here for better performance.
When constraint decoding and normal decod-
ing give different predications, we only know that
the former is probably better than the latter. Al-
though there is no explicit evidence for us to mea-
sure how much difference in accuracy between the
764
Partition Sections # of word
CTB
Training 1? 270 0.47M
400 ? 931
1001 ? 1151
Developing 301 ? 325 6.66K
Testing 271 ? 300 7.82K
Table 2: Data partitioning for CTB 5.0.
two predications, we can approximate how much
new knowledge that a naturally annotated sentence
brings. For a sentence x, given the predications of
constraint decoding and normal decoding, y? and
y, the difference of their scores ? = S(y) ? S(y?)
indicates the degree to which the current model
mistakes. This indicator helps us to select more
valuable training examples.
The strategy of learning with natural annota-
tions can be adapted to other situations. For ex-
ample, if we have a list of words or phrases (espe-
cially in a specific domain such as medicine and
chemical), we can generate annotated sentences
automatically by string matching in a large amount
of raw text. It probably provides a simple and
effective domain adaptation strategy for already
trained models.
5 Experiments
We use the Penn Chinese Treebank 5.0 (CTB)
(Xue et al, 2005) as the existing annotated cor-
pus for Chinese word segmentation. For conve-
nient of comparison with other work in word seg-
mentation, the whole corpus is split into three par-
titions as follows: chapters 271-300 for testing,
chapters 301-325 for developing, and others for
training. We choose the Chinese wikipedia 1 (ver-
sion 20120812) as the external knowledge source,
because it has high quality in contents and it is
much better than usual web text. Structural infor-
mations, including hyperlinks, fonts and colors are
used to derive the annotation information.
To further evaluate the improvement brought
by the fuzzy knowledge in Chinese wikipedia, a
series of testing sets from different domains are
adopted. The four testing sets from SIGHAN
Bakeoff 2010 (Zhao and Liu, 2010) are used, they
are drawn from the domains of literature, finance,
computer science and medicine. Although the ref-
erence sets are annotated according to a different
1http://download.wikimedia.org/backup-index.html.
 95.6
 95.8
 96
 96.2
 96.4
 96.6
 96.8
 97
 97.2
 97.4
 1  2  3  4  5  6  7  8  9  10
Ac
cu
ra
cy
 (F
1%
)
Training iterations
Figure 3: Learning curve of the averaged percep-
tron classifier on the CTB developing set.
word segmentation standard (Yu et al, 2001), the
quantity of accuracy improvement is still illustra-
tive since there are no vast diversities between the
two segmentation standards. We also annotated
another three testing sets 2, their texts are drawn
from the domains of chemistry, physics and ma-
chinery, and each contains 500 sentences.
5.1 Baseline Classifier for Word
Segmentation
We train the baseline perceptron classifier for
word segmentation on the training set of CTB
5.0, using the developing set to determine the
best training iterations. The performance mea-
surement for word segmentation is balanced F-
measure, F = 2PR/(P +R), a function of preci-
sion P and recall R, where P is the percentage of
words in segmentation results that are segmented
correctly, and R is the percentage of correctly seg-
mented words in the gold standard words.
Figure 3 shows the learning curve of the aver-
aged perceptron on the developing set. The sec-
ond column of Table 3 lists the performance of
the baseline classifier on eight testing sets, where
newswire denotes the testing set of the CTB it-
self. The classifier performs much worse on the
domains of chemistry, physics and machinery, it
indicates the importance of domain adaptation for
word segmentation (Gao et al, 2004; Ma and
Way, 2009; Gao et al, 2010). The accuracy on the
testing sets from SIGHAN Bakeoff 2010 is even
lower due to the difference in both domains and
word segmentation standards.
2They are available at http://nlp.ict.ac.cn/ jiangwenbin/.
765
Dataset Baseline (F%) Enhanced (F%)
Newswire 97.35 98.28 +0.93
Out-of-Domain
Chemistry 93.61 95.68 +2.07
Physics 95.10 97.24 +2.14
Machinery 96.08 97.66 +1.58
Literature 92.42 93.53 +1.11
Finance 92.50 93.16 +0.66
Computer 89.46 91.19 +1.73
Medicine 91.88 93.34 +1.46
Average 93.01 94.54 +1.53
Table 3: Performance of the baseline classifier and
the classifier enhanced with natural annotations in
Chinese wikipedia.
5.2 Classifier Enhanced with Natural
Annotations
The Chinese wikipedia contains about 0.5 million
items. From their description text, about 3.9 mil-
lions of sentences with natural annotations are ex-
tracted. With the CTB training set as the exist-
ing corpus C, about 0.8 million sentences are re-
served according to Algorithm 2, the segmenta-
tions given by constraint decoding are used as ad-
ditional training data for the enhanced classifier.
According to the previous description, the dif-
ference of the scores of constraint decoding and
normal decoding, ? = S(y) ? S(y?), indicates
the importance of a constraint segmentation to the
improvement of the baseline classifier. The con-
straint segmentations of the reserved sentences are
sorted in descending order according to the dif-
ference of the scores of constraint decoding and
normal decoding, as described previously. From
the beginning of the sorted list, different amounts
of segmented sentences are used as the additional
training data for the enhanced character classifier.
Figure 4 shows the performance curve of the en-
hanced classifiers on the developing set of CTB.
We found that the highest accuracy was achieved
when 160, 000 sentences were used, while more
additional training data did not give continuous
improvement. A recent related work about self-
training for segmentation (Liu and Zhang, 2012)
also reported a very similar trend, that only a mod-
erate amount of raw data gave the most obvious
improvements.
The performance of the enhanced classifier is
listed in the third column of Table 3. On the
CTB testing set, training data from the Chinese
 97.1
 97.2
 97.3
 97.4
 97.5
 97.6
 97.7
 97.8
Ac
cu
ra
cy
 (F
1%
)
Count of selected sentences
10000 20000 40000 80000 160000 320000 640000
using selected sentences
using all sentences
Figure 4: Performance curve of the classifier en-
hanced with selected sentences of different scales.
Model Accuracy (F%)
(Jiang et al, 2008) 97.85
(Kruengkrai et al, 2009) 97.87
(Zhang and Clark, 2010) 97.79
(Wang et al, 2011) 98.11
(Sun, 2011b) 98.17
Our Work 98.28
Table 4: Comparison with state-of-the-art work in
Chinese word segmentation.
wikipedia brings an F-measure increment of 0.93
points. On out-of-domain testing sets, the im-
provements are much larger, an average increment
of 1.53 points is achieved on seven domains. It
is probably because the distribution of the knowl-
edge in the CTB training data is concentrated in
the domain of newswire, while the contents of
the Chinese wikipedia cover a broad range of do-
mains, it provides knowledge complementary to
that of CTB.
Table 4 shows the comparison with other
work in Chinese word segmentation. Our model
achieves an accuracy higher than that of the
state-of-the-art models trained on CTB only, al-
though using a single classifier with only local
features. From the viewpoint of resource uti-
lization, the comparison between our system and
previous work without using additional training
data is unfair. However, we believe this work
shows another interesting way to improve Chi-
nese word segmentation, it focuses on the utiliza-
tion of fuzzy and sparse knowledge on the Internet
rather than making full use of a specific human-
annotated corpus. On the other hand, since only
a single classifier and local features are used in
our method, better performance could be achieved
766
resorting to complicated features, system com-
bination and other semi-supervised technologies.
What is more, since the text on Internet is wide-
coveraged and real-time updated, our strategy also
helps a word segmenter be more domain adaptive
and up to date.
6 Related Work
Li and Sun (2009) extracted character classifi-
cation instances from raw text for Chinese word
segmentation, resorting to the indication of punc-
tuation marks between characters. Sun and Xu
(Sun and Xu, 2011) utilized the features derived
from large-scaled unlabeled text to improve Chi-
nese word segmentation. Although the two work
also made use of large-scaled raw text, our method
is essentially different from theirs in the aspects
of both the source of knowledge and the learning
strategy.
Lots of efforts have been devoted to semi-
supervised methods in sequence labeling and word
segmentation (Xu et al, 2008; Suzuki and Isozaki,
2008; Haffari and Sarkar, 2008; Tomanek and
Hahn, 2009; Wang et al, 2011). A semi-
supervised method tries to find an optimal hyper-
plane of both annotated data and raw data, thus to
result in a model with better coverage and higher
accuracy. Researchers have also investigated un-
supervised methods in word segmentation (Zhao
and Kit, 2008; Johnson and Goldwater, 2009;
Mochihashi et al, 2009; Hewlett and Cohen,
2011). An unsupervised method mines the latent
distribution regularity in the raw text, and auto-
matically induces word segmentation knowledge
from it. Our method also needs large amounts of
external data, but it aims to leverage the knowl-
edge in the fuzzy and sparse annotations. It is
fundamentally different from semi-supervised and
unsupervised methods in that we aimed to exca-
vate a totally different kind of knowledge, the nat-
ural annotations implied by the structural informa-
tion in web text.
In recent years, much work has been devoted to
the improvement of word segmentation in a vari-
ety of ways. Typical approaches include the in-
troduction of global training or complicated fea-
tures (Zhang and Clark, 2007; Zhang and Clark,
2010), the investigation of word internal structures
(Zhao, 2009; Li, 2011), the adjustment or adapta-
tion of word segmentation standards (Wu, 2003;
Gao et al, 2004; Jiang et al, 2009), the integrated
solution of segmentation and related tasks such as
part-of-speech tagging and parsing (Zhou and Su,
2003; Zhang et al, 2003; Fung et al, 2004; Gold-
berg and Tsarfaty, 2008), and the strategies of hy-
brid or stacked modeling (Nakagawa and Uchi-
moto, 2007; Kruengkrai et al, 2009; Wang et al,
2010; Sun, 2011b).
In parsing, Pereira and Schabes (1992) pro-
posed an extended inside-outside algorithm that
infers the parameters of a stochastic CFG from a
partially parsed treebank. It uses partial bracket-
ing information to improve parsing performance,
but it is specific to constituency parsing, and its
computational complexity makes it impractical for
massive natural annotations in web text. There
are also work making use of word co-occurrence
statistics collected in raw text or Internet n-grams
to improve parsing performance (Nakov and
Hearst, 2005; Pitler et al, 2010; Zhou et al, 2011;
Bansal and Klein, 2011). When enriching the re-
lated work during writing, we found a work on de-
pendency parsing (Spitkovsky et al, 2010) who
utilized parsing constraints derived from hypertext
annotations to improve the unsupervised depen-
dency grammar induction. Compared with their
method, the strategy we proposed is formal and
universal, the discriminative learning strategy and
the quantitative measurement of fuzzy knowledge
enable more effective utilization of the natural an-
notation on the Internet when adapted to parsing.
7 Conclusion and Future Work
This work presents a novel discriminative learning
algorithm to utilize the knowledge in the massive
natural annotations on the Internet. Natural anno-
tations implied by structural information are used
to decrease the searching space of the classifier,
then the constraint decoding in the pruned search-
ing space gives predictions not worse than the nor-
mal decoding does. Annotation differences be-
tween the outputs of constraint decoding and nor-
mal decoding are used to train the enhanced classi-
fier, linguistic knowledge in the human-annotated
corpus and the natural annotations of web text
are thus integrated together. Experiments on Chi-
nese word segmentation show that, the enhanced
word segmenter achieves significant improvement
on testing sets of different domains, although us-
ing a single classifier with only local features.
Since the contents of web text cover a broad
range of domains, it provides knowledge comple-
767
mentary to that of human-annotated corpora with
concentrated distribution of domains. The content
on the Internet is large-scaled and real-time up-
dated, it compensates for the drawback of expen-
sive building and updating of corpora. Our strat-
egy, therefore, enables us to build a classifier more
domain adaptive and up to date. In the future, we
will compare this method with self-training to bet-
ter illustrate the importance of boundary informa-
tion, and give error analysis on what types of er-
rors are reduced by the method to make this inves-
tigation more complete. We will also investigate
more efficient algorithms to leverage more mas-
sive web text with natural annotations, and further
extend the strategy to other NLP problems such as
named entity recognition and parsing.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China (Con-
tracts 61202216), 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03). Qun Liu?s
work was partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part
of the CNGL at Dublin City University. Sincere
thanks to the three anonymous reviewers for their
thorough reviewing and valuable suggestions!
References
Mohit Bansal and Dan Klein. 2011. Web-scale fea-
tures for full-scale parsing. In Proceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8, Philadelphia, USA.
Pascale Fung, Grace Ngai, Yongsheng Yang, and Ben-
feng Chen. 2004. A maximum-entropy chinese
parser augmented by transformation-based learning.
In Proceedings of TALIP.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In Proceed-
ings of ACL.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning
Huang. 2005. Chinese word segmentation and
named entity recognition: A pragmatic approach.
Computational Linguistics.
Wenjun Gao, Xipeng Qiu, and Xuanjing Huang. 2010.
Adaptive chinese word segmentation with online
passive-aggressive algorithm. In Proceedings of
CIPS-SIGHAN Workshop.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL-HLT.
Gholamreza Haffari and Anoop Sarkar. 2008.
Homotopy-based semi-supervised hidden markov
models for sequence labeling. In Proceedings of
COLING.
Daniel Hewlett and Paul Cohen. 2011. Fully unsu-
pervised word segmentation with bve and mdl. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagging?a case study. In
Proceedings of the 47th ACL.
Mark Johnson and Sharon Goldwater. 2009. Improv-
ing nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adaptor
grammars. In Proceedings of NAACL.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun.ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint chinese word segmentation and
pos tagging. In Proceedings of ACL-IJCNLP.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Computational Linguistics.
Zhongguo Li. 2011. Parsing the internal structure of
words: A new paradigm for chinese word segmenta-
tion. In Proceedings of ACL.
Yang Liu and Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and pos-tagging.
In Proceedings of COLING.
Yanjun Ma and Andy Way. 2009. Bilingually moti-
vated domain-adapted word segmentation for statis-
tical machine translation. In Proceedings of EACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the HLT-NAACL.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of ACL-IJCNLP.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL.
Preslav Nakov and Marti Hearst. 2005. Using the
web as an implicit training set: Application to struc-
tural ambiguity resolution. In Proceedings of HLT-
EMNLP.
768
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of ACL.
Emily Pitler, Shane Bergsma, Dekang Lin, and Ken-
neth Church. 2010. Using web-scale n-grams to
improve base np parsing performance. In Proceed-
ings of COLING.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
ACL.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP.
Maosong Sun. 2011a. Natural language processing
based on naturally annotated web resources. CHI-
NESE INFORMATION PROCESSING.
Weiwei Sun. 2011b. A stacked sub-word model for
joint chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of ACL.
Katrin Tomanek and Udo Hahn. 2009. Semi-
supervised active learning for sequence labeling. In
Proceedings of ACL.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A character-based joint model for chinese word seg-
mentation. In Proceedings of COLING.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP.
Andi Wu. 2003. Customizable segmentation of mor-
phologically derived words in chinese. Computa-
tional Linguistics and Chinese Language Process-
ing.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as lmr tagging. In Proceedings of
SIGHAN Workshop.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming
Duan, Shiyong Kang, Honglin Sun, Hui Wang,
Qiang Zhao, and Weidong Zhan. 2001. Processing
norms of modern chinese corpus. Technical report.
Yue Zhang and Stephen Clark. 2007. Chinese seg-
mentation with a word-based perceptron algorithm.
In Proceedings of ACL 2007.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and pos-tagging using
a single discriminative model. In Proceedings of
EMNLP.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of SIGHAN Workshop.
Hai Zhao and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named entity
recognition. In Proceedings of SIGHAN Workshop.
Hongmei Zhao and Qun Liu. 2010. The cips-sighan
clp 2010 chinese word segmentation bakeoff. In
Proceedings of CIPS-SIGHAN Workshop.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
EACL.
Guodong Zhou and Jian Su. 2003. A chinese effi-
cient analyser integrating word segmentation, part-
ofspeech tagging, partial parsing and full parsing. In
Proceedings of SIGHAN Workshop.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings of ACL.
769
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1063?1072,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingually-Guided Monolingual Dependency Grammar Induction
Kai Liu??, Yajuan Lu??, Wenbin Jiang?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{liukai,lvyajuan,jiangwenbin,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
?University of Chinese Academy of Sciences
Abstract
This paper describes a novel strategy for
automatic induction of a monolingual de-
pendency grammar under the guidance
of bilingually-projected dependency. By
moderately leveraging the dependency in-
formation projected from the parsed coun-
terpart language, and simultaneously min-
ing the underlying syntactic structure of
the language considered, it effectively in-
tegrates the advantages of bilingual pro-
jection and unsupervised induction, so as
to induce a monolingual grammar much
better than previous models only using
bilingual projection or unsupervised in-
duction. We induced dependency gram-
mar for five different languages under the
guidance of dependency information pro-
jected from the parsed English translation,
experiments show that the bilingually-
guided method achieves a significant
improvement of 28.5% over the unsuper-
vised baseline and 3.0% over the best pro-
jection baseline on average.
1 Introduction
In past decades supervised methods achieved the
state-of-the-art in constituency parsing (Collins,
2003; Charniak and Johnson, 2005; Petrov et al,
2006) and dependency parsing (McDonald et al,
2005a; McDonald et al, 2006; Nivre et al, 2006;
Nivre et al, 2007; Koo and Collins, 2010). For
supervised models, the human-annotated corpora
on which models are trained, however, are expen-
sive and difficult to build. As alternative strate-
gies, methods which utilize raw texts have been in-
vestigated recently, including unsupervised meth-
ods which use only raw texts (Klein and Man-
ning, 2004; Smith and Eisner, 2005; William et
al., 2009), and semi-supervised methods (Koo et
al., 2008) which use both raw texts and annotat-
ed corpus. And there are a lot of efforts have also
been devoted to bilingual projection (Chen et al,
2010), which resorts to bilingual text with one lan-
guage parsed, and projects the syntactic informa-
tion from the parsed language to the unparsed one
(Hwa et al, 2005; Ganchev et al, 2009).
In dependency grammar induction, unsuper-
vised methods achieve continuous improvements
in recent years (Klein and Manning, 2004; Smith
and Eisner, 2005; Bod, 2006; William et al, 2009;
Spitkovsky et al, 2010). Relying on a predefined
distributional assumption and iteratively maximiz-
ing an approximate indicator (entropy, likelihood,
etc.), an unsupervised model usually suffers from
two drawbacks, i.e., lower performance and high-
er computational cost. On the contrary, bilin-
gual projection (Hwa et al, 2005; Smith and Eis-
ner, 2009; Jiang and Liu, 2010) seems a promis-
ing substitute for languages with a
large amount of bilingual sentences and an exist-
ing parser of the counterpart language. By project-
ing syntactic structures directly (Hwa et al, 2005;
Smith and Eisner, 2009; Jiang and Liu, 2010)
across bilingual texts or indirectly across multi-
lingual texts (Snyder et al, 2009; McDonald et
al., 2011; Naseem et al, 2012), a better depen-
dency grammar can be easily induced, if syntactic
isomorphism is largely maintained between target
and source languages.
Unsupervised induction and bilingual projec-
tion run according to totally different principles,
the former mines the underlying structure of the
monolingual language, while the latter leverages
the syntactic knowledge of the parsed counter-
1063
Bilingual corpus Joint Optimization
Bilingually-guided
Parsing model
Unsupervised
objective
Projection
objective
Random
Treebank
Evolved
treebank
Target
sentences
Source
sentences projection
Figure 1: Training the bilingually-guided parsing model by iteration.
part language. Considering this, we propose a
novel strategy for automatically inducing a mono-
lingual dependency grammar under the guidance
of bilingually-projected dependency information,
which integrates the advantage of bilingual pro-
jection into the unsupervised framework. A
randomly-initialized monolingual treebank
evolves in a self-training iterative procedure, and
the grammar parameters are tuned to simultane-
ously maximize both the monolingual likelihood
and bilingually-projected likelihood of the evolv-
ing treebank. The monolingual likelihood is sim-
ilar to the optimization objectives of convention-
al unsupervised models, while the bilingually-
projected likelihood is the product of the projected
probabilities of dependency trees. By moderately
leveraging the dependency information projected
from the parsed counterpart language, and simul-
taneously mining the underlying syntactic struc-
ture of the language considered, we can automat-
ically induce a monolingual dependency grammar
which is much better than previous models only
using bilingual projection or unsupervised induc-
tion. In addition, since both likelihoods are fun-
damentally factorized into dependency edges (of
the hypothesis tree), the computational complexi-
ty approaches to unsupervised models, while with
much faster convergence. We evaluate the final
automatically-induced dependency parsing mod-
el on 5 languages. Experimental results show
that our method significantly outperforms previ-
ous work based on unsupervised method or indi-
rect/direct dependency projection, where we see
an average improvement of 28.5% over unsuper-
vised baseline on all languages, and the improve-
ments are 3.9%/3.0% over indirect/direct base-
lines. And our model achieves the most signif-
icant gains on Chinese, where the improvements
are 12.0%, 4.5% over indirect and direct projec-
tion baselines respectively.
In the rest of the paper, we first describe the un-
supervised dependency grammar induction frame-
work in section 2 (where the unsupervised op-
timization objective is given), and introduce the
bilingual projection method for dependency pars-
ing in section 3 (where the projected optimiza-
tion objective is given); Then in section 4 we
present the bilingually-guided induction strategy
for dependency grammar (where the two objec-
tives above are jointly optimized, as shown in Fig-
ure 1). After giving a brief introduction of previ-
ous work in section 5, we finally give the experi-
mental results in section 6 and conclude our work
in section 7.
2 Unsupervised Dependency Grammar
Induction
In this section, we introduce the unsupervised ob-
jective and the unsupervised training algorithm
which is used as the framework of our bilingually-
guided method. Unlike previous unsupervised
work (Klein and Manning, 2004; Smith and Eis-
ner, 2005; Bod, 2006), we select a self-training
approach (similar to hard EM method) to train
the unsupervised model. And the framework of
our unsupervised model builds a random treebank
on the monolingual corpus firstly for initialization
and trains a discriminative parsing model on it.
Then we use the parser to build an evolved tree-
bank with the 1-best result for the next iteration
run. In this way, the parser and treebank evolve in
an iterative way until convergence. Let?s introduce
the parsing objective firstly:
Define ei as the ith word in monolingual sen-
tence E; deij denotes the word pair dependency re-
lationship (ei ? ej). Based on the features around
deij , we can calculate the probability Pr(y|deij )
that the word pair deij can form a dependency arc
1064
as:
Pr(y|deij ) =
1
Z(deij )
exp(
?
n
?n ? fn(deij , y)) (1)
where y is the category of the relationship of deij :
y = + means it is the probability that the word
pair deij can form a dependency arc and y = ?
means the contrary. ?n denotes the weight for fea-
ture function fn(deij , y), and the features we used
are presented in Table 1 (Section 6). Z(deij) is a
normalizing constant:
Z(deij ) =
?
y
exp(
?
n
?n ? fn(deij , y)) (2)
Given a sentence E, parsing a dependency tree
is to find a dependency tree DE with maximum
probability PE :
PE = argmax
DE
?
deij?DE
Pr(+|deij ) (3)
2.1 Unsupervised Objective
We select a simple classifier objective function as
the unsupervised objective function which is in-
stinctively in accordance with the parsing objec-
tive:
?(?) =
?
de?DE
Pr(+|de)
?
de?D?E
Pr(?|de) (4)
where E is the monolingual corpus and E ? E,
DE is the treebank that contains all DE in the cor-
pus, and D?E denotes all other possible dependen-
cy arcs which do not exist in the treebank.
Maximizing the Formula (4) is equivalent to
maximizing the following formula:
?1(?) =
?
de?DE
logPr(+|de)
+
?
de?D?E
logPr(?|de)
(5)
Since the size of edges between DE and D?E is
disproportionate, we use an empirical value to re-
duce the impact of the huge number of negative
instances:
?2(?) =
?
de?DE
logPr(+|de)
+ |DE |
|D?E |
?
de?D?E
logPr(?|de)
(6)
where |x| is the size of x.
Algorithm 1 Training unsupervised model
1: build random DE
2: ?? train(DE , D?E)
3: repeat
4: for each E ? E do ? E step
5: DE ? parse(E,?)
6: ?? train(DE , D?E) ? M step
7: until convergence
Bush held talk with Sharona
bushi yu juxingshalong huitanle
? ?
?? ? ???? ???
Figure 2: Projecting a Chinese dependency tree
to English side according to DPA. Solid arrows
are projected dependency arcs; dashed arrows are
missing dependency arcs.
2.2 Unsupervised Training Algorithm
Algorithm 1 outlines the unsupervised training in
its entirety, where the treebank DE and unsuper-
vised parsing model with ? are updated iteratively.
In line 1 we build a random treebank DE on
the monolingual corpus, and then train the parsing
model with it (line 2) through a training procedure
train(?, ?) which needs DE and D?E as classifica-
tion instances. From line 3-7, we train the unsu-
pervised model in self training iterative procedure,
where line 4-5 are similar to the E-step in EM al-
gorithm where calculates objective instead of ex-
pectation of 1-best tree (line 5) which is parsed
according to the parsing objective (Formula 3) by
parsing process parse(?, ?), and update the tree
bank with the tree. Similar to M-step in EM, the
algorithm maximizes the whole treebank?s unsu-
pervised objective (Formula 6) through the train-
ing procedure (line 6).
3 Bilingual Projection of Dependency
Grammar
In this section, we introduce our projection objec-
tive and training algorithm which trains the model
with arc instances.
Because of the heterogeneity between dif-
ferent languages and word alignment errors, pro-
jection methods may contain a lot of noises. Take
Figure 2 as an example, following the Direct
Projection Algorithm (DPA) (Hwa et al, 2005)
(Section 5), the dependency relationships between
words can be directly projected from the source
1065
Algorithm 2 Training projection model
1: DP , DN ? proj(F ,DF , A,E)
2: repeat ? train(DP , DN )
3: ??? grad(DP , DN , ?(?))
4: ?? climb(?,??, ?)
5: until maximization
language to the target language. Therefore, we
can hardly obtain a treebank with complete trees
through direct projection. So we extract projected
discrete dependency arc instances instead of tree-
bank as training set for the projected grammar in-
duction model.
3.1 Projection Objective
Correspondingly, we select an objective which has
the same form with the unsupervised one:
?(?) =
?
de?DP
log Pr(+|de)
+
?
de?DN
logPr(?|de)
(7)
where DP is the positive dependency arc instance
set, which is obtained by direct projection methods
(Hwa et al, 2005; Jiang and Liu, 2010) and DN is
the negative one.
3.2 Projection Algorithm
Basically, the training procedure in line 2,7 of Al-
gorithm 1 can be divided into smaller iterative
steps, and Algorithm 2 outlines the training step
of projection model with instances. F in Algo-
rithm 2 is source sentences in bilingual corpus,
and A is the alignments. Function grad(?, ?, ?)
gives the gradient (??) and the objective is op-
timized with a generic optimization step (such as
an LBFGS iteration (Zhu et al, 1997)) in the sub-
routine climb(?, ?, ?).
4 Bilingually-Guided Dependency
Grammar Induction
This section presents our bilingually-guided gram-
mar induction model, which incorporates unsuper-
vised framework and bilingual projection model
through a joint approach.
According to following observation: unsuper-
vised induction model mines underlying syntactic
structure of the monolingual language, however, it
is hard to find good grammar induction in the ex-
ponential parsing space; bilingual projection ob-
tains relatively reliable syntactic knowledge of the
parsed counterpart, but it possibly contains a lot
of noises (e.g. Figure 2). We believe that unsu-
pervised model and projection model can comple-
ment each other and a joint model which takes bet-
ter use of both unsupervised parse trees and pro-
jected dependency arcs can give us a better parser.
Based on the idea, we propose a nov-
el strategy for training monolingual grammar in-
duction model with the guidance of unsuper-
vised and bilingually-projected dependency infor-
mation. Figure 1 outlines our bilingual-guided
grammar induction process in its entirety. In our
method, we select compatible objectives for unsu-
pervised and projection models, in order to they
can share the same grammar parameters. Then
we incorporate projection model into our iterative
unsupervised framework, and jointly optimize un-
supervised and projection objectives with evolv-
ing treebank and constant projection information
respectively. In this way, our bilingually-guided
model?s parameters are tuned to simultaneous-
ly maximizing both monolingual likelihood and
bilingually-projected likelihood by 4 steps:
1. Randomly build treebank on target sentences
for initialization, and get the projected arc in-
stances through projection from bitext.
2. Train the bilingually-guided grammar induc-
tion model by multi-objective optimization
method with unsupervised objective and pro-
jection objective on treebank and projected
arc instances respectively.
3. Use the parsing model to build new treebank
on target language for next iteration.
4. Repeat steps 1, 2 and 3 until convergence.
The unsupervised objective is optimized by the
loop??tree bank?optimized model?new tree
bank?. The treebank is evolved for runs. The
unsupervised model gets projection constraint im-
plicitly from those parse trees which contain in-
formation from projection part. The projection ob-
jective is optimized by the circulation??projected
instances?optimized model?, these projected in-
stances will not change once we get them.
The iterative procedure proposed here is not a
co-training algorithm (Sarkar, 2001; Hwa et al,
2003), because the input of the projection objec-
tive is static.
1066
4.1 Joint Objective
For multi-objective optimization method, we em-
ploy the classical weighted-sum approach which
just calculates the weighted linear sum of the ob-
jectives:
OBJ =
?
m
weightmobjm (8)
We combine the unsupervised objective (For-
mula (6)) and projection objective (Formula (7))
together through the weighted-sum approach in
Formula (8):
?(?) = ??2(?) + (1 ? ?)?(?) (9)
where ?(?) is our weight-sum objective. And ?
is a mixing coefficient which reflects the relative
confidence between the unsupervised and projec-
tion objectives. Equally, ? and (1??) can be seen
as the weights in Formula (8). In that case, we can
use a single parameter ? to control both weights
for different objective functions. When ? = 1 it
is the unsupervised objective function in Formula
(6). Contrary, if ? = 0, it is the projection objec-
tive function (Formula (7)) for projected instances.
With this approach, we can optimize the mixed
parsing model by maximizing the objective in For-
mula (9). Though the function (Formula (9)) is
an interpolation function, we use it for training
instead of parsing. In the parsing procedure, our
method calculates the probability of a dependency
arc according to the Formula (2), while the inter-
polating method calculates it by:
Pr(y|deij) =?Pr1(y|deij )
+ (1 ? ?)Pr2(y|deij )
(10)
where Pr1(y|deij ) and Pr2(y|deij ) are the proba-
bilities provided by different models.
4.2 Training Algorithm
We optimize the objective (Formula (9)) via a
gradient-based search algorithm. And the gradi-
ent with respect to ?k takes the form:
??(?k) = ?
??2(?)
??k
+ (1 ? ?)??(?)??k
(11)
Algorithm 3 outlines our joint training proce-
dure, which tunes the grammar parameter ? simul-
taneously maximize both unsupervised objective
Algorithm 3 Training joint model
1: DP , DN ? proj(F,DF , A,E)
2: build random DE
3: ?? train(DP , DN )
4: repeat
5: for each E ? E do ? E step
6: DE ? parse(E,?)
7: ??(?)? grad(DE, D?E , DP , DN , ?(?))
8: ??climb(?(?),??(?), ?) ? M step
9: until convergence
and projection objective. And it incorporates un-
supervised framework and projection model algo-
rithm together. It is grounded on the work which
uses features in the unsupervised model (Berg-
Kirkpatrick et al, 2010).
In line 1, 2 we get projected dependency in-
stances from source side according to projec-
tion methods and build a random treebank (step
1). Then we train an initial model with projection
instances in line 3. From line 4-9, the objective is
optimized with a generic optimization step in the
subroutine climb(?, ?, ?, ?, ?). For each sentence we
parse its dependency tree, and update the tree into
the treebank (step 3). Then we calculate the gra-
dient and optimize the joint objective according to
the evolved treebank and projected instances (step
2). Lines 5-6 are equivalent to the E-step of the
EM algorithm, and lines 7-8 are equivalent to the
M-step.
5 Related work
The DMV (Klein and Manning, 2004) is a single-
state head automata model (Alshawi, 1996) which
is based on POS tags. And DMV learns the gram-
mar via inside-outside re-estimation (Baker, 1979)
without any smoothing, while Spitkovsky et al
(2010) utilizes smoothing and learning strategy
during grammar learning and William et al (2009)
improves DMV with richer context.
The dependency projection method DPA (H-
wa et al, 2005) based on Direct Correspondence
Assumption (Hwa et al, 2002) can be described
as: if there is a pair of source words with a de-
pendency relationship, the corresponding aligned
words in target sentence can be considered as hav-
ing the same dependency relationship equivalent-
ly (e.g. Figure 2). The Word Pair Classification
(WPC) method (Jiang and Liu, 2010) modifies the
DPA method and makes it more robust. Smith
and Eisner (2009) propose an adaptation method
founded on quasi-synchronous grammar features
1067
Type Feature Template
Unigram wordi posi wordi ? posi
wordj posj wordj ? posj
Bigram wordi ? posj wordj ? posi posi ? posj
wordi ? wordj wordi ? posi ? wordj wordi ? wordj ? posj
wordi ? posi ? posj posi ? wordj ? posj
wordi ? posi ? wordj ? posj
Surrounding posi?1 ? posi ? posj posi ? posi+1 ? posj posi ? posj?1 ? posjposi ? posj ? posj+1 posi?1 ? posi ? posj?1 posi ? posi+1 ? posj+1posi?1 ? posj?1 ? posj posi+1 ? posj ? posj+1 posi?1 ? posi ? posj+1posi ? posi+1 ? posj?1 posi?1 ? posj ? posj+1 posi+1 ? posj?1 ? posjposi?1 ? posi ? posj?1 ? posj posi ? posi+1 ? posj ? posj+1posi ? posi+1 ? posj?1 ? posj posi?1 ? posi ? posj ? posj+1
Table 1: Feature templates for dependency parsing. For edge deij : wordi is the parent word and wordj
is the child word, similar to ?pos?. ?+1? denotes the preceding token of the sentence, similar to ?-1?.
for dependency projection and annotation, which
requires a small set of dependency annotated cor-
pus of target language.
Similarly, using indirect information from mul-
tilingual (Cohen et al, 2011; Ta?ckstro?m et al,
2012) is an effective way to improve unsupervised
parsing. (Zeman and Resnik, 2008; McDonald et
al., 2011; S?gaard, 2011) employ non-lexicalized
parser trained on other languages to process a
target language. McDonald et al (2011) adapts
their multi-source parser according to DCA, while
Naseem et al (2012) selects a selective sharing
model to make better use of grammar information
in multi-sources.
Due to similar reasons, many works are devoted
to POS projection (Yarowsky et al, 2001; Shen et
al., 2007; Naseem et al, 2009), and they also suf-
fer from similar problems. Some seek for unsu-
pervised methods, e.g. Naseem et al (2009), and
some further improve the projection by a graph-
based projection (Das and Petrov, 2011).
Our model differs from the approaches above
in its emphasis on utilizing information from both
sides of bilingual corpus in an unsupervised train-
ing framework, while most of the work above only
utilize the information from a single side.
6 Experiments
In this section, we evaluate the performance of the
MST dependency parser (McDonald et al, 2005b)
which is trained by our bilingually-guided model
on 5 languages. And the features used in our ex-
periments are summarized in Table 1.
6.1 Experiment Setup
Datasets and Evaluation Our experiments are
run on five different languages: Chinese(ch),
Danish(da), Dutch(nl), Portuguese(pt) and
Swedish(sv) (da, nl, pt and sv are free data sets
distributed for the 2006 CoNLL Shared Tasks
(Buchholz and Marsi, 2006)). For all languages,
we only use English-target parallel data: we take
the FBIS English-Chinese bitext as bilingual cor-
pus for English-Chinese dependency projection
which contains 239K sentence pairs with about
8.9M/6.9M words in English/Chinese, and for
other languages we use the readily available data
in the Europarl corpus. Then we run tests on the
Penn Chinese Treebank (CTB) and CoNLL-X test
sets.
English sentences are tagged by the implemen-
tations of the POS tagger of Collins (2002), which
is trained on WSJ. The source sentences are then
parsed by an implementation of 2nd-ordered MST
model of McDonald and Pereira (2006), which is
trained on dependency trees extracted from Penn
Treebank.
As the evaluation metric, we use parsing accu-
racy which is the percentage of the words which
have found their correct parents. We evaluate on
sentences with all length for our method.
Training Regime In experiments, we use the
projection method proposed by Jiang and Liu
(2010) to provide the projection instances. And
we train the projection part ? = 0 first for initial-
ization, on which the whole model will be trained.
Availing of the initialization method, the model
can converge very fast (about 3 iterations is suffi-
cient) and the results are more stable than the ones
trained on random initialization.
Baselines We compare our method against
three kinds of different approaches: unsupervised
method (Klein and Manning, 2004); single-
source direct projection methods (Hwa et al,
2005; Jiang and Liu, 2010); multi-source in-
direct projection methods with multi-sources (M-
1068
60.0
61.5
          
 
 
ch
50.3
51.2
          
 
 
da
59.5
60.5
          
ac
cu
ra
cy
%
 
nl
70.5
74.5
          
 
 
pt
61.5
65.0
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 
alpha
sv
Figure 3: The performance of our model with re-
spect to a series of ratio ?
cDonald et al, 2011; Naseem et al, 2012).
6.2 Results
We test our method on CTB and CoNLL-X free
test data sets respectively, and the performance is
summarized in Table 2. Figure 3 presents the per-
formance with different ? on different languages.
Compare against Unsupervised Baseline Ex-
perimental results show that our unsupervised
framework?s performance approaches to the DMV
method. And the bilingually-guided model can
promote the unsupervised method consisten-
cy over all languages. On the best results? aver-
age of four comparable languages (da, nl, pt, sv),
the promotion gained by our model is 28.5% over
the baseline method (DMV) (Klein and Manning,
2004).
Compare against Projection Baselines For
all languages, the model consistent-
ly outperforms on direct projection baseline.
On the average of each language?s best result, our
model outperforms all kinds of baselines, yielding
3.0% gain over the single-source direct-projection
method (Jiang and Liu, 2010) and 3.9% gain over
the multi-source indirect-projection method (Mc-
Donald et al, 2011). On the average of all results
with different parameters, our method also gain-
s more than 2.0% improvements on all baselines.
Particularly, our model achieves the most signif-
icant gains on Chinese, where the improvements
are 4.5%/12.0% on direct/indirect projection base-
Accuracy%
Model ch da nl pt sv avg
DMV 42.5? 33.4 38.5 20.1 44.0 ?.?
DPA 53.9 ?.? ?.? ?.? ?.? ?.?
WPC 56.8 50.1 58.4 70.5 60.8 59.3
Transfer 49.3 49.5 53.9 75.8 63.6 58.4
Selective 51.2 ?.? 55.9 73.5 61.5 ?.?
unsuper 22.6 41.6 15.2 45.7 42.4 33.5
avg 61.0 50.7 59.9 72.0 63.1 61.3
max 61.3 51.1 60.1 74.2 64.6 62.3
Table 2: The directed dependency accuracy with
different parameter of our model and the base-
lines. The first section of the table (row 3-7)
shows the results of the baselines: a unsupervised
method baseline (Klein and Manning, 2004)(D-
MV); a single-source projection method baseline
(Hwa et al, 2005) (DPA) and its improve-
ment (Jiang and Liu, 2010)(WPC); two multi-
source baselines (McDonald et al, 2011)(Trans-
fer) and (Naseem et al, 2012)(Selective). The
second section of the table (row 8) presents the
result of our unsupervised framework (unsuper).
The third section gives the mean value (avg) and
maximum value (max) of our model with different
? in Figure 3.
*: The result is based on sentences with 10
words or less after the removal of punctuation, it
is an incomparable result.
lines.
The results in Figure 3 prove that our unsuper-
vised framework ? = 1 can promote the grammar
induction if it has a good start (well initialization),
and it will be better once we incorporate the infor-
mation from the projection side (? = 0.9). And
the maximum points are not in ? = 1, which im-
plies that projection information is still available
for the unsupervised framework even if we employ
the projection model as the initialization. So we
suggest that a greater parameter is a better choice
for our model. And there are some random factors
in our model which make performance curves with
more fluctuation. And there is just a little improve-
ment shown in da, in which the same situation is
observed by (McDonald et al, 2011).
6.3 Effects of the Size of Training Corpus
To investigate how the size of the training corpus
influences the result, we train the model on ex-
tracted bilingual corpus with varying sizes: 10K,
50K, 100K, 150K and 200K sentences pairs.
As shown in Figure 4, our approach continu-
1069
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
10K 50K 100K 150K 200K
ac
cu
ra
cy
%
size of training set
our model
baseline
Figure 4: Performance on varying sizes (average
of 5 languages, ? = 0.9)
 51
 52
 53
 54
 55
 56
 57
 58
 59
 60
 61
 62
 63
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
ac
cu
ra
cy
%
noise rate
our model
baseline
Figure 5: Performance on different projection
quality (average of 5 languages, ? = 0.9). The
noise rate is the percentage of the projected in-
stances being messed up.
ously outperforms the baseline with the increasing
size of training corpus. It is especially noteworthy
that the more training data is utilized the more su-
periority our model enjoys. That is, because our
method not only utilizes the projection informa-
tion but also avails itself of the monolingual cor-
pus.
6.4 Effect of Projection Quality
The projection quality can be influenced by the
quality of the source parsing, alignments, projec-
tion methods, corpus quality and many other fac-
tors. In order to detect the effects of varying pro-
jection qualities on our approach, we simulate the
complex projection procedure by messing up the
projected instances randomly with different noise
rates. The curves in Figure 5 show the perfor-
mance of WPC baseline and our bilingual-guided
method. For different noise rates, our model?s re-
sults consistently outperform the baselines. When
the noise rate is greater than 0.2, our improvement
49.5
...
54.6
...
58.2
58.6
59.0
59.4
59.8
60.2
0 0.02 0.04 0.06 0.08 0.1 ... 0.2 ... 0.3
ac
cu
ra
cy
%
alpha
our model
baseline(58.5)
Figure 6: The performance curve of our model
(random initialization) on Chinese, with respect to
a series of ratio ?. The baseline is the result of
WPC model.
increases with the growth of the noise rate. The re-
sult suggests that our method can solve some prob-
lems which are caused by projection noise.
6.5 Performance on Random Initialization
We test our model with random initialization on
different ?. The curve in Figure 6 shows the per-
formance of our model on Chinese.
The results seem supporting our unsupervised
optimization method when ? is in the range of
(0, 0.1). It implies that the unsupervised structure
information is useful, but it seems creating a nega-
tive effect on the model when ? is greater than 0.1.
Because the unsupervised part can gain constraints
from the projection part. But with the increase of
?, the strength of constraint dwindles, and the
unsupervised part will gradually lose control. And
bad unsupervised part pulls the full model down.
7 Conclusion and Future Work
This paper presents a bilingually-guided strate-
gy for automatic dependency grammar induction,
which adopts an unsupervised skeleton and lever-
ages the bilingually-projected dependency infor-
mation during optimization. By simultaneous-
ly maximizing the monolingual likelihood and
bilingually-projected likelihood in the EM proce-
dure, it effectively integrates the advantages of
bilingual projection and unsupervised induction.
Experiments on 5 languages show that the novel
strategy significantly outperforms previous unsu-
pervised or bilingually-projected models.
Since its computational complexity approaches to
the skeleton unsupervised model (with much few-
er iterations), and the bilingual text aligned to
1070
resource-rich languages is easy to obtain, such a
hybrid method seems to be a better choice for au-
tomatic grammar induction. It also indicates that
the combination of bilingual constraint and unsu-
pervised methodology has a promising prospect
for grammar induction. In the future work we will
investigate such kind of strategies, such as bilin-
gually unsupervised induction.
Acknowledgments
The authors were supported by National
Natural Science Foundation of China, Con-
tracts 61202216, 863 State Key Project (No.
2011AA01A207), and National Key Technology
R&D Program (No. 2012BAH39B03), Key
Project of Knowledge Innovation Program of Chi-
nese Academy of Sciences (No. KGZD-EW-501).
Qun Liu?s work is partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. We
would like to thank the anonymous reviewers for
their insightful comments and those who helped
to modify the paper.
References
H. Alshawi. 1996. Head automata for speech transla-
tion. In Proc. of ICSLP.
James K Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65:S132.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero,
and D. Klein. 2010. Painless unsupervised learn-
ing with features. In HLT: NAACL, pages 582?590.
Rens Bod. 2006. An all-subtrees approach to unsu-
pervised parsing. In Proc. of the 21st ICCL and the
44th ACL, pages 865?872.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proc. of the
2002 Conference on EMNLP. Proc. CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative r-
eranking. In Proc. of the 43rd ACL, pages 173?180,
Ann Arbor, Michigan, June.
W. Chen, J. Kazama, and K. Torisawa. 2010. Bi-
text dependency parsing with bilingual subtree con-
straints. In Proc. of ACL, pages 21?29.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsu-
pervised structure prediction with non-parallel mul-
tilingual guidance. In Proc. of the Conference on
EMNLP, pages 50?61.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proc. of the
2002 Conference on EMNLP, pages 1?8, July.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. In Computational
Linguistics.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projec-
tions. In Proc. of ACL.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection
constraints. In Proc. of IJCNLP of the AFNLP: Vol-
ume 1-Volume 1, pages 369?377.
R. Hwa, P. Resnik, A. Weinberg, and O. Kolak. 2002.
Evaluating translational correspondence using anno-
tation projection. In Proc. of ACL, pages 392?399.
R. Hwa, M. Osborne, A. Sarkar, and M. Steedman.
2003. Corrected co-training for statistical parsers.
In ICML-03 Workshop on the Continuum from La-
beled to Unlabeled Data in Machine Learning and
Data Mining, Washington DC.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural language
engineering, 11(3):311?325.
W. Jiang and Q. Liu. 2010. Dependency parsing
and projection based on word-pair classification. In
Proc. of ACL, pages 12?20.
D. Klein and C.D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependen-
cy and constituency. In Proc. of ACL, page 478.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of the 48th ACL,
pages 1?11, July.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. pages 595?
603.
R. McDonald and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
Proc. of the 11th Conf. of EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. of ACL, pages 91?98.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b. Non-projective dependency parsing using s-
panning tree algorithms. In Proc. of EMNLP, pages
523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of CoNLL, pages 216?
220.
1071
R. McDonald, S. Petrov, and K. Hall. 2011. Multi-
source transfer of delexicalized dependency parsers.
In Proc. of EMNLP, pages 62?72. ACL.
T. Naseem, B. Snyder, J. Eisenstein, and R. Barzilay.
2009. Multilingual part-of-speech tagging: Two un-
supervised approaches. Journal of Artificial Intelli-
gence Research, 36(1):341?385.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In Proc. of the 50th ACL, pages 629?637,
July.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
CoNLL, pages 221?225.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st ICCL
& 44th ACL, pages 433?440, July.
A. Sarkar. 2001. Applying co-training methods to sta-
tistical parsing. In Proc. of NAACL, pages 1?8.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In Annual
Meeting-, volume 45, page 760.
N.A. Smith and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proc. of ACL, pages 354?362.
D.A. Smith and J. Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous gram-
mar features. In Proc. of EMNLP: Volume 2-Volume
2, pages 822?831.
B. Snyder, T. Naseem, and R. Barzilay. 2009. Unsu-
pervised multilingual grammar induction. In Proc.
of IJCNLP of the AFNLP: Volume 1-Volume 1, pages
73?81.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
of the 49th ACL: HLT, pages 682?686.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010. From baby steps to leapfrog: How
?less is more? in unsupervised dependency parsing.
In HLT: NAACL, pages 751?759, June.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure.
William, M. Johnson, and D. McClosky. 2009. Im-
proving unsupervised dependency parsing with rich-
er contexts and smoothing. In Proc. of NAACL,
pages 101?109.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. of HLT,
pages 1?8.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proc. of the IJCNLP-08. Proc. CoNLL.
Ciyou Zhu, Richard H Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. Algorithm 778: L-bfgs-b: Fortran
subroutines for large-scale bound-constrained opti-
mization. ACM Transactions on Mathematical Soft-
ware (TOMS), 23(4):550?560.
1072
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 358?363,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Novel Graph-based Compact Representation of Word Alignment
Qun Liu?? Zhaopeng Tu? Shouxun Lin?
?Centre for Next Generation Locolisation ?Key Lab. of Intelligent Info. Processing
Dublin City University Institute of Computing Technology, CAS
qliu@computing.dcu.ie {tuzhaopeng,sxlin}@ict.ac.cn
Abstract
In this paper, we propose a novel compact
representation called weighted bipartite
hypergraph to exploit the fertility model,
which plays a critical role in word align-
ment. However, estimating the probabili-
ties of rules extracted from hypergraphs is
an NP-complete problem, which is com-
putationally infeasible. Therefore, we pro-
pose a divide-and-conquer strategy by de-
composing a hypergraph into a set of inde-
pendent subhypergraphs. The experiments
show that our approach outperforms both
1-best and n-best alignments.
1 Introduction
Word alignment is the task of identifying trans-
lational relations between words in parallel cor-
pora, in which a word at one language is usually
translated into several words at the other language
(fertility model) (Brown et al, 1993). Given that
many-to-many links are common in natural lan-
guages (Moore, 2005), it is necessary to pay atten-
tion to the relations among alignment links.
In this paper, we have proposed a novel graph-
based compact representation of word alignment,
which takes into account the joint distribution of
alignment links. We first transform each align-
ment to a bigraph that can be decomposed into a
set of subgraphs, where all interrelated links are
in the same subgraph (? 2.1). Then we employ
a weighted partite hypergraph to encode multiple
bigraphs (? 2.2).
The main challenge of this research is to effi-
ciently calculate the fractional counts for rules ex-
tracted from hypergraphs. This is equivalent to the
decision version of set covering problem, which is
NP-complete. Observing that most alignments are
not connected, we propose a divide-and-conquer
strategy by decomposing a hypergraph into a set
Figure 1: A bigraph constructed from an align-
ment (a), and its disjoint MCSs (b).
of independent subhypergraphs, which is compu-
tationally feasible in practice (? 3.2). Experimen-
tal results show that our approach significantly im-
proves translation performance by up to 1.3 BLEU
points over 1-best alignments (? 4.3).
2 Graph-based Compact Representation
2.1 Word Alignment as a Bigraph
Each alignment of a sentence pair can be trans-
formed to a bigraph, in which the two disjoint ver-
tex sets S and T are the source and target words re-
spectively, and the edges are word-by-word links.
For example, Figure 1(a) shows the corresponding
bigraph of an alignment.
The bigraph usually is not connected. A graph
is called connected if there is a path between every
pair of distinct vertices. In an alignment, words in
a specific portion at the source side (i.e. a verb
phrase) usually align to those in the corresponding
portion (i.e. the verb phrase at the target side), and
would never align to other words; and vice versa.
Therefore, there is no edge that connects the words
in the portion to those outside the portion.
Therefore, a bigraph can be decomposed into
a unique set of minimum connected subgraphs
(MCSs), where each subgraph is connected and
does not contain any other MCSs. For example,
the bigraph in Figure 1(a) can be decomposed into
358
the
book
is
on
the
desk
?
?
??
?
D
the
book
is
on
the
desk
?
?
??
?
e1
F
the
book
is
on
the
desk
?
?
??
?
E
e2 e3
e4
e5
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) the
resulting hypergraph that takes the two alignments as samples.
the MCSs in Figure 1(b). We can see that all in-
terrelated links are in the same MCS. These MCSs
work as fundamental units in our approach to take
advantage of the relations among the links. Here-
inafter, we use bigraph to denote the alignment of
a sentence pair.
2.2 Weighted Bipartite Hypergraph
We believe that offering more alternatives to ex-
tracting translation rules could help improve trans-
lation quality. We propose a new structure called
weighted bipartite hypergraph that compactly en-
codes multiple alignments.
We use an example to illustrate our idea. Fig-
ures 2(a) and 2(b) show two bigraphs of the same
sentence pair. Intuitively, we can encode the
union set of subgraphs in a bipartite hypergraph,
in which each MCS serves as a hyperedge, as in
Figure 2(c). Accordingly, we can calculate how
well a hyperedge is by calculating its relative fre-
quency, which is the probability sum of bigraphs
in which the corresponding MCS occurs divided
by the probability sum of all possible bigraphs.
Suppose that the probabilities of the two bigraphs
in Figures 2(a) and 2(b) are 0.7 and 0.3, respec-
tively. Then the weight of e1 is 1.0 and e2 is
0.7. Therefore, each hyperedge is associated with
a weight to indicate how well it is.
Formally, a weighted bipartite hypergraph H is
a triple ?S, T,E? where S and T are two sets of
vertices on the source and target sides, and E are
hyperedges associated with weights. Currently,
we estimate the weights of hyperedges from an n-
best list by calculating relative frequencies:
w(ei) =
?
BG?N p(BG) ? ?(BG, gi)?
BG?N p(BG)
Here N is an n-best bigraph (i.e., alignment) list,
p(BG) is the probability of a bigraph BG in the n-
best list, gi is the MCS that corresponds to ei, and
?(BG, gi) is an indicator function which equals 1
when gi occurs in BG, and 0 otherwise.
It is worthy mentioning that a hypergraph en-
codes much more alignments than the input n-best
list. For example, we can construct a new align-
ment by using hyperedges from different bigraphs
that cover all vertices.
3 Graph-based Rule Extraction
In this section we describe how to extract transla-
tion rules from a hypergraph (? 3.1) and how to
estimate their probabilities (? 3.2).
3.1 Extraction Algorithm
We extract translation rules from a hypergraph
for the hierarchical phrase-based system (Chiang,
2007). Chiang (2007) describes a rule extrac-
tion algorithm that involves two steps: (1) extract
phrases from 1-best alignments; (2) obtain vari-
able rules by replacing sub-phrase pairs with non-
terminals. Our extraction algorithm differs at the
first step, in which we extract phrases from hyper-
graphs instead of 1-best alignments. Rather than
restricting ourselves by the alignment consistency
in the traditional algorithm, we extract all possible
candidate target phrases for each source phrase.
To maintain a reasonable rule table size, we fil-
ter out less promising candidates that have a frac-
tional count lower than a threshold.
3.2 Calculating Fractional Counts
The fractional count of a phrase pair is the proba-
bility sum of the alignments with which the phrase
pair is consistent (?3.2.2), divided by the probabil-
ity sum of all alignments encoded in a hypergraph
(?3.2.1) (Liu et al, 2009).
359
Intuitively, our approach faces two challenges:
1. How to calculate the probability sum of all
alignments encoded in a hypergraph (?3.2.1)?
2. How to efficiently calculate the probability
sum of all consistent alignments for each
phrase pair (?3.2.2)?
3.2.1 Enumerating All Alignments
In theory, a hypergraph can encode all possible
alignments if there are enough hyperedges. How-
ever, since a hypergraph is constructed from an n-
best list, it can only represent partial space of all
alignments (p(A|H) < 1) because of the limiting
size of hyperedges learned from the list. There-
fore, we need to enumerate all possible align-
ments in a hypergraph to obtain the probability
sum p(A|H).
Specifically, generating an alignment from a hy-
pergraph can be modelled as finding a complete
hyperedge matching, which is a set of hyperedges
without common vertices that matches all vertices.
The probability of the alignment is the product of
hyperedge weights. Thus, enumerating all possi-
ble alignments in a hypergraph is reformulated as
finding all complete hypergraph matchings, which
is an NP-complete problem (Valiant, 1979).
Similar to the bigraph, a hypergraph is also usu-
ally not connected. To make the enumeration prac-
tically tractable, we propose a divide-and-conquer
strategy by decomposing a hypergraph H into a set
of independent subhypergraphs {h1, h2, . . . , hn}.
Intuitively, the probability of an alignment is the
product of hyperedge weights. According to the
divide-and-conquer strategy, the probability sum
of all alignments A encoded in a hypergraph H is:
p(A|H) =
?
hi?H
p(Ai|hi)
Here p(Ai|hi) is the probability sum of all sub-
alignments Ai encoded in the subhypergraph hi.
3.2.2 Enumerating Consistent Alignments
Since a hypergraph encodes many alignments, it is
unrealistic to enumerate all consistent alignments
explicitly for each phrase pair.
Recall that a hypergraph can be decomposed
to a list of independent subhypergraphs, and an
alignment is a combination of the sub-alignments
from the decompositions. We observe that a
phrase pair is absolutely consistent with the sub-
alignments from some subhypergraphs, while pos-
sibly consistent with the others. As an example,
E
the
book
is
on
the
desk
?
?
??
?
e1
D
e2 e3
e4
e5
the
book
is
on
the
desk
?
?
??
?
e1
e2 e3
e4
e5
h1
h3
h2
Figure 3: A hypergraph with a candidate phrase
in the grey shadow (a), and its independent subhy-
pergraphs {h1, h2, h3}.
consider the phrase pair in the grey shadow in Fig-
ure 3(a), it is consistent with all sub-alignments
from both h1 and h2 because they are outside and
inside the phrase pair respectively, while not con-
sistent with the sub-alignment that contains hyper-
edge e2 from h3 because it contains an alignment
link that crosses the phrase pair.
Therefore, to calculate the probability sum of all
consistent alignments, we only need to consider
the overlap subhypergraphs, which have at least
one hyperedge that crosses the phrase pair. Given
a overlap subhypergraph, the probability sum of
consistent sub-alignments is calculated by sub-
tracting the probability sum of the sub-alignments
that contain crossed hyperedges, from the proba-
bility sum of all sub-alignments encoded in a hy-
pergraph.
Given a phrase pair P , let OS and NS de-
notes the sets of overlap and non-overlap subhy-
pergraphs respectively (NS = H ?OS). Then
p(A|H,P ) =
?
hi?OS
p(Ai|hi, P )
?
hj?NS
p(Aj|hj)
Here the phrase pair is absolutely consistent with
the sub-alignments from non-overlap subhyper-
graphs (NS), and we have p(A|h, P ) = p(A|h).
Then the fractional count of a phrase pair is:
c(P |H) = p(A|H,P )p(A|H) =
?
hi?OS p(A|hi, P )?
hi?OS p(A|hi)
After we get the fractional counts of transla-
tion rules, we can estimate their relative frequen-
cies (Och and Ney, 2004). We follow (Liu et al,
2009; Tu et al, 2011) to learn lexical tables from
n-best lists and then calculate the lexical weights.
360
Rules from. . . Rules MT03 MT04 MT05 Avg.
1-best 257M 33.45 35.25 33.63 34.11
10-best 427M 34.10 35.71 34.04 34.62
Hypergraph 426M 34.71 36.24 34.41 35.12
Table 1: Evaluation of translation quality.
4 Experiments
4.1 Setup
We carry out our experiments on Chinese-English
translation tasks using a reimplementation of the
hierarchical phrase-based system (Chiang, 2007).
Our training data contains 1.5 million sentence
pairs from LDC dataset.1 We train a 4-gram
language model on the Xinhua portion of the
GIGAWORD corpus using the SRI Language
Toolkit (Stolcke, 2002) with modified Kneser-Ney
Smoothing (Kneser and Ney, 1995). We use min-
imum error rate training (Och, 2003) to optimize
the feature weights on the MT02 testset, and test
on the MT03/04/05 testsets. For evaluation, case-
insensitive NIST BLEU (Papineni et al, 2002) is
used to measure translation performance.
We first follow Venugopal et al (2008) to pro-
duce n-best lists via GIZA++. We produce 10-best
lists in two translation directions, and use ?grow-
diag-final-and? strategy (Koehn et al, 2003) to
generate the final n-best lists by selecting the
top n alignments. We re-estimated the probabil-
ity of each alignment in the n-best list using re-
normalization (Venugopal et al, 2008). Finally we
construct weighted alignment hypergraphs from
these n-best lists.2 When extracting rules from hy-
pergraphs, we set the pruning threshold t = 0.5.
4.2 Tractability of Divide-and-Conquer
Strategy
Figure 4 shows the distribution of vertices (hy-
peredges) number of the subhypergraphs. We can
see that most of the subhypergraphs have just less
than two vertices and hyperedges.3 Specifically,
each subhypergraph has 2.0 vertices and 1.4 hy-
1The corpus includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.
2Here we only use 10-best lists, because the alignments
beyond top 10 have very small probabilities, thus have negli-
gible influence on the hypergraphs.
3It?s interesting that there are few subhypergraphs that
have exactly 2 hyperedges. In this case, the only two hy-
peredges fully cover the vertices and they differ at the word-
by-word links, which is uncommon in n-best lists.
 0
 0.2
 0.4
 0.6
 0.8
 1
1 2 3 4 5 6 7 8 9 10
pe
rc
en
ta
ge
number of vertices (hyperedges)
vertices
hyperedges
Figure 4: The distribution of vertices (hyperedges)
number of the subhypergraphs.
peredges on average. This suggests that the divide-
and-conquer strategy makes the extraction compu-
tationally tractable, because it greatly reduces the
number of vertices and hyperedges. For computa-
tional tractability, we only allow a subhypergraph
has at most 5 hyperedges. 4
4.3 Translation Performance
Table 1 shows the rule table size and transla-
tion quality. Using n-best lists slightly improves
the BLEU score over 1-best alignments, but at
the cost of a larger rule table. This is in ac-
cord with intuition, because all possible transla-
tion rules would be extracted from different align-
ments in n-best lists without pruning. This larger
rule table indeed leads to a high rule coverage, but
in the meanwhile, introduces translation errors be-
cause of the low-quality rules (i.e., rules extracted
only from low-quality alignments in n-best lists).
By contrast, our approach not only significantly
improves the translation performance over 1-best
alignments, but also outperforms n-best lists with
a similar-scale rule table. The absolute improve-
ments of 1.0 BLEU points on average over 1-best
alignments are statistically significant at p < 0.01
using sign-test (Collins et al, 2005).
4If a subhypergraph has more than 5 hyperedges, we
forcibly partition it into small subhypergraphs by iteratively
removing lowest-probability hyperedges.
361
Rules from. . . Shared Non-shared AllRules BLEU Rules BLEU Rules BLEU
10-best 1.83M 32.75 2.81M 30.71 4.64M 34.62
Hypergraph 1.83M 33.24 2.89M 31.12 4.72M 35.12
Table 2: Comparison of rule tables learned from n-best lists and hypergraphs. ?All? denotes the full rule
table, ?Shared? denotes the intersection of two tables, and ?Non-shared? denotes the complement. Note
that the probabilities of ?Shared? rules are different for the two approaches.
Why our approach outperforms n-best lists? In
theory, the rule table extracted from n-best lists
is a subset of that from hypergraphs. In prac-
tice, however, this is not true because we pruned
the rules that have fractional counts lower than a
threshold. Therefore, the question arises as to how
many rules are shared by n-best and hypergraph-
based extractions. We try to answer this ques-
tion by comparing the different rule tables (filtered
on the test sets) learned from n-best lists and hy-
pergraphs. Table 2 gives some statistics. ?All?
denotes the full rule table, ?Shared? denotes the
intersection of two tables, and ?Non-shared? de-
notes the complement. Note that the probabil-
ities of ?Shared? rules are different for the two
approaches. We can see that both the ?Shared?
and ?Non-shared? rules learned from hypergraphs
outperform n-best lists, indicating: (1) our ap-
proach has a better estimation of rule probabili-
ties because we estimate the probabilities from a
much larger alignment space that can not be rep-
resented by n-best lists, (2) our approach can ex-
tract good rules that cannot be extracted from any
single alignments in the n-best lists.
5 Related Work
Our research builds on previous work in the field
of graph models and compact representations.
Graph models have been used before in word
alignment: the search space of word alignment can
be structured as a graph and the search problem
can be reformulated as finding the optimal path
though this graph (e.g., (Och and Ney, 2004; Liu et
al., 2010)). In addition, Kumar and Byrne (2002)
define a graph distance as a loss function for
minimum Bayes-risk word alignment, Riesa and
Marcu (2010) open up the word alignment task to
advances in hypergraph algorithms currently used
in parsing. As opposed to the search problem, we
propose a graph-based compact representation that
encodes multiple alignments for machine transla-
tion.
Previous research has demonstrated that com-
pact representations can produce improved re-
sults by offering more alternatives, e.g., using
forests over 1-best trees (Mi and Huang, 2008;
Tu et al, 2010; Tu et al, 2012a), word lattices
over 1-best segmentations (Dyer et al, 2008),
and weighted alignment matrices over 1-best word
alignments (Liu et al, 2009; Tu et al, 2011; Tu et
al., 2012b). Liu et al, (2009) estimate the link
probabilities from n-best lists, while Gispert et
al., (2010) learn the alignment posterior probabil-
ities directly from IBM models. However, both of
them ignore the relations among alignment links.
By contrast, our approach takes into account the
joint distribution of alignment links and explores
the fertility model past the link level.
6 Conclusion
We have presented a novel compact representa-
tion of word alignment, named weighted bipar-
tite hypergraph, to exploit the relations among
alignment links. Since estimating the probabil-
ities of rules extracted from hypergraphs is an
NP-complete problem, we propose a computation-
ally tractable divide-and-conquer strategy by de-
composing a hypergraph into a set of independent
subhypergraphs. Experimental results show that
our approach outperforms both 1-best and n-best
alignments.
Acknowledgement
The authors are supported by 863 State Key
Project No. 2011AA01A207, National Key Tech-
nology R&D Program No. 2012BAH39B03 and
National Natural Science Foundation of China
(Contracts 61202216). Qun Liu?s work is partially
supported by Science Foundation Ireland (Grant
No.07/CE/I1142) as part of the CNGL at Dublin
City University. We thank Junhui Li, Yifan He
and the anonymous reviewers for their insightful
comments.
362
References
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 531?540.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
545?554.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 48?54.
Shankar Kumar and William Byrne. 2002. Mini-
mum Bayes-risk word alignments of bilingual texts.
In Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, pages
140?147.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1017?1026.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrim-
inative word alignment by linear modeling. Compu-
tational Linguistics, 36(3):303?339.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, October.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of Seventh Inter-
national Conference on Spoken Language Process-
ing, volume 3, pages 901?904. Citeseer.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 1092?1100.
Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Extracting hierarchical rules from a weighted
alignment matrix. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1294?1303.
Zhaopeng Tu, Wenbin Jiang, Qun Liu, and Shouxun
Lin. 2012a. Dependency forest for sentiment anal-
ysis. In Springer-Verlag Berlin Heidelberg, pages
69?77.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012b. Combining mul-
tiple alignments to improve machine translation. In
Proceedings of the 24th International Conference on
Computational Linguistics, pages 1249?1260.
Leslie G Valiant. 1979. The complexity of comput-
ing the permanent. Theoretical Computer Science,
8(2):189?201.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-best
alignments and parses in mt training. In Proceed-
ings of AMTA, pages 192?201.
363
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 364?369,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Stem Translation with Affix-Based Rule Selection
for Agglutinative Languages
Zhiyang Wang?, Yajuan Lu??, Meng Sun?, Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{wangzhiyang,lvyajuan,sunmeng,liuqun}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Current translation models are mainly de-
signed for languages with limited mor-
phology, which are not readily applicable
to agglutinative languages as the differ-
ence in the way lexical forms are gener-
ated. In this paper, we propose a nov-
el approach for translating agglutinative
languages by treating stems and affixes
differently. We employ stem as the atomic
translation unit to alleviate data spare-
ness. In addition, we associate each stem-
granularity translation rule with a distri-
bution of related affixes, and select desir-
able rules according to the similarity of
their affix distributions with given spans to
be translated. Experimental results show
that our approach significantly improves
the translation performance on tasks of
translating from three Turkic languages to
Chinese.
1 Introduction
Currently, most methods on statistical machine
translation (SMT) are developed for translation
of languages with limited morphology (e.g., En-
glish, Chinese). They assumed that word was the
atomic translation unit (ATU), always ignoring the
internal morphological structure of word. This
assumption can be traced back to the original
IBM word-based models (Brown et al, 1993) and
several significantly improved models, including
phrase-based (Och and Ney, 2004; Koehn et al,
2003), hierarchical (Chiang, 2005) and syntac-
tic (Quirk et al, 2005; Galley et al, 2006; Liu et
al., 2006) models. These improved models worked
well for translating languages like English with
large scale parallel corpora available.
Different from languages with limited morphol-
ogy, words of agglutinative languages are formed
mainly by concatenation of stems and affixes.
Generally, a stem can attach with several affixes,
thus leading to tens of hundreds of possible inflect-
ed variants of lexicons for a single stem. Modeling
each lexical form as a separate word will generate
high out-of-vocabulary rate for SMT. Theoretical-
ly, ways like morphological analysis and increas-
ing bilingual corpora could alleviate the problem
of data sparsity, but most agglutinative languages
are less-studied and suffer from the problem of
resource-scarceness. Therefore, previous research
mainly focused on the different inflected variants
of the same stem and made various transformation
of input by morphological analysis, such as (Lee,
2004; Goldwater and McClosky, 2005; Yang and
Kirchhoff, 2006; Habash and Sadat, 2006; Bisazza
and Federico, 2009; Wang et al, 2011). These
work still assume that the atomic translation unit
is word, stem or morpheme, without considering
the difference between stems and affixes.
In agglutinative languages, stem is the base
part of word not including inflectional affixes.
Affix, especially inflectional affix, indicates dif-
ferent grammatical categories such as tense, per-
son, number and case, etc., which is useful for
translation rule disambiguation. Therefore, we
employ stem as the atomic translation unit and
use affix information to guide translation rule
selection. Stem-granularity translation rules have
much larger coverage and can lower the OOV
rate. Affix based rule selection takes advantage
of auxiliary syntactic roles of affixes to make a
better rule selection. In this way, we can achieve
a balance between rule coverage and matching
accuracy, and ultimately improve the translation
performance.
364
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
(B)Translation rules with affix distribution
zunyi yighin ||| ????? ||| i:0 gha:0.09 zunyi yighin ||| ?????? ||| i:0 da:0.24
zunyi
/STM
i
/SUF
yighin
/STM
da
/SUF
zunyi yighin ||| ?????? ||| i da
(A) Instances of translation rule
(1) (2)
zunyi
/STM
i
/SUF
yighin
/STM
gha
/SUF
zunyi yighin ||| ?? ??? ||| i gha
(3)
Original:zunyi yighin+i+da
Meaning:on zunyi conference
Original:zunyi yighin+i+gha
Meaning:of zunyi conference
Figure 1: Translation rule extraction from Uyghur to Chinese. Here tag ?/STM? represents stem and
?/SUF? means suffix.
2 Affix Based Rule Selection Model
Figure 1 (B) shows two translation rules along
with affix distributions. Here a translation rule
contains three parts: the source part (on stem lev-
el), the target part, and the related affix distribution
(represented as a vector). We can see that, al-
though the source part of the two translation rules
are identical, their affix distributions are quite
different. Affix ?gha? in the first rule indicates
that something is affiliated to a subject, similar to
?of? in English. And ?da? in second rule implies
location information. Therefore, given a span
?zunyi/STM yighin/STM+i/SUF+da/SUF+...? to
be translated, we hope to encourage our model to
select the second translation rule. We can achieve
this by calculating similarity between the affix
distributions of the translation rule and the span.
The affix distribution can be obtained by keep-
ing the related affixes for each rule instance during
translation rule extraction ((A) in Figure 1). After
extracting and scoring stem-granularity rules in a
traditional way, we extract stem-granularity rules
again by keeping affix information and compute
the affix distribution with tf-idf (Salton and Buck-
ley, 1987). Finally, the affix distribution will be
added to the previous stem-granularity rules.
2.1 Affix Distribution Estimation
Formally, translation rule instances with the same
source part can be treated as a document collec-
tion1, so each rule instance in the collection is
1We employ concepts from text classification to illustrate
how to estimate affix distribution.
some kind of document. Our goal is to classify the
source parts into the target parts on the document
collection level with the help of affix distribu-
tion. Accordingly, we employ vector space model
(VSM) to represent affix distribution of each rule
instance. In this model, the feature weights are
represented by the classic tf-idf (Salton and Buck-
ley, 1987):
tf i,j =
ni,j?
k nk,j
idf i,j = log
|D|
|j : ai ? rj|
tfidf i,j = tf i,j ? idf i,j
(1)
where tfidf i,j is the weight of affix ai in transla-
tion rule instance rj . ni,j indicates the number of
occurrence of affix ai in rj . |D| is the number
of rule instance with the same source part, and
|j : ai ? rj| is the number of rule instance which
contains affix ai within |D|.
Let?s take the suffix ?gha? from (A1) in Figure
1 as an example. We assume that there are only
three instances of translation rules extracted from
parallel corpus ((A) in Figure 1). We can see that
?gha? only appear once in (A1) and also appear
once in whole instances. Therefore, tfgha,(A1) is
0.5 and idfgha,(A1) is log(3/2). tfidfgha,(A1) is
the product of tfgha,(A1) and idfgha,(A1) which
is 0.09.
Given a set of N translation rule instances with
the same source and target part, we define the
centroid vector dr according to the centroid-based
classification algorithm (Han and Karypis, 2000),
dr =
1
N
?
i?N
di (2)
365
Data set #Sent. #Type #Tokenword stem morph word stem morph
UY-CH-Train. 50K 69K 39K 42K 1.2M 1.2M 1.6M
UY-CH-Dev. 0.7K*4 5.9K 4.1K 4.6K 18K 18K 23.5K
UY-CH-Test. 0.7K*1 4.7K 3.3K 3.8K 14K 14K 17.8K
KA-CH-Train. 50K 62K 40K 42K 1.1M 1.1M 1.3M
KA-CH-Dev. 0.7K*4 5.3K 4.2K 4.5K 15K 15K 18K
KA-CH-Test. 0.2K*1 2.6K 2.0K 2.3K 8.6K 8.6K 10.8K
KI-CH-Train. 50K 53K 27K 31K 1.2M 1.2M 1.5M
KI-CH-Dev. 0.5K*4 4.1K 3.1K 3.5K 12K 12K 15K
KI-CH-Test. 0.2K*4 2.2K 1.8K 2.1K 4.7K 4.7K 5.8K
Table 1: Statistics of data sets. ?N means the number of reference, morph is short to morpheme. UY,
KA, KI, CH represent Uyghur, Kazakh, Kirghiz and Chinese respectively.
dr is the final affix distribution.
By comparing the similarity of affix distribu-
tions, we are able to decide whether a translation
rule is suitable for a span to be translated. In
this work, similarity is measured using the cosine
distance similarity metric, given by
sim(d1,d2) =
d1 ? d2
?d1? ? ?d2?
(3)
where di corresponds to a vector indicating affix
distribution, and ??? denotes the inner product of
the two vectors.
Therefore, for a specific span to be translated,
we first analyze it to get the corresponding stem
sequence and related affix distribution represented
as a vector. Then the stem sequence is used to
search the translation rule table. If the source part
is matched, the similarity will be calculated for
each candidate translation rule by cosine similarity
(as in equation 3). Therefore, in addition to the
traditional translation features on stem level, our
model also adds the affix similarity score as a
dynamic feature into the log-linear model (Och
and Ney, 2002).
3 Related Work
Most previous work on agglutinative language
translation mainly focus on Turkish and Finnish.
Bisazza and Federico (2009) and Mermer and
Saraclar (2011) optimized morphological analysis
as a pre-processing step to improve the translation
between Turkish and English. Yeniterzi and Oflaz-
er (2010) mapped the syntax of the English side
to the morphology of the Turkish side with the
factored model (Koehn and Hoang, 2007). Yang
and Kirchhoff (2006) backed off surface form to
stem when translating OOV words of Finnish.
Luong and Kan (2010) and Luong et al (2010)
focused on Finnish-English translation through
improving word alignment and enhancing phrase
table. These works still assumed that the atomic
translation unit is word, stem or morpheme, with-
out considering the difference between stems and
affixes.
There are also some work that employed the
context information to make a better choice of
translation rules (Carpuat and Wu, 2007; Chan et
al., 2007; He et al, 2008; Cui et al, 2010). all the
work employed rich context information, such as
POS, syntactic, etc., and experiments were mostly
done on less inflectional languages (i.e. Chinese,
English) and resourceful languages (i.e. Arabic).
4 Experiments
In this work, we conduct our experiments on
three different agglutinative languages, including
Uyghur, Kazakh and Kirghiz. All of them are
derived from Altaic language family, belonging to
Turkic languages, and mostly spoken by people in
Central Asia. There are about 24 million people
take these languages as mother tongue. All of
the tasks are derived from the evaluation of Chi-
na Workshop of Machine Translation (CWMT)2.
Table 1 shows the statistics of data sets.
For the language model, we use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train
a 5-gram model with the target side of training
corpus. And phrase-based Moses3 is used as our
2http://mt.xmu.edu.cn/cwmt2011/en/index.html.
3http://www.statmt.org/moses/
366
UY-CH KA-CH KI-CH
word 31.74+0.0 28.64+0.0 35.05+0.0
stem 33.74+2.0 30.14+1.5 35.52+0.47
morph 32.69+0.95 29.21+0.57 34.97?0.08
affix 34.34+2.6 30.19+2.27 35.96+0.91
Table 2: Translation results from Turkic languages
to Chinese. word: ATU is surface form,
stem: ATU is represented stem, morph: ATU
denotes morpheme, affix: stem translation with
affix distribution similarity. BLEU scores in
bold means significantly better than the baseline
according to (Koehn, 2004) for p-value less than
0.01.
baseline SMT system. The decoding weights are
optimized with MERT (Och, 2003) to maximum
word-level BLEU scores (Papineni et al, 2002).
4.1 Using Unsupervised Morphological
Analyzer
As most agglutinative languages are resource-
poor, we employ unsupervised learning method
to obtain the morphological structure. Follow-
ing the approach in (Virpioja et al, 2007), we
employ the Morfessor4 Categories-MAP algorith-
m (Creutz and Lagus, 2005). It applies a hierar-
chical model with three categories (prefix, stem,
and suffix) in an unsupervised way. From Table 1
we can see that vocabulary sizes of the three lan-
guages are reduced obviously after unsupervised
morphological analysis.
Table 2 shows the translation results. All the
three translation tasks achieve obvious improve-
ments with the proposed model, which always per-
forms better than only employ word, stem and
morph. For the Uyghur to Chinese translation
(UY-CH) task in Table 2, performances after unsu-
pervised morphological analysis are always better
than the baseline. And we gain up to +2.6 BLEU
points improvements with affix compared to the
baseline. For the Kazakh to Chinese translation
(KA-CH) task, the improvements are also signifi-
cant. We achieve +2.27 and +0.77 improvements
compared to the baseline and stem, respectively.
As for the Kirghiz to Chinese translation (KI-CH)
task, improvements seem relative small compared
to the other two language pairs. However, it also
gains +0.91 BLEU points over the baseline.
4http://www.cis.hut.fi/projects/morpho/
UY Unsup Sup
stem #Type 39K 21K#Token 1.2M 1.2M
affix #Type 3.0K 0.3K#Token 0.4M 0.7M
Table 3: Statistics of training corpus after unsuper-
vised(Unsup) and supervised(Sup) morphological
analysis.
 31.5 32
 32.5 33
 33.5 34
 34.5 35
 35.5 36
 36.5
word morph stem affix
B
L
E
U
 
s
c
o
r
e
(
%
)
UnsupervisedSupervised
Figure 2: Uyghur to Chinese translation results
after unsupervised and supervised analysis.
4.2 Using Supervised Morphological
Analyzer
Taking it further, we also want to see the effect of
supervised analysis on our model. A generative
statistical model of morphological analysis for
Uyghur was developed according to (Mairehaba
et al, 2012). Table 3 shows the difference of
statistics of training corpus after supervised and
unsupervised analysis. Supervised method gen-
erates fewer type of stems and affixes than the
unsupervised approach. As we can see from
Figure 2, except for the morph method, stem
and affix based approaches perform better after
supervised analysis. The results show that our
approach can obtain even better translation per-
formance if better morphological analyzers are
available. Supervised morphological analysis gen-
erates more meaningful morphemes, which lead to
better disambiguation of translation rules.
5 Conclusions and Future Work
In this paper we propose a novel framework for
agglutinative language translation by treating stem
and affix differently. We employ the stem se-
quence as the main part for training and decod-
ing. Besides, we associate each stem-granularity
translation rule with an affix distribution, which
could be used to make better translation decisions
by calculating the affix distribution similarity be-
367
tween the rule and the instance to be translated.
We conduct our model on three different language
pairs, all of which substantially improved the
translation performance. The procedure is totally
language-independent, and we expect that other
language pairs could benefit from our approach.
Acknowledgments
The authors were supported by 863 State
Key Project (No. 2011AA01A207), and
National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge
Innovation Program of Chinese Academy of
Sciences (No. KGZD-EW-501). Qun Liu?s work
is partially supported by Science Foundation
Ireland (Grant No.07/CE/I1142) as part of the
CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their
insightful comments and those who helped to
modify the paper.
References
Arianna Bisazza and Marcello Federico. 2009. Mor-
phological pre-processing for Turkish to English
statistical machine translation. In Proceedings of
IWSLT, pages 129?135.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: pa-
rameter estimation. Comput. Linguist., 19(2):263?
311.
Marine Carpuat and Dekai Wu. 2007. Improving
statistical machine translation using word sense
disambiguation. In Proceedings of EMNLP-CoNLL,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves
statistical machine translation. In Proceedings of
ACL, pages 33?40.
David Chiang. 2005. A hierarchical phrase-
based model for statistical machine translation. In
Proceedings of ACL, pages 263?270.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of AKRR, pages
106?113.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2010. A joint rule selection
model for hierarchical phrase-based translation. In
Proceedings of ACL, Short Papers, pages 6?11.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of COLING/ACL, pages 961?968.
Sharon Goldwater and David McClosky. 2005.
Improving statistical MT through morphological
analysis. In Proceedings of HLT-EMNLP, pages
676?683.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation.
In Proceedings of NAACL, Short Papers, pages 49?
52.
Eui-Hong Sam Han and George Karypis. 2000.
Centroid-based document classification: analysis
experimental results. In Proceedings of PKDD,
pages 424?431.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008.
Improving statistical machine translation using
lexicalized rule selection. In Proceedings of
COLING, pages 321?328.
Philipp Koehn and Hieu Hoang. 2007. Factored
translation models. In Proceedings of EMNLP-
CoNLL, pages 868?876.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of NAACL, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388?395.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In Proceedings of
HLT-NAACL, Short Papers, pages 57?60.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of COLING-ACL, pages
609?616.
Minh-Thang Luong and Min-Yen Kan. 2010.
Enhancing morphological alignment for translating
highly inflected languages. In Proceedings of
COLING, pages 743?751.
Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.
2010. A hybrid morpheme-word representation
for machine translation of morphologically rich
languages. In Proceedings of EMNLP, pages 148?
157.
Aili Mairehaba, Wenbin Jiang, Zhiyang Wang, Yibu-
layin Tuergen, and Qun Liu. 2012. Directed graph
model of Uyghur morphological analysis. Journal
of Software, 23(12):3115?3129.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish morphological segmentation for
statistical machine translation. In Workshop of MT
and Morphologically-rich Languages.
368
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The
alignment template approach to statistical machine
translation. Comput. Linguist., pages 417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: syntactically
informed phrasal SMT. In Proceedings of ACL,
pages 271?279.
Gerard Salton and Chris Buckley. 1987. Term
weighting approaches in automatic text retrieval.
Technical report.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Proceedings of
ICSLP, pages 311?318.
Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs
induced in an unsupervised manner. In Proceedings
of MT SUMMIT, pages 491?498.
Zhiyang Wang, Yajuan Lu?, and Qun Liu. 2011.
Multi-granularity word alignment and decoding for
agglutinative language translation. In Proceedings
of MT SUMMIT, pages 360?367.
Mei Yang and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly
inflected languages. In Proceedings of EACL, pages
1017?1020.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-
to-morphology mapping in factored phrase-based
statistical machine translation from English to
Turkish. In Proceedings of ACL, pages 454?464.
369
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 382?386,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingual Lexical Cohesion Trigger Model for Document-Level
Machine Translation
Guosheng Ben? Deyi Xiong?? Zhiyang Teng? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{benguosheng,tengzhiyang,lvyajuan,liuqun}@ict.ac.cn
?School of Computer Science and Technology,Soochow University
{dyxiong}@suda.edu.cn
?Centre for Next Generation Localisation, Dublin City University
{qliu}@computing.dcu.ie
Abstract
In this paper, we propose a bilingual lexi-
cal cohesion trigger model to capture lex-
ical cohesion for document-level machine
translation. We integrate the model into
hierarchical phrase-based machine trans-
lation and achieve an absolute improve-
ment of 0.85 BLEU points on average over
the baseline on NIST Chinese-English test
sets.
1 Introduction
Current statistical machine translation (SMT) sys-
tems are mostly sentence-based. The major draw-
back of such a sentence-based translation fash-
ion is the neglect of inter-sentential dependencies.
As a linguistic means to establish inter-sentential
links, lexical cohesion ties sentences together in-
to a meaningfully interwoven structure through
words with the same or related meanings (Wong
and Kit, 2012).
This paper studies lexical cohesion devices and
incorporate them into document-level machine
translation. We propose a bilingual lexical cohe-
sion trigger model to capture lexical cohesion for
document-level SMT. We consider a lexical co-
hesion item in the source language and its corre-
sponding counterpart in the target language as a
trigger pair, in which we treat the source language
lexical cohesion item as the trigger and its target
language counterpart as the triggered item. Then
we use mutual information to measure the strength
of the dependency between the trigger and trig-
gered item.
We integrate this model into a hierarchical
phrase-based SMT system. Experiment results
?Corresponding author
show that it is able to achieve substantial improve-
ments over the baseline.
The remainder of this paper proceeds as fol-
lows: Section 2 introduces the related work and
highlights the differences between previous meth-
ods and our model. Section 3 elaborates the pro-
posed bilingual lexical cohesion trigger model, in-
cluding the details of identifying lexical cohesion
devices, measuring dependency strength of bilin-
gual lexical cohesion triggers and integrating the
model into SMT. Section 4 presents experiments
to validate the effectiveness of our model. Finally,
Section 5 concludes with future work.
2 Related Work
As a linguistic means to establish inter-sentential
links, cohesion has been explored in the literature
of both linguistics and computational linguistics.
Cohesion is defined as relations of meaning that
exist within the text and divided into grammatical
cohesion that refers to the syntactic links between
text items and lexical cohesion that is achieved
through word choices in a text by Halliday and
Hasan (1976). In order to improve the quality of
machine translation output, cohesion has served as
a high level quality criterion in post-editing (Vas-
concellos, 1989). As a part of COMTIS project,
grammatical cohesion is integrated into machine
translation models to capture inter-sentential links
(Cartoni et al, 2011). Wong and Kit (2012) in-
corporate lexical cohesion to machine translation
evaluation metrics to evaluate document-level ma-
chine translation quality. Xiong et al (2013) inte-
grate various target-side lexical cohesion devices
into document-level machine translation. Lexical
cohesion is also partially explored in the cache-
based translation models of Gong et al (2011) and
translation consistency constraints of Xiao et al
382
(2011).
All previous methods on lexical cohesion for
document-level machine translation as mentioned
above have one thing in common, which is that
they do not use any source language information.
Our work is mostly related to the mutual infor-
mation trigger based lexical cohesion model pro-
posed by Xiong et al (2013). However, we sig-
nificantly extend their model to a bilingual lexical
cohesion trigger model that captures both source
and target-side lexical cohesion items to improve
target word selection in document-level machine
translation.
3 Bilingual Lexical Cohesion Trigger
Model
3.1 Identification of Lexical Cohesion Devices
Lexical cohesion can be divided into reiteration
and collocation (Wong and Kit, 2012). Reitera-
tion is a form of lexical cohesion which involves
the repetition of a lexical item. Collocation is a
pair of lexical items that have semantic relation-
s, such as synonym, near-synonym, superordinate,
subordinate, antonym, meronym and so on. In
the collocation, we focus on the synonym/near-
synonym and super-subordinate semantic relation-
s 1. We define lexical cohesion devices as content
words that have lexical cohesion relations, namely
the reiteration, synonym/near-synonym and super-
subordinate.
Reiteration is common in texts. Take the fol-
lowing two sentences extracted from a document
for example (Halliday and Hasan, 1976).
1. There is a boy climbing the old elm.
2. That elm is not very safe.
We see that word elm in the first sentence is re-
peated in the second sentence. Such reiteration de-
vices are easy to identify in texts. Synonym/near-
synonym is a semantic relationship set. We can
use WordNet (Fellbaum, 1998) to identify them.
WordNet is a lexical resource that clusters words
with the same sense into a semantic group called
synset. Synsets in WordNet are organized ac-
cording to their semantic relations. Let s(w) de-
note a function that defines all synonym words of
w grouped in the same synset in WordNet. We
can use the function to compute all synonyms and
near-synonyms for word w. In order to represen-
t conveniently, s0 denotes the set of synonyms in
1Other collocations are not used frequently, such as
antonyms. So we we do not consider them in our study.
s(w). Near-synonym set s1 is defined as the union
of all synsets that are defined by the function s(w)
where w? s0. It can be formulated as follows.
s1 =
?
w?s0
s(w) (1)
s2 =
?
w?s1
s(w) (2)
s3 =
?
w?s2
s(w) (3)
Similarly sm can be defined recursively as follows.
sm =
?
w?sm?1
s(w) (4)
Obviously, We can find synonyms and near-
synonyms for word w according to formula (4).
Superordinate and subordinate are formed by
words with an is-a semantic relation in WordNet.
As the super-subordinate relation is also encoded
in WordNet, we can define a function that is simi-
lar to s(w) identify hypernyms and hyponyms.
We use rep, syn and hyp to represent the lex-
ical cohesion device reiteration, synonym/near-
synonym and super-subordinate respectively here-
after for convenience.
3.2 Bilingual Lexical Cohesion Trigger
Model
In a bilingual text, lexical cohesion is present in
the source and target language in a synchronous
fashion. We use a trigger model capture such a
bilingual lexical cohesion relation. We define xRy
(R?{rep, syn, hyp}) as a trigger pair where x is
the trigger in the source language and y the trig-
gered item in the target language. In order to cap-
ture these synchronous relations between lexical
cohesion items in the source language and their
counterparts in the target language, we use word
alignments. First, we identify a monolingual lexi-
cal cohesion relation in the target language in the
form of tRy where t is the trigger, y the triggered
item that occurs in a sentence succeeding the sen-
tence of t, and R?{rep, syn, hyp}. Second, we
find word x in the source language that is aligned
to t in the target language. We may find multiple
words xk1 in the source language that are aligned
to t. We use all of them xiRt(1?i?k) to define
bilingual lexical cohesion relations. In this way,
we can create bilingual lexical cohesion relations
xRy (R?{rep, syn, hyp}): x being the trigger and
y the triggered item.
383
The possibility that y will occur given x is equal
to the chance that x triggers y. Therefore we mea-
sure the strength of dependency between the trig-
ger and triggered item according to pointwise mu-
tual information (PMI) (Church and Hanks, 1990;
Xiong et al, 2011).
The PMI for the trigger pair xRy where x is the
trigger, y the triggered item that occurs in a target
sentence succeeding the target sentence that aligns
to the source sentence of x, and R?{rep, syn, hyp}
is calculated as follows.
PMI(xRy) = log( p(x, y,R)p(x,R)p(y,R) ) (5)
The joint probability p(x, y,R) is:
p(x, y,R) = C(x, y,R)?
x,y C(x, y,R)
(6)
where C(x, y,R) is the number of aligned bilin-
gual documents where both x and y occur
with the relation R in different sentences, and?
x,y C(x, y,R) is the number of bilingual docu-
ments where this relation R occurs. The marginal
probabilities of p(x,R) and p(y,R) can be calcu-
lated as follows.
p(x,R) =
?
y
C(x, y,R) (7)
p(y,R) =
?
x
C(x, y,R) (8)
Given a target sentence ym1 , our bilingual lexical
cohesion trigger model is defined as follows.
MIR(ym1 ) =
?
yi
exp(PMI(?Ryi)) (9)
where yi are content words in the sentence ym1 and
PMI(?Ryi)is the maximum PMI value among all
trigger words xq1 from source sentences that have
been recently translated, where trigger words xq1
have an R relation with word yi.
PMI(?Ryi) = max1?j?qPMI(xjRyi) (10)
Three models MIrep(ym1 ), MIsyn(ym1 ),
MIhyp(ym1 ) for the reiteration device, the
synonym/near-synonym device and the super-
subordinate device can be formulated as above.
They are integrated into the log-linear model of
SMT as three different features.
3.3 Decoding
We incorporate our bilingual lexical cohesion trig-
ger model into a hierarchical phrase-based system
(Chiang, 2007). We add three features as follows.
? MIrep(ym1 )
? MIsyn(ym1 )
? MIhyp(ym1 )
In order to quickly calculate the score of each fea-
ture, we calculate PMI for each trigger pair be-
fore decoding. We translate document one by one.
During translation, we maintain a cache to store
source language sentences of recently translated
target sentences and three sets Srep, Ssyn, Shyp
to store source language words that have the re-
lation of {rep, syn, hyp} with content words gen-
erated in target language. During decoding, we
update scores according to formula (9). When one
sentence is translated, we store the corresponding
source sentence into the cache. When the whole
document is translated, we clear the cache for the
next document.
4 Experiments
4.1 Setup
Our experiments were conducted on the NIST
Chinese-English translation tasks with large-scale
training data. The bilingual training data contain-
s 3.8M sentence pairs with 96.9M Chinese word-
s and 109.5M English words from LDC2. The
monolingual data for training data English lan-
guage model includes the Xinhua portion of the
Gigaword corpus. The development set is the
NIST MT Evaluation test set of 2005 (MT05),
which contains 100 documents. We used the sets
of MT06 and MT08 as test sets. The numbers of
documents in MT06, MT08 are 79 and 109 respec-
tively. For the bilingual lexical cohesion trigger
model, we collected data with document bound-
aries explicitly provided. The corpora are select-
ed from our bilingual training data and the whole
Hong Kong parallel text corpus3, which contains
103,236 documents with 2.80M sentences.
2The corpora include LDC2002E18, LDC2003E07, LD-
C2003E14,LDC2004E12,LDC2004T07,LDC2004T08(Only
Hong Kong News), LDC2005T06 and LDC2005T10.
3They are LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2004T08 (Hong Kong Hansard-
s/Laws/News).
384
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) in both direction-
s and applying ?grow-diag-final-and? refinemen-
t (Koehn et al, 2003). We apply SRI Language
Modeling Toolkit (Stolcke, 2002) to train a 4-
gram language model with Kneser-Ney smooth-
ing. Case-insensitive NIST BLEU (Papineni et
al., 2002) was used to measure translation per-
formance. We used minimum error rate training
MERT (Och, 2003) for tuning the feature weights.
4.2 Distribution of Lexical Cohesion Devices
in the Target Language
Cohesion Device Percentage(%)
rep 30.85
syn 17.58
hyp 18.04
Table 1: Distributions of lexical cohesion devices
in the target language.
In this section we want to study how these
lexical cohesion devices distribute in the train-
ing data before conducting our experiments on
the bilingual lexical cohesion model. Here
we study the distribution of lexical cohesion in
the target language (English). Table 1 shows
the distribution of percentages that are counted
based on the content words in the training da-
ta. From Table 1, we can see that the reitera-
tion cohesion device is nearly a third of all con-
tent words (30.85%), synonym/near-synonym and
super-subordinate devices account for 17.58% and
18.04%. Obviously, lexical cohesion devices are
frequently used in real-world texts. Therefore cap-
turing lexical cohesion devices is very useful for
document-level machine translation.
4.3 Results
System MT06 MT08 Avg
Base 30.43 23.32 26.88
rep 31.24 23.70 27.47
syn 30.92 23.71 27.32
hyp 30.97 23.48 27.23
rep+syn+hyp 31.47 23.98 27.73
Table 2: BLEU scores with various lexical co-
hesion devices on the test sets MT06 and MT08.
?Base? is the traditonal hierarchical system, ?Avg?
is the average BLEU score on the two test sets.
Results are shown in Table 2. From the table,
we can see that integrating a single lexical cohe-
sion device into SMT, the model gains an improve-
ment of up to 0.81 BLEU points on the MT06 test
set. Combining all three features rep+syn+hyp to-
gether, the model gains an improvement of up to
1.04 BLEU points on MT06 test set, and an av-
erage improvement of 0.85 BLEU points on the
two test sets of MT06 and MT08. These stable
improvements strongly suggest that our bilingual
lexical cohesion trigger model is able to substan-
tially improve the translation quality.
5 Conclusions
In this paper we have presented a bilingual lex-
ical cohesion trigger model to incorporate three
classes of lexical cohesion devices, namely the
reiteration, synonym/near-synonym and super-
subordinate devices into a hierarchical phrase-
based system. Our experimental results show
that our model achieves a substantial improvement
over the baseline. This displays the advantage of
exploiting bilingual lexical cohesion.
Grammatical and lexical cohesion have often
been studied together in discourse analysis. In
the future, we plan to extend our model to cap-
ture both grammatical and lexical cohesion in
document-level machine translation.
Acknowledgments
This work was supported by 863 State Key Project
(No.2011AA01A207) and National Key Technol-
ogy R&D Program(No.2012BAH39B03). Qun
Liu was also partially supported by Science Foun-
dation Ireland (Grant No.07/CE/I1142) as part of
the CNGL at Dublin City University. We would
like to thank the anonymous reviewers for their in-
sightful comments.
References
Bruno Cartoni, Andrea Gesmundo, James Hender-
son, Cristina Grisot, Paola Merlo, Thomas Mey-
er, Jacques Moeschler, Sandrine Zufferey, Andrei
Popescu-Belis, et al 2011. Improving mt coher-
ence through text-level processing of input texts:
the comtis project. http://webcast. in2p3. fr/videos-
the comtis project.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201?228.
Kenneth Ward Church and Patrick Hanks. 1990. Word
385
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22?29.
Christine Fellbaum. 1998. Wordnet: An electronic
lexical database.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
M.A.K Halliday and Ruqayia Hasan. 1976. Cohesion
in english. English language series, 9.
Philipp Koehn, Franz Josef Och, and Daniel Mar-
cu. 2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment mod-
els. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, S-
apporo, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic e-
valuation of machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the internation-
al conference on spoken language processing, vol-
ume 2, pages 901?904.
Muriel Vasconcellos. 1989. Cohesion and coherence
in the presentation of machine translation products.
Georgetown University Round Table on Languages
and Linguistics, pages 89?105.
Billy T. M. Wong and Chunyu Kit. 2012. Extend-
ing machine translation evaluation metrics with lex-
ical cohesion to document level. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1060?1068, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131?138.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual in-
formation triggers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1288?1297, Portland, Oregon, USA, June. Associa-
tion for Computational Linguistics.
Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,
and Qun Liu. 2013. Modeling lexical cohesion for
document-level machine translation. In Proceedings
of the Twenty-Third international joint conference
on Artificial Intelligence, Beijing,China.
386
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 591?596,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Iterative Transformation of Annotation Guidelines for
Constituency Parsing
Xiang Li 1, 2 Wenbin Jiang 1 Yajuan Lu? 1 Qun Liu 1, 3
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
{lixiang, jiangwenbin, lvyajuan}@ict.ac.cn
2University of Chinese Academy of Sciences
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
This paper presents an effective algorith-
m of annotation adaptation for constituen-
cy treebanks, which transforms a treebank
from one annotation guideline to anoth-
er with an iterative optimization proce-
dure, thus to build a much larger treebank
to train an enhanced parser without in-
creasing model complexity. Experiments
show that the transformed Tsinghua Chi-
nese Treebank as additional training da-
ta brings significant improvement over the
baseline trained on Penn Chinese Tree-
bank only.
1 Introduction
Annotated data have become an indispensable
resource for many natural language processing
(NLP) applications. On one hand, the amount of
existing labeled data is not sufficient; on the other
hand, however there exists multiple annotated da-
ta with incompatible annotation guidelines for the
same NLP task. For example, the People?s Daily
corpus (Yu et al, 2001) and Chinese Penn Tree-
bank (CTB) (Xue et al, 2005) are publicly avail-
able for Chinese segmentation.
An available treebank is a major resource for
syntactic parsing. However, it is often a key bottle-
neck to acquire credible treebanks. Various tree-
banks have been constructed based on differen-
t annotation guidelines. In addition to the most
popular CTB, Tsinghua Chinese Treebank (TC-
T) (Zhou, 2004) is another real large-scale tree-
bank for Chinese constituent parsing. Figure 1 il-
lustrates some differences between CTB and TCT
in grammar category and syntactic structure. Un-
fortunately, these heterogeneous treebanks can not
be directly merged together for training a parsing
model. Such divergences cause a great waste of
human effort. Therefore, it is highly desirable to
transform a treebank into another compatible with
another annotation guideline.
In this paper, we focus on harmonizing het-
erogeneous treebanks to improve parsing perfor-
mance. We first propose an effective approach to
automatic treebank transformation from one an-
notation guideline to another. For convenience
of reference, a treebank with our desired anno-
tation guideline is named as target treebank, and
a treebank with a differtn annotation guideline is
named as source treebank. Our approach proceeds
in three steps. A parser is firstly trained on source
treebank. It is used to relabel the raw sentences
of target treebank, to acquire parallel training da-
ta with two heterogeneous annotation guidelines.
Then, an annotation transformer is trained on the
parallel training data to model the annotation in-
consistencies. In the last step, a parser trained on
target treebank is used to generate k-best parse
trees with target annotation for source sentences.
Then the optimal parse trees are selected by the an-
notation transformer. In this way, the source tree-
bank is transformed to another with our desired
annotation guideline. Then we propose an op-
timization strategy of iterative training to further
improve the transformation performance. At each
iteration, the annotation transformation of source-
to-target and target-to-source are both performed.
The transformed treebank is used to provide better
annotation guideline for the parallel training da-
ta of next iteration. As a result, the better paral-
lel training data will bring an improved annotation
transformer at next iteration.
We perform treebank transformation from TC-
591
zjXXXXXEE
djHHH
np
ZZFirst Joint Conference on Lexical and Computational Semantics (*SEM), pages 514?518,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ICT:A System Combination for Chinese Semantic Dependency Parsing
Hao Xiong and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao, liuqun}@ict.ac.cn
Abstract
The goal of semantic dependency parsing is to
build dependency structure and label seman-
tic relation between a head and its modifier.
To attain this goal, we concentrate on obtain-
ing better dependency structure to predict bet-
ter semantic relations, and propose a method
to combine the results of three state-of-the-art
dependency parsers. Unfortunately, we made
a mistake when we generate the final output
that results in a lower score of 56.31% in term
of Labeled Attachment Score (LAS), reported
by organizers. After giving golden testing set,
we fix the bug and rerun the evaluation script,
this time we obtain the score of 62.8% which
is consistent with the results on developing set.
We will report detailed experimental results
with correct program as a comparison stan-
dard for further research.
1 Introduction
In this year?s Semantic Evaluation Task, the organiz-
ers hold a task for Chinese Semantic Dependency
Parsing. The semantic dependency parsing (SDP)
is a kind of dependency parsing. It builds a depen-
dency structure for a sentence and labels the seman-
tic relation between a head and its modifier. The
semantic relations are different from syntactic rela-
tions. They are position independent, e.g., the pa-
tient can be before or behind a predicate. On the
other hand, their grains are finer than syntactic re-
lations, e.g., the syntactic subject can be agent or
experiencer. Readers can refer to (Wanxiang Che,
2012) for detailed introduction.
Figure 1: The pipeline of our system, where we com-
bine the results of three dependency parsers and use max-
entropy classifier to predict the semantic relations.
Different from most methods proposed in
CoNLL-2008 1 and 2009 2, in which some re-
searchers build a joint model to simultaneously gen-
erate dependency structure and its syntactic relations
(Surdeanu et al, 2008; Hajic? et al, 2009), here,
we first employ several parsers to generate depen-
dency structure and then propose a method to com-
bine their outputs. After that, we label relation be-
tween each head and its modifier via the traversal
of this refined parse tree. The reason why we use
a pipeline model while not a joint model is that
the number of semantic relations annotated by or-
ganizers is more than 120 types, while in the for-
mer task is only 21 types. Compared to the former
task, the large number of types will obviously drop
the performance of classifier. On the other hand, the
performance of syntactic dependency parsing is ap-
proaching to perfect, intuitively, that better depen-
dency structure does help to semantic parsing, thus
we can concentrate on improving the accuracy of de-
pendency structure construction.
The overall framework of our system is illustrated
1http://www.yr-bcn.es/conll2008/
2http://ufal.mff.cuni.cz/conll2009-st/
514
in figure 1, where three dependency parsers are em-
ployed to generate the dependency structure, and a
maximum entropy classifier is used to predict rela-
tion for head and its modifier over combined parse
tree. Final experimental results show that our sys-
tem achieves 80.45% in term of unlabeled attach-
ment score (UAS), and 62.8 % in term of LAS. Both
of them are higher than the baseline without using
system combinational techniques.
In the following of this paper, we will demonstrate
the detailed information of our system, and report
several experimental results.
2 System Description
As mentioned, we employ three single dependency
parsers to generate respect dependency structure. To
further improve the accuracy of dependency struc-
ture construction, we blend the syntactic outputs and
find a better dependency structure. In the followings,
we will first introduce the details of our strategy for
dependency structure construction.
2.1 Parsers
We implement three transition-based dependency
parsers with three different parsing algorithms:
Nivre?s arc standard, Nivre?s arc eager (see Nivre
(2004) for a comparison between the two Nivre al-
gorithms), and Liang?s dynamic algorithm(Huang
and Sagae, 2010). We use these algorithms for
several reasons: first, they are easy to implement
and their reported performance are approaching to
state-of-the-art. Second, their outputs are projective,
which is consistent with given corpus.
2.2 Parser Combination
We use the similar method presented in Hall et al
(2011) to advance the accuracy of parses. The parses
of each sentence are combined into a weighted di-
rected graph. The left procedure is similar to tradi-
tional graph-based dependency parsing except that
the number of edges in our system is smaller since
we reserve best edges predicted by three single
parsers. We use the popular Chu-Liu-Edmonds al-
gorithm (Chu and Liu, 1965; Edmonds et al, 1968)
to find the maximum spanning tree (MST) of the
new constructed graph, which is considered as the
final parse of the sentence. Specifically, we use the
parsing accuracy on developing set to represent the
weight of graph edge. Formally, the weight of graph
edge is computed as follows,
we =
?
p?P
Accuracy(p) ? I(e, p) (1)
where the Accuracy(p) is the parsing score of
parse tree p whose value is the score of parsing accu-
racy on developing set, and I(e, p) is an indicator, if
there is such dependency in parse tree p, it returns 1,
otherwise returns 0. Since the value of Accuracy(p)
ranges from 0 to 1, we doesn?t need to normalize its
value.
Thus, the detailed procedure for dependency
structure construction is,
? Parsing each sentence using Nivre?s arc stan-
dard, Nivre?s arc eager and Liang?s dynamic al-
gorithm, respectively.
? Combining parses outputted by three parsers
into weighted directed graph, and representing
its weight using equation 1.
? Using Chu-Liu-Edmonds algorithm to search
final parse for each sentence.
2.3 Features for Labeling
After given dependency structure, for each relation
between head and its modifier, we extract 31 types
of features, which are typically exploited in syntac-
tic dependency parsing, as our basic features. Based
on these basic features, we also add a additional dis-
tance metric for each features and obtain 31 types of
distance incorporated features. Besides that, we use
greedy hill climbing approach to select additional 29
features to obtain better performance. Table 1 shows
the basic features used in our system,
And the table 2 gives the additional features. It
is worth mentioning, that the distance is calculated
as the difference between the head and its modifier,
which is different from the calculation reported by
most literatures.
2.4 Classifier
We use the classifier from Le Zhang?s Maximum
Entropy Modeling Toolkit3 and use the L-BFGS
3http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit
.html
515
Features
Basic
mw:modifier?s word
mp:modifier?s POS tag
hw:head?s word
hp:head?s POS tag
Combination
hw|hp,mw|mp,hw|mw
hp|mp,hw|mp,hp|mw
hw|hp|mw
hw|hp|mp
hw|mw|mp
hp|mw|mp
hp|mp|mp-1
hp|mp|mp+1
hp|hp-1|mp
hp|hp+1|mp
hp|hp-1|mp-1
hp|hp-1|mp+1
hp|hp+1|mp-1
hp|hp+1|mp+1
hp-1|mp|mp-1
hp-1|mp|mp+1
hp+1|mp|mp-1
hp+1|mp|mp+1
hw|hp|mw|mp
hp|hp-1|mp|mp-1
hp|hp+1|mp|mp+1
hp|hp+1|mp|mp-1
hp|hp-1|mp|mp+1
Table 1: The basic features used in our system. -1 and
+1 indicate the one on the left and right of given word.
parameter estimation algorithm with gaussian prior
smoothing(Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 1000 iter-
ations according to the previous experience.
3 Experiments
The given corpus consists of 8301 sentences
for training(TR), and 569 sentences for develop-
ing(DE). For tuning parameters, we just use TR por-
tion, while for testing, we combine two parts and
retrain the parser to obtain better results. Surely, we
also give results of testing set trained on TR portion
for comparison. In the following of this section, we
will report the detailed experimental results both on
Features
Distance dist:basic features with distance
Additional
lmw:leftmost word of modifier
rnw:rightnearest word of modifier
gfw:grandfather of modifier
lmp,rnp,gfp
lmw|lmp,rnw|rnp,lmw|rnw
lmp|rnp,lmw|mw,lmp|mp
rnw|mw,rnp|mp,gfw|mw
gfp|mp,gfw|hw,gfp|hp
gfw|mw|gfp|mp
lmw|lmp|mw|mp
rnw|rnp|mw|mp
lmw|rnw|mw,lmp|rnp|mp
gfw|hw|gfp|hp
gfw|mw|hw,gfp|mp|hp
gfw|mw|hw|gfp|mp|hp
lmw|rnw|lmp|rnp|mw|mp
lmw|rnw|lmp|rnp
Table 2: The additional features used in our system.
developing and testing set.
3.1 Results on Developing Set
We first report the accuracy of dependency construc-
tion on developing set using different parsing al-
gorithms in table 3. Note that, the features used
in our system are similar to that used in their pub-
lished papers(Nivre, 2003; Nivre, 2004; Huang and
Sagae, 2010). From table 3 we find that although
Precision (%)
Nivre?s arc standard 78.86
Nivre?s arc eager 79.11
Liang?s dynamic 79.78
System Combination 80.85
Table 3: Syntactic precision of different parsers on devel-
oping set.
using simple method for combination over three sin-
gle parsers, the system combination technique still
achieves 1.1 points improvement over the highest
single system. Since the Liang?s algorithm is a dy-
namic algorithm, which enlarges the searching space
in decoding, while the former two Nivre?s arc al-
516
gorithms actually still are simple beam search al-
gorithm, thus the Liang?s algorithm achieves better
performance than Nivre?s two algorithm, which is
consistent with the experiments in Liang?s paper.
To acknowledge that the better dependency struc-
ture does help to semantic relation labeling, we fur-
ther predict semantic relations on different depen-
dency structures. For comparison, we also report the
performance on golden structure. Since our combi-
Precision (%)
Nivre?s arc standard 60.84
Nivre?s arc eager 60.76
Liang?s dynamic 61.43
System Combination 62.92
Golden Tree 76.63
Table 4: LAS of semantic relations over different parses
on developing set.
national algorithm requires weight for each edges,
we use the developing parsing accuracy 0.7886,
0.7911, and 0.7978 as corresponding weights for
each single system. Table 4 shows, that the pre-
diction of semantic relation could benefit from the
improvement of dependency structure. We also no-
tice that even given the golden parse tree, the per-
formance of relation labeling is still far from per-
fect. Two reasons could be explained for that: first
is the small size of supplied corpus, second is that
the relation between head and its modifier is too
fine-grained to distinguish for a classifier. More-
over, here we use golden segmentation for parsing,
imagining that an automatic segmenter would fur-
ther drop the accuracy both on syntactic and seman-
tic parsing.
3.2 Results on Testing Set
Since there is a bug4 in our final results submitted
to organizers, here, in order to confirm the improve-
ment of our method and supply comparison standard
for further research, we reevaluate the correct output
and report its performance on different training set.
Table 5 and table 6 give the results trained on dif-
ferent corpus. We can see that when increasing the
4The bug is come from that when we converting the CoNLL-
styled outputs generated by our combination system into plain
text. While in developing stage, we directly used CoNLL-styled
outputs as our input, thus we didn?t realize this mistake.
training size, the performance is slightly improved.
Also, we find the results on testing set is consistent
with that on developing set, where best dependency
structure achieves the best performance.
LAS (%) UAS(%)
Nivre?s arc standard 60.38 78.19
Nivre?s arc eager 60.78 78.62
Liang?s dynamic 60.85 79.09
System Combination 62.76 80.23
Submitted Error Results 55.26 71.85
Table 5: LAS and UAS on testing set trained on TR.
LAS (%) UAS(%)
Nivre?s arc standard 60.49 78.25
Nivre?s arc eager 60.99 78.78
Liang?s dynamic 61.29 79.59
System Combination 62.80 80.45
Submitted Error Results 56.31 73.20
Table 6: LAS and UAS on testing set trained on TR and
DE.
4 Conclusion
In this paper, we demonstrate our system framework
for Chinese Semantic Dependency Parsing, and re-
port the experiments with different configurations.
We propose to use system combination to better the
dependency structure construction, and then label
semantic relations over refined parse tree. Final ex-
periments show that better syntactic parsing do help
to improve the accuracy of semantic relation predic-
tion.
Acknowledgments
The authors were supported by National Science
Foundation of China, Contracts 90920004, and
High-Technology R&D Program (863) Project No
2011AA01A207 and 2012BAH39B03. We thank
Heng Yu for generating parse tree using Liang?s al-
gorithm. We thank organizers for their generous
supplied resources and arduous preparation. We also
thank anonymous reviewers for their thoughtful sug-
gestions.
517
References
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14(1396-
1400):270.
J. Edmonds, J. Edmonds, and J. Edmonds. 1968. Opti-
mum branchings. National Bureau of standards.
J. Hajic?, M. Ciaramita, R. Johansson, D. Kawahara, M.A.
Mart??, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,
J. ?Ste?pa?nek, et al 2009. The conll-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 1?18. Association for Computa-
tional Linguistics.
J. Hall, J. Nilsson, and J. Nivre. 2011. Single malt or
blended? a study in multilingual parser optimization.
Trends in Parsing Technology, pages 19?33.
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1077?1086. Association
for Computational Linguistics.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT. Cite-
seer.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Proceedings of the Workshop on In-
cremental Parsing: Bringing Engineering and Cogni-
tion Together, pages 50?57. Association for Computa-
tional Linguistics.
M. Surdeanu, R. Johansson, A. Meyers, L. Ma`rquez, and
J. Nivre. 2008. The conll-2008 shared task on joint
parsing of syntactic and semantic dependencies. In
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177.
Association for Computational Linguistics.
Ting Liu Wanxiang Che. 2012. Semeval-2012 Task 5:
Chinese Semantic Dependency Parsing. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012).
518
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 715?720,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
ICT: A Translation based Method for Cross-lingual Textual Entailment 
 
 
Fandong Meng, Hao Xiong and Qun Liu 
Key Lab. of Intelligent Information Processing 
Institute of Computing Technology 
Chinese Academy of Sciences 
P.O. Box 2704, Beijing 100190, China 
{mengfandong,xionghao,liuqun}@ict.ac.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present our system descrip-
tion in task of Cross-lingual Textual Entail-
ment. The goal of this task is to detect 
entailment relations between two sentences 
written in different languages. To accomplish 
this goal, we first translate sentences written 
in foreign languages into English. Then, we 
use EDITS1, an open source package, to rec-
ognize entailment relations. Since EDITS only 
draws monodirectional relations while the task 
requires bidirectional prediction, thus we ex-
change the hypothesis and test to detect en-
tailment in another direction. Experimental 
results show that our method achieves promis-
ing results but not perfect results compared to 
other participants. 
1 Introduction 
In Cross-Lingual Textual Entailment task (CLTE) 
of 2012, the organizers hold a task for Cross-
Lingual Textual Entailment. The Cross-Lingual 
Textual Entailment task addresses textual entail-
ment (TE) recognition under a new dimension 
(cross-linguality), and within a new challenging 
application scenario (content synchronization) 
Readers can refer to M. Negri et al 2012.s., for 
more detailed introduction. 1 
Textual entailment, on the other hand, recog-
nize, generate, or extract pairs of natural language 
expressions, and infer that if one element is true, 
whether the other element is also true. Several 
methods are proposed by previous researchers. 
There have been some workshops on textual en-
tailment in recent years. The recognizing textual 
entailment challenges (Bar-Haim et al 2006; 
Giampiccolo, Magnini, Dagan, & Dolan, 2007; 
Giampiccolo, Dang, Magnini, Dagan, & Dolan, 
2008), currently in the 7th year, provide additional 
significant thrust. Consequently, there are a large 
number of published articles, proposed methods, 
and resources related to textual entailment. A spe-
cial issue on textual entailment was also recently 
published, and its editorial provides a brief over-
view of textual entailment methods (Dagan, Dolan, 
Magnini, & Roth, 2009).  
Textual entailment recognizers judge whether 
or not two given language expressions constitute a 
correct textual entailment pair. Different methods 
may operate at different levels of representation of 
the input expressions. For example, they may treat 
the input expressions simply as surface strings, 
they may operate on syntactic or semantic repre-
sentations of the input expressions, or on represen-
tations combining information from different 
                                                          
1http://edits.fbk.eu/ 
715
levels. Logic-based approach is to map the lan-
guage expressions to logical meaning representa-
tions, and then rely on logical entailment checks, 
possibly by invoking theorem provers (Rinaldi et 
al., 2003; Bos & Markert, 2005; Tatu & Moldovan, 
2005, 2007). An alternative to use logical meaning 
representations is to start by mapping each word of 
the input language expressions to a vector that 
shows how strongly the word co-occurs with par-
ticular other words in corpora (Lin, 1998b), possi-
bly also taking into account syntactic information, 
for example requiring that the co-occurring words 
participate in particular syntactic dependencies 
(Pad?o & Lapata, 2007). Several textual entailment 
recognizing methods operate directly on the input 
surface strings. For example, they compute the 
string edit distance (Levenshtein, 1966) of the two 
input strings, the number of their common words, 
or combinations of several string similarity 
measures (Malakasiotis & Androutsopoulos, 2007). 
Dependency grammar parsers (Melcuk, 1987; Ku-
bler, McDonald, & Nivre, 2009) are popular in 
textual entailment research. However, cross-lingual 
textual entailment brings some problems on past 
algorithms. On the other hand, many methods can?t 
be applied to it directly.  
In this paper, we propose a translation based 
method for cross-lingual textual entailment, which 
has been described in Mehdad et al 2010. First, we 
translate one part of the text, which termed as ?t1? 
and written in one language, into English, which 
termed as ?t2?. Then, we use EDITS, an open 
source package, to recognize entailment relations 
between two parts. Large-scale experiments are 
conducted on four language pairs, French-English, 
Spanish-English, Italian-English and German-
English. Although our method achieves promising 
results reported by organizers, it is still far from 
perfect compared to other participants. 
The remainder of this paper is organized as 
follows. We describe our system framework in 
section 2. We report experimental results in section 
3 and draw our conclusions in the last section. 
2 System Description 
Figure 1 illustrates the overall framework of our 
system, where a machine translation model is em-
ployed to translate foreign language into English, 
since original EDITS could only deal with the text 
in the same language pairs.  
   In the following of this section, we will de-
scribe the translation module and configuration of 
EDITS in details. 
 
Figure 1:  The framework of our system. 
 
2.1 Machine Translation 
Recently, machine translation has attracted inten-
sive attention and has been well studied in natural 
language community. Effective models, such as 
Phrase-Based model (Koehn et al, 2003), Hierar-
chical Phrase-Based model (HPB) (Chiang, 2005), 
and Syntax-Based (Liu et al, 2006) model have 
been proposed to improve the translation quality. 
However, since current translation models require 
parallel corpus to extract translation rules, while 
parallel corpus on some language pairs such as 
Italian-English and Spanish-English are hard to 
obtain, therefore, we could use Google Translation 
Toolkit (GTT) to generate translation. 
Specifically, WMT 2 released some bilingual 
corpus for training, thus we use some portion to 
train a French-English translation engine using 
hierarchical phrase-based model. We also exploit 
system combination technique (A Rosti et al, 2007) 
to improve translation quality via blending the 
translation of our models and GTT?s. It is worth 
noting that GTT only gives 1-best translation, thus 
we duplicate 50 times to generate 50-best for sys-
tem combination.  
                                                          
2  http://www.statmt.org/wmt12/ 
716
2.2 Textual Entailment 
Many methods have been proposed to recognize 
textual entailment relations between two expres-
sions written in the same language. Since edit dis-
tance algorithms are effective on this task, we 
choose this method. And we use popular toolkit, 
EDITS, to accomplish the textual entailment task. 
EDITS is an open source software, which is 
used for recognizing entailment relations between 
two parts of text, termed as ?T? and ?H?. The sys-
tem is based on the edit distance algorithms, and 
computes the ?T?-?H? distance as the cost of the 
edit operations (i.e. insertion, deletion and substitu-
tion) that are necessary to transform ?T? into ?H?. 
EDITS requires that three modules are defined: an 
edit distance algorithm, a cost scheme for the three 
edit operations, and a set of rules expressing either 
entailment or contradiction. Each module can be 
easily configured by the user as well as the system 
parameters. EDITS can work at different levels of 
complexity, depending on the linguistic analysis 
carried on over ?T? and ?H?. Both linguistic pro-
cessors and semantic resources that are available to 
the user can be integrated within EDITS, resulting 
in a flexible, modular and extensible approach to 
textual entailment. 
 
 
Figure 2: An Example of two expressions 
EDITS can recognize.  
 
Figure 2 shows an example of two expressions 
that EDITS can recognize. EDITS will give an an-
swer that whether expression ?H? is true given that 
expression ?T? is true. The result is a Boolean val-
ue. If ?H? is true given ?T? is true, then the result 
is ?YES?, otherwise ?NO?. 
EDITS implements a distance-based frame-
work which assumes that the probability of an en-
tailment relation between a given ?T?-?H? pair is 
inversely proportional to the distance between ?T? 
and ?H? (i.e. the higher the distance, the lower is 
the probability of entailment). Within this frame-
work the system implements and harmonizes dif-
ferent approaches to distance computation, 
providing both edit distance algorithms, and simi-
larity algorithms. Each algorithm returns a normal-
ized distance score (a number between 0 and 1). At 
a training stage, distance scores calculated over 
annotated ?T?-?H? pairs are used to estimate a 
threshold that best separates positive from negative 
examples. The threshold, which is stored in a 
Model, is used at a test stage to assign an entail-
ment judgment and a confidence score to each test 
pair. 
 
 
Figure 3: Our configured file for training 
 
Figure 3 shows our configuration file for train-
ing models, we choose ?distance? algorithm in 
EDITS, and ?default_matcher?, and ?ignore_case? , 
and some other default but effective configured 
parameters. 
 
 
Figure 4: The overall training and decoding 
procedure in our system. 
 
Figure 4 shows our training and decoding 
procedure. As EDITS can only recognize textual 
entailment from one part to the other, we manually 
change the tag ?H? with ?T?, and generate the re-
sults again, and then compute two parts? entailment 
relations. For example, if ?T?-?H? is ?YES?, and 
?H?-?T? is ?NO?, then the entailment result be-
tween them is ?forward?; if ?T?-?H? is ?NO?, and 
?H?-?T? is ?YES?, then the entailment result be-
tween them is ?backward?; if both  ?T?-?H? and 
?H?-?T? are ?YES?, the result is ?bidirectional?; 
717
otherwise ?no_entailment?. 
3 Experiments and Results 
Since organizers of SemEval 2012 task 8 supply a 
piece of data for training, we thus exploit it to op-
timize parameters for EDITS. Table 1 shows the F-
measure score of training set analyzed by EDITS, 
where ?FE? represents French-English, ?SE? rep-
resents Spanish-English, ?IE? represents Italian-
English and ?GE? represents Italian-English.  
 
Judgment  FE SE IE GE 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.339 
0.611 
0.533 
0.515 
0.516 
0.373 
0.574 
0.535 
0.502 
0.506 
0.440 
0.493 
0.494 
0.506 
0.488 
0.327 
0.552 
0.494 
0.495 
0.482 
Table 1:  Results on training set. 
 
From Table 1, we can see that the perfor-
mance of ?forward? prediction is lower than others. 
One explanation is that the ?T? is translated from 
foreign language, which is error unavoidable. Thus 
some rules used for checking ?T?, such as stop-
word list will be disabled. Then it is possible to 
induce a ?NO? relation between ?T? and ?H? that 
results in lower recall of ?forward?. 
Since for French-English, we build a system 
combination for improving the quality of transla-
tion. Table 2 shows the results of BLEU score of 
translation quality, and F-score of entailment 
judgment.  
 
System  BLEU4 F-score 
HPB 
GTT 
COMB 
28.74 
30.08 
30.57 
0.496 
0.508 
0.516 
Table 2:  Performance of different translation 
model, where COMB represents system com-
bination. 
 
From table 2, we find that the translation qual-
ity slightly affect the correctness of entailment 
judgment. However, the difference of performance 
in entailment judgment is smaller than that in 
translation quality. We explain that the translation 
models exploit phrase-based rules to direct the 
translation, and the translation errors mainly come 
from the disorder between each phrases.  While a 
distance based entailment model generally consid-
ers the similarity of phrases between test and hy-
pothesis, thus the disorder of phrases influences the 
judgment slightly.   
Using the given training data for tuning pa-
rameters, table 3 to table 6 shows the detailed ex-
perimental results on testing data, where P 
represents precision and R indicates recall, and 
both of them are calculated by given evaluation 
script. 
  
French -- English 
Judgment P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.517 
0.385 
0.444 
0.192 
0.496 
0.656 
0.480 
0.306 
0.506 
0.485 
0.462 
0.456 
0.570 
 Table 3: Test results on French-English 
 
Spanish -- English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.750 
0.440 
0.395 
0.436 
0.240 
0.472 
0.560 
0.520 
0.364 
0.456 
0.464 
0.474 
0.448 
0.632 
Table 4: Test results on Spanish-English 
  
Italian ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.661 
0.554 
0.427 
0.383 
0.296 
0.368 
0.448 
0.704 
0.409 
0.442 
0.438 
0.496 
0.454 
0.566 
 Table 5: Test results on Italian-English 
 
German ? English 
Judgment  P R F-measure 
forward 
backward 
no_entailment 
bidirectional 
Overall 
Best System 
0.718 
0.493 
0.390 
0.439 
0.224 
0.552 
0.512 
0.552 
0.341 
0.521 
0.443 
0.489 
0.460 
0.558 
Table 6: Test results on German-English 
718
 
After given golden testing reference, we also 
investigate the effect of training set to testing set. 
We choose testing set from RTE1 and RTE2, both 
are English text, as our training set for optimiza-
tion of EDITS, and the overall results are shown in 
table 7 to table 10, where CLTE is training set giv-
en by this year?s organizers. 
 
French -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.306 
0.506 
0.485 
0.462 
0.456 
0.248 
0.425 
0.481 
0.472 
0.430 
0.289 
0.440 
0.485 
0.485 
0.444 
Table 7: Test results on French-English 
given different training set. 
 
Spanish ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.364 
0.456 
0.464 
0.474 
0.448 
0.293 
0.332 
0.386 
0.484 
0.400 
0.297 
0.372 
0.427 
0.503 
0.424 
Table 8: Test results on Spanish-English 
given different training set. 
 
Italian -- English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.409 
0.442 
0.438 
0.496 
0.454 
0.333 
0.394 
0.410 
0.474 
0.420 
0.335 
0.436 
0.421 
0.480 
0.432 
Table 9: Test results on Italian-English 
given different training set. 
 
German ? English 
Judgment  CLTE RTE1 RTE2 
forward 
backward 
no_entailment 
bidirectional 
Overall 
0.341 
0.521 
0.443 
0.489 
0.460 
0.377 
0.372 
0.437 
0.487 
0.434 
0.425 
0.460 
0.457 
0.508 
0.470 
Table 10: Test results on German-English 
given different training set. 
 
Results in table 7 and table 8 shows that mod-
els trained on ?CLTE? have better performance 
than those trained on RTE1 and RTE2, except ?bi-
directional? judgment type. In Table 9, all results 
decoding by models trained on ?CLTE? are the 
best. And in Table 10, only a few results decoding 
by models trained on ?RTE1? and ?RTE2? have 
higher score. The reason may be that, the test cor-
pora are bilingual, there are some errors in the ma-
chine translation procedure when translate one part 
of the test from its language into the other. When 
training on these bilingual text and decoding these 
bilingual text, these two procedure have error con-
sistency. Some errors may be counteracted. If we 
train on RTE, a standard monolingual text, and 
decode a bilingual text, more errors may exist be-
tween the two procedures. So we believe that, if 
we use translation based strategy (machine transla-
tion and monolingual textual entailment) to gener-
ate cross-lingual textual entailment, we should use 
translation based strategy to train models, rather 
than use standard monolingual texts. 
4 Conclusion 
In this paper, we demonstrate our system frame-
work for this year?s cross-lingual textual entail-
ment task. We propose a translation based model 
to address cross-lingual entailment. We first trans-
late all foreign languages into English, and then 
employ EDITS to induce entailment relations. Ex-
periments show that our method achieves promis-
ing results but not perfect results compared to other 
participants. 
Acknowledgments 
The authors were supported by National Science 
Foundation of China, Contracts 90920004, and 
High-Technology R&D Program (863) Project No 
2011AA01A207 and 2012BAH39B03.  We thank 
organizers for their generous supplied resources 
and arduous preparation. We also thank anony-
mous reviewers for their thoughtful suggestions. 
References  
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B., & Szpektor, I. 2006.The 2nd 
PASCAL recognising textual entailment challenge. In 
Proc. of the 2nd PASCAL ChallengesWorkshop on 
Recognising Textual Entailment, Venice, Italy. 
719
Bos, J., & Markert, K. 2005. Recognising textual en-
tailment with logical inference. In Proc. Of the Conf. 
on HLT and EMNLP, pp. 628?635, Vancouver, BC, 
Canada. 
Dagan, I., Dolan, B., Magnini, B., & Roth, D. 2009. 
Recognizing textual entailment: Rational,evaluation 
and approaches. Nat. Lang. Engineering, 15(4), i?
xvii. Editorial of the special issue on Textual Entail-
ment. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL 2005, pages 263?270. 
Giampiccolo, D., Dang, H., Magnini, B., Dagan, I., & 
Dolan, B. 2008. The fourth PASCAL recognizing tex-
tual entailment challenge. In Proc. of the Text Anal-
ysis Conference, pp. 1?9, Gaithersburg, MD. 
Giampiccolo, D., Magnini, B., Dagan, I., & Dolan, B. 
2007. The third PASCAL recognizing textual entail-
ment challenge. In Proc. of the ACL-Pascal Work-
shop on Textual Entailment and Paraphrasing, pp. 1?
9, Prague, Czech Republic. 
I. Dagan and O. Glickman.2004. Probabilistic Textual 
Entailment: Generic Applied Modeling of Language 
Variability. Proceedings of the PASCAL Workshop 
of Learning Methods for Text Understanding and 
Mining. 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010.A Survey of Paraphrasing and Textual Entail-
ment Methids. Journal of Artificial Intelligence Re-
search, 32, 135-187. 
Kouylekov, M. and Negri, M. 2010. An open-source 
package for recognizing textual entailment. Proceed-
ings of the ACL 2010 System Demonstrations, 42-47. 
Kubler, S., McDonald, R., & Nivre, J. 2009. Dependen-
cy Parsing. Synthesis Lectures on HLT. Morgan and 
Claypool Publishers. 
Levenshtein, V. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet 
Physice-Doklady, 10, 707?710. 
Lin, D. 1998b. An information-theoretic definition of 
similarity. In Proc. of the 15th Int. Conf. on Machine 
Learning, pp. 296?304, Madison, WI. Morgan 
Kaufmann, San Francisco, CA. 
Malakasiotis, P., & Androutsopoulos, I. 2007. Learning 
textual entailment using SVMs and string similarity 
measures. In Proc. of the ACL-PASCAL Workshop 
on Textual Entailment and Paraphrasing, pp. 42?47, 
Prague. ACL. 
Mehdad, Y. and Negri, M. and Federico, M.2010. To-
wards Cross-Lingual Textual Entailment. Human 
Language Technologies.The 2010 Annual Confer-
ence of the NAACL. 321-324. 
Mehdad, Y. and Negri, M. and Federico, M.2011. Using 
bilingual parallel corpora for cross-lingual textual 
entailment. Proceedings of ACL-HLT 
Melcuk, I. 1987. Dependency Syntax: Theory and Prac-
tice. State University of New York Press. 
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and 
D. Giampiccolo.2012. Semeval-2012 Task 8: Cross-
ligual Textual Entailment for Content Synchronizatio
n. In Proceedings of the 6th International Workshop 
on Semantic Evaluation (SemEval 2012).  
Negri, M. and Bentivogli, L. and Mehdad, Y. and 
Giampiccolo, D. and Marchetti, A.2011. Divide and 
conquer: crowdsourcing the creation of cross-lingual 
textual entailment corpora. Proceedings of the Con-
ference on Empirical Methods in Natural Language 
Processing. 
Pad?o, S., & Lapata, M. 2007. Dependency-based con-
struction of semantic space models. Comp. Ling., 
33(2), 161?199. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings 
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Edmonton, 
Canada, July. 
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & 
Molla, D. 2003. Exploiting paraphrases in a question 
answering system. In Proc. of the 2nd Int. Workshop 
in Paraphrasing, pp. 25?32, Saporo, Japan. 
Rosti, A. and Matsoukas, S. and Schwartz, R. Improved 
word-level system combination for machine transla-
tion, ANNUAL MEETING-ASSOCIATION FOR 
COMPUTATIONAL LINGUISTICS,2007 
Tatu, M., & Moldovan, D. 2005. A semantic approach 
to recognizing textual entailment. In Proc. of the 
Conf. on HLT and EMNLP, pp. 371?378, Vancouver, 
Canada. 
Tatu, M., & Moldovan, D. 2007. COGEX at RTE 3. In 
Proc. of the ACL-PASCAL Workshop on Textual 
Entailment and Paraphrasing, pp. 22?27, Prague, 
Czech Republic. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree?to 
string alignment template for statistical machine 
translation. In Proceedings of ACL 2006, pages 609?
616, Sydney, Australia, July. 
720
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 76?80,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
ETS: An Error Tolerable System for Coreference Resolution
Hao Xiong , Linfeng Song , Fandong Meng , Yang Liu , Qun Liu and Yajuan Lu?
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao,songlinfeng,mengfandong,yliu,liuqun,lvyajuan}@ict.ac.cn
Abstract
This paper presents our error tolerable sys-
tem for coreference resolution in CoNLL-
2011(Pradhan et al, 2011) shared task (closed
track). Different from most previous reported
work, we detect mention candidates based on
packed forest instead of single parse tree, and
we use beam search algorithm based on the
Bell Tree to create entities. Experimental re-
sults show that our methods achieve promising
results on the development set.
1 Introduction
Over last decades, there has been increasing inter-
est on coreference resolution within NLP commu-
nity. The task of coreference resolution is to iden-
tify expressions in a text that refer to the same dis-
course entity. This year, CoNLL1 holds a shared
task aiming to model unrestricted coreference in
OntoNotes.2 The OntoNotes project has created a
large-scale, accurate corpus for general anaphoric
coreference that covers entities and events not lim-
ited to noun phrases or a limited set of entity types.
And Pradhan et al (2007) have ever used this corpus
for similar unrestricted coreference task.
Our approach to this year?s task could be divided
into two steps: mention identification and creation
of entities. The first stage is conducted on the anal-
ysis of parse trees produced by input data. The of-
ficial data have provided gold and automatic parse
trees for each sentences in training and development
1http://conll.bbn.com/
2http://www.bbn.com/ontonotes/
set. However, according to statistics, almost 3%
mentions have no corresponding constituents in au-
tomatic parse trees. Since only automatic parse trees
will be provided in the final test set, the effect of
parsing errors are inevitable. To alleviate this issue,
based on given automatic parse trees, we modify a
state-of-the-art parser (Charniak and Johnson, 2005)
to generate packed forest, and determine mention
candidates among all constituents from both given
parse tree and packed forest. The packed forest is a
compact representation of all parse trees for a given
sentence. Readers can refer to (Mi et al, 2008) for
detailed definitions.
Once the mentions are identified, the left step is
to group mentions referring to same object into sim-
ilar entity. This problem can be viewed as binary
classification problem of determining whether each
mention pairs corefer. We use a Maximum Entropy
classifier to predict the possibility that two mentions
refer to the similar entity. And mainly following the
work of Luo et al (2004), we use a beam search
algorithm based on Bell Tree to obtain the global
optimal classification.
As this is the first time we participate competi-
tion of coreference resolution, we mainly concen-
trate on developing fault tolerant capability of our
system while omitting feature engineering and other
helpful technologies.
2 Mention Detection
The first step of the coreference resolution tries to
recognize occurrences of mentions in documents.
Note that we recognize mention boundaries only on
development and test set while generating training
76
Figure 1: Left side is parse tree extracted from develop-
ment set, and right side is a forest. ?my daughter? is a
mention in this discourse, however it has no correspond-
ing constituent in parse tree, but it has a corresponding
constituent NP0 in forest.
instances using gold boundaries provided by official
data.
The first stage of our system consists of following
three successive steps:
? Extracting constituents annotated with NP,
NNP, PRP, PRP$ and VBD POS tags from sin-
gle parse tree.
? Extracting constituents with the same tags as
the last step from packed forest.
? Extracting Named Entity recognized by given
data.
It is worth mentioning that above three steps will
produce duplicated mentions, we hence collect all
mentions into a list and discard duplicated candi-
dates. The contribution of using packed forest is that
it extends the searching space of mention candidates.
Figure 1 presents an example to explain the advan-
tage of employing packed forest to enhance the men-
tion detection process. The left side of Figure 1 is
the automatic parse tree extracted from development
set, in which mention ?my daughter? has no corre-
sponding constituent in its parse tree. Under nor-
mal strategy, such mention will not be recognized
and be absent in the clustering stage. However, we
find that mention has its constituent NP0 in packed
forest. According to statistics, when using packed
forest, only 0.5% mentions could not be recognized
while the traditional method is 3%, that means the
theoretical upper bound of our system reaches 99%
compared to baseline?s 97%.
Since the requirement of this year?s task is
to model unrestricted coreference, intuitively, we
should not constraint in recognizing only noun
phrases but also adjective phrase, verb and so on.
However, we find that most mentions appeared in
corpus are noun phrases, and our experimental re-
sults indicate that considering constituents annotated
with above proposed POS tags achieve the best per-
formance.
3 Determining Coreference
This stage is to determine which mentions belong to
the same entity. We train a Maximum Entropy clas-
sifier (Le, 2004) to decide whether two mentions are
coreferent. We use the method proposed by Soon, et
al.?s to generate the training instances, where a posi-
tive instance is formed between current mention Mj
and its closest preceding antecedent Mi, and a neg-
ative instance is created by paring Mj with each of
the intervening mentions, Mi+1, Mi+2,...,Mj?1.
We use the following features to train our classi-
fier.
Features in Soon et al?s work (Soon et al, 2001)
Lexical features
IS PREFIX: whether the string of one mention is
prefix of the other;
IS SUFFIX: whether the string of one mention is
suffix of the other;
ACRONYM: whether one mention is the acronym
of the other;
Distance features
SENT DIST: distance between the sentences con-
taining the two mentions;
MEN DIST: number of mentions between two
mentions;
Grammatical features
IJ PRONOUN: whether both mentions are pro-
noun;
I NESTED: whether mention i is nested in an-
other mention;
J NESTED: whether mention j is nested in an-
other mention;
Syntax features
HEAD: whether the heads of two mentions have
the same string;
HEAD POS: whether the heads of two mentions
have the same POS;
HEA POS PAIRS: pairs of POS of the two men-
tions? heads;
77
Semantic features
WNDIST: distance between two mentions in
WordNet;
I ARG0: whether mention i has the semantic role
of Arg0;
J ARG0: whether mention j has the semantic role
of Arg0;
IJ ARGS: whether two mentions have the seman-
tic roles for similar predicate;
In the submitted results, we use the L-BFGS pa-
rameter estimation algorithm with gaussian prior
smoothing (Chen and Rosenfeld, 1999). We set the
gaussian prior to 2 and train the model in 100 itera-
tions.
3.1 Creation of Entities
This stage aims to create the mentions detected in
the first stage into entities, according to the predic-
tion of classifier. One simple method is to use a
greedy algorithm, by comparing each mention to its
previous mentions and refer to the one that has the
highest probability. In principle, this algorithm is
too greedy and sometimes results in unreasonable
partition (Ng, 2010). To address this problem, we
follow the literature (Luo et al, 2004) and propose
to use beam search to find global optimal partition.
Intuitively, creation of entities can be casted as
partition problem. And the number of partitions
equals the Bell Number (Bell, 1934), which has a
?closed? formula B(n) = 1e
??
k=0
kn
k! . Clearly, this
number is very huge when n is large, enumeration of
all partitions is impossible, so we instead designing
a beam search algorithm to find the best partition.
Formally, the task is to optimize the following ob-
jective,
y? = argmax
??P
?
e??
Prob(e) (1)
where P is all partitions, Prob(e) is the cost of
entity e. And we can use the following formula to
calculate the Prob(e),
Prob(e) =
?
i?e,j?e
pos(mi,mj)
+
?
i?e,j /?e
neg(mi,mj)
(2)
where pos(mi,mj) is the score predicted by clas-
sifier that the possibility two mentions mi and mj
group into one entity, and neg(mi,mj) is the score
that two mentions are not coreferent.
Theoretically, we can design a dynamic algorithm
to obtain the best partition schema. Providing there
are four mentions from A to D, and we have ob-
tained the partitions of A, B and C. To incorporate
D, we should consider assigning D to each entity of
every partition, and generate the partitions of four
mentions. For detailed explanation, the partitions
of three mentions are [A][B][C], [AB][C], [A][BC]
and [ABC], when considering the forth mention D,
we generate the following partitions:
? [A][B][C][D], [AD][B][C], [A][BD][C],
[A][B][CD]
? [AB][C][D], [ABD][C],[AB][CD]
? [A][BC][D], [AD][BC], [A][BCD]
? [ABC][D], [ABCD]
The score of partition [AD][B][C] can be
calculated by score([A][B][C]) + pos(A,D) +
neg(B,D) + neg(C,D). Since we can computer
pos and neg score between any two mentions in
advance, this problem can be efficiently solved by
dynamic algorithm. However, in practice, enumer-
ating the whole partitions is intractable, we instead
exploiting a beam with size k to store the top k parti-
tions of current mention size, according to the score
the partition obtain. Due to the scope limitation, we
omit the detailed algorithm, readers can refer to Luo
et al (2004) for detailed description, since our ap-
proach is almost similar to theirs.
4 Experiments
4.1 Data Preparation
The shared task provided data includes information
of lemma, POS, parse tree, word sense, predicate
arguments, named entity and so on. In addition to
those information, we use a modified in house parser
to generate packed forest for each sentence in devel-
opment set, and prune the packed forest with thresh-
old p=3 (Huang, 2008). Since the OntoNotes in-
volves multiple genre data, we merge all files and
78
Mention MUC BCUBED CEAFM CEAFE BLANC
baseline 58.97% 44.17% 63.24% 45.08% 37.13% 62.44%
baseline gold 59.18% 44.48% 63.46% 45.37% 37.47% 62.36%
sys forest 59.07% 44.4% 63.39% 45.29% 37.41% 62.41%
sys btree 59.44% 44.66% 63.77% 45.62% 37.82% 62.47%
sys forest btree 59.71% 44.97% 63.95% 45.91% 37.96% 62.52%
Table 1: Experimental results on development set (F score).
Mention MUC BCUBED CEAFM CEAFE BLANC
sys1 54.5% 39.15% 63.91% 45.32% 37.16% 63.18%
sys2 53.06% 35.55% 59.68% 38.24% 32.03% 50.13%
Table 2: Experimental results on development set with different training division (F score).
take it as our training corpus. We use the sup-
plied score toolkit 3 to compute MUC, BCUBED,
CEAFM, CEAFE and BLANC metrics.
4.2 Experimental Results
We first implement a baseline system (baseline)
that use single parse tree for mention detection
and greedy algorithm for creation of entities. We
also run the baseline system using gold parse tree,
namely baseline gold. To investigate the contribu-
tion of packed forest, we design a reinforced sys-
tem, namely sys forest. And another system, named
as sys btree, is used to see the contribution of beam
search with beam size k=10. Lastly, we combine
two technologies and obtain system sys forest btree.
Table 1 shows the experimental results on devel-
opment data. We find that the system using beam
search achieve promising improvement over base-
line. The reason for that has been discussed in last
section. We also find that compared to baseline,
sys forest and baseline gold both achieve improve-
ment in term of some metrics. And we are glad to
find that using forest, the performance of our sys-
tem is approaching the system based on gold parse
tree. But even using the gold parse tree, the im-
provement is slight. 4 One reason is that we used
some lexical and grammar features which are dom-
3http://conll.bbn.com/download/scorer.v4.tar.gz
4Since under task requirement, singleton mentions are fil-
tered out, it is hard to recognize the contribution of packed for-
est to mention detection, while we may incorrectly resolve some
mentions into singletons that affects the score of mention detec-
tion.
inant during prediction, and another explanation is
that packed forest enlarges the size of mentions but
brings difficulty to resolve them.
To investigate the effect of different genres to de-
velop set, we also perform following compared ex-
periments:
? sys1: all training corpus + WSJ development
corpus
? sys2: WSJ training corpus + WSJ development
corpus
Table 2 indicates that knowledge from other genres
can help coreference resolution. Perhaps the reason
is the same as last experiments, where syntax diver-
sity affects the task not very seriously.
5 Conclusion
In this paper, we describe our system for CoNLL-
2011 shared task. We propose to use packed for-
est and beam search to improve the performance of
coreference resolution. Multiple experiments prove
that such improvements do help the task.
6 Acknowledgement
The authors were supported by National Natural
Science Foundation of China, Contracts 90920004.
We would like to thank the anonymous reviewers
for suggestions, and SHUGUANG COMPUTING
PLATFORM for supporting experimental platform.
79
References
E.T. Bell. 1934. Exponential numbers. The American
Mathematical Monthly, 41(7):411?419.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 173?180.
Association for Computational Linguistics.
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaussian
prior for smoothing maximum entropy models. Tech-
nical report, CMU-CS-99-108.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Columbus, Ohio, June.
Z. Le. 2004. Maximum entropy modeling toolkit for
Python and C++.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the bell tree. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 135?es. As-
sociation for Computational Linguistics.
H. Mi, L. Huang, and Q. Liu. 2008. Forestbased transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199.
Citeseer.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1396?1411, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
80
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 71?75,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
ICT: System Description for CoNLL-2012
Hao Xiong and Qun Liu
Key Lab. of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{xionghao, liuqun}@ict.ac.cn
Abstract
In this paper, we present our system de-
scription for the CoNLL-2012 coreference
resolution task on English, Chinese and
Arabic. We investigate a projection-based
model in which we first translate Chinese
and Arabic into English, run a publicly
available coreference system, and then use
a new projection algorithm to map the
coreferring entities back from English in-
to mention candidates detected in the Chi-
nese and Arabic source. We compare to
a baseline that just runs the English coref-
erence system on the supplied parses for
Chinese and Arabic. Because our method
does not beat the baseline system on the
development set, we submit outputs gen-
erated by the baseline system as our final
submission.
1 Introduction
Modeling multilingual unrestricted coreference in
the OntoNotes data is the shared task for CoNLL-
2012. This is an extension of the CoNLL-
2011 shared task and would involve automatic
anaphoric mention detection and coreference res-
olution across three languages ? English, Chinese
and Arabic ? using OntoNotes v5.0 corpus, giv-
en predicted information on the syntax, proposi-
tion, word sense and named entity layers. Au-
tomatic identification of coreferring entities and
events in text has been an uphill battle for sev-
eral decades, partly because it can require world
knowledge which is not well-defined and partly
owing to the lack of substantial annotated data.
Figure 1: The overall process of our system, where
we use Google Translator to translate Chinese and
Arabic into English.
For more details, readers can refer to (Pradhan et
al., 2012).
Before this year?s task, researchers proposed t-
wo typical novel methods to address the prob-
lem of natural language processing across multiple
languages: projection and joint learning (Rahman
and Ng, 2012). Specific to this year?s coreference
resolution task, for projection based method, we
could first develop a strong resolver or utilize a
publicly available system on English, and trans-
late other languages into English, eventually, we
could project the coreferring entities resolved on
English back into other language sides. General-
ly, a projection method is easier to develop since
it doesn?t need sentence alignment across multiple
languages. Thus, in this year?s task, we investigate
a translation based model to resolve coreference
on English, Chinese and Arabic. The whole pro-
cess is illustrated in figure 1, in which we first use
Google Translator to translate Chinese and Ara-
bic into English, and we then employ a strong En-
glish coreference resolver to generate coreferring
entities, after mapping entities from English into
71
Chinese and Arabic mention candidates, we could
obtain coreferring entities for these languages.
Intuitively, the performance of coreference re-
solver on English should perform better than that
on Chinese and Arabic since we have substantial
corpus for English and coreference resolution on
English is well studied compared to another two
languages. Thus we could imagine that projecting
the results from English into Chinese and Arabic
should still beats the baseline system using mono-
lingual resolution method. However, in our exper-
iments, we obtain negative results on developing
set that means our projection based model perfor-
m worse than the baseline system. According to
our experimental results on developing set, final-
ly, we submit results of baseline system in order to
obtain better ranking.
The rest of this paper is organized as follows, in
section 2, we will introduce our method in details,
and section 3 is our experimental results, we draw
conclusion in section 4.
2 Projection based Model
As the last section mentioned, we propose to use
a projection based model to resolve coreference
on multiple languages. The primary procedures
of our method could be divided into three steps:
first step is translation, where Google Translator is
employed to translate Chinese and Arabic into En-
glish, second is coreference resolution for English,
last is the projection of coreferring entities. Since
the first step is clear that we extract sentences from
Chinese and Arabic documents and translate them
into English using Google Translator, hence in this
section we will mainly describe the configuration
of our English resolver and details of projection
method.
2.1 English Resolver
In last year?s evaluation task, the Standford
Natural Language Processing Group ranked the
first position and they also open their toolkit for
research community, namely Standford CoreNLP
(Lee et al, 2011) 1, better yet, their toolkit is op-
timized for CoNLL task. Thus we could use their
toolkit as our English resolver and concentrate
on bettering the projection of coreferring entities.
1http://nlp.stanford.edu/software/
corenlp.shtml
Figure 2: A minimum cost and maximum flow
structure is used to solve the problem that map-
ping coreferring entities into each mention candi-
dates with highest probability.
We use the basic running script that is ?java -cp
joda-time.jar:stanford-corenlp.jar:stanford-
corenlp-models.jar:xom.jar -Xmx3g e-
du.stanford.nlp.pipeline.StanfordCoreNLP
-filelist filelist.txt? to resolve the resolution,
where ?filelist? involves all documents need to be
performed coreference resolution.
2.2 Projection of Coreferring Entities
After generating coreferring entities on English,
the key step of our system is how to map them into
mention candidates detected on Chinese and Ara-
bic. For instance, assuming we translate Chinese
documents into English and obtain coreferring en-
tities e1, e2, ei,.., eE on translated English doc-
uments through aforementioned step, meanwhile,
we consider all noun phrases(NP) in original Chi-
nese documents and generate mention candidates
m1, m2, mj ,.., mM . Therefore, our task is to map
each ei into one mention candidate mj with high-
est probability, and it can be obtained by the max-
72
Algorithm 1 Algorithm for computing similarity
between two phrases in different languages.
1: Input: we1 , .., wen , wc1 , .., wcm , Phrase Table
PT
2: s[n] = [0,? inf, ..,? inf]
3: for i? 1..n do
4: for j ? 0..10 do
5: s[i + j] = max(s[i + j], s[i ? 1] +
p(i, i + j))
6: Output: s[n]V
imization of the following formula,
P? =
?
ei?E,mj?M
{a(i, j)b(j, i)p(i, j)} (1)
with constrains
?
i,j{a(i, j)} = 1 and
?
i,j{b(j, i)} = 1, where p(i, j) is the prob-
ability of ei mapping into mj and a(i, j) as
well as b(i, j) are integers guaranteeing each
coreferring entity map into one mention and each
mention has only one entity to be mapped into.
To solve this problem, we reduce it as a Cost
Flow problem since it is easier to understand
and implement compared to other methods such
as integer linear programming. Note that the
number of mention candidates is theoretically
larger than that of coreferring entities, thus this
problem couldn?t be reduced as the bipartite graph
matching problem since it needs equal number of
nodes in two parts.
Figure 2 shows the graph structure designed to
solve this problem, where the symbols labeled on
each edge is a two tuples(Cost,Flow), indicating
the cost and flow for each edge. Since object of
Cost Flow problem is to minimize the cost while
maximizing the flows, thus we compute the c(i, j)
as 1 ? p(i, j) in order to be consistent with the
equation 1. To satisfy two constraints aforemen-
tioned, we set up two dummy nodes ?Start? and
?End?, and connect ?Start? to each entity ei with
cost 0 and flow 1 ensuring each entity is available
to map one mention. We also link each mention
candidate mj to node ?End? with the same val-
ue ensuring each mention could be mapped into
by only one entity. Clearly, there is an edge with
tuple (1?p(i, j), 1) between each entity end men-
tion indicating that each entity could map into any
mention while with different probabilities. Thus,
solving this Cost-Flow problem is equal to maxi-
mizing the equation 1 with two constraints. Since
Cost-Flow problem is well studied, thus some al-
gorithm can solve this problem in polynomial time
(Ahuja et al, 1993). One may argue that we can
modify translation decoder to output alignments
between Chinese and translated English sentence,
unfortunately, Google Translator API doesn?t sup-
ply these information while its translation quality
is obviously better than others for translating doc-
uments in OntoNotes, moreover, it is impossible to
output alignment for each word since some trans-
lation rules used for directing translation include
some unaligned words, thus an algorithm to map
each entity into each mention is more applicable.
Clearly, another problem is how to compute
p(i, j) for each edge between entity and mention
candidate. This problem could be casted as how
to compute similarity of phrases across multiple
languages. Formally, given an English phrases
we1 , .., wen and a Chinese phrase wc1 , .., wcm , the
problem is how to compute the similar score S be-
tween them. Although we could compute lexical,
syntactic or semantic similar score to obtain ac-
curate similarity, here for simplicity, we just com-
pute the lexical similarity using the phrase table
extracted by a phrased-based machine translation
decoder (Koehn et al, 2003). Phrase table is a rich
resource that contains probability score for phrase
in one language translated into another language,
thus we could design a dynamic algorithm shown
in Algorithm 1 to compute the similar score. E-
quation in line 5 is used to reserve highest simi-
lar score for its sub-phrases, and p(i, i + j) is the
similar score between sub-phrases wi, .., wi+j and
its translation. When we compute the score of the
sub-phrases wi, .., wi+j , we literately pick one pti
from PT and check whether wc1 , .., wcm involves
pti?s target side, if that we record its score un-
til we obtain a higher score obtained by another
ptj and then update it. For instance, assuming the
Chinese input sentence is ?????????
?? ?? ? ?? ? ?? ?? ??, and the
Google translation of this sentence is ?The world
?s fifth Disneyland will soon open to the public .
?. Following the aforementioned steps, we utilize
English resolver to find a coreferring entity: ?The
world ?s fifth Disneyland?, and find two translation
rules involving the former English phrase from the
73
bilingual phrase table: ?The world ?s fifth Disney-
land => ??????????? (probabili-
ty=0.6) ? and ?The world ?s fifth Disneyland =>
?????????? (probability=0.4)?. S-
ince the Chinese translation of both rules all con-
tain the noun phrase ??? ?? ? ??? ?
?? in the original Chinese input, we thus add this
noun phrase into the coreferring entities as the En-
glish resolve finding with the probability 0.6.
3 Experiments
3.1 English Results
In this section, we will report our experimental re-
sults in details. We use Standford CoreNLP toolkit
to generate results for English. Table 1 lists the F-
score obtained on developing set.
3.2 Chinese and Arabic Results
As last section mentioned, we first translate
Chinese and Arabic into English and then use
CoreNLP to resolve coreference on English. To
obtain high translation quality, we use Google
Translator Toolkit 2. And to compute similarity
score, we run Giza++(Och and Ney, 2003) 3, an
open source toolkit for word alignment, to perfor-
m word alignment. For Chinese, we use 1 million
bilingual corpus provided by NIST MT evaluation
task to extract phrase table, and for Arabic its size
is 2 million. Note that, we extract phrase table
from English to Chinese and Arabic with maxi-
mum phrase length 10. The reason is that our al-
gorithm check English phrase whose length is less
than 10 tokens. To compare our results, we al-
so use CoreNLP to generate results for Chinese
and Arabic. Since CoreNLP use some syntac-
tic knowledge to resolving coreference, it can al-
so output coreferring entities for other languages.
From table 2 we find that although CoreNLP is not
designed for other languages, it still obtain accept-
able scores and beat our projection based mod-
el. The main reason is that our method is coarse
and obtain lower precision for mention detection,
while CoreNLP use some manually written rules
to detect mention candidates. Another explana-
tion is that projection based model is hard to map
2http://www.google.cn/url?source=
transpromo&rs=rsmf&q=http://translate.
google.com/toolkit
3http://code.google.com/p/giza-pp/
some phrases back into original languages, such
as ?that, it, this?. Moreover, translation quality for
some corpus like web corpus is far from perfect,
translation errors will surely affect the precision of
coreference resolution. Thus, for the final testing
set, we run the CoreNLP to generate the results.
3.3 Testing Results
Since CoreNLP beats our system in Chinese and
Arabic, thus we run CoreNLP for all three lan-
guages. Table 3 lists the final results, and we also
give results using golden parse tree for prediction
in table 4. From these two tables, we find that for
any language, the system using golden parse tree
show better performance than the one using pre-
dicted system in term of each metric. The reason
is that the CoreNLP resolve coreference on parse
tree and employ some parse features to corefer. On
the other hand, we could also see that the improve-
ment is slight, because parsing errors affect lit-
tle on finding mention candidates benefiting from
high precision on noun phrase prediction. Final-
ly, since we use an open source toolkit to generate
results, unfortunately, we have no ranking in this
task.
4 Conclusion
In this paper, we present a projection based mod-
el for coreference resolution. We first translate
Chinese and Arabic into English, and then em-
ploy a strong English resolver to generate core-
ferring entities, after that a projection algorithm is
designed to map coreferring entities into mention
candidates detected in Chinese and Arabic. How-
ever, since our approach is coarse and due to limit
time preparing for this task, the output generate
by CoreNLP beats our results in three languages,
thus we submit results generated by CoreNLP as
our final submission.
Acknowledgments
The authors were supported by National Science
Foundation of China, Contracts 90920004, and
High-Technology R&D Program (863) Project No
2011AA01A207 and 2012BAH39B03. We thank
organizers for their generous supplied resources
and arduous preparation. We also thank anony-
mous reviewers for their thoughtful suggestions.
74
Mention MUC BCUB CEAFE
CoreNLP 73.68% 64.58% 70.60% 46.64
Table 1: Experimental results on developing set(F-score) for English.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 52.15% 38.16% 60.38% 34.58
Projection-Chinese 48.51% 32.31% 63.77% 24.72
CoreNLP-Arabic 52.97% 27.88% 60.75% 40.52
Projection-Arabic 42.68% 22.39% 62.18% 32.83
Table 2: Experimental results on developing set(F-score) for Chinese and Arabic using CoreNLP and
our system.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 49.82% 37.83% 60.30% 34.93
CoreNLP-Arabic 53.89% 28.31% 61.83% 42.97
CoreNLP-English 73.69% 63.82% 68.52% 45.36
Table 3: Experimental results on testing set(F-score) using predicted parse tree.
Mention MUC BCUB CEAFE
CoreNLP-Chinese 53.42% 40.60% 60.37% 35.75
CoreNLP-Arabic 55.17% 30.54% 62.36% 43.03
CoreNLP-English 75.58% 66.14% 69.55% 46.54
Table 4: Experimental results on testing set(F-score) using golden parse tree.
References
R.K. Ahuja, T.L. Magnanti, and J.B. Orlin. 1993. Net-
work flows: theory, algorithms, and applications.
1993.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the Fifteenth
Conference on Computational Natural Language
Learning: Shared Task, pages 28?34. Association
for Computational Linguistics.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestrict-
ed coreference in OntoNotes. In Proceedings of
the Sixteenth Conference on Computational Natural
Language Learning (CoNLL 2012), Jeju, Korea.
Altaf Rahman and Vincent Ng. 2012. Translation-
based projection for multilingual coreference reso-
lution. In NAACL 2012.
75
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 177?184,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Shallow Semantically-Informed PBSMT and HPBSMT
Tsuyoshi Okita
Qun Liu
Josef van Genabith
Dublin City University
Glasnevin, Dublin 9, Ireland
{tokita,qliu,josef}@computing.dcu.ie
Abstract
This paper describes shallow
semantically-informed Hierarchical
Phrase-based SMT (HPBSMT) and
Phrase-Based SMT (PBSMT) systems
developed at Dublin City University
for participation in the translation task
between EN-ES and ES-EN at the Work-
shop on Statistical Machine Translation
(WMT 13). The system uses PBSMT
and HPBSMT decoders with multiple
LMs, but will run only one decoding
path decided before starting translation.
Therefore the paper does not present a
multi-engine system combination. We
investigate three types of shallow seman-
tics: (i) Quality Estimation (QE) score,
(ii) genre ID, and (iii) context ID derived
from context-dependent language models.
Our results show that the improvement is
0.8 points absolute (BLEU) for EN-ES
and 0.7 points for ES-EN compared to
the standard PBSMT system (single best
system). It is important to note that we
developed this method when the standard
(confusion network-based) system com-
bination is ineffective such as in the case
when the input is only two.
1 Introduction
This paper describes shallow semantically-
informed Hierarchical Phrase-based SMT
(HPBSMT) and Phrase-Based SMT (PBSMT)
systems developed at Dublin City University
for participation in the translation task between
EN-ES and ES-EN at WMT 13. Our objectives
are to incorporate several shallow semantics into
SMT systems. The first semantics is the QE score
for a given input sentence which can be used to
select the decoding path either of HPBSMT or
PBSMT. Although we call this a QE score, this
score is not quite a standard one which does not
have access to translation output information. The
second semantics is genre ID which is intended to
capture domain adaptation. The third semantics
is context ID: this context ID is used to adjust the
context for the local words. Context ID is used in
a continuous-space LM (Schwenk, 2007), but is
implicit since the context does not appear in the
construction of a continuous-space LM. Note that
our usage of the term semantics refers to meaning
constructed by a sentence or words. The QE
score works as a sentence level switch to select
HPBSMT or PBSMT, based on the semantics
of a sentence. The genre ID gives an indication
that the sentence is to be translated by genre ID-
sensitive MT systems, again based on semantics
on a sentence level. The context-dependent LM
can be interpreted as supplying the local context
to a word, capturing semantics on a word level.
The architecture presented in this paper is sub-
stantially different from multi-engine system com-
bination. Although the system has multiple paths,
only one path is chosen at decoding when process-
ing unseen data. Note that standard multi-engine
system combination using these three semantics
has been presented before (Okita et al, 2012b;
Okita et al, 2012a; Okita, 2012). This paper also
compares the two approaches.
The remainder of this paper is organized as fol-
lows. Section 2 describes the motivation for our
approach. In Section 3, we describe our proposed
systems, while in Section 4 we describe the exper-
imental results. We conclude in Section 5.
2 Motivation
Model Difference of PBSMT and HPBSMT
Our motivation is identical with a system combi-
nation strategy which would obtain a better trans-
lation if we can access more than two translations.
Even though we are limited in the type of MT sys-
177
tems, i.e. SMT systems, we can access at least
two systems, i.e. PBSMT and HPBSMT systems.
The merit that accrues from accessing these two
translation is shown in Figure 1. In this exam-
ple between EN-ES, the skirts of the distribution
shows that around 20% of the examples obtain the
same BLEU score, 37% are better under PBSMT,
and 42% under HPBSMT. Moreover, around 10%
of sentences show difference of 10 BLEU points.
Even a selection of outputs would improve the re-
sults. Unfortunately, some pitfall of system com-
bination (Rosti et al, 2007) impact on the process
when the number of available translation is only
two. If there are only two inputs, (1) the mismatch
of word order and word selection would yield a
bad combination since system combination relies
on monolingual word alignment (or TER-based
alignment) which seeks identical words, and (2)
Minimum Bayes Risk (MBR) decoding, which is
a first step, will not work effectively since it re-
lies on voting. (In fact, only selecting one of the
translation outputs is even effective: this method
is called system combination as well (Specia et al,
2010).) Hence, although the aim is similar, we do
not use a system combination strategy, but we de-
velop a semantically-informed SMT system.
Figure 1: Figure shows the difference of sentence-
based performance between PBSMT and HPB-
SMT systems.
Relation of Complexity of Source Sentence and
Performance of HPBSMT and PBSMT It is
interesting to note that PBSMT tends to be bet-
ter than HPBSMT for European language pairs
as the recent WMT workshop shows, while HPB-
SMT shows often better performance for distant
language pairs such as EN-JP (Okita et al, 2010b)
and EN-ZH in other workshops.
Under the assumption that we use the same
training corpus for training PBSMT and HPBSMT
systems, our hypothesis is that we may be able
to predict the quality of translation. Note that al-
though this is the analogy of quality estimation,
the setting is slightly different in that in test phase,
we will not be given a translation output, but only
a source sentence. Our aim is to predict whether
HPBSMT obtains better translation output than
PBSMT or not. Hence, our aim does not require
that the quality prediction here is very accurate
compared to the standard quality estimation task.
We use a feature set consisting of various charac-
teristics of input sentences.
3 Our Methods: Shallow Semantics
Our system accommodates PBSMT and HPBSMT
with multiple of LMs. A decoder which handles
shallow semantic information is shown in Table
3.1.
3.1 QE Score
Quality estimation aims to predict the quality of
translation outputs for unseen data (e.g. by build-
ing a regressor or a classifier) without access to
references: the inputs are translation outputs and
source sentences in a test phase, while in a training
phase the corresponding BLEU or HTER scores
are used. In this subsection, we try to build a re-
gressor with the similar settings but without sup-
plying the translation outputs. That is, we supply
only the input sentences. (Since our method is not
a quality estimation for a given translation output,
quality estimation may not be an entirely appro-
priate term. However, we borrow this term for this
paper.) If we can build such a regressor for PB-
SMT and HPBSMT systems, we would be able
to select a better translation output without actu-
ally translating them for a given input sentence.
Note that we translate the training set by PBSMT
and HPBSMT in a training phase only to supply
their BLEU scores to a regressor (since a regres-
sor is a supervised learning method). Then, we
use these regressors for a given unseen source sen-
tence (which has no translation output attached) to
predict their BLEU scores for PBSMT and HPB-
SMT.
Our motivation came from the comparison of
a sequential learning system and a parser-based
system. The typical decoder of the former is a
178
Viterbi decoder while that of the latter is a Cocke-
Younger-Kasami (CYK) decoder (Younger, 1967).
The capability of these two systems provides
an intuition about the difference of PBSMT and
HPBSMT: the CYK decoder-based system has
some capability to handle syntactic constructions
while the Viterbi decoder-based system has only
the capability of learning a sequence. For ex-
Input: Foreign sent f=f1,...,f1f , language model,
translation model, rule table.
Output: English translation e
ceScore = predictQEScore(fi)
if (ceScore == HPBSMTBetter)
for span length l=1 to 1f do
for start=0..1f -1 do
genreID = predictGenreID(fi)
end = start + 1
forall seq s of entries and words in span
[start,end] do
forall rules r do
if rule r applies to chart seq s then
create new chart entry c
with LM(genreID)
add chart entry c to chart
return e from best chart entry in span [0,1f ]
else:
genreID = predictGenreID(fi)
place empty hypothesis into stack 0
for all stacks 0...n-1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hyp with LM(ID)
place in stack
recombine with existing hyp if
possible
prune stack if too big
return e
predictQEScore()
predictGenreID()
predictContextID(wordi, wordi?1)
Table 1: Decoding algorithm: the main algorithm
of PBSMT and HPBSMT are from (Koehn, 2010).
The modification is related to predictQEScore(),
predictGenreID(), and predictContextID().
ample, the (context-free) grammar-based system
has the capability of handling various difficul-
ties caused by inserted clauses, coordination, long
Multiword Expressions, and parentheses, while
the sequential learning system does not (This is
since this is what the aim of the context-free
grammar-based system is.) These difficulties are
manifest in input sentences.
0 50 100 150 200 250 300
sample ID
?1.0
?0.5
0.0
0.5
1.0
d
i
f
f
e
r
e
n
c
e
 
o
f
 
B
L
E
U
 
p
o
i
n
t
s
true BLEU difference of PBSMT and HPBSMT
predicted BLEU difference of PBSMT and HPBSMT
Figure 2: A blue line shows the true BLEU dif-
ference between PBSMT and HPBSMT (y-axis)
where x-axis is the sample IDs reordered in de-
scending order (blue), while green dots show the
BLEU absolute difference (y-axis) of the typical
samples where x-axis is shared with the above.
This example is sampled 300 points from new-
stest2013 (ES-EN). Even if the regressor does not
achieve a good performance, the bottom line of the
overall performance is already really high in this
tricky problem. Roughly, even if we plot randomly
we could achieve around 80 - 90% of correctness.
Around 50% of samples (middle of the curve) do
not care (since the true performance of PBSMT
and HPBSMT are even), there is a slope in the left
side of the curve where random plot around this
curve would achieve 15 - 20% among 25% of cor-
rectness (the performance of PBSMT is superior),
and there is another slope in the right side of the
curve where random plot would achieve again 15
- 20% among 25% (the performance of HPBSMT
is superior). In this case, accuracy is 86%.
If we assume that this is one major difference
between these two systems, the complexity of the
input sentence will correlate with the difference of
translation quality of these two systems. In this
subsection, we assume that this is one major dif-
ference of these two systems and that the complex-
ity of the input sentence will correlate with the dif-
ference of translation quality of these two systems.
Based on these assumptions, we build a regressor
179
for each system for a given input sentence where in
a training phase we supply the BLEU score mea-
sured using the training set. One remark is that the
BLEU score which we predict is only meaning-
ful in a relative manner since we actually generate
a translation output in preparation phase (there is
a dependency to the mean of BLEU score in the
training set). Nevertheless, this is still meaningful
as a relative value if we want to talk about their
difference, which is what we want in our settings
to predict which system, either PBSMT or HPB-
SMT, will generate a better output.
The main features used for training the regres-
sor are as follows: (1) number of / length of in-
serted clause / coordination / multiword expres-
sions, (2) number of long phrases (connection by
?of?; ordering of words), (3) number of OOV
words (which let it lower the prediction quality),
(4) number of / length of parenthesis, etc. We ob-
tained these features using parser (de Marneffe et
al., 2006) and multiword extractor (Okita et al,
2010a).
3.2 Genre ID
Genre IDs allow us to apply domain adaptation
technique according to the genre ID of the testset.
Among various methods of domain adaptation, we
investigate unsupervised clustering rather than al-
ready specified genres.
We used (unsupervised) classification via La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
to obtain genre ID. LDA represents topics as
multinomial distributions over the W unique
word-types in the corpus and represents docu-
ments as a mixture of topics.
Let C be the number of unique labels in the
corpus. Each label c is represented by a W -
dimensional multinomial distribution ?c over the
vocabulary. For document d, we observe both the
words in the document w(d) as well as the docu-
ment labels c(d). Given the distribution over top-
ics ?d, the generation of words in the document is
captured by the following generative model.
1. For each label c ? {1, . . . C}, sample a distri-
bution over word-types ?c ? Dirichlet(?|?)
2. For each document d ? {1, . . . , D}
(a) Sample a distribution over its observed
labels ?d ? Dirichlet(?|?)
(b) For each word i ? {1, . . . , NWd }
i. Sample a label z(d)i ?
Multinomial(?d)
ii. Sample a word w(d)i ?
Multinomial(?c) from the la-
bel c = z(d)i
Using topic modeling (or LDA) as described
above, we perform the in-domain data partitioning
as follows, building LMs for each class, and run-
ning a decoding process for the development set,
which will obtain the best weights for cluster i.
1. Fix the number of clusters C, we explore val-
ues from small to big.1
2. Do unsupervised document classification (or
LDA) on the source side of the training, de-
velopment and test sets.
3. Separate each class of training sets and build
LM for each cluster i (1 ? i ? C).
4. Separate each class of development set (keep
the original index and new index in the allo-
cated separated dataset).
5. (Using the same class of development set):
Run the decoder on each class to obtain the
n-best lists, run a MERT process to obtain the
best weights based on the n-best lists, (Repeat
the decoding / MERT process several itera-
tions. Then, we obtain the best weights for a
particular class.)
For the test phase,
1. Separate each class of the test set (keep the
original index and new index in the allocated
separated dataset).
2. Suppose the test sentence belongs to cluster
i, run the decoder of cluster i.
3. Repeat the previous step until all the test sen-
tences are decoded.
3.3 Context ID
Context ID semantics is used through the re-
ranking of the n-best list in a MERT process
(Schwenk, 2007; Schwenk et al, 2012; Le et al,
2012). 2-layer ngram-HMM LM is a two layer
version of the 1-layer ngram-HMM LM (Blun-
som and Cohn, 2011) which is a nonparametric
1Currently, we do not have a definite recommendation on
this. It needs to be studied more deeply.
180
Bayesian method using hierarchical Pitman-Yor
prior. In the 2-layer LM, the hidden sequence of
the first layer becomes the input to the higher layer
of inputs. Note that such an architecture comes
from the Restricted Boltzmann Machine (Smolen-
sky, 1986) accumulating in multiple layers in or-
der to build deep belief networks (Taylor and Hin-
ton, 2009). Although a 2-layer ngram-HMM LM
is inferior in its performance compared with other
two LMs, the runtime cost is cheaper than these.
ht denotes the hidden word for the first layer, h?t
denotes the hidden word for the second layer, wi
denotes the word in output layer. The generative
model for this is shown below.
ht|h?t ? F (??st) (1)
wt|ht ? F (?st) (2)
wi|w1:i?1 ? PY(di, ?i, Gi) (3)
where ? is a concentration parameter, ? is a
strength parameter, and Gi is a base measure.
Note that these terms belong to the hierarchical
Pitman-Yor language model (Teh, 2006). We used
a blocked inference for inference. The perfor-
mance of 2-layer LM is shown in Table 3.
4 Experimental Settings
We used Moses (Koehn et al, 2007) for PBSMT
and HPBSMT systems in our experiments. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations
of HMM, 3 iterations of Model 3, and 3 iter-
ations of Model 4. For phrase extraction the
grow-diag-final heuristics described in (Koehn et
al., 2003) is used to derive the refined alignment
from bidirectional alignments. We then perform
MERT process (Och, 2003) which optimizes the
BLEU metric, while a 5-gram language model is
derived with Kneser-Ney smoothing (Kneser and
Ney, 1995) trained with SRILM (Stolcke, 2002).
For the HPBSMT system, the chart-based decoder
of Moses (Koehn et al, 2007) is used. Most of the
procedures are identical with the PBSMT systems
except the rule extraction process (Chiang, 2005).
The procedures to handle three kinds of se-
mantics are implemented using the already men-
tioned algorithm. We use libSVM (Chang and Lin,
2011), and Mallet (McCallum, 2002) for Latent
Dirichlet Allocation (LDA) (Blei et al, 2003).
For the corpus, we used all the resources pro-
vided for the translation task at WMT13 for lan-
output layer
2?layer conditional RBM language model
ngram language model
1st RBM
2nd RBM
hidden layer
output layer
N
projection layer
discrete representation
N
P
neural network
probability estimation
continuous?space language
model [Schwenk, 2007]
1st hidden layer
2?layer ngram?HMM language model
2nd hidden layer
output layer
ngram language model
Figure 3: Figure shows the three kinds of context-
dependent LM. The upper-side shows continuous-
space language model (Schwenk, 2007). The
lower-left shows ours, i.e. the 2-layer ngram-
HMM LM. The lower-right shows the 2-layer con-
ditional Restricted Boltzmann Machine LM (Tay-
lor and Hinton, 2009).
guage model, that is parallel corpora (Europarl
V7 (Koehn, 2005), Common Crawl corpus, UN
corpus, and News Commentary) and monolingual
corpora (Europarl V7, News Commentary, and
News Crawl from 2007 to 2012).
Experimental results are shown in Table 2.
The left-most column (sem-inform) shows our re-
sults. The sem-inform made a improvement of 0.8
BLEU points absolute compared to the PBSMT
results in EN-ES, while the standard system com-
bination lost 0.1 BLEU points absolute compared
to the single worst. For ES-EN, the sem-inform
made an improvement of 0.7 BLEU points abso-
lute compared to the PBSMT results. These im-
provements over both of PBSMT and HPBSMT
are statistically significant by a paired bootstrap
test (Koehn, 2004).
5 Conclusion
This paper describes shallow semantically-
informed HPBSMT and PBSMT systems devel-
oped at Dublin City University for participation in
the translation task at the Workshop on Statistical
Machine Translation (WMT 13). Our system has
181
EN-ES sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 30.3 29.5 28.2 28.1 28.5
BLEU(11b) 30.3 29.5 28.2 28.1 28.5
BLEU-cased 29.0 28.4 27.1 27.0 27.5
BLEU-cased(11b) 29.0 28.4 27.1 27.0 27.5
NIST 7.91 7.74 7.35 7.35 7.36
Meteor 0.580 0.579 0.577 0.577 0.578
WER 53.7 55.4 59.3 59.2 58.9
PER 41.3 42.4 46.0 45.8 45.5
ES-EN sem-inform PBSMT HPBSMT syscomb aug-syscomb
BLEU 31.1 30.4 23.1? 28.8 29.9
BLEU(11b) 31.1 30.4 23.1? 28.8 29.9
BLEU-cased 29.7 29.1 22.3? 27.9 28.8
BLEU-cased(11b) 29.7 29.1 22.3? 27.9 28.8
NIST 7.87 7.79 6.67? 7.40 7.71
Meteor 0.615 0.612 0.533? 0.612 0.613
WER 54.8 55.4 62.5? 59.3 56.1
PER 41.3 41.8 48.3? 45.8 41.9
Table 2: Table shows the score where ?sem-inform? shows our system. Underlined figure shows the
official score. ?syscomb? denotes the confusion-network-based system combination using BLEU, while
?aug-syscomb? uses three shallow semantics described in QE score (Okita et al, 2012a), genre ID (Okita
et al, 2012b), and context ID (Okita, 2012). Note that the inputs for syscomb and aug-syscomb are the
output of HPBSMT and PBSMT. HPBSMT from ES to EN has marked with ?, which indicates that this
is trained only with Europarl V7.
2-layer ngram- SRI-
EN HMM LM LM
newstest12 130.4 140.3
newstest11 146.2 157.1
newstest10 156.4 166.8
newstest09 176.3 187.1
Table 3: Table shows the perplexity of context-
dependent language models, which is 2-layer
ngram HMM LM, and that of SRILM (Stolcke,
2002) in terms of newstest09 to 12.
PBSMT and HPBSMT decoders with multiple
LMs, but our system will execute only one path,
which is different from multi-engine system
combination. We consider investigate three types
of shallow semantic information: (i) a Quality
Estimate (QE) score, (ii) genre ID, and (iii) a
context ID through context-dependent language
models. Our experimental results show that the
improvement is 0.8 points absolute (BLEU) for
EN-ES and 0.7 points for ES-EN compared to
the standard PBSMT system (single best system).
We developed this method when the standard
(confusion network-based) system combination is
ineffective such as in the case when the input is
only two.
A further avenue would be the investigation of
other semantics such as linguistic semantics, in-
cluding co-reference resolution or anaphora reso-
lution, hyper-graph decoding, and text understand-
ing. Some of which are investigated in the context
of textual entailment task (Okita, 2013b) and we
would like to extend this to SMT task. Another
investigation would be the integration of genre ID
into the context-dependent LM. The preliminary
work shows that such integration would decrease
the overall perplexity (Okita, 2013a).
Acknowledgments
We thank Antonio Toral and Santiago Corte?s
Va??lo for providing parts of their processing
data. This research is supported by the Science
Foundation Ireland (Grant 07/CE/I1142) as part
of the Centre for Next Generation Localisation
(http://www.cngl.ie) at Dublin City Uni-
versity.
182
References
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research, 3:9931022.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
pitman-yor process hmm for unsupervised part of
speech induction. In Proceedings of Annual Meet-
ing of the Association for Computational Linguistics
(ACL11), pages 865?874.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-05), pages
263?270.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the Sixth International Conference
on Language Resources and Evaluation (LREC-
2006), pages 449?454.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aurelien Max,
Artem Sokolov, Guillaume Wisniewski, and Fran-
cois Yvon. 2012. Limsi at wmt12. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 330?337.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010a. Multi-Word Ex-
pression sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Lingual Information Access (CLIA2010, collocated
with COLING2010), Beijing, China., pages 1?8.
Tsuyoshi Okita, Jie Jiang, Rejwanul Haque, Hala Al-
Maghout, Jinhua Du, Sudip Kumar Naskar, and
Andy Way. 2010b. MaTrEx: the DCU MT System
for NTCIR-8. In Proceedings of the MII Test Col-
lection for IR Systems-8 Meeting (NTCIR-8), pages
377?383.
Tsuyoshi Okita, Raphae?l Rubino, and Josef van Gen-
abith. 2012a. Sentence-level quality estima-
tion for mt system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 55?64.
Tsuyoshi Okita, Antonio Toral, and Josef van Gen-
abith. 2012b. Topic modeling-based domain adap-
tation for system combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 45?54.
Tsuyoshi Okita. 2012. Neural Probabilistic Language
Model for System Combination. In Proceedings
of ML4HMT Workshop (collocated with COLING
2012), pages 65?76.
Tsuyoshi Okita. 2013a. Joint space neural probabilis-
tic language model for statistical machine transla-
tion. Technical Report at arXiv, 1301(3614).
Tsuyoshi Okita. 2013b. Local graph matching with
active learning for recognizing inference in text at
ntcir-10. NTCIR 10 Conference, pages 499?506.
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics, pages 312?319.
183
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a gpu for statistical machine trans-
lation. In NAACL-HLT workshop on the Future of
Language Modeling for HLT, pages 11?19.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Paul Smolensky. 1986. Chapter 6: Information pro-
cessing in dynamical systems: Foundations of har-
mony theory. In Rumelhart, David E.; McLel-
land, James L. Parallel Distributed Processing:
Explorations in the Microstructure of Cognition,
1:194281.
Lucia Specia, D. Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estima-
tion. Machine Translation, Springer, 24(1):39?50.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Graham Taylor and Geoffrey Hinton. 2009. Factored
conditional restricted boltzmann machines for mod-
eling motion style. In Proceedings of the 26th Inter-
national Conference on Machine Learning (ICML),
pages 1025?1032.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-06),
Prague, Czech Republic, pages 985?992.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189208.
184
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CNGL-DCU-Prompsit Translation Systems for WMT13
Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,
Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland
?Prompsit Language Engineering, Spain
?ICT, Chinese Academy of Sciences, China
?,[CNGL, Dublin City University, Ireland
?,?{rrubino, atoral, xfwu, qliu}@computing.dcu.ie
?santiago@prompsit.com
?junxie@ict.ac.cn
[stephen.doherty@dcu.ie
Abstract
This paper presents the experiments con-
ducted by the Machine Translation group
at DCU and Prompsit Language Engineer-
ing for the WMT13 translation task. Three
language pairs are considered: Spanish-
English and French-English in both direc-
tions and German-English in that direc-
tion. For the Spanish-English pair, the use
of linguistic information to select paral-
lel data is investigated. For the French-
English pair, the usefulness of the small in-
domain parallel corpus is evaluated, com-
pared to an out-of-domain parallel data
sub-sampling method. Finally, for the
German-English system, we describe our
work in addressing the long distance re-
ordering problem and a system combina-
tion strategy.
1 Introduction
This paper presents the experiments conducted
by the Machine Translation group at DCU1 and
Prompsit Language Engineering2 for the WMT13
translation task on three language pairs: Spanish-
English, French-English and German-English.
For these language pairs, the language and trans-
lation models are built using different approaches
and datasets, thus presented in this paper in sepa-
rate sections.
In Section 2, the systems built for the Spanish-
English pair in both directions are described. We
investigate the use of linguistic information to se-
lect parallel data. In Section 3, we present the sys-
tems built for the French-English pair in both di-
1http://www.nclt.dcu.ie/mt/
2http://www.prompsit.com/
rections. The usefulness of the small in-domain
parallel corpus is evaluated, compared to an out-
of-domain parallel data sub-sampling method. In
Section 4, for the German-English system, aiming
at exploring the long distance reordering problem,
we first describe our efforts in a dependency tree-
to-string approach, before combining different hi-
erarchical systems with a phrase-based system and
show a significant improvement over three base-
line systems.
2 Spanish-English
This section describes the experimental setup for
the Spanish-English language pair.
2.1 Setting
Our setup uses the MOSES toolkit, version
1.0 (Koehn et al, 2007). We use a pipeline
with the phrase-based decoder with standard pa-
rameters, unless noted otherwise. The decoder
uses cube pruning (-cube-pruning-pop-limit 2000
-s 2000), MBR (-mbr-size 800 -mbr-scale 1) and
monotone at punctuation reordering.
Individual language models (LMs), 5-gram and
smoothed using a simplified version of the im-
proved Kneser-Ney method (Chen and Goodman,
1996), are built for each monolingual corpus using
IRSTLM 5.80.01 (Federico et al, 2008). These
LMs are then interpolated with IRSTLM using
the test set of WMT11 as the development set. Fi-
nally, the interpolated LMs are merged into one
LM preserving the weights using SRILM (Stol-
cke, 2002).
We use all the parallel corpora available for
this language pair: Europarl (EU), News Com-
mentary (NC), United Nations (UN) and Common
Crawl (CC). Regarding monolingual corpora, we
use the freely available monolingual corpora (Eu-
213
roparl, News Commentary, News 2007?2012) as
well as the target side of several parallel corpora:
Common Crawl, United Nations and 109 French?
English corpus (only for English as target lan-
guage). Both the parallel and monolingual data
are tokenised and truecased using scripts from the
MOSES toolkit.
2.2 Data selection
The main contribution in our participation regards
the selection of parallel data. We follow the
perplexity-based approach to filter monolingual
data (Moore and Lewis, 2010) extended to filter
parallel data (Axelrod et al, 2011). In our case, we
do not measure perplexity only on word forms but
also using different types of linguistic information
(lemmas and named entities) (Toral, 2013).
We build LMs for the source and target sides
of the domain-specific corpus (in our case NC)
and for a random subset of the non-domain-
specific corpus (EU, UN and CC) of the same size
(number of sentences) of the domain-specific cor-
pus. Each parallel sentence s in the non-domain-
specific corpus is then scored according to equa-
tion 1 where PPIsl(s) is the perplexity of s in
the source side according to the domain-specific
LM and PPOsl(s) is the perplexity of s in the
source side according to the non-domain-specific
LM. PPItl(s) and PPOtl(s) contain the corre-
sponding values for the target side.
score(s) = 12 ? (PPIsl(s)? PPOsl(s))
+(PPItl(s)? PPOtl(s)) (1)
Table 1 shows the results obtained using four
models: word forms (forms), forms and named en-
tities (forms+nes), lemmas (lem) and lemmas and
named entities (lem+nes). Details on these meth-
ods can be found in Toral (2013).
For each corpus we selected two subsets (see in
bold in Table 1), the one for which one method
obtained the best perplexity (top 5% of EU us-
ing forms, 2% of UN using lemmas and 50% of
CC using forms and named entities) and a big-
ger one used to compare the performance in SMT
(top 14% of EU using lemmas and named entities
(lem+nes), top 12% of UN using forms and named
entities and the whole CC). These subsets are used
as training data in our systems.
As we can see in the table, the use of lin-
guistic information allows to obtain subsets with
lower perplexity than using solely word forms, e.g.
1057.7 (lem+nes) versus 1104.8 (forms) for 14%
of EU. The only exception to this is the subset that
comprises the top 5% of EU, where perplexity us-
ing word forms (957.9) is the lowest one.
corpus size forms forms+nes lem lem+nes
EU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7
UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6
CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1
Table 1: Perplexities in data selection
2.3 Results
Table 2 presents the results obtained. Note that
these were obtained during development and thus
the systems are tuned on WMT?s 2011 test set and
tested on WMT?s 2012 test set.
All the systems share the same LM. The first
system (no selection) is trained with the whole NC
and EU. The second (small) and third (big) sys-
tems use as training data the whole NC and sub-
sets of EU (5% and 14%, respectively), UN (2%
and 12%, respectively) and CC (50% and 100%,
respectively), as shown in Table 1.
System #sent. BLEU BLEUcased
no selection 2.1M 31.99 30.96
small 1.4M 33.12 32.05
big 3.8M 33.49 32.43
Table 2: Number of sentences and BLEU scores
obtained on the WMT12 test set for the different
systems on the EN?ES translation task.
The advantage of data selection is clear. The
second system, although smaller in size compared
to the first (1.4M sentence pairs versus 2.1M),
takes its training from a more varied set of data,
and its performance is over one absolute BLEU
point higher.
When comparing the two systems that rely on
data selection, one might expect the one that uses
data with lower perplexity (small) to perform bet-
ter. However, this is not the case, the third system
(big) performing around half an absolute BLEU
point higher than the second (small). This hints
at the fact that perplexity alone is not an optimal
metric for data selection, but size should also be
considered. Note that the size of system 3?s phrase
table is more than double that of system 2.
214
3 French-English
This section describe the particularities of the MT
systems built for the French-English language pair
in both directions. The goal of the experimen-
tal setup presented here is to evaluate the gain of
adding small in-domain parallel data into a trans-
lation system built on a sub-sample of the out-of-
domain parallel data.
3.1 Data Pre-processing
All the available parallel and monolingual data for
the French-English language pair, including the
last versions of LDC Gigaword corpora, are nor-
malised and special characters are escaped using
the scripts provided by the shared task organisers.
Then, the corpora are tokenised and for each lan-
guage a true-case model is built on the concatena-
tion of all the data after removing duplicated sen-
tences, using the scripts included in MOSES dis-
tribution. The corpora are then true-cased before
being used to build the language and the transla-
tion models.
3.2 Language Model
To build our final language models, we first build
LMs on each corpus individually. All the monolin-
gual corpora are considered, as well as the source
or target side of the parallel corpora if the data
are not already in the monolingual data. We build
modified Kneser-Ney discounted 5-gram LMs us-
ing the SRILM toolkit for each corpus and sepa-
rate the LMs in three groups: one in-domain (con-
taining news-commentary and news crawl cor-
pora), another out-of-domain (containing Com-
mon Crawl, Europarl, UN and 109 corpora), and
the last one with LDC Gigaword LMs (the data
are kept separated by news source, as distributed
by LDC). The LMs in each group are linearly in-
terpolated based on their perplexities obtained on
the concatenation of all the development sets from
previous WMT translation tasks. The same devel-
opment corpus is used to linearly interpolate the
in-domain and LDC LMs. We finally obtain two
LMs, one containing out-of-domain data which is
only used to filter parallel data, and another one
containing in-domain data which is used to filter
parallel data, tuning the translation model weights
and at decoding time. Details about the number of
n-grams in each language model are presented in
Table 3.
French English
out in out in
1-gram 4.0 3.3 4.2 10.7
2-gram 43.0 44.0 48.2 161.9
3-gram 54.2 61.8 63.4 256.8
4-gram 99.7 119.2 103.2 502.7
5-gram 136.4 165.0 125.4 680.7
Table 3: Number of n-grams (in millions) for the
in-domain and out-of-domain LMs in French and
English.
3.3 Translation Model
Two phrase-based translation models are built
using MGIZA++ (Gao and Vogel, 2008) and
MOSES3, with the default alignment heuris-
tic (grow-diag-final) and bidirectional reordering
models. The first translation model is in-domain,
built with the news-commentary corpus. The sec-
ond one is built on a sample of all the other paral-
lel corpora available for the French-English lan-
guage pair. Both corpora are cleaned using the
script provided with Moses, keeping the sentences
with a length below 80 words. For the second
translation model, we used the modified Moore-
Lewis method based on the four LMs (two per
language) presented in section 3.2. The sum of
the source and target perplexity difference is com-
puted for each sentence pair of the corpus. We set
an acceptance threshold to keep a limited amount
of sentence pairs. The kept sample finally con-
tains ? 3.7M sentence pairs to train the translation
model. Statistics about this data sample and the
news-commentary corpus are presented in Table 4.
The test set of WMT12 translation task is used to
optimise the weights for the two translation mod-
els with the MERT algorithm. For this tuning step,
the limit of target phrases loaded per source phrase
is set to 50. We also use a reordering constraint
around punctuation marks. The same parameters
are used during the decoding of the test set.
news-commentary sample
tokens FR 4.7M 98.6M
tokens EN 4.0M 88.0M
sentences 156.5k 3.7M
Table 4: Statistics about the two parallel corpora,
after pre-processing, used to train the translation
models.
3Moses version 1.0
215
3.4 Results
The two translation models presented in Sec-
tion 3.3 allow us to design three translation sys-
tems: one using only the in-domain model, one
using only the model built on the sub-sample of
the out-of-domain data, and one using both mod-
els by giving two decoding paths to Moses. For
this latter system, the MERT algorithm is also used
to optimise the translation model weights. Results
obtained on the WMT13 test set, measured with
the official automatic metrics, are presented in Ta-
ble 5. The submitted system is the one built on
the sub-sample of the out-of-domain parallel data.
This system was chosen during the tuning step be-
cause it reached the highest BLEU scores on the
development corpus, slightly above the combina-
tion of the two translation models.
News-Com. Sample Comb.
FR-EN
BLEUdev 26.9 30.0 29.9
BLEU 27.0 30.8 30.4
BLEUcased 26.1 29.8 29.3
TER 62.9 58.9 59.3
EN-FR
BLEUdev 27.1 29.7 29.6
BLEU 26.6 29.6 29.4
BLEUcased 25.8 28.7 28.5
TER 65.1 61.8 62.0
Table 5: BLEU and TER scores obtained by our
systems. BLEUdev is the score obtained on the
development set given by MERT, while BLEU,
BLEUcased and TER are obtained on the test set
given by the submission website.
For both FR-EN and EN-FR tasks, the best re-
sults are reached by the system built on the sub-
sample taken from the out-of-domain parallel data.
Using only News-Commentary to build a trans-
lation model leads to acceptable BLEU scores,
with regards to the size of the training corpus.
When the sub-sample of the out-of-domain par-
allel data is used to build the translation model,
adding a model built on News-Commentary does
not improve the results. The difference between
these two systems in terms of BLEU score (both
cased sensitive and insensitive) indicates that sim-
ilar results can be achieved, however it appears
that the amount of sentence pairs in the sample
is large enough to limit the impact of the small
in-domain corpus parallel. Further experiments
are still required to determine the minimum sam-
ple size needed to outperform both the in-domain
system and the combination of the two translation
models.
4 German-English
In this section we describe our work on German
to English subtask. Firstly we describe the De-
pendency tree to string method which we tried but
unfortunately failed due to short of time. Secondly
we discuss the baseline system and the preprocess-
ing we performed. Thirdly a system combination
method is described.
4.1 Dependency Tree to String Method
Our original plan was to address the long distance
reordering problem in German-English transla-
tion. We use Xie?s Dependency tree to string
method(Xie et al, 2011) which obtains good re-
sults on Chinese to English translation and ex-
hibits good performance at long distance reorder-
ing as our decoder.
We use Stanford dependency parser4 to parse
the English side of the data and Mate-Tool5 for
the German side. The first set of experiments did
not lead to encouraging results and due to insuffi-
cient time, we decide to switch to other decoders,
based on statistical phrase-based and hierarchical
approaches.
4.2 Baseline System
In this section we describe the three baseline sys-
tem we used as well as the preprocessing technolo-
gies and the experiments set up.
4.2.1 Preprocessing and Corpus
We first use the normalisation scripts provided by
WMT2013 to normalise both English and Ger-
man side. Then we escape special characters on
both sides. We use Stanford tokeniser for English
and OpenNLP tokeniser6 for German. Then we
train a true-case model using with Europarl and
News-Commentary corpora, and true-case all the
corpus we used. The parallel corpus is filtered
with the standard cleaning scripts provided with
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://code.google.com/p/mate-tools/
6http://opennlp.sourceforge.net/
models-1.5/
216
MOSES. We split the German compound words
with jWordSplitter7.
All the corpus provided for the shared task are
used for training our translation models, while
WMT2011 and WMT2012 test sets are used to
tune the models parameters. For the LM, we
use all the monolingual data provided, including
LDC Gigaword. Each LM is trained with the
SRILM toolkit, before interpolating all the LMs
according to their weights obtained by minimiz-
ing the perplexity on the tuning set (WMT2011
and WMT2012 test sets). As SRILM can only
interpolate 10 LMs, we first interpolate a LM with
Europarl, News Commentary, News Crawl (2007-
2012, each year individually, 6 separate parts),
then we interpolate a new LM with this interpo-
lated LM and LDC Gigawords (we kept the Gi-
gaword subsets separated according to the news
sources as distributed by LDC, which leads to 7
corpus).
4.2.2 Three baseline systems
We use the data set up described by the former
subsection and build up three baseline systems,
namely PB MOSES (phrase-based), Hiero MOSES
(hierarchical) and CDEC (Dyer et al, 2010). The
motivation of choosing Hierarchical Models is to
address the German-English?s long reorder prob-
lem. We want to test the performance of CDEC and
Hiero MOSES and choose the best. PB MOSES is
used as our benchmark. The three results obtained
on the development and test sets for the three base-
line system and the system combination are shown
in the Table 6.
Development Test
PB MOSES 22.0 24.0
Hiero MOSES 22.1 24.4
CDEC 22.5 24.4
Combination 23.0 24.8
Table 6: BLEU scores obtained by our systems on
the development and test sets for the German to
English translation task.
From the Table 6 we can see that on develop-
ment set, CDEC performs the best, and its much
better than MOSES?s two decoder, but on test
set, Hiero MOSES and CDEC performs as well as
each other, and they both performs better than PB
Model.
7http://www.danielnaber.de/
jwordsplitter/
4.3 System Combination
We also use a word-level combination strat-
egy (Rosti et al, 2007) to combine the three trans-
lation hypotheses. To combine these systems, we
first use the Minimum Bayes-Risk (MBR) (Kumar
and Byrne, 2004) decoder to obtain the 5 best hy-
pothesis as the alignment reference for the Con-
fusion Network (CN) (Mangu et al, 2000). We
then use IHMM (He et al, 2008) to choose the
backbone build the CN and finally search for and
generate the best translation.
We tune the system parameters on development
set with Simple-Simplex algorithm. The param-
eters for system weights are set equal. Other pa-
rameters like language model, length penalty and
combination coefficient are chosen when we see a
good improvement on development set.
5 Conclusion
This paper presented a set of experiments con-
ducted on Spanish-English, French-English and
German-English language pairs. For the Spanish-
English pair, we have explored the use of linguistic
information to select parallel data and use this as
the training for SMT. However, the comparison of
the performance obtained using this method and
the purely statistical one (i.e. perplexity on word
forms) remains to be carried out. Another open
question regards the optimal size of the selected
data. As we have seen, minimum perplexity alone
cannot be considered an optimal metric since us-
ing a larger set, even if it has higher perplexity,
allowed us to obtain notably higher BLEU scores.
The question is then how to decide the optimal size
of parallel data to select.
For the French-English language pair, we inves-
tigated the usefulness of the small in-domain par-
allel data compared to out-of-domain parallel data
sub-sampling. We show that with a sample con-
taining ? 3.7M sentence pairs extracted from the
out-of-domain parallel data, it is not necessary to
use the small domain-specific parallel data. Fur-
ther experiments are still required to determine the
minimum sample size needed to outperform both
the in-domain system and the combination of the
two translation models.
Finally, for the German-English language pair,
we presents our exploitation of long ordering
problem. We compared two hierarchical models
with one phrase-based model, and we also use a
system combination strategy to further improve
217
the translation systems performance.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran) and through Science Foundation Ireland
as part of the CNGL (grant 07/CE/I1142).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57. Association for
Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word er-
ror minimization and other applications of confu-
sion networks. Computer Speech & Language,
14(4):373?400.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 228?235.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In John H. L. Hansen and
Bryan L. Pellom, editors, INTERSPEECH. ISCA.
Antonio Toral. 2013. Hybrid Selection of Language
Model Training Data Using Linguistic Information
and Perplexity. In Proceedings of the Second Work-
shop on Hybrid Approaches to Machine Translation
(HyTra), ACL 2013.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226. Association for Computational
Linguistics.
218
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 435?439,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU Participation in WMT2013 Metrics Task
Xiaofeng Wu?, Hui Yu?,Qun Liu?
?CNGL, Dublin City University, Ireland
?ICT, Chinese Academy of Sciences, China
?{xfwu, qliu}@computing.dcu.ie
?yuhui@ict.ac.cn
Abstract
In this paper, we propose a novel syntac-
tic based MT evaluation metric which only
employs the dependency information in
the source side. Experimental results show
that our method achieves higher correla-
tion with human judgments than BLEU,
TER, HWCM and METEOR at both sen-
tence and system level for all of the four
language pairs in WMT 2010.
1 Introduction
Automatic evaluation plays a more important role
in the evolution of machine translation. At the ear-
liest stage, the automatic evaluation metrics only
use the lexical information, in which, BLEU (Pap-
ineni et al, 2002) is the most popular one. BLEU
is simple and effective. Most of the researchers
regard BLEU as their primary evaluation metric
to develop and compare MT systems. However,
BLEU only employs the lexical information and
cannot adequately reflect the structural level sim-
ilarity. Translation Error Rate (TER) (Snover et
al., 2006) measures the number of edits required to
change the hypothesis into one of the references.
METEOR (Lavie and Agarwal, 2007), which de-
fines loose unigram matching between the hypoth-
esis and the references with the help of stem-
ming and Wordnet-looking-up, is also a lexical
based method and achieves the first-class human-
evaluation-correlation score. AMBER (Chen and
Kuhn, 2011; Chen et al, 2012) incorporates recall,
extra penalties and some text processing variants
on the basis of BLEU. The main weakness of all
the above lexical based methods is that they cannot
adequately reflect the structural level similarity.
To overcome the weakness of the lexical based
methods, many syntactic based metrics were pro-
posed. Liu and Gildea (2005) proposed STM, a
constituent tree based approach, and HWCM, a
dependency tree based approach.
Both of the two methods compute the similar-
ity between the sub-trees of the hypothesis and the
reference. Owczarzak et al(2007a; 2007b; 2007c)
presented a method using the Lexical-Functional
Grammar (LFG) dependency tree. MAXSIM
(Chan and Ng, 2008) and the method proposed
by Zhu et al(2010) also employed the syntac-
tic information in association with lexical infor-
mation.With the syntactic information which can
reflect structural information, the correlation with
the human judgments can be improved to a certain
extent.
As we know that the hypothesis is potentially
noisy, and these errors expand through the parsing
process. Thus the power of syntactic information
could be considerably weakened.
In this paper, we attempt to overcome the short-
coming of the syntactic based methods and pro-
pose a novel dependency based MT evaluation
metric. The proposed metric only employs the ref-
erence dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid the error propaga-
tion. In our metric, F-score is calculated using the
string of hypothesis and the dependency based n-
grams which are extracted from the reference de-
pendency tree.
Experimental results show that our method
achieves higher correlation with human judgments
than BLEU, HWCM, TER and METEOR at both
sentence level and system level for all of the four
language pairs in WMT 2010.
2 Background: HWCM
HWCM is a dependency based metric which ex-
tracts the headword chains, a sequence of words
which corresponds to a path in the dependency
tree, from both the hypothesis and the reference
dependency tree. The score of HWCM is obtained
435
Figure 1: The dependency tree of the reference
Figure 2: The dependency tree of the hypothesis
by formula (1).
HWCM = 1D
D?
n=1
?
g?chainn(hyp) countclip(g)?
g?chainn(hyp) count(g)
(1)
In formula (1), D is the maximum length of the
headword chain. chainn(hyp) denotes the set of
the headword chains with length of n in the tree of
hypothesis. count(g) denotes the number of times
g appears in the headword chain of the hypothe-
sis dependency tree and countclip(g) denotes the
clipped number of times when g appears in the the
headword chain of the reference dependency trees.
Clipped means that the count computed from the
headword chain of the hypothesis tree should not
exceed the maximum number of times when g oc-
curs in headword chain of any single reference
tree. The following are two sentences represent-
ing as reference and hypothesis, and Figure 1 and
Figure 2 are the dependency trees respectively.
reference: It is not for want of trying .
hypothesis: This is not for lack of trying .
In the example above, there are 8 1-word, 7 2-
word and 3 3-word headword chains in the hy-
pothesis dependency tree. The number of 1-word
and 2-word headword chains in the hypothesis tree
which can match their counterparts in the refer-
ence tree is 5 and 4 respectively. The 3-word head-
word chains in the hypothesis dependency tree are
is for lack, for lack of and lack of trying. Due to
the difference in the dependency structures, they
have no matches in the reference side.
3 A Novel Dependency Based MT
Evaluation Method
In this new method, we calculate F-score using the
string of hypothesis and the dep-n-grams which
are extracted from the reference dependency tree.
The new method is named DEPREF since it is
a DEPendency based method only using depen-
dency tree of REference to calculate the F-score.
In DEPREF, after the parsing of the reference sen-
tences, there are three steps below being carried
out. 1) Extracting the dependency based n-gram
(dep-n-gram) in the dependency tree of the refer-
ence. 2) Matching the dep-n-gram with the string
of hypothesis. 3) Obtaining the final score of a hy-
pothesis. The detail description of our method will
be found in paper (Liu et al, 2013) . We only give
the experiment results in this paper.
4 Experiments
Both the sentence level evaluation and the system
level evaluation are conducted to assess the per-
formance of our automatic metric. At the sentence
level evaluation, Kendall?s rank correlation coeffi-
cient ? is used. At the system level evaluation, the
Spearman?s rank correlation coefficient ? is used.
4.1 Data
There are four language pairs in our experiments
including German-to-English, Czech-to-English,
French-to-English and Spanish-to-English, which
are all derived from WMT2010. Each of the
four language pairs consists of 2034 sentences and
the references of the four language pairs are the
same. There are 24 translation systems for French-
to-English, 25 for German-to-English, 12 for
Czech-to-English and 15 for Spanish-to-English.
We parsed the reference into constituent tree by
Berkeley parser and then converted the constituent
tree into dependency tree by Penn2Malt 1. Pre-
sumably, we believe that the performance will be
even better if the dependency trees are manually
revised.
In the experiments, we compare the perfor-
mance of our metric with the widely used lexical
based metrics BLEU, TER, METEOR and a de-
pendency based metric HWCM. In order to make
a fair comparison with METEOR which is known
to perform best when external resources like stem
and synonym are provided, we also provide results
of DEPREF with external resources.
1http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
436
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.2554 0.2748 0.2805 0.2197
TER 0.2526 0.2907 0.2638 0.2105
HWCM
N=1 0.2067 0.2227 0.2188 0.2022
N=2 0.2587 0.2601 0.2408 0.2399
N=3 0.2526 0.2638 0.2570 0.2498
N=4 0.2453 0.2672 0.2590 0.2436
DEPREF 0.3337 0.3498 0.3190 0.2656
Table 1.A Sentence level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.3186 0.3482 0.3258 0.2745
DEPREF 0.3281 0.3606 0.3326 0.2834
Table 1.B Sentence level correlations of the metrics with stemming and synonym.
Table 1: The sentence level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 1.A.
4.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of the four language pairs and
the average scores are shown in Table 1.A (without
external resources) and Table 1.B (with stemming
and synonym), Our method performs best when
maximum length of dep-n-gram is set to 3, so we
present only the results when the maximum length
of dep-n-gram equals 3.
From Table 1.A, we can see that all our methods
are far more better than BLEU, TER and HWCM
when there is no external resources applied on all
of the four language pairs. In Table 1.B, external
resources is considered. DEPREF is also better
than METEOR on the four language pairs. From
the comparison between Table 1.A and Table 1.B,
we can conclude that external resources is help-
ful for DEPREF on most of the language pairs.
When comparing DEPREF without external re-
sources with METEOR, we find that DEPREF ob-
tains better results on Czech-English and German-
English.
4.3 System-level Evaluation
We also evaluated the metrics with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in DEPREF is correlated with the posi-
tion of the words, so the traditional way of com-
puting system level score, like what BLEU does,
is not feasible for DEPREF. Therefore, we resort
to the way of adding the sentence level scores to-
gether to obtain the system level score. At system
level evaluation, we employ Spearman?s rank cor-
relation coefficient ?. The correlations of the four
language pairs and the average scores are shown
in Table 2.A (without external resources) and Ta-
ble 2.B (with stem and synonym).
From Table 2.A, we can see that the correla-
tion of DEPREF is better than BLEU, TER and
HWCM on German-English, Spanish-English and
French-English. On Czech-English, our metric
DEPREF is better than BLEU and TER. In Table
2.B (with stem and synonym), DEPREF obtains
better results than METEOR on all of the language
pairs except one case that DEPREF gets the same
result as METEOR on Czech-English. When com-
paring DEPREF without external resources with
METEOR, we can find that DEPREF gets bet-
ter result than METEOR on Spanish-English and
French-English.
From Table 1 and Table 2, we can conclude
that, DEPREF without external resources can ob-
tain comparable result with METEOR, and DE-
PREF with external resources can obtain better re-
sults than METEOR. The only exception is that at
the system level evaluation, Czech-English?s best
score is abtained by HWCM. Notice that there are
only 12 systems in Czech-English, which means
there are only 12 numbers to be sorted, we believe
437
Metrics Czech-English German-English Spanish-English French-English
BLEU 0.8400 0.8808 0.8681 0.8391
TER 0.7832 0.8923 0.9033 0.8330
HWCM
N=1 0.8392 0.7715 0.7231 0.6730
N=2 0.8671 0.8600 0.7670 0.8026
N=3 0.8811 0.8831 0.8286 0.8209
N=4 0.8811 0.9046 0.8242 0.8148
DEPREF 0.8392 0.9238 0.9604 0.8687
Table 2.A System level correlations of the metrics without external resources.
Metrics Czech-English German-English Spanish-English French-English
METEOR 0.8392 0.9269 0.9516 0.8652
DEPREF 0.8392 0.9331 0.9692 0.8730
Table 2.B System level correlations of the metrics with stemming and synonym.
Table 2: The system level correlations with the human judgments for Czech-to-English, German-to-
English, Spanish-to-English and French-to-English. The number in bold is the maximum value in each
column. N stands for the max length of the headword chains in HWCM in Table 2.A.
the spareness issure is more serious in this case.
5 Conclusion
In this paper, we propose a new automatic MT
evaluation method DEPREF. The experiments are
carried out at both sentence-level and system-level
using four language pairs from WMT 2010. The
experiment results indicate that DEPREF achieves
better correlation than BLEU, HWCM, TER and
METEOR at both sentence level and system level.
References
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Boxing Chen and Roland Kuhn. 2011. Amber: A
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 71?77, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an mt evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, WMT ?12, pages 59?63, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Q. Liu, H. Yu, X. Wu, J. Xie, Y. Lu, and S. Lin.
2013. A Novel Dependency Based MT Evaluation
Method. Under Review.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
438
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
439
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 59?65,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Parallel FDA5 for Fast Deployment of Accurate
Statistical Machine Translation Systems
Ergun Bic?ici
Centre for Next Generation Localisation
School of Computing
Dublin City University
ergun.bicici@computing.dcu.ie
Qun Liu
Centre for Next Generation Localisation
School of Computing
Dublin City University
qliu@computing.dcu.ie
Andy Way
Centre for Next Generation Localisation
School of Computing
Dublin City University
away@computing.dcu.ie
Abstract
We use parallel FDA5, an efficiently pa-
rameterized and optimized parallel im-
plementation of feature decay algorithms
for fast deployment of accurate statistical
machine translation systems, taking only
about half a day for each translation di-
rection. We build Parallel FDA5 Moses
SMT systems for all language pairs in
the WMT14 translation task and obtain
SMT performance close to the top Moses
systems with an average of 3.49 BLEU
points difference using significantly less
resources for training and development.
1 Introduction
Parallel FDA5 is developed for fast deployment
of accurate statistical machine translation systems
using an efficiently parameterized and optimized
parallel implementation of feature decay algo-
rithms (Bic?ici and Yuret, 2014). Parallel FDA5
takes about half a day for each translation direc-
tion. We achieve SMT performance that is on par
with the top constrained Moses SMT systems.
Statistical machine translation (SMT) is a data
intensive problem. If you have the translations for
the source sentences you are translating in your
training set or even portions of it, then the trans-
lation task becomes easier. If some tokens are not
found in the training data then you cannot trans-
late them and if some translated word do not ap-
pear in your language model (LM) corpus, then it
becomes harder for the SMT engine to find its cor-
rect position in the translation. The importance of
parallel FDA5 increases with the proliferation of
training material available for building SMT sys-
tems. Table 2 presents the statistics of the avail-
able training and LM corpora for the constrained
(C) systems as well as the statistics of the Parallel
FDA5 selected training and LM corpora.
Parallel FDA5 runs separate FDA5 models on
randomized subsets of the training data and com-
bines the selections afterwards. We run parallel
FDA5 SMT experiments using Moses (Koehn et
al., 2007) in all language pairs in WMT14 (Bojar
et al., 2014) and obtain SMT performance close to
the top constrained Moses systems training using
all of the training material. Parallel FDA5 allows
rapid prototyping of SMT systems for a given tar-
get domain or task and can be very useful for MT
in target domains with limited resources or in dis-
aster and crisis situations (Lewis et al., 2011).
2 Parallel FDA5 for Instance Selection
2.1 FDA5
FDA is developed mainly for building high per-
formance SMT systems using fewer yet relevant
data that is selected for increasing the coverage of
the test set features while maximizing their diver-
sity (Bic?ici and Yuret, 2011; Bic?ici, 2011). Par-
allel FDA parallelize instance selection and sig-
nificantly reduces the time to deploy accurate MT
systems in the presence of large training data from
weeks to half a day and still achieve state-of-
the-art SMT performance (Bic?ici, 2013). FDA5
is developed for efficient parameterization, opti-
mization, and implementation of FDA (Bic?ici and
Yuret, 2014). FDA5 can be used in both trans-
ductive learning scenarios where test set is used to
select the training data or in active learning sce-
narios where training set itself is used to obtain a
sorting of the training data and select.
We run transductive learning experiments in
this work such that the instance selection is per-
formed for the given test set. According to
SMT experiments performed on the 2 million sen-
tence English-German section of the Europarl cor-
pus (Bic?ici and Yuret, 2014), FDA5 can increase
the performance by 0.41 BLEU points compared
to using all of the available training data and by
59
Algorithm 1: Parallel FDA5
Input: Parallel training sentences U , test set
features F , and desired number of
training instances N .
Output: Subset of the parallel sentences to be
used as the training data L ? U .
1 U ? shuffle(U)
2 U ,M ? split(U , N)
3 L? {}
4 foreach U
i
? U do
5 ?L
i
, s
i
? ? FDA5(U
i
,F ,M)
6 L? L ? ?L
i
, s
i
?
7 L ? merge(L)
3.22 BLEU points compared to random selection.
FDA5 is also used for selecting the training set
in the WMT14 medical translation task (Calixto
et al., 2014) and the tuning set in the WMT14
German-English translation task (Li et al., 2014).
FDA5 has 5 parameters that effect the instance
scores based on the three formulas used:
? Initialization:
init(f) = log(|U|/C
U
(f))
i
|f |
l
(1)
? Decay:
decay(f) = init(f)(1+C
L
(f))
?c
d
C
L
(f)
(2)
? Sentence score:
sentScore(S) =
1
|S|
s
?
f?F (S)
fvalue(f)
(3)
C
L
(f) returns the count of feature f in L. d
is the feature score polynomial decay factor, c is
the feature score exponential decay factor, s is
the sentence score length exponent, i is the initial
feature score idf exponent, and l is the initial
feature score n-gram length exponent. FDA5 is
available at http://github.com/bicici/FDA
and the FDA5 optimizer is available at
http://github.com/bicici/FDAOptimization.
2.2 Parallel FDA5
Parallel FDA5 (ParFDA5) is presented in Algo-
rithm 1, which first shuffles the training sentences,
U and runs individual FDA5 models on the multi-
ple splits from which equal number of sentences,
M , are selected. We use ParFDA5 for select-
ing parallel training data and LM data for build-
ing SMT systems. merge combines k sorted ar-
rays, L
i
, into one sorted array in O(Mk log k) us-
ing their scores, s
i
, where Mk is the total number
of elements in all of the input arrays.
1
ParFDA5
makes FDA5 more scalable to domains with large
training corpora and allows rapid deployment of
SMT systems. By selecting from random splits of
the original corpus, we work with different n-gram
feature distributions in each split and prevent fea-
ture values from becoming negligible, which can
enhance the diversity.
2.3 Language Model Data Selection
We select the LM training data with ParFDA5
based on the following observation (Bic?ici, 2013):
No word not appearing in the training
set can appear in the translation.
It is impossible for an SMT system to translate a
word unseen in the training corpus nor can it trans-
late it with a word not found in the target side of
the training set
2
. Thus we are only interested
in correctly ordering the words appearing in the
training corpus and collecting the sentences that
contain them for building the LM. At the same
time, a compact and more relevant LM corpus is
also useful for modeling longer range dependen-
cies with higher order n-gram models. We use
1-gram features for LM corpus selection since we
don?t know which phrases will be generated by the
translation model. After the LM corpus selection,
the target side of the parallel training data is added
to the LM corpus.
3 Results
We run ParFDA5 SMT experiments for all lan-
guage pairs in both directions in the WMT14
translation task (Bojar et al., 2014), which include
English-Czech (en-cs), English-German (en-de),
English-French (en-fr), English-Hindi (en-hi), and
English-Russian (en-ru). We true-case all of the
corpora, use 150-best lists during tuning, set the
LM order to a value between 7 and 10 for all lan-
guage pairs, and train the LM using SRILM (Stol-
cke, 2002). We set the maximum sentence length
filter to 126 and for GIZA++ (Och and Ney, 2003),
1
(Cormen et al., 2009), question 6.5-9. Merging k sorted
lists into one sorted list using a min-heap for k-way merging.
2
Unless the translation is a verbatim copy of the source.
60
S ? T
Training Data LM Data
Data #word S (M) #word T (M) #sent (K) SCOV TCOV #word (M) TCOV
en-cs C 253.5 223.4 16068 0.8282 0.7046 717.0 0.8539
en-cs ParFDA5 22.0 19.6 1205 0.8161 0.6062 325.8 0.8238
cs-en C 223.4 253.5 16068 0.7046 0.8282 5541.9 0.9552
cs-en ParFDA5 19.3 22.0 1205 0.7046 0.7581 351.0 0.9132
en-de C 116.0 109.5 4511 0.812 0.7101 1573.8 0.8921
en-de ParFDA5 16.7 16.8 845 0.8033 0.6316 206.9 0.8184
de-en C 109.5 116.0 4511 0.7101 0.812 5446.8 0.9525
de-en ParFDA5 17.8 19.6 845 0.7087 0.753 339.5 0.9082
en-fr C 1096.1 1287.8 40344 0.8885 0.9163 2534.5 0.9611
en-fr ParFDA5 22.6 26.6 1008 0.8735 0.8412 737.4 0.9491
fr-en C 1287.8 1096.1 40344 0.9163 0.8885 6255.8 0.9675
fr-en ParFDA5 20.9 19.3 1008 0.8963 0.7845 463.4 0.9282
en-hi C 3.4 5.0 306 0.5467 0.5986 36.3 0.7972
en-hi ParFDA5 3.3 4.9 254 0.5467 0.5976 41.2 0.8115
hi-en C 5.0 3.4 306 0.5986 0.5467 5350.4 0.9473
hi-en ParFDA5 5.0 3.3 284 0.5985 0.5466 966.8 0.9209
en-ru C 49.6 46.1 2531 0.7992 0.6823 590.8 0.8679
en-ru ParFDA5 19.6 18.6 1107 0.7991 0.6388 282.1 0.8447
ru-en C 46.1 49.6 2531 0.6823 0.7992 5380.6 0.9567
ru-en ParFDA5 16.6 19.4 1107 0.6821 0.7586 225.1 0.9009
Table 2: The data statistics for the available training and LM corpora for the constrained (C) submissions
compared with the ParFDA5 selected training and LM corpora statistics. #words is in millions (M) and
#sents is in thousands (K).
S ? T d c s i l
T
r
a
i
n
i
n
g
,
n
=
2
en-de 1.0 0.5817 1.4176 5.0001 -3.154
de-en 1.0 1.0924 1.3604 5.0001 -4.341
en-cs 1.0 0.0676 0.8299 5.0001 -0.8788
cs-en 1.0 1.5063 0.7777 3.223 -2.3824
en-ru 1.0 0.6519 1.6877 5.0001 -1.1888
ru-en 1.0 1.607 3.0001 0.0 -1.8247
en-hi 1.0 3.0001 3.0001 1.5701 -1.5699
hi-en 1.0 0.0 1.1001 5.0001 -0.8264
en-fr 1.0 0.8143 0.801 3.5996 -1.3394
fr-en 1.0 0.19 1.0106 5.0001 1.238
L
M
,
n
=
1
en-de 1.0 0.1924 1.0487 5.0001 4.9404
de-en 1.0 1.7877 3.0001 3.1213 -0.4147
en-cs 1.0 0.4988 1.1586 5.0001 -5.0001
cs-en 0.9255 0.2787 0.7439 3.7264 -2.0564
en-ru 1.0 1.4419 2.239 1.5543 -0.5097
ru-en 1.0 2.4844 3.0001 4.6669 3.7978
en-hi 1.0 0.0 0.0 5.0001 -4.944
hi-en 1.0 0.3053 3.0001 5.0001 4.1216
en-fr 1.0 3.0001 2.0452 3.0229 3.4364
fr-en 1.0 0.7467 0.7641 5.0001 5.0001
Table 1: Optimized ParFDA5 parameters for se-
lecting the training set using 2-grams or the LM
corpus using 1-grams.
max-fertility is set to 10, with the number of itera-
tions set to 7,3,5,5,7 for IBM models 1,2,3,4, and
the HMM model and 70 word classes are learned
over 3 iterations with the mkcls tool during train-
ing. The development set contains 5000 sentences,
2000 of which are randomly sampled from pre-
vious years? development sets (2008-2012) and
3000 come from the development set for WMT14.
3.1 Optimized ParFDA5 Parameters
Table 1 presents the optimized ParFDA5 parame-
ters obtained using the development set. Transla-
tion direction specific differences are visible. A
negative value for l shows that FDA5 prefers
shorter features, which we observe mainly when
the target language is English. We also observe
higher exponential decay rates when the target lan-
guage is mainly English. For optimizing the pa-
rameters for selecting LM corpus instances, we
still use a parallel corpus and instead of optimiz-
ing for TCOV, we optimize for SCOV such that
we select instances that are relevant for the target
training corpus but still maximize the coverage of
source features and be able to represent the source
sentences within a translation task. The selected
LM corpus is prepared for a translation task.
3.2 Data Selection
We select the same number of sentences with Par-
allel FDA (Bic?ici, 2013), which is roughly 15%
of the training corpus for en-de, 35% for ru-en,
6% for cs-en, and 2% for en-fr. After the training
set selection, we select the LM data using the tar-
get side of the training set as the target domain to
select LM instances for. For en and fr, we have
access to the LDC Gigaword corpora (Parker et
al., 2011; Graff et al., 2011), from which we ex-
tract only the story type news. We select 15 mil-
lion sentences for each LM not including the se-
61
S ? T
Time (Min) Space (MB)
ParFDA5 Moses
Overall
Moses
Train LM Total Train Tune Total PT LM ALL
en-cs 5 28 34 375 702 1162 1196 1871 5865 19746
cs-en 7 65 72 358 448 867 939 1808 4906 18650
en-de 8 29 38 302 1059 1459 1497 1676 2923 18313
de-en 8 85 93 358 474 924 1017 1854 5219 19247
en-fr 23 60 84 488 781 1372 1456 2309 9577 24362
fr-en 21 99 120 315 490 897 1017 1845 4888 17466
en-hi 2 9 11 91 366 511 522 269 817 4292
hi-en 1 36 37 91 330 467 504 285 9697 3845
en-ru 11 25 35 358 369 837 872 2174 4770 21283
ru-en 10 62 71 309 510 895 966 1939 2735 19537
Table 3: The space and time required for building the ParFDA5 Moses SMT systems. The sizes are in
MB and time in minutes. PT stands for the phrase table. ALL does not contain the size of the LM.
BLEUc
S ? en en? T
cs-en de-en fr-en hi-en ru-en en-cs en-de en-fr en-hi en-ru
WMT14C 0.288 0.28 0.35 0.139 0.318 0.21 0.201 0.358 0.111 0.287
ParFDA5 0.256 0.239 0.319 0.105 0.282 0.172 0.168 0.325 0.07 0.257
diff 0.032 0.041 0.031 0.034 0.036 0.038 0.033 0.033 0.041 0.03
LM order 9 9 9 9 9 9 9 7 10 9
Table 4: BLEUc for the top constrained result in WMT14 (WMT14C) and for ParFDA5 results, their
difference to WMT14C, and the LM order used are presented. Average difference is 3.49 BLEU points.
lected training set, which is added later. The statis-
tics of the ParFDA5 selected training data and the
available training data for the constrained transla-
tion task is given in Table 2. The size of the LM
corpora includes both the LDC and the monolin-
gual LM corpora provided by WMT14. Table 2
shows the significant size differences between the
constrained dataset (C) and the ParFDA5 selected
data. Table 2 also present the source and target
coverage (SCOV and TCOV) in terms of the 2-
grams of the test set observed in the training data
or the LM data. The quality of the training cor-
pus can be measured by TCOV, which is found to
correlate well with the BLEU performance achiev-
able (Bic?ici and Yuret, 2011; Bic?ici, 2011).
3.3 Computing Statistics
We quantify the time and space requirements for
running ParFDA5 SMT systems for each trans-
lation direction. The space and time required
for building the ParFDA5 Moses SMT systems
are given in Table 3 where the sizes are in MB
and the time in minutes. PT stands for the
phrase table. We used Moses version 2.1.1, from
www.statmt.org/moses. Building a ParFDA5
Moses SMT system takes about half a day.
3.4 Translation Results
The results of our two ParFDA5 SMT experiments
for each language pair and their tokenized BLEU
performance, BLEUc, together with the LM order
used and the top constrained submissions to the
WMT14 are given in Table 4
3
, which use phrase-
based Moses for comparison
4
. We observed sig-
nificant gains (+0.23 BLEU points) using higher
order LMs last year (Bic?ici, 2013) and therefore
we use LMs of order 7 to 10. The test set con-
tains 10,000 sentences and only 3000 of which are
used for evaluation, which can make the transduc-
tive learning application of ParFDA5 harder. In
the transductive learning setting, ParFDA5 is se-
lecting target test task specific SMT resources and
therefore, having irrelevant instances in the test set
may decrease the performance by causing FDA5
to select more domain specific data and less task
specific. ParFDA5 significantly reduces the time
required for training, development, and deploy-
ment of an SMT system for a given translation
3
We use the results from matrix.statmt.org.
4
Phrase-based Moses systems usually rank in the top 3.
62
ppl
OOV log OOV = ?19 log OOV = ?11
Translation T order train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red. train FDA5 FDA5 LM % red.
en-cs en
3
866 1205 525 0.39
1764 1731 938 0.47 1370 1218 805 0.41
4 1788 1746 877 0.51 1389 1229 753 0.46
5 1799 1752 868 0.52 1398 1233 745 0.47
6 1802 1753 867 0.52 1400 1234 744 0.47
cs-en cs
3
557 706 276 0.5
480 419 333 0.31 408 342 307 0.25
4 487 422 292 0.4 415 344 269 0.35
5 495 424 285 0.42 421 346 263 0.38
6 497 425 284 0.43 423 346 262 0.38
en-de en
3
1666 2116 744 0.55
1323 1605 747 0.44 831 890 607 0.27
4 1307 1596 689 0.47 821 885 560 0.32
5 1307 1596 680 0.48 822 885 553 0.33
6 1308 1596 679 0.48 822 885 552 0.33
de-en de
3
691 849 417 0.4
482 498 394 0.18 386 379 345 0.11
4 470 490 344 0.27 376 373 301 0.2
5 470 490 336 0.29 377 373 293 0.22
6 471 490 334 0.29 377 373 292 0.23
en-fr en
3
270 411 153 0.43
185 167 173 0.07 173 151 166 0.04
4 170 160 135 0.21 159 144 130 0.19
5 171 160 126 0.27 160 145 121 0.24
fr-en fr
3
306 604 179 0.42
349 325 275 0.21 320 275 261 0.19
4 338 321 235 0.3 310 271 224 0.28
5 342 322 228 0.33 314 272 217 0.31
en-hi en
3
2035 2123 950 0.53
242 246 114 0.53 168 168 96 0.43
4 237 241 87 0.63 164 165 73 0.55
5 238 242 78 0.67 165 165 66 0.6
6 239 242 75 0.68 165 165 64 0.62
hi-en hi
3
1842 1860 623 0.66
1894 1898 482 0.75 915 911 377 0.59
4 1910 1914 398 0.79 923 919 312 0.66
5 1915 1919 378 0.8 925 921 296 0.68
6 1915 1919 378 0.8 926 921 296 0.68
en-ru en
3
959 1176 585 0.39
1067 1171 668 0.37 814 840 566 0.3
4 1053 1159 603 0.43 803 831 511 0.36
5 1052 1159 591 0.44 802 831 501 0.38
6 1052 1159 588 0.44 802 831 498 0.38
ru-en ru
3
558 689 340 0.39
385 398 363 0.06 334 334 333 0.0
4 377 391 325 0.14 327 328 298 0.09
5 378 392 318 0.16 328 329 292 0.11
6 378 392 318 0.16 328 329 291 0.11
Table 5: Perplexity comparison of the LM built from the training corpus (train), ParFDA5 selected
training corpus (FDA5), and the ParFDA5 selected LM corpus (FDA5 LM). % red. column lists the
percentage of reduction.
task. The average difference to the top constrained
submission in WMT14 is 3.49 BLEU points. For
en-ru and en-cs, true-casing the LM using a true-
caser trained on all of the available training data
decreased the performance by 0.5 and 0.9 BLEU
points respectively and for cs-en and fr-en, in-
creased the performance by 0.2 and 0.5 BLEU
points. We use the true-cased LM results using
a true-caser trained on all of the available train-
ing data for all language pairs where for hi-en,
the true-caser is trained on the ParFDA5 selected
training data.
3.5 LM Data Quality
A LM training data selected for a given transla-
tion task allows us to train higher order language
models, model longer range dependencies better,
and at the same time, achieve lower perplexity
as given in Table 5. We compare the perplexity
of the ParFDA5 selected LM with a LM trained
on the ParFDA5 selected training data and a LM
trained using all of the available training corpora.
To be able to compare the perplexities, we take
the OOV tokens into consideration during calcu-
lations (Bic?ici, 2013). We present results for the
cases when we handle OOV words with a cost
of ?19 or ?11 each in Table 5. We are able to
achieve significant reductions in the number of
OOV tokens and the perplexity, reaching up to
66% reduction in the number of OOV tokens and
up to 80% reduction in the perplexity.
63
BLEUc
S ? en en? T
cs-en de-en fr-en ru-en en-cs en-de en-fr en-ru
ParFDA5 0.256 0.239 0.319 0.282 0.172 0.168 0.325 0.257
ParFDA 0.243 0.241 0.254 0.223 0.171 0.179 0.238 0.173
diff 0.013 -0.002 0.065 0.059 0.001 -0.011 0.087 0.084
Table 7: Parallel FDA5 WMT14 results compared with parallel FDA WMT13 results. Training set sizes
are given in millions (M) of words on the target side. Average difference is 3.7 BLEU points.
BLEUc
S ? en en? T
cs-en fr-en en-cs en-fr
ParFDA5 0.256 0.319 0.172 0.325
ParFDA5 15% 0.248 0.321 0.178 0.333
diff -0.008 0.002 0.006 0.008
Table 6: ParFDA5 results, ParFDA5 results using
15% of the training set, and their difference.
3.6 Using 15% of the Available Training Set
In the FDA5 results (Bic?ici and Yuret, 2014),
we found that selecting 15% of the best train-
ing set size maximizes the performance for the
English-German out-of-domain translation task
and achieves 0.41 BLEU points improvement over
a baseline system using all of the available train-
ing data. We run additional experiments select-
ing 15% of the training data for fr-en and cs-en
language pairs to see the effect of increased train-
ing sets selected with ParFDA5. The results are
given in Table 6 where most of the results improve.
The slight performance decrease for cs-en may be
due to using a true-caser trained on only the se-
lected training data. We observe larger gains in
the en? T translations.
3.7 ParFDA5 versus Parallel FDA
We compare this year?s results with the results
we obtained last year (Bic?ici, 2013) in Table 7.
The task setting is different in WMT14 since the
test set contains 10,000 sentences but only 3000
of these are used as the actual test set, which
can make the transductive learning application of
ParFDA5 harder. We select the same number of
instances for the training sets but 5 million more
instances for the LM corpus this year. The aver-
age difference to the top constrained submission
in WMT13 was 2.88 BLEU points (Bic?ici, 2013)
and this has increased to 3.49 BLEU points in
WMT14. On average, the performance improved
3.7 BLEU points when compared with ParFDA re-
sults last year. For the fr-en, en-fr, and en-ru trans-
lation directions, we observe increases in the per-
formance. This may be due to better modeling of
the target domain by better parameterization and
optimization that FDA5 is providing. We observe
some decrease in the performance in en-de and de-
en results. Since the training material remained
the same for WMT13 and WMT14 and the mod-
eling power of FDA5 increased, building a domain
specific rather than a task specific ParFDA5 model
may be the reason for the decrease.
4 Conclusion
We use parallel FDA5 for solving computational
scalability problems caused by the abundance of
training data for SMT models and LMs and still
achieve SMT performance that is on par with
the top performing SMT systems. Parallel FDA5
raises the bar of expectations from SMT with
highly accurate translations and lower the bar to
entry for SMT into new domains and tasks by al-
lowing fast deployment of SMT systems in about
half a day. Parallel FDA5 enables a shift from gen-
eral purpose SMT systems towards task adaptive
SMT solutions.
Acknowledgments
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283, Ed-
64
inburgh, Scotland, July. Association for Computa-
tional Linguistics.
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic?ici. 2011. The Regression Model of Machine
Translation. Ph.D. thesis, Koc? University. Supervi-
sor: Deniz Yuret.
Ergun Bic?ici. 2013. Feature decay algorithms for fast
deployment of accurate statistical machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Ond?rej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matou?s Mach?a?cek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Iacer Calixto, Ali Hosseinzadeh Vahid, Xiaojun Zhang,
Jian Zhang, Xiaofeng Wu, Andy Way, and Qun Liu.
2014. Experiments in medical translation shared
task at wmt 2014. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Baltimore,
USA, June. Association for Computational Linguis-
tics.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2009. Introduction to
Algorithms (3. ed.). MIT Press.
David Graff,
?
Angelo Mendonc?a, and Denise DiPersio.
2011. French Gigaword third edition, Linguistic
Data Consortium.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177?180, Prague, Czech Republic, June.
William Lewis, Robert Munro, and Stephan Vogel.
2011. Crisis mt: Developing a cookbook for mt
in crisis situations. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
501?511, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Liangyou Li, Xiaofeng Wu, Santiago Cortes Vaillo,
Jun Xie, Jia Xu, Andy Way, and Qun Liu. 2014.
The dcu-ictcas-tsinghua mt system at wmt 2014 on
german-english translation task. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901?904.
65
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136?141,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU-ICTCAS MT system at WMT 2014 on German-English
Translation Task
Liangyou Li
?
, Xiaofeng Wu
?
, Santiago Cort
?
es Va??llo
?
Jun Xie
?
, Andy Way
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.ie
xiejun@ict.ac.cn
Abstract
This paper describes the DCU submis-
sion to WMT 2014 on German-English
translation task. Our system uses phrase-
based translation model with several pop-
ular techniques, including Lexicalized
Reordering Model, Operation Sequence
Model and Language Model interpolation.
Our final submission is the result of sys-
tem combination on several systems which
have different pre-processing and align-
ments.
1 Introduction
On the German-English translation task of WMT
2014, we submitted a system which is built with
Moses phrase-based model (Koehn et al., 2007).
For system training, we use all provided
German-English parallel data, and conducted sev-
eral pre-processing steps to clean the data. In ad-
dition, in order to improve the translation quali-
ty, we adopted some popular techniques, includ-
ing three Lexicalized Reordering Models (Axel-
rod et al., 2005; Galley and Manning, 2008), a 9-
gram Operation Sequence Model (Durrani et al.,
2011) and Language Model interpolation on sev-
eral datasets. And then we use system combina-
tion on several systems with different settings to
produce the final outputs.
Our phrase-based systems are tuned with k-best
MIRA (Cherry and Foster, 2012) on development
set. We set the maximum iteration to be 25.
The Language Models in our systems are
trained with SRILM (Stolcke, 2002). We trained
Corpus Filtered Out (%)
Bilingual 7.17
Monolingual (English) 1.05
Table 1: Results of language detection: percentage
of filtered out sentences
a 5-gram model with Kneser-Ney discounting
(Chen and Goodman, 1996).
In the next sections, we will describe our system
in detail. In section 2, we will explain our pre-
processing steps on corpus. Then in section 3, we
will describe some techniques we have tried for
this task and the experiment results. In section 4,
our final configuration for submitted system will
be presented. And we conclude in the last section.
2 Pre-processing
We use all the training data for German-English
translation, including Europarl, News Commen-
tary and Common Crawl. The first thing we no-
ticed is that some Non-German and Non-English
sentences are included in our training data. So we
apply Language Detection (Shuyo, 2010) for both
monolingual and bilingual corpora. For mono-
lingual data (only including English sentences in
our task), we filter out sentences which are detect-
ed as other language with probability more than
0.999995. And for bilingual data, A sentence
pair is filtered out if the language detector detect-
s a different language with probability more than
0.999995 on either the source or the target. The
filtering results are given in Table 1.
In our experiment, German compound word-
s are splitted based on frequency (Koehn and
136
Knight, 2003). In addition, for both monolingual
and bilingual data, we apply tokenization, nor-
malizing punctuation and truecasing using Moses
scripts. For parallel training data, we also filter out
sentence pairs containing more than 80 tokens on
either side and sentence pairs whose length ratio
between source and target side is larger than 3.
3 Techniques
In our preliminary experiments, we take newstest
2013 as our test data and newstest 2008-2012 as
our development data. In total, we have more
than 10,000 sentences for tuning. The tuning step
would be very time-consuming if we use them al-
l. So in this section, we use Feature Decay Al-
gorithm (FDA) (Bic?ici and Yuret, 2014) to select
2000 sentences as our development set. Table 2
shows that system performance does not increase
with larger tuning set and the system using only
2K sentences selected by FDA is better than the
baseline tuned with all the development data.
In this section, alignment model is trained
by MGIZA++ (Gao and Vogel, 2008) with
grow-diag-final-and heuristic function.
And other settings are mostly default values in
Moses.
3.1 Lexicalized Reordering Model
German and English have different word order
which brings a challenge in German-English ma-
chine translation. In our system, we adopt three
Lexicalized Reordering Models (LRMs) for ad-
dressing this problem. They are word-based LRM
(wLRM), phrase-based LRM (pLRM) and hierar-
chal LRM (hLRM).
These three models have different effect on the
translation. Word-based and phrase-based LRMs
are focus on local reordering phenomenon, while
hierarchical LRM could be applied into longer re-
ordering problem. Figure 1 shows the differences
(Galley and Manning, 2008). And Table 3 shows
effectiveness of different LRMs.
In our system based on Moses, we
use wbe-msd-bidirectional-fe,
phrase-msd-bidirectional-fe and
hier-mslr-bidirectional-fe to specify
these three LRMs. From Table 2, we could see
that LRMs significantly improves the translation.
Figure 1: Occurrence of a swap according to
the three orientation models: word-based, phrase-
based, and hierarchical. Black squares represen-
t word alignments, and gray squares represen-
t blocks identified by phrase-extract. In (a), block
b
i
= (e
i
, f
a
i
) is recognized as a swap according to
all three models. In (b), b
i
is not recognized as a
swap by the word-based model. In (c), b
i
is rec-
ognized as a swap only by the hierarchical model.
(Galley and Manning, 2008)
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) (Durrani
et al., 2011) explains the translation procedure as
a linear sequence of operations which generates
source and target sentences in parallel. Durrani
et al. (2011) defined four translation operations:
Generate(X,Y), Continue Source Concept, Gener-
ate Source Only (X) and Generate Identical, as
well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward. These oper-
ations are described as follows.
? Generate(X,Y) make the words in Y and the
first word in X added to target and source
string respectively.
? Continue Source Concept adds the word in
the queue from Generate(X,Y) to the source
string.
? Generate Source Only (X) puts X in the
source string at the current position.
? Generate Identical generates the same word
for both sides.
? Insert Gap inserts a gap in the source side for
future use.
? Jump Back (W) makes the position for trans-
lation be the Wth closest gap to the current
position.
? Jump Forward moves the position to the in-
dex after the right-most source word.
137
Systems Tuning Set newstest 2013
Baseline ? 24.1
+FDA ? 24.2
+LRMs 24.0 25.4
+OSM 24.4 26.2
+LM Interpolation 24.6 26.4
+Factored Model ? 25.9
+Sparse Feature 25.6 25.9
+TM Combination 24.1 25.4
+OSM Interpolation 24.4 26.0
Table 2: Preliminary results on tuning set and test set (newstest 2013). All scores on test set are case-
sensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning
result. Baseline uses all the data from newstest 2008-2012 for tuning.
Systems Tuning Set (uncased) newstest 2013
Baseline+FDA ? 24.2
+wLRM 23.8 25.1
+pLRM 23.9 25.2
+hLRM 24.0 25.4
+pLRM 23.8 25.1
+hLRM 23.7 25.2
Table 3: System BLEU[%] scores when different LRMs are adopted.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is:
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
In this paper we train a 9-gram OSM on train-
ing data and integrate this model directly into log-
linear framework (OSM is now available to use
in Moses). Our experiment shows OSM improves
our system by about 0.8 BLEU (see Table 2).
3.3 Language Model Interpolation
In our baseline, Language Model (LM) is trained
on all the monolingual data provided. In this sec-
tion, we try to build a large language model by in-
cluding data from English Gigaword fifth edition
(only taking partial data with size of 1.6G), En-
glish side of UN corpus and English side of 10
9
French-English corpus. Instead of training a s-
ingle model on all data, we interpolate language
models trained on each subset (monolingual data
provided is splitted into three parts: News 2007-
2013, Europarl and News Commentary) by tuning
weights to minimize perplexity of language model
measured on the target side of development set.
In our experiment, after interpolation, the lan-
guage model doesn?t get a much lower perplexity,
but it slightly improves the system, as shown in
Table 2.
3.4 Other Tries
In addition to the techniques mentioned above, we
also try some other approaches. Unfortunately al-
l of these methods described in this section are
non-effective in our experiments. The results are
shown in Table 2.
? Factored Model (Koehn and Hoang, 2007):
We tried to integrate a target POS factored
model into our system with a 9-gram POS
language model to address the problem of
word selection and word order. But ex-
periment doesn?t show improvement. The
English POS is from Stanford POS Tagger
(Toutanova et al., 2003).
? Translation Model Combination: In this ex-
periment, we try to use the method of (Sen-
nrich, 2012) to combine phrase tables or re-
ordering tables from different subsets of data
138
to minimize perplexity measured on develop-
ment set. We try to split the training data in
two ways. One is according to data source,
resulting in three subsets: Europarl, News
Commentary and Common Crawl. Another
one is to use data selection. We use FDA to
select 200K sentence pairs as in-domain data
and the rest as out-domain data. Unfortunate-
ly both experiments failed. In Table 2, we on-
ly report results of phrase table combination
on FDA-based data sets.
? OSM Interpolation: Since OSM in our sys-
tem could be taken as a special language
model, we try to use the idea of interpolation
similar with language model to make OSM
adapted to some data. Training data are s-
plitted into two subsets with FDA. We train
9-gram OSM on each subsets and interpolate
them according to OSM trained on the devel-
opment set.
? Sparse Features: For each source phrase,
there is usually more than one corresponding
translation option. Each different translation
may be optimal in different contexts. Thus
in our systems, similar to (He et al., 2008)
which proposed a Maximum Entropy-based
rule selection for the hierarchical phrase-
based model, features which describe the
context of phrases, are designed to select the
right translation. But different with (He et
al., 2008), we use sparse features to mod-
el the context. And instead of using syn-
tactic POS, we adopt independent POS-like
features: cluster ID of word. In our experi-
ment mkcls was used to cluster words into 50
groups. And all features are generalized to
cluster ID.
4 Submission
Based on our preliminary experiments in the sec-
tion above, we use LRMs, OSM and LM inter-
polation in our final system for newstest 2014.
But as we find that Language Models trained on
UN corpus and 10
9
French-English corpus have
a very high perplexity and in order to speed up
the translation by reducing the model size, in this
section, we interpolate only three language model-
s from monolingual data provided, English Giga-
word fifth edition and target side of training data.
In addition, we also try some different methods for
final submission. And the results are shown in Ta-
ble 4.
? Development Set Selection: Instead of using
FDA which is dependent on test set, we use
the method of (Nadejde et al., 2013) to se-
lect tuning set from newstest 2008-2013 for
the final system. We only keep 2K sentences
which have more than 30 words and higher
BLEU score. The experiment result is shown
in Table 4 ( The system is indicated as Base-
line).
? Pre-processing: In our preliminary exper-
iments, sentences are tokenized without
changing hyphen. Thus we build another sys-
tem where all the hyphens are tokenized ag-
gressively.
? SyMGIZA++: Better alignment could lead to
better translation. So we carry out some ex-
periments on SyMGIZA++ aligner (Junczys-
Dowmunt and Sza, 2012), which modifies the
original IBM/GIZA++ word alignment mod-
els to allow to update the symmetrized mod-
els between chosen iterations of the original
training algorithms. Experiment shows this
new alignment improves translation quality.
? Multi-alignment Selection: We also try to use
multi-alignment selection (Tu et al., 2012)
to generate a ?better? alignment from three
alignmens: MGIZA++ with function grow-
diag-final-and, SyMGIZA++ with function
grow-diag-final-and and fast alignment (Dy-
er et al., 2013). Although this method show
comparable or better result on development
set, it fails on test set.
Since we build a few systems with different
setting on Moses phrase-based model, a straight-
forward thinking is to obtain the better transla-
tion from several different translation systems. So
we use system combination (Heafield and Lavie,
2010) on the 1-best outputs of three systems (in-
dicated with
?
in table 4). And this results in our
best system so far, as shown in Table 4. In our final
submission, this result is taken as primary.
5 Conclusion
This paper describes our submitted system to
WMT 2014 in detail. This system is based on
139
Systems Tuning Set newstest 2014
Baseline
?
34.2 25.6
+SyMGIZA++
?
34.3 26.0
+Multi-Alignment Selection 34.4 25.6
+Hyphen-Splitted 33.9 25.9
+SyMGIZA++
?
34.0 26.0
+Multi-Alignment Selection 34.0 25.7
System Combination ? 26.5
Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and
case-insensitive BLEU[%] on tuning set which is directly from tuning result. Baseline is the phrase-based
system with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.
Systems indicated with
?
are used for system combination.
Moses phrase-based model, and integrates Lexi-
calized Reordering Models, Operation Sequence
Model and Language Model interpolation. Al-
so system combination is used on several system-
s which have different pre-processing and align-
ment.
Acknowledgments
This work is supported by EC Marie-Curie initial
training Network EXPERT (EXPloiting Empiri-
cal appRoaches to Translation) project (http:
//expert-itn.eu). Thanks to Johannes Lev-
eling for his help on German compound splitting.
And thanks to Jia Xu and Jian Zhang for their ad-
vice and help on this paper and experiments.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAA-
CL HLT ?12, pages 427?436, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36, January.
Marcin Junczys-Dowmunt and Arkadiusz Sza. 2012.
Symgiza++: Symmetrized word alignment models
for statistical machine translation. In Pascal Bouvry,
MieczysawA. Kopotek, Franck Leprvost, Magorza-
ta Marciniak, Agnieszka Mykowiecka, and Henryk
140
Rybiski, editors, Security and Intelligent Informa-
tion Systems, volume 7053 of Lecture Notes in Com-
puter Science, pages 379?390. Springer Berlin Hei-
delberg.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the Tenth Conference on European Chapter of the
Association for Computational Linguistics - Volume
1, EACL ?03, pages 187?193, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s syntax-based machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 539?549,
Avignon, France, April. Association for Computa-
tional Linguistics.
Nakatani Shuyo. 2010. Language detection library for
java.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference Spoken Language Processing, pages
901?904, Denver, CO.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
141
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 215?220,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
DCU-Lingo24 Participation in WMT 2014 Hindi-English Translation task
Xiaofeng Wu, Rejwanul Haque*, Tsuyoshi Okita
Piyush Arora, Andy Way, Qun Liu
CNGL, Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
{xf.wu,tokita,parora,away,qliu}@computing.dcu.ie
*Lingo24, Edinburgh, UK
rejwanul.haque@lingo24.com
Abstract
This paper describes the DCU-Lingo24
submission to WMT 2014 for the Hindi-
English translation task. We exploit
miscellaneous methods in our system,
including: Context-Informed PB-SMT,
OOV Word Conversion (OWC), Multi-
Alignment Combination (MAC), Oper-
ation Sequence Model (OSM), Stem-
ming Align and Normal Phrase Extraction
(SANPE), and Language Model Interpola-
tion (LMI). We also describe various pre-
processing steps we tried for Hindi in this
task.
1 Introduction
This paper describes the DCU-Lingo24 submis-
sion to WMT 2014 for the Hindi-English transla-
tion task.
All our experiments on WMT 2014 are built
upon the Moses phrase-based model (PB-SMT)
(Koehn et al., 2007) and tuned with MERT
(Och, 2003). Starting from this baseline system,
we exploit various methods including Context-
Informed PB-SMT (CIPBSMT), zero-shot learn-
ing (Palatucci et al., 2009) using neural network-
based language modelling (Bengio et al., 2000;
Mikolov et al., 2013) for OOV word conversion,
various lexical reordering models (Axelrod et al.,
2005; Galley and Manning, 2008), various Mul-
tiple Alignment Combination (MAC) (Tu et al.,
2012), Operation Sequence Model (OSM) (Dur-
rani et al., 2011) and Language Model Interpola-
tion(LMI).
In the next section, the preprocessing steps are
explained. In Section 3 a detailed explanation of
the technique we exploit is provided. Then in Sec-
tion 4, we provide our experimental results and re-
sultant discussion.
2 Pre-processing Steps
We use all the training data provided for Hindi?
English translation. Following Bojar et al. (2010),
we apply a number of normalisation methods on
the Hindi corpus. The HindEnCorp parallel cor-
pus compiles several sources of parallel data. We
observe that the source-side (Hindi) of the TIDES
data source contains font-related noise, i.e. many
Hindi sentences are a mixture of two different en-
codings: UTF-8
1
and WX
2
notations. We pre-
pared a WX-to-UTF-8 font conversion script for
Hindi which converts all WX encoded characters
into UTF-8, thus removing all WX encoding ap-
pearing in the TIDES data.
We also observe that a portion of the English
training corpus contained the following bracket-
like sequences of characters: -LRB-, -LSB-, -
LCB-, -RRB-, -RSB-, and -RCB-.
3
For consis-
tency, those character sequences in the training
data were replaced by the corresponding brackets.
For English ? both monolingual and the target
side of the bilingual data ? we perform tokeniza-
tion, normalization of punctuation, and truecasing.
For parallel training data, we filter sentences pairs
containing more than 80 tokens on either side and
1
http://en.wikipedia.org/wiki/UTF-8
2
http://en.wikipedia.org/wiki/WX_notation
3
The acronyms stand for (Left|Right)
(Round|Square|Curly) Bracket.
215
sentence pairs with length difference larger than 3
times.
3 Techniques Deployed
3.1 Combination of Various Lexical
Reordering Model (LRM)
Clearly, Hindi and English have quite different
word orders, so we adopt three lexical reordering
models to address this problem. They are word-
based LRM and phrase-based LRM, which mainly
focus on local reordering phenomena, and hierar-
chical phrase-based LRM, which mainly focuses
on longer distance reordering (Galley and Man-
ning, 2008).
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) of Dur-
rani et al. (2011) defines four translation opera-
tions: Generate(X,Y), Continue Source Concept,
Generate Source Only (X) and Generate Identical,
as well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is calculated as in (1):
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
We employ a 9-order OSM in our framework.
3.3 Language Model Interpolation (LMI)
We build a large language model by including data
from the English Gigaword fifth edition, the En-
glish side of the UN corpus, the English side of the
10
9
French?English corpus and the English side of
the Hindi?English parallel data provided by the or-
ganisers. We interpolate language models trained
using each dataset, with the monolingual data pro-
vided split into three parts (news 2007-2013, Eu-
roparl (?) and news commentary) and the weights
tuned to minimize perplexity on the target side of
the devset.
The language models in our systems are trained
with SRILM (Stolcke, 2002). We train a 5-gram
model with Kneser-Ney discounting (Chen and
Goodman, 1996).
3.4 Context-informed PB-SMT
Haque et al. (2011) express a context-dependent
phrase translation as a multi-class classification
problem, where a source phrase with given addi-
tional context information is classified into a dis-
tribution over possible target phrases. The size of
this distribution needs to be limited, and would
ideally omit irrelevant target phrase translations
that the standard PB-SMT (Koehn et al., 2003) ap-
proach would normally include. Following Haque
et al. (2011), we derive a context-informed feature
?
h
mbl
that is expressed as the conditional probabil-
ity of the target phrase e?
k
given the source phrase
?
f
k
and its context information (CI), as in (2):
?
h
mbl
= log P(e?
k
|
?
f
k
,CI(
?
f
k
)) (2)
Here, CI may include any feature that can pro-
vide useful information to disambiguate the given
source phrase. In our experiment, we use CCG su-
pertag (Steedman, 2000) as a contextual features.
CCG supertag expresses the specific syntactic be-
haviour of a word in terms of the arguments it
takes, and more generally the syntactic environ-
ment in which it appears.
We consider the CCG supertags of the context
words, as well as of the focus phrase itself. In our
model, the supertag of a multi-word focus phrase
is the concatenation of the supertags of the words
composing that phrase. We generate a window
of size 2l + 1 features (we set l:=2), including
the concatenated complex supertag of the focus
phrase. Accordingly, the supertag-based contex-
tual information (CI
st
) is described as in (3):
CI
st
(
?
f
k
) = {st(f
i
k
?l
), ..., st(f
i
k
?1
), st(
?
f
k
),
st(f
j
k
+1
), ..., st(f
j
k
+l
)}
(3)
For the Hindi-to-English translation task, we use
part-of-speech (PoS) tags
4
of the source phrase
and the neighbouring words as the contextual fea-
ture, owing to the fact that supertaggers are readily
available only for English.
We use a memory-based machine learning
(MBL) classifier (TRIBL: (Daelemans, 2005))
5
that is able to estimate P(e?
k
|
?
f
k
,CI(
?
f
k
)) by
similarity-based reasoning over memorized
nearest-neighbour examples of source?target
phrase translations. Thus, we derive the feature
?
h
mbl
defined in Equation (2). In addition to
?
h
mbl
,
4
In order to obtain PoS tags of Hindi words,
we used the LTRC shallow parser for Hindi from
http://ltrc.iiit.ac.in/analyzer/hindi/shallow-parser-hin-
4.0.fc8.tar.gz.
5
An implementation of TRIBL is freely available as part
of the TiMBL software package, which can be downloaded
from http://ilk.uvt.nl/timbl.
216
we derive a simple two-valued feature
?
h
best
,
defined in Equation (4):
?
h
best
=
{
1 if e?
k
maximizes P(e?
k
|
?
f
k
,CI(
?
f
k
))
u 0 otherwise
(4)
where
?
h
best
is set to 1 when e?
k
is one of the tar-
get phrases with highest probability according to
P(e?
k
|
?
f
k
,CI(
?
f
k
)) for each source phrase
?
f
k
; oth-
erwise
?
h
best
is set to 0.000001. We performed ex-
periments by integrating these two features
?
h
mbl
and
?
h
best
directly into the log-linear model of
Moses. Their weights are optimized using mini-
mum error-rate training (MERT)(Och, 2003) on a
held-out development set for each of the experi-
ments.
3.5 Morphological Segmentation
Haque et al. (2012) applied a morphological suffix
separation process in a Bengali-to-English trans-
lation task and showed that suffix separation sig-
nificantly reduces data sparseness in the Bengali
corpus. They also showed an SMT model trained
on the suffix-stripped training data significantly
outperforms the state-of-the-art PB-SMT baseline.
Like Bengali, Hindi is a morphologically very rich
and highly inflected Indian language. As done
previously for Bengali-to-English (Haque et al.,
2012), we employ a suffix-stripping method for
lemmatizing inflected Hindi words in the WMT
Hindi-to-English translation task. Following Das-
gupta and Ng (2006), we developed an unsu-
pervised morphological segmentation method for
Hindi. We also used a Hindi lightweight stem-
mer (Ramanathan and Rao, 2003) in order to pre-
pare a training corpus with only Hindi stems. We
prepared Hindi-to-English SMT systems on the
both types of training data (i.e. suffix-stripped and
stemmed).
6
3.6 Multi-Alignment Combination (MAC)
Word alignment is a critical component of MT
systems. Various methods for word alignment
have been proposed, and different models can pro-
duce signicantly different outputs. For example,
Tu et al. (2012) demonstrates that the alignment
agreement between the two best-known alignment
tools, namely Giza++(Och and Ney, 2003) and
6
Suffixes were separated and completely removed from
the training data.
the Berkeley aligner
7
(Liang et al., 2006), is be-
low 70%. Taking into consideration the small size
of the the corpus, in order to extract more ef-
fective phrase tables, we concatenate three align-
ments: Giza++ with grow-diag-final-and, Giza++
with intersection, and that derived from the Berke-
ley aligner.
3.7 Stemming Alignment and Normal Phrase
Extraction (SANPE)
The rich morphology of Hindi will cause word
alignment sparsity, which results in poor align-
ment quality. Furthermore, word stemming on
the Hindi side usually results in too many English
words being aligned to one stemmed Hindi word,
i.e. we encounter the problem of phrase over-
extraction. Therefore, we conduct word alignment
with the stemmed version of Hindi, and then at
the phrase extraction step, we replace the stemmed
form with the original Hindi form.
3.8 OOV Word Conversion Method
Our algorithm for OOV word conversion uses the
recently developed zero-shot learning (Palatucci
et al., 2009) using neural network language mod-
elling (Bengio et al., 2000; Mikolov et al., 2013).
The same technique is used in (Okita et al., 2014).
This method requires neither parallel nor compa-
rable corpora, but rather two monolingual corpora.
In our context, we prepare two monolingual cor-
pora on both sides, which are neither parallel nor
comparable, and a small amount of already known
correspondences between words on the source and
target sides (henceforth, we refer to this as the
?dictionary?). Then, we train both sides with the
neural network language model, and use a contin-
uous space representation to project words to each
other on the basis of a small amount of correspon-
dences in the dictionary. The following algorithm
shows the steps involved:
1. Prepare the monolingual source and target
sentences.
2. Prepare the dictionary which consists of U
entries of source and target sentences com-
prising non-stop-words.
3. Train the neural network language model on
the source side and obtain the real vectors of
X dimensions for each word.
7
http://code.google.com/p/berkeleyaligner/
217
4. Train the neural network language model on
the target side and obtain the real vectors of
X dimensions for each word.
5. Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary items in two continuous spaces us-
ing canonical component analysis (CCA).
In our experiments we use U the same as the en-
tries of Wiki corpus, which is provided among
WMT14 corpora, and X as 50. The resulted pro-
jection by this algorithm can be used as the OOV
word conversion which projects from the source
language which among OOV words into the tar-
get language. The overall algorithm which uses
the projection which we build in the above step is
shown in the following.
1. Collect unknown words in the translation out-
puts.
2. Do Hindi named-entity recognition (NER) to
detect noun phrases.
3. If they are noun phrases, do the projection
from each unknown word in the source side
into the target words (We use the projection
prepared in the above steps). If they are not
noun phrases, run the transliteration to con-
vert each of them.
We perform Hindi NER by training CRF++ (Kudo
et al., 2004) using the Hindi named entity corpus,
and use the Hindi shallow parser (Begum et al.,
2008) for preprocessing of the inputs.
4 Results and Discussion
4.1 Data
We conduct our experiments on the standard
datasets released in the WMT14 shared translation
task. We use HindEnCorp
8
(Bojar et al., 2014)
parallel corpus for MT system building. We also
used the CommonCrawl Hindi monolingual cor-
pus (Bojar et al., 2014) in order to build an addi-
tional language model for Hindi.
For the Hindi-to-English direction, we also em-
ployed monolingual English data used in the other
translation tasks for building the English language
model.
8
http://ufallab.ms.mff.cuni.cz/ bojar/hindencorp/
4.2 Moses Baseline
We employ a standard Moses PB-SMT model as
our baseline. The Hindi side is preprocessed but
unstemmed. We use Giza++ to perform word
alignment, the phrase table is extracted via the
grow-diag-final-and heuristic and the max-phrase-
length is set to 7.
4.3 Automatic Evaluation
Experiments BLEU
Moses Baseline 8.7
Context-Based 9.4
Context-Based + CommonCrawl LM 11.4
Table 1: BLEU scores of the English-to-Hindi MT
Systems on the WMT test set.
Experiments BLEU
Moses Baseline 10.1
Context-Based 10.1
Suffix-Stripped 10.0
OWC 11.2
OSM 10.3
Three LRMs 10.5
MAC 10.7
SANPE 10.6
LMI 10.9
LMI+SANPE+MAC+ThreeLRMs+OSM 11.7
Table 2: BLEU scores of the Hindi-to-English MT
Systems on the WMT test set.
We prepared a number of MT systems for both
English-to-Hindi and Hindi-to-English, and sub-
mitted their runs in the WMT 2014 Evaluation
Matrix. The BLEU scores of the different English-
to-Hindi MT systems (Moses Baseline, Context-
Based (CCG) MT system, and Context-Based
(CCG) MT system with an additional LM built
on the CommonCrawl Hindi monolingual corpus
(Bojar et al., 2014)) on the WMT 2014 English-
to-Hindi test set are reported in Table 1. As can
be seen from Table 1, Context-Based (CCG) MT
system produces 0.7 BLEU points improvement
(8.04% relative) over the Moses Baseline. When
we add an additional large LM built on the Com-
monCrawl data to the Context-Based (CCG) MT
system, we achieved a 2 BLEU-point improve-
ment (21.3% relative) (cf. last row in Table 1) over
218
the Context-Based (CCG) MT system.
9
The BLEU scores of the different Hindi-to-
English MT systems on the WMT 2014 Hindi-
to-English test set are reported in Table 2. The
first row of Table 2 shows the BLEU score for
the Baseline MT system. We note that the per-
formance of the Context-Based (PoS) MT system
obtains identical performance to the Moses base-
line (10.1 BLEU points) on the WMT 2014 Hindi-
to-English test set.
We employed a source language (Hindi) nor-
malisation technique, namely suffix separation,
but unfortunately this did not bring about any
improvement for the Hindi-to-English translation
task. The improvement gained by individually
employing OSM, three lexical reordering mod-
els, Multi-alignment Combination, Stem-align and
normal Phrase Extraction and Language Model In-
terpolation can be seen in Table 2. Our best sys-
tem is achieved by combining OSM, Three LMR,
MAC, SANPE and LMI, which results in a 1.6
BLEU point improvement over the Baseline.
5 Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Rafiya Begum, Samar Husain, Arun Dhwaj,
Dipti Misra Sharma, Lakshmi Bai, and Rajeev
Sangal. 2008. Dependency annotation scheme for
indian languages. In Proceedings of The Third In-
ternational Joint Conference on Natural Language
Processing (IJCNLP).
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems.
Ond Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in english-to-hindi machine translation.
In LREC.
9
Please note that this is an unconstrained submission.
Ondrej Bojar, V. Diatka, Rychly P., Pavel Stranak,
A. Tamchyna, and Daniel Zeman. 2014. Hindi-
english and hindi-only corpus for machine transla-
tion. In LREC.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Walter Daelemans. 2005. Memory-based language
processing. Cambridge University Press.
Sajib Dasgupta and Vincent Ng. 2006. Unsupervised
morphological parsing of bengali. Language Re-
sources and Evaluation, 40(3-4):311?330.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Rejwanul Haque, Sudip Kumar Naskar, Antal van den
Bosch, and Andy Way. 2011. Integrating source-
language context into phrase-based statistical ma-
chine translation. Machine translation, 25(3):239?
285.
Rejwanul Haque, Sergio Penkale, Jie Jiang, and Andy
Way. 2012. Source-side suffix stripping for bengali-
to-english smt. In Asian Language Processing
(IALP), 2012 International Conference on, pages
193?196. IEEE.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
219
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Appliying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. ArXiv.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way,
and Qun Liu. 2014. Dcu terminology translation
system for medical query subtask at wmt14.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), December.
Ananthakrishnan Ramanathan and Durgesh D Rao.
2003. A lightweight stemmer for hindi. In the Pro-
ceedings of EACL.
Mark Steedman. 2000. The syntactic process, vol-
ume 35. MIT Press.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference Spoken Language Processing,
pages 901?904, Denver, CO.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
220
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 239?245,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU Terminology Translation System for the Medical Query Subtask
at WMT14
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way, Qun Liu
Dublin City University, School of Computing
Glasnevin, Dublin 9
Ireland
{tokita,avahid,away,qliu}@computing.dcu.ie
Abstract
This paper describes the Dublin City
University terminology translation system
used for our participation in the query
translation subtask in the medical trans-
lation task in the Workshop on Statisti-
cal Machine Translation (WMT14). We
deployed six different kinds of terminol-
ogy extraction methods, and participated
in three different tasks: FR?EN and EN?
FR query tasks, and the CLIR task. We
obtained 36.2 BLEU points absolute for
FR?EN and 28.8 BLEU points absolute
for EN?FR tasks where we obtained the
first place in both tasks. We obtained 51.8
BLEU points absolute for the CLIR task.
1 Introduction
This paper describes the terminology translation
system developed at Dublin City University for
our participation in the query translation subtask at
the Workshop on Statistical Machine Translation
(WMT14). We developed six kinds of terminol-
ogy extraction methods for the problem of medi-
cal terminology translation, especially where rare
and new words are considered. We have several
motivations which we address before providing a
description of the actual algorithms undeprinning
our work.
First, terminology translation cannot be seen
just as a simple extension of the translation process
if we use an analogy from human translation. Ter-
minology translation can be considered as more
important and a quite different task than transla-
tion per se, so we need a considerably different
way of solving this particular problem. Bilingual
terminology selection has been claimed to be the
touchstone in human translation, especially where
scientific and legal translation are concerned. Ter-
minology selection is often the hardest and most
time-consuming process in the translation work-
flow. Depending on the particular requirements of
the use-case (Way, 2013), users may not object to
disfluent translations, but will invariably be very
sensitive to the wrong selection of terminology,
even if the meaning of the chosen terms is correct.
This is especially true if this selected terminology
does not match with that preferred by the users
themselves, in which case users are likely to ex-
press some kind of complaint; it may even be that
the entire translation is rejected as sub-standard or
inappropriate on such grounds.
Second, we look at how to handle new and rare
words. If we inspect the process of human trans-
lation more closely, it is easy to identify several
differences compared to the methods used in sta-
tistical MT (SMT). Unless stipulated by the client,
the selection of bilingual terminology can be a
highly subjective process. Accordingly, it is not
necessarily the bilingual term-pair with the highest
probability that is chosen by the human translator.
It is often the case that statistical methods often
forget about or delete less frequent n-grams, but
rely on more frequent n-grams using maximum
likelihood or Maximum A Priori (MAP) meth-
ods. If some terminology is highly suitable, a
human translator can use it quite freely. Further-
more, there are a lot of new words in reality for
which new target equivalents have to be created by
the translators themselves, so the question arises
as to how human translators actually select ap-
propriate new terminology. Transliteration, which
is often supported by many Asian languages in-
cluding Hindi, Japanese, and Chinese, is perhaps
the easiest things to do under such circumstances.
Slight modifications of alphabets/accented charac-
ters can sometimes successfully create a valid new
term, even for European languages.
The remainder of this paper is organized as fol-
lows. Section 2 describes our algorithms. Our
decoding strategy in Section 3. Our experimen-
239
tal settings and results are presented in Section 4,
and we conclude in Section 5.
2 Our Methods
Apart from the conventional statistical approach to
extract bilingual terminology, this medical query
task reminds us of two frequently occurring prob-
lems which are often ignored: (i) ?Can we forget
about terminology which occurs only once in a
corpus??, and (ii) ?What can we do if the termi-
nology does not occur in a corpus?? These two
problems require computationally quite different
approaches than what is usually done in the stan-
dard statistical approach. Furthermore, the medi-
cal query task in WMT14 provides a wide range of
corpora: parallel and monolingual corpora, as well
as dictionaries. These two interesting aspects mo-
tivate our extraction methods which we present in
this section, including one relatively new Machine
Learning algorithm of zero-shot learning arising
from recent developments in the neural network
community (Bengio et al., 2000; Mikolov et al.,
2013b).
2.1 Translation Model
Word alignment (Brown et al., 1993) and phrase
extraction (Koehn et al., 2003) can capture bilin-
gual word- and phrase-pairs with a good deal of
accuracy. We omit further details of these stan-
dard methods which are freely available elsewhere
in the SMT literature (e.g. (Koehn, 2010)).
2.2 Extraction from Parallel Corpora
(Okita et al., 2010) addressed the problem of
capturing bilingual term-pairs from parallel data
which might otherwise not be detected by the
translation model. Hence, the requirement in
Okita et al. is not to use SMT/GIZA++ (Och and
Ney, 2003) to extract term-pairs, which are the
common focus in this medical query translation
task.
The classical algorithm of (Kupiec, 1993) used
in (Okita et al., 2010) counts the statistics of ter-
minology c(e
term
i
, f
term
j
|s
t
) on the source and
the target sides which jointly occur in a sentence
s
t
after detecting candidate terms via POS tag-
ging, which are then summed up over the entire
corpus
?
N
t=1
c(e
term
i
, f
term
j
|s
t
). Then, the al-
gorithm adjusts the length of e
term
i
and f
term
j
.
It can be said that this algorithm captures term-
pairs which occur rather frequently. However, this
apparent strength can also be seen in disadvanta-
geous terms since the search for terminology oc-
curs densely in each of the sentences which in-
creases the computational complexity of this algo-
rithm, and causes the method to take a consider-
able time to run. Furthermore, if we suppose that
most frequent term-pairs are to be extracted via a
standard translation model (as described briefly in
the previous section), our efforts to search among
frequent pairs is not likely to bring about further
gain.
It is possible to approach this in a reverse man-
ner: ?less frequent pairs can be outstanding term
candidates?. Accordingly, if our aim changes to
capture only those less frequent pairs, the situation
changes dramatically. The number of terms we
need to capture is considerably decreased. Many
sentences do not include any terminology at all,
and only a relatively small subset of sentences in-
cludes a few terms, such that term-pairs become
sparse with regard to sentences. Term-pairs can
be found rather easily if a candidate term-pair co-
occurs on the source and the target sides and on
the condition that the items in the term-pair actu-
ally correspond with one another.
This condition can be easily checked in various
ways. One way is to translate the source side of
the targeted pairs with the alignment option in the
Moses decoder (Koehn et al., 2007), which we did
in this evaluation campaign. Another way is to use
asupervised aligner, such as the Berkeley aligner
(Haghighi et al., 2009), to align the targeted pairs
and check whether they are actually aligned or not.
We assume two predefined sets of terms at
the outset, E
term
= {e
term
1
, . . . , e
term
n
} and
F
term
= {f
term
1
, . . . , f
term
n
}. We search for
possible alignment links between the term-pair
only when they co-occur in the same sentence.
One obvious advantage of this approach is the
computational complexity which is fairly low.
Note that the result of (Okita et al., 2010)
shows that the frequency-based approach of (Ku-
piec, 1993) worked well for NTCIR patent termi-
nology (Fujii et al., 2010), which otherwise would
have been difficult to capture via the traditional
SMT/GIZA++ method. In contrast, however, this
did not work well on the Europarl corpus (Koehn,
2005).
240
2.3 Terminology Dictionaries
Terminology dictionaries themselves are obvi-
ously among the most important resources for
bilingual term-pairs. In this medical query transla-
tion subtask, two corpora are provided for this pur-
pose: (i) Unified Medical Language System cor-
pus (UMLS corpus),
1
and (ii) Wiki entries.
2
2.4 Extraction from Terminology
Dictionaries: lower-order n-grams
Terminology dictionaries provide reliable higher-
order n-gram pairs. However, they do not often
provide the correspondences between the lower-
order n-grams contained therein. For example, the
UMLS corpus provides a term-pair of ?abdominal
compartment syndrome ||| syndrome du compar-
timent abdominal? (EN|||FR). However, such ter-
minology dictionaries often do not explicitly pro-
vide the correspondent pairs ?abdominal ||| ab-
dominal? (EN|||FR) or ?syndrome ||| syndrome?
(EN|||FR). Clearly, these terminology dictionaries
implicitly provide the correspondent pairs. Note
that UMLS and Wiki entries provide terminol-
ogy dictionaries. Hence, it is possible to obtain
some suggestion by higher order n-gram models if
we know their alignments between words on the
source and target sides. Algorithm 1 shows the
overall procedure.
Algorithm 1 Lower-order n-gram extraction algo-
rithm
1: Perform monolingual word alignment for
higher-order n-gram pairs.
2: Collect only the reliable alignment pairs (i.e.
discard unreliable alignment pairs).
3: Extract the lower-order word pairs of our in-
terest.
2.5 Extraction from Monolingual Corpora:
Transliteration and Abbreviation
Monolingual corpora can be used in various ways,
including:
1. Transliteration: Many languages support the
fundamental mechanism of between Euro-
pean and Asian languages. Japanese even
supports a special alphabet ? katakana ? for
this purpose. Chinese and Hindi also per-
mit transliteration using their own alphabets.
1
http://www.nlm.nih.gov/research/umls/.
2
http://www.wikipedia.org.
However, even among European languages,
this mechanism makes it possible to find
possible translation counterparts for a given
term. In this query task, we did this only
for the French-to-English direction and only
for words containing accented characters (by
rule-based conversion).
2. Abbreviation: It is often the case that abbre-
viations should be resolved in the same lan-
guage. If the translation includes some ab-
breviation, such as ?C. difficile?, this needs
to be investigated exhaustively in the same
language. However, in the specific domain
of medical terminology, it is quite likely that
possible phrase matches will be successfully
identified.
2.6 Extraction from Monolingual Corpora:
Zero-Shot Learning
Algorithm 2 Algorithm to connect two word em-
bedding space
1: Prepare the monolingual source and target
sentences.
2: Prepare the dictionary which consists of U
entries of source and target sentences among
non-stop-words.
3: Train the neural network language model on
the source side and obtain the continuous
space real vectors of X dimensions for each
word.
4: Train the neural network language model on
the target side and obtain the continuous space
real vectors of X dimensions for each word.
5: Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary in two continuous spaces using
canonical component analysis (CCA).
Another interesting terminology extraction
method requires neither parallel nor comparable
corpora, but rather just monolingual corpora on
both sides (possibly unrelated to each other) to-
gether with a small amount of dictionary entries
which provide already known correspondences
between words on the source and target sides
(henceforth, we refer to this as the ?dictionary?).
This method uses the recently developed zero-shot
learning (Palatucci et al., 2009) using neural net-
work language modelling (Bengio et al., 2000;
Mikolov et al., 2013b). Then, we train both sides
241
with the neural network language model, and use
a continuous space representation to project words
to each other on the basis of a small amount of
correspondences in the dictionary. If we assume
that each continuous space is linear (Mikolov et
al., 2013c), we can connect them via linear projec-
tion (Mikolov et al., 2013b). Algorithm 2 shows
this situation.
In our experiments we use U the same as the
entries of Wiki and X as 50. Algorithm 3 shows
the algorithm to extract the counterpart of OOV
words.
Algorithm 3 Algorithm to extract the counterpart
of OOV words.
1: Prepare the projection by Algorithm 2.
2: Detect unknown words in the translation out-
puts.
3: Do the projection of it (the source word) into
the target word using the trained linear map-
pings in the training step.
3 Decoding Strategy
We deploy six kinds of extraction methods: (1)
translation model, (2) extraction from parallel cor-
pora, (3) terminology dictionaries, (4) lower-order
n-grams, (5) transliteration and abbreviation, and
(6) zero-shot learning. Among these we deploy
four of them ? (2), (4), (5) and (6) ? in a limited
context, while the remaining two are used with-
out any context, mainly owing to time constraints;
only when we did not find the correspondent pairs
via (1) and (3), did we complement this by the
other methods.
The detected bilingual term-pairs using (1) and
(3) can be combined using various methods. One
way is to employ a method similar to (confu-
sion network-based) system combination (Okita
and van Genabith, 2011; Okita and van Genabith,
2012). First we make a lattice: if we regard one
candidate of (1) and two candidates in (3) as trans-
lation outputs where the words of two candidates
in (3) are connected using an underscore (i.e. one
word), we can make a lattice. Then, we can deploy
monotonic decoding over them. If we do this for
the devset and then apply it to the test set, we can
incorporate a possible preference learnt from the
development set, i.e. whether the query transla-
tor prefers method (1) or UMLS/Wiki translation.
MERT process and language model are applied in
a similar manner with (confusion network-based)
system combination (cf. (Okita and van Genabith,
2011)).
We note also that a lattice structure is useful for
handling grammatical coordination. Since queries
are formed by real users, reserved words for
database query such as ?AND? (or ?ET? (FR)) and
?OR? (or ?OU? (FR)) are frequently observed in
the test set. Furthermore, there is repeated use of
?and? more than twice, for example ?douleur ab-
nominal et Helicobacter pylori et cancer?, which
makes it very difficult to detect the correct coor-
dination boundaries. The lattice on the input side
can express such ambiguity at the cost of splitting
the source-side sentence in a different manner.
4 Experimental Results
The baseline is obtained in the following way. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, and 3 iterations
of Model 4. For phrase extraction the grow-diag-
final heuristics described in (Koehn et al., 2003) is
used to derive the refined alignment from bidirec-
tional alignments. We then perform MERT (Och,
2003) which optimizes parameter settings using
the BLEUmetric (Papineni et al., 2002), while a 5-
gram language model is derived with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained using
SRILM (Stolcke, 2002). We use the whole train-
ing corpora including the WMT14 translation task
corpora as well as medical domain data. UMLS
and Wikipedia are used just as training corpora for
the baseline.
For the extraction from parallel corpora (cf.
Section 2.2), we used Genia tagger (Tsuruoka and
Tsujii, 2005) and the Berkeley parser (Petrov and
Klein, 2007). For the zero-shot learning (cf. Sec-
tion 2.6) we used scikit learn (Pedregosa et al.,
2011), word2vec (Mikolov et al., 2013a), and a
recurrent neural network (Mikolov, 2012). Other
tools used are in-house software.
Table 2 shows the results for the FR?EN query
task. We obtained 36.2 BLEU points absolute,
which is an improvement of 6.3 BLEU point ab-
solute (21.1% relative) over the baseline. Table
3 shows the results for the EN?FR query task.
We obtained 28.8 BLEU points absolute, which
is an improvement of 8.7 BLEU points abso-
242
lute (43% relative) over the baseline. Our sys-
tem was the best system for both of these tasks.
These improvements over the baseline were sta-
tistically significant by a paired bootstrap test
(Koehn, 2004).
Query task FR?EN
Our method baseline
BLEU 36.2 29.9
BLEU cased 30.9 26.5
TER 0.340 0.443
Table 1: Results for FR?EN query task.
extraction LM MERT BLEU (cased)
(1) - (6) all Y 30.9
(1), (2), (3) all Y 30.3
(1), (3), (6) all Y 30.1
(1), (3), (4) all Y 29.1
(1), (3), (5) all Y 29.0
(1) and (3) all Y 29.0
(1) and (3) medical Y 27.5
(1) and (3) WMT Y 27.0
(1) and (3) medical N 25.1
(1) and (3) WMT N 24.3
(1) medical Y 25.9
(1) WMT Y 25.0
Table 2: Table shows the effects of extraction
methods, language model and MERT process. All
the measurements are by BLEU (cased). In this
table, ?medical? indicates a language model built
on all the medical corpora while ?WMT? indicates
a language model built on all the non-medical cor-
pora. Note that some sentence in testset can be
considered as non-medical domain. Extraction
methods (1) - (6) correspond to those described in
Section 2.1 - 2.6.
Table 4 shows the results for CLIR task. We
obtained 51.8 BLEU points absolute, which is an
improvement of 9.4 BLEU point absolute (22.2%
relative) over the baseline. Although CLIR task al-
lowed 10-best lists, our submission included only
1-best list. This resulted in the score of P@5 of
0.348 and P@10 of 0.346 which correspond to
the second place, despite a good result in terms
of BLEU. This is since unlike BLEU score P@5
and P@10 measure whether the whole elements
in reference and hypothesis are matched or not.
We noticed that our submission included a lot of
Query task EN?FR
Our method baseline
BLEU 28.8 20.1
BLEU cased 27.7 18.7
TER 0.483 0.582
Table 3: Results for EN?FR query task.
near miss sentences only in terms of capitaliza-
tion: ?abnominal pain and Helicobacter pylori and
cancer? (reference) and ?abnominal pain and heli-
cobacter pylori and cancer? (submission). These
are counted as incorrect in terms of P@5 and
P@10.
3
Noted that after submission we obtained
the revised score of P@5 of 0.560 and P@10 of
0.560 with the same method but with 2-best lists
which handles the capitalization varieties.
CLIR task FR?EN
Our method baseline
BLEU 51.8 42.2
BLEU cased 46.0 38.3
TER 0.364 0.398
P@5 0.348 (0.560
?
) ?
P@10 0.346 (0.560
?
) ?
NDCG@5 0.306 ?
NDCG@10 0.307 ?
MAP 0.2252 ?
Rprec 0.2358 ?
bpref 0.3659 ?
relRet 1524 ?
Table 4: Results for CLIR task.
5 Conclusion
This paper provides a description of the Dublin
City University terminology translation system for
our participation in the query translation subtask
in the medical translation task in the Workshop on
Statistical Machine Translation (WMT14). We de-
ployed six different kinds of terminology extrac-
tion methods. We obtained 36.2 BLEU points ab-
solute for FR?EN, and 28.8 BLEU points abso-
lute for EN?FR tasks, obtaining first place on both
tasks. We obtained 51.8 BLEU points absolute for
the CLIR task.
3
The method which incorporates variation in capitaliza-
tion in its n-best lists outperforms the best result in terms of
P@5 and P@10.
243
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of CNGL
at Dublin City University.
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems, pages
1137?1155.
Peter F. Brown, Vincent J.D Pietra, Stephen A.D.Pietra,
and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, Vol.19, Issue 2,
pages 263?311.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2010. Overview of the
patent translation task at the NTCIR-8 workshop.
In Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293?
302.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In In Proceedings of the Confer-
ence of Association for Computational Linguistics,
pages 923?931.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181?184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115?124.
Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388?395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79?86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Julian. Kupiec. 1993. An algorithm for finding Noun
phrase correspondences in bilingual corpora. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 17?22.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. ArXiv:1309.4168.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT 2005), pages 746?
751.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. PhD thesis at Brno Uni-
versity of Technology.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Tsuyoshi Okita and Josef van Genabith. 2011. DCU
Confusion Network-based System Combination for
ML4HMT. Shared Task on Applying Machine
Learning techniques to optimising the division of
labour in Hybrid MT (ML4HMT-2011, collocated
with LIHMT-2011), pages 93?98.
Tsuyoshi Okita and Josef van Genabith. 2012. Mini-
mum Bayes Risk Decoding with Enlarged Hypoth-
esis Space in System Combination. In Proceed-
ings of 13th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING 2012), pages 40?51.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010. Multi-word
expression-sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Ling ual Information Access (CLIA2010, collocated
with COLING2010), pages 26?34.
244
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), pages 1410?1418.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: A Method For Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of AAAI (Nectar Track), pages 1663?1666.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901?904.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology / Em-
pirical Methods on Natural Language Processing
(HLT/EMNLP 2005), pages 467?474.
Andy Way. 2013. Traditional and emerging use-cases
for machine translation. In Proceedings of Translat-
ing and the Computer 35.
245
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 420?425,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
RED: DCU-CASICT Participation in WMT2014 Metrics Task
Xiaofeng Wu
?
, Hui Yu
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content
School of Computing, Dublin City University
Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, China
{xiaofengwu,qliu}@computing.dcu.ie, yuhui@ict.ac.cn
Abstract
Based on the last year?s DCU-CASIST
participation on WMT metrics task, we
further improve our model in the follow-
ing ways: 1) parameter tuning 2) support
languages other than English. We tuned
our system on all the data of WMT 2010,
2012 and 2013. The tuning results as well
as the WMT 2014 test results are reported.
1 Introduction
Automatic evaluation plays a more and more im-
portant role in the evolution of machine transla-
tion. There are roughly two categories can be seen:
namely lexical information based and structural
information based.
1) Lexical information based approaches,
among which, BLEU (?), Translation Error Rate
(TER) (?) and METEOR (?) are the most popular
ones and, with simplicity as their merits, cannot
adequately reflect the structural level similarity.
2) A lot of structural level based methods
have been exploited to overcome the weakness
of the lexical based methods, including Syntactic
Tree Model(STM)(?), a constituent tree based ap-
proach, and Head Word Chain Model(HWCM)(?),
a dependency tree based approach. Both of
the methods compute the similarity between the
sub-trees of the hypothesis and the reference.
Owczarzak et al (?; ?; ?) presented a method
using the Lexical-Functional Grammar (LFG) de-
pendency tree. MAXSIM (?) and the method pro-
posed by Zhu et al (?) also employed the syntac-
tic information in association with lexical infor-
mation. As we know that the hypothesis is poten-
tially noisy, and these errors are enlarged through
the parsing process. Thus the power of syntactic
information could be considerably weakened.
In this paper, based on our attempt on WMT
metrics task 2013 (?), we propose a metrics named
RED ( REference Dependency based automatic
evaluation method). Our metrics employs only the
reference dependency tree which contains both the
lexical and syntactic information, leaving the hy-
pothesis side unparsed to avoid error propagation.
2 Parameter Tuning
In RED, we use F -score as our final score.
F -score is calculated by Formula (1), where ? is
a value between 0 and 1.
F -score =
precision ? recall
? ? precision+ (1? ?) ? recall
(1)
The dependency tree of the reference and the
string of the translation are used to calculate the
precision and recall. In order to calculate preci-
sion, the number of the dep-ngrams (the ngrams
obtained from dependency tree
1
) should be given,
but there is no dependency tree for the transla-
tion in our method. We know that the number
of dep-ngrams has an approximate linear relation-
ship with the length of the sentence, so we use the
length of the translation to replace the number of
the dep-ngrams in the translation dependency tree.
Recall can be calculated directly since we know
the number of the dep-ngrams in the reference.
The precision and recall are computed as follows.
precision
n
=
?
d?D
n
p
(d,hyp)
len
h
recall
n
=
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n.
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. p
(d,hpy)
is 0 if there is no
match and a positive number between 0 and 1 oth-
erwise(?).
1
We define two types of dep-ngrams: 1) the head word
chain(?); 2) fix-floating(?))
420
The final score of RED is achieved using For-
mula (2), in which a weighted sum of the dep-
ngrams? F -score is calculated. w
ngram
(0 ?
w
ngram
? 1) is the weight of dep-ngram with the
length of n. F -score
n
is the F -score for the dep-
ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? F -score
n
) (2)
Other parameters to be tuned includes:
? Stem and Synonym
Stem(?) and synonym (WordNet
2
) are intro-
duced with the following three steps. First,
we obtain the alignment with METEOR
Aligner (?) in which not only exact match
but also stem and synonym are considered.
We use stem, synonym and exact match as the
three match modules. Second, the alignment
is used to match for a dep-ngram. We think
the dep-ngram can match with the transla-
tion if the following conditions are satisfied.
1) Each of the words in the dep-ngram has
a matched word in the translation according
to the alignment; 2) The words in dep-ngram
and the matched words in translation appear
in the same order; 3) The matched words
in translation must be continuous if the dep-
ngram is a fixed-floating ngram. At last, the
match module score of a dep-ngram is cal-
culated according to Formula (3). Different
match modules have different effects, so we
give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (3)
m
i
is the match module (exact, stem or syn-
onym) of the ith word in a dep-ngram. w
m
i
is the match module weight of the ith word in
a dep-ngram. n is the number of words in a
dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t con-
sider the dependency tree of the reference,
because paraphrases may not be contained in
the head word chain and fixed-floating struc-
tures. Therefore we first obtain the align-
2
http://wordnet.princeton.edu/
ment with METEOR Aligner, only consid-
ering paraphrase; Then, the matched para-
phrases are extracted from the alignment and
defined as paraphrase-ngram. The score of
a paraphrase is 1 ? w
par
, where w
par
is the
weight of paraphrase-ngram.
? Function word
We introduce a parameterw
fun
(0 ? w
fun
?
1) to distinguish function words and content
words. w
fun
is the weight of function words.
The function word score of a dep-ngram or
paraphrase-ngram is computed according to
Formula (4).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(4)
C
fun
is the number of function words in the
dep-ngram or paraphrase-ngram. C
con
is the
number of content words in the dep-ngram or
paraphrase-ngram.
REDp =
N
?
n=1
(w
ngram
? F -score
pn
) (5)
F -score
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(6)
precision
p
and recall
P
in Formula (6) are cal-
culated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length
of n in the reference. count
n
(par) is the num-
ber of paraphrases with length of n in refer-
ence. score
par
n
is the match score of paraphrase-
ngrams with the length of n. score
dep
n
is the
match score of dep-ngrams with the length of n.
score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
)
score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
421
Metrics BLEU TER HWCM METEOR RED RED-sent RED-syssent
WMT 2010
cs-en 0.255 0.253 0.245 0.319 0.328 0.342 0.342
de-en 0.275 0.291 0.267 0.348 0.361 0.371 0.375
es-en 0.280 0.263 0.259 0.326 0.333 0.344 0.347
fr-en 0.220 0.211 0.244 0.275 0.283 0.329 0.328
ave 0.257 0.254 0.253 0.317 0.326 0.346 0.348
WMT 2012
cs-en 0.157 - 0.158 0.212 0.165 0.218 0.212
de-en 0.191 - 0.207 0.275 0.218 0.283 0.279
es-en 0.189 - 0.203 0.249 0.203 0.255 0.256
fr-en 0.210 - 0.204 0.251 0.221 0.250 0.253
ave 0.186 - 0.193 0.246 0.201 0.251 0.250
WMT 2013
cs-en 0.199 - 0.153 0.265 0.228 0.260 0.256
de-en 0.220 - 0.182 0.293 0.267 0.298 0.297
es-en 0.259 - 0.220 0.324 0.312 0.330 0.326
fr-en 0.224 - 0.194 0.264 0.257 0.267 0.266
ru-en 0.162 - 0.136 0.239 0.200 0.262 0.225
ave 0.212 - 0.177 0.277 0.252 0.283 0.274
WMT 2014
hi-en - - - 0.420 - 0.383 0.386
de-en - - - 0.334 - 0.336 0.338
cs-en - - - 0.282 - 0.283 0.283
fr-en - - - 0.406 - 0.403 0.404
ru-en - - - 0.337 - 0.328 0.329
ave - - - 0.355 - 0.347 0.348
Table 1: Sentence level correlations tuned on WMT 2010, 2012 and 2013; tested on WMT 2014. The
value in bold is the best result in each raw. ave stands for the average result of the language pairs on each
year. RED stands for our untuned system, RED-sent is G.sent.2, RED-syssent is G.sent.1
P
n
is the set of paraphrase-ngrams with the
length of n. D
n
is the set of dep-ngrams with the
length of n.
There are totally nine parameters in RED. We
tried two parameter tuning strategies: Genetic
search algorithm (?) and a Grid search over two
subsets of parameters. The results of Grid search
is more stable, therefore our final submission is
based upon Grid search. We separate the 9 pa-
rameters into two subsets. When searching Sub-
set 1, the parameters in Subset 2 are fixed, and
vice versa. Several iterations are executed to fin-
ish the parameter tuning process. For system
level coefficient score, we set two optimization
goals: G.sys.1) to maximize the sum of Spear-
man?s ? rank correlation coefficient on system
level and Kendall?s ? correlation coefficient on
sentence level or G.sys.2) only the former; For
sentence level coefficient score, we also set two
optimization goals: G.sent.1) the same as G.sys.1,
G.sent.2) only the latter part of G.sys.1.
3 Experiments
In this section we report the experimental results
of RED on the tuning set, which is the combi-
nation of WMT2010, WMT2012 and WMT2013
data set, as well as the test results on the
WMT2014. Both the sentence level evaluation and
the system level evaluation are conducted to assess
the performance of our automatic metrics. At the
sentence level evaluation, Kendall?s rank correla-
tion coefficient ? is used. At the system level eval-
uation, the Spearman?s rank correlation coefficient
? is used.
3.1 Data
There are four language pairs in WMT2010 and
WMT2012 including German-English, Czech-
English, French-English and Spanish-English. For
WMT2013, except these 4 language pairs, there is
also Russian-English. As the test set, WMT 2014
has also five language pairs, but the organizer re-
moved Spanish-English and replace it with Hindi-
English. For into-English tasks, we parsed the En-
422
Metrics BLEU TER HWCM METEOR RED RED-sys RED-syssent
WMT 2010
cs-en 0.840 0.783 0.881 0.839 0.839 0.937 0.881
de-en 0.881 0.892 0.905 0.927 0.933 0.95 0.948
es-en 0.868 0.903 0.824 0.952 0.969 0.965 0.969
fr-en 0.839 0.833 0.815 0.865 0.873 0.875 0.905
ave 0.857 0.852 0.856 0.895 0.903 0.931 0.925
WMT 2012
cs-en 0.886 0.886 0.943 0.657 1 1 1
de-en 0.671 0.624 0.762 0.885 0.759 0.935 0.956
es-en 0.874 0.916 0.937 0.951 0.951 0.965 0.958
fr-en 0.811 0.821 0.818 0.843 0.818 0.871 0.853
ave 0.810 0.811 0.865 0.834 0.882 0.942 0.941
WMT 2013
cs-en 0.936 0.800 0.818 0.964 0.964 0.982 0.972
de-en 0.895 0.833 0.816 0.961 0.951 0.958 0.978
es-en 0.888 0.825 0.755 0.979 0.930 0.979 0.965
fr-en 0.989 0.951 0.940 0.984 0.989 0.995 0.984
ru-en 0.670 0.581 0.360 0.789 0.725 0.847 0.821
ave 0.875 0.798 0.737 0.834 0.935 0.952 0.944
WMT 2014
hi-en 0.956 0.618 - 0.457 - 0.676 0.644
de-en 0.831 0.774 - 0.926 - 0.897 0.909
cs-en 0.908 0.977 - 0.980 - 0.989 0.993
fr-en 0.952 0.952 - 0.975 - 0.981 0.980
ru-en 0.774 0.796 - 0.792 - 0.803 0.797
ave 0.826 0.740 - 0.784 - 0.784 0.770
Table 2: System level correlations tuned on WMT 2010, 2012 and 2013, tested on 2014. The value in
bold is the best result in each raw. ave stands for the average result of the language pairs on each year.
RED stands for our untuned system, RED-sys is G.sys.2, RED-syssent is G.sys.1
Metrics BLEU TER METEOR RED RED-sent RED-syssent
WMT 2010
en-fr 0.33 0.31 0.369 0.338 0.390 0.369
en-de 0.15 0.08 0.166 0.141 0.214 0.185
WMT 2012
en-fr - - 0.26 0.171 0.273 0.266
en-de - - 0.180 0.129 0.200 0.196
WMT 2013
en-fr - - 0.236 0.220 0.237 0.239
en-de - - 0.203 0.185 0.215 0.219
WMT 2014
en-fr - - 0.278 - 0.297 0.293
en-de - - 0.233 - 0.236 0.229
Table 3: Sentence level correlations tuned on WMT 2010, 2012 and 2013, and tested on 2014. The
value in bold is the best result in each raw. RED stands for our untuned system, RED-sent is G.sent.2,
RED-syssent is G.sent.1
glish reference into constituent tree by Berkeley
parser and then converted the constituent tree into
dependency tree by Penn2Malt
3
. We also con-
ducted English-to-French and English-to-German
experiments. The German and French dependency
parser we used is Mate-Tool
4
.
3
http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
4
https://code.google.com/p/mate-tools/
In the experiments, we compare the perfor-
mance of our metric with the widely used lexi-
cal based metrics BLEU, TER, METEOR and a
dependency based metrics HWCM. The results of
RED are provided with exactly the same external
resources like METEOR. The results of BLEU,
TER and METOER are obtained from official re-
port of WMT 2010, 2012, 2013 and 2014 (if they
423
Metrics BLEU TER METEOR RED RED-sys RED-syssent
WMT 2010
en-fr 0.89 0.89 0.912 0.881 0.932 0.928
en-de 0.66 0.65 0.688 0.657 0.734 0.734
WMT 2012
en-fr 0.80 0.69 0.82 0.639 0.914 0.914
en-de 0.22 0.41 0.180 0.143 0.243 0.243
WMT 2013
en-fr 0.897 0.912 0.924 0.914 0.931 0.936
en-de 0.786 0.854 0.879 0.85 0.8 0.8
WMT 2014
en-fr 0.934 0.953 0.940 - 0.942 0.943
en-de 0.065 0.163 0.128 - 0.047 0.047
Table 4: System level correlations for English to Franch and German, tuned on WMT 2010, 2012 and
2013; tested on WMT 2014. The value in bold is the best result in each raw. RED stands for our untuned
system, RED-sys is G.sys.2, RED-syssent is G.sys.1
are available). The experiments of HWCM is per-
formed by us.
3.2 Sentence-level Evaluation
Kendall?s rank correlation coefficient ? is em-
ployed to evaluate the correlation of all the MT
evaluation metrics and human judgements at the
sentence level. A higher value of ? means a bet-
ter ranking similarity with the human judges. The
correlation scores of are shown in Table 1. Our
method performs best when maximum length of
dep-n-gram is set to 3, so we present only the
results when the maximum length of dep-n-gram
equals 3. From Table 1, we can see that: firstly, pa-
rameter tuning improve performance significantly
on all the three tuning sets; secondly, although
the best scores in the column RED-sent are much
more than RED-syssent, but the outperform is
very small, so by setting these two optimization
goals, RED can achieve comparable performance;
thirdly, without parameter tuning, RED does not
perform well on WMT 2012 and 2013, and even
with parameter tuning, RED does not outperform
METEOR as much as WMT 2010; lastly, on the
test set, RED does not outperform METEOR.
3.3 System-level Evaluation
We also evaluated the RED scores with the human
rankings at the system level to further investigate
the effectiveness of our metrics. The matching of
the words in RED is correlated with the position
of the words, so the traditional way of computing
system level score, like what BLEU does, is not
feasible for RED. Therefore, we resort to the way
of adding the sentence level scores together to ob-
tain the system level score. At system level evalu-
ation, we employ Spearman?s rank correlation co-
efficient ?. The correlations and the average scores
are shown in Table 2.
From Table 2, we can see similar trends as in
Table 1 with the following difference: firstly, with-
out parameter tuning, RED perform comparably
with METEOR on all the three tuning sets; sec-
ondly, on test set, RED also perform comparably
with METEOR. thirdly, RED perform very bad on
Hindi-English, which is a newly introduced task
this year.
3.4 Evaluation of English to Other
Languages
We evaluate both sentence level and system level
score of RED on English to French and German.
The reason we only conduct these two languages
are that the dependency parsers are more reliable
in these two languages. The results are shown in
Table 3 and 4.
From Table 3 and 4 we can see that the tuned
version of RED still perform slightly better than
METEOR with the only exception of system level
en-de.
4 Conclusion
In this paper, based on the last year?s DCU-
CASICT submission, we further improved our
method, namely RED. The experiments are car-
ried out at both sentence-level and system-level
using to-English and from-English corpus. The
experiment results indicate that although RED
achieves better correlation than BLEU, HWCM,
TER and comparably performance with METEOR
at both sentence level and system level, the per-
formance is not stable on all language pairs, such
as the sentence level correlation score of Hindi to
424
English and the system level score of English to
German. To further study the sudden diving of the
performance is our future work.
Acknowledgments
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the CNGL Centre for Global Intelligent Content
(www.cngl.ie) at Dublin City University and Na-
tional Natural Science Foundation of China (Grant
61379086).
References
Ergun Bic?ici and Deniz Yuret. 2011. Instance selec-
tion for machine translation using feature decay al-
gorithms. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 272?283. As-
sociation for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. Maxsim: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 85?91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, StatMT ?07, pages 228?231, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization, pages 25?32.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-based automatic eval-
uation for machine translation. In Proceedings of
the NAACL-HLT 2007/AMTA Workshop on Syntax
and Structure in Statistical Translation, SSST ?07,
pages 80?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007b. Evaluating machine translation with
lfg dependencies. Machine Translation, 21(2):95?
119, June.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007c. Labelled dependencies in machine
translation evaluation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
StatMT ?07, pages 104?111, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th annual
meeting on association for computational linguis-
tics, pages 311?318. Association for Computational
Linguistics.
Martin F Porter. 2001. Snowball: A language for stem-
ming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Xiaofeng Wu, Hui Yu, and Qun Liu. 2013. Dcu partic-
ipation in wmt2013 metrics task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation. Association for Computational Linguistics.
H. Yu, X. Wu, Q. Liu, and S. Lin. 2014. RED: A
Reference Dependency Based MT Evaluation Met-
ric. In To be published.
Junguo Zhu, Muyun Yang, Bo Wang, Sheng Li, and
Tiejun Zhao. 2010. All in strings: a powerful string-
based automatic mt evaluation metric with multi-
ple granularities. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics:
Posters, COLING ?10, pages 1533?1540, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
425
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 122?131,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Transformation and Decomposition for Efficiently Implementing and
Improving Dependency-to-String Model In Moses
Liangyou Li
?
, Jun Xie
?
, Andy Way
?
and Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,away,qliu}@computing.dcu.ie
junxie@ict.ac.cn
Abstract
Dependency structure provides grammat-
ical relations between words, which have
shown to be effective in Statistical Ma-
chine Translation (SMT). In this paper, we
present an open source module in Moses
which implements a dependency-to-string
model. We propose a method to trans-
form the input dependency tree into a cor-
responding constituent tree for reusing the
tree-based decoder in Moses. In our ex-
periments, this method achieves compara-
ble results with the standard model. Fur-
thermore, we enrich this model via the
decomposition of dependency structure,
including extracting rules from the sub-
structures of the dependency tree during
training and creating a pseudo-forest in-
stead of the tree per se as the input dur-
ing decoding. Large-scale experiments
on Chinese?English and German?English
tasks show that the decomposition ap-
proach improves the baseline dependency-
to-string model significantly. Our sys-
tem achieves comparable results with the
state-of-the-art hierarchical phrase-based
model (HPB). Finally, when resorting to
phrasal rules, the dependency-to-string
model performs significantly better than
Moses HPB.
1 Introduction
Dependency structure models relations between
words in a sentence. Such relations indicate
the syntactic function of one word to another
word. As dependency structure directly encodes
semantic information and has the best inter-lingual
phrasal cohesion properties (Fox, 2002), it is be-
lieved to be helpful to translation.
In recent years, dependency structure has been
widely used in SMT. For example, Shen et al.
(2010) present a string-to-dependency model by
using the dependency fragments of the neighbour-
ing words on the target side, which makes it easier
to integrate a dependency language model. How-
ever such string-to-tree systems run slowly in cu-
bic time (Huang et al., 2006).
Another example is the treelet approach
(Menezes and Quirk, 2005; Quirk et al., 2005),
which uses dependency structure on the source
side. Xiong et al. (2007) extend the treelet ap-
proach to allow dependency fragments with gaps.
As the treelet is defined as an arbitrary connected
sub-graph, typically both substitution and inser-
tion operations are adopted for decoding. How-
ever, as translation rules based on the treelets
do not encode enough reordering information di-
rectly, another heuristic or separate reordering
model is usually needed to decide the best target
position of the inserted words.
Different from these works, Xie et al. (2011)
present a dependency-to-string (Dep2Str) model,
which extracts head-dependent (HD) rules from
word-aligned source dependency trees and target
strings. As this model specifies reordering infor-
mation in the HD rules, during translation only the
substitution operation is needed, because words
are reordered simultaneously with the rule being
applied. Meng et al. (2013) and Xie et al. (2014)
extend the model by augmenting HD rules with the
help of either constituent tree or fixed/float struc-
ture (Shen et al., 2010). Augmented rules are cre-
ated by the combination of two or more nodes in
122
the HD fragment, and are capable of capturing
translations of non-syntactic phrases. However,
the decoder needs to be changed correspondingly
to handle these rules.
Attracted by the simplicity of the Dep2Str
model, in this paper we describe an easy way to
integrate the model into the popular translation
framework Moses (Koehn et al., 2007). In or-
der to share the same decoder with the conven-
tional syntax-based model, we present an algo-
rithm which transforms a dependency tree into a
corresponding constituent tree which encodes de-
pendency information in its non-leaf nodes and is
compatible with the Dep2Str model. In addition,
we present a method to decompose a dependency
structure (HD fragment) into smaller parts which
enrich translation rules and also allow us to cre-
ate a pseudo-forest as the input. ?Pseudo? means
the forest is not obtained by combining several
trees from a parser, but rather that it is created
based on the decomposition of an HD fragment.
Large-scale experiments on Chinese?English and
German?English tasks show that the transforma-
tion and decomposition are effective for transla-
tion.
In the remainder of the paper, we first describe
the Dep2Str model (Section 2). Then we describe
how to transform a dependency tree into a con-
stituent tree which is compatible with the Dep2Str
model (Section 3). The idea of decomposition in-
cluding extracting sub-structural rules and creat-
ing a pseudo-forest is presented in Section 4. Then
experiments are conducted to compare translation
results of our approach with the state-of-the-art
HPB model (Section 5). We conclude in Section 6
and present avenues for future work.
2 Dependency-to-String Model
In the Dep2Str model (Xie et al., 2011), the HD
fragment is the basic unit. As shown in Figure
1, in a dependency tree, each non-leaf node is the
head of some other nodes (dependents), so an HD
fragment is composed of a head node and all of its
dependents.
1
In this model, there are two kinds of rules for
translation. One is the head rule which specifies
the translation of a source word:
Juxing
??? holds
1
In this paper, HD fragment of a node means the HD frag-
ment with this node as the head. Leaf nodes have no HD
fragments.
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Figure 1: Example of a dependency tree, with
head-dependent fragments being indicated by dot-
ted lines.
The other one is the HD rule which consists of
three parts: the HD fragment s of the source
side (maybe containing variables), a target string
t (maybe containing variables) and a one-to-one
mapping ? from variables in s to variables in t, as
in:
s = (
Boliweiya
????)
Juxing
?? (x
1
:
Xuanju
?? )
t = Bolivia holds x
1
? = {x
1
:
Xuanju
??? x
1
}
where the underlined element denotes the leaf
node. Variables in the Dep2Str model are con-
strained either by words (like x
1
:??) or Part-of-
Speech (POS) tags (like x
1
:NN).
Given a source sentence with a dependency tree,
a target string and the word alignment between the
source and target sentences, this model first an-
notates each node N with two annotations: head
span and dependency span.
2
These two spans
specify the corresponding target position of a node
(by the head span) or sub-tree (by the depen-
dency span). After annotation, acceptable HD
fragments
3
are utilized to induce lexicalized HD
2
Some definitions: Closure clos(S) of set S is the small-
est superset of S in which the elements (integers) are contin-
uous. Let H be the set of indexes of target words aligned to
node N . Head span hsp(N) of node N is clos(H). Head
span hsp(N) is consistent if it does not overlap with head
span of any other node. Dependency span dsp(N) of node
N is the union of all consistent head spans in the subtree
rooted at N .
3
A head-dependent fragment is acceptable if the head
span of the head node is consistent and none of the depen-
dency spans of its dependents is empty. We could see that
in an acceptable fragment, the head span of the head node
and dependency spans of dependents are not overlapped with
each other.
123
          Boliweiya Juxing  Xuanju
R ule:  ( ????)  ?? ( x1 :??) Boliv ia hold s  x1
                       Xuanju
R ule:  ( x1: N N )  ?? x1 elec tions
           Guohui
R ule:  ?? p ar liam ent
           Zongtong Yu      Guohui
R ule:    ( ??)    ( ?)  x1:?? p r es id ential and  x1
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Zongtong
??/NN
Yu
?/CC
Guohui
??/NNBoliv ia hold s elec tions
Boliv ia hold s
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Boliv ia hold s  p r es id ential and  p ar liam ent elec tions
Boliv ia hold s  p r es id ential and
Guohui
??/NN elec tions
( a)
( b )
( c )
( d )
( e)
Figure 2: Example of a derivation. Underlined el-
ements indicate leaf nodes.
rules (the head node and leaf node are represented
by words, while the internal nodes are replaced by
variables constrained by word) and unlexicalized
HD rules (nodes are replaced by variables con-
strained by POS tags).
In HD rules, an internal node denotes the whole
sub-tree and is always a substitution site. The head
node and leaf nodes can be represented by either
words or variables. The target side corresponding
to an HD fragment and the mapping between vari-
ables are determined by the head span of the head
node and the dependency spans of the dependents.
A translation can be obtained by applying rules
to the input dependency tree. Figure 2 shows a
derivation for translating a Chinese sentence into
an English string. The derivation proceeds from
top to bottom. Variables in the higher-level HD
rules are substituted by the translations of lower
HD rules recursively.
The final translation is obtained by finding the
best derivation d
?
from all possible derivations
D which convert the source dependency structure
into a target string, as in Equation (1):
d
?
= argmax
d?D
p(d) ? argmax
d?D
?
i
?
i
(d)
?
i
(1)
where ?
i
(d) is the ith feature defined in the deriva-
tion d, and ?
i
is the weight of the feature.
3 Transformation of Dependency Trees
In this section, we introduce an algorithm to trans-
form a dependency tree into a corresponding con-
stituent tree, where words of the source sentence
are leaf nodes and internal nodes are labelled with
head words or POS tags which are constrained by
dependency information. Such a transformation
makes it possible to use the traditional tree-based
decoder to translate a dependency tree, so we can
easily integrate the Dep2Str model into the popu-
lar framework Moses.
In a tree-based system, the CYK algorithm
(Kasami, 1965; Younger, 1967; Cocke and
Schwartz, 1970) is usually employed to translate
the input sentence with a tree structure. Each time
a continuous sequence of words (a phrase) in the
source sentence is translated. Larger phrases can
be translated by combining translations of smaller
phrases.
In a constituent tree, the source words are leaf
nodes and all non-leaf nodes covering a phrase are
labelled with categories which are usually vari-
ables defined in the tree-based model. For trans-
lating a phrase covered by a non-leaf node, the de-
coder for the constituent tree can easily find ap-
plied rules by directly matching variables in these
rules to tree nodes. However, in a dependency tree,
each internal node represents a word of the source
sentence. Variables covering a phrase cannot be
recognized directly. Therefore, to share the same
decoder with the constituent tree, the dependency
tree needs to be transformed into a constituent-
style tree.
As we described in Section 2, each variable in
the Dep2Str model represents a word (for the head
and leaf node) or a sequence of continuous words
(for the internal node). Thus it is intuitive to use
these variables to label non-leaf nodes of the pro-
duced constituent tree. Furthermore, in order to
preserve the dependency information of each HD
fragment, the created constituent node needs to be
constrained by the dependency information in the
HD fragment.
Our transformation algorithm is shown in Al-
gorithm 1, which proceeds recursively from top to
bottom on each HD fragment. There are a maxi-
mum of three types of nodes in an HD fragment:
head node, leaf nodes, and internal nodes. The
124
Algorithm 1 Algorithm for transforming a depen-
dency tree to constituent tree. Dnode means node
in dependency tree. Cnode means node in con-
stituent tree.
function CNODE(label, span)
create a new Cnode CN
CN.label? label
CN.span? span
end function
function TRANSFNODE(Dnode H)
pos? POS of H
constrain pos . with H0, like: NN:H0
CNODE(label,H.position)
for each dependent N of H do
pos? POS of N
word? word of N
constrain pos . with Li or Ri, like: NN:R1
constrain word . with Li or Ri
if N is leaf then
CNODE(pos,N.position)
else
CNODE(word,H.span)
CNODE(pos,H.span)
TRANSFNODE(N )
end if
end for
end function
leaf nodes and internal nodes are dependents of
the head node. For the leaf node and head node,
we create constituent nodes that just cover one
word. For an internal node N , we create con-
stituent nodes that cover all the words in the sub-
tree rooted at N . In Algorithm 1, N.position
means the position of the word represented by the
node N . N.span denotes indexes of words cov-
ered by the sub-tree rooted at node N .
Taking the dependency tree in Figure 1 as an
example, its transformation result for integration
with Moses is shown in Figure 3. In the Dep2Str
model, leaf nodes can be replaced by a vari-
able constrained by its POS tag, so for leaf node
?
Zongtong
?? ? in HD fragment ?
Zongtong
(??)
Yu
(?)
Guohui
???,
we create a constituent node ?NN:L2?, where
?NN? is the POS tag and ?L2? denotes that the leaf
node is the second left dependent of the head node.
For the internal node ?
Guohui
??? in the HD fragment
?
Guohui
(??)
Xuanju
???, we create two constituent nodes
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1 V V : H 0 N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : L 1
N N : R 1
S
Guohui
??: L 1
Xuanju
??: R 1
Figure 3: The corresponding constituent tree af-
ter transforming the dependency tree in Figure 1.
Note in our implementation, we do not distinguish
the leaf node and internal node of a dependency
tree in the produced constituent tree and induced
rules.
which cover all words in the dependency sub-tree
rooted at this node, with one of them labelled by
the word itself. Both nodes are constrained by de-
pendency information ?L1?. After such a transfor-
mation is conducted on each HD fragment recur-
sively, we obtain a constituent tree.
This transformation makes our implementation
of the Dep2Str model easier, because we can use
the tree-to-string decoder in Moses. All we need
to do is to write a new rule extractor which extracts
head rules and HD rules (see Section 2) from the
word-aligned source dependency trees and target
strings, and represents these rules in the format de-
fined in Moses.
4
Note that while this conversion is performed
on an input dependency tree during decoding, the
training part, including extracting rules and cal-
culating translation probabilities, does not change,
so the model is still a dependency-to-string model.
4
Taking the rule in Section 2 as an example, its represen-
tation in Moses is:
s =
Boliweiya
????
Juxing
??
Xuanju
[??:R1][X] [H1]
t = Bolivia holds
Xuanju
[??:R1][X] [X]
? = {2 ? 2}
where ?H1? denotes the position of the head word is 1, ?R1?
indicates the first right dependent of the head word, ?X? is the
general label for the target side and ? is the set of alignments
(the index-correspondences between s and t). The format has
been described in detail at http://www.statmt.org/
moses/?n=Moses.SyntaxTutorial.
125
In addition, our transformation is different from
other works which transform a dependency tree
into a constituent tree (Collins et al., 1999; Xia and
Palmer, 2001). In this paper, the produced con-
stituent tree still preserves dependency relations
between words, and the phrasal structure is di-
rectly derived from the dependency structure with-
out refinement. Accordingly, the constituent tree
may not be a linguistically well-formed syntactic
structure. However, it is not a problem for our
model, because in this paper what matters is the
dependency structure which has already been en-
coded into the (ill-formed) constituent tree.
4 Decomposition of Dependency
Structure
The Dep2Str model treats a whole HD fragment
as the basic unit, which may result in a sparse-
data problem. For example, an HD fragment with
a verb as head typically consists of more than four
nodes (Xie et al., 2011). Thus in this section, in-
spired by the treelet approach, we describe a de-
composition method to make use of smaller frag-
ments.
In an HD fragment of a dependency tree, the
head determines the semantic category, while
the dependent gives the semantic specification
(Zwicky, 1985; Hudson, 1990). Accordingly, it
is reasonable to assume that in an HD fragment,
dependents could be removed or new dependents
could be attached as needed. Thus, in this paper,
we assume that a large HD fragment is formed by
attaching dependents to a small HD fragment. For
simplicity and reuse of the decoder, such an at-
tachment is carried out in one step. This means
that an HD fragment is decomposed into two
smaller parts in a possible decomposition. This
decomposition can be formulated as Equation (2):
L
i
? ? ?L
1
HR
1
? ? ?R
j
= L
m
? ? ?L
1
HR
1
? ? ?R
n
+ L
i
? ? ?L
m+1
HR
n+1
? ? ?R
j
subject to
i ? 0, j ? 0
i ? m ? 0, j ? n ? 0
i+ j > m+ n > 0
(2)
whereH denotes the head node, L
i
denotes the ith
left dependent and R
j
denotes the jth right depen-
dent. Figure 4 shows an example.
s m ar t/ JJ
v er y/ R BS he/ P R P
s m ar t/ JJ
is / V BZ
S he/ P R P
s m ar t/ JJ
is / V BZ v er y/ R B
+
Figure 4: An example of decomposition on a head-
dependent fragment.
Algorithm 2 Algorithm for the decomposition of
an HD fragment into two sub-fragments. Index of
nodes in a fragment starts from 0.
function DECOMP(HD fragment frag)
fset ? {}
len? number of nodes in frag
hidx? the index of head node in frag
for s = 0 to hidx do
for e = hidx to len? 1 do
if 0 < e? s < len? 1 then
create sub-fragment core
core? nodes from s to e
add core to fset
create sub-fragment shell
initialize shell with head node
shell? nodes not in core
add shell to fset
end if
end for
end for
end function
Such a decomposition of an HD fragment en-
ables us to create translation rules extracted from
sub-structures and create a pseudo-forest from
the input dependency tree to make better use of
smaller rules.
4.1 Sub-structural Rules
In the Dep2Str model, rules are extracted on
an entire HD fragment. In this paper, when
the decomposition is considered, we also extract
sub-structural rules by taking each possible sub-
fragment as a new HD fragment. The algorithm
for recognizing the sub-fragments is shown in Al-
gorithm 2.
In Algorithm 2, we find all possible decom-
126
positions of an HD fragment. Each decom-
position produces two sub-fragments: core and
shell. Both core and shell include the head node.
core contains the dependents surrounding the head
node, with the remaining dependents belonging to
shell. Taking Figure 4 as an example, the bottom-
right part is core, while the bottom-left part is
shell. Each core and shell could be seen as a
new HD fragment. Then HD rules are extracted as
defined in the Dep2Str model.
Note that different from the augmented HD
rules, where Meng et al. (2013) annotate rules with
combined variables and Xie et al. (2014) create
special rules from HD rules at runtime by com-
bining several nodes, our sub-structural rules are
standard HD rules, which are extracted from the
connected sub-structures of a larger HD fragment
and can be used directly in the model.
4.2 Pseudo-Forest
Although sub-structural rules are effective in our
experiments (see Section 5), we still do not use
them to their best advantage, because we only en-
rich smaller rules in our model. During decod-
ing, for a large input HD fragment, the model is
still more likely to resort to glue rules. However,
the idea of decomposition allows us to create a
pseudo-forest directly from the dependency tree to
alleviate this problem to some extent.
As described above, an HD fragment can be
seen as being created by combining two smaller
fragments. This means, for an HD fragment in the
input dependency tree, we can translate one of its
sub-fragments first, then obtain the whole trans-
lation by combining with translations of another
sub-fragment. From Algorithm 2, we know that
the sub-fragment core covers a continuous phrase
of the source sentence. Accordingly, we can trans-
late this fragment first and then build the whole
translation by translating another sub-fragment
shell. Figure 5 gives an example of translating
an HD fragment by combining the translations of
its sub-fragments.
Instead of taking the dependency tree as the in-
put and looking for all rules for translating sub-
fragments of a whole HD, we directly encode the
decomposition into the input dependency tree with
the result being a pseudo-forest. Based on the
transformation algorithm in Section 3, the pseudo-
forest can also be represented in the constituent-
tree style, as shown in Figure 6.
          Yu  Guohui
R ule:  ( ?)  ?? and  p ar lim ent
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Zongtong
R ule:  ( ??)  x 1 : N N p r es id ential x1
p r es id ential and  p ar liam ent
Zongtong
??        and  p ar liam ent
Guohui
??/NN
( a)
( b )
( c )
Figure 5: An example of translating a large HD
fragment with the help of translations of its de-
composed fragments.
S
N N : L 1 V V : H 0
N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : R 1
Xuanju
??: R 1
N N : L 1
Guohui
??: L 1
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1
N N : H 0
V V : H 0
V V : H 0
Figure 6: An example of a pseudo-forest for the
dependency tree in Figure 1. It is represented us-
ing the constituent-tree style described in Section
3. Edges drawn in the same type of line are owned
by the same sub-tree. Solid lines are shared edges.
In the pseudo-forest, we actually only create a
forest structure for each HD fragment. For ex-
ample, based on Figure 5, we create a constituent
node labelled with ?NN:H0? that covers the sub-
fragment ?
Yu
(?)
Guohui
???. In so doing, a new node la-
belled with ?NN:L1? is also created, which covers
the Node ?
Zongtong
?? ?, because it is now the first left
dependent in the sub-fragment ?
Zongtong
(??)
Guohui
?? ?.
Compared to the forest-based model (Mi et al.,
2008), such a pseudo-forest cannot efficiently re-
duce the influence of parsing errors, but it is easily
available and compatible with the Dep2Str Model.
127
corpus sentences words(ch) words(en)
train 1,501,652 38,388,118 44,901,788
dev 878 22,655 26,905
MT04 1,597 43,719 52,705
MT05 1,082 29,880 35,326
Table 1: Chinese?English corpus. For the English
dev and test sets, words counts are averaged across
4 references.
corpus sentences words(de) words(en)
train 2,037,209 52,671,991 55,023,999
dev 3,003 72,661 74,753
test12 3,003 72,603 72,988
test13 3,000 63,412 64,810
Table 2: German?English corpus. In the dev and
test sets, there is only one English reference for
each German sentence.
5 Experiments
We conduct large-scale experiments to exam-
ine our methods on the Chinese?English and
German?English translation tasks.
5.1 Data
The Chinese?English training corpus is from
the LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004T07,
the Hansards portion of LDC2004T08 and
LDC2005T06. We take NIST 2002 as the de-
velopment set to tune weights, and NIST 2004
(MT04) and NIST 2005 (MT05) as the test data to
evaluate the systems. Table 1 provides a summary
of the Chinese?English corpus.
The German?English training corpus is from
WMT 2014, including Europarl V7 and News
Commentary. News-test 2011 is taken as the de-
velopment set, while News-test 2012 (test12) and
News-test 2013 (test13) are our test sets. Table 2
provides a summary of the German?English cor-
pus.
5.2 Baseline
For both language pairs, we filter sentence pairs
longer than 80 words and keep the length ratio
less than or equal to 3. English sentences are to-
kenized with scripts in Moses. Word alignment is
performed by GIZA++ (Och and Ney, 2003) with
the heuristic function grow-diag-final-and (Koehn
et al., 2003). We use SRILM (Stolcke, 2002) to
Systems MT05
XJ 33.91
D2S 33.79
Table 3: BLEU score [%] of the Dep2Str model
before (XJ) and after (D2S) dependency tree be-
ing transformed. Systems are trained on a selected
1.2M Chinese?English corpus.
train a 5-gram language model on the Xinhua por-
tion of the English Gigaword corpus 5th edition
with modified Kneser-Ney discounting (Chen and
Goodman, 1996). Minimum Error Rate Train-
ing (Och, 2003) is used to tune weights. Case-
insensitive BLEU (Papineni et al., 2002) is used to
evaluate the translation results. Bootstrap resam-
pling (Koehn, 2004) is also performed to compute
statistical significance with 1000 iterations.
We implement the baseline Dep2Str model
in Moses with methods described in this paper,
which is denoted as D2S. The first experiment we
do is to sanity check our implementation. Thus
we take a separate system (denoted as XJ) for
comparison which implements the Dep2Str model
based on (Xie et al., 2011). As shown in Table
3, using the transformation of dependency trees,
the Dep2Str model implemented in Moses (D2S)
is comparable with the standard implementation
(XJ).
In the rest of this section, we describe exper-
iments which compare our system with Moses
HPB (default setting), and test whether our de-
composition approach improves performance over
the baseline D2S.
As described in Section 2, the Dep2Str model
only extracts phrase rules for translating a source
word (head rule). This model could be enhanced
by including phrase rules that cover more than one
source word. Thus we also conduct experiments
where phrase pairs
5
are added into our system. We
set the length limit for phrase 7.
5.3 Chinese?English
In the Chinese?English translation task, the Stan-
ford Chinese word segmenter (Chang et al., 2008)
is used to segment Chinese sentences into words.
The Stanford dependency parser (Chang et al.,
2009) parses a Chinese sentence into the projec-
tive dependency tree.
5
In this paper, the use of phrasal rules is similar to that of
the HPB model, so they can be handled by Moses directly.
128
Systems MT04 MT05
Moses HPB 35.56 33.99
D2S 33.93 32.56
+pseudo-forest 34.28 34.10
+sub-structural rules 34.78 33.63
+pseudo-forest 35.46 34.13
+phrase 36.76* 34.67*
Table 4: BLEU score [%] of our method and
Moses HPB on the Chinese?English task. We use
bold font to indicate that the result of our method
is significantly better than D2S at p ? 0.01 level,
and * to indicate the result is significantly better
than Moses HPB at p ? 0.01 level.
Table 4 shows the translation results. We find
that the decomposition approach proposed in this
paper, including sub-structural rules and pseudo-
forest, improves the baseline system D2S sig-
nificantly (absolute improvement of +1.53/+1.57
(4.5%/4.8%, relative)). As a result, our sys-
tem achieves comparable (-0.1/+0.14) results with
Moses HPB. After including phrasal rules, our
system performs significantly better (absolute im-
provement of +1.2/+0.68 (3.4%/2.0%, relative))
than Moses HPB on both test sets.
6
5.4 German?English
We tokenize German sentences with scripts in
Moses and use mate-tools
7
to perform morpho-
logical analysis and parse the sentence (Bohnet,
2010). Then the MaltParser
8
converts the parse
result into the projective dependency tree (Nivre
and Nilsson, 2005).
Experimental results in Table 5 show that incor-
porating sub-structural rules improves the base-
line D2S system significantly (absolute improve-
ment of +0.47/+0.63, (2.3%/2.8%, relative)), and
achieves a slightly better (+0.08) result on test12
than Moses HPB. However, in the German?
English task, the pseudo-forest produces a neg-
ative effect on the baseline system (-0.07/-0.45),
despite the fact that our system combining both
methods together is still better (+0.2/+0.11) than
the baseline D2S. In the end, by resorting to
6
In our preliminary experiments, phrasal rules are also
able to significantly improve our system on their own on both
Chinese?English and German?English tasks, but the best per-
formance is achieved by combining them with sub-structural
rules and/or pseudo-forest.
7
http://code.google.com/p/mate-tools/
8
http://www.maltparser.org/
Systems test12 test13
Moses HPB 20.44 22.77
D2S 20.05 22.13
+pseudo-forest 19.98 21.68
+sub-structural rules 20.52 22.76
+phrase 20.91* 23.46*
+pseudo-forest 20.25 22.24
+phrase 20.75* 23.20*
Table 5: BLEU score [%] of our method and
Moses HPB on German?English task. We use
bold font to indicate that the result of our method
is significantly better than baseline D2S at p ?
0.01 level, and * to indicate the result is signifi-
cantly better than Moses HPB at p ? 0.01 level.
Systems
# Rules
CE task DE task
Moses HPB 388M 684M
D2S 27M 41M
+sub-structural rules 116M 121M
+phrase 215M 274M
Table 6: The number of rules in different sys-
tems On the Chinese?English (CE) and German?
English (DE) corpus. Note that pseudo-forest (not
listed) does not influence the number of rules.
phrasal rules, our system achieves the best perfor-
mance overall which is significantly better (abso-
lute improvement of +0.47/+0.59 (2.3%/2.6%, rel-
ative)) than Moses HPB.
5.5 Discussion
Besides long-distance reordering (Xie et al.,
2011), another attraction of the Dep2Str model is
its simplicity. It can perform fast translation with
fewer rules than HPB. Table 6 shows the number
of rules in each system. It is easy to see that all of
our systems use fewer rules than HPB. However,
the number of rules is not proportional to transla-
tion quality, as shown in Tables 4 and 5.
Experiments on the Chinese?English corpus
show that it is feasible to translate the dependency
tree via transformation for the Dep2Str model de-
scribed in Section 2. Such a transformation causes
the model to be easily integrated into Moses with-
out making changes to the decoder, while at the
same time producing comparable results with the
standard implementation (shown in Table 3).
The decomposition approach proposed in this
129
paper also shows a positive effect on the base-
line Dep2Str system. Especially, sub-structural
rules significantly improve the Dep2Str model on
both Chinese?English and German?English tasks.
However, experiments show that the pseudo-forest
significantly improves the D2S system on the
Chinese?English data, while it causes translation
quality to decline on the German?English data.
Since using the pseudo-forest in our system is
aimed at translating larger HD fragments via split-
ting it into pieces, we hypothesize that when trans-
lating German sentences, the pseudo-forest ap-
proach more likely results in much worse rules be-
ing applied. This is probably due to the shorter
Mean Dependency Distance (MDD) and freer
word order of German sentences(Eppler, 2013).
6 Conclusion
In this paper, we present an open source mod-
ule which integrates a dependency-to-string model
into Moses.
This module transforms an input depen-
dency tree into a corresponding constituent tree
during decoding which makes Moses perform
dependency-based translation without necessitat-
ing any changes to the decoder. Experiments on
Chinese?English show that the performance if our
system is comparable with that of the standard
dependency-based decoder.
Furthermore, we enhance the model by de-
composing head-dependent fragments into smaller
pieces. This decomposition enriches the Dep2Str
model with more rules during training and allows
us to create a pseudo-forest as input instead of
a dependency tree during decoding. Large-scale
experiments on Chinese?English and German?
English tasks show that this decomposition can
significantly improve the baseline dependency-
to-string model on both language pairs. On
the German?English task, sub-structural rules are
more useful than the pseudo-forest input. In the
end, by resorting to phrasal rules, our system
performs significantly better than the hierarchical
phrase-based model in Moses.
Our implementation of the dependency-to-
string model with methods described in this pa-
per is available at http://computing.dcu.
ie/
?
liangyouli/dep2str.zip. In the fu-
ture, we would like to conduct more experiments
on other language pairs to examine this model,
as well as reducing the restrictions on decompo-
sition.
Acknowledgments
This research has received funding from the Peo-
ple Programme (Marie Curie Actions) of the Eu-
ropean Union?s Seventh Framework Programme
FP7/2007-2013/ under REA grant agreement no.
317471. This research is also supported by the
Science Foundation Ireland (Grant 12/CE/I2267)
as part of the Centre for Next Generation Local-
isation at Dublin City University. The authors of
this paper also thank the reviewers for helping to
improve this paper.
References
Bernd Bohnet. 2010. Very High Accuracy and Fast
Dependency Parsing is Not a Contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 89?97, Beijing,
China.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese Word Seg-
mentation for Machine Translation Performance. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 224?232, Columbus,
Ohio.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative Re-
ordering with Chinese Grammatical Relations Fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59, Boulder, Colorado.
Stanley F. Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. In Proceedings of the 34th Annual
Meeting on Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, Courant Institute of Math-
ematical Sciences, New York University, New York,
NY.
Michael Collins, Lance Ramshaw, Jan Haji?c, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 505?512, College
Park, Maryland.
Eva M. Duran Eppler. 2013. Dependency Distance
and Bilingual Language Use: Evidence from Ger-
man/English and Chinese/English Data. In Proceed-
ings of the Second International Conference on De-
pendency Linguistics (DepLing 2013), pages 78?87,
Prague, August.
130
Heidi J. Fox. 2002. Phrasal Cohesion and Statis-
tical Machine Translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in Nat-
ural Language Processing - Volume 10, pages 304?
3111, Philadelphia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop
on Computationally Hard Problems and Joint Infer-
ence in Speech and Language Processing, pages 1?
8, New York City, New York.
Richard Hudson. 1990. English Word Grammar.
Blackwell, Oxford, UK.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-Analysis Algorithm for Context-Free Lan-
guages. Technical report, Air Force Cambridge Re-
search Lab, Bedford, MA.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, pages 177?180, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Edmonton, Canada.
Arul Menezes and Chris Quirk. 2005. Dependency
Treelet Translation: The Convergence of Statistical
and Example-Based Machine-translation? In Pro-
ceedings of the Workshop on Example-based Ma-
chine Translation at MT Summit X, September.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u,
and Qun Liu. 2013. Translation with Source Con-
stituency and Dependency Trees. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1066?1076, Seattle,
Washington, USA, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of ACL-08: HLT,
pages 192?199, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 99?106, Ann Arbor,
Michigan.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, pages 160?167,
Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Transla-
tion. Computational Linguistics, 36(4):649?671,
December.
Andreas Stolcke. 2002. SRILM-an Extensible Lan-
guage Modeling Toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286, November.
Fei Xia and Martha Palmer. 2001. Converting De-
pendency Structures to Phrase Structures. In Pro-
ceedings of the First International Conference on
Human Language Technology Research, pages 1?5,
San Diego.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-string Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226, Edinburgh, United Kingdom.
Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment
Dependency-to-String Translation with Fixed and
Floating Structures. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics, pages 2217?2226, Dublin, Ireland.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A De-
pendency Treelet String Correspondence Model for
Statistical Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 40?47, Prague, June.
Daniel H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n
3
. Information
and Control, 10(2):189?208.
Arnold M. Zwicky. 1985. Heads. Journal of Linguis-
tics, 21:1?29, 3.
131
