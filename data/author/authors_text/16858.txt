Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 88?94,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
UBIU for Multilingual Coreference Resolution in OntoNotes
Desislava Zhekova Sandra Ku?bler Joshua Bonner Marwa Ragheb Yu-Yin Hsu
Indiana University
Bloomington, IN, USA
{dzhekova, skuebler, jebonner, mragheb, hsuy}@indiana.edu
Abstract
The current work presents the participa-
tion of UBIU (Zhekova and Ku?bler, 2010)
in the CoNLL-2012 Shared Task: Model-
ing Multilingual Unrestricted Coreference in
OntoNotes (Pradhan et al, 2012). Our system
deals with all three languages: Arabic, Chi-
nese and English. The system results show
that UBIU works reliably across all three lan-
guages, reaching an average score of 40.57 for
Arabic, 46.12 for Chinese, and 48.70 for En-
glish. For Arabic and Chinese, the system pro-
duces high precision, while for English, preci-
sion and recall are balanced, which leads to
the highest results across languages.
1 Introduction
Multilingual coreference resolution has been gain-
ing considerable interest among researchers in re-
cent years. Yet, only a very small number of sys-
tems target coreference resolution (CR) for more
than one language (Mitkov, 1999; Harabagiu and
Maiorano, 2000; Luo and Zitouni, 2005). A first
attempt at gaining insight into the comparability of
systems on different languages was accomplished in
the SemEval-2010 Task 1: Coreference Resolution
in Multiple Languages (Recasens et al, 2010). Six
systems participated in that task, UBIU (Zhekova
and Ku?bler, 2010) among them. However, since sys-
tems participated across the various languages rather
irregularly, Recasens et al (2010) reported that the
data points were too few to allow for a proper com-
parison between different approaches. Further sig-
nificant issues concerned system portability across
the various languages and the respective language
tuning, the influence of the quantity and quality of
diverse linguistic annotations as well as the perfor-
mance and behavior of various evaluation metrics.
The CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes (Pradhan et al,
2011) targeted unrestricted CR, which aims at iden-
tifying nominal coreference but also event corefer-
ence, within an English data set from the OntoNotes
corpus. Not surprisingly, attempting to include such
event mentions had a detrimental effect on over-
all accuracy, and the best performing systems (e.g.,
(Lee et al, 2011)) did not attempt event anaphora.
The current shared task extends the task definition to
three different languages (Arabic, Chinese and En-
glish), which can prove challenging for rule-based
approaches such as the best performing system from
2011 (Lee et al, 2011).
In the current paper, we present UBIU, a memory-
based coreference resolution system, and its re-
sults in the CoNLL-2012 Shared Task. We give an
overview of UBIU in Section 2. In Section 3, we
present the system results, after which Section 4 lays
out some conclusive remarks.
2 UBIU
UBIU (Zhekova and Ku?bler, 2010) is a corefer-
ence resolution system designed specifically for a
multilingual setting. As shown by Recasens et al
(2010), multilingual coreference resolution can be
approached by various machine learning methods
since machine learning provides a possibility for ro-
bust abstraction over the variation of language phe-
nomena and specificity. Therefore, UBIU employs88
a machine learning approach, memory-based learn-
ing (MBL) since it has proven to be a good so-
lution to various natural language processing tasks
(Daelemans and van den Bosch, 2005). We em-
ploy TiMBL (Daelemans et al, 2010), which uses
k nearest neighbour classification to assign class la-
bels to the targeted instances. The classifier set-
tings we used were determined by a non-exhaustive
search over the development data and are as follows:
the IB1 algorithm, similarity is computed based on
weighted overlap, gain ratio is used for the relevance
weights and the number of nearest neighbors is set to
k=3 (cf. (Daelemans et al, 2010) for an explanation
of the system parameters).
In UBIU, we use a pairwise mention model (Soon
et al, 2001; Broscheit et al, 2010) since this model
has proven more robust towards multiple languages
(Wunsch, 2009) than more elaborate ones. We con-
centrate on nominal coreference resolution, i.e. we
ignore the more unrestricted cases of event corefer-
ence. Below, we describe the modules used in UBIU
in more detail.
2.1 Preprocessing
The preprocessing module oversees the proper for-
matting of the data for all modules applied in later
stages during coreference resolution. During pre-
processing, we use the speaker information, if pro-
vided, and replace all 1st person singular pronouns
from the token position with the information pro-
vided in the speaker column and adjust the POS tag
correspondingly.
2.2 Mention Detection
Mention detection is the process of detecting the
phrases that are potentially coreferent and are thus
considered candidates for the coreference process.
Mention detection in UBIU is based on the parse and
named entity information provided by the shared
task. This step is crucial for the overall system per-
formance, and we aim for high recall at this stage.
Singleton mentions that are added in this step can
be filtered out in later stages. However, if we fail
to detect a mention in this stage, it cannot be added
later. We predict a mention for each noun phrase and
named entity provided in the data. Additionally, we
extract mentions for possessive pronouns in English
as only those did not correspond to a noun phrase in
MD
R P F1
Arabic 97.13 19.06 31.87
Chinese 98.33 31.64 47.88
English 96.73 30.75 46.67
Table 1: Mention detection (development set).
the syntactic structure provided by the task. In Ara-
bic and Chinese, possessives are already marked as
noun phrases.
The system results on mention detection on the
development set are listed in Table 1. The results
show that we reach very high recall but low preci-
sion, as intended. The majority of the errors are due
to discrepancies between noun phrases and named
entities on the one hand and mentions on the other.
Furthermore, since we do not target event corefer-
ence, we do not add mentions for the verbs in the
data, which leads to a reduction of recall.
In all further system modules, we represent a
mention by its head, which is extracted via heuris-
tic methods. For Arabic, we select the first noun or
pronoun while for Chinese and English, we extract
the the pronoun or the last noun of a mention unless
it is a common title. Additionally, we filter out men-
tions that correspond to types of named entities that
in a majority of the cases in the training data are not
coreferent (i.e. cardinals, ordinals, etc.).
One problem with representing mentions mostly
by their head is that it is difficult to decide between
the different mention spans of a head. Since auto-
matic mentions are considered correct only if they
match the exact span of a gold mention, we include
all identified mention spans for every extracted head
for classification, which can lead to losses in evalu-
ation. For example, consider the instance from the
development set in (1): the noun phrase the Avenue
of Stars is coreferent and thus marked as a gold men-
tion (key 7). UBIU extracts two different spans for
the same head Avenue: the Avenue (MD 3) and the
Avenue of Stars (MD 5).
(1)
token POS parse key MD output
the DT (NP(NP* (7 (3|(5 (9
Avenue NNP *) - 3) 9)
of IN (PP* - - -
Stars NNPS (NP*))) 7) (4)|5) -
Both mention spans are passed to the coreference
resolver, together with additional features (i.e. men-89
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
long 100.0 100.0 100.0 100.0 100.0
short 50.00 0 66.66 66.66 44.44
Table 2: The scores for the short example in (1).
tion length, head modification, etc.) that will allow
the resolver to distinguish between the spans. The
classifier decides that the shorter mention is coref-
erent and that the longer mention is a singleton. In
order to show the effect of this decision, we assume
that there is one coreferent mention to key 7. We
consider the two possible spans and show the re-
spective scores in Table 2. The evaluation in Table 2
shows that providing the correct coreference link but
the wrong, short mention span, the Avenue, has con-
siderable effects to the overall performance. First,
as defined by the task, the mention is ignored by all
evaluation metrics leading to a decrease in mention
detection and coreference performance. Moreover,
the fact that this mention is ignored means that the
second mention becomes a singleton and is not con-
sidered by MUC either, leading to an F1 score of 0.
This example shows the importance of selecting the
correct mention span.
2.3 Singleton Classification
A singleton is a mention which corefers with no
other mention, either because it does not refer to any
entity or because it refers to an entity with no other
mentions in the discourse. Because singletons com-
prise the majority of mentions in a discourse, their
presence can have a substantial effect on the perfor-
mance of machine learning approaches to CR, both
because they complicate the learning task and be-
cause they heavily skew the proportion in the train-
ing data towards negative instances, which can bias
the learner towards assuming no coreference relation
between pairs of mentions. For this reason, informa-
tion concerning singletons needs to be incorporated
into the CR process so that such mentions can be
eliminated from consideration.
Boyd et al (2005), Ng and Cardie (2002), and
Evans (2001) experimented with machine learning
approaches to detect and/or eliminate singletons,
finding that such a module provides an improve-
ment in CR performance provided that the classifier
# Feature Description
1 the depth of the mention in the syntax tree
2 the length of the mention
3 the head token of the mention
4 the POS tag of the head
5 the NE of the head
6 the NE of the mention
7 PR if the head is premodified, PO if it is not; UN otherwise
8 D if the head is in a definite mention; I otherwise
9 the predicate argument corresponding to the mention
10 left context token on position token -3
11 left context token on position token -2
12 left context token on position token -1
13 left context POS tag of token on position token -3
14 left context POS tag of token on position token -2
15 left context POS tag of token on position token -1
10 right context token on position token +1
11 right context token on position token +2
12 right context token on position token +3
13 right context POS tag of token on position token +1
14 right context POS tag of token on position token +2
15 right context POS tag of token on position token +3
16 the syntactic label of the mother node
17 the syntactic label of the grandmother node
18 a concatenation of the labels of the preceding nodes
19 C if the mention is in a PP; else I
Table 3: The features used by the singleton classifier.
does not eliminate non-singletons too frequently. Ng
(2004) additionally compared various feature- and
constraint-based approaches to incorporating single-
ton information into the CR pipeline. Feature-based
approaches integrate information from the single-
ton classifier as features while constraint-based ap-
proaches filter singletons from the mention set. Fol-
lowing these works, we include a k nearest neigh-
bor classifier for singleton mentions in UBIU with
19 commonly-used features described below. How-
ever, unlike Ng (2004), we use a combination of the
feature- and constraint-based approaches to incorpo-
rate the classifier?s results.
Each training/testing instance represents a noun
phrase or a named entity from the data together with
features describing this phrase in its discourse. The
list of features is shown in Table 3. The instances
that are classified by the learner as singletons with
a distance to their nearest neighbor below a thresh-
old (i.e., half the average distance observed in the
training data) are filtered from the mention set, and
are thus not considered in the pairwise coreference
classification. For the remainder of the mentions, the
class that the singletons classifier has assigned to the
instance is used as a feature in the coreference clas-
sifier. Experiments on the development set showed90
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
Arabic
+SC 58.36 34.75 58.26 37.39 43.47
-SC 56.12 34.96 58.52 36.05 43.18
Chinese
+SC 52.30 42.70 61.11 32.86 45.56
-SC 50.40 41.19 60.96 32.47 44.87
English
+SC 67.38 53.20 59.23 34.90 49.11
-SC 65.55 51.57 59.18 34.38 48.38
Table 4: Evaluation of using (+SC) or not (-SC) the sin-
gleton classifier in UBIU on the development set.
that the most important features across all languages
are the POS tag of the head word, definiteness, and
the mother node in the syntactic representation. In-
formation about head modification is helpful for En-
glish and Arabic, but not for Chinese.
The results of using the singleton classifier in
UBIU on the development set are shown in Table 4.
They show a moderate improvement for all evalu-
ation metrics and all languages, with the exception
of MUC and B3 for Arabic. The most noticeable
improvement can be observed in mention detection,
which gains approx. 2% in all languages. A man-
ual inspection of the development data shows that
the version using the singleton classifier extracts a
slightly higher number of coreferent mentions than
the version without. However, the reduction of men-
tions that are never coreferent, which was the main
goal of the singleton classifier, is also present in the
version without the classifier, so that the results of
the classifier only have a minimal influence on the
final results.
2.4 Coreference Classification
Coreference classification is the process in which
all identified mentions are paired up and features
are extracted to build feature vectors that represent
the mention pairs in their context. Each mention
is represented in the feature vector by its syntactic
head. The vectors for the pairs are then used by the
memory-based learner TiMBL.
As anaphoric mentions, we consider all definite
phrases; we then create a pair for each anaphor with
each mention preceding it within a window of 10
(English, Chinese) or 7 (Arabic) sentences. We con-
sider a shorter window of sentences for Arabic be-
cause of its NP-rich syntactic structure and its longer
sentences, which leads to an increased number of
possible mention pairs. The set of features that we
use, listed in Table 5, is an extension of the set by
Rahman and Ng (2009). Before classification, we
apply a morphological filter, which excludes vectors
that disagree in number or gender (applied only if
the respective information is provided or can be de-
duced from the data).
Both the anaphor and the antecedent carry a la-
bel assigned to them by the singletons classifier.
Yet, we consider as anaphoric only the heads of
definite mentions. Including a feature representing
the class assigned by the singletons classifier for
each anaphor triggers a conservative learner behav-
ior, i.e., fewer positive classes are assigned. Thus, to
account for this behavior, we ignore those labels for
the anaphor and include only one feature (no. 25 in
Table 5) in the vector for the antecedent.
2.5 Postprocessing
In postprocessing, we create the equivalence classes
of mentions that were classified as coreferent and
# Feature Description
1 mj - the antecedent
2 mk - the mention (further m.) to be resolved
3 C if mj is a pronoun; else I
4 C if mk is a pronoun; else I
5 the concatenated values of feature 3 and feature 4
6 C if the m. are the same string; else I
7 C if one m. is a substring of the other; else I
8 C if both m. are pronominal and are the same string; else I
9 C if both are non-pronominal and are the same string; else I
10 C if both are pronouns; I if neither is a pronoun; else U
11 C if both are proper nouns; I if neither is; else U
12 C if both m. have the same speaker; I if they do not
13 C if both m. are the same named entity; I if they are not and
U if they are not assigned a NE
14 token distance between mj and mk
15 sentence distance between mj and mk
16 normalised levenstein distance for both m.
17 PR if mj is premodified, PO if it is not; UN otherwise
18 PR if mk is premodified, PO if it is not; UN otherwise
19 the concatenated values for feature 17 and 18
20 D if mj is in a definite m.; I otherwise
21 C if mj is within the subject; I-within an object; U otherwise
22 C if mk is within the subject; I-within an object; U otherwise
23 C if neither is embedded in a PP; I otherwise
24 C if neither is embedded in a NP; I otherwise
25 C if mj has been classified as singleton; I otherwise
26 C if both are within ARG0-ARG4; I-within ARGM; else U
27 C if mj is within ARG0-ARG4; I-within ARGM; else U
28 C if mk is within ARG0-ARG4; I-within ARGM; else U
29 concatenated values for features 27 and 28
30 the predicate argument label for mj
31 the predicate argument label for mk
32 C if both m. agree in number; else I
33 C if both m. agree in gender; else I
Table 5: The features used by the coreference classifier.91
MD MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
Automatic Mention Detection
auto
Arabic 27.54 80.34 41.02 19.64 62.13 29.85 41.91 90.72 57.33 56.79 24.81 34.53 40.57
Chinese 35.12 72.52 47.32 31.19 57.97 40.56 49.49 77.65 60.45 45.92 25.24 32.58 44.53
English 65.78 68.49 67.11 54.28 52.79 53.52 62.26 54.90 58.35 33.52 34.96 34.22 48.70
gold
Arabic 28.00 82.21 41.78 15.47 45.92 23.15 39.22 84.86 53.65 55.10 24.22 33.65 36.82
Chinese 37.84 74.84 50.27 33.95 60.29 43.44 50.95 77.28 61.41 46.68 26.13 33.50 46.12
English 66.05 69.62 67.79 54.45 53.59 54.02 61.66 55.62 58.48 33.82 34.65 34.23 48.91
Gold Mention Boundaries
auto
Arabic 27.48 75.53 40.29 18.75 56.47 28.16 42.67 89.25 57.74 55.53 25.36 34.82 40.24
Chinese 36.97 73.98 49.30 32.09 58.30 41.39 49.43 77.38 60.32 46.35 25.71 33.07 44.93
English 66.45 70.91 68.61 54.96 54.67 54.82 61.85 55.60 58.56 34.38 34.67 34.53 49.30
gold
Arabic 28.06 82.39 41.87 15.56 46.18 23.28 39.23 84.95 53.67 55.10 24.20 33.63 36.86
Chinese 37.89 74.79 50.30 33.93 60.19 43.39 50.87 77.27 61.35 46.62 26.13 33.49 46.08
English 65.82 71.72 68.65 54.68 55.51 55.09 61.22 56.59 58.82 34.85 34.04 34.44 49.45
Gold Mentions
auto
Arabic 100 100 100 42.48 80.36 55.58 50.87 89.69 64.92 71.96 34.52 46.66 55.72
Chinese 100 100 100 42.02 79.57 55.00 50.22 80.81 61.94 60.27 27.08 37.37 51.44
English 100 100 100 68.38 78.11 72.92 63.04 58.60 60.74 52.64 37.10 43.53 59.06
gold
Arabic 100 100 100 45.58 73.27 56.20 52.27 82.35 63.95 70.17 37.54 48.91 56.35
Chinese 100 100 100 44.12 80.89 57.10 51.79 80.53 63.04 60.37 27.69 37.96 52.70
English 100 100 100 68.54 78.10 73.01 63.14 58.63 60.80 52.84 37.44 43.83 59.21
Table 6: UBIU system performance in the shared task.
insert the appropriate class/entity IDs in the data,
removing mentions that constitute a class on their
own ? singletons. We bind all pronouns (except the
ones that were labeled as singletons by the singleton
classifier) that were not assigned an antecedent to
the last seen subject and if such is not present to the
last seen mention. We consider all positively classi-
fied instances in the clustering process.
3 Evaluation
The results of the final system evaluation are pre-
sented in Table 6. Comparing the results for mention
detection (MD) on the development set (see Table 1,
which shows MD before the resolution step) and the
final test set (Table 6, showing MD after resolution
and the deletion of singletons), we encounter a rever-
sal of precision and recall tendencies (even though
the results are not fully comparable since they are
based on different data sets). This is due to the fact
that during mention detection, we aim for high re-
call, and after coreference resolution, all mentions
identified as singletons by the system are excluded
from the answer set. Thus mentions that are coref-
erent in the key set but wrongly classified in the an-
swer set are removed, leading to a decrease in re-
call. With regard to MD precision, a considerable
increase is recorded, showing that the majority of
the mentions that the system indicates as coreferent
have the correct mention spans. Additionally, the
problem of selecting the correct span (as described
in Section 2) is another factor that has a considerable
effect on precision at that stage ? mentions that were
accurately attached to the correct coreference chain
are not considered if their span is not identical to the
span of their counterparts in the key set.
Automatic Mention Detection In the first part in
Table 6, we show the system scores for UBIU?s per-
formance when no mention information is provided
in the data. We report both gold (using gold linguis-
tic annotations) and auto (using automatically an-
notated data) settings. A comparison of the results
shows that there are only minor differences between
them with gold outperforming auto apart from Ara-
bic for which there is a drop of 3.75 points in the
gold setting. However, the small difference between
all results shows that the quality of the automatic an-
notation is good enough for a CR system and that
further improvements in the quality of the linguistic
information will not necessarily improve CR.
If we compare results across languages, we see
that Arabic has the lowest results. One of the rea-
sons for this decreased performance can be found in
the NP-rich syntactic structure of Arabic. This leads
to a high number of identified mentions and in com-
bination with the longer sentence length to a higher92
number of training/test instances. Another reason
for the drop in performance for Arabic can be found
in the lack of annotations expected by our system
(named entities and predicted arguments) that were
not provided by the task due to time constraints and
the accuracy of the annotations. Further, Arabic is
a morphologically rich language for which only the
simplified standard POS tags were provided and not
the gold standard ones that contain much richer and
thus more helpful morphology information.
The results for Chinese and English are relatively
close. We can also see that the CEAFE results are
extremely close, with a difference of less than 1%.
MUC, in contrast, shows the largest differences with
more than 30% between Arabic and English in the
gold setting. It is also noteworthy that the results for
English show a balance between precision and recall
while both Arabic and Chinese favor precision over
recall in terms of mention detection, MUC, and B3.
The reasons for this difference between languages
need to be investigated further.
Gold Mention Boundaries The results for this set
of experiments is based on a version of the test set
that contains the gold boundaries of all mentions, in-
cluding singletons. Thus, we use these gold men-
tion boundaries instead of the ones generated by our
system. These experiments give us an insight on
how well UBIU performs on selecting the correct
boundaries. Since we do not expect the system?s
selection to be perfect, we would expect to see im-
proved system performance given the correct bound-
aries. The results are shown in the second part of
Table 6. As for using automatically generated men-
tions the tendencies in scores between gold and auto
linguistic annotations are kept. A further compari-
son of the overall results between the two settings
also shows only minor changes. The only exception
is the auto setting for Arabic, for which we see drop
in MD precision of approximately 5%. This also re-
sults in lower MUC and B3 precision and CEAFE
recall. The reasons for this drop in performance
need to be investigated further. The fact that most
results for both auto and gold settings change only
sightly shows that having information about the cor-
rect mention boundaries is not very helpful. Thus,
the system seems to have reached its optimal per-
formance on selecting mention boundaries given the
information that it has.
Gold Mentions The last set of experiments is
based on a version of the test set that contains the
gold mentions, i.e., all mentions that are coreferent,
but without any information about the identity of the
coreference chains. The results of this set of exper-
iments gives us information about the quality of the
coreference classifier. The results are shown in the
third part of Table 6. Using gold parses leads to
only minor improvement of the overall system per-
formance, yet, in that case all languages, including
Arabic, show consistent increase of results. Alto-
gether, there is a major improvement of the scores in
MD, MUC, and CEAFE . The B
3 scores only show
minor improvements, resulting from a slight drop in
precision across languages. The results also show
considerably higher precision than recall for MUC
and B3, and higher recall for CEAFE . This means
that the coreference decisions that the system makes
are highly reliable but that it still has a preference
for treating coreferent mentions as singletons.
A comparison across languages shows that pro-
viding gold mentions has a considerable positive ef-
fect on the system performance for Arabic since for
that setting Chinese leads to lower overall scores.
We assume that this is again due to the NP-rich syn-
tactic structure of Arabic and the fact that provid-
ing the mentions decreases drastically the number of
mentions the system works with and has to choose
from during the resolution process.
4 Conclusion and Future Work
We presented the UBIU system for coreference res-
olution in a multilingual setting. The system per-
formed reliably across all three languages of the
CoNLL 2012 shared task. For the future, we are
planning an in-depth investigation of the perfor-
mance of the mention detection module and the sin-
gleton classifier, as well as in investigation into more
complex models for coreference classification than
the mention pair model.
Acknowledgments
This work is based on research supported by the US
Office of Naval Research (ONR) Grant #N00014-
10-1-0140. We would also like to thank Kiran Ku-
mar for his help with tuning the system.93
References
Adriane Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: A machine
learning approach incorporating linguistically moti-
vated patterns. In Proceedings of the ACL Workshop
on Feature Engineering for Machine Learning in Nat-
ural Language Processing, FeatureEng ?05, pages 40?
47, Ann Arbor, MI.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A Multilingual Anaphora Resolution
System. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval), pages 104?
107, Uppsala, Sweden.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2010. TiMBL: Tilburg Memory
Based Learner, version 6.3,reference guide. Techni-
cal Report ILK 10-01, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45 ? 57.
Sanda M. Harabagiu and Steven J. Maiorano. 2000.
Multilingual coreference resolution. In Proceedings
of ANLP 2000, Seattle, WA.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, OR.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
Lingual Coreference Resolution with Syntactic Fea-
tures. In Proceedings of HLT/EMNLP 2005, Vancou-
ver, Canada.
Ruslan Mitkov. 1999. Multilingual anaphora resolution.
Machine Translation, 14(3-4):281?299.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings COLING ?02,
pages 1?7, Taipei, Taiwan.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?04, Barcelona, Spain.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011, Portland, OR.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977, Singapore.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Holger Wunsch. 2009. Rule-Based and Memory-Based
Pronoun Resolution for German: A Comparison and
Assessment of Data Sources. Ph.D. thesis, Universita?t
Tu?bingen.
Desislava Zhekova and Sandra Ku?bler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 96?99, Uppsala,
Sweden.
94
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 169?179,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Inter-annotator Agreement for Dependency Annotation of Learner
Language
Marwa Ragheb
Indiana University
Bloomington, IN USA
mragheb@indiana.edu
Markus Dickinson
Indiana University
Bloomington, IN USA
md7@indiana.edu
Abstract
This paper reports on a study of inter-
annotator agreement (IAA) for a dependency
annotation scheme designed for learner En-
glish. Reliably-annotated learner corpora are
a necessary step for the development of POS
tagging and parsing of learner language. In
our study, three annotators marked several
layers of annotation over different levels of
learner texts, and they were able to obtain
generally high agreement, especially after dis-
cussing the disagreements among themselves,
without researcher intervention, illustrating
the feasibility of the scheme. We pinpoint
some of the problems in obtaining full agree-
ment, including annotation scheme vagueness
for certain learner innovations, interface de-
sign issues, and difficult syntactic construc-
tions. In the process, we also develop ways to
calculate agreements for sets of dependencies.
1 Introduction
Learner corpora have been essential for develop-
ing error correction systems and intelligent tutor-
ing systems (e.g., Nagata et al, 2011; Rozovskaya
and Roth, 2010). So far, error annotation has been
the main focus, to the exclusion of corpora and an-
notation for more basic NLP development, despite
the need for parse information for error detection
(Tetreault et al, 2010), learner proficiency identifi-
cation (Hawkins and Buttery, 2010), and acquisition
research (Ragheb and Dickinson, 2011). Indeed,
there is very little work on POS tagging (Thoue?sny,
2009; van Rooy and Scha?fer, 2002; de Haan, 2000)
or parsing (Rehbein et al, 2012; Krivanek and Meur-
ers, 2011; Ott and Ziai, 2010) learner language, and,
not coincidentally, there is a lack of annotated data
and standards for these tasks. One issue is in know-
ing how to handle innovative learner forms: some
map to a target form before annotating syntax (e.g.,
Hirschmann et al, 2010), while others propose di-
rectly annotating the text (e.g., Ragheb and Dick-
inson, 2011). We follow this latter strand and fur-
ther our work towards a syntactically-annotated cor-
pus of learner English by: a) presenting an annota-
tion scheme for dependencies, integrated with other
annotation layers, and b) testing the inter-annotator
agreement for this scheme. Despite concerns that di-
rect annotation of the linguistic properties of learn-
ers may not be feasible (e.g., Rose?n and Smedt,
2010), we find that annotators have generally strong
agreement, especially after adjudication, and the
reasons for disagreement often have as much to do
with the complexities of syntax or interface issues as
they do with learner innovations.
Probing grammatical annotation can lead to ad-
vancements in research on POS tagging and syntac-
tic parsing of learner language, for it shows what can
be annotated reliably and what needs additional di-
agnostics. We specifically report on inter-annotator
agreement (IAA) for the annotation scheme de-
scribed in section 2, focusing on dependency an-
notation. There are numerous studies investigating
inter-annotator agreement between coders for differ-
ent types of grammatical annotation schemes, focus-
ing on part-of-speech, syntactic, or semantic anno-
tation (e.g., Passonneau et al, 2006; Babarczy et al,
2006; Civit et al, 2003). For learner language, a
169
number of error annotation projects include mea-
sures of interannotator agreement, (see, e.g., Boyd,
2012; Lee et al, 2012; Rozovskaya and Roth, 2010;
Tetreault and Chodorow, 2008; Bonaventura et al,
2000), but as far as we are aware, there have been no
studies on IAA for grammatical annotation.
We have conducted an IAA study to investigate
the quality and robustness of our annotation scheme,
as reported in section 3. In section 4, we report quan-
titative results and a qualitative analysis of this study
to tease apart disagreements due to inherent ambigu-
ity or text difficulty from those due to the annotation
scheme and/or the guidelines. The study has already
reaped benefits by helping us to revise our annota-
tion scheme and guidelines, and the insights gained
here should be applicable for future development of
other annotation schemes and to parsing studies.
On a final note, our dependency annotation allows
for multiple heads for each token in the corpus, vi-
olating the so-called single-head constraint (Ku?bler
et al, 2009). In the process of evaluating these de-
pendencies (see section 4.1), we also make some mi-
nor contributions towards comparing sets of depen-
dencies, moving beyond just F-measure (e.g., Cer
et al, 2010) to account for partial agreements.
2 Annotation scheme
We present a sketch of the annotation scheme here,
outlining the layers and the general motivation. Our
general perspective is to annotate as closely as pos-
sible to what the learner wrote, marking grammat-
ical properties even if the meaning of the sentence
or clause is unclear within the particular grammat-
ical analysis. For example, in the learner sentence
(1), the verb admit clearly occurs in the form of
an active verb, and is annotated as such, regard-
less of the (passive) meaning of the sentence (cf.
was admitted). In this case, basing the annotation
on syntactic evidence makes for a more straightfor-
ward task. Moreover, adhering to a syntactic anal-
ysis helps outline the grammatical properties of a
learner?s interlanguage and can thus assist in auto-
matic tasks such as native language identification
(e.g., Tetreault et al, 2012), and proficiency level de-
termination (Yannakoudakis et al, 2011).
(1) When I admit to Korea University, I decide
...
Another part of the motivation for shying away
from marking target forms and annotating the syn-
tactic properties of those (cf., e.g., Rehbein et al,
2012) is that, for general essays from learners of
many levels, the grammatical evidence can be un-
derstood even when the intended meaning is not.
Consider (2): in the context of the learner?s es-
say, the sentence probably means that this person
guards their personal belongings very well because
of prevalent theft in the city they are talking about.
(2) Now I take very hard my personal stuffs.
Annotating the syntax of a target form here could
obscure the grammatical properties of the learner?s
production (e.g., pluralizing a mass noun). Encour-
aging annotators to focus on the syntactic properties
and not intended meanings makes identifying the de-
pendency relations in a sentence like this one easy.
Another aspect of our annotation scheme is that
we do not directly annotate errors (except for lexi-
cal violations; see section 2.1). Annotators had ac-
cess to an extensive manual detailing the annotation
scheme, which will be made public soon.1 A brief
outline of the guidelines is in section 3.3.
2.1 Initial annotation layers
Using ideas developed for annotating learner lan-
guage (Ragheb and Dickinson, 2012, 2011; D??az-
Negrillo et al, 2010; Dickinson and Ragheb, 2009),
we annotate several layers before targeting depen-
dencies: 1) lemmas (i.e., normalized forms), 2) mor-
phological part-of-speech (POS), 3) distributional
POS, and 4) lexical violations.
The idea for lemma annotation is to normalize a
word to its dictionary form. In (3), for example, the
misspelled excersice is normalized to the correctly-
spelled exercise for the lemma annotation. We spec-
ify that only ?reasonable? orthographic or phonetic
changes are allowed; thus, for prison, it is lemma-
annotated as prison, not person. In this case, the
lemma annotation does not affect the rest of the an-
notation, as prison and person are both nouns, but
for no, the entire analysis changes based on whether
we annotate the lemma as no or not. Marking no
makes the final tree more difficult, but fits with the
principle of staying true to the form the learner has
1See: http://cl.indiana.edu/?salle
170
presented. As we will see in section 4.3, determining
the lemma can pose challenges for building trees.
(3) After to start , I want to tell that this excer-
sice is very important in the life , no only as
a prison .
We annotate two POS layers, one capturing mor-
phological evidence and one for distributional. For
most words, the layers include the same informa-
tion, but mismatches arise with non-canonical struc-
tures. For instance, in (3) the verb (to) start has a
morphological POS of base form verb (VV0), but
it appears in a context where some other verb form
would better be licensed, e.g., a gerund. Since we
do not want to overstate claims, we allow for un-
derspecified POS tags and annotate the distributional
POS simply as verb (VV). The use of two POS lay-
ers captures the mismatch between morphology and
distribution without referencing a unified POS.
Finally, annotators can mark lexical violations
when nothing else appears to capture a non-standard
form. Specifically, lexical violations are for syntac-
tically ungrammatical forms where the specific word
choice seems to cause the ungrammaticality. In (4),
for example, about should be marked as a lexical vi-
olation. Lexical violations were intended as a last re-
sort, but as we will see in section 4.3, there was con-
fusion about when to use lexical violations and when
to use other annotations, e.g., POS mismatches.
(4) ... I agree about me that my country ?s help
and cooperation influenced . . .
2.2 Dependencies
While the initial annotation layers are used to build
the syntactic annotation, the real focus of the anno-
tation concerns dependencies. Using a set of 45 de-
pendencies,2 we mark two types of annotations here:
1) dependency relations rooted in the lemma and the
morphological POS tag, and 2) subcategorization in-
formation, reflecting not necessarily what is in the
tree, but what is required. Justification for a mor-
phological, or morphosyntactic, layer of dependen-
cies, along with a layer of subcategorization, is given
in Ragheb and Dickinson (2012). Essentially, these
two layers allow one to capture issues involving ar-
gument structure (e.g., missing argument), without
2We use a label set adapted from Sagae et al (2010).
having to make the kind of strong claims a layer of
distributional dependencies would require. In (5),
for example, wondered subcategorizes for a finite
complement (COMP), but finds a non-finite comple-
ment (XCOMP), as the tree is based on the morpho-
logical forms (e.g., to).
(5) I wondered what success to be .
An example tree is shown in figure 1, where we
can see a number of properties of our trees: a) we
annotate many ?raised? subjects, such as I being the
subject (SUBJ) of both would and like, thereby al-
lowing for multiple heads for a single token; b) we
ignore semantic anomalies, such as the fact that life
is the subject of be (successful); and c) dependencies
can be selected for, but not realized, as in the case of
career subcategorizing for a determiner (DET).
3 Inter-annotator agreement study
3.1 Selection of annotation texts
From a learner corpus of written essays we have col-
lected from students entering Indiana University, we
chose a topic (What Are Your Plans for Life?) and
randomly selected six essays, based on both learner
proficiency (beginner, intermediate, advanced) and
the native language of the speaker (L1).3 From each
essay, we selected the first paragraph and put the six
paragraphs into two texts; each text contained, in
order, one beginner, one intermediate, and one ad-
vanced paragraph. Text 1 contained 19 sentences
(333 tokens), and Text 2 contained 22 sentences
(271 tokens). Annotators were asked to annotate
only these excerpts, but had access to the entire es-
says, if they wanted to view them.
While the total number of tokens is only 604, the
depth of the annotation is quite significant, in that
there are at least seven decisions to be made for ev-
ery token: lemma, lexical violation, morphological
POS, distributional POS, subcategorization, attach-
ment, and dependency label, in addition to possi-
ble extra dependencies for a given word, i.e., a few
thousand decisions. It is hard to quantify the ef-
fort, as some layers are automatically pre-annotated
(see section 3.5) and some are used sparingly (lexi-
cal violations), but we estimate around 2000 new or
changed annotations from each annotator.
3Korean, Spanish, Chinese, Arabic, Japanese, Hungarian.
171
ROOT I would like my life to be successful in career ...
<ROOT> <SUBJ,VC> <SUBJ,OBJ,XCOMP> <DET> <VC> <SUBJ,PRED> <POBJ> <DET> ...
SUBJ
SUBJ
ROOT VC
DET
OBJ
SUBJ
XCOMP
VC PRED JCT POBJ
Figure 1: Morphosyntactic dependency tree with subcategorization information
3.2 Annotators
This study involved three annotators, who were un-
dergraduate students at Indiana. They were native
speakers of English and majors in Linguistics (2 ju-
niors, 1 senior). Two had had a syntax course before
the semester, and one was taking it concurrently.
We trained them over the course of an academic
semester (fall 2012), by means of weekly meetings
to discuss relevant readings, familiarize them with
the scheme, and give feedback about their annota-
tion. The IAA study took place Nov. 9?Dec. 15.
Annotators were taking course credit for partici-
pating in this project. This being the case, they were
encouraged to learn from the experience, and part
of their training was to make notes of challenging
cases and their decision-making process. This has
provided significant depth in qualitatively analyzing
the IAA outcomes (section 4.3).
3.3 Guidelines
At the start of the study, the annotators were given
a set of guidelines (around 100 pages) to reference
as they made decisions. These guidelines outline
the general principles of the scheme (e.g., give the
learner the benefit of the doubt), an overview of the
annotation layers, and annotation examples for each
layer. The guidelines refer to the label sets used
for POS (Sampson, 1995) and dependencies (Sagae
et al, 2010), but emphasize the properties of our
scheme. Although the guidelines discuss general
syntactic treatment (e.g., ?attach high? in the case of
attachment ambiguities), a considerable focus is on
handling learner innovations, across different layers.
While we cannot list every example of how learners
innovate, we include instructions and examples that
should generalize to other non-native constructions
(e.g., when to underspecify a label). Examples of
Text 1 Text 2
Time Avg. Min. Max. Time Avg. Min. Max.
A 224 11.8 3 25 151 6.9 2 21
B 280 14.7 4 30 170* 8.5 3 20
C 480 25.3 8 60 385 17.5 10 45
Table 1: Annotation time, in minutes, for phase 1 (*times
for two sentences were not reported and are omitted)
how to treat difficult syntactic constructions are also
illustrated (e.g., coordination).
3.4 Annotation task
Via oral and written instructions, the annotators
were asked to independently annotate the two texts
and take notes on difficult issues, in addition to
marking how long they spent on each sentence.
Times are reported in table 1 for the first phase, as
described next. Longer sentences take more time
(cf. Text 1 vs. Text 2), and annotator times vary,
but, given the times of nearly 30?60 minutes per sen-
tence at the start of the semester, these times seemed
reasonable for the depth of annotation required.
The annotation task proceeded in phases. Phase
1: Text 1 was annotated over the course of one
week, and Text 2 over the next week. Phase 2: Af-
ter an hour-long meeting with annotators covering
general annotation points that seemed to be prob-
lematic (e.g., lemma definitions), they were given
another week to individually go over their annota-
tions and make modifications. At the meeting, noth-
ing about the scheme or guidelines was added, and
no specific examples from the data being annotated
were used (only ones from earlier in the semester).
Phase 3: Each annotator received a document point-
ing out pairwise disagreements between annotators,
in a simple textual format like (6). Each annota-
172
tor was asked to use this document and make any
changes where they thought that their analysis was
not the best one, given the other two. This process
took approximately a week. Phase 4: The annota-
tors met (for three hours) and discussed remaining
differences, to see whether they could reach a con-
sensus. Each annotator fixed their own file based on
the results of this discussion. At each point, we took
a snapshot of the data, but at no point did we provide
feedback to the annotators on their decisions.
(6) Sentence 2, word 1: relation ... JCT NJCT
3.5 Annotation interface
The annotation is done via the Brat rapid annotation
tool (Stenetorp et al, 2012).4 This online interface,
shown in figure 2, allows an annotator to drag an
arrow between words to create a dependency. An-
notators were given automatically-derived POS tags
from TnT (Brants, 2000), trained on the SUSANNE
corpus (Sampson, 1995), but created the dependen-
cies from scratch.5 Subcategorizations, lemmas, and
lexical violations are annotated within one of the
POS layers; lemmas are noted by the blue shading,
and the presence of other layers is noted by asterisks,
an interface point discussed in section 4.2.3. Anno-
tators liked the tool, but complained of its slowness.
4 Evaluation
4.1 Methods of comparison
For lemma and POS annotation, we can calculate
basic agreement statistics, as there is one annotation
for each token. But our primary focus is on subcat-
egorization and dependency annotation, where there
can be multiple elements (or none) for a given token.
For subcategorization, we treat elements as mem-
bers of a set, as annotators were told that order was
unimportant (e.g., <SUBJ,OBJ> = <OBJ,SUBJ>);
we discuss metrics for this in section 4.1.1. For de-
pendencies, we adapt standard parse evaluation (see
Ku?bler et al, 2009, ch. 6). In brief, unlabeled at-
tachment agreement (UAA) measures the number
of attachments annotators agree upon for each token,
disregarding the label, whereas labeled attachment
4http://brat.nlplab.org
5Annotators need to provide the dependency annotations
since we lacked an appropriate L2 parser. It is a goal of this
project to provide annotated data for parser development.
agreement (LAA) requires both the attachment and
labeling to be the same to count as an agreement.
Label only agreement (LOA) ignores the head a
token attaches to and only compares labels.
All three metrics (UAA, LAA, LOA) require cal-
culations for sets of dependencies, described in sec-
tions 4.1.1 and 4.1.2. In figure 3, for instance, one
annotator (accidentally) drew a JCT arrow in the
wrong direction, resulting in two heads for is. For
is, the annotator?s set of dependencies is {(0,ROOT),
(1,JCT)}, compared to another?s of {(0,ROOT)}. We
thus treat dependencies as sets of (head, label) pairs.
4.1.1 Metrics
For sets, we use two different calculations. First is
MASI (Measuring Agreement on Set-valued Items,
Passonneau et al, 2006), which assigns each com-
parison between sets a value between 0 and 1, as-
signing partial credit for partial set matches and al-
lowing one to treat agreement on a per-token basis.
We use a simplified form of MASI as follows: 1 =
identical sets, 23 = one set is a subset of the other,
1
3
= the intersection of the sets is non-null, and so are
the set differences, & 0 = disjoint sets.6
The second method is a global comparison
method (GCM), which counts all the elements in
each annotator?s sets in the whole file and counts
up the total number of agreements. In the following
subcategorization example over three tokens, there
are two agreements, compared to four total elements
used by A1 (GCMA1 = 24 ) and compared to three
elements used by A2 (GCMA2 = 23 ). These metrics
are essentially precision and recall, depending upon
which annotator is seen as the ?gold? (Ku?bler et al,
2009, ch. 6). For MASI scores, we have 0, 1, and 13 ,
respectively, giving 113/3, or 0.44.
? A1: {SUBJ}, A2: {}
? A1: {SUBJ}, A2: {SUBJ}
? A1: {SUBJ,PRED}, A2: {SUBJ,OBJ}
Since every word is annotated, the methods as-
sign similar numbers for dependencies. Subcatego-
rization gives different results, due to empty sets. If
annotator 1 and annotator 2 both mark an empty set,
6Since our sets tend to be small (rarely bigger than two), we
do not expect much change with a full MASI calculation.
173
Figure 2: Example of the annotation interface
root In my opinion , My Age is Very Young
JCT
DET
POBJ
PUNCT
DET SUBJ
ROOT
JCT
PRED
Figure 3: A mistaken arrow (JCT) leading to two dependencies for is ((0,ROOT),(1,JCT))
we count full agreement for MASI, i.e., a score of 1;
for GCM, nothing gets added to the totals.
We could, of course, report various coefficients
commonly used in IAA studies, such as kappa or
alpha (see Artstein and Poesio, 2008), but, given
the large number of classes and lack of predominant
classes, chance agreement seems very small.
4.1.2 Dependency-specific issues
As a minor point: for dependencies, we calcu-
late agreements for matches in only attachment or
labeling. Consider (7), where there is one match
only in attachment ((24,OBJ)-(24,JCT)), counting to-
wards UAA, and one only in labeling ((24,SUBJ)-
(22,SUBJ)) for LOA. Importantly, we have to ensure
that (24,SUBJ) and (24,JCT) are not linked.
(7) A1: {(24,SUBJ), (24,OBJ)}
A2: {(22,SUBJ), (24,JCT)}
In general, we prioritize identical attachment over
labeling, if a dependency could match in either.
We wrote a short script to align attachment/label
matches between two sets, but omit details here, due
to space. We generally do not have large sets of de-
pendencies to compare, but these technical decisions
should allow for any situation in the future.
4.2 Results
4.2.1 Bird?s-eye view
Table 2 presents an overview of pairwise agree-
ments between annotators for all 604 tokens. Of the
four phases of annotation, we report two: the files
they annotated (and revised) independently (phase
2) and the final files after discussion of problematic
cases (phase 4). Annotators reported feeling rushed
during phase 1, so phase 2 numbers likely better
indicate the ability to independently annotate, and
phase 4 can help to investigate the reasons for lin-
gering disagreements. The numbers for subcatego-
rization and dependency (UAA, LAA) agreements
are the MASI agreement rates.
A few observations are evident from these fig-
ures. First, for both POSm (morphology) and POSd
(distribution), the high agreement rates reflect the
fact that annotators made very few changes to the
automatic pre-annotation, partly because such lay-
ers were not heavily emphasized. Lemmas were
also pre-annotated, as identical to the surface form,
but more changes were made here (decapitaliza-
tion, affix-stripping, etc.). Comparing phases 2 and
4 shows an improvement in agreement, although
agreement seems like it could be higher, given the
simplicity of lemma information. We discuss lem-
mas, and associated lexical violations, more in sec-
174
Annotators lemma POSm POSd Subcat. UAA LAA
P2 P4 P2 P4 P2 P4 P2 P4 P2 P4 P2 P4
A, B 93.4 96.9 99.0 98.7 99.2 98.7 85.5 94.0 86.6 97.0 80.0 95.2
B, C 94.4 97.7 99.0 99.5 98.7 99.3 86.1 95.7 86.7 97.1 80.3 96.0
C, A 92.4 96.9 99.7 99.7 98.5 99.3 86.1 96.6 86.9 97.7 82.4 96.7
Table 2: Overview of agreement rates before & after discussion (phases 2 & 4)
tion 4.3.
Dependency-related annotations had no pre-
annotation. While the starting value of agreement
rates for these last three layers is not as high as for
lemma and POS annotation, agreement rates around
80?85% still seem moderately high. More important
is how much the agreement rates improved after dis-
cussion, achieving approximately 95% agreement.
This was without any direct intervention from the re-
searchers regarding how to annotate disagreements.
We examine dependencies in section 4.2.2 and sub-
categorization in 4.2.3, breaking results down by
text to see differences in difficulty.
4.2.2 Dependencies
We report MASI agreement rates for dependen-
cies in tables 3 and 4 for Text 1 and Text 2, re-
spectively.7 Comparing the starting agreement val-
ues (e.g., 73.6% vs. 87.8% LAA for annotators A
and B), it is clear that text difficulty had an enor-
mous impact on annotator agreement. The clear dif-
ference in tokens per sentence (17.5 in Text 1 vs.
12.3 in Text 2; see section 3.1) contributed to the
differences. The reported difficulty from annotators
referred to more non-native properties present in the
text, and, to a smaller extent, the presence of more
complex syntactic structures. Though we take up
some of these issues up again in section 4.3, an in-
depth analysis of how text difficulty affects the an-
notation task is beyond the scope of this paper, and
we leave it for future investigation.
Looking at the agreement rates for Text 1 in ta-
ble 3, we can see that the initial rates of agree-
ment for UAA and LOA are moderately high, indi-
cating that annotator training and guideline descrip-
tions were working moderately well. However, they
7We only report MASI scores for dependencies, since the
GCM scores are nearly the same. For example, for raters A &
B, the GCM value for phase 4 is 96.15% with respect to either
annotator vs. 96.10% for MASI.
Ann. UAA LAA LOA
P2 P4 P2 P4 P2 P4
A, B 81.8 96.1 73.6 93.4 80.3 95.5
B, C 80.9 96.2 73.4 94.4 79.3 97.1
A,C 83.6 97.6 79.7 96.7 81.8 97.9
Table 3: MASI percentages for dependencies, Text 1
Ann. UAA LAA LOA
P2 P4 P2 P4 P2 P4
A, B 92.6 98.1 87.8 97.4 89.3 97.8
B, C 93.8 98.3 88.7 97.9 90.2 98.6
A, C 90.9 97.9 85.7 96.8 87.6 97.9
Table 4: MASI percentages for dependencies, Text 2
are only 73% for LAA. Note, though, that this may
be more related to issues of fatigue and hurry than
of understanding of the guidelines: the numbers im-
prove considerably by phase 4. The labeled attach-
ment rates, for example, increase between 17 and 21
percent, to reach values around 95%.
For Text 2 in table 4, we notice again the higher
phase 2 rates and the similar improvement in phase
4, with LAA around 97%. Encouragingly, despite
the initially lower agreements for Text 1, annotators
were able to achieve nearly the same level of agree-
ment as for the ?easier? text. This illustrates that
annotators can learn the scheme, even for difficult
sentences, though there may be a tradeoff between
speed and accuracy.
4.2.3 Subcategorization
For subcategorization, we present both MASI and
GCM percentage rates, as they give different em-
phases. Results are again broken down by text, in
tables 5 and 6. As with dependencies, we see solid
improvement from phase 2 to phase 4, and we see
175
generally higher agreement for Text 2.
Ann. MASI GCM1 GCM2
P2 P4 P2 P4 P2 P4
A,B 84.3 92.4 81.9 90.8 72.8 88.1
B,C 83.6 93.8 74.4 91.6 73.6 90.2
A,C 84.9 96.1 83.0 96.4 73.1 92.2
Table 5: Agreement rates for subcategorization, Text 1
Ann. MASI GCM1 GCM2
P2 P4 P2 P4 P2 P4
A,B 87.1 95.9 88.9 96.0 77.2 94.1
B,C 89.3 98.0 88.3 98.0 82.0 96.8
A,C 87.6 97.2 91.2 97.3 73.7 94.2
Table 6: Agreement rates for subcategorization, Text 2
The GCM numbers are much lower because of the
way empty subcategorization values are handled?
being counted towards agreement for MASI and
not for GCM (see section 4.1.1). A further issue,
though, is that one annotator often simply left out
subcategorization annotation for a token. In table 6,
for example, annotators A and C have vastly differ-
ent GCM values for phase 2 (91.2% vs. 73.7%), due
to annotator C annotating many more subcategoriza-
tion labels. This is discussed more in section 4.3.2.
4.3 Qualitative differences
We highlight some of the important issues that stand
out when we take a closer look at the nature of the
disagreements in the final phase.
4.3.1 Text-related issues
As pointed out earlier regarding the differences
between Text 1 and Text 2 (section 4.2.2), some dis-
agreements are likely due to the nature of the text
itself, both because of its non-native properties and
because of the syntactic complexity. Starting with
unique learner innovations leading to non-uniform
treatment, several cases stemmed from not agreeing
on the lemma, when a word looks non-English or
does not fit the context. An example is cares in (8):
although the guidelines should lead the annotators to
choose care as the lemma, staying true to the learner
form, one annotator chose to accommodate the con-
text and changed the lemma to case. This relying
too heavily on intended meaning and not enough on
syntactic evidence?as the scheme is designed for?
was a consistent problem.
(8) My majors are bankruptcy , corporate reor-
ganizations . . . and arquisisiton cares .
For (8), the trees do not change because the dif-
ferent lemmas are of the same syntactic category,
but more problematic are cases where the trees differ
based on different readings. In the learner sentence
(9), the non-agreement between this and cause led to
a disagreement of this being a COORD of and vs. this
being an APPOS (appositive) of factors. The anno-
tator reported that the choice for this latter analysis
came from treating this as these, again contrary to
guidelines but consistent with one meaning.
(9) Sometimes animals are subjected to changed
environmental factors during their develop-
mental process and this cause FA .
Another great source of disagreement stems from
the syntactic complexity of some of the structures,
even if native-like, though this can be intertwined
with non-native properties, as in (10). Although an-
notators eventually agreed on the annotation here,
there was initial disagreement on the coordination
structure of this sentence, questioning whether to be
coordinates with pursuing or only with to earn, or
whether pursuing coordinates only with to earn (the
analysis they finally chose).
(10) My most important goals are pursuing the
profession to be a top marketing manager
and then to earn a lot of money to buy a
beautiful house and a good car .
4.3.2 Task-related issues
Annotator disagreements stemmed not only from
the text, but from other factors as well, such as as-
pects of the scheme that needed more clarification,
some interface issues, and the fact that the guidelines
though extensive, are still not comprehensive.
A few parts of the annotation scheme were con-
fusing to annotators and likely need refinement. For
example, if the form of a word was incorrect, we
saw a lot of lexical violation annotation, even if it
176
was only an issue of grammatical marking and POS
(e.g., did/VVD instead of done/VVN), as opposed
to a truly different word choice. We are currently
tightening the annotation scheme and adding clarifi-
cations about lexical violations in our guidelines.
As another example, verb raising was often not
marked (cf. figure 1), in spite of the scheme and
guidelines requiring it. In their comments, annota-
tors mentioned that it seemed ?redundant? to them
and that it caused arcs to cross, which they found
?unappealing.? One annotator commented that they
did not have enough syntactic background to see
why marking multiple subjects was necessary. We
are thus considering a simpler treatment. Another
option in the future is to hire annotators with more
background in syntax.
The interface may be partly to blame for some dis-
agreements, including subcategorizations which an-
notators often left unmarked (section 4.2.3) or only
partly marked (e.g., leaving off a SUBJect for a verb
which has been raised). There are a few reasons for
this. First, marking subcategorization likely needed
more emphasis in the training period, seeing as how
it relates to complicated linguistic notions like dis-
tinguishing arguments and adjuncts. Secondly, the
interface is an issue, as the subcategorization field is
not directly visible, compared to the arcs drawn for
dependencies; in figure 2, for instance, subcatego-
rization can only be seen in the asterisks, which need
to be clicked on to be seen and changed. Relatedly,
because it is not always necessary, subcategorization
may seem more optional and thus forgettable.
By the nature of being an in-progress project, the
guidelines were necessarily not comprehensive. As
one example, the TRANS(ition) label was only gen-
erally defined, leading to disagreements. As another,
a slash could indicate coordination (actor/actress),
and annotators differed on its POS labeling, as either
CC (coordinating conjunction), or a PUNCT (punc-
tuation). The different POS labels then led to vastly
different dependency graphs. In spite of a lengthy
section on how to handle coordination in the guide-
lines, it seems that an additional case needs to be
added to the guidelines to cover when punctuation is
used as a conjunction.
5 Conclusion and outlook
Developing reliable annotation schemes for learner
language is an important step towards better POS
tagging and parsing of learner corpora. We have de-
scribed an inter-annotator agreement study that has
helped shed light on several issues, such as the re-
liability of our annotation scheme, and has helped
identify room for improvement. This study shows
that it is possible to apply a multi-layered depen-
dency annotation scheme to learner text with consid-
erably good agreement rates between three trained
annotators. In the future, we will of course be
applying the (revised) annotation scheme to larger
data sets, but we hope other grammatical annota-
tion schemes can learn from our experience. In the
shorter term, we are constructing a gold standard of
the text files used here, to test annotation accuracy
and whether any (or all) annotators had consistent
difficulties. Another next step is to gather a larger
pool of data and focus more on analyzing the ef-
fects of L1 and learner proficiency level on anno-
tation. Finally, given that syntactic representations
can assist in automating tasks such as developmen-
tal profiling of learners (e.g., Vyatkina, 2013), gram-
matical error detection (Tetreault et al, 2010), iden-
tification of native language (e.g., Tetreault et al,
2012), and proficiency level determination (Dickin-
son et al, 2012)?all of which impact NLP-based
educational tools?one can explore the effect of spe-
cific syntactic decisions on such tasks, as a way to
provide feedback on the annotation scheme.
Acknowledgments
We would like to thank the three annotators for their
help with this experiment. We also thank the IU CL
discussion group, as well as the three anonymous
reviewers, for their feedback and comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Compu-
tational Linguistics, 34(4):555?596.
Anna Babarczy, John Carroll, and Geoffrey Samp-
son. 2006. Definitional, personal, and mechanical
constraints on part of speech annotation perfor-
mance. Natural Language Engineering, 12:77?
90.
177
Patrizia Bonaventura, Peter Howarth, and Wolfgang
Menzel. 2000. Phonetic annotation of a non-
native speech corpus. In Proceedings Interna-
tional Workshop on Integrating Speech Technol-
ogy in the (Language) Learning and Assistive In-
terface, InStil, pages 10?17.
Adriane Amelia Boyd. 2012. Detecting and Diag-
nosing Grammatical Errors for Beginning Learn-
ers of German: From Learner Corpus Annotation
to Constraint Satisfaction Problems. Ph.D. thesis,
Ohio State University.
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference (ANLP
2000), pages 224?231. Seattle, WA.
Daniel Cer, Marie-Catherine de Marneffe, Daniel
Jurafsky, and Christopher D. Manning. 2010.
Parsing to Stanford dependencies: Trade-offs be-
tween speed and accuracy. In Proceedings of
LREC-10. Malta.
M. Civit, A. Ageno, B. Navarro, N. Buf??, and M. A.
Mart??. 2003. Qualitative and quantitative analy-
sis of annotators? agreement in the development
of Cast3LB. In Proceedings of 2nd Workshop on
Treebanks and Linguistics Theories (TLT-2003),
pages 33?45.
Pieter de Haan. 2000. Tagging non-native En-
glish with the TOSCA-ICLE tagger. In Christian
Mair and Markus Hundt, editors, Corpus Linguis-
tics and Linguistic Theory, pages 69?79. Rodopi,
Amsterdam.
Ana D??az-Negrillo, Detmar Meurers, Salvador
Valera, and Holger Wunsch. 2010. Towards in-
terlanguage POS annotation for effective learner
corpora in SLA and FLT. Language Forum, 36(1?
2):139?154. Special Issue on New Trends in Lan-
guage Teaching.
Markus Dickinson, Sandra Ku?bler, and Anthony
Meyer. 2012. Predicting learner levels for online
exercises of Hebrew. In Proceedings of the Sev-
enth Workshop on Building Educational Applica-
tions Using NLP, pages 95?104. Association for
Computational Linguistics, Montre?al, Canada.
Markus Dickinson and Marwa Ragheb. 2009. De-
pendency annotation for learner corpora. In Pro-
ceedings of the Eighth Workshop on Treebanks
and Linguistic Theories (TLT-8), pages 59?70.
Milan, Italy.
John A. Hawkins and Paula Buttery. 2010. Criterial
features in learner corpora: Theory and illustra-
tions. English Profile Journal, 1(1):1?23.
Hagen Hirschmann, Anke Lu?deling, Ines Rehbein,
Marc Reznicek, and Amir Zeldes. 2010. Syntactic
overuse and underuse: A study of a parsed learner
corpus and its target hypothesis. Talk given at
the Ninth Workshop on Treebanks and Linguistic
Theory.
Julia Krivanek and Detmar Meurers. 2011. Compar-
ing rule-based and data-driven dependency pars-
ing of learner language. In Proceedings of the Int.
Conference on Dependency Linguistics (Depling
2011). Barcelona.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan & Claypool
Publishers.
Sun-Hee Lee, Markus Dickinson, and Ross Israel.
2012. Developing learner corpus annotation for
Korean particle errors. In Proceedings of the
Sixth Linguistic Annotation Workshop, LAW VI
?12, pages 129?133. Association for Computa-
tional Linguistics, Stroudsburg, PA, USA.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1210?1219. Portland, OR.
Niels Ott and Ramon Ziai. 2010. Evaluating de-
pendency parsing performance on German learner
language. In Proceedings of TLT-9, volume 9,
pages 175?186.
Rebecca Passonneau, Nizar Habash, and Owen
Rambow. 2006. Inter-annotator agreement on a
multilingual semantic annotation task. In Pro-
ceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
pages 1951?1956.
Marwa Ragheb and Markus Dickinson. 2011.
Avoiding the comparative fallacy in the annota-
tion of learner corpora. In Selected Proceedings of
the 2010 Second Language Research Forum: Re-
178
considering SLA Research, Dimensions, and Di-
rections, pages 114?124. Cascadilla Proceedings
Project, Somerville, MA.
Marwa Ragheb and Markus Dickinson. 2012. Defin-
ing syntax for learner language annotation. In
Proceedings of the 24th International Conference
on Computational Linguistics (Coling 2012),
Poster Session. Mumbai, India.
Ines Rehbein, Hagen Hirschmann, Anke Lu?deling,
and Marc Reznicek. 2012. Better tags give better
trees - or do they? Linguistic Issues in Language
Technology (LiLT), 7(10).
Victoria Rose?n and Koenraad De Smedt. 2010. Syn-
tactic annotation of learner corpora. In Hilde Jo-
hansen, Anne Golden, Jon Erik Hagen, and Ann-
Kristin Helland, editors, Systematisk, variert, men
ikke tilfeldig. Antologi om norsk som andrespra?k i
anledning Kari Tenfjords 60-a?rsdag [Systematic,
varied, but not arbitrary. Anthology about Norwe-
gian as a second language on the occasion of Kari
Tenfjord?s 60th birthday], pages 120?132. Novus
forlag, Oslo.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL errors: Challenges and rewards. In Proceed-
ings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational
Applications, pages 28?36. Los Angeles, Califor-
nia.
Kenji Sagae, Eric Davis, Alon Lavie, and
Brian MacWhinney an Shuly Wintner. 2010.
Morphosyntactic annotation of childes tran-
scripts. Journal of Child Language, 37(3):705?
729.
Geoffrey Sampson. 1995. English for the Computer:
The SUSANNE Corpus and Analytic Scheme.
Clarendon Press, Oxford.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi
Tsujii. 2012. brat: a web-based tool for nlp-
assisted text annotation. In Proceedings of the
Demonstrations at the 13th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 102?107. Avignon,
France.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in na-
tive language identification. In Proceedings of
COLING 2012, pages 2585?2602. Mumbai, In-
dia.
Joel Tetreault and Martin Chodorow. 2008. Na-
tive judgments of non-native usage: experiments
in preposition error detection. In Proceedings
of the Workshop on Human Judgements in Com-
putational Linguistics, HumanJudge ?08, pages
24?32. Association for Computational Linguis-
tics, Stroudsburg, PA, USA.
Joel Tetreault, Jennifer Foster, and Martin
Chodorow. 2010. Using parse features for
preposition selection and error detection. In
Proceedings of the ACL 2010 Conference Short
Papers, pages 353?358. Uppsala, Sweden.
Sylvie Thoue?sny. 2009. Increasing the reliability of
a part-of-speech tagging tool for use with learner
language. Presentation given at the Automatic
Analysis of Learner Language (AALL?09) work-
shop on automatic analysis of learner language:
from a better understanding of annotation needs
to the development and standardization of anno-
tation schemes.
Bertus van Rooy and Lande Scha?fer. 2002. The ef-
fect of learner errors on POS tag errors during au-
tomatic POS tagging. Southern African Linguis-
tics and Applied Language Studies, 20:325?335.
Nina Vyatkina. 2013. Specific syntactic complex-
ity: Developmental profiling of individuals based
on an annotated learner corpus. The Modern Lan-
guage Journal, 97(S1):1?20.
Helen Yannakoudakis, Ted Briscoe, and Ben Med-
lock. 2011. A new dataset and method for au-
tomatically grading ESOL texts. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 180?189. Portland, OR.
179
