Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 45?48, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multimodal Generation in the COMIC Dialogue System
Mary Ellen Foster and Michael White
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
{M.E.Foster,Michael.White}@ed.ac.uk
Andrea Setzer and Roberta Catizone
Natural Language Processing Group
Department of Computer Science, University of Sheffield
{A.Setzer,R.Catizone}@dcs.shef.ac.uk
Abstract
We describe how context-sensitive, user-
tailored output is specified and produced
in the COMIC multimodal dialogue sys-
tem. At the conference, we will demon-
strate the user-adapted features of the dia-
logue manager and text planner.
1 Introduction
COMIC1 is an EU IST 5th Framework project com-
bining fundamental research on human-human inter-
action with advanced technology development for
multimodal conversational systems. The project
demonstrator system adds a dialogue interface to a
CAD-like application used in bathroom sales situa-
tions to help clients redesign their rooms. The input
to the system includes speech, handwriting, and pen
gestures; the output combines synthesised speech, a
talking head, and control of the underlying applica-
tion. Figure 1 shows screen shots of the COMIC
interface.
There are four main phases in the demonstra-
tor. First, the user specifies the shape of their
own bathroom, using a combination of speech in-
put, pen-gesture recognition and handwriting recog-
nition. Next, the user chooses a layout for the sani-
tary ware in the room. After that, the system guides
the user in browsing through a range of tiling op-
tions for the bathroom. Finally, the user is given a
1COnversational Multimodal Interaction with Computers;
http://www.hcrc.ed.ac.uk/comic/.
three-dimensional walkthrough of the finished bath-
room. We will focus on how context-sensitive, user-
tailored output is generated in the third, guided-
browsing phase of the interaction. Figure 2 shows
a typical user request and response from COMIC in
this phase. The pitch accents and multimodal ac-
tions are indicated; there is also facial emphasis cor-
responding to the accented words.
The primary goal of COMIC?s guided-browsing
phase is to help users become better informed about
the range of tiling options for their bathroom. In
this regard, it is similar to the web-based system
M-PIRO (Isard et al, 2003), which generates per-
sonalised descriptions of museum objects, and con-
trasts with task-oriented embodied dialogue systems
such as SmartKom (Wahlster, 2003). Since guided
browsing requires extended descriptions, in COMIC
we have placed greater emphasis on producing high-
quality adaptive output than have previous embodied
dialogue projects such as August (Gustafson et al,
1999) and Rea (Cassell et al, 1999). To generate
its adaptive output, COMIC uses information from
the dialogue history and the user model throughout
the generation process, as in FLIGHTS (Moore et
al., 2004); both systems build upon earlier work on
adaptive content planning (Carenini, 2000; Walker
et al, 2002). An experimental study (Foster and
White, 2005) has shown that this adaptation is per-
ceptible to users of COMIC.
2 Dialogue Management
The task of the Dialogue and Action Manager
(DAM) is to decide what the system will show and
say in response to user input. The input to the
45
(a) Bathroom-design application (b) Talking head
Figure 1: Components of the COMIC interface
User Tell me about this design [click on Alt Mettlach]
COMIC [Look at screen]
THIS DESIGN is in the CLASSIC style.
[circle tiles]
As you can see, the colours are DARK RED and OFF WHITE.
[point at tiles]
The tiles are from the ALT METTLACH collection by VILLEROY AND BOCH.
[point at design name]
Figure 2: Sample COMIC input and output
DAM consists of multiple scored hypotheses con-
taining high-level, modality-independent specifica-
tions of the user input; the output is a similar high-
level specification of the system action. The DAM
itself is modality-independent. For example, the in-
put in Figure 2 could equally well have been the user
simply pointing to a design on the screen, with no
speech at all. This would have resulted in the same
abstract DAM input, and thus in the same output: a
request to show and describe the given design.
The COMIC DAM (Catizone et al, 2003) is
a general-purpose dialogue manager which can
handle different dialogue management styles such
as system-driven, user-driven or mixed-initiative.
The general-purpose part of the DAM is a sim-
ple stack architecture with a control structure;
all the application-dependent information is stored
in a variation of Augmented Transition Networks
(ATNs) called Dialogue Action Forms (DAFs).
These DAFs represent general dialogue moves, as
well as sub-tasks or topics, and are pushed onto and
popped off of the stack as the dialogue proceeds.
When processing a user input, the control struc-
ture decides whether the DAM can stay within the
current topic (and thus the current DAF), or whether
a topic shift has occurred. In the latter case, a new
DAF is pushed onto the stack and executed. After
that topic has been exhausted, the DAM returns to
the previous topic automatically. The same princi-
ple holds for error handling, which is implemented
at different levels in our approach.
In the guided-browsing phase of the COMIC sys-
tem, the user may browse tiling designs by colour,
style or manufacturer, look at designs in detail, or
change the amount of border and decoration tiles.
The DAM uses the system ontology to retrieve de-
signs according to the chosen feature, and consults
the user model and dialogue history to narrow down
the resulting designs to a small set to be shown and
described to the user.
46
3 Presentation Planning
The COMIC fission module processes high-level
system-output specifications generated by the DAM.
For the example in Figure 2, the DAM output indi-
cates that the given tile design should be shown and
described, and that the description must mention the
style. The fission module fleshes out such specifica-
tions by selecting and structuring content, planning
the surface form of the text to realise that content,
choosing multimodal behaviours to accompany the
text, and controlling the output of the whole sched-
ule. In this section, we describe the planning pro-
cess; output coordination is dealt with in Section 6.
Full technical details of the fission module are given
in (Foster, 2005).
To create the textual content of a description, the
fission module proceeds as follows. First, it gath-
ers all of the properties of the specified design from
the system ontology. Next, it selects the properties
to include in the description, using information from
the dialogue history and the user model, along with
any properties specifically requested by the dialogue
manager. It then creates a structure for the selected
properties and creates logical forms as input for the
OpenCCG surface realiser. The logical forms may
include explicit alternatives in cases where there are
multiple ways of expressing a property; for exam-
ple, it could say either This design is in the classic
style or This design is classic. OpenCCG makes use
of statistical language models to choose among such
alternatives. This process is described in detail in
(Foster and White, 2004; Foster and White, 2005).
In addition to text, the output of COMIC
also incorporates multimodal behaviours including
prosodic specifications for the speech synthesiser
(pitch accents and boundary tones), facial behaviour
specifications (expressions and gaze shifts), and de-
ictic gestures at objects on the application screen us-
ing a simulated pointer. Pitch accents and bound-
ary tones are selected by the realiser based on the
context-sensitive information-structure annotations
(theme/rheme; marked/unmarked) included in the
logical forms. At the moment, the other multimodal
coarticulations are specified directly by the fission
module, but we are currently experimenting with
using the OpenCCG realiser?s language models to
choose them, using example-driven techniques.
4 Surface Realisation
Surface realisation in COMIC is performed by the
OpenCCG2 realiser, a practical, open-source realiser
based on Combinatory Categorial Grammar (CCG)
(Steedman, 2000b). It employs a novel ensemble of
methods for improving the efficiency of CCG reali-
sation, and in particular, makes integrated use of n-
gram scoring of possible realisations in its chart re-
alisation algorithm (White, 2004; White, 2005). The
n-gram scoring allows the realiser to work in ?any-
time? mode?able at any time to return the highest-
scoring complete realisation?and ensures that a
good realisation can be found reasonably quickly
even when the number of possibilities is exponen-
tial. This makes it particularly suited for use in an
interactive dialogue system such as COMIC.
In COMIC, the OpenCCG realiser uses factored
language models (Bilmes and Kirchhoff, 2003) over
words and multimodal coarticulations to select the
highest-scoring realisation licensed by the grammar
that satisfies the specification given by the fission
module. Steedman?s (Steedman, 2000a) theory of
information structure and intonation is used to con-
strain the choice of pitch accents and boundary tones
for the speech synthesiser.
5 Speech Synthesis
The COMIC speech-synthesis module is imple-
mented as a client to the Festival speech-synthesis
system.3 We take advantage of recent advances in
version 2 of Festival (Clark et al, 2004) by using
a custom-built unit-selection voice with support for
APML prosodic annotation (de Carolis et al, 2004).
Experiments have shown that synthesised speech
with contextually appropriate prosodic features can
be perceptibly more natural (Baker et al, 2004).
Because the fission module needs the timing in-
formation from the speech synthesiser to finalise the
schedules for the other modalities, the synthesiser
first prepares and stores the waveform for its input
text; the sound is then played at a later time, when
the fission module indicates that it is required.
2http://openccg.sourceforge.net/
3http://www.cstr.ed.ac.uk/projects/festival/
47
6 Output Coordination
In addition to planning the presentation content as
described earlier, the fission module also controls
the system output to ensure that all parts of the pre-
sentation are properly coordinated, using the tim-
ing information returned by the speech synthesiser
to create a full schedule for the turn to be generated.
As described in (Foster, 2005), the fission module
allows multiple segments to be prepared in advance,
even while the preceding segments are being played.
This serves to minimise the output delay, as there is
no need to wait until a whole turn is fully prepared
before output begins, and the time taken to speak the
earlier parts of the turn can also be used to prepare
the later parts.
7 Acknowledgements
This work was supported by the COMIC project
(IST-2001-32311). This paper describes only part
of the work done in the project; please see http://
www.hcrc.ed.ac.uk/comic/ for full details. We
thank the other members of COMIC for their col-
laboration during the course of the project.
References
Rachel Baker, Robert A.J. Clark, and Michael White.
2004. Synthesizing contextually appropriate intona-
tion in limited domains. In Proceedings of 5th ISCA
workshop on speech synthesis.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proceedings of HLT-03.
Giuseppe Carenini. 2000. Generating and Evaluating
Evaluative Arguments. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Justine Cassell, Timothy Bickmore, Mark Billinghurst,
Lee Campbell, Kenny Chang, Hannes Vilhja?lmsson,
and Hao Yan. 1999. Embodiment in conversational
interfaces: Rea. In Proceedings of CHI99.
Roberta Catizone, Andrea Setzer, and Yorick Wilks.
2003. Multimodal dialogue management in the
COMIC project. In Proceedings of EACL 2003 Work-
shop on Dialogue Systems: Interaction, adaptation,
and styles of management.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 ? build your own general purpose
unit selection speech synthesiser. In Proceedings of
5th ISCA workshop on speech synthesis.
Berardina de Carolis, Catherine Pelachaud, Isabella
Poggi, and Mark Steedman. 2004. APML, a
mark-up language for believable behaviour generation.
In H Prendinger, editor, Life-like Characters, Tools,
Affective Functions and Applications, pages 65?85.
Springer.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for text planning with XSLT. In Proceedings
of NLPXML-2004.
Mary Ellen Foster and Michael White. 2005. Assessing
the impact of adaptive generation in the COMIC multi-
modal dialogue system. In Proceedings of IJCAI-2005
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems. To appear.
Mary Ellen Foster. 2005. Interleaved planning and out-
put in the COMIC fission module. Submitted.
Joakim Gustafson, Nikolaj Lindberg, and Magnus Lun-
deberg. 1999. The August spoken dialogue system.
In Proceedings of Eurospeech 1999.
Amy Isard, Jon Oberlander, Ion Androtsopoulos, and
Colin Matheson. 2003. Speaking the users? lan-
guages. IEEE Intelligent Systems, 18(1):40?45.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, compara-
tive descriptions in spoken dialogue. In Proceedings
of FLAIRS 2004.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
Mark Steedman. 2000b. The Syntactic Process. MIT
Press.
Wolfgang Wahlster. 2003. SmartKom: Symmetric mul-
timodality in an adaptive and reusable dialogue shell.
In Proceedings of the Human Computer Interaction
Status Conference 2003.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D.
Moore, M. Johnston, and G. Vasireddy. 2002. Speech-
plans: Generating evaluative responses in spoken dia-
logue. In Proceedings of INLG 2002.
Michael White. 2004. Reining in CCG chart realization.
In Proceedings of INLG 2004.
Michael White. 2005. Efficient realization of coordinate
structures in Combinatory Categorial Grammar. Re-
search on Language and Computation. To appear.
48
Techniques for Text Planning with XSLT
Mary Ellen Foster and Michael White
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
Edinburgh EH8 9LW
{mef,mwhite}@inf.ed.ac.uk
Abstract
We describe an approach to text planning that uses
the XSLT template-processing engine to create log-
ical forms for an external surface realizer. Using a
realizer that can process logical forms with embed-
ded alternatives provides a substitute for backtrack-
ing in the text-planning process. This allows the text
planner to combine the strengths of the AI-planning
and template-based traditions in natural language
generation.
1 Introduction
In the traditional pipeline view of natural language
generation (Reiter and Dale, 2000), many steps in-
volve converting between increasingly specific tree
representations. As Wilcock (2001) points out,
this sort of tree-to-tree transformation is a task
to which XML?and particularly XSLT template
processing?is particularly suited.
In this paper, we describe how we plan text by
treating the XSLT processor as a top-down rule-
expanding planner that translates dialogue-manager
specifications into logical forms to be sent to the
OpenCCG text realizer (White and Baldridge, 2003;
White, 2004a; White, 2004b). XSLT is used to per-
form many text-planning tasks, including structur-
ing and aggregating the content, performing lexical
choice via the selection of logical-form templates,
and generating multiple alternative realizations for
messages where possible.
Using an external realizer at the end of the plan-
ning process provides two advantages. First, we
can use the realizer to deal with those aspects of
surface realization that are difficult to implement
in XSLT, but that the realizer is designed to han-
dle (e.g., syntactic agreement via unification). Sec-
ond, we take advantage of OpenCCG?s use of statis-
tical language models by sending multiple alterna-
tive logical forms to the realizer, and having it make
the final choice of surface form. Allowing the text
planner to produce multiple alternatives also obvi-
ates the need for backtracking, which is not some-
thing that is otherwise easily incorporated into the a
system based on XSLT processing.
We have implemented this approach in two di-
alogue systems. In this paper, we concentrate on
how text is planned in the COMIC multimodal di-
alogue system (den Os and Boves, 2003). Similar
techniques are also used in the FLIGHTS spoken-
dialogue system (Moore et al, 2004), which gener-
ates user-tailored descriptions and comparisons of
flight itineraries.
The rest of this paper is organized as follows:
Section 2 gives an overview of the COMIC dia-
logue system and the OpenCCG text realizer. Sec-
tion 3 then shows how the COMIC text planner
generates logical forms for the realizer from high-
level dialogue-manager specifications. Section 4
describes how the interface between the text planner
and the realizer allows us to send multiple alterna-
tive logical forms, and shows the advantages of this
approach. Section 5 discusses related work, while
Section 6 outlines the future plans for this work and
gives some conclusions.
2 Systems
2.1 COMIC
COMIC1 (den Os and Boves, 2003) is an ongo-
ing project investigating multimodal dialogue sys-
tems. The demonstrator adds a dialogue interface
to a CAD-like application used in bathroom sales
situations to help clients redesign their rooms. The
input to the system includes speech, handwriting,
and pen gestures; the output combines synthesized
speech, a ?talking head? avatar, and control of the
underlying application. Figure 1 shows screen shots
of the avatar and the bathroom-design application.
COMIC produces a variety of output, using its
full range of modalities. In this paper, we will
concentrate on the textual content of those turns in
which the system describes one or more options for
1COnversational Multimodal Interaction with Computers;
http://www.hcrc.ed.ac.uk/comic/.
(a) Avatar (b) Bathroom-design application
Figure 1: Components of the COMIC demonstrator
decorating the user?s bathroom, as in the following
description of a set of tiles:
(1) Here is a country design. It uses tiles from
Coem?s Armonie series. The tiles are terra-
cotta and beige, giving the room the feeling of
a Tuscan country home. There are floral motifs
on the decorative tiles.
2.2 OpenCCG
The OpenCCG realizer (White and Baldridge,
2003) is a practical, open-source realizer based on
Combinatory Categorial Grammar (CCG; Steed-
man, 2000). It employs a novel ensemble of meth-
ods for improving the efficiency of CCG realization,
and in particular, makes integrated use of n-gram
scoring of possible realizations in its chart realiza-
tion algorithm (White, 2004a; White, 2004b). The
n-gram scoring allows the realizer to work in ?any-
time? mode?able at any time to return the highest-
scoring complete realization?and ensures that a
good realization can be found reasonably quickly
even when the number of possibilities is exponen-
tial.
Like other realizers, the OpenCCG realizer is par-
tially responsible for determining word order and
inflection. For example, the realizer determines that
also should preferably follow the verb in There are
also floral motifs on the decorative tiles, whereas in
other cases it typically precedes the verb, as in It
also has abstract shapes. It also enforces subject-
verb agreement, e.g., between are and motifs, and
it and has, respectively. Less typically, in COMIC
and FLIGHTS, the OpenCCG realizer additionally
determines the type of pitch accents, and the type
and placement of boundary tones, based on the in-
formation structure of its input logical forms.
3 Text Planning in COMIC
Broadly speaking, text planning in COMIC follows
the standard pipeline model of natural language
generation (Reiter and Dale, 2000). The input to
the COMIC text planner, from the dialogue man-
ager, specifies the content of the description at a
high level; the output consists of logical forms for
the OpenCCG realizer.
The module is implemented in Java and uses
Apache Xalan2 to process the XSLT templates. The
initial implementation of the presentation-planning
module?of which the XSLT-based sentence plan-
ner described here is just a part?took approxi-
mately one month. After that, the module was de-
bugged and updated incrementally over a period
of several months, during which time additional
templates were created to support updates in the
OpenCCG grammar. The development process was
made easier by the ability to use OpenCCG to parse
a target sentence, and then base a template on the
resulting logical form.
The current presentation planner uses 14 tem-
plates for content structuring and aggregation (Sec-
tion 3.2), and just over 100 to build the logical forms
(Section 3.3). The tasks described here take lit-
tle time to perform (i.e., hundreds of milliseconds);
2http://xml.apache.org/xalan-j/
<rdf:Description rdf:about="#Tileset9">
<rdf:type>
<daml:Class rdf:about="#Tileset"/>
</rdf:type>
<comic:has_id>
<xsd:string xsd:value="9"/>
</comic:has_id>
<comic:has_commentary
rdf:resource="#Commentary9"/>
<comic:has_decoration>
<xsd:string xsd:value="floral-motifs"/>
</comic:has_decoration>
<comic:has_series>
<xsd:string xsd:value="Armonie"/>
</comic:has_series>
<comic:has_manufacturer>
<xsd:string xsd:value="Coem"/>
</comic:has_manufacturer>
<comic:has_colour rdf:resource="#Terracotta"/>
<comic:has_colour rdf:resource="#Beige"/>
<comic:has_style rdf:resource="#Country"/>
</rdf:Description>
Figure 2: Ontology properties of tileset 9
<object type="describe">
<slot name="has_object">
<object type="Tileset">
<slot name="has_id">
<value type="String">9</value>
</slot>
</object>
</slot>
<slot name="has_feature">
<value type="String">has_colour</value>
</slot>
</object>
Figure 3: Dialogue-manager specification
most of the module?s time is spent communicating
with other modules in the system.
3.1 Content Selection
The features of the available designs are stored
in the system ontology. This is represented in
DAML+OIL (soon to be OWL) and includes tile
properties such as style, colour, and decoration.
There is also canned-text commentary associated
with some features (e.g., the Tuscan country home
text in (1)). The ontology instance corresponding to
design (?tileset?) 9 is shown in Figure 2.
For a description like (1), the dialogue-manager
specifies only the tileset to be described, and option-
ally a set of features to include in the description.
Figure 3 shows a dialogue-manager message3 indi-
cating that tileset 9 should be described, and that the
description must include the colour.
To select the content of the description, we first
retrieve all of the features of the indicated design
3The object-slot-value syntax used here allows messages
containing ontology instances to be validated easily against an
XML schema.
from the ontology, using the Jena semantic web
framework.4 We then use the system dialogue his-
tory to filter the retrieved features by removing any
that have already been described to the user. Finally,
we add back to the set any features specifically re-
quested by the dialogue manager, even if they have
been included in a previous description.
3.2 Content Structuring
The result of content selection is an unordered set
of tileset features; this set is converted into a text
plan as follows. First, for each selected feature, a
message is created in XML that combines the in-
formation gathered from the ontology with infor-
mation from the system dialogue history. Figure 4
shows the messages corresponding to the colour
feature and to the associated canned-text commen-
tary. The dialogue-history information is included
in the same-as-last (i.e., whether this value is the
same as the corresponding value of the previous tile-
set) and already-said attributes.
The unordered set of messages is converted to an
ordered list using a small number of heuristics: for
example, features requested by the dialogue man-
ager are always put at the start of the list, while
canned-text commentary always goes immediately
after the feature to which it refers. These heuristics
provide a partial ordering, which is then converted
to a total ordering by breaking ties at random.
The next step is to aggregate the flat list of mes-
sages. In many NLG systems, aggregation is a task
that is done at the syntactic level; in COMIC, we
instead work at the conceptual level. Thanks to the
fact that we produce multiple alternative syntactic
structures (see Section 4), we can be confident that,
whatever the final set of messages, there will be
some syntactic structure available to realize them.
The aggregation is done using a set of XSLT tem-
plates that combine adjacent messages based on var-
ious criteria. For example, the template shown in
Figure 5 combines a feature-value message with
the associated canned-text commentary.5 Figure 6
shows the combined message that results when the
messages in Figure 4 are processed by this template.
The sentence boundaries in the final text are de-
termined by the content structure: each aggregated
message after aggregation corresponds to exactly
one sentence in the output.
4http://jena.sourceforge.net/
5This template is simplified; there are actually many more
tests, and aggregation is performed in several passes to allow
multi-level aggregation. The set namespace refers to a Java Set
instance that stores message IDs to avoid processing a message
twice.
<messages>
<msg id="t2-1-5" prop="has_colour" type="prop-has-val" same-as-last="false" already-said="false">
<slot name="object" value="tileset9"/>
<slot name="value" value="terracotta beige"/>
</msg>
<msg full-sentence="false" id="t2-1-6" prop="has_colour" type="canned-text">
<slot name="object" value="tileset9"/>
<slot name="value" value="give-tuscan-feeling"/>
</msg>
</messages>
Figure 4: Initial messages
<xsl:template match="messages">
<xsl:variable name="void" select="set:clear()"/>
<messages>
<xsl:for-each select="msg">
<xsl:variable name="next" select="following-sibling::msg[1]"/>
<xsl:choose>
<!-- Return nothing if we?ve already processed this message. -->
<xsl:when test="set:contains(@id)"/>
<!-- Add canned text to a sentence. -->
<xsl:when test="@prop=$next/@prop and @type=?prop-has-val?
and $next/@type=?canned-text? and not($next/@full-sentence=?true?)">
<msg type="same-prop-canned-text" id="{concat(@id, ?+?, $next/@id)}">
<slot name="prop"> <xsl:copy-of select="."/> </slot>
<slot name="text"> <xsl:copy-of select="$next"/> </slot>
</msg>
<xsl:variable name="void" select="set:add(string(@id))"/>
<xsl:variable name="void" select="set:add(string($next/@id))"/>
</xsl:when>
<!-- ... other tests ... -->
<!-- Nothing matched: just copy the message across. -->
<xsl:otherwise> <xsl:copy-of select="."/> </xsl:otherwise>
</xsl:choose>
</xsl:for-each>
</messages>
</xsl:template>
Figure 5: Aggregation template (simplified)
3.3 Sentence Planning
After the content of a description has been selected
and structured, the logical forms to send to the re-
alizer are created by applying further XSLT tem-
plates. Every such template matches a message
with particular properties, and produces a logical
form for the realizer, possibly combining the results
of other templates to produce its own final result.
XSLT modes are used to select different templates
in different target syntactic contexts.
Two sample templates are shown in Figure 7.
The first template produces the logical form for
a sentence (mode="s") describing the colours of
a tileset (e.g., The tiles are terracotta and beige).
The second template creates a logical form repre-
senting a commentary message as a verb phrase6
(mode="vp"), and then appends it as an elaboration
6Canned-text commentary is represented in the realizer lex-
icon as a multi-word verb.
to a sentence about the same property. When the
messages in Figure 6 are transformed by these tem-
plates, the result is the logical form shown in Fig-
ure 8, which corresponds to the sentence The tiles
are terracotta and beige, giving the room the feel-
ing of a Tuscan country home.
Referring expressions are generated based on the
number of mentions of the referent: the first refer-
ence gets a full NP (e.g., this design), while subse-
quent mentions are pronominalized.
The logical form created for each top-level mes-
sage is sent to the OpenCCG realizer, which then
generates and returns the corresponding surface
form. As described below, the logical forms may
incorporate alternatives, in which case the realizer
chooses the logical form to use.
4 Sending Alternatives to the Realizer
Many messages can be realized by several differ-
ent logical forms. For example, to inform the user
<messages>
<msg id="t2-1-5+t2-1-6" type="same-prop-canned-text">
<slot name="prop">
<msg id="t2-1-5" prop="has_colour" type="prop-has-val" same-as-last="false" already-said="false">
<slot name="object" value="tileset9"/>
<slot name="value" value="terracotta beige"/>
</msg>
</slot>
<slot name="text">
<msg full-sentence="false" id="t2-1-6" prop="has_colour" type="canned-text">
<slot name="object" value="tileset9"/>
<slot name="text" value="give-tuscan-feeling"/>
</msg>
</slot>
</msg>
</messages>
Figure 6: Combined messages
<xsl:template match="msg[@type=?prop-has-val? and @prop=?has_colour?]" mode="s">
<node pred="be" tense="pres">
<rel name="Arg"> <node pred="tile" det="the" num="pl"/> </rel>
<rel name="Prop"> <xsl:apply-templates select="slot[@name=?value?]" mode="np"/> </rel>
</node>
</xsl:template>
<xsl:template match="msg[@type=?same-prop-canned-text?]" mode="s">
<node pred="elab-rel">
<rel name="Core"> <xsl:apply-templates select="slot[@name=?prop?]/msg" mode="s"/> </rel>
<rel name="Trib"> <xsl:apply-templates select="slot[@name=?text?]/msg" mode="vp"/> </rel>
</node>
</xsl:template>
Figure 7: Sentence-planning templates (simplified)
that a particular design is in the country style, the
options include This design is in the country style
and This design is country. Often, the text planner
has no reason to prefer one alternative over another.
Rather than picking an arbitrary option within the
text planner (as did, e.g., van Deemter et al (1999)),
we instead defer the choice and send all of the valid
alternatives to the realizer, in a packed representa-
tion. This makes the implementation of the text
planner more straightforward. Figure 9 shows an
example of such a logical form, incorporating both
of the above options under a <one-of> element.
To process a logical form with embedded alterna-
tives, the COMIC realizer makes use of the same
n-gram language models that it uses to guide its
search for the realization of a single logical form.
Since OpenCCG cannot yet handle the realization
of logical forms with embedded alternatives directly
(though this capability is planned), in the current
system the packed alternatives are first multiplied
out into a list of top-level alternatives, whose order
is randomly shuffled. The realizer then computes
the best realization for each top-level alternative in
turn, keeping track of the overall best scoring com-
plete realization, until either the anytime time limit
is reached or the list is exhausted. To allow for some
free variation, a new realization?s score must exceed
the current best one by a certain threshold before it
is considered significantly better.
As a concrete example, consider the case where
the system must confirm that the user intends to re-
fer to a tileset with a specific feature. The feature
could be included in the logical form in two ways: it
could be attached directly to the design node (2?3),
or it could instead be included as a non-restrictive
modifier (4).
(2) Do you mean this country design?
(3) Do you mean this design by Coem?
(4) Do you mean this design, with tiles by Coem?
When the modifier can be placed before design, as
in (2), the directly-attached structure is acceptable.
However, for some features, the modifier can only
be placed after the modified noun, as in (3). In
these cases, the preferred structure is instead the
non-restrictive one in (4); this breaks the sentence
into two intonational phrases, which makes it eas-
ier to understand when it is output by the speech
synthesizer. This preference is implemented by in-
cluding only sentences of the preferred type when
<!-- The tiles are terracotta and beige, giving the room the feeling of a Tuscan country home. -->
<lf id="t2-1-5+t2-1-6">
<node mood="dcl" info="rh" pred="elab-rel" id="n7">
<rel name="Core">
<node tense="pres" id="n2" pred="be">
<rel name="Arg"> <node det="the" pred="tile" id="n1" num="pl"/> </rel>
<rel name="Prop">
<node id="n3" pred="and">
<rel name="List">
<node id="n4" kon="+" pred="terracotta"> <rel name="Of"> <node idref="n1"/> </rel> </node>
<node kon="+" id="n6" pred="beige"> <rel name="Of"> <node idref="n1"/> </rel> </node>
</rel>
</node>
</rel>
</node>
</rel>
<rel name="Trib">
<node id="n8" pred="give-tuscan-feeling"> <rel name="Arg"> <node idref="n1"/> </rel> </node>
</rel>
</node>
</lf>
Figure 8: Generated logical form
<lf id="t2-1-2">
<!-- This design is ... -->
<node tense="pres" mood="dcl" info="rh" pred="be" id="n13">
<rel name="Arg">
<node id="n1" num="sg" pred="design" kon="+">
<rel name="Det"> <node kon="+" pred="this" id="n18"/> </rel>
</node>
</rel>
<rel name="Prop">
<one-of>
<!-- ... in the country style. -->
<node pred="in" id="n14">
<rel name="Fig"> <node idref="n1"/> </rel>
<rel name="Ground">
<node num="sg" det="the" pred="style" id="n15">
<rel name="HasProp"> <node id="n16" kon="+" pred="country"/> </rel>
</node>
</rel>
</node>
<!-- ... country. -->
<node id="n20" kon="+" pred="country"> <rel name="Of"> <node idref="n1"/> </rel> </node>
</one-of>
</rel>
</node>
</lf>
Figure 9: Logical form containing alternatives
building the language model for OpenCCG. The re-
alizer will then give (4) a higher n-gram score than
(3), and will therefore choose the desired structure.
In addition to simplifying the implementation, re-
taining multiple alternatives through the planning
process also increases the robustness of the system,
and provides a substitute for backtracking. Particu-
larly during development, there may be times when
a required template simply does not exist; for ex-
ample, the second template in Figure 7 will fail if
the canned-text commentary cannot be realized as a
verb phrase. In such cases, the text planner prunes
out the failing possibilities before sending the set of
options to the realizer, using the template shown in
Figure 10.
5 Related Work
The work presented here continues in the tradi-
tion of several recent NLG systems that use what
could be called generalized template-based process-
ing. By generalized, we mean that, rather than ma-
nipulating flat strings with no underlying linguis-
tic representation, these systems instead work with
structured fragments, which are often processed re-
cursively. Other systems that fall into this cate-
gory include EXEMPLARS (White and Caldwell,
1998), D2S (van Deemter et al, 1999), Interact
<xsl:template match="one-of">
<!-- Recursive pruning step -->
<xsl:variable name="pruned-alts">
<xsl:for-each select="*">
<xsl:variable name="pruned-alt"> <xsl:apply-templates select="."/> </xsl:variable>
<xsl:if test="not(xalan:nodeset($pruned-alt)//fail)">
<xsl:copy-of select="$pruned-alt"/>
</xsl:if>
</xsl:for-each>
</xsl:variable>
<xsl:variable name="num-remaining" select="count(xalan:nodeset($pruned-alts)/*)"/>
<!-- Propagation step -->
<xsl:choose>
<!-- keep one-of when multiple alts succeed -->
<xsl:when test="$num-remaining &gt; 1"> <one-of> <xsl:copy-of select="$pruned-alts"/> </one-of> </xsl:when>
<!-- filter out one-of when just one choice remains -->
<xsl:when test="$num-remaining = 1"> <xsl:copy-of select="$pruned-alts"/> </xsl:when>
<!-- fail if none remain -->
<xsl:otherwise> <fail/> </xsl:otherwise>
</xsl:choose>
</xsl:template>
Figure 10: Failure-pruning template
(Wilcock, 2001; Wilcock, 2003), and SmartKom
(Becker, 2002).
The main novel contribution of the text-planning
approach described here is in its use of an exter-
nal realizer that processes logical forms with em-
bedded alternatives. This eliminates the need to
use a backtracking AI planner (Becker, 2002) or to
make arbitrary choices when multiple alternatives
are available (van Deemter et al, 1999). The real-
izer also uses a completely different algorithm than
the XSLT template processing?bottom-up, chart-
based search rather than top-down rule expansion?
which allows it to deal with those aspects of NLG
that are more easily addressed using this kind of
processing strategy.
Our approach to text planning draws from both
the AI-planning and the template-based traditions
in natural language generation. Most previous
NLG systems that use AI planners use them pri-
marily to do hierarchical decomposition of com-
municative goals; the work described here uses
XSLT to achieve the same end, with a substitute
for backtracking provided by the realizer?s support
for multiple alternatives. The system is nonetheless
equally based on (generalized) template processing.
This demonstrates that, rather than being in con-
flict, the two traditions actually have complemen-
tary strengths, which can usefully be combined in a
single system (contra Reiter, 1995; cf. van Deemter
et al, 1999).
6 Conclusions and Future Work
We have described a successful implementation of
the classic NLG pipeline that uses XSLT template
processing as a top-down rule-expanding planner.
Implementing the necessary steps using XSLT was
generally straightforward, and the ability to use off-
the shelf, well-tested and well-documented tools
such as Java and Xalan adds to the ease of imple-
mentation and robustness.
Our implementation creates logical forms for the
OpenCCG realizer; this allows the realizer to be
used for those parts of the generation process to
which XSLT is less well-suited. We also take ad-
vantage of the fact that the realizer uses statistical
language models in its search for a surface form
by generating logical forms with embedded alter-
natives, allowing the realizer to choose the one to
use. This both adds robustness to the system and
eliminates the need for backtracking within the text
planner.
The current implementation is fast and reliable: it
correctly processes all input from the dialogue man-
ager, and the time it takes to do so is relatively short
compared to that required by the other processing
and communication tasks in COMIC.
The entire COMIC demonstrator will shortly be
evaluated. As part of this evaluation, we plan to
measure users? recall of the information that the sys-
tem presents to them, where that information is gen-
erated at different levels of detail.
At the moment, the logical form for each mes-
sage is created in isolation. In future versions of
COMIC, we plan to use ideas from centering theory
to help ensure coherence by planning a coherent se-
quence of logical forms for a description. We will
implement this in a way similar to that described by
Kibble and Power (2000).
We will also incorporate a model of the user?s
preferences into a later version of COMIC. The
model will be used both to rank the options to be
presented to the user, and to generate user-tailored
descriptions of those options, as in FLIGHTS
(Moore et al, 2004).
Finally, we plan to extend the use of data-driven
techniques in the realizer, and to make use of these
techniques to help in choosing among alternatives
in the other COMIC output modalities.
Acknowledgements
Thanks to Jon Oberlander, Johanna Moore, and
the anonymous reviewers for helpful comments and
discussion. This work was supported in part by the
COMIC (IST-2001-32311) and FLIGHTS (EPSRC-
GR/R02450/01) projects.
References
Tilman Becker. 2002. Practical, template-based
natural language generation with TAG. In Pro-
ceedings of TAG+6.
Kees van Deemter, Emiel Krahmer, and Marie?t The-
une. 1999. Plan-based vs. template-based NLG:
a false opposition? In Proceedings of ?May I
speak freely?? workshop at KI-99.
Rodger Kibble and Richard Power. 2000. An inte-
grated framework for text planning and pronomi-
nalisation. In Proceedings of INLG 2000.
Johanna Moore, Mary Ellen Foster, Oliver Lemon,
and Michael White. 2004. Generating tailored,
comparative descriptions in spoken dialogue. In
Proceedings of FLAIRS 2004.
Els den Os and Lou Boves. 2003. Towards ambi-
ent intelligence: Multimodal computers that un-
derstand our intentions. In Proceedings of eChal-
lenges e-2003.
Ehud Reiter. 1995. NLG vs. templates. In Proceed-
ings of EWNLG-95.
Ehud Reiter and Robert Dale. 2000. Building Nat-
ural Language Generation Systems. Cambridge
University Press.
Mark Steedman. 2000. The Syntactic Process.
MIT Press.
Michael White. 2004a. Efficient realization of
coordinate structures in Combinatory Categorial
Grammar. Research on Language and Computa-
tion. To appear.
Michael White. 2004b. Reining in CCG chart re-
alization. In Proceedings of INLG 2004. To ap-
pear.
Michael White and Jason Baldridge. 2003. Adapt-
ing chart realization to CCG. In Proceedings of
EWNLG-03.
Michael White and Ted Caldwell. 1998. EXEM-
PLARS: A practical, extensible framework for
dynamic text generation. In Proceedings of INLG
1998.
Graham Wilcock. 2001. Pipelines, templates and
transformations: XML for natural language gen-
eration. In Proceedings of NLPXML-2001.
Graham Wilcock. 2003. Integrating natural lan-
guage generation with XML web technology. In
Proceedings of EACL-2003 Demo Sessions.
Interleaved Preparation and Output
in the COMIC Fission Module
Mary Ellen Foster
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW United Kingdom
M.E.Foster@ed.ac.uk
Abstract
We give a technical description of the
fission module of the COMIC mul-
timodal dialogue system, which both
plans the multimodal content of the sys-
tem turns and controls the execution of
those plans. We emphasise the parts of
the implementation that allow the sys-
tem to begin producing output as soon as
possible by preparing and outputting the
content in parallel. We also demonstrate
how the module was designed to ensure
robustness and configurability, and de-
scribe how the module has performed
successfully as part of the overall sys-
tem. Finally, we discuss how the tech-
niques used in this module can be ap-
plied to other similar dialogue systems.
1 Introduction
In a multimodal dialogue system, even minor de-
lays in processing at each stage can add up to pro-
duce a system that produces an overall sluggish
impression. It is therefore critical that the output
system avoid as much as possible adding any de-
lays of its own to the sequence; there should be as
little time as possible between the dialogue man-
ager?s selection of the content of the next turn and
the start of that turn?s output. When the output in-
corporates temporal modalities such as speech, it
is possible to take advantage of this by planning
later parts of the turn even as the earlier parts are
being played. This means that the initial parts of
the output can be produced more quickly, and any
delay in preparing the later parts is partly or en-
tirely eliminated. The net effect is that the over-
all perceived delay in the output is much shorter
than if the whole turn had been prepared before
any output was produced.
In this paper, we give a technical description of
the output system of the COMIC multimodal dia-
logue system, which is designed to allow exactly
this interleaving of preparation and output. The
paper is arranged as follows. In Section 2, we
begin with a general overview of multimodal di-
alogue systems, concentrating on the design de-
cisions that affect how output is specified and
produced. In Section 3, we then describe the
COMIC multimodal dialogue system and show
how it addresses each of the relevant design de-
cisions. Next, in Section 4, we describe how
the segments of an output plan are represented in
COMIC, and how those segments are prepared and
executed in parallel. In Section 5, we discuss two
aspects of the module implementation that are rel-
evant to its role within the overall COMIC system:
the techniques that were used to ensure the robust-
ness of the fission module, and how it can be con-
figured to support a variety of requirements. In
Section 6, we then assess the practical impact of
the parallel processing on the overall system re-
sponsiveness, and show that the output speed has
a perceptible effect on the overall user experiences
with the system. Finally, in Section 7, we outline
the aspects of the COMIC output system that are
applicable to similar systems.
ASR
Pen
...
Fusion Dialoguemanager
Speech
Animation
...
Fission
Dialogue anddomain knowledge
Input modules Output modules
User
Figure 1: High-level architecture of a typical multimodal dialogue system
2 Output in Multimodal Dialogue
Systems
Most multimodal dialogue systems use the basic
high-level architecture shown in Figure 1. Input
from the user is analysed by one or more input-
processing modules, each of which deals with an
individual input channel; depending on the ap-
plication, the input channels may include speech
recognition, pen-gesture or handwriting recogni-
tion, or information from visual sensors, for ex-
ample. The messages from the various sources are
then combined by a fusion module, which resolves
any cross-modal references and produces a com-
bined representation of the user input. This com-
bined representation is sent to the dialogue man-
ager, which uses a set of domain and dialogue
knowledge sources to process the user input, in-
teract with the underlying application if necessary,
and specify the content to be output by the sys-
tem in response. The output specification is sent to
the fission module, which creates a presentation to
meet the specification, using a combination of the
available output channels. Again, depending on
the application, a variety of output channels may
be used; typical channels are synthesised speech,
on-screen displays, or behaviour specifications for
an animated agent or a robot.
This general structure is typical across mul-
timodal dialogue systems; however, there are a
number of design decisions that must be made
when implementing a specific system. As this pa-
per concentrates on the output components high-
lighted in Figure 1, we will discuss the design de-
cisions that have a particular impact on those parts
of the dialogue system: the domain of the applica-
tion, the output modalities, the turn-taking proto-
col, and the division of labour among the modules.
We will use as examples the the WITAS (Lemon
et al, 2002), MATCH (Walker et al, 2002), and
SmartKom (Wahlster, 2005) systems.
The domain of the system and the interactions
that it is intended to support both have an influ-
ence on the type of output that is to be gener-
ated. Many systems are designed primarily to sup-
port information exploration and presentation, and
concentrate on effectively communicating the nec-
essary information to the user. SmartKom and
MATCH both fall into this category: SmartKom
deals with movie and television listings, while
MATCH works in the domain of restaurant rec-
ommendations. In a system such as WITAS,
which incorporates real-time control of a robot he-
licopter, very different output must be generated
to communicate the current state and goals of the
robot to the user.
The choice of output modalities also affects the
output system?different combinations of modali-
ties require different types of temporal and spatial
coordination, and different methods of allocating
the content across the channels. Most multimodal
dialogue systems use synthesised speech as an out-
put modality, often in combination with lip-synch
and other behaviours of an animated agent (e.g.,
MATCH, SmartKom). Various types of visual
output are also often employed, including inter-
active maps (MATCH, WITAS), textual informa-
tion presentations (SmartKom, MATCH), or im-
ages from visual sensors (WITAS). Some systems
also dynamically adapt the output channels based
on changing constraints; for example, SmartKom
chooses a spoken presentation over a visual one in
an eyes-busy situation.
Another factor that has an effect on the design
of the output components is the turn-taking proto-
col selected by the system. Some systems?such
as WITAS?support barge-in (Stro?m and Seneff,
2000); that is, the user may interrupt the system
output at any time. Allowing the user to inter-
rupt can permit a more intuitive interaction with
the system; however, supporting barge-in creates
many technical complications. For example, it is
crucial that the output system be prepared to stop
at any point, and that any parts of the system that
track the dialogue history be made aware of how
much of the intended content was actually pro-
duced. For simplicity, many systems?including
SmartKom?instead use half-duplex turn-taking:
when the system is producing output, the input
modules are not active. This sort of system is tech-
nically more straightforward to implement, but re-
quires that the user be given very clear signals as
to when the system is and is not paying attention to
their input. MATCH uses a click-to-talk interface,
where the user presses a button on the interface
to indicate that they want to speak; it is not clear
whether the system supports barge-in.
The division of labour across the modules also
differs among implemented systems. First of all,
not all systems actually incorporate a separate
component that could be labelled fission: for ex-
ample, in WITAS, the dialogue manager itself also
addresses the tasks of presentation planning and
coordination. The components of the typical nat-
ural language generation ?pipeline? (Reiter and
Dale, 2000) may be split across the modules in a
variety of ways. When it comes to content selec-
tion, for instance, in MATCH the dialogue man-
ager specifies the content at a high level, while the
text planner selects and structures the actual facts
to include in the presentation; in WITAS, on the
other hand, the specific content is selected by the
dialogue manager. The tasks of text planning and
sentence planning may be addressed by various
combinations of the fission module and any text-
generation modules involved?SmartKom creates
the text in a separate generation module, while
in MATCH text and sentence planning is more
tightly integrated with content selection.
Coordination across multiple output channels is
also implemented in various ways. If the only pre-
sentation modality is an animated agent, in many
cases the generated text is sent directly to the
agent, which then communicates privately with
the speech synthesiser to ensure synchronisation.
This ?visual text-to-speech? configuration is the
default behaviour of the Greta (de Rosis et al,
2003) and RUTH (DeCarlo et al, 2004) animated
presentation agents, for instance. However, if the
behaviour of the agent must be coordinated with
other forms of output, it is necessary that the be-
haviour of all synchronised modules be coordi-
nated centrally. How this is accomplished in prac-
tice depends on the capabilities of selected speech
synthesiser that is used. In SmartKom, for exam-
ple, the presentation planner pre-synthesises the
speech and uses the schedule returned by the syn-
thesiser to create the full multimodal schedule; in
MATCH, on the other hand, the speech synthe-
siser sends progress messages as it plays its out-
put, which are used to control the output in the
other modalities at run time.
3 The COMIC Dialogue System
COMIC1 (COnversational Multimodal Interac-
tion with Computers) is an EU IST 5th frame-
work project combining fundamental research on
human-human dialogues with advanced technol-
ogy development for multimodal conversational
systems. The COMIC multimodal dialogue sys-
tem adds a dialogue interface to a CAD-like ap-
plication used in sales situations to help clients
redesign their bathrooms. The input to COMIC
consists of speech, pen gestures, and handwriting;
turn-taking is strictly half-duplex, with no barge-in
or click-to-talk. The output combines the follow-
ing modalities:
1http://www.hcrc.ed.ac.uk/comic/
?[Nod] Okay. [Choose design] [Look at screen] THIS design [circling gesture] is CLAS-
SIC. It uses tiles from VILLEROY AND BOCH?s CENTURY ESPRIT series. There are
FLORAL MOTIFS and GEOMETRIC SHAPES on the DECORATIVE tiles.?
Figure 2: COMIC interface and sample output
? Synthesised speech, created using the
OpenCCG surface realiser (White, 2005a;b)
and synthesised by a custom Festival 2 voice
(Clark et al, 2004) with support for APML
prosodic markup (de Carolis et al, 2004).
? Facial expressions and gaze shifts of a talking
head (Breidt et al, 2003).
? Direct commands to the design application.
? Deictic gestures at objects on the application
screen, using a simulated mouse pointer.
Figure 2 shows the COMIC interface and a typical
output turn, including commands for all modali-
ties; the small capitals indicate pitch accents in the
speech, with corresponding facial emphasis.
The specifications from the COMIC dialogue
manager are high-level and modality-independent;
for example, the specification of the output shown
in Figure 2 would indicate that system should
show a particular set of tiles on the screen, and
should give a detailed description of those tiles.
When the fission module receives input from the
dialogue manager, it selects and structures mul-
timodal content to create an output plan, us-
ing a combination of scripted and dynamically-
generated output segments. The fission module
addresses the tasks of low-level content selec-
tion, text planning, and sentence planning; sur-
face realisation of the sentence plans is done by
the OpenCCG realiser. The fission module also
controls the output of the planned presentation by
sending appropriate messages to the output mod-
ules including the text realiser, speech synthesiser,
talking head, and bathroom-design GUI. Coordi-
nation across the modalities is implemented using
a technique similar to that used in SmartKom: the
synthesised speech is prepared in advance, and the
timing information from the synthesiser is used to
create the schedule for the other modalities.
The plan for an output turn in COMIC is rep-
resented in a tree structure; for example, Figure 3
shows part of the plan for the output in Figure 2.
A plan tree like this is created from the top down,
with the children created left-to-right at each level,
and is executed in the same order. The planning
and execution processes for a turn are started to-
gether and run in parallel, which makes it possible
to begin producing output as soon as possible and
to continue planning while output is active. In the
following section, we describe the set of classes
and algorithms that make this interleaved prepara-
tion and execution possible.
The COMIC fission module is implemented in a
combination of Java and XSLT. The current mod-
ule consists of 18 000 lines of Java code in 88
source files, and just over 9000 lines of XSLT tem-
plates. In the diagrams and algorithm descriptions
that follow, some non-essential details are omitted
for simplicity.
ASRP
en. . Fusi uFoDP auFl Rogusi uFoDP
mAnoFsi uFoDPoFsl rpFFol ch mt sSFuFs oruFdR. k sccch wcccI
O. . Usp sFl RuuP? l UP. ? rui Du
? . i m? Up? ch
Figure 3: Output plan
4 Representing an Output Plan
Each node in a output-plan tree such as that shown
in Figure 3 is represented by an instance of the
Segment class. The structure of this abstract class
is shown in Figure 4; the fields and methods de-
fined in this class control the preparation and out-
put of the corresponding segment of the plan tree,
and allow preparation and output to proceed in
parallel.
Each Segment instance stores a reference to its
parent in the tree, and defines the following three
methods:
? plan() Begins preparing the output.
? execute() Produces the prepared output.
? reportDone() Indicates to the Segment?s
parent that its output has been completed.
plan() and execute() are abstract methods of
the Segment class; the concrete implementations
of these methods on the subclasses of Segment
are described later in this section. Each Segment
also has the following Boolean flags that control
its processing; all are initially false.
? ready This flag is set internally once the Seg-
ment has finished all of its preparation and is
ready to be output.
? skip This flag is set internally if the Segment
encounters a problem during its planning, and
indicates that the Segment should be skipped
when the time comes to produce output.
Segment
# parent : Sequence
# ready : boolean
# skip : boolean
# active : boolean
+ plan()
+ execute()
# reportDone()
Figure 4: Structure of the Segment class
? active This flag is set externally by the Seg-
ment?s parent, and indicates that this Seg-
ment should produce its output as soon as it
is ready.
The activity diagram in Figure 5 shows how
these flags and methods are used during the prepa-
ration and output of a Segment. Note that a
Segment may send asynchronous queries to other
modules as part of its planning. When such a
query is sent, the Segment sets its internal state
and exits its plan() method; when the response
is received, preparation continues from the last
state reached. Since planning and execution pro-
ceed in parallel across the tree, and the planning
process may be interrupted to wait for responses
from other modules, the ready and active flags
may be set in either order on a particular Seg-
ment. Once both of these flags have been set, the
execute() method is called automatically. If both
skip and active are set, the Segment instead au-
tomatically calls reportDone() without ever ex-
ASRPen
. FusRi uoDF. FusaFRl g . FusmroA
FpFi cuFen
aFAhaut hPFen
 RSSFl sdgsARaFPu
 RSSFl sdgsARaFPu
 RSSmARaFPuki woSl Imt hPFen
. ci i Fmm Oaaha
 hU U cPoi RuFm? ouwshuwFasUhl cSFm
Figure 5: Segment preparation and output
Seg
me
nt
# p
are
nt :
 Se
que
nce
# r
ead
y : 
boo
lea
n
# a
ctiv
e : 
boo
lea
n
# s
kip 
: bo
ole
an
+ p
lan
()
+ e
xec
ute
()
# r
epo
rtD
one
()
Seq
uen
ce
# c
hild
ren
 : L
ist<
Seg
me
nt>
# c
ur :
 int
+ e
xec
ute
()
+ c
hild
IsD
one
()
Bal
list
icS
egm
ent
+ p
lan
()
+ e
xec
ute
()
Sta
nda
lon
eAv
ata
rCo
mm
and
+ e
xec
ute
()
+ h
and
lePo
olM
ess
age
()
Sta
nda
lon
eAv
ata
rCo
mm
and
+ e
xec
ute
()
+ h
and
lePo
olM
ess
age
()
Sen
ten
ce
+ e
xec
ute
()
+ h
and
lePo
olM
ess
age
()
Scr
ipt
edS
ent
enc
e
+ p
lan
()
- se
ndS
pee
chM
ess
age
()
Pla
nne
dSe
nte
nce
+ p
lan
()
- se
ndR
eal
iser
Mes
sag
e()
Pla
nne
dSe
que
nce
+ h
and
lePo
olM
ess
age
()
Tur
n
+ p
lan
()
+ h
and
lePo
olM
ess
age
()
Tur
nSe
que
nce
+ a
ddT
urn
()
+ p
lan
()
+ c
hild
IsD
one
()
Scr
ipt
edS
equ
enc
e
+ p
lan
()
Ask
See
# p
rop
Nam
e : 
Stri
ng
# p
rop
Val
 : S
trin
g
+ p
lan
()
De
scr
ibe
# o
bjID
s : 
List
<St
ring
>
# r
equ
est
edF
eat
s : 
List
<St
ring
>
# u
mR
esp
ons
e : 
UM
Ran
king
s
# d
eta
iled
 : b
oole
an
# p
rop
Val
 : S
trin
g
+ p
lan
()
Figure 6: Segment class hierarchy
ecuting; this allows Segments with errors to be
skipped without affecting the output of the rest of
the turn.
The full class hierarchy under Segment is shown
in Figure 6. There are three main top-level sub-
classes of Segment, which differ primarily based
on how they implement execute():
Sequence An ordered sequence of Segments. It
is executed by activating each child in turn.
BallisticSegment A single command whose du-
ration is determined by the module producing the
output. It is executed by sending a message to the
appropriate module and waiting for that module to
report back that it has finished.
Sentence A single sentence, incorporating coor-
dinated output in all modalities. Its schedule is
computed in advance, as part of the planning pro-
cess; it is executed by sending a ?go? command to
the appropriate output modules.
In the remainder of this section, we discuss each
of these classes and its subclasses in more detail.
4.1 Sequence
All internal nodes in a presentation-plan tree
(coloured blue in Figure 3) are instances of some
type of Sequence. A Sequence stores a list of child
Segments, which it plans and activates in order,
along with a pointer to the currently active Seg-
ment. Figure 7 shows the pseudocode for the main
methods of a typical Sequence.
Note that a Sequence calls sets its ready flag as
soon as all of its necessary child Segments have
been created, and only then begins calling plan()
on them. This allows the Sequence?s execute()
method to be called as soon as possible, which
is critical to allowing the fission module to begin
producing output from the tree before the full tree
has been created.
When execute() is called on a Sequence, it
calls activate() on the first child in its list. All
subsequent children are activated by calls to the
childIsDone() method, which is called by each
child as part of its reportDone() method after its
execution is completed. Note that this ensures that
the children of a Sequence will always be executed
in the proper order, even if they are prepared out of
public void plan() {
// Create child Segments
cur = 0;
ready = true;
for( Segment seg: children ) {
seg.plan();
}
}
public void execute() {
children.get( 0 ).activate();
}
public void childIsDone() {
cur++;
if( cur >= children.size() ) {
reportDone();
} else {
children.get( cur ).activate();
}
}
Figure 7: Pseudocode for Sequence methods
order. Once all of the Sequence?s children have re-
ported that they are done, the Sequence itself calls
reportDone().
The main subclasses of Sequence, and their rel-
evant features, are as follows:
TurnSequence The singleton class that is the
parent of all Turns. It is always active, and new
children can be added to its list at any time.
Turn Corresponds to a single message from the
dialogue manager; the root of the output plan in
Figure 3 is a Turn. Its plan() implementation
creates a Segment corresponding to each dialogue
act from the dialogue manager; in some cases, the
Turn adds additional children not directly speci-
fied by the DAM, such as the verbal acknowledge-
ment and the gaze shift in Figure 3.
ScriptedSequence A sequence of canned output
segments stored as an XSLT template. A Scripted-
Sequence is used anywhere in the dialogue where
dynamically-generated content is not necessary;
for example, instructions to the user and acknowl-
edgements such as the leftmost subtree in Figure 3
are stored as ScriptedSequences.
PlannedSequence In contrast to a ScriptedSe-
quence, a PlannedSequence creates its children
dynamically depending on the dialogue context.
The principal type of PlannedSequence is a de-
scription of one or more tile designs, such as that
shown in Figure 2. To create the content of such
a description, the fission module uses information
from the system ontology, the dialogue history,
and the model of user preferences to select and
structure the facts about the selected design and
to create the sequence of sentences to realise that
content. This process is described in detail in (Fos-
ter and White, 2004; 2005).
4.2 BallisticSegment
A BallisticSegment is a single command for a sin-
gle output module, where the output module is al-
lowed to choose the duration at execution time.
In Figure 3, the orange Nod, Choose design, and
Look at screen nodes are examples of BallisticSeg-
ments. In its plan() method, a BallisticSegment
transforms its input specification into an appropri-
ate message for the target output module. When
execute() is called, the BallisticSegment sends
the transformed command to the output module
and waits for that module to report back that it is
done; it calls reportDone() when it receives that
acknowledgement.
4.3 Sentence
The Sentence class represents a single sentence,
combining synthesised speech, lip-synch com-
mands for the talking head, and possible coordi-
nated behaviours on the other multimodal chan-
nels. The timing of a sentence is based on the
timing of the synthesised speech; all multimodal
behaviours are scheduled to coincide with partic-
ular words in the text. Unlike a BallisticSegment,
which allows the output module to determine the
duration at execution time, a Sentence must pre-
pare its schedule in advance to ensure that output
is coordinated across all of the channels. In Fig-
ure 3, all of the green leaf nodes containing text
are instances of Sentence.
There are two types of Sentences: ScriptedSen-
tences and PlannedSentences. A ScriptedSentence
is generally created as part of a ScriptedSequence,
and is based on pre-written text that is sent directly
to the speech synthesiser, along with any neces-
sary multimodal behaviours. A PlannedSentence
forms part of a PlannedSequence, and is based on
logical forms for the OpenCCG realiser (White,
2005a;b). The logical forms may contain multiple
possibilities for both the text and the multimodal
behaviours; the OpenCCG realiser uses statistical
language models to make a final choice of the ac-
tual content of the sentence.
The first step in preparing either type of Sen-
tence is to send the text to the speech synthe-
siser (Figure 8(a)). For a ScriptedSentence, the
canned text is sent directly to the speech synthe-
siser; for a PlannedSentence, the logical forms are
sent to the realiser, which then creates the text
and sends it to the synthesiser. In either case, the
speech-synthesiser input also includes marks at all
points where multimodal output is intended. The
speech synthesiser prepares and stores the wave-
form based on the input text, and returns timing in-
formation for the words and phonemes, along with
the timing of any multimodal coordination marks.
The fission module uses the returned timing
information to create the final schedule for all
modalities. It then sends the animation schedule
(lip-synch commands, along with any coordinated
expression or gaze behaviours) to the talking-head
module so that it can prepare its animation in ad-
vance (Figure 8(b)). Once the talking-head mod-
ule has prepared the animation for a turn, it returns
a ?ready? message. The design application does
not need its schedule in advance, so once the re-
sponse is received from the talking head, the Sen-
tence has finished its preparation and is able to set
its ready flag.
When a Sentence is executed by its parent,
it selects a desired start time slightly in the fu-
ture and sends two messages, as shown in Fig-
ure 8(c). First, it sends a ?go? message with
the selected starting time to the speech-synthesis
and talking-head modules; these modules then
play the prepared output for that turn at the given
time. The Sentence also sends the concrete sched-
ule for any coordinated gesture commands to the
bathroom-design application at this point. Af-
ter sending its messages, the Sentence waits until
the scheduled duration has elapsed, and then calls
reportDone().
ASRRSPe
n. FuSR. s
i oeDa. RSR. s
l FugSemra. Fp
c . RSmerFhhuStFDSPe
 P mStFudPsk R
w. e. sFD. prD. I D
OFee. prD. I D
l Sk Sem
(a) Preparing the speech
ASRRSPe
n. FuSR. s
i oeDa. RSR. s
l FugSemra. Fp
c . RSmerFhhuStFDSPe
 aPe. d . Rkrd PDSPeR
wn. FpoI
(b) Preparing the animation
ASRRSPe
n. FuSR. s
i oeDa. RSR. s
l FugSemra. Fp
c . RSmerFhhuStFDSPe
i DFsDrDS .
d . RDks.Rta. pku.
(c) Producing the output
Figure 8: Planning and executing a Sentence
5 Robustness and Configurability
In the preceding section, we gave a description of
the data structures and methods that are used when
preparing and executing and output plan. In this
section, we describe two other aspects of the mod-
ule that are important to its functioning as part of
the overall dialogue system: its ability to detect
and deal with errors in its processing, and the var-
ious configurations in which it can be run.
5.1 Error Detection and Recovery
Since barge-in is not implemented in COMIC, the
fission module plays an important role in turn-
taking for the whole COMIC system: it is the
module that informs the input components when
the system output is finished, so that they are able
to process the next user input. The fission mod-
ule therefore incorporates several measures to en-
sure that it is able to detect and recover from un-
expected events during its processing, so that the
dialogue is able to continue even if there are errors
in some parts of the output.
Most input from external modules is validated
against XML schemas to ensure that it is well-
formed, and any messages that fail to validate are
not processed further. As well, all queries to exter-
nal modules are sent with configurable time-outs,
and any Segment that is expecting a response to a
query is also prepared to deal with a time-out.
If a problem occurs while preparing any Seg-
ment for output?either due to an error in internal
processing, or because of an issue with some ex-
ternal module?that Segment immediately sets its
skip flag and stops the preparation process. As
described in Section 4, any Segments with this flag
set are then skipped at execution time. This en-
sures that processing is able to continue as much
as possible despite the errors, and that the fission
module is still able to produce output from the
parts of an output plan unaffected by the problems
and to perform its necessary turn-taking functions.
5.2 Configurability
The COMIC fission module can be run in sev-
eral different configurations, to meet a variety of
evaluation, demonstration, and development situa-
tions. The fission module can be configured not to
wait for ?ready? and ?done? responses from either
or both of the talking-head and design-application
modules; the fission module simply proceeds with
the rest of its processing as if the required response
had been received. This allows the whole COMIC
system to be run without those output modules en-
abled. This is useful during development of other
parts of the system, and for running demos and
evaluation experiments where not all of the output
channels are used. The module also has a num-
ber of other configuration options to control fac-
tors such as query time-outs and the method of se-
lecting multimodal coarticulations.
As well, the fission module has the ability to
generate multiple alternative versions of a single
turn, using different user models, dialogue-history
settings, or multimodal planning techniques; this
is useful both as a testing tool and as part of a sys-
tem demonstration. The module can also store all
of the generated output to a script, and to play back
the scripted output at a later time using a subset of
the full system. This allows alternative versions
of the system output to be directly compared in
user evaluation studies such as (Foster, 2004; Fos-
ter and White, 2005).
6 Output Speed
In the final version of the COMIC system, the av-
erage time2 that the speech synthesiser takes to
prepare the waveform for a sentence is 1.9 sec-
onds, while the average synthesised length of a
sentence is 2.7 seconds. This means that, on aver-
age, each sentence takes long enough to play that
the next sentence is ready as soon as it is needed;
and even when this is not the case, the delay be-
tween sentences is still greatly reduced by the par-
allel planning process.
The importance of beginning output as soon as
possible was demonstrated by a user evaluation of
an interim version of COMIC (White et al, 2005).
Subjects in that study used the full COMIC sys-
tem in one of two configurations: an ?expressive?
condition, where the talking head used all of the
expressions it was capable of, or a ?zombie? con-
dition where all of the behaviours of the head were
disabled except for lip-synch. One effect of this
2On a Pentium 4 1.6GHz computer.
difference was that the system gave a consistently
earlier response in the expressive condition?a fa-
cial response was produced an average of 1.4 sec-
onds after the dialogue-manager message, while
spoken input did not begin for nearly 4 seconds.
Although that version of the system was very slow,
the subjects in the expressive condition were sig-
nificantly less likely to mention the overall slow-
ness than the subjects in the zombie condition.
After this interim evaluation, effort was put into
further reducing the delay in the final system. For
example, we now store the waveforms for ac-
knowledgements and other frequently-used texts
pre-synthesised in the speech module instead of
sending them to Festival, and other internal pro-
cessing bottlenecks were eliminated. Using the
same computers as the interim evaluation, the fis-
sion delay for initial output is under 0.5 seconds in
the final system.
7 Conclusions
The COMIC fission module is able to prepare and
control the output of multimodal turns. It prepares
and executes its plans in parallel, which allows it
to begin producing output as soon as possible and
to continue with preparing later parts of the pre-
sentation while executing earlier parts. It is able
to produce output coordinated and synchronised
across multiple modalities, to detect and recover
from a variety of errors during its processing, and
to be run in a number of different configurations
to support testing, demonstrations, and evaluation
experiments. The parallel planning process is able
to make a significant reduction in the time taken to
produce output, which has a perceptible effect on
user satisfaction with the overall system.
Some aspects of the fission module are specific
to the design of the COMIC dialogue system; for
example, the module performs content-selection
and sentence-planning tasks that in other systems
might be addressed by a dialogue manager or text-
generation module. Also, aspects of the commu-
nication with the output modules are tailored to
the particular modules involved: the fission mod-
ule makes use of features of the OpenCCG realiser
to help choose the content of many of its turns, and
the implementation of the design application is ob-
viously COMIC-specific.
However, the general technique of interleaving
preparation and execution, using the time while
the system is playing earlier parts of a turn to
prepare the later parts, is easily applicable to any
system that produces temporal output, as long as
the same module is responsible for preparing and
executing the output. There is nothing COMIC-
specific about the design of the Segment class or
its immediate sub-classes.
As well, the method of coordinating distributed
multimodal behaviour with the speech timing
(Section 4.3) is a general one. Although the cur-
rent implementation relies on the output modules
to respect the schedules that they are given?with
no adaptation at run time?in practice the coordi-
nation in COMIC has been generally successful,
providing that three conditions are met. First, the
selected starting time must be far enough in the fu-
ture that it can be received and processed by each
module in time. Second, the clocks on all comput-
ers involved in running the system must be syn-
chronised precisely. Finally, the processing load
on each computer must be low enough that timer
events do not get delayed or pre-empted.
Since the COMIC system does not support
barge-in, the current fission module always pro-
duces the full presentation that is planned, bar-
ring processing errors. However, since the mod-
ule produces its output incrementally, it would be
straightforward to extend the processing to allow
execution to be interrupted after any Segment, and
to know how much of the planned output was ac-
tually produced.
Acknowledgements
Many thanks to Peter Poller, Tilman Becker, and
especially Michael White for helpful advice on
and discussions about the implementation of the
COMIC fission module, and to the anonymous re-
viewers for their comments on the initial version
of this paper. This work was supported by the
COMIC project (IST-2001-32311).
References
M. Breidt, C. Wallraven, D.W. Cunningham, and
H.H. Bu?lthoff. 2003. Facial animation based on
3d scans and motion capture. In Neill Campbell,
editor, SIGGRAPH 03 Sketches & Applications.
ACM Press.
R.A.J. Clark, K. Richmond, and S. King. 2004.
Festival 2 ? build your own general purpose unit
selection speech synthesiser. In Proceedings,
5th ISCA workshop on speech synthesis.
B. de Carolis, C. Pelachaud, I. Poggi, and
M. Steedman. 2004. APML, a mark-up lan-
guage for believable behaviour generation. In
H. Prendinger, editor, Life-like Characters,
Tools, Affective Functions and Applications,
pages 65?85. Springer.
F. de Rosis, C. Pelachaud, I. Poggi, V. Carofiglio,
and B. De Carolis. 2003. From Greta?s mind
to her face: modelling the dynamics of affective
states in a conversational embodied agent. In-
ternational Journal of Human-Computer Stud-
ies, 59(1?2):81?118.
D. DeCarlo, M. Stone, C. Revilla, and J.J. Ven-
ditti. 2004. Specifying and animating facial
signals for discourse in embodied conversa-
tional agents. Computer Animation and Virtual
Worlds, 15(1):27?38.
M.E. Foster. 2004. User evaluation of generated
deictic gestures in the T24 demonstrator. Public
deliverable 6.5, COMIC project.
M.E. Foster and M. White. 2004. Techniques
for text planning with XSLT. In Proceedings,
NLPXML-2004.
M.E. Foster and M. White. 2005. Assessing the
impact of adaptive generation in the COMIC
multimodal dialogue system. In Proceedings,
IJCAI 2005 Workshop on Knowledge and Rea-
soning in Practical Dialogue systems.
O. Lemon, A. Gruenstein, and S. Peters. 2002.
Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des
Langues (TAL), 43(2):131?154.
E Reiter and R Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge Univer-
sity Press.
N. Stro?m and S. Seneff. 2000. Intelligent barge-
in in conversational systems. In Proceedings,
ICSLP-2000, volume 2, pages 652?655.
W. Wahlster, editor. 2005. SmartKom: Foun-
dations of Multimodal Dialogue Systems.
Springer. In press.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor,
J.D. Moore, M. Johnston, and G. Vasireddy.
2002. Speech-plans: Generating evaluative re-
sponses in spoken dialogue. In Proceedings,
INLG 2002.
M. White. 2005a. Designing an extensible API
for integrating language modeling and realiza-
tion. In Proceedings, ACL 2005 Workshop on
Software.
M. White. 2005b. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation. To appear.
M. White, M.E. Foster, J. Oberlander, and
A. Brown. 2005. Using facial feedback to
enhance turn-taking in a multimodal dialogue
system. In Proceedings, HCI International
2005 Thematic Session on Universal Access in
Human-Computer Interaction.
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879?887,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Comparing Objective and Subjective Measures of Usability
in a Human-Robot Dialogue System
Mary Ellen Foster and Manuel Giuliani and Alois Knoll
Informatik VI: Robotics and Embedded Systems
Technische Universita?t Mu?nchen
Boltzmannstra?e 3, 85748 Garching bei Mu?nchen, Germany
{foster,giuliani,knoll}@in.tum.de
Abstract
We present a human-robot dialogue sys-
tem that enables a robot to work together
with a human user to build wooden con-
struction toys. We then describe a study in
which na??ve subjects interacted with this
system under a range of conditions and
then completed a user-satisfaction ques-
tionnaire. The results of this study pro-
vide a wide range of subjective and ob-
jective measures of the quality of the in-
teractions. To assess which aspects of the
interaction had the greatest impact on the
users? opinions of the system, we used a
method based on the PARADISE evalua-
tion framework (Walker et al, 1997) to de-
rive a performance function from our data.
The major contributors to user satisfac-
tion were the number of repetition requests
(which had a negative effect on satisfac-
tion), the dialogue length, and the users?
recall of the system instructions (both of
which contributed positively).
1 Introduction
Evaluating the usability of a spoken language dia-
logue system generally requires a large-scale user
study, which can be a time-consuming process
both for the experimenters and for the experimen-
tal subjects. In fact, it can be difficult even to
define what the criteria are for evaluating such a
system (cf. Novick, 1997). In recent years, tech-
niques have been introduced that are designed to
predict user satisfaction based on more easily mea-
sured properties of an interaction such as dialogue
length and speech-recognition error rate. The de-
sign of such performance methods for evaluating
dialogue systems is still an area of open research.
The PARADISE framework (PARAdigm for
DIalogue System Evaluation; Walker et al (1997))
describes a method for using data to derive a per-
formance function that predicts user-satisfaction
scores from the results on other, more easily com-
puted measures. PARADISE uses stepwise mul-
tiple linear regression to model user satisfaction
based on measures representing the performance
dimensions of task success, dialogue quality, and
dialogue efficiency, and has been applied to a wide
range of systems (e.g., Walker et al, 2000; Litman
and Pan, 2002; Mo?ller et al, 2008). If the result-
ing performance function can be shown to predict
user satisfaction as a function of other, more eas-
ily measured system properties, it will be widely
applicable: in addition to making it possible to
evaluate systems based on automatically available
data from log files without the need for extensive
experiments with users, for example, such a per-
formance function can be used in an online, incre-
mental manner to adapt system behaviour to avoid
entering a state that is likely to reduce user satis-
faction, or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
Automated evaluation metrics that rate sys-
tem behaviour based on automatically computable
properties have been developed in a number of
other fields: widely used measures include BLEU
(Papineni et al, 2002) for machine translation and
ROUGE (Lin, 2004) for summarisation, for exam-
ple. When employing any such metric, it is cru-
cial to verify that the predictions of the automated
evaluation process agree with human judgements
of the important aspects of the system output. If
not, the risk arises that the automated measures do
not capture the behaviour that is actually relevant
for the human users of a system. For example,
Callison-Burch et al (2006) presented a number of
879
counter-examples to the claim that BLEU agrees
with human judgements. Also, Foster (2008) ex-
amined a range of automated metrics for evalua-
tion generated multimodal output and found that
few agreed with the preferences expressed by hu-
man judges.
In this paper, we apply a PARADISE-style pro-
cess to the results of a user study of a human-robot
dialogue system. We build models to predict the
results on a set of subjective user-satisfaction mea-
sures, based on objective measures that were either
gathered automatically from the system logs or de-
rived from the video recordings of the interactions.
The results indicate that the most significant con-
tributors to user satisfaction were the number of
system turns in the dialogues, the users? ability to
recall the instructions given by the robot, and the
number of times that the user had to ask for in-
structions to be repeated. The former two mea-
sures were positively correlated with user satisfac-
tion, while the latter had a negative impact on user
satisfaction; however the correlation in all cases
was relatively low. At the end of the paper, we
discuss possible reasons for these results and pro-
pose other measures that might have a larger effect
on users? judgements.
2 Task-Based Human-Robot Dialogue
This study makes use of the JAST human-robot
dialogue system (Rickert et al, 2007) which sup-
ports multimodal human-robot collaboration on a
joint construction task. The user and the robot
work together to assemble wooden construction
toys on a common workspace, coordinating their
actions through speech, gestures, and facial dis-
plays. The robot (Figure 1) consists of a pair
of manipulator arms with grippers, mounted in
a position to resemble human arms, and an an-
imatronic talking head (van Breemen, 2005) ca-
pable of producing facial expressions, rigid head
motion, and lip-synchronised synthesised speech.
The system can interact in English or German.
The robot is able to manipulate objects in the
workspace and to perform simple assembly tasks.
In the system that was used in the current study,
the robot instructs the user on building a partic-
ular compound object, explaining the necessary
assembly steps and retrieving pieces as required,
with the user performing the actual assembly ac-
tions. To make joint action necessary for success
in the assembly task, the workspace is divided into
Figure 1: The JAST dialogue robot
SYSTEM First we will build a windmill. Okay?
USER Okay.
SYSTEM To make a windmill, we must make a
snowman.
SYSTEM [picking up and holding out red cube]
To make a snowman, insert the green bolt
through the end of this red cube and screw
it into the blue cube.
USER [takes cube, performs action] Okay.
SYSTEM [picking up and holding out a small
slat] To make a windmill, insert the yellow
bolt through the middle of this short slat and
the middle of another short slat and screw it
into the snowman.
USER [takes slat, performs action] Okay.
SYSTEM Very good!
Figure 2: Sample human-robot dialogue
880
(a) Windmill (b) Snowman (c) L Shape (d) Railway signal
Figure 3: The four target objects used in the experiment
two areas?one belonging to the robot and one to
the user?so that the robot must hand over some
pieces to the user. Figure 2 shows a sample dia-
logue in which the system explains to the user how
to build an object called a ?windmill?, which has a
sub-component called a ?snowman?.
3 Experiment Design
The human-robot system was evaluated via a user
study in which subjects interacted with the com-
plete system; all interactions were in German. As
a between-subjects factor, we manipulated two as-
pects of the generated output: the strategy used by
the dialogue manager to explain a plan to the user,
and the type of referring expressions produced by
the system. Foster et al (2009) give the details
of these factors and describes the effects of each
individual manipulation. In this paper, we concen-
trate on the relationships among the different fac-
tors that were measured during the study: the effi-
ciency and quality of the dialogues, the users? suc-
cess at building the required objects and at learn-
ing the construction plans for new objects, and the
users? subjective reactions to the system.
3.1 Subjects
43 subjects (27 male) took part in this experi-
ment; the results of one additional subject were
discarded due to technical problems with the sys-
tem. The mean age of the subjects was 24.5, with a
minimum of 14 and a maximum of 55. Of the sub-
jects who indicated an area of study, the two most
common areas were Informatics (12 subjects) and
Mathematics (10). On a scale of 1?5, subjects
gave a mean assessment of their knowledge of
computers at 3.4, of speech-recognition systems
at 2.3, and of human-robot systems at 2.0. The
subjects were compensated for their participation
in the experiment.
3.2 Scenario
In this experiment, each subject built the same
three objects in collaboration with the system,
always in the same order. The first target
was a ?windmill? (Figure 3a), which has a sub-
component called a ?snowman? (Figure 3b). Once
the windmill was completed, the system then
walked the user through building an ?L shape?
(Figure 3c). Finally, the robot instructed the user
to build a ?railway signal? (Figure 3d), which com-
bines an L shape with a snowman. During the con-
struction of the railway signal, the system asked
the user if they remembered how to build a snow-
man and an L shape. If the user did not remember,
the system explained the building process again; if
they did remember, the system simply told them to
build another one.
3.3 Dependent Variables
We gathered a wide range of dependent measures:
objective measures derived from the system logs
and video recordings, as well as subjective mea-
sures based on the users? own ratings of their ex-
perience interacting with the system.
3.3.1 Objective Measures
We collected a range of objective measures from
the log files and videos of the interactions. Like
Litman and Pan (2002), we divided our objective
measures into three categories based on those used
in the PARADISE framework: dialogue efficiency,
dialogue quality, and task success.
The dialogue efficiency measures concentrated
on the timing of the interaction: the time taken to
complete the three construction tasks, the number
of system turns required for the complete interac-
tion, and the mean time taken by the system to re-
spond to the user?s requests.
We considered four measures of dialogue qual-
ity. The first two measures looked specifically for
signs of problems in the interaction, using data au-
881
tomatically extracted from the logs: the number of
times that the user asked the system to repeat its
instructions, and the number of times that the user
failed to take an object that the robot attempted to
hand over. The other two dialogue quality mea-
sures were computed based on the video record-
ings: the number of times that the user looked at
the robot, and the percentage of the total inter-
action that they spent looking at the robot. We
considered these gaze-based measures to be mea-
sures of dialogue quality since it has previously
been shown that, in this sort of task-based interac-
tion where there is a visually salient object, par-
ticipants tend to look at their partner more often
when there is a problem in the interaction (e.g.,
Argyle and Graham, 1976).
The task success measures addressed user suc-
cess in the two main tasks undertaken in these in-
teractions: assembling the target objects following
the robot?s instructions, and learning and remem-
bering to make a snowman and an L shape. We
measured task success in two ways, correspond-
ing to these two main tasks. The user?s success in
the overall assembly task was assessed by count-
ing the proportion of target objects that were as-
sembled as intended (i.e., as in Figure 3), which
was judged based on the video recordings. To
test whether the subjects had learned how to build
the sub-components that were required more than
once (the snowman and the L shape), we recorded
whether they said yes or no when they were asked
if they remembered each of these components dur-
ing the construction of the railway signal.
3.3.2 Subjective Measures
In addition to the above objective measures, we
also gathered a range of subjective measures. Be-
fore the interaction, we asked subjects to rate their
current level on a set of 22 emotions (Ortony
et al, 1988) on a scale from 1 to 4; the subjects
then rated their level on the same emotional scales
again after the interaction. After the interaction,
the subjects also filled out a user-satisfaction ques-
tionnaire, which was based on that used in the user
evaluation of the COMIC dialogue system (White
et al, 2005), with modifications to address spe-
cific aspects of the current dialogue system and the
experimental manipulations in this study. There
were 47 items in total, each of which requested
that the user choose their level of agreement with
a given statement on a five-point Likert scale. The
items were divided into the following categories:
Mean (Stdev) Min Max
Length (sec) 305.1 (54.0) 195.2 488.4
System turns 13.4 (1.73) 11 18
Response time (sec) 2.79 (1.13) 1.27 7.21
Table 1: Dialogue efficiency results
Opinion of the robot as a partner 21 items ad-
dressing the ease with which subjects were
able to interact with the robot
Instruction quality 6 items specifically address-
ing the quality of the assembly instructions
given by the robot
Task success 11 items asking the user to rate how
well they felt they performed on the various
assembly tasks
Feelings of the user 9 items asking users to rate
their feelings while using the system
At the end of the questionnaire, subjects were also
invited to give free-form comments.
4 Results
In this section, we present the results of each of
the individual dependent measures; in the follow-
ing section, we examine the relationship among
the different types of measures. These results are
based on the data from 40 subjects: we excluded
results from two subjects for whom the video data
was not clear, and from one additional subject who
appeared to be ?testing? the system rather than
making a serious effort to interact with it.
4.1 Objective Measures
Dialogue efficiency The results on the dialogue
efficiency measures are shown in Table 1. The
average subject took 305.1 seconds?that is, just
over five minutes?to build all three of the objects,
and an average dialogue took 13 system turns to
complete. When a user made a request, the mean
delay before the beginning of the system response
was about three seconds, although for one user this
time was more than twice as long. This response
delay resulted from two factors. First, prepar-
ing long system utterances with several referring
expressions (such as the third and fourth system
turns in Figure 2) takes some time; second, if a
user made a request during a system turn (i.e., a
?barge-in? attempt), the system was not able to re-
spond until the current turn was completed.
882
Mean (Stdev) Min Max
Repetition requests 1.86 (1.79) 0 6
Failed hand-overs 1.07 (1.35) 0 6
Looks at the robot 23.55 (8.21) 14 50
Time looking at robot (%) 27 (8.6) 12 51
Table 2: Dialogue quality results
These three measures of efficiency were cor-
related with each other: the correlation between
length and turns was 0.38; between length and re-
sponse time 0.47; and between turns and response
time 0.19 (all p < 0.0001).
Dialogue quality Table 2 shows the results for
the dialogue quality measures: the two indica-
tions of problems, and the two measures of the
frequency with which the subjects looked at the
robot?s head. On average, a subject asked for an
instruction to be repeated nearly two times per
interaction, while failed hand-overs occurred just
over once per interaction; however, as can be seen
from the standard-deviation values, these mea-
sures varied widely across the data. In fact, 18
subjects never failed to take an object from the
robot when it was offered, while one subject did so
five times and one six times. Similarly, 11 subjects
never asked for any repetitions, while five subjects
asked for repetitions five or more times.1 On aver-
age, the subjects in this study spent about a quarter
of the interaction looking at the robot head, and
changed their gaze to the robot 23.5 times over
the course of the interaction. Again, there was a
wide range of results for both of these measures:
15 subjects looked at the robot fewer than 20 times
during the interaction, 20 subjects looked at the
robot between 20 to 30 times, while 5 subjects
looked at the robot more than 30 times.
The two measures that count problems were
mildly correlated with each other (R2 = 0.26, p <
0.001), as were the two measures of looking at the
robot (R2 = 0.13, p < 0.05); there was no correla-
tion between the two classes of measures.
Task success Table 3 shows the success rate for
assembling each object in the sequence. Objects
in italics represent sub-components, as follows:
the first snowman was constructed as part of the
windmill, while the second formed part of the rail-
way signal; the first L-shape was a goal in itself,
1The requested repetition rate was significantly affected
by the description strategy used by the dialogue manager; see
Foster et al (2009) for details.
Object Rate Memory
Snowman 0.76
Windmill 0.55
L shape 0.90
L shape 0.90 0.88
Snowman 0.86 0.70
Railway signal 0.71
Overall 0.72 0.79
Table 3: Task success results
while the second was also part of the process of
building the railway signal. The Rate column indi-
cates subjects? overall success at building the rel-
evant component?for example, 55% of the sub-
jects built the windmill correctly, while both of
the L-shapes were built with 90% accuracy. For
the second occurrence of the snowman and the L-
shape, the Memory column indicates the percent-
age of subjects who claimed to remember how to
build it when asked. The Overall row at the bottom
indicates subjects? overall success rate at building
the three main target objects (windmill, L shape,
railway signal): on average, a subject built about
two of the three objects correctly.
The overall correct-assembly rate was corre-
lated with the overall rate of remembering objects:
R2 = 0.20, p < 0.005. However, subjects who said
that they did remember how to build a snowman or
an L shape the second time around were no more
likely to do it correctly than those who said that
they did not remember.
4.2 Subjective Measures
Two types of subjective measures were gath-
ered during this study: responses on the user-
satisfaction questionnaire, and self-assessment of
emotions. Table 4 shows the mean results for each
category from the user-satisfaction questionnaire
across all of the subjects, in all cases on a 5-point
Likert scale. The subjects in this study gave a
generally positive assessment of their interactions
with the system?with a mean overall satisfaction
score of 3.75?and rated their perceived task suc-
cess particularly highly, with a mean score of 4.1.
To analyse the emotional data, we averaged all
of the subjects? emotional self-ratings before and
after the experiment, counting negative emotions
on an inverse scale, and then computed the differ-
ence between the two means. Table 5 shows the re-
sults from this analysis; note that this value was as-
sessed on a 1?4 scale. While the mean emotional
883
Question category Mean (Stdev)
Robot as partner 3.63 (0.65)
Instruction quality 3.69 (0.71)
Task success 4.10 (0.68)
Feelings 3.66 (0.61)
Overall 3.75 (0.57)
Table 4: User-satisfaction questionnaire results
Mean (Stdev) Min Max
Before the study 2.99 (0.32) 2.32 3.68
After the study 3.05 (0.32) 2.32 3.73
Change +0.06 (0.24) ?0.55 +0.45
Table 5: Mean emotional assessments
score across all of the subjects did not change over
the course of the experiment, the ratings of indi-
vidual subjects did show larger changes. As shown
in the final row of the table, one subject?s mean rat-
ing decreased by 0.55 over the course of the inter-
action, while that of another subject increased by
0.45. There was a slight correlation between the
subjects? description of their emotional state after
the experiment and their responses to the question-
naire items asking for feelings about the interac-
tion: R2 = 0.14, p < 0.01.
5 Building Performance Functions
In the preceding section, we presented results on a
number of objective and subjective measures, and
also examined the correlation among measures of
the same type. The results on the objective mea-
sures varied widely across the subjects; also, the
subjects generally rated their experience of using
the system positively, but again with some varia-
tion. In this section, we examine the relationship
among measures of different types in order to de-
termine which of the objective measures had the
largest effect on users? subjective reactions to the
dialogue system.
To determine the relationship among the fac-
tors, we employed the procedure used in the
PARADISE evaluation framework (Walker et al,
1997). The PARADISE model uses stepwise mul-
tiple linear regression to predict subjective user
satisfaction based on measures representing the
performance dimensions of task success, dialogue
quality, and dialogue efficiency, resulting in a pre-
dictor function of the following form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Using stepwise linear regression, we computed
a predictor function for each of the subjective mea-
sures that we gathered during our study: the mean
score for each of the individual user-satisfaction
categories (Table 4), the mean score across the
whole questionnaire (the last line of Table 4), as
well as the difference between the users? emo-
tional states before and after the study (the last line
of Table 5). We included all of the objective mea-
sures from Section 4.1 as initial predictors.
The resulting predictor functions are shown in
Table 6. The following abbreviations are used for
the factors that occur in the table: Rep for the num-
ber of repetition requests, Turns for the number of
system turns, Len for the length of the dialogue,
and Mem for the subjects? memory for the com-
ponents that were built twice. The R2 column in-
dicates the percentage of the variance that is ex-
plained by the performance function, while the
Significance column gives significance values for
each term in the function.
Although the R2 values for the predictor func-
tions in Table 6 are generally quite low, indicat-
ing that the functions do not explain most of the
variance in the data, the factors that remain after
stepwise regression still provide an indication as
to which of the objective measures had an effect
on users? opinions of the system. In general, users
who had longer interactions with the system (in
terms of system turns) and who said that they re-
membered the robot?s instructions tended to give
the system higher scores, while users who asked
for more instructions to be repeated tended to give
it lower scores; for the robot-as-partner questions,
the length of the dialogue in seconds also made a
slight negative contribution. None of the other ob-
jective factors contributed significantly to any of
the predictor functions.
6 Discussion
That the factors included in Table 6 were the most
significant contributors to user satisfaction is not
surprising. If a user asks for instructions to be re-
884
Measure Function R2 Significance
Robot as partner 3.60+0.53?N (Turns)?0.39?N (Rep)?0.18?N (Len) 0.12 Turns: p < 0.01,
Rep: p < 0.05,
Length: p? 0.17
Instruction quality 3.66?0.22?N (Rep) 0.081 Rep: p < 0.05
Task success 4.07+0.20?N (Mem) 0.058 Mem: p? 0.07
Feelings 3.63+0.34?N (Turns)?0.32?N (Rep) 0.044 Turns: p? 0.06, Rep:
p? 0.08
Overall 3.73?0.36?N (Rep)+0.31?N (Turns) 0.062 Rep: p < 0.05,
Turns: p? 0.06
Emotion change 0.07+0.14?N (Turns)+0.11?N (Mem)?0.090?N (Rep) 0.20 Turns: p < 0.05,
Mem: p < 0.01,
Rep: p? 0.17
Table 6: Predictor functions
peated, this is a clear indication of a problem in
the dialogue; similarly, users who remembered the
system?s instructions were equally clearly having
a relatively successful interaction.
In the current study, increased dialogue length
had a positive contribution to user satisfaction; this
contrasts with results such as those of Litman and
Pan (2002), who found that increased dialogue
length was associated with decreased user satis-
faction. We propose two possible explanations for
this difference. First, the system analysed by Lit-
man and Pan (2002) was an information-seeking
dialogue system, in which efficient access to the
information is an important criterion. The current
system, on the other hand, has the goal of joint task
execution, and pure efficiency is a less compelling
measure of dialogue quality in this setting. Sec-
ond, it is possible that the sheer novelty factor of
interacting with a fully-embodied humanoid robot
affected people?s subjective responses to the sys-
tem, so that subjects who had longer interactions
also enjoyed the experience more. Support for this
explanation is provided by the fact that dialogue
length was only a significant factor in the more
?subjective? parts of the questionnaire, but did not
have a significant impact on the users? judgements
about instruction quality or task success. Other
studies of human-robot dialogue systems have also
had similar results: for example, the subjects in the
study described by Sidner et al (2005) who used
a robot that moved while talking reported higher
levels of engagement in the interaction, and also
tended to have longer conversations with the robot.
While the predictor functions give useful in-
sights into the relative contribution of the objective
measures to the subjective user satisfaction, the
R2 values are generally lower than those found in
other PARADISE-style evaluations. For example,
Walker et al (1998) reported an R2 value of 0.38,
the values reported by Walker et al (2000) on the
training sets ranged from 0.39 to 0.56, Litman and
Pan (2002) reported an R2 value of 0.71, while
the R2 values reported by Mo?ller et al (2008)
for linear regression models similar to those pre-
sented here were between 0.22 and 0.57. The low
R2 values from this analysis clearly suggest that,
while the factors included in Table 6 did affect
users? opinions?particularly their opinion of the
robot as a partner and the change in their reported
emotional state?the users? subjective judgements
were also affected by factors other than those cap-
tured by the objective measures considered here.
In most of the previous PARADISE-style stud-
ies, measures addressing the performance of the
automated speech-recognition system and other
input-processing components were included in the
models. For example, the factors listed by Mo?ller
et al (2008) include several measures of word er-
ror rate and of parsing accuracy. However, the sce-
nario that was used in the current study required
minimal speech input from the user (see Figure 2),
so we did not include any such input-processing
factors in our models.
Other objective factors that might be relevant
for predicting user satisfaction in the current study
include a range of non-verbal behaviour from the
users. For example, the user?s reaction time to in-
structions from the robot, the time the users need
to adapt to the robot?s movements during hand-
over actions (Huber et al, 2008), or the time taken
for the actual assembly of the objects. Also, other
measures of the user?s gaze behaviour might be
885
useful: more global measures such as how often
the users look at the robot arms or at the objects on
the table, as well as more targeted measures exam-
ining factors such as the user?s gaze and other be-
haviour during and after different types of system
outputs. In future studies, we will also gather data
on these additional non-verbal behaviours, and we
expect to find higher correlations with subjective
judgements.
7 Conclusions and Future Work
We have presented the JAST human-robot dia-
logue system and described a user study in which
the system instructed users to build a series of tar-
get objects out of wooden construction toys. This
study resulted in a range of objective and subjec-
tive measures, which were used to derive perfor-
mance functions in the style of the PARADISE
evaluation framework. Three main factors were
found to affect the users? subjective ratings: longer
dialogues and higher recall performance were as-
sociated with increased user satisfaction, while di-
alogues with more repetition requests tended to be
associated with lower satisfaction scores. The ex-
plained variance of the performance functions was
generally low, suggesting that factors other than
those measured in this study contributed to the
user satisfaction scores; we have suggested several
such factors.
The finding that longer dialogues were associ-
ated with higher user satisfaction disagrees with
the results of many previous PARADISE-style
evaluation studies. However, it does confirm and
extend the results of previous studies specifically
addressing interactions between users and embod-
ied agents: as in the previous studies, the users in
this case seem to view the agent as a social entity
with whom they enjoy having a conversation.
A newer version of the JAST system is currently
under development and will shortly undergo a user
evaluation. This new system will support an ex-
tended set of interactions where both agents know
the target assembly plan, and will will also in-
corporate enhanced components for vision, object
recognition, and goal inference. When evaluat-
ing this new system, we will include similar mea-
sures to those described here to enable the eval-
uations of the two systems to be compared. We
will also gather additional objective measures in
order to measure their influence on the subjective
results. These additional measures will include
those mentioned at the end of the preceding sec-
tion, as well as measures targeted at the revised
scenario and the updated system capabilities?for
example, an additional dialogue quality measure
will assess how often the goal-inference system
was able to detect and correctly respond to an error
by the user.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka for his help in
running the experiment and analysing the data.
References
M. Argyle and J. A. Graham. 1976. The Cen-
tral Europe experiment: Looking at persons and
looking at objects. Environmental Psychology
and Nonverbal Behavior, 1(1):6?16. doi:10.
1007/BF01115461.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of the AISB
2005 Creative Robotics Symposium.
C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of
EACL 2006. ACL Anthology E06-1032.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for
an embodied conversational agent. In Proceed-
ings of INLG 2008. ACL Anthology W08-1113.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI 2009.
M. Huber, M. Rickert, A. Knoll, T. Brandt, and
S. Glasauer. 2008. Human-robot interaction in
handing-over tasks. In Proceedings of IEEE
RO-MAN 2008. doi:10.1109/ROMAN.2008.
4600651.
C. Y. Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop on Text Summariza-
tion. ACL Anthology W04-1013.
2http://www.euprojects-jast.net/
3http://www.ics.forth.gr/indigo/
886
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue sys-
tem. User Modeling and User-Adapted Inter-
action, 12(2?3):111?137. doi:10.1023/A:
1015036910358.
S. Mo?ller, K.-P. Engelbrecht, and R. Schleicher.
2008. Predicting the quality and usability of
spoken dialogue systems. Speech Communica-
tion, 50:730?744. doi:10.1016/j.specom.
2008.03.001.
D. G. Novick. 1997. What is effective-
ness? In Working notes, CHI ?97 Work-
shop on HCI Research and Practice Agenda
Based on Human Needs and Social Responsi-
bility. http://www.cs.utep.edu/novick/
papers/eff.chi.html.
A. Ortony, G. L. Clore, and A. Collins. 1988. The
Cognitive Structure of Emotions. Cambridge
University Press.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: A method for automatic evalua-
tion of machine translation. In Proceedings of
ACL 2002. ACL Anthology P02-1040.
M. Rickert, M. E. Foster, M. Giuliani, T. By,
G. Panin, and A. Knoll. 2007. Integrating lan-
guage, vision and action for human robot dialog
systems. In Proceedings of HCI International
2007. doi:10.1007/978-3-540-73281-5_
108.
C. L. Sidner, C. Lee, C. D. Kidd, N. Lesh, and
C. Rich. 2005. Explorations in engagement
for humans and robots. Artificial Intelligence,
166(1?2):140?164. doi:10.1016/j.artint.
2005.03.005.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. A. Walker, J. Fromer, G. D. Fabbrizio, C. Mes-
tel, and D. Hindle. 1998. What can I say?: Eval-
uating a spoken language interface to email.
In Proceedings of CHI 1998. doi:10.1145/
274644.274722.
M. A. Walker, D. J. Litman, C. A. Kamm, and
A. Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In Proceed-
ings of ACL/EACL 1997. ACL Anthology P97-
1035.
M. White, M. E. Foster, J. Oberlander, and
A. Brown. 2005. Using facial feedback to en-
hance turn-taking in a multimodal dialogue sys-
tem. In Proceedings of HCI International 2005.
887
Data-driven Generation of Emphatic Facial Displays
Mary Ellen Foster
Department of Informatics, Technical University of Munich
Boltzmannstra?e 3, 85748 Garching, Germany
foster@in.tum.de
Jon Oberlander
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW, United Kingdom
jon@inf.ed.ac.uk
Abstract
We describe an implementation of data-
driven selection of emphatic facial dis-
plays for an embodied conversational
agent in a dialogue system. A corpus of
sentences in the domain of the target dia-
logue system was recorded, and the facial
displays used by the speaker were anno-
tated. The data from those recordings was
used in a range of models for generating
facial displays, each model making use of
a different amount of context or choosing
displays differently within a context. The
models were evaluated in two ways: by
cross-validation against the corpus, and by
asking users to rate the output. The predic-
tions of the cross-validation study differed
from the actual user ratings. While the
cross-validation gave the highest scores to
models making a majority choice within a
context, the user study showed a signifi-
cant preference for models that produced
more variation. This preference was espe-
cially strong among the female subjects.
1 Introduction
It has long been documented that there are char-
acteristic facial displays that accompany the em-
phasised parts of spoken utterances. For example,
Ekman (1979) says that eyebrow raises ?appear to
coincide with primary vocal stress, or more sim-
ply with a word that is spoken more loudly.? Cor-
relations have also been found between prosodic
features and events such as head nodding and the
amplitude of mouth movements. When Krah-
mer and Swerts (2004) performed an empirical,
cross-linguistic evaluation of the influence of brow
movements on the perception of prosodic stress,
they found that subjects preferred eyebrow move-
ments to be correlated with the most prominent
word in an utterance and that eyebrow movements
boosted the perceived prominence of the word
they were associated with.
While many facial displays have been shown
to co-occur with prosodic accents, the converse
is not true: in normal embodied speech, many
pitch accents and other prosodic events are unac-
companied by any facial display, and when dis-
plays are used, the selection varies widely. Cas-
sell and Tho?risson (1999) demonstrated that ?en-
velope? facial displays related to the process of
conversation have a greater impact on successful
interaction with an embodied conversational agent
than do emotional displays. However, no descrip-
tion of face motion is sufficiently detailed that it
can be used as the basis for selecting emphatic fa-
cial displays for an agent. This is therefore a task
for which data-driven techniques are beneficial.
In this paper, we address the task of selecting
emphatic facial displays for the talking head in
the COMIC1 multimodal dialogue system. In the
basic COMIC process for generating multimodal
output (Foster et al, 2005), facial displays are se-
lected using simple rules based only on the pitch
accents specified by the text generation system. In
order to make a more sophisticated and naturalis-
tic selection of facial displays, we recorded a sin-
gle speaker reading a set of sentences drawn from
the COMIC domain, and annotated the facial dis-
plays that he used and the contexts in which he
used them. We then created models based on the
data from this corpus and used them to choose the
facial displays for the COMIC talking head.
1http://www.hcrc.ed.ac.uk/comic/
353
The rest of this paper is arranged as follows.
First, in Section 2, we describe previous ap-
proaches to selecting non-verbal behaviour for
embodied conversational agents. In Section 3, we
then show how we collected and annotated a cor-
pus of facial displays, and give some generalisa-
tions about the range of displays found in the cor-
pus. After that, in Section 4, we outline how we
implemented a range of models for selecting be-
haviours for the COMIC agent using the corpus
data, using varying amounts of context and differ-
ent selection strategies within a context. Next, we
give the results of two evaluation studies compar-
ing the quality of the output generated by the var-
ious models: a cross-validation study against the
corpus (Section 5) and a direct user evaluation of
the output (Section 6). In Section 7, we discuss the
results of these two evaluations. Finally, in Sec-
tion 8, we draw some conclusions from the current
study and outline potential follow-up work.
2 Choosing Non-Verbal Behaviour for
Embodied Conversational Agents
Embodied Conversational Agents (ECAs) are
computer interfaces that are represented as hu-
man bodies, and that use their face and body in
a human-like way in conversations with the user
(Cassell et al, 2000). The main benefit of ECAs
is that they allow users to interact with a computer
in the most natural possible setting: face-to-face
conversation. However, to realise this advantage
fully, the agent must produce high-quality output,
both verbal and non-verbal. A number of previous
systems have based the choice of non-verbal be-
haviours for an ECA on the behaviours of humans
in conversational situations. The implementations
vary as to how directly they use the human data.
In some systems, motion specifications for the
agent are created from scratch, using rules derived
from studying human behaviour. For the REA
agent (Cassell et al, 2001a), for example, ges-
turing behaviour was selected to perform particu-
lar communicative functions, using rules based on
studies of typical North American non-verbal dis-
plays. Similarly, the Greta agent (de Carolis et al,
2002) selected its performative facial displays us-
ing hand-crafted rules to map from affective states
to facial motions. Such implementations do not
make direct use of any recorded human motions;
this means that they generate average behaviours
from a range of people, but it is difficult to adapt
them to reproduce the behaviour of an individual.
In contrast, other ECA implementations have
selected non-verbal behaviour based directly on
motion-capture recordings of humans. Stone et al
(2004), for example, recorded an actor performing
scripted output in the domain of the target system.
They then segmented the recordings into coher-
ent phrases and annotated them with the relevant
semantic and pragmatic information, and com-
bined the segments at run-time to produce com-
plete performance specifications that were then
played back on the agent. Cunningham et al
(2004) and Shimodaira et al (2005) used similar
techniques to base the appearance and motions of
their talking heads directly on recordings of hu-
man faces. This technique is able to produce more
naturalistic output than the more rule-based sys-
tems described above; however, capturing the mo-
tion requires specialised hardware, and the agent
must be implemented in such a way that it can ex-
actly reproduce the human motions.
A middle ground is to use a purely synthetic
agent?one whose behaviour is controlled by
high-level instructions, rather than based directly
on human motions?but to create the instructions
for that agent using the data from an annotated cor-
pus of human behaviour. Like a motion-capture
implementation, this technique can also produce
increased naturalism in the output and also al-
lows choices to be based on the motions of a sin-
gle performer if necessary. However, annotating
a video corpus can be less technically demand-
ing than capturing and directly re-using real mo-
tions, especially when the corpus and the number
of features under consideration are small. This ap-
proach has been taken, for example, by Cassell
et al (2001b) to choose posture shifts for REA,
and by Kipp (2004) to select gestures for agents,
and it is also the approach that we adopt here.
3 Recording and Annotation
The recording script for the data collection con-
sisted of 444 sentences in the domain of the
COMIC multimodal dialogue system; all of the
sentences described one or more features of one or
more bathroom-tile designs. The sentences were
generated by the full COMIC output planner, and
were selected to provide coverage of all of the
syntactic patterns available to the system. In ad-
dition to the surface text, each sentence included
all of the contextual information from the COMIC
354
46. More about the current design
they dislike the first feature, but like the second one
There are GEOMETRIC SHAPES on the
decorative tiles, but the tiles ARE from the
ARMONIE series.
Figure 1: Sample prompt slide
planner: the predicted pitch accents?selected ac-
cording to Steedman?s (2000) theory of informa-
tion structure and intonation?along with any in-
formation from the user model and dialogue his-
tory. The sentences were presented one at a time
to the speaker, who was instructed to read each
sentence out loud as expressively as possible while
looking into a camera directed at his face. The seg-
ments for which the presentation planner specified
pitch accents were highlighted, and any applicable
user-model and dialogue-history information was
included. Figure 1 shows a sample prompt slide.
The recorded videos were annotated by the first
author, using a purpose-built tool that allowed any
set of facial displays to be associated with any seg-
ment of the sentence. First, the video was split into
clips corresponding to each sentence. After that,
the facial displays in each clip were annotated.
The following were the displays that were consid-
ered: eyebrow raising and lowering; eye squinting;
head nodding (up, small down, large down); head
leaning (left and right); and head turning (left and
right). Figure 2 shows examples of two typical
display combinations. Any combination of these
facial displays could be associated with any of the
relevant segments in the text. The relevant seg-
ments included all mentions of tile-design prop-
erties (e.g., colours, designers), modifiers such
as once again and also, deictic determiners (this,
these), and verbs in contrastive contexts (e.g., are
in Figure 1). The annotation scheme treated all fa-
cial displays as batons rather than underliners (Ek-
man, 1979); that is, each display was associated
with a single segment. If a facial display spanned
a longer phrase in the speech, it was annotated as a
series of identical batons on each of the segments.
Any predicted pitch accents and dialogue-
history and user-model information from the
COMIC presentation planner were also associated
with each segment, as appropriate. We chose not
to restrict our annotation to those segments with
predicted pitch accents, because the speaker also
made a large number of facial displays on seg-
ments with no predicted pitch accent; instead, we
incorporated the predicted accent as an additional
contextual factor. For the most part, the pitch ac-
cents used by the speaker followed the specifica-
tions on the slides. We did not explicitly consider
the rhetorical or syntactic structure, as did, e.g.,
de Carolis et al (2000); in general, the structure
was fully determined by the context.
There were a total of 1993 relevant segments in
the recorded sentences. Overall, the most frequent
display combination was a small downward nod
on its own, which occurred on just over 25% of the
segments. The second largest class was no motion
at all (20% of the segments), followed by down-
ward nods (large and small) accompanied by brow
raises. Further down the order, the various lateral
motions appear; for this speaker, these were pri-
marily turns to the right (Figure 2(a)) and leans to
the left (Figure 2(b)).
The distribution of facial displays in specific
contexts differed from the overall distribution. The
biggest influence was the user-model evaluation:
left leans, brow lowering, and eye squinting were
all relatively more frequent on objects with nega-
tive user-model evaluations, while right turns and
brow raises occurred more often in positive con-
texts. Other factors also had an influence: for ex-
ample, nodding and brow raises were both more
frequent on segments for which the COMIC plan-
ner specified a pitch accent. Foster (2006) gives a
detailed analysis of these recordings.
4 Modelling the Corpus Data
We built a range of models using the data from
the annotated corpus to select facial displays to
accompany generated text. For each segment in
the text, a model selected a display combination
from among the displays used by the speaker in a
similar context. All of the models used the corpus
counts of displays associated with the segments di-
rectly, with no back-off or smoothing.
The models differed from one another in two
ways: the amount of context that they used, and
the way in which they made a selection within a
context. There were three levels of context:
No context These models used the overall corpus
counts for all segments.
355
(a) Right turn + brow raise (b) Left lean + brow lower
Figure 2: Typical speaker motions from the recording
Surface only These models used only the context
provided by the word(s)?or, in some cases,
a domain-specific semantic class. For exam-
ple, a model would use the class DECORA-
TION rather than the specific word artwork.
Full context In addition to the surface form, these
models also used the pitch-accent specifica-
tions and contextual information supplied by
the COMIC presentation planner. The con-
textual information was associated with the
tile-design properties included in the sen-
tence and indicated (a) whether that property
had been mentioned before, (b) whether it
was explicitly contrasted with a property of
a previous design, and (c) the expected user
evaluation of that property.
Within a context, there were two strategies for se-
lecting a facial display:
Majority Choose the combination that occurred
the largest number of times in the context.
Weighted Make a random choice from all com-
binations seen in the context, weighting the
choice according to the relative frequency.
For example, in the no-context case, a majority-
choice model would choose the small downward
nod (the majority option) for every segment, while
a weighted-choice model would choose a small
downward nod with probability 0.25, no motion
with probability 0.2, and the other displays with
correspondingly decreasing probabilities.
These two factors produced a set of 6 models
in total (3 context levels ? 2 selection strategies).
Throughout the rest of this paper, we will use two-
character labels to refer to the models. The first
character of each label indicates the amount of
     


	
	











 

 











Figure 3: Mean F score for all models
context that was used, while the second indicates
the selection method within that context: for ex-
ample, SM corresponds to a model that used the
surface form only and made a majority choice.
5 Evaluation 1: Cross-validation
We first compared the performance of the models
using 10-fold cross-validation against the corpus.
For each fold, we built models using 90% of the
sentences in the corpus, and then used those mod-
els to predict the facial displays for the sentences
in the other 10% of the corpus. We measured the
recall and precision on a sentence by comparing
the predicted facial displays for each segment to
the actual displays used by the speaker and aver-
aging those scores across the sentence. We then
used the recall and precision scores for a sentence
to compute a sentence-level F score.
Averaged across all of the cross-validation
folds, the NM model had the highest recall score,
while the FM model scored highest for precision
and F score. Figure 3 shows the average sentence-
level F score for all of the models. All but one
of the differences shown are significant at the p <
356
(a) Neutral (b) Right turn + brow raise (c) Left lean + brow lower
Figure 4: Synthesised version of motions from Figure 2
0.01 level on a paired T test; the performance of
the NM and FW models was indistinguishable on
F score, although the FW model scored higher on
precision and the NM model on recall.
That the majority-choice models generally
scored better on this measure than the weighted-
choice models is not unexpected: a weighted-
choice model is more likely to choose a less-
common display, and if it chooses it in a context
where the speaker did not, the score for that sen-
tence is decreased. It is also not surprising that,
within a selection strategy, the models that take
into account more of the context did better than
those that use less of it; this is simply an indica-
tion that there are patterns in the corpus, and that
all of the contextual information contributes to the
selection of displays.
6 Evaluation 2: User Ratings
The majority-choice models performed better on
the cross-validation study than the weighted-
choice ones did; however, this does not does not
mean that users will necessarily like their output
in practice. A large amount of the lateral motion
and eyebrow movements occurs in the second- or
third-largest class in a number of contexts, and is
therefore less likely to be selected by a majority-
choice model. If users like to see motion other
than simple nodding, it might be that the sched-
ules generated by the weighted-choice models are
actually preferred. To address this question, we
performed a user evaluation.
6.1 Experiment Design
Materials For this study, we generated 30 new
sentences from the COMIC system. The sen-
tences were selected to ensure that they covered
the full range of syntactic structures available to
COMIC and that none of them was a duplicate
of anything from the recording script. We then
generated a facial schedule for each sentence us-
ing each of the six models. Note that, for some
of the sentences, more than one model produced
an identical sequence of facial displays, either be-
cause the majority choice in a broader context was
the same as in a more narrow context, or because
a weighted-choice model ended up selecting the
majority option in every case. All such identical
schedules were retained in the set of materials; in
Section 6.2, we discuss their impact on the results.
We then made videos of every schedule for ev-
ery sentence, using the Festival speech synthesiser
(Clark et al, 2004) and the RUTH talking head
(DeCarlo et al, 2004). Figure 4 shows synthesised
versions of the facial displays from Figure 2.
Procedure 33 subjects took part in the experi-
ment: 17 female subjects and 16 males. They
were primarily undergraduate students, between
20 and 24 years old, native speakers of English,
with an intermediate amount of computer experi-
ence. Each subject in the study was shown videos
of all 30 sentences in an individually-chosen ran-
dom order. For each sentence, the subject saw
two versions, each generated by a different model,
and was asked to choose which version they liked
better. The displayed versions were counterbal-
anced so that every subject performed each pair-
wise comparison of models twice, once in each
order. The study was run over the web.
6.2 Results2
Figure 5(a) shows the overall preference rates for
all of the models. For each model, the value shown
2 We do not include those trials where both videos were
identical; if these are included, the results are similar, but the
distinctions described here just fail to reach significance.
357
     


	
	














 










Proceedings of the Linguistic Annotation Workshop, pages 25?32,
Prague, June 2007. c?2007 Association for Computational Linguistics
Associating Facial Displays with Syntactic Constituents for Generation
Mary Ellen Foster
Informatik VI: Robotics and Embedded Systems
Technical University of Munich
Boltzmannstra?e 3, 85748 Garching, Germany
foster@in.tum.de
Abstract
We present an annotated corpus of conversa-
tional facial displays designed to be used for
generation. The corpus is based on a record-
ing of a single speaker reading scripted out-
put in the domain of the target generation
system. The data in the corpus consists of
the syntactic derivation tree of each sentence
annotated with the full syntactic and prag-
matic context, as well as the eye and eye-
brow displays and rigid head motion used
by the the speaker. The behaviours of the
speaker show several contextual patterns,
many of which agree with previous findings
on conversational facial displays. The cor-
pus data has been used in several studies ex-
ploring different strategies for selecting fa-
cial displays for a synthetic talking head.
1 Introduction
An increasing number of systems designed to au-
tomatically generate linguistic and multimodal out-
put now make use of corpora to help in decision-
making (cf. Belz and Varges, 2005). Some imple-
mentations use corpora to help select output that is
grammatical or fluent; for example, Langkilde and
Knight (1998) and White (2006) both used n-gram
language models to guide stochastic surface realis-
ers. In other systems, corpora are used to make
decisions based on pragmatic factors such as the
reading level of the target user (Williams and Re-
iter, 2005) or the visual features of an object be-
ing described (Cassell et al, 2007). The latter type
of domain-specific contextual information is not of-
ten included in generally-available corpora. For this
reason, developers of generation systems that need
this type of information often create and make use
of application-specific corpora.
The easiest method of including the necessary
pragmatic information in a corpus is to base the cor-
pus on output generated in situations where the con-
textual factors are known; this eliminates the need to
annotate these factors explicitly. Stone et al (2004),
for example, created a multimodal corpus based on
the voice and body language of an actor performing
scripted output in the domain of the target genera-
tion system: an animated instructor character for a
snowboarding video game. The contextual informa-
tion in the corpus scripts included the move that the
player attempted in the game and the result of that
attempt. Similarly, van Deemter et al (2006) cre-
ated a corpus of multimodal referring expressions
produced in specific pragmatic contexts and used it
to compare several referring-expression generation
algorithms to human performance.
In this work, the task is to select facial displays
for an animated talking head to use while present-
ing output in the COMIC multimodal dialogue sys-
tem (Foster et al, 2005), which generates spoken
descriptions and comparisons of bathroom-tile op-
tions. The output of the COMIC text planner in-
cludes a range of information in addition to the text:
the syntactic derivation tree, the user?s evaluation
of the object being described, the information sta-
tus (new or old, contrastive) of each fact described,
and the predicted speech-synthesiser prosody. All of
this contextual information can be used to help select
25
appropriate facial displays to accompany the spo-
ken presentation; however?as in the other systems
mentioned above?this requires a corpus where the
full context for every facial display is known. To cre-
ate such a corpus, we recorded a speaker performing
scripted output in the domain of COMIC.
This paper is arranged as follows. In Section 2,
we first describe how the scripts for the corpus were
created and how the recording was made. Section 3
then presents the annotation scheme and the tool that
was used to perform the annotation, while Section 4
describes the measures that were taken to ensure that
the annotation was reliable. Section 5 then sum-
marises the high-level patterns that were found in the
displays annotated in the corpus and compares them
to other findings on conversational facial displays.
At the end of the section, we use the corpus data to
test two assumptions that were made in the annota-
tion scheme. After that, in Section 6, we describe
several experiments in which different methods of
using the data in this corpus to select facial displays
for a synthetic head have been compared. Finally,
in Section 7, we summarise the contributions of this
paper and draw some conclusions about the useful-
ness of this corpus for its intended task.
2 Recording
For this corpus, we recorded a single speaker read-
ing a set of 444 scripted sentences in the domain of
the COMIC multimodal dialogue system. The sen-
tences were generated by the full COMIC output-
generation process, which uses the OpenCCG sur-
face realiser (White, 2006) to create texts includ-
ing prosodic specifications for the speech synthe-
siser and incorporates information from the dialogue
history and a model of the user?s likes and dislikes.
Every node in the OpenCCG derivation tree for
each sentence in the script was initially annotated
with all of the available syntactic and pragmatic in-
formation from the output planner, including the fol-
lowing features:
? The user-model evaluation of the object being
described (positive or negative);
? Whether the fact being presented was previ-
ously mentioned in the discourse (as I said be-
fore, . . . ) or is new information;
?Although it's in the family style, the tiles are by Alessi Tiles.?
? although it's in the family style? although? it's in the family style? it? 's in the family style? 's? in the family style? in? the family style? the? family style? family? style
? the tiles are by Alessi Tiles? the tiles? the? tiles? are by Alessi Tiles? are? by Alessi Tiles? Alessi Tiles
User model: badClause: first
User model: goodClause: second
Accent: L+H*
Accent: H*
Figure 1: Annotated OpenCCG derivation tree
? Whether the fact is explicitly compared or con-
trasted with a feature of the previous tile design
(once again . . . but here . . . );
? Whether the node is in the first clause of a two-
clause sentence, in the second clause, or is an
only clause;1
? The surface string associated with the node;
? The surface string, with words replaced by se-
mantic classes or stems drawn from the gram-
mar (e.g., this design is classic becomes this
[mental-obj] be [style]); and
? Any pitch accents specified by the text planner.
Figure 1 illustrates the annotated OpenCCG
derivation tree for a sample sentence drawn from
the recording script. The annotations indicate that
every node in the first half of this sentence is associ-
ated with a negative user-model evaluation and is in
the first clause of a two-clause sentence, while every
node in the second half is linked to a positive eval-
uation and is in the second clause of the sentence.
The figure also shows the pitch accents selected by
the output planner according to Steedman?s (2000)
theory of information structure and intonation.
For the recording, the sentences in the script were
presented one at a time to the speaker; the presen-
1No sentence in the script had more than two clauses.
26
tation included both the linguistic content (with ac-
cented words highlighted) as well as the intended
pragmatic context. Each sentence was displayed in
a large font on a laptop computer directly in front
of the speaker, with the camera positioned directly
above the laptop to ensure that the speaker was look-
ing towards the camera at all times. The speaker was
instructed to read each sentence out loud as expres-
sively as possible into the camera.
3 Annotation
Once all of the sentences in the script had been
recorded as described in the preceding section, the
next step was to annotate the facial displays that oc-
curred. We first used Anvil (Kipp, 2004) to split
the video into individual clips corresponding to each
sentence. This section describes how the facial dis-
plays in each of the clips were then annotated.
3.1 Annotation scheme
We annotated the speaker?s facial displays by linking
each to the span of nodes in the OpenCCG derivation
tree with which it was temporally related. Making
cross-modal links at this level made it possible to
use the annotated information directly in the output-
generation process for the experiments described in
Section 6.
A display was associated with the full span of
words that it coincided with temporally, as follows.
If a single node in the derivation tree covered ex-
actly all of the relevant words, then the annotation
was placed on that node; if the words spanned by a
display did not coincide with a single node, it was
attached to the set of nodes that did span the neces-
sary words. For example, in the derivation shown in
Figure 1, the sequence the family style is associated
with a single node, so a motion temporally associ-
ated with that sequence would be attached to that
node. On the other hand, if there were a motion as-
sociated with the tiles are, it would be attached to
both the the tiles node and the are node.
The following were the features that were consid-
ered; for each feature, we note the corresponding
Action Unit (AU) from the well-known Facial Ac-
tion Coding System (Ekman et al, 2002).
? Eyebrows: up (AU 1+2) or down (AU 4)
? Eye squinting (AU 43)
Figure 2: Annotation tool
? Head nodding: up (AU 53) or down (AU 54)
? Head leaning: left (AU 55) or right (AU 56)
? Head turning: left (AU 57) or right (AU 58)
This set of displays was chosen based on a combi-
nation of three factors: the emphatic facial displays
documented in the literature, the capabilities of the
target talking head, and the actual displays of the
speaker during the recording session.
3.2 Annotation tool
The tool for the annotation was a custom-written
program that allowed the coder to play back a
recorded sentence at full speed or slowed down, and
to associate any combination of displays with any
node or set of nodes in the OpenCCG derivation tree
of the sentence. The tool also allowed the coder to
play back a proposed annotation sequence on a syn-
thetic talking head to verify that it was as close as
possible to the actual motions. Figure 2 shows a
screenshot of the annotation tool in use on the sen-
tence from Figure 1. In the screenshot, a left turn is
attached to the entire sentence (i.e., the root node),
while a series of nods is associated with single leaf
nodes in the first half of the sentence. The annotator
has already attached a brow raise to the word are in
the second half and is in the process of adding a nod
to the same word.
The output of the annotation tool is an XML doc-
ument including the original contextually-annotated
27
<node surf=" although it ?s in the family style the tiles are by Alessi_Tiles" LEAN="left"
sc=" although [pro3n] be in the [style] [abstraction] the [phys -obj] be by [manufacturer ]">
<node surf=" although it ?s in the family style" um="b" first ="y"
sc=" although [pro3n] be in the [style] [abstraction ]">
<node surf=" although" um="b" first ="y" NOD="down" />
<node surf="it ?s in the family style" um="b" first ="y"
sc="[ pro3n] be in the [style] [abstraction ]">
<node surf="it" stem=" pro3n" um="b" first ="y" NOD="down" />
<node surf="?s in the family style" um="b" first ="y" sc="be in the [style] [abstraction ]">
<node surf="?s" stem="be" um="b" first ="y" NOD="down" />
<node surf="in the family style" um="b" first ="y" sc="in the [style] [abstraction ]">
<node surf="in" um="b" first ="y" NOD="down" />
<node surf="the family style" um="b" first ="y" sc="the [style] [abstraction ]">
<node surf="the" um="b" first ="y" />
<node surf=" family style" um="b" first ="y" sc="[ style] [abstraction ]">
<node surf=" family" sc="[ style]" accent ="L+H*" um="b" first ="y" NOD="down" />
<node surf="style" sc="[ abstraction ]" um="b" first ="y" />
</node >
</node >
</node >
</node >
</node >
</node >
<node surf="the tiles are by Alessi_Tiles" um="g" first ="n"
sc="the [phys -obj] be by [manufacturer ]">
<node surf="the tiles" um="g" first ="n" sc="the [phys -obj]">
<node surf="the" um="g" first ="n" />
<node surf="tiles" sc="[phys -obj]" stem="tile" um="g" first ="n" />
</node >
<node surf="are by Alessi_Tiles" um="g" first ="n" sc="be by [manufacturer ]">
<node surf="are" stem="be" accent ="H*" um="g" first ="n" BROW="up" NOD="down" />
<node surf="by Alessi_Tiles" um="g" first ="n" sc="by [manufacturer ]">
<node surf="by" um="g" first ="n" />
<node surf=" Alessi_Tiles" sc="[ manufacturer ]" accent ="H*" um="g" first ="n" />
</node >
</node >
</node >
</node >
Figure 3: Annotated sentence from the corpus
OpenCCG derivation tree of each sentence, with
each node additionally labelled with a (possibly
empty) set of facial displays. Figure 3 shows the
fully-annotated version of the sentence from Fig-
ure 1. This document includes the contextual fea-
tures from the original tree, indicated by italics: ev-
ery node in the first subtree has um="b" and first="y",
while every node in the second subtree has um="g"
and first="n", while the accented items also have an
accent feature. Every node also specifies the string
generated by the subtree that it spans, both in its sur-
face form (surf) and with semantic-class and stem
replacement (sc). This tree also includes the facial
displays added by the coder in Figure 2, indicated
by underlining: (LEAN="left") attached to the root
node), a number of downward nods (NOD="down") on
individual words in the first half of the sentence, and
a nod accompanied by a brow raise (BROW="up") on
are near the end.
4 Reliability of the annotation
Several measures were taken to ensure that the an-
notation process was reliable. As the first step, two
independent coders each separately processed the
same set of 20 sentences, using an initial annotation
scheme. The outputs of these two coders were com-
pared, and the coders discussed the differences and
agreed on a revised scheme. One of these coders
then used the final scheme to process the entire set
of 444 sentences. As a further test of reliability, an
28
additional coder was instructed on the use of the an-
notation tool and scheme and used them to process
286 sentences (approximately 65% of the corpus).
To assess the degree of agreement between these
two coders, we used a version of the ? agreement
coefficient proposed by Artstein and Poesio (2005).
? is designed as a coefficient that is weighted, that
applies to multiple coders, and that uses a separate
probability distribution for each coder. Weighted
coefficients like ? permit degrees of agreement to
be measured, so that partial agreement is penalised
less severely than total disagreement. Like other
weighted coefficients, ? is based on the ratio be-
tween the observed and expected disagreement on
the corpus.
To use this coefficient, it is necessary to define
a measure that computes the distance between two
proposed annotations. In this case, to compute the
observed disagreement Do(S) on a sentence S, we
use a measure similar to that proposed by Passon-
neau (2004) for measuring agreement on set-valued
annotations. For each display proposed by each
coder on the sentence, we search for a correspond-
ing display proposed by the other coder?one with
the same value (e.g., a brow raise) and covering a
similar span of nodes. If both proposed exactly the
same display, that indicates no disagreement (0); if
one display covers a strict subset of the nodes cov-
ered by the other, that indicates minor disagreement
(13 ); if the nodes covered by the two proposals over-
lap, that is a more major disagreement (23 ); and if no
corresponding display can be found from the second
coder, then that indicates the maximum level of dis-
agreement (1). The total observed disagreement on
a sentence is the sum of the disagreement level for
each display proposed by each coder.
The expected disagreement De(S) for a sentence
S depends on the length of that sentence, as fol-
lows. We first use the corpus counts to compute
the probability of each coder assigning each pos-
sible facial display to word spans of all possible
lengths. We then use these probabilities to estimate
the likelihood of the two coders assigning identical,
super/subset, overlapping, or disjoint annotations to
the sentence, for each possible display. The total
expected disagreement for the sentence is the sum
of these probabilities across all displays, using the
same weights as the observed disagreement above.
The overall observed disagreement in the corpus
Do is the arithmetic mean of the disagreement on
each sentence; similarly, the overall expected dis-
agreement De is the mean of the expected disagree-
ment across all of the sentences. To compute the
value of ? for the output of the two coders, we sub-
tract the ratio of these two values from 1:
? = 1?
Do
De
As Artstein and Poesio (2005) point out, for
weighted measures such as ?, there is no signif-
icance test for agreement, and the actual value is
strongly affected by the distance metric that is se-
lected. However, ? values can be compared with
one another to assess degrees of agreement. The
overall ? value between the two coders on the full
set of 286 sentences processed by both was 0.561,
with ? values on individual facial displays ranging
from a high of 0.661 on nodding to a low of 0.285
on squinting (a very rare motion). To put these val-
ues into context, we computed ? on the set of 20
sentences processed by the final coder as part of the
training process (which are not included in the set
of 286). The overall ? value for these sentences is
0.231, with negative values for some of the individ-
ual displays. This demonstrates that the training pro-
cess had a positive effect on agreement.
5 Patterns in the corpus
We investigated the contextual features to see which
had the most significant effect on the facial displays
occurring on a node. To determine this, we used
multinomial logit regression to select the factors and
factor interactions that had the most significant ef-
fects on the distribution of each display; this form of
regression is appropriate when, as in this case, the
response variable is categorical. In this section, we
list the most significant factors and give a qualitative
description of the impact of each.
The single most influential contextual factor was
the user-model evaluation, which had an effect on all
of the facial displays. In positive user-model con-
texts, eyebrow raising and turning to the right were
relatively more frequent (Figure 4(a)); in negative
contexts, on the other hand, the rates of eyebrow
lowering, squinting, and leaning to the left were all
higher (Figure 4(b)). Other factors also affected the
29
(a) Positive (b) Negative
Figure 4: Characteristic facial displays for different user-model evaluations
distribution of facial displays. In the first half of
two-clause sentences, brow lowering was also more
frequent, as was upward nodding, while downward
nodding and right turns showed up more often in the
second clause of two-clause sentences. Nodding and
brow raising were both more frequent on nodes with
any sort of predicted pitch accent.
Several of these factors agree with previous find-
ings on conversational body language. The in-
creased frequency of nodding and brow raising on
accented words agrees with many previous stud-
ies: Ekman (1979), Cav? et al (1996), Graf et al
(2002), Keating et al (2003), Krahmer and Swerts
(2004), and Flecha-Garc?a (2006) all noted similar
displays on prosodically accented parts of the sen-
tence. The speaker?s tendency to move right on pos-
itive descriptions and left on negative descriptions
is also consistent with other findings. According
to the work of Davidson and colleagues (Davidson
and Irwin, 1999), emotion and affect processing are
asymmetrically organised in the human brain. The
right hemisphere is associated with negative affect
(and withdrawal behaviours), and the left with posi-
tive affect (and approach behaviours). Because both
perceptual and motor systems are contra-laterally or-
ganised, this means that higher levels of right hemi-
sphere activity are associated with attention being
oriented towards the left, while higher levels of left
hemisphere activity are associated with attention be-
ing oriented to the right; this fits with our speaker?s
pattern of movements.
The annotation scheme described here allowed a
display to be associated with any contiguous span of
words in the sentence. Annotators were encouraged
to use syntactic constituents wherever possible, but
also had the option to select multiple nodes where a
display did not correspond with a single constituent
in the derivation tree. Earlier versions of the annota-
tion scheme did not support this degree of flexibility,
so we used the patterns in the corpus to test whether
the modifications to the scheme were useful.
In a previous study using the same video record-
ings but a different, simpler scheme (Foster and
Oberlander, 2006), facial displays could only be as-
sociated with single leaf nodes (i.e., words); that is,
in the terminology of Ekman (1979), all motions
were considered to be batons rather than underlin-
ers. Based on the data in the current corpus, that
restriction was clearly unrealistic: the mean number
of nodes spanned by a display in the full corpus was
1.95, with a maximum of 15 and a standard devia-
tion of 2. The results were similar in the sub-corpus
produced by the final coder, in which the mean num-
ber of nodes spanned by a display was 2.25.
The annotation rules for this study did not ini-
tially permit displays to be associated with more
than nodes in the derivation tree. This capability
was added following inter-coder discussions after
the initial test annotation to deal with cases where
the speaker?s displays did not correspond to syntac-
tic constituents?for example, if the speaker raised
his eyebrows on the tiles are or some other such
non-standard constituent. The data in the annotated
corpus supports this modification. Approximately
6% of the annotations in the main corpus?165 of
2826?were attached to more than one node in the
derivation tree; for the final coder, 4.5% of annota-
tions were on multiple nodes.
30
6 Generation experiments
The primary reason for creating this corpus of fa-
cial displays was to use the resulting data to select
facial displays for the artificial talking head in the
COMIC multimodal dialogue system. Several dif-
ferent strategies have been implemented to use the
corpus data for this task, and a number of automated
and human evaluations have been carried out com-
paring the different implementations.
As described in the preceding section, the fac-
tor with the largest influence on the displays of
the recorded speaker was the user-model evaluation.
Two studies (Foster, 2007b) were carried out to test
the generality of the characteristic positive and neg-
ative displays (Figure 4). In the first study, users
were asked to identify the intended user-model po-
larity of a description presented by the talking head
based only on the facial displays. The participants
were generally able to recognise the characteristic
positive and negative facial displays; they also iden-
tified the displays intended to be neutral (nodding
alone) as positive, and tended to judge videos with
no facial displays to be negative. In the second study,
users? subjective preferences were gathered between
videos in which the user-model evaluation expressed
in speech was either consistent or inconsistent with
the facial displays. In this study, the participants
generally preferred the videos that showed consis-
tent content on the two output channels.
In another study (Foster and Oberlander, 2007),
two different data-driven strategies were imple-
mented that used the corpus data to select facial dis-
plays to accompany speech. One strategy always se-
lected the highest-probability option in all contexts,
while the other made a stochastic choice among all
of the options weighted by the corpus probabili-
ties. These two strategies were compared against
each other using both automated and human eval-
uation methods: the majority strategy scored more
highly on the automated cross-validation, while the
weighted strategy was strongly preferred by human
judges. The judges also preferred resynthesised ver-
sions of the original facial displays from the corpus
to the output of either of the generation strategies.
Two further human evaluation studies compared
the weighted data-driven generation strategy from
the preceding study to a rule-based strategy that
selected the most characteristic displays based
only on the user-model evaluation (Foster, 2007a).
When users? subjective judgements were gathered
as above, they had a mild preference for the out-
put of the weighted strategy over that of the rule-
based strategy. In a second study, videos generated
by the weighted strategy significantly decreased par-
ticipants? ability to select descriptions that were cor-
rectly tailored to a given set of user preferences,
while videos generated by the rule-based strategy
had no such impact.
7 Conclusions
We have described the collection and annotation of
an application-specific corpus of conversational fa-
cial displays. The designs of both the corpus and
the annotation scheme were driven by the needs of
a specific generation system, which makes use of a
range of pragmatic information while creating out-
put. To use this information to make corpus-based
decisions, it is necessary that the full context of ev-
ery utterance and facial display in the corpus be
available. Rather than adding this information to an
existing corpus, we chose?like Stone et al (2004)
and van Deemter et al (2006), for example?to cre-
ate a corpus based on known contexts so that the
full information for every sentence was known be-
fore the fact.
The final annotation scheme required each facial
display to be linked to the set of nodes in the syntac-
tic derivation tree of the sentence that exactly cov-
ered the words temporally associated with the dis-
play. Two coders separately processed the sentences
in the corpus; on the sentences processed by both
coders (about 65% of the corpus), the agreement as
measured by ? was 0.561.
A number of contextual factors had an influ-
ence on the displays used by the recorded speaker.
The single most influential factor was the user-
model evaluation of the object being described.
The speaker?s characteristic side-to-side motions on
these sentences agree with findings on the relation-
ship between brain hemispheres and affect. In ad-
dition, in user studies, human judges were reliably
able to identify the intended affect based on resyn-
thesised versions of these characteristic displays.
Other patterns in the data also agree with exist-
31
ing findings on facial displays: for example, the
speaker tended to nod and raise his eyebrows more
frequently on words with prosodic accents.
Several experiments have been performed in
which the annotated data from this corpus was used
to select the facial displays to accompany the out-
put of an animated talking head. These studies have
found interesting results on both the relationship be-
tween automated and human judgements of output
quality and the relative utility of rule-based and data-
driven approaches for selecting conversational facial
displays.
Acknowledgements
This research was supported by the EU projects
COMIC (IST-2001-32311) and JAST (FP6-003747-
IP). Thanks to Amy Isard, Ron Petrick, and Tom
Segler for annotation assistance, and to Jon Ober-
lander and the LAW reviewers for useful comments.
References
R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta).
Technical Report CSM-437, University of Essex Department
of Computer Science.
A. Belz and S. Varges, editors. 2005. Corpus Linguistics 2005
Workshop on Using Corpora for Natural Language Genera-
tion. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.
J. Cassell, S. Kopp, P. Tepper, K. Ferriman, and K. Striegnitz.
2007. Trading spaces: How humans and humanoids use
speech and gesture to give directions. In T. Nishida, edi-
tor, Engineering Approaches to Conversational Informatics.
Wiley. In press.
C. Cav?, I. Gua?tella, R. Bertrand, S. Santi, F. Harlay, and R. Es-
pesser. 1996. About the relationship between eyebrowmove-
ments and F0 variations. In Proceedings of the 4th Interna-
tional Conference on Spoken Language Processing (ICSLP
1996).
R. J. Davidson and W. Irwin. 1999. The functional neu-
roanatomy of emotion and affective style. Trends in Cog-
nitive Sciences, 3(1):11?21. doi:10.1016/S1364-6613(98)
01265-0.
K. van Deemter, I. van der Sluis, and A. Gatt. 2006. Building
a semantically transparent corpus for the generation of refer-
ring expressions. In Proceedings of the Fourth International
Natural Language Generation Conference, pages 130?132.
Sydney, Australia. ACL Anthology W06-1420.
P. Ekman. 1979. About brows: Emotional and conversational
signals. In M. von Cranach, K. Foppa, W. Lepenies, and
D. Ploog, editors, Human Ethology: Claims and limits of a
new discipline. Cambridge University Press.
P. Ekman, W. V. Friesen, and J. C. Hager. 2002. Facial Action
Coding System. A Human Face, Salt Lake City.
M. L. Flecha-Garc?a. 2006. Eyebrow raising in dialogue:
Discourse structure, utterance function, and pitch accents.
Ph.D. thesis, Department of Theoretical and Applied Lin-
guistics, University of Edinburgh.
M. E. Foster. 2007a. Comparing rule-based and data-driven se-
lection of facial displays. In Proceedings of the ACL 2007
Workshop on Embodied Language Processing.
M. E. Foster. 2007b. Generating embodied descriptions tailored
to user preferences. In submission.
M. E. Foster and J. Oberlander. 2006. Data-driven generation
of emphatic facial displays. In Proceedings of the 11th Con-
ference of the European Chapter of the Association for Com-
putational Linguistics (EACL 2006), pages 353?360. Trento,
Italy. ACL Anthology E06-1045.
M. E. Foster and J. Oberlander. 2007. Corpus-based generation
of conversational facial displays. In submission.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Mul-
timodal generation in the COMIC dialogue system. In Pro-
ceedings of the ACL 2005 Demo Session. ACL Anthology
W06-1403.
H. Graf, E. Cosatto, V. Strom, and F. Huang. 2002. Visual
prosody: Facial movements accompanying speech. In Pro-
ceedings of the 5th IEEE International Conference on Auto-
matic Face and Gesture Recognition (FG 2002), pages 397?
401. doi:10.1109/AFGR.2002.1004186.
P. Keating, M. Baroni, S. Mattys, R. Scarborough, and A. Al-
wan. 2003. Optical phonetics and visual perception of lexi-
cal and phrasal stress in English. In Proceedings of the 15th
International Congress of Phonetic Sciences (ICPhS), pages
2071?2074.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
E. Krahmer and M. Swerts. 2004. More about brows: A cross-
linguistic study via analysis-by-synthesis. In C. Pelachaud
and Z. Ruttkay, editors, From Brows to Trust: Evaluating
Embodied Conversational Agents, pages 191?216. Kluwer.
doi:10.1007/1-4020-2730-3_7.
I. Langkilde and K. Knight. 1998. The practical value of n-
grams in generation. In Proceedings of the 9th International
Natural Language Generation Workshop (INLG 1998). ACL
Anthology W98-1426.
R. J. Passonneau. 2004. Computing reliability for coreference
annotation. In Proceedings, Fourth International Conference
on Language Resources and Evaluation (LREC 2004), vol-
ume 4, pages 1503?1506. Lisbon.
M. Steedman. 2000. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 31(4):649?689.
doi:10.1162/002438900554505.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,
and C. Bregler. 2004. Speaking with hands: Creating
animated conversational characters from recordings of hu-
man performance. ACM Transactions on Graphics (TOG),
23(3):506?513. doi:10.1145/1015706.1015753.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research on
Language and Computation, 4(1):39?75. doi:10.1007/
s11168-006-9010-2.
S. Williams and E. Reiter. 2005. Deriving content selection
rules from a corpus of non-naturally occurring documents
for a novel NLG application. In Belz and Varges (2005).
32
Proceedings of the Workshop on Embodied Language Processing, pages 1?8,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Comparing Rule-based and Data-driven Selection of Facial Displays
Mary Ellen Foster
Informatik VI: Robotics and Embedded Systems
Technische Universit?t M?nchen
Boltzmannstra?e 3, 85748 Garching, Germany
foster@in.tum.de
Abstract
The non-verbal behaviour of an embodied
conversational agent is normally based on
recorded human behaviour. There are two
main ways that the mapping from human be-
haviour to agent behaviour has been imple-
mented. In some systems, human behaviour
is analysed, and then rules for the agent are
created based on the results of that analysis;
in others, the recorded behaviour is used di-
rectly as a resource for decision-making, us-
ing data-driven techniques. In this paper, we
implement both of these methods for select-
ing the conversational facial displays of an
animated talking head and compare them in
two user evaluations. In the first study, par-
ticipants were asked for subjective prefer-
ences: they tended to prefer the output of the
data-driven strategy, but this trend was not
statistically significant. In the second study,
the data-driven facial displays affected the
ability of users to perceive user-model tai-
loring in synthesised speech, while the rule-
based displays did not have any effect.
1 Introduction
There is no longer any question that the production
of language and its accompanying non-verbal be-
haviour are tightly linked (e.g., Bavelas and Chovil,
2000). The communicative functions of body lan-
guage listed by Bickmore and Cassell (2005) include
conversation initiation and termination, turn-taking
and interruption, content elaboration and emphasis,
and feedback and error correction; non-verbal be-
haviours that can achieve these functions include
gaze modification, facial expressions, hand gestures,
and posture shifts, among others.
When choosing non-verbal behaviours to accom-
pany the speech of an embodied conversational
agent (ECA), it is necessary to translate general find-
ings from observing human behaviour into concrete
selection strategies. There are two main implemen-
tation techniques that have been used for making this
decision. In some systems, recorded behaviours are
analysed and rules are created by hand based on the
analysis; in others, recorded human data is used di-
rectly in the decision process. The former technique
is similar to the classic role of corpora in natural-
language generation described by Reiter and Dale
(2000), while the latter is more similar to the more
recent data-driven techniques that have been adopted
(Belz and Varges, 2005).
Researchers that have used rule-based techniques
to create embodied-agent systems include: Poggi
and Pelachaud (2000), who concentrated on generat-
ing appropriate affective facial displays based on de-
scriptions of typical facial expressions of emotion;
Cassell et al (2001a), who selected gestures and
facial expressions to accompany text using heuris-
tics derived from studies of typical North Ameri-
can non-verbal-displays; and Marsi and van Rooden
(2007), who generated typical certain and uncertain
facial displays for a talking head in an information-
retrieval system. Researchers that used data-driven
techniques include: Stone et al (2004), who cap-
tured the motions of an actor performing scripted
output and then used that data to create performance
1
specifications on the fly; Cassell et al (2001b), who
selected posture shifts for an embodied agent based
on recorded human behaviour; and Kipp (2004),
who annotated the gesturing behaviour of skilled
public speakers and derived ?gesture profiles? to use
in the generation process.
Using rules derived from the data can produce dis-
plays that are easily identifiable and is straightfor-
ward to implement. On the other hand, making di-
rect use of the data can produce output that is more
similar to actual human behaviour by incorporating
naturalistic variation, although it generally requires
a more complex selection algorithm. In this paper,
we investigate the relative utility of the two imple-
mentation strategies for a particular decision: select-
ing the conversational facial displays of an animated
talking head. We use two methods for comparison:
gathering users? subjective preferences, and measur-
ing the impact of both selection strategies on users?
ability to perceive user tailoring in speech.
In Section 2, we first describe how we recorded
and annotated a corpus of facial displays in the do-
main of the target generation system. Section 3 then
presents the two strategies that were implemented
to select facial displays based on this corpus: one
using a simple rule derived from the most character-
istic behaviours in the corpus, and one that made a
weighted choice among all of the options found in
the corpus for each context. The next sections de-
scribe two user studies comparing these strategies:
in Section 4, we compare users? subjective prefer-
ences, while in Section 5 we measure the impact of
each strategy on user?s ability to select spoken de-
scriptions correctly tailored to a given set of user
preferences. Finally, in Section 6, we discuss the
results of these two studies, draw some conclusions,
and outline potential future work.
2 Corpus collection and annotation1
The recording scripts for the corpus were created
by the output planner of the COMIC multimodal
dialogue system (Foster et al, 2005) and consisted
of a total of 444 sentences describing and compar-
ing various tile-design options. The surface form of
each sentence was created by the OpenCCG surface
realiser (White, 2006), using a grammar that spec-
1Foster (2007) gives more details of the face-display corpus.
ified both the words and the intended prosody for
the speech synthesiser. We attached all of the rele-
vant contextual, syntactic, and prosodic information
to each node in the OpenCCG derivation tree, in-
cluding the user-model evaluation of the object be-
ing described (positive, negative, or neutral), the pre-
dicted pitch accent, the clause of the sentence (first,
second, or only), and whether the information being
presented was new to the discourse.
The sentences in the script were presented one
at a time to a speaker who was instructed to read
each out loud as expressively as possible into a cam-
era directed at his face. The following facial dis-
plays were then annotated on the recordings: eye-
brow motions (up or down), eye squinting, and rigid
head motion on all three axes (nodding, leaning, and
turning). Each of these displays was attached to
the node or nodes in the OpenCCG derivation tree
that exactly covered the span of words temporally
associated with the display. Two coders separately
processed the sentences in the corpus. Using a ver-
sion of the ? weighted agreement measure proposed
by Artstein and Poesio (2005)?which allows for
a range of agreement levels?the agreement on the
sentences processed by both coders was 0.561.
When the distribution of facial displays in the
corpus was analysed, it was found that the single
biggest influence on the speaker?s behaviour was
the user-model evaluation of the features being de-
scribed. When he described features of the design
that had positive user-model evaluations, he was
more likely to turn to the right and to raise his eye-
brows (Figure 1(a)); on the other hand, on features
with negative user-model evaluations, he was more
likely to lean to the left, lower his eyebrows, and
squint his eyes (Figure 1(b)). The overall most fre-
quent display in all contexts was a downward nod on
its own. Other factors that had a significant effect on
the facial displays included the predicted pitch ac-
cent, the clause of the sentence (first or second), and
the number of words spanned by a node.
3 Selection strategies
Based on the recorded behaviour of the speaker,
we implemented two different methods for selecting
facial displays to accompany synthesised speech.
Both methods begin with the OpenCCG derivation
2
(a) Positive (b) Negative
Figure 1: Characteristic facial displays from the corpus
Although it?s in the family style, the tiles are by Alessi.
Original nd=d nd=d nd=d nd=d nd=d,bw=u
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ln=l . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Data-driven nd=d nd=d . . tn=r . .
Rule-based ln=l,bw=d,sq tn=r,bw=u
Figure 2: Face-display schedules for a sample sentence
tree for a sentence?that is, a tree in the same for-
mat as those that were used for the corpus annota-
tion, including all of the contextual features. They
then proceed top-down through the derivation tree,
considering each node in turn and determining the
display combination (if any) to accompany it.
The rule-based strategy specifies motions only on
nodes corresponding to mentions of specific proper-
ties of a tile design: manufacturer and series names,
colours, and decorations. The display combination
is determined by the user-model evaluation of the
property being described, based on the behaviours
of the recorded speaker. For a positive evaluation,
this strategy selects a right turn and brow raise; for
a negative evaluation, it selects a left turn, brow
lower, and eye squint; while for neutral evaluations,
it chooses a downward nod.
In contrast, the data-driven strategy considers all
nodes in the derivation tree. For each node, it selects
from all of the display combinations that occurred
on similar nodes in the corpus, weighted by the fre-
quency. As a concrete example, in a hypothetical
context where the speaker made no motion 80% of
the time, nodded 15% of the time, and turned to the
right in the other 5%, this strategy would select no
motion with probability 0.8, a nod with probability
0.15, and a right turn with probability 0.05.
Figure 2 shows a sample sentence from the cor-
pus, the original facial displays, and the displays se-
lected by each of the strategies. In the figure, nd=d
indicates a downward nod, bw=u and bw=d a brow
raise and lower, respectively, sq an eye squint, ln=l
a left lean, and tn=r a right turn.
4 Subjective preferences
As a first comparison of the two implementation
strategies, we gathered users? subjective preferences
between three different types of face-display sched-
ules: the displays selected by each of the generation
strategies described in the preceding section, as well
as the original displays annotated in the corpus.
4.1 Participants
This experiment was run through the Language Ex-
periments Portal,2 a website dedicated to online psy-
cholinguistic experiments. There were a total of 36
participants: 20 females and 16 males. 23 of the par-
ticipants were between 20 and 29 years old, 9 were
over 30, and 4 were under 20. 21 described them-
selves as expert computer users, 14 as intermediate
users, and one as a beginner. 18 were native speak-
ers of English, while the others had a range of other
native languages.
2http://www.language-experiments.org/
3
Figure 3: RUTH talking head
4.2 Methodology
Each participant saw videos of two possible synthe-
sised face-display schedules accompanying a series
of 18 sentences. Both videos had the same syn-
thesised speech, but each had a different different
facial-display schedule. For each pair, the partici-
pant was asked to select which of the two versions
they preferred. There were three different schedule
types: the original displays annotated in the corpus,
along with the output of both of the selection strate-
gies. Participants made each pairwise comparison
between these types six times, three times in each or-
der. All participants saw the same set of sentences,
in a random order: the pairwise choices were also
allocated to sentences randomly.
4.3 Materials
To create the materials for this experiment, we ran-
domly selected 18 sentences from the corpus and
generated facial displays for each, using both of the
strategies. The data-driven schedules were gener-
ated through 10-fold cross-validation as part of a
previous study (Foster and Oberlander, 2007): that
is, the display counts from 90% of the corpus were
used to select the displays to use for the sentences
in the held-out 10%. The rule-based schedules were
generated by running the rule-based procedure from
Section 3 on the same OpenCCG derivation trees.
Videos were then created of all of the schedules for
all of the sentences, using the Festival speech syn-
thesiser (Clark et al, 2004) and the RUTH animated
talking head (DeCarlo et al, 2004) (Figure 3).
Original ? Rule-based Weighted ? Rule-based Original ? Weighted010
203040
506070
8090100
110120130 123 120 12393 97 92
Comparison
Choice co
unt
Figure 4: Subjective-preference results
4.4 Results
The overall results of this study are shown in Fig-
ure 4. Not all participants responded to all items,
so there were a total of 648 responses: 216 compar-
ing the original corpus schedules to the rule-based
schedules, 217 for the data-driven vs. rule-based
comparison, and 215 for the original vs. data-driven
comparison. To assess the significance of the pref-
erences, we use a binomial test, which provides an
exact measure of the statistical significance of de-
viations from a theoretically expected classification
into two categories. This test indicates that there
was a mildly significant preference for the original
schedules over the output of each of the strategies
(p< 0.05 in both cases). While there was also a ten-
dency to prefer the output of the data-driven strategy
over that of the rule-based strategy, this preference
was not significant (p? 0.14). No demographic fac-
tor had a significant effect on these results.
4.5 Discussion
Although there was no significant preference be-
tween the output of the two strategies, the generated
schedules were very different. The rule-based strat-
egy used only the three display combinations de-
scribed in Section 3 and selected an average of 1.78
displays per sentence on the 18 sentences used in
this study, while the data-driven strategy selected 12
different display combinations across the sentences
and chose an average of 5.06 displays per sentence.
For comparison, the original sentences from the cor-
pus used a total of 15 different combinations on the
4
(1) Here is a family design. Its tiles are from the Lollipop collection by Agrob Buchtal. Although the tiles have a blue colour
scheme, it does also feature green.
(2) Here is a family design. As you can see, the tiles have a blue and green colour scheme. It has floral motifs and artwork
on the decorative tiles.
Figure 5: Tile-design description tailored to two user models (conflicting concession highlighted)
same sentences and had an average of 4.83 displays
per sentence. In other words, in terms of the range
of displays, the schedules generated by the data-
driven strategy are fairly similar to those in the cor-
pus, while those from the rule-based strategy do not
resemble the corpus very much at all.
In another study (Foster and Oberlander, 2007),
the weighted data-driven strategy used here was
compared to a majority strategy that always chose
the highest-probability option in every context. In
other words, in the hypothetical context mentioned
earlier where the top option occurred 80% of the
time, the majority strategy would always choose that
option. This strategy scored highly on an automated
cross-validation study; however, human judges very
strongly preferred the output of the weighted strat-
egy described in this paper (p < 0.0001). This con-
trasts with the weak preference for the weighted
strategy over the rule-based strategy in the current
experiment. The main difference between the out-
put of the majority strategy on the one hand, and that
of the two strategies described here on the other, is
in the distribution of the face-display combinations:
over 90% of the that the majority strategy selected
a display, it used a downward nod on its own, while
both of the other strategies tended to generate a more
even distribution of displays across the sentences.
This suggests that the distribution of facial displays
is more important than strict corpus similarity for
determining subjective preferences.
The participants in this study generally preferred
the original corpus displays to the output of either
of the generation strategies. This suggests that a
more sophisticated data-driven implementation that
reproduces the corpus data more faithfully could
be successful. For example, the process of select-
ing facial displays could be integrated directly into
the OpenCCG realiser?s n-gram-guided search for a
good realisation (White, 2006), rather than being run
on the output of the realiser as was done here.
5 Perception of user tailoring in speech
The results of the preceding experiment indicate that
participants mildly preferred the output of the data-
driven strategy to that of the rule-based strategy;
however, this preference was not statistically signif-
icant. In this second experiment, we compare the
face-display schedules generated by both strategies
in a different way: measuring the impact of each
schedule type on users? ability to detect user-model
tailoring in synthesised speech.
Foster andWhite (2005) performed an experiment
in which participants were shown a series of pairs of
COMIC outputs (e.g., Figure 5) and asked to choose
which was correctly tailored to a given set of user
preferences. The participants in that study were able
to select the correctly-tailored output only on trials
where one option contained a concession to a neg-
ative preference that the other did not. For exam-
ple, the description in (1) contains the concession Al-
though the tiles have a blue colour scheme, as if the
user disliked the colour blue, while (2) has no such
concession. Figure 6 shows the results from that
study when outputs were presented as speech; the
results for text were nearly identical. The first pair
of bars represent the choices made on trials where
there was a conflicting concession, while the second
pair show the choices made on trials with no con-
flicting concession. Using a binomial test, the dif-
ference for the conflicting-concession trials is sig-
nificant at p < 0.0001, while there is no significant
difference for the other trials (p? 0.4).
In this experiment, use the same experimental ma-
terials, but we use the talking head to present the sys-
tem turns. This experiment allows us to answer two
questions: whether the addition of a talking head af-
fects users? ability to perceive tailoring in speech,
and whether there is a difference between the impact
of the two selection strategies.
5
Orig nal ? r Rl r ignal  u
eu- u
busu
duWu
hut u
0u 0u
- s
se - 0
12345 6  753
O7ral5R
lr8i 
Figure 6: Results for speech-only presentation
5.1 Participants
Like the previous study, this one was also run over
the web. There were 32 participants: 19 females and
13 males. 18 of the participants were between 20
and 29 years old, 10 were over 30, and 4 were un-
der 20. 15 described themselves as expert computer
users, 15 as intermediate users, and 2 as beginners.
30 of the participants were native English speakers.
5.2 Methodology
Participants in this experiment observed an eight-
turn dialogue between the system and a user with
specific likes and dislikes. The user preferences
were displayed on screen at all times; the user input
was presented as written text on the screen, while the
system outputs were played as RUTH videos in re-
sponse to the user clicking on a button. There were
two versions of each system turn, one tailored to the
preferences of the given user and one to the prefer-
ences of another user; the user task was to select the
correctly tailored version. The order of presentation
was counterbalanced so that the correctly tailored
version was the first option in four of the trials and
the second in the other four. Participants were as-
signed in rotation to one of four randomly-generated
user models. As an additional factor, half of the par-
ticipants saw videos with facial displays generated
by the data-driven strategy, while the other half saw
videos generated by the rule-based strategy.
5.3 Materials
The user models and dialogues were identical to
those used by Foster and White (2005). For each
sentence in each system turn, we annotated the
nodes of the OpenCCG derivation tree with all of
the necessary information for generation: the user-
model evaluation, the pitch accents, the clause of
the sentence, and the surface string. We then used
those annotated trees to create face-display sched-
ules using both of the selection strategies, using the
full corpus as context for the data-driven strategy,
and prepared RUTH videos of all of the generated
schedules as in the previous study.
5.4 Results
The results of this study are shown in Figure 7: Fig-
ure 7(a) shows the results for the participants using
the rule-based schedules, while Figure 7(b) shows
the results with the data-driven schedules. Just as
in the speech-only condition, the participants in this
experiment responded essentially at chance on tri-
als where there was no conflicting concession to
negative preferences. For the trials with a conflict-
ing concession, participants using rule-based videos
selected the targeted version significantly more of-
ten (p < 0.01), while the results for participants us-
ing the data-driven videos show no significant trend
(p ? 0.49). None of the demographic factors af-
fected these results.
To assess the significance of the difference be-
tween the two selection strategies, we compared
the results on the conflicting-concession trials from
each of the groups to the corresponding results from
the speech-only experiment, using a ?2 test. The
results for the judges using the rule-based videos
are very similar to those of the judges using only
speech (?2 = 0.21, p = 0.65). However, there is a
significant difference between the responses of the
speech-only judges and those of the judges using the
weighted schedules (?2 = 4.72, p < 0.05).
5.5 Discussion
The materials for this study were identical to those
used by Foster and White (2005); in fact, the wave-
forms for the synthesised speech were identical.
However, the participants in this study who saw
the videos generated by the data-driven strategy
6
Orig nal ? r Rl r ignal  ue
- u- ebu
besu
sedude
euee
Wu eh
besb bu
t 0123 4  531
O5ral3R
lr6i 
(a) Rule-based schedules
Orig nal ? r Rl r ignal  ue
- u- e
bube
suse
dude
euee ee
- W
dh
bs
t 0123 4  531
O5ral3R
lr6i 
(b) Data-driven schedules
Figure 7: Results of the perception study
were significantly worse at identifying the correctly-
tailored speech than were the participants in the pre-
vious study, while the performance of the partic-
ipants who saw rule-based videos was essentially
identical to that of the speech-only subjects.
The schedules selected by the data-driven strat-
egy for this evaluation include a variety of facial dis-
plays; sometimes these displays are actually the op-
posite of what would be selected by the rule-based
strategy. For example, the head moves to the right
when describing a negative fact in 23 of the 520
data-driven schedules, and moves to the left when
describing a neutral or positive fact in 20 cases. A
description includes up to three sentences, and a trial
involved comparing two descriptions, so a total of 75
of the trials (52%) for the data-driven participants
involved at least one of these these potentially mis-
leading head movements. Across all of the trials for
the participants using data-driven videos, there were
38 conflicting-concession trials with no such head
movement. The performance on these trials was es-
sentially the identical to that on the full set of tri-
als: the correctly targeted description was chosen 20
times, and the other version 18 times. So the worse
performance with the data-driven schedules cannot
be attributed solely to the selected facial displays
conflicting with the linguistic content.
Another possibility is that the study participants
who used the data-driven schedules were distracted
by the expressive motions of the talking head and
failed to pay attention to the content of the speech.
This appears to have been the case in the COMIC
whole-system evaluation (White et al, 2005), for
example, where the performance of the male par-
ticipants on a recall task was significantly worse
when a more expressive talking head was used. On
this study, there was no effect of gender (or any of
the other demographic factors) on the pattern of re-
sponses; however, it could be that a similar effect
occurred in this study for all of the participants.
6 Conclusions and future work
The experiments in this paper have compared the
two main current implementation techniques for
choosing non-verbal behaviour for an embodied
conversational agent: using rules derived from the
study of human behaviour, and using recorded hu-
man behaviour directly in the generation process.
The results of the subjective-preference evaluation
indicate that participants tended to prefer the out-
put generated by the data-driven strategy, although
this preference was not significant. In the second
study, videos generated by the data-driven strat-
egy significantly decreased participants? ability to
detect correctly-tailored spoken output when com-
pared to a speech-only presentation; on the other
hand, videos generated by the rule-based strategy
did not have a significant impact on this task.
These results indicate that, at least for this cor-
pus and this generation task, the choice of gener-
ation strategy depends largely on which aspect of
the system is more important: to create an agent
7
that users like subjectively, or to ensure that users
fully understand all aspects of the output presented
in speech. If the former is more important, than an
implementation that uses the data directly appears
to be a slightly better option; if the latter is more im-
portant, then the rule-based strategy seems superior.
On the subjective-preference evaluation, users
preferred the original corpus motions over either of
the generated versions. As discussed in Section 4.5,
this suggests that there is room for a more sophisti-
cated data-driven selection strategy that reproduces
the corpus data more closely. The output of such a
generation strategy might also have a different effect
on the perception task.
Both of these studies used the RUTH talking head
(Figure 3), which has no body and, while human in
appearance, is not particularly realistic. We used this
head to investigate the the generation of a limited set
of facial displays, based on contextual information
including the user-model evaluation, the predicted
prosody, the clause of the sentence, and the surface
string. More information about the relative utility
of different techniques for selecting non-verbal be-
haviour for embodied agents can be gathered by ex-
perimenting with a wider range of agents and of
non-verbal behaviours. Other possible agent types
include photorealistic animated agents, agents with
fully articulated virtual bodies, and physically em-
bodied robot agents. The possibilities for non-verbal
behaviours include deictic, iconic, and beat gestures,
body posture, gaze behaviour, and facial expressions
of various types of affect, while any source of syn-
tactic or pragmatic context could be used to help
make the selection. Experimenting with other com-
binations of agent properties and behaviours can im-
prove our knowledge of the relative utility of differ-
ent mechanisms for selecting non-verbal behaviour.
References
R. Artstein and M. Poesio. 2005. Kappa3 = alpha (or beta).
Technical Report CSM-437, University of Essex Department
of Computer Science.
J. B. Bavelas and N. Chovil. 2000. Visible acts of mean-
ing: An integrated message model of language in face-to-
face dialogue. Journal of Language and Social Psychology,
19(2):163?194. doi:10.1177/0261927X00019002001.
A. Belz and S. Varges, editors. 2005. Corpus Linguistics 2005
Workshop on Using Corpora for Natural Language Genera-
tion. http://www.itri.brighton.ac.uk/ucnlg/ucnlg05/.
T. Bickmore and J. Cassell. 2005. Social dialogue with em-
bodied conversational agents. In J. van Kuppevelt, L. Dy-
bkj?r, and N. Bernsen, editors, Advances in Natural, Multi-
modal Dialogue Systems. Kluwer, New York. doi:10.1007/
1-4020-3933-6_2.
J. Cassell, T. Bickmore, H. Vilhj?lmsson, and H. Yan. 2001a.
More than just a pretty face: Conversational protocols and
the affordances of embodiment. Knowledge-Based Systems,
14(1?2):55?64. doi:10.1016/S0950-7051(00)00102-7.
J. Cassell, Y. Nakano, T. W. Bickmore, C. L. Sidner, and
C. Rich. 2001b. Non-verbal cues for discourse structure. In
Proceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL 2001). ACL Anthology
P01-1016.
R. A. J. Clark, K. Richmond, and S. King. 2004. Festival 2 ?
build your own general purpose unit selection speech synthe-
siser. In Proceedings of the 5th ISCA Workshop on Speech
Synthesis.
D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004. Spec-
ifying and animating facial signals for discourse in embod-
ied conversational agents. Computer Animation and Virtual
Worlds, 15(1):27?38. doi:10.1002/cav.5.
M. E. Foster. 2007. Associating facial displays with syntactic
constituents for generation. In Proceedings of the ACL 2007
Workshop on Linguistic Annotation (The LAW).
M. E. Foster and J. Oberlander. 2007. Corpus-based generation
of conversational facial displays. In submission.
M. E. Foster andM.White. 2005. Assessing the impact of adap-
tive generation in the COMIC multimodal dialogue system.
In Proceedings of the IJCAI 2005 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005. Mul-
timodal generation in the COMIC dialogue system. In Pro-
ceedings of the ACL 2005 Demo Session. ACL Anthology
W06-1403.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
E. Marsi and F. van Rooden. 2007. Expressing uncertainty with
a talking head. In Proceedings of the Workshop on Multi-
modal Output Generation (MOG 2007).
I. Poggi and C. Pelachaud. 2000. Performative facial expres-
sions in animated faces. In J. Cassell, J. Sullivan, S. Prevost,
and E. Churchill, editors, Embodied Conversational Agents,
pages 154?188. MIT Press.
E. Reiter and R. Dale. 2000. Building Natural Language Gen-
eration Systems. Cambridge University Press. doi:10.2277/
052102451X.
M. Stone, D. DeCarlo, I. Oh, C. Rodriguez, A. Lees, A. Stere,
and C. Bregler. 2004. Speaking with hands: Creating
animated conversational characters from recordings of hu-
man performance. ACM Transactions on Graphics (TOG),
23(3):506?513. doi:10.1145/1015706.1015753.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research on
Language and Computation, 4(1):39?75. doi:10.1007/
s11168-006-9010-2.
M. White, M. E. Foster, J. Oberlander, and A. Brown. 2005.
Using facial feedback to enhance turn-taking in a multimodal
dialogue system. In Proceedings of HCI International 2005.
8
Book Review
Spoken Dialogue Systems
Kristiina Jokinen and Michael McTear
(University of Helsinki, University of Ulster)
Princeton, NJ: Morgan & Claypool (Synthesis Lectures on Language
Technologies, edited by Graeme Hirst, volume 5), 2009, xiv+151pp;
paperback, ISBN 978-1-59829-599-3, $40.00; ebook, ISBN 978-1-59829-600-6,
doi 10.2200/S00204ED1V01Y200910HLT005, $30.00 or by subscription
Reviewed by
Mary Ellen Foster
Heriot-Watt University
This book gives a short but comprehensive overview of the field of spoken dialogue
systems, outlining the issues involved in building and evaluating this type of system
and making liberal use of techniques and examples from a wide range of implemented
systems. It provides an excellent review of the research, with particularly relevant dis-
cussions of error handling and system evaluation, and is suitable both as an intro-
duction to this research area and as a survey of current state-of-the-art techniques.
The book is structured into seven chapters. Chapter 1 provides an introduction to
the research area and briefly introduces the topics covered in the book. The end of the
chapter consists of a list of links to tools and components that can be used for dialogue
system development, which?although currently useful?seems likely to go out of date
quickly.
Chapter 2 addresses the task of dialogue management, beginning by describing
simple graph- and frame-based methods for dialogue control, and continuing with a
discussion of VoiceXML. The chapter ends with an extended discussion of recent work
in statistical approaches to dialogue control and modeling. It is unfortunate that the
discussion of other methods such as the information state approach and plan-based
models is postponed to Chapter 4, but otherwise this chapter provides a good summary
of both classic and recent approaches.
Chapter 3 discusses error handling, which is divided into three processes: error de-
tection, error prediction (i.e., the online prediction of errors based on dialogue features),
and error recovery. After surveying a range of previous approaches to these three sub-
tasks, the authors go on to discuss several more recent, data-driven approaches. Error
handling is both a vital component of any spoken dialogue system designed for real-
world use and an active area of current research, so this compact summary of techniques
and issues is welcome.
Chapter 4 contains case studies illustrating a range of dialogue control strategies
and models. It begins with a description of the information state approach, and then
moves on to discuss plan-based approaches as exemplified in the TRAINS and TRIPS
projects. This is followed by a discussion of two systems that employ software agents for
dialogue management: the Queen?s Communicator and the AthosMail system. Finally,
two systems which make use of statistical models are presented: the Microsoft Bayesian
receptionist, which models conversation as decision making under uncertainty, and
the DIHANA system, which employs corpus-based dialogue management. The case
studies in this chapter provide detailed examples of a range of techniques, along with
Computational Linguistics Volume 36, Number 4
some discussion of the advantages and disadvantages of each approach, adding to the
relevance of the book for developers of future dialogue systems.
Chapter 5 discusses four aspects of spoken dialogue systems that go beyond the
straightforward information exchange scenarios considered in the preceding chapters.
The first section covers aspects of collaborative planning along with Jokinen?s (2009)
Constructive Dialogue Modeling approach. The section on adaptation and user model-
ing provides a brief but useful survey of approaches to this task. The discussion of mul-
timodality is longer and more concrete, but concentrates almost entirely on multimodal
input processing; it would have been helpful to include a similar summary of the issues
involved in multimodal output generation. The final section on ?natural interaction?
briefly discusses two topics: non-verbal communication for embodied conversational
agents and multimodal corpus annotation.
Chapter 6?the longest in the book?gives a thorough treatment of the issues in-
volved in evaluating spoken dialogue systems, beginning with an historical overview.
It continues with a discussion of terminology and techniques and describes a wide
range of subjective and objective evaluation measures that have been applied to the
evaluation task. Next, two frameworks are presented that are designed to provide a
general methodology for evaluation: PARADISE (Walker et al 1997) and Quality of
Experience (Mo?ller 2005). This is followed by a discussion of concepts from HCI-style
usability evaluations and how they can be applied to spoken dialogue systems, and
then a section dealing with semiautomatic evaluation and the role of standardization.
The final section discusses challenges that arise when evaluating advanced dialogue
systems incorporating multimodality and adaptivity, and when evaluating systems
designed for real-world applications.
Finally, Chapter 7 briefly discusses conversational dialogue systems (i.e., com-
panion/chatbot systems), whose emphasis is on social communication rather than the
information exchange tasks considered for most of the book. It also addresses the
relationship between commercial and academic approaches to spoken dialogue.
It is interesting to note that both of the authors have also written books of their own
that address aspects of spoken dialogue systems: McTear (2004) gives a comprehensive
overview of the research area, including a series of hands-on tutorials on building
systems using tools such as the CSLU toolkit and VoiceXML, and Jokinen (2009) pro-
vides a detailed description of a particular style of dialogue management, Constructive
Dialogue Modeling.
This book fills a different niche: It has neither the exercises and tutorials of the
McTear book, nor the in-depth description of a single formalism provided by Jokinen.
Instead, it concentrates on outlining the issues involved in building a spoken dialogue
system and on describing a wide range of existing techniques and systems. Although
the book is short, it provides an excellent starting point for researchers new to the
field, and every section has a good selection of references which would easily allow
the interested reader to follow up any particular topic in more depth. The chapters on
error handling and evaluation provide particularly useful summaries of the issues and
techniques in these active research areas.
Although the preface says that the book is aimed at both engineering and humani-
ties students, I suspect that readers without any mathematical background would have
difficulty with some of the more formal sections. However, the necessary background
knowledge is not great, and in general the book is clearly written and understandable;
it is suitable as both an introduction to this research area and a survey of current state-
of-the-art techniques.
782
Book Review
References
Jokinen, K. 2009. Constructive Dialogue
Modelling: Speech Interaction and Rational
Agents. Wiley Series in Agent Technology.
John Wiley & Sons, Ltd., Hoboken, NJ.
McTear, M. F. 2004. Spoken Dialogue
Technology: Toward the Conversational User
Interface. Springer-Verlag, Berlin.
Mo?ller, S. 2005. Quality of Telephone-Based
Spoken Dialogue Systems. Springer,
New York.
Walker, M. A., D. J. Litman, C. A. Kamm, and
A. Abella. 1997. PARADISE: a framework
for evaluating spoken dialogue agents. In
Proceedings of EACL 1997, pages 271?280,
Madrid.
This book review was edited by Pierre Isabelle.
Mary Ellen Foster is a research fellow in the Interaction Lab of the School of Mathematical and
Computer Sciences at Heriot-Watt University. Her research interests include embodied commu-
nication, natural language generation, and multimodal dialogue systems. Her address is School
of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh EH14 4AS, United
Kingdom; e-mail: M.E.Foster@hw.ac.uk.
783

Automated Metrics That Agree With Human Judgements
On Generated Output for an Embodied Conversational Agent
Mary Ellen Foster
Informatik VI: Robotics and Embedded Systems
Technische Universita?t Mu?nchen
Boltzmannstra?e 3, D-85748 Garching bei Mu?nchen, Germany
foster@in.tum.de
Abstract
When evaluating a generation system, if a cor-
pus of target outputs is available, a common
and simple strategy is to compare the system
output against the corpus contents. However,
cross-validation metrics that test whether the
system makes exactly the same choices as the
corpus on each item have recently been shown
not to correlate well with human judgements
of quality. An alternative evaluation strategy
is to compute intrinsic, task-specific proper-
ties of the generated output; this requires more
domain-specific metrics, but can often pro-
duce a better assessment of the output. In this
paper, a range of metrics using both of these
techniques are used to evaluate three meth-
ods for selecting the facial displays of an em-
bodied conversational agent, and the predic-
tions of the metrics are compared with human
judgements of the same generated output. The
corpus-reproduction metrics show no relation-
ship with the human judgements, while the
intrinsic metrics that capture the number and
variety of facial displays show a significant
correlation with the preferences of the human
users.
1 Introduction
Evaluating the output of a generation system is
known to be difficult: since generation is an open-
ended task, the criteria for success can be difficult
to define (cf. Mellish and Dale, 1998). In the cur-
rent state of the art, there are two main strategies
for evaluating the output of a generation system: the
behaviour or preferences of humans in response to
the output may be measured, or automated measures
may be computed on the output itself. A study in-
volving human judges is the most complete and con-
vincing evaluation of generated output. However,
such a study is not always practical, as recruiting
sufficient subjects can be time-consuming and ex-
pensive. So automated metrics are also used in ad-
dition to?or instead of?human studies.
When automatically evaluating generated output,
the goal is to find metrics that can easily be com-
puted and that can also be shown to correlate with
human judgements of quality. Such metrics have
been introduced in other fields, including PAR-
ADISE (Walker et al, 1997) for spoken dialogue
systems, BLEU (Papineni et al, 2002) for ma-
chine translation,1 and ROUGE (Lin, 2004) for sum-
marisation. Many automated generation evaluations
measure the similarity between the generated output
and a corpus of ?gold-standard? target outputs, often
using measures such as precision and recall. Such
measures of corpus similarity are straightforward to
compute and easy to interpret; however, they are not
always appropriate for generation systems. One of
the main advantages of choosing dynamic genera-
tion over canned output is its flexibility and its abil-
ity to produce a range of different outputs; as pointed
out by Paris et al (2007), ?[e]valuation studies that
ignore the potential of the system to generate a range
of appropriate outputs will be necessarily limited.?
Indeed, several recent studies (Stent et al, 2005;
Belz and Reiter, 2006; Foster and White, 2007) have
shown that strict corpus-similarity measures tend to
favour repetitive generation strategies that do not di-
verge much, on average, from the corpus data, while
human judges often prefer output with more variety.
1Although Callison-Burch et al (2006) have recently called
into question the utility of BLEU.
95
Automated metrics that take into account other
properties than strict corpus similarity have also
been used to evaluate the output of generation sys-
tems. Walker (2005) describes several evaluations
that used corpus data in a different way: each of the
corpus examples was associated with some reward
function (e.g., subjective user evaluation or task suc-
cess), and machine-learning techniques such as re-
inforcement learning or boosting were then used to
train the output planner. Foster and White (2007)
found that automated metrics based on factors other
than corpus similarity (e.g., the amount of variation
in the output) agreed better with user preferences
than did the corpus-similarity scores. Belz and Gatt
(2008) compare the predictions of a range of mea-
sures, both intrinsic and extrinsic, that were used
to evaluate the systems in a shared-task referring-
expression generation challenge. One main finding
from this comparison was that there was no signif-
icant correlation between the intrinsic and extrinsic
(task success) measures for this task.
All of the above studies considered only systems
that generate text, but many of the same factors also
apply to the generation of non-verbal behaviours for
an embodied conversational agent (ECA) (Cassell
et al, 2000). The behaviour of such an agent is nor-
mally based on recorded human behaviour, which
can provide targets similar to those used in corpus-
based evaluations of text-generation systems. How-
ever, just as in text generation, a multimodal system
that scores well on corpus similarity tends to pro-
duce highly repetitive non-verbal behaviours, so it
is equally important to gather human judgements to
accompany any automated evaluation.
This paper presents three corpus-driven methods
of selecting facial displays for an embodied conver-
sational agent and describes two studies comparing
the output of the different methods. All methods are
based on annotated data drawn from a corpus of hu-
man facial displays, and each uses the corpus data
in a different way. The first evaluation study uses
human judges to compare the output of the selection
methods against one another, while the second study
uses a range of automated metrics: several corpus-
reproduction measures, along with metrics based on
intrinsic properties of the outputs. The results of the
two studies are compared using multiple regression,
and the implications are discussed.
2 Corpus-based generation of facial
displays for an ECA
The experiments in this paper make use of the out-
put components of the COMIC multimodal dialogue
system (Foster et al, 2005), which adds a multi-
modal talking-head interface to a CAD-style system
for redesigning bathrooms. The studies focus on the
task of selecting appropriate ECA head and eyebrow
motions to accompany the turns in which the sys-
tem describes and compares the options for tiling
the room, as those are the parts of the output with
the most interesting and varied content.
The implementations were based on a corpus of
conversational facial displays derived from the be-
haviour of a single speaker reading approximately
450 scripted sentences generated by the COMIC
output-generation system. The OpenCCG syntac-
tic derivation trees (White, 2006) for the sentences
form the basis of the corpus. The leaf nodes in
these trees correspond to the individual words, while
the internal nodes correspond to multi-word con-
stituents. Every node in each tree was initially
labelled with all of the applicable contextual fea-
tures produced by the output planner: the user-
preference evaluation of the tile design being de-
scribed (positive/negative/neutral), the information
status (given/new) of each piece of information, and
the predicted speech-synthesiser prosody. The an-
notators then linked each facial display produced
by the speaker to the node or span of nodes in the
derivation tree covering the words temporally asso-
ciated with the display. Full details of this corpus are
given in Foster (2007a).
The most common display used by the speaker
was a downward nod, while the user-preference
evaluation had the single largest differential effect
on the displays used. When the speaker described
features of the design that the user was expected to
like, he was relatively more likely to turn to the right
and to raise his eyebrows (Figure 1(a)); on features
that the user was expected to dislike, on the other
hand, there was a higher probability of left leaning,
lowered eyebrows, and narrowed eyes (Figure 1(b)).
In a previous study, users were generally able to
recognise these ?positive? and ?negative? displays
when they were resynthesised on an embodied con-
versational agent (Foster, 2007b).
96
(a) Positive (b) Negative
Figure 1: Characteristic facial displays from the corpus
Based on this corpus, three different strategies
were implemented for selecting facial displays to ac-
company the synthesised speech: one strategy us-
ing only the three characteristic displays described
above, along with two data-driven strategies drawing
on the full corpus data. All of the strategies use the
same basic process to select the displays to accom-
pany a sentence. Beginning with the contextually-
annotated syntactic tree for the sentence, the system
proceeds depth-first, selecting a face-display combi-
nation to accompany each node in turn. The main
difference among the strategies is the way that each
selects the displays for a node as it is encountered.
The rule-based strategy includes displays only on
derivation-tree nodes corresponding to specific tile-
design properties: that is, manufacturer and series
names, colours, and decorative motifs. The dis-
plays for such a node are entirely determined by
the user-preference evaluation of the property be-
ing described, and are based on the corpus patterns
described above: for every node associated with a
positive evaluation, this strategy selects a right turn
and brow raise; for a negative node, it selects a left
turn, brow lower, and eye squint; while for all other
design-property nodes, it chooses a downward nod.
While the rule-based strategy selects displays
only on nodes describing tile-design features, the
two data-driven strategies consider all nodes in the
syntactic tree for a sentence as possible sites for a fa-
cial display. To choose the displays for a given node,
the system considers the set of displays that occurred
on all nodes in the corpus with the same syntactic,
semantic, and pragmatic context, and then chooses a
display from this set in one of two ways. The ma-
jority strategy selects the most common option in all
cases, while the weighted strategy makes a stochas-
tic choice among all of the options based on the rel-
ative frequency. As a concrete example, consider a
hypothetical context in which the speaker made no
motion 80% of the time, a downward nod 15% of
the time, and a downward nod with a brow raise the
other 5% of the time. For nodes with this context,
the majority strategy would always choose no mo-
tion, while the weighted strategy would choose no
motion with probability 0.8, a downward nod with
probability 0.15, and a nod with a brow raise with
probability 0.05.
Table 1 shows a sample sentence from the corpus,
the original facial displays used by the speaker, and
the displays selected by each of the strategies. In the
figure, nd=d indicates a downward nod, bw=u and
bw=d a brow raise and lower, respectively, sq an eye
squint, ln=l a left lean, and tn=r a right turn. Most
of the displays in these schedules are associated with
leaf nodes in the derivation tree, and therefore with
single words in the output. However, both the left
lean in the original schedule and the right turn in
the weighted schedule are associated with internal
nodes in the tree, and therefore cover more than one
word in the surface string.
3 User-preference studies
As a first comparison of the evaluation strategies,
human judges were asked to compare videos based
on the output of each of the generation strategies
to one another and to resynthesised versions of the
original displays from the corpus. This section gives
the details of a study in which the judges chose
97
Although it?s in the family style, the tiles are by Alessi.
Original nd=d nd=d nd=d nd=d nd=d,bw=u
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ln=l . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Rule-based ln=l,bw=d tn=r,bw=u
Majority nd=d nd=d
Weighted nd=d nd=d . . tn=r . .
Table 1: Face-display schedules for a sample sentence
Figure 2: RUTH talking head
among the original displays and the output of the
weighted and rule-based strategies. At the end of
the section, the results of this study are discussed to-
gether with the results of a similar previous study
comparing the two data-driven strategies to each
other; the full details of the earlier study are given
in Foster and Oberlander (2007).
3.1 Subjects
Subjects were recruited for this experiment through
the Language Experiments Portal,2 a website dedi-
cated to online psycholinguistic experiments. There
were 36 subjects (20 female), 50% of whom identi-
fied themselves as native English speakers; most of
the subjects were between 20 and 29 years old.
3.2 Materials
The materials for this experiment were based on 18
randomly-selected sentences from the corpus. For
each sentence, face-display schedules were gener-
ated using both the rule-based and the weighted
strategies. The Festival speech synthesiser (Clark
2http://www.language-experiments.org/
et al, 2004) and the RUTH animated talking head
(DeCarlo et al, 2004) (Figure 2) were used to create
video clips of the two generated schedules for each
sentence, along with a video clip showing the origi-
nal facial displays annotated in the corpus.
3.3 Method
Each subject saw a series of pairs of videos. Both
videos in a pair had identical spoken content, but the
face-display schedules differed: each trial included
two of rule-based, weighted, and original. For each
pair of videos, the subject was asked to select which
of the two versions they preferred. Subjects made
each pairwise comparison between schedule types
six times?three times in each order?for a total of
18 judgements. All subjects saw the same set of sen-
tences, in an individually randomly-selected order:
the pairwise choices between schedule types were
also allocated to items at random.
3.4 Results and analysis
The overall pairwise preferences of the subjects in
this study are shown in Figure 3(a). A ?2 goodness-
of-fit test can be used to evaluate the significance
of the choices made on each individual comparison.
For the comparison between original and rule-based
schedules, the preference is significant: ?2(1,N =
216)= 4.17, p< 0.05. The results are similar for the
original vs. weighted comparison: ?2(1,N = 215) =
4.47, p < 0.05. However, the preferences for the
weighted vs. rule-based comparison are not signifi-
cant: ?2(1,N = 217) = 2.44, p? 0.12.
Figure 3(b) shows the results from a similar pre-
vious study (Foster and Oberlander, 2007) in which
the subjects compared the two data-driven strate-
gies to the original displays, using a design identi-
cal to that used in the current study with 54 sub-
jects and 24 sentences. The responses given by
the subjects in this study also showed a signifi-
98
123
93
Origin
al vs. R
ule-ba
sedW
eighte
d vs. R
ule-ba
sed
Origin
al vs. W
eighte
d
0255075100125
120
97
123
92
(a) Original, weighted, rule-based
295
153
Origin
al vs. M
ajority
Weigh
ted vs
. Majori
ty O
riginal
 vs. W
eighte
d
050100150200250300
278
170
251
197
(b) Original, weighted, majority (Foster and Oberlander, 2007)
Figure 3: Pairwise preferences from the user evaluations
cant preference for the original schedules over the
weighted ones (?2(1,N = 448) = 6.51, p < 0.05).
Both the weighted and the original schedules were
very strongly preferred over the majority schedules
(?2(1,N = 448) = 45 and 26, respectively; p 
0.0001). The original vs. weighted comparison was
included in both studies (the rightmost pair of bars
on the two graphs in Figure 3), and the response pat-
terns across the two studies for this comparison did
not differ significantly from each other: ?2(1,N =
664) = 0.02, p? 0.89.
3.5 Discussion
Taken together, the results of these two studies sug-
gest a rough preference ordering among the dif-
ferent strategies for generating facial displays. In
both studies, the judges significantly preferred the
original displays from the corpus over any of the
automatically-generated alternatives. This suggests
that, for this generation task, the data in the cor-
pus can indeed be treated as a ?gold standard??
unlike, for example, the corpus used by Belz and
Reiter (2006), where the human judges sometimes
preferred generated output to the corpus data. The
schedules generated by the majority strategy, on the
other hand, were very obviously disliked by the
judges in the Foster and Oberlander (2007) study.
The ranking between the rule-based and weighted
schedules from the current study is less clear, al-
though there was a tendency to prefer the latter.
4 Automated evaluation
Since the subjects in the user-preference studies gen-
erally selected the corpus schedules over any of
the alternatives, any automated metric for this task
should favour output that resembles the examples in
the corpus. The most obvious form of corpus simi-
larity is exact reproduction of the displays in the cor-
pus, which suggests using metrics such as precision
and recall that favour generation strategies whose
output on every item is as close as possible to what
was annotated in the corpus for that sentence. In
Section 4.1, several such corpus-reproduction met-
rics are described and their results presented.
For this type of open-ended generation task,
though, it can be overly restrictive to allow only
the displays that were annotated in the corpus for
a sentence and to penalise any deviation. Indeed, as
mentioned in the introduction, a number of previous
studies have found that the output of generation sys-
tems that score well on this type of metric is often
disliked in practice by users. Section 4.2 therefore
presents several intrinsic metrics that aim to cap-
ture corpus similarity of a different type: rather than
requiring the system to exactly reproduce the cor-
pus on each sentence, these metrics instead favour
strategies resulting in global behaviour that exhibits
similar patterns to those found in the corpus, without
necessarily agreeing exactly with the corpus on any
specific sentence.
99
0.52
0.31
0.18
0.82
0.34
0.29
0.24
0.12
0.75
0.23
0.22
0.10
0.06
0.77
0.14
Precis
ion
Recal
l
F Scor
eN
ode A
cc.
Beta
00.20.40.60.81
Majority Weighte
d
Rule-b
ased
(a) Corpus-reproduction metrics
5.38
3.26
4.58
2.98
3.15
1.31
2.07
1.24
Token
s
Types
TTR
0123456
Origin
al
Weigh
ted
Majority Rule-ba
sed
0.640
.69 0.
480.6
8
00.20.40.60.81
(b) Intrinsic metrics
Figure 4: Results of the automated evaluations
4.1 Corpus-reproduction metrics
This first set of metrics compared the generated
schedules against the original schedules annotated in
the corpus, using 10-fold cross-validation. The first
three metrics that were tested are standard for this
sort of corpus-comparison task: recall, precision,
and F score. Recall was computed as the propor-
tion of the corpus displays for a sentence that were
reproduced exactly in the generated output, while
precision was the proportion of generated displays
that had exact matches in the corpus; the F score for
a sentence is then the harmonic mean of these two
values, as usual. The leftmost three columns in Ta-
ble 2 show the precision, recall, and F score for the
sample schedules in Table 1.
In addition to the above commonly-used metrics,
two other corpus-reproduction metrics were also
computed. The first, node accuracy, represents the
proportion of nodes in the derivation tree for a sen-
tence where the proposed displays were correct, in-
cluding those nodes where the system correctly se-
lected no motion?a baseline system that never pro-
poses any motion scores 0.79 on this measure. The
fourth column of Table 2 shows the node-accuracy
score for the sample sentences. The final corpus-
reproduction metric compared the proposed displays
to the annotated corpus displays using the ? agree-
ment measure (Artstein and Poesio, 2005). ? is a
weighted measure that permits different levels of
P R F NAcc Tok Typ TTR
Original ? ? ? ? 6 3 0.5
Rule-based 0 0 0 0.65 2 2 1
Majority 0.50 0.14 0.11 0.70 2 1 0.5
Weighted 0.67 0.29 0.20 0.74 3 2 0.67
Table 2: Automated evaluation of the sample schedules
agreement when annotations overlap, and that can
therefore capture a more fine-grained form of agree-
ment than other measures such as ?.
Figure 4(a) shows the results for all of these
corpus-reproduction measures, averaged across the
sentences in the corpus; the results for the weighted
and majority strategies are from Foster and Ober-
lander (2007). The majority strategy scored uni-
formly higher than the weighted strategy on all of
these measures?particularly on precision?while
the weighted strategy in turn scored higher than the
rule-based strategy on all measures except for node
accuracy. Using a Wilcoxon rank sum test with a
Bonferroni correction for multiple comparisons, all
of the differences among the strategies on precision,
recall, F score, and node accuracy are significant at
p < 0.001. Significance cannot be assessed for the
differences in ? scores, as noted by Artstein and Poe-
sio (2005), but the results are similar. Also, the node
accuracy score for the majority strategy is signifi-
cantly better than the no-motion baseline of 0.79,
while those for the weighted and rule-based strate-
gies are worse (also all p < 0.001).
100
As expected?and as noted by Foster and Ober-
lander (2007)?all of the corpus-reproduction met-
rics strongly favoured the weighted strategy over the
weighted strategy and generally penalised the rule-
based strategy. Since the majority strategy always
chooses the most probable option, it is not surpris-
ing that it agrees more often with the corpus than
do the other strategies, which deliberately select less
frequent options; this led to its relatively high scores
on the corpus-reproduction metrics. It is also not
surprising that the weighted strategy beat the rule-
based strategy on most of these metrics, as the for-
mer selects from the most frequent options, while
the latter uses the most marked options, which are
not generally the most frequent.
4.2 Intrinsic metrics
The metrics in the preceding section compared the
displays selected for a sentence against the displays
found in the corpus for that sentence. This sec-
tion describes other measures that are computed di-
rectly on the generated schedules, without any ref-
erence to the corpus data. For each sentence, the
following values were counted: the total number of
face-display combinations (i.e., the number of dis-
play tokens), and the number of different combina-
tions (types). In addition to being used as metrics
themselves, these two counts were also used to com-
pute a third value: the type/token ratio (TTR) (i.e.,
# types
# tokens ), which captures the diversity of the dis-
plays selected for each sentence.
These intrinsic metrics were computed on each
sentence produced in the cross-validation study from
the preceding section and then averaged to produce
the final results. Since these metrics do not require
the original corpus data for comparison, they were
also computed on the original corpus schedules. The
rightmost columns in Table 2 show the intrinsic re-
sults for the sample schedules in Table 1.
The overall results for these metrics across the en-
tire corpus are shown in Figure 4(b). The original
corpus had both the most displays types and the most
tokens; the values for weighted choice were a fairly
close second, those for majority choice third, while
the rule-based strategy scored lowest on both of
these metrics. Except for the difference between ma-
jority and rule-based on the facial-display types?
which is not significant?all of the differences be-
tween schedule types on these two measures are sig-
nificant at p < 0.001 on a Wilcoxon rank sum test
with Bonferroni correction. When it comes to the
type/token ratio, the value for the majority-choice
schedule is significantly lower than that for the other
three schedule types (all p < 0.0001), while the
value for weighted choice is somewhat higher than
that for the original schedules (p < 0.01); no other
differences are significant.
Since the original corpus schedules scored the
highest on the user study, these metrics should be
considered in the context of of how close the results
are to those of the corpus. Figure 4(b) shows that
the weighted strategy is most similar to the corpus in
both the number and the diversity of displays it se-
lects, while the other two strategies have much lower
diversity. However, even though the rule-based strat-
egy selects fewer displays all of the other strategies,
its TTR is more similar to that of the corpus and the
weighted strategy, while the majority strategy has a
much lower TTR. In fact, in the schedules generated
by the majority-choice strategy, nearly 90% of the
displays that were selected were downward nods.
5 Comparing the automated metrics with
human preferences
Qualitatively, the results of the corpus-reproduction
metrics differ greatly from the preferences of the
human judges. The users generally liked the ma-
jority schedules the least, while all of these met-
rics scored this strategy the highest. Among the
intrinsic metrics, the type and token counts placed
the weighted schedules closest to the corpus, while
the majority and rule-based strategies were further
away; this agrees with the human results for the
two data-driven strategies, but not for the rule-based
strategy. On the other hand, the TTR indicated that
the output of the rule-based and weighted schedules
was similar to the schedules found in the corpus,
while the majority-choice strategy produced sen-
tences with TTRs more different from the corpus,
generally agreeing with the human results.
To permit a more quantitative comparison be-
tween the predictions of the automated metrics and
the judges? preferences, the pairwise preferences
from the user study were converted into a numeric
value called the selection ratio. The selection ratio
101
for an item (i.e., a sentence with a particular set of
facial displays) was computed as the number of tri-
als on which that item was selected, divided by the
total number of trials on which that item was an op-
tion. For example, an item that was always preferred
over any of the alternatives on all trials would score
1.0 on this measure, while an item that was selected
a quarter of the time would score 0.25. The selec-
tion ratios of the items used in the human-preference
studies ranged from 0.13 to 0.85. As a concrete ex-
ample, when the sentence in Table 1 was used in the
Foster and Oberlander (2007) study, the selection ra-
tios were 0.43 for the original version, 0.33 for the
majority version, and 0.24 for the weighted version.
The relationship between the selection ratio and
the full set of automated metrics from the preced-
ing section was assessed through multiple linear re-
gression. An initial model including all of the auto-
mated metrics as predictor variables had an adjusted
R2 value of 0.413. Performing stepwise selection on
this initial model resulted in a final model with two
significant predictor variables?display tokens and
TTR?and an adjusted R2 of 0.422. The regression
coefficients for both of these predictor variables are
positive, with high significance (p < 0.001). While
the R2 values indicate that neither the initial nor the
final model fully explains the selection ratios from
the user study, the details of the models themselves
are relevant to the overall goal of finding automated
metrics that agree with human preferences.
The results of the stepwise selection have backed
up the qualitative intuition that none of the corpus-
reproduction metrics had any relationship to the
users? preferences, while the number and diversity
of displays per sentence appear to have contributed
much more strongly to the choices made by the hu-
man judges. This adds to the growing body of ev-
idence that intrinsic measures are the preferred op-
tion for evaluating the output of generation systems,
particularly those that are designed to incorporate
variation into their output, while measures based on
strict corpus similarity are less likely to be useful.
6 Conclusions
This paper has presented three methods for using
corpus data to select facial displays for an embod-
ied agent and shown the results from two studies
comparing the output generated by these methods.
When human judges rated the output, they preferred
the original displays from the corpus and strongly
disliked the displays selected by a majority-choice
strategy, with the weighted and rule-based strategies
in between. In the automated evaluation, the metrics
that directly compared the generated output against
the corpus data favoured the majority strategy and
did not show any relationship with the user prefer-
ences. On the other hand, the number of displays
accompanying a sentence and the diversity of those
displays both had a positive relationship with the
rate at which users selected that display schedule.
These results confirm those of previous text-
generation evaluations and extend these results to
the multimodal-generation case. This adds to the
body of evidence that, even though direct corpus re-
production is often the easiest factor to analyse au-
tomatically, it is rarely an accurate reflection of user
reactions to generated output. If a system performs
well on this type of metric, its output tends to be con-
strained to a small space; for example, the majority-
choice strategy used in these studies nearly always
selected a nodding expression. For most generation
tasks, output options beyond those in the corpus are
often equally valid, and users seem to prefer a sys-
tem that makes use of this wider space of variations.
This suggests that corpus-based generation sys-
tems should use strategies that retain the full range
of variation, and?perhaps most importantly?that
metrics based on factors other than strict similarity
are more likely to capture human preferences when
evaluating generated output.
The user study described here was based only on
the preferences of human judges. In future, it would
be informative to include more task-based measures
such as task success and time taken, as user pref-
erences do not always correlate with performance
(Nielsen and Levy, 1994), to see if a different style
of automated measure agrees better with the results
of this sort of user study.
Acknowledgements
This work was partly supported by the EU projects
COMIC (IST-2001-32311) and JAST (FP6-003747-
IP). Thanks to Jon Oberlander, Jean Carletta, and the
INLG reviewers for helpful feedback.
102
References
R. Artstein and M. Poesio. 2005. Kappa3 = al-
pha (or beta). Technical Report CSM-437,
University of Essex Department of Com-
puter Science. http://cswww.essex.ac.uk/
technical-reports/2005/csm-437.pdf.
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evalu-
ation measures for referring expression generation. In
Proceedings of ACL/HLT 2008.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings of
EACL 2006. acl:E06-1040.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine transla-
tion research. In Proceedings of EACL 2006. acl:
E06-1032.
J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, ed-
itors. 2000. Embodied Conversational Agents. MIT
Press.
R. A. J. Clark, K. Richmond, and S. King. 2004. Festi-
val 2 ? build your own general purpose unit selection
speech synthesiser. In Proceedings of the 5th ISCA
Workshop on Speech Synthesis. http://www.ssw5.
org/papers/1047.pdf.
R. Dale and M. White, editors. 2007. Report
from the Workshop on Shared Tasks and Com-
parative Evaluation in Natural Language Genera-
tion. http://ling.ohio-state.edu/nlgeval07/
NLGEval07-Report.pdf.
D. DeCarlo, M. Stone, C. Revilla, and J. Venditti. 2004.
Specifying and animating facial signals for discourse
in embodied conversational agents. Computer Anima-
tion and Virtual Worlds, 15(1):27?38. doi:10.1002/
cav.5.
M. E. Foster. 2007a. Associating facial displays with syn-
tactic constituents for generation. In Proceedings of
the ACL 2007 Linguistic Annotation Workshop. acl:
W07-1504.
M. E. Foster. 2007b. Generating embodied descrip-
tions tailored to user preferences. In Proceedings
of Intelligent Virtual Agents 2007. doi:10.1007/
978-3-540-74997-4_24.
M. E. Foster and J. Oberlander. 2007. Corpus-based
generation of head and eyebrow motion for an em-
bodied conversational agent. Language Resources
and Evaluation, 41(3?4):305?323. doi:10.1007/
s10579-007-9055-3.
M. E. Foster and M. White. 2007. Avoiding repetition in
generated text. In Proceedings of ENLG 2007. acl:
W07-2305.
M. E. Foster, M. White, A. Setzer, and R. Catizone. 2005.
Multimodal generation in the COMIC dialogue sys-
tem. In Proceedings of the ACL 2005 Demo Session.
acl:P05-3012.
C. Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proceedings of the ACL 2004
Workshop on Text Summarization. acl:W04-1013.
C. Mellish and R. Dale. 1998. Evaluation in the con-
text of natural language generation. Computer Speech
and Language, 12(4):349?373. doi:10.1006/csla.
1998.0106.
J. Nielsen and J. Levy. 1994. Measuring usability: pref-
erence vs. performance. Communications of the ACM,
37(4):66?75. doi:10.1145/175276.175282.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL 2002. acl:
P02-1040.
C. Paris, D. Scott, N. Green, K. McCoy, , and D. Mc-
Donald. 2007. Desiderata for evaluation of natural lan-
guage generation. In Dale and White (2007), chapter 2.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Computational Linguistics and Intelligent
Text Processing, pages 341?351. Springer. doi:10.
1007/b105772.
M. A. Walker. 2005. Can we talk? Methods for evalu-
ation and training of spoken dialogue systems. Lan-
guage Resources and Evaluation, 39(1):65?75. doi:
10.1007/s10579-005-2696-1.
M. A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.
1997. PARADISE: A general framework for eval-
uating spoken dialogue agents. In Proceedings of
ACL/EACL 1997. acl:P97-1035.
M. White. 2006. Efficient realization of coordinate struc-
tures in Combinatory Categorial Grammar. Research
on Language and Computation, 4(1):39?75. doi:
10.1007/s11168-006-9010-2.
103
Situated Reference in a Hybrid Human-Robot Interaction System
Manuel Giuliani1 and Mary Ellen Foster2 and Amy Isard3
Colin Matheson3 and Jon Oberlander3 and Alois Knoll1
1Informatik VI: Robotics and Embedded Systems, Technische Universita?t Mu?nchen
2School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
3Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh
Abstract
We present the situated reference genera-
tion module of a hybrid human-robot in-
teraction system that collaborates with a
human user in assembling target objects
from a wooden toy construction set. The
system contains a sub-symbolic goal in-
ference system which is able to detect the
goals and errors of humans by analysing
their verbal and non-verbal behaviour. The
dialogue manager and reference genera-
tion components then use situated refer-
ences to explain the errors to the human
users and provide solution strategies. We
describe a user study comparing the results
from subjects who heard constant refer-
ences to those who heard references gener-
ated by an adaptive process. There was no
difference in the objective results across
the two groups, but the subjects in the
adaptive condition gave higher subjective
ratings to the robot?s abilities as a conver-
sational partner. An analysis of the objec-
tive and subjective results found that the
main predictors of subjective user satisfac-
tion were the user?s performance at the as-
sembly task and the number of times they
had to ask for instructions to be repeated.
1 Introduction
When two humans jointly carry out a mutual task
for which both know the plan?for example, as-
sembling a new shelf?it frequently happens that
one makes an error, and the other has to assist
and to explain what the error was and how it can
be solved. Humans are skilled at spotting errors
committed by another, as well as errors which
they made themselves. Recent neurological stud-
ies have shown that error monitoring?i.e., ob-
serving the errors made by oneself or by others?
plays an important role in joint activity. For ex-
ample, Bekkering et al (2009) have demonstrated
that humans show the same brain activation pat-
terns when they make an error themselves and
when they observe someone else making an error.
In this paper, we describe a human-robot inter-
action (HRI) system that is able both to analyse
the actions and the utterances of a human part-
ner to determine if the human made an error in
the assembly plan, and to explain to the human
what went wrong and what to do to solve the prob-
lem. This robot combines approaches from sub-
symbolic processing and symbolic reasoning in a
hybrid architecture based on that described in Fos-
ter et al (2008b).
During the construction process, it is frequently
necessary to refer to an object which is being used
to assemble the finished product, choosing an un-
ambigious reference to distinguish the object from
the others available. The classic reference gen-
eration algorithm, on which most subsequent im-
plementations are based, is the incremental algo-
rithm of Dale and Reiter (1995), which selects
a set of attributes of a target object to single it
out from a set of distractor objects. In real-world
tasks, the speaker and hearer often have more con-
text in common than just the knowledge of object
attributes, and several extensions have been pro-
posed, dealing with visual and discourse salience
(Kelleher and Kruijff, 2006) and the ability to pro-
duce multimodal references including actions such
as pointing (van der Sluis, 2005; Kranstedt and
Wachsmuth, 2005).
Foster et al (2008a) noted another type of mul-
timodal reference which is particularly useful in
embodied, task-based contexts: haptic-ostensive
reference, in which an object is referred to as it
is being manipulated by the speaker. Manipulat-
ing an object, which must be done in any case as
part of the task, also makes an object more salient
and therefore affords linguistic references that in-
Figure 1: The dialogue robot
dicate the increased accessibility of the referent.
This type of reference is similar to the placing-for
actions noted by Clark (1996).
An initial approach for generating referring ex-
pressions that make use of haptic-ostensive refer-
ence was described in (Foster et al, 2009a). With
this system, a study was conducted comparing the
new reference strategy to the basic Dale and Reiter
incremental algorithm. Na??ve users reported that it
was significantly easier to understand the instruc-
tions given by the robot when it used references
generated by the more sophisticated algorithm. In
this paper, we perform a similar experiment, but
making use of a more capable human-robot in-
teraction system and a more complete process for
generating situated references.
2 Hybrid Human-Robot Dialogue
System
The experiment described in this paper makes use
of a hybrid human-robot dialogue system which
supports multimodal human-robot collaboration
on a joint construction task. The robot (Figure 1)
has a pair of manipulator arms with grippers,
mounted in a position to resemble human arms,
and an animatronic talking head (van Breemen,
2005) capable of producing facial expressions,
rigid head motion, and lip-synchronised synthe-
sised speech. The subject and the robot work to-
gether to assemble wooden construction toys on
a common workspace, coordinating their actions
through speech (English or German), gestures, and
facial expressions.
The robot can pick up and move objects in the
workspace and perform simple assembly tasks. In
the scenario considered here, both of the partici-
pants know the assembly plan and jointly execute
it. The robot assists the human, explains necessary
assembly steps in case the human makes an error,
and offers pieces as required. The workspace is di-
vided into two areas?one belonging to the robot
and one to the human?to make joint action nec-
essary for task success.
The system has components which use both
sub-symbolic and symbolic processing. It in-
cludes a goal inference module based on dynamic
neural fields (Erlhagen and Bicho, 2006; Bicho
et al, 2009), which selects the robot?s next actions
based on the human user?s actions and utterances.
Given a particular assembly plan and the knowl-
edge of which objects the user has picked up, this
module can determine when the user has made
an error. The system also incorporates a dialogue
manager based on the TrindiKit dialogue manage-
ment toolkit (Larsson and Traum, 2000), which
implements the information-state based approach
to dialogue management. This unique combina-
tion of abilities means that when the robot detects
that its human partner has made an error?for ex-
ample, picking up or requesting an assembly piece
that is not needed in the current step of the building
plan?it can explain to the human what the error
was and what can be done to correct the mistake?
for example by picking up or indicating the correct
assembly piece.
Messages from all of the system?s input chan-
nels (speech, object recognition, and gesture
recognition) are processed and combined by a
multimodal fusion component based on (Giuliani
and Knoll, 2008), which is the link between the
symbolic and the sub-symbolic parts of the sys-
tem. The fusion component then communicates
with the goal inference module, which calculates
the next action instructions for the robot and also
determines if the user made an error. From there,
fusion combines the information from goal infer-
ence with the input data and sends unified hy-
potheses to the dialogue manager.
When it receives the fusion hypotheses, the dia-
logue manager uses the dialogue history and the
physical and task context to choose a response.
It then sends a high-level specification of the de-
1. System First we will build a windmill.
2. User Okay.
3. User {picks up a yellow cube, unnecessary piece for a
windmill}
4. System You don?t need a yellow cube to build a windmill.
5. System To build a windmill, you first need to build a
tower.
6. System [picking up and holding out red cube] To build
the tower, insert the green bolt through the end of this
red cube and screw it into the blue cube.
7. User [takes cube, performs action] Okay.
Figure 2: Sample human-robot dialogue, showing
adaptively-generated situated references
sired response to the output planner, which in turn
sends commands to each output channel: linguis-
tic content (including multimodal referring ex-
pressions), facial expressions and gaze behaviours
of the talking head, and actions of the robot ma-
nipulators. The linguistic outputs are realised us-
ing the OpenCCG surface realiser (White, 2006).
3 Reference Generation
In this system, two strategies were implemented
for generating references to objects in the world:
a constant version that uses only the basic incre-
mental algorithm (Dale and Reiter, 1995) to se-
lect properties, and an adaptive version that uses
more of the physical, dialogue and task context
to help select the references. The constant sys-
tem can produce a definite or indefinite reference,
and the most appropriate combination of attributes
according to the incremental algorithm. The adap-
tive system also generates pronominal and deictic
references, and introduces the concept of multiple
types of distractor sets depending on context.
Figure 2 shows a fragment of a sample interac-
tion in which the user picks up an incorrect piece:
the robot detects the error and describes the correct
assembly procedure. The underlined references
show the range of output produced by the adap-
tive reference generation module; for the constant
system, the references would all have been ?the
red cube?. The algorithms used by the adaptive
reference generation module are described below.
3.1 Reference Algorithm
The module stores a history of the referring ex-
pressions spoken by both the system and the user,
and uses these together with distractor sets to se-
lect referring expressions. In this domain there are
two types of objects which we need to refer to:
concrete objects in the world (everything which is
on the table, or in the robot?s or user?s hand), and
objects which do not yet exist, but are in the pro-
cess of being created. For non-existent objects we
do not build a distractor set, but simply use the
name of the object. In all other cases, we use one
of three types of distractor set:
? all the pieces needed to build a target object;
? all the objects referred to since the last men-
tion of this object; or
? all the concrete objects in the world.
The first type of set is used if the object under
consideration (OUC) is a negative reference to a
piece in context of the creation of a target object.
In all other cases, the second type is used if the
OUC has been mentioned before and the third type
if it has not.
When choosing a referring expression, we first
process the distractor set, comparing the proper-
ties of the OUC with the properties of all distrac-
tors. If a distractor has a different type from the
OUC, it is removed from the distractor set. With
all other properties, if the distractor has a different
value from the OUC, it is removed from the dis-
tractor set, and the OUC?s property value is added
to the list of properties to use.
We then choose the type of referring expression.
We first look for a previous reference (PR) to the
OUC, and if one exists, determine whether it was
in focus. Depending on the case, we use one of the
following reference strategies.
No PR If the OUC does not yet exist or we are
making a negative reference, we use an indef-
inite article. If the robot is holding the OUC,
we use a deictic reference. If the OUC does
exist and there are no distractors, we use a
definite; if there are distractors we use an in-
definite.
PR was focal If the PR was within the same turn,
we choose a pronoun for our next reference.
If it was in focus but in a previous turn, if
the robot is holding the OUC we use a deictic
reference, and if the robot is not holding it,
we use a pronoun.
PR was not focal If the robot is holding the
OUC, we make a deictic reference. Other-
wise, if the PR was a pronoun, definite, or de-
ictic, we use a definite article. If the PR was
indefinite and there are no distractors, we use
a definite article, if there are distractors, we
use an indefinite article.
If there are any properties in the list, and the
reference chosen is not a pronoun, we add them.
3.2 Examples of the Reference Algorithm
We will illustrate the reference-selection strategy
with two cases from the dialogue in Figure 2.
Utterance 4 ?a yellow cube?
This object is going to be referred to in a negative
context as part of a windmill under construction,
so the distractor set is the set of objects needed to
make a windmill: {red cube, blue cube, small slat,
small slat, green bolt, red bolt}.
We select the properties to use in describing the
object under consideration, processing the distrac-
tor set. We first remove all objects which do not
share the same type as our object under considera-
tion, which leaves {red cube, blue cube}. We then
compare the other attributes of our new object with
the remaining distractors - in this case ?colour?.
Since neither cube shares the colour ?yellow? with
the target object, both are removed from the dis-
tractor set, and ?yellow? is added to the list of
properties to use.
There is no previous reference to this object,
and since we are making a negative reference,
we automatically choose an indefinite article. We
therefore select the reference ?a yellow cube?.
Utterance 6 ?it? (a green bolt)
This object has been referred to before, earlier in
the same utterance, so the distractor set is all the
references between the earlier one and this one?
{red cube}. Since this object has a different type
from the bolt we want to describe, the distractor
set is now empty, and nothing is added to the list
of properties to use.
There is a previous definite reference to the ob-
ject in the same utterance: ?the green bolt?. This
reference was focal, so we are free to use a pro-
noun if appropriate. Since the previous reference
was definite, and the object being referred to does
exist, we choose to use a pronoun. We therefore
select the reference ?it?.
4 Experiment Design
In the context of the HRI system, a constant refer-
ence strategy is sufficient in that it makes it possi-
ble for the robot?s partner to know which item is
needed. On the other hand, while the varied forms
produced by the more complex mechanism can in-
crease the naturalness of the system output, they
may actually be insufficient if they are not used
in appropriate current circumstances?for exam-
ple, ?this cube? is not a particularly helpful refer-
ence if a user has no way to tell which ?this? is.
As a consequence, the system for generating such
references must be sensitive to the current state
of joint actions and?in effect?of joint attention.
The difference between the two systems is a test of
the adaptive version?s ability to adjust expressions
to pertinent circumstances. It is known that peo-
ple respond well to reduced expressions like ?this
cube? or ?it? when another person uses them ap-
propriately (Bard et al, 2008); we need to see if
the robot system can also achieve the benefits that
situated reference could provide.
To address this question, the human-robot di-
alogue system was evaluated through a user study
in which subjects interacted with the complete sys-
tem. Using a between-subjects design, this study
compared the two reference strategies, measuring
the users? subjective reactions to the system along
with their overall performance in the interaction.
Based on the findings from the user evaluation de-
scribed in (Foster et al, 2009a)?in which the pri-
mary effect of varying the reference strategy was
on the users? subjective opinion of the robot?the
main prediction for this study was as follows:
? Subjects who interact with a system using
adaptive references will rate the quality of
the robot?s conversation more highly than the
subjects who hear constant references.
We made no specific prediction regarding the
effect of reference strategy on any of the objec-
tive measures: based on the results of the user
evaluation mentioned above, there is no reason to
expect an effect either way. Note that?as men-
tioned above?if the adaptive version makes in-
correct choices, that may have a negative impact
on users? ability to understand the system?s gener-
ated references. For this reason, even a finding of
(a) Windmill (b) Railway signal
Figure 3: Target objects for the experiment
no objective difference would demonstrate that the
adaptive references did not harm the users? ability
to interact with the system, as long as it was ac-
companied by the predicted improvement in sub-
jective judgements.
4.1 Subjects
41 subjects (33 male) took part in this experiment.
The mean age of the subjects was 24.5, with a min-
imum of 19 and a maximum of 42. Of the subjects
who indicated an area of study, the two most com-
mon areas were Mathematics (14 subjects) and In-
formatics (also 14 subjects). On a scale of 1 to 5,
subjects gave a mean assessment of their knowl-
edge of computers at 4.1, of speech-recognition
systems at 2.0, and of human-robot systems at 1.7.
Subjects were compensated for their participation
in the experiment.
4.2 Scenario
This study used a between-subjects design with
one independent variable: each subject interacted
either with a system that used a constant strategy
to generate referring expressions (19 subjects), or
else with a system that used an adaptive strategy
(22 subjects).1
Each subject built two objects in collaboration
with the system, always in the same order. The
first target object was the windmill (Figure 3a);
after the windmill was completed, the robot and
human then built a railway signal (Figure 3b). For
both target objects, the user was given a building
plan (on paper). To induce an error, both of the
plans given to the subjects instructed them to use
an incorrect piece: a yellow cube instead of a red
cube for the windmill, and a long (seven-hole) slat
instead of a medium (five-hole) slat for the rail-
1The results of an additional three subjects in the constant-
reference condition could not be analysed due to technical
difficulties.
way signal. The subjects were told that the plan
contained an error and that the robot would cor-
rect them when necessary, but did not know the
nature of the error.
When the human picked up or requested an in-
correct piece during the interaction, the system de-
tected the error and explained to the human what
to do in order to assemble the target object cor-
rectly. When the robot explained the error and
when it handed over the pieces, it used referring
expressions that were generated using the constant
strategy for half of the subjects, and the adaptive
strategy for the other half of the subjects.
4.3 Experimental Set-up and Procedure
The participants stood in front of the table facing
the robot, equipped with a headset microphone for
speech recognition. The pieces required for the
target object?plus a set of additional pieces in or-
der to make the reference task more complex?
were placed on the table, using the same layout
for every participant. The layout was chosen to
ensure that there would be points in the interaction
where the subjects had to ask the robot for build-
ing pieces from the robot?s workspace, as well as
situations in which the robot automatically handed
over the pieces. Along with the building plan men-
tioned above, the subjects were given a table with
the names of the pieces they could build the ob-
jects with.
4.4 Data Acquisition
At the end of a trial, the subject responded to
a usability questionnaire consisting of 39 items,
which fell into four main categories: Intelligence
of the robot (13 items), Task ease and task suc-
cess (12 items), Feelings of the user (8 items),
and Conversation quality (6 items). The items on
the questionnaire were based on those used in the
user evaluation described in (Foster et al, 2009b),
but were adapted for the scenario and research
questions of the current study. The questionnaire
was presented using software that let the subjects
choose values between 1 and 100 with a slider. In
addition to the questionnaire, the trials were also
video-taped, and the system log files from all tri-
als were kept for further analysis.
5 Results
We analysed the data resulting from this study in
three different ways. First, the subjects? responses
Table 1: Overall usability results
Constant Adaptive M-W
Intell. 79.0 (15.6) 74.9 (12.7) p = 0.19, n.s.
Task 72.7 (10.4) 71.1 (8.3) p = 0.69, n.s.
Feeling 66.9 (15.9) 66.8 (14.2) p = 0.51, n.s.
Conv. 66.1 (13.6) 75.2 (10.7) p = 0.036, sig.
Overall 72.1 (11.2) 71.8 (9.1) p = 0.68, n.s.
to the questionnaire items were compared to de-
termine if there was a difference between the re-
sponses given by the two groups. A range of sum-
mary objective measures were also gathered from
the log files and videos?these included the dura-
tion of the interaction measured both in seconds
and in system turns, the subjects? success at build-
ing each of the target objects, the number of times
that the robot had to explain the construction plan
to the user, and the number of times that the users
asked the system to repeat its instructions. Finally,
we compared the results on the subjective and ob-
jective measures to determine which of the objec-
tive factors had the largest influence on subjective
user satisfaction.
5.1 Subjective Measures
The subjects in this study gave a generally positive
assessment of their interactions with the system on
the questionnaire?with a mean overall satisfac-
tion score of 72.0 out of 100?and rated the per-
ceived intelligence of the robot particularly highly
(overall mean of 76.8). Table 1 shows the mean
results from the two groups of subjects for each
category on the user-satisfaction questionnaire, in
all cases on a scale from 0?100 (with the scores
for negatively-posed questions inverted).
To test the effect of reference strategy on the
usability-questionnaire responses, we performed a
Mann-Whitney test comparing the distribution of
responses from the two groups of subjects on the
overall results, as well as on each sub-category of
questions. For most categories, there was no sig-
nificant difference between the responses of the
two groups, with p values ranging from 0.19 to
0.69 (as shown in Table 1). The only category
where a significant difference was found was on
the questionnaire items that asked the subjects to
assess the robot?s quality as a conversational part-
ner; for those items, the mean score from sub-
jects who heard the adaptive references was sig-
nificantly higher (p < 0.05) than the mean score
from the subjects who heard references generated
by the constant reference module. Of the six ques-
Table 2: Objective results (all differences n.s.)
Measure Constant Adaptive M-W
Duration (s.) 404.3 (62.8) 410.5 (94.6) p = 0.90
Duration (turns) 29.8 (5.02) 31.2 (5.57) p = 0.44
Rep requests 0.26 (0.45) 0.32 (0.78) p = 0.68
Explanations 2.21 (0.63) 2.41 (0.80) p = 0.44
Successful trials 1.58 (0.61) 1.55 (0.74) p = 0.93
tions that were related to the conversation quality,
the most significant impact was on the two ques-
tions which assessed the subjects? understanding
of what they were able to do at various points dur-
ing the interaction.
5.2 Objective Measures
Based on the log files and video recordings, we
computed a range of objective measures. These
measures were divided into three classes, based
on those used in the PARADISE dialogue-system
evaluation framework (Walker et al, 2000):
? Two dialogue efficiency measures: the mean
duration of the interaction as measured both
in seconds and in system turns;
? Two dialogue quality measures: the number
of times that the robot gave explanations, and
the number of times that the user asked for
instructions to be repeated; and
? One task success measure: how many of the
(two) target objects were constructed as in-
tended (i.e., as shown in Figure 3).
For each of these measures, we tested whether the
difference in reference strategy had a significant
effect, again via a Mann-Whitney test. Table 2 il-
lustrates the results on these objective measures,
divided by the reference strategy.
The results from the two groups of subjects
were very similar on all of these measures: on
average, the experiment took 404 seconds (nearly
seven minutes) to complete with the constant strat-
egy and 410 seconds with the adaptive, the mean
number of system turns was close to 30 in both
cases, just over one-quarter of all subjects asked
for instructions to be repeated, the robot gave just
over two explanations per trial, and about three-
quarters of all target objects (i.e. 1.5 out of 2)
were correctly built. The Mann-Whitney test con-
firms that none of the differences between the two
groups even came close to significance on any of
the objective measures.
5.3 Comparing Objective and Subjective
Measures
In the preceding sections, we presented results on
a number of objective and subjective measures.
While the subjects generally rated their experi-
ence of using the system positively, there was
some degree of variation, most of which could not
be attributed to the difference in reference strat-
egy. Also, the results on the objective measures
varied widely across the subjects, but again were
not generally affected by the reference strategy.
In this section, we examine the relationship be-
tween these two classes of measures in order to
determine which of the objective measures had the
largest effect on users? subjective reactions to the
HRI system.
Being able to predict subjective user satisfac-
tion from more easily-measured objective proper-
ties can be very useful for developers of interac-
tive systems: in addition to making it possible to
evaluate systems based on automatically available
data without the need for extensive experiments
with users, such a performance function can also
be used in an online, incremental manner to adapt
system behaviour to avoid entering a state that is
likely to reduce user satisfaction (Litman and Pan,
2002), or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
We employed the procedure used in the PAR-
ADISE evaluation framework (Walker et al,
2000) to explore the relationship between the sub-
jective and objective factors. The PARADISE
model uses stepwise multiple linear regression to
predict subjective user satisfaction based on mea-
sures representing the performance dimensions of
task success, dialogue quality, and dialogue effi-
ciency, resulting in a predictor function of the fol-
lowing form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Table 3 shows the predictor functions that were
derived for each of the classes of subjective mea-
sures in this study, using all of the objective mea-
sures from Table 2 as initial factors. The R2 col-
umn indicates the percentage of the variance in the
target measure that is explained by the predictor
function, while the Significance column gives sig-
nificance values for each term in the function.
In general, the two factors with the biggest in-
fluence on user satisfaction were the number of
repetition requests (which had a uniformly neg-
ative effect on user satisfaction), and the num-
ber of target objects correctly built by the user
(which generally had a positive effect). Aside
from the questions on user feelings, the R2 values
are generally in line with those found in previous
PARADISE evaluations of other dialogue systems
(Walker et al, 2000; Litman and Pan, 2002), and
in fact are much higher than those found in a pre-
vious similar study (Foster et al, 2009b).
6 Discussion
The subjective responses on the relevant items
from the usability questionnaire suggest that
the subjects perceived the robot to be a bet-
ter conversational partner if it used contextually
varied, situationally-appropriate referring expres-
sions than if it always used a baseline, constant
strategy; this supports the main prediction for this
study. The result also agrees with the findings of
a previous study (Foster et al, 2009a)?this sys-
tem did not incorporate goal inference and had a
less-sophisticated reference strategy, but the main
effect of changing reference strategy was also on
the users? subjective opinions of the robot?s inter-
active ability. These studies together support the
current effort in the natural-language generation
community to devise more sophisticated reference
generation algorithms.
On the other hand, there was no significant dif-
ference between the two groups on any of the
objective measures: the dialogue efficiency, dia-
logue quality, and task success were nearly iden-
tical across the two groups of subjects. A de-
tailed analysis of the subjects? gaze and object-
manipulation behaviour immediately after various
forms of generated references from the robot also
failed to find any significant differences between
the various reference types. These overall results
are not particularly surprising: studies of human-
human dialogue in a similar joint construction task
(Bard et al, In prep.) have demonstrated that the
collaborators preserve quality of construction in
Table 3: PARADISE predictor functions for each category on the usability questionnaire
Measure Function R2 Significance
Intelligence 76.8+7.00?N (Correct)?5.51?N (Repeats) 0.39 Correct: p < 0.001,
Repeats: p < 0.005
Task 72.4+3.54?N (Correct)?3.45?N (Repeats)?2.17?N (Explain) 0.43 Correct: p < 0.005,
Repeats: p < 0.01,
Explain: p? 0.10
Feeling 66.9?6.54?N (Repeats)+4.28?N (Seconds) 0.09 Repeats: p < 0.05,
Seconds: p? 0.12
Conversation 71.0+5.28?N (Correct)?3.08?N (Repeats) 0.20 Correct: p < 0.01,
Repeats: p? 0.10
Overall 72.0+4.80?N (Correct)?4.27?N (Repeats) 0.40 Correct: p < 0.001,
Repeats: p < 0.005
all cases, though circumstances may dictate what
strategies they use to do this. Combined with the
subjective findings, this lack of an objective effect
suggests that the references generated by the adap-
tive strategy were both sufficient and more natural
than those generated by the constant strategy.
The analysis of the relationship between the
subjective and objective measures analysis has
also confirmed and extended the findings from a
similar analysis (Foster et al, 2009b). In that
study, the main contributors to user satisfaction
were user repetition requests (negative), task suc-
cess, and dialogue length (both positive). In the
current study, the primary factors were similar,
although dialogue length was less prominent as
a factor and task success was more prominent.
These findings are generally intuitive: subjects
who are able to complete the joint construction
task are clearly having more successful interac-
tions than those who are not able to complete the
task, while subjects who need to ask for instruc-
tions to be repeated are equally clearly not hav-
ing successful interactions. The findings add ev-
idence that, in this sort of task-based, embodied
dialogue system, users enjoy the experience more
when they are able to complete the task success-
fully and are able to understand the spoken contri-
butions of their partner, and also suggest that de-
signers should concentrate on these aspects of the
interaction when designing the system.
7 Conclusions
We have presented the reference generation mod-
ule of a hybrid human-robot interaction system
that combines a goal-inference component based
on sub-symbolic dynamic neural fields with a
natural-language interface based on more tradi-
tional symbolic techniques. This combination of
approaches results in a system that is able to work
together with a human partner on a mutual con-
struction task, interpreting its partner?s verbal and
non-verbal behaviour and responding appropri-
ately to unexpected actions (errors) of the partner.
We have then described a user evaluation of this
system, concentrating on the impact of different
techniques for generating situated references in
the context of the robot?s corrective feedback. The
results of this study indicate that using an adaptive
strategy to generate the references significantly in-
creases the users? opinion of the robot as a con-
versational partner, without having any effect on
any of the other measures. This result agrees with
the findings of the system evaluation described in
(Foster et al, 2009a), and adds evidence that so-
phisticated generation techniques are able to im-
prove users? experiences with interactive systems.
An analysis of the relationship between the ob-
jective and subjective measures found that the
main contributors to user satisfaction were the
users? task performance (which had a positive ef-
fect on most measures of satisfaction), and the
number of times the users had to ask for instruc-
tions to be repeated (which had a generally neg-
ative effect). Again, these results agree with the
findings of a previous study (Foster et al, 2009b),
and also suggest priorities for designers of this
type of task-based interactive system.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka and Levent Kent
for help in running the experiment and analysing
the data.
2http://www.jast-project.eu/
3http://www.ics.forth.gr/indigo/
References
E. G. Bard, R. Hill, and M. E. Foster. 2008. What
tunes accessibility of referring expressions in
task-related dialogue? In Proceedings of the
30th Annual Meeting of the Cognitive Science
Society (CogSci 2008). Chicago.
E. G. Bard, R. L. Hill, M. E. Foster, and M. Arai.
In prep. How do we tune accessibility in joint
tasks: Roles and regulations.
H. Bekkering, E.R.A. de Bruijn, R.H. Cuijpers,
R. Newman-Norlund, H.T. van Schie, and
R. Meulenbroek. 2009. Joint action: Neurocog-
nitive mechanisms supporting human interac-
tion. Topics in Cognitive Science, 1(2):340?
352.
E. Bicho, L. Louro, N. Hipolito, and W. Erlhagen.
2009. A dynamic field approach to goal infer-
ence and error monitoring for human-robot in-
teraction. In Proceedings of the Symposium on
?New Frontiers in Human-Robot Interaction?,
AISB 2009 Convention. Heriot-Watt University
Edinburgh.
H. H. Clark. 1996. Using Language. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(2):233?263.
W. Erlhagen and E. Bicho. 2006. The dynamic
neural field approach to cognitive robotics.
Journal of Neural Engineering, 3(3):R36?R54.
M. E. Foster, E. G. Bard, R. L. Hill, M. Guhe,
J. Oberlander, and A. Knoll. 2008a. The roles
of haptic-ostensive referring expressions in co-
operative, task-based human-robot dialogue. In
Proceedings of HRI 2008.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009a. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI-09.
M. E. Foster, M. Giuliani, and A. Knoll. 2009b.
Comparing objective and subjective measures
of usability in a human-robot dialogue system.
In Proceedings of ACL-IJCNLP 2009.
M. E. Foster, M. Giuliani, T. Mu?ller, M. Rickert,
A. Knoll, W. Erlhagen, E. Bicho, N. Hipo?lito,
and L. Louro. 2008b. Combining goal inference
and natural-language dialogue for human-robot
joint action. In Proceedings of the 1st Interna-
tional Workshop on Combinations of Intelligent
Methods and Applications at ECAI 2008.
M. Giuliani and A. Knoll. 2008. MultiML:
A general-purpose representation language for
multimodal human utterances. In Proceedings
of ICMI 2008.
J. D. Kelleher and G.-J. M. Kruijff. 2006. Incre-
mental generation of spatial referring expres-
sions in situated dialog. In Proceedings of
COLING-ACL 2006.
A. Kranstedt and I. Wachsmuth. 2005. Incremen-
tal generation of multimodal deixis referring to
objects. In Proceedings of ENLG 2005.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6(3&4):323?340.
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system.
User Modeling and User-Adapted Interaction,
12(2?3):111?137.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of AISB 2005
Creative Robotics Symposium.
I. F. van der Sluis. 2005. Multimodal Reference:
Studies in Automatic Generation of Multimodal
Referring Expressions. Ph.D. thesis, University
of Tilburg.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. White. 2006. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation, 4(1):39?75.
Proceedings of the SIGDIAL 2013 Conference, pages 223?232,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Training and evaluation of an MDP model for social multi-user
human-robot interaction
Simon Keizer, Mary Ellen Foster,
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh (UK)
{s.keizer,m.e.foster,o.lemon}@hw.ac.uk
Andre Gaschler, Manuel Giuliani
fortiss GmbH
Munich (Germany)
{gaschler,giuliani}@fortiss.org
Abstract
This paper describes a new approach to
automatic learning of strategies for social
multi-user human-robot interaction. Us-
ing the example of a robot bartender that
tracks multiple customers, takes their or-
ders, and serves drinks, we propose a
model consisting of a Social State Recog-
niser (SSR) which processes audio-visual
input and maintains a model of the social
state, together with a Social Skills Execu-
tor (SSE) which takes social state updates
from the SSR as input and generates robot
responses as output. The SSE is modelled
as two connected Markov Decision Pro-
cesses (MDPs) with action selection poli-
cies that are jointly optimised in interaction
with a Multi-User Simulation Environment
(MUSE). The SSR and SSE have been in-
tegrated in the robot bartender system and
evaluated with human users in hand-coded
and trained SSE policy variants. The re-
sults indicate that the trained policy out-
performed the hand-coded policy in terms
of both subjective (+18%) and objective
(+10.5%) task success.
1 Introduction
As the use of robot technology in the home as well
as in public spaces is increasingly gaining attention,
the need for effective and robust models for natural
and social human robot interaction becomes more
important. Whether it involves robot companions
(Vardoulakis et al, 2012), game-playing robots
(Klotz et al, 2011; Brooks et al, 2012; Cuaya?huitl
and Kruijff-Korbayova?, 2012), or robots that help
people with exercising (Fasola and Mataric, 2013),
human users should be able to interact with such
service robots in an effective and natural way, us-
ing speech as well as other modalities of commu-
nication. Furthermore, with the emergence of new
application domains there is a particular need for
methods that enable rapid development of mod-
els for such new domains. In this respect, data-
driven approaches are appealing for their capability
to automatically exploit empirical data to arrive at
realistic and effective models for interpreting user
behaviour, as well as to learn strategies for effective
system behaviour.
In spoken dialogue systems research, statisti-
cal methods for spoken language understanding,
dialogue management, and natural language gen-
eration have proven to be feasible for effective
and robust interactive systems (Rieser and Lemon,
2011; Lemon and Pietquin, 2012; Young et al,
2010; Young et al, 2013). Although such methods
have recently also been applied to (multi-modal)
human-robot interaction (Stiefelhagen et al, 2007;
Cuaya?huitl et al, 2012), work on multi-user human-
robot interaction has been limited to non-statistical,
hand-coded models (Klotz et al, 2011).
On the other hand, substantial work has been
done in the field of situated multi-party interaction
in general, including data-driven approaches. In
particular, Bohus & Horvitz (2009) have addressed
the task of recognising engagement intentions using
online learning in the setting of a screen-based em-
bodied virtual receptionist, and have also worked
on multi-party turn-taking in this context (Bohus
and Horvitz, 2011).
In this paper we describe a statistical approach
to automatic learning of strategies for selecting ef-
fective as well as socially appropriate robot actions
in a multi-user context. The approach has been de-
veloped using the example of a robot bartender (see
Figure 1) that tracks multiple customers, takes their
orders, and serves drinks. We propose a model con-
sisting of a Social State Recogniser (SSR) which
processes audio-visual input and maintains a model
of the social state, and a Social Skills Executor
(SSE) which takes social state updates from the
SSR as input and generates robot responses as out-
223
put. The SSE is modelled as a hierarchy of two con-
nected Markov Decision Processes (MDPs) with
action selection policies that are jointly optimised
in interaction with a Multi-User Simulation Envi-
ronment (MUSE).
Figure 1: The robot bartender with two customers
In the remainder of this paper we will describe
the robot system in more detail (Section 2), fol-
lowed by descriptions of the SSR (Section 3), the
SSE (Section 4), and MUSE (Section 5). In Sec-
tion 6 we then discuss in more detail the MDP
model for the SSE and the process of jointly opti-
mising the policies, and present evaluation results
on simulated data. Next, we present results of the
first evaluation of the integrated SSE-MDP compo-
nent with human users (Section 7). The paper is
concluded in Section 8.
2 Robot bartender system
The robot system we used for evaluating the models
is equipped with vision and speech input processing
modules, as well as modules controlling two robot
arms and a talking head. Based on observations
about the users in the scene and their behaviour, the
system must maintain a model of the social context,
and decide on effective and socially appropriate
responses in that context. Such a system must be
able to engage in, maintain, and close interactions
with users, take a user?s order by means of a spoken
conversation, and serve their drinks. The overall
aim is to generate interactive behaviour that is both
task- effective and socially appropriate: in addition
to efficiently taking orders and serving drinks, the
system should, e.g., deal with customers on a first-
come, first-served basis, and should manage the
customers? patience by asking them politely to wait
until the robot is done serving another customer.
As shown in Figure 1, the robot hardware con-
sists of a pair of manipulator arms with grippers,
mounted to resemble human arms, along with
an animatronic talking head capable of produc-
ing facial expressions, rigid head motion, and lip-
synchronised synthesised speech. The input sen-
sors include a vision system which tracks the loca-
tion, facial expressions, gaze behaviour, and body
language of all people in the scene in real time
(Pateraki et al, 2013), along with a linguistic pro-
cessing system (Petrick et al, 2012) combining a
speech recogniser with a natural-language parser
to create symbolic representations of the speech
produced by all users. More details of the architec-
ture and components are provided in (Foster et al,
2012). An alternative embodiment of the system is
also available on the NAO platform.
3 Social State Recogniser
The primary role of the Social State Recogniser
(SSR) is to turn the continuous stream of messages
produced by the low-level input and output com-
ponents of the system into a discrete representa-
tion of the world, the robot, and all entities in the
scene, integrating social, interaction-based, and
task-based properties. The state is modelled as a
set of relations such as facePos(A)=(x,y,z) or
closeToBar(A); see (Petrick and Foster, 2013)
for details on the representation used.
In addition to storing all of the low-level sensor
information, the SSR also infers additional rela-
tions that are not directly reported by the sensors.
For example, it fuses information from vision and
speech to determine which user should be assigned
to a recognised spoken contribution. It also pro-
vides a constant estimate of whether each customer
is currently seeking attention from the bartender
(seeksAttention(A)): the initial version of this
estimator used a hand-coded rule based on the ob-
servation of human behaviour in real bars (Huth
et al, 2012), while a later version (Foster, 2013)
makes use of a supervised learning classifier trained
on labelled recordings of humans interacting with
the first version of the robot bartender.
The SSR provides a query interface to allow
other system components access to the relations
stored in the state, and also publishes an updated
state to the SSE every time there is a change which
might require a system action in response (e.g.,
a customer appears, begins seeking attention, or
makes a drink order).
224
4 Social Skills Executor
The Social Skills Executor (SSE) controls the be-
haviour of the robot system, based on the social
state updates it receives from the SSR. The out-
put of the SSE consists of a combination of non-
communicative robot actions and/or communica-
tive actions with descriptions of their multi-modal
realisations. In the bartender domain, the non-
communicative actions typically involve serving
a specific drink to a specific user, whereas the com-
municative actions have the form of dialogue acts
(Bunt et al, 2010), directed at a specific user, e.g.
setQuestion(drink) (?What would you like to
drink??) or initialGreeting() (?Hello?).
In our design of the SSE, the decision making
process resulting in such outputs (including the ?no
action? output) consists of three stages: 1) social
multi-user coordination: managing the system?s
engagement with the users present in the scene (e.g.,
accept a user?s bid for attention, or proceed with an
engaged user), 2) single-user interaction: if pro-
ceeding with an engaged user, generating a high-
level response to that user, in the form of a com-
municative act or physical action (e.g., greeting the
user or serving him a drink), and 3) multi-modal
fission: selecting a combination of modalities for
realising a chosen response (e.g., a greeting can be
realised through speech and/or a nodding gesture).
One advantage of such a hierarchical design is that
strategies for the different stages can be developed
independently. Another is that it makes automatic
policy optimisation more scalable.
5 Multi-User Simulated Environment
In order to test and evaluate the SSE, as well as to
train SSE action selection policies, we developed
a Multi-User Simulated Environment (MUSE).
MUSE allows for rapidly exploring the large space
of possible states in which the SSE must select
actions. A reward function that incorporates in-
dividual rewards from all simulated users in the
environment is used to encode preferred system
behaviour in a principled way. A simulated user
assigns a reward if they are served the correct drink,
and gives penalties associated with their waiting
time and various other forms of undesired system
responses (see Section 6.1 for more details about
the reward function). All of this provides a practi-
cal platform for evaluating different strategies for
effective and socially appropriate behaviour. It also
paves the way for automatic optimisation of poli-
cies, for example by using reinforcement learning
techniques, as we will discuss in Section 6.1.
The simulated environment replaces the vision
and speech processing modules in the actual robot
bartender system, which means that it generates 1)
vision signals in every time-frame, and 2) speech
processing results, corresponding to sequences of
time-frames where a user spoke. The vision obser-
vations contain information about users that have
been detected, where they are in the scene, whether
they are speaking, and where their attention is di-
rected to. Speech processing results are represented
semantically, in the form of dialogue acts (e.g.,
inform(drink=coke), ?I would like a coke?). As
described in Section 3, the SSR fuses the vision and
speech input, for example to associate an incoming
dialogue act with a particular user.
The simulated signals are the result of combin-
ing the output from the simulated users in the en-
vironment. Each simulated user is initialised with
a random goal (in our domain a type of drink they
want to order), enters the scene at some point, and
starts bidding for attention at some point. Each
simulated user also maintains a state and gener-
ates responses given that state. These responses
include communicative actions directed at the bar-
tender, which are translated into a multi-channel
vision input stream processed by the SSR, and, in
case the user realises the action through speech,
a speech processing event after the user has fin-
ished speaking. Additionally, the simulated users
start with a given patience level, which is reduced
in every frame that the user is bidding for atten-
tion or being served by the system. If a user?s pa-
tience has reduced to zero, s/he gives up and leaves
the bar. However, it is increased by a given fixed
amount when the system politely asks the user to
wait, encoded as a pausing dialogue act. The be-
haviour of the simulated users is partly controlled
by a set of probability distributions that allow for
a certain degree of variation. These distributions
have been informed by statistics derived from a
corpus of human-human customer-bartender inter-
actions (Huth et al, 2012).
In addition to information about the simulated
users, MUSE also provides feedback about the
execution of robot actions to the SSR, in partic-
ular the start and end of all robot speech and non-
communicative robot actions. This type of informa-
tion simulates the feedback that is also provided in
the actual bartender system by the components that
directly control the robot head and arms. Figure 2
225
Figure 2: Social state recognition and social skills execution in a multi-user simulated environment.
shows the architecture of the system interacting
with the simulated environment.
6 MDP model for multi-user interaction
To enable automatic optimisation of strategies for
multi-user social interaction, the SSE model as de-
scribed in Section 4 was cast as a hierarchy of two
Markov Decision Processes (MDPs), correspond-
ing to the social multi-user coordination and single-
user interaction stages of decision making. Both
MDPs have their own state spaces S1 and S2, each
defined by a set of state features, extracted from
the estimated social state made available by the
SSR?see Tables 1 and 3. They also have their own
action setsA1 andA2, corresponding to the range
of decisions that can be made at the two stages (Ta-
bles 2 and 4), and two policies pi1 : S1 ? A1 and
pi2 : S2 ? A2, mapping states to actions.
6.1 Policy optimisation
Using the MDP model as described above, we
jointly optimise the two policies, based on the re-
wards received through the SSR from the simulated
environment MUSE. Since MUSE gives rewards
on a frame-by-frame basis, they are accumulated
in the social state until the SSR publishes a state
update. The SSE stores the accumulated reward
together with the last state encountered and action
taken in that state, after which that reward is reset
in the social state. After each session (involving
interactions with two users in our case), the set
of encountered state-action pairs and associated
rewards is used to update the policies.
The reward provided by MUSE in each frame
is the sum of rewards Ri given by each individual
simulated user i, and a number of general penalties
arising from the environment as a whole. User
rewards consist of a fixed reward in case their goal
is satisfied (i.e., when they have been served the
drink they wanted and ordered), a penalty in case
they are still waiting to be served, a penalty in case
they are engaged with the system but have not been
served their drink yet, and additional penalties, for
example when the system turns his attention to
another user when the user is still talking to it, or
when the system serves a drink before the user has
ordered, or when the system serves another drink
when the user already has been served their drink.
General penalties are given for example when the
system is talking while no users are present.
The policies are encoded as functions that assign
a value to each state-action pair; these so-called
Q-values are estimates of the long-term discounted
cumulative reward. Given the current state, the
policy selects the action with the highest Q-value:
pi(s) = arg max
a
Q(s, a) (1)
Using a Monte-Carlo Control algorithm (Sutton
and Barto, 1998), the policies are optimised by
running the SSR and SSE against MUSE and using
the received reward signal to update the Q-values
after each interaction sequence. During training,
the SSE uses an -greedy policy, i.e., it takes a
random exploration action with probability  = 0.2.
226
Index Feature Values
4 ? i Interaction status for user i + 1 nonEngaged/seeksAttention/engaged
4 ? i + 1 Location of user i + 1 notPresent/!closeToBar/closeToBar
4 ? i + 2 User i + 1 was served a drink no/yes
4 ? i + 3 User i + 1 asked to wait no/yes
Table 1: State features for the social multi-user coordination policy. For each user, 4 features are included
in the state space, resulting in 32 ? 22 = 36 states for interactions with up to 1 user, increasing to 1296
states for interactions with up to 2 users and 46, 656 states for up to 3 users.
Index Action
0 No action
3 ? i + 1 Ask user i + 1 to wait
3 ? i + 2 Accept bid for attention from user i + 1
3 ? i + 3 Proceed interaction with (engaged) user i + 1
Table 2: Actions for the social multi-user coordination policy.
In the policy update step, a discount factor ? = 0.95
is used, which controls the impact that rewards
received later in a session have on the value of state-
action pairs encountered earlier in that session.
Figure 3 shows the learning curve of a joint
policy optimisation, showing average rewards ob-
tained after running the SSE with trained policies
for 500 runs, at several stages of the optimisation
process (after every 2500 sessions/runs/iterations,
the trained policy was saved for evaluation). In this
particular setup, simulated users gave a reward of
550 upon goal completion but in the total score this
is reduced considerably due to waiting time (-2 per
frame), task completion time (-1 per frame) and
various other potential penalties. Also indicated
are the performance levels of two hand-coded SSE
policies, one of which uses a strategy of asking a
user to wait when already engaged with another
user (labelled HDC), and one in which that second
user is ignored until it is done with the engaged user
(labelled HDCnp). The settings for user patience
as discussed in Section 5 determine which of these
policies works best; ideally these settings should be
derived from data if available. Nevertheless, even
with the hand-coded patience settings, the learning
curve indicates that both policies are outperformed
in simulation after 10k iterations, suggesting that
the best strategy for managing user patience can be
found automatically.
7 Human user evaluation
The SSE described above has been integrated in
the full robot bartender system and evaluated for
the first time with human users. In the experiment,
both a hand-coded version and a trained version
of the SSE component were tested; see Table 6 in
Appendix A for the trajectory of state-action pairs
of an example session. The hand-coded version
uses the policy labelled HDC, not HDCnp (see
Section 6.1). In each of the sessions carried out, one
recruited subject and one confederate (one of the
experimenters) approached the bartender together
as clients and both tried to order a drink (coke or
lemonade). After each interaction, the subject filled
out the short questionnaire shown in Figure 4.
Q1: Did you successfully order a drink from the bartender?
[Y/N]
Please state your opinion on the following statements:
[ 1:strongly disagree; 2:disagree; 3:slightly disagree;
4:slightly agree; 5:agree; 6:strongly agree ]
Q2: It was easy to attract the bartender?s attention [1?6]
Q3: The bartender understood me well [1?6]
Q4: The interaction with the bartender felt natural [1?6]
Q5: Overall, I was happy about the interaction [1?6]
Figure 4: Questionnaire from the user study.
37 subjects took part in this study, resulting in a
total of 58 recorded drink-ordering interactions:
29 that used the hand-coded SSE for interaction
management, and 29 that used the trained SSE.
The results from the experiment are summarised
in Table 5. We analysed the results using a linear
mixed model, treating the SSE policy as a fixed fac-
tor and the subject ID as a random factor. Overall,
the pattern of the subjective scores suggests a slight
preference for the trained SSE version, although
227
Index Feature Values
0 Reactive pressure none/thanking/greeting/goodbye/apology
1 Status of user goal unknown/usrInf/sysExpConf/sysImpConf/
grounded/drinkServed/sysAsked
2 Own proc. state none/badASR
Table 3: State features for the single-user interaction policy. In this case, there are 5 ? 7 ? 2 = 70 states.
Index Action Example
0 No action
1 returnGreeting() ?Hello?
2 autoPositive() ?Okay?
3 acceptThanking() ?You?re welcome?
4 autoNegative() ?What did you say??
5 setQuestion(drink) ?What drink would you like??
6 acceptRequest(drink=x) + serveDrink(x) ?Here?s your coke?
Table 4: Actions for the single-user interaction policy, which correspond to possible dialogue acts, except
for ?no action? and serving a drink. The specific drink types required for two of the actions are extracted
from the fully specified user goal in the social state maintained by the SSR.
only the difference in perceived success was statis-
tically significant at the p < 0.05 level. The actual
success rate of the trained policy was also some-
what higher, although not significantly so. Also,
the interactions with the trained SSE took slightly
longer than the ones with the hand-coded SSE in
terms of the number of system turns (i.e., the num-
ber of times the SSE receives a state update and
selects a response action, excluding the times when
it selects a non-action); however, this did not have
any overall effect on the users? subjective ratings.
The higher success rate for the trained SSE could
be partly explained by the fact that fewer ASR prob-
lems were encountered when using this version;
however, since the SSE was not triggered when a
turn was discarded due to low-confidence ASR, this
would not have had an effect on the number of sys-
tem turns. There was another difference between
the hand-coded and trained policies that could have
affected both the success rate and the number of
system turns: for interactions in which a user has
not ordered yet, nor been asked for their order, the
hand-coded strategy randomly chooses between
asking the user for their order and doing nothing,
letting the user take the initiative to place the order,
whereas the trained policy always asks the user for
their order (this action has the highest Q-value, al-
though in fact the value for doing nothing in such
cases is also relatively high).
We also carried out a stepwise multiple linear
regression on the data from the user experiment
to determine which of the objective measures had
the largest effect, as suggested by the PARADISE
evaluation framework (Walker et al, 2000). The re-
sulting regression functions are shown in Figure 5.
In summary, all of the subjective responses were
significantly affected by the objective task success
(i.e., the number of drinks served); the number of
low-ASR turns also affected most of the responses,
while various measures of dialogue efficiency (such
as the system response time and the time taken to
serve drinks) also had a significant impact. In gen-
eral, these regression functions explain between
15?25% of the variance in the subjective measures.
As an initial analysis of the validity of the sim-
ulated environment, we compared the state distri-
bution of the simulated data accumulated during
policy optimisation with that of the human user
evaluation data. In terms of coverage, we found
that only 46% of all states encountered in the real
data were also encountered during training. How-
ever, many of these states do not occur very often
and many of them do not require any action by
the robot (a trained policy can easily be set to take
no-action for unseen states). If we only include
states that have been encountered at least 20 times,
the coverage increases to over 70%. For states en-
countered at least 58 times, the coverage is 100%,
though admittedly this covers only the 10 most
frequently encountered states. The similarity of
the two distributions can be quantified by comput-
ing the KL-divergence, but since such a number is
228
Figure 3: Learning curve for joint optimisation of SSE-MDP policies.
System NS PSucc* PAtt PUnd PNat POv NDSrvd NST NBAsr
SSE-TRA 29 97% 4.10 4.21 3.00 3.83 1.97 (98.5%) 7.38 3.14
SSE-HDC 29 79% 4.14 3.83 2.93 3.83 1.76 (88.0%) 6.86 3.82
TOTAL 58 88% 4.12 4.02 2.97 3.83 1.86 (93.0%) 7.12 3.48
Table 5: Overview of system performance results from the experiment. In the leftmost column SSE-TRA
and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of
sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived
understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average
scores resulting from the 5 respective questionnaire questions; NDSrvd indicates the average number of
drinks served per session (out of 2 maximum ? the percentage is given in brackets); NST indicates the
average number of system turns per session; while NBAsr indicates the average number of cases where
the user speech was ignored because the ASR confidence was below a predefined threshold. The marked
column indicates that the difference between the two SSE versions was significant at the p < 0.05 level.
hard to interpret in itself, this will only be useful
if there were a state distribution from an alterna-
tive simulator or an improved version of MUSE for
comparison.
8 Conclusion
In this paper we presented a new approach to au-
tomatic learning of strategies for social multi-user
human-robot interaction, demonstrated using the
example of a robot bartender that tracks multiple
customers, takes their orders, and serves drinks.
We presented a model consisting of a Social State
Recogniser (SSR) which processes audio-visual in-
put and maintains a model of the social state, and
a Social Skills Executor (SSE) which takes social
state updates from the SSR as input and generates
robot responses as output. The main contribution
of this work has been a new MDP-based model
for the SSE, incorporating two connected MDPs
with action selection policies that are jointly op-
timised in interaction with a Multi-User Simula-
tion Environment (MUSE). In addition to showing
promising evaluation results with simulated data,
we also presented results from a first evaluation of
the SSE component with human users. The experi-
ments showed that the integrated SSE component
worked quite well, and that the trained SSE-MDP
achieved higher subjective and objective success
rates (+18% and +10.5% respectively).
Our model currently only utilises two policies,
but in more complex scenarios the task could be
further modularised and extended by introducing
more MDPs, for example for multimodal fission
and natural language generation. The approach of
using a hierarchy of MDPs has some similarity with
the Hierarchical Reinforcement Learning (HRL)
approach which uses a hierarchy of Semi-Markov
Decision Processes (SMDPs). In (Cuaya?huitl et al,
229
PSucc = 0.88 + 0.14 ? N(NDSrvd) ? 0.07 ? N(NBAsr) (r2 = 0.21)
PAtt = 4.12 + 0.76 ? N(NDSrvd) ? 0.46 ? N(RTm) ? 0.38 ? N(FDTm) (r2 = 0.22)
PUnd = 4.02 + 0.41 ? N(NDSrvd) ? 0.36 ? N(NBAsr) ? 0.40 ? N(NST) ? 0.41 ? N(RTm) ? 0.39 ? N(STm) (r2 = 0.24)
PNat = 2.97 + 0.36 ? N(NDSrvd) ? 0.29 ? N(NBAsr) ? 0.31 ? N(NST) ? 0.44 ? N(RTm) (r2 = 0.16)
POv = 3.83 + 0.65 ? N(NDSrvd) ? 0.38 ? N(NBAsr) ? 0.52 ? N(RTm) (r2 = 0.24)
Figure 5: PARADISE regression functions from the user study. The labels are the same as those in Table 5,
with the following additions: RTm is the mean system response time per user, STm is the mean serving
time per user, and FDTm is the mean time to serve the first drink; all times are measured in milliseconds.
N represents a Z score normalisation function (Cohen, 1995).
2012) for example, this hierarchy is motivated by
the identification of multiple tasks that the robot
can carry out and for which multiple SMDP agents
are defined. In every step of the interaction, control
lies with a single SMDP agent somewhere in the
hierarchy; once it arrives at its final state it returns
control to its parent SMDP. An additional transi-
tion model is introduced to permit switching from
an incomplete SMDP to another SMDP at the same
level, making interactions more flexible. In our ap-
proach, control always starts at the top level MDP
and lower level MDPs are triggered depending on
the action taken by their parent MDP. For social
interaction with multiple users, flexible switching
between interactions with different users is impor-
tant, so an arguably more sophisticated HRL ap-
proach to multi-user interaction will rely heavily
on the transition model. Another approach to mod-
ularising the task domain through multiple policies
is described in (Lison, 2011), where ?meta-control?
of the policies relies on an activation vector. As in
the HRL SMDP approach, this approach has not
been applied in the context of multi-user interaction.
In any case, a more thorough and possibly experi-
mental analysis comparing our approach with these
other approaches would be worth investigating.
In the future, we plan to extend our MDP model
to a POMDP (Partially Observable MDP) model,
taking uncertainty about both speech and visual
input into account in the optimisation of SSE poli-
cies by incorporating alternative hypotheses and
confidence scores provided by the input modules
into the social state. Since hand-coding strategies
becomes more challenging in the face of increased
uncertainty due to noisy input, the appeal of auto-
matic strategy learning in a POMDP framework
becomes even stronger. In a previous offline ver-
sion of our combined SSR and SSE, we have shown
in preliminary simulation experiments that even in
an MDP setting, an automatically trained SSE pol-
icy outperforms a hand-coded policy when noise is
added to the speech channel (Keizer et al, 2013).
Another direction of research is to annotate the
data collected in the described experiment for fur-
ther analysis and use it to improve the features of
the simulated environment. The improved models
should lead to trained policies that perform better
when evaluated again with human users. We will
also make use of the findings of the PARADISE
regression to fine-tune the reward function used
for policy optimisation: note that two of the main
features indicated by the PARADISE procedure?
task success and dialogue efficiency?are already
those included in the current reward function, and
we will add a feature to account for the effects of
ASR performance. We are also considering using
collected data for direct supervised or off-policy
reinforcement learning of SSE strategies.
Finally, we aim to extend our domain both in
terms of interactive capabilities (e.g., handling com-
munication problems, social obligations manage-
ment, turn-taking) and task domain (e.g., handling
more than the current maximum of 2 users, group
orders, orders with multiple items). In order to
make the (PO)MDP model more scalable and thus
keeping the learning algorithms tractable, we also
aim to incorporate techniques such as value func-
tion approximation into our model.
Acknowledgments
The research leading to these results has received
funding from the European Union?s Seventh Frame-
work Programme (FP7/2007?2013) under grant
agreement no. 270435, JAMES: Joint Action for
Multimodal Embodied Social Systems, http://
james-project.eu/. Thanks to Ingmar Kessler
for help in running the user experiment.
230
References
Dan Bohus and Eric Horvitz. 2009. Learning to pre-
dict engagement with a spoken dialog system in
open-world settings. In Proceedings SIGdial, Lon-
don, UK.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn
taking in situated dialog: Study, lessons, and direc-
tions. In Proceedings SIGdial, Portland, OR.
A. Brooks, J. Gray, G. Hoffman, A. Lockerd, H. Lee,
and C. Breazeal. 2012. Robot?s play: Interactive
games with sociable machines. Computers in Enter-
tainment, 2(3).
H. Bunt, J. Alexandersson, J. Carletta, J.-W. Choe, A.C.
Fang, K. Hasida, K. Lee, V. Petukhova, A. Popescu-
Belis, L. Romary, C. Soria, and D. Traum. 2010.
Towards an ISO standard for dialogue act annotation.
In Proceedings LREC, Valletta, Malta.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Heriberto Cuaya?huitl and Ivana Kruijff-Korbayova?.
2012. An interactive humanoid robot exhibiting flex-
ible sub-dialogues. In Proceedings NAACL HLT,
Montreal, Canada.
H. Cuaya?huitl, I. Kruijff-Korbayova?, and N. Dethlefs.
2012. Hierarchical dialogue policy learning using
flexible state transitions and linear function approxi-
mation. In Proceedings COLING, Mumbai, India.
Juan Fasola and Maja J. Mataric. 2013. A socially as-
sistive robot exercise coach for the elderly. Journal
of Human Robot Interaction, 2(3). To appear.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings ICMI, Santa Monica, CA.
Mary Ellen Foster. 2013. How can I help you?
Comparing engagement classification strategies for
a robot bartender. Submitted.
K. Huth, S. Loth, and J.P. De Ruiter. 2012. Insights
from the bar: A model of interaction. In Proceedings
of Formal and Computational Approaches to Multi-
modal Communication.
Simon Keizer, Mary Ellen Foster, Zhuoran Wang, and
Oliver Lemon. 2013. Machine learning of social
states and skills for multi-party human-robot inter-
action. Submitted.
David Klotz, Johannes Wienke, Julia Peltason, Britta
Wrede, Sebastian Wrede, Vasil Khalidov, and Jean-
Marc Odobez. 2011. Engagement-based multi-
party dialog with a humanoid robot. In Proceedings
SIGdial, Portland, OR.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-driven Methods for Adaptive Spoken Dialogue
Systems: Computational Learning for Conversa-
tional Interfaces. Springer.
Pierre Lison. 2011. Multi-policy dialogue manage-
ment. In Proceedings SIGdial, Portland, OR.
Maria Pateraki, Markos Sigalas, Georgios Chliveros,
and Panos Trahanias. 2013. Visual human-robot
communication in social settings. In the Work-
shop on Semantics, Identification and Control of
Robot-Human-Environment Interaction, held within
the IEEE International Conference on Robotics and
Automation (ICRA).
Ronald P. A. Petrick and Mary Ellen Foster. 2013.
Planning for social interaction in a robot bartender
domain. In Proceedings ICAPS, Rome, Italy.
Ronald P. A. Petrick, Mary Ellen Foster, and Amy Isard.
2012. Social state recognition and knowledge-level
planning for human-robot interaction in a bartender
domain. In AAAI 2012 Workshop on Grounding Lan-
guage for Physical Systems, Toronto, ON, Canada,
July.
Verena Rieser and Oliver Lemon. 2011. Rein-
forcement Learning for Adaptive Dialogue Systems.
Springer.
R. Stiefelhagen, H. Ekenel, C. Fu?gen, P. Gieselmann,
H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and
A. Waibel. 2007. Enabling multimodal human-
robot interaction for the Karlsruhe humanoid robot.
IEEE Transactions on Robotics, 23(5):840?851.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
L. Pfeifer Vardoulakis, L. Ring, B. Barry, C. Sidner,
and T. Bickmore. 2012. Designing relational agents
as long term social companions for older adults. In
Proceedings IVA, Santa Cruz, CA.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Blaise Thomson, and Kai Yu. 2010. The
Hidden Information State model: a practical frame-
work for POMDP based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
Steve Young, M. Gas?ic?, B. Thomson, and J. Williams.
2013. POMDP-based statistical spoken dialogue
systems: a review. Proceedings of the IEEE. To
appear.
231
Appendix A: Example session with two guests ordering a drink
Timestamp Level 1 MDP Level 2 MDP DescriptionState features Action State features Action
13:28:45:966 0 1 0 0 0 0 0 0 0 - - A1 visible, but not close to bar; no response
generated yet.
13:28:48:029 1 2 0 0 0 0 0 0 2 - - A1 not close to bar and seeking attention: BT
acknowledges this and engages with A1.
13:28:53:680 3 2 0 0 1 2 0 0 4 - - A2 visible, close to the bar, and seeking atten-
tion; BT is already engaged with A1 and there-
fore asks A2 to wait.
13:28:55:715 3 2 0 0 1 2 0 1 3 0 0 0 1 BT continues his interaction with A1 and asks
for their order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 BT continues with A1 and waits for them to
order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 Same as above: BT still waiting for A1?s order.
. . . Due to repeated ASR failures, this state action
pair is encountered several times.
13:29:52:066 3 2 0 0 1 2 0 1 3 0 1 0 2 A1?s has now been successfully recognised; BT
serves the ordered drink to A1.
13:30:12:013 3 2 1 0 1 2 0 1 5 - - A2 still seeking attention; BT can now acknowl-
edge this and engage with A1.
13:30:13:307 1 2 1 0 3 2 0 1 6 0 0 0 1 BT continues with A2 and asks for their order.
13:30:14:475 1 2 1 0 3 2 0 0 6 0 6 0 0 BT continues with A2 and waits for them to
order
13:30:17:737 1 2 1 0 3 2 0 0 6 0 1 0 2 A2?s recognised; BT serves ordered drink to A2.
13:30:37:623 1 2 1 0 3 2 1 0 0 - - Both A1 and A2 have been served; BT does
nothing
13:30:41:440 1 2 1 0 3 2 1 0 0 - - Same as above.
. . .
Table 6: SSE-MDP trajectory for one session from the evaluation data, showing the states and response
actions taken for both MDPs. The states are represented via their value indices, corresponding to Tables 1
and 3; the action indices similarly correspond to the actions in Tables 2 and 4. In the descriptions, A1 and
A2 refer to the first and second user detected; BT refers to the bartender.
232
