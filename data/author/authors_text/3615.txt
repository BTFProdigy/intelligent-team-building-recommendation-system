Obtaining Japanese Lexical Units for Semantic Frames
from Berkeley FrameNet Using a Bilingual Corpus
Toshiyuki Kanamaru
Kyoto University
Yoshida Nihonmatsu-cho, Sakyo-ku
Kyoto, 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Masaki Murata Kow Kuroda Hitoshi Isahara
National Institute of Information and
Communications Technology (NICT)
3-5 Hikaridai, Seikacho, Sorakugun
Kyoto, 619-0289, Japan
{murata,kuroda,isahara}@nict.go.jp
Abstract
An attempt was made to semi-automatically ob-
tain ?lexical units? (LUs) for Japanese from
the English LUs defined in the semantic frame
database provided by Berkeley FrameNet (BFN)
using an English-Japanese bilingual corpus.
This task was a prerequisite to building a com-
plete database of semantic frames for Japanese.
In the task, a Japanese word is first translated
into an English word or phrase, E. E is one
of the lexical units that evoked a particular se-
mantic frame, F , in the BFN database. When
other lexical units of F are translated back into
Japanese, this defines a candidate set of F for
the lexical units of F in Japanese. The via-
bility of the proposed method was tested on a
Japanese verb (X-ga Y -wo) osou (roughly mean-
ing ?X attack(s) Y ,? ?X hit(s) Y ,? ?X surprise(s)
Y ? in English, showing that it is a relatively pol-
ysemous word). The resulting translation was
compared to semantic descriptions provided by
IPAL and Nihongo Goi-Taikei (A Japanese Lex-
icon), two well-known language resources for
Japanese, and also by the Frame Oriented Con-
cept Analysis of Language (FOCAL). The com-
parison revealed that FOCAL, BFN, Goi Taikei,
and IPAL provided finer-grained descriptions in
this specific order.
1 Introduction
Making use of deep semantics in information pro-
cessing is one of the major problems confronting
today?s NLP community. More and more NLP
researchers are realizing that they need seman-
tic/lexical resources that go beyond such ones as
WordNet (Fellbaum, 1998) that only specify hier-
archical semantic relationships. One of the cru-
cial reasons for this is that raw linguistic data
embodies semantic associations that are difficult
to capture in terms of such hierarchical relation-
ships, one of which is the so-called ?semantic
field? effect, a class of associative relationships
among words (or concepts). To deal with these
issues, deeper semantics are needed with descrip-
tions that incorporate ontological inferences. Let
us assume that X attacked Y is to be interpreted.1
This is a complex situation. In interpreting The
man attacked a bank, it may be necessary to spec-
ify (by inference) that the subject used a weapon
(e.g., a gun) and his purpose was to obtain money
(illegally), whereas in interpreting The wolf at-
tacked a flock of sheep, it may be necessary to
specify that the subject never used a weapon and
its purpose was to eat one or two individual sheep
(rather than the entire flock) after killing them.
Relevant inferences are clearly situation-based, or
?case-based? in the sense of Case-based Reason-
ing (Kolodner, 1993), and difficult to specify in
terms of the lexical semantic descriptions avail-
able in resources such as WordNet (Fellbaum,
1998) which don?t specify associative relation-
ships among concepts, including the relationships
between ROBBER (e.g., a man) and WAREHOUSE
OF VALUABLES (e.g., a bank, museum, jewelry
shop), and the one between a PREDATOR (e.g., a
wolf) and its PREY (e.g., sheep, rabbit). Thus, the
NLP community has a critical need for resources
that encode this kind of information.
Along with PropBank (Kingsbury and Palmer,
2002; Ellsworth et al, 2004), Berkeley FrameNet
1One of the anonymous reviewers told us that it was un-
clear how ontological inferences of this sort are related to
BFN?s frame definitions. The question boils down to the
question of definition, i.e., what kind of information we need
to define semantic frames to encode, and as we will see later,
this is exactly the question addressed by FOCAL claiming
that BFN frames are too coarse-grained to be used as an ef-
fective knowledge-base for ontological inferences.
11
(BFN) (Baker et al, 1998) is an ongoing research
project that is attempting to meet the demand for
resources that encode deeper lexical semantics by
providing a semantic frame lexicon (sometimes
called the ?FrameNet?) and a corpus annotated
for semantic information encoded in terms of se-
mantic frames.
Thus far, BFN has produced ?a lexical database
that currently contains more than 8,900 lexical
units, more than 6,100 of which are fully anno-
tated, in more than 625 semantic frames, exem-
plified in more than 135,000 annotated sentences?
(cited from the FrameNet web page). Other
ongoing projects, i.e., the German FrameNet
or ?SALSA? (Erk et al, 2003), the Spanish
FrameNet (Subirats and Petruck, 2003), and the
Japanese FrameNet (Ohara et al, 2003), are try-
ing to build lexical resources that are compatible
with the BFN, but for Japanese at least, no data
has been released in a usable form, except for a
few annotation examples for verbs of motion.
In sum, no useful resource exists for frame-
based description/analysis of Japanese. This is
one of the reasons that we attempted the task in
this paper, along with our efforts to assess the use-
fulness of the database provided by BFN.
The anonymous reviewers of our paper pointed
out that there have been some similar projects
and other methodologies that have tried to trans-
late BFN into other languages automatically, such
as BiFrameNet (Chen and Fung, 2004) and Ro-
mance FrameNet2, and that it would have been
better to include the comparison against them.
BiFrameNet presented an automatic approach
to constructing a bilingual semantic network us-
ing the Chinese HowNet, which is a Chinese
ontology. While it is an interesting approach,
we have not compared their results with ours,
mainly because they seem to have used differ-
ent resources and had somewhat different goals,
along with the space consideration.
No papers are released, let alne being avail-
able to us, related to the Romance FrameNet
project for the time being. We couldn?t help
putting a comparison with it on hold.3
2http://ic2.epfl.ch/?pallotta/rfn/
3One of the anonymous reviewers criticized us for failing
to mention Romance FrameNet project in our paper; it is just
unreasonable. The project was announced on June 1 on the
2 Proposed Procedure
We used a bilingual corpus (Utiyama and Isahara,
2003) to examine which semantic frames of BFN
contained LUs relevant to the Japanese verb osou.
JFN, for example, used a mono-lingual corpus to
construct the semantic frames. In cases like this,
the construction might be inefficient because they
have to construct all semantic frames by them-
selves. But this affects on the reliability of the
frames identified and described. This risk of arbi-
trary description can be reduced by using a bilin-
gual corpus, if it is of high-quality.
2.1 Identifying English equivalents of ?osou?
We chose Japanese-English alignments from the
bilingual corpus in which the Japanese text con-
tained osou, i.e., the target verb. We obtained 135
alignments from the corpus.
The bilingual corpus is consists of two subcor-
pra. One subcorpus is made of one-to-one align-
ments. Another is of one-to-many alignments. In
the latter, one Japanese sentence is aligned with
several English sentences.
In the first case, it was straightforward to spec-
ify an English word or phrase that translated the
target verb, osou. In the second case, however, it
is not. So, we singled out an English sentence that
corresponds to a Japanese sentence that contained
osou. In this process, the identification of osou?s
English translations was done manually.
After this procedure, the following five verbs
were identified as English translations of osou:
assault, attack, hit, pound, and strike4.
2.2 Identifying relevant semantic frames
Based on these five verbs, we extracted seman-
tic frames using FrameSQL (Sato, 2003). Seman-
tic frames with LUs that included any of the five
verbs were chosen from the BFN semantic frame
database (referred to here as BFN).
Corpora Mailing List, just one week before the submission
deadline. This means that we had little chance to know about
the project unless we were ?insiders.?
4There were a few other verbs or constructions that
served as English translations of osou in the alignments: for
example, besiege, engulf, feel pain, occur, hurt, kill, rob,
shoot, stab, suffer, wreak on were used as its translations.
But we filtered out those less frequent items (whose fre-
quency is less than 3) for purposes of simplicity.
12
Based on Frame Semantics (Fillmore, 1982),
BFN posits that a semantic frame is an organi-
zation of ?semantic roles,? which BFN terms as
?Frame Elements? (FEs). Usually, LUs are in-
stantiations or lexical realizations of FEs. Thus,
an LU in a frame, F , is a word, or phrase, that, ac-
cording to the assumptions of Frame Semantics,
?evokes? frame F . The definition of the ?Attack?
frame in the BFN database is used in Figure 1to
illustrate the procedure. As indicated, assault, at-
tack and strike are listed as LUs of the ?Attack?
frame.
After manually examining all the semantic
frames thus obtained, the five BFN frames were
recognized as relevant to the various senses of the
target word osou: 1. ?Attack?; 2. ?Cause harm?
3. ?Experience bodily harm? 4. ?Cause impact?
5. ?Impact?
Semantic frames in the BFN database are sup-
posedly related to one another. There are vari-
ous relationships, some of which are sometimes
encoded by establishing explicit ?frame-to-frame
relations? (such as ?is used? relation) between
two frames. Using this information, we obtained
the following relationships between the five
frames: 1. ?Attack?; 2. ?Cause harm?, is used:
?Experience bodily harm?; 3. ?Cause impact?,
uses: ?Impact?
2.3 Identifying relevant frame-evoking LUs
in English
Each semantic frame has a number of FEs, each of
which has lexical realizations, which called LUs.
In the work reported here, only verbal LUs were
selected as relevant from the English LUs made
available in the BFN database.5 Admittedly, there
5 On this point, we recognize a certain kind of discrep-
ancy between the theory and the practice in the BFN frame-
work. If a LU is, according to its defintion, a lexical realiza-
tion of a certain FE of a certain frame, more nominals should
be identified and listed as LUs. For example, in Jack or-
dered a hamburger at McDonald?s, hamburger is a noun that
evokes the ?Cooking creation? frame. While the ?Selling?
frame is evoked by order.v, this means that, according the
definition of LU, hamburger.n needs to be identified as an
LU of the ?Cooking creation? frame; more specifically, it is
an LU that instantiates the ?Food? FE of the frame. It is ob-
vious that the QUALIA STRUCTURE (Pustejovsky, 1995) of
hamburger.n contains information of this sort. We suspect
that this aspect of ?frame-evocation by nominals? does not
seem to be properly recognized and coded, and that BFN?s
current practice of mostly identifying predicates as LUs is
somewhat misleading, if we could say so, because it con-
are a few nominal LUs in certain frames in the
BFN, but we ignored them because they found
them to be less relevant to our specific task.
After identifying all the relevant LUs for the
three frames above, we obtained all the English
verbs that translated the senses of the target word
osou identified in terms of Frame Semantics.
For example, the relevant LUs for the ?Attack?
frame are the following verbs: ambush, assault,
attack, charge, invade, jump, lay, set, storm, and
strike
As was the case with the ?Attack? frame, we
extracted the relevant LUs for the ?Cause harm?
and ?Cause impact? frames. We manually
merged the extracted LUs, and obtained 93 ver-
bal LUs relevant to the Japanese verb osou.
2.4 Obtaining LU candidates for Japanese
FEs
Table 1: 15 most frequently occurring nouns
Noun Freq.
jiken (incident) 39
boukou (criminal assault) 32
josei (woman) 28
taiho (arrest) 23
hikoku (accused, defendant) 21
yougi (charge, suspicion) 20
kougeki (attack) 20
shounen (boy) 14
tero (terrorism) 14
shougai (injury) 13
higai (damage, harm) 12
kenkei (prefectural police department) 12
manshon (apartment) 12
butai (military unit) 10
fujo (girl and woman) 10
Using the bilingual corpus again, we gathered
alignments that had English texts containing the
English LUs specified in the way previously de-
scribed. We obtained 262 alignments. This proce-
dure defined a set of Japanese sentences contain-
ing Japanese words or phrases that were natural
translations of the LUs in the BFN.
ceals the fact that there can be, and actually are, many kinds
of frame-evoking effects. BFN has been concentrating on
identifying LUs for ?governors,? not LUs for the entire set of
FEs, for whatever reason. In this respect, it is crucial to note
that not all frame-evokers are frame-governors: hamburger.n
clearly evokes the ?Cooking creation? frame, but there the
noun does not govern the ?Cooking creation? frame. Ar-
guably, it is unreasonable and even gratuitous to posit the
?Hamburger? frame to make hamburger.n a governor.
13
Attack
Definition:
An Assailant physically attacks a Victim (which is usually but not always sentient), causing or intending to cause the Victim
physical injury. The Weapon used by the Assailant may also be mentioned, in addition to the usual Place, Time, Purpose, and
Reason. Sometimes a location is used metonymically to stand for the Assailant or the Victim, and in such cases the Place FE
will be annotated on a second FE layer.
As soon as he stepped out of the bar he was SET upon by four men in ski-masks.
Is he INVADING Iraq just to cover other shortcomings?
Then Jon-O?s forces AMBUSHED them on the left flank from a line of low hills.
FEs:
Core:
Assailant [Asl] The person (or other self-directed entity) that is attempting physical harm to the Victim.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Victim [Vic] This FE is the being or entity that is injured by the Assailant?s attack.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Lexical Units
ambush.n, ambush.v, assail.v, assault.n, assault.v, attack.n, attack.v, charge.n, charge.v, fall.v, incursion.n, invade.v, inva-
sion.n, jump.v, lay ((into)).v, offensive.n, onset.n, onslaught.n, raid.v, set.v, storm. v, strike.n, strike.v
Created by infinity on Fri Nov 22 14:05:22 PST 2002
Figure 1: BFN definition of ?Attack? frame (partial)
It should be noted, however, that there is no es-
tablished method of recognizing these units au-
tomatically; they are part of a text without being
marked as such. To solve this problem, we hy-
pothesized that their statistical properties in the
texts could be used to pick them up; i.e., we as-
sumed that these LUs were relatively specific to
these types of texts and would appear at higher
frequencies than usual in the collected text.
We collected nouns with higher frequencies un-
der this assumption using a KH Coder 6.
The results were sorted according to the parts
of speech. The high-frequency nouns thus ob-
tained are listed in Table 1.
This provided little information about the se-
mantic classification of the nouns because there
was no indication of the LUs that they instan-
tiated. Semantic groupings are latent, how-
ever. This meant that we were able to ?clus-
ter? the nouns based on certain generic proper-
ties to obtain an initial approximation of these
groupings. We used a tool called msort (stand-
ing for ?meaning sort?) (Murata et al, 2001) to
establish generic, domain-independent semantic
6The KH Coder is a free analyzer that uses a combination
of ChaSen (Matsumoto et al, 1999) and MySQL. This is
freely available at http://khc.sourceforge.net/.
groupings.78
Nouns occurring more than three times were
obtained, as shown below:9
human dansei (man), danshi (boy), josei (woman), fujo
(woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), . . .
organization kokka (country), gaikoku (foreign country),
kokusai (international), sekai (world), . . .
product yakubutsu (drug), manshon (apartment), heya
(room), keesu (case), naifu (knife), shoujuu (rifle), . . .
7msort sorts a given set of nouns based on their encod-
ings in a Japanese thesaurus Bunrui Goi-hyou (National Lan-
guage Research Institute, 1964).
8One of the anonymous reviewers commented on this
?domain-independence? with a critical tone, questioning the
validity of the proposed method. This evaluation is clearly
based on a misunderstanding: the semantic association, or
conceptual dependence, between the ?Assailant? and the
?Victim? FEs is already encoded when we collected only
sentences whose main verbs are osou (in Japanese texts) or
its translations (in English texts). What we have done with
msort is to get subgroupings given a larger semantic group-
ing of ?harm-causing? at a more generic level. Based on
our coding experience, we are sure that subclassfication of a
given semantic class is based on ?semantic types? rather than
semantic roles. To give proper subgroupings of the events
that the ?Attack? frame is relevant, it is necessary to know
whether an ?Assailant? is a human ([+human, +animate,
. . . ]) or an animal ([?human, +animate, . . . ]), or whether
a ?Victim? is a human ([+human, +animate, . . . ]) or an
animal ([?human, +animate, . . . ]). If we insist that such
subclassifications in terms of semantic types into messy de-
tails are irrelevant, we are committing what we meant by
?mere generalizations for generalizations,? failing to recog-
nized what is really needed in NLP tasks.
9The listings ending with ?. . . ? are partial.
14
body part itai (body), soshiki (organization)
plant dansei (man), josei (woman), soshiki (tissue)
space genba (field), chiiki (region), mokuteki (purpose),
hokubu (northern area), shinai (city center)
amount gruupu (group)
relation jijou (circumstances), keesu (case), jitai (matter),
jiken (incident), ryakushiki (informality), kankei (rela-
tionship), mokuteki (purpose), genkou (current), . . .
activity jisatsu (suicide), satsugai (slaying), shougai (in-
jury), juushou (serious injuries), ishiki (conscious-
ness), utagai (doubt), yougi (suspicion), sousa (inves-
tigation), sousaku (search), shirabe (investigation), . . .
2.5 Identifying LUs for Japanese FEs
Based on the generic semantic groupings pro-
duced by msort, we classified nouns into sub-
classes by intution, so that they corresponded to
the FEs of the BFN frames in the following way:
Recall that a semantic frame is a collection of
semantic roles, or FEs. In the case of ?Attack?,
the frame has two ?core? FEs, i.e., ?Assailant?
and ?Victim?, and some other ?peripheral? or
?noncore? FEs such as ?Place?, ?Time?, and
?Weapon?. Thus, ?Attack? denotes a situation
in which an agent recognizable as an ?Assailant?
causes (or tries to cause) some ?Harm? or ?Injury?
to someone or a group of people recognizable as a
?Victim? at some ?Place? and ?Time?, sometimes
using an item recognizable as a ?Weapon?.
This means that all we need to do is to clas-
sify the nouns in Table 1 into semantic classes
such as ?Assailant?, ?Victim?, ?Place?, ?Time?, or
?Weapon?, with appropriate subclasses where hu-
man assailants are distinguished from nonhuman
assailants.10 The groupings provided by msort
turned out to be useful for this purpose.11
Using this procedure, the nouns obtained on a
frequency-basis for ?Attack? were classified into
the two core FEs, as follows:
10It is important to note that the target data selection pro-
cedure of BFN is biased. For example, they put aside a num-
ber of problematic cases like metaphorical expressions, and
this is clearly reflected in the current frame definitions. We
repeated noticed that metaphorically extended senses of a
word were systematically dropped in the current release of
BFN. For illustration, the sense of attack.n in heart attack
is not described in BFN. Descriptive ?gaps? of this sort are
clearly undesirable; some specific kinds of mapping prob-
lems between English LUs provided in BFN and Japanese
LUs arise from this.
11We were sometimes unable to identify an FE for a noun
class based solely on the output of msort. In these cases, we
looked at its usage in the corpus to determine its FE.
? ?Assailant?: dansei (man), goutou (burglary/burglar,
robbery/robber), heishi (soldier), hikoku (accused per-
son), butai (military unit), kyoudan (religious group)
? ?Victim?: danshi (boy), josei (woman), fujo (girl and
woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), shoujo (girl),
aite (opponent), nihonjin (Japanese), . . .
2.6 Advantages of proposed method
Using msort turned out to be more beneficial
than anticipated when it came to selecting non-
core FEs. msort helped to determine noncore
FEs correctly to a certain extent. The ?Attack?
frame, for example, includes noncore FEs such as
?Place?, ?Time?, ?Purpose?, and ?Reason? in ad-
dition to its core FEs, ?Assailant? and ?Victim?.
msort automatically groups naifu (knife), raifuru
(rifle), and pisutoru (pistol) into the ?product?
category, which corresponds to the ?Weapon? FE.
Similarly, it automatically groups chiiki (Regional
site), hokubu (northern area), and shinai (Inner
city) into the ?location? category, which corre-
sponds to ?Place?. Thus, part of the FE assign-
ment task can be done automatically using msort.
The procedure also produced some interesting
results. For example, the proposed method auto-
matically specifies a set of lexical items (or lex-
ical units) that clearly have the frame-evocation
effect but that are not properly identified as frame
elements of a semantic frame in BFN, either in
terms of core FEs or peripheral FEs (= noncore
FEs). The semantic groupings that were thus au-
tomatically identified are enumerated below:
1. Names denoting an act(ion) of N (N suru (or sareru))
(?(make) do N?): ranbou (violence), boukou (crimi-
nal assault), bouryoku (violence), jikkou (execution),
shuugeki (assault), kougeki (attack)
2. Names denoting a state of affairs N (V shita + N) (N
that S V ): satsugai (slaying), shougai (injury), goutou
(burglary/burglar, robbery/robber), satsujin (murder),
sasshou (killing and wounding)
3. Result ((Y ni) V shite, N wo owaseta) (?did V , and in-
flicted N to Y ): juushou (serious injuries)
4. Parts of the compound words: kyoushuu (assault
force) (a part of ?assault? force)
5. LUs of crime-related frames resulting from ?Attack?:
utagai (doubt), yougi (charge, suspicion), sousa (in-
vestigation), sousaku (search), shirabe (investigation),
kentou (investigation), hanketsu (judgement), . . .
A second look at the lexical items in 1 above
confirmed that most of these words or phrases can
15
be seen as LUs that realize, in Japanese, some of
the FEs of BFN?s ?Attack? frame.12 As sets of
lexical items were not classified automatically, we
had to determine all classifications manually.
2.7 Overall results
When the procedure was applied to ?Attack?,
?Cause harm? and ?Cause impact?, the following
Japanese LUs for their major FEs were specified:
1. Core FEs of ?Attack?:
?Assailant?: dansei (man), goutou (burglary/burglar, rob-
bery/robber), heishi (soldier), hikoku (accused
person), . . .
?Victim?: danshi (boy), josei (woman), fujo (girls and
women), joshi (girl), danji (young boy), . . .
2. Noncore FEs of ?Attack?:
?Place?: genba (field), chiiki (region), hokubu (northern
part), shinai (city center)
?Weapon?: naifu (knife), shoujuu (rifle), tanjuu (pistol)
3. Core FEs of ?Cause harm?:
?Body part? : senaka (back)
4. Core FEs of ?Cause impact?:
?Impactee?: doru (dollar), shijou (market), ginkou (bank),
shokoku (some countries)
?Impactor?: saigai (disaster), jishin (earthquake), fukyou
(depression), dageki (damage)
3 Comparison with other resources
To evaluate our results, we compared them with
other Japanese resources and methods for anal-
ysis, i.e., IPAL (IPA, 1987) and Nihongo Goi
Taikei (a Japanese lexicon) (hereafter called Goi
Taikei) (Ikehara et al, 1997), which are widely
used lexical resources, and semantic frame anal-
ysis by FOCAL (Nakamoto et al, to appear;
Kuroda et al, 2004), which is a recent frame-
work being developed with the aim of provid-
ing BFN-style semantic annotation and analy-
sis for Japanese independent of the Japanese
FrameNet (Ohara et al, 2003).
3.1 Comparison with Goi Taikei descriptions
Goi Taikei contains detailed information on the
predicate-argument structure classified according
to usage. Its semantic description of osou is given
below:
12For the reason of this argument, see note 5 above.
(1) 20 zokusei henka (property change) (motion)
N1 ga N2 wo osou
N1 strike N2
N1 (1270 shimpai (concern) 1262 kanashimi (sorrow)
2056 sainann (disaster) 2359 kishou (atmospheric
phenomena) 1000 tyuushou (abstract)) N2 (2 gutai
(object))
(2) 23 shintai dousa (physical motion) (motion)
N1 ga N2 wo osou
N1 attack N2
N1 (3 shutai (subject) 535 doubutsu (animal) 2416 by-
ouki (disease)) N2 (2 gutai (object))
(3) 23 shintai dousa (physical motion)
31 kanjou dousa (affective motion) (motion)
N1 ga N2 no fui wo osou
N1 surprise N2
N1 (4 hito (man) 1001 tyuushoubutsu (abstruc-
tion/abstraction?) 1235 koto (event)) N2 (4 hito
(man))
The word meanings were classified from the
properties of osou for nouns related to surface
cases of the verb. When we compared the frames
in BFN and the description provided by Goi
Taikei, and examined how the BFN frames corre-
sponded to the Goi Taikei definitions, we obtained
the following relationships:
Table 2: BFN/Goi-Taikei correspondences
Attack (2) 23 shintai dousa (physical motion)
Cause harm (1) 20 zokusei henka (property change)
Cause impact (1) 20 zokusei henka (property change)
First, we did not obtain the meaning ?An unex-
pected event occurred? like (3) in the Goi Taikei.
It was difficult to extract words whose meanings
described a manner of action, such as fui wo (by
surprise) using this method. It was also insuffi-
cient to extract only co-occurring nouns from sen-
tences related to verbs. As might be expected,
there was a close relationship between (2) and the
?Attack? frame. However, we were unable to find
?Assailant?s such as sickness in the BFN FEs. Fi-
nally, the ?Cause impact? frame and (1) were very
similar, except that assailant in (1) includes feel-
ings such as worry or sadness.
There was a good correlation between the se-
mantic frame constructed from BFN and the one
from Goi Taikei. With this method, however, we
met difficulties in extracting frames that did not
appear on the surface, such as ?manner of action?.
16
3.2 Comparison with IPAL descriptions
We compared the frames we obtained with the
definitions from the IPA Lexicon (IPA, 1987). Be-
low is an excerpt from the description of osou
from IPAL:
? Caption: osou001001 Semantic definition: An unde-
sirable thing unexpectedly occurs to someone.
Sentence valence pattern: N1 -ga N2 -wo
Noun phrase 1: bouto (rioter), goutou (burglary),
kuma (bear), sentouki (fighter plane), boufuu (wind
storm), jishinn (earthquake), ekibyou (plague), keizai
kiki (economic crisis)
Noun phrase 2: tabibito (traveler), fune (ship), nin-
gen (human)/kokudo (national land), kuni (country),
kouban (police box)
Example 1: Boufuu ga fune wo osotta. (A stormy wind
struck a ship.)
? Caption: osou001002
Semantic definition: Undesirable feelings and physio-
logical phenomena happening suddenly.
Sentence pattern: N1 -ga N2 -wo
Noun phrase 1: takamaru fuann (increased anxiety),
shi no kyoufu (fear of death), iyana kimochi (unpleas-
ant feelings)/ hageshii hiroukan (acute tiredness), ne-
muke (drowsiness)
Noun phrase 2: kare (he)
Example 1: Nemuke ga totsuzen kare wo osotta.
(Drowsiness fell upon him suddenly.)
Example 2: Kanojo ha fuann ni osowareta. (She be-
came uneasy suddenly.)
The IPAL description of osou identifies its two
senses13 We compared the BFN frames and the
IPAL descriptions (in terms of predicate frames)
and obtained the following correspondences:
Table 3: BFN/IPAL correspondences
Attack osou001001
Cause harm osou001001
Cause impact osou001001
All of the frames obtained from BFN seemed to
be classified into the first meaning in IPAL, e.g.,
there were no BFN frames in which ?Assailant?
recognized ?sickness.? With IPAL definitions,
it was difficult to distinguish the difference be-
tween The bear attacked the traveler and *An eco-
nomic crisis attacked the traveler, the latter of
which sounds unnatural and quite odd, whereas
we can do it with BFN definitions: the former
13A term, ?predicate frame,? is used in the IPAL to char-
acterize semantic properties of a predicate. While the idea
of predicate frames is somewhat related to semantic frames,
predicate frames are not defined as semantic frames in the
sense of Frame Semantics/BFN.
can be classified as an expression in the ?Attack?
frame, whereas the latter can not. The reason
for this is probably that BFN frames successfully
specify the semantic interdependence between the
?Assailant? and ?Victim? roles, whereas such in-
terdependece is not encoded in the IPAL descrip-
tions. We believe this is one of the strengths of
frame-based semantic description.
BFN definitions are not detailed enough, how-
ever. They face problems when we try to ac-
count for the constrast between The shark at-
tacked the swimmer and ?*The shark attacked the
bank, for example. The latter sentences doesn?t
makes sense unless it is reinterpreted some way,
while it is straightforward to interpret the first sen-
tence against a predatory situation.
In interpreting the second, there is a clear con-
flict or ?competition? between two strong read-
ings: one interpretation (reading 1) is against the
situation of ?Predation?, where the shark is inter-
preted as a ?Predator? and the bank as a ?Prey?.
Another (reading 2) is against the situation of
?Bank Robbery?, where the shark is interpreted
as a ?Bank Robber? and the bank as a ?Warehouse
of Valuables? (or simply as a ?Bank?). If reading
2 wins out, an implicit ?type coercion? (Puste-
jovsky, 1995) takes place to the shark so that the
referent of the shark is switched to a human who
acts as a ?Robber? with a nickname ?shark.? If
reading 1 wins out, by contrast, another kind of
implicit type coercion takes place to the bank so
that the referent of the bank is switched to an ani-
mal (an instance of fish, dolphin, or whale) which
acts as a ?Prey?, being called ?the bank? for some
unclear reasons. The preference of the reinter-
pretation for reading 2 over the other can be ac-
counted for if we are allowed to say that to find
someone being called ?shark? is more likely than
to find some animal being called ?bank.?
What this suggests is this: pieces of semantic
information that would account for ?selectional
restrictions? of this sort are not specified in the
BFN definitions (yet). Therefore, it can be said
that the frames constructed from BFN do not
classify all meanings of osou in the same way
IPAL does not, but these frames specify some
finer-grained, selectional aspects of osou?s lexical
meaning than the IPAL description. As we will
see in the next section, this is one of the strong
17
motivations that a framework called FOCAL has
tried to extend the BFN.
3.3 Comparison with FOCAL descriptions
FOCAL is a theoretical framework for semantic
analysis and annotation. Its development has been
strongly influenced by BFN, but it also tries to
extend BFN?s scope of semantic analysis to the
next stage.
In the case of X-ga Y-wo osou, FOCAL recog-
nizes 15 frames in total, listed in Table 4, specify-
ing their hierarchical organization.14
These frames are identified and classified based
on the semantic co-variations between ?Harm
Cause(r))? X , a special case of ?Cause(r)?,
and ?Harm Experiencer? Y , a special case of
?Experiencer?. This is important to note that FO-
CAL puts more emphasis on the specification of
the semantic co-variation between X and Y in
terms of semantic features because they are cru-
cial characteristics of a semantic frame, which are
not captured in the Goi Taikei and IPAL descrip-
tions, and are not clearly encoded even in the BFN
description.
In FOCAL, frames are defined as idealized
models of situations such as Robbery, Predation,
assuming that human understanding is situation-
based. The descriptive task of FOCAL, then, is
to recognize situations and give adequately de-
tailed descriptions to them. Given R is a set of
situation-specific roles {r1, . . . , rn}, which are
called semantic roles in BFN. Semantic frames
are useful only if they serves as specifications of
the co-variations among such Rs.
For example, F06, as a subclass of the ?Attack?
class event is defined as follows:
Definition of F06: Attack(R) = Attack(Predator(X),
Prey(Y ))
= Hunt(Hunter(X), Target(Y ), Purpose(Z))
where Z = Eat(Eater(X), Food(Y ), Purpose(Z?));
where Z? = Satisfy (r1(Z), Hunger)
There seems to be no English noun that names r1.
These are the frames that account for more or
less all possible readings of X-ga Y -wo osou. The
14 Space limitation disallowed us to show that the 15
frames thus recognized are nearly optimal to exhaustively
specify all the situations against which the senses of osou are
determined. This was confirmed by multivariate analyses on
psychological experiments (Nakamoto et al, to appear). We
regret this because the result would surely have answered the
question from one of the anonymous reviewers.
Table 4: 15 FOCAL frames with groups G1?G5
G1 F01 harm to Y caused by conflict between
groups X and Y
G1 F02 harm to Y caused by X?s invasion
G1 F03 harm to Y caused by X?s robbery
G1 F04 harm to Y caused by X?s violence
G1 F05 harm to Y caused by X?s raping
G2 F06 harm to Y caused by X?s preying attack
G2 F07 harm to Y caused by X?s nonpreying attack
(e.g., X?s defense)
G3 F08 harm to Y due to an unexpected accident X
G3 F09 harm to Y caused by a natural phenomenon
X (on a smaller scale, e.g., gust)
G3 F10 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., earthquake, flood)
G3 F11 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., spread of an
epidemic)
G4 F12 harm to Y caused by a social phenomenon X
G5 F13 harm to Y caused by a disease X
(nontemporary, e.g., cancer)
G5 F14 harm to Y caused by a disease symptom X
(temporary, e.g., heart attack)
G5 F15 harm to Y caused by a bad feeling X
(temporary, e.g., drowsiness)
validity of this claim was confirmed through psy-
chological experiments, and reported in (Kuroda
et al, 2004; Nakamoto et al, to appear). The
BFN identifies 3 frames relevant to the semantics
of osou, while FOCAL uses a total of 15 frames
to determine the range of situations against which
people understand the sentences whose main verb
is osou.
The 3 BFN frames have been compared with
the 15 frames below to assess how well they cor-
respond to one another:
Table 5: BFN/FOCAL correspondences
Attack Part of G1 F01?F05
Cause harm [UNCLEAR] [UNCLEAR]
Cause impact [UNCLEAR] [UNCLEAR]
[UNCLEAR] G5 F13?F15
This comparison revealed several differences.
First, FOCAL specifies situations that the
?Attack? frame applies to in much greater de-
tail, although its descriptions are based on se-
mantic frames like BFN?s descriptions are. This
is mainly because FOCAL identifies frames in
terms of conceivable differences in the ?pur-
poses,? or ?intended effects? of the ?Harm
18
Cause(r)?15, of which BFN?s ?Assailant? is a spe-
cial case. This suggests that BFN frames can be
further elaborated according to the subclassifica-
tion of ?Assailant? in terms of its purpose.16
The same is conversely true of ?Cause harm?
and ?Cause impact? frames. These BFN frames
need to be generalized so that they include nonhu-
man, nonintentional agents, which is not done in
the current BFN. Better matches would be found
if the ?Cause harm? and ?Cause impact? frames
were further classified according to the properties
of the ?Harm causer? and ?Impactor? just as in the
?Attack? frame.
While FOCAL explicitly groups the F01?F05
frames into G1 and combines it with another
group, G2, to yield a more general semantic class
{G1, G2}, it is not clear whether BFN captures
this hybrid class, since the hierarchical relation-
ships among frames are not sufficiently specified.
In fact, the comparison with FOCAL revealed
that BFN does not classify the ?Assailant? types in
as much detail as FOCAL does. According to FO-
CAL?s assumptions, it is ?Assailant??s ?Purpose?
(including the ?null? value) that defines the differ-
ences in otherwise similar situations. To identify
such subtle differences is exactly what humans
are very good at and computers are not. Speci-
fication of information of this kind is one of the
serious demands arising from many of the NLP
tasks.
To conclude, we noted that the granularity
of the semantic descriptions provided by BFN,
IPAL, Goi Taikei, and FOCAL had the following
hierarchy: FOCAL > BFN ? Goi Taikei > IPAL
This suggests that, while BFN is clearly useful
for a variety of purposes, its semantic descrip-
tions are not detailed enough, particularly when
dealing with the polysemy of relatively frequent
words like osou in Japanese or hit in English.
While our result is only suggestive at best, let
15This is not the same as BFN?s ?Harm causer? role,
which is much more specific than ?Harm Cause(r)? in FO-
CAL?s sense.
16The question of ?where to stop,? addressed by one of
the anonymous reviewers, would have been answered if we
had enough space to show that those 15 frames/situations are
nearly optimal to account for all the semantic classifications
reflected in selectional restrictions, as explained in note 14.
Clearly, we do not need to identify all semantically possible
subclassifications; we just need to identify psychologically
real subclassifications.
us make a brief comment on some methodologi-
cal aspects of the BFN framework.
Overall, BFN definitions for semantic frames
are much more oriented or even ?biased? for de-
scriptions of activities intended and caused by
human, volitional agents. In fact, BFN took a
methodological decision not to include metaphor-
ical uses and other ?problematic? uses of words
for ease of lexicon-building, thereby sacrificing
its descriptive range, causing a problem with bi-
ased data coverage, as far as we could see. In
the case of osou, for example, there were clearly
many examples in which harm is not caused by
a human, i.e., cases described by FOCAL frame
clusters G2: F06?F07, G3: F08?F11, G4: F12,
and G5: F13?F15. Therefore, as far as we are
concerned with the viability of the frame-based
description of situations that can be expressed us-
ing osou in Japanese, the current status of the
BFN database is only partially successful in that it
successfully captures the class of situations spec-
ified by G1.
4 Conclusion
We proposed a new translation-like method using
BFN to find Japanese LUs that corresponded to
English LUs in BFN semantic frames. We eval-
uated a technique of identifying Japanese LUs
based on English LUs using a bilingual corpus.
We evaluated the results by comparing them with
other Japanese language resources and analyses,
IPAL, Goi Taikei, and FOCAL. The comparison
revealed that FOCAL, BFN, Goi Taikei, and IPAL
provided finer-grained descriptions in this specific
order.
Our method allowed us to easily find Japanese
LUs that corresponded to LUs in BFN seman-
tics and at the same level of granularity as BFN.
Even if all the relevant sentenceswere not manu-
ally examined when the semantic frame was con-
structed, we were able to collect several members
of FEs. Our method also automatically specified
a set of lexical titems that clearly had the frame-
evocation effect but that were not properly iden-
tified as Frame Elements of a semantic frame in
BFN.
There are several problems still remaining that
need to be addressed. Because the bilingual cor-
pus used was a newspaper corpus, the target se-
19
mantic domains were limited. There is therefore
a possibility that we failed to identify certain se-
mantic frames. We plan to do further experiments
using a greater number of bilingual corpora with
a wider domain coverage.
In the comparison of the analyses by BFN and
by FOCAL, only one target verb osou is used in
this work. Clearly, this is insufficient and our re-
sult is only suggestive at best. To draw a realis-
tic conclusion, we will definitely need to examine
more target words and make the comparison more
reliable.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the COLING-ACL ?98, Montreal;
Canada.
Benfung Chen and Pascale Fung. 2004. Biframenet:
Bilingual frame semantics resource construction by
cross-lingual induction. In Proceedings of the 20th
International Conference on Computational Lin-
guistics (COLING 2004).
Michael Ellsworth, Katrin Erk, Paul Kingsbury, and
Sebastian Pado?. 2004. PropBank, SALSA, and
FrameNet: How design determines product. In
Proceedings of the LREC 2004 Workshop on Build-
ing Lexical Resources from Semantically Annotated
Corpora, Lisbon.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and
Manfred Pinkal. 2003. Towards a resource for
lexical semantics: A large German corpus with ex-
tensive semantic annotation. In Proceedings of the
ACL-03.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistic Society of Korea, editor, Linguistics in the
Morning Calm, pages 111?137, Seoul. Hanshin.
Satoru Ikehara, Mahahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei: A Japanese Lexicon. Iwanami Shoten,
Tokyo. (in Japanese, 5 volumes/CDROM).
IPA, 1987. IPA Lexicon of the Japanese Language for
Computers: Basic Verbs. Information-Technology
Promotion Agency. (in Japanese).
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC-2002).
Kolodner, Janet. L. 2004. Case-Based Reasoning.
Morgan Kauffman.
Kow Kuroda, Keiko Nakamoto, Toshiyuki Kanamaru,
Masahiro Tatsuoka, and Hajime Nozawa. 2004.
A scope of concept analysis based on ?seman-
tic frames?: Berkeley FrameNet and Beyond. In
Conference Handbook of the 5th Meeting of The
Japanese Cognitive Linguistics Association, pages
133?153. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 1999. Japanese
Morphological Analysis System ChaSen version
2.2.1. NAIST Technical Report NAIST-IS-TR. (in
Japanese).
Masaki Murata, Kyoko Kanzaki, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. Meaning sort
? three examples: dictionary construction, tagged
corpus construction, and information presentation
system ?. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
Second International Conference, CICLing 2001,
Mexico City, February 2001 Proceedings, pages
305?318. Springer Publisher.
Keiko Nakamoto, Kow Kuroda, and Hajime Nozawa.
to appear. Defining the feature rating task as
a(nother) powerful method to explore sentence
meanings: With a special interest with how they are
mentally represented. In Japanese Journal of Cog-
nitive Psychology. (in Japanese).
National Language Research Institute. 1964. Bunrui
Goihyo (Word List by Semantic Principles). Syuei
Shuppan. (in Japanese).
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press.
Kyoko Hirose Ohara, Seiko Fujii, Hiroaki Saito, Shun
Ishizaki, Toshio Ohori, and Ryoko Suzuki. 2003.
The Japanese FrameNet project: A preliminary re-
port. In Proceedings of Pacific Association for
Computational Linguistics, pages 249?254.
Hiroaki Sato. 2003. FrameSQL: A software tool for
FrameNet. In ASIALEX ?03 Tokyo Proceedings,
pages 251?258. Asian Association of Lexicogra-
phy.
Carlos Subirats and Miriam R. L. Petruck. 2003. Sur-
prise: Spanish FrameNet. Presentation at Work-
shop on Frame Semantics, International Congress
of Linguists. July 29, 2003, Prague, Czech Repub-
lic.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the Annual
Meeting of the ACL-03, pages 72?79. ACL-2003.
20
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Dependency Structure Analysis and Sentence Boundary
Detection in Spontaneous Japanese
Kazuya Shitaoka? Kiyotaka Uchimoto? Tatsuya Kawahara? Hitoshi Isahara?
?School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan,
{shitaoka,kawahara}@ar.media.kyoto-u.ac.jp
?National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan,
{uchimoto,isahara}@nict.go.jp
Abstract
This paper describes a project to detect dependen-
cies between Japanese phrasal units called bunsetsus,
and sentence boundaries in a spontaneous speech
corpus. In monologues, the biggest problem with de-
pendency structure analysis is that sentence bound-
aries are ambiguous. In this paper, we propose
two methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese speech:
One is based on statistical machine translation us-
ing dependency information and the other is based
on text chunking using SVM. An F-measure of 84.9
was achieved for the accuracy of sentence bound-
ary detection by using the proposed methods. The
accuracy of dependency structure analysis was also
improved from 75.2% to 77.2% by using automat-
ically detected sentence boundaries. The accuracy
of dependency structure analysis and that of sen-
tence boundary detection were also improved by in-
teractively using both automatically detected depen-
dency structures and sentence boundaries.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project has been sponsor-
ing the construction of a large spontaneous
Japanese speech corpus, Corpus of Spontaneous
Japanese (CSJ) (Maekawa et al, 2000). The
CSJ is the biggest spontaneous speech corpus in
the world, and it is a collection of monologues
and dialogues, the majority being monologues
such as academic presentations. The CSJ in-
cludes transcriptions of speeches as well as audio
recordings. Approximately one tenth of the CSJ
has been manually annotated with information
about morphemes, sentence boundaries, depen-
dency structures, discourse structures, and so
on. The remaining nine tenths of the CSJ
have been annotated semi-automatically. A fu-
ture goal of the project is to extract sentence
boundaries, dependency structures, and dis-
course structures from the remaining transcrip-
tions. This paper focuses on methods for au-
tomatically detecting sentence boundaries and
dependency structures in Japanese spoken text.
In many cases, Japanese dependency struc-
tures are defined in terms of the dependency
relationships between Japanese phrasal units
called bunsetsus. To define dependency rela-
tionships between all bunsetsus in spontaneous
speech, we need to define not only the depen-
dency structures in all sentences but also the
inter-sentential relationships, or, discourse re-
lationships, between the sentences, as depen-
dency relationships between bunsetsus. How-
ever, it is difficult to define and detect discourse
relationships between sentences because of sig-
nificant inconsistencies in human annotations
of discourse structures, especially with regard
to spontaneous speech. We also need to know
intra-sentential dependency structures in order
to use the results of dependency structure anal-
ysis for sentence compaction in automatic text
summarization or case frame acquisition. Be-
cause it is difficult to define discourse relation-
ships between sentences, depending on the ac-
tual application, it is usually enough to define
and detect the dependency structure of each
sentence. Therefore, the CSJ was annotated
with intra-sentential dependency structures for
sentences in the same way this is usually done
for a written text corpus. However, there is
a big difference between a written text corpus
and a spontaneous speech corpus: In sponta-
neous speech, especially when it is long, sen-
tence boundaries are often ambiguous. In the
CSJ, therefore, sentence boundaries were de-
fined based on clauses whose boundaries were
automatically detected by using surface infor-
mation (Maruyama et al, 2003), and they were
detected manually (Takanashi et al, 2003). Our
definition of sentence boundaries follows the
definition used in the CSJ.
Almost all previous research on Japanese de-
pendency structure analysis dealt with depen-
dency structures in written text (Fujio and Mat-
sumoto, 1998; Haruno et al, 1998; Uchimoto et
al., 1999; Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2000). Although Matsubara and col-
leagues did investigate dependency structures
in spontaneous speech (Matsubara et al, 2002),
the target speech was dialogues where the ut-
terances were short and sentence boundaries
could be easily defined based on turn-taking
data. In contrast, we investigated dependency
structures in spontaneous and long speeches in
the CSJ. The biggest problem in dependency
structure analysis with spontaneous and long
speeches is that sentence boundaries are am-
biguous. Therefore, sentence boundaries should
be detected before or during dependency struc-
ture analysis in order to obtain the dependency
structure of each sentence.
In this paper, we first describe the problems
with dependency structure analysis of sponta-
neous speech. Because the biggest problem is
ambiguous sentence boundaries, we focus on
sentence boundary detection and propose two
methods for improving the accuracy of detec-
tion.
2 Dependency Structure Analysis
and Sentence Boundary Detection
in Spontaneous Japanese
First, let us briefly describe how dependency
structures can be represented in a Japanese sen-
tence. In Japanese sentences, word order is
rather free, and subjects and objects are often
omitted. In languages having such characteris-
tics, the syntactic structure of a sentence is gen-
erally represented by the relationship between
phrasal units, or bunsetsus, based on a depen-
dency grammar. Phrasal units, or bunsetsus,
are minimal linguistic units obtained by seg-
menting a sentence naturally in terms of seman-
tics and phonetics. Each bunsetsu consists of
one or more morphemes. For example, the sen-
tence ???????????? (kare-wa yukkuri
aruite-iru, He is walking slowly)? can be divided
into three bunsetsus, ??? (kare-wa, he)?, ???
?? (yukkuri, slowly)? and ?????? (aruite-
iru, is walking)?. In this sentence, the first and
second bunsetsus depend on the third one.
There are many differences between writ-
ten text and spontaneous speech, and there
are problems peculiar to spontaneous speech
in dependency structure analysis and sentence
boundary detection. The following sections de-
scribe some typical problems and our solutions.
2.1 Problems with Dependency
Structure Analysis
Ambiguous sentence boundaries
As described in Section 1, in this study, we
assumed that ambiguous sentence bound-
aries is the biggest problem in dependency
structure analysis of spontaneous speech.
So in this paper, we mainly focus on this
problem and describe our solution to it.
Independent bunsetsus
In spontaneous speech, we sometimes find
that modifiees are missing because utter-
ance planning changes in the middle of the
speech. Also, we sometimes find bunsetsus
whose dependency relationships are useless
for understanding the utterance. These in-
clude fillers such as ???? (anoh, well)?
and ???? (sonoh, well)?, adverbs that
behave like fillers such as ??? (mou)?,
responses such as ??? (hai, yes)? and ?
?? (un, yes)?, conjunctions such as ??
(de, and)?, and disfluencies. In these cases,
bunsetsus are assumed to be independent,
and as a result, they have no modifiees in
the CSJ. For example, 14,988 bunsetsus in
188 talks in the CSJ are independent.
We cannot ignore fillers, responses, and
disfluencies because they frequently ap-
pear in spontaneous speech. However,
we can easily detect them by using the
method proposed by Asahara and Mat-
sumoto (Asahara and Matsumoto, 2003).
In this paper, fillers, responses, and disflu-
encies were eliminated before dependency
structure analysis and sentence boundary
detection by using morphological informa-
tion and labels. In the CSJ, fillers and re-
sponses are interjections, and almost all of
them are marked with label (F). Disfluen-
cies are marked with label (D).
In this paper, every independent bunsetsu
was assumed to depend on the next one.
However, practically speaking, indepen-
dent bunsetsus should be correctly detected
as ?independent?. This detection is one of
our future goals.
Crossed dependency
In general, dependencies in Japanese writ-
ten text do not cross. In contrast, de-
pendencies in spontaneous speech some-
times do. For example, ???? (kore-ga,
this)? depends on ????? (tadashii-to, is
right)? and ??? (watashi-wa, I)? depends
on ??? (omou, think)? in the sentence ??
??????????????, where ???
denotes a bunsetsu boundary. Therefore,
the two dependencies cross.
However, there are few number of crossed
dependencies in the CSJ: In 188 talks, we
found 689 such dependencies for total of
170,760 bunsetsus. In our experiments,
therefore, we assumed that dependencies
did not cross. Correctly detecting crossed
dependencies is one of our future goals.
Self-correction
We often find self-corrections in sponta-
neous speech. For example, in the 188 talks
in the CSJ there were 2,544 self-corrections.
In the CSJ, self-corrections are represented
as dependency relationships between bun-
setsus, and label D is assigned to them.
Coordination and appositives are also rep-
resented as dependency relationships be-
tween bunsetsus, and labels P and A are
assigned to them, respectively. The defi-
nitions of coordination and appositives fol-
low those of the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). Both
the labels and the dependencies should
be detected for applications such as au-
tomatic text summarization. However, in
this study, we detected only the dependen-
cies between bunsetsus, and we did it in the
same manner as in previous studies using
written text.
Inversion
Inversion occurs more frequently in spon-
taneous speech than in written text. For
example, in the 188 talks in the CSJ there
were 172 inversions. In the CSJ, inver-
sions are represented as dependency rela-
tionships going in the direction from right
to left. In this study, we thought it impor-
tant to detect dependencies, and we man-
ually changed their direction to that from
left to right. The direction of dependency
has been changed to that from left to right.
2.2 Problems with Sentence Boundary
Detection
In spontaneous Japanese speech, sentence
boundaries are ambiguous. In the CSJ, there-
fore, sentence boundaries were defined based
on clauses whose boundaries were automatically
detected using surface information (Maruyama
et al, 2003), and they were detected manually
(Takanashi et al, 2003). Clause boundaries can
be classified into the following three groups.
Absolute boundaries , or sentence bound-
aries in their usual meaning. Such bound-
aries are often indicated by verbs in their
basic form.
Strong boundaries , or points that can be re-
garded as major breaks in utterances and
that can be used for segmentation. Such
boundaries are often indicated by clauses
whose rightmost words are ?? (ga, but)?,
or ?? (shi, and)?.
Weak boundaries , or points that can
be used for segmentation because they
strongly depend on other clauses. Such
boundaries are often indicated by clauses
whose rightmost words are ??? (node, be-
cause)?, or ??? (tara, if)?.
These three types of boundary differ in the
degree of their syntactic and semantic com-
pleteness and the dependence of their sub-
sequent clauses. Absolute boundaries and
strong boundaries are usually defined as sen-
tence boundaries. However, sentence bound-
aries in the CSJ are different from these two
types of clause boundaries, and the accuracy
of rule-based automatic sentence boundary de-
tection in the 188 talks in the CSJ has an F-
measure of approximately 81, which is the ac-
curacy for a closed test. Therefore, we need a
more accurate sentence boundary detection sys-
tem.
Shitaoka et al (Shitaoka et al, 2002) pro-
posed a method for detecting sentence bound-
aries in spontaneous Japanese speech. Their
definition of sentence boundaries is approxi-
mately the same as that of absolute bound-
aries described above. In this method, sen-
tence boundary candidates are extracted by
character-based pattern matching using pause
duration. However, it is difficult to extract
appropriate candidates by this method be-
cause there is a low correlation between pauses
and the strong and weak boundaries described
above. It is also hard to detect noun-final
clauses by character-based pattern matching.
One method based on machine learning, a
method based on maximum entropy models,
has been proposed by Reynar and Ratnaparkhi
(Reynar and Ratnaparkhi, 2000). However, the
target in their study was written text. This
method cannot readily used for spontaneous
speech because in speech, there are no punc-
tuation marks such as periods. Other features
of utterances should be used to detect sentence
boundaries in spontaneous speech.
3 Approach of Dependency
Structure Analysis and Sentence
Boundary Detection
The outline of the processes is shown in Fig-
ure 1.
0: Morphological
Analysis
1: Sentence Boundary
Detection (Baseline)
3: Dependency Structure
Analysis (Baseline)
2: Sentence Boundary
Detection (SVM)
5: Sentence Boundary
Detection (Language model)
6: Sentence Boundary
Detection (SVM)
7: Dependency Structure
Analysis (Again)
clause
expression
pause
duration
word 3-gram model
pause
duration
clause
expression
word
information
(A)
(B)
word
Information
distance
between 
bunsetsus
(C)
(A) + information of 
dependencies
(B) + information of
dependencies
4: Dependency 
Structure Analysis
Figure 1: Outline of dependency structure anal-
ysis and sentence boundary detection.
3.1 Dependency Structure Analysis
In statistical dependency structure analysis of
Japanese speech, the likelihood of dependency
is represented by a probability estimated by a
dependency probability model.
Given sentence S, let us assume that it is
uniquely divided into n bunsetsus, b1, . . . , bn,
and that it is represented as an ordered set of
bunsetsus, B = {b1, . . . , bn}. Let D be an or-
dered set of dependencies in the sentence and let
D
i
be a dependency whose modifier is bunsetsu
b
i
(i = 1, . . . , n ? 1). Let us also assume that
D = {D1, . . . ,Dn?1}. Statistical dependency
structure analysis finds dependencies that max-
imize probability P (D|S) given sentence S.
The conventional statistical model (Collins,
1996; Fujio and Matsumoto, 1998; Haruno et
al., 1998; Uchimoto et al, 1999) uses only
the relationship between two bunsetsus to es-
timate the probability of dependency, whereas
the model in this study (Uchimoto et al, 2000)
takes into account not only the relationship be-
tween two bunsetsus but also the relationship
between the left bunsetsu and all the bunsetsus
to its right. This model uses more information
than the conventional model.
We implemented this model within a max-
imum entropy modeling framework. The fea-
tures used in the model were basically attributes
of bunsetsus, such as character strings, parts
of speech, and types of inflections, as well as
those that describe the relationships between
bunsetsus, such as the distance between bun-
setsus. Combinations of these features were also
used. To find D
best
, we analyzed the sentences
backwards (from right to left). In the backward
analysis, we can limit the search space effec-
tively by using a beam search. Sentences can
also be analyzed deterministically without great
loss of accuracy (Uchimoto et al, 1999). So we
analyzed a sentence backwards and determinis-
tically.
3.2 Sentence Boundary Detection
Based on Statistical Machine
Translation (Conventional method
(Shitaoka et al, 2002))
The framework for statistical machine trans-
lation is formulated as follows. Given in-
put sequence X, the goal of statistical ma-
chine translation is to find the best output se-
quence, Y , that maximizes conditional proba-
bility P (Y |X):
max
Y
P (Y |X) = max
Y
P (Y )P (X |Y ) (1)
The problem of sentence boundary detection
can be reduced to the problem of translat-
ing a sequence of words, X, that does not in-
clude periods but instead includes pauses into
a sequence of words, Y , that includes peri-
ods. Specifically, in places where a pause
might be converted into a period, which means
P (X|Y ) = 1, the decision whether a period
should be inserted or not is made by comparing
language model scores P (Y ?) and P (Y ??). Here,
the difference between Y ? and Y ?? is in that one
includes a period in a particular place and the
other one does not.
We used a model that uses pause duration
and surface expressions around pauses as trans-
lation model P (X|Y ). We used expressions
around absolute and strong boundaries as de-
scribed in Section 2.2 as surface expressions
around pauses. A pause preceding or follow-
ing surface expressions can be converted into
a period. Specifically, pauses following expres-
sions ?? (to)?, ??? (nai)?, and ?? (ta)?, and
pauses preceding expression ?? (de)?, can be
converted into a period when these pauses are
longer than average. A pause preceding or fol-
lowing other surface expressions can be con-
verted into a period even if its duration is short.
To calculate P (Y ), we used a word 3-gram
model trained with transcriptions in the CSJ.
3.3 Sentence Boundary Detection
Using Dependency Information
(Method 1)
There are three assumptions that should be sat-
isfied by the rightmost bunsetsu in every sen-
tence. In the following, this bunsetsu is referred
to as the target bunsetsu.
(1) One or more bunsetsus depend on the
target bunsetsu. (Figure 2)
Since every bunsetsu depends on another bun-
setsu in the same sentence, the second rightmost
bunsetsu always depends on the rightmost bun-
setsu in any sentence, except in inverted sen-
tences. In inverted sentences in this study, we
changed the direction of all dependencies to that
from left to right.
One or more  
Bunsetsus depend   
Figure 2: One or more bunsetsus depend on
the target bunsetsu. (?|? represents a sentence
boundary.)
(2) There is no bunsetsu that depends
on a bunsetsu beyond the target bunsetsu.
(Figure 3)
Each bunsetsu in a sentence depends on a bun-
setsu in the same sentence.
(3) The probability of the target bun-
setsu is low. (Figure 4)
The target bunsetsu does not depend on any
bunsetsu.
No bunsetsu depend in this way
Figure 3: There is no bunsetsu that depends on
a bunsetsu beyond the target bunsetsu.
This probability should be low
Figure 4: Probability of the target bunsetsu is
low.
Bunsetsus that satisfy assumptions (1)-(3)
are extracted as rightmost bunsetsu candidates
in a sentence. Then, for every point follow-
ing the extracted bunsetsus and for every pause
preceding or following the expressions described
in Section 3.2, a decision is made regarding
whether a period should be inserted or not.
In assumption (2), bunsetsus that depend on a
bunsetsu beyond 50 bunsetsus are ignored be-
cause no such long-distance dependencies were
found in the 188 talks in the CSJ used in our ex-
periments. Bunsetsus whose dependency prob-
ability is very low are also ignored because there
is a high possibility that these bunsetsus? depen-
dencies are incorrect. Let this threshold proba-
bility be p, and let the threshold probability in
assumption (3) be q. The optimal parameters p
and q are determined by using held-out data.
In this approach, about one third of all
bunsetsu boundaries are extracted as sentence
boundary candidates. So, an output sequence
is selected from all possible conversion patterns
generated using two words to the left and two
words to the right of each sentence boundary
candidate. To perform this operation, we used
a beam search with a width of 10 because a
number of conversion patterns can be generated
with such a search.
3.4 Sentence Boundary Detection
Based on Machine Learning
(Method 2)
We use Support Vector Machine (SVM) as a
machine learning model and we approached the
problem of sentence boundary detection as a
text chunking task. We used YamCha (Kudo
and Matsumoto, 2001) as a text chunker, which
is based on SVM and uses polynomial kernel
functions. To determine the appropriate chunk
label for a target word, YamCha uses two words
to the right and two words to the left of the
target word as statistical features, and it uses
chunk labels that are dynamically assigned to
the two preceding or the two following words
as dynamic features, depending on the analysis
direction. To solve the multi-class problem, we
used pairwise classification. This method gen-
erates N ? (N ? 1)/2 classifiers for all pairs of
classes, N , and makes a final decision by their
weighted voting.
The features used in our experiments are the
following:
1. Morphological information of the three words
to the right and three words to the left of the
target word, such as character strings, pronun-
ciation, part of speech, type of inflection, and
inflection form
2. Pause duration normalized in terms of Maha-
lanobis distance
3. Clause boundaries
4. Dependency probability of the target bunsetsu
5. The number of bunsetsus that depend on the
target bunsetsu and their dependency proba-
bilities
We used the IOE labeling scheme for proper
chunking, and the following parameters for
YamCha.
? Degree of polynomial kernel: 3rd
? Analysis direction: Left to right
? Multi-class method: Pairwise
4 Experiments and Discussion
In our experiments, we used the transcriptions
of 188 talks in the CSJ. We used 10 talks for
testing. Dependency structure analysis results
were evaluated for closed- and open-test data in
terms of accuracy, which was defined as the per-
centage of correct dependencies out of all depen-
dencies. In Tables 1 to 3, we use words ?closed?
and ?open? to describe the results obtained for
closed- and open-test data, respectively. Sen-
tence boundary detection results were evaluated
in terms of F-measure.
First, we show the baseline accuracy of depen-
dency structure analysis and sentence boundary
detection. The method described in Section 3.2
was used as a baseline method for sentence
boundary detection (Process 1 in Figure 1). To
train the language model represented by P (Y ),
we used the transcriptions of 178 talks exclud-
ing the test data. The method described in Sec-
tion 3.1 was used as a baseline method for de-
pendency structure analysis. (Process 3 in Fig-
ure 1) As sentence boundaries, we used the re-
sults of the baseline method for sentence bound-
ary detection. We obtained an F-measure of
75.6, a recall of 64.5%, and a precision of 94.2%
for the sentence boundary detection in our ex-
periments. The dependency structure analysis
accuracy was 75.2% for the open data and 80.7%
for the closed data.
The dependency probability of the rightmost
bunsetsus in a given sentence was not calculated
in our model. So, we assumed that the right-
most bunsetsus depended on the next bunsetsu
and that the dependency probability was 0.5
when we used dependency information in the
experiments described in the following sections.
4.1 Sentence Boundary Detection
Results Obtained by Method 1
We evaluated the results obtained by the
method described in Section 3.3. The results
of baseline dependency structure analysis were
used as dependency information (Process 5 in
Figure 1).
First, we investigated the optimal values of
parameters p and q described in Section 3.3 by
using held-out data, which differed from the test
data and consisted of 15 talks. The optimal val-
ues of p and q were, respectively, 0 and 0.9 for
the open-test data, and 0 and 0.8 for the closed-
test data. These values were used in the follow-
ing experiments. The value of p was 0, and these
results show that bunsetsus that depended on a
bunsetsu beyond 50 bunsetsus were ignored as
described in assumption (2) in Section 3.3.
The obtained results are shown in Table 1.
When dependency information was used, the F-
measure increased by approximately 1.4 for the
open-test data and by 2.0 for the closed test
data, respectively. Although the accuracy of de-
pendency structure analysis for closed test data
was about 5.5% higher than that for the open-
test data, the difference between the accuracies
of sentence boundary detection for the closed-
and open-test data was only about 0.6%. These
results indicate that equivalent accuracies can
be obtained for both open- and closed-test data
in detecting dependencies related to sentence
boundaries.
When all the extracted candidates were con-
sidered as sentence boundaries without us-
ing language models, the accuracy of sentence
boundary detection obtained by using the base-
line method was 68.2%(769/1,127) in recall and
81.5%(769/943) in precision, and that obtained
by using Method 1 was 87.2%(983/1,127) in re-
call and 27.7%(983/3,544) in precision. The re-
sults show that additional 214 sentence bound-
ary candidates were correctly extracted by us-
ing dependency information. However, only
108 sentence boundaries were chosen out of
the 214 candidates when language models were
used. We investigated in detail the points
that were not chosen and found errors in noun-
final clauses, clauses where the rightmost con-
stituents were adjectives or verbs such as ??
?? (it to-omou, think)? or ????? (it wa-
muzukashii, difficult)?, and clauses where the
rightmost constituents were ?????? (it to-
Table 1: Sentence boundary detection results
obtained by using dependency information.
recall precision F
With dependency 74.1% 82.5% 78.0
information (open) (835/1,127) (835/1,012)
With dependency 74.2% 83.5% 78.6
information (closed) (836/1,127) (836/1,001)
baseline 64.5% 94.2% 76.6
(727/1,127) (727/772)
iu-no-wa, because)? and ????? (it to-si-te-
wa, as)?, and so on. Some errors, except for
those in noun-final clauses, could have been cor-
rectly detected if we had had more training
data.
We also found that periods were sometimes
erroneously inserted when preceding expres-
sions were ?? (ga, but)?, ???? (mashite,
and)?, and ????? (keredomo, but)?, which
are typically the rightmost constituents of a sen-
tence, as weel as ?? (te, and)?, which is not,
typically, the rightmost constituent of a sen-
tence. The language models were not good at
discriminating between subtle differences.
4.2 Sentence Boundary Detection
Results Obtained by Method 2
We evaluated the results obtained by the
method described in Section 3.4 (Process 6 in
Figure 1). For training, we used 178 talks ex-
cluding test data.
The results are shown in Table 2. The F-
measure was about 6.9 points higher than that
described in Section 4.1. The results show
that the approach based on machine learning
is more effective than that based on statisti-
cal machine translation. The results also show
that the accuracy of sentence boundary detec-
tion can be increased by using dependency in-
formation in Method 2. However, we found that
the amount of accuracy improvement achieved
by using dependency information depended on
the method used. This may be because other
features used in SVM may provide information
similar to dependency information. For exam-
ple, Feature 1 described in Section 3.4 might
provide information similar to that in Features
4 and 5. Although in our experiments we used
only three words to the right and three words
to the left of the target word, the degradation
in accuracy without dependency information
was slight. This may be because long-distance
dependencies may not be related to sentence
boundaries, or because Feature 5 does not con-
tribute to increasing the accuracy because the
accuracy of dependency structure analysis in de-
tecting long-distance dependencies is not high.
Table 2: Sentence boundary detection results
obtained by using SVM.
recall precision F
With dependency 80.0% 90.3% 84.9
information (open) (902/1,127) (902/999)
With dependency 79.7% 90.5% 84.9
information (closed) (900/1,127) (900/994)
Without 79.3% 90.1% 84.4
dependency information (894/1,127) (894/992)
Table 3: Dependency structure analysis results
obtained with automatically detected sentence
boundaries.
open closed
With results in Section 4.1 75.8% 81.2%
With results in Section 4.2 77.2% 82.5%
Baseline 75.2% 80.7%
4.3 Dependency Structure Analysis
Results
We evaluated the results of dependency struc-
ture analysis obtained when sentence bound-
aries detected automatically by the two meth-
ods described above were used as inputs (Pro-
cess 7 in Figure 1). The results are shown in
Table 3. The accuracy of dependency structure
analysis improved by about 2% when the most
accurate and automatically detected sentence
boundaries were used as inputs. This is be-
cause more sentence boundaries were detected
correctly, and the number of bunsetsus that de-
pended on those in other sentences decreased.
We investigated the accuracy of dependency
structure analysis when 100% accurate sentence
boundaries were used as inputs. The accuracy
was 80.1% for the open-test data, and 86.1%
for the closed-test data. Even when the sen-
tence boundary detection was perfect, the er-
ror rate was approximately 14% even for the
closed-test data. The accuracy of dependency
structure analysis for spoken text was about 8%
lower than that for written text (newspapers).
We speculate that this is because spoken text
has no punctuation marks and many bunsetsus
depend on others far from them because of in-
sertion structures. These problems need to be
addressed in future studies.
5 Conclusion
This paper described a project to detect depen-
dencies between bunsetsus and sentence bound-
aries in a spontaneous speech corpus. It is
more difficult to detect dependency structures
in spontaneous spoken speech than in written
text. The biggest problem is that sentence
boundaries are ambiguous. We proposed two
methods for improving the accuracy of sentence
boundary detection in spontaneous Japanese
speech. Using these methods, we obtained an
F-measure of 84.9 for the accuracy of sentence
boundary detection. The accuracy of depen-
dency structure analysis was also improved from
75.2% to 77.2% by using automatically detected
sentence boundaries. The accuracy of depen-
dency structure analysis and that of sentence
boundary detection were improved by interac-
tively using automatically detected dependency
information and sentence boundaries.
There are several future directions. In the fu-
ture, we would like to solve the problems that
we found in our experiments. In particular, we
want to reduce the number of errors due to in-
serted structures and solve other problems de-
scribed in Section 2.1.
References
Masayuki Asahara and Yuji Matsumoto. 2003. Filler and
Disfluency Identification Based on Morphological Analysis
and Chunking. In Proceedings of the ISCA & IEEE Work-
shop on Spontaneous Speech Processing and Recognition,
pages 163?166.
Michael Collins. 1996. A New Statistical Parser Based on
Bigram Lexical Dependencies. In Proceedings of the ACL,
pages 184?191.
Masakazu Fujio and Yuji Matsumoto. 1998. Japanese Depen-
dency Structure Analysis based on Lexicalized Statistics.
In Proceedings of the EMNLP, pages 87?96.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi Ooyama.
1998. Using Decision Trees to Construct a Practical
Parser. In Proceedings of the COLING-ACL, pages 505?
511.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Structure Analysis Based on Support Vector Ma-
chines. In Proceedings of the EMLNP, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building a
Japanese Parsed Corpus while Improving the Parsing Sys-
tem. In Proceedings of the NLPRS, pages 451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi
Isahara. 2000. Spontaneous Speech Corpus of Japanese.
In Proceedings of the LREC2000, pages 947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Kumano, and
Hideki tanaka. 2003. Rules for Automatic Clause Bound-
ary Detection and Their Evaluation. In Proceedings of
the Nineth Annual Meeting of the Association for Natural
Language proceeding, pages 517?520. (in Japanese).
Shigeki Matsubara, Takahisa Murase, Nobuo Kawaguchi, and
Yasuyoshi Inagaki. 2002. Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language. In Proceedings
of the COLING2002, pages 640?645.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 2000. A Max-
imum Entropy Approach to Identifying Sentence Bound-
aries. In Proceedings of the ANLP, pages 16?19.
Kazuya Shitaoka, Tatsuya Kawahara, and Hiroshi G. Okuno.
2002. Automatic Transformation of Lecture Transcrip-
tion into Document Style using Statistical Framework. In
IPSJ?WGSLP SLP-41-3, pages 17?24. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiyotaka Uchi-
moto, and Hitoshi Isahara. 2003. Identification of ?Sen-
tences? in Spontaneous Japanese ? Detection and Mod-
ification of Clause Boundaries ?. In Proceedings of the
ISCA & IEEE Workshop on Spontaneous Speech Process-
ing and Recognition, pages 183?186.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese Dependency Structure Analysis Based on
Maximum Entropy Models. In Proceedings of the EACL,
pages 196?203.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine, and Hi-
toshi Isahara. 2000. Dependency Model Using Posterior
Context. In Proceedings of the IWPT, pages 321?322.
Construction of an Objective Hierarchy of Abstract Concepts           
via Directional Similarity  
Kyoko Kanzaki  Eiko Yamamoto Hitoshi Isahara 
Computational Linguistics Group, 
National Institute of Information and Communications 
Technology 
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan,  
{kanzaki, eiko, isahara}@nict.go.jp 
Qing Ma 
Faculty of Science  
and Technology 
Ryukoku University 
Seta, Otsu,520-2194, Japan 
qma@math.ryukoku.ac.jp
Abstract 
The method of organization of word mean-
ings is a crucial issue with lexical databases. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our 
initial task to this end is to determine adjec-
tive hyperonyms. In order to find adjective 
hyperonyms, we utilize abstract nouns. We 
constructed linguistic data by extracting se-
mantic relations between abstract nouns and 
adjectives from corpus data and classifying 
abstract nouns based on adjective similarity 
using a self-organizing semantic map, which 
is a neural network model (Kohonen 1995). 
In this paper we describe how to hierarchi-
cally organize abstract nouns (adjective hy-
peronyms) in a semantic map mainly using 
CSM. We compare three hierarchical organi-
zations of abstract nouns, according to CSM, 
frequency (Tf.CSM) and an alternative simi-
larity measure based on coefficient overlap, to 
estimate hyperonym relations between words. 
1. Introduction 
A lexical database is necessary for computers, 
and even humans, to fully understand a word's 
meaning because the lexicon is the origin of lan-
guage understanding and generation. Progress is 
being made in lexical database research, notably 
with hierarchical semantic lexical databases such 
as WordNet, which is used for NLP research 
worldwide.  
When compiling lexical databases, it is impor-
tant to consider what rules or phenomena should 
be described as lexical meanings and how these 
lexical meanings should be formalized and stored 
electronically. This is a common topic of discus-
sion in computational linguistics, especially in 
the domain of computational lexical semantics. 
The method of organization of word meanings 
is also a crucial issue with lexical databases. In 
current lexical databases and/or thesauri, abstract 
nouns indicating concepts are identified manually 
and words are classified in a top-down manner 
based on human intuition. This is a good way to 
make a lexical database for users with a specific 
purpose. However, word hierarchies based on 
human intuition tend to vary greatly depending 
on the lexicographer, and there is often dis-
agreement as to the make-up of the hierarchy. If 
we could find an objective method to organize 
word meanings based on real data, we would 
avoid this variability. 
Our purpose in this research is to extract word 
hierarchies from corpora automatically. Our ini-
tial task to this end is to determine adjective hy-
peronyms. In order to find adjective hyperonyms, 
we utilize abstract nouns. Past linguistic research 
has focused on classifying the semantic relation-
ship between abstract nouns and adjectives 
(Nemoto 1969, Takahashi 1975).  
We constructed linguistic data by extracting 
semantic relations between abstract nouns and 
adjectives from corpus data and classifying ab-
stract nouns based on adjective similarity using a 
self-organizing semantic map (SOM), which is a 
neural network model (Kohonen 1995). The rela-
tive proximity of words in the semantic map in-
dicates their relative similarity.  
In previous research, word meanings have 
been statistically modeled based on syntactic in-
formation derived from a corpus. Hindle (1990) 
used noun-verb syntactic relations, and Hatzivas-
siloglou and McKeown (1993) used coordinated 
adjective-adjective modifier pairs. These meth-
ods are useful for the organization of words deep 
within a hierarchy, but do not seem to provide a 
solution for the top levels of the hierarchy.  
To find an objective hierarchical word struc-
ture, we utilize the complementary similarity 
measure (CSM), which estimates a one-to-many 
relation, such as superordinate?subordinate rela-
tions (Hagita and Sawaki 1995, Yamamoto and 
Umemura 2002).  
In this paper we propose an automated method 
for constructing adjective hierarchies by connect-
ing strongly related abstract nouns in a top-down 
fashion within a semantic map, mainly using 
CSM. We compare three hierarchical organiza-
tions of abstract nouns, according to CSM, fre-
quency (Tf.CSM) and an alternative similarity 
measure based on coefficient overlap, to estimate 
hyperonym relations between words. 
2. Linguistic clues to extract adjective hy-
peronyms from corpora 
In order to automatically extract adjective hy-
peronyms we use syntactic and semantic relations 
between words.  
There is a good deal of linguistic research fo-
cused on the syntactic and semantic functions of 
abstract nouns, including Nemoto (1969), Taka-
hashi (1975), and Schmid (2000). Takahashi 
(1975) illustrated the sentential function of ab-
stract nouns with the following examples. 
a.  Yagi  wa  seishitsu  ga  otonashii. 
(goat) topic (nature) subject (gentle) 
        The nature of goats is gentle 
b.   Zou    wa   hana   ga     nagai. 
    (elephant) topic  (a nose) subject  (long) 
         The nose of an elephant is long 
He examined the differences in semantic func-
tion between ?seishitsu (nature)? in (a) and ?hana 
(nose)? in (b), and explained that ?seishitsu (na-
ture)? in (a) indicates an aspect of something, i.e., 
the goat, and ?hana (nose)? in (b) indicates part 
of something, i.e., the elephant. He recognized 
abstract nouns in (a) as a hyperonym of the at-
tribute that the predicative adjectives express. 
Nemoto (1969) identified expressions such as 
?iro ga akai (the color is red)? and ?hayasa ga 
hayai (the speed is fast)? as a kind of meaning 
repetition, or tautology.  
In this paper we define such abstract nouns 
that co-occur with adjectives as adjective hy-
peronyms. We semi-automatically extracted from 
corpora 365 abstract nouns used as this kind of 
head noun, according to the procedures described 
in Kanzaki et al (2000). We collected abstract 
nouns from two year's worth of articles from the 
Mainichi Shinbun newspaper, and extracted ad-
jectives co-occurring with abstract nouns in the 
manner of (a) above from 100 novels, 100 essays 
and 42 year's worth of newspaper articles, includ-
ing 11 year's worth of Mainichi Shinbun articles, 
10 year's worth of Nihon Keizai Shinbun (Japa-
nese economic newspaper) articles, 7 year's wor-
th of Sangyoukinyuuryuutsu Shinbun (an eco-
nomic newspaper) articles, and 14 year's worth of 
Yomiuri Shinbun articles. The total number of 
abstract noun types is 365, the number of adjec-
tive types is 10,525, and the total number of ad-
jective tokens is 35,173. The maximum number 
of co-occurring adjectives for a given abstract 
noun is 1,594. 
3. On the Self-Organizing Semantic Map  
3.1  Input data 
Abstract nouns are located in the semantic map 
based on the similarity of co-occurring adjectives 
after iteratively learning over input data. 
In this research, we focus on abstract nouns 
co-occurring with adjectives. In the semantic 
map, there are 365 abstract nouns co-occurring 
with adjectives. The similarities between the 365 
abstract nouns are determined according to the 
number of common co-occurring adjectives. We 
made a list such as the following. 
OMOI (feeling): ureshii (glad), kanashii (sad), 
shiawasena (happy), ? 
KIMOCHI (though): ureshii (glad), tanoshii (pleased), 
hokorashii (proud), ? 
KANTEN (viewpoint): igakutekina (medical), 
rekishitekina (historical), ... 
When two (or more) sets of adjectives with 
completely different characteristics co-occur with 
an abstract noun and the meanings of the abstract 
noun can be distinguished correspondingly, we 
treat them as two different abstract nouns. For 
example, the Japanese abstract noun ?men? is 
treated as two different abstract nouns with 
?men1? meaning ?one side (of the characteristics 
of someone or something)? and ?men2? meaning 
?surface?. The former co-occurs with ?gentle?, 
?kind? and so on. The latter co-occurs with 
?rough?, ?smooth? and so on. 
3.2  The Self-Organizing Semantic Map 
Ma (2000) classified co-occurring words using 
a self-organizing semantic map (SOM). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
   
We made a semantic map of the above-
mentioned 365 abstract nouns using SOM, based 
on the cosine measure. The distribution of the 
words in the map gives us a sense of the semantic 
distribution of the words. However, we could not 
precisely identify the relations between words in 
the map (Fig 1). In Fig. 1 lines on the maps indi-
cate close relations between word pairs. In the 
cosine-based semantic map, there is no clear cor-
respondence between word similarities and the 
distribution of abstract nouns in the map.    
To solve this problem we introduced the 
complementary similarity measure (CSM). This 
similarity measure estimates one-to-many 
relations, such as superordinate?subordinate 
relations (Hagita and Sawaki 1995, Yamamoto 
and Umemura 2002). We can find the 
hierarchical distribution of words in the semantic 
map according to the value of CSM (Fig 2). In 
the CSM-based SOM, lines are concentrated at 
the bottom right hand corner, that is, most ab-
stract nouns are located at the bottom right-hand 
corner.  
Next, we find hierarchical relations between 
whole abstract nouns, not between word pairs, on 
the map automatically. 
4. How to construct hierarchies of nominal 
adjective hyperonyms in the Semantic 
Map 
4.1 Similarity measures, CSM and Yates? 
correction 
A feature of CSM is its ability to estimate hi-
erarchical relations between words. This similar-
ity measure was developed for the recognition of 
degraded machine-printed text (Hagita and Sa-
waki, 1995). Yates? correction is often used in 
order to increase the accuracy of approximation. 
Hierarchical relations can be extracted accurately 
when the CSM value is high. Yates? correction 
can extract different relations from high CSM 
values. When the CSM value is low, the result is 
not reliable, in which case we use Yates? correc-
tion. 
According to Yamamoto and Umemura (2002), 
who adopted CSM to classify words, CSM is cal-
culated as follows. 
))(( dbca
bcadCSM
++
?
=  
Yates? correction is calculated as follows. 
))()()((
)2/|(| 2
dbcadcba
nbcadnYates
++++
??
=  
Here n is the sum of the number of co-
occurring adjectives; a indicates the number of 
times the two labels appear together; b indicates 
the number of times ?label 1? occurs but ?label 
2? does not; c is the number of times ?label 2? 
occurs but ?label 1? does not; and d is the num-
ber of times neither label occurs. In our research, 
each ?label? is an abstract noun, a indicates the 
number of adjectives co-occurring with both ab-
stract nouns, b and c indicate the number of ad-
jectives co-occurring with either abstract noun 
Figure 1. The Cosine-based SOM of word similarity Figure 2. The CSM-based SOM of word similarity
(?label 1? and ?label 2?, respectively), and d in-
dicates the number of adjectives co-occurring 
with neither abstract noun. We calculated hierar-
chical relations between word pairs using these 
similarity measures. 
4.2 Construction of a hierarchy of abstract 
nouns using CSM and Yates' correc-
tion 
The hierarchy construction process is as fol-
lows: 
1) Based on the results of CSM, ?koto (mat-
ter)? is the hyperonym of all abstract nouns. 
First, we connect super/sub-ordinate words 
with the highest CSM value while keeping the 
super-subordinate relation.  
2) When the normalized value of CSM is 
lower, the number of extracted word pairs be-
comes increasing overwhelmingly, and the reli-
ability of CSM diminishes. Word pairs with a 
normalized CSM value of less than 0.4 are lo-
cated far from the common hyperonym ?koto 
(matter)? on the semantic map. If we construct a 
hierarchy using CSM values only, a long hierar-
chy containing irrelevant words emerges. In this 
case, the word pairs calculated by Yates' correc-
tion are more accurate than those from CSM. We 
combine words using Yates? correction, when the 
value of CSM is less than 0.4. When we connect 
word pairs with a high Yates? value, we find a 
hyperonym of the super-ordinate noun of the pair 
and connect the pair to the hyperonym. If a word 
pair appears only in the Yates' correction data, 
that is, we cannot connect the pair with high 
Yates? value to the hyperonym with high CSM 
value, they are combined with ?koto (matter)?. 
3) Finally, if a short hierarchy is contained in a 
longer hierarchy, it is merged with the longer 
hierarchy and we insert ?koto (matter)? at the 
root of all hierarchies. 
4.3  Results 
The number of groups obtained was 161. At its 
deepest, the hierarchy was 15 words deep, and at 
its shallowest, it was 4 words deep. The 
following is a breakdown of the number of 
groups at different depths in the hierarchy.  
The greatest concentration of groups is at 
depth 7. There are 140 groups from depth 5 to 
depth 10, which is 87% of all groups. 
 
 
 
 
 
 
 
The word that has the strongest relation with 
?koto (matter)? is ?men1 (side1)?. The number of 
groups in which ?koto (matter)? and ?men1 
(side1)? are hyperonyms is 96 (59.6%). The larg-
est number of groups after that is a group in 
which ?koto (matter)?, ?men1 (side1)? and 
?imeeji (image)? are hyperonyms. The number of 
groups in this case is 59 groups, or 36.6% of the 
total. With respect to the value of CSM, the co-
occurring adjectives are similar to ?men1 (side1)? 
and ?imeeji (image)?.  
Other words that have a direct relation with 
?koto (matter)? are ?joutai (state)? and ?toki 
(when)?. They have the most number of groups 
after ?men1 (side1)? among all the children of 
?koto (matter)?. The number of groups subsumed 
by ?joutai (state)? group and ?toki (when)? are 21 
and 19, respectively. Other direct hyponyms of 
?koto (matter)? are: 
ki (feeling): 6 groups  
ippou (while or grow ?er and er): 3 groups  
me2 (eyes): 3 groups  
katachi1 (in the form of): 3 groups  
iikata (how to say): 2 groups  
yarikata (how to): 2 groups 
There is little hierarchical structure to these 
groups, as they co-occur with few adjectives. 
4.4 The Hierarchies of abstract concepts in 
the semantic map 
In the following semantic maps, where abstract 
nouns are distributed using SOM and CSM (see 
Section 3), hierarchies of abstract nouns are 
drawn with lines. The bottom right hand corner is 
?koto (matter)?, a starting point for the distribu-
tion of abstract nouns.  
Five main types of hierarchies are found from 
patterns of lines on the map, as follows: 
The first figure, Fig.3, is hierarchies of ?kanji 
(feeling), kimochi (feeling) ?? on the semantic 
map. The location of hierarchies of ?yousu (as-
pect), omomochi (look), kaotsuki (on one?s face), 
?? is similar to this type of the location. Hierar-
chies of ?sokumen (one side), imi (meaning), 
kanten (viewpoint),  kenchi (standpoint) ?? on 
Depth 4 5 6 7 8 9 
Groups 3 16 27 32 23 23 
Depth 10 11 12 13 14 15 
Groups 19 7 3 4 3 1 
Table 1: The depth of the hierarchy by CSM
  
 
 
 
 
 
the map are shown in Fig. 4. The lines of the hi-
erarchies go up from the bottom right hand cor-
ner to the upper left hand corner and then turn 
towards the upper right hand corner. The loca-
tion of hierarchies of ?nouryoku (ability), sainou 
(talent) ?? is similar to this one. 
The hyperonym of ?teido (degree)? is ?joutai 
(state)?. In Fig.5 these abstract nouns are located 
at the bottom of the map. The location of hierar-
chies of ?kurai (rather than)? and ?hou (compara-
tively)? are similar to this one. The hierarchies of 
?joutai (state), joukyou (situation), yousou (as-
pect), jousei (the state of affairs)? are shown in 
Fig.6. The lines are found at a higher location 
than the line of ?teido(degree)?. The lines of the 
hierarchies of ?joutai (state), ori (when), sakari 
(in the hight of), sanaka (while)? are similar to 
these lines. 
The lines of the hierarchies of ?seikaku (char-
acter)?, ?gaikan (appearance)?and ?utsukushisa 
(beauty)? are similar to each other. We show the 
hierarchies of ?seikaku (character)? in Fig.7. The-
se lines in Fig.7 are located from the right end to 
the upper left hand corner. From the following, 
we can find five main types of hierarchies. 
From the starting point ? koto (matter)?, 
-The hierarchies of ?men (side), inshou (impres-
sion), kanji (feeling), kibun (mood), kimochi 
(feeling)? 
-The hierarchies of ?men (side), sokumen (one- 
side), imi (meaning), kanten (viewpoint), kenchi 
(standpoint)? 
-The hierarchies of ?joutai (state), teido (degree)? 
-The hierarchies of ?joutai (state), jousei (situa-
tion)?  
-The hierarchies of ?men (side), inshou (impres-
sion), seikaku (character) or gaikan (appear-
ance) or utsukushisa (beauty)?.  
The lines in Fig.8 are not peculiar, and appear 
in an area of the hierarchies of ?seikaku (charac-
Fig.3: Hierarchies of  
?kimochi (feeling)? 
Fig.4:Hierarchies of 
?sokumen (one side)? 
Fig.5:Hierarchies of 
?teido (degree)? 
Fig8: Hierarchies of 
?kanshoku (feel)? 
Fig.6:  Hierarchies of  
?jousei (situation)? 
Fig.7:Hierarchies of 
?seikaku (character)? 
ter)? in Fig.7. As Fig.8 shows, the hierarchies of 
?men (side), inshou (impression), kanji (feeling), 
kanshoku (feel) or kansei (sensitivity)? are lo-
cated in the area of the hierarchies of ?seikaku 
(character)?, above the hierarchies of ?kimochi 
(feeling)? in Fig.3. 
5. Comparison of hierarchies of super-
ordinate nouns of adjectives. 
We compare the hierarchy mentioned above 
with ones obtained from two kinds of data. 
1) Hierarchies obtained by: 
CSM and Yate?s correction 
corpus occurrence data (no frequency). 
2) Hierarchies obtained by: 
Tf.CSM and Yate?s correction 
corpus frequency data. 
3) Hierarchies obtained by: 
Overlap coefficient and Yates' correction 
corpus occurrence data (no frequency). 
 
As both CSM and the Overlap coefficient are 
?measures of inclusion?, we compared CSM and 
Tf.CSM with the Overlap coefficient. 
The number of groups that were obtained by 
CSM, Tf.CSM and the Overlap coefficient are 
the following. 
Table 2. Total number of groups obtained from CSM, 
Tf.CSM and Ovlp (Overlap) 
 groups 
CSM 161 
Tf.CSM 158 
Ovlp 240 
The Depth of hierarchies obtained from CSM, 
Tf.CSM, and the Overlap coefficient are as fol-
lows: 
Table 3. The hierarchy depth for CSM, Tf.CSM,  
and the Overlap coefficient 
 
In the case of CSM, there are 32 groups at 
depth 7, which is the greatest number of groups. 
The greatest concentration of groups is at depth 5 
to 10. In the case of Tf.CSM, the greatest number 
of groups is 25 at depth 8. The greatest concen-
tration of groups is at depth 5 to 13. In the case of 
the overlap coefficient, the greatest number of 
groups is 61 at depth 5. The greatest concentra-
tion of groups is at depth 3 to 7. 
0
10
20
30
40
50
60
70
3 4 5 6 7 8 9 10 11 12 13 14 15
CSM
Tf.CSM
Ovlp
 
 
 
From this result, we can see that hierarchies 
generated by Tf.CSM are relatively deep, and 
those generated by the Overlap coefficient are 
relatively shallow.  
In the case of the Overlap coefficient, abstract 
nouns in lower layers are sometimes directly re-
lated to abstract nouns in the highest layers. On 
the other hand, in hierarchies generated by CSM 
and Tf.CSM, abstract nouns in the highest layers 
are related to those in the lowest layers via ab-
stract nouns in the middle layers. The following 
indicates the number of overlapping hierarchies 
for CSM, Tf.CSM and Overlap. 
Table 4. The number of overlapping hierarchies 
among CSM, Tf.CSM and Overlap 
CSM&Tf.CSM 37 
CSM&Ovlp 7 
Tf.CSM&Ovlp 2 
CSM&Tf.CSM&Ovlp 7 
The hierarchy generated by Tf.CSM is the 
deepest, and includes some hierarchies generated 
by CSM and the Overlap coefficient. The hierar-
chy generated by CSM is more similar to the one 
made by Tf.CSM than that for the Overlap coef-
ficient: the number of completely corresponding 
hierarchies for CSM and Tf.CSM is 37, that for 
CSM and the Overlap coefficient is 7, and that 
for Tf.CSM and the Overlap coefficient is 2. The 
total number of hierarchies that correspond com-
pletely between CSM, Tf.CSM and the Overlap 
coefficient is 7, and the number of hierarchies 
which are generated by two of the methods and 
included in the third is 57. 
depth 3 4 5 6 7 8 9
CSM 0 3 16 27 32 23 23
Tf.CSM 1 5 10 18 13 25 11
Ovlp 32 56 61 57 21 7 2
depth 10 11 12 13 14 15 
CSM 19 7 3 4 3 1 
Tf.CSM 24 13 14 14 7 2 
Ovlp 2 0 0 0 0 0 
Figure 9. Distribution of hierarchy depth for CSM, 
Tf.CSM, and Overlap coefficient 
We investigated these 64 hierarchies precisely, 
checking adjectives appearing at each depth as 
indicated by an abstract noun in this paper.  In 6 
of these hierarchies, the same adjectives were 
found at all levels of the hierarchy. In 14 of the 
remaining 58 hierarchies, the same adjectives 
were found in all but the deepest level.  These 
20 hierarchies are the most plausible in the strict 
sense of the word. Below, we give examples of 
these hierarchies. In the next stage of this re-
search, we intend to investigate the remaining 44 
hierarchies to determine the reason for the differ-
ence in adjective content. 
The common hyperonym: koto (matter) --- 
men1 (side) --- 
sokumen (one side) --- 
imi (meaning) --- 
kanten (viewpoint) --- 
me2 (eyes) --- 
mikata (view) --- 
hyouka (evaluation) --- 
ippou (while or grow -er and er) --- 
ikioi (force) --- 
sokudo (speed) --- 
jikoku (time) --- 
6. Conclusion 
We have suggested how to make a hierarchy 
of adjectives automatically by connecting 
strongly-related abstract nouns in a top-down 
fashion. We generated a word hierarchy from 
corpus data by using a combination of two 
methods: a self-organizing semantic map and a 
directional similarity measure. As our directional 
similarity measure, we utilized the complement-
ary similarity measure (CSM). Then we com-
pared the hierarchy generated by CSM with that 
generated by Tf.CSM and the Overlap coefficient. 
In the case of Tf.CSM, the hierarchy is deeper 
than the others because there are more abstract 
nouns in the middle layer. In the case of the 
Overlap coefficient, the hierarchy is shallow, but 
there are more hyponyms in the lower layer than 
with the other two methods. As a result, the 
hierarchies generated by CSM have more com-
mon hierarchical relations than those generated 
by the other two methods. In future work, we will 
analyze common hierarchies made by the three 
methods in detail and examine differences among 
them in order to generate an abstract conceptual 
hierarchy of adjectives. We will then compare 
our hierarchy with thesauri compiled manually. 
After we have completed the experiment on Jap-
anese adjectives, we are keen to investigate dif-
ferences and similarities in adjective hypero-
nyms between Japanese and other languages such 
as English by means of our method. 
Acknowledgement 
We would like to thank Dr. Masaki Murata of 
NICT for allowing us to use his drawing tool. 
References  
Nemoto, K. 1969. The combination of the noun with 
?ga-Case? and the adjective, Language research2 
for the computer, National Language Research In-
stitute: 63-73/ 
Takahashi, T. 1975. A various phase related to the 
part-whole relation investigated in the sentence, 
Studies in the Japanese language 103, The society 
of Japanese Linguistics: 1-16. 
Kohonen, T. 1995. Self-Organizing Maps, Springer. 
Hindle, D. 1990. Noun Classification From Predicate-
Argument Structures, In the Proceedings of the 28th 
Annual Meeting of the Association for Computa-
tional Linguistics: 268-275 
Hatzivassiloglou,V. and McKeown,R.K. 1993. To-
wards the Automatic Identification of Adjectival 
Scales: Clustering Adjectives According to Mean-
ing, In the Proceedings of the 31st Annual Meeting 
of the Association for Computational Linguistics: 
172-182.  
Hagita, N. and Sawaki, M. 1995. Robust Recognition 
of Degraded Machine-Printed Characters using 
Complimentary Similarity Measure and Error-
Correction Learning?In the Proceedings of the 
SPIE ?The International Society for Optical Engi-
neering, 2442: 236-244. 
Yamamoto, E. and Umemura, K. 2002. A Similarity 
Measure for estimation of One?to-Many Relation-
ship in Corpus, Journal of Natural Language Proc-
essing: 45-75.  
Hans-Jorg Shmid. 2000. English Abstract Nouns as 
Conceptual Shells, Mouton de Gruyter. 
Kanzaki, K., Ma., Q. and Isahara, H. (2000), Similari-
ties and Differences among Semantic Behaviors of 
Japanese Adnominal Constituents, In the Proceed-
ings of the Syntactic and Semantic Complexity in 
Natural Language Processing Systems, ANLP and 
NAACL. 
Ma, Q., Kanzaki, K., Murata, M., Uchimoto, K. and 
Isahara, H. 2000. Self-Organization Semantic Maps 
of Japanese Noun in Terms of Adnominal Constitu-
ents, In Proceedings of IJCNN?2000, Como, Italy, 
vol.6.: 91-96. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 113?120
Manchester, August 2008
Learning Reliable Information for Dependency Parsing Adaptation
Wenliang Chen
?
, Youzheng Wu
?
, Hitoshi Isahara
?
?
Language Infrastructure Group
?
Spoken Language Communication Group, ATR
?
Machine Translation Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, youzheng.wu, isahara}@nict.go.jp
Abstract
In this paper, we focus on the adaptation
problem that has a large labeled data in the
source domain and a large but unlabeled
data in the target domain. Our aim is to
learn reliable information from unlabeled
target domain data for dependency pars-
ing adaptation. Current state-of-the-art sta-
tistical parsers perform much better for
shorter dependencies than for longer ones.
Thus we propose an adaptation approach
by learning reliable information on shorter
dependencies in an unlabeled target data
to help parse longer distance words. The
unlabeled data is parsed by a dependency
parser trained on labeled source domain
data. The experimental results indicate
that our proposed approach outperforms
the baseline system, and is better than cur-
rent state-of-the-art adaptation techniques.
1 Introduction
Dependency parsing aims to build the dependency
relations between words in a sentence. There
are many supervised learning methods for training
high-performance dependency parsers(Nivre et al,
2007), if given sufficient labeled data. However,
the performance of parsers declines when we are in
the situation that a parser is trained in one ?source?
domain but is to parse the sentences in a second
?target? domain. There are two tasks(Daum?e III,
2007) for the domain adaptation problem. The
first one is that we have a large labeled data in the
source domain and a small labeled data in target
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
domain. The second is similar, but instead of hav-
ing a small labeled target data, we have a large but
unlabeled target data. In this paper, we focus on
the latter one.
Current statistical dependency parsers perform
worse while the distance of two words is becoming
longer for domain adaptation. An important char-
acteristic of parsing adaptation is that the parsers
perform much better for shorter dependencies than
for longer ones (the score at length l is much higher
than the scores at length> l ).
In this paper, we propose an approach by using
the information on shorter dependencies in auto-
parsed target data to help parse longer distance
words for adapting a parser. Compared with the
adaptation methods of Sagae and Tsujii (2007) and
Reichart and Rappoport (2007), our approach uses
the information on word pairs in auto-parsed data
instead of using the whole sentences as newly la-
beled data for training new parsers. It is difficult
to detect reliable parsed sentences, but we can find
relative reliable parsed word pairs according to de-
pendency length. The experimental results show
that our approach significantly outperforms base-
line system and current state of the art techniques.
2 Motivation and prior work
In dependency parsing, we assign head-dependent
relations between words in a sentence. A simple
example is shown in Figure 1, where the arc be-
tween a and hat indicates that hat is the head of a.
Current statistical dependency parsers perform
better if the dependency lengthes are shorter (Mc-
Donald and Nivre, 2007). Here the length of the
dependency from word w
i
to word w
j
is simply
equal to |i ? j|. Figure 2 shows the results (F
1
113
The  boy  saw    a       red       hat    .
Figure 1: An example for dependency relations.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  2  4  6  8  10  12  14  16  18  20
F1
Dependency Length
sameDomain
diffDomain
Figure 2: The scores relative to dependency length.
?SameDomain? refers to training and testing in the
same domain, and ?diffDomain? refers to training
and testing in two domains (domain adaptation).
score)
1
on our testing data, provided by a deter-
ministic parser, which is trained on labeled source
data. Comparing two curves at the figure, we find
that the scores of diffDomain decreases muchmore
sharply than the scores of sameDomain, when de-
pendency length increases. The score decreases
from about 92% at length 1 to 50% at 7. When
lengthes are larger than 7, the scores are below
50%. We also find that the score at length l is much
higher (around 10%) than the score at length l + 1
from length 1 to 7. There is only one exception that
the score at length 4 is a little less than the score at
length 5. But this does not change so much and the
scores at length 4 and 5 are much higher than the
one at length 6.
Two words (word w
i
and word w
j
) having a
dependency relation in one sentence can be adja-
cent words (word distance = 1), neighboring words
(word distance = 2), or the words with distance >
2 in other sentences. Here the distance of word
pair (word w
i
and word w
j
) is equal to |i ? j|. For
example, ?a? and ?hat? has dependency relation in
the sentence at Figure 1. They can also be adjacent
words in the sentence ?The boy saw a hat.? and
the words with distance = 3 in ?I see a red beauti-
ful hat.?. This makes it possible for the word pairs
with different distances to share the information.
According to the above observations, we present
1
F
1
= 2 ? precision ? recall/(precision + recall) where
precision is the percentage of predicted arcs of length d that
are correct and recall is the percentage of gold standard arcs
of length d that are correctly predicted.
an idea that the information on shorter depen-
dencies in auto-parsed target data is reliable for
parsing the words with longer distance for do-
main adaptation. Here, ?shorter? is not exactly
short. That is to say, the information on depen-
dency length l in auto-parsed data can be used to
help parse the words whose distances are longer
than l when testing, where l can be any number.
We do not use the dependencies whose lengthes
are too long because the accuracies of long depen-
dencies are very low.
In the following content, we demonstrate our
idea with an example. The example shows how to
use the information on length 1 to help parse two
words whose distance is longer than 1. Similarly,
the information on length l can also be used to help
parse the words whose distance is longer than l.
Figure 2 shows that the dependency parser per-
forms best at tagging the relations between adja-
cent words. Thus, we expect that dependencies of
adjacent words in auto-parsed target data can pro-
vide useful information for parsing words whose
distances are longer than 1. We suppose that our
task is Chinese dependency parsing adaptation.
Here, we have two words ???JJ(large-scale)?
and ???NN(exhibition)?. Figure 3 shows
the examples in which word distances of these
two words are different. For the sentences in
the bottom part, there is a ambiguity of ?JJ
+ NN1 + NN2? at ?? ?JJ(large-scale)/?
?NN(art)/??NN(exhibition)?, ???JJ(large-
scale)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)? and ???JJ(large-scale)/?
?NR(China)/? ?NN(culture)/? ?NN(art)/?
?NN(exhibition)?. Both NN1 and NN2 could be
the head of JJ. In the examples in the upper part,
???JJ(large-scale)? and ???NN(exhibition)?
are adjacent words, for which current parsers
can work well. We use a parser to parse the
sentences in the upper part. ???(exhibition)? is
assigned as the head of ???(large-scale)?. Then
we expect the information from the upper part
can help parse the sentences in the bottom part.
Now, we consider what a learning model could
do to assign the appropriate relation between ??
?(large-scale)? and ???(exhibition)? in the
bottom part. We provide additional information
that ???(exhibition)? is the possible head of ??
?(large-scale)? in the auto-parsed data (the upper
part). In this way, the learning model may use this
information to make correct decision.
114
A1)?	
///?
A2)?///Coling 2008: Companion volume ? Posters and Demonstrations, pages 119?122
Manchester, August 2008
Construction of an Infrastructure for Providing Users
with Suitable Language Resources
Hitomi Tohyama? Shunsuke Kozawa? Kiyotaka Uchimoto?
Shigeki Matsubara? and Hitoshi Isahara?
?Nagoya University, Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
{hitomi,kozawa,matubara}@el.itc.nagoya-u.ac.jp
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimioto,isahara}@nict.go.jp
Abstract
Our research organization has been con-
structing a large scale database named
SHACHI by collecting detailed meta in-
formation on language resources (LRs) in
Asia and Western countries. The metadata
database contains more than 2,000 com-
piled LRs such as corpora, dictionaries,
thesauruses and lexicons, forming a large
scale metadata of LRs archive. Its meta-
data, an extended version of OLAC meta-
data set conforming to Dublin Core, have
been collected semi-automatically. This
paper explains the design and the structure
of the metadata database, as well as the re-
alization of the catalogue search tool.
1 Introduction
The construction of LRs such as corpora, dictio-
naries, thesauruses, etc., has boomed for years
throughout the world in its aim of encouraging
research and development in the main media of
spoken and written languages, and its importance
has also been widely recognized. Of the organiza-
tions willing to store and distribute LRs, there ex-
ist some consortia fulfilling their function such as
LDC1, ELRA2, CLARIN3, and OLAC4, in West-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1LDC:Linguistic Data Consortium,
http://www.ldc.upenn.edu/
2ELRA: European LRs Association
3CLARIN: Common Language Resources and Technolo-
gies Infrastructure, http://www.ilsp.gr/clarin eng.html
4OLAC: Open Language Archives Community,
http://www.language-archives.org/
ern countries, and GSK5 which does so mainly in
Japan. However, those released LRs are scarcely
connected with each other because of the dif-
ference between written and spoken language as
well as the difference between languages such
as Japanese, English, and Chinese (OLAC User
Guide, 2008).
This situation makes it difficult for researchers
and users to find LRs which are useful for their re-
searches. In the meantime, by connecting system-
atically existing various LRs with Wrapper Pro-
gram, the attempt to realize multilingual transla-
tion services has already begun (Ishida et al 2008,
Hayashi et al 2008). Moreover, since language in-
formation tags given to those LRs and their data
formats are multifarious, each LR is operated in-
dividually. As LR development generally entails
enormous cost, it is highly desirable that the re-
search efficiency be enhanced by systematically
combining those existing LRs altogether and ex-
tending them, which will encourage an efficient
development of unprecedented LRs.
Our research organization has been constructing
a large scale metadata database named SHACHI6
by collecting detailed meta information on LRs
in Western and Asian countries. This research
project aims to extensively collect metadata such
as tag sets, formats, and usage information about
researches on those LRs. and recorded contents of
LRs existing at home and abroad and store them
systematically. Meanwhile, we have already de-
veloped a search system of LRs by the use of meta
information and are attempting the experiment of
widely providing meta information on our stored
5GSK: Gengo Shigen Kyokai; Language Resource Asso-
ciation, http://www.gsk.or.jp/
6SHACHI: Metadata Database of Language Resources-
SHACHI, Shachi means ?orca? in English.
119
Figure 1: A sample page of SHACHI catalogue (ex. Euro WordNet)
LRs to those from researchers to common users.
This metadata database has been now open to the
public in the Web and allows every Internet user
to access it for the search and read information of
LRs at will.
2 Purpose of Metadata Database
Construction
The purpose of the construction of the database is
the following fivefold.
1. To store language resource metadata:
SHACHI semi-manually collects detailed
metadata of language resources and con-
structs their detailed catalogues. Figure 1
shows a sample page of a LR catalogue stored
in SHACHI (ex. Euro WordNet). The cata-
logue provides more detailed meta informa-
tion than other LR consortia do.
2. To systematize language resource meta-
data: Language resource ontology is tenta-
tively constructed by classifying types of lan-
guage resources (in this paper, it is called ?on-
tology?). Figure 2 shows an example of its
ontology. At the moment, it is under investi-
gation what is the most useful and functional
ontology for users by developing some on-
tologies such as human-made ontology, semi-
automatically produced ontology, and auto-
matically produced ontology.
3. To make each language resource related
to each other: The detailed metadata en-
abled us to describe characteristics of each
language resource and to expectably specify
relationships among language resources. Fig-
ure 3 shows a part of the SHACHI search
screen. It shows language resources found as
a search result, the references to which these
language resources conform as well as other
language resources whose formats are com-
mon to theirs.
4. To statistically investigate language re-
sources: By statistically analyzing the meta-
data, users are able to grasp what kinds of lan-
guage resources exist in different part of the
world and to understand current tendencies of
language resources which have been available
to the public.
5. To promote the distribution of language re-
sources: Since this metadata database en-
ables users to easily gain access to language
resources in accordance with their needs,
owing to fully equipped search functions,
SHACHI will be able to support an effective
use and an efficient development of language
resources.
Some 2,000 resources of metadata have already
been collected in the database so far and they will
be enlarged by a further 3,000. To that end, it is
120
Figure 2: Automatically produced ontology
indispensable for us to work in cooperation with
language resource consortia at home and abroad
and to take the initiative in contributing to Asian
language resources.
3 SHACHI Metadata Set
3.1 Policy for Collecting Metadata
The LRs which our metadata database stores
should satisfy the following conditions:
? Those resources should be stored in a digital
format.
? Those resources should be one of the follow-
ing: corpus, dictionary, thesaurus, or lexicon.
(Numeric data are not considered to be the
subject of collection for SHACHI.)
? Those resources should be collected from En-
glish websites and its data must be open to the
public.
? Those resources should be created by re-
search institutions, researchers, or business
organizations. (Developed tools such as facet
search.)
LRs metadata database SHACHI covers meta
information provided by LR consortia such as
ELRA, LDC, and OLAC whose more detailed
metadata are fed into the database by semi-
automatic means of importing.
3.2 Extensions of Metadata Element
Since users sometimes search for LRs without a
clear objective, it is necessary for language re-
source providers to construct language resource
ontology. This database conforms to the OLAC
metadata set which is based on 15 kinds of fun-
damental elements of Dublin Core7 and consti-
tutes an extended vision of OLAC with 19 newly
added metadata elements which were judged to
be indispensable for describing characteristics of
LRs. SHACHI provides usage information about
how and in which situation language resource re-
searchers utilized each language resource, which
is also important for users. The usage informa-
tion about LRs is automatically retrieved from aca-
demic article databases (Kozawa et al 2008). (See
?Utilization? in Figure1).
3.3 Systematic Storage of LRs
Clear description of the relations among LRs can
be applied to the efficient development of LRs
and search tools for common users of database.
Figure 2 shows ontology generated through auto-
matic means, based on language resource metadata
stored in SHACHI. We first surveyed the frequency
of possible values of metadata element choicesand
generated the ontology by hierarchicalizing meta
elements of our meta categories. While ontology
can be constructed in various ways from different
standpoints, our ontology is particularly designed
for users to enable to find them efficiently by fol-
lowing the hierarchical classes of our ontology.
4 Search Tools for Providing
Users-Oriented Information
Figure 3 shows a screen image of a search re-
sult through SHACHI. This section discusses three
search functions provided in SHACHI.
4.1 Three Types of Search Functions
For the purpose of facilitating users of this meta-
data database to find their intended language re-
source catalogues, SHACHI provides three search
functions:
1. Keyword search function: This tool is suit-
able for users who have clear images to search
7Dublin Core Metadata Initiative, http://dublincore.org/
121
Figure 3: Catalogue search tool
for specified LRs and a technical knowledge
of language processing. It allows them to in-
put keywords as they want and to search all
words stored in SHACHI metadata archive.
2. Facet search function: This tool is suitable
for users who have a vague idea of what kind
of LR they want. It is equipped with a choice
of 15 kinds of metadata elements selected
from the SHACHI metadata set. The users
narrow down the target LRs one by one in
order to find the intended one. For example,
with one click on ?age?, three choices such as
?Childrenfs utterance??, ?Adultsf utterance??
and ?Both are OK?? will be shown.
3. Ontology search function: This tool was de-
veloped by adopting the idea acquired by sys-
tematizing LRs registered in SHACHI. When
using the ontology search function, users find
the intended LRs by following the vertical re-
lationship of the ontology. It was ascertained
that ontology search function tool had the
merit of enabling users to discover LRs that
have not been ever found by keyword search
and facet search functions.
5 Conclusion
In this paper, we reported on the design of
SHACHI, a metadata database of LRs now be-
ing developed, the expansion and construction of
metadata for it, and an actualization of a search
function. At present, it contains approximately
2,000 pieces of meta information on LRs such
as corpora, dictionaries and thesauruses. One of
SHACHIfs characteristic features is that with a
collection of tag sets, format samples, and us-
age information on LRs which is automatically re-
trieved from scholarly papers given to LRs. From
now on the SHACHI project is intended to promote
cooperation among other LRs consortia abroad as
well as in Japan and to take the initiative in con-
tributing to the development of LRs in Asia.
References
Ishida,T., Nadamoto, A., Murakami,Y., Inaba, R. et al
2008. A Non-Profit Operation Model for the Lan-
guage Grid, In proceedings of the 1st International
Conference on Global Interoperability for language
Resources, pp.114-121.
Kozawa, S., Tohyama, H., Uchimoto, K., Matsubara,
S., and Isahara. H. 2008. Automatic Acquisition of
Usage Infor-mation for Language Resources, In pro-
ceedings of the 6th edition of the Language Re-
sources and Evaluation Conference.
OLAC (Open Language Archives Communi-ty), 2008.
Searching of OLAC Metadata: User Guide, http://
www.language-archives.org/tools/search/searchDoc.html
Yoshihito Hayashi, Thierry Declerck, Paul Buitelaar,
Monica Monachini. 2008. Ontologies for a Global
Language Infrastructure, In proceedings of the 1st
International Conference on Global Interoperability
for language Resources, pp.105-112.
122
Coling 2008: Companion volume ? Posters and Demonstrations, pages 123?126
Manchester, August 2008
Experiments in Base-NP Chunking and Its Role in
Dependency Parsing for Thai
Shisanu Tongchim, Virach Sornlertlamvanich
Thai Computational Linguistics Laboratory
NICT Asia Research Center
112 Paholyothin Road
Klong 1, Klong Luang
Pathumthani 12120, Thailand
{shisanu,virach}@tcllab.org
Hitoshi Isahara
NICT
3-5, Hikari-dai, Seika-cho
Soraku-gun, Kyoto, 619-0289, Japan
isahara@nict.go.jp
Abstract
This paper studies the role of base-NP in-
formation in dependency parsing for Thai.
The baseline performance reveals that the
base-NP chunking task for Thai is much
more difficult than those of some lan-
guages (like English). The results show
that the parsing performance can be im-
proved (from 60.30% to 63.74%) with the
use of base-NP chunk information, al-
though the best chunker is still far from
perfect (F?=1 = 83.06%).
1 Introduction
Many NLP applications require syntactic informa-
tion and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages. In case of Thai, the research in
developing tools for syntactic analysis and syntac-
tically annotated corpora is still limited. Most re-
search in the past has focused on morphological
analysis (i.e. word segmentation, part-of-speech
(POS) tagging). This can be viewed as a bottle-
neck for developing NLP applications that require
a deeper understanding of the language.
We have an ongoing project in developing a syn-
tactically annotated corpus. To accelerate the cor-
pus annotation, some syntactic analysis tools can
be applied in a preprocessing step before correct-
ing the results by human annotators. In this pa-
per, we use the first portion of completely anno-
tated corpus to examine the dependency parsing
and base-NP chunking. The findings will provide
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
some guidelines in selecting a parser and a base-
NP chunker for our corpus annotation workflow.
2 Dependency Parsing for Thai
The dependency structure for Thai is more flexible
than some languages like Japanese (Sekine et al,
2000), Turkish (Eryigit and Oflazer, 2006), while
it is close to Chinese (Cheng et al, 2005) and En-
glish (Nivre and Scholz, 2004). An example of
a Thai sentence with dependency relations is out-
lined in Fig. 1. Note that the dependency links are
drawn from the dependents to their heads. The de-
pendency relations of Thai are bidirectional in na-
ture and the root node can be found in arbitrary
positions. Some languages (e.g. Japanese) have
more constrained dependency structures, for ex-
ample, the dependency relations are only from left
to right and the root node is at the rightmost. Due
to the lack of structural constraints and larger num-
ber of possible candidates, finding the correct de-
pendency structure for Thai is more difficult.
??? ??????? ??? ????? ?? ???? ???????
Teacher assign for(to) each person read book
?The teacher assigns each person to read a book?
Figure 1: An example of a Thai sentence with de-
pendency relations.
There are only few studies investigating the de-
pendency parsing for Thai. To our knowledge, the
first research regarding dependency analysis was
done in (Aroonmanakun, 1989). However, this re-
search is based on a very small corpus (50 sen-
tences). The lack of syntactically annotated cor-
123
pora may be a possible explanation why not much
research has been done in this area. Some have
been developed, but they are relatively small or not
public, for example, a treebank of 400 sentences
used in (Satayamas et al, 2005).
To overcome the shortage of corpora, we initiate
the development of a syntactically annotated cor-
pus. This corpus will be used as a fundamental lin-
guistic resource for various projects. To improve
the annotation workflow, we use the first portion
of completely annotated corpus in experimenting
with dependency parsing and base-NP chunking.
The results will be used to improve the preprocess-
ing step of annotation.
Two dependency parsers are included in our ex-
periments. Both are data-driven.
? Model 1 : The first model has been widely
studied in parsing Japanese text. Some ma-
chine learning techniques are used to estimate
the probability that word wi modifies word
wj . Thus, the probability matrix of binary de-
pendency relations can be derived from this
estimation. Some search algorithms are then
used to find the most probable dependency
structure. In this study, we use support vec-
tor machines (SVMs) to estimate the proba-
bility values and use a beam search algorithm
to find the most likely dependency structure.
In parsing Japanese text, the root position is
not an issue. For Thai, however, we have to
identify the root position before finding the
complete dependency relations. Thus, we in-
corporate an additional module to identify the
root node of the sentence. This root finding
module is also based on an SVM.
The root finding module selects the word with
highest probability of being the root node.
The following features are used in the root
model: 1. POS, 2. position, 3. number of
verbs, 4. number of equivalent POS in front
of this word, 5. number of equivalent POS af-
ter this word, 6. number of equivalent major
POS in front of this word and 7. number of
equivalent major POS after this word.
For building the dependency model (e.g. re-
lation between wi and wj), the following fea-
tures are used: 1. POS of wi and wj , 2. de-
pendency direction, 3. distance, 4. major cat-
egory of wi and wj , 5. major POS of wi and
wj and 6. positions of wi and wj .
Table 1: Performance of dependency parsing
RA DA CSA
Model 1? 85.4% 76.0% 44.8%
Model 1? 86.2% 77.5% 47.9%
Model 2? 89.31% 83.53% 60.30%
Model 2? 91.22% 86.03% 65.27%
Note: ? (without chunk), ? (with chunk)
After identifying the root node and creating
the probability matrix, the beam search (beam
width=3) is performed.
? Model 2 : For the second model, we adopt
MaltParser 1.0.4 (Nivre et al, 2007) which is
a shift-reduce parser. Machine learning algo-
rithms are used to predict the sequence of ac-
tions for parsing. In this study, we use the
default setting that utilizes an SVM for pre-
dicting parsing actions.
Assuming that {i0, i1, i2, i4} are the first four
tokens in the remaining input and {s0, s1} are
the two topmost tokens on the stack, we use
the default features including: 1. POS of {i0,
i1, i2, i3, s0, s1}, 2. word form of {s0, i0,
i1, head(s0)}, 3. dependency type of s0 and
its leftmost and rightmost dependent and the
leftmost dependent of i0.
To examine the role of base-NP chunk infor-
mation in dependency parsing, we include chunk
labels in the feature sets of both parsers. Base-
NP chunks are represented by using the IOB2 for-
mat (Sang and Veenstra, 1999). In the first parsing
model, the chunk label of the current word is added
as a feature of the root model, while the chunk la-
bels of both considered words are added in the de-
pendency model. We also add a feature showing
that both words reside in the same chunk or not to
the dependency model. In the second model, we
include chunk labels of {s0, s1, i0, i1, i2, i3} as its
feature set.
We use a section of completely annotated corpus
consisting of 2616 sentences to experiment with
dependency parsing. The sentence length ranges
between 2 words to 20 words with an average of
5.68. These Thai sentences are part of our Thai-
Japanese parallel corpus developed for the MT
project. Since our MT project aims for the con-
versation domain, the source sentences are adopted
mainly from dialogues and conversation books. A
morphological analyzer is applied to these Thai
124
sentences for word segmentation and POS tagging,
and the results are revised manually by our annota-
tors. The sentences are then assigned chunk labels
with IOB2 representation and syntactic structure
respectively.
The corpus is divided into 2355 sentences as
the training set and 261 sentences as the test set.
The experiment is done with gold-standard POS
tags and chunk labels. Three performance met-
rics are used: 1. Root accuracy (RA): a portion
of sentences with correctly identified roots, 2. De-
pendency accuracy (DA): a ratio of correct de-
pendency relations to all dependency links and 3.
Complete sentence accuracy (CSA): a portion of
sentences with correct roots and dependency pat-
terns.
Table 1 shows the accuracy of two parsers with
and without using chunk information. The results
show that chunk information helps in improving
the performance of both parsers, especially in the
number of completely correct sentences. Malt-
Parser (Model 2) which is a shift-reduce parser
performs better in parsing Thai sentences. This
conforms with previously published literature that
shift-reduce parsers have been widely applied to
languages with dependency structure close to Thai
(e.g. English and Chinese), while variants of
Model 1 are applied to languages with more con-
straints in dependency structure (like Japanese).
Although the parsing accuracy can be improved
by chunk information, the results are based on
gold-standard chunk labels. To examine the possi-
bility for deriving chunk labels automatically, we
implement and evaluate base-NP chunkers in the
next section.
3 Base-NP Chunking
We implement a simplified version of Kudo?s
chunker (Kudo and Matsumoto, 2001). Kudo?s
chunker obtained very promising results on
standard English chunking tasks (e.g. preci-
sion=94.2%, recall=94.3%, F?=1 = 94.2%). We
use forward parsing method and employ an SVM
for identifying chunk labels. The original feature
set of Kudo?s chunker consists of: word form, POS
and previous chunk labels. Specifically, the fol-
lowing features are used in identifying the chunk
label of the word wi: word form and POS of
{wi?2, wi?1, wi, wi+1, wi+2}, chunk labels of
{wi?2, wi?1}. However, some preliminary results
show that the original feature set does not work
well with our problem. The obtained model suffers
from overfitting and lack of generalization. Thus,
we modify the feature set as: POS of {wi?2, wi?1,
wi, wi+1}, chunk labels of {wi?2, wi?1} and the
current size of NP chunk in front of wi. An SVM
is trained to estimate the probability of the current
word being each of three chunk labels (B, I, O). A
beam search strategy is used to find the most prob-
able chunk sequence.
In additional to the SVM-based chunkers, we
also examine chunkers based on conditional ran-
dom fields (CRFs). We use the implementation
of CRF++ (Kudo, 2008). CRFs outperform sev-
eral methods on this task (Sha and Pereira, 2003).
Three CRF-based chunkers are included in the ex-
periment: the first one uses word form and POS as
its feature set, the second one includes word class
(function word, content word) as an additional fea-
ture, the third one uses the previous three features
and the major POS category.
We use the training set and test set from pre-
vious section to experiment with chunking. Ta-
ble 2 shows the performance of all chunkers. A
baseline algorithm selects the chunk label which is
most frequently associated with POS of the cur-
rent word. From the results, all chunkers out-
perform the baseline algorithm. The best per-
formance can be obtained by one of CRF-based
chunkers (F?=1 = 83.06%). The inclusion of
more features for CRF-based chunkers helps in
improving the performance. In contrast, SVM-
based chunkers tend to suffer from overfitting
when adding more features. The results also con-
firm the findings of (Sha and Pereira, 2003) that
CRF-based chunkers can beat any single model.
However, the results are still lower than the re-
sults found in English experiments. A reason
may be that Thai NPs are more ambiguous than
English NPs. This is confirmed by a compari-
son between our baseline result (F?=1=55.4%) and
some baseline results of English base-NP chunk-
ing task (e.g. precision=81.9%, recall=78.2%,
F?=1=80.0% (Ramshaw and Marcus, 1995)).
Since the baseline algorithms work exactly in the
same way, the results imply that the Thai chunking
task is more difficult.
We also examine the use of the best chunker as
a preprocessing step of dependency parsing. Us-
ing the parser Model 2, the results are as follows:
RA=90.84%, DA=84.99%, CSA=63.74%. Over-
all, the accuracy of using predicted chunk labels is
125
Table 2: Performance of base-NP chunking
Pr. R. F?=1
Baseline 48.5% 64.5% 55.4%
SVM+beam search
beam width=1 70.1% 65.5% 67.7%
beam width=3 70.6% 66.6% 68.5%
beam width=5 69.6% 65.5% 67.5%
beam width=10 71.0% 66.9% 68.9%
beam width=20 71.0% 66.9% 68.9%
CRF
word+POS 84.79% 78.52% 81.54%
word+POS+class 85.34% 79.93% 82.54%
word+POS+class+main POS 86.04% 80.28% 83.06%
lower than the use of gold-standard chunk labels,
but still better than without any chunk information.
Although the chunking accuracy is not high as in
the reported results of English chunking tasks, the
results show that the dependency parsing still ben-
efits from the predicted chunk information.
4 Conclusions
The results from the chunking task show that the
chunk identification for Thai is not trivial due to
ambiguities in Thai NPs. The CRF-based chunkers
(best:F?=1 = 83.06%) are found to be more effec-
tive than the SVM-based chunkers (best:F?=1 =
68.9%).
Using the predicted chunk labels from the best
chunker in dependency parsing, the performance
of the best dependency parser can be improved
from CSA:60.30% to CSA:63.74%. This accu-
racy may further be improved if the performance
of chunker can be increased (as is shown in parsing
accuracy when using gold-standard chunk labels).
References
Aroonmanakun, Wirote. 1989. A dependency analy-
sis of thai sentences for a computerized parsing sys-
tem. Master thesis, Department of Linguistics, Chu-
lalongkorn University.
Cheng, Yuchang, Masayuki Asahara, and Yuji Mat-
sumoto. 2005. Chinese deterministic dependency
analyzer: Examining effects of global features and
root node finder. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 17?24.
Eryigit, Gu?lsen and Kemal Oflazer. 2006. Statistical
dependency parsing for turkish. In EACL. The As-
sociation for Computer Linguistics.
Kudo, Taku and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL.
Kudo, Taku. 2008. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
Nivre, Joakim and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of Coling 2004, pages 64?70, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, Gu?ls?en Eryig?it, Sandra Ku?bler, Stetoslav
Marinov, and Erwin Marsi. 2007. Maltparser: A
language-independent system for data-driven depen-
dency parsing. Natural Language Engineering Jour-
nal, 13(2):99?135.
Ramshaw, Lance A. and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very Large
Corpora, pages 82?94.
Sang, Erik F. Tjong Kim and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of the ninth
conference on European chapter of the Association
for Computational Linguistics, pages 173?179, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Satayamas, Vee, Chalatip Thumkanon, and Asanee
Kawtrakul. 2005. Bootstrap cleaning and quality
control for Thai tree bank construction. In The 9th
National Computer Science and Engineering Con-
ference, Bangkok, Thailand, Oct 27?Oct 28. (In
Thai).
Sekine, Satoshi, Kiyotaka Uchimoto, and Hitoshi Isa-
hara. 2000. Backward beam search algorithm for
dependency analysis of japanese. In COLING, pages
754?760. Morgan Kaufmann.
Sha, Fei and Fernando C. N. Pereira. 2003. Shal-
low parsing with conditional random fields. In HLT-
NAACL.
126
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1129?1133,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Two-stage Parser for Multilingual Dependency Parsing
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a two-stage multilingual de-
pendency parsing system submitted to the
Multilingual Track of CoNLL-2007. The
parser first identifies dependencies using a
deterministic parsing method and then labels
those dependencies as a sequence labeling
problem. We describe the features used in
each stage. For four languages with differ-
ent values of ROOT, we design some spe-
cial features for the ROOT labeler. Then we
present evaluation results and error analyses
focusing on Chinese.
1 Introduction
The CoNLL-2007 shared tasks include two tracks:
the Multilingual Track and Domain Adaptation
Track(Nivre et al, 2007). We took part the Multi-
lingual Track of all ten languages provided by the
CoNLL-2007 shared task organizers(Hajic? et al,
2004; Aduriz et al, 2003; Mart?? et al, 2007; Chen
et al, 2003; Bo?hmova? et al, 2003; Marcus et al,
1993; Johansson and Nugues, 2007; Prokopidis et
al., 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003) .
In this paper, we describe a two-stage parsing
system consisting of an unlabeled parser and a se-
quence labeler, which was submitted to the Multi-
lingual Track. At the first stage, we use the pars-
ing model proposed by (Nivre, 2003) to assign the
arcs between the words. Then we obtain a depen-
dency parsing tree based on the arcs. At the sec-
ond stage, we use a SVM-based approach(Kudo and
Matsumoto, 2001) to tag the dependency label for
each arc. The labeling is treated as a sequence la-
beling problem. We design some special features
for tagging the labels of ROOT for Arabic, Basque,
Czech, and Greek, which have different labels for
ROOT. The experimental results show that our ap-
proach can provide higher scores than average.
2 Two-Stage Parsing
2.1 The Unlabeled Parser
The unlabeled parser predicts unlabeled directed de-
pendencies. This parser is primarily based on the
parsing models described by (Nivre, 2003). The al-
gorithm makes a dependency parsing tree in one left-
to-right pass over the input, and uses a stack to store
the processed tokens. The behaviors of the parser
are defined by four elementary actions (where TOP
is the token on top of the stack and NEXT is the next
token in the original input string):
? Left-Arc(LA): Add an arc from NEXT to TOP;
pop the stack.
? Right-Arc(RA): Add an arc from TOP to
NEXT; push NEXT onto the stack.
? Reduce(RE): Pop the stack.
? Shift(SH): Push NEXT onto the stack.
Although (Nivre et al, 2006) used the pseudo-
projective approach to process non-projective de-
pendencies, here we only derive projective depen-
dency tree. We use MaltParser(Nivre et al, 2006)
1129
V0.41 to implement the unlabeled parser, and use
the SVM model as the classifier. More specifically,
the MaltParser use LIBSVM(Chang and Lin, 2001)
with a quadratic kernel and the built-in one-versus-
all strategy for multi-class classification.
2.1.1 Features for Parsing
The MaltParser is a history-based parsing model,
which relies on features of the derivation history
to predict the next parser action. We represent the
features extracted from the fields of the data repre-
sentation, including FORM, LEMMA, CPOSTAG,
POSTAG, and FEATS. We use the features for all
languages that are listed as follows:
? The FORM features: the FORM of TOP and
NEXT, the FORM of the token immediately
before NEXT in original input string, and the
FORM of the head of TOP.
? The LEMMA features: the LEMMA of TOP
and NEXT, the LEMMA of the token immedi-
ately before NEXT in original input string, and
the LEMMA of the head of TOP.
? The CPOS features: the CPOSTAG of TOP and
NEXT, and the CPOSTAG of next left token of
the head of TOP.
? The POS features: the POSTAG of TOP and
NEXT, the POSTAG of next three tokens af-
ter NEXT, the POSTAG of the token immedi-
ately before NEXT in original input string, the
POSTAG of the token immediately below TOP,
and the POSTAG of the token immediately af-
ter rightmost dependent of TOP.
? The FEATS features: the FEATS of TOP and
NEXT.
But note that the fields LEMMA and FEATS are not
available for all languages.
2.2 The Sequence Labeler
2.2.1 The Sequence Problem
We denote by x = x
1
, ..., xn a sentence with n
words and by y a corresponding dependency tree. A
dependency tree is represented from ROOT to leaves
1The tool is available at
http://w3.msi.vxu.se/?nivre/research/MaltParser.html
with a set of ordered pairs (i, j) ? y in which xj is a
dependent and xi is the head. We have produced the
dependency tree y at the first stage. In this stage, we
assign a label l
(i,j) to each pair.
As described in (McDonald et al, 2006), we treat
the labeling of dependencies as a sequence labeling
problem. Suppose that we consider a head xi with
dependents xj1, ..., xjM . We then consider the la-
bels of (i, j1), ..., (i, jM) as a sequence. We use the
model to find the solution:
lmax = arg max
l
s(l, i, y, x) (1)
And we consider a first-order Markov chain of la-
bels.
We used the package YamCha (V0.33)2 to imple-
ment the SVM model for labeling. YamCha is a
powerful tool for sequence labeling(Kudo and Mat-
sumoto, 2001).
2.2.2 Features for Labeling
After the first stage, we know the unlabeled de-
pendency parsing tree for the input sentence. This
information forms the basis for part of the features
of the second stage. For the sequence labeler, we
define the individual features, the pair features, the
verb features, the neighbor features, and the position
features. All the features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The pair features: the direction of depen-
dency, the combination of lemmata of the
parent and child node, the combination of
parent?s LEMMA and child?s CPOSTAG, the
combination of parent?s CPOSTAG and child?s
LEMMA, and the combination of FEATS of
parent and child.
? The verb features: whether the parent or child
is the first or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
2YamCha is available at
http://chasen.org/?taku/software/yamcha/
1130
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
2.2.3 Features for the Root Labeler
Because there are four languages have different
labels for root, we define the features for the root
labeler. The features are listed as follows:
? The individual features: the FORM, the
LEMMA, the CPOSTAG, the POSTAG, and
the FEATS of the parent and child node.
? The verb features: whether the child is the first
or last verb in the sentence.
? The neighbor features: the combination of
CPOSTAG and LEMMA of the left and right
neighbors of the parent and child, number of
children, CPOSTAG sequence of children.
? The position features: whether the child is the
first or last word in the sentence and whether
the child is the first word of left or right of par-
ent.
3 Evaluation Results
We evaluated our system in the Multilingual Track
for all languages. For the unlabeled parser, we chose
the parameters for the MaltParser based on perfor-
mance from a held-out section of the training data.
We also chose the parameters for Yamcha based on
performance from training data.
Our official results are shown at Table 1. Perfor-
mance is measured by labeled accuracy and unla-
beled accuracy. These results showed that our two-
stage system can achieve good performance. For all
languages, our system provided better results than
average performance of all the systems(Nivre et al,
2007). Compared with top 3 scores, our system
provided slightly worse performance. The reasons
may be that we just used projective parsing algo-
rithms while all languages except Chinese have non-
projective structure. Another reason was that we did
not tune good parameters for the system due to lack
of time.
Data Set LA UA
Arabic 74.65 83.49
Basque 72.39 78.63
Catalan 86.66 90.87
Chinese 81.24 85.91
Czech 73.69 80.14
English 83.81 84.91
Greek 74.42 81.16
Hungarian 75.34 79.25
Italian 82.04 85.91
Turkish 76.31 81.92
average 78.06 83.22
Table 1: The results of proposed approach. LA-
BELED ATTACHMENT SCORE(LA) and UNLA-
BELED ATTACHMENT SCORE(UA)
4 General Error Analysis
4.1 Chinese
For Chinese, the system achieved 81.24% on labeled
accuracy and 85.91% on unlabeled accuracy. We
also ran the MaltParser to provide the labels. Be-
sides the same features, we added the DEPREL fea-
tures: the dependency type of TOP, the dependency
type of the token leftmost of TOP, the dependency
type of the token rightmost of TOP, and the de-
pendency type of the token leftmost of NEXT. The
labeled accuracy of MaltParser was 80.84%, 0.4%
lower than our system.
Some conjunctions, prepositions, and DE3 at-
tached to their head words with much lower ac-
curacy: 74% for DE, 76% for conjunctions, and
71% for prepositions. In the test data, these words
formed 19.7%. For Chinese parsing, coordination
and preposition phrase attachment were hard prob-
lems. (Chen et al, 2006) defined the special features
for coordinations for chunking. In the future, we
plan to define some special features for these words.
Now we focused words where most of the errors
occur as Table 2 shows. For ??/DE?, there was
32.4% error rate of 383 occurrences. And most of
them were assigned incorrect labels between ?prop-
erty? and ?predication?: 45 times for ?property? in-
stead of ?predication? and 20 times for ?predica-
tion? instead of ?property?. For examples, ??/DE?
3including ??/?/?/??.
1131
num any head dep both
?/ DE 383 124 35 116 27
a/ C 117 38 36 37 35
?/ P 67 20 6 19 5
??/ N 31 10 8 4 2
?/ V 72 8 8 8 8
Table 2: The words where most of errors occur in
Chinese data.
in ???/?/??/??(popular TV channel)? was
to be tagged as ?property? instead of ?predication?,
while ??/DE? in ????/?/??(volunteer of
museum)? was to be tagged as ?predication? instead
of ?property?. It was very hard to tell the labels be-
tween the words around ???. Humans can make
the distinction between property and predication for
???, because we have background knowledge of
the words. So if we can incorporate the additional
knowledge for the system, the system may assign
the correct label.
For ?a/C?, it was hard to assign the head, 36
wrong head of all 38 errors. It often appeared at
coordination expressions. For example, the head
of ?a? at ??/?/?/?/a/?/?/?/??/(Besides
extreme cool and too amazing)? was ????, and
the head of ?a? at ????/??/?/??/a/?/?
?/?/??(Give the visitors solid and methodical
knowledge)? was ????.
5 Conclusion
In this paper, we presented our two-stage depen-
dency parsing system submitted to the Multilingual
Track of CoNLL-2007 shared task. We used Nivre?s
method to produce the dependency arcs and the se-
quence labeler to produce the dependency labels.
The experimental results showed that our system can
provide good performance for all languages.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library
for support vector machines. Software available at
http://www. csie. ntu. edu. tw/cjlin/libsvm, 80:604?
611.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006.
An empirical study of chinese chunking. In COL-
ING/ACL 2006(Poster Sessions), Sydney, Australia,
July.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 216?220, New
York City, June. Association for Computational Lin-
guistics.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
1132
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, and S Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2003. An efficient algorithm for projective
dependency parsing. Proceedings of the 8th Inter-
national Workshop on Parsing Technologies (IWPT),
pages 149?160.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
1133
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
Analysis of an Iterative Algorithm for
Term-Based Ontology Alignment
Shisanu Tongchim, Canasai Kruengkrai, Virach Sornlertlamvanich,
Prapass Srichaivattana, and Hitoshi Isahara
Thai Computational Linguistics Laboratory,
National Institute of Information and Communications Technology,
112,Paholyothin Road, Klong 1, Klong Luang, Pathumthani 12120, Thailand
{shisanu, canasai, virach, prapass}@tcllab.org, isahara@nict.go.jp
Abstract. This paper analyzes the results of automatic concept align-
ment between two ontologies. We use an iterative algorithm to perform
concept alignment. The algorithm uses the similarity of shared terms in
order to find the most appropriate target concept for a particular source
concept. The results show that the proposed algorithm not only finds
the relation between the target concepts and the source concepts, but
the algorithm also shows some flaws in the ontologies. These results can
be used to improve the correctness of the ontologies.
1 Introduction
To date, several linguistic ontologies in different languages have been developed
independently. The integration of these existing ontologies is useful for many
applications. Aligning concepts between ontologies is often done by humans,
which is an expensive and time-consuming process. This motivates us to find an
automatic method to perform such task. However, the hierarchical structures of
ontologies are quite different. The structural inconsistency is a common problem
[1]. Developing a practical algorithm that is able to deal with this problem is a
challenging issue.
The objective of this research is to investigate an automated technique for
ontology alignment. The proposed algorithm links concepts between two ontolo-
gies, namely the MMT semantic hierarchy and the EDR concept dictionary. The
algorithm finds the most appropriate target concept for a given source concept
in the top-down manner. The experimental results show that the algorithm can
find reasonable concept mapping between these ontologies. Moreover, the results
also suggest that this algorithm is able to detect flaws and inconsistency in the
ontologies. These results can be used for developing and improving the ontologies
by lexicographers.
The rest of this paper is organized as follows: Section 2 discusses related
work. Section 3 provides the description of the proposed algorithm. Section
4 presents experimental results and discussion. Finally, Section 5 concludes
our work.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 346?356, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 347
2 Related Work
Daude? et al [2] used a relaxation labeling algorithm ? a constraint satisfaction
algorithm ? to map the verbal, adjectival and adverbial parts between two dif-
ferent WordNet versions, namely WordNet 1.5 and WordNet 1.6. The structural
constraints are used by the algorithm to adjust the weights for the connections
between WN1.5 and WN1.6. Later, some non-structural constraints are included
in order to improve the performance [3].
Asanoma [4] presented an alignment technique between the noun part of
WordNet and Goi-Taikei ?s Ontology. The proposed technique utilizes sets of
Japanese and/or English words and semantic classes from dictionaries in an MT
system, namely ALT-J/E.
Chen and Fung [5] proposed an automatic technique to associate the English
FrameNet lexical entries to the appropriate Chinese word senses. Each FrameNet
lexical entry is linked to Chinese word senses of a Chinese ontology database
called HowNet. In the beginning, each FrameNet lexical entry is associated with
Chinese word senses whose part-of-speech is the same and Chinese word/phrase
is one of the translations. In the second stage of the algorithm, some links are
pruned out by analyzing contextual lexical entries from the same semantic frame.
In the last stage, some pruned links are recovered if their scores are greater than
the calculated threshold value.
Ngai et al [6] also conducted some experiments by using HowNet. They
presented a method for performing alignment between HowNet and WordNet.
They used a word-vector based method which was adopted from techniques
used in machine translation and information retrieval. Recently, Yeh et al [7]
constructed a bilingual ontology by aligning Chinese words in HowNet with
corresponding synsets defined in WordNet. Their alignment approach utilized
the co-occurrence of words in a parallel bilingual corpus.
Khan and Hovy [8] presented an algorithm to combine an Arabic-English
dictionary with WordNet. Their algorithm also tries to find links from Arabic
words to WordNet first. Then, the algorithm prunes out some links by trying to
find a generalization concept.
Doan et al [9] proposed a three steps approach for mapping between ontologies
on the semantic web. The first step used machine learning techniques to determine
the joint distribution of any concept pair. Then, a user-supplied similarity function
is used to compute similarity of concept pairs based on the joint distribution from
the first step. In the final step, a relaxation labeling algorithm is used to find the
mapping configuration based on the similarity from the previous step.
3 Proposed Algorithm
In this section, we describe an approach for ontology alignment based on term
distribution. To alleviate the structural computation problem, we assume that
the considered ontology structure has only the hierarchical (or taxonomic) rela-
tion. One may simply think of this ontology structure as a general tree, where
each node of the tree is equivalent to a concept.
348 S. Tongchim et al
Given two ontologies called the source ontology Ts and the target ontology
Tt, our objective is to align all concepts (or semantic classes) between these
two ontologies. Each ontology consists of the concepts, denoted by C1, . . . , Ck. In
general, the concepts and their corresponding relations of each ontology can be
significantly different due to the theoretical background used in the construction
process. However, for the lexical ontologies such as the MMT semantic hierarchy
and the EDR concept dictionary, it is possible that the concepts may contain
shared members in terms of English words. Thus, we can match the concepts
between two ontologies using the similarity of the shared words.
In order to compute the similarity between two concepts, we must also con-
sider their related child concepts. Given a root concept Ci, if we flatten the
hierarchy starting from Ci, we obtain a nested cluster, whose largest cluster
dominates all sub-clusters. As a result, we can represent the nested cluster with
a feature vector ci = (w1, . . . , w
|V|
)T , where features are the set of unique En-
glish words V extracted from both ontologies, and wj is the number of the word
j occurring the nested cluster i. We note that a word can occur more than once,
since it may be placed in several concepts on the lexical ontology according to
its sense.
After concepts are represented with the feature vectors, the similarity be-
tween any two concepts can be easily computed. A variety of standard similarity
measures exists, such as the Dice coefficient, the Jaccard coefficient, and the co-
sine similarity [10]. In our work, we require a similarity measure that can reflect
the degree of the overlap between two concepts. Thus, the Jaccard coefficient is
suitable for our task. Recently, Strehl and Ghosh [11] have proposed a version
of the Jaccard coefficient called the extended Jaccard similarity that can work
with continuous or discrete non-negative features. Let ?xi? be the L2 norm of a
given vector xi. The extended Jaccard similarity can be calculated as follows:
JaccardSim(xi,xj) =
xTi xj
?xi?2 + ?xj?2 ? xTi xj
. (1)
We now describe an iterative algorithm for term-based ontology alignment.
As mentioned earlier, we formulate that the ontology structure is in the form of
the general tree. Our algorithm aligns the concepts on the source ontology Ts to
the concepts on the target ontology Tt by performing search and comparison in
the top-down manner.
Given a concept Ci ? Ts, the algorithm attempts to find the most appro-
priate concept B? ? Tt, which is located on an arbitrary level of the hierar-
chy. The algorithm starts by constructing the feature vectors for the current
root concept on the level l and its child concepts on the level l + 1. It then
calculates the similarity scores between a given source concept and candidate
target concepts. If the similarity scores of the child concepts are not greater
than the root concept, then the algorithm terminates. Otherwise, it selects a
child concept having the maximum score to be the new root concept, and it-
erates the same searching procedure. Algorithms 1 and 2 outline our ontology
alignment process.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 349
Algorithm 1. OntologyAlignment
input : The source ontology Ts and the target ontology Tt.
output : The set of the aligned concepts A.
begin
Set the starting level, l ? 0;
while Ts?l? ? Ts?max? do
Find all child concepts on this level, {Ci}ki=1 ? Ts?l?;
Flatten {Ci}ki=1 and build their corresponding feature vectors, {ci}ki=1;
For each ci, find the best matched concepts on Tt,
B ? FindBestMatched(ci);
A ? A ? {B, Ci};
Set l ? l + 1;
end
end
Algorithm 2. FindBestMatched(ci)
begin
Set the starting level, l ? 0;
BestConcept ? Tt(root concept);
repeat
stmp ? JaccardSim(ci, BestConcept);
if Tt?l? > Tt?max? then
return BestConcept;
Find all child concepts on this level, {B}hj=1 ? Tt?l?;
Flatten {Bj}hj=1 and build corresponding feature vectors, {bj}hi=1;
sj? ? argmaxjJaccardSim(ci, {bj}hj=1);
if sj? > stmp then
BestConcept ? Bj? ;
Set l ? l + 1;
until BestConcept does not change;
return BestConcept;
end
Figure 1 shows a simple example that describes how the algorithm works.
It begins with finding the most appropriate concept on Tt for the root concept
1 ? Ts. By flattening the hierarchy starting from given concepts (?1? on Ts,
and ?a?, ?a-b?, ?a-c? for Tt), we can represent them with the feature vectors and
measure their similarities. On the first iteration, the child concept ?a-c? obtains
the maximum score, so it becomes the new root concept. Since the algorithm
cannot find improvement on any child concepts in the second iteration, it stops
the loop and the target concept ?a-c? is aligned with the source concept ?1?. The
algorithm proceeds with the same steps by finding the most appropriate concepts
on Tt for the concepts ?1-1? and ?1-2?. It finally obtains the resulting concepts
?a-c-f? and ?a-c-g?, respectively.
350 S. Tongchim et al
Fig. 1. An example of finding the most appropriate concept on Tt for the root concept
1 ? Ts
4 Experiments and Evaluation
4.1 Data Sets
Two dictionaries are used in our experiments. The first one is the EDR Elec-
tronic Dictionary [12]. The second one is the electronic dictionary of Multilingual
Machine Translation (MMT) project [13].
The EDR Electronic Dictionary consists of lexical knowledge of Japanese
and English divided into several sub-dictionaries (e.g., the word dictionary, the
bilingual dictionary, the concept dictionary, and the co-occurrence dictionary)
and the EDR corpus. In the revised version (version 1.5), the Japanese word
dictionary contains 250,000 words, while the English word dictionary contains
190,000 words. The concept dictionary holds information on the 400,000 concepts
that are listed in the word dictionary. Each concept is marked with a unique
hexadecimal number.
For the MMT dictionary, we use the Thai-English Bilingual Dictionary that
contains around 60,000 lexical entries. The Thai-English Bilingual Dictionary
also contains semantic information about the case relations and the word con-
cepts. The word concepts are organized in a manner of semantic hierarchy. Each
word concept is a group of lexical entries classified and ordered in a hierarchical
level of meanings. The MMT semantic hierarchy is composed of 160 concepts.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 351
In our experiments, we used a portion of the MMT semantic hierarchy and the
EDR concept dictionary as the source and the target ontologies, respectively. We
considered the ?animal? concept as the root concepts and extracted its related con-
cepts. In the EDR concept dictionary, however, the relations among concepts are
very complex and organized in the form of the semantic network. Thus, we pruned
some links to transform the network to a tree structure. Starting from the ?animal?
concept, there are more than 200 sub-concepts (containing about 7,600 words) in
the EDR concept dictionary, and 14 sub-concepts (containing about 400 words) in
the MMT semantic hierarchy. It is important to note that these two ontologies are
considerably different in terms of the number of concepts and words.
4.2 Experimental Results
The proposed algorithm is used to find appropriate EDR concepts for each one of
14 MMT concepts. The results are shown in Table 1. From the table, there are 6 re-
lations (marked with the symbol ?*?) that aremanually classified as exact mapping.
This classification is done by inspecting the structures of both ontologies by hand.
If the definition of a given MMT concept appears in the EDR concept and the algo-
rithm seems to correctly match the most suitable EDR concept, this mapping will
be classified as exact mapping. The remaining 8 MMT concepts, e.g. ?cold-blood?
and ?amphibian?, are mapped to closely related EDR concepts, although they are
not considered to be exact mapping. The EDR concepts found by our algorithm for
these 8 MMT concepts are considered to be only the subset of the source concepts.
For example, the ?amphibian? concept of the MMT is mapped to the ?toad? concept
of the EDR. The analysis in the later section will explain why some MMT concepts
are mapped to specific sub-concepts.
Our algorithm works by flattening the hierarchy starting from the consid-
ered concept in order to construct a word list represented that concept. The
word lists are then compared to match the concepts. In practice, only a por-
tion of word list is intersected. Figure 2 illustrates what happens in general.
Note that the EDR concept dictionary is much larger than the MMT semantic
MMT
321
EDR
Fig. 2. A schematic of aligned concepts
352 S. Tongchim et al
Table 1. Results of aligned concepts between the MMT and the EDR
MMT concept EDR concept
vertebrate vertebrate ?
| ? warm-blood mammal
| | ? mammal mammal ?
| | ? bird bird ?
|
| ? cold-blood reptile
| ? fish fish ?
| ? amphibian toad
| ? reptile reptile ?
| ? snake snake ?
invertebrate squid
| ? worm leech
| ? insect hornet
| ? shellfish crab
| ? other sea creature squid
? These concepts are manually classified as exact mapping.
hierarchy. Thus, it always has EDR words that are not matched with any MMT
words. These words are located in the section 3 of the figure 2. The words
in the section 1 are more important since they affects the performance of the
algorithm. We assume that the EDR is much larger than the MMT. There-
fore, most MMT words should be found in the EDR. The MMT words that
cannot found any related EDR words may be results of incorrect spellings, spe-
cific words (i.e. only found in Thai language). In case of incorrect spelling and
other similar problems, the results of the algorithm can be used to improve the
MMT ontology.
By analyzing the results, we can classify the MMT words that cannot find
any associated EDR words into 4 categories.
1. Incorrect spelling or wrong grammar : Some English words in the MMT
semantic hierarchy are simply incorrect spelling, or they are written with
wrong grammar. For example, one description of a tiger species is written as
?KIND A TIGER?. Actually, this instance should be ?KIND OF A TIGER?.
The algorithm can be used to find words that possible have such a problem.
Then, the words can be corrected by lexicographers.
2. Inconsistency : The English translation of Thai words in the MMT semantic
hierarchy was performed by several lexicographers. When dealing with Thai
words that do not have exact English words, lexicographers usually enter
phrases as descriptions of these words. Since there is no standard of writing
the descriptions, these is incompatibility between descriptions that explain
the same concept. For example, the following phrases are used to describe
fishes that their English names are not known.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 353
? Species of fish
? A kind of fish
? Species of fresh water fish
3. Thai specific words : The words that we used in our experiments are animals.
Several animals are region specific species. Therefore, they may not have any
associated English words. In this case, some words are translated by using
short phrases as English descriptions of these Thai words. Another way to
translate these words is to use scientific names of species.
The problems mentioned earlier make it more difficult to match concepts by
the algorithm. However, we can use the algorithm to identify where the problems
occur. Then, we can use these results to improve the MMT ontology.
The proposed algorithm works in the top-down manner. That is, the algo-
rithm attempts to find the most appropriate concept from the top level, and
it will move down if the lower concepts yield better scores. In order to analyze
the algorithm, we trace the algorithm during moving through the EDR concepts.
The first example of the bird concept alignment is shown in Table 2. The concept
alignment of this example is considered to be exact mapping. The first column
indicates the level of EDR concepts. The second and third columns indicate the
number of MMT words and the number of EDR words after flattening respec-
tively. The fourth column shows the number of intersected words between the
MMT and the EDR. From the table, the algorithm moves through the EDR con-
cepts in order to find the most specific concept that still maintains shared terms.
This example shows that the algorithm passes through 3 concepts until it stops
at the ?bird? concept of the EDR. At the final step, the algorithm decides to trade
few shared terms for a more specific EDR concept. Note that the MMT is not
completely cleaned. When moving down to the EDR bird concept, three shared
terms are lost. Our analysis shows that these terms are bat species. They are
all wrongly classified to the MMT bird concept by some lexicographers. Thus,
these shared terms will not intersect with any words in the EDR bird concept
when the algorithm proceeds to the lower step. This result suggests that our
algorithm is quite robust. The algorithm still finds an appropriate concept even
the MMT ontology has some flaws.
Another analysis of exact mapping is shown in Table 3. The algorithm moves
through 4 concepts until matching the EDR snake concept with the MMT snake
concept. In this example, the number of members in the MMT snake concept is
quite small. However, the number of shared terms is sufficient to correctly locate
the EDR snake concept.
Table 2. Concept alignment for the ?bird? concept
Level MMT words EDR words Intersected words
1 67 2112 26
2 67 1288 26
3 67 373 23
354 S. Tongchim et al
Table 3. Concept alignment for the ?snake? concept
Level MMT words EDR words Intersected words
1 17 2112 8
2 17 1288 8
3 17 71 8
4 17 26 8
The third example shown in Table 4 illustrates the case that is considered to
be subset mapping. That is, the EDR concept selected by the algorithm is sub-
concept of the MMT concept. This case happens several times since the EDR
is more fine-grained than the MMT. If the members of MMT concept do not
cover enough, the algorithm tends to return only sub-concepts. From the table,
the MMT amphibian concept covers only toad and frog species (3 members).
Thus, the algorithm moves down to a very specific concept, namely the EDR
toad concept. Another example of subset mapping is shown in Table 5. This
example also shows that the members of MMT concept do not cover enough.
These results can be used to improve the MMT ontology. If the MMT con-
cepts are extended enough, we expect that the correctness of alignment should
be improved.
Table 4. Concept alignment for the ?amphibian? concept
Level MMT words EDR words Intersected words
1 3 2112 2
2 3 1288 2
3 3 23 2
4 3 16 2
5 3 2 1
Table 5. Concept alignment for the ?other sea creature? concept
Level MMT words EDR words Intersected words
1 17 2112 5
2 17 746 5
3 17 78 3
4 17 3 2
5 Conclusion
We have proposed an iterative algorithm to deal with the problem of automated
ontology alignment. This algorithm works in the top-down manner by using the
similarity of the terms from each ontology. We use two dictionaries in our exper-
iment, namely the MMT semantic hierarchy and the EDR concept dictionary.
Analysis of an Iterative Algorithm for Term-Based Ontology Alignment 355
The results show that the algorithm can find reasonable EDR concepts for given
MMT concepts. Moreover, the results also suggest that the algorithm can be
used as a tool to locate flaws in the MMT ontology. These results can be used
to improve the ontology.
There are several possible extensions to this study. The first one is to examine
this algorithm with larger data sets or other ontologies. The second one is to
improve and correct the ontologies by using the results from the algorithm.
Then, we plan to apply this algorithm to the corrected ontologies, and examine
the correctness of the results. The third one is to use structural information of
ontologies in order to improve the correctness.
References
1. Ide, N. and Ve?ronis, J.: Machine Readable Dictionaries: What have we learned,
where do we go?. Proceedings of the International Workshop on the Future of
Lexical Research, Beijing, China (1994) 137?146
2. Daude?, J., Padro?, L. and Rigau, G.: Mapping WordNets Using Structural Informa-
tion. Proceedings of the 38th Annual Meeting of the Association for Computational
Linguistics, Hong Kong, (2000)
3. Daude?, J., Padro?, L. and Rigau, G.: A Complete WN1.5 to WN1.6 Mapping.
Proceedings of NAACL Workshop ?WordNet and Other Lexical Resources: Appli-
cations, Extensions and Customizations?, Pittsburg, PA, United States, (2001)
4. Asanoma, N.: Alignment of Ontologies: WordNet and Goi-Taikei. Proceedings of
NAACL Workshop ?WordNet and Other Lexical Resources: Applications, Exten-
sions and Customizations?, Pittsburg, PA, United States, (2001) 89?94
5. Chen, B. and Fung, P.: Automatic Construction of an English-Chinese Bilingual
FrameNet. Proceedings of Human Language Technology conference, Boston, MA
(2004) 29?32
6. Ngai, G., Carpuat , M. and Fung, P.: Identifying Concepts Across Languages: A
First Step towards a Corpus-based Approach to Automatic Ontology Alignment.
Proceedings of the 19th International Conference on Computational Linguistics,
Taipei, Taiwan (2002)
7. Yeh, J.-F., Wu, C.-H., Chen, M.-J. and Yu, L.-C.: Automated Alignment and
Extraction of a Bilingual Ontology for Cross-Language Domain-Specific Applica-
tions. International Journal of Computational Linguistics and Chinese Language
Processing. 10 (2005) 35?52
8. Khan, L. and Hovy, E.: Improving the Precision of Lexicon-to-Ontology Alignment
Algorithms. Proceedings of AMTA/SIG-IL First Workshop on Interlinguas, San
Diego, CA (1997)
9. Doan, A., Madhavan, J., Domingos, P., and Halevy, A.: Learning to Map Between
Ontologies on the Semantic Web. Proceedings of the 11th international conference
on World Wide Web, ACM Press (2002) 662?673
10. Manning, C. D., and Schu?tze, H.: Foundations of Statistical Natural Language
Processing. MIT Press. Cambridge, MA (1999)
356 S. Tongchim et al
11. Strehl, A., Ghosh, J., and Mooney, R. J.: Impact of Similarity Measures on Web-
page Clustering. Proceedings of AAAI Workshop on AI for Web Search (2000)
58?64
12. Miyoshi, H., Sugiyama, K., Kobayashi, M. and Ogino, T.: An Overview of the EDR
Electronic Dictionary and the Current Status of Its Utilization. Proceedings of the
16th International Conference on Computational Linguistics (1996) 1090?1093
13. CICC: Thai Basic Dictionary. Center of the International Cooperation for Com-
puterization, Technical Report 6-CICC-MT55 (1995)
A System to Solve Language Tests for Second Grade Students 
Manami Saito 
Nagaoka University of Technology 
saito@nlp.nagaokaut.ac.jp 
Kazuhide Yamamoto 
Nagaoka University of Technology 
yamamoto@fw.ipsj.or.jp 
Satoshi Sekine 
New York University 
Language Craft 
sekine@cs.nyu.edu 
Hitoshi Isahara 
National Institute of Information and Com-
munications Technology 
isahara@nict.go.jp 
 
 
Abstract 
This paper describes a system which 
solves language tests for second grade 
students (7 years old). In Japan, there 
are materials for students to measure 
understanding of what they studied, 
just like SAT for high school students 
in US. We use textbooks for the stu-
dents as the target material of this study. 
Questions in the materials are classified 
into four types: questions about Chi-
nese character (Kanji), about word 
knowledge, reading comprehension, 
and composition. This program doesn?t 
resolve the composition and some other 
questions which are not easy to be im-
plemented in text forms. We built a 
subsystem for each finer type of ques-
tions. As a result, we achieved 55% - 
83% accuracy in answering questions 
in unseen materials. 
1 Introduction 
This paper describes a system which solves lan-
guage tests for second grade students (7 years 
old). We have the following two objections. 
First, we aim to realize the NLP technologies 
into the form which can be easily observed by 
ordinary people. It is difficult to evaluate NLP 
technology clearly by ordinary people. Thus, we 
set the target to answer second grade Japanese 
language test, as an example of intelligible ap-
plication to ordinary people. The ability of this 
program will be shown by scores which are fa-
miliar to ordinary people. 
Second aim is to observe the problems of the 
NLP technologies by degrading the level of tar-
get materials. Those of the current NLP research 
are usually difficult, such as newspapers or tech-
nological texts. They require high accuracy lan-
guage processing, complex world knowledge or 
semantic processing. The NLP problems would 
become more apparent when we degrade the 
target materials. Although questions for second 
grade students also require world knowledge, it 
is expected that the questions become simpler 
and are resolved without tangled techniques.  
2 Related Works 
Hirschman et al (1999) and Charniak et al 
(2000) proposed systems to solve ?Reading 
Comprehension.? Hirschman et al (1999) de-
veloped ?Deep Read,? which is a system to se-
lect sentences in the text which include answers 
to a question. In their experiments, the types of 
questions are limited to ?When,? ?Who? and so 
on. The system is basically an information re-
trieval system which selects a sentence, instead 
of a document, based on the bag-of-words 
method. That system retrieves the sentence con-
taining the answer at 30-40% of the time on the 
tests of third to sixth grade materials. In short, 
Deep Read is very restricted compared to our 
system. Charniak et al (2000) built a system 
improved over the Deep Read by giving more 
weights for verb and subject, and introduced 
heuristic rules for ?Why? question. Though, the 
essential target and method are the same as that 
of Deep Read. 
43
3 Question Classification 
First, we bought five language test books for 
second grade students and one of them, pub-
lished by KUMON, was used as a training text 
to develop our system. The other four books are 
referred occasionally. Second, we classified the 
questions in the training text into four types: 
questions about Chinese character (Kanji), ques-
tions on word knowledge, reading comprehen-
sion, and composition. We will call these types 
as major types. Each of the ?major types? is 
classified into several ?minor types.? Table 1 
shows four major types and their minor types. In 
practice, each minor type farther has different 
style of questions; such as description question, 
choice question, and true-false question. The 
questions can be classified into approximately 
100 categories. We observed that some ques-
tions in other books are mostly similar; however 
there are several questions which are not cov-
ered by the categories. 
 
Major type Minor type 
Kanji Reading, Writing, Radical, The 
order of writing, Classification 
Word knowl-
edge 
Katakana, How to use Kana, Ap-
propriate Noun, To fill blanks for 
Verb, Adjective, and Adjunct, 
Synonym, Antonym, Particle, 
Conjunction, Onomatopoeia, Po-
lite Expression, Punctuation mark
Reading com-
prehension 
Who, What, When, Where, How, 
Why question, Extract specific 
phrases, Progress order of a story, 
Prose and Verse  
Composition Constructing sentence, How to 
write composition 
Table 1. Question types 
4 Answering questions and evaluation 
result 
In this section, we describe the programs to 
solve the questions for each of the 100 catego-
ries and these evaluation results. First we classi-
fied questions. Some questions are difficult to 
cover by the system such as the stroke order of 
writing Kanji. For about 90% of all the question 
types other than such questions, we created pro-
grams for basically one or two categories of 
questions. There are 47 programs for the catego-
ries found in the training data. 
In each section of 4.1 to 4.3, we describe 
how to solve questions of typical types and the 
evaluation results. The evaluation results of the 
total system will be reported in the following 
section. 
Table 2 to 4 show the distributions of minor 
types in each major type, ?Kanji,? ?Word 
knowledge,? and ?Reading comprehension,? in 
the training data and the evaluation results. 
Training and evaluation data have no overlap.  
4.1 Kanji questions 
Most of the Kanji questions are categorized into 
?reading? and ?writing.? Morphological analysis 
is used in the questions of both reading and writ-
ing Kanji; we found that large corpus is effec-
tive to choose the answer from Kanji candidates 
given from dictionary. Table 2 shows it in detail. 
This system cannot answer to the questions 
which are asking the order of writing Kanji, be-
cause it is difficult to put it into digital format.  
The system made 7 errors out of 334 ques-
tions. The most of the questions are the errors in 
reading Kanji by morphological analysis. 
In particular, morphological analysis is the 
only effective method to answer questions on 
this type. It would be very helpful, if we had a 
large corpus considering reading information, 
but there is no such corpus.  
 
Training 
data 
Test data Ques-
tion type
The rate 
of Q in 
training 
data[%]
The used 
knowledge 
and tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Reading 27 Kanji dictionary, 
Morphological 
analysis 
 
96(100) 
 
6(8,8) 
Writing 61 Word diction-
ary, Large 
corpus 
 
220(222) 
 
63(66,66) 
Order of 
writing 
6 -  
0(20) 
 
0(0,2) 
Combi-
nation 
of Kanji 
parts 
3 -  
0(10) 
 
0(0,0) 
Classify 
Kanji 
3 Word diction-
ary, Thesaurus 
 
11(12) 
 
0(0,0) 
Total 100 - 327(364) 69(74,76) 
Table 2. Question types for Kanji 
4.2  Word knowledge questions 
The word knowledge question dealt with vo-
cabulary, different kinds of words and the struc-
ture of sentence. These don?t include Kanji 
questions and reading comprehension. Table 3 
shows different types of questions in this type. 
44
For the questions on antonym, the system can 
answer correctly by choosing most relevant an-
swer candidate using the large corpus out of 
multiple candidates found in the antonym dic-
tionary. 
The questions about synonyms ask relations 
of priority/inferiority between words and choos-
ing the word in a different group. These ques-
tions can usually be answered using thesaurus. 
Ex.1 shows a question about particle, Japa-
nese postposition, which asks to select the most 
appropriate particle for the sentence. 
The system produces all possible sentences 
with the particle choices, and finds most likely 
sentences in a corpus. In Ex.1, all combinations 
are not in a corpus, therefore shorter parts of the 
sentence are used to find in the corpus (e.g. ??
?? (1) ????, ???? (2) ????). In this 
case, the most frequent particle for (1) is ??? in 
a corpus, so this system outputs incorrect answer. 
Ex.1 [?/?/?]????()??????? 
?????? 
 Select particle which fits the sentence from 
{wo,to,ni} 
[1] ??? (1) ??? (2) ??? 
apple-(1) orange-(2) buy 
(1) correct=? (to) system=? (wo) 
(2) correct=? (wo) system=? (wo) 
 
The questions of Katakana can be answered 
mostly by the dictionary. The accuracy of this 
type is not so high, found in Table 2, because 
there are questions asking the origin of the 
words, most Katakana words in Japanese has a 
few origins: borrowed words, onomatopoeia, 
and others. Because we don?t have such knowl-
edge, we could not answer those questions.   
The questions of onomatopoeia include those 
shown in Ex.2. The system uses co-occurrence 
of words in the given sentence and each answer 
candidate to choose the correct answer in Ex.2, 
?????.? However, it was not chosen be-
cause the co-occurrence frequency of ????
?,? the word in the sentence, and ?????,? 
incorrect answer, is higher.  
Ex.2 ??? ???? ???? ??? 
? [ ] ?? ??????? ??????? 
Choose the most appropriate onomatopoeia 
(1) ??? ??? ???? ????  
????(A large object is rowing slowly) 
[??????????????] 
The questions of word knowledge are classi-
fied into 29 types. We made a subsystem for 
each type. As there are possibly more types in 
other books, making a subsystem for each type 
is very costly. One of the future directions of 
this study is to solve this problem. 
Training 
data 
Test data Question 
type 
The rate 
of Q in 
training 
data[%] 
The used 
knowledge and 
tools Correct 
ans. 
(total) 
Correct ans. 
(known type 
Q., total) 
Anonym 18 Antonym 
dictionary, 
Large corpus 
26(27) 12(15,21) 
Synonym 11 Thesaurus 14(17) 34(44,83) 
Particle 19 Large corpus 25(28) 16(17,17) 
Katakana 25 Word diction-
ary, Morpho-
logical analysis 
18(37) 19(22,52) 
Onomato-
poeia 
19 Large corpus, 
Morphological 
analysis 
18(29) 16(20,31) 
Structure 
of sen-
tence 
5 Morphological 
analysis 
7(7) 20(22,22) 
How to 
use kana 
2 - 0(3) 0(0,19) 
Dictation 
of verb 
2 - 0(3) 0(0,0) 
Total 100 - 108(151) 117(140,245)
Table 3. Question types for Word knowledge 
4.3 Reading comprehension questions 
The reading comprehension questions need to 
read a story and answer questions on the story. 
We will describe five typical techniques that are 
used at different types of questions, shown in 
Table 4. 
Pattern matching (a) is used in questions to 
fill blanks in an expression which describes a 
part of the story. In general, the sequence of 
word used in the matching is the entire expres-
sion, but if no match was found, smaller por-
tions are used in the matching. 
Ex.3 Fill blanks in the expression 
Story?partial??????? ???? 
?? ?? ????????? ????  
?? ???? ?????? 
(In a few days, the flower withers and gradu-
ally changes its color to black.) 
Expression???? (1)?(2) ?? ????? 
(The flower (1) and change its color to (2).) 
Answer?(1) ???? (withers) 
(2) ???? (black) 
The effectiveness of this technique is found 
in this example. The other methods will be 
needed when questions will be more difficult. At 
the time, this technique is very useful to solve 
many questions in reading comprehension.  
45
When the question is ?Why? question, key-
words such as ????  (thus)? and ????
(because)? are used. 
For example, when questions start with 
?When (??)? and ?Where (??),? we can 
restrict the type of answer word to time or loca-
tion, respectively. If the question includes the 
expression of ????? (??? is a particle to 
indicate direction/specification), the answer is 
also likely to be expressed with ?ni? right after 
the location in the story. (The kind of NE 
(Named Entity) and particle right after word 
(b)) 
For the questions asking the time or location 
about the entire story, this system outputs the 
appropriate type of the word which appeared 
first in the story. Although there are mistakes 
due to morphological analysis and NE extraction, 
this technique is also consistently very useful.  
The technique which is partial matching 
with keywords (c) is used to seek an answer 
from story for ?how,? ?why? or ?of what? ques-
tions. Keywords from the question are used to 
locate the answer in the story. 
Ex.4 ???????? ?????????
? ???
Frequency in the large corpus is used to 
find the appropriate sentence conjunction. (d) 
Answer is chosen by comparing the mutual in-
formation of the candidate conjunctions and the 
last basic block of the previous sentence. How-
ever, this technique turns out to be not effective. 
Discourse analysis considering wider context is 
required to solve this question. 
The technique which uses distance between 
keywords in question and answers (e) is sup-
plementary to the abovementioned methods. If 
multiple answers are found, the answer candi-
date that is the closest in the story text to the 
keywords in questions is generated. These key-
words are content words and unknown words in 
the text. This technique is found very effective. 
In Table 4, the column ?Used method? shows 
the techniques used to solve the type of ques-
tions, in the order of priority. ?f? in the table 
denotes means that we use a method which was 
not mentioned above.  
????(How big are chicks when 
they hatched?) 
Text?partial??????????? ??  
 ???????????? ??????? 
(The size of chicks when they hatched is about 
the size of your thumb.) 
 
 
 Answer?????? ???? (size of 
thumb)  
 
The rate of Training data Test data
questions in Used corrent wns. corrent ans.
training data [%] methods (total) (known type Q, total)
Who said 5 b,a,f
The others 0 b,e,f
Like what c,b,e
Of what c,f,e
What doing c,a,f
What is a,e
What do A say b,a,f,e
Whole story b
Part of story -
Whole story b
Part of story b,f,c
16 c,f 11(18) 0(1, 1)
10 c 8(11) 0(0, 1)
2 b,c,f 1(2) 0(0, 0)
10 a 10(12) 4(9, 9)
4 - 0(5) 0(0, 0)
2 d 1(2) 1(3, 3)
10 f 8(11) 0(0, 0)
10 f 7(12) 3(3, 3)
1 - 0(1) 0(0, 6)
100 - 74(116) 10(22, 34)
Why
How
How long, how often, how large
Total
Paragraph
The others
To fill blanks
Not have interrogative pronoun
Conjunction
Progress order of a story
Where 4 3(5) 0(1
When 4 3(5) 0(0
Who
Question type
17(26) 1(1, 6)What 22
5(6) 1(4, 4)
, 1)
, 0)
 
Table 4. Question types for Reading comprehension 
46
5 Evaluation 
We collected questions for the test from differ-
ent books of the training data. The proposition 
of the number of questions for different sections 
is not the same as that of the training data. Table 
2 to 4 show the evaluation results in the test data 
for each type. Table 5 shows the summary of the 
evaluation result. In the test, we use only the 
questions of the type in training data. The tables 
also show the total number of questions, the 
number of questions which are solved correctly, 
and the number of questions which are not one 
of the types the system targeted (not a type in 
the training data). 
The ratio of the questions covered by the sys-
tem, questions in test data which have the same 
type in the training data are 97.4% in Kanji, 
57.1% in word knowledge, and 64.7% in read-
ing comprehension. It indicates that about a half 
of the questions in word knowledge isn?t cov-
ered. As the result, accuracy on the questions of 
the covered types in word knowledge is 83.6%, 
but it drops to 47.8% for the entire questions. It 
is because our system classified the questions 
into many small types and builds a subsystem 
for each type. 
The accuracy for the questions of covered 
type is 83.4%. In particular, for the questions of 
Kanji and word knowledge, the scores in the test 
data are nearly the same as those in the training 
data. It presents that the accuracy of the system 
is provided that the question is in the covered 
type. However, the score of reading comprehen-
sion is lower in the test data. We believe that 
this is mainly due to the small test data of read-
ing comprehension (only 34) and that the accu-
racy for ?Who? questions and the questions to 
fill blanks in the test data are quite difficult com-
pared to the training data. 
Num. Num. Num. RCA * RCA* RCA*
of of of (known (total? in total
all Q known corrent type Q) [%] of known
type Q ans. [%] type Q [%]
Kanji 76 74 69 93.2 90.8 89.8
Word knowledge 245 140 117 83.6 47.8 71.5
Reading 
Comprehension
Total 355 235 196 83.4 55.2 80.3
45.5 29.4 63.834 22 10
 
Table 5. Evaluations at test data 
 
* Rate of Correct Answer 
6 Discussions 
We will discuss the problems and future direc-
tions found by the experiments. 
6.1 Recognition and Classification of Ques-
tions 
In order to solve a question in language test, 
students have to recognize the type of the ques-
tion. The current system skips this process. In 
this system, we set up about 100 types of ques-
tions referring the training data and a subpro-
gram solves questions corresponding to each 
type. There are two problems to be solved. First, 
we have to design the appropriate classification 
and avoid unknown types in the test data. From 
the experiment, we found that the current types 
are not enough to solve this problem. Second, 
the program has to classify the questions auto-
matically. We are building this system and are 
forecasting it quite optimistically once a good 
format is provided. 
6.2 Effectiveness of Large Corpus 
The large corpus of newspapers and the Web are 
used effectively in many different cases. We 
will describe several examples. 
In Japanese, there are different Kanji for the 
same reading. For example, Kanji for ???(au: 
to see, to solve correctly) are ???(to see)? or 
???(to solve correctly)? for ?????(to see 
people)? and ??????(to solve an answer 
correctly),? respectively. This type of questions 
can be solved by counting the expressions with 
Kanji in the corpus. It is similar to word sense 
disambiguation. 
In the questions of particle complement, such 
as ??? (umbrella) ??/?/? (locative-, con-
junctive-, and objective particles) ?? (home) 
??/?/?????? (to left) ? (Intentional 
sentence is ?I left the umbrella at home?)?, it can 
be solved by counting the expressions with each 
particle in a corpus. This method is mentioned in 
Matsui?2004?but the evaluation result was 
not reported. When the answer is not found for 
the entire expression, the answer is searched by 
deleting some contexts. Most questions of filling 
blank types, similar strategy is helpful to find 
the correct answer. 
In summary, the experiments showed that the 
large corpus is quite useful in several types of 
47
questions. We believe it would be quite difficult 
to achieve the same accuracy by compiled 
knowledge, such as a dictionary of verbs, anto-
nyms, synonyms, and relation words, and a the-
saurus.  
6.3 World Knowledge 
The questions sometimes need various types of 
world knowledge. For example, ?A student en-
ters junior high school after graduated from 
elementary school.? And ?People become happy, 
if he receives something nice from someone.? It 
is a difficult problem how to describe and how 
use that knowledge. Another type of world 
knowledge includes origin of words, such as 
foreign borrowed word or onomatopoeia. As far 
as we know, there is no comprehensive knowl-
edge of such in electronic form. It is required to 
design attributes of world knowledge and to use 
them flexibly when applying then to solve the 
questions. 
6.4 Difference between Reading Compre-
hension and Question Answering 
The current QA systems identify the NE type of 
questions and seek the answer candidate of the 
type. However, the questions in the reading 
comprehension don?t limit the answer types to 
person and organization, even if the question is 
?Who? type question. For example, ?raccoon 
dog behind our house? or ?the moon? can be the 
answer. Also, the answer is not always a noun 
phrase, but can be a clause, for example, ?the 
time when new leaves growing on a branch? for 
questions asking time. There are different kinds 
of questions, which are asking not the time of 
specific event but the time or season of the en-
tire story. For example ?When is this story 
about?? In this case, the question can?t be an-
swered by just extracting a noun phrase.  
However, at the moment, we can?t conclude 
if the question can or cannot be answered with-
out really understanding it. Sometime, we can 
find a correct answer without reading the story 
down the line or understanding the story per-
fectly. It is one of the future works. 
6.5 Other techniques: discourse and 
anaphora 
Some techniques other than morphological 
analysis, frequency of appearance in a corpus, 
and question answering methods are used in our 
system. We raise two issues. One of those is the 
discourse analysis. It is required in the questions 
to assign the order of paragraphs, and to select 
appropriate sentence conjunction. The other is 
anaphora analysis, which is very important, not 
only to indicate the antecedent, but also to find 
the link of mentions of entities.  
7 Conclusion 
 several inter-
esting NLP problems were found. 
Hi
Comprehension system?.  
Ch
er-based Language Understanding 
K.
of 
K. 
atural Language Processing, 2004, 
 
We develop a system to solve questions of sec-
ond grade language tests. Our objectives are to 
demonstrate the NLP technologies to ordinary 
people, and to observe the problems of NLP by 
degrading the level of target materials. We 
achieved 55% - 83% accuracy and
References 
rschman, L., Light, M., Breck, E. and Burger, J. D. 
?Deep READ: a Reading 
ACL, 1999, pp 325-332. 
arniak et al, ?Reading Comprehension Programs 
in a Statistical-language-Processing Class?. Work-
shop on Reading Comprehension Tests as Evalua-
tion for Comput
Systems. 2000. 
 Matsui: ?Search Technologies on WWW which 
utilize search engines?. (In Japanese) Journal 
Japanese Language, February, 2004, pp 34-43. 
Yoshihira, Y. Takeda, S. Sekine: ?KWIC System 
on WEB documents?, (In Japanese) 10th Annual 
Meeting of N
pp 137-139. 
 
F  
(http://languagecraft.jp/dennou/) 
igure 1. A Snapshot of the system
48
Building an Annotated Japanese-Chinese Parallel Corpus  
? A Part of NICT Multilingual Corpora  
Yujie Zhang and  Kiyotaka Uchimoto and Qing Ma and Hitoshi Isahara 
 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289 
(yujie, uchimoto,qma, isahara)@nict.go.jp
 
Abstract 
We are constricting a Japanese-Chinese 
parallel corpus, which is a part of the 
NICT Multilingual Corpora. The corpus is 
general domain, of large scale of about 
40,000 sentence pairs, long sentences, 
annotated with detailed information and 
high quality. To the best of our knowledge, 
this will be the first annotated Japanese-
Chinese parallel corpus in the world. We 
created the corpus by selecting Japanese 
sentences from Mainichi Newspaper and 
then manually translating them into 
Chinese. We then annotated the corpus 
with morphological and syntactic 
structures and alignments at word and 
phrase levels. This paper describes the 
specification in human translation and the 
scheme of detailed information annotation, 
and the tools we developed in the corpus 
construction. The experience we obtained 
and points we paid special attentions are 
also introduced for share with other 
researches in corpora construction.     
1 Introduction 
A parallel corpus is a collection of articles, 
paragraphs, or sentences in two different languages. 
Since a parallel corpus contains translation 
correspondences between the source text and its 
translations at different level of constituents, it is a 
critical resource for extracting translation 
knowledge in machine translation (MT). Although 
recently some versions of machine translation 
software have become available in the market, 
translation quality is still a significant problem. 
Therefore, a detailed examination into human 
translation is still required. This will provide a basis 
for radically improving machine translation in the 
near future. In addition, in MT system development, 
the example-based method and the statistics-based 
method are widely researched and applied. So, 
parallel corpora are required by the translation 
studies and practical system development.   
The raw text of a parallel corpus contains 
implicit knowledge. If we annotate some 
information, we can get explicit knowledge from 
the corpus. The more information that is annotated 
on a parallel corpus, the more knowledge we can 
get from the corpus. The parallel corpora of 
European languages are usually raw texts without 
annotation on syntactic structure since their 
syntactic structures are similar and MT does not 
require such annotation information. However, 
when language pairs are different in syntactic 
structures, such as the pair of English and Japanese 
and the pair of Japanese and Chinese, 
transformation between syntactic structures is 
difficult. A parallel corpus annotated with syntactic 
structures would thus be helpful to MT.  Besides 
MT, an annotated parallel corpus can be applied to 
cross-lingual information retrieval, language 
teaching, machine-aided translation, bilingual 
lexicography, and word-sense disambiguation.  
Parallel corpora between European languages 
are well developed and are available through the 
Linguistic Data Consortium (LDC). However, 
parallel corpora between European languages and 
Asian languages are less developed, and parallel 
corpora between two Asian languages are even less 
developed.  
The National Institute of Information and 
Communications Technology therefore started a 
project to build multilingual parallel corpora in 
2002 (Uchimoto et al, 2004). The project focuses 
on Asian language pairs and annotation of detailed 
information, including syntactic structure and 
alignment at word and phrase levels. We call the 
corpus the NICT Multilingual Corpora. The corpus 
will be open to the public in the near future. 
2 Overview of the NICT Multilingual 
Corpora 
At present, a Japanese-English parallel corpus and a 
Japanese-Chinese parallel corpus are under 
construction following systematic specifications. 
The parallel texts in each corpus consist of the 
original text in the source language and its 
translations in the target language. The original data 
is from newspaper articles or journals, such as 
85
Mainichi Newspaper in Japanese. The original 
articles were translated by skilled translators. In 
human translation, the articles of one domain were 
all assigned to the same translators to maintain 
consistent terminology in the target language. 
Different translators then revised the translated 
articles. Each article was translated one sentence to 
one sentence, so the obtained parallel corpora are 
already sentence aligned.  
  The details of the current version of the NICT 
Multilingual Corpora are listed in Table 1. 
Corpora Total Original Translation 
Japanese 
(19,669 
sentences, 
Mainichi 
Newspaper) 
English 
Translation 
Japanese-
English 
Parallel 
Corpus 
37,987 
sentence 
pairs; 
(English 
900,000 
words) English 
(18,318 
Sentences, 
Wall Street 
Journal) 
Japanese 
Translation 
Japanese-
Chinese 
Parallel 
Corpus 
38,383 
sentence 
pairs; 
(Chinese 
1,410,892 
Characters, 
926,838 
words) 
Japanese 
(38,383 
sentences, 
Mainichi 
Newspaper) 
Chinese 
Translation 
Table 1 Details of current version of NICT Multilingual 
Corpora 
 
   The following is an example of English and 
Chinese translations of a Japanese sentence from 
Mainichi Newspaper. 
[Ex. 1] 
J: ????????????????????????
?????? 
E: They were all about nineteen years old and had 
no strength left even to answer questions. 
C: ?????????????????????
?????????????  
 
In addition to the human translation, another big 
task is annotating the information. We finish the 
task by two steps: automatic annotation and human 
revision. In automatic annotation, we applied 
existing analysis techniques and tag sets. In human 
revision, we developed assisting tools that have 
powerful functions to help annotators in revision. 
The annotation task for each language included 
morphological and syntactic structure annotation.  
The annotation task for each language pair included 
alignments at word and phrase level.  
The NICT Multilingual Corpora constructed in 
this way have the following characteristics. 
(1) Since the original data is from newspaper and 
journals, the domain of each corpus is therefore rich.  
(2) Each corpus consists of original sentences and 
their translations, so they are already sentence 
aligned. In translation of each sentence, the context 
of the article is also considered. Thus, the context of 
each original article is also well maintained in its 
translation, which can be exploited in the future. 
(3) The corpora are annotated at high quality with 
morphological and syntactic structures and 
word/phrase alignment.  
 In the following section, we will describe the 
details in the construction of the Japanese-Chinese 
parallel corpus. 
3 Human Translation from Japanese to 
Chinese   
About 40,000 Japanese sentences from issues of 
Mainichi Newspaper were translated by skilled 
translators. The translation guidelines were as 
follows. 
(1) One Japanese sentence is translated into one 
Chinese sentence. 
(2) Among several translation candidates, the one 
that is close to the original sentence in syntactic 
structure is preferred. The aim is to avoid 
translating a sentence too freely, i.e., 
paraphrasing. 
(3) To obtain intelligible Chinese translations, 
information of the proceeding sentences in the 
same article should be added. Especially, a 
subject should be supplemented because a 
subject is usually required in Chinese, while in 
Japanese subjects are often omitted . 
(4)  To obtain natural Chinese translations, 
supplement, deletion, replacement, and 
paraphrase  should be made when necessary. 
When a translation is very long, word order can 
be changed or commons can be inserted. These 
are the restrictions on (2), i.e., the naturalness 
of the Chinese translations is the priority.  
 
  One problem in translation is how to translate 
proper nouns in the newspaper articles. We pay 
special attentions to them in the following way.  
(1) Proper nouns  
When proper nouns did not exist in Japanese-
Chinese dictionaries, new translations were created 
and then confirmed using the Chinese web. For 
kanji in proper nouns, if there was a Chinese 
character having the same orthography as the kanji, 
the Chinese character was used in the Chinese 
translation; if there was a traditional Chinese 
character having the same orthography as the kanji, 
the simplified character of the traditional Chinese 
character was used in the translation; otherwise, a 
Chinese character whose orthography is similar to 
that of the kanji was used in the translation.  
(2) Special things in Japan 
86
 Explanations were added if necessary. For example, 
?????, translated from ????? (grand sumo 
tournament), is well known in China, while ????, 
translated from ???? (spring labor offensive), is 
not known in China. In this case, an explanation 
???????? was added behind the unfamiliar 
term. We attempt to introduce new words about 
Japanese culture into Chinese through the 
construction of the corpus.    
 
   Producing high-quality Chinese translations is 
crucial to this parallel corpus. We controlled the 
quality by the following treatments.  
(1) The first revision of a translated article was 
conducted by a different translator after the first 
translation. The reviewers checked whether the 
meanings of the Chinese translations corresponded 
accurately to the meanings of the original sentences 
and modified the Chinese translations if necessary. 
(2) The second revision was conducted by Chinese 
natives without referring to the original sentences. 
The reviewers checked whether the Chinese 
translations were natural and passed the unnatural 
translations back to translators for modification. 
(3) The third revision was conducted by a Chinese 
native in the annotation process of Chinese 
morphological information. The words that did not 
exist in the dictionary of contemporary Chinese 
were checked to determine whether they were new 
words. If not, the words were designated as 
informal or not written language and were replaced 
with suitable words. The word sequences that 
missed the Chinese language model?s part-of-
speech chain were also adjusted.        
 
 Until now, 38,383 Japanese sentences have 
been translated to Chinese, and of those, 22,000 
Chinese translations have been revised three times, 
and we are still working on the remaining 18,000 
Chinese translations.  
4 Morphological Information Annotation 
Annotation consists of automatic analyses and 
manual revision. 
4.1 Annotation on Japanese Sentences  
Japanese morphological and syntactic analyses 
follow the definitions of part-of-speech categories 
and syntactic labels of the Corpus of Spontaneous 
Japanese (Maekawa, 2000).  
A morphological analyzer developed in that 
project was applied for automatic annotation on the 
Japanese sentences and then the automatically 
tagged sentences were revised manually. An 
annotated senetence is illustrated in Figure 1, which 
is the Japanese sentence in Ex. 1 in Section 2.  
 
 
 
 
 
 
# S-ID:950104141-008 
* 0 2D 
???? ???? * ?? * * * 
* 1 2D 
?? ?????? * ?? ?? * * 
? ?? * ??? ???????? * * 
?? ??? * ??? ???????? * * 
? ? * ?? ???? * * 
* 2 6D 
?? ???? * ?? ???? * * 
? ? ? ??? * ??? ???????? 
? ? * ?? ?? * * 
* 3 4D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 4 5D 
??? ???? ??? ?? * ???? ??? 
* 5 6D 
?? ???? * ?? ???? * * 
? ? * ?? ??? * * 
* 6 -1D 
??? ???? ?? ?? * ???? ???? 
? ? ?? ??? ?????? ???? ??? 
?? ?? ?? ??? ???? ???? ??? 
? ? * ?? ?? * * 
EOJ 
 
Figure 1. An annotated Japanese sentence 
 
The data of one sentence begins from the line ?# S-
ID... ? and ends with the mark ?EOJ?. The line 
headed by ?*? indicates the beginning of a phrase 
and the following lines are morphemes in that 
phrase. For example, the line ?* 0 2D? indicates the 
phrase whose number is 0. The following line ???
??  ? ? ? ?  * ? ?  * * *? indicates the 
morpheme in the phrase. There are seven fields in 
each morpheme line, token form, phonetic alphabet, 
dictionary form, part-of-speech, sub-part-of-speech, 
verbal category and conjugation form. In the line ?* 
0 2D?, the numeral 2 in ?2D? indicates that the 
phrase 0 ?????? modifies the phrase 2 ???
? ? ?. The syntactic structure analysis adopts 
dependency-structure analysis in which modifier-
modified relations between phrases are determined. 
The dependency-structure of the example in Figure 
1 is demonstrated in Figure 2. 
 
???? ?????? ???? ??? ??? ??? ???????
 
       Figure 2  Example of syntactic structure 
 
87
4.2 Annotation on Chinese Sentences 
For Chinese morphological analysis, we used the 
analyser developed by Peking University, where the 
research on definition of Chinese words and the 
criteria of word segmentation has been conducted 
for over ten years. The achievements include a 
grammatical knowledge base of contemporary 
Chinese, an automatic morphological analyser, and 
an annotated People?s Daily Corpus. Since the 
definition and tagset are widely used in Chinese 
language processing, we also took the criteria as the 
basis of our guidelines.  
A morphological analyzer developed by Peking 
University (Zhou and Yu, 1994) was applied for 
automatic annotation of the Chinese sentences and 
then the automatically tagged sentences were 
revised by humans. An annotated sentence is 
illustrated in Figure 3, which is the Chinese 
sentence in Ex. 1 in Section 2. 
 
S-ID: 950104141-008 
??/r  ??/j  ??/n  ?/d  ?/v   ??/m  ?/q 
??/m  ?/u  ???/n   ?/w  ??/r  ??/d  
?/p  ??/v  ??/n  ?/u  ??/n  ?/d   
??/v  ?/w 
Figure 3  An annotated Chinese sentence  
 
4.3 Tool for Manual Revision 
We developed a tool to assist annotators in revision. 
The tool has both Japanese and Chinese versions. 
Here, we introduce the Chinese version. The input 
of the tool is the automatically segmented and part-
of-speech tagged sentences and the output is revised 
data. The basic functions include separating a 
sequence of characters into two words, combining 
two segmented words into one word, and selecting 
a part-of-speech for a segmented word from a list of 
parts-of-speech. In addition, the tool has the 
following functions. 
(1) Retrieves a word in the grammatical knowledge 
base of contemporary Chinese of Peking University 
(Yu et al, 1997).  
This is convenient when annotators want to 
confirm whether a segmented word is authorized by 
the grammatical knowledge base, and when they 
want to know the parts-of-speech of a word defined 
by the grammatical knowledge base.  
(2) Retrieves a word in other annotated corpora or 
the sentences that have been revised.   
This is convenient when annotators want to see 
how the same word has been annotated before.  
(3) Retrieves a word in the current file.  
It collects all the sentences in the current file 
that contain the same word and then sorts their 
context on the left and right of the word. By 
referring to the sorted contexts, annotators can 
select words with the same syntactic roles and 
change all of the parts-of-speech to a certain one all 
in one operation. This is convenient when 
annotators want to process the same word in 
different sentences, aiming for consistency in 
annotation.     
(4) Adds new words to the grammatical knowledge 
base dynamically.  
The updated grammatical knowledge base can 
be used by the morphological analyser in the next 
analysis. 
(5) Indexes to sentences by an index file.  
The automatically discovered erroneous 
annotations can be stored in one index file, pointing 
to the sentences that are to be revised.  
 
The interface of the tool is shown in Figure 4 
and Figure 5. 
 
Figure 4 Interface of the manual revision tool (Retrieves 
a word in the grammatical knowledge base of 
contemporary Chinese) 
 
Figure 5    Interface of the manual revision tool 
(Retrieves a word in the current file) 
  
 
In Figure 4, the small window in the lower left 
displays the retrieved result of the word ? ??? in 
the grammatical knowledge base; the lower right 
window displays the retrieved result of the same 
word in the annotated People?s Daily Corpus. 
88
In Figure 5, the small window in the lower left is used to 
define retrieval conditions in the current file. In this 
example, the orthography of ???? is defined. The 
lower right window displays the sentences containing the 
word  ???? retrieved from the current file. The left and 
right contexts of one word are shown with the retrieved 
word. The contents of any column can be sorted by 
clicking the top line of the column. 
5 Annotation of word alignment  
Since automatic word alignment techniques cannot 
reach as high a level as the morphological analyses, 
we adopt a practical method of using multiple 
aligners. One aligner is a lexical knowledge-based 
approach, which was implemented by us based on 
the work of Ker (Ker and Chang, 1997). Another 
aligner is the well-known GIZA++ toolkit, which is 
a statistics-based approach. For GIZA++, two 
directions were adopted: the Chinese sentences 
were used as source sentences and the Japanese 
sentences as target sentences, and vice versa.  
The results produced by the lexical knowledge-
based aligner, C? J of GIZA++, and J?C of 
GIZA++ were selected in a majority decision. If an 
alignment result was produced by two or three 
aligners at the same time, the result was accepted. 
Otherwise, was abandoned.  In this way, we aimed 
to utilize the results of each aligner and maintain 
high precision at the same time. Table 2 showed the 
evaluation results of the multi-aligner on 1,127 test 
sentence pairs, which were manually annotated with 
gold standards, totally 17,332 alignments.  
 
 Precision 
(%) 
Recall 
(%) 
F-measure
Multi-aligner 79.3 62.7 70
Table 2 Evaluation results of the multi-aligner 
  
The multi-aligner produced satisfactory results. 
This performance is evidence that the multi-aligner 
is feasible for use in assisting word alignment 
annotation.  
For manual revision, we also developed an 
assisting tool, which consist of a graphical interface 
and internal data management. Annotators can 
correct the output of the automatic aligner and add 
alignments that it has not identified. In addition to 
assisting with word alignment, the tool also 
supports annotation on phrase alignment. Since 
Japanese sentences have been annotated with phrase 
structures, annotators can select each phrase on the 
Japanese side and then align them with words on 
the Chinese side. For idioms in Japanese sentences, 
two or more phrases can be selected. 
The input and output file of the manual 
annotation is in XML format. The data of one 
sentence pair consists of the Chinese sentence 
annotated with morphological information, the 
Japanese sentence annotated with morphological 
and syntactic structure information, word alignment, 
and phrase alignment.  
The alignment annotation at word and phrase is 
ongoing, the former focusing on lexical translations 
and the latter focusing on pattern translations. After 
a certain amount of data is annotated, we plan to 
exploit the annotated data to improve the 
performance of automatic word alignment. We will 
also investigate a method to automatically identify 
phrase alignments from the annotated word 
alignment and a method to automatically discover 
the syntactic structures on the Chinese side from the 
annotated phrase alignments.       
6 Conclusion  
We have described the construction of a Japanese-
Chinese parallel corpus, a part of the NICT 
Multilingual Corpus. The corpus consists of about 
40,000 pairs of Japanese sentences and their 
Chinese translations. The Japanese sentences are 
annotated with morphological and syntactic 
structures and the Chinese sentences are annotated 
with morphological information. In addition, word 
and phrase alignments are annotated. A high quality 
of annotation was obtained through manual 
revisions, which were greatly assisted by the 
revision tools we developed in the project. To the 
best of our knowledge, this will be the first 
annotated Japanese-Chinese parallel corpus in the 
world.  
In the future, we will finish the annotation on the 
remaining data and add syntactic structures to the 
Chinese sentences.  
  
References  
Dice, L.R. 1945. Measures of the amount of 
ecologic association between species. Journal of 
Ecology (26), pages 297?302. 
Ker, S.J., Chang, J.S. 1997. A Class-based 
Approach to Word Alignment. Computational 
Linguistics, Vol. 23, Num. 2, pages 313?343. 
Liu Q. 2004.  Research into some aspects of 
Chinese-English machine translation. Doctoral 
Dissertation.  
Maekawa, K., Koiso, H., Furui, F., Isahara, H. 2000. 
Spontaneous Speech Corpus of Japanese. 
Proceedings of LREC2000, pages 947?952. 
LDC. 1992.  Linguistic data Consortium. 
http://www.ldc.upenn.edu/. 
Uchimoto, K. and Zhang,Y., Sudo, K., Murata, M., and 
Sekine, S.,  Isahara,  H. Multilingual Aligned Parallel 
89
Treebank Corpus Reflecting Contextual Information 
and Its Applications. Proceedings of the MLR2004: 
PostCOLING Workshop on Multilingual Linguistic 
Resources, pages 63-70. 
Yamada, K., Knight, K. 2001.A syntax-based Statistical 
Translation Model. In Proceedings of the ACL , pages 
523-530.  
Yu, Shiwen. 1997. Grammatical Knowledge Base of 
Contemporary Chinese. Tsinghua Publishing 
Company. 
Zhang, Y., Ma, Q., Isahara, H. 2005. Automatic 
Construction of Japanese-Chinese Translation 
Dictionary Using English as Intermediary. Journal of 
Natural Language Processing, Vol. 12, No. 2, pages 
63-85. 
Zhou, Q., Yu, S. 1994.  Blending Segmentation with 
Tagging in Chinese Language Corpus 
Processing.  In Proc. of COLING-94, pages 
1274?1278. 
 
 
90
Information Retrieval Capable of Visualization and High Precision
Qing Ma1,2 and Kousuke Enomoto1
1Ryukoku University / 2NICT, Japan
qma@math.ryukoku.ac.jp
Masaki Murata and Hitoshi Isahara
NICT, Japan
{murata,isahara}@nict.go.jp
Abstract
We present a neural-network based self-
organizing approach that enables vi-
sualization of the information retrieval
while at the same time improving its
precision. In computer experiments,
two-dimensional documentary maps in
which queries and documents were
mapped in topological order accord-
ing to their similarities were created.
The ranking of the results retrieved us-
ing the maps was better than that of
the results obtained using a conven-
tional TFIDF method. Furthermore, the
precision of the proposed method was
much higher than that of the conven-
tional TFIDF method when the process
was focused on retrieving highly rel-
evant documents, suggesting that the
proposed method might be especially
suited to information retrieval tasks in
which precision is more critical than re-
call.
1 Introduction
Information retrieval (IR) has been studied since
an earlier stage [e.g., (Menzel, 1966)] and sev-
eral kinds of basic retrieval models have been pro-
posed (Salton and Buckley, 1988) and a number
of improved IR systems based on these models
have been developed by adopting various NLP
techniques [e.g., (Evans and Zhai, 1996; Mitra
et al, 1997; Mandara, et al, 1998; Murata, et
al., 2000)]. However, an epoch-making technique
that surpasses the TFIDF weighted vector space
model, the main approach to IR at present, has not
yet been invented and IR is still relatively impre-
cise. There are also challenges presenting a large
number of retrieval results to users in a visual and
intelligible form.
Our aim is to develop a high-precision, visual
IR system that consists of two phases. The first
phase is carried out using conventional IR tech-
niques in which a large number of related docu-
ments are gathered from newspapers or websites
in response to a query. In the second phase the
visualization of the retrieval results and picking
are performed. The visualization process clas-
sifies the query and retrieval results and places
them on a two-dimensional map in topological
order according to the similarity between them.
To improve the precision of the retrieval process,
the picking process involves further selection of a
small number of highly relevant documents based
on the classification results produced by the visu-
alization process.
This paper presents a new approach by using
the self-organizing map (SOM) proposed by Ko-
honen (Kohonen, 1997) for this second IR phase1.
To enable the second phase to be slotted into a
practical IR system as described above, visual-
1There have been a number of studies of SOM on data
mining and visualization [e.g., (Kohonen, et al, 2000)] since
the WEBSOM was developed in 1996. To our knowledge,
however, these works mainly focused on confirming the ca-
pabilities of SOM in the self-organization and/or in the vi-
sualization. In this study, we slot the SOM-based processing
into a practical IR system that enables visualization of the
IR while at the same time improving its precision. The an-
other feature of our study differing from others is that we
performed comparative studies with TFIDF-based IR meth-
ods, the major approach to IR in NLP field.
138
ization and picking should be carried out for a
single query and set of related documents. In
this paper, however, for the purpose of evaluating
the proposed system, correct answer data, consist-
ing of multiple queries and related documents as
used in the 1999 IR contest, IREX (Murata, et
al., 2000), was used. The procedure of the sec-
ond IR-phase in this paper is therefore as follows.
Given a set of queries and related documents, a
documentary map is first automatically created
through self-organization. This map provides vis-
ible and continuous retrieval results in which all
queries and documents are placed in topological
order according to their similarity2. The docu-
mentary map provides users with an easy method
of finding documents related to their queries and
also enables them to see the relationships between
documents with regard to the same query, or even
the relationships between documents across dif-
ferent queries. In addition, the documents related
to a query can be ranked by simply calculating
the Euclidean distances between the points of the
queries and the points of the documents in the
map and then choosing the N closest documents
in ranked order as the retrieval results for each
query. If a small N is set, then the retrieval results
are limited to the most highly relevant documents,
thus improving the retrieval precision.
Computer experiments showed that meaning-
ful two-dimensional documentary maps could be
created; The ranking of the results retrieved us-
ing the map was better than that of the results ob-
tained using a conventional TFIDF method. Fur-
thermore, the precision of the proposed method
was much higher than that of the conventional
TFIDF method when the retrieval process focused
on retrieving the most highly relevant documents,
which indicates that the proposed method might
be particularly useful for picking the best docu-
ments, thus greatly improving the IR precision.
2 Self-organizing documentary maps
and ranking related documents
A SOM can be visualized as a two-dimensional
array of nodes on which a high-dimensional in-
2For a specific query, other queries and documents in the
map are considered to be irrelevant (i.e., documents unre-
lated to the query). This map is therefore equivalent to a
map consisting of one query and related and unrelated docu-
ments, which will be adopted in the practical IR system that
we aim to develop.
put vector can be mapped in an orderly manner
through a learning process. After the learning, a
meaningful nonlinear coordinate system for dif-
ferent input features is created over the network.
This learning process is competitive and unsuper-
vised and is called a self-organizing process.
Self-organizing documentary maps are ones in
which given queries and all related documents
in the collection are mapped in order of similar-
ity, i.e., queries and documents with similar con-
tent are mapped to (or best-matched by) nodes
that are topographically close to one another, and
those with dissimilar content are mapped to nodes
that are topographically far apart. Ranking is the
procedure of ranking documents related to each
query from the map by calculating the Euclidean
distances between the points of the queries and
the points of the documents in the map and choos-
ing the N closest documents as the retrieval result.
2.1 Data
The queries are those used in a dry run of the
1999 IREX contest and the documents relating to
the queries are original Japanese newspaper arti-
cles used in the contest as the correct answers. In
this study, only nouns (including Japanese verbal
nouns) were selected for use.
2.2 Data coding
Suppose we have a set of queries:
Q = {Q i (i = 1, ? ? ? , q)}, (1)
where q is the total number of queries, and a set
of documents:
A = {Ai j (i = 1, ? ? ? , q, j = 1, ? ? ? , ai)},
(2)
where ai is the total number of documents related
to Q i. For simplicity, where there is no need to
distinguish between queries and documents, we
use the same term ?documents? and the same no-
tation Di to represent either a query Q i or a doc-
ument Ai j. That is, we define a new set
D = {Di (i = 1, ? ? ? , d)} = Q
?
A (3)
which includes all queries and documents. Here,
d is the total number of queries and documents,
i.e.,
d = q +
q?
i=1
ai. (4)
139
Each document, Di, can then be defined by the
set of nouns it contains as
Di = {noun(i)1 , w(i)1 , ? ? ? , noun(i)ni , w(i)ni }, (5)
where noun(i)k (k = 1, ? ? ? , ni) are all different
nouns in the document Di and w(i)k is a weight
representing the importance of noun(i)k (k =
1, ? ? ? , ni) in document Di. The weights are com-
puted by their tf or tfidf values. That is,
w(i)j = tf(i)j or tf(i)j idfj . (6)
In the case of using tf, the weights are normalized
such that
w(i)1 + ? ? ?+ w(i)ni = 1. (7)
Also, when using the Japanese thesaurus, Bun-
rui Goi Hyou (The National Institute for Japanese
Language, 1964) (BGH for short), synonymous
nouns in the queries are added to the sets of
nouns from the queries shown in Eq. (5) and their
weights are set to be the same as those of the orig-
inal nouns.
Suppose we have a correlative matrix whose el-
ement dij is some metric of correlation, or a sim-
ilarity distance, between the documents Di and
Dj ; i.e., the smaller the dij , the more similar the
two documents. We can then code document Di
with the elements in the i-th row of the correlative
matrix as
V (Di) = [di1, di2, ? ? ? , did]T . (8)
The V (Di) ? <d is the input to the SOM. There-
fore, the method to compute the similarity dis-
tance dij is the key to creating the maps. Note
that the individual dij of vector V (Di) only re-
flects the relationships between a pair of docu-
ments when they are considered independently.
To establish the relationships between the doc-
ument Di and all other documents, representa-
tions such as vector V (Di) are required. Even
if we have these high-dimensional vectors for
all the documents, it is still difficult to estab-
lish their global relationships. We therefore need
to use an SOM to reveal the relationships be-
tween these high-dimensional vectors and repre-
sent them two-dimensionally. In other words, the
role of the SOM is merely to self-organize vec-
tors; the quality of the maps created depends on
the vectors provided.
In computing the similarity distance dij be-
tween documents, we take two factors into ac-
count: (1) the larger the number of common
nouns in two documents, the more similar the two
documents should be (i.e., the shorter the simi-
larity distance); (2) the distance between any two
queries should be based on their application to the
IR processing; i.e., by considering the procedure
used to rank the documents relating to each query
from the map. For this reason, the document-
similarity distance between queries should be set
to the largest value. To satisfy these two factors,
dij is calculated as follows:
dij =
?
??????
??????
1 if both Di and Dj
are queries
1? |Cij ||Di|+|Dj |?|Cij | not the case mentioned
above and i 6= j
0, if i=j
(9)
where |Di| and |Dj | are values (the numbers of
elements) of sets of documents Di and Dj de-
fined by Eq. (5) and |Cij | is the value of the in-
tersection Cij of the two sets Di and Dj . |Cij |
is therefore some metric of document similarity
(the inverse of the similarity distance dij) between
documents Di and Dj which is normalized by
|Di|+|Dj |?|Cij |. Before describing the methods
for computing them, we first rewrite the definition
of documents given by Eq. (5) for Di and Dj as
follows.
Di = {(c1, w(i)c1 , ? ? ? , cl, w(i)cl ),
(n(i)1 , w(i)1 , ? ? ? , n(i)mi , w(i)mi)}, (10)
and
Dj = {(c1, w(j)c1 , ? ? ? , cl, w(j)cl ),
(n(j)1 , w(j)1 , ? ? ? , n(j)mj , w(j)mj )}, (11)
where ck (k = 1, ? ? ? , l) are the common nouns of
documents Di and Dj and n(i)k (k = 1, ? ? ? ,mi)
and n(j)k (k = 1, ? ? ? ,mj) are nouns of documents
Di and Dj which differ from each other. By com-
paring Eq. (5) and Eqs. (10) and (11), we know
140
that l+mi +mj = ni + nj . Thus, |Di| (or |Dj |)
of Eq. (9) can be calculated as follows.
|Di| =
l?
k=1
w(i)ck +
mi?
k=1
w(i)k . (12)
For calculating |Cij |, on the other hand, since the
weights (of either common or different nouns)
generally differ between two documents, we de-
vised four methods which are expressed as fol-
lows.
Method A:
|Cij | =
l?
k=1
max(w(i)ck , w(j)ck ). (13)
Method B:
|Cij | =
l?
k=1
w(i)ck + w(j)ck
2 . (14)
Method C:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document
?l
k=1
w(i)ck+w
(j)
ck
2 . if both are
documents
(15)
Method D:
|Cij | =
?
?????
?????
?l
k=1 max(w(i)ck , w(j)ck ) if one is a query
and the other
is a document?l
k=1 min(w(i)ck , w(j)ck ). if both are
documents
(16)
Note that we need not consider the case where
both are queries for calculating |Cij | because this
has been considered independently as shown by
Eq. (9).
3 Experimental Results
3.1 Data
Six queries Q i (i = 1, ? ? ? , q, q = 6) and 433
documents Ai j (i = 1, ? ? ? , q, q = 6, j =
1, ? ? ? , ai and
?q
i=1 ai = 433) used in the dry run
Table 1: Distribution of documents used in the
experiments
a1 a2 a3 a4 a5 a6
?6
i=1 ai
80 89 42 108 49 65 433
of the 1999 IREX contest were used for our ex-
periments. The distribution of these documents,
i.e., the number ai (i = 1, ? ? ? , q, q = 6) of docu-
ments related to each query, is shown in Table 1.
It should be noted that since the proposed IR
approach will be slotted into a practical IR sys-
tem in the second phase in which a small number
(say below 1,000, or even below 500) of the re-
lated documents should have been collected, this
experimental scale is definitely a practical one.
3.2 SOM
We used a SOM of a 40?40 two-dimensional ar-
ray. Since the total number d of queries and doc-
uments to be mapped was 439, i.e., d = q +?6
i=1 ai = 439, the number of dimensions of in-
put n was 439. In the ordering phase, the number
of learning steps T was set at 10,000, the initial
value of the learning rate ?(0) at 0.1, and the ini-
tial radius of the neighborhood ?(0) at 30. In the
fine adjustment phase, T was set at 15,000, ?(0)
at 0.01, and ?(0) at 5. The initial reference vec-
tors mi(0) consisted of random values between 0
and 1.0.
3.3 Results
We first performed a preliminary experiment and
analysis to determine which of the four methods
was the optimal one for calculating |Cij | shown
in Eqs. (13)-(16). Table 2 shows the IR precision,
i.e., the precision of the ranking results obtained
from the self-organized documentary maps cre-
ated using the four methods. The IR precision was
calculated by follows.
P = 1q
q?
i=1
#related to Q i in the retrieved ai documents
ai ,
(17)
where q is the total number of queries, # means
number, and ai is the total number of documents
related to Q i as shown in Table 1.
In the case of using tf values as weights of
nouns, method B obviously did not work. Al-
141
Table 2: IR precision for the four methods for cal-
culating |Cij |
Weight Method
A
Method
B
Method
C
Method
D
tf 0.33 0.20 0.41 0.45
tfidf 0.85 0.76 0.91 0.78
though the similarity between queries was manda-
torily set to the largest value, all six queries were
mapped in almost the same position, thus produc-
ing the poorest result. We consider the reason for
this was as follows. In general, the number of
words in a query is much smaller than the num-
ber of words in the documents, and the number
of queries is much smaller than the number of
documents collected. As described in section 2,
each query was defined by a vector consisting of
all similarities between the query and five other
queries and all documents in the collection. We
think that using the average weights of words ap-
pearing in the queries and documents to calculate
the similarities between queries and documents,
as in method B, tends to produce similar vectors
for the queries. All of these query vectors are then
mapped to almost the same position. With coding
method A, because the larger of the two weights
of a query and a document is used, the same prob-
lem could also arise in practice. There were no es-
sential differences between coding methods C and
D, which were almost equally precise. Neither of
these methods have the shortcomings described
above for methods A and B. However, when tfidf
values were used as the weights of the nouns, even
methods A and B worked quite well. Therefore, if
we use tfidf values as the weights of the nouns, we
may use either of the four methods. Based on this
analysis and the preliminary experimental result
that method C and D had highest precisions in the
cases of using tf and tfidf values as weights of the
nouns, respectively, we used methods C and D for
calculating |Cij | in all the remaining experiments.
Table 3 shows the IR precision obtained using
various methods. From this table we can see that
the proposed method in the case of SOM (w=tfidf,
C), i.e., using method C for calculating |Cij |, us-
ing tfidf values as the weights of nouns, and not
using the Japanese thesaurus (BGH), in the case
of SOM (w=tfidf, D), i.e., using method D, us-
ing tfidf values, and not using the BGH, and in
Table 3: IR precision obtained using various
methods
TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
0.67 0.75 0.45 0.91 0.77 0.78 0.73
Table 4: IR precision for top N related documents
N TFIDF TFIDF
(BGH)
SOM
(w=tf,
D)
SOM
(w=
tfidf,
C)
SOM
(w=
tfidf,
C,
BGH)
SOM
(w=
tfidf,
D)
SOM
(w=
tfidf,
D,
BGH)
10 0.83 0.88 0.75 1.0 0.97 1.0 0.97
20 0.79 0.86 0.68 0.99 0.95 0.98 0.97
30 0.73 0.84 0.62 0.99 0.94 0.97 0.91
40 0.71 0.82 0.58 0.98 0.90 0.97 0.87
the case of SOM (w=tfidf, C, BGH), i.e., using
method C, using tfidf values, and using the BGH
produced the highest, second highest, and third
highest precision, respectively, of all the methods
including the conventional TFIDF method. When
the BGH was used, however, the IR precision of
the proposed method dropped inversely, whereas
that of the conventional TFIDF improved. The
lower precision of the proposed method when us-
ing BGH might be due to the calculation of the
denominator of Eq. (9); this will be investigated
in future study.
Table 4 shows the IR precision obtained using
various methods when the retrieval process is fo-
cused on the top N related documents. From this
table we can see that the IR precision of the pro-
posed method, no matter whether the BGH was
used or not, or whether method C or D was used
for calculating |Cij |, was much higher than that
of the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents. This result demonstrated that the pro-
posed method might be especially useful for pick-
ing highly relevant documents, thus greatly im-
proving the precision of IR.
Figure 1 shows the left-top area of a self-
organized documentary map obtained using the
proposed method in the case of SOM (w=tfidf,
D)3. From this map, we can see that query Q 4
3Note that the map obtained using the proposed method
in the case of SOM (w=tfidf, C), which had the highest IR
precision, was better than this.
142
Figure 1: Left-top area of self-organized docu-
mentary map
and its related documents A4 ? (where * denotes
an Arabic numeral), Q 2 and its related docu-
ments A2 ? were mapped in positions near each
other. Similar results were obtained for the other
queries which were not mapped in the area of the
figure. This map provides visible and continu-
ous retrieval results in which all queries and docu-
ments are placed in topological order according to
their similarities. The map provides an easy way
of finding documents related to queries and also
shows the relationships between documents with
regard to the same query and even the relation-
ships between documents across different queries.
Finally, it should be noted that each map that
consists of 400 to 500 documents was obtained in
10 minutes by using a personal computer with a
3GHZ CPU of Pentium 4.
4 Conclusion
This paper described a neural-network based self-
organizing approach that enables information re-
trieval to be visualized while improving its preci-
sion. This approach has a practical use by slot-
ting it into a practical IR system as the second-
phase processor. Computer experiments of practi-
cal scale showed that two-dimensional documen-
tary maps in which queries and documents are
mapped in topological order according to their
similarities can be created and that the ranking
of the results retrieved using the created maps
is better than that produced using a conventional
TFIDF method. Furthermore, the precision of the
proposed method was much higher than that of
the conventional TFIDF method when the pro-
cess was focused on retrieving the most relevant
documents, suggesting that the proposed method
might be especially suited to information retrieval
tasks in which precision is more important than
recall.
In future work, we first plan to re-confirm the
effectiveness of using the BGH and to further im-
prove the IR accuracy of the proposed method.
We will then begin developing a practical IR sys-
tem capable of visualization and high precision
using a two-phase IR procedure. In the first phase,
a large number of related documents are gath-
ered from newspapers or websites in response to
a query presented using conventional IR; the sec-
ond phase involves visualization of the retrieval
results and picking the most relevant results.
References
H. Menzel. 1966. Information needs and uses in science
and technology. Annual Review of Information Science
and Technology, 1, pp. 41-69.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing & Management, 24(5), pp. 513-523.
D. A. Evans and C. Zhai. 1996. Noun-phrase analysis in
unrestricted text for information retrieval. ACL?96, pp.
17-24.
M. Mitra, C. Buckley, A. Singhal, and C. Cardie, C.
1997. An analysis of statistical and syntactic phrases.
RIAO?97, pp. 200-214.
R. Mandara, T. Tokunana, and H. Tanaka 1998. The use
of WordNet in information retrieval. COLING-ACL?98
Workshop: Usage of WordNet in Natural Language
Processing Systems, pp. 31-37.
M. Murata, Q. Ma, K. Uchimoto, H. Ozaku, M. Uchiyama,
and H. Hitoshi 2000. Japanese probabilistic informa-
tion retrieval using location and category information.
IRAL?2000.
T. Kohonen 1997. Self-organizing maps. Springer, 2nd
Edition.
T. Kohonen, S. Kaski, K. Lagus, J. Salojarrvi, J. Honkela,
V. Paatero, and A. Saarela. 2000. Self Organization of
a Massive Document Collection. IEEE Trans. Neural
Networks, 11, 3, pp. 574-585.
The National Institute for Japanese Language. 1964. Bunrui
Goi Hyou (Japanese Thesaurus). Dainippon-tosho.
143
Toward a Unified Evaluation Method for Multiple Reading Support 
Systems: A Reading Speed-based Procedure 
Katsunori KOTANI 
National Institute of Infor-
mation and Communications 
Technology  
3-5 Hikaridai, Seika-cho, 
Soraku-gun, Kyoto, Japan, 
619-0289 
kat@khn.nict.go.jp 
Takehiko YOSHIMI 
Ryukoku University 
1-5, Yokotani, Setaoe-cho, 
Otsu-shi, Shiga, Japan, 
520-2195 
Takeshi KUTSUMI 
Sharp Corporation 
492 Minosho-cho, Yamato-
koriyama-shi, Nara, Japan, 
639-1185 
 
 
Ichiko SATA 
Sharp Corporation 
492 Minosho-cho, Yamatokoriyama-shi, 
Nara, Japan, 639-1185 
Hitoshi ISAHARA 
National Institute of Information and 
Communications Technology  
3-5 Hikaridai, Seika-cho, Soraku-gun, 
Kyoto, Japan, 619-0289 
 
Abstract 
This paper proposes a unified evalua-
tion method for multiple reading sup-
port systems such as a sentence 
translation system and a word transla-
tion system.  In reading a non-native 
language text, these systems aim to 
lighten the reading burden.  When we 
evaluate the performance of these sys-
tems, we cannot rely solely on these 
tests, as the output forms are different.  
Therefore, we must assess the perform-
ance of these systems based on the us-
ers? reading comprehension and 
reading speed.  We will further support 
our findings with experimental results.  
They show that the reading-speed pro-
cedure is able to evaluate the support 
systems, as well as, the comprehension-
based procedure proposed by Ohguro 
(1993) and Fuji et al (2001). 
1 Introduction 
This paper presents an evaluation method for 
different reading support systems such as a sen-
tence-machine translation system (henceforth, 
an MT-system) and a word/phrase translation 
system (henceforth, a w/p-MT-system).  Al-
though, there are various manual/automatic 
evaluation methods for these systems, e.g., 
BLEU (Papineni et al 2002), these methods are 
basically incapable of dealing with an MT-
system and a w/p-MT-system at the same time, 
as they have different output forms.  On the con-
trary, there are further methods which examine 
the efficacy of these systems (Ohguro 1993; Fuji 
et al 2001).  These studies demonstrate the ef-
fectiveness of the reading support systems by 
comparing reading comprehension test scores 
between an English-only text and the one with 
outputs of either an MT-system (Fuji et al 2001) 
or a w/p-MT-system (Ohguro 1993). 
In our evaluation method, we examined the 
system based not only con comprehension but 
also on speed, i.e., reading efficacy (Alderson 
2000).  If the system supports a user in an appro-
priate way, then the reading efficacy would in-
crease from the bottom line, i.e., text without any 
support.  The previous studies focused mainly on 
reading comprehension.  We will now broaden 
our examination to include reading speed. 
We are able to evaluate a system based on 
single sentences, as we measure sentence-
reading speed.  In contrast, we are unable to 
carry out such a local domain evaluation solely 
based on the comprehension performance. 
This paper is organized as follows: Section 2 
reviews the previous studies, which evaluated 
reading support systems based on the compre-
hension performance, i.e., Ohguro (1993) and 
Fuji et al (2001); Section 3 describes our 
evaluation method, which evaluates both an 
244
MT-system and a w/p-MT-system based on 
speed performance; Section 4 reports the ex-
perimental results.  Through the experiments, 
we confirmed that the speed performance-based 
evaluation basically parallels the comprehension 
performance-based evaluation; and finally Sec-
tion 5 presents our conclusions and future work. 
2 The Comprehension-based Methods 
2.1 Ohguro (1993) 
Ohguro (1993) carried out an experiment in 
which the efficacy of an English-Japanese w/p-
MT-system was examined and reported that a 
w/p-MT-system would be of more aid to those 
with a lower reading ability.  Fifty-four non-
native English speakers took part in the experi-
ment.  Ohguro (1993) prepared 28 texts with 80 
comprehension questions extracted from various 
Test of English for International Communication 
(TOEIC) texts. 
The experiment held two phases.  First, all 
the participants read 14 English-only texts and 
answered 40 comprehension questions.  On the 
basis of the test score, the participants were di-
vided into two groups so as to balance the read-
ing ability between them.  Then, Ohguro (1993) 
gave English-only texts to one group, the control 
group, and provided texts supported with the a 
w/p-MT-system to the other group. 
Ohguro (1993) hypothesized that the control 
group would get similar test scores on both tests, 
as opposed to varying test scores from the other 
group.  In addition, it was predicted that the 
scores of the non-control group would depend on 
the reading ability of the group members with 
respect to TOEIC scores (Hypothesis I).  That is, 
a higher test score would be expected for those 
with a lower TOEIC score group.  Thus, Hy-
pothesis I was incorrect given the results.  Oh-
guro (1993) reanalysed the increase in the test 
scores by dividing that group into two.  Under 
this revised analysis, he hypothesized that a 
greater increase in score would be shown in the 
second test by those with lower initial scores (the 
revised Hypothesis I).  This revised hypothesis 
was correct given the result.  Ohguro (1993) con-
cluded that the supporting effect of a w/p-MT-
system was greater for those who had a lower 
reading ability than those highly skilled readers. 
2.2 Fuji et al (2001) 
Fuji et al (2001) examined how the efficacy of 
an English-Japanese MT system varied depend-
ing on English reading ability.  Approximately 
200 non-native English speakers participated in 
the experiment.  The participants were divided 
into 12 groups based on their TOEIC scores.  
The score range was between (i) less than 395 
and (ii) more than 900.  Fuji et al (2001) pre-
pared three types of texts.  One was an English-
only text as a control text, another contained 
only translated sentences by an MT-system, 
and the other involved both English texts and 
the MT-system outputs.  Each participant read 
14 texts, and answered 40 comprehension ques-
tions. 
Through this experiment, Fuji et al (2001) 
observed that translation-only texts would de-
grade the test scores for the higher TOEIC score 
group, while the lower score group exhibited no 
degrading effect.  In addition, they found that 
English texts with MT-outputs might increase 
the test scores for the lower score group more 
greatly than the higher score group. 
With respect to the test completion time, Fuji 
et al (2001) observed that an MT-system highly 
shortened the time for the lower score group 
relative to the higher score group. 
2.3 Summary 
Through the surveys of these studies, we were 
able to confirm that both a w/p-MT-system and 
an MT-system exhibited greater supporting ef-
fects on the lower TOEIC score group than the 
higher TOEIC score group. 
3 Evaluation with Reading Speed 
3.1 The purpose 
The purpose of our evaluation is to pursue the 
efficacy of reading support systems with respect 
not only to the users? reading ability but also to 
the readability of a complete text or a single sen-
tence.  That is, we would like to explicate 
through the evaluation whether the supporting 
effect might change due to the text properties 
such as complexity of a syntactic structure, fa-
miliarity of words, and so on. 
In order to depict such a local effect, we as-
sume that the comprehension-based evaluation 
245
would be inappropriate, as it is inefficient to 
assign a comprehension question to each sen-
tence.  Suppose that we could evaluate reading 
support systems regarding such a local domain.  
Then, we could choose which system is proper, 
depending on his/her reading ability and the 
readability of a text.  Such usage of reading sup-
port systems would be useful. 
3.2 Reading Speed as an Evaluation Criterion 
In our evaluation method, we adopt reading 
speed performance as an evaluation criterion in 
addition to the comprehension performance.  
There are three reasons for this adoption of read-
ing speed. 
First, in contrast to reading comprehension, 
we can measure sentence-reading speed, and 
thus we can examine system efficacy on a sen-
tence-level. 
Secondly, reading speed can be measured 
with any texts which is readable by the reading 
support systems.  For instance, we can evaluate 
system efficacy for texts such as newspapers, 
magazine articles, web pages, emails, and so on.  
By contrast, the comprehension-based evalua-
tion requires comprehension questions. 
Thirdly, as shown below, we have statisti-
cally found that the reading speed reflects the 
readability of a sentence.  We confirmed the 
positive correlation (r=0.7, p<0.01) between 
reading speed and readability of a text calculated 
with the so-called readability formula (Flesch 
1948).  Given this positive correlation, we as-
sumed that reading speed indicates readability.  
Thus, a direct relationship exists between read-
ability and reading speed. 
3.3 Reading Speed-based Evaluation 
Method 
Assuming that reading speed reflects text read-
ability, we can further assume that the reading 
support systems would affect text readability.  
That is, the positive supporting effect of a sys-
tem would increase the text readability.  Given 
this, we can evaluate the efficacy of a system on 
the basis of reading speed. 
Our evaluation method accepts the positive 
effect of a system if the reading speed is in-
creased.  When the reading speed remains in-
variant, or decreases, the method regards a 
system as inefficient.  Thus, if we compare the 
reading speed between a supported and a non-
supported text, the increase of speed should be 
greater for those who have a lower reading abil-
ity than the highly skilled people on the basis of 
previous studies. 
4 Evaluation Experiment 
4.1 The Experimental Purpose 
We conducted an experiment in order to exam-
ine the validity of our method.  Given the read-
ing speed evaluation method, it is predicted 
that reading speed would reflect readability of 
a text (Hypothesis 1) and reader?s ability (Hy-
pothesis 2). 
As for readability of a text, we assume that 
supporting systems would increase readability 
of a text.  Therefore, we set the following hy-
pothesis: 
 
Hypothesis 1: 
A non-supported English text would be the 
most difficult to read, whereas a manually 
translated Japanese text would be the easiest.  
Supported text would fall mid-range. 
 
The efficacy of the supporting systems is in-
versely related to the reader?s ability, as the pre-
vious studies have shown.  Therefore, we 
propose the following hypothesis: 
 
Hypothesis2: 
The inverse relation is detectable between the 
reading ability and the reading speed increase. 
 
4.2 The Experimental Design 
One hundred and two non-native English 
speakers participated in the experiment.  We 
divided the participants into three groups 
based on their TOEIC scores: (i) those with a 
lower score (400-595 pts.), (ii) those with an 
intermediate score (600-795 pts.); and (iii) 
those with a higher score (800-995 pts.).  The 
group sizes were: (i) = 36, (ii) = 36, and (iii) = 
30.  We statistically compared average test 
scores and reading speed among these groups. 
We prepared eighty-four texts out of our 
sourced TOEIC texts.  Each text consists of a 
passage and some comprehension questions.  
We added outputs of supporting systems to 
each text. 
246
In this experiment, we examined the effi-
cacy of the following supporting systems: a 
sentence translation system, a word/phrase 
translation system, and a chunker.  Thus, we 
created four types of test texts: (i) English 
texts glossed with sentence translations (here-
after, E&MT); (ii) machine-translated texts 
(MT); (iii) English texts glossed with word 
translation (RUB); and (iv) English texts with 
word/phrase boundary markers (CHU). 
In addition, we prepared two types of con-
trol texts.  One is a raw English text, and the 
other is a human-translated Japanese text.  We 
randomly selected sixteen texts from each text 
group and distributed eighty-four to each par-
ticipant.  Thus, the participants are exposed to 
a variety of texts. 
In the experiment we used a reading process 
monitoring tool and recorded the reading time 
per sentence (see Yoshimi et al 2005 for further 
description).  We calculated the sentence read-
ing speed based on words per minute (WPM) 
read.  As the cursor moves over each number 
bar, the text is displayed sentence-by-sentence.  
See Figure 1.  There is no limit to how many 
times a sentence can be viewed. 
 
 
Figure 1. Screenshot of the monitoring tool 
We omitted the machine-translated words 
and focused solely on the number of English 
words to calculate the reading speed.  Therefore, 
we were able to directly compare the reading 
speed of a supported text to that of a non-
supported English text. 
The goal of this study is to depict the effi-
cacy of the support systems.  Hence, the actual 
reading speed of an English and Japanese 
mixed text was out of the scope.  If reading 
speed was calculated based on both English 
and Japanese words, the reading speed of a 
supported text would be faster than an English 
text, even though the reading time was the 
same.  This is due to a greater number of words 
in the supported text.  Therefore, we calculated 
reading speed based solely on English words to 
account for this implausible effect.  We also 
applied this procedure to a manually translated 
Japanese text. 
4.3 Experimental Results 
4.3.1 Tested Data 
Before presenting the experimental results, 
one clarification is in order here.  We chose to 
analyse a manageable 13 reading texts of the 
whole data, i.e., 84 reading texts.  The texts 
we used varied in topic, style, and length.  For 
instance, they were article-based texts, reports, 
and advertisements.  Among these texts, we 
examined article type texts. 
There were two reasons for this limitation.  
One concern was with the performance of the 
reading support systems.  We assumed that the 
system performance was dependant on text 
styles, and that the system would most effec-
tively support reading of article type texts be-
cause they contained less stylistic variations 
compared with other types of texts, particu-
larly, advertisements. 
The other concern was with text length.  
Article type texts tended to be longer than the 
others, and hence were more conducive to the 
supporting effect of the systems as shown in 
Table 1. 
 
Text Words Sentences 
Non-article texts* 89.6 5.9 
Article texts 142.9 9.6 
Table 1. Article texts and non-article texts  
*reports, advertisements, and announcements averaged 
together 
4.3.2 Testing Hypothesis 1: Reading Speed 
We are able to conclude in Hypothesis 1 that 
the reading speed of a supported text is slower 
than that of a non-supported English text.  See 
Table 2.  Therefore, the hypothesis is incorrect 
with respect to the slowest speeds.  However, in 
regards to the fastest reading speed, Hypothesis 
1 was supported. 
 
 
 
 
247
Text* Mean SD 95% CI of Mean
ENG 75.1 31.9 70.1 to 80.3 
CHU 74.1 36.5 68.3 to 80.1 
RUB 65.5 28.0 61.1 to 70.1 
MT 102.6 57.0 93.2 to 111.9 
E&MT 70.3 31.7 65.3 to 75.2 
JPN 163.1 80.7 149.7 to 176.6 
Table 2. Mean reading speed 
*ENG, English texts; CHU, English texts marked with 
word/phrase boundary; RUB, English texts glossed with 
machine-translated words; MT, machine-translated texts; 
E&MT, English texts glossed with machine-translated 
sentences; JPN, manually-translated texts 
4.3.3 Testing Hypothesis 1: Comprehension 
Hypothesis 1 was not supported for the lowest 
comprehension scores, paralleling reading 
speed results.  Thus, the lowest score was 
found in the MT texts as shown in Table 3.  
The results supported the hypothesis in respect 
to the JPN texts scoring highest. 
 
Text Mean SD 95% CI of Mean 
ENG 0.84 0.22 0.80 to 0.87 
CHU 0.84 0.25 0.80 to 0.88 
RUB 0.83 0.23 0.79 to 87 
MT 0.81 0.22 0.77 to 0.85 
E&MT 0.90 0.16 0.88 to 0.93 
JPN 0.93 0.15 0.90 to 0.95 
Table 3. Mean percentatge of questions answered correctly. 
In order to analyse the reading data in more 
detail, we compared the correct answer rates 
among the TOEIC test score groups.  We di-
vided the participants into three groups based on 
TOEIC scores: 400-595 (BEGinner), 600-795 
(INTermediate), and 800-995 (ADVanced). 
The correct answer rate of each group is 
shown in Table 4.  In the BEG class, the lowest 
rate was found in English texts, and the highest 
was seen in Japanese texts.  Although the high-
est rate can be seen in Japanese texts, the lowest 
was found in MT texts in the INT class and 
ADV class. 
On the basis of comprehension test results, 
we confirmed that all the supporting systems 
increased comprehension test scores for the 
BEG class, E&MT for the INT class, but not for 
the ADV class. 
 
 
 BEG INT ADV 
ENG 0.68 0.89 0.93 
CHU 0.74 0.85 0.92 
RUB 0.74 0.83 0.92 
MT 0.77 0.82 0.84 
E&MT 0.87 0.93 0.91 
JPN 0.87 0.96 0.94 
Table 4. The correct answer rate by TOEIC score group 
On the basis of this result, we conclude that 
the reading support systems help the lowest 
TOEIC score group participants, while the sup-
porting effect would be minor for the higher 
score group. 
We analysed the mean rate with one-way 
ANOVA by contrasting the ENG texts or the JPN 
texts.  The result is shown in Table 5.  The asterisk 
refers to a non-significant difference, while the 
check mark shows a significant difference. 
In the BEG class, the rate of correct answers 
in the ENG texts was significantly lower than in 
the E&MT texts.  There was no text that signifi-
cantly differed from the JPN texts. 
In the INT class, there was no significant dif-
ference compared with the ENG texts, while the 
rate of the JPN texts significantly differed from 
the CHU, RUB, and MT texts. 
In the ADV class, there was no significant 
difference comparing with the ENG texts.  The 
rate of the JPN texts showed a significant differ-
ence from the MT texts. 
 
BEG INT ADV  
ENG JPN ENG JPN ENG JPN
CHU * * * ? * * 
RUB * * * ? * * 
MT * * * ? * ? 
E&MT ? * * * * * 
Table 5. ANOVA results for the correct rate by TOEIC 
score group 
4.3.4 Testing Hypothesis 2 
We found variances in the Hypothesis 1.  
Thus, the most readable text was the JPN texts, 
whereas the least readable text was not the ENG 
texts but the RUB texts(Table 3).  In addition, 
the other supported texts, the CHU, RUB, and 
E&MT texts were less readable than the non-
supported ENG texts.  However, the MT texts 
were more readable than the ENG texts.  There-
248
fore, we were able to conclude that Hypothesis 1 
was supported among the ENG, MT, and JPN 
texts. 
Given this, we focused on these texts and 
found that Hypotheses 2 was correct.  As Table 
6 shows, the reading speed of the MT texts was 
faster than the ENG texts in all the groups.  The 
increase of the speed was inversely related to the 
readers? ability.  Thus, the increase was 47.3 in 
the BEG class; 25.4 in the INT class; and 10.9 in 
the ADV class. 
 
 BEG INT ADV 
ENG 62.4 73.2 89.2 
MT 109.7 98.6 100.1 
JPN 172.2 152.1 170.9 
Table 6. The reading speed (WPM) by TOEIC score range 
We analysed the mean reading speed (Table 
7) with one-way ANOVA by contrasting the 
ENG texts or the JPN texts.  The speed of the MT 
texts was significantly faster than that of the ENG 
texts in the BEG and INT classes.  However, in 
the ADV class, there was no text that signifi-
cantly deferred from the ENG texts.  The reading 
speed of the JPN texts was significantly faster 
than the other texts in all the classes.  See Table 8. 
 
Text BEG INT ADV 
ENG 62.4 73.2 89.2 
CHU 63.2 63.4 98.1 
RUB 58.4 60.0 80.3 
MT 109.6 98.6 100.1 
E&MT 71.4 60.8 80.7 
JPN 172.2 152.2 1701.0 
Table 7. The reading speed (WPM) by TOEIC score range 
BEG INT ADV  
ENG JPN ENG JPN ENG JPN
CHU * ? * ? * ? 
RUB * ? * ? * ? 
MT ? ? ? ? * ? 
E&MT * ? * ? * ? 
Table 8. ANOVA results for the reading speed by TOEIC 
score group 
 
5. Conclusion 
In this paper, we presented the reading speed-
based evaluation method for reading support 
systems.  On the basis of the experiment, we 
found that the method articulated the perform-
ance of the systems, such as a chunker, a word-
translation system, and a sentence-translation 
system.  We found that only a sentence-
translation showed the supporting effect.  How-
ever, this supporting effect was not available for 
the advanced English learners. 
We have not yet discussed crossing effects of 
comprehension result and speed result, but we 
will expect the further study would reveal it. 
References 
Alderson, J. C. 2000. Assessing Reading. Cambridge 
University Press: Cambridge. 
Flesch, R. 1948. A New Readability Yardstick. Jour-
nal of Applied Psychology 32: 221-233. 
Fuji, M., N. Hatanaka, E. Ito, S. Kamei, H. Kumai, T. 
Sukehiro, T. Yoshimi, & H. Isahara. 2001. 
Evaluation Method for Determining Groups of 
Users Who Find MT ?Useful.? Proceedings of the 
MT Summit VIII. 
Ohguro, Y. 1993. Evaluating the Validity of Printing 
Japanese Words alongside English Text. Technical 
Report on Information Processing Society of Ja-
pan. 93-NL-79: 127-134. 
Papineni, K., S. Roukos, T. Ward, & W.-J. Zhu. 2002. 
BLEU: A Method for Automatic Evaluation of 
Machine Translation. Proceedings of the 40th An-
nual Meeting of the Association for the Computa-
tional Linguistics: 311-318. 
Yoshimi, T., K. Kotani, T. Kutsumi, I. Sata, & H. 
Isahara. 2005. A Method of Measuring Reading 
Time for Assessing EFL-Learners? Reading Abil-
ity.  JSiSE 22: 24-29. 
249
Trend Survey on Japanese Natural Language Processing Studies
over the Last Decade
Masaki Murata?, Koji Ichii?, Qing Ma?,?, Tamotsu Shirado?,
Toshiyuki Kanamaru?,?, and Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{murata,qma,shirado,kanamaru,isahara}@nict.go.jp
?Port and Airport Research Institute
Nagase 3-1-1, Yokosuka, Kanagawa 239-0826, Japan, ichii@pari.go.jp
?Ryukoku University, Otsu 520-2194, Japan, qma@math.ryukoku.ac.jp
?Kyoto University, Yoshida-Nihonmatsu, Sakyo, Kyoto 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Abstract
Using natural language processing, we
carried out a trend survey on Japanese
natural language processing studies that
have been done over the last ten years.
We determined the changes in the num-
ber of papers published for each re-
search organization and on each re-
search area as well as the relationship
between research organizations and re-
search areas. This paper is useful
for both recognizing trends in Japanese
NLP and constructing a method of sup-
porting trend surveys using NLP.
1 Introduction
We conducted a trend survey on Japanese nat-
ural language processing studies that have been
done over the last ten years. We used biblio-
graphic information from journal papers and an-
nual conference papers of the Association for
Natural Language Processing, Japan (The Asso-
ciation for Natural Language Processing, 1995-
2004; The Association for Natural Language Pro-
cessing, 1994-2003). Just ten years have passed
since the association was established. Therefore,
we can use the bibliographic information from the
past ten years. In this study, we investigated what
kinds of studies have been presented in journal
papers and annual conference papers on the Asso-
ciation for Natural Language Processing, Japan.
We first digitized documents listed in the bibli-
ographic information and then extracted various
pieces of useful information for the trend survey.
Figure 1: Change in the number of papers
We also examined the changes in the number of
papers put up by each Japanese research orga-
nization and the changes in the number of pa-
pers written on specific research areas. More-
over, we examined the relationship between each
Japanese research organization and each research
area. This study is useful for trend surveys of
studies performed by members of in the Associa-
tion for Natural Language Processing, Japan.
2 Trend survey on NLP research studies
We show the changes in the number of journal
papers and conference papers in Figure 1. Jour-
nal papers are reviewed, but conference papers are
not reviewed in the association. In comparing the
journal papers and conference papers, we found
that the number of conference papers was much
larger than that of journal papers. We also found
that although both types of papers decreased in
number at some point, they both demonstrate an
upward trend.
Conference papers have a temporal peak in the
fourth year and a temporal drop in the sixth year,
250
Figure 2: Change in the number of journal papers
by each research organization (The two numbers in
the parentheses indicate the total number of papers
and the average value of published years.)
while journal papers have a peak in the sixth year
and a drop in the eighth year. The temporal peak
and drop of the journal papers occurred just two
years after the peak and drop of the conference
papers. We presume this is because journal papers
need more time for reviewing and publishing, and
because journal papers are presented later than
conference papers for studies performed at the
same time.
3 Trend survey on research
organizations
Next, we investigated the change in the number
of papers put out by each research organization.
The results are represented in contour in Figures
2 and 3. The height in contour (the depth of a
black color) indicates the number of papers. We
calculated the average (we call it average value)
of the average, the mode, and the median of the
published years by using the data of the number
of papers performed by each research organiza-
tion. In the figures, each research organization is
listed in ascending order of the average value. We
added the total number of papers and the average
value to each research organization in the figures.
Therefore, research organizations that had many
papers in the earlier years are displayed higher
on the list, while research organizations that had
Figure 3: Change in the number of conference pa-
pers by each research organization
many papers in the later years are displayed lower.
Here, we displayed only research organizations
that had many total papers. If a research orga-
nization?s name was changed during the ten-year
period, we used the name that had the most usage
on published papers for displaying it.1
From these figures, we can see that ATR and
CRL (NICT) put out many journal papers, and
NTT, ATR, Tokyo Institute of Technology, CRL,
and the University of Tokyo put out many confer-
ence papers. We also found that while NTT and
ATR had many papers in the earlier years, CRL
and the Univ. of Tokyo had many papers in the
later years. We can expect that because CRL and
the Univ. of Tokyo demonstrate an upward ten-
dency, their quantity of papers will continue to in-
crease in the future. Using these figures, we can
see very easily in which reference year each re-
search organization put out many papers.
4 Trend survey on research areas
Next, we investigated the change in the number
of papers in each research area. The results are in
Figures 4, 5, and 6. (Because the volume of data
for conference papers was large, it was divided
into two figures.). For journal papers, the height
1When we counted the frequency of a research organiza-
tion whose name was changed, we used all the names of it
including old and new names.
251
Figure 4: Change in the number of journal papers
in each research area
in contour indicates the number of papers. For
conference papers, the height in contour indicates
the base two logarithm of the number of papers
added by one. Using the same method as that de-
scribed above, we calculated the average of the
average, mode, and median of the years papers
were published using the data of the number of
papers in each research area. In the figures, each
research area is displayed in ascending order of
the average value. We added the total number of
papers and the average value to each research area
in the figures. Here, we divided the title of each
paper into words using ChaSen software (Mat-
sumoto et al, 1999), and we treaded each word as
a research area. A paper with a particular word in
Figure 5: Change in the number of conference pa-
pers in each research area (part I)
its title was categorized in the research area indi-
cated by the word. Wemanually eliminated words
that were not indicative of a research area, for ex-
ample, ?teki? (of) and ?kenkyu? (study).
From these figures, it is clear that the research
areas of ?Japanese? and ?analysis? were studied
in an especially large number of papers. We
also found that for journal papers, because the
research areas of ?verb?, ?noun?, ?disambigua-
tion?, ?probability?, ?corpus?, and ?polysemic?
were displayed higher on the list, these areas were
studied thoroughly in the earlier years. Likewise,
we found that the research areas of ?morphol-
ogy?, ?dependency?, ?dialogue?, and ?speech?
were studied thoroughly in the sixth year and the
252
Figure 6: Change in the number of conference pa-
pers in each research area (part II)
research areas of ?summarization?, ?retrieval?,
?translation? and so on were studied well in the
later years. Special journal issues on ?summariza-
tion? were published in the sixth and ninth years,
so the research area of ?summarization? was rep-
resented in many papers in those years. We can
expect that because the research area of ?transla-
tion? demonstrates an upward tendency, the num-
ber of papers on this topic will continue to in-
crease in the future.
In terms of conference papers, we found that
the research areas of ?bilingual?, ?morphology?,
?probability?, ?dictionary?, ?statistics?, and so on
were studied well in the earlier years. In the lower
part of the figures, such research areas as ?re-
Figure 7: Change in the number of conference pa-
pers at each research organization in the research
area of ?translation?
Figure 8: Change in the number of conference pa-
pers in each research area in the research area of
?translation?
trieval?, ?summarization?, ?question? and ?para-
phrase? are found. Thus, we can see that these
research areas were studied thoroughly in recent
years. We can see very easily in which reference
years each research area was studied using these
figures.
5 Trend survey using part of data
Although we have focused on using all the data
in the trend survey so far, we can narrow down
the survey by looking only at a certain part of
the data. For example, when we want to exam-
253
Figure 9: Relationship between research organizations and research areas in journal papers (The name
of each research organization is given a ??? symbol.)
ine a trend survey on translation in more detail,
all we have to do is to extract papers on transla-
tion and use them for a trend survey. We carried
out a trend survey on machine translation in this
manner. We first extracted papers whose titles in-
cluded the word ?translation? and then performed
the same investigations as in Sections 3 and 4.
The results are in Figures 7 and 8. The height in
contour (the depth of a color) indicates the num-
ber of papers. From Figure 7, we can see that
NTT had many papers in the earlier years, and
ATR had many papers in later years. From Figure
7, we can also see that studies on translation of-
ten dealt with specific topics such as ?semantics?,
?knowledge? and ?dictionary? in earlier years and
?support?, ?example?, and ?retrieval? in more re-
cent years.
6 Relationship between research
organizations and research topics
Finally, we investigated the various research ar-
eas that research organizations studied more fre-
quently during the ten-year period. Here, we
show only the results for journal papers. We used
the same method as in the previous sections for
extracting research organizations and research ar-
eas from the data. We counted the cooccurrent
frequency of each research organization and each
research area. We then constructed a cross table
in this manner and then performed the dual scal-
ing method (Weller and Romney, 1990; Ueda et
al., 2003). The result is depicted in Figure 9. The
dual scaling method displays the relationship be-
tween research organizations and research areas.
In Figure 9, ?translation? appears in the lower
left quadrant, ?learning? appears in the lower
right quadrant, ?statistics? and ?retrieval? appear
in the upper right quadrant, and ?noun? and ?sen-
tence? appear in the upper left quadrant. In the
vicinity around these words, the research areas
and organizations relating to them appear. For ex-
ample, in the upper right quadrant, Hitachi and
University of Tokushima appear near ?statistics?
and ?retrieval?, which were frequent study topics
for them. Similarly, ?summarization? appears in
the near upper right area of the source origin and
is surrounded by JAIST, Toyohashi University of
Technology, and Tokyo Institute of Technology.,
indicating it was a frequent topic of study at those
institutions. We can easily see which research
topics were primarily studied by each organiza-
tion using this figure.
Also in Figure 9, research areas on numeri-
cals such as ?probability? and ?learning? appear
254
on the right side. Therefore, we can interpret the
figure as depicting quantitative research topics on
the right side and qualitative research topics on
the left side. Research areas using complicated
processing such as ?learning? and ?translation?
appear in the lower area and research areas deal-
ing with theory such as ?probability?, ?grammar?,
?sentence?, and ?noun? appear in the upper area.
Therefore, we can interpret the figure as depict-
ing theoretical research topics in the upper area
and research topics using complicated processing
in the lower area.
7 Conclusion
In this paper, we described a trend survey carried
out on Japanese natural language processing stud-
ies done over the last ten years. We were able to
investigate trend surveys on research areas very
easily by treating divided words in titles by a mor-
phological analyzer as the indications of research
areas. We displayed the changes in the number of
papers put out by each research organization and
written on specific research topics. We also dis-
played the relationship between research organi-
zations and research areas using the dual scaling
method. The simple methods we used that are de-
scribed here made it possible to show many useful
results.
This paper has the following two significant ef-
fects:
 This paper explained a trend survey on
Japanese natural language processing. By
reading it, we can understand the trends in
research on Japanese natural language pro-
cessing. For example, we can find out
which research areas were studied more of-
ten and we can see which research organiza-
tions were involved in studying natural lan-
guage processing. We can also see which re-
search organization studied a particular re-
search area most often over the ten-year pe-
riod.
 We used natural language processing to
carry out the trend survey described here.
For example, we automatically detected the
indication of a research area from words
used in titles by using a morphological ana-
lyzer. In addition, we displayed words that
were extracted by the morphological ana-
lyzer in several ways to display the results
of the trend survey effectively. The methods
used in this paper would be useful in other
trend surveys.
In short, this paper is useful for recognizing trends
in Japanese NLP and for constructing methods of
supporting trend surveys using NLP.
In the future, we would like to perform an in-
ternational trend survey on natural language pro-
cessing using international conference and jour-
nal papers such as IJCNLP, ACL, and the Journal
of Computational Linguistics. We would also like
to do trend surveys on other topics such as AI, bi-
ology, politics, and sociology.
The kinds of investigations we did can easily be
altered to do many other kinds of investigations
as well. For example, we can use the dual scal-
ing method by investigating the relationship be-
tween the reference years and the research organi-
zations/areas. We can also use the representation
in contour for the relationship between research
organizations and research areas. Although we
showed the data in ascending order of the aver-
age value of the published years, we could show
the data in different order, for example, the or-
der of the total number of papers or the order of
the location, i.e., showing similar research orga-
nizations/areas that are located near each other by
clustering research organizations/areas using their
cooccurrent words. We would like to continue
to study these kinds of support methods for trend
surveys in the future.
References
The Association for Natural Language Processing. 1994-
2003. Journal of Natural Language Processing.
The Association for Natural Language Processing. 1995-
2004. Proceedings of the Annual Meeting of The Associ-
ation for Natural Language Processing.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, and Masayuki Asahara.
1999. Japanese morphological analysis system ChaSen
version 2.0 manual 2nd edition.
Taichiro Ueda, Masao Karita, and Kazue Honda. 2003. Jis-
sen Workshop Excel Tettei Katsuyou Tahenryou Kaiseki.
Shuuwa System. (in Japanese).
Susan C. Weller and A. Kimball Romney. 1990. Metric
Scaling : Correspondence Analysis (Quantitative Appli-
cations in the Social Sciences). SAGE Publications.
255
Error Annotation for Corpus of Japanese Learner English 
Emi Izumi                    Kiyotaka Uchimoto                 Hitoshi Isahara 
National Institute of Information and Communications Technology (NICT), 
Computational Linguistics Group 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 
{emi,uchimoto,isahara}@nict.go.jp 
 
 
Abstract 
In this paper, we discuss how error 
annotation for learner corpora should 
be done by explaining the state of the 
art of error tagging schemes in learner 
corpus research. Several learner 
corpora, including the NICT JLE 
(Japanese Learner English) Corpus that 
we have compiled are annotated with 
error tagsets designed by categorizing 
?likely? errors implied from the 
existing canonical grammar rules or 
POS (part-of-speech) system in 
advance. Such error tagging can help to 
successfully assess to what extent 
learners can command the basic 
language system, especially grammar, 
but is insufficient for describing 
learners? communicative competence. 
To overcome this limitation, we re-
examined learner language in the NICT 
JLE Corpus by focusing on 
?intelligibility? and ?naturalness?, and 
determined how the current error tagset 
should be revised. 
1 Introduction 
The growth of corpus research in recent years is 
evidenced not only by the growing number of 
new corpora but also by their wider variety. 
Various ?specialized corpora? have recently 
been created. One of them is the ?learner 
corpus?, which is a collection of the language 
spoken or written by non-native speakers. The 
primary purpose of learner corpora is to offer 
Second Language Acquisition (SLA) 
researchers and language teaching professionals 
resources for their research. In order to develop 
a curriculum or pedagogy of language teaching, 
it would be beneficial to have interlanguage data 
so that researchers can scientifically describe the 
characteristics of each developmental stage of 
their interlanguage. One of the most effective 
ways of doing this is to analyze learner errors. 
Some of the existing learner corpora are 
annotated for errors, and our learner corpus 
called the ?NICT JLE (Japanese Learner 
English) Corpus? is one of them. This is a two- 
million-word speech corpus of Japanese learner 
English. The source of the corpus data is 1,281 
audio-recorded speech samples of an English 
oral proficiency interview test ACTFL-ALC 
Standard Speaking Test (SST). The advantage of 
using the SST data as a source is that each 
speaker?s data includes his or her proficiency 
level based on the SST scoring method, which 
makes it possible to easily analyze and compare 
the characteristics of interlanguage of each 
developmental stage. This is one of the 
advantages of the NICT JLE corpus that is 
rarely found in other learner corpora. 
Although there are a lot of advantages of 
error-annotated learner corpora, we found some 
difficulties in designing an error tagset that 
covers important features of learner errors. The 
current version of our error tagset targets 
morphological, grammatical, and lexical errors, 
and we found it can help to successfully assess 
to what extent learners can command the basic 
language system, especially grammar. However, 
we also found that it is not sufficient to measure 
learners? communicative skills. In order to 
determine how the current error tagset should be 
extended to cover more communicative aspects 
71
of learner language, we re-examined the learner 
data in the NICT JLE Corpus by focusing on 
?intelligibility? and ?naturalness?. 
In this paper, we discuss how error 
annotation for learner corpora should be 
designed and actually performed. The remainder 
of this paper is organized as follows. Section 2 
outlines the influence that Error Analysis (EA) 
in SLA research in the 1970s had on error 
annotation for learner corpora to enable us to 
rethink the concept of error annotation. Section 
3 provides some examples of learner corpus 
projects in which error tagging is performed. 
Section 4 describes the current error tagging 
scheme for the NICT JLE Corpus. Section 5 
examines how we can expand it to make it more 
useful for measuring learners? communicative 
skills. Finally, section 6 draws some general 
conclusions. 
2 Error Tagging and EA 
The idea of trying to tag errors in learner 
corpora might come from the notion of EA in 
SLA research in the 1970s. In order to design an 
error tagset and to actually perform tagging, we 
would like to reconfirm the concept of the 
traditional EA by considering about the 
definition of learner errors, the importance of 
analyzing learner errors for describing learner 
language, the actual EA procedures, and 
problems, and limitations of EA. 
2.1 Definition of Learner Errors 
Errors in a second language (L2) are often 
compared with errors in the first language (L1). 
According to Ellis (1994), before EA was 
introduced, L2 errors were often considered as 
?undesirable forms?. On the other hand, errors 
made by young L1 learners were regarded as the 
?transitional phase? in L1 acquisition, while 
errors made by adult L1 speakers were seen just 
as slips of the tongue. L2 errors were often 
shoved back into the closet as a negative aspect 
of learner language. 
In EA, errors are treated as evidence that 
plays an important role in describing learner 
language. Corder (1981) asserts that talking 
about learner errors only with terms like 
?deviant? or ?ill-formed? is inappropriate 
because it leads to learner errors being treated 
just as superficial deviations. Even if learners 
produce outputs whose surface structures are 
well-formed, this is not enough to prove that 
they have acquired the same language system as 
that L1 speakers have. 
In EA, learner errors are treated as something 
that proves that learners are in the transitional 
phase in L2 acquisition in a similar way to 
treating the language of L1 children. However, it 
is problematic to assume these two are exactly 
the same language. In L1 and L2 acquisition, 
there are certain processes in common, but they 
have not been scientifically confirmed. Many 
differences are found between these two. The 
learner language including errors can be defined 
as ?interlanguage?, which lies between L1 and 
L2 (Selinker, 1972). According to Corder (1981), 
learner errors are evidence of learning strategies 
in which learners are ?investigating? the system 
of the new language (L2) by examining to what 
extent L1 and L2 are similar and how different 
they are. 
2.2 Importance of EA 
Analyzing learner errors is important for 
teachers, researchers, and learners themselves in 
the following way (Corder, 1981). First, for 
teachers, errors can give them hints about the 
extent to which the learners have acquired the 
language system by that time and what they still 
have to learn. For SLA researchers, errors can 
reveal the process by which L2 is acquired and 
the kinds of strategies or methodology the 
learners use in that process. Finally, for learners 
themselves, as stated in 2.1, making errors is one 
of the most important learning strategies for 
testing the interlanguage hypothesis that learners 
have established about L2. In other words, 
knowing what kinds of errors were made by 
themselves or by other learners can be ?negative 
evidence (or feedback)? given directly or 
indirectly to learners that an interlanguage 
hypothesis is incorrect (Ellis, 1997). 
2.3 EA Procedure 
In general, the EA procedure can be divided into 
four stages as shown in Figure 1 (Ellis, 1994). 
 
72
Identifying Errors
Describing Errors
Explaining Errors
Evaluating Errors
 
Figure1. EA Procedure. 
 
In the first stage, identifying errors, it is 
necessary to localize errors by pointing out 
which letters, words, and phrases, or how 
sentence structures or word order, are incorrect. 
In the second stage, identified errors should be 
described by being linguistically categorized 
depending on, for example, their POS (part-of-
speech), linguistic level (morpheme, syntax, 
lexis, or discourse), or how they deviate from 
the correct usage on the surface structure 
(redundancy, omission, or replacement). Thirdly, 
?explaining errors? means identifying why those 
errors occurred. This is a very important task in 
order to figure out the learners? cognitive stage. 
Some causes of learner errors have been 
recognized in common such as errors caused by 
language transfer, learning and communication 
strategy-based errors, and the transfer of training 
and induced errors. Finally, errors are evaluated. 
This can be done by estimating intelligibility or 
near-nativeness of erroneous outputs. In other 
words, ?error gravity? is estimated by examining 
how each error interferes with the intelligibility 
of the entire outputs. 
2.4 Problems and Limitations of 
Traditional EA 
Although it is widely recognized that EA 
contributes to describing learner language and 
the improving second language pedagogy, 
several problems and limitations have been 
pointed out mainly because a concrete 
methodology of EA has not been established yet. 
Most importantly, EA cannot be successful 
without robust error typology, which is often 
very difficult to obtain. Since it used to be 
difficult to collect or access large databases of 
learner language, a robust error typology that 
covers almost all error types was not established 
in traditional EA. 
Another criticism against EA is that errors 
reflect only one side of learner language. A lot 
of people point out that if a researcher analyzes 
only errors and neglects what learners can do 
correctly, he/she will fail to capture the entire 
picture of learner language. It is time-consuming 
to count both correct and incorrect usages in 
learner data, and this must have been quite 
difficult to do in the past before computing 
technology was developed. 
Furthermore, the real significance of EA 
cannot be identified without using diachronic 
data in order to describe learners? developmental 
stages. The types and frequencies of errors 
change with each acquisition phase. Without 
longitudinal data of learner language, it is 
difficult to obtain a reliable result by EA. 
2.5 From EA to Error?coded Learner 
Corpora 
The problems and limitations of traditional EA 
are mainly due to the deficiency of computing 
technology and the lack of large databases in 
early times. However, now that computing 
technology has advanced, and a lot of learner 
data is available, it might be possible to perform 
EA more effectively mainly by annotating errors. 
Although the basic motivations for error 
annotation are the same as those of traditional 
EA, such as describing learner language and 
improving second-language pedagogy, several 
new applications of EA might become possible 
such as the development of a new computer-
aided language learning (CALL) environment 
that can process learners? erroneous input and 
give feedback automatically. 
Degneaux, et al (1998) call EA based on 
learner corpora ?computer-aided error analysis 
(CEA)?, and expect that the rapid progress of 
computing technology and learner corpora will 
be able to solve the problems and overcome the 
limitations of traditional EA. Surely, thanks to 
the quantitative database of learner language, we 
will become able to cover a wider range of 
learner errors. Advances in computing 
technology make it possible to perform 
statistical analysis with quantitative data more 
easily. However, it must be noted that human 
researchers still have a lot of work to do in the 
same manner as in traditional EA, such as 
establishing an error typology for error tagging 
73
or examining results obtained from CEA 
carefully. 
3 Related Work 
There are a few learner corpus projects that 
implement CEA. For example, in the 
International Corpus of Learner English (ICLE) 
project, which was launched by Professor 
Sylviane Granger at the University of Louvain, 
Belgium, and has been a ?pioneer? in learner 
corpus research since the early 1990s, they 
performed error tagging with a custom-designed 
error tagset (Degneaux, et al, 1996). The 
grammatical, lexical and pragmatic errors are 
dealt with in their error tagset and the corrected 
form is also indicated for each error. We guess 
that error categorization has been done mainly 
by translating the basic English grammar or 
lexical rules into an error ontology to try to 
cover as many types of errors as possible. The 
ICLE team currently comprises 17 partners 
internationally, and the corpus encloses 17 
subcorpora of learners of the different mother 
tongues (Bulgarian, Czech, Dutch, Finnish, 
French, German, Italian, Polish, Russian, 
Spanish, Swedish, and so on). A comparison of 
this data will make possible ?contrastive 
interlanguage analysis (CIA)? proposed by 
Granger (2002). CIA involves both NN/NNS 
and NNS/NNS comparisons (NS: native 
speakers; NNS: non-native speakers), as shown 
in Figure 2. NS/NNS comparisons might reveal 
how and why learner language is non-nativelike. 
NNS/NNS comparisons help researchers to 
distinguish features shared by several learner 
populations, which are more likely to be 
developmental from ones peculiar to a certain 
NNS group, which may be L1-dependent. 
CIA
NS NNS NNS NNSvs vs  
Figure 2. Contrastive Interlanguage Analaysis 
(Granger, 2002). 
Another corpus thathas been error tagged is 
the ?Japanese EFL Learner (JEFLL) Corpus?. 
This corpus, which was created by Professor 
Yukio Tono at Meikai University in Japan, has 
three parts: i) the L2 learner corpora which 
include written (composition) and spoken 
(picture description) data of Japanese learner 
English, ii) the L1 corpora consisting of 
Japanese written texts for the same tasks as 
those in the first part and Japanese newspaper 
articles, and iii) the EFL textbook corpus, which 
is the collection of EFL textbooks used officially 
at every junior high school in Japan (Tono, 
2002). Compared with the ICLE, which has 
been annotated with the generic error tagset, the 
error tagging for the JEFFL Corpus focuses on 
specific types of errors, especially major 
grammatical morphemes such as articles, plural 
forms of nouns, and third person singular 
present forms of verbs, and so on. We assume 
that those items were selected due to the corpus 
developer?s research interests. Although 
completeness for covering all errors would be 
decreased by focusing on a limited number of 
error types, annotators will be able to perform 
tagging more stably without being confused 
among various different types of errors. 
The Cambridge Learners? Corpus (CLC), 
which has been compiled by Cambridge 
University Press and Cambridge ESOL (English 
for Speakers of Other Languages), is also an 
error-coded learner corpus. It forms part of the 
Cambridge International Corpus (CIC) and is a 
large collection of essay writing from learners of 
English all over the world. This corpus has been 
utilized for the development of publications by 
authors and writers in Cambridge University 
Press and by members of staff at Cambridge 
ESOL. Over eight million words of the CLC 
have been error-coded with a Learner Error 
Coding System devised by Cambridge 
University Press. In order to make the tagged 
data as consistent as possible, tagging has been 
done by only one annotator since it started in 
1993. Their error tagset covers 80 types of errors. 
The annotator chooses an appropriate tag for 
each error mainly by identifying which POS the 
error involves and how it deviates from the 
correct usage (redundancy, omission, or 
replacement). A corrected form is also indicated 
for each error. 
4 Error Tags in the NICT JLE Corpus 
In this section, we introduce the error annotation 
scheme we used for the NICT JLE Corpus. 
We are aware that it is quite difficult to 
design a consistent error tagset as the learner 
74
errors extend across various linguistic areas, 
including grammar, lexis, and phoneme, and so 
on. We designed the original error tagset only 
for morphological, grammatical, and lexical 
errors, which are relatively easy to categorize 
compared with other error types, such as 
discourse errors and other types of errors related 
to more communicative aspects of learners? 
language. As shown in Figure 3, our error tags 
contain three pieces of information: POS, 
morphological/grammatical/lexical rules, and a 
corrected form. For errors that cannot be 
categorized as they do not belong to any word 
class, such as the misordering of words, we 
prepared special tags. The error tagset currently 
consists of 46 tags (Table 1). 
POS
(i.e. n =noun)
Grammatical system
(i.e. num =number)
Erroneous part
Corrected form
<n_num crr=?X?>?</n_num>
 
example) I belong to two baseball <n_num crr= 
?teams?>team</n_num>. 
Figure 3. Structure of an Error Tag and an 
Example of an Error-tagged Sentence 
The tags are based on XML (extensible 
markup language) syntax. One advantage of 
using XML is that it can clearly identify the 
structure of the text and it is also very beneficial 
when corpus data is utilized for web-based 
pedagogical tools or databases as a hypertext. 
The error tagset was designed based on the 
concept of the ICLE?s error tagging, that is, to 
deal with as many morphological, grammatical, 
and lexical errors as possible to have a generic 
error tagset. However, there are several 
differences between these two tagsets. For 
example, in the ICLE, only replacement-type 
errors are linguistically categorized, and 
redundant- and omission-type errors are not 
categorized any more and just called as ?word 
redundant? or ?word missing?, while in our 
error tagset, all these three types of errors are 
linguistically categorized. 
Although our error tagset covers major 
grammatical and lexical errors, annotators often 
have difficulties to select the most appropriate 
one for each error in actual tagging process. For 
example, one erroneous part can often been 
interpreted as more than one error type, or 
sometimes multiple errors are overlapping in the 
same position. 
To solve these problems, tagging was done 
under a few basic principles as follows. 
1) Because of the limitation of XML syntax 
(i.e. Crossing of different tags is not 
allowed.), each sentence should be 
corrected in a small unit (word or phrase) 
and avoid to change a sentence structure 
unnecessarily. 
2) If one phenomenon can be interpreted as 
more than one error type, select an error 
type with which an erroneous sentence 
can be reconstructed into a correct one 
without changing the sentence structure 
drastically. In this manner, errors should 
be annotated as locally as possible, but 
there is only one exception for 
prefabricated phrases. For example, if a 
sentence ?There are lot of books.? should 
be corrected into ?There are a lot of 
books.?, two ways of tagging are 
possible as shown in a) and b). 
a) There are <at crr= ?a?></at> lot of 
books. 
b) There are <o_lxc crr= ?a lot of?>lot 
of</o_lxc> books. 
In a), just an article ?a? is added before 
?lot of?, while in b), ?lot of? is corrected 
into ?a lot of? as a prefabricated phrase. 
In this case, b) is preferred. 
3) If multiple errors overlap in the same or 
partly-same position, choose error tags 
with which an erroneous sentence can be 
reconstructed into a correct one step by 
step in order to figure out as many errors 
as possible. For example, in the case that 
a sentence ?They are looking monkeys.? 
should be corrected into a sentence 
?They are watching monkeys.?, two ways 
of tagging are possible as shown in c) 
and d). 
c) They are <v_lxc crr= ?watching?> 
looking</v_lxc> monkeys. 
d) They are <v_lxc crr= ?watching?> 
looking<prp_lxc2 crr= ?at?> 
</prp_lxc2></v_lxc> monkeys. 
In c), ?looking? is replaced with 
?watching? in one step, while in d), 
missing of a preposition ?at? is pointed 
out first, then, ?looking at? is replaced 
75
with ?watching?. In our error tagging 
scheme, d) is more preferred. 
Tag Error category
<n_inf>?</n_inf> Noun inflection
<n_num>?</n_num> Noun number
<n_cs>?</n_cs> Noun case
<n_cnt>?</n_cnt> Countability of noun
<n_cmp>?</n_cmp> Complement of noun
<n_lxc>?</n_lxc> Lexis
<v_inf>?</v_inf> Verb inflection
<v_agr>?</v_agr> Subject-verb disagreement
<v_fml>?</v_fml> Verb form
<v_tns>?</v_tns> Verb tense
<v_asp>?</v_asp> Verb aspect
<v_vo>?</v_vo> Verb voice
<v_fin>?</v_fin> Usage of finite/infinite verb
<v_ng>?</v_ng> Verb negation
<v_qst>?</v_qst> Question
<v_cmp>?</v_cmp> Complement of verb
<v_lxc>?</v_lxc> Lexis
<mo_lxc>?</mo_lxc> Lexis
<aj_inf>?</aj_inf> Adjective inflection
<aj_us>?</aj_us> Usage of positive/comparative/superlative of adjective
<aj_num>?</aj_num> Adjective number
<aj_agr>?</aj_agr> Number disagreement of adjective
<aj_qnt>?</aj_qnt> Quantitative adjective
<aj_cmp>?</aj_cmp> Complement of adjective
<aj_lxc>?</aj_lxc> Lexis
<av_inf>?</av_inf> Adverb inflection
<av_us>?</av_us> Usage of positive/comparative/superlative of adverb
<av_pst>? </av_pst> Adverb position
<av_lxc>?</av_lxc> Lexis
<prp_cmp>?</prp_cmp> Complement of preposition
<prp_lxc1>?</prp_lxc1> Normal preposition
<prp_lxc2>?</prp_lxc2> Dependent preposition
<at>?</at> Article
<pn_inf>?</pn_inf> Pronoun inflection
<pn_agr>?</pn_agr> Number/sex disagreement of pronoun
<pn_cs>?</pn_cs> Pronoun case
<pn_lxc>?</pn_lxc> Lexis
<con_lxc>?</con_lxc> Lexis
<rel_cs>?</rel_cs> Case of relative pronoun
<rel_lxc>?</rel_lxc> Lexis
<itr_lxc>?</itr_lxc> Lexis
<o_je>?</o_je> Japanese English
<o_lxc>?</o_lxc> Collocation
<o_odr>?</o_odr> Misordering of words
<o_uk>?</o_uk> Unknown type errors
<o_uit>?</o_uit> Unintelligible utterance
RELATIVE PRONOUN
INTERROGATIVE
OTHERS
PREPOSITION
ARTICLE
PRONOUN
CONJUNCTION
NOUN
VERB
MODAL VERB
ADVERB
ADJECTIVE
 
Table 1. Error Tags for the NICT JLE Corpus. 
4.1 Advantages of Current Error Tagset 
Error tagging for learner corpora including the 
NICT JLE Corpus and the other corpora listed in 
Section 3 is carried out mainly by categorizing 
?likely? errors implied from the existing 
canonical grammar rules or POS system in 
advance. In this sub-section, we examine the 
advantages of this type of error tagging through 
research and development done by using these 
corpora. 
Tono (2002) tried to determine the order in 
which Japanese learners acquire the major 
English grammatical morphemes using the error 
tag information in the JEFFL Corpus. Izumi and 
Isahara (2004) did the same investigation based 
on the NICT JLE Corpus and found that there 
was a significant correlation between their 
sequence and Tono?s except for a few 
differences that we assume arose from the 
difference in the languga e production medium 
(written or spoken). Granger (1999) found that 
French learners of English tended to make verb 
errors in the simple present and past tenses 
based on the French component of the ICLE. 
Izumi et al (2004) also developed a framework 
for automated error detection based on machine 
learning in which the error-tagged data of the 
NICT JLE Corpus was used as training data. In 
the experiment, they obtained 50% recall and 
76% precision. 
Error tagging based on the existing canonical 
grammar rules or POS system can help to 
successfully assess to what extent learners can 
command the basic language system, especially 
grammar. This can assist people such as teachers 
who want to improve their grammar teaching 
method, researchers who want to construct a 
model of learners? grammatical competence, and 
learners who are studying for exams with 
particular emphasis on grammatical accuracy. 
5 Future Improvement 
Finally, let us explain our plans for future 
improving and extending error tagging for the 
NICT JLE Corpus.  
5.1 Problems of Current Error Tagset 
Although the current error tagging scheme is 
beneficial in the ways mentioned in 4.1, it 
cannot be denied that much could be improved 
to make it useful for teachers and researchers 
who want to know learners? communicative 
skills rather than grammatical competence. The 
same can be said for learners themselves. In the 
past, English education in Japan mainly focused 
on developing grammatical competence in the 
past. However, in recent years, because of the 
recognition of English as an important 
communication tool among peoples with 
different languages or cultures, acquiring 
communicative competence, especially 
76
production skills, has become the main goal for 
learners. One of the most important things for 
acquiring communicative skills might be 
producing outputs that can be understood 
properly by others. In other words, for many 
learners, conveying their messages clearly is 
often more important than just producing 
grammatically-correct sentences. 
It is necessary to make the current error 
tagset more useful for measuring learners? 
communicative competence. To do this, firstly 
we need to know what kind of learners? outputs 
can be understood by native speakers and in 
what cases they fail to convey their messages 
properly. By doing this, it should become 
possible to differentiate fatal errors that prevent 
the entire output from being understood from 
small errors that do not interfere with 
understanding. 
Another goal of studying English for learners, 
especially at the advanced level, is to speak like 
a native speaker. Some learners mind whether 
their English sounds natural or not to native 
speakers. In the current error tagging, both 
obvious errors and expressions that are not 
errors but are unnatural are treated at the same 
level. It would be better to differentiate them in 
the new error annotation scheme. 
5.2 Survey for Extending Current Error 
Tagset 
To solve the problems of our current error 
tagging system discussed in 5.1, we decided to 
do a survey to: 
1) Identify fatal errors and small ones by 
examining ?learners? outputs that can 
be understood properly by native 
speakers? and ?those that do not make 
sense to native speakers?. 
2) Identify unnatural and non-nativelike 
expressions and examine why they 
sound unnatural. 
We will do this mainly by examining the 
learner data corrected by a native speaker. 
Correction by NS 
We asked a native speaker of English to correct 
raw learner data (15 interviews, 17,068 words, 
1,657 sentences) from the NICT JLE Corpus 
and add one of the following three comments 
(Table 2) to each part. 
Comment 1 It is obviously an error, but does 
not interfere with understanding. 
Comment 2 The meaning of the utterance does 
not make sense at all. 
Comment 3 It is not an error, and the 
utterance makes sense, but it 
sounds unnatural. 
Table 2. Comments added to each error 
The person who did the corrections is a 
middle-aged British man who has lived in Japan 
for 14 years. He does not have experience as an 
English teacher, but used to teach Japanese 
Linguistics at a British University. Although he 
is familiar with English spoken by Japanese 
people because of his long residence in Japan 
and the knowledge of the Japanese language, we 
asked him to apply the corrections objectively 
with considering whether or not each utterance 
was generally intelligible to native speakers. 
Corrected Parts 
A total of 959 errors were corrected and 724 of 
these were labeled with Comment 1, 57 with 
Comment 2, and 178 with Comment 3, 
respectively (Table 3). 
Comment 1 724 
Comment 2 57 
Comment 3 178 
Total 959 
Table 3. Number of Errors Labeled with Each 
Comment. 
In order to examine what kind of differences 
can be found among errors labeled with these 
comments, we categorized them into four types 
(morpheme, grammar, lexis, and discourse) 
depending on which linguistic level each of 
them belongs to based on corrected forms and 
additional comments made by the labeler (Table 
4). 
 Comment1 Comment2 Comment3 Total 
Morpheme 6 0 0 6 
Grammar 429 0 52 481 
Lexis 286 43 78 407 
Discourse 3 14 48 65 
Total 724 57 178 959 
Table 4. Linguistic Level Involved in Each Error. 
As a whole, the most common type was 
grammar (481), but most of the grammatical 
errors (or cases of unnaturalness) were labeled 
with Comment 1, which implies that in most 
cases, the grammatical errors do not have a fatal 
influence making the entire output unintelligible. 
The second-most common type was lexical 
errors (or cases of unnaturalness) (407). Half of 
them were labeled with Comment 1, but 23 
errors got Comment 2. This means that some 
77
errors can interfere with understanding. 
Discourse errors accounted for a fraction of a 
percent of all errors (65). However, compared 
with other types of errors, the percentage of 
Comment 2 was the highest (14 out of 65), 
which means that discourse errors can greatly 
interfere with the intelligibility of the entire 
output. The main difference between the 
discourse errors labeled with Comment 2 and 
those labeled with Comment 3 was that most of 
the latter related to collocational expressions, 
while the former involved non-collocational 
phrases where learners need to construct a 
phrase or sentence by combining single words. 
In the following sections, we examine the 
characteristics of each type of error (or cases of 
unnaturalness) in detail. 
Comment 1 
Half of the Comment 1 errors were grammatical 
ones. Most of them were local errors such as 
subject-verb disagreement or article errors. 
There were 286 lexical errors, but in most cases, 
they were not very serious, for example lexical 
confusions among semantically similar 
vocabulary items. 
Comment 2 
Most of the Comment 2 errors had something to 
do with lexis or discourse. 
 
1) Too abrupt literary style (discourse error) 
ex) I?ve been to the restaurant is first. I 
took lunch. The curry the restaurant 
serves is very much, so I was surprised 
and I?m now a little sleepy. 
2) Unclear context (discourse error) 
3) Unclear anaphora (pronouns and 
demonstratives) (discourse error) 
4) Mis-selection of vocabulary (lexical 
error) 
5) Omission of an important word (subject, 
predicate or object) (lexical or syntax 
error) 
ex) She didn?t (*) so much about fashion. 
ex) Last year, I enjoyed living alone, but 
nowadays, it?s a little bit troublesome 
because I have to (*) all of the things by 
myself. 
6) Japanese English/Direct translation 
(lexical error) 
ex) bed town (as ?bedroom suburbs?) 
ex) claim (as ?complaint?) 
Comment 3 
There were grammatical, lexical and discourse 
problems with the parts labeled with Comment 
1) Verbose expressions (discourse-level 
unnaturalness) 
ex) T: Can I call you Hanako? 
L: Yes, please call me Hanako. 
better  Yes, please do. 
ex) Three couples are there and they?re having 
dinner. 
better  Three couples are having dinner. 
ex) I told my friends about this, and my friends 
agreed with me. 
better  I told my friends about this, and they 
agreed with me. 
 
2) Socio-linguistically inappropriate expressions 
(discourse/pragmatic-level unnaturalness) 
ex) What? 
better  I beg your pardon? 
ex) Good. 
better  I?m fine. 
 
3) Abrupt expressions (discourse/pragmatic-
level unnaturalness) 
ex) T: Have you been busy lately? 
 L: No. 
better  No, not really. 
 
4) Overstatement (discourse/pragmatic-level 
unnaturalness) 
(In a normal context) 
ex) T: How are you? 
L: I?m very fine. 
better  I?m fine. 
 
5) There are more appropriate words or 
expressions. (discourse/pragmatic-level of 
unnaturalness) 
ex) To go to high school in the mainland, I went 
out of the island. 
better  ... I left the island. 
5.3 Limitation of Current Error 
Annotation Scheme 
It is obvious that discourse and some types of 
lexical errors can often impede the 
understanding of the entire utterance. 
Although our current error tagset does not 
cover discourse errors, it is still possible to 
78
?just? assign any one of error tags to the 
erroneous parts shown in 5.2. There are two 
reasons for this. One is that, in the current error 
tagging principle, it is possible to replace, add or 
delete all POS in order to make it possible to 
?reconstruct? an erroneous sentence into a 
correct one. The other reason is that since 
discourse structure is liked to grammatical and 
lexical selections, it is possible to translate terms 
for describing discourse into terms for 
describing grammar or lexis. 
However, annotating discourse errors with 
tags named with grammatical or lexical terms 
cannot represents the nature of discourse errors. 
Since discourse errors are often related to 
intelligibility of learners? outputs, describing 
those errors with appropriate terms is quite 
important for making the current error tagset 
something helpful for measuring learners? 
communicative competence. We will need to 
know what kind of discourse errors are made by 
learners, and classify them to build in the error 
tagset. Some parts labeled with Comment 3 
were also related to discourse-level problems. It 
would be beneficial to provide learners with 
feedback such as ?Your English sounds 
unnatural because it?s socio-linguistically 
inappropriate?. Therefore, it is also necessary to 
classify discourse-level unnaturalness in learners 
language. 
5.4 Works for Expansion to New Error 
Tagset 
We decided the basic principles for revising the 
current error tagset as following. 
1) Classify second language discourse 
errors and building them into a new 
error tagset. 
2) Differentiate unnatural expressions 
from errors. Information on why it 
sounds unnatural will also be added. 
3) Add information on linguistic level 
(morpheme, grammar, lexis, and 
discourse) to each tag. 
4) Do a further survey on how we can 
differentiate errors that interfere with 
understanding and those that do not, 
and add information on error gravity to 
each tag. 
Classifying discourse errors will be the most 
important task in the tagset revision. In several 
studies, second language discourse has already 
been discussed (James, 1998), but there is no 
commonly recognized discourse error typology. 
Although grammatical and lexical errors can be 
classified based on the existing canonical 
grammar rules or POS system, in order to 
construct the discourse error typology, we will 
need to do more investigation into ?real? 
samples of learners? discourse errors. 
Adding the information on linguistic level 
(morpheme, grammar, lexis and discourse) to 
each tag is also important. From the survey, we 
found that the linguistic level of errors is 
strongly related to the intelligibility of the entire 
output. If linguistic level information is added to 
each error tag, this might help to measure the 
intelligibility of learners? utterances, that is, 
learners? communicative competence. 
6 Conclusion 
In this paper, we discussed how the error 
annotation scheme for learner corpora should be 
designed mainly by explaining the current error 
tagging scheme for the NICT JLE Corpus and 
its future expansion. Through learner data 
corrected by a native speaker, we decided to 
introduce discourse errors into the error 
annotation in order to cover learners? 
communicative competence, which cannot be 
measured with the current error tagging scheme. 
 
References 
Corder, P. (1981). Error Analysis and Interlanguage. 
Oxford: Oxford University Press. 
Degneaux, E., Denness, S., Granger, S., & Meunier, 
F. (1996). Error Tagging Manual Version 1.1. 
Centre for English Corpus Linguistics, Universite 
Catholique de Louvain. 
Degneaux, E., Denness, S., & Ganger, S. (1998). 
Computer-aided error analysis, System, 26, 163-
174. 
Ellis, R. (1994). The Study of Second Language 
Acquisition. Oxford: Oxford University Press. 
Ellis, R. (1997). Second Language Acquisition. 
Oxford: Oxford University Press. pp. 47, 67. 
Granger, S. (1999). Use of tenses by advanced EFL 
learners: evidence from an error-tagged computer 
corpus. In Hasselgard, H., & Oksefjell, S. (Eds). 
Out of Corpora. (pp. 191-202). Amsterdam: 
Rodopi. 
79
Granger, S. (2002) A bird?s-eye view of learner 
corpus research. In Granger, S., Hung, J., and 
Tyson, P.S. (Eds.). (2002). Computer Learner 
Corpora, Second Language Acquisition and 
Foreign Language Teaching, Amsterdam: John 
Benjamins Publishing Company. 
Izumi, E., Uchimoto, K., & Isahara, H. (2004). The 
overview of the SST speech corpus of Japanese 
learner English and evaluation through the 
experiment on automatic detection of learners' 
errors. In Proceedings of Language Resource and 
Evaluation Conference (LREC) 2004, Portugal, 
1435-1438. 
Izumi, E., & Isahara, H. (2004). Investigation into 
language learners' acquisition order based on the 
error analysis of the learner corpus. In 
Proceedings of Pacific-Asia Conference on 
Language, Information and Computation 
(PACLIC) 18 Satellite Workshop on E-Learning. 
Tokyo, Japan. 
James, C. (1998). Errors in Language Learning and 
Use: exploring error analysis. Essex: Longman. 
Selinker, L. (1972). Interlanguage. In Robinett, B. W., 
& Schachter, J. (Eds.). (1983). Second Language 
Learning: Contrastive analysis, error analysis, 
and related aspects. (pp. 173-196). Michigan: The 
University of Michigan Press. 
Tono, Y. (2002). The Role of Learner Corpora in 
SLA Research and Foreign Language Teaching: 
The Multiple Comparison Approach. Unpublished 
Ph.D. Thesis. Lancaster University, UK. 
CLC (Cambridge Learners Corpus)?s Website: 
http://uk.cambridge.org/elt/corpus/clc.htm 
80
Dependency Parsing with Short Dependency Relations in Unlabeled Data
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, dk, uchimoto, yujie, isahara}@nict.go.jp
Abstract
This paper presents an effective dependency
parsing approach of incorporating short de-
pendency information from unlabeled data.
The unlabeled data is automatically parsed
by a deterministic dependency parser, which
can provide relatively high performance for
short dependencies between words. We then
train another parser which uses the informa-
tion on short dependency relations extracted
from the output of the first parser. Our pro-
posed approach achieves an unlabeled at-
tachment score of 86.52, an absolute 1.24%
improvement over the baseline system on
the data set of Chinese Treebank.
1 Introduction
In dependency parsing, we attempt to build the
dependency links between words from a sen-
tence. Given sufficient labeled data, there are sev-
eral supervised learning methods for training high-
performance dependency parsers(Nivre et al, 2007).
However, current statistical dependency parsers pro-
vide worse results if the dependency length be-
comes longer (McDonald and Nivre, 2007). Here
the length of a dependency from word w
i
and word
w
j
is simply equal to |i ? j|. Figure 1 shows the
F
1
score1 provided by a deterministic parser rela-
tive to dependency length on our testing data. From
1precision represents the percentage of predicted arcs of
length d that are correct and recall measures the percentage of
gold standard arcs of length d that are correctly predicted.
F
1
= 2? precision? recall/(precision + recall)
the figure, we find that F
1
score decreases when de-
pendency length increases as (McDonald and Nivre,
2007) found. We also notice that the parser pro-
vides good results for short dependencies (94.57%
for dependency length = 1 and 89.40% for depen-
dency length = 2). In this paper, short dependency
refers to the dependencies whose length is 1 or 2.
 30
 40
 50
 60
 70
 80
 90
 100
 0  5  10  15  20  25  30
F1
Dependency Length
baseline
Figure 1: F-score relative to dependency length
Labeled data is expensive, while unlabeled data
can be obtained easily. In this paper, we present an
approach of incorporating unlabeled data for depen-
dency parsing. First, all the sentences in unlabeled
data are parsed by a dependency parser, which can
provide state-of-the-art performance. We then ex-
tract information on short dependency relations from
the parsed data, because the performance for short
dependencies is relatively higher than others. Fi-
nally, we train another parser by using the informa-
tion as features.
The proposed method can be regarded as a semi-
supervised learning method. Currently, most semi-
88
supervised methods seem to do well with artificially
restricted labeled data, but they are unable to outper-
form the best supervised baseline when more labeled
data is added. In our experiments, we show that our
approach significantly outperforms a state-of-the-art
parser, which is trained on full labeled data.
2 Motivation and previous work
The goal in dependency parsing is to tag dependency
links that show the head-modifier relations between
words. A simple example is in Figure 2, where the
link between a and bird denotes that a is the depen-
dent of the head bird.
I    see    a    beautiful    bird    .
Figure 2: Example dependency graph.
We define that word distance of word w
i
and word
w
j
is equal to |i ? j|. Usually, the two words in a
head-dependent relation in one sentence can be adja-
cent words (word distance = 1) or neighboring words
(word distance = 2) in other sentences. For exam-
ple, ?a? and ?bird? has head-dependent relation in
the sentence at Figure 2. They can also be adjacent
words in the sentence ?I see a bird.?.
Suppose that our task is Chinese dependency
parsing. Here, the string ????JJ(Specialist-
level)/? ?NN(working)/? ?NN(discussion)?
should be tagged as the solution (a) in Figure
3. However, our current parser may choose the
solution (b) in Figure 3 without any additional
information. The point is how to assign the head for
????(Specialist-level)?. Is it ???(working)?
or ???(discussion)??
  
  
  
(b)
(a)
Figure 3: Two solutions for ????(Specialist-
level)/??(working)/??(discussion)?
As Figure 1 suggests, the current dependency
parser is good at tagging the relation between ad-
jacent words. Thus, we expect that dependencies
of adjacent words can provide useful information
for parsing words, whose word distances are longer.
When we search the string ????(Specialist-
level)/??(discussion)? at google.com, many rele-
vant documents can be retrieved. If we have a good
parser, we may assign the relations between the two
words in the retrieved documents as Figure 4 shows.
We can find that ???(discussion)? is the head of
????(Specialist-level)? in many cases.
1)?525	26
///,//?
2)?Hypothesis Selection in Machine Transliteration: A Web Mining Approach
Jong-Hoon Oh and Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology (NICT)
3-5 Hikaridai,Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{rovellia,isahara}@nict.go.jp
Abstract
We propose a new method of selecting hy-
potheses for machine transliteration. We
generate a set of Chinese, Japanese, and Ko-
rean transliteration hypotheses for a given
English word. We then use the set of translit-
eration hypotheses as a guide to finding rel-
evant Web pages and mining contextual in-
formation for the transliteration hypotheses
from the Web page. Finally, we use the
mined information for machine-learning al-
gorithms including support vector machines
and maximum entropy model designed to
select the correct transliteration hypothesis.
In our experiments, our proposed method
based on Web mining consistently outper-
formed systems based on simple Web counts
used in previous work, regardless of the lan-
guage.
1 Introduction
Machine transliteration has been a great challenge
for cross-lingual information retrieval and machine
translation systems. Many researchers have devel-
oped machine transliteration systems that accept a
source language term as input and then output its
transliteration in a target language (Al-Onaizan and
Knight, 2002; Goto et al, 2003; Grefenstette et al,
2004; Kang and Kim, 2000; Li et al, 2004; Meng et
al., 2001; Oh and Choi, 2002; Oh et al, 2006; Qu
and Grefenstette, 2004). Some of these have used
the Web to select machine-generated transliteration
hypotheses and have obtained promising results (Al-
Onaizan and Knight, 2002; Grefenstette et al, 2004;
Oh et al, 2006; Qu and Grefenstette, 2004). More
precisely, they used simple Web counts, estimated as
the number of hits (Web pages) retrieved by a Web
search engine.
However, there are several limitations imposed on
the ability of Web counts to select a correct translit-
eration hypothesis. First, the assumption that hit
counts approximate the Web frequency of a given
query usually introduces noise (Lapata and Keller,
2005). Moreover, some Web search engines disre-
gard punctuation and capitalization when matching
search terms (Lapata and Keller, 2005). This can
cause errors if such Web counts are relied on to se-
lect transliteration hypotheses. Second, it is not easy
to consider the contexts of transliteration hypothe-
ses with Web counts because Web counts are esti-
mated based on the number of retrieved Web pages.
However, as our preliminary work showed (Oh et
al., 2006), transliteration or translation pairs often
appear as parenthetical expressions or tend to be in
close proximity in texts; thus context can play an im-
portant role in selecting transliteration hypotheses.
For example, there are several Chinese, Japanese,
and Korean (CJK) transliterations and their counter-
parts in a parenthetical expression, as follows.
1) 
1

2
(Adrienne
1
Clarkson
2
)
2) ?
1
	?

2
(glucose
1
oxidase
2
)
3) 
1
	

2
(diphenol
1
oxidase
2
)
Note that the subscripted numbers in all examples
represent the correspondence between the English
word and its CJK counterpart. These parentheti-
cal expressions are very useful in selecting translit-
233
eration hypotheses because it is apparent that they
are translation pairs or transliteration pairs. How-
ever, we cannot fully use such information with Web
counts.
To address these problems, we propose a new
method of selecting transliteration hypotheses. We
were interested in how to mine information relevant
to the selection of hypotheses and how to select cor-
rect transliteration hypotheses using the mined in-
formation. To do this, we generated a set of CJK
transliteration hypotheses for a given English word.
We then used the set of transliteration hypotheses
as a guide to finding relevant Web page and min-
ing contextual information for the transliteration hy-
potheses from the Web page. Finally, we used
the mined information for machine-learning algo-
rithms including support vector machines (SVMs)
and maximum entropy model designed to select the
correct transliteration hypothesis.
This paper is organized as follows. Section 2 de-
scribes previous work based on simple Web counts.
Section 3 describes a way of generating transliter-
ation hypotheses. Sections 4 and 5 introduce our
methods of Web mining and selecting transliteration
hypotheses. Sections 6 and 7 deal with our exper-
iments and the discussion. Conclusions are drawn
and future work is discussed in Section 8.
2 Related work
Web counts have been used for selecting translit-
eration hypotheses in several previous work (Al-
Onaizan and Knight, 2002; Grefenstette et al, 2004;
Oh et al, 2006; Qu and Grefenstette, 2004). Be-
cause the Web counts are estimated as the number of
hits by a Web search engine, they greatly depend on
queries sent to a search engine. Previous work has
used three types of queries?monolingual queries
(MQs) (Al-Onaizan and Knight, 2002; Grefen-
stette et al, 2004; Oh et al, 2006), bilingual
simple queries (BSQs) (Oh et al, 2006; Qu and
Grefenstette, 2004), and bilingual bigram queries
(BBQs) (Oh et al, 2006). If we let S be a source
language term and H = {h
1
, ? ? ? , hr} be a set of
machine-generated transliteration hypotheses of S,
the three types of queries can be defined as
MQ: hi (e.g., ,?, and	).
BSQ: s and hi without quotations (e.g., Clinton 
 , Clinton ?, and Clinton 
	).
BBQ: Quoted bigrams composed of S and hi (e.g.,
?Clinton ?, ?Clinton ??, and
?Clinton	?).
MQ is not able to determine whether hi is a counter-
part of S, but whether hi is a frequently used target
term in target-language texts. BSQ retrieves Web
pages if S and hi are present in the same document
but it does not take the distance between S and hi
into consideration. BBQ retrieves Web pages where
?S hi? or ?hi S? are present as a bigram. The rel-
ative order of Web counts over H makes it possible
to select transliteration hypotheses in the previous
work.
3 Generating Transliteration Hypotheses
Let S be an English word, P be a pronuncia-
tion of S, and T be a target language translitera-
tion corresponding to S. We implement English-
to-CJK transliteration systems based on three dif-
ferent transliteration models ? a grapheme-based
model (S ? T ), a phoneme-based model (S ? P
and P ? T ), and a correspondence-based model
(S ? P and (S, P ) ? T ) ? as described in our
preliminary work (Oh et al, 2006). P and T are seg-
mented into a series of sub-strings, each of which
corresponds to a source grapheme. We can thus
write S = s
1
, ? ? ? , sn = sn
1
, P = p
1
, ? ? ? , pn = pn
1
,
and T = t
1
, ? ? ? , tn = tn
1
, where si, pi, and ti rep-
resent the ith English grapheme, English phonemes
corresponding to si, and target language graphemes
corresponding to si, respectively. Given S, our
transliteration systems generate a sequence of ti cor-
responding to either si (in Eq. (1)) or pi (in Eq. (2))
or both of them (in Eq. (3)).
PrG(T |S) = Pr(tn
1
|sn
1
) (1)
PrP (T |S) = Pr(pn
1
|sn
1
)? Pr(tn
1
|pn
1
) (2)
PrC(T |S) = Pr(pn
1
|sn
1
)? Pr(tn
1
|sn
1
, pn
1
) (3)
The maximum entropy model was used to estimate
probabilities in Eqs. (1)?(3) (Oh et al, 2006). We
produced the n-best transliteration hypotheses using
a stack decoder (Schwartz and Chow, 1990). We
234
then created a set of transliteration hypotheses com-
prising the n-best transliteration hypotheses.
4 Web Mining
Let S be an English word and H = {h
1
, ? ? ? , hr} be
its machine-generated set of transliteration hypothe-
ses. We use S and H to generate queries sent to a
search engine1 to retrieve the top-100 snippets. A
correct transliteration and its counterpart tend to be
in close proximity on CJK Web pages. Our goal in
Web mining was to find such Web pages and mine
information that would help to select transliteration
hypotheses from these pages.
To find these Web pages, we used three kinds of
queries, Q
1
=(S and hi), Q2=S, and Q3=hi, where
Q
1
is the same as BSQ?s query and Q
3
is the same
as MQ?s. The three queries usually result in different
sets of Web pages. We categorize the retrieved Web
pages by Q
1
, Q
2
, and Q
3
into W
1
, W
2
, and W
3
. We
extract three kinds of features from Wl as follows,
where l = 1, 2, 3.
? Freq(hi,Wl): the number of occurrences of hi
in Wl
? DFreqk(hi,Wl): Co-occurrence of S and hi
with distance dk ? D in the same snippet of
Wl.
? PFreqk(hi,Wl): Co-occurrence of S and hi
as parenthetical expressions with distance dk ?
D in the same snippet of Wl. Parenthetical ex-
pressions are detected when either S or hi is in
parentheses.
We define D = {d
1
, d
2
, d
3
} with three ranges of
distances between S and hi, where d1(d < 5),
d
2
(5 ? d < 10), and d
3
(10 ? d ? 15). We counted
distance d with the total number of characters (or
words)2 between S and hi. Here, we can take the
contexts of transliteration hypotheses into account
using DFreq and PFreq; while Freq is counted
regardless of the contexts of the transliteration hy-
potheses.
Figure 1 shows examples of how to calculate
Freq, DFreqk, and PFreqk, where S = Clinton,
1We used Google (http://www.google.com)
2Depending on whether the languages had spacing units,
words (for English and Korean) or characters (for Chinese and
Japanese) were chosen to calculate d.
????????
1
(Bill Clinton1)????????????
????????????????????????????
??(My Life)????
2
?????????????????
???????????
3
(Hillary Rodham Clinton
2
)??1997
????? ...
1
(Bill Clinton1)
(My Life)
2
3
(Hillary Rodham Clinton
2
) 1997
...
W1: Q1=(Clinton ???)
::???
4
?Clinton
3
????????
1
??Kerry?::
?
2
??John Kerry???????????????????
??????????
5
?Clinton
4
????????????
?????????????????????Bush??"??
?"???? ???
6
?Clinton
5
???
3
??Kerry? ...
::
4
Clinton
3 1
Kerry ::
2
John Kerry
5
Clinton
4
Bush "
"
6
Clinton
5 3
Kerry ...
Snippet1
Snippet2
Figure 1: Web corpora collected by Clinton and 

Snippet
1

1

2

3
Clinton
1
1 41 68
Clinton
2
72 29 2
Snippet
2

4

5

6
Clinton
3
0 36 81
Clinton
4
40 0 37
Clinton
5
85 41 0
Snippet
2

1

2

3
Clinton
3
6 9 85
Clinton
4
32 29 42
Clinton
5
77 74 1
Table 1: Distance between Clinton and Chinese
transliteration hypotheses in Fig. 1
hi= in W1 collected by Q1=(Clinton 
). The subscripted numbers of Clinton and 
 were used to indicate how many times they oc-
curred in W
1
. In Fig. 1,  occurs six times
thus Freq(hi,W1) = 6. Table 1 lists the dis-
tance between Clinton andwithin each snip-
pet of W
1
. We can obtain DFreq
1
(hi,W1) =
5. PFreq
1
(hi,Wl) is calculated by detecting
parenthetical expressions between S and hi when
DFreq
1
(hi,Wl) is counted. Because all S in
W
1
(Clinton
1
to Clinton
5
) are in parentheses,
PFreq
1
(hi,W1) is the same as DFreq1(hi,W1).
We ignore Freq, DFreqk, and PFreqk when hi
is a substring of other transliteration hypotheses be-
cause hi usually has a higher Freq, DFreqk, and
PFreqk than hj if hi is a substring of hj . Let a
235
set of transliteration hypotheses for S = Clinton
be H= {h
1
= , h
2
= }. Here, h
2
is a
substring of h
1
. In Fig. 1, h
2
appears six times as
a substring of h
1
and three times independently in
Snippet
2
. Moreover, independently used h
2
(
1
,

2
, and 
3
) and S (Clinton
3
and Clinton
5
) are
sufficiently close to count DFreqk and PFreqk.
Therefore, the Freq, DFreqk, and PFreqk of h1
will be lower than those of h
2
if we do not take
the substring relation between h
1
and h
2
into ac-
count. Considering the substring relation, we ob-
tain Freq(h
2
,W
1
) = 3, DFreq
1
(h
2
,W
1
) = 1,
DFreq
2
(h
2
,W
1
) = 2, PFreq
1
(h
2
,W
1
) = 1, and
PFreq
2
(h
2
,W
1
) = 2.
5 Hypothesis Selection
We select transliteration hypotheses by ranking
them. A set of transliteration hypotheses, H =
{h
1
, h
2
, ? ? ? , hr}, is ranked to enable a correct hy-
pothesis to be identified. We devise a rank function,
g(hi) in Eq. (4), that ranks a correct transliteration
hypothesis higher and the others lower.
g(hi) : H ? {R : R is ordering of hi ? H} (4)
Let xi ? X be a feature vector of hi ? H, yi =
{+1,?1} be the training label for xi, and T D =
{td
1
=< x
1
, y
1
>, ? ? ? , tdz =< xz, yz >} be the
training data for g(hi). We prepare the training data
for g(hi) as follows.
1. Given each English word S in the training-set,
generate transliteration hypotheses H.
2. Given hi ? H, assign yi by looking for S and
hi in the training-set ? yi = +1 if hi is a cor-
rect transliteration hypothesis corresponding to
S, otherwise yi = ?1.
3. For each pair (S, hi), generate its feature vector
xi.
4. Construct a training data set, T D:
? T D = T D
+
?
T D
?
? T D
+
 tdi where yi = +1
? T D
?
 tdj where yj = ?1
We used two machine-learning algorithms, sup-
port vector machines (SVMs)3 and maximum en-
tropy model4 for our implementation of g(hi). The
SVMs assign a value to each transliteration hypoth-
esis (hi) using
gSVM (hi) = w ? xi + b (5)
where w denotes a weight vector. Here, we use the
predicted value of gSVM (hi) rather than the pre-
dicted class of hi given by SVMs because our rank-
ing function, as represented by Eq. (4), determines
the relative ordering between hi and hj in H. A
ranking function based on the maximum entropy
model assigns a probability to hi using
gMEM (hi) = Pr(yi = +1|xi) (6)
We can finally obtain a ranked list for the given H?
the higher the g(hi) value, the better the hi.
5.1 Features
We represent the feature vector, xi, with two types
of features. The first is the confidence scores of hi
given by Eqs. (1)?(3) and the second is Web-based
features ? Freq, DFreqk, and PFreqk. To nor-
malize Freq, DFreqk, and PFreqk, we use their
relative frequency over H as in Eqs. (7)?(9), where
k = 1, 2, 3 and l = 1, 2, 3.
RF (hi,Wl) =
Freq(h
i
,W
l
)
?
h
j
?H
Freq(h
j
,W
l
)
(7)
RDFk(hi,Wl) =
DFreq
k
(h
i
,W
l
)
?
h
j
?H
DFreq
k
(h
j
,W
l
)
(8)
RPFk(hi,Wl) =
PFreq
k
(h
i
,W
l
)
?
h
j
?H
PFreq
k
(h
j
,W
l
)
(9)
Figure 2 shows how to construct feature vector
xi from a given English word, Rachel, and its Chi-
nese hypotheses, H, generated from our translitera-
tion systems. We can obtain r Chinese translitera-
tion hypotheses and classify them into positive and
negative samples according to yi. Note that yi = +1
if and only if hi is registered as a counterpart of S
in the training data. The bottom of Fig. 2 shows our
feature set representing xi. There are three confi-
dence scores in P (hi|S) according to transliteration
models and the three Web-based features Web(W
1
),
Web(W
2
), and Web(W
3
).
3SVM light (Joachims, 2002)
4
?Maximum Entropy Modeling Toolkit? (Zhang, 2004)
236
??????????????????
hr?h5h4h3h2h1H
-1-1-1-1-1+1
yr?y5y4y3y2y1Y
Rachel
RF(hi,W1)
RDF1(hi,W1) 
RDF2(hi,W1)
RDF3(hi,W1)
RPF1(hi,W1) 
RPF2(hi,W1)
RPF3(hi,W1)
Web (W1)
RF(W3)
RDF1(hi,W3) 
RDF2(hi,W3)
RDF3(hi,W3)
RPF1(hi,W3) 
RPF2(hi,W3)
RPF3(hi,W3)
RF(hi,W2)
RDF1(hi,W2) 
RDF2(hi,W2)
RDF3(hi,W2)
RPF1(hi,W2) 
RPF2(hi,W2)
RPF3(hi,W2)
PrG(hi|S) 
PrP(hi|S)
PrC(hi|S)
Web (W3)Web (W2)Pr(hi|S)xi
td1 ? TD+ td2, td3,  td4, td5,?,tdr? TD-
xr?x5x4x3x2x1X
Figure 2: Feature vectors
6 Experiments
We evaluated the effectiveness of our system in se-
lecting CJK transliteration hypotheses. We used the
same test set used in Li et al (2004) (ECSet) for Chi-
nese transliterations (Xinhua News Agency, 1992)
and those used in Oh et al (2006) for Japanese
and Korean transliterations ? EJSET and EK-
SET (Breen, 2003; Nam, 1997). We divided the test
ECSet EJSet EKSet
Training Set 31,299 8,335 5,124
Development Set 3,478 1,041 1,024
Blind Test Set 2,896 1,041 1,024
Total 37,694 10,417 7,172
Table 2: Test data sets
data into training, development, and blind test sets
as in Table 2. The training set was used to train our
three transliteration models to generate the n-best
transliteration hypotheses5. The development set
was used to train hypothesis selection based on sup-
port vector machines and maximum entropy model.
We used the blind test set for evaluation. The eval-
uation was done in terms of word accuracy (WA).
WA is the proportion of correct transliterations in
the best hypothesis by a system to correct transliter-
ations in the blind test set.
System ECSet EJSet EKSet
KANG00 N/A N/A 54.1
GOTO03 N/A 54.3 N/A
LI04 70.1 N/A N/A
GM 69.0 61.6 59.0
PM 56.6 54.4 56.7
CM 69.9 65.0 65.1
Table 3: WA of individual transliteration systems
(%)
6.1 Results: Web counts vs. Web mining
We compared our transliteration system with three
previous ones, all of which were based on a
grapheme-based model (Goto et al, 2003; Kang and
Kim, 2000; Li et al, 2004). LI046 is an English-
to-Chinese transliteration system, which simultane-
ously takes English and Chinese contexts into con-
sideration (Li et al, 2004). KANG00 is an English-
to-Korean transliteration system and GOTO03 is an
English-to-Japanese one ? they segment a chunk of
English graphemes and identify the most relevant
sequence of target graphemes corresponding to the
chunk (Goto et al, 2003; Kang and Kim, 2000) 7.
GM, PM, and CM, which are respectively based
on Eqs. (1)?(3), are the transliteration systems we
used for generating transliteration hypotheses. Our
transliteration systems showed comparable or better
performance than the previous ones regardless of the
language.
We compared simple Web counts with our Web
mining for hypothesis selection. We used the same
set of transliteration hypotheses H then compared
their performance in hypothesis selection with two
measures, relative frequency and g(hi). Tables 4 and
5 list the results. Here, ?Upper bound? is a system
that always selects the correct transliteration hypoth-
esis if there is a correct one inH. ?Upper bound? can
5We set n = 10 for the n-best. Thus, n ? r ? 3? n where
H = {h
1
, h
2
, ? ? ? , h
r
}
6The WA of LI04 was taken from the literature, where the
training data were the same as the union of our training set and
the development set while the test data were the same as in our
test set. In other words, LI04 used more training data than ours
did. With the same setting as LI04, our GM, PM, and CM pro-
duced respective WAs of 70.0, 57.7, and 71.7.
7We implemented KANG00 (Kang and Kim, 2000) and
GOTO03 (Goto et al, 2003), and tested them with the same
data as ours.
237
System ECSet EJSet EKSet
WC
MQ 16.1 40.4 34.7
BSQ 45.8 74.0 72.4
BBQ 34.9 78.1 79.3
WM
RF (W
1
) 62.9 78.4 77.1
RDF (W
1
) 70.8 80.4 80.2
RPF (W
1
) 73.5 79.7 79.4
RF (W
2
) 63.5 76.2 74.8
RDF (W
2
) 67.1 79.2 78.9
RPF (W
2
) 69.6 79.1 78.4
RF (W
3
) 37.9 53.9 55.8
RDF (W
3
) 76.4 69.0 70.2
RPF (W
3
) 76.8 68.3 68.7
Upper bound 94.6 93.5 93.2
Table 4: Web counts (WC) vs. Web mining (WM):
hypothesis selection by relative frequency (%)
System ECSet EJSet EKSet
WC MEMWC 74.7 86.1 85.6
SVMWC 74.8 86.9 86.5
WM MEMWM 82.0 88.2 85.8
SVMWM 83.9 88.5 86.7
Upper bound 94.6 93.5 93.2
Table 5: Web counts (WC) vs. Web mining (WM):
hypothesis selection by g(hi) (%)
also be regarded as the ?Coverage? of H generated
by our transliteration systems. MQ, BSQ, and BBQ
in the upper section of Table 4, represent hypothesis
selection systems based on the relative frequency of
Web counts over H, the same measure used in Oh et
al. (2006):
WebCountsx(hi)
?
h
j
?H
WebCountsx(hj)
(10)
where WebCountsx(hi) is a function returning
Web counts retrieved by x ? {MQ,BSQ,BBQ}
RF (Wl), RDF (Wl), and RPF (Wl) in Table 4 rep-
resent hypothesis selection systems with their rela-
tive frequency, where RDF (Wl) and RPF (Wl) use
?
3
k=1 RDFk(hj ,Wl) and
?
3
k=1 RPFk(hj ,Wl),
respectively. The comparison in Table 4 shows
which is best for selecting transliteration hy-
potheses when each relative frequency is used
alone. Table 5 compares Web counts with fea-
tures mined from the Web when they are used
as features in g(hi) ? {Pr(hi|S), Web(Wl)} in
MEMWM and SVMWM (our proposed method),
while {Pr(hi|S), WebCountsx(hi)} in MEMWC
and SVMWC . Here, Web(Wl) is a set of mined
features from Wl as described in Fig .2.
????????(a Man To Call My Own) ??
???????????- ????????(a Man To Call 
My Own), ????ranchhouse???????????????
??????????????? ????????????
?????????????...
(a Man To Call My Own) 
- (a Man To Call 
My Own), ranchhouse
...
??????(4/03)????
???????,?????????,???????,???
??????????????????,????????,?
???? ... ???????(Academy)??????????,
???????????????...
(4/03)
, , ,
, ,
... (Academy) ,
...
Snippet1 retrieved by BSQ: Aman ????
Snippet2 retrieved by MQ: ???? (meaning Agard)
?????|Cliff De Young| ??| ??| ??| EO????
????? | The Secret Life of Zoey (TV) ?????2002 ???
????????????? , ????? , ????? , ??
???? , Avery Raskin. ??????Larry Carter. ???4.92?
|Cliff De Young| | | | EO
| The Secret Life of Zoey (TV) 2002 
, , , 
, Avery Raskin. Larry Carter. 4.92
UNESCO. General Conference; 32nd; Election of member
????????????????. ?. 1987--1991. ???????
????????????. ????. (1976). 1987--1991. ????
??????????. 2001--2005. ????. 1993--1997....
UNESCO. General Conference; 32nd; Election of e ber
? ? ? . . 1987--1991. ?
? ? . . (1976). 1987--1991. ?
? . 2001--2005. . 1993--1997....
Snippet3 retrieved by MQ: ?????? (meaning Rawcliffe)
Snippet4 retrieved by MQ: ?????? (meaning Aldersey)
Figure 3: Snippets causing errors in Web counts
The results in the tables show that our systems
consistently outperformed systems based on Web
counts, especially for Chinese. This was due to the
difference between languages. Japanese and Chi-
nese do not use spaces between words. However,
Japanese is written using three different alphabet
systems, called Hiragana, Katakana, and Kanji, that
assist word segmentation. Moreover, words written
in Katakana are usually Japanese transliterations of
foreign words. This makes it possible for a Web
search engine to effectively retrieve Web pages con-
taining given Japanese transliterations. Like En-
glish, Korean has spaces between words (or word
phrases). As the spaces in the languages reduce am-
biguity in segmenting words, a Web search engine
can correctly identify Web pages containing given
Korean transliterations. In contrast, there is a se-
vere word-segmentation problem with Chinese that
causes Chinese Web search engines to incorrectly
retrieve Web pages, as shown in Fig. 3. For example,
Snippet
1
is not related to ?Aman? but to ?a man?.
238
Snippet
2
contains a super-string of a given Chinese
query, which corresponds to ?Academy? rather than
to ?Agard?, which is the English counterpart of the
Chinese transliteration. Moreover, Web search
engines ignore punctuation marks in Chinese. In
Snippet
3
and Snippet
4
, ?,? and ?? in the under-
lined terms are disregarded, so the Web counts based
on such Web documents are noisy. Thus, noise in
the Chinese Web counts causes systems based on
Web counts to produce more errors than our sys-
tems do. Our proposed method can filter out such
noise because our systems take punctuation marks
and the contexts of transliterations in Web mining
into consideration. Thus, our systems based on fea-
tures mined from the Web were able to achieve the
best performance. The results revealed that our sys-
tems based on the Web-mining technique can effec-
tively be used to select transliteration hypotheses re-
gardless of the language.
6.2 Contribution of Web corpora
ECSet EJSet EKSet
SVM MEM SVM MEM SVM MEM
Base 73.3 73.8 67.0 66.1 66.0 66.4
W
1
81.7 79.7 87.6 87.3 86.1 85.1
W
2
80.8 79.5 86.9 86.0 83.8 82.1
W
3
77.2 76.7 83.0 82.8 79.8 77.3
W
1+2
83.8 82.3 88.5 87.9 86.3 85.9
W
1+3
81.9 80.1 87.6 87.8 86.1 84.7
W
2+3
81.4 79.8 88.0 87.7 85.1 84.3
W
All
83.9 82.0 88.5 88.2 86.7 85.8
Table 6: Contribution of Web corpora
In Web mining, we used W
1
, W
2
, and W
3
, col-
lected by respective queries Q
1
=(S and hi), Q2=S,
and Q
3
=hi. To investigate their contribution, we
tested our proposed method with different combina-
tions of Web corpora. ?Base? is a baseline system
that only uses Pr(hi|S) as features but does not use
features mined from the Web. We added features
mined from different combinations of Web corpora
to ?Base? from W
1
to WAll.
In Table 6, we can see that W
1
, a set of Web pages
retrieved by Q
1
, tends to give more relevant infor-
mation than W
2
and W
3
, because Q
1
can search
more Web pages containing both S and hi in the top-
100 snippets if S and hi are a correct transliteration
pair. Therefore, its performance tends to be superior
in Table 6 if W
1
is used, especially for ECSet. How-
ever, as W
1
occasionally retrieves few snippets, it is
not able to provide sufficient information. Using W
2
or W
3
, we can address the problem. Thus, combina-
tions of W
1
and others (W
1+2
, W
1+3
, WAll) pro-
vided better WA than W
1
.
7 Discussion
Several Web mining techniques for translitera-
tion lexicons have been developed in the last few
years (Jiang et al, 2007; Oh and Isahara, 2006).
The main difference between ours and those previ-
ous ones is in the way a set of transliteration hy-
potheses (or candidates) is created.
Jiang et al (2007) generated Chinese transliter-
ations for given English words and searched the
Web using the transliterations. They generated only
the best transliteration hypothesis and focused on
Web mining to select transliteration lexicons rather
than selecting transliteration hypotheses. The best
transliteration hypothesis was used to guide Web
searches. Then, transliteration candidates were
mined from the retrieved Web pages. Therefore,
their performance greatly depended on their abil-
ity to mine transliteration candidates from the Web.
However, this system might create errors if it can-
not find a correct transliteration candidate from the
retrieved Web pages. Because of this, their sys-
tem?s coverage and WA were relatively poor than
ours 8. However, our transliteration process was able
to generate a set of transliteration hypotheses with
excellent coverage and could thus achieve superior
WA.
Oh and Isahara (2006) searched the Web using
given source words and mined the retrieved Web
pages to find target-language transliteration candi-
dates. They extracted all possible sequences of
target-language characters from the retrieved Web
snippets as transliteration candidates for which the
beginnings and endings of the given source word
8Since both Jiang et al?s (2007) and ours used Chinese
transliterations of personal names as a test set, we can indirectly
compare our coverage and WA with theirs (Jiang et al, 2007).
Jiang et al (2007) achieved a 74.5% coverage of transliteration
candidates and 47.5% WA, while ours achieved a 94.6% cov-
erage of transliteration hypotheses and 82.0?83.9% WA
239
and the extracted transliteration candidate were pho-
netically similar. However, while this can exponen-
tially increase the number of transliteration candi-
dates, ours used the n-best transliteration hypothe-
ses but still achieved excellent coverage.
8 Conclusion
We have described a novel approach to selecting
transliteration hypotheses based on Web mining. We
first generated CJK transliteration hypotheses for a
given English word and retrieved Web pages us-
ing the transliteration hypotheses and the given En-
glish word as queries for a Web search engine. We
then mined features from the retrieved Web pages
and trained machine-learning algorithms using the
mined features. Finally, we selected transliteration
hypotheses by ranking them. Our experiments re-
vealed that our proposed method worked well re-
gardless of the language, while simple Web counts
were not effective, especially for Chinese.
Because our method was very effective in select-
ing transliteration pairs, we expect that it will also
be useful for selecting translation pairs. We plan to
extend our method in future work to selecting trans-
lation pairs.
References
Y. Al-Onaizan and Kevin Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proc. of ACL ?02, pages 400?408.
J. Breen. 2003. EDICT Japanese/English dictionary .le.
The Electronic Dictionary Research and Development
Group, Monash University. http://www.csse.
monash.edu.au/
?
jwb/edict.html.
I. Goto, N. Kato, N. Uratani, and T. Ehara. 2003.
Transliteration considering context information based
on the maximum entropy method. In Proc. of MT-
Summit IX, pages 125?132.
Gregory Grefenstette, Yan Qu, and David A. Evans.
2004. Mining the Web to create a language model
for mapping between English names and phrases and
Japanese. In Proc. of Web Intelligence, pages 110?
116.
Long Jiang, Ming Zhou, Lee-Feng Chien, and Cheng
Niu. 2007. Named entity translation with Web min-
ing and transliteration. In Proc. of IJCAI, pages 1629?
1634.
Thorsten Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines: Methods, Theory and
Algorithms. Kluwer Academic Publishers.
I. H. Kang and G. C. Kim. 2000. English-to-Korean
transliteration using multiple unbounded overlapping
phoneme chunks. In Proc. of COLING ?00, pages
418?424.
Mirella Lapata and Frank Keller. 2005. Web-based
models for natural language processing. ACM Trans.
Speech Lang. Process., 2(1):3.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of ACL
?04, pages 160?167.
H.M. Meng, Wai-Kit Lo, Berlin Chen, and K. Tang.
2001. Generating phonetic cognates to handle named
entities in English-Chinese cross-language spoken
document retrieval. In Proc. of Automatic Speech
Recognition and Understanding, 2001. ASRU ?01,
pages 311?314.
Y. S. Nam. 1997. Foreign dictionary. Sung An Dang.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation and
contextual rules. In Proc. of COLING2002, pages
758?764.
Jong-Hoon Oh and Hitoshi Isahara. 2006. Mining the
Web for transliteration lexicons: Joint-validation ap-
proach. In Web Intelligence, pages 254?261.
Jong-Hoon Oh, Key-Sun Choi, and Hitoshi Isahara.
2006. A comparison of different machine transliter-
ation models. Journal of Artificial Intelligence Re-
search (JAIR), 27:119?151.
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in
Latin script via language identification and corpus val-
idation. In Proc. of ACL ?04, pages 183?190.
Richard Schwartz and Yen-Lu Chow. 1990. The N-best
algorithm: An efficient and exact procedure for finding
the N most likely sentence hypothesis. In Procs. of
ICASSP ?90, pages 81?84.
Xinhua News Agency. 1992. Chinese transliteration of
foreign personal names. The Commercial Press.
L. Zhang. 2004. Maximum entropy model-
ing toolkit for python and C++. http:
//homepages.inf.ed.ac.uk/s0450736/
software/maxent/manual.pdf.
240
Synset Assignment for Bi-lingual Dictionary with Limited Resource 
 
       Virach Sornlertlamvanich  
Thatsanee Charoenporn  
Chumpol Mokarat 
Thai Computational Linguistics Lab.  
NICT Asia Research Center, 
Thailand Science Park,  
Pathumthani, Thailand 
{virach,thatsanee,chumpol}@tcllab.org 
Hitoshi Isahara 
National Institute of Information 
and Communications Technology 
3-5 Hikaridai, Seika-cho, soraku-gaun, 
Kyoto, Japan 619-0289 
isahara@nict.go.jp 
 
Hammam Riza  
IPTEKNET, Agency for the Assess-
ment and Application of Technology,     
Jakarta Pusat 10340, Indonesia  
hammam@iptek.net.id 
 
Purev Jaimai  
Center for Research on Language 
Processing, National University of 
Mongolia, Ulaanbaatar, Mongolia  
purev@num.edu.mn 
 
Abstract 
This paper explores an automatic WordNet 
synset assignment to the bi-lingual diction-
aries of languages having limited lexicon 
information. Generally, a term in a bi-
lingual dictionary is provided with very 
limited information such as part-of-speech, 
a set of synonyms, and a set of English 
equivalents. This type of dictionary is 
comparatively reliable and can be found in 
an electronic form from various publishers. 
In this paper, we propose an algorithm for 
applying a set of criteria to assign a synset 
with an appropriate degree of confidence to 
the existing bi-lingual dictionary. We show 
the efficiency in nominating the synset 
candidate by using the most common lexi-
cal information. The algorithm is evaluated 
against the implementation of Thai-
English, Indonesian-English, and Mongo-
lian-English bi-lingual dictionaries. The 
experiment also shows the effectiveness of 
using the same type of dictionary from dif-
ferent sources.  
1 Introduction 
The Princeton WordNet (PWN) (Fellbaum, 1998) 
is one of the most semantically rich English lexical 
databases that are widely used as a lexical knowl-
edge resource in many research and development 
topics. The database is divided by part of speech 
into noun, verb, adjective and adverb, organized in 
sets of synonyms, called synset, each of which 
represents ?meaning? of the word entry.  
Though WordNet was already used as a starting 
resource for developing many language WordNets, 
the construction of the WordNet for any languages 
can be varied according to the availability of the 
language resources. Some were developed from 
scratch, and some were developed from the combi-
nation of various existing lexical resources. Span-
ish and Catalan WordNets, for instance, are auto-
matically constructed using hyponym relation, 
monolingual dictionary, bilingual dictionary and 
taxonomy (Atserias et al, 1997). Italian WordNet 
(Magnini et al, 1994) is semi-automatically con-
structed from definition in monolingual dictionary, 
bilingual dictionary, and WordNet glosses. Hun-
garian WordNet uses bilingual dictionary, mono-
lingual explanatory dictionary, and Hungarian the-
saurus in the construction (Proszeky et al, 2002), 
etc. 
673
This paper presents a new method particularly to 
facilitate the WordNet construction by using the 
existing resources having only English equivalents 
and the lexical synonyms. Our proposed criteria 
and algorithm for application are evaluated by im-
plementing to Asian languages which occupy quite 
different language phenomena in terms of gram-
mars and word unit. 
To evaluate our criteria and algorithm, we use 
the PWN version 2.1 containing 207,010 senses 
classified into adjective, adverb, verb, and noun. 
The basic building block is a ?synset? which is 
essentially a context-sensitive grouping of syno-
nyms which are linked by various types of relation 
such as hyponym, hypernymy, meronymy, anto-
nym, attributes, and modification. Our approach is 
conducted to assign a synset to a lexical entry by 
considering its English equivalent and lexical 
synonyms. The degree of reliability of the assign-
ment is defined in terms of confidence score (CS) 
based on our assumption of the membership of the 
English equivalent in the synset. A dictionary from 
different source is also a reliable source to increase 
the accuracy of the assignment because it can ful-
fill the thoroughness of the list of English equiva-
lent and the lexical synonyms. 
The rest of this paper is organized as follows: 
Section 2 describes our criteria for synset assign-
ment. Section 3 provides the results of the experi-
ments and error analysis on Thai, Indonesian, and 
Mongolian. Section 4 evaluates the accuracy of the 
assignment result, and the effectiveness of the 
complimentary use of a dictionary from different 
sources. Section 5 shows a collaborative interface 
for revising the result of synset assignment. And 
Section 6 concludes our work. 
2 Synset Assignment 
A set of synonyms determines the meaning of a 
concept. Under the situation of limited resources 
on a language, English equivalent word in a bi-
lingual dictionary is a crucial key to find an 
appropriate synset for the entry word in question. 
The synset assignment criteria described in this 
Section relies on the information of English 
equivalent and synonym of a lexical entry, which 
is most commonly encoded in a bi-lingual 
dictionary. 
Synset Assignment Criteria 
Applying the nature of WordNet which introduces 
a set of synonyms to define the concept, we set up 
four criteria for assigning a synset to a lexical entry. 
The confidence score (CS) is introduced to 
annotate the likelihood of the assignment. The 
highest score, CS=4, is assigned to the synset that 
is evident to include more than one English 
equivalent of the lexical entry in question. On the 
contrary, the lowest score, CS=1, is assigned to 
any synset that occupies only one of the English 
equivalents of the lexical entry in question when 
multiple English equivalents exist. 
The details of assignment criteria are elaborated 
as in the followings. Li denotes the lexical entry, Ej 
denotes the English equivalent, Sk denotes the syn-
set, and ? denotes the member of a set: 
Case 1: Accept the synset that includes more 
than one English equivalent with confidence score 
of 4. 
Figure 1 simulates that a lexical entry L0 has two 
English equivalents of E0 and E1. Both E0 and E1 
are included in a synset of S1. The criterion implies 
that both E0 and E1 are the synset for L0 which can 
be defined by a greater set of synonyms in S1. 
Therefore the relatively high confidence score, 
CS=4, is assigned for this synset to the lexical en-
try. 
 
Figure 1. Synset assignment with SC=4 
Example: 
L0:  
E0: aim  E1: target 
S0: purpose, intent, intention, aim, design 
S1: aim, object, objective, target 
S2: aim 
In the above example, the synset, S1, is assigned 
to the lexical entry, L0, with CS=4. 
Case 2: Accept the synset that includes more 
than one English equivalent of the synonym of the 
lexical entry in question with confidence score of 3.  
In case that Case 1 fails in finding a synset that 
includes more than one English equivalent, the 
English equivalent of a synonym of the lexical en-
try is picked up to investigate. 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
674
 Figure 2. Synset assignment with SC=3 
Figure 2 simulates that an English equivalent of 
a lexical entry L0 and its synonym L1 are included 
in a synset S1. In this case the synset S1 is assigned 
to both L0 and L1 with CS=3. The score in this case 
is lower than the one assigned in Case 1 because 
the synonym of the English equivalent of the lexi-
cal entry is indirectly implied from the English 
equivalent of the synonym of the lexical entry. The 
newly retrieved English equivalent may not be dis-
torted. 
Example: 
L0: 	
  L1: 
 
E0: stare  E1: gaze 
S0: gaze, stare S1: stare 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=3. 
Case 3: Accept the only synset that includes the 
only one English equivalent with confidence score 
of 2. 
 
Figure 3. Synset assignment with SC=2 
Figure 3 simulates the assignment of CS-2 when 
there is only one English equivalent and there is no 
synonym of the lexical entry. Though there is no 
any English equivalent to increase the reliability of 
the assignment, in the same time there is no 
synonym of the lexical entry to distort the relation. 
In this case, the only one English equivalent shows 
it uniqueness in the translation that can maintain a 
degree of the confidence. 
Example: 
L0:           E0: obstetrician     
S0: obstetrician, accoucheur 
In the above example, the synset, S0, is assigned 
to the lexical entry, L0, with CS=2. 
Case 4: Accept more than one synset that in-
cludes each of the English Equivalent with confi-
dence score of 1. 
Case 4 is the most relax rule to provide some re-
lation information between the lexical entry and a 
synset. Figure 4 simulates the assignment of CS=1 
to any relations that do not meet the previous crite-
ria but the synsets that include one of the English 
equivalent of the lexical entry. 
 
Figure 4. Synset assignment with SC=1 
Example: 
L0: 
 
E0: hole  E1: canal 
S0: hole, hollow   
S1: hole, trap, cakehole, maw, yap, gop 
S2: canal, duct, epithelial duct, channel 
In the above example, each synset, S0, S1, and S2 
is assigned to lexical entry L0, with CS=1. 
3 Experiment results 
We applied the synset assignment criteria to a 
Thai-English dictionary (MMT dictionary) (CICC, 
1995) with the synset from WordNet 2.1. To com-
pare the ratio of assignment for Thai-English dic-
tionary, we also investigate the synset assignment 
of Indonesian-English and Mongolian-English dic-
tionaries. 
 WordNet (synset) T-E Dict (entry) 
 total assigned total assigned 
Noun 145,103 18,353 (13%) 43,072
11,867 
(28%)
Verb 24,884 1,333 (5%) 17,669
2,298 
(13%)
Adjective 31,302 4,034 (13%) 18,448
3,722 
(20%)
Adverb 5,721 737 (13%) 3,008
1,519 
(51%)
total 207,010 24,457 (12%) 82,197
19,406 
(24%)
Table 1. Synset assignment to T-E dictionary 
In our experiment, there are only 24,457 synsets 
from 207,010 synsets, which is 12% of the total 
number of the synset that can be assigned to Thai 
lexical entries. Table 1 shows the successful rate in 
assigning synset to Thai-English dictionary. About 
24 % of Thai lexical entries are found with the 
English equivalents that meet one of our criteria.  
Going through the list of unmapped lexical en-
try, we can classify the errors into three groups:- 
1. Compound 
The English equivalent is assigned in a com-
L0 E0 
S0 ?
 
S1 
?
 
E1 
?
 
S2 
?
 
L1 
L0 E0 S0 
?
 
L0 
E0 
S0 ?
 
S1 
?
 
E1 
S2 
?
 
675
pound, especially in case that there is no an 
appropriate translation to represent exactly 
the same sense. For example, 
L: 		Non-Factoid Japanese Question Answering through Passage Retrieval
that Is Weighted Based on Types of Answers
Masaki Murata and Sachiyo Tsukawaki
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{murata,tsuka}@nict.go.jp
Qing Ma
Ryukoku University
Otsu, Shiga, 520-2194, Japan
qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru
Kyoto University
Yoshida-Nihonmatsu-Cho, Sakyo
Kyoto, 606-8501 Japan
kanamaru@hi.h.kyoto-u.ac.jp
Hitoshi Isahara
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We constructed a system for answering non-
factoid Japanese questions. We used var-
ious methods of passage retrieval for the
system. We extracted paragraphs based on
terms from an input question and output
them as the preferred answers. We classified
the non-factoid questions into six categories.
We used a particular method for each cate-
gory. For example, we increased the scores
of paragraphs including the word ?reason?
for questions including the word ?why.? We
participated at NTCIR-6 QAC-4, where our
system obtained the most correct answers
out of all the eight participating teams. The
rate of accuracy was 0.77, which indicates
that our methods were effective.
1 Introduction
A question-answering system is an application de-
signed to produce the correct answer to a question
given as input. For example, when ?What is the
capital of Japan?? is given as input, a question-
answering system may retrieve text containing sen-
tences like ?Tokyo is Japan?s capital and the coun-
try?s largest and most important city?, and ?Tokyo
is also one of Japan?s 47 prefectures?, from Web-
sites, newspaper articles, or encyclopedias. The sys-
tem then outputs ?Tokyo? as the correct answer.
We believe question-answering systems will become
a more convenient alternative to other systems de-
signed for information retrieval and a basic compo-
nent of future artificial intelligence systems. Numer-
ous researchers have recently been attracted to this
important topic. These researchers have produced
many interesting studies on question-answering sys-
tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarke
et al, 2001; Dumis et al, 2002; Magnini et al, 2002;
Moldovan et al, 2003). Evaluation conferences and
contests on question-answering systems have also
been held. In particular, the U.S.A. has held the Text
REtrieval Conferences (TREC) (TREC-10 commit-
tee, 2001), and Japan has hosted the Question-
Answering Challenges (QAC) (National Institute of
Informatics, 2002) at NTCIR (NII Test Collection
for IR Systems ) 3. These conferences and contests
have aimed at improving question-answering sys-
tems. The researchers who participate in these create
question-answering systems that they then use to an-
swer the same questions, and each system?s perfor-
mance is then evaluated to yield possible improve-
ments.
We addressed non-factoid question answering in
NTCIR-6 QAC-4. For example, when the question
was ?Why are people opposed to the Private Infor-
mation Protection Law?? the system retrieved sen-
tences based on terms appearing in the question and
output an answer using the retrieved sentences. Nu-
merous studies have addressed issues that are in-
volved in the answering of non-factoid questions
(Berger et al, 2000; Blair-Goldensohn et al, 2003;
727
Xu et al, 2003; Soricut and Brill, 2004; Han et al,
2005; Morooka and Fukumoto, 2006; Maehara et
al., 2006; Asada, 2006).
We constructed a system for answering non-
factoid Japanese questions for QAC-4. We used
methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions
including the word ?why.? We performed exper-
iments using the NTCIR-6 QAC-4 data collection
and tested the effectiveness of our methods.
2 Categories of Non-Factoid Questions
We used six categories of non-factoid questions in
this study. We constructed the categories by con-
sulting the dry run data in QAC-4.
1. Definition-oriented questions (Questions that
require a definition to be given in response.)
e.g., K-1 to wa nandesuka? (What is K-1?)
2. Reason-oriented questions (Questions that re-
quire a reason to be given in response.)
e.g., kojin jouhou hokogou ni hantai shiteiru
hito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-
mation Protection Law?)
3. Method-oriented questions (Questions that re-
quire an explanation of a method to be given in
response.)
e.g., sekai isan wa donoyouni shite kimeru no
desuka?? (How is a World Heritage Site deter-
mined?)
4. Degree-oriented questions (Questions that re-
quire an explanation of the degree of something
to be given in response.)
5. Change-oriented questions (Questions that re-
quire a description of things that change to be
given in response.)
e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)
6. Detail-oriented questions (Questions that re-
quire a description of the particulars or details
surrounding a sequence of events to be given in
response.)
e.g., donoyouna keii de ryuukyuu oukoku wa ni-
hon no ichibu ni natta no desuka? (How did
Ryukyu come to belong to Japan?)
3 Question-answering Systems in this
Study
The system has three basic components:
1. Prediction of type of answer
The system predicts the answer to be a partic-
ular type of expression based on whether the
input question is indicated by an interrogative
pronoun, an adjective, or an adverb. For exam-
ple, if the input question is ?Why are people
opposed to the Private Information Protection
Law??, the word ?why? suggests that the an-
swer will be an expression that describes a rea-
son.
2. Document retrieval
The system extracts terms from the input ques-
tion and retrieves documents by using these
terms. Documents that are likely to contain
the correct answer are thus gathered during the
retrieval process. For example, for the input
question ?Why are people opposed to the Pri-
vate Information Protection Law??, the system
extracts ?people,? ?opposed,? ?Private,? ?Infor-
mation,? ?Protection,? and ?Law? as terms and
retrieves the appropriate documents based on
these.
3. Answer detection
The system separates the retrieved documents
into paragraphs and retrieves those that contain
terms from the input question and a clue ex-
pression (e.g., ?to wa? (copula sentence) for the
definition sentence). The system outputs the re-
trieved paragraphs as the preferred answer.
3.1 Prediction of type of answer
We used the following rules for predicting the type
of answer. We constructed the rules by consulting
the dry run data in QAC-4.
728
1. Definition-oriented questions Questions in-
cluding expressions such as ?to wa nani,?
?donna,? ?douiu,? ?douitta,? ?nanimono,?
?donoyouna mono,? ?donna mono,? and ?douiu
koto? (which all mean ?what is?) are rec-
ognized by the system as being definition-
oriented questions.
2. Reason-oriented questions Questions including
expressions such as ?naze? (why), ?naniyue?
(why), ?doushite? (why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system as
being reason-oriented questions.
3. Method-oriented questions Questions includ-
ing expressions such as ?dou,? ?dousureba,?
?douyatte,? ?dono youni shite,? ?ikani shite,?
?ikani,? and ?donnna houhou de? (which all
mean ?how?) are recognized by the system as
being method-oriented questions.
4. Degree-oriented questions Questions including
expressions such as ?dorekurai? (how much),
?dorekurai no? (to what extent), and ?dono
teido? (to what extent), are recognized by the
system as being degree-oriented questions.
5. Change-oriented questions Questions includ-
ing expressions such as ?naniga chigau? (What
is different), ?donoyuni kawaru? (How is ...
changed), and ?dokoga kotonaru? (What is dif-
ferent), are recognized by the system as being
change-oriented questions.
6. Detail-oriented questions Questions including
expressions such as ?dono you na keii,? ?dono
you na ikisatsu,? and ?dono you na nariyuki?
(which all mean ?how was?) are recognized by
the system as being detail-oriented questions.
3.2 Document retrieval
Our system extracts terms from a question by using
the morphological analyzer, ChaSen (Matsumoto et
al., 1999). The analyzer first eliminates preposi-
tions, articles, and similar parts of speech. It then
retrieves documents by using the extracted terms.
The documents are retrieved as follows:
We first retrieve the top k
dr1
documents with the
highest scores calculated using the equation
Score(d)
=
?
term t
?
?
?
tf(d, t)
tf(d, t) + kt
length(d) + k
+
? + k
+
? log
N
df(t)
?
?
?
,
(1)
where d is a document, t is a term extracted from
a question, and tf(d, t) is the frequency of t oc-
curring in d. Here, df(t) is the number of docu-
ments in which t appears, N is the total number
of documents, length(d) is the length of d, and ?
is the average length of all documents. Constants
k
t
and k
+
are defined based on experimental re-
sults. We based this equation on Robertson?s equa-
tion (Robertson and Walker, 1994; Robertson et al,
1994). This approach is very effective, and we have
used it extensively for information retrieval (Murata
et al, 2000; Murata et al, 2001; Murata et al, 2002).
The question-answering system uses a large number
for k
t
.
We extracted the top 300 documents and used
them in the next procedure.
3.3 Answer detection
In detecting answers, our system first generates can-
didate expressions for them from the extracted docu-
ments. We use two methods for extracting candidate
expressions. Method 1 uses a paragraph as a candi-
date expression. Method 2 uses a paragraph, two
continuous paragraphs, or three continuous para-
graphs as candidate expressions.
We award each candidate expression the follow-
ing score.
Score(d)
= ?mint1?T log
?
t2?T3
(2dist(t1, t2)df(t2)
N
)
+ 0.00000001 ? length(d)
= maxt1?T
?
t2?T3
log
N
2dist(t1, t2) ? df(t2)
+ 0.00000001 ? length(d)
(2)
729
T3 = {t|t ? T, 2dist(t1, t)df(t)
N
? 1}, (3)
where d is a candidate expression, T is the set of
terms in the question, dist(t1, t2) is the distance
between t1 and t2 (defined as the number of char-
acters between them with dist(t1, t2) = 0.5 when
t1 = t2), and length(d) is the number of charac-
ters in a candidate expression. The numerical term,
0.00000001 ? length(d), is used for increasing the
scores of long paragraphs.
For reason-oriented questions, our system uses
some reason terms such as ?riyuu? (reason),
?gen?in? (cause), and ?nazenara? (because) as terms
for Eq. 2 in addition to terms from the input ques-
tion. This is because we would like to increase the
score of a document that includes reason terms for
reason-oriented questions.
For method-oriented questions, our system uses
some method terms such as ?houhou? (method),
?tejun? (procedure), and ?kotoniyori? (by doing) as
terms for second document retrieval (re-ranking) in
addition to terms from the input question.
For detail-oriented questions, our system uses
some method terms such as ?keii? (a detail, or a se-
quence of events), ?haikei? (background), and ?rek-
ishi? (history) as terms for second document re-
trieval (re-ranking) in addition to terms from the in-
put question.
For degree-oriented questions, when candidate
paragraphs include numerical expressions, the score
(Score(d)) is multiplied by 1.1.
For definition-oriented questions, the system first
extracts focus expressions. When the question in-
cludes expressions such as ?X-wa?, ?X-towa?, ?X-
toiunowa?, and ?X-tte?, X is extracted as a fo-
cus expression. The system multiplies the score,
(Score(d)), of the candidate paragraph having ?X-
wa?, ?X-towa or something by 1.1. When the can-
didate expression includes focus expressions having
modifiers (including modifier clauses and modifier
phrases), the modifiers are used as candidate expres-
sions, and the scores of the candidate expressions are
multiplied by 1.1.
Below is an example of a candidate expression
that is a modifier clause in a sentence.
Table 1: Results
Method Correct A B C D
Method 1 57 18 42 10 89
Method 2 77 5 67 19 90
(There were a total of 100 questions.)
Question sentence: sekai isan jouyaku to
wa dono youna jouyaku desu ka?
(What is the Convention concerning the
Protection of the World Cultural and Nat-
ural Heritage?)
Sentence including answers:
1972 nen no dai 17 kai yunesuko soukai de
saitaku sareta sekai isan jouyaku ....
(Convention concerning the Pro-
tection of the World Cultural
and Natural Heritage, which
was adopted in 1972 in the 17th gen-
eral assembly meeting of the UN Educational,
Scientific and Cultural Organization.)
Finally, our system extracts candidate expressions
having high scores, (Score(d)s), as the preferred
output. Our system extracts candidate expressions
having scores that are no less than the highest score
multiplied by 0.9 as the preferred output.
We constructed the methods for answer detection
by consulting the dry run data in QAC-4.
4 Experiments
The experimental results are listed in Table 1. One
hundred non-factoid questions were used in the ex-
periment. The questions, which were generated by
the QAC-4 organizers, were natural and not gener-
ated by using target documents. The QAC-4 orga-
nizers checked four or fewer outputs for each ques-
tion. Methods 1 and 2 were used to determine what
we used as answer candidate expressions (Method 1
uses one paragraph as a candidate answer. Method
2 uses one paragraph, two paragraphs, or three para-
graphs as candidate answers.).
?A,? ?B,? ?C,? and ?D? are the evaluation criteria.
?A? indicates output that describes the same content
as that in the answer. Even if there is a supplemen-
tary expression in the output, which does not change
730
the content, the output is judged to be ?A.? ?B? in-
dicates output that contains some content similar to
that in the answer but contains different overall con-
tent. ?C? indicates output that contains part of the
same content as that in the answer. ?D? indicates
output does not contain any of the same content as
that in the answer. The numbers for ?A,? ?B,? ?C,?
and ?D? in Table 1 indicate the number of questions
where an output belongs to ?A,? ?B,? ?C,? and ?D?.
?Correct? indicates the number of questions where
an output belongs to ?A,? ?B,? or ?C?. The evalu-
ation criteria ?Correct? was also used officially at
NTCIR-6 QAC-4.
We found the following.
? Method 1 obtained higher scores in evaluation
A than Method 2. This indicates that Method 1
can extract a completely relevant answer more
accurately than Method 2.
? Method 2 obtained higher scores in evaluation
?Correct? than Method 1. The rate of accuracy
for Method 2 was 0.77 according to evaluation
?Correct?. This indicates that Method 2 can ex-
tract more partly relevant answers than Method
1. When we want to extract completely relevant
answers, we should use Method 1. When we
want to extract more answers, including partly
relevant answers, we should use Method 2.
? Method 2 was the most accurate (0.77) of those
used by all eight participating teams. We could
detect paragraphs as answers including input
terms and the key terms related to answer types
based the methods discussed in Section 3.3.
Our system obtained the best results because
our method of detecting answers was the most
effective.
Below is an example of the output of Method 1,
which was judged to be ?A.?
Question sentence:
jusei ran shindan wa douiu baai ni okon-
awareru noka?
(When is amniocentesis performed on a
pregnant woman?)
System output:
omoi idenbyou no kodono ga umareru no
wo fusegu.
(To prevent the birth of children with seri-
ous genetic disorders )
Examples of answers given by organizers:
omoi idenbyou
(A serious genetic disorder)
omoi idenbyou no kodomo ga umareru
kanousei ga takai baai
(To prevent the birth of children with seri-
ous genetic disorders.)
5 Conclusion
We constructed a system for answering non-factoid
Japanese questions. An example of a non-factoid
question is ?Why are people opposed to the Pri-
vate Information Protection Law?? We used vari-
ous methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions in-
cluding the word ?why.? We participated at NTCIR-
6 QAC-4, where our system obtained the most cor-
rect answers out of all the eight participating teams.
The rate of accuracy was 0.77, which indicates that
our methods were effective.
We would like to apply our method and system to
Web data in the future. We would like to construct a
sophisticated system that can answer many kinds of
complicated queries such as non-factoid questions
based on a large amount of Web data.
Acknowledgements
We are grateful to all the organizers of NTCIR-6
who gave us the chance to participate in their con-
test to evaluate and improve our question-answering
system. We greatly appreciate the kindness of all
those who helped us.
731
References
Yoshiaki Asada. 2006. Processing of definition type
questions in a question answering system. Master?s
thesis, Yokohama National University. (in Japanese).
AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR-2000), pages 192?199.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and
Andrew Hazen Schlaikjer. 2003. A hybrid approach
for qa track definitional questions. In Proceedings
of the 12th Text Retrieval Conference (TREC-2003),
pages 185?192.
Charles L. A. Clarke, Gordon V. Cormack, and
Thomas R. Lynam. 2001. Exploiting redundancy
in question answering. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, and
Andrew Ng. 2002. Web question answering: Is more
always better? In Proceedings of the 25th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, and
Hae-Chang Rim. 2005. Phrase-based definitional
question answering using definition terminology. In
Lecture Notes in Computer Science 3689, pages 246?
259.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. IBM?s Statistical Ques-
tion Answering System. In TREC-9 Proceedings.
Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line ency-
clopedia. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Hideyuki Maehara, Jun?ichi Fukumoto, and Noriko
Kando. 2006. A BE-based automated evaluation
for question-answering system. IEICE-WGNLC2005-
109, pages 19?24. (in Japanese).
Bernardo Magnini, Matto Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? Exploiting
web redundancy for answer validation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems,
21(2):133?154.
Kokoro Morooka and Jun?ichi Fukumoto. 2006. Answer
extraction method for why-type question answering
system. IEICE-WGNLC2005-107, pages 7?12. (in
Japanese).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara. 2000.
Japanese probabilistic information retrieval using lo-
cation and category information. The Fifth Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 81?88.
Masaki Murata, Masao Utiyama, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2001. CRL at NTCIR2.
Proceedings of the Second NTCIR Workshop Meeting
on Evaluation of Chinese & Japanese Text Retrieval
and Text Summarization, pages 5?21?5?31.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002.
High performance information retrieval using many
characteristics and many techniques. Proceedings of
the Third NTCIR Workshop (CLIR).
National Institute of Informatics. 2002. Proceedings of
the Third NTCIR Workshop (QAC).
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: Beyond the factoid. In In Proceedings
of the Human Language Technology and Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-2004), pages
57?64.
TREC-10 committee. 2001. The tenth text retrieval con-
ference. http://trec.nist.gov/pubs/trec10/t10 proceed-
ings.html.
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC 2003 QA at BBN: answering definitional ques-
tions. In Proceedings of the 12th Text Retrieval Con-
ference (TREC-2003), pages 98?106.
732
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 13?18,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
KUI: an ubiquitous tool for collective intelligence development 
Thatsanee Charoenporn, Virach Sornlertlamvanich 
and Kergrit Robkop 
Thai Computational Linguistics Laboratory 
NICT Asia Research Center, Thailand 
{virach,thatsanee,kergrit}@tcllab.org 
Hitoshi Isahara 
National Institute for 
Communications Tech-
nology (NICT), Japan 
ishara@nict.go.jp
 
 
Abstract 
Collective intelligence is the capability for 
a group of people to collaborate in order to 
achieve goals in a complex context than its 
individual member. This common concept 
increases topic of interest in many sciences 
including computer science where com-
puters are bring about as group support 
elements. This paper presents a new plat-
form, called Knowledge Unifying Initiator 
(KUI) for knowledge development which 
enables connection and collaboration 
among individual intelligence in order to 
accomplish a complex mission. KUI is a 
platform to unify the various thoughts fol-
lowing the process of thinking, i.e., initiat-
ing the topic of interest, collecting the 
opinions to the selected topics, localizing 
the opinions through the translation or cus-
tomization and posting for public hearing 
to conceptualize the knowledge. The proc-
ess of thinking is done under the selectional 
preference simulated by voting mechanism 
in case that many alternatives occur. By 
measuring the history of participation of 
each member, KUI adaptively manages the 
reliability of each member?s opinion and 
vote according to the estimated Ex-
pertScore. 
1 Introduction 
The Internet is a must for forming an online com-
munity in the present day. Many tools have been 
developed to support such an online community 
work. For instance, SourceForge.net (http://www. 
sourceforge.net) facilitates project based Open 
Source software development. Open Source soft-
ware developers deploy SourceForge.net to an-
nounce their initiation, to call for participation, to 
distribute their works and to receive feedbacks. 
SourceForge.net is said to be the largest Open 
Source software development community. 
Wiki.org (http://www.wiki.org) facilitates a data-
base for creating and editing Web page content. It 
keeps the history of the online editing works which 
allows multiple authoring. Wiki is especially de-
rived for several online collaborative works such 
as wikipedia, wikitionary, wikibooks, etc. In addi-
tion, PhpWiki is one of the derived works of wiki 
as a handy software tool for managing the organ-
izational documentation. This collaborative work-
ing environment has changed our working style to 
a more efficient manner. In the same time, the 
flood of information under the open collaborative 
works is now challenging us for an efficient man-
agement system. The disorder of the information 
causes difficulties in the requirement of the sys-
tematic maintenance for retrieval, extraction, or 
even summarization from the stored information. 
To understand the intention of an article (or a solu-
tion), we not only rely on the trace or the history of 
editing, but we also constantly recall the back-
ground of our decision in producing the article (or 
the solution). 
Why don't we organize the information in the 
development process beforehand rather than limit-
ing our capability in making use of the un-
structured information? Google (http://www. 
google.com) successfully responds our needs in 
looking for documents from the WWW. However, 
the results from the search can simply over a mil-
lion sites and just some tens out of which are 
13
viewed for the search. This most powerful search-
ing tool does not digest the information to meet 
final our requirement. It only thoroughly shows the 
results of the related document. 
Back to the principle of collective intelligent 
(Smith, 1994; Johnson et al, 1998; Levy, 1997) in 
which ?two minds are better than one?, mountains 
of knowledge are contributed by this internet 
community. But the most intelligence is the intelli-
gence of knowledge connections in which new 
technologies can take part in helping individuals to 
think and develop their concept collectively. 
We proposed and developed KUI (Knowledge 
Unifying Initiator) (KUI, 2006; Sornlertlamvanich, 
2006) to be a Knowledge User Interface (KUI) for 
online collaborative work to help community to 
think and to develop things together. KUI is a plat-
form to unify the various thoughts following the 
process of thinking, i.e., initiating the topic of in-
terest, collecting the opinions to the selected top-
ics, localizing the opinions through the translation 
or customization and finally posting for public 
hearing to conceptualize the knowledge. The proc-
ess of thinking is done under the selectional prefer-
ence simulated by voting mechanism in case that 
many alternatives occur. 
2 Collaborative tool for managing collec-
tive intelligence 
We developed KUI (Knowledge Unifying Initia-
tor) for being a knowledge development supporting 
tool of a web community. Actually, KUI is a plat-
form to unify various thoughts created by follow-
ing process of thinking, i.e., (1) new task, to allow 
a participant to initiate a task, (2) opinion, to allow 
a participant to post his own opinion, (3) localiza-
tion, to allow a participant to bring in a new 
knowledge into the community by translation, and 
(4) public-hearing, to allow a participant to post a 
draft of concept for conceptualizing the knowl-
edge. The process of thinking is done under the 
selectional preference simulated by voting mecha-
nism in case that many alternatives occur. 
In this section, we describe the concept behind 
KUI, the knowledge development process, and the 
features in KUI. 
2.1 What is KUI? 
KUI or Knowledge Unifying Initiator is a GUI for 
knowledge engineering, in other words Knowledge 
User Interface (KUI). It provides a web interface 
accessible for pre-registered members only for the 
accountability reason. An online registration is of-
fered to manage the account by profiling the login 
participant in making contribution to the commu-
nity. A contributor can comfortably move around 
in the virtual space from desk to desk to participate 
in a particular task. A login member will be as-
signed to a desk when a participation task is de-
fined. Members can then participate in the chat 
group of the same desk. A desk functions as a 
meeting place for collaborative work that needs 
some discussion through the chat function, or al-
low a contributor to work individually by using the 
message slot to record each own opinion. The 
working space can be expanded by closing the un-
necessary frames so that the contributor can con-
centrate on a particular task. All working topics 
can also be statistically viewed through the pro-
vided tabs. These tabs help contributors to under-
stand KUI in the aspects of the current status of 
contribution and the available tasks. A web com-
munity can be formed to create a domain specific 
knowledge efficiently through the features pro-
vided by KUI. These KUI features fulfill the proc-
ess of human thought to record the knowledge. 
In addition, KUI also provides a KUI look up 
function for viewing the composed knowledge. It 
is equipped with a powerful search and statistical 
browse in many aspects. Moreover, the chat log is 
provided to learn about the intention of the knowl-
edge composers. We frequently want to know 
about the background of the solution for better un-
derstanding or to remind us about the decision, but 
we cannot find one. To avoid the repetition of a 
mistake, we systematically provide the chat log to 
keep the trace of discussion or the comments to 
show the intention of knowledge composers. 
2.2 Knowledge Development in KUI 
Adopting the concept of Open Source software 
development, we will be possibly able to develop a 
framework for domain specific knowledge devel-
opment under the web community environment. 
Sharing and collaboration are the considerable fea-
tures of the framework. The knowledge will be 
finally shared among the communities by receiving 
the consensus from the participants in each step. 
To facilitate the knowledge development, the proc-
ess is deliberated into four steps (Sornlertlam-
vanich, 2006). 
14
New Task 
A new task (Topic of interest) can be posted to 
draw intention from participants. The only selected 
tasks by a major vote will then be proceed for fur-
ther discussion in the requested type of task i.e., 
Opinion Poll, Localization or Public-Hearing. 
 
 
 
   Figure 1. Process of knowledge development 
 
Opinion Poll 
The selected task is posted to call for opinions 
from the participants in this step. Opinion poll is 
conducted to get the population of each opinion. 
The result of the opinion poll provides the variety 
of opinions that reflects the current thought of the 
communities together with the consensus to the 
opinions. 
 
Localization 
Translation is a straightforward implementation of 
the localization. Collaborative translation helps 
producing the knowledge in multiple languages in 
the most efficient way. Multi-lingual texts are gen-
erated in this type of task. 
 
Public-Hearing 
The result of discussion will be revised and con-
firmed by gathering the opinions to develop the 
final draft of the proposal. Suggestions for revision 
are ranked according to the vote. The author may 
consider the weight of suggestion to make decision 
on the final revision. 
The developed knowledge is started from post-
ing 'New Task', participants express their supports 
by casting a vote. Upon a threshold the    'New 
Task' is selected for conducting a poll on 'Opinion', 
or introducing to the community by  'Localization', 
or posting a draft for 'Public-Hearing' to gather 
feedbacks from the community. The transition 
from 'Opinion' to either 'Localization' or 'Public-
Hearing' occurs when the 'Opinion' has a concrete 
view for implementation. The discussion in 'Local-
ization' and 'Public-Hearing' is however inter-
changeable due to purpose of implementation 
whether to adopt the knowledge to the local com-
munity or to get feedbacks from the community. 
The knowledge creating is managed in 4 differ-
ent categories corresponding to the stage of knowl-
edge. Each individual in the community casts a 
vote to rank the appropriateness of solutions at 
each category. The community can then form the 
community knowledge under the 'Selectional Pref-
erence' background. 
Topic 
     of  
Interest 
Localization 
Opinion 
Public Hear-
ing 
2.3 Features in KUI 
These KUI features fulfill the process of hu-
man thought to record the knowledge. 
 
Poll-based Opinion or Public-Hearing 
A contributor may choose to work individually by 
posting an opinion e.g. localization, suggestion 
etc., or join a discussion desk to conduct 'Public-
Hearing' with others on the selected topic. The dis-
cussion can be conducted via the provided 'Chat' 
frame before concluding an opinion. Any opinions 
or suggestions are committed to voting. Opinions 
can be different but majority votes will cast the 
belief of the community. These features naturally 
realize the online collaborative works to create the 
knowledge. 
 
Individual or Group Work 
Thought may be formed individually or though a 
concentrated discussion. KUI facilitates a window 
for submitting an opinion and another window for 
submitting a chat message. Each suggestion can be 
cast through the 'Opinion' window marked with a 
degree of its confidence. By working individually, 
comments to a suggestion can be posted to mark its 
background to make it more understanding. On the 
other hand, when working as a group, discussions 
among the group participants will be recorded. The 
discussion can be resumed at any points to avoid 
the iterating words. 
 
Record of Intention 
The intention of each opinion can be reminded by 
the recorded comments or the trace of discussions. 
Frequently, we have to discuss again and again on 
the result that we have already agreed. Misinterpre-
15
tation of the previous decision is also frequently 
faced when we do not record the background of 
decision. Record of intention is therefore necessary 
in the process of knowledge creation. The knowl-
edge interpretation also refers to the record of in-
tention to obtain a better understanding. 
 
Selectional Preference 
Opinions can be differed from person to person 
depending on the aspects of the problem. It is not 
always necessary to say what is right or what is 
wrong. Each opinion should be treated as a result 
of intelligent activity. However, the majority ac-
cepted opinions are preferred at the moment. Ex-
periences could tell the preference via vote casting. 
The dynamically vote ranking will tell the selec-
tional preference of the community at each mo-
ment 
3 KUI for Collective Intelligent Develop-
ment 
Related to the principle of KUI and its features, 
KUI provide many collaborative tools or applica-
tion as followings. 
  
Translating 
Translating is a type of text for language expert 
group contribution. Since the existing knowledge 
in one language is invaluable to other language 
communities. Translating such knowledge will 
help bridging the different language communities. 
It will also bring the individual to an unlimited in-
formation space beyond the language barrier. Con-
tribution in term and phrase translation is to create 
a multi-lingual terminology and an aligned multi-
lingual corpus.  
KUI-Translating Room facilitates an individual 
to view either the current translation tasks in the 
task list or the discussion forum of each translating 
task. Online lookup is also provided to consult a 
term translation.  
Individual participated in KUI-Translating can 
cast a vote for a new task, a vote for multiple tasks 
is allowed, select a topic to participate in the dis-
cussion forum, translate the existing terms into 
your own language, chat with your friends to find 
the best translation, cast a vote to your favorite 
translation, invite assistants to your own initiated 
private task, and propose a new task for commu-
nity voting as well.  
Polling 
Opinion Poll is conducted for getting the popula-
tion of each opinion. The result of the opinion poll 
shows the variety of opinions that reflects the cur-
rent thought of the communities together with the 
consensus to the opinions.  
Similar to KUI-Translating, an individual can 
view the current polling task in the task list as well 
as the discussion forum of each polling task via 
KUI-Polling. And current result of polling can be 
view via online lookup function.    
 
Public-Hearing 
Public Hearing is a way to make a complete docu-
ment from the draft. The result from discussion 
will be received and confirmed by gathering the 
opinions to reflect in the final document. Voting of 
the opinion will help the author to select the ap-
propriate opinion of the community. 
An individual can view the current public hear-
ing tasks in the task list as well as the discussion 
forum of each public hearing task via KUI-Polling. 
And current result of polling can be view via 
online lookup function.    
 
 
Figure 2. KUI-Translating page 
 
Writing 
Writing your document online will keep your 
document in access anywhere and anytime. Indi-
vidual does not have to carry all the documents 
with him/her. Only online, one can work on it. 
Sharing the editing online will also support the 
collaborative work.  
16
With KUI-Writing, individual can create or im-
port a new document, edit the existing document, 
chat with friends about the document, and save or 
export the document.  
 
Correspondent to other collaborative tools, all of 
KUI-application provides function to cast a vote 
for either a new task or multiple tasks. Individual 
can select a topic to participate or post new topic, 
chat with others, invite assistants to his/her own 
initiated task, and so on. 
 
The majority vote will select the best solution 
for the collaborative task. 
4 ExpertScore 
KUI heavily depends on members? voting score to 
produce a reliable result. Therefore, we introduce 
an adjustable voting score to realize a self-
organizing system. Each member is initially pro-
vided a default value of voting score equals to one. 
The voting score is increased according to Ex-
pertScore which is estimated by the value of Ex-
pertise, Contribution, and Continuity of the par-
ticipation history of each member. Expertise is a 
composite score of the accuracy of opinion and 
vote, as shown in Equation 1. Contribution is a 
composite score of the ratio of opinion and vote 
posting comparing to the total, as shown in Equa-
tion 2. Continuity is a regressive function based on 
the assumption that the absence of participation of 
a member will gradually decrease its ExpertScore 
to one after a year (365 days) of the absence, as 
shown in Equation 3. 
 
)3(
365
1
)2(
)(
)(
)(
)(
)1(
)(
)(
)(
)(
4
???
???
??=
+=
+=
???????????????
?
??
D
Continuity
TotalVotecount
Votecount
onTotalOpinicount
Opinioncount
onContributi
Votecount
BestVotecount
Opinioncount
nBestOpiniocount
Expertise
??
??
  
 
Where, 
1=+++ ????  
D is number of recent absent date 
 
As a result, the ExpertScore can be estimated by 
Equation 4. 
 
???
???
?
???
???
?
++
+
?
???
?
???
? ??
???
??=
)(
)(
)(
)(
)(
)(
)(
)(
365
1
4
TotalVotecount
Votecount
onTotalOpinicount
Opinioncount
Votecount
BestVotecount
Opinioncount
nBestOpiniocount
D
eExpertScor
??
??
                          
???(4) 
 
The value of ExperScore is ranged between 1 to 
365 according to the accuracy and the rate of con-
tribution of each member. This means that reliable 
members are rewarded better score for each vote. 
However, the expertise of the member is decreased 
according to the continuity of the participation. By 
means of the ExpertScore, we can rank the opin-
ions precisely and yield reliable results, especially 
for the results produced by an online community. 
 
 
Figure 3. KUI-Polling page 
5 Application Show Case 
KUI for Collaborative Translation Task 
In this collaborative text translation, individual 
participants of different mother language work 
online as a virtual group by using KUI. There are 
several translation task required the collaborative 
translation such as Asian WordNet (originally from 
WordNet (Miller, 1995; http://wordnet. prince-
ton.edu/), Medical Translation, OSS Glossary and 
so on. And some are ready for individual use for 
example NICT?s Japanese ? English News Articles 
Alignment, Open Office Glossary, Swadesh List, 
Technical Term Dictionary. 
The volunteer participants are to translate the 
English text into their native languages, by using 
KUI. They act as a virtual group and participate in 
the translation via this web interface. With differ-
ent backgrounds and degrees of translation abili-
ties, they, therefore, can discuss or exchange their 
opinion while translating each utterance. The 
17
communication is not only for getting to know 
each other, but also for better understanding of the 
utterance before translation. Figure 4 shows the 
participation work flow. 
 
 
                Figure 4. Participant work flow 
 
 
 
Figure 5. Lookup page of Asian WordNet 
 
 
6 Conclusion 
We proposed an efficient online collaborative 
framework in producing and maintaining knowl-
edge according to the principle of collective intel-
ligent. KUI was designed to support an open web 
community by introducing a voting system and a 
mechanism to realize the function of selectional 
preference. It was efficiently introduced to encour-
age the communication among individuals from 
different background. KUI was also proved to sup-
port the collaborative work in producing many 
kinds of tasks. The translated text, an example, will 
be voluntarily maintained by the online partici-
pants under the selectional preference based on the 
voting function. Correspondent to collective intel-
ligent collaborative tool, KUI enables to connect 
and collaborate among individual intelligence in 
order to accomplish a complex mission. Of course, 
?two minds are better than one?.  
 
Acknowledgment 
 
Thanks to KUI community for the invaluable con-
tribution to this project. 
References 
http://www.google.com 
http://www.sourceforge.net 
http://www.wiki.org 
 
N. Johnson, S. Rasmussen, C. Joslyn, L. Rocha, S. 
Smith and M. Kantor. Symbiotic Intelligence: 
Self-organizing Knowledge on Distributed Net-
works Driven by Human Interaction, Int. Con-
ference on Artificial Life, Boston. 1998. 
 
KUI. http://www.tcllab.org/kui/ (2006) 
 
Levy. Collective Intelligence: Mankind?s Emerg-
ing World in Cyberspace, New York, 1997. 
 
G. A. Miller. WordNet: A Lexical Databases for 
English. Communications of the ACM, 39-41, 
November, 1995 
 
J.B. Smith.  Collective Intelligence in Computer-
Based Collaboration. Erlbaum, New York, 
1994. 
V. Sornlertlamvanich. KUI: The OSS-Styled 
Knowledge Development System. Handbook of 
The 7th AOSS Symposium, Kuala Lumpur, Ma-
laysia, 2006. 
 
WordNet. http://wordnet.princeton.edu/ 
18
Enhanced Tools for Online Collaborative Language Resource  
Development 
Virach Sornlertlamvanich  
Thatsanee Charoenporn  
Suphanut Thayaboon 
Chumpol Mokarat 
Thai Computational Linguistics Lab.  
NICT Asia Research Center, 
Pathumthani, Thailand 
{virach, thatsanee, suphanut,  
chumpol}@tcllab.org 
Hitoshi Isahara 
National Institute of Information 
and Communications Technology 
3-5 Hikaridai, Seika-cho, soraku-gaun, 
Kyoto, Japan 619-0289 
isahara@nict.go.jp 
 
Abstract 
This paper reports our recent work of tool 
development for language resource con-
struction. To make a revision of Asian 
WordNet which is automatically generated 
by using the existing English translation 
dictionary, we propose an online collabora-
tive tool which can organize multiple trans-
lations. To support the work of syntactic 
dependency tree annotation, we develop an 
editing suite which integrates the utilities 
for word segmentation, POS tagging and 
dependency tree into a sequence of editing. 
1 Introduction 
Though WordNet was already used as a starting 
resource for developing many language WordNets, 
the constructions of the WordNet for languages 
can be varied according to the availability of the 
language resources. Some were developed from 
scratch, and some were developed from the combi-
nation of various existing lexical resources. 
This paper presents an online collaborative tool 
particularly to facilitate the construction of the 
Asian WordNet which is automatically generated 
by using the existing resources having only Eng-
lish equivalents and the lexical synonyms. 
In addition, to support the work of syntactic de-
pendency tree annotation, we develop an editing 
suite which integrates the utilities for word seg-
mentation, POS tagging and dependency tree. The 
tool is organized in 4 steps, namely, sentence se-
lection, word segmentation, POS tagging, and syn-
tactic dependency tree annotation. 
The rest of this paper is organized as follows: 
Section 2 describes the collaborative interface for 
revising the result of synset translation. Section 3 
describes the tool for annotating Thai syntactic 
dependency tree corpus. And, Section 4 concludes 
our work. 
2 Collaborative Tools for Asian WordNet 
Construction 
There are some efforts in developing Wordnets for 
some of Asian languages, e.g. Chinese, Japanese, 
Korean, and Hindi. The number of languages that 
have been successfully developed their Wordnets 
is still limited to some active research in the area. 
However, the extensive development of Wordnet 
in other languages is of the efforts to support the 
NLP research and implementation. It is not only to 
facilitate the implementation of NLP applications 
for the language, but also provide an inter-linkage 
among the Wordnets for different languages to de-
velop multi-lingual applications. 
We adopt the proposed criteria for automatic 
synset assignment for Asian languages which has 
limited language resources. Based on the result 
from the above synset assignment algorithm, we 
introduce KUI (Knowledge Unifying Initiator), 
(Sornlertlamvanich et al, 2007) to establish an 
online collaborative work in refining the WorNets. 
KUI is a community software tool which allows 
registered members including language experts to 
revise and vote for the synset assignment. The sys-
tem manages the synset assignment according to 
the preferred score obtained from the revision 
process. As a result, the community-based Word-
Nets will be accomplished and exported into the 
original form of WordNet database. Via the synset 
The 6th Workshop on Asian Languae Resources, 2008
105
ID assigned in the WordNet, the system can gener-
ate a cross language WordNet. Through this effort, 
a translated version of Asian WordNet can be es-
tablished. 
Table 1 shows a record of WordNet displayed 
for translation in KUI interface. English entry to-
gether with its part-of-speech, synset, and gloss are 
provided if exists. The members will examine the 
assigned lexical entry and decide whether to vote 
for it or propose a new translation. 
Car 
[Options] 
POS : NOUN 
Synset : auto, automobile, machine, motorcar 
Gloss : a motor vehicle with four wheels; usually propelled 
by an internal combustion engine; 
Table 1. A record for a synset 
 
Figure 1. KUI interface (www.tcllab.org/kui) 
Figure 1 illustrates the translation page of KUI 
In the working area, the login member can partici-
pate in proposing a new translation or voting for 
the preferred translation to revise the synset as-
signment. Statistics of the progress as well as many 
useful functions such as item search, chat, and list 
of online participants are also provided to under-
stand the progress of work and to work online with 
other members. 
3 Tool for Constructing a Syntactic De-
pendency Tree Annotated Corpus 
The tool is organized in 4 steps, namely, sentence 
selection, word segmentation, POS tagging, and 
syntactic dependency tree annotation, shown in 
Figure 2. Sentence segmentation is yet another 
crucial issue for the Thai language. We, however, 
will not discuss about the issue in this work. The 
input is already a list of sentences provided for an-
notator to select. 
 
Figure 2. Syntactic Dependency Tree Annotation 
3.1 POS Annotation 
The result from the automatic word segmentation 
and POS tagging program is generated with alter-
native POSs for revision. A dropdown list of POSs 
is provided for annotator to correct the POS. Since 
word segmentation is processed together with POS 
tagging, the annotator is also provided a GUI to 
merge or to divide the proposed word unit. 
 
3.2 Syntactic Dependency Tree Annotation 
The result from POS annotation in Section 3.1 is 
passed to define the syntactic dependency between 
words. The dependency is assigned to form a 
phrase and a sentence respectively. The final out-
put will be marked in the XML manner and shown 
as a tree for confirmation. 
 
4 Conclusion 
Our current work on the web-based collaborative 
tool for Asian WordNet construction and tool for 
Syntactic Dependency Tree Annotation are devel-
oped as an open platform for online contribution. 
A user-friendly interface and self-organizing utili-
ties are intentionally prepared to support the online 
collaborative work. 
References 
Virach Sornlertlamvanich, Thatsanee Charoenporn, 
Kergit Robkop, and Hitoshi Isahara. 2007. Collabo-
rative Platform for Multilingual Resource Develop-
ment and Intercultural Communication, IWIC2007, 
Springer, LNCS4568:91-102.
The 6th Workshop on Asian Languae Resources, 2008
106
Proceedings of NAACL HLT 2007, pages 33?40,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Evaluation of Machine Translation Based on Rate of
Accomplishment of Sub-goals
Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan
{uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp
Abstract
The quality of a sentence translated by a
machine translation (MT) system is dif-
ficult to evaluate. We propose a method
for automatically evaluating the quality
of each translation. In general, when
translating a given sentence, one or more
conditions should be satisfied to maintain
a high translation quality. In English-
Japanese translation, for example, prepo-
sitions and infinitives must be appropri-
ately translated. We show several proce-
dures that enable evaluating the quality of
a translated sentence more appropriately
than using conventional methods. The
first procedure is constructing a test set
where the conditions are assigned to each
test-set sentence in the form of yes/no
questions. The second procedure is devel-
oping a system that determines an answer
to each question. The third procedure is
combining a measure based on the ques-
tions and conventional measures. We also
present a method for automatically gener-
ating sub-goals in the form of yes/no ques-
tions and estimating the rate of accom-
plishment of the sub-goals. Promising re-
sults are shown.
1 Introduction
In machine translation (MT) research, appropriately
evaluating the quality of MT results is an important
issue. In recent years, many researchers have tried
to automatically evaluate the quality of MT and im-
prove the performance of automatic MT evaluations
(Niessen et al, 2000; Akiba et al, 2001; Papineni et
al., 2002; NIST, 2002; Leusch et al, 2003; Turian et
al., 2003; Babych and Hartley, 2004; Lin and Och,
2004; Banerjee and Lavie, 2005; Gimen?ez et al,
2005) because improving the performance of auto-
matic MT evaluation is expected to enable us to use
and improve MT systems efficiently. For example,
Och reported that the quality of MT results was im-
proved by using automatic MT evaluation measures
for the parameter tuning of an MT system (Och,
2003). This report shows that the quality of MT re-
sults improves as the performance of automatic MT
evaluation improves.
MT systems can be ranked if a set of MT re-
sults for each system and their reference translations
are given. Usually, about 300 or more sentences
are used to automatically rank MT systems (Koehn,
2004). However, the quality of a sentence translated
by an MT system is difficult to evaluate. For exam-
ple, the results of five MTs into Japanese of the sen-
tence ?The percentage of stomach cancer among the
workers appears to be the highest for any asbestos
workers.? are shown in Table 1. A conventional au-
tomatic evaluation method ranks the fifth MT result
first although its human subjective evaluation is the
lowest. This is because conventional methods are
based on the similarity between a translated sentence
and its reference translation, and they give the trans-
lated sentence a high score when the two sentences
are globally similar to each other in terms of lexical
overlap. However, in the case of the above example,
33
Table 1: Examples of conventional automatic evaluations.
Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos work-
ers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda .
System MT results BLEU NIST Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
0.2111 2.1328 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
0.2572 2.1234 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no tame
ni mo mottomo takai youni mie masu
0 1.8094 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni wa
mottomo takaku mie masu .
0 1.5902 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo mot-
tomo takai youni mieru .
0.2692 2.2640 1 2
the most important thing to maintain a high trans-
lation quality is to correctly translate ?for? into the
target language, and it would be difficult to detect
the importance just by comparing an MT result and
its reference translations even if the number of ref-
erence translations is increased.
In general, when translating a given sentence, one
or more conditions should be satisfied to maintain a
high translation quality. In this paper, we show that
constructing a test set where the conditions that are
mainly established from a linguistic point of view
are assigned to each test-set sentence in the form
of yes/no questions, developing a system that de-
termines an answer to each question, and combin-
ing a measure based on the questions and conven-
tional measures enable the evaluation of the quality
of a translated sentence more appropriately than us-
ing conventional methods. We also present a method
for automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals.
2 Test Set for Evaluating Machine
Translation Quality
2.1 Test Set
Two main types of data are used for evaluating MT
quality. One type of data is constructed by arbi-
trarily collecting sentence pairs in the source- and
target-languages, and the other is constructed by in-
tensively collecting sentence pairs that include lin-
guistic phenomena that are difficult to automatically
translate. Recently, MT evaluation campaigns such
as the International Workshop on Spoken Language
Translation 1, NISTMachine Translation Evaluation
2
, and HTRDP Evaluation 3 were organized to sup-
port the improvement of MT techniques. The data
used in the evaluation campaigns were arbitrarily
collected from newspaper articles or travel conver-
sation data for fair evaluation. They are classified
as the former type of data mentioned above. On the
other hand, the data provided by NTT (Ikehara et al,
1994) and that constructed by JEIDA (Isahara, 1995)
are classified as the latter type. Almost all the data
mentioned above consist of only parallel translations
in two languages. Data with information for evaluat-
ing MT results, such as JEIDA?s are rarely found. In
this paper, we call data that consist of parallel trans-
lations collected for MT evaluation and that the in-
formation for MT evaluation is assigned to, a test
set.
The most characteristic information assigned to
the JEIDA test set is the yes/no question for assess-
ing the translation results. For example, a yes/no
question such as ?Is ?for? translated into an expres-
sion representing a cause/reason such as ?de??? (in
Japanese) is assigned to a test-set sentence. We can
evaluate MT results objectively by answering the
question. An example of a test-set sample consist-
ing of an ID, a source-language sample sentence, its
reference translation, and a question is as follows.
1http://www.slt.atr.jp/IWSLT2006/
2http://www.nist.gov/speech/tests/mt/index.htm
3http://www.863data.org.cn/
34
ID 1.1.7.1.3-1
Sample sen-
tence
The percentage of stomach can-
cer among the workers appears
to be the highest for any asbestos
workers.
Reference
translation
(in Japanese)
roudousha no igan no wariai wa
, asubesuto roudousha no tame
ni saikou to naru youda .
Question Is ?appear to? translated into an
auxiliary verb such as ?youda??
The questions are classified mainly in terms of
grammar, and the numbers to the left of the hyphen-
ation of each ID such as 1.1.7.1.3 represent the cat-
egories of the questions. For example, the above
question is related to catenative verbs.
The JEIDA test set consists of two parts, one for
the evaluation of English-Japanese MT and the other
for that of Japanese-English MT. We focused on the
part for English-Japanese MT. This part consists of
769 sample sentences, each of which has a yes/no
question.
The 769 sentences were translated by using five
commercial MT systems to investigate the relation-
ship between subjective evaluation based on yes/no
questions and conventional subjective evaluation
based on fluency and adequacy. The instruction for
the subjective evaluation based on fluency and ad-
equacy followed that given in the TIDES specifi-
cation (TIDES, 2002). The subjective evaluation
based on yes/no questions was done by manually
answering each question for each translation. The
subjective evaluation based on the yes/no questions
was stable; namely, it was almost independent of
the human subjects in our preliminary investigation.
There were only two questions for which the an-
swers generated inconsistency in the subjective eval-
uation when 1,500 question-answer pairs were ran-
domly sampled and evaluated by two human sub-
jects.
Then, we investigated the correlation between the
two types of subjective evaluation. The correlation
coefficients mentioned in this paper are statistically
significant at the 1% or less significance level. The
Spearman rank-order correlation coefficient is used
in this paper. In the subjective evaluation based on
yes/no questions, yes and no were numerically trans-
formed into 1 and ?1. For 3,845 translations ob-
tained by using five MT systems, the correlation co-
efficients between the subjective evaluations based
on yes/no questions and based on fluency and ade-
quacy were 0.48 for fluency and 0.63 for adequacy.
These results indicate that the two subjective evalu-
ations have relatively strong correlations. The cor-
relation is especially strong between the subjective
evaluation based on yes/no questions and adequacy.
2.2 Expansion of JEIDA Test Set
Each sample sentence in the JEIDA test set has only
one question. Therefore, in the subjective evalua-
tion using the JEIDA test set, translation errors that
do not involve the pre-assigned question are ignored
even if they are serious. Therefore, translations that
have serious errors that are not related to the ques-
tion tend to be evaluated as being of high quality.
To solve this problem, we expanded the test set by
adding new questions about translations with the se-
rious errors.
Sentences whose average grades were three or
less for fluency and adequacy for the translation re-
sults of the five MT systems were selected for the
expansion. Besides them, sentences whose average
grades were more than three for fluency and ade-
quacy for the translation results of the five MT sys-
tems were selected when a majority of evaluation
results based on yes/no questions about the transla-
tions of the five MT systems were no. The number
of selected sentences was 150. The expansion was
manually performed using the following steps.
1. Serious translation errors are extracted from the
MT results.
2. For each extracted error, questions strongly re-
lated to the error are searched for in the test set.
If related questions are found, the same types
of questions are generated for the selected sen-
tence, and the same ID as that of the related
question is assigned to each generated question.
Otherwise, questions are newly generated, and
a new ID is assigned to each generated ques-
tion.
3. Each MT result is evaluated according to each
added question.
Eventually, one or more questions were assigned to
each selected sentence in the test set. Among the 150
35
Table 2: Expanded test-set samples.
ID 1.1.7.1.3-1
Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for any
asbestos workers.
Reference translation
(in Japanese)
roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda
.
Question (Q-0) Is ?appear to? translated into an auxiliary verb such as ?youda??
ID 1.1.6.1.3-5
Expanded Translation error ?For? is not translated appropriately.
Question-1 (Q-1) Is ?for? translated into an expression representing a cause/reason such as ?. . .de??
ID Additional-1
Expanded Translation error Some expressions are not translated.
Question-2 (Q-2) Are all English words translated into Japanese?
Table 3: Examples of subjective evaluations based on yes/no questions.
Answer
System MT results Q-0 Q-1 Q-2 Fluency Adequacy
1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwata
roudousha no tame ni demo mottomo ookii youdearu .
Yes No Yes 2 3
2 roudousha no aida no igan no paasenteeji wa, arayuru asubesuto
roudousha no tame ni mottomo takai youni omowa re masu .
Yes Yes Yes 2 3
3 roudousha no aida no igan no paasenteeji wa donna asubesuto no
tame ni mo mottomo takai youni mie masu
Yes No No 1 2
4 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni
wa mottomo takaku mie masu .
Yes No No 1 2
5 roudousha no naka no igan no wariai wa donna asubesuto ni mo
mottomo takai youni mieru .
Yes No No 1 2
selected sentences, questions were newly assigned
to 103 sentences. The number of added questions
was 148. The maximum number of questions added
to a sentence was five. After expanding the test set,
the correlation coefficients between the subjective
evaluations based on yes/no questions and based on
fluency and adequacy increased from 0.48 to 0.51
for fluency and from 0.63 to 0.66 for adequacy. The
differences between the correlation coefficients ob-
tained before and after the expansion are statistically
significant at the 5% or less significance level for
adequacy. These results indicate that the expansion
of the test set significantly improves the correlation
between the subjective evaluations based on yes/no
questions and based on adequacy. When two or
more questions were assigned to a test-set sentence,
the subjective evaluation based on the questions was
decided by the majority answer. The majority an-
swers, yes and no, were numerically transformed
into 1 and ?1. Ties between yes and no were trans-
formed into 0. Examples of added questions and
the subjective evaluations based on the questions are
shown in Tables 2 and 3.
3 Automatic Evaluation of Machine
Translation Based on Rate of
Accomplishment of Sub-goals
3.1 A New Measure for Evaluating Machine
Translation Quality
The JEIDA test set was not designed for auto-
matic evaluation but for human subjective evalua-
tion. However, a measure for automatic MT evalu-
ation that strongly correlates fluency and adequacy
is likely to be established because the subjective
evaluation based on yes/no questions has a rela-
tively strong correlation with the subjective evalua-
tion based on fluency and adequacy, as mentioned in
Section 2. In this section, we describe a method for
automatically evaluating MT quality by predicting
an answer to each yes/no question and using those
answers.
Hereafter, we assume that each yes/no question is
defined as a sub-goal that a given translation should
satisfy and that the sub-goal is accomplished if the
answer to the corresponding yes/no question to the
sub-goal is yes. We also assume that the sub-goal
is unaccomplished if the answer is no. A new eval-
uation score, A, is defined based on a multiple lin-
36
Table 4: Examples of Patterns.
Sample sentence She lived there by herself.
Question Is ?by herself? translated as ?hitori de??
Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in a
translation. Otherwise, the answer is no.
Sample sentence They speak English in New Zealand.
Question The personal pronoun ?they? is omitted in a translation like ?nyuujiilando de wa eigo wo hanasu??
Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the
answer is no.
ear regression model as follows using the rate of ac-
complishment of the sub-goals and the similarities
between a given translation and its reference trans-
lation. The best-fitted line for the observed data is
calculated by the method of least-squares (Draper
and Smith, 1981).
A =
m
?
i=1
?
S
i
? S
i
(1)
+
n
?
j=1
(?
Q
j
? Q
j
+ ?
Q
?
j
? Q
?
j
) + ?

Q
j
=
{
1 : if subgoal is accomplished
0 : otherwise (2)
Q
?
j
=
{
1 : if subgoal is unaccomplished
0 : otherwise (3)
Here, the term Q
j
corresponds to the rate of accom-
plishment of the sub-goal having the i-th ID, and
?
Q
j
is a weight for the rate of accomplishment. The
term Q
?
j
corresponds to the rate of unaccomplish-
ment of the sub-goal having the i-th ID, and ?
Q
?
j
is a
weight for the rate of unaccomplishment. The value
n indicates the number of types of sub-goals. The
term ?

is constant.
The term S
i
indicates a similarity between a trans-
lated sentence and its reference translation, and ?
S
i
is a weight for the similarity. Many methods for cal-
culating the similarity have been proposed (Niessen
et al, 2000; Akiba et al, 2001; Papineni et al, 2002;
NIST, 2002; Leusch et al, 2003; Turian et al, 2003;
Babych and Hartley, 2004; Lin and Och, 2004;
Banerjee and Lavie, 2005; Gimen?ez et al, 2005).
In our research, 23 scores, namely BLEU (Papineni
et al, 2002) with maximum n-gram lengths of 1, 2,
3, and 4, NIST (NIST, 2002) with maximum n-gram
lengths of 1, 2, 3, 4, and 5, GTM (Turian et al, 2003)
with exponents of 1.0, 2.0, and 3.0, METEOR (ex-
act) (Banerjee and Lavie, 2005), WER (Niessen et
al., 2000), PER (Leusch et al, 2003), and ROUGE
(Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and
4 variants (LCS, S?, SU?, W-1.2), were used to cal-
culate each similarity S
i
. Therefore, the value of m
in Eq. (1) was 23. Japanese word segmentation was
performed by using JUMAN 4 in our experiments.
As you can see, the definition of our new measure
is based on a combination of an evaluation measure
focusing on local information and that focusing on
global information.
3.2 Automatic Estimation of Rate of
Accomplishment of Sub-goals
The rate of accomplishment of sub-goals is esti-
mated by determining the answer to each question
as yes or no. This section describes a method based
on simple patterns for determining the answers.
An answer to each question is automatically de-
termined by checking whether patterns are included
in a translation or not. The patterns are constructed
for each question. All of the patterns are expressed
in hiragana characters. Before applying the pat-
terns to a given translation, the translation is trans-
formed into hiragana characters, and all punctuation
is eliminated. The transformation to hiragana char-
acters was performed by using JUMAN in our ex-
periments.
Test-set sentences, the questions assigned to
them, and the patterns constructed for the questions
are shown in Table 4. In the patterns, the symbol ?|?
represents ?OR?.
3.3 Automatic Sub-goal Generation and
Automatic Estimation of Rate of
Accomplishment of Sub-goals
We found that expressions important for maintain-
ing a high translation quality were often commonly
4http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html
37
included in the reference translations for each test-
set sentence. We also found that the expression was
also related to the yes/no question assigned to the
test-set sentence. Therefore, we automatically gen-
erate yes/no questions in the following steps.
1. For each test-set sentence, a set of words com-
monly appearing in the reference translations
are extracted.
2. For each combination of n words in the set
of words extracted in the first step, skip word
n-grams commonly appearing in the reference
translations in the same word order are selected
as a set of common skip word n-grams.
3. For each test-set sentence, the sub-goal is de-
fined as the yes/no question ?Are all of the com-
mon skip word n-grams included in the transla-
tion??
If no common skip word n-grams are found, the
yes/no question is not generated. The answer to the
yes/no question is determined to be yes if all of the
common skip word n-grams are included in a trans-
lation. Otherwise, the answer is determined to be
no.
This scheme assigns greater weight to important
phrases that should be included in the translation to
maintain a high translation quality. Our observation
is that those important phrases are often common
between human translations. A similar scheme was
proposed by Babych and Hartley (Babych and Hart-
ley, 2004) for BLEU. In their scheme, greater weight
is assigned to components that are salient through-
out the document. Therefore, their scheme focuses
on global context while our scheme focuses on local
context. We believe that the two schemes are com-
plementary to each other.
4 Experiments and Discussion
In our experiments, the translation results of three
MT systems and their subjective evaluation results
were used as a development set for constructing the
patterns described in Section 3.2 and for tuning the
parameters ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in Eq. (1). The
translations and evaluation results of the remaining
two MT systems were used as an evaluation set for
testing.
In the development set, each test-set sentence has
at least one question, at least one reference transla-
tion, three MT results, and subjective evaluation re-
sults of the three MT results. The patterns for deter-
mining yes/no answers were manually constructed
for the questions assigned to the 769 test-set sen-
tences. There were 917 questions assigned to them.
Among them, the patterns could be constructed for
898 questions assigned to 767 test-set sentences.
The remaining 19 questions were skipped because
making simple patterns as described in Section 3.2
was difficult; for example, one of the questions
was ?Is the whole sentence translated into one sen-
tence??. The yes/no answer determination accura-
cies obtained by using the patterns are shown in Ta-
ble 5.
Table 5: Results of yes/no answer determination.
Test set Accuracy
Development 97.6% (2,629/2,694)
Evaluation 82.8% (1,487/1,796)
We investigated the correlation between the eval-
uation score, A in Eq. (1) and the subjective eval-
uations, fluency and adequacy, for the 769 test-set
sentences. First, to maximize the correlation coeffi-
cients between the evaluation score, A, and the hu-
man subjective evaluations, fluency and adequacy,
the optimal values of ?
S
i
, ?
Q
j
, ?
Q
?
j
, and ?

in
Eq. (1) were investigated using the development
set within a framework of multiple linear regression
modeling (Draper and Smith, 1981). Then, the cor-
relation coefficients were investigated by using the
optimal value set. The results are shown in Table 6,
7, and 8. In these tables, ?Conventional method? in-
dicates the correlation coefficients obtained when A
was calculated by using only similarities S
i
. ?Con-
ventional method (combination)? is a combination
of existing automatic evaluation methods from the
literature. ?Our method (automatic)? indicates the
correlation coefficients obtained when the results of
the automatic determination of yes/no answers were
used to calculate Q
j
and Q?
j
in Eq. (1). For the 19
questions for which the patterns could not be con-
structed, Q
j
was set at 0. ?Our method (full au-
tomatic)? indicates the correlation coefficients ob-
tained when the results of the automatic sub-goal
generation and determination of rate of accomplish-
38
Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy. (A reference transla-
tion is used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.43 0.48 0.42 0.48
Conventional method (combination) 0.52 0.51 0.49 0.47
Our method (automatic) 0.90? 0.59? 0.89? 0.62?
Our method (upper bound) 0.90? 0.62? 0.90? 0.68?
Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy. (Three reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.47 0.51 0.45 0.51
Conventional method (combination) 0.54 0.54 0.51 0.52
Our method (automatic) 0.90? 0.60? 0.90? 0.64?
Our method (full automatic) 0.85? 0.58 0.84? 0.60?
Our method (upper bound) 0.90? 0.62? 0.90? 0.69?
Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy. (Five reference
translations are used to calculate S
i
.)
Method fluency adequacy
Development set Evaluation set Development set Evaluation set
Conventional method (WER) 0.49 0.53 0.46 0.53
Conventional method (combination) 0.56 0.56 0.52 0.54
Our method (automatic) 0.90? 0.60 0.90? 0.63?
Our method (full automatic) 0.86? 0.59 0.85? 0.60?
Our method (upper bound) 0.91? 0.63? 0.90? 0.69?
In these tables, ? indicates significance at the 5% or less significance level.
ment of sub-goals were used to calculate Q
j
and Q?
j
in Eq. (1). Skip word trigrams, skip word bigrams,
and skip word unigrams were used for generating
the sub-goals according to our preliminary experi-
ments. ?Our method (upper bound)? indicates the
correlation coefficients obtained when human judg-
ments on the questions were used to calculate Q
j
and Q?
j
.
As shown in Table 6, 7, and 8, our methods signif-
icantly outperform the conventional methods from
literature. Note that WER outperformed other indi-
vidual measures like BLEU and NIST in our exper-
iments, and the combination of existing automatic
evaluation methods from the literature outperformed
individual lexical similarity measures by themselves
in almost all cases. The differences between the
correlation coefficients obtained using our method
and the conventional methods are statistically sig-
nificant at the 5% or less significance level for flu-
ency and adequacy, even if the number of reference
translations increases, except in three cases shown
in Table 7 and 8. This indicates that considering
the rate of accomplishment of sub-goals to automat-
ically evaluate the quality of each translation is use-
ful, especially when the number of reference trans-
lations is small.
The differences between the correlation coeffi-
cients obtained using two automatic methods are not
significant. These results indicate that we can reduce
the development cost for constructing sub-goals.
However, there are still significant gaps between the
correlation coefficients obtained using a fully auto-
matic method and upper bounds. These gaps indi-
cate that we need further improvement in automatic
sub-goal generation and automatic estimation of rate
of accomplishment of sub-goals, which is our future
work.
Human judgments of adequacy and fluency are
known to be noisy, with varying levels of intercoder
agreement. Recent work has tended to apply cross-
judge normalization to address this issue (Blatz et
al., 2003). We would like to evaluate against the
normalized data in the future.
39
5 Conclusion and Future Work
We demonstrated that the quality of a translated sen-
tence can be evaluated more appropriately than by
using conventional methods. That was demonstrated
by constructing a test set where the conditions that
should be satisfied to maintain a high translation
quality are assigned to each test-set sentence in the
form of a question, by developing a system that de-
termines an answer to each question, and by com-
bining a measure based on the questions and con-
ventional measures. We also presented a method for
automatically generating sub-goals in the form of
yes/no questions and estimating the rate of accom-
plishment of the sub-goals. Promising results were
obtained.
In the near future, we would like to expand the
test set to improve the upper bound obtained by
our method. We are also planning to expand the
method and improve the accuracy of the automatic
sub-goal generation and determination of the rate of
accomplishment of sub-goals. The sub-goals of a
given sentence should be generated by considering
the complexity of the sentence and the alignment in-
formation between the original source-language sen-
tence and its translation. Further advanced genera-
tion and estimation would give us information about
the erroneous parts of MT results and their quality.
We believe that future research would allow us to
develop high-quality MT systems by tuning the sys-
tem parameters based on the automatic MT evalua-
tion measures.
Acknowledgments
The guideline for expanding the test set is based on that con-
structed by the Technical Research Committee of the AAMT
(Asia-Pacific Association for Machine Translation) The authors
would like to thank the committee members, especially, Mr.
Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro,
Mr. Masaru Fuji, and Ms. Yoshiko Matsukawa for their coop-
eration. This research is partially supported by special coordi-
nation funds for promoting science and technology.
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001.
Using Multiple Edit Distances to Automatically Rank Ma-
chine Translation Output. In Proceedings of the MT Summit
VIII, pages 15?20.
Bogdan Babych and Anthony Hartley. 2004. Extending the
BLEU MT Evaluation Method with Frequency Weightings.
In Proceedings of the 42nd ACL, pages 622?629.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An au-
tomatic metric for mt evaluation with improved correlation
with human judgments. In Proceedings of Workshop on In-
trinsic and Extrinsic Evaluation Measures for MT and/or
Summarization, pages 65?72.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence Estimation for Machine Trans-
lation. Technical report, Center for Language and Speech
Processing, Johns Hopkins University. Summer Workshop
Final Report.
Norman R. Draper and Harry Smith. 1981. Applied Regression
Analysis. 2nd edition. Wiley.
Jesus? Gimen?ez, Enrique Amigo
?
, and Chiori Hori. 2005. Ma-
chine translation evaluation inside qarla. In Proceedings of
the IWSLT?05.
Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura. 1994. Cri-
teria for Evaluating the Linguistic Quality of Japanese to
English Machine Translations. Transactions of the JSAI,
9(4):569?579. (in Japanese).
Hitoshi Isahara. 1995. JEIDA?s Test-Sets for Quality Evalua-
tion of MT Systems ? Technical Evaluation from the Devel-
oper?s Point of View.
Philipp Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In Proceedings of the 2004
Conference on EMNLP, pages 388?395.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2003. A
Novel String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of the
MT Summit IX, pages 240?247.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th COLING,
pages 501?507.
Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out, pages 74?81.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and Hermann
Ney. 2000. An Evaluation Tool for Machine Translation:
Fast Evaluation for MT Research. In Proceedings of the
LREC 2000, pages 39?45.
NIST. 2002. Automatic Evaluation of Machine Translation
Quality Using N-gram Co-Occurrence Statistics. Technical
report, NIST.
Franz Josef Och. 2003. Minimum Error Training in Statistical
Machine Translation. In Proceedings of the 41st ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.
2002. BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th ACL, pages
311?318.
TIDES. 2002. Linguistic Data Annotation Specifi-
cation: Assessment of Fluency and Adequacy in
Arabic-English and Chinese-English Translations.
http://www.ldc.upenn.edu/Projects/TIDES/Translation/
TransAssess02.pdf.
Joseph P. Turian, Luke Shen, and I. Dan Melamed. 2003. Eval-
uation of Machine Translation and its Evaluation. In Pro-
ceedings of the MT Summit IX, pages 386?393.
40
Proceedings of NAACL HLT 2007, pages 484?491,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Comparison of Pivot Methods for Phrase-based Statistical Machine
Translation
Masao Utiyama and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5 Hikari-dai, Soraku-gun, Kyoto 619-0289 Japan
{mutiyama,isahara}@nict.go.jp
Abstract
We compare two pivot strategies for
phrase-based statistical machine transla-
tion (SMT), namely phrase translation
and sentence translation. The phrase
translation strategy means that we di-
rectly construct a phrase translation ta-
ble (phrase-table) of the source and tar-
get language pair from two phrase-tables;
one constructed from the source language
and English and one constructed from En-
glish and the target language. We then use
that phrase-table in a phrase-based SMT
system. The sentence translation strat-
egy means that we first translate a source
language sentence into n English sen-
tences and then translate these n sentences
into target language sentences separately.
Then, we select the highest scoring sen-
tence from these target sentences. We con-
ducted controlled experiments using the
Europarl corpus to evaluate the perfor-
mance of these pivot strategies as com-
pared to directly trained SMT systems.
The phrase translation strategy signifi-
cantly outperformed the sentence transla-
tion strategy. Its relative performance was
0.92 to 0.97 compared to directly trained
SMT systems.
1 Introduction
The rapid and steady progress in corpus-based ma-
chine translation (Nagao, 1981; Brown et al, 1993)
has been supported by large parallel corpora such
as the Arabic-English and Chinese-English paral-
lel corpora distributed by the Linguistic Data Con-
sortium and the Europarl corpus (Koehn, 2005),
which consists of 11 European languages. How-
ever, large parallel corpora do not exist for many
language pairs. For example, there are no pub-
licly available Arabic-Chinese large-scale parallel
corpora even though there are Arabic-English and
Chinese-English parallel corpora.
Much work has been done to overcome the lack
of parallel corpora. For example, Resnik and Smith
(2003) propose mining the web to collect parallel
corpora for low-density language pairs. Utiyama
and Isahara (2003) extract Japanese-English parallel
sentences from a noisy-parallel corpus. Munteanu
and Marcu (2005) extract parallel sentences from
large Chinese, Arabic, and English non-parallel
newspaper corpora.
Researchers can also make the best use of exist-
ing (small) parallel corpora. For example, Nie?en
and Ney (2004) use morpho-syntactic information to
take into account the interdependencies of inflected
forms of the same lemma in order to reduce the
amount of bilingual data necessary to sufficiently
cover the vocabulary in translation. Callison-Burch
et al (2006a) use paraphrases to deal with unknown
source language phrases to improve coverage and
translation quality.
In this paper, we focus on situations where no par-
allel corpus is available (except a few hundred paral-
lel sentences for tuning parameters). To tackle these
extremely scarce training data situations, we pro-
pose using a pivot language (English) to bridge the
484
source and target languages in translation. We first
translate source language sentences or phrases into
English and then translate those English sentences
or phrases into the target language, as described in
Section 3. We thus assume that there is a parallel
corpus consisting of the source language and En-
glish as well as one consisting of English and the tar-
get language. Selecting English as a pivot language
is a reasonable pragmatic choice because English is
included in parallel corpora more often than other
languages are, though any language can be used as a
pivot language.
In Section 2, we describe a phrase-based statisti-
cal machine translation (SMT) system that was used
to develop the pivot methods described in Section
3. This is the shared task baseline system for the
2006 NAACL/HLT workshop on statistical machine
translation (Koehn and Monz, 2006) and consists of
the Pharaoh decoder (Koehn, 2004), SRILM (Stol-
cke, 2002), GIZA++ (Och and Ney, 2003), mkcls
(Och, 1999), Carmel,1 and a phrase model training
code.
2 Phrase-based SMT
We use a phrase-based SMT system, Pharaoh,
(Koehn et al, 2003; Koehn, 2004), which is based
on a log-linear formulation (Och and Ney, 2002). It
is a state-of-the-art SMT system with freely avail-
able software, as described in the introduction.
The system segments the source sentence into so-
called phrases (a number of sequences of consecu-
tive words). Each phrase is translated into a target
language phrase. Phrases may be reordered.
Let f be a source sentence (e.g, French) and e be a
target sentence (e.g., English), the SMT system out-
puts an e? that satisfies
e? = argmaxe Pr(e|f) (1)
= argmaxe
M?
m=1
?mhm(e, f) (2)
where hm(e, f) is a feature function and ?m is a
weight. The system uses a total of eight feature
functions: a trigram language model probability of
the target language, two phrase translation probabil-
ities (both directions), two lexical translation prob-
1http://www.isi.edu/licensed-sw/carmel/
abilities (both directions), a word penalty, a phrase
penalty, and a linear reordering penalty. For details
on these feature functions, please refer to (Koehn et
al., 2003; Koehn, 2004; Koehn et al, 2005). To set
the weights, ?m, we carried out minimum error rate
training (Och, 2003) using BLEU (Papineni et al,
2002) as the objective function.
3 Pivot methods
We use the phrase-based SMT system described in
the previous section to develop pivot methods. We
use English e as the pivot language. We use French
f and German g as examples of the source and target
languages in this section.
We describe two types of pivot strategies, namely
phrase translation and sentence translation.
The phrase translation strategy means that we di-
rectly construct a French-German phrase translation
table (phrase-table for short) from a French-English
phrase-table and an English-German phrase-table.
We assume that these French-English and English-
German tables are built using the phrase model train-
ing code in the baseline system described in the
introduction. That is, phrases are heuristically ex-
tracted from word-level alignments produced by do-
ing GIZA++ training on the corresponding parallel
corpora (Koehn et al, 2003).
The sentence translation strategy means that we
first translate a French sentence into n English sen-
tences and translate these n sentences into German
separately. Then, we select the highest scoring sen-
tence from the German sentences.
3.1 Phrase translation strategy
The phrase translation strategy is based on the fact
that the phrase-based SMT system needs a phrase-
table and a language model for translation. Usually,
we have the language model of a target language.
Consequently, we only need to construct a phrase-
table to train the phrase-based SMT system.
We assume that we have a French-English phrase-
table TFE and an English-German phrase-table
TEG. From these tables, we construct a French-
German phrase-table TFG, which requires estimat-
ing four feature functions; phrase translation prob-
abilities for both directions, ?(f? |g?) and ?(g?|f?) and
lexical translation probabilities for both directions,
485
pw(f? |g?) and pw(g?|f?), where f? and g? are French and
German phrases that are parts of phrase translation
pairs in TFE and TEG, respectively.2
We estimate these probabilities using the proba-
bilities available in TFE and TEG as follows.3
?(f? |g?) =
?
e??TFE?TEG
?(f? |e?)?(e?|g?) (3)
?(g?|f?) =
?
e??TFE?TEG
?(g?|e?)?(e?|f?) (4)
pw(f? |g?) =
?
e??TFE?TEG
pw(f? |e?)pw(e?|g?) (5)
pw(g?|f?) =
?
e??TFE?TEG
pw(g?|e?)pw(e?|f?) (6)
where e? ? TFE?TEG means that the English phrase
e? is included in both TFE and TEG as part of phrase
translation pairs. ?(f? |e?) and ?(e?|f?) are phrase
translation probabilities for TFE and ?(e?|g?) and
?(g?|e?) are those for TEG. pw(f? |e?) and pw(e?|f?) are
lexical translation probabilities for TFE and pw(e?|g?)
and pw(g?|e?) are those for TEG.
The definitions of the phrase and lexical transla-
tion probabilities are as follows (Koehn et al, 2003).
?(f? |e?) = count(f? , e?)?
f? ? count(f? ?, e?)
(7)
where count(f? , e?) gives the total number of times
the phrase f? is aligned with the phrase e? in the par-
allel corpus. Eq. 7 means that ?(f? |e?) is calculated
using maximum likelihood estimation.
The definition of the lexical translation probabil-
ity is
pw(f? |e?) = maxa pw(f? |e?,a) (8)
pw(f? |e?,a) =
n?
i=1
Ew(fi|e?,a) (9)
Ew(fi|e?,a) = 1|{j|(i, j) ? a}|
?
?(i,j)?a
w(fi|ej)
(10)
2Feature functions scores are calculated using these proba-
bilities. For example, for a translation probability of a French
sentence f = f?1 . . . f?K and a German sentence g = g?1 . . . g?K ,
h(g, f) = log?Ki=1 ?(f?i|g?i), where K is the number of
phrases.
3Wang et al (2006) use essentially the same definition to
induce the translation probability of the source and target lan-
guage word alignment that is bridged by an intermediate lan-
guage. Callison-Burch et al (2006a) use a similar definition for
a paraphrase probability.
w(f |e) = count(f, e)?
f ? count(f ?, e)
(11)
where count(f, e) gives the total number of times
the word f is aligned with the word e in the par-
allel corpus. Thus, w(f |e) is the maximum likeli-
hood estimation of the word translation probability
of f given e. Ew(fi|e?,a) is calculated from a word
alignment a between a phrase pair f? = f1f2 . . . fn
and e? = e1e2 . . . em where fi is connected to several
(|{j|(i, j) ? a}|) English words. Thus, Ew(fi|e?,a)
is the average (or mixture) of w(fi|ej). This means
that Ew(fi|e?,a) is an estimation of the probabil-
ity of fi in a. Consequently, pw(f? |e?,a) estimates
the probability of f? given e? and a using the prod-
uct of the probabilities Ew(fi|e?,a). This assumes
that the probability of fi is independent given e? and
a. pw(f? |e?) takes the highest pw(f? |e?,a) if there
are multiple alignments a. This discussion, which
is partly based on Section 4.1.2 of (Och and Ney,
2004), means that the lexical translation probability
pw(f? |e?) is another probability estimated using the
word translation probability w(f |e).
The justification of Eqs. 3?6 is straightforward.
From the discussion above, we know that the prob-
abilities, ?(f? |e?), ?(e?|f?), ?(g?|e?), ?(e?|g?), pw(f? |e?),
pw(e?|f?), pw(g?|e?), and pw(e?|g?) are probabilities in
the ordinary sense. Thus, we can derive ?(f? |g?),
?(g?|f?), pw(f? |g?), and pw(g?|f?) by assuming that
these probabilities are independent given an English
phrase e? (e.g., ?(f? |g?, e?) = ?(f? |e?)).
We construct a TFG that consists of all French-
German phrases whose phrase and lexical transla-
tion probabilities as defined in Eqs. 3?6 are greater
than 0. We use the term PhraseTrans to denote SMT
systems that use the phrase translation strategy de-
scribed above.
3.2 Sentence translation strategy
The sentence translation strategy uses two inde-
pendently trained SMT systems. We first trans-
late a French sentence f into n English sentences
e1, e2, ..., en using a French-English SMT system.
Each ei (i = 1 . . . n) has the eight scores calcu-
lated from the eight feature functions described in
Section 2. We denote these scores hei1, hei2, . . . hei8.
Second, we translate each ei into n German sen-
tences gi1,gi2, . . . ,gin using an English-German
486
SMT system. Each gij (j = 1 . . . n) has the eight
scores, which are denoted as hgij1, hgij2, . . . , hgij8.
This situation is depicted as
f ? ei (hei1, hei2, . . . , hei8)
? gij (hgij1, hgij2, . . . , hgij8)
We define the score of gij , S(gij), as
S(gij) =
8?
m=1
(?emheim + ?gmhgijm) (12)
where ?em and ?gm are weights set by performing
minimum error rate training4 as described in Section
2. We select the highest scoring German sentence
g? = argmaxgij S(gij) (13)
as the translation of the French sentence f .
A drawback of this strategy is that translation
speed is about O(n) times slower than those of the
component SMT systems. This is because we have
to run the English-German SMT system n times for
a French sentence. Consequently, we cannot set n
very high. When we used n = 15 in the experi-
ments described in Section 4, it took more than two
days to translate 3064 test sentences on a 3.06GHz
LINUX machine.
Note that when n = 1, the above strategy pro-
duces the same translation with the simple sequen-
tial method that we first translate a French sentence
into an English sentence and then translate that sen-
tence into a German sentence.
We use the terms SntTrans15 and SntTrans1 to de-
note SMT systems that use the sentence translation
strategy with n = 15 and n = 1, respectively.
4 Experiments
We conducted controlled experiments using the
Europarl corpus. For each language pair de-
scribed below, the Europarl corpus provides three
4We use a reranking strategy for the sentence translation
strategy. We first obtain n2 German sentences for each French
sentence by applying two independently trained French-English
and English-German SMT systems. Each of the translated Ger-
man sentences has the sixteen scores as described above. The
weights in Eq. 12 are tuned against reference German sentences
by performing minimum error rate training. These weights are
in general different from those of the original French-English
and English-German SMT systems.
types of parallel corpora; the source language?
English, English?the target language, and the source
language?the target language. This means that we
can directly train an SMT system using the source
and target language parallel corpus as well as pivot
SMT systems using English as the pivot language.
We use the term Direct to denote directly trained
SMT systems. For each language pair, we com-
pare four SMT systems; Direct, PhraseTrans, Snt-
Trans15, and SntTrans1.5
4.1 Training, tuning and testing SMT systems
We used the training data for the shared task of
the SMT workshop (Koehn and Monz, 2006) to
train our SMT systems. It consists of three paral-
lel corpora: French-English, Spanish-English, and
German-English.
We used these three corpora to extract a set of
sentences that were aligned to each other across all
four languages. For that purpose, we used English
as the pivot. For each distinct English sentence, we
extracted the corresponding French, Spanish, and
German sentences. When an English sentence oc-
curred multiple times, we extracted the most fre-
quent translation. For example, because ?Resump-
tion of the session? was translated into ?Wiederauf-
nahme der Sitzungsperiode? 120 times and ?Wieder-
aufnahme der Sitzung? once, we extracted ?Wieder-
aufnahme der Sitzungsperiode? as its translation.
Consequently, we extracted 585,830 sentences for
each language. From these corpora, we constructed
the training parallel corpora for all language pairs.
We followed the instruction of the shared task
baseline system to train our SMT systems.6 We
used the trigram language models provided with the
shared task. We did minimum error rate training on
the first 500 sentences in the shared task develop-
ment data to tune our SMT systems and used the
5As discussed in the introduction, we intend to use the pivot
strategies in a situation where a very limited amount of parallel
text is available. The use of the Europarl corpus is not an accu-
rate simulation of the intended situation because it enables us to
use a relatively large parallel corpus for direct training. How-
ever, it is necessary to evaluate the performance of the pivot
strategies against that of Direct SMT systems under controlled
experiments in order to determine how much the pivot strate-
gies can be improved. This is a first step toward the use of pivot
methods in situations where training data is extremely scarce.
6The parameters for the Pharaoh decoder were ?-dl 4 -b 0.03
-s 100?. The maximum phrase length was 7.
487
3064 test sentences for each language as our test set.
Our evaluation metric was %BLEU scores, as cal-
culated by the script provided along with the shared
task.7 We lowercased the training, development and
test sentences.
4.2 Results
Table 1 compares the BLEU scores of the four SMT
systems; Direct, PhraseTrans, SntTrans15, and Snt-
Trans1 for each language pair. The columns SE and
ET list the BLEU scores of the Direct SMT sys-
tems trained on the source language?English and
English?the target language parallel corpora. The
numbers in the parentheses are the relative scores
of the pivot SMT systems, which were obtained
by dividing their BLEU scores by that of the cor-
responding Direct system. For example, for the
Spanish?French language pair, the BLEU score of
the Direct SMT system was 35.78, that of the
PhraseTrans SMT system was 32.90, and the rela-
tive performance was 0.92 = (32.90/35.78). For
the SntTrans15 SMT system, the BLEU score was
29.49 and the relative performance was 0.82 =
(29.49/35.78).
The BLEU scores of the Direct SMT systems
were higher than those of the PhraseTrans SMT sys-
tems for all six source-target language pairs. The
PhraseTrans SMT systems performed better than
the SntTrans15 SMT systems for all pairs. The
SntTrans15 SMT systems were better than the Snt-
Trans1 SMT systems for four pairs. According
to the sign test, under the null hypothesis that the
BLEU scores of two systems are equivalent, finding
one system obtaining better BLEU scores on all six
language pairs is statistically significant at the 5 %
level. Obtaining four better scores is not statistically
significant. Thus, Table 1 indicates
Direct > PhraseTrans > SntTrans15 ? SntTrans1
where ?>? and ??? means that the differences of
the BLEU scores of the corresponding SMT systems
are statistically significant and insignificant, respec-
tively.
7Callison-Burch et al (2006b) show that in general a higher
BLEU score is not necessarily indicative of better translation
quality. However, they also suggest that the use of BLEU is
appropriate for comparing systems that use similar translation
strategies, which is the case with our experiments.
As expected, the Direct SMT systems outper-
formed the other systems. We regard the BLEU
scores of the Direct systems as the upperbound. The
SntTrans15 SMT systems did not significantly out-
perform the SntTrans1 SMT systems. We think that
this is because n = 15 was not large enough to cover
good translation candidates.8 Selecting the highest
scoring translation from a small pool did not always
lead to better performance. To improve the perfor-
mance of the sentence translation strategy, we need
to use a large n. However, this is not practical be-
cause of the slow translation speed, as discussed in
Section 3.2.
The PhraseTrans SMT systems significantly out-
performed the SntTrans15 and SntTrans1 systems.
That is, the phrase translation strategy is better
than the sentence translation strategy. Since the
phrase-tables constructed using the phrase transla-
tion strategy can be integrated into the Pharaoh de-
coder as well as the directly extracted phrase-tables,
the PhraseTrans SMT systems can fully exploit the
power of the decoder. This led to better performance
even when the induced phrase-tables were noisy, as
described below.
The relative performance of the PhraseTrans
SMT systems compared to the Direct SMT systems
was 0.92 to 0.97. These are very promising re-
sults. To show how these systems translated the
test sentences, we translated some outputs of the
Spanish-French Direct and PhraseTrans SMT sys-
tems into English using the French-English Direct
system. These are shown in Table 3 with the refer-
ence English sentences.
The relative performance seems to be related to
the BLEU scores for the Direct SMT systems. It
was relatively high (0.95 to 0.97) for the difficult (in
terms of BLEU) language pairs but relatively low
(0.92) for the easy language pairs; Spanish?French
and French?Spanish. There is a lot of room for
improvement for the relatively easy language pairs.
This relationship is stronger than the relationship be-
tween the BLEU scores for SE/ET and those for the
PhraseTrans systems, where no clear trend exists.
Table 2 shows the number of phrases stored in the
phrase-tables. The Direct SMT systems had 7.3 to
8A typical reranking approach to SMT (Och et al, 2004)
uses a 1000?best list.
488
Source?Target Direct PhraseTrans SntTrans15 SntTrans1 SE ET
Spanish?French 35.78 > 32.90 (0.92) > 29.49 (0.82) > 29.16 (0.81) 29.31 28.80
French?Spanish 34.16 > 31.49 (0.92) > 28.41 (0.83) > 27.99 (0.82) 27.59 29.07
German?French 23.37 > 22.47 (0.96) > 22.03 (0.94) > 21.64 (0.93) 22.40 28.80
French?German 15.27 > 14.51 (0.95) > 14.03 (0.92) < 14.21 (0.93) 27.59 15.81
German?Spanish 22.34 > 21.76 (0.97) > 21.36 (0.96) > 20.97 (0.94) 22.40 29.07
Spanish?German 15.50 > 15.11 (0.97) > 14.46 (0.93) < 14.61 (0.94) 29.31 15.81
Table 1: BLEU scores and relative performance
No. of phrases (?M? means 106)
Direct PhraseTrans common R P
S?F 18.2M 190.8M 6.3M 34.7 3.3
F?S 18.2M 186.8M 6.3M 34.7 3.4
G?F 7.3M 174.9M 3.1M 43.2 1.8
F?G 7.3M 168.2M 3.1M 43.2 1.9
G?S 7.5M 179.6M 3.3M 44.1 1.9
S?G 7.6M 176.6M 3.3M 44.1 1.9
?S?, ?F?, and ?G? are the acronyms of Spanish, French, and
German, respectively. ?X?Y? means that ?X? is the source lan-
guage and ?Y? is the target language.
Table 2: Statistics for the phrase-tables
18.2 million phrases, and the PhraseTrans systems
had 168.2 to 190.8 million phrases. The numbers of
phrases stored in the PhraseTrans systems were very
large compared to those of Direct systems.9 How-
ever, this does not cause a computational problem in
decoding because those phrases that do not appear in
source sentences are filtered so that only the relevant
phrases are used during decoding.
The figures in the common column are the number
of phrases common to the Direct and PhraseTrans
systems. R (recall) and P (precision) are defined as
follows.
R = No. of common phrases ? 100
No. of phrases in Direct system
9In Table 2, the PhraseTrans systems have more than 10x
as many phrases as the Direct systems. This can be explained
as follows. Let fi be the fanout of an English phrase i, i.e.,
fi is the number of phrase pairs containing the English phrase
i in a phrase-table, then the size of the phrase-table is s1 =?n
i=1 fi, where n is the number of distinct English phrases.
When we combine two phrase-tables, the size of the combined
phrase table is roughly s2 =
?n
i=1 f2i . Thus, the relative size
of the combined phrase table is roughly r = s2s1 =
E(f2)
E(f) ,
where E(f) = s1n and E(f2) = s2n are the averages over
fi and f2i , respectively. As an example, we calculated these
averages for the German-English phrase table. E(f) was 1.5,
E(f2) was 43.7, and r was 28.9. This shows that even if an
average fanout is small, the size of a combined phrase table can
be very large.
P = No. of common phrases ? 100
No. of phrases in PhraseTrans system
Recall was reasonably high. However, the upper
bound of recall was 100 percent because we used
a multilingual corpus whose sentences were aligned
to each other across all four languages, as described
in Section 4.1. Thus, there is a lot of room for im-
provement with respect to recall. Precision, on the
other hand, was very low. However, translation per-
formance was not significantly affected by this low
precision, as is shown in Table 1. This indicates that
recall is more important than precision in building
phrase-tables.
5 Related work
Pivot languages have been used in rule-based ma-
chine translation systems. Boitet (1988) discusses
the pros and cons of the pivot approaches in multi-
lingual machine translation. Schubert (1988) argues
that a pivot language needs to be a natural language,
due to the inherent lack of expressiveness of artifi-
cial languages.
Pivot-based methods have also been used in other
related areas, such as translation lexicon induc-
tion (Schafer and Yarowsky, 2002), word alignment
(Wang et al, 2006), and cross language information
retrieval (Gollins and Sanderson, 2001). The trans-
lation disambiguation techniques used in these stud-
ies could be used for improving the quality of phrase
translation tables.
In contrast to these, very little work has been
done on pivot-based methods for SMT. Kauers et
al. (2002) used an artificial interlingua for spoken
language translation. Gispert and Marin?o (2006)
created an English-Catalan parallel corpus by auto-
matically translating the Spanish part of an English-
Spanish parallel corpus into Catalan with a Spanish-
Catalan SMT system. They then directly trained an
SMT system on the English-Catalan corpus. They
489
showed that this direct training method is superior
to the sentence translation strategy (SntTrans1) in
translating Catalan into English but is inferior to
it in the opposite translation direction (in terms of
the BLEU score). In contrast, we have shown that
the phrase translation strategy consistently outper-
formed the sentence translation strategy in the con-
trolled experiments.
6 Conclusion
We have compared two types of pivot strategies,
namely phrase translation and sentence translation.
The phrase translation strategy directly constructs a
phrase translation table from a source language and
English phrase-table and a target language and En-
glish phrase-table. It then uses this phrase table in
a phrase-based SMT system. The sentence transla-
tion strategy first translates a source language sen-
tence into n English sentences and translates these n
sentences into target language sentences separately.
Then, it selects the highest scoring sentence from the
target language sentences.
We conducted controlled experiments using the
Europarl corpus to compare the performance of
these two strategies to that of directly trained SMT
systems. The experiments showed that the perfor-
mance of the phrase translation strategy was statis-
tically significantly better than that of the sentence
translation strategy and that its relative performance
compared to the directly trained SMT systems was
0.92 to 0.97. These are very promising results.
Although we used the Europarl corpus for con-
trolled experiments, we intend to use the pivot strate-
gies in situations where very limited amount of par-
allel corpora are available for a source and target lan-
guage but where relatively large parallel corpora are
available for the source language?English and the
target language?English. In future work, we will
further investigate the pivot strategies described in
this paper to confirm that the phrase translation strat-
egy is better than the sentence translation strategy in
the intended situation as well as with the Europarl
corpus.10
10As a first step towards real situations, we conducted addi-
tional experiments. We divided the training corpora in Section
4 into two halves. We used the first 292915 sentences to train
source-English SMT systems and the remaining 292915 ones
to train English-target SMT systems. Based on these source-
References
Christian Boitet. 1988. Pros and cons of the pivot and
transfer approaches in multilingual machine transla-
tion. In Dan Maxwell, Klaus Schubert, and Toon
Witkam, editors, New Directions in Machine Trans-
lation. Foris. (appeared in Sergei Nirenburg, Harold
Somers and Yorick Wilks (eds.) Readings in Machine
Translation published by the MIT Press in 2003).
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006a. Improved statistical machine transla-
tion using paraphrases. In NAACL.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006b. Re-evaluating the role of BLEU in
machine translation research. In EACL.
Adria? de Gispert and Jose? B. Mari no. 2006. Catalan-
English statistical machine translation without parallel
corpus: Bridging through Spanish. In Proc. of LREC
5th Workshop on Strategies for developing Machine
Translation for Minority Languages.
Tim Gollins and Mark Sanderson. 2001. Improving
cross language information retrieval with triangulated
translation. In SIGIR.
Manuel Kauers, Stephan Vogel, Christian Fu?gen, and
Alex Waibel. 2002. Interlingua based statistical ma-
chine translation. In ICSLP.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between eu-
ropean languages. In Proceedings on the Workshop on
Statistical Machine Translation, pages 102?121, New
York City, June. Association for Computational Lin-
guistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
English and English-target SMT systems, we trained Phrase-
Trans and SntTrans1 SMT systems. Other experimental condi-
tions were the same as those described in Section 4. The table
below shows the BLUE scores of these SMT systems. It indi-
cates that the PhraseTrans systems consistently outperformed
the SntTrans1 systems.
Source-Target PhraseTrans SntTrans1
Spanish-French 31.57 28.36
French-Spanish 30.18 27.75
German-French 20.48 19.83
French-German 14.38 14.11
German-Spanish 19.58 18.67
Spanish-German 14.80 14.46
490
Ref i hope with all my heart , and i must say this quite emphatically , that an opportunity will arise when this
document can be incorporated into the treaties at some point in the future .
Dir i hope with conviction , and put great emphasis , that again is a serious possibility of including this in the treaties .
PT i hope with conviction , and i very much , insisted that never be a serious possibility of including this in the
treaties .
Ref should this fail to materialise , we should not be surprised if public opinion proves sceptical about europe , or even
rejects it .
Dir otherwise , we must not be surprised by the scepticism , even the rejection of europe in the public .
PT otherwise , we must not be surprised by the scepticism , and even the rejection of europe in the public .
Ref the intergovernmental conference - to address a third subject - on the reform of the european institutions is also of
decisive significance for us in parliament .
Dir the intergovernmental conference - and this i turn to the third issue on the reform of the european institutions is of
enormous importance for the european parliament .
PT the intergovernmental conference - and this brings me to the third issue - on the reform of the european institutions
has enormous importance for the european parliament .
Table 3: Reference sentences (Ref) and the English translations (by the French-English Direct system) of
the outputs of the Spanish-French Direct and PhraseTrans SMT systems (Dir and PT).
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation. In
IWSLT.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguistics,
31(4):477?504.
Makoto Nagao. 1981. A framework of a mechani-
cal translation between Japanese and English by anal-
ogy principle. In the International NATO Symposium
on Artificial and Human Intelligence. (appeared in
Sergei Nirenburg, Harold Somers and Yorick Wilks
(eds.) Readings in Machine Translation published by
the MIT Press in 2003).
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In HLT-NAACL.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In CoNLL.
Klaus Schubert. 1988. Implicitness as a guiding princi-
ple in machine translation. In COLING.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news articles
and sentences. In ACL, pages 72?79.
Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word
alignment for languages with scarce resources using
bilingual corpora of other language pairs. In COL-
ING/ACL 2006 Main Conference Poster Sessions.
491
Hierarchy Extraction based on Inclusion of Appearance  
Eiko Yamamoto Kyoko Kanzaki Hitoshi Isahara 
Computational Linguistics Group, 
National Institute of Information and Communications Technology 
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan. 
eiko@nict.go.jp kanzaki@nict.go.jp isahara@nict.go.jp 
 
Abstract 
In this paper, we propose a method of auto-
matically extracting word hierarchies based on 
the inclusion relation of appearance patterns 
from corpora. We apply a complementary 
similarity measure to find a hierarchical word 
structure. This similarity measure was devel-
oped for the recognition of degraded machine-
printed text in the field and can be applied to 
estimate one-to-many relations. Our purpose is 
to extract word hierarchies from corpora 
automatically. As the initial task, we attempt 
to extract hierarchies of abstract nouns co-
occurring with adjectives in Japanese and 
compare with hierarchies in the EDR elec-
tronic dictionary.  
1 Introduction 
The hierarchical relations of words are useful as 
language resources. Hierarchical semantic lexical 
databases such as WordNet (Miller et al, 1990) 
and the EDR electronic dictionary (1995) are used 
for NLP research worldwide to fully understand a 
word meaning. In current thesauri in the form of 
hierarchical relations, words are categorized manu-
ally and classified in a top-down manner based on 
human intuition. This is a good way to make a 
lexical database for users having a specific purpose. 
However, word hierarchies based on human intui-
tion tend to vary greatly depending on the lexicog-
rapher. In addition, hierarchical relations based on 
various data may be needed depending on each 
user.  
Accordingly, we try to extract a hierarchical re-
lation of words automatically and statistically. In 
previous research, ways of extracting from defini-
tion sentences in dictionaries (Tsurumaru et al, 
1986; Shoutsu et al, 2003) or from a corpus by 
using patterns such as ?a part of?, ?is-a?, or ?and? 
(Berland and Charniak, 1999; Caraballo, 1999) 
have been proposed. Also, there is a method that 
uses the dependence relation between words taken 
from a corpus (Matsumoto et al, 1996). In contrast, 
we propose a method based on the inclusion rela-
tion of appearance patterns from corpora. 
In this paper, to verify the suitability of our 
method, we attempt to extract hierarchies of ab-
stract nouns co-occurring with adjectives in Japa-
nese. We select two similarity measures to estimate 
the inclusion relation between word appearance 
patterns. One is a complementary similarity meas-
ure; i.e., a similarity measure developed for the 
recognition of degraded machine-printed text in the 
field (Hagita and Sawaki, 1995). This measure can 
be used to estimate one-to-many relations such as 
superordinate?subordinate relations from appear-
ance patterns (Yamamoto and Umemura, 2002). 
The second similarity measure is the overlap coef-
ficient, which is a similarity measure to calculate 
the rate of overlap between two binary vectors. 
Using each measure, we extract hierarchies from a 
corpus. After that, we compare these with the EDR 
electronic dictionary. 
2 Experiment Corpus 
A good deal of linguistic research has focused on 
the syntactic and semantic functions of abstract 
nouns (Nemoto, 1969; Takahashi, 1975; Schmid, 
2000; Kanzaki et al, 2003). In the example, ?Yagi 
(goat) wa seishitsu (nature) ga otonashii (gentle) 
(The nature of goats is gentle).?, Takahashi (1975) 
recognized that the abstract noun ?seishitsu (na-
ture)? is a hypernym of the attribute that the predi-
cative adjective ?otonashi (gentle)? expresses. 
Kanzaki et al (2003) defined such abstract nouns 
that co-occur with adjectives as adjective hy-
pernyms, and extracted these co-occurrence rela-
tions between abstract nouns and adjectives from 
many corpora such as newspaper articles. In the 
linguistic data, there are sets of co-occurring 
adjectives for each abstract noun ? the total num-
ber of abstract noun types is 365 and the number of 
adjective types is 10,525. Some examples are as 
follows.  
OMOI  (feeling): ureshii (glad), kanashii (sad), 
shiawasena (happy), ? 
KANTEN (viewpoint): igakutekina (medical), 
rekishitekina (historical), ... 
3 Complementary Similarity Measure 
The complementary similarity measure (CSM) is 
used in a character recognition method for binary 
images which is robust against heavy noise or 
graphical designs (Sawaki and Hagita, 1996). Ya-
mamoto et al (2002) applied CSM to estimate one-
to-many relations between words. They estimated 
one-to-many relations from the inclusion relations 
between the appearance patterns of two words.  
The appearance pattern is expressed as an n-
dimensional binary feature vector. Now, let F = (f1, 
f2, ?, fn) and T = (t1, t2, ?, tn) (where fi, ti = 0 or 
1) be the feature vectors of the appearance patterns 
for a word and another word, respectively. The 
CSM of F to T is defined as 
dcban
tfdtfc
tfbtfa
dbca
bcadTFCSM
n
i ii
n
i ii
n
i ii
n
i ii
+++=
???=??=
??=?=
++
?=
??
??
==
==
,)1()1(,)1(
,)1(,
))((
),(
11
11
 
The CSM of F to T represents the degree to 
which F includes T; that is, the inclusion relation 
between the appearance patterns of two words.  
In our experiment, each ?word? is an abstract 
noun. Therefore, n is the number of adjectives in 
the corpus, a indicates the number of adjectives co-
occurring with both abstract nouns, b and c indi-
cate the number of adjectives co-occurring with 
either abstract noun, and d indicates the number of 
adjectives co-occurring with neither abstract noun. 
4 Overlap Coefficient 
The overlap coefficient (OVLP) is a similarity 
measure for binary vectors (Manning and Schutze, 
1999). OVLP is essentially a measure of inclusion. 
It has a value of 1.0 if every dimension with a non-
zero value for the first vector is also non-zero for 
the second vector or vice versa. In other words, the 
value is 1.0 when the first vector completely in-
cludes the second vector or vice versa. OVLP of F 
and T is defined as 
),(),(
),(
cabaMIN
a
TFMIN
TF
TFOVLP ++==
I
 
5 EDR hierarchy 
The EDR Electronic Dictionary (1995) was de-
veloped for advanced processing of natural lan-
guage by computers and is composed of eleven 
sub-dictionaries. The sub-dictionaries include a 
concept dictionary, word dictionaries, bilingual 
dictionaries, etc. We verify and analyse the hierar-
chies that are extracted based on a comparison with 
the EDR dictionary. However, the hierarchies in 
EDR consist of hypernymic concepts represented 
by sentences. On the other hand, our extracted hi-
erarchies consist of hypernyms such as abstract 
nouns. Therefore, we have to replace the concept 
composed of a sentence with the sequence of the 
words. We replace the description of concepts with 
entry words from the ?Word List by Semantic 
Principles? (1964) and add synonyms. We also add 
to abstract nouns in order to reduce any difference 
in representation. In this way, conceptual hierar-
chies of adjectives in the EDR dictionary are de-
fined by the sequence of words. 
6 Hierarchy Extraction Process 
The processes for hierarchy extraction from the 
corpus are as follows. ?TH? is a threshold value for 
each pair under consideration. If TH is low, we can 
obtain long hierarchies. However, if TH is too low, 
the number of word pairs taken into consideration 
increases overwhelmingly and the measurement 
reliability diminishes. In this experiment, we set 
0.2 as TH. 
1. Compute the similarity between appear-
ance patterns for each pair of words. The 
hierarchical relation between the two 
words in a pair is determined by the simi-
larity value. We express the pair as (X, Y), 
where X is a hypernym of Y and Y is a 
hyponym of X. 
2. Sort the pairs by the normalized similari-
ties and reduce the pairs where the simi-
larity is less than TH.  
3. For each abstract noun, 
A) Choose a pair (B, C) where word B is 
the hypernym with the highest value. 
The hierarchy between B and C is set 
to the initial hierarchy.  
B) Choose a pair (C, D) where hyponym 
D is not contained in the current hier-
archy and has the highest value in pairs 
where the last word of the current hier-
archy C is a hypernym. 
C) Connect hyponym D with the tail of 
the current hierarchy.  
D) While such a pair can be chosen, repeat 
B) and C). 
E) Choose a pair (A, B) where hypernym 
A is not contained in the current hier-
archy and has the highest value in pairs 
where the first word of the current hi-
erarchy B is a hypernym. 
F) Connect hypernym A with the head of 
the current hierarchy. 
G) While such a pair can be chosen, repeat 
E) and F). 
4. For the hierarchies that are built, 
A) If a short hierarchy is included in a 
longer hierarchy with the order of the 
words preserved, the short one is 
dropped from the list of hierarchies. 
B) If a hierarchy has only one or a few 
different words from another hierarchy, 
the two hierarchies are merged. 
7 Extracted Hierarchy 
Some extracted hierarchies are as follows. In our 
experiment, we get koto (matter) as the common 
hypernym.  
koto (matter) -- joutai (state) -- kankei (relation) 
-- kakawari (something to do with) -- tsukiai 
(have an acquaintance with) 
koto (matter) -- toki (when) -- yousu (aspect) -- 
omomochi (one?s face) -- manazashi (a look) -- 
iro (on one?s face) -- shisen (one?s eye) 
8 Comparison 
We analyse extracted hierarchies by using the 
number of nodes that agree with the EDR hierar-
chy. Specifically, we count the number of nodes 
(nouns) which agree with a word in the EDR hier-
archy, preserving the order of each hierarchy. Here, 
two hierarchies are ?A - B - C - D - E? and ?A - B 
- D - F - G.? They have three agreement nodes; ?A 
- B - D.?  
Table 1 shows the distribution of the depths of a 
CSM hierarchy, and the number of nodes that 
agree with the EDR hierarchy at each depth. Table 
2 shows the same for an OVLP one. ?Agreement 
Level? is the number of agreement nodes. The bold 
font represents the number of hierarchies com-
pletely included in the EDR hierarchy.  
8.1 Depth of Hierarchy 
The number of hierarchies made from the EDR 
dictionary (EDR hierarchy) is 932 and the deepest 
level is 14. The number of CSM hierarchies is 105 
and the depth is from 3 to 14 (Table 1). The num-
ber of OVLP hierarchies is 179 and the depth is 
from 2 to 9 (Table 2). These results show that 
CSM builds a deeper hierarchy than OVLP, though 
the number of hierarchies is less than OVLP. Also, 
the deepest level of CSM equals that of EDR. 
Therefore, comparison with the EDR dictionary is 
an appropriate way to verify the hierarchies that we 
have extracted.  
In both tables, we find most hierarchies have an 
agreement level from 2 to 4. The deepest agree-
ment level is 6. For an agreement level of 5 or bet-
ter, the OVLP hierarchy includes only two hierar-
chies while the CSM hierarchy includes nine hier-
archies. This means CSM can extract hierarchies 
having more nodes which agree with the EDR hi-
erarchy than is possible with OVLP.  
 
Depth of 
Hierarchy
Agreement Level 
  1        2        3       4       5       6  
3 1 4 1  
4 8 6 2 
5 9 8  1
6 8 9 4 1
7 2 6 1 1
8 1 5 2 2
9 3 2 3 1
10  1  2
11  4 1 
12  1  1
13  1  2
14    1
Table 1: Distribution of CSM hierarchy for each 
depth 
Depth of 
Hierarchy
Agreement Level 
1        2         3        4       5       6 
2 1   
3 2 8 1  
4 25 9 1 
5 24 13 7 
6 21 31 5 
7 5 12 1 1
8 3 5 2 1
9 1 3 1 
Table 2: Distribution of OVLP hierarchy for 
each depth 
Also, many abstract nouns agree with the hy-
peronymic concept around the top level. In current 
thesauri, the categorization of words is classified in 
a top-down manner based on human intuition. 
Therefore, we believe the hierarchy that we have 
built is consistent with human intuition, at least 
around the top level of hyperonymic concepts. 
9 Conclusion 
We have proposed a method of automatically ex-
tracting hierarchies based on an inclusion relation 
of appearance patterns from corpora. In this paper, 
we attempted to extract objective hierarchies of 
abstract nouns co-occurring with adjectives in 
Japanese. In our experiment, we showed that com-
plementary similarity measure can extract a kind of 
hierarchy from corpora, though it is a similarity 
measure developed for the recognition of degraded 
machine-printed text. Also, we can find interesting 
hierarchies which suit human intuition, though 
they are different from exact hierarchies. Kanzaki 
et al (2004) have applied our approach to verify 
classification of abstract nouns by using self-
organization map. We can look a suitability of our 
result at that work. 
In our future work, we will use our approach for 
other parts of speech and other types of word. 
Moreover, we will compare with current alterna-
tive approaches such as those based on sentence 
patterns.  
References  
Berland, M. and Charniak, E. 1999. Finding Parts 
in Very Large Corpora, In Proceedings of the 
37th Annual Meeting of the Association for Com-
putational Linguistics, pp.57-64. 
Caraballo, S. A. 1999. Automatic Construction of a 
Hypernym-labeled Noun Hierarchy from Text, 
In Proceedings of the 37th Annual Meeting of the 
Association for Computational Linguistics, 
pp.120-126. 
EDR Electronic Dictionary. 1995. 
 http://www2.nict.go.jp/kk/e416/EDR/index.html 
Hagita, N. and Sawaki, M. 1995. Robust Recogni-
tion of Degraded Machine-Printed Characters us-
ing Complementary Similarity Measure and Er-
ror-Correction Learning?In Proceedings of the 
SPIE ?The International Society for Optical En-
gineering, 2442: pp.236-244. 
Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M., 
and Isahara, H. 2003. Adjectives and their Ab-
stract concepts --- Toward an objective thesaurus 
from Semantic Map. In Proceedings of the Sec-
ond International Workshop on Generative Ap-
proaches to the Lexicon, pp.177-184. 
Kanzaki, K., Ma, Q., Yamamoto, E., Murata, M., 
and Isahara, H. 2004. Extraction of Hyperonymy 
of Adjectives from Large Corpora by using the 
Neural Network Model. In Proceedings of the 
Fourth International Conference on Language 
Resources and Evaluation, Volume II, pp.423-
426. 
Kay, M. 1986. Parsing in Functional Unification 
Grammar. In ?Readings in Natural Language 
Processing?, Grosz, B. J., Spark Jones, K. and 
Webber, B. L., ed., pp.125-138, Morgan Kauf-
mann Publishers, Los Altos, California. 
Manning, C. D. and Schutze, H. 1999. Foundations 
of Statistical Natural Language Processing, The 
MIT Press, Cambridge MA. 
Matsumoto, Y. and Sudo, S., Nakayama, T., and 
Hirao, T. 1996. Thesaurus Construction from 
Multiple Language Resources, In IPSJ SIG 
Notes NL-93, pp.23-28 (In Japanese). 
Miller, A., Beckwith, R., Fellbaum, C., Gros, D., 
Millier, K., and Tengi, R. 1990. Five Papers on 
WordNet, Technical Report CSL Report 43, 
Cognitive Science Laboratory, Princeton Univer-
sity. 
Mosteller, F. and Wallace, D. 1964. Inference and 
Disputed Authorship: The Federalist. Addison-
Wesley, Reading, Massachusetts. 
Nemoto, K. 1969. The combination of the noun 
with ?ga-Case? and the adjective, Language re-
search2 for the computer, National Language 
Research Institute, pp.63-73 (In Japanese). 
Shmid, H-J. 2000. English Abstract Nouns as Con-
ceptual Shells, Mouton de Gruyter. 
Shoutsu, Y., Tokunaga, T., and Tanaka, H. 2003. 
The integration of Japanese dictionary and the-
saurus, In IPSJ SIG Notes NL-153, pp.141-146 
(In Japanese). 
Sparck Jones, K. 1972. A statistical interpretation 
of term specificity and its application in retrieval. 
Journal of Documentation, 28(1): pp.11-21. 
Takahashi, T. 1975. A various phase related to the 
part-whole relation investigated in the sentence, 
Studies in the Japanese language 103, The 
Society of Japanese Linguistics, pp.1-16 (In 
Japanese). 
Tsurumaru, H., Hitaka, T., and Yoshita, S. 1986. 
Automatic extraction of hierarchical relation be-
tween words, In IPSJ SIG Notes NL-83, pp.121-
128 (In Japanese). 
Yamamoto, E. and Umemura, K. 2002. A Similar-
ity Measure for Estimation of One?to-Many Re-
lationship in Corpus, In Journal of Natural Lan-
guage Processing, pp.45-75 (In Japanese). 
Word List by Semantic Principles. 1964. National 
Language Research Institute Publications, Shuei 
Shuppan (In Japanese). 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 117?120, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Organizing English Reading Materials for Vocabulary Learning
Masao Utiyama, Midori Tanimura and Hitoshi Isahara
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto 619-0289 Japan
{mutiyama,mtanimura,isahara}@nict.go.jp
Abstract
We propose a method of organizing read-
ing materials for vocabulary learning. It
enables us to select a concise set of
reading texts (from a target corpus) that
contains all the target vocabulary to be
learned. We used a specialized vocab-
ulary for an English certification test as
the target vocabulary and used English
Wikipedia, a free-content encyclopedia, as
the target corpus. The organized reading
materials would enable learners not only
to study the target vocabulary efficiently
but also to gain a variety of knowledge
through reading. The reading materials
are available on our web site.
1 Introduction
EFL (English as a foreign language) learners and
teachers can easily access a wide range of English
reading materials on the Internet. For example, cur-
rent news stories can be read on web sites such as
those for CNN,1 TIME,2 or the BBC.3 Specialized
reading materials for EFL learners are also provided
on web sites like EFL Reading.4
This situation, however, does not mean that EFL
learners and teachers can easily select proper texts
suited to their specific purposes, for example, learn-
ing vocabulary through reading. On the contrary,
1http://www.cnn.com/
2http://www.time.com/time/
3http://www.bbc.co.uk/
4http://www.gradedreading.pwp.blueyonder.co.uk/
EFL teachers have to carefully select texts, if they
want their students to learn a specialized vocabulary
through reading in a particular discipline such as
medicine, engineering, or economics. However, it is
problematic for teachers to select materials for learn-
ing a target vocabulary with short authentic texts.
It is possible to automate this selection process
given the target vocabulary to be learned and the tar-
get corpus from which texts are gathered (Utiyama
et al, 2004). In this research (Utiyama et al, 2004),
we used a specialized vocabulary for an English
certification test as the target vocabulary and used
newspaper articles from The Daily Yomiuri as the
target corpus. We then organized a set of reading
materials, which we called courseware5, using the
algorithm in Section 2. The courseware consisted
of 116 articles and contained all the target vocabu-
lary. We used the courseware in university English
classes from May 2004 to January 2005. We found
that the courseware was effective in learning vocab-
ulary (Tanimura and Utiyama, in preparation).
Based on the promising results, our next goal is
to distribute courseware (produced with our algo-
rithm) to EFL teachers and learners so that we can
receive wider feedback. To this end, the course-
ware we constructed (Utiyama et al, 2004) is inade-
quate because it was prepared from The Daily Yomi-
uri, which is copyrighted. We therefore replaced
The Daily Yomiuri with English Wikipedia,6 a free-
content encyclopedia, and developed new course-
5Courseware usually includes software in addition to other
materials. However, in this paper, the term courseware is used
to refer to the reading materials only.
6http://en.wikipedia.org/wiki/Main Page
117
ware. It is available on our web site.7
In the following, will we first summarize our al-
gorithm and then describe details on the courseware
we constructed from English Wikipedia.
2 Algorithm
We want to prepare efficient courseware for learning
a target vocabulary. We defined efficiency in terms
of the amount of reading materials that must be read
to learn a required vocabulary. That is, efficient
courseware is as short as possible, while containing
the required vocabulary. We used a greedy method
to develop the efficient courseware (Utiyama et al,
2004).
Let C be the courseware under development and
V be the target vocabulary to be learned. We iter-
atively select a document (from the target corpus)
that has the largest number of new types8 (types con-
tained in V but not in C) and put it into C until C
covering all of V . ?C covers all of V ? means that
each word in V occurs at least once in a document
in C.
More concretely, let Vtodo be the part of V not
covered by C, and let Vdone be V ?Vtodo. We iter-
atively put document d into C that maximizes G(?),
G(d|?, Vtodo, Vdone)
= ?g(d|Vtodo) + (1? ?)g(d|Vdone), (1)
until C covers all of V . We then define g(?) as
g(d|Vx)
= k1 + 1
k1((1? b) + b |W (d)|E(|W (?)|) ) + 1
|W (d) ? Vx|, (2)
where W (d) is the set of types in d, E(|W (?)|) is
the average for |W (?)| over the whole corpus, and
k1 and b are parameters that depend on the corpus.
We set k1 as 1.5 and b as 0.75. g(d|Vx) takes a large
value when there is a large number of common types
between W (d) and Vx and d is short. These effects
are due to |W (d)?Vx| and |W (d)|E(|W (?)|) respectively. As
g(?) is based on the Okapi BM25 function (Robert-
son and Walker, 2000), which has been shown to be
quite efficient in information retrieval,9 we expected
7http://www.kotonoba.net/?mutiyama/vocabridge/
8A type refers to a unique word, while a token refers to each
occurrence of a type.
9BM25 and its variants have been proven to be quite effi-
cient in information retrieval. Readers are referred to papers by
the Text REtrieval Conference (TREC, http://trec.nist.gov/), for
example.
g(?) to be effective in retrieving documents relevant
to the target vocabulary.
In Eq. (1), ? is used to combine the scores of
document d, which are obtained by using Vtodo andVdone. It is defined as
? = |Vdone|1 + |Vdone|
(3)
This implies that even if |W (d) ? Vtodo| is 1, it is
as important as |W (d) ? Vdone| = |Vdone|. Con-
sequently, G(?) uses documents that have new types
of the given vocabulary in preference to documents
that have covered types.
To summarize, efficient courseware is constructed
by putting document d with maximum G(?) into C
until C covers all of V . This allows us to construct
efficient courseware because G(?) takes a large value
when a document has a large number of new types
and is short.
3 Experiment
This section describes how the courseware was con-
structed by applying the method described in the
previous section. We will first describe the vocab-
ulary and corpus used to construct the courseware
and then present the statistics for the courseware.
3.1 Vocabulary
We used the specialized vocabulary used in the
Test of English for International Communication
(TOEIC) because it is one of the most popular En-
glish certification tests in Japan. The vocabulary was
compiled by Chujo (2003) and Chujo et al (2004),
who confirmed that the vocabulary was useful in
preparing for the TOEIC test. The vocabulary had
640 entries and we used 638 words from it that oc-
curred at least once in the corpus as the target vocab-
ulary.
3.2 Corpus
We used articles from English Wikipedia as the tar-
get corpus, which is a free-content encyclopedia that
anyone can edit. The version we used in this study
had 478,611 articles. From these, we first discarded
stub and other non-normal articles. We also dis-
carded short articles of less than 150 words. We then
selected 60,498 articles that were referred to (linked)
by more than 15 articles. This 15-link threshold was
118
set empirically to screen out noisy articles. Finally,
we extracted a 150-word excerpt from the lead part
of each of these 60,498 articles to prepare the target
corpus. We set 150-word limit on an empirical basis
to reduce the burden imposed on learners. In short,
the target corpus consisted of 60,498 excerpts from
the English Wikipedia. In the rest of the paper, we
will use the term an article to refer to an excerpt that
was extracted according to this procedure.
3.3 Example article
Figure 1 has an example of the articles in the course-
ware. It was the first article obtained with the al-
gorithm. It shares 27 types and 49 tokens with the
target vocabulary. These words are printed in bold.
Corporate finance
Corporate finance is the specific area of finance dealing with the fi-
nancial decisions corporations make, and the tools and analysis used
to make the decisions. The discipline as a whole may be divided between
long-term and short-term decisions and techniques. Both share the same
goal of enhancing firm value by ensuring that return on capital exceeds
cost of capital. Capital investment decisions comprise the long-term
choices about which projects receive investment, whether to finance that
investment with equity or debt, and when or whether to pay dividends to
shareholders. Short-term corporate finance decisions are called working
capital management and deal with balance of current assets and cur-
rent liabilities by managing cash, inventories, and short-term borrowing
and lending (e.g., the credit terms extended to customers). Corporate fi-
nance is closely related to managerial finance, which is slightly broader in
scope, describing the financial techniques available to all forms of busi-
ness ... (more)
Figure 1: Example article
3.4 Courseware statistics
3.4.1 Basic courseware statistics
Table 1 lists basic statistics for the courseware
constructed from the target vocabulary and corpus.10
The courseware consisted of 131 articles. Each
article was 150 words long because only excerpts
were used. The average number of tokens per ar-
ticle shared with the vocabulary (?num. of com-
mon tokens? in the Table) was 18.4 and that of
types (?num. of common types?) was 12.4. About
12.3%(= 18.4150 ? 100) of the tokens in each article
were covered by the vocabulary. Each article in the
10On our web site, we prepared 10 sets of article sets called
course-1 to course-10. These 10 courses were obtained by re-
peatedly applying our algorithm to the English Wikipedia re-
moving articles included in earlier courses. The statistics pre-
sented in this paper were calculated from the first courseware,
course-1.
courseware was referred to by 70.7 articles on av-
erage as can be seen from the bottom row. Table
1 indicates that articles in the courseware included
many target words and were heavily referred to by
other articles.
3.4.2 Distribution of covered types
Figure 2 plots the increase in the number of cov-
ered types against the order (ranking) of articles that
were put into the courseware. The horizontal axis
represents the ranking of articles. The vertical axis
indicates the number of covered types. The increase
was sharpest when the ranking value was lowest (left
of figure). The dotted horizontal lines indicate 50%
and 90% of the target vocabulary. These lines cross
the curved solid line at the 22nd and 83rd articles,
i.e., 16.8% and 63.4% of the courseware, respec-
tively. This means that learners can learn most of the
target vocabulary from the beginning of the course-
ware. This is desirable because learners sometimes
do not have enough time to read all the courseware.
0
100
200
300
400
500
600
700
0 20 40 60 80 100 120 140
nu
m.
 of 
typ
es
 
article ranking
90%50%
Figure 2: Increase in the number of covered types
3.4.3 Document frequency distribution
Figure 3 has target words that occurred in eight ar-
ticles or more. The numbers in parentheses indicate
the document frequencies (DFs) of the words, where
the DF of a word is the number of articles in which
the word occurred. These words were the most ba-
sic words in the target vocabulary with respect to the
courseware.
Table 2 lists the distribution of DFs. The first
column lists the different DFs of the target words.
The values in the ?#DF? column are the numbers of
119
Table 1: Basic courseware statistics (number of articles: 131, length of each article: 150 words)
Average SD Min Median Max
Num. of common tokens 18.4 10.8 1 16 55
Num. of common types 12.4 5.5 1 12 27
Num. of incoming links 70.7 145.3 16 32 1056
SD means standard deviation.
words that occurred in the corresponding DF arti-
cles. The ?CUM? and ?CUM%? columns show the
cumulative numbers and percentages of words cal-
culated from the values in the second column. As we
can see from Table 2, more than 50% of the target
words occurred in multiple articles. Consequently,
learners were likely to be sufficiently exposed to ef-
ficiently learn the target vocabulary.
service (19), form (17), information (12), feature (12), op-
eration (11), cost (11), individual (10), department (10),
consumer (9), company (9), product (9), complete (9),
range (9), law (9), associate (9), cause (9), consider (9),
offer (9), provide (9), present (8), activity (8), due (8),
area (8), bill (8), require (8), order (8)
Figure 3: Target words and their DFs.
Table 2: Document frequency distribution
DF #DF CUM CUM%
19 1 1 0.2
17 1 2 0.3
12 2 4 0.6
11 2 6 0.9
10 2 8 1.3
9 11 19 3.0
8 7 26 4.1
7 20 46 7.2
6 25 71 11.1
5 35 106 16.6
4 36 142 22.3
3 71 213 33.4
2 118 331 51.9
1 307 638 100.0
4 Conclusion
While many teachers agree that vocabulary learn-
ing can be fostered by presenting words in context
rather than isolating them from this, it is very dif-
ficult to prepare reading materials that contain the
specialized vocabulary to be learned. We have pro-
posed a method of automating this preparation pro-
cess (Utiyama et al, 2004). We have found that our
reading materials prepared from The Daily Yomiuri
were effective in vocabulary learning (Tanimura and
Utiyama, in preparation).
Our next goal is to distribute courseware (pro-
duced with our algorithm) to EFL teachers and
learners so that we can receive wider feedback. To
this end, we replaced The Daily Yomiuri, which
is copyrighted, with the English Wikipedia, which
is a free-content encyclopedia, and developed new
courseware whose statistics were presented and dis-
cussed in this paper. This courseware, which is
available on our web site, can be used to supplement
classroom learning activities as well as self-study.
We hope it will help EFL learners to learn and teach-
ers to teach a broader range of vocabulary.
References
K. Chujo, T. Ushida, A. Yamazaki, M. Genung, A. Uchi-
bori, and C. Nishigaki. 2004. Bijuaru beishikku
niyoru TOEIC-yoo goiryoku yoosei sofutowuea no
shisaku (3) [The development of English CD-ROM
material to teach vocabulary for the TOEIC test (uti-
lizing Visual Basic): Part 3]. Journal of the College of
Industrial Technology, Nihon University, 37, 29-43.
K. Chujo. 2003. Eigo shokyuushamuke TOEIC Goi 1 &
2 no sentei to sono kouka [Selecting TOEIC vocabu-
lary 1 & 2 for beginning-level students and measuring
its effect on a sample TOEIC test]. Journal of the Col-
lege of Industrial Technology Nihon University, 36:
27-42.
S. E. Robertson and S. Walker. 2000. Okapi/Keenbow at
TREC-8. In Proc. of TREC 8, pages 151?162.
Midori Tanimura and Masao Utiyama. in prepara-
tion. Reading materials for learning TOEIC vocabu-
lary based on corpus data.
Masao Utiyama, Midori Tanimura, and Hitoshi Isahara.
2004. Constructing English reading courseware. In
PACLIC-18, pages 173?179.
120
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 97?104,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Empirical Study of Chinese Chunking
Wenliang Chen, Yujie Zhang, Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
In this paper, we describe an empirical
study of Chinese chunking on a corpus,
which is extracted from UPENN Chinese
Treebank-4 (CTB4). First, we compare
the performance of the state-of-the-art ma-
chine learning models. Then we propose
two approaches in order to improve the
performance of Chinese chunking. 1) We
propose an approach to resolve the spe-
cial problems of Chinese chunking. This
approach extends the chunk tags for ev-
ery problem by a tag-extension function.
2) We propose two novel voting meth-
ods based on the characteristics of chunk-
ing task. Compared with traditional vot-
ing methods, the proposed voting methods
consider long distance information. The
experimental results show that the SVMs
model outperforms the other models and
that our proposed approaches can improve
performance significantly.
1 Introduction
Chunking identifies the non-recursive cores of
various types of phrases in text, possibly as a
precursor to full parsing or information extrac-
tion. Steven P. Abney was the first person
to introduce chunks for parsing(Abney, 1991).
Ramshaw and Marcus(Ramshaw and Marcus,
1995) first represented base noun phrase recog-
nition as a machine learning problem. In 2000,
CoNLL-2000 introduced a shared task to tag
many kinds of phrases besides noun phrases in
English(Sang and Buchholz, 2000). Addition-
ally, many machine learning approaches, such as
Support Vector Machines (SVMs)(Vapnik, 1995),
Conditional Random Fields (CRFs)(Lafferty et
al., 2001), Memory-based Learning (MBL)(Park
and Zhang, 2003), Transformation-based Learn-
ing (TBL)(Brill, 1995), and Hidden Markov Mod-
els (HMMs)(Zhou et al, 2000), have been applied
to text chunking(Sang and Buchholz, 2000; Ham-
merton et al, 2002).
Chinese chunking is a difficult task, and much
work has been done on this topic(Li et al, 2003a;
Tan et al, 2005; Wu et al, 2005; Zhao et al,
2000). However, there are many different Chinese
chunk definitions, which are derived from differ-
ent data sets(Li et al, 2004; Zhang and Zhou,
2002). Therefore, comparing the performance of
previous studies in Chinese chunking is very dif-
ficult. Furthermore, compared with the other lan-
guages, there are some special problems for Chi-
nese chunking(Li et al, 2004).
In this paper, we extracted the chunking corpus
from UPENN Chinese Treebank-4(CTB4). We
presented an empirical study of Chinese chunk-
ing on this corpus. First, we made an evaluation
on the corpus to clarify the performance of state-
of-the-art models in Chinese chunking. Then we
proposed two approaches in order to improve the
performance of Chinese chunking. 1) We pro-
posed an approach to resolve the special prob-
lems of Chinese chunking. This approach ex-
tended the chunk tags for every problem by a tag-
extension function. 2) We proposed two novel vot-
ing methods based on the characteristics of chunk-
ing task. Compared with traditional voting meth-
ods, the proposed voting methods considered long
distance information. The experimental results
showed the proposed approaches can improve the
performance of Chinese chunking significantly.
The rest of this paper is as follows: Section 2
describes the definitions of Chinese chunks. Sec-
97
tion 3 simply introduces the models and features
for Chinese chunking. Section 4 proposes a tag-
extension method. Section 5 proposes two new
voting approaches. Section 6 explains the exper-
imental results. Finally, in section 7 we draw the
conclusions.
2 Definitions of Chinese Chunks
We defined the Chinese chunks based on the CTB4
dataset1. Many researchers have extracted the
chunks from different versions of CTB(Tan et al,
2005; Li et al, 2003b). However, these studies did
not provide sufficient detail. We developed a tool2
to extract the corpus from CTB4 by modifying the
tool Chunklink3.
2.1 Chunk Types
Here we define 12 types of chunks4: ADJP, ADVP,
CLP, DNP, DP, DVP, LCP, LST, NP, PP, QP,
VP(Xue et al, 2000). Table 1 provides definitions
of these chunks.
Type Definition
ADJP Adjective Phrase
ADVP Adverbial Phrase
CLP Classifier Phrase
DNP DEG Phrase
DP Determiner Phrase
DVP DEV phrase
LCP Localizer Phrase
LST List Marker
NP Noun Phrase
PP Prepositional Phrase
QP Quantifier Phrase
VP Verb Phrase
Table 1: Definition of Chunks
2.2 Data Representation
To represent the chunks clearly, we represent the
data with an IOB-based model as the CoNLL00
shared task did, in which every word is to be
tagged with a chunk type label extended with I
(inside a chunk), O (outside a chunk), and B (in-
side a chunk, but also the first word of the chunk).
1More detailed information at
http://www.cis.upenn.edu/ chinese/.
2Tool is available at
http://www.nlplab.cn/chenwl/tools/chunklinkctb.txt.
3Tool is available at http://ilk.uvt.nl/software.html#chunklink.
4There are 15 types in the Upenn Chinese TreeBank. The
other chunk types are FRAG, PRN, and UCP.
Each chunk type could be extended with I or B
tags. For instance, NP could be represented as
two types of tags, B-NP or I-NP. Therefore, we
have 25 types of chunk tags based on the IOB-
based model. Every word in a sentence will be
tagged with one of these chunk tags. For in-
stance, the sentence (word segmented and Part-of-
Speech tagged) ??-NR(He) /??-VV(reached)
/??-NR(Beijing) /??-NN(airport) /?/? will
be tagged as follows:
Example 1:
S1: [NP?][VP??][NP??/??][O?]
S2: ?B-NP /??B-VP /??B-NP /??I-NP /?O /
Here S1 denotes that the sentence is tagged with
chunk types, and S2 denotes that the sentence is
tagged with chunk tags based on the IOB-based
model.
With data representation, the problem of Chi-
nese chunking can be regarded as a sequence tag-
ging task. That is to say, given a sequence of
tokens (words pairing with Part-of-Speech tags),
x = x1, x2, ..., xn, we need to generate a sequence
of chunk tags, y = y1, y2, ..., yn.
2.3 Data Set
CTB4 dataset consists of 838 files. In the ex-
periments, we used the first 728 files (FID from
chtb 001.fid to chtb 899.fid) as training data, and
the other 110 files (FID from chtb 900.fid to
chtb 1078.fid) as testing data. In the following
sections, we use the CTB4 Corpus to refer to the
extracted data set. Table 2 lists details on the
CTB4 Corpus data used in this study.
Training Test
Num of Files 728 110
Num of Sentences 9,878 5,290
Num of Words 238,906 165,862
Num of Phrases 141,426 101,449
Table 2: Information of the CTB4 Corpus
3 Chinese Chunking
3.1 Models for Chinese Chunking
In this paper, we applied four models, includ-
ing SVMs, CRFs, TBL, and MBL, which have
achieved good performance in other languages.
We only describe these models briefly since full
details are presented elsewhere(Kudo and Mat-
sumoto, 2001; Sha and Pereira, 2003; Ramshaw
and Marcus, 1995; Sang, 2002).
98
3.1.1 SVMs
Support Vector Machines (SVMs) is a pow-
erful supervised learning paradigm based on the
Structured Risk Minimization principle from com-
putational learning theory(Vapnik, 1995). Kudo
and Matsumoto(Kudo and Matsumoto, 2000) ap-
plied SVMs to English chunking and achieved
the best performance in the CoNLL00 shared
task(Sang and Buchholz, 2000). They created 231
SVMs classifiers to predict the unique pairs of
chunk tags.The final decision was given by their
weighted voting. Then the label sequence was
chosen using a dynamic programming algorithm.
Tan et al (Tan et al, 2004) applied SVMs to
Chinese chunking. They used sigmoid functions
to extract probabilities from SVMs outputs as the
post-processing of classification. In this paper, we
used Yamcha (V0.33)5 in our experiments.
3.1.2 CRFs
Conditional Random Fields is a powerful se-
quence labeling model(Lafferty et al, 2001) that
combine the advantages of both the generative
model and the classification model. Sha and
Pereira(Sha and Pereira, 2003) showed that state-
of-the-art results can be achieved using CRFs in
English chunking. CRFs allow us to utilize a large
number of observation features as well as differ-
ent state sequence based features and other fea-
tures we want to add. Tan et al (Tan et al, 2005)
applied CRFs to Chinese chunking and their ex-
perimental results showed that the CRFs approach
provided better performance than HMM. In this
paper, we used MALLET (V0.3.2)6(McCallum,
2002) to implement the CRF model.
3.1.3 TBL
Transformation based learning(TBL), first in-
troduced by Eric Brill(Brill, 1995), is mainly
based on the idea of successively transforming the
data in order to correct the error. The transforma-
tion rules obtained are usually few , yet power-
ful. TBL was applied to Chinese chunking by Li
et al(Li et al, 2004) and TBL provided good per-
formance on their corpus. In this paper, we used
fnTBL (V1.0)7 to implement the TBL model.
5Yamcha is available at
http://chasen.org/ taku/software/yamcha/
6MALLET is available at
http://mallet.cs.umass.edu/index.php/Main Page
7fnTBL is available at
http://nlp.cs.jhu.edu/ rflorian/fntbl/index.html
3.1.4 MBL
Memory-based Learning (also called instance
based learning) is a non-parametric inductive
learning paradigm that stores training instances in
a memory structure on which predictions of new
instances are based(Walter et al, 1999). The simi-
larity between the new instance X and example Y
in memory is computed using a distance metric.
Tjong Kim Sang(Sang, 2002) applied memory-
based learning(MBL) to English chunking. MBL
performs well for a variety of shallow parsing
tasks, often yielding good results. In this paper,
we used TiMBL8(Daelemans et al, 2004) to im-
plement the MBL model.
3.2 Features
The observations are based on features that are
able to represent the difference between the two
events. We utilize both lexical and Part-Of-
Speech(POS) information as the features.
We use the lexical and POS information within
a fixed window. We also consider different combi-
nations of them. The features are listed as follows:
? WORD: uni-gram and bi-grams of words in
an n window.
? POS: uni-gram and bi-grams of POS in an n
window.
? WORD+POS: Both the features of WORD
and POS.
where n is a predefined number to denote window
size.
For instance, the WORD features at the 3rd
position (??-NR) in Example 1 (set n as 2):
?? L2 ?? L1 ?? 0 ?? R1 ? R2?(uni-
gram) and ?? ?? LB1?? ?? B0?? ?
? RB1 ?? ? RB2?(bi-gram). Thus features
of WORD have 9 items(5 from uni-gram and
4 from bi-grams). In the similar way, fea-
tures of POS also have 9 items and features of
WORD+POS have 18 items(9+9).
4 Tag-Extension
In Chinese chunking, there are some difficult prob-
lems, which are related to Special Terms, Noun-
Noun Compounds, Named Entities Tagging and
Coordination. In this section, we propose an ap-
proach to resolve these problems by extending the
chunk tags.
8TiMBL is available at http://ilk.uvt.nl/timbl/
99
In the current data representation, the chunk
tags are too generic to construct accurate models.
Therefore, we define a tag-extension function fs
in order to extend the chunk tags as follows:
Te = fs(T,Q) = T ?Q (1)
where, T denotes the original tag set, Q denotes
the problem set, and Te denotes the extended tag
set. For instance, we have an q problem(q ? Q).
Then we extend the chunk tags with q. For NP
Recognition, we have two new tags: B-NP-q and
I-NP-q. Here we name this approach as Tag-
Extension.
In the following three cases study, we demon-
strate that how to use Tag-Extension to resolve the
difficult problems in NP Recognition.
1) Special Terms: this kind of noun phrases
is special terms such as ??/ ??(Life)/ ?
?(Forbidden Zone)/ ?/?, which are bracketed
with the punctuation ??, ?, ?, ?, ?, ??.
They are divided into two types: chunks with these
punctuation and chunks without these punctua-
tion. For instance, ??/ ??/ ??/ ?/? is an
NP chunk (?B-NP/ ??I-NP/ ??I-NP/ ?I-
NP/) while ??/??(forever)/ ??(full-blown)/
?(DE)/???(Chinese Redbud)/?/? is tagged
as (?O/ ??O /??O/ ?O/ ???B-NP/
?O/). We extend the tags with SPE for Special
Terms: B-NP-SPE and I-NP-SPE.
2) Coordination: These problems are related
to the conjunctions ??(and), ?(and), ?(or),
?(and)?. They can be divided into two types:
chunks with conjunctions and chunks without
conjunctions. For instance, ???(HongKong)/
?(and)/??(Macau)/? is an NP chunk (??B-
NP/ ?I-NP/ ??I-NP/), while in ???(least)/
??(salary)/ ?(and)/ ???(living mainte-
nance)/? it is difficult to tell whether ???? is a
shared modifier or not, even for people. We extend
the tags with COO for Coordination: B-NP-COO
and I-NP-COO.
3) Named Entities Tagging: Named Enti-
ties(NE)(Sang and Meulder, 2003) are not dis-
tinguished in CTB4, and they are all tagged as
?NR?. However, they play different roles in
chunks, especial in noun phrases. For instance,
???-NR(Macau)/ ??-NN(Airport)? and ??
?-NR(Hong Kong)/??-NN(Airport)? vs ???
?-NR(Deng Xiaoping)/ ??-NN(Mr.)? and ??
??-NR(Song Weiping) ??-NN(President)?.
Here ???? and ???? are LOCATION, while
????? and ????? are PERSON. To investi-
gate the effect of Named Entities, we use a LOCA-
TION dictionary, which is generated from the PFR
corpus9 of ICL, Peking University, to tag location
words in the CTB4 Corpus. Then we extend the
tags with LOC for this problem: B-NP-LOC and
I-NP-LOC.
From the above cases study, we know the steps
of Tag-Extension. Firstly, identifying a special
problem of chunking. Secondly, extending the
chunk tags via Equation (1). Finally, replacing the
tags of related tokens with new chunk tags. After
Tag-Extension, we use new added chunk tags to
describe some special problems.
5 Voting Methods
Kudo and Matsumoto(Kudo and Matsumoto,
2001) reported that they achieved higher accuracy
by applying voting of systems that were trained
using different data representations. Tjong Kim
Sang et al(Sang and Buchholz, 2000) reported
similar results by combining different systems.
In order to provide better results, we also ap-
ply the voting of basic systems, including SVMs,
CRFs, MBL and TBL. Depending on the charac-
teristics in the chunking task, we propose two new
voting methods. In these two voting methods, we
consider long distance information.
In the weighted voting method, we can assign
different weights to the results of the individ-
ual system(van Halteren et al, 1998). However,
it requires a larger amount of computational ca-
pacity as the training data is divided and is re-
peatedly used to obtain the voting weights. In
this paper, we give the same weight to all ba-
sic systems in our voting methods. Suppose, we
have K basic systems, the input sentence is x =
x1, x2, ..., xn, and the results of K basic systems
are tj = t1j , t2j , ..., tnj , 1 ? j ? K. Then our
goal is to gain a new result y = y1, y2, ..., yn by
voting.
5.1 Basic Voting
This is traditional voting method, which is the
same as Uniform Weight in (Kudo and Mat-
sumoto, 2001). Here we name it as Basic Voting.
For each position, we have K candidates from K
basic systems. After voting, we choose the candi-
date with the most votes as the final result for each
position.
9More information at http://www.icl.pku.edu
100
5.2 Sent-based Voting
In this paper, we treat chunking as a sequence la-
beling task. Here we apply this idea in computing
the votes of one sentence instead of one word. We
name it as Sent-based Voting. For one sentence,
we have K candidates, which are the tagged se-
quences produced by K basic systems. First, we
vote on each position, as done in Basic Voting.
Then we compute the votes of every candidate by
accumulating the votes of each position. Finally,
we choose the candidate with the most votes as
the final result for the sentence. That is to say, we
make a decision based on the votes of the whole
sentence instead of each position.
5.3 Phrase-based Voting
In chunking, one phrase includes one or more
words, and the word tags in one phrase depend on
each other. Therefore, we propose a novel vot-
ing method based on phrases, and we compute the
votes of one phrase instead of one word or one sen-
tence. Here we name it as Phrase-based Voting.
There are two steps in the Phrase-based Voting
procedure. First, we segment one sentence into
pieces. Then we calculate the votes of the pieces.
Table 3 is the algorithm of Phrase-based Voting,
where F (tij , tik) is a binary function:
F (tij , tik) =
{
1 : tij = tik
0 : tij 6= tik (2)
In the segmenting step, we seek the ?O? or ?B-
XP? (XP can be replaced by any type of phrase)
tags, in the results of basic systems. Then we get a
new piece if all K results have the ?O? or ?B-XP?
tags at the same position.
In the voting step, the goal is to choose a result
for each piece. For each piece, we have K candi-
dates. First, we vote on each position within the
piece, as done in Basic Voting. Then we accumu-
late the votes of each position for every candidate.
Finally, we pick the one, which has the most votes,
as the final result for the piece.
The difference in these three voting methods is
that we make the decisions in different ranges: Ba-
sic Voting is at one word; Phrase-based Voting is
in one piece; and Sent-based Voting is in one sen-
tence.
6 Experiments
In this section, we investigated the performance of
Chinese chunking on the CTB4 Corpus.
Input:
Sequence: x = x1, ..., xn;
K results: tj = t1j , ..., tnj , 1 ? j ? K.
Output:
Voted results: y = y1, y2, ..., yn
Segmenting: Segment the sentence into pieces.
Pieces[]=null; begin = 1
For each i in (2, n){
For each j in (1,K)
if(tij is not ?O? and ?B-XP?) break;
if(j > K){
add new piece: p = xbegin, ..., xi?1 into Pieces;
begin = i; }}
Voting: Choose the result with the most votes for each
piece: p = xbegin, ..., xend.
Votes[K] = 0;
For each k in (1,K)
V otes[k] =
?
begin?i?end,1?j?K
F (tij , tik) (3)
kmax = argmax1?k?K(V otes[k]);
Choose tbegin,kmax , ..., tend,kmax as the result for
piece p.
Table 3: Algorithm of Phrase-based Voting
6.1 Experimental Setting
To investigate the chunker sensitivity to the size
of the training set, we generated different sizes of
training sets, including 1%, 2%, 5%, 10%, 20%,
50%, and 100% of the total training data.
In our experiments, we used all the default pa-
rameter settings of the packages. Our SVMs and
CRFs chunkers have a first-order Markov depen-
dency between chunk tags.
We evaluated the results as CONLL2000 share-
task did. The performance of the algorithm was
measured with two scores: precision P and recall
R. Precision measures how many chunks found by
the algorithm are correct and the recall rate con-
tains the percentage of chunks defined in the cor-
pus that were found by the chunking program. The
two rates can be combined in one measure:
F1 = 2? P ?RR+ P (4)
In this paper, we report the results with F1 score.
6.2 Experimental Results
6.2.1 POS vs. WORD+POS
In this experiment, we compared the perfor-
mance of different feature representations, in-
101
 70
 75
 80
 85
 90
 95
 0.01  0.02  0.05  0.1  0.2  0.5  1
F
1
Size of Training data
SVM_WPSVM_PCRF_WPCRF_P
Figure 1: Results of different features
cluding POS and WORD+ POS(See section 3.2),
and set the window size as 2. We also inves-
tigated the effects of different sizes of training
data. The SVMs and CRFs approaches were used
in the experiments because they provided good
performance in chunking(Kudo and Matsumoto,
2001)(Sha and Pereira, 2003).
Figure 1 shows the experimental results, where
xtics denotes the size of the training data, ?WP?
refers to WORD+POS, ?P? refers to POS. We can
see from the figure that WORD+POS yielded bet-
ter performance than POS in the most cases. How-
ever, when the size of training data was small,
the performance was similar. With WORD+POS,
SVMs provided higher accuracy than CRFs in
all training sizes. However, with POS, CRFs
yielded better performance than SVMs in large
scale training sizes. Furthermore, we found SVMs
with WORD+POS provided 4.07% higher accu-
racy than with POS, while CRFs provided 2.73%
higher accuracy.
6.2.2 Comparison of Models
In this experiment, we compared the perfor-
mance of the models, including SVMs, CRFs,
MBL, and TBL, in Chinese chunking. In the ex-
periments, we used the feature WORD+POS and
set the window size as 2 for the first two mod-
els. For MBL, WORD features were within a one-
window size, and POS features were within a two-
window size. We used the original data for TBL
without any reformatting.
Table 4 shows the comparative results of the
models. We found that the SVMs approach was
superior to the other ones. It yielded results that
were 0.72%, 1.51%, and 3.58% higher accuracy
than respective CRFs, TBL, and MBL approaches.
SVMs CRFs TBL MBL
ADJP 84.45 84.55 85.95 80.48
ADVP 83.12 82.74 81.98 77.95
CLP 5.26 0.00 0.00 3.70
DNP 99.65 99.64 99.65 99.61
DP 99.70 99.40 99.70 99.46
DVP 96.77 92.89 99.61 99.41
LCP 99.85 99.85 99.74 99.82
LST 68.75 68.25 56.72 64.75
NP 90.54 89.79 89.82 87.90
PP 99.67 99.66 99.67 99.59
QP 96.73 96.53 96.60 96.40
VP 89.74 88.50 85.75 82.51
+ 91.46 90.74 89.95 87.88
Table 4: Comparative Results of Models
Method Precision Recall F1
CRFs 91.47 90.01 90.74
SVMs 92.03 90.91 91.46
V1 91.97 90.66 91.31
V2 92.32 90.93 91.62
V3 92.40 90.97 91.68
Table 5: Voting Results
Giving more details for each category, the SVMs
approach provided the best results in ten cate-
gories, the CRFs in one category, and the TBL in
five categories.
6.2.3 Comparison of Voting Methods
In this section, we compared the performance of
the voting methods of four basic systems, which
were used in Section 6.2.2. Table 5 shows the
results of the voting systems, where V1 refers
to Basic Voting, V2 refers to Sent-based Voting,
and V3 refers to Phrase-based Voting. We found
that Basic Voting provided slightly worse results
than SVMs. However, by applying the Sent-
based Voting method, we achieved higher accu-
racy than any single system. Furthermore, we
were able to achieve more higher accuracy by ap-
plying Phrase-based Voting. Phrase-based Voting
provided 0.22% and 0.94% higher accuracy than
respective SVMs, CRFs approaches, the best two
single systems.
The results suggested that the Phrase-based Vot-
ing method is quite suitable for chunking task. The
Phrase-based Voting method considers one chunk
as a voting unit instead of one word or one sen-
tence.
102
SVMs CRFs TBL MBL V3
NPR 90.62 89.72 89.89 87.77 90.92
COO 90.61 89.78 90.05 87.80 91.03
SPE 90.65 90.14 90.31 87.77 91.00
LOC 90.53 89.83 89.69 87.78 90.86
NPR* - - - - 91.13
Table 6: Results of Tag-Extension in NP Recogni-
tion
6.2.4 Tag-Extension
NP is the most important phrase in Chinese
chunking and about 47% phrases in the CTB4 Cor-
pus are NPs. In this experiment, we presented the
results of Tag-Extension in NP Recognition.
Table 6 shows the experimental results of Tag-
Extension, where ?NPR? refers to chunking with-
out any extension, ?SPE? refers to chunking
with Special Terms Tag-Extension, ?COO? refers
to chunking with Coordination Tag-Extension,
?LOC? refers to chunking with LOCATION Tag-
Extension, ?NPR*? refers to voting of eight sys-
tems(four of SPE and four of COO), and ?V3?
refers to Phrase-based Voting method.
For NP Recognition, SVMs also yielded the
best results. But it was surprised that TBL pro-
vided 0.17% higher accuracy than CRFs. By ap-
plying Phrase-based Voting, we achieved better re-
sults, 0.30% higher accuracy than SVMs.
From the table, we can see that the Tag-
Extension approach can provide better results. In
COO, TBL got the most improvement with 0.16%.
And in SPE, TBL and CRFs got the same improve-
ment with 0.42%. We also found that Phrase-
based Voting can improve the performance signif-
icantly. NPR* provided 0.51% higher than SVMs,
the best single system.
For LOC, the voting method helped to improve
the performance, provided at least 0.33% higher
accuracy than any single system. But we also
found that CRFs and MBL provided better results
while SVMs and TBL yielded worse results. The
reason was that our NE tagging method was very
simple. We believe NE tagging can be effective
in Chinese chunking, if we use a highly accurate
Named Entity Recognition system.
7 Conclusions
In this paper, we conducted an empirical study of
Chinese chunking. We compared the performance
of four models, SVMs, CRFs, MBL, and TBL.
We also investigated the effects of using different
sizes of training data. In order to provide higher
accuracy, we proposed two new voting methods
according to the characteristics of the chunking
task. We proposed the Tag-Extension approach to
resolve the special problems of Chinese chunking
by extending the chunk tags.
The experimental results showed that the SVMs
model was superior to the other three models.
We also found that part-of-speech tags played an
important role in Chinese chunking because the
gap of the performance between WORD+POS and
POS was very small.
We found that the proposed voting approaches
can provide higher accuracy than any single sys-
tem can. In particular, the Phrase-based Voting ap-
proach is more suitable for chunking task than the
other two voting approaches. Our experimental
results also indicated that the Tag-Extension ap-
proach can improve the performance significantly.
References
Steven P. Abney. 1991. Parsing by chunks. In
Robert C. Berwick, Steven P. Abney, and Carol
Tenny, editors, Principle-Based Parsing: Computa-
tion and Psycholinguistics, pages 257?278. Kluwer,
Dordrecht.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2004. Timbl: Tilburg
memory-based learner v5.1.
James Hammerton, Miles Osborne, Susan Armstrong,
and Walter Daelemans. 2002. Introduction to spe-
cial issue on machine learning approaches to shallow
parsing. JMLR, 2(3):551?558.
Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In In
Proceedings of CoNLL-2000 and LLL-2000, pages
142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In In Proceedings of
NAACL01.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
103
Heng Li, Jonathan J. Webster, Chunyu Kit, and Tian-
shun Yao. 2003a. Transductive hmm based chi-
nese text chunking. In Proceedings of IEEE NLP-
KE2003, pages 257?262, Beijing, China.
Sujian Li, Qun Liu, and Zhifeng Yang. 2003b. Chunk-
ing parsing with maximum entropy principle (in chi-
nese). Chinese Journal of Computers, 26(12):1722?
1727.
Hongqiao Li, Changning Huang, Jianfeng Gao, and Xi-
aozhong Fan. 2004. Chinese chunking with another
type of spec. In The Third SIGHAN Workshop on
Chinese Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Seong-Bae Park and Byoung-Tak Zhang. 2003.
Text chunking by combining hand-crafted rules and
memory-based learning. In ACL, pages 497?504.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
David Yarovsky and Kenneth Church, editors, Pro-
ceedings of the Third Workshop on Very Large Cor-
pora, pages 82?94, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunk-
ing. In Proceedings of CoNLL-2000 and LLL2000,
pages 127?132, Lisbin, Portugal.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003.
Erik F. Tjong Kim Sang. 2002. Memory-based shal-
low parsing. JMLR, 2(3):559?594.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2004. Chinese chunk identification using svms
plus sigmoid. In IJCNLP, pages 527?536.
Yongmei Tan, Tianshun Yao, Qing Chen, and Jingbo
Zhu. 2005. Applying conditional random fields
to chinese shallow parsing. In Proceedings of
CICLing-2005, pages 167?176, Mexico City, Mex-
ico. Springer.
Hans van Halteren, Jakub Zavrel, and Walter Daele-
mans. 1998. Improving data driven wordclass tag-
ging by system combination. In COLING-ACL,
pages 491?497.
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Daelemans Walter, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing.
Shih-Hung Wu, Cheng-Wei Shih, Chia-Wei Wu,
Tzong-Han Tsai, and Wen-Lian Hsu. 2005. Ap-
plying maximum entropy to robust chinese shallow
parsing. In Proceedings of ROCLING2005.
Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony
Kroch. 2000. The bracketing guidelines for the
penn chinese treebank. Technical report, University
of Pennsylvania.
Yuqi Zhang and Qiang Zhou. 2002. Chinese base-
phrases chunking. In Proceedings of The First
SIGHAN Workshop on Chinese Language Process-
ing.
Tiejun Zhao, Muyun Yang, Fang Liu, Jianmin Yao, and
Hao Yu. 2000. Statistics based hybrid approach to
chinese base phrase identification. In Proceedings
of Second Chinese Language Processing Workshop.
GuoDong Zhou, Jian Su, and TongGuan Tey. 2000.
Hybrid text chunking. In Claire Cardie, Walter
Daelemans, Claire Ne?dellec, and Erik Tjong Kim
Sang, editors, Proceedings of the CoNLL00, Lis-
bon, 2000, pages 163?165. Association for Compu-
tational Linguistics, Somerset, New Jersey.
104
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 324?330,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detection of Quotations and Inserted Clauses and its Application
to Dependency Structure Analysis in Spontaneous Japanese
Ryoji Hamabe  Kiyotaka Uchimoto
 School of Informatics,
Kyoto University
Yoshida-honmachi, Sakyo-ku,
Kyoto 606-8501, Japan
Tatsuya Kawahara  Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
Abstract
Japanese dependency structure is usu-
ally represented by relationships between
phrasal units called bunsetsus. One of the
biggest problems with dependency struc-
ture analysis in spontaneous speech is that
clause boundaries are ambiguous. This
paper describes a method for detecting
the boundaries of quotations and inserted
clauses and that for improving the depen-
dency accuracy by applying the detected
boundaries to dependency structure anal-
ysis. The quotations and inserted clauses
are determined by using an SVM-based
text chunking method that considers in-
formation on morphemes, pauses, fillers,
etc. The information on automatically an-
alyzed dependency structure is also used
to detect the beginning of the clauses.
Our evaluation experiment using Corpus
of Spontaneous Japanese (CSJ) showed
that the automatically estimated bound-
aries of quotations and inserted clauses
helped to improve the accuracy of depen-
dency structure analysis.
1 Introduction
The ?Spontaneous Speech: Corpus and Pro-
cessing Technology? project sponsored the con-
struction of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). The CSJ is the
biggest spontaneous speech corpus in the world,
consisting of roughly 7M words with the total
speech length of 700 hours, and is a collection of
monologues such as academic presentations and
simulated public speeches. The CSJ includes tran-
scriptions of the speeches as well as audio record-
ings of them. Approximately one tenth of the
speeches in the CSJ were manually annotated with
various kinds of information such as morphemes,
sentence boundaries, dependency structures, and
discourse structures.
In Japanese sentences, word order is rather
free, and subjects or objects are often omitted.
In Japanese, therefore, the syntactic structure of
a sentence is generally represented by the re-
lationships between phrasal units, or bunsetsus
in Japanese, based on a dependency grammar,
as represented in the Kyoto University text cor-
pus (Kurohashi and Nagao, 1997). In the same
way, the syntactic structure of a sentence is repre-
sented by dependency relationships between bun-
setsus in the CSJ. For example, the sentence ???
?????????? (He is walking slowly) can
be divided into three bunsetsus, ???, kare-wa?
(he), ?????, yukkuri? (slowly), and ????
??, arui-te-iru? (is walking). In this sentence,
the first and second bunsetsus depend on the third
one. The dependency structure is described as fol-
lows.
??????????? (he)
???????????????????? (slowly)
??????????? (is walking)
In this paper, we first describe the problems
with dependency structure analysis of spontaneous
speech. We focus on ambiguous clause boundaries
as the biggest problem and present a solution.
2 Problems with Dependency Structure
Analysis in Spontaneous Japanese
There are many differences between written text
and spontaneous speech, and consequently, prob-
lems peculiar to spontaneous speech arise in de-
324
pendency structure analysis, such as ambiguous
clause boundaries, independent bunsetsus, crossed
dependencies, self-corrections, and inversions. In
this study, we address the problem of ambiguous
clause boundaries in dependency structure analy-
sis in spontaneous speech. We treated the other
problems in the same way as Shitaoka et al (Shi-
taoka et al, 2004). For example, inversions are
represented as dependency relationships going in
the direction from right to left in the CSJ, and their
direction was changed to that from left to right in
our experiments. In this paper, therefore, all the
dependency relationships were assumed to go in
the direction from left to right (Uchimoto et al,
2006).
There are several types of clause boundaries
such as sentence boundaries, boundaries of quo-
tations and inserted clauses. In the CSJ, clause
boundaries were automatically detected by using
surface information (Maruyama et al, 2003), and
sentence boundaries were manually selected from
them (Takanashi et al, 2003). Boundaries of
quotations and inserted clauses were also defined
and detected manually. Dependency relationships
between bunsetsus were annotated within sen-
tences (Uchimoto et al, 2006). Our definition of
clause boundaries follows the definition used in
the CSJ.
Shitaoka et al worked on automatic sen-
tence boundary detection by using SVM-based
text chunking. However, quotations and inserted
clauses were not considered. In this paper, we fo-
cus on these problems in a context of ambiguous
clause boundaries.
Quotations
In written text, quotations are often bracketed by
??(angle brackets), but no brackets are inserted in
spontaneous speech.
ex) ???????????????? (I want to
go there at any rate) is a quotation. In the CSJ,
quotations were manually annotated as follows.
???????????????? (here)
???????????????? (since early times)
???????????????? (once)
???????????????? (at any rate)
???????????????? (want to go)
???????????????? (is the place I think)
Inserted Clauses
In spontaneous speech, speakers insert clauses in
the middle of other clauses. This occurs when
speakers change their speech plans while produc-
Detection of
Sentence Boundary
Dependency Structure
Analysis (Baseline)
Detection of Quotations
and Inserted Clauses
Dependency Structure
Analysis (Enhanced)
word information
filler existence
pause duration
speaking rate
information of dependencies
word information
distance between bunsetsus
(A) + boundaries of quotations
and inserted clauses
...(A)
Figure 1: Outline of proposed processes
ing utterances, which results in supplements, an-
notations, or paraphrases of main clauses.
ex) ???????????? (where I arrived at
night) is an inserted clause.
????????????????? (hotel)
????????????????? (room)
????????????????? (inside)
????????????????? (without delay)
????????????????? (at night)
????????????????? (arrived)
????????????????? (I checked)
Dependency relationships are closed within a
quotation or an inserted clause. Therefore, de-
pendencies except the rightmost bunsetsu in each
clause do not cross boundaries of the same clause,
meaning no dependency exists between the bun-
setsu inside a clause and that outside the clause.
However, automatically detected dependencies of-
ten cross clause boundaries erroneously because
sentences including quotations or inserted clauses
can have complicated clause structures. This is
one of the reasons dependency structure analysis
of spontaneous speech has more errors than that of
written texts. We propose a method for improving
dependency structure analysis based on automatic
detection of quotations and inserted clauses.
3 Dependency Structure Analysis and
Detection of Quotations and Inserted
Clauses
The outline of the proposed processes is shown in
Figure 1. Here, we use ?clause? to describe a quo-
tation and an inserted clause.
3.1 Dependency Structure Analysis
In this research, we use the method proposed by
Uchimoto et al (Uchimoto et al, 2000) to ana-
325
lyze dependency structures. This method is a two-
step procedure, and the first step is preparation of
a dependency matrix in which each element repre-
sents the likelihood that one bunsetsu depends on
another. The second step of the analysis is find-
ing an optimal set of dependencies for the entire
sentence. The likelihood of dependency is repre-
sented by a probability, using a dependency proba-
bility model. The model in this study (Uchimoto et
al., 2000) takes into account not only the relation-
ship between two bunsetsus but also the relation-
ship between the left bunsetsu and all the bunsetsu
to its right.
We implemented this model within a maximum
entropy modeling framework. The features used
in the model were basically attributes related to
the target two bunsetsus: attributes of a bunsetsu
itself, such as character strings, parts of speech,
and inflection types of a bunsetsu together with at-
tributes between bunsetsus, such as the distance
between bunsetsus, etc. Combinations of these
features were also used. In this work, we added
to the features whether there is a boundary of quo-
tations or inserted clauses between the target bun-
setsus. If there is, the probability that the left bun-
setsu depends on the right bunsetsu is estimated to
be low.
In the CSJ, some bunsetsus are defined to have
no modifiee. In our experiments, we defined their
dependencies as follows.
  The rightmost bunsetsu in a quotation or an
inserted clause depends on the rightmost one
in the sentence.
  If a sentence boundary is included in a quo-
tation or an inserted clause, the bunsetsu to
the immediate left of the boundary depends
on the rightmost bunsetsu in the quotation or
the inserted clause.
  Other bunsetsus that have no modifiee de-
pend on the next one.
3.2 Detection of Quotations and Inserted
Clauses
We regard the problem of clause boundary de-
tection as a text chunking task. We used Yam-
Cha (Kudo and Matsumoto, 2001) as a text chun-
ker, which is based on Support Vector Machine
(SVM). We used the chunk labels consisting of
three tags which correspond to sentence bound-
aries, boundaries of quotations, and boundaries of
inserted clauses, respectively. The tag for sentence
Table 1: Tag categories used for chunking
Tag Explanation of tag
B Beginning of a clause
E End of a clause
I Interior of a clause (except B and E)
O Exterior of a clause
S Clause consisting of one bunsetsu
boundaries can be either E (the rightmost bunsetsu
in a sentence) or I (the others). The tags for the
boundaries of quotations and inserted clauses are
shown in Table 1. An example of chunk labels as-
signed to each bunsetsu in a sentence is as follows.
ex) ???????? (It is because of the budget)
is a quotation, and ??????????????
(which I think is because of the budget) is an in-
serted clause. For a chunk label, for example, the
bunsetsu that the chunk label (I, B, B) is assigned
to means that it is not related to a sentence bound-
ary but is related to the beginning of a quotation
and an inserted clause.
(I,O,O)??????????????? (now)
(I,B,B)?????? ? (budget)
(I,E,I)??????????????? (because of)
(I,O,E)??????????????? (I think)
(I,O,O)??????????????? (in summer)
(I,O,O)??????????????? (three times)
(E,O,O)??????????????? (they do it)
The three tags of each chunk label are simulta-
neously estimated. Therefore, the relationships
between sentence boundaries, quotations, and in-
serted clauses are considered in this model. For in-
stance, quotations and inserted clauses should not
cross the sentence boundaries, and the chunk label
such as (E,I,O) is never estimated because this la-
bel means that a sentence boundary exists within a
quotation.
We used the following parameters for YamCha.
  Degree of polynomial kernel: 3rd
  Analysis direction: Right to left
  Dynamic features: Following three chunk la-
bels
  Multi-class method: Pairwise
The chunk label is estimated for each bunsetsu,
The features used to estimate the chunk labels are
as follows.
(1) word information We used word information
such as character strings, pronunciation, part
of speech, inflection type, and inflection
form. Specific expressions are often used at
the ends of quotations and inserted clauses.
326
B?
E
(1) No bunsetsu to left of B
depends on bunsetsu between B and E
?
B
?
E
(2) Bunsetsu to immediate left of B
depends on bunsetsu to right of E
?
?
Figure 2: Dependency structures of bunsetsus to
left of beginning of quotations or inserted clauses
For instance, ????, to-omou? (think) and
?????, tte-iu? (say) are used at the ends
of quotations. Expressions such as ???
?, desu-ga? and ?????, keredo-mo? are
used at the ends of inserted clauses.
(2) fillers and pauses Fillers and pauses are often
inserted just before or after quotations and in-
serted clauses. Pause duration is normalized
in a talk with its mean and variance.
(3) speaking rate Inside inserted clauses, speak-
ers tend to speak fast. The speaking rate is
also normalized in a talk.
Detecting the ends of clauses appears easy be-
cause specific expressions are frequently used at
the ends of clauses as previously mentioned. How-
ever, determining the beginnings of clauses is dif-
ficult in a single process because all features men-
tioned above are local information. Therefore, the
global information is also used to detect the begin-
ning of the clauses. If the end of a clause is given,
the bunsetsus to the left of the clause should sat-
isfy the two conditions described in Figure 2. Our
method uses the constraint as global information.
They are considered as additional features based
on dependency probabilities estimated for the bun-
setsus to the left of the clause. Thus, our chunk-
ing method has two steps. First, clause boundaries
are detected based on the three types of features
itemized above. Second, the beginnings of clauses
are determined after adding to the features the fol-
lowing probabilities obtained by automatic depen-
dency structure analysis.
(4) probability that bunsetsu to left of target de-
pends on bunsetsu inside clause
(5) probability that bunsetsu to immediate left
of target depends on bunsetsu to right of clause
Figure 2 shows that the target bunsetsu is likely
to be the beginning of the clause if probability (4)
is low and probability (5) is high. For instance,
the following example sentence has an inserted
clause. In the first chunking step, the bunsetsu
????????? (which is a story) is found to
be the end of the inserted clause.
ex) ??????????????? (which is a
story that I heard from my father) is an inserted
clause.
????????????????? (this)
????????????????? (area)
????????????????? (from my father)
????????????????? (heard)
????????????????? (story)
????????????????? (in the old days)
????????????????? (was a rice field)
The three bunsetsus ????, atari-wa?, ???
?, kii-ta?, and ????????, hanashi-na-
ndesu-kedo? are less likely to be the beginning
of the inserted clause because in the three cases
the bunsetsu to the immediate left depends on the
target bunsetsu. On the other hand, the bunsetsu
????, chichi-kara? is the most likely to be the
beginning since the bunsetsu to its immediate left
????, atari-wa? depends on the bunsetsu to the
right of the inserted clause ??????????,
tanbo-datta-ndesu?.
4 Experiments and Discussion
For experimental evaluation, we used the tran-
scriptions of 188 talks in the CSJ, which contain
6,255 quotations and 818 inserted clauses. We
used 20 talks for testing. The test data included
643 quotations and 76 inserted clauses. For train-
ing, we used 168 talks excluding the test data to
conduct the open test and all the 188 talks to con-
duct the closed test.
First, we detected sentence boundaries by using
the method (Shitaoka et al, 2004) and analyzed
the dependency structure of each sentence by the
method described in Section 3.1 without using in-
formation on quotations and inserted clauses. We
obtained an F-measure of 85.9 for the sentence
boundary detection, and the baseline accuracy of
the dependency structure analysis was 77.7% for
the open test and 86.5% for the closed test.
327
(a) Results of clause boundary detection
The results obtained by the method described in
Section 3.2 are shown in Table 2. The table shows
five kinds of results:
  results obtained without dependency struc-
ture (in the first chunking step)
  results obtained with dependency structure
analyzed for the open test (in the second
chunking step)
  results obtained with dependency structure
analyzed for the closed test (in the second
chunking step)
  results obtained with manually annotated de-
pendency structure (in the second chunking
step)
  the rate that the ends of clauses are detected
correctly
These results indicate that around 90% of quo-
tations were detected correctly, and the boundary
detection accuracy of quotations was improved by
using automatically analyzed dependency struc-
ture. We found that features (4) and (5) in Section
3.2 obtained from automatically analyzed depen-
dency structure contributed to the improvement.
In the following example, a part of the quotation
?????????????? (my good virtue)
was erroneously detected as a quotation in the first
chunking step. But, in the second chunking step,
automatically analyzed dependency structure con-
tributed to detection of the correct part ?????
???????????? (this is my good virtue)
as a quotation.
????????????????? (this)
????????????????? (my)
????????????????? (good)
????????????????? (virtue)
????????????????? (I)
????????????????? (think)
We also found that the boundary detection accu-
racy of quotations was significantly improved by
using manually annotated dependency structure.
This indicates that the boundary detection accu-
racy of quotations improves as the accuracy of de-
pendency structure analysis improves.
By contrast, only a few inserted clauses were
detected even if dependency structures were used.
Most of the ends of the inserted clauses were de-
tected incorrectly as sentence boundaries. The
main reason for this is our method could not distin-
guish between the ends of the inserted clauses and
those of the sentences, since the same words often
appeared at the ends of both, and it was difficult
Table 2: Clause boundary detection results (sen-
tence boundaries automatically detected)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
41.1% 44.3% 42.6 1.3% 20.0% 2.5
(264/643) (264/596) (1/76) (1/5)
With dependency information (open)
42.1% 45.5% 43.7 2.6% 40.0% 4.9
(271/643) (271/596) (2/76) (2/5)
With dependency information (closed)
50.9% 54.9% 52.8 2.6% 40.0% 4.9
(327/643) (327/596) (2/76) (2/5)
With dependency information (correct)
74.2% 80.0% 77.0 2.6% 33.3% 4.9
(477/643) (477/596) (2/76) (2/6)
Correct end of clauses
89.1% 96.1% 92.5 2.6% 40.0% 4.9
(573/643) (573/596) (2/76) (2/5)
Table 3: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries automatically detected)
Without boundaries of quotations open 77.7%
and inserted clauses closed 86.5%
With boundaries of quotations and open 78.5%
inserted clauses (automatically detected) closed 86.6%
With boundaries of quotations and open 79.4%
inserted clauses (correct) closed 87.4%
to learn the difference between them even though
our method used features based on acoustic infor-
mation.
(b) Dependency structure analysis results
We investigated the accuracies of dependency
structure analysis obtained when the automatically
or manually detected boundaries of quotations and
inserted clauses were used. The results are shown
in Table 3. Although the accuracy of detecting the
boundaries of quotations and inserted clauses us-
ing automatically analyzed dependency structure
was not high, the accuracy of dependency struc-
ture analysis was improved by 0.7% absolute for
the open test. This shows that the model for depen-
dency structure analysis could robustly learn use-
ful information on clause boundaries even if errors
were included in the results of clause boundary de-
tection. In the following example, for instance,
????????????? (to go out with its
face stuck) was correctly detected as a quotation
in the first chunking step. Then, the initial in-
appropriate modifiee ??????, oboe-te-ki-te?
(learn) of the bunsetsu inside the quotation ???
?, hasan-de? (stick) was correctly modified to the
bunsetsu inside the quotation ?????????,
de-te-shimau-to-iu? (to go) by using the automati-
cally detected boundary of the quotation.
328
??????????????????? (face)
??????????????????? (stick)
??????????????????? (out)
??????????????????? (to go)
??????????????????? (stunt)
??????????????????? (somewhere)
??????????????????? (learn)
(c) Results obtained when correct sentence
boundaries are given
We investigated the clause boundary detection
accuracy of quotations and inserted clauses and
the dependency accuracy when correct sentence
boundaries were given manually. The results are
shown in Tables 4 and 5, respectively.
When correct sentence boundaries were given,
the accuracy of clause detection and dependency
structure analysis was improved significantly. Ta-
ble 4 shows that the boundary detection accuracy
of inserted clauses as well as that of quotations
was significantly improved by using information
of dependencies. Table 5 indicates that when us-
ing automatically detected clause boundaries, the
accuracy of dependency structure analysis was im-
proved by 0.7% for the open test, and it was further
improved by using correct clause boundaries.
These experimental results show that detecting
the boundaries of quotations and inserted clauses
as well as sentence boundaries is sensitive to the
accuracy of dependency structure analysis and the
improvements of the boundary detection of quo-
tations and inserted clauses contribute to improve-
ment of dependency structure analysis. Especially,
the difference between Table3 and 5 shows that
the sentence boundary detection accuracy is more
sensitive to the accuracy of dependency structure
analysis than the boundary detection accuracy of
quotations and inserted clauses. This indicates that
sentence boundaries rather than quotations and in-
serted clauses should be manually examined first
to effectively improve the accuracy of dependency
structure analysis in a semi-automatic way.
5 Conclusion
This paper described the method for detecting the
boundaries of quotations and inserted clauses and
that for applying it to dependency structure analy-
sis. The experiment results showed that the auto-
matically estimated boundaries of quotations and
inserted clauses contributed to improvement of de-
pendency structure analysis. In the future, we plan
to solve the problems found in the experiments and
investigate the robustness of our method when the
Table 4: Clause boundary detection results (sen-
tence boundaries given)
Quotations Inserted clauses
recall precision F recall precision F
Without dependency information
46.0% 50.8% 48.3 22.4% 23.6% 23.0
(296/643) (296/583) (17/76) (17/72)
With dependency information (open)
46.7% 53.3% 49.8 30.3% 38.3% 33.8
(300/643) (300/563) (23/76) (23/60)
With dependency information (closed)
55.1% 62.9% 58.7 30.3% 39.0% 34.1
(354/643) (354/563) (23/76) (23/59)
With dependency information (correct)
75.3% 86.0% 80.3 46.1% 60.3% 52.2
(484/643) (484/563) (35/76) (35/58)
Correct end of clauses
86.5% 95.4% 90.7 64.5% 68.1% 66.2
(556/643) (556/583) (49/76) (49/72)
Table 5: Dependency structure analysis results ob-
tained with clause boundaries (sentence bound-
aries given)
Without boundaries of quotations open 81.0%
and inserted clauses closed 90.3%
With boundaries of quotations and open 81.7%
inserted clauses (automatically detected) closed 90.3%
With boundaries of quotations open 82.8%
and inserted clauses (correct) closed 91.3%
results of automatic speech recognition are given
as the inputs. We will also study use of informa-
tion on quotations and inserted clauses to text for-
matting, such as text summarization.
References
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL.
Sadao Kurohashi and Makoto Nagao. 1997. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. In Proceedings of the NLPRS, pages
451?456.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous Speech Corpus of
Japanese. In Proceedings of the LREC2000, pages
947?952.
Takehiko Maruyama, Hideki Kashioka, Tadashi Ku-
mano, and Hideki Tanaka. 2003. Rules for Auto-
matic Clause Boundary Detection and Their Evalu-
ation. In Proceedings of the Nineth Annual Meeting
of the Association for Natural Language proceeding,
pages 517?520. (in Japanese).
Katsuya Takanashi, Takehiko Maruyama, Kiy-
otaka Uchimoto, and Hitoshi Isahara. 2003.
Identification of ?Sentences? in Spontaneous
329
Japanese ? Detection and Modification of Clause
Boundaries ?. In Proceedings of the ISCA & IEEE
Workshop on Spontaneous Speech Processing and
Recognition, pages 183?186.
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency Model Us-
ing Posterior Context. In Proceedings of the IWPT,
pages 321?322.
Kiyotaka Uchimoto, Ryoji Hamabe, Take-
hiko Maruyama, Katsuya Takanashi, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2006. Dependency-
structure Annotation to Corpus of Spontaneous
Japanese. In Proceedings of the LREC2006, pages
635-638.
Kazuya Shitaoka, Kiyotaka Uchimoto, Tatsuya Kawa-
hara, and Hitoshi Isahara. 2004. Dependency Struc-
ture Analysis and Sentence Boundary Detection in
Spontaneous Japanese. In Proceedings of the COL-
ING2004, pages 1107?1113.
330
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 587?594,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine-Learning-Based Transformation of Passive Japanese Sentences
into Active by Separating Training Data into Each Input Particle
Masaki Murata
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
murata@nict.go.jp
Tamotsu Shirado
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
shirado@nict.go.jp
Toshiyuki Kanamaru
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
kanamaru@nict.go.jp
Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We developed a new method of transform-
ing Japanese case particles when trans-
forming Japanese passive sentences into
active sentences. It separates training data
into each input particle and uses machine
learning for each particle. We also used
numerous rich features for learning. Our
method obtained a high rate of accuracy
(94.30%). In contrast, a method that did
not separate training data for any input
particles obtained a lower rate of accu-
racy (92.00%). In addition, a method
that did not have many rich features for
learning used in a previous study (Mu-
rata and Isahara, 2003) obtained a much
lower accuracy rate (89.77%). We con-
firmed that these improvements were sig-
nificant through a statistical test. We
also conducted experiments utilizing tra-
ditional methods using verb dictionar-
ies and manually prepared heuristic rules
and confirmed that our method obtained
much higher accuracy rates than tradi-
tional methods.
1 Introduction
This paper describes how passive Japanese sen-
tences can be automatically transformed into ac-
tive. There is an example of a passive Japanese
sentence in Figure 1. The Japanese suffix reta
functions as an auxiliary verb indicating the pas-
sive voice. There is a corresponding active-voice
sentence in Figure 2. When the sentence in Fig-
ure 1 is transformed into an active sentence, (i) ni
(by), which is a case postpositional particle with
the meaning of ?by?, is changed into ga, which is
a case postpositional particle indicating the sub-
jective case, and (ii) ga (subject), which is a
case postpositional particle indicating the subjec-
tive case, is changed into wo (object), which is
a case postpositional particle indicating the objec-
tive case. In this paper, we discuss the transfor-
mation of Japanese case particles (i.e., ni ? ga)
through machine learning.1
The transformation of passive sentences into ac-
tive is useful in many research areas including
generation, knowledge extraction from databases
written in natural languages, information extrac-
tion, and answering questions. For example, when
the answer is in the passive voice and the ques-
tion is in the active voice, a question-answering
system cannot match the answer with the question
because the sentence structures are different and
it is thus difficult to find the answer to the ques-
tion. Methods of transforming passive sentences
into active are important in natural language pro-
cessing.
The transformation of case particles in trans-
forming passive sentences into active is not easy
because particles depend on verbs and their use.
We developed a new method of transforming
Japanese case particles when transforming pas-
sive Japanese sentences into active in this study.
Our method separates training data into each in-
put particle and uses machine learning for each in-
put particle. We also used numerous rich features
for learning. Our experiments confirmed that our
method was effective.
1In this study, we did not handle the transformation of
auxiliary verbs and the inflection change of verbs because
these can be transformed based on Japanese grammar.
587
inu ni watashi ga kama- reta.
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 1: Passive sentence
inu ni watashi ga kama- reta.
ga wo
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 3: Example in corpus
inu ga watashi wo kanda.
(dog) subject (I) object (bite)
(Dog bit me.)
Figure 2: Active sentence
2 Tagged corpus as supervised data
We used the Kyoto University corpus (Kurohashi
and Nagao, 1997) to construct a corpus tagged for
the transformation of case particles. It has ap-
proximately 20,000 sentences (16 editions of the
Mainichi Newspaper, from January 1st to 17th,
1995). We extracted case particles in passive-
voice sentences from the Kyoto University cor-
pus. There were 3,576 particles. We assigned a
corresponding case particle for the active voice to
each case particle. There is an example in Figure
3. The two underlined particles, ?ga? and ?wo?
that are given for ?ni? and ?ga? are tags for case
particles in the active voice. We called the given
case particles for the active voice target case par-
ticles, and the original case particles in passive-
voice sentences source case particles. We created
tags for target case particles in the corpus. If we
can determine the target case particles in a given
sentence, we can transform the case particles in
passive-voice sentences into case particles for the
active voice. Therefore, our goal was to determine
the target case particles.
3 Machine learning method (support
vector machine)
We used a support vector machine as the basis
of our machine-learning method. This is because
support vector machines are comparatively better
than other methods in many research areas (Kudoh
and Matsumoto, 2000; Taira and Haruno, 2001;
Small Margin Large Margin
Figure 4: Maximizing margin
Murata et al, 2002).
Data consisting of two categories were classi-
fied by using a hyperplane to divide a space with
the support vector machine. When these two cat-
egories were, positive and negative, for example,
enlarging the margin between them in the train-
ing data (see Figure 42), reduced the possibility of
incorrectly choosing categories in blind data (test
data). A hyperplane that maximized the margin
was thus determined, and classification was done
using that hyperplane. Although the basics of this
method are as described above, the region between
the margins through the training data can include
a small number of examples in extended versions,
and the linearity of the hyperplane can be changed
to non-linear by using kernel functions. Classi-
fication in these extended versions is equivalent
to classification using the following discernment
function, and the two categories can be classified
on the basis of whether the value output by the
function is positive or negative (Cristianini and
Shawe-Taylor, 2000; Kudoh, 2000):
2The open circles in the figure indicate positive examples
and the black circles indicate negative. The solid line indi-
cates the hyperplane dividing the space, and the broken lines
indicate the planes depicting margins.
588
f(x) = sgn
(
l
?
i=1
?iyiK(xi,x) + b
)
(1)
b =
maxi,y
i
=?1
bi + mini,y
i
=1
bi
2
bi = ?
l
?
j=1
?jyjK(xj,xi),
where x is the context (a set of features) of an in-
put example, xi indicates the context of a training
datum, and yi (i = 1, ..., l, yi ? {1,?1}) indicates
its category. Function sgn is:
sgn(x) = 1 (x ? 0), (2)
?1 (otherwise).
Each ?i (i = 1, 2...) is fixed as a value of ?i that
maximizes the value of L(?) in Eq. (3) under the
conditions set by Eqs. (4) and (5).
L(?) =
l
?
i=1
?
i
?
1
2
l
?
i,j=1
?
i
?
j
y
i
y
j
K(x
i
,x
j
) (3)
0 ? ?
i
? C (i = 1, ..., l) (4)
l
?
i=1
?
i
y
i
= 0 (5)
Although function K is called a kernel function
and various functions are used as kernel functions,
we have exclusively used the following polyno-
mial function:
K(x,y) = (x ? y + 1)d (6)
C and d are constants set by experimentation. For
all experiments reported in this paper, C was fixed
as 1 and d was fixed as 2.
A set of xi that satisfies ?i > 0 is called a sup-
port vector, (SVs)3, and the summation portion of
Eq. (1) is only calculated using examples that are
support vectors. Equation 1 is expressed as fol-
lows by using support vectors.
f(x) = sgn
?
?
?
i:x
i
?SV
s
?iyiK(xi,x) + b
?
?(7)
b =
bi:y
i
=?1,x
i
?SV
s
+ bi:y
i
=1,x
i
?SV
s
2
bi = ?
?
i:x
i
?SV
s
?jyjK(xj ,xi),
3The circles on the broken lines in Figure 4 indicate sup-
port vectors.
Table 1: Features
F1 part of speech (POS) of P
F2 main word of P
F3 word of P
F4 first 1, 2, 3, 4, 5, and 7 digits of category number
of P5
F5 auxiliary verb attached to P
F6 word of N
F7 first 1, 2, 3, 4, 5, and 7 digits of category number
of N
F8 case particles and words of nominals that have de-
pendency relationship with P and are other than
N
F9 first 1, 2, 3, 4, 5, and 7 digits of category num-
ber of nominals that have dependency relationship
with P and are other than N
F10 case particles of nominals that have dependency
relationship with P and are other than N
F11 the words appearing in the same sentence
F12 first 3 and 5 digits of category number of words
appearing in same sentence
F13 case particle taken by N (source case particle)
F14 target case particle output by KNP (Kurohashi,
1998)
F15 target case particle output with Kondo?s method
(Kondo et al, 2001)
F16 case patterns defined in IPAL dictionary (IPAL)
(IPA, 1987)
F17 combination of predicate semantic primitives de-
fined in IPAL
F18 predicate semantic primitives defined in IPAL
F19 combination of semantic primitives of N defined
in IPAL
F20 semantic primitives of N defined in IPAL
F21 whether P is defined in IPAL or not
F22 whether P can be in passive form defined in
VDIC6
F23 case particles of P defined in VDIC
F24 type of P defined in VDIC
F25 transformation rule used for P and N in Kondo?s
method
F26 whether P is defined in VDIC or not
F27 pattern of case particles of nominals that have de-
pendency relationship with P
F28 pair of case particles of nominals that have depen-
dency relationship with P
F29 case particles of nominals that have dependency
relationship with P and appear before N
F30 case particles of nominals that have dependency
relationship with P and appear after N
F31 case particles of nominals that have dependency
relationship with P and appear just before N
F32 case particles of nominals that have dependency
relationship with P and appear just after N
589
Table 2: Frequently occurring target case particles in source case particles
Source case particle Occurrence rate Frequent target case Occurrence rate
particles in in
source case particles source case particles
ni (indirect object) 27.57% (493/1788) ni (indirect object) 70.79% (349/493)
ga (subject) 27.38% (135/493)
ga (subject) 26.96% (482/1788) wo (direct object) 96.47% (465/482)
de (with) 17.17% (307/1788) ga (subject) 79.15% (243/307)
de (with) 13.36% (41/307)
to (with) 16.11% (288/1788) to (with) 99.31% (286/288)
wo (direct object) 6.77% (121/1788) wo (direct object) 99.17% (120/121)
kara (from) 4.53% ( 81/1788) ga (subject) 49.38% ( 40/ 81)
kara (from) 44.44% ( 36/ 81)
made (to) 0.78% ( 14/1788) made (to) 100.00% ( 14/ 14)
he (to) 0.06% ( 1/1788) ga (subject) 100.00% ( 1/ 1)
no (subject) 0.06% ( 1/1788) wo (direct object) 100.00% ( 1/ 1)
Support vector machines are capable of han-
dling data consisting of two categories. Data con-
sisting of more than two categories is generally
handled using the pair-wise method (Kudoh and
Matsumoto, 2000).
Pairs of two different categories (N(N-1)/2
pairs) are constructed for data consisting of N cat-
egories with this method. The best category is de-
termined by using a two-category classifier (in this
paper, a support vector machine4 is used as the
two-category classifier), and the correct category
is finally determined on the basis of ?voting? on
the N(N-1)/2 pairs that result from analysis with
the two-category classifier.
The method discussed in this paper is in fact a
combination of the support vector machine and the
pair-wise method described above.
4 Features (information used in
classification)
The features we used in our study are listed in Ta-
ble 1, where N is a noun phrase connected to the
4We used Kudoh?s TinySVM software (Kudoh, 2000) as
the support vector machine.
5The category number indicates a semantic class of
words. A Japanese thesaurus, the Bunrui Goi Hyou (NLRI,
1964), was used to determine the category number of each
word. This thesaurus is ?is-a? hierarchical, in which each
word has a category number. This is a 10-digit number that
indicates seven levels of ?is-a? hierarchy. The top five lev-
els are expressed by the first five digits, the sixth level is ex-
pressed by the next two digits, and the seventh level is ex-
pressed by the last three digits.
6Kondo et al constructed a rich dictionary for Japanese
verbs (Kondo et al, 2001). It defined types and characteris-
tics of verbs. We will refer to it as VDIC.
case particle being analyzed, and P is the phrase?s
predicate. We used the Japanese syntactic parser,
KNP (Kurohashi, 1998), for identifying N, P, parts
of speech and syntactic relations.
In the experiments conducted in this study, we
selected features. We used the following proce-
dure to select them.
? Feature selection
We first used all the features for learning. We
next deleted only one feature from all the fea-
tures for learning. We did this for every fea-
ture. We decided to delete features that would
make the most improvement. We repeated
this until we could not improve the rate of ac-
curacy.
5 Method of separating training data
into each input particle
We developed a new method of separating train-
ing data into each input (source) particle that uses
machine learning for each particle. For example,
when we identify a target particle where the source
particle is ni, we use only the training data where
the source particle is ni. When we identify a tar-
get particle where the source particle is ga, we use
only the training data where the source particle is
ga.
Frequently occurring target case particles are
very different in source case particles. Frequently
occurring target case particles in all source case
particles are listed in Table 2. For example, when
ni is a source case particle, frequently occurring
590
Table 3: Occurrence rates for target case particles
Target case Occurrence rate
particle Closed Open
wo (direct object) 33.05% 29.92%
ni (indirect object) 19.69% 17.79%
to (with) 16.00% 18.90%
de (with) 13.65% 15.27%
ga (subject) 11.07% 10.01%
ga or de 2.40% 2.46%
kara (from) 2.13% 3.47%
Other 2.01% 1.79%
target case particles are ni or ga. In contrast, when
ga is a source case particle, a frequently occurring
target case particle is wo.
In this case, it is better to separate training data
into each source particle and use machine learn-
ing for each particle. We therefore developed this
method and confirmed that it was effective through
experiments (Section 6).
6 Experiments
6.1 Basic experiments
We used the corpus we constructed described in
Section 2 as supervised data. We divided the su-
pervised data into closed and open data (Both the
closed data and open data had 1788 items each.).
The distribution of target case particles in the data
are listed in Table 3. We used the closed data to
determine features that were deleted in feature se-
lection and used the open data as test data (data
for evaluation). We used 10-fold cross validation
for the experiments on closed data and we used
closed data as the training data for the experiments
on open data. The target case particles were deter-
mined by using the machine-learning method ex-
plained in Section 3. When multiple target parti-
cles could have been answers in the training data,
we used pairs of them as answers for machine
learning.
The experimental results are listed in Tables 4
and 5. Baseline 1 outputs a source case particle
as the target case particle. Baseline 2 outputs the
most frequent target case particle (wo (direct ob-
ject)) in the closed data as the target case particle
in every case. Baseline 3 outputs the most fre-
quent target case particle for each source target
case particle in the closed data as the target case
particle. For example, ni (indirect object) is the
most frequent target case particle when the source
case particle is ni, as listed in Table 2. Baseline 3
outputs ni when the source case particle is ni. KNP
indicates the results that the Japanese syntactic
parser, KNP (Kurohashi, 1998), output. Kondo in-
dicates the results that Kondo?s method, (Kondo et
al., 2001), output. KNP and Kondo can only work
when a target predicate is defined in the IPAL dic-
tionary or the VDIC dictionary. Otherwise, KNP
and Kondo output nothing. ?KNP/Kondo + Base-
line X? indicates the use of outputs by Baseline
X when KNP/Kondo have output nothing. KNP
and Kondo are traditional methods using verb dic-
tionaries and manually prepared heuristic rules.
These traditional methods were used in this study
to compare them with ours. ?Murata 2003? indi-
cates results using a method they developed in a
previous study (Murata and Isahara, 2003). This
method uses F1, F2, F5, F6, F7, F10, and F13 as
features and does not have training data for any
source case particles. ?Division? indicates sepa-
rating training data into each source particle. ?No-
division? indicates not separating training data for
any source particles. ?All features? indicates the
use of all features with no features being selected.
?Feature selection? indicates features are selected.
We did two kinds of evaluations: ?Eval. A? and
?Eval. B?. There are some cases where multiple
target case particles can be answers. For example,
ga and de can be answers. We judged the result to
be correct in ?Eval. A? when ga and de could be
answers and the system output the pair of ga and
de as answers. We judged the result to be correct
in ?Eval. B? when ga and de could be answers and
the system output ga, de, or the pair of ga and de
as answers.
Table 4 lists the results using all data. Table 5
lists the results where a target predicate is defined
in the IPAL and VDIC dictionaries. There were
551 items in the closed data and 539 in the open.
We found the following from the results.
Although selection of features obtained higher
rates of accuracy than use of all features in the
closed data, it did not obtain higher rates of accu-
racy in the open data. This indicates that feature
selection was not effective and we should have
used all features in this study.
Our method using all features in the open data
and separating training data into each source parti-
cle obtained the highest rate of accuracy (94.30%
in Eval. B). This indicates that our method is ef-
591
Table 4: Experimental results
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 58.67% 61.41% 62.02% 64.60%
Baseline 2 33.05% 33.56% 29.92% 30.37%
Baseline 3 84.17% 88.20% 84.17% 88.20%
KNP 27.35% 28.69% 27.91% 29.14%
KNP + Baseline 1 64.32% 67.06% 67.79% 70.36%
KNP + Baseline 2 48.10% 48.99% 45.97% 46.48%
KNP + Baseline 3 81.21% 84.84% 80.82% 84.45%
Kondo 39.21% 40.88% 39.32% 41.00%
Kondo + Baseline 1 65.27% 68.57% 67.34% 70.41%
Kondo + Baseline 2 54.87% 56.54% 53.52% 55.26%
Kondo + Baseline 3 78.08% 81.71% 78.30% 81.88%
Murata 2003 86.86% 89.09% 87.86% 89.77%
Our method, no-division + all features 89.99% 92.39% 90.04% 92.00%
Our method, no-division + feature selection 91.28% 93.40% 90.10% 92.00%
Our method, division + all features 91.22% 93.79% 92.28% 94.30%
Our method, division + feature selection 92.06% 94.41% 91.89% 93.85%
Table 5: Experimental results on data that can use IPAL and VDIC dictionaries
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 57.71% 58.98% 58.63% 58.81%
Baseline 2 37.39% 37.39% 37.29% 37.29%
Baseline 3 84.03% 86.57% 86.83% 88.31%
KNP 74.59% 75.86% 75.88% 76.07%
Kondo 76.04% 77.50% 78.66% 78.85%
Our method, no-division + all features 94.19% 95.46% 94.81% 94.81%
Our method, division + all features 95.83% 96.91% 97.03% 97.03%
fective.
Our method that used all the features and did
not separate training data for any source particles
obtained an accuracy rate of 92.00% in Eval. B.
The technique of separating training data into each
source particles made an improvement of 2.30%.
We confirmed that this improvement has a signifi-
cance level of 0.01 by using a two-sided binomial
test (two-sided sign test). This indicates that the
technique of separating training data for all source
particles is effective.
Murata 2003 who used only seven features and
did not separate training data for any source par-
ticles obtained an accuracy rate of 89.77% with
Eval. B. The method (92.00%) of using all fea-
tures (32) made an improvement of 2.23% against
theirs. We confirmed that this improvement had
a significance level of 0.01 by using a two-sided
binomial test (two-sided sign test). This indicates
that our increased features are effective.
KNP and Kondo obtained low accuracy rates
(29.14% and 41.00% in Eval. B for the open data).
We did the evaluation using data and proved that
these methods could work well. A target predicate
in the data is defined in the IPAL and VDIC dictio-
naries. The results are listed in Table 5. KNP and
Kondo obtained relatively higher accuracy rates
(76.07% and 78.85% in Eval. B for the open data).
However, they were lower than that for Baseline 3.
Baseline 3 obtained a relatively high accuracy
rate (84.17% and 88.20% in Eval. B for the open
data). Baseline 3 is similar to our method in terms
of separating the training data into source parti-
cles. Baseline 3 separates the training data into
592
Table 6: Deletion of features
Deleted Closed data Open data
features Eval. A Eval. B Eval. A Eval. B
Acc. Diff. Acc. Diff. Acc. Diff. Acc. Diff.
Not deleted 91.22% ? 93.79% ? 92.28% ? 94.30% ?
F1 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F2 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F3 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F4 91.50% 0.28% 94.13% 0.34% 91.72% -0.56% 93.68% -0.62%
F5 91.22% 0.00% 93.62% -0.17% 91.95% -0.33% 93.96% -0.34%
F6 91.00% -0.22% 93.51% -0.28% 92.23% -0.05% 94.24% -0.06%
F7 90.66% -0.56% 93.18% -0.61% 91.78% -0.50% 93.90% -0.40%
F8 91.22% 0.00% 93.79% 0.00% 92.39% 0.11% 94.24% -0.06%
F9 91.28% 0.06% 93.62% -0.17% 92.45% 0.17% 94.07% -0.23%
F10 91.33% 0.11% 93.85% 0.06% 92.00% -0.28% 94.07% -0.23%
F11 91.50% 0.28% 93.74% -0.05% 92.06% -0.22% 93.79% -0.51%
F12 91.28% 0.06% 93.62% -0.17% 92.56% 0.28% 94.35% 0.05%
F13 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F14 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.41% 0.11%
F15 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F16 91.39% 0.17% 93.90% 0.11% 92.34% 0.06% 94.30% 0.00%
F17 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F18 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.46% 0.16%
F19 91.33% 0.11% 93.90% 0.11% 92.28% 0.00% 94.30% 0.00%
F20 91.11% -0.11% 93.68% -0.11% 92.34% 0.06% 94.35% 0.05%
F21 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F22 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F23 91.28% 0.06% 93.79% 0.00% 92.28% 0.00% 94.24% -0.06%
F24 91.22% 0.00% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F25 89.54% -1.68% 92.11% -1.68% 90.04% -2.24% 92.39% -1.91%
F26 91.16% -0.06% 93.74% -0.05% 92.28% 0.00% 94.30% 0.00%
F27 91.22% 0.00% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F28 90.94% -0.28% 93.51% -0.28% 92.11% -0.17% 94.13% -0.17%
F29 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.30% 0.00%
F30 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F31 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.24% -0.06%
F32 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
source particles and uses the most frequent tar-
get case particle. Our method involves separating
the training data into source particles and using
machine learning for each particle. The fact that
Baseline 3 obtained a relatively high accuracy rate
supports the effectiveness of our method separat-
ing the training data into source particles.
6.2 Experiments confirming importance of
features
We next conducted experiments where we con-
firmed which features were effective. The results
are listed in Table 6. We can see the accuracy rate
for deleting features and the accuracy rate for us-
ing all features. We can see that not using F25
greatly decreased the accuracy rate (about 2%).
This indicates that F25 is particularly effective.
F25 is the transformation rule Kondo used for P
and N in his method. The transformation rules in
Kondo?s method were made precisely for ni (indi-
rect object), which is particularly difficult to han-
dle. F25 is thus effective. We could also see not
using F7 decreased the accuracy rate (about 0.5%).
F7 has the semantic features for N. We found that
the semantic features for N were also effective.
6.3 Experiments changing number of
training data
We finally did experiments changing the number
of training data. The results are plotted in Figure
5. We used our two methods of all features ?Di-
vision? and ?Non-division?. We only plotted the
593
Figure 5: Changing number of training data
accuracy rates for Eval. B in the open data in the
figure. We plotted accuracy rates when 1, 1/2, 1/4,
1/8, and 1/16 of the training data were used. ?Divi-
sion?, which separates training data for all source
particles, obtained a high accuracy rate (88.36%)
even when the number of training data was small.
In contrast, ?Non-division?, which does not sepa-
rate training data for any source particles, obtained
a low accuracy rate (75.57%), when the number of
training data was small. This indicates that our
method of separating training data for all source
particles is effective.
7 Conclusion
We developed a new method of transform-
ing Japanese case particles when transforming
Japanese passive sentences into active sentences.
Our method separates training data for all input
(source) particles and uses machine learning for
each particle. We also used numerous rich features
for learning. Our method obtained a high rate of
accuracy (94.30%). In contrast, a method that did
not separate training data for all source particles
obtained a lower rate of accuracy (92.00%). In ad-
dition, a method that did not have many rich fea-
tures for learning used in a previous study obtained
a much lower accuracy rate (89.77%). We con-
firmed that these improvements were significant
through a statistical test. We also undertook ex-
periments utilizing traditional methods using verb
dictionaries and manually prepared heuristic rules
and confirmed that our method obtained much
higher accuracy rates than traditional methods.
We also conducted experiments on which fea-
tures were the most effective. We found that
Kondo?s transformation rule used as a feature in
our system was particularly effective. We also
found that semantic features for nominal targets
were effective.
We finally did experiments on changing the
number of training data. We found that our
method of separating training data for all source
particles could obtain high accuracy rates even
when there were few training data. This indicates
that our method of separating training data for all
source particles is effective.
The transformation of passive sentences into ac-
tive sentences is useful in many research areas
including generation, knowledge extraction from
databases written in natural languages, informa-
tion extraction, and answering questions. In the
future, we intend to use the results of our study for
these kinds of research projects.
References
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
IPA. 1987. (Information?Technology Promotion Agency,
Japan). IPA Lexicon of the Japanese Language for Com-
puters IPAL (Basic Verbs). (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. Transactions of Infor-
mation Processing Society of Japan, 42(3):465?477. (in
Japanese).
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec-
tor learning for chunk identification. CoNLL-2000, pages
142?144.
Taku Kudoh. 2000. TinySVM: Support Vector Machines.
http://cl.aist-nara.ac.jp/?taku-ku//software/TinySVM/
index.html.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Univer-
sity text corpus project. 3rd Annual Meeting of the Asso-
ciation for Natural Language Processing, pages 115?118.
(in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case Struc-
ture Analyzer KNP version 2.0b6. Department of Infor-
matics, Kyoto University. (in Japanese).
Masaki Murata and Hitoshi Isahara, 2003. Conversion of
Japanese Passive/Causative Sentences into Active Sen-
tences Using Machine Learning, pages 115?125. Springer
Publisher.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002. Com-
parison of three machine-learning methods for Thai part-
of-speech tagging. ACM Transactions on Asian Language
Information Processing, 1(2):145?158.
NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.
Hirotoshi Taira and Masahiko Haruno. 2001. Feature se-
lection in svm text categorization. In Proceedings of
AAAI2001, pages 480?486.
594
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extracting Word Sets with Non-Taxonomical Relation 
Eiko Yamamoto Hitoshi Isahara 
Computational Linguistics Group 
National Institute of Information and Communications Technology 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan 
{eiko, isahara}@nict.go.jp 
Abstract 
At least two kinds of relations exist among 
related words: taxonomical relations and 
thematic relations. Both relations identify 
related words useful to language under-
standing and generation, information re-
trieval, and so on. However, although 
words with taxonomical relations are easy 
to identify from linguistic resources such as 
dictionaries and thesauri, words with the-
matic relations are difficult to identify be-
cause they are rarely maintained in linguis-
tic resources. In this paper, we sought to 
extract thematically (non-taxonomically) 
related word sets among words in docu-
ments by employing case-marking particles 
derived from syntactic analysis. We then 
verified the usefulness of word sets with 
non-taxonomical relation that seems to be a 
thematic relation for information retrieval. 
1. Introduction 
Related word sets are useful linguistic resources 
for language understanding and generation, infor-
mation retrieval, and so on. In previous research on 
natural language processing, many methodologies 
for extracting various relations from corpora have 
been developed, such as the ?is-a? relation (Hearst 
1992), ?part-of? relation (Berland and Charniak 
1999), causal relation (Girju 2003), and entailment 
relation (Geffet and Dagan 2005).  
Related words can be used to support retrieval in 
order to lead users to high-quality information. 
One simple method is to provide additional words 
related to the key words users have input, such as 
an input support function within the Google search 
engine. What kind of relation between the key 
words that have been input and the additional word 
is effective for information retrieval? 
As for the relations among words, at least two 
kinds of relations exist: the taxonomical relation 
and the thematic relation. The former is a relation 
representing the physical resemblance among ob-
jects, which is typically a semantic relation such as 
a hierarchal, synonymic, or antonymic relation; 
the latter is a relation between objects through a 
thematic scene, such as ?milk? and ?cow? as recol-
lected in the scene ?milking a cow,? and ?milk? 
and ?baby,? as recollected in the scene ?giving 
baby milk,? which include causal relation and en-
tailment relation. Wisniewski and Bassok (1999) 
showed that both relations are important in recog-
nizing those objects. However, while taxonomical 
relations are comparatively easy to identify from 
linguistic resources such as dictionaries and 
thesauri, thematic relations are difficult to identify 
because they are rarely maintained in linguistic 
resources. 
In this paper, we sought to extract word sets 
with a thematic relation from documents by em-
ploying case-marking particles derived from syn-
tactic analysis. We then verified the usefulness of 
word sets with non-taxonomical relation that seems 
to be a thematic relation for information retrieval. 
2. Method 
In order to derive word sets that direct users to 
obtain information, we applied a method based on 
the Complementary Similarity Measure (CSM), 
which can determine a relation between two words 
in a corpus by estimating inclusive relations 
between two vectors representing each appearance 
pattern for each words (Yamamoto et al 2005).  
141
We first extracted word pairs having an inclu-
sive relation between the words by calculating the 
CSM values. Extracted word pairs are expressed 
by a tuple <wi, wj>, where CSM(Vi, Vj) is greater 
than CSM(Vj, Vi) when words wi and wj have each 
appearance pattern represented by each binary vec-
tor Vi and Vj. Then, we connected word pairs with 
CSM values greater than a certain threshold and 
constructed word sets. A feature of the CSM-based 
method is that it can extract not only pairs of re-
lated words but also sets of related words because 
it connects tuples consistently. 
Suppose we have <A, B>, <B, C>, <Z, B>, <C, 
D>, <C, E>, and <C, F> in the order of their CSM 
values, which are greater than the threshold. For 
example, let <B, C> be an initial word set {B, C}. 
First, we find the tuple with the greatest CSM 
value among the tuples in which the word C at the 
tail of the current word set is the left word, and 
connect the right word behind C. In this example, 
word ?D? is connected to {B, C} because <C, D> 
has the greatest CSM value among the three tuples 
<C, D>, <C, E>, and <C, F>, making the current 
word set {B, C, D}. This process is repeated until 
no tuples exist. Next, we find the tuple with the 
greatest CSM value among the tuples in which the 
word B at the head of the current word set is the 
right word, and connect the left word before B. 
This process is repeated until no tuples exist. In 
this example, we obtain the word set {A, B, C, D}.  
Finally, we removed ones with a taxonomical 
relation by using thesaurus. The rest of the word 
sets have a non-taxonomical relation ? including 
a thematic relation ? among the words. We then 
extracted those word sets that do not agree with the 
thesaurus as word sets with a thematic relation. 
3. Experiment 
In our experiment, we used domain-specific Japa-
nese documents within the medical domain 
(225,402 sentences, 10,144 pages, 37MB) gathered 
from the Web pages of a medical school and the 
2005 Medical Subject Headings (MeSH) thesau-
rus 1 . Recently, there has been a study on query 
expansion with this thesaurus as domain informa-
tion (Friberg 2007). 
                                                 
1 The U.S. National Library of Medicine created, maintains, 
and provides the MeSH? thesaurus.  
We extracted word sets by utilizing inclusive re-
lations of the appearance pattern between words 
based on a modified/modifier relationship in 
documents. The Japanese language has case-
marking particles that indicate the semantic rela-
tion between two elements in a dependency rela-
tion. Then, we collected from documents depend-
ency relations matching the following five pat-
terns; ?A <no (of)> B,? ?P <wo (object)> V,? ?Q 
<ga (subject)> V,? ?R <ni (dative)> V,? and ?S 
<ha (topic)> V,? where A, B, P, Q, R, and S are 
nouns, V is a verb, and <X> is a case-marking par-
ticle. From such collected dependency relations, 
we compiled the following types of experimental 
data; NN-data based on co-occurrence between 
nouns for each sentence, NV-data based on a de-
pendency relation between noun and verb for each 
case-marking particle <wo>, <ga>, <ni>, and <ha>, 
and SO-data based on a collocation between sub-
ject and object that depends on the same verb V 
as the subject. These data are represented with a 
binary vector which corresponds to the appearance 
pattern of a noun and these vectors are used as ar-
guments of CSM. 
We translated descriptors in the MeSH thesaurus 
into Japanese and used them as Japanese medical 
terms. The number of terms appearing in this ex-
periment is 2,557 among them. We constructed 
word sets consisting of these medical terms. Then, 
we chose 977 word sets consisting of three or more 
terms from them, and removed word sets with a 
taxonomical relation from them with the MeSH 
thesaurus in order to obtain the rest 847 word sets 
as word sets with a thematic relation. 
4. Verification 
In verifying the capability of our word sets to re-
trieve Web pages, we examined whether they 
could help limit the search results to more informa-
tive Web pages with Google as a search engine.  
We assume that addition of suitable key words 
to the query reduces the number of pages retrieved 
and the remaining pages are informative pages. 
Based on this assumption, we examined the de-
crease of the retrieved pages by additional key 
words and the contents of the retrieved pages in 
order to verify the availability of our word sets.  
Among 847 word sets, we used 294 word sets in 
which one of the terms is classified into one cate-
gory and the rest are classified into another. 
142
ovary - spleen - palpation (NN) 
variation - cross reactions - outbreaks - secretion (Wo) 
bleeding - pyrexia - hematuria - consciousness disorder  
- vertigo - high blood pressure (Ga) 
space flight - insemination - immunity (Ni) 
cough - fetus  
- bronchiolitis obliterans organizing pneumonia (Ha) 
latency period - erythrocyte - hepatic cell (SO) 
Figure 1. Examples of word sets used to verify. 
Figure 1 shows examples of the word sets, 
where terms in a different category are underlined. 
In retrieving Web pages for verification, we in-
put the terms composed of these word sets into the 
search engine. We created three types of search 
terms from the word set we extracted. Suppose the 
extracted word set is {X1, ..., Xn, Y}, where Xi is 
classified into one category and Y is classified into 
another. The first type uses all terms except the one 
classified into a category different from the others: 
{X1, ..., Xn} removing Y. The second type uses all 
terms except the one in the same category as the 
rest: {X1, ..., Xk-1, Xk+1, ..., Xn} removing Xk from 
Type 1. In our experiment, we removed the term 
Xk with the highest or lowest frequency among Xi. 
The third type uses terms in Type 2 and Y: {X1, ..., 
Xk-1, Xk+1, ..., Xn, Y}. 
In other words, when we consider the terms in 
Type 2 as base key words, the terms in Type 1 are 
key words with the addition of one term having the 
highest or lowest frequency among the terms in the 
same category; i.e., the additional term has a fea-
ture related to frequency in the documents and is 
taxonomically related to other terms. The terms in 
Type 3 are key words with the addition of one term 
in a category different from those of the other 
component terms; i.e., the additional term seems to 
be thematically related ? at least non-
taxonomically related ? to other terms. 
First, we quantitatively compared the retrieval 
results. We used the estimated number of pages 
retrieved by Google?s search engine. Suppose that 
we first input Type 2 as key words into Google, 
did not satisfy the result extracted, and added one 
word to the previous key words. We then sought to 
determine whether to use Type 1 or Type 3 to ob-
tain more suitable results. The results are shown in 
Figures 2 and 3, which include the results for the 
highest frequency and the lowest frequency, re-
spectively. In these figures, the horizontal axis is 
the number of pages retrieved with Type 2 and the 
vertical axis is the number of pages retrieved when 
 
 
 
 
 
 
 
 
 
 
 1
10
100
1000
10000
100000
1000000
10000000
100000000
1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000
Number of Web pages retrieved with Type2 (base key words)
Nu
m
be
r 
of
 W
eb
 p
ag
es
 r
et
rie
ve
d 
wh
en
 a
 t
er
m
 is
 a
dd
ed
 t
o 
Ty
pe
2
Type3: With additional term in a different category Type1: With additional term in same category
Figure 2. Fluctuation of number of pages retrieved 
(with the high frequency term). 
NV Type of Data NN 
Wo Ga Ni Ha
Word sets for verification 175 43 23 13 26
Cases in which Type 3 
defeated Type 1 in retrieval 108 37 15 12 18
Table 1. Number of cases in which Type 3 de-
feated Type 1 with the high frequency term. 
a certain term is added to Type 2. The circles (?) 
show the retrieval results with additional key word 
related taxonomically (Type 1). The crosses (?) 
show the results with additional key word related 
non-taxonomically (Type 3). The diagonal line 
shows that adding one term to the base key words 
does not affect the number of Web pages retrieved. 
In Figure 2, most crosses fall further below the 
line. This graph indicates that when searching by 
Google, adding a search term related non-
taxonomically tends to make a bigger difference 
than adding a term related taxonomically and with 
high frequency. This means that adding a term re-
lated non-taxonomically to the other terms is cru-
cial to retrieving informative pages; that is, such 
terms are informative terms themselves. Table 1 
shows the number of cases in which term in differ-
ent category decreases the number of hit pages 
more than high frequency term. By this table, we 
found that most of the additional terms with high 
frequency contributed less than additional terms 
related non-taxonomically to decreasing the num-
ber of Web pages retrieved. This means that, in 
comparison to the high frequency terms, which 
might not be so informative in themselves, the 
terms in the other category ? related non-
taxonomically ? are effective for retrieving useful 
Web pages. 
In Figure 3, most circles fall further below the 
line, in contrast to Figure 2. This indicates that 
143
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Fluctuation of number of pages retrieved 
(with the low frequency term). 
NV Type of Data NN 
Wo Ga Ni Ha
Word sets for verification 175 43 23 13 26
Cases in which Type 3 
defeated Type 1 in retrieval 61 18 7 6 13
Table 2. Number of cases in which Type 3 de-
feated Type 1 with the low frequency term. 
adding a term related taxonomically and with low 
frequency tends to make a bigger difference than 
adding a term with high frequency. Certainly, addi-
tional terms with low frequency would be informa-
tive terms, even though they are related taxonomi-
cally, because they may be rare terms on the Web 
and therefore the number of pages containing the 
term would be small. Table 2 shows the number of 
cases in which term in different category decreases 
the number of hit pages more than low frequency 
term. In comparing these numbers, we found that 
the additional term with low frequency helped to 
reduce the number of Web pages retrieved, making 
no effort to determine the kind of relation the term 
had with the other terms. Thus, the terms with low 
frequencies are quantitatively effective when used 
for retrieval. However, if we compare the results 
retrieved with Type 1 search terms and Type 3 
search terms, it is clear that big differences exist 
between them.  
For example, consider ?latency period - erythro-
cyte - hepatic cell? obtained from SO-data in Fig-
ure 1. ?Latency period? is classified into a category 
different from the other terms and ?hepatic cell? 
has the lowest frequency in this word set. When we 
used all the three terms, we obtained pages related 
to ?malaria? at the top of the results and the title of 
the top page was ?What is malaria?? in Japanese. 
With ?latency period? and ?erythrocyte,? we again 
obtained the same page at the top, although it was 
not at the top when we used ?erythrocyte? and 
?hepatic cell? which have a taxonomical relation. 
Type3: With additional term in a different category Type1: With additional term in same category
1
10
100
1000
10000
100000
1000000
10000000 As we showed above, the terms with thematic 
relations with other search terms are effective at 
directing users to informative pages. Quantitatively, 
terms with a high frequency are not effective at 
reducing the number of pages retrieved; qualita-
tively, low frequency terms may not effective to 
direct users to informative pages. We will continue 
our research in order to extract terms in thematic 
relation more accurately and verify the usefulness 
of them more quantitatively and qualitatively. 
5. Conclusion 
We sought to extract word sets with a thematic 
relation from documents by employing case-
marking particles derived from syntactic analysis. 
We compared the results retrieved with terms re-
lated only taxonomically and the results retrieved 
with terms that included a term related non-
taxonomically to the other terms. As a result, we 
found adding term which is thematically related to 
terms that have already been input as key words is 
effective at retrieving informative pages.  
References 
Berland, M. and Charniak, E. 1999. Finding parts in 
very large corpora, In Proceedings of ACL 99, 57?64. 
Friberg, K. 2007. Query expansion using domain infor-
mation in compounds, In Proceedings of NAACL-
HLT 2007 Doctoral Consortium, 1?4. 
Geffet, M. and Dagan, I. 2005. The distribution inclu-
sion hypotheses and lexical entailment. In Proceed-
ings of ACL 2005, 107?114. 
Girju, R. 2003. Automatic detection of causal relations 
for question answering. In Proceedings of ACL 
Workshop on Multilingual summarization and ques-
tion answering, 76?114. 
Hearst, M. A. 1992, Automatic acquisition of hyponyms 
from large text corpora, In Proceedings of Coling 92, 
539?545. 
Wisniewski, E. J. and Bassok. M. 1999. What makes a 
man similar to a tie? Cognitive Psychology, 39: 208?
238. 
Yamamoto, E., Kanzaki, K., and Isahara, H. 2005. Ex-
traction of hierarchies based on inclusion of co-
occurring words with frequency information. In Pro-
ceedings of IJCAI 2005, 1166?1172. 
1000 00
1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 10000000000
Number of Web pages retrieved with Type2 (base key words)
um
be
r 
of
 W
eb
 p
ag
es
 r
et
rie
ve
d 
w
he
n 
a 
te
rm
 is
 a
dd
ed
 t
o 
Ty
pe
2
000
N
144
Multilingual Aligned Parallel Treebank Corpus Reflecting
Contextual Information and Its Applications
Kiyotaka Uchimoto? Yujie Zhang? Kiyoshi Sudo?
Masaki Murata? Satoshi Sekine? Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{uchimoto,yujie,murata,isahara}@nict.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
{sudo,sekine}@cs.nyu.edu
Abstract
This paper describes Japanese-English-Chinese
aligned parallel treebank corpora of newspaper
articles. They have been constructed by trans-
lating each sentence in the Penn Treebank and
the Kyoto University text corpus into a cor-
responding natural sentence in a target lan-
guage. Each sentence is translated so as to
reflect its contextual information and is anno-
tated with morphological and syntactic struc-
tures and phrasal alignment. This paper also
describes the possible applications of the par-
allel corpus and proposes a new framework to
aid in translation. In this framework, paral-
lel translations whose source language sentence
is similar to a given sentence can be semi-
automatically generated. In this paper we show
that the framework can be achieved by using
our aligned parallel treebank corpus.
1 Introduction
Recently, accurate machine translation systems
can be constructed by using parallel corpora
(Och and Ney, 2000; Germann et al, 2001).
However, almost all existing machine transla-
tion systems do not consider the problem of
translating a given sentence into a natural sen-
tence reflecting its contextual information in the
target language. One of the main reasons for
this is that we had many problems that had to
be solved by one-sentence to one-sentence ma-
chine translation before we could solve the con-
textual problem. Another reason is that it was
difficult to simply investigate the influence of
the context on the translation because sentence
correspondences of the existing bilingual doc-
uments are rarely one-to-one, and are usually
one-to-many or many-to-many.
On the other hand, high-quality treebanks
such as the Penn Treebank (Marcus et al, 1993)
and the Kyoto University text corpus (Kuro-
hashi and Nagao, 1997) have contributed to
improving the accuracies of fundamental tech-
niques for natural language processing such as
morphological analysis and syntactic structure
analysis. However, almost all of these high-
quality treebanks are based on monolingual cor-
pora and do not have bilingual or multilin-
gual information. There are few high-quality
bilingual or multilingual treebank corpora be-
cause parallel corpora have mainly been actively
used for machine translation between related
languages such as English and French, there-
fore their syntactic structures are not required
so much for aligning words or phrases. How-
ever, syntactic structures are necessary for ma-
chine translation between languages whose syn-
tactic structures are different from each other,
such as in Japanese-English, Japanese-Chinese,
and Chinese-English machine translations, be-
cause it is more difficult to automatically align
words or phrases between two unrelated lan-
guages than between two related languages. Ac-
tually, it has been reported that syntactic struc-
tures contribute to improving the accuracy of
word alignment between Japanese and English
(Yamada and Knight, 2001). Therefore, if we
had a high-quality parallel treebank corpus, the
accuracies of machine translation between lan-
guages whose syntactic structures are differ-
ent from each other would improve. Further-
more, if the parallel treebank corpus had word
or phrase alignment, the accuracy of automatic
word or phrase alignment would increase by
using the parallel treebank corpus as training
data. However, so far, there is no aligned par-
allel treebank corpus whose domain is not re-
stricted. For example, the Japanese Electronics
Industry Development Association?s (JEIDA?s)
bilingual corpus (Isahara and Haruno, 2000)
has sentence, phrase, and proper noun align-
ment. However, it does not have morphologi-
cal and syntactic information, the alignment is
partial, and the target is restricted to a white
paper. The Advance Telecommunications Re-
search dialogue database (ATR, 1992) is a par-
allel treebank corpus between Japanese and En-
glish. However, it does not have word or phrase
alignment, and the target domain is restricted
to travel conversation.
Therefore, we have been constructing aligned
parallel treebank corpora of newspaper articles
between languages whose syntactic structures
are different from each other since 2001; they
meet the following conditions.
1. It is easy to investigate the influence of the con-
text on the translation, which means the sen-
tences that come before and after a particular
sentence, and that help us to understand the
meaning of a particular word such as a pro-
noun.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. They are open to the public.
To construct parallel corpora that satisfy these
conditions, each sentence in the Penn Tree-
bank (Release 2) and the Kyoto University text
corpus (Version 3.0) has been translated into
a corresponding natural sentence reflecting its
contextual information in a target language by
skilled translators, revised by native speakers,
and each parallel translation has been anno-
tated with morphological and syntactic struc-
tures, and phrasal alignment. Henceforth, we
call the parallel corpus that is constructed by
pursuing the above policy an aligned parallel
treebank corpus reflecting contextual informa-
tion. In this paper, we describe an aligned par-
allel treebank corpus of newspaper articles be-
tween Japanese, English, and Chinese, and its
applications.
2 Construction of Aligned Parallel
Treebank Corpus Reflecting
Contextual Information
2.1 Human Translation of Existing
Monolingual Treebank
The Penn Treebank is a tagged corpus of Wall
Street Journal material, and it is divided into 24
sections. The Kyoto University text corpus is a
tagged corpus of the Mainichi newspaper, which
is divided into 16 sections according to the cat-
egories of articles such as the sports section and
the economy section. To maintain the consis-
tency of expressions in translation, a few partic-
ular translators were assigned to translate arti-
cles in a particular section, and the same trans-
lator was assigned to the same section. The
instructions to translators for Japanese-English
translation is basically as follows.
1. One-sentence to one-sentence translation as a
rule
Translate a source sentence into a target sen-
tence. In case the translated sentence becomes
unnatural by pursuing this policy, leave a com-
ment.
2. Natural translation reflecting contextual infor-
mation
Except in the case that the translated sentence
becomes unnatural by pursuing policy 1, trans-
late a source sentence into a target sentence
naturally.
By deletion, replacement, or supplementation,
let the translated sentence be natural in the
context.
In an entire article, the translated sentences
must maintain the same meaning and informa-
tion as those of the original sentences.
3. Translations of proper nouns
Find out the translations of proper nouns by
looking up the nouns in a dictionary or by using
a web search. In case a translation cannot be
found, use a temporary name and report it.
We started the construction of Japanese-
Chinese parallel corpus in 2002. The Japanese
sentences of the Kyoto University text corpus
were also translated into Chinese by human
translators. Then each translated Chinese sen-
tence was revised by a second Chinese native.
The instruction to the translators is the same
as that given in the Japanese-English human
translations.
The breakdown of the parallel corpora is
shown in Table 1. We are planning to trans-
late the remaining 18,714 sentences of the Kyoto
University text corpus and the remaining 30,890
sentences of the Penn Treebank. As for the nat-
uralness of the translated sentences, there are
207 (1%) unnatural English sentences of the
Kyoto University text corpus, and 462 (2.5%)
unnatural Japanese sentences of the Penn Tree-
bank generated by pursuing policy 1.
2.2 Morphological and Syntactic
Annotation
In the following sections, we describe the anno-
tated information of the parallel treebank cor-
pus based on the Kyoto University text corpus.
2.2.1 Morphological and Syntactic
Information of Japanese-English
corpus
Translated English sentences were analyzed by
using the Charniak Parser (Charniak, 1999).
Then, the parsed sentences were manually re-
vised. The definitions of part-of-speech (POS)
categories and syntactic labels follow those of
the Treebank I style (Marcus et al, 1993).
We have finished revising the 10,328 parsed
sentences that appeared from January 1st to
11th. An example of morphological and syn-
tactic structures is shown in Figure 1. In this
figure, ?S-ID? means the sentence ID in the
Kyoto University text corpus. EOJ means the
boundary between a Japanese parsed sentence
and an English parsed sentence. The definition
of Japanese morphological and syntactic infor-
mation follows that of the Kyoto University text
corpus (Version 3.0). The syntactic structure is
represented by dependencies between Japanese
phrasal units called bunsetsus. The phrasal
Table 1: Breakdown of the parallel corpora
Original corpus Languages # of parallel sentences
Kyoto University text corpus Japanese-English 19,669 (from Jan. 1st to 17th in 1995)
Japanese-Chinese 38,383 (all)
Penn Treebank Japanese-English 18,318 (from section 0 to 9)
Total Japanese-English 37,987 (Approximately 900,000 English words)
Japanese-Chinese 38,383 (Approximately 900,000 Chinese words)
# S-ID:950104141-008
* 0 2D
???? ???? * ?? * * *
* 1 2D
?? ?????? * ?? ?? * *
? ?? * ??? ???????? * *
?? ??? * ??? ???????? * *
? ? * ?? ???? * *
* 2 6D
?? ???? * ?? ???? * *
? ? ? ??? * ??? ????????
? ? * ?? ?? * *
* 3 4D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 4 5D
??? ???? ??? ?? * ???? ???
* 5 6D
?? ???? * ?? ???? * *
? ? * ?? ??? * *
* 6 -1D
??? ???? ?? ?? * ?????? ??????
? ? ?? ??? ?????? ???? ???
?? ?? ?? ??? ????????? ???????? ???
? ? * ?? ?? * *
EOJ
(S1 (S (NP (PRP They))
(VP (VP (VBD were)
(NP (DT all))
(ADJP (NP (QP (RB about)
(CD nineteen))
(NNS years))
(JJ old)))
(CC and)
(VP (VBD had)
(S (NP (DT no)
(NN strength))
(VP (VBN left)
(SBAR (S (VP (ADVP (RB even))
(TO to)
(VP (VB answer)
(NP (NNS questions))))))))))
(. .)))
EOE
Figure 1: Example of morphological and syn-
tactic information.
units or bunsetsus are minimal linguistic units
obtained by segmenting a sentence naturally in
terms of semantics and phonetics, and each of
them consists of one or more morphemes.
2.2.2 Chinese Morphological
Information of Japanese-Chinese
corpus
Chinese sentences are composed of strings of
Hanzi and there are no spaces between words.
The morphological annotation, therefore, in-
cludes providing tags of word boundaries and
POSs of words. We analyzed the Chinese sen-
tences by using the morphological analyzer de-
veloped by Peking University (Zhou and Duan,
1994). There are 39 categories in this POS set.
Then the automatically tagged sentences were
revised by the third native Chinese. In this
pass the Chinese translations were revised again
while the results of word segmentation and POS
tagging were revised. Therefore the Chinese
translations are obtained with a high quality.
We have finished revising the 12,000 tagged sen-
tences. The revision of the remaining sentences
is ongoing. An example of tagged Chinese sen-
tences is shown in Figure 2. The letters shown
Figure 2: Example of morphological informa-
tion of Chinese corpus.
after ?/? indicate POSs. The Chinese sentence is
the translation of the Japanese sentence in Fig-
ure 1. The Chinese sentences are GB encoded.
The 38,383 translated Chinese sentences have
1,410,892 Hanzi and 926,838 words.
2.3 Phrasal Alignment
This section describes the annotated informa-
tion of 19,669 sentences of the Kyoto University
text corpus.
The minimum alignment unit should be as
small as possible, because bigger units can be
constructed from units of the minimum size.
However, we decided to define a bunsetsu as the
minimum alignment unit. One of the main rea-
sons for this is that the smaller the unit is, the
higher the human annotation cost is. Another
reason is that if we define a word or a morpheme
as a minimum alignment unit, expressions such
as post-positional particles in Japanese and arti-
cles in English often do not have alignments. To
effectively absorb those expressions and to align
as many parts as possible, we found that a big-
ger unit than a word or a morpheme is suitable
as the minimum alignment unit. We call the
minimum alignment based on bunsetsu align-
ment units the bunsetsu unit translation pair.
Bigger pairs than the bunsetsu unit translation
pairs can be automatically extracted based on
the bunsetsu unit translation pairs. We call all
of the pairs, including bunsetsu unit transla-
tion pairs, translation pairs. The bunsetsu unit
translation pairs for idiomatic expressions often
become unnatural. In this case, two or more
bunsetsu units are combined and handled as a
minimum alignment unit. The breakdown of
the bunsetsu unit translation pairs is shown in
Table 2.
Table 2: Breakdown of the bunsetsu unit trans-
lation pairs.
(1) total # of translation pairs 172,255
(2) # of different translation pairs 146,397
(3) # of Japanese expressions 110,284
(4) # of English expressions 111,111
(5) average # of English expressions 1.33
corresponding to a Japanese expression ((2)/(3))
(6) average # of Japanese expressions 1.32
corresponding to a English expression ((2)/(4))
(7) # of ambiguous Japanese expressions 15,699
(8) # of ambiguous English expressions 12,442
(9) # of bunsetsu unit translation pairs 17,719
consisting of two or more bunsetsus
An example of phrasal alignment is shown in
Figure 3. A Japanese sentence is shown from
the line after the S-ID to the EOJ. Each line
indicates a bunsetsu. Each rectangular line in-
dicates a dependency between bunsetsus. The
leftmost number in each line indicates the bun-
setsu ID. The corresponding English sentence is
shown in the next line after that of the EOJ
(End of Japanese) until the EOE (End of En-
glish). The English expressions corresponding
to each bunsetsu are tagged with the corre-
sponding bunsetsu ID such as <P id=?bunsetsu
ID?></P>. When there are two or more fig-
ures in the tag id such as id=?1,2?, it means two
or more bunsetsus are combined and handled as
a minimum alignment unit.
For example, we can extract the following
translation pairs from Figure 3.
 (J) ??? (yunyuu-ga) / ????? (kaikin-sa-reta);
(E)that had been under the ban
 (J) ??????? (beikoku-san-ringo-no); (E)of apples
imported from the U.S.
 (J) ???? (dai-ichi-bin-ga); (E)The first cargo
 (J)???????(uridasa-reta); (E)was brought to the
market.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga); (E)The first cargo / of apples im-
ported from the U.S.
# S-ID:950110003-001
1 ????????????????
2 ????????????????
3 ????????????????
4 ????????????????
5 ????????????????
6 ????????????????
7 ????????????????
8 ????????????????
9 ????????????????
10 ???????????????
11 ????????????????
EOJ
<P id="4">The first cargo</P> <P id="3">of apples
imported from the U.S.</P> <P id="1,2">that had been
under the ban</P> <P id="7">completed</P> <P id="6">
quarantine</P> <P id="7">and</P> <P id="11">was brought
to the market</P> <P id="10">for the first time</P>
<P id="5">on the 9th</P> <P id="9">at major supermarket
chain stores</P> <P id="8">in the Tokyo metropolitan
area</P> <P id="11">.</P>
EOE
Figure 3: Example of phrasal alignment.
 (J) ??????? (beikoku-san-ringo-no) / ????
(dai-ichi-bin-ga) /???????(uridasa-reta); (E)The
first cargo / of apples imported from the U.S. / was
brought to the market.
Here, Japanese and English expressions are
divided by the symbol ?;?, and ?/? means a
bunsetsu boundary.
An overview of the criteria of the alignment
is as follows. Align as many parts as possible,
except if a certain part is redundant. More de-
tailed criteria will be attached with our corpus
when it is open to the public.
1. Alignment of English grammatical elements
that are not expressed in Japanese
English articles, possessive pronouns, infinitive
to, and auxiliary verbs are joined with nouns
and verbs.
2. Alignment between a noun and its substitute
expression
A noun can be aligned with its substitute ex-
pression such as a pronoun.
3. Alignment of Japanese ellipses
An English expression is joined with its related
elements. For example, the English subject is
joined with its related verb.
4. Alignment of supplementary or explanatory ex-
pression in English
Supplementary or explanatory expressions in
English are joined with their related words.
? Ex.?
# S-ID:950104142-003
1 ???????????
2 ???????????
3 ???????????
4 ???????????
5 ???????????
6 ???????????
EOJ
<P id="1">The Chinese character used for "ka"</P>
has such meanings as "beautiful" and "splendid."
EOE
?"?? (ka)??? (niwa)" corresponds to
"The Chinese character used for "ka""
5. Alignment of date and time
When a Japanese noun representing date and
time is adverbial, the English preposition is
joined with the date and time.
6. Alignment of coordinate structures
When English expressions represented by ?X
(A + B)? correspond to Japanese expressions
represented by ?XA + XB?, the alignment of
X overlaps.
? Ex.?
# S-ID:950106149-005
1 ?????????????
2 ?????????????
3 ?????????????
4 ?????????????
5 ?????????????
6 ?????????????
7 ?????????????
8 ?????????????
EOJ
In the Kinki Region, disposal of wastes started
<P id="2"><P id="4"> at offshore sites of</P>
Amagasaki</P> and <P id="4">Izumiotsu</P> from
1989 and 1991 respectively.
EOE
?"??? (Amagasaki-oki) ? (de)" corresponds to
"at offshore sites of Amagasaki"
?"???? (Izumiotsu-oki) ? (de)" corresponds to
"at offshore sites of ? Izumiotsu"
3 Applications of Aligned Parallel
Treebank Corpus
3.1 Use for Evaluation of Conventional
Methods
The corpus as described in Section 2 can be
used for the evaluation of English-Japanese and
Japanese-English machine translation. We can
directly compare various methods of machine
translation by using this corpus. It can be sum-
marized as follows in terms of the characteristics
of the corpus.
One-sentence to one-sentence translation
can be simply used for the evaluation of
various methods of machine translation.
Morphological and syntactic information
can be used for the evaluation of methods
that actively use morphological and syntactic
information, such as methods for example-
based machine translation (Nagao, 1981;
Watanabe et al, 2003), or transfer-based
machine translation (Imamura, 2002).
Phrasal alignment is used for the evaluation of
automatically acquired translation knowledge
(Yamamoto and Matsumoto, 2003).
An actual comparison and evaluation is our
future work.
3.2 Analysis of Translation
One-sentence to one-sentence translation
reflects contextual information. Therefore, it
is suitable to investigate the influence of the
context on the translation. For example, we
can investigate the difference in the use of
demonstratives and pronouns between English
and Japanese. We can also investigate the
difference in the use of anaphora.
Morphological and syntactic information
and phrasal alignment can be used to investi-
gate the appropriate unit and size of transla-
tion rules and the relationship between syntac-
tic structures and phrasal alignment.
3.3 Use in Conventional Systems
One-sentence to one-sentence translation
can be used for training a statistical translation
model such as GIZA++ (Och and Ney, 2000),
which could be a strong baseline system for
machine translation.
Morphological and syntactic information
and phrasal alignment can be used to acquire
translation knowledge for example-based ma-
chine translation and transfer-based machine
translation.
In order to show what kind of units are help-
ful for example-based machine translation, we
investigated whether the Japanese sentences of
newspaper articles appearing on January 17,
1995, which we call test-set sentences, could be
translated into English sentences by using trans-
lation pairs appearing from January 1st to 16th
as a database. First, we found that only one out
of 1,234 test-set sentences agreed with one out
of 18,435 sentences in the database. Therefore,
a simple sentence search will not work well. On
the other hand, 6,659 bunsetsus out of 12,632
bunsetsus in the test-set sentences agreed with
those in the database. If words in bunsetsus are
expanded into their synonyms, the combination
of the expanded bunsetsus sets in the database
may cover the test-set sentences. Next, there-
fore, we investigated whether the Japanese test-
set sentences could be translated into English
sentences by simply combining translation pairs
appearing in the database. Given a Japanese
sentence, words were extracted from it and
translation pairs that include those words or
their synonyms, which were manually evalu-
ated, were extracted from the database. Then,
the English sentence was manually generated by
just combining English expressions in the ex-
tracted translation pairs. One hundred two rel-
atively short sentences (the average number of
bunsetsus is about 9.8) were selected as inputs.
The number of equivalent translations, which
mean that the translated sentence is grammat-
ical and has the same meaning as the source
sentence, was 9. The number of similar transla-
tions, which mean that the translated sentence
is ungrammatical, or different or wrong mean-
ings of words, tenses, and prepositions are used
in the translated sentence, was 83. The num-
ber of other translations, which mean that some
words are missing, or the meaning of the trans-
lated sentence is completely different from that
of the original sentence, was 10. For example,
the original parallel translation is as follows:
Japanese:????????????????????????
????????????????????????
English: New Party Sakigake proposed that towards the or-
dinary session, both parties found a council to dis-
cuss policy and Diet management.
Given the Japanese sentence, the translated
sentence was:
Translation:Sakigake Party suggested to set up an organiza-
tion between the two parties towards the regular
session of the Diet to discuss under the theme of
policies and the management of the Diet.
This result shows that only 9% of input sen-
tences can be translated into sentences equiv-
alent to the original ones. However, we found
that approximately 90% of input sentences can
be translated into English sentences that are
equivalent or similar to the original ones.
3.4 Similar Parallel Translation
Generation
The original aim of constructing an aligned par-
allel treebank corpus as described in Section 2 is
to achieve a new framework for translation aid
as described below.
It would be very convenient if multilingual
sentences could be generated by just writing
sentences in our mother language. Today, it
can be formally achieved by using commercial
machine translation systems. However, the au-
tomatically translated sentences are often in-
comprehensible. Therefore, we have to revise
the original and translated sentences by find-
ing and referring to parallel translation whose
source language sentence is similar to the orig-
inal one. In many cases, however, we cannot
find such similar parallel translations to the in-
put sentence. Therefore, it is difficult for users
who do not have enough knowledge of the target
languages to generate comprehensible sentences
in several languages by just searching similar
parallel translations in this way. Therefore, we
propose to generate similar parallel translations
whose source language sentence is similar to
the input sentence. We call this framework for
translation aid similar parallel translation gen-
eration.
We investigated whether the framework can
be achieved by using our aligned parallel tree-
bank corpus. As the first step of this study,
we investigated whether an appropriate parallel
translation can be generated by simply combin-
ing translation pairs extracted from our aligned
parallel treebank corpus in the following steps.
1. Extract each content word with its adjacent
function word in each bunsetsu in a given sen-
tence
2. The extracted content words and their adjacent
function words are expanded into their syn-
onyms and class words whose major and minor
POS categories are the same
3. Find translation pairs including the expanded
content words with their expanded adjacent
function words in the given sentence
4. For each bunsetsu, select a translation pair that
has similar dependency relationship to those in
the given sentence
5. Generate a parallel translation by combining
the selected translation pairs
The input sentences were randomly selected
from 102 sentences described in Section 3.3.
The above steps, except the third step, were
basically conducted manually. The Examples
of the input sentences and generated parallel
translations are shown in Figure 4.
The basic unit of translation pairs in our
aligned parallel treebank corpus is a bunsetsu,
and the basic unit in the selection of transla-
tion pairs is also a bunsetsu. One of the ad-
vantages of using a bunsetsu as a basic unit is
that a Japanese expression represented as one
of various expressions in English, or omitted in
English, such as Japanese post-positional par-
ticles, is paired with a content word. There-
fore, the translation of such an expression is ap-
propriately selected together with the transla-
tion of a content word when a certain trans-
lation pair is selected. If the translation of
such an expression was selected independently
of the translation of a content word, the com-
bination of each translation would be ungram-
matical or unnatural. Another advantage of the
basic unit, bunsetsu, is that we can easily refer
to dependency information between bunsetsus
when we select an appropriate translation pair
because the original treebank has the depen-
dency information between bunsetsus. These
advantages are utilized in the above generation
steps. For example, in the first step, a content
word ??? (kokkai, Diet session)? in the sec-
ond example in Figure 4 was extracted from the
bunsetsu ????? (tsuujo-kokkai, the ordinary
Diet session) ? (ni, case marker)?, and it was
expanded into its class word ?? (kai, meeting)?
in the second step. Then, a translation pair
?(J)??????????? (kokuren-kodomo-
no-kenri-iinkai)? (ni, case marker); (E)the UN
Committee on the Rights of the Child /(J)
?? (taishi); (E)towards? was extracted as a
translation pair in the third step. Since the
dependency between ????????????
(kokuren-kodomo-no-kenri-iinkai, the UN Com-
mittee on the Rights of the Child)? and ???
(taishi, towards)? is similar to that between ?
???? (tsuujo-kokkai, the ordinary Diet ses-
sion)? (ni, case marker)? and ??? (muke, to-
wards)? in the input sentence, this translation
pair was selected in the fourth step. Finally,
the bunsetsu ???????????? (kokuren-
kodomo-no-kenri-iinkai, the UN Committee on
the Rights of the Child) ? (ni, case marker)?
and its translation ?the UN Committee on the
Rights of the Child? was used for generation of
a parallel translation in the fifth step.
When we use the generated parallel transla-
tion for the exact translation of the input sen-
tence, we should replace ??????????
?? (kokuren-kodomo-no-kenri-iinkai)? and its
translation ?the UN Committee on the Rights
of the Child? with ????? (tsuujo-kokkai, the
ordinary Diet session)? and its translation ?the
ordinary Diet session? by consulting a bilingual
dictionary. In this example, ??? (sono)? and
?them? should also be replaced with ??? (ry-
oto)? and ?both parties?. It is easy to identify
words in the generated translation that should
be replaced with words in the input sentence
because each bunsetsu in translation pairs is al-
ready aligned. In such cases, templates such as
?[?? (kaigi)]? (ni)?? (muke)? and ?towards
[council]? can be automatically generated by
generalizing content words expanded in the sec-
ond step and their translation in the generated
translation. The average number of English ex-
pressions corresponding to a Japanese expres-
sion is 1.3 as shown in Table 2. Even when there
are two or more possible English expressions, an
appropriate English expression can be chosen
by selecting a Japanese expression by referring
to dependencies in extracted translation pairs.
Therefore, in many cases, English sentences can
be generated just by reordering the selected ex-
pressions. The English word order was esti-
mated manually in this experiment. However,
we can automatically estimate English word or-
der by using a language model or an English
surface sentence generator such as FERGUS
(Bangalore and Rambow, 2000). Unnatural or
ungrammatical parallel translations are some-
times generated in the above steps. However,
comprehensible translations can be generated
as shown in Figure 4. The biggest advantage
of this framework is that comprehensible target
sentences can be generated basically by refer-
ring only to source sentences. Although it is
costly to search and select appropriate transla-
tion pairs, we believe that human labor can be
reduced by developing a human interface. For
example, when we use a Japanese text gener-
ation system from keywords (Uchimoto et al,
2002), users should only select appropriate key-
words.
We are investigating whether or not we can
generate similar parallel translations to all of
the Japanese sentences appearing on January
17, 1995. So far, we found that we can gen-
erate similar parallel translations to 691 out of
840 sentences (the average number of bunsetsus
is about 10.3) including the 102 sentences de-
scribed in Section 3.3. We found that we could
not generate similar parallel translations to 149
out of 840 sentences.
In the proposed framework of similar paral-
lel translation generation, the language appear-
ing in a corpus corresponds to a controlled lan-
guage, and users are allowed to use only the
controlled language to write sentences in the
source language. We believe that high-quality
bilingual or multilingual documents can be gen-
erated by letting us adapt ourselves to the con-
trolled environment in this way.
4 Conclusion
This paper described aligned parallel treebank
corpora of newspaper articles between lan-
guages whose syntactic structures are different
from each other; they meet the following condi-
tions.
1. It is easy to investigate the influence of the con-
text on the translation.
2. The annotated information in the existing
monolingual high-quality treebanks can be uti-
lized.
3. It is open to the public.
To construct parallel corpora that satisfy
these conditions, each sentence in the existing
monolingual high-quality treebanks has been
translated into a corresponding natural sentence
reflecting its contextual information in a target
language by skilled translators, and each par-
allel translation has been annotated with mor-
phological and syntactic structures and phrasal
alignment.
This paper also described the possible ap-
plications of the parallel corpus and proposed
a similar parallel translation generation frame-
work. In this framework, a parallel translation
whose source language sentence is similar to a
given sentence can be semi-automatically gen-
erated. In this paper we demonstrated that
the framework could be achieved by using our
aligned parallel treebank corpus.
In the near future, the aligned parallel tree-
bank corpora will be open to the public, and
expanded. We are planning to use the corpora
actively for machine translation, as a transla-
tion aid, and for second language learning. We
are also planning to develop automatic or semi-
automatic alignment system and an efficient in-
terface for machine translation aid.
Input sentence
(Japanese only)
???????????????????????????????????????????(Prime Minister
Murayama and Finance Minister Takemura met in the presidential office and they exchanged their
opinions, mainly on the issue of the new faction being formed by the New Democratic Union.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????
(E) Finance Minister Takemura held the meeting at the official residence to exchange views about the
formation of the new party of the New Democratic Union.
Input sentence
(Japanese only)
????????????????????????????????????????????????(New
Party Sakigake proposed that towards the ordinary session, both parties found a council to discuss policy
and Diet management.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
???????
(E) Sakigake proposed to set up an organization between them towards the UN Committee on the Rights
of the Child to discuss under the theme of policies and the management of the Diet.
Input sentence
(Japanese only)
?????????????????????????????????????????????(The meeting
was also intended to slow the movement towards the new party by the New Democratic Union, which is
trying to deepen the relationship with the New Frontier Party.)
Generated paral-
lel translation
(J) ?????????????????????????????????????????????
(E) The meeting had meanings to restrict the movement that the new party of New Democratic Union
is progressing to strengthen the coalition with The New Frontier Party.
Input sentence
(Japanese only)
?????????????????????????????????????????????????
???????(Lower House Diet Member Tatsuo Kawabata of the New Frontier Party decided on the
16th that he would hand in notification of his secession to the party on the 17th, in order to form a new
faction with Sadao Yamahana?s group.)
Generated paral-
lel translation
(J) ????????????????????????????????????????????????
?????????
(E) On 16th Tatsuo Kawabata, a member of the House of Representatives of the New Frontier Party
decided to submit The notice to leave the party to the Shinsei Party on the 17th in order to establish a
new faction with Yuukichi Amano and others.
Input sentence
(Japanese only)
???????????????????????????(As for the faction name in the Upper House,
they will decide after they consider how to form a relationship with Democratic Reform Union.)
Generated paral-
lel translation
(J) ?????????????????????
(E) The name of the faction will be decided after discussing the relationship with the JTUC.
Figure 4: Example of generated similar parallel translations.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data.
References
ATR. 1992. Dialogue Database. http://www.red.atr.co.jp/
database page/taiwa.html.
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
E. Charniak. 1999. A Maximum-Entropy-Inspired Parser.
Technical Report CS-99-12.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada
2001. Fast Decoding and Optimal Decoding for Machine
Translation. In Proceedings of the ACL-EACL, pages 228?
235.
K. Imamura. 2002. Application of translation knowledge ac-
quired by hierarchical phrase alignment for pattern-based
MT. In Proceedings of the TMI, pages 74?84.
H. Isahara and M. Haruno. 2000. Japanese-English aligned
bilingual corpora. In Jean Veronis, editor, Parallel Text
Processing - Alignment and Use of Translation Corpora,
pages 313?334. Kluwer Academic Publishers.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. Nagao. 1981. A Framework of a Mechanical Translation
between Japanese and English by Analogy Principle. In
Proceedings of the International NATO Symposium on Ar-
tificial and Human Intelligence.
F. J. Och and H. Ney. 2000. Improved Statistical Alignment
Models. In Proceedings of the ACL, pages 440?447.
K. Uchimoto, S. Sekine, and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of the COLING,
pages 1037?1043.
H. Watanabe, S. Kurohashi, and E. Aramaki. 2003. Finding
Translation Patterns from Paired Source and Target De-
pendency Structures. In Michael Carl and Andy Way, ed-
itors, Recent Advances in Example-Based Machine Trans-
lation, pages 397?420. Kluwer Academic Publishers.
K. Yamada and K. Knight. 2001. A Syntax-based Statistical
Translation Model. In Proceedings of the ACL, pages 523?
530.
K. Yamamoto and Y. Matsumoto. 2003. Extracting Transla-
tion Knowledge from Parallel Corpora. In Michael Carl
and Andy Way, editors, Recent Advances in Example-
Based Machine Translation, pages 365?395. Kluwer Aca-
demic Publishers.
Q. Zhou and H. Duan. 1994. Segmentation and POS Tag-
ging in the Construction of Contemporary Chinese Cor-
pus. Journal of Computer Science of China, Vol.85. (in
Chinese)
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 118?121,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with Conditional Random Fields
Wenliang Chen and Yujie Zhang and Hitoshi Isahara
Computational Linguistics Group
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
{chenwl, yujie, isahara}@nict.go.jp
Abstract
We present a Chinese Named Entity
Recognition (NER) system submitted to
the close track of Sighan Bakeoff2006.
We define some additional features via do-
ing statistics in training corpus. Our sys-
tem incorporates basic features and addi-
tional features based on Conditional Ran-
dom Fields (CRFs). In order to correct in-
consistently results, we perform the post-
processing procedure according to n-best
results given by the CRFs model. Our fi-
nal system achieved a F-score of 85.14 at
MSRA, 89.03 at CityU, and 76.27 at LDC.
1 Introduction
Named Entity Recognition task in the 2006 Sighan
Bakeoff includes three corpora: Microsoft Re-
search (MSRA), City University of Hong Kong
(CityU), and Linguistic Data Consortium (LDC).
There are four types of Named Entities in the cor-
pora: Person Name, Organization Name, Location
Name, and Geopolitical Entity (only included in
LDC corpus).
We attend the close track of all three cor-
pora. In the close track, we can not use any
external resources. Thus except basic features,
we define some additional features by applying
statistics in training corpus to replace external re-
sources. Firstly, we perform word segmentation
using a simple left-to-right maximum matching al-
gorithm, in which we use a word dictionary gen-
erated by doing n-gram statistics. Then we de-
fine the features based on word boundaries. Sec-
ondly, we generate several lists according to the
relative position to Named Entity (NE). We de-
fine another type of features based on these lists.
Using these features, we build a Conditional Ran-
dom Fields(CRFs)-based Named Entity Recogni-
tion (NER) System. We use the system to generate
n-best results for every sentence, and then perform
a post-processing.
2 Conditional Random Fields
2.1 The model
Conditional Random Fields(CRFs), a statistical
sequence modeling framework, was first intro-
duced by Lafferty et alLafferty et al, 2001).
The model has been used for chunking(Sha and
Pereira, 2003). We only describe the model
briefly since full details are presented in the pa-
per(Lafferty et al, 2001).
In this paper, we regard Chinese NER as a se-
quence labeling problem. For our sequence label-
ing problem, we create a linear-chain CRFs based
on an undirected graph G = (V,E), where V is
the set of random variables Y = {Yi|1 ? i ? n},
for each of n tokens in an input sentence and
E = {(Yi?1, Yi)|1 ? i ? n} is the set of n ? 1
edges forming a linear chain. For each sentence x,
we define two non-negative factors:
exp(?Kk=1 ?kfk(yi?1, yi, x)) for each edge
exp(?K?k=1 ?
?
kf
?
k(yi, x)) for each node
where fk is a binary feature function, and K and
K ? are the number of features defined for edges
and nodes respectively. Following Lafferty et
al(Lafferty et al, 2001), the conditional probabil-
ity of a sequence of tags y given a sequence of
tokens x is:
P (y|x) = 1Z(x)exp(
?
i,k
?kfk(yi?1, yi, x) +
?
i,k
??kf
?
k(yi, x))
(1)
where Z(x) is the normalization constant. Given
the training data D, a set of sentences (characters
118
Tag Meaning
0 (zero) Not part of a named entity
PER A person name
ORG An organization name
LOC A location name
GPE A geopolitical entity
Table 1: Named Entities in the Data
with their corresponding tags), the parameters of
the model are trained to maximize the conditional
log-likelihood. When testing, given a sentence x
in the test data, the tagging sequence y is given by
Argmaxy?P (y?|x).
CRFs allow us to utilize a large number of ob-
servation features as well as different state se-
quence based features and other features we want
to add.
2.2 CRFs for Chinese NER
Our CRFs-based system has a first-order Markov
dependency between NER tags.
In our experiments, we do not use feature selec-
tion and all features are used in training and test-
ing. We use the following feature functions:
f(yi?1, yi, x, i) = p(x, i)q(yi?1, yi) (2)
where p(x, i) is a predicate on the input sequence
x and current position i and q(yi?1, yi) is a predi-
cate on pairs of labels. For instance, p(x, i) might
be ?the char at position i is?(and)?.
In our system, we used CRF++ (V0.42)1 to im-
plement the CRFs model.
3 Chinese Named Entity Recognition
The training data format is similar to that of the
CoNLL NER task 2002, adapted for Chinese. The
data is presented in two-column format, where the
first column consists of the character and the sec-
ond is a tag.
Table 1 shows the types of Named Entities in the
data. Every character is to be tagged with a NE
type label extended with B (Beginning character
of a NE) and I (Non-beginning character of a NE),
or 0 (Not part of a NE).
To obtain a good-quality estimation of the con-
ditional probability of the event tag, the observa-
tions should be based on features that represent the
difference of the two events. In our system, we de-
fine three types of features for the CRFs model.
1CRF++ is available at
http://chasen.org/ taku/software/CRF++/
3.1 Basic Features
The basic features of our system list as follows:
? . Cn(n = ?2,?1, 0, 1, 2)
? . CnCn+1(n = ?1, 0)
Where C refers to a Chinese character while C0
denotes the current character and Cn(C?n) de-
notes the character n positions to the right (left)
of the current character.
For example, given a character sequence ???
????, when considering the character C0 de-
notes ???, C?1 denotes ???, C?1C0 denotes ??
??, and so on.
3.2 Word Boundary Features
The sentences in training data are based on char-
acters. However, there are many features related to
the words. For instance, the word ???? can be a
important feature for Person Name. We perform
word segmentation using the left-to-right maxi-
mum matching algorithm, in which we use a word
dictionary generated by doing n-gram statistics in
training corpus. Then we use the word boundary
tags as the features for the model.
Firstly, we construct a word dictionary by ex-
tracting N-grams from training corpus as follows:
1. Extract arbitrary N-grams (2 ? n ? 10,
Frequency ? 10 ) from training corpus. We
get a list W1.
2. Use a tool to perform statistical substring
reduction in W1[ described in (Lv et al,
2004)]2. We get a list W2.
3. Construct a character list (CH)3, in which the
characters are top 20 frequency in training
corpus.
4. Remove the strings from W2, which contain
the characters in the list CH. We get final N-
grams list W3.
Secondly, we use W3 as a dictionary for left-
to-right maximum matching word segmentation.
We assign word boundary tags to sentences. Each
character can be assigned one of 4 possible bound-
ary tags: ?B? for a character that begins a word
and is followed by another character, ?M? for a
2Tools are available at
http://homepages.inf.ed.ac.uk/s0450736/Software
3To collect some characters such as punctuation, ???,
??? and so on.
119
character that occurs in the middle of a word, ?E?
for a character that ends a word, and ?S? for a char-
acter that occurs as a single-character word.
The word boundary features of our system list
as follows:
? . WTn(n = ?1, 0, 1)
Where WT refers to the word boundary tag while
WT0 denotes the tag of current character and
WTn(WT?n) denotes the tag n positions to the
right (left) of the current character.
3.3 Char Features
If we can use external resources, we often use the
lists of surname, suffix of named entity and prefix
of named entity for Chinese NER. In our system,
we generate these lists automatically from training
corpus by the procedure as follows:
? PSur: uni-gram characters, first characters of Person
Name. (surname)
? PC: uni-gram characters in Person Name.
? PPre: bi-gram characters before Person Name. (prefix
of Person Name)
? PSuf: bi-gram characters after Person Name. (suffix of
Person Name)
? LC: uni-gram characters in Location Name or Geopo-
litical entity.
? LSuf: uni-gram characters, the last characters of Loca-
tion Name or Geopolitical Entity. (suffix of Location
Name or Geopolitical Entity)
? OC: uni-gram characters in Organization Name.
? OSuf: uni-gram characters, the last characters of Orga-
nization Name. (suffix of Organization Name)
? OBSuf: bi-gram characters, the last two characters of
Organization Name. (suffix of Organization Name)
We remove the items in uni-gram lists if their fre-
quencies are less than 5 and in bi-gram lists if
their frequencies are less than 2. Based on these
lists, we assign the tags to every character. For in-
stance, if a character is included in PSur list, then
we assign a tag ?PSur 1?, otherwise assign a tag
?PSur 0?. Then we define the char features as fol-
lows:
? . PSur0PC0;
? . PSurnPCnPSurn+1PCn+1(n = ?1, 0);
? . PPre0;
? . PSuf0;
? . LC0OC0;
S is the list of sentences, S = {s1, s2, ..., sn}.
T is m-best results of S, T = {t1, t2, ..., tn}, which ti
is a set of m-best results of si.
pij is the score of tij , that is the jth result in ti.
Collect NE list:
Loop i in [1, n]
if(pi0 ? 0.5){
Exacting all NEs from ti0 to add into NEList.}
Replacing:
Loop i in [1, n]
if(pi0 ? 0.5){
FinalResult(si) = ti0.}
else{
TmpResult = ti0.
Loop j in [m, 1]
if(the NEs in tij is included in NEList){
Replace the matching string in TmpResult with new
NE tags.}
FinalResult(si) = TmpResult.
}
Table 2: The algorithm of Post-processing
? . LCnOCnLCn+1OCn+1(n = ?1, 0);
? . LSuf0OSuf0;
? . LSufnOSufnLSufn+1OSufn+1(n = ?1, 0);
4 Post-Processing
There are inconsistently results, which are tagged
by the CRFs model. Thus we perform a post-
processing step to correct these errors.
The post-processing tries to assign the correct
tags according to n-best results for every sentence.
Our system outputs top 20 labeled sequences for
each sentence with the confident scores. The post-
processing algorithm is shown at Table 2. Firstly,
we collect NE list from high confident results.
Secondly, we re-assign the tags for low confident
results using the NE list.
5 Evaluation Results
5.1 Results on Sighan bakeoff 2006
We evaluated our system in the close track, on
all three corpora, namely Microsoft Research
(MSRA), City University of Hong Kong (CityU),
and Linguistic Data Consortium (LDC). Our offi-
cial Bakeoff results are shown at Table 3, where
the columns P, R, and FB1 show precision, recall
and F measure(? = 1). We used all three types of
features in our final system.
In order to evaluate the contribution of fea-
tures, we conducted the experiments of each type
of features using the test sets with gold-standard
dataset. Table 4 shows the experimental results,
120
MSRA P R FB1
LOC 92.81 88.53 90.62
ORG 81.93 81.07 81.50
PER 85.41 74.15 79.38
Overall 88.14 82.34 85.14
CityU P R FB1
LOC 92.21 92.00 92.11
ORG 87.83 74.23 80.46
PER 92.77 89.05 90.87
Overall 91.43 86.76 89.03
LDC P R FB1
GPE 83.78 80.36 82.04
LOC 51.11 21.70 30.46
ORG 71.79 60.82 65.85
PER 82.40 75.58 78.84
Overall 80.26 72.65 76.27
Table 3: Our official Bakeoff results
MSRA CityU LDC
F1 84.73 88.26 76.18
+F2 88.67 76.30
+F3 88.74
Post 85.23 89.03 76.66
Table 4: Results of different combinations
where F1 refers to use basic features, F2 refers to
use the word boundary features, F3 refers to use
the char features, and Post refers to perform the
post-processing.
The results indicated that word boundary fea-
tures helped on LDC and CityU, char features only
helped on CityU and the post-processing always
helped to improve the performance.
6 Conclusion
This paper presented our Named Entity Recogni-
tion system for the close track of Bakeoff2006.
Our approach was based on Conditional Random
Fields model. Except basic features, we defined
the additional features by doing statistics in train-
ing corpus. In addition, we performed a post-
processing according to n-best results generated
by the CRFs model. The evaluation results showed
that our system achieved state-of-the-art perfor-
mance on all three corpora in the close track.
References
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In International Conference on Ma-
chine Learning (ICML01).
Xueqiang Lv, Le Zhang, and Junfeng Hu. 2004. Statis-
tical substring reduction in linear time. In Proceed-
ings of IJCNLP-04, HaiNan island, P.R.China.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL03.
121
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 1?8,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Enhancing the Japanese WordNet
Francis Bond,? Hitoshi Isahara,? Sanae Fujita,?
Kiyotaka Uchimoto,? Takayuki Kuribayashi? and Kyoko Kanzaki?
? NICT Language Infrastructure Group, ? NICT Language Translation Group
<bond@ieee.org,{isahara,uchimoto,kuribayashi,kanzaki}@nict.go.jp>
? Sanae Fujita, NTT Communications Science Laboratory
<sanae@kecl.cslab.ntt.co.jp>
Abstract
The Japanese WordNet currently has
51,000 synsets with Japanese entries. In
this paper, we discuss three methods of
extending it: increasing the cover, linking
it to examples in corpora and linking it
to other resources (SUMO and GoiTaikei).
In addition, we outline our plans to make
it more useful by adding Japanese defini-
tion sentences to each synset. Finally, we
discuss how releasing the corpus under an
open license has led to the construction
of interfaces in a variety of programming
languages.
1 Introduction
Our goal is to make a semantic lexicon of
Japanese that is both accesible and usable. To
this end we are constructing and releasing the
Japanese WordNet (WN-Ja) (Bond et al, 2008a).
We have almost completed the first stage,
where we automatically translated the English
and Euro WordNets, and are hand correcting it.
We introduce this in Section 2. Currently, we
are extending it in three main areas: the first
is to add more concepts to the Japanese Word-
Net, either by adding Japanese to existing En-
glish synsets or by creating new synsets (? 3).
The second is to link the synsets to text exam-
ples (? 4). Finally, we are linking it to other re-
sources: the Suggested Upper Merged Ontology
(SUMO) (Niles and Pease, 2001), the Japanese
semantic lexicon GoiTaikei (Ikehara et al, 1997),
and a collection of illustrations taken from the
Open ClipArt Library (Phillips, 2005) (? 5).
2 Current State
Currently, the WN-Ja consists of 157,000 senses
(word-synset pairs) 51,000 concepts (synsets) and
81,000 unique Japanese words (version 0.91). The
relational structure (hypernym, meronym, do-
main, . . . ) is based entirely on the English Word-
Net 3.0 (Fellbaum, 1998). We have Japanese
words for 43.0% of the synsets in the English
WordNet. Of these synsets, 45% have been
checked by hand, 8% were automatically cre-
ated by linking through multiple languages and
46% were automatically created by adding non-
ambiguous translations, as described in Bond
et al (2008a). There are some 51,000 synsets with
Japanese candidate words that have not yet been
checked. For up-to-date information on WN-Ja
see: nlpwww.nict.go.jp/wn-ja.
An example of the entry for the synset
02076196-n is shown in Figure 1. Most fields
come from the English WordNet. We have added
the underlined fields (Ja Synonyms, Illustration,
links to GoiTaikei, SUMO) and are currently
adding the translated definition (Def (Ja)). In
the initial automatic construction there were 27
Japanese words associated with the synset,1 in-
cluding many inappropriate translations for other
senses of seal (e.g., ?? hanko ?stamp?). These
were reduced to three after checking: ????,
?? azarashi ?seal? and ?? ? shi-ru ?seal?.
Synsets with? in their names are those for which
there is currently no Japanese entry in the Word-
Net.
The main focus of this year?s work has been
this manual trimming of badly translated words.
The result is a WordNet with a reasonable cov-
erage of common Japanese words. The precision
per sense is just over 90%. We have aimed at high
coverage at the cost of precision for two reasons:
(i) we think that the WordNet must have a rea-
1????, ???, ????, ??, ?, ??, ??, ?
?, ??, ?, ??, ??, ??, ??, ??, ??, ??, ?
?, ???, ?, ???, ??, ??, ??, ??, ?? ?, ?
?, ??, ??, ??, ??, ??, ??,??,?, ??, ??
1
sonable coverage to be useful for NLP tasks and
(ii) we expect to continue refining the accuracy
over the following years. Our strategy is thus dif-
ferent from Euro WordNet (Vossen, 1998), where
initial emphasis was on building a consistent and
complete upper ontology.
3 Increasing Coverage
We are increasing the coverage in two ways. The
first is to continue to manually correct the auto-
matically translated synsets. This is being done
both by hand, as time permits, and also by com-
paring against other resources such as GoiTaikei
and Wikipedia. When we check for poor candi-
dates, we also add in missing words as they occur
to us.
More interestingly, we wish to add synsets for
Japanese concepts that may not be expressed in
the English WordNet. To decide which new con-
cepts to add, we will be guided by the other tasks
we are doing: annotation and linking. We intend
to create new synsets for words found in the cor-
pora we annotate that are not currently covered,
as well as for concepts that we want to link to.
An example for the first is the concept ?? go-
han ?cooked rice?, as opposed to the grain ?
kome ?rice?. An example of the second is???
? ?shinguru ?single: a song usually extracted
from a current or upcoming album to promote
the album?. This is a very common hypernym in
Wikipedia but missing from the English Word-
Net.
As far as possible, we want to coordinate the
creation of new synsets with other projects: for
example KorLex: the Korean WordNet aleady
makes the cooked rice/grain distinction, and the
English WordNet should also have a synset for
this sense of single.
4 Text Annotation
We are in the process of annotating four texts
(Table 1). The first two are translations of Word-
Net annotated English Texts (SemCor and the
WordNet definitions), the third is the Japanese
newspaper text that forms the Kyoto Corpus
and the fourth is an open corpus of bilingual
Japanese-English sentences (Tanaka). In 2009,
we expect to finish translating and annotate all
of SemCor, translate the WordNet definitions and
Name Sentences Words Content Words
SemCor 12,842 224,260 120,000
Definitions 165,977 1,468,347 459,000
Kyoto 38,383 969,558 527,000
Tanaka 147,190 1,151,892 360,000
Table 1: Corpora to be Sense Tagged
start annotation on the Kyoto and Tanaka Cor-
pora.
This annotation is essential for finding missing
senses in the Japanese WordNet, as well as get-
ting the sense distributions that are needed for
supervised word sense disambiguation.
4.1 SemCor
SemCor is a textual corpus in which words have
been both syntactically and semantically tagged.
The texts included in SemCor were extracted
from the Brown corpus (Francis and Kucera,
1979) and then linked to senses in the English
WordNet. The frequencies in this corpus were
used to give the sense frequencies in WordNet
(Fellbaum, 1998). A subset of this corpus (Mul-
tiSemCor) was translated into Italian and used
as a corpus for the Italian WordNet (Bentivogli
et al, 2004). We are translating this subset into
Japanese.
In the same way as Bentivogli et al (2004), we
are exploiting Cross-Language Annotation Trans-
fer to seed the Japanese annotation. For exam-
ple, consider (1)2. The content words answer,
was, simple, honest are tagged in SemCor. They
can be aligned with their translations ?? ko-
tae ?answer?, ?? kantan ?simple?, ?? soc-
choku ?honest? and ??? datta ?was?. This
allows us to tag the Japanese translation with
the same synsets as the English, and thus disam-
biguate them.
(1) His answeri wasj simplek but honestl .
??i ? ??k ???? ??l ? ??
???j ?
However, just because all the English words
have sysnets in WordNet, it is not always the
case for the translations. For example, the En-
glish phrase last night can be translated into ?
? zen?ya ?last-night?. Here the two English
words (and synsets) link to a single Japanese
2Sentence 96 in b13.
2
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Synset 02076196-n
Synonyms
?
?
ja ??, ????, ???
en seal9
fr phoque
?
? Illustration
animal/seal.png
Def (en) ?any of numerous marine mammals that come on shore to breed; chiefly of cold regions?
Def (ja) ???????????????????????????????
Hypernyms ?????/pinniped
Hyponyms ?/crabeater seal ?/eared seal ??/earless seal
GoiTaikei ??537:beast??
SUMO ? Carnivore
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Example Entry for Seal/??
word which has no suitable synset in the English
WordNet. In this case, we need to create a new
synset unique to the Japanese WordNet.3
We chose a translated SemCor as the basis of
annotation for two main reasons: (i) the cor-
pus can be freely redistributed ? we expect
the glosses to be useful as an aligned corpus of
Japanese-English-Italian and (ii) it has other an-
notations associated with it: Brown corpus POS
annotation, Penn Treebank syntactic annotation.
4.2 WordNet Definitions
Our second translated corpus is formed from
the WordNet definitions (and example sentences)
themselves (e.g., the def field shown in Figure 1).
The English definitions have been annotated with
word senses in the Princeton WordNet Gloss Cor-
pus. In the same way that we do for SemCor, we
are translating the definitions and examples, and
using the existing annotation to seed our annota-
tion.
Using the definitions as the base for a sense
annotated corpus is attractive for the following
reasons: (i) the translated corpus can be freely
redistributed ? we expect the definitions to be
useful as an aligned corpus and also to be useful
for many other open lexicons; (ii) the definitions
are useful for Japanese native speakers using the
WordNet, (iii) the definitions are useful for unsu-
pervised sense disambiguation techniques such as
LESK (Baldwin et al, 2008); (iv) other projects
3Arguably, the fact that one says last night (not yester-
day night) for the night proceeding today and tomorrow
night (not next night) for the night following today sug-
gests that these multi-word expressions are lexicalized and
synsets should be created for them in the English Word-
Net. However, in general we expect to create some synsets
that will be unique to the Japanese WordNet.
have also translated synset definitions (e.g. Span-
ish and Korean), so we can hope to create a multi-
lingual corpus here as well and (v) the definitions
can be used as a machine readable dictionary, and
various information extracted from there (Barn-
brook, 2002; Nichols et al, 2006)
4.3 Kyoto Text Corpus
The Kyoto Text Corpus consists of newspaper
text from the Mainichi Newspaper (1995), seg-
mented and annotated with Japanese POS tags
and dependency trees (Kurohashi and Nagao,
2003). The corpus is made up of two parts. The
first consists of 17 full days of articles and the sec-
ond of one year?s editorials. We hope to annotate
at least parts of it during 2009.
Even though the Kyoto Text Corpus is not
freely redistributable, we have chosen to anno-
tate it due to the wealth of annotation associated
with it: dependency trees, predicate-argument re-
lations and co-reference (Iida et al, 2007), trans-
lations into English and Chinese (Uchimoto et al,
2004) and sense annotations from the Hinoki
project (Bond et al, 2006). We also felt it was
important to tag some native Japanese text, not
only translated text.
4.4 Tanaka Corpus
Finally, we will also tag the Tanaka Corpus, an
open corpus of Japanese-English sentence pairs
compiled by Professor Yasuhito Tanaka at Hyogo
University and his students (Tanaka, 2001) and
released into the public domain. The corrected
version we use has around 140,000 sentence pairs.
This corpus is attractive for several reasons.
(i) it is freely redistributable; (ii) it has been in-
dexed to entries in the Japanese-English dictio-
3
nary JMDict (Breen, 2003); (iii) part of it has
also been used in an open HPSG-based treebank
(Bond et al, 2008b); (iv) further, translations in
other languages, most notably French, have been
added by the TATOEBA project.4 Our plan is
to tag this automatically using the tools devel-
oped for the Kyoto corpus annotation, and then
to open the data to the community for refinement.
We give a typical example sentence in (2).
(2) ??????????????????
?Some birds are sitting on the branch of that
tree.? (en)
?Des oiseaux se reposent sur la branche de cet
arbre.? (fr)
5 Linking to other resources
We currently link the Japanese WordNet to three
other resources: the Suggested Upper Merged
Ontology; GoiTaikei, a Japanese Lexicon; and a
collection of pictures from the Open Clip Art Li-
brary (OCAL: Phillips (2005)).
For SUMO we used existing mappings. For the
other resources, we find confident matches auto-
matically and then generalize from them. We find
matches in three ways:
MM Monosemous monolingual matches
e.g. cricket bat or ?? azarashi ?seal?
MB Monosemous bilingual matches
e.g. ????seal?
HH Hypernym/Hyponym pairs
e.g. ?seal ? mammal?
We intend to use the same techniques to link
other resources, such as the concepts from the
EDR lexicon (EDR, 1990) and the automati-
cally extracted hypernym-hyponym links from
Torishiki-kai (Kuroda et al, 2009).
5.1 SUMO
The Suggested Upper Merged Ontology (SUMO)
is a large formal public ontology freely released
by the IEEE (Niles and Pease, 2001).
Because the structure of the Japanese Word-
Net is closely linked to that of the English Word-
Net, we were able to take advantage of the ex-
isting mappings from the English WordNet to
SUMO. There are 102,669 mappings from SUMO
4wwwcyg.utc.fr/tatoeba/
Carnivore Business Competition
Figure 2: SUMO illustrations
to WordNet: 3,593 equivalent, 10,712 where the
WordNet synset subsumes the SUMO concept,
88,065 where the SUMO concept subsumes the
WordNet concept, 293 where the negation of the
SUMO concept subsumes the WordNet synset
and 6 where the negation of the SUMO concept
is equivalent to the WordNet synset. According
to the mapping, synset 02076196-n ?? azarashi
?seal?, shown in Figure 1 is subsumed by the
SUMO concept ??Carnivore??. There is no link
between seal and carnivore in WordNet, which
shows how different ontologies can complement
each other.
Linking to SUMO also allowed us to use the
SUMO illustrations.5 These consist of 12,237
links linking 4,607 concepts to the urls of 10,993
illustrations. These are mainly taken from
from Wikimedia (upload.wikimedia.org), with
around 1,000 from other sources. The pictures
can be linked quite loosely to the concepts. For
example, ??Carnivore?? is illustrated by a lion eat-
ing meat, and ??BusinessCompetition?? by a pic-
ture of Wall Street.
As we wanted our illustrations to be more con-
crete, we only use SUMO illustrations where the
SUMO-WordNet mapping is equivalence. This
gave 4,384 illustrations for 999 synsets.
5.2 GoiTaikei
Linking Goi-Taikei, we used not only the
Japanese dictionary published in Ikehara et al
(1997), but also the Japanese-English dictionary
used in the machine translation system ALT-J/E
(Ikehara et al, 1991). We attempted to match
synsets to semantic categories by matching the
5Available at http://sigmakee.cvs.sourceforge.
net/viewvc/sigmakee/KBs/pictureList.kif, thanks to
Adam Pease for letting us know about them.
4
Japanese, English and English-Japanese pairs to
unambiguous entries in Goi-Taikei. For example,
the synset shown in Figure 1 was automatically
assigned the semantic category ??537:beast??, as
?? appears only once in WN-Ja, with the synset
shown, and once in the Japanese dictionary for
ALT-J/E with a single semantic category.
We are currently evaluating our results against
an earlier attempt to link WordNet and GoiTaikei
that also matched synset entries to words in Goi-
Taikei (Asanoma, 2001), but did not add an extra
constraint (that they must be either monosemous
or match as a hypernym-hyponym pair).
Once we have completed the mapping, we will
use it to check for inconsistencies in the two re-
sources.
5.3 Open ClipArt Library
In order to make the sense distinctions more vis-
ible we have semi-automatically linked synsets
to illustrations from the Open Clip Art Library
(OCAL: Phillips (2005)) using the mappings pro-
duced by Bond et al (2008a).
We manually checked the mappings and added
a goodness score. Illustrations are marked as:
3 the best out of multiple illustrations
2 a good illustration for the synset
1 a suitable illustration, but not perfect
This tag was used for black and white im-
ages, outlines, and so forth.
After the scoring, there were 874 links for 541
synsets (170 scored 1, 642 scored 2 and 62 scored
3). This is only a small subset of illustrations in
OCAL and an even smaller proportion of word-
net. However, because any illustrated synset alo
(in theory) illustrates its hypernyms, we have in-
directly illustrated far more than 541 synsets:
these figures are better than they seem.
There are far fewer OCAL illustrations than
the SUMO linked illustrations. However, they are
in general more representative illustrations (espe-
cially those scored 2 and above), and the source of
the clipart is available as SVG source so it is easy
to manipulate them. We think that this makes
them particularly useful for a variety of tasks.
One is pedagogical ? it is useful to have pic-
tures in learners? dictionaries. Another is in cross-
cultural communication - for example in Pangea,
where children use pictons (small concept repre-
senting pictures) to write messages (Takasaki and
Mori, 2007).
The OCAL illustrations mapped through
WordNet to 541 SUMO concepts. We have given
these links to the SUMO researchers.
6 Interfaces
We released the Japanese WordNet in three for-
mats: tab-delimited text, XML and as an SQLite
database. The license was the same as English
WordNet. This is a permissive license, the data
can be reused within proprietary software on the
condition that the license is distributed with that
software (similar to the MIT X license). The
license is also GPL-compatible, meaning that
the GPL permits combination and redistribution
with software that uses it.
The tab delimited format consists of just a list
of synsets, Japanese words and the type of link
(hand, multi-lingual or monosemous):
02076196-n ?? hand
02076196-n ???? hand
02076196-n ??? hand
We also output in WordNet-LMF (Francopoulo
et al, 2006; Soria et al, 2009), to make the
program easily available for other WordNet re-
searchers. In this case the synset structure was
taken from the English WordNet and the lem-
mas from the Japanese WordNet. Because of the
incomplete coverage, not all synsets contain lem-
mas. This format is used by the Kyoto Project,
and we expect it to become the standard ex-
change format for WordNets (Vossen et al, 2008).
Finally, we also created an SQL database. This
contains information from the English WordNet,
the Japanese WordNet, and links to illustra-
tions. We chose SQLite,6 a self-contained, zero-
configuration, SQL database engine whose source
code is in the public domain. The core structure
is very simple with six tables, as shown in Fig-
ure 3.
As we prepared the release we wrote a perl
module for a basic interface. This was used to
develop a web interface: Figure 4 shows a screen-
shot.
6http://www.sqlite.org
5
word
wordid 
 lang 
 lemma 
 pron 
 pos 
sense
synset 
 wordid 
 lang 
 rank 
 lexid 
 freq 
 src 
1..*1
synset
pos 
 name 
 src 
11..*
synsetDef
synset 
 lang 
 def 
 sid 
11
synlink
synset1 
 synset2 
 link 
 src 
1 1..*
xlink
synset 
 resource 
 xref 
 misc 
 confidence
1 1..*
Figure 3: Database Schema
Figure 4: Web Search Screenshot
6
7 Discussion
In contrast to earlier WordNets, the Japanese
WordNet was released with two known major im-
perfections: (i) the concept hierarchy was en-
tirely based on English with no adaptation to
Japanese and (ii) the data was released with some
unchecked automatically created entries. The re-
sult was a WordNet that did not fully model the
lexical structure of Japanese and was known to
contain an estimated 5% errors. The motivation
behind this was twofold. Firstly, we wanted to try
and take advantage of the open source model. If
the first release was good enough to be useful, we
hoped to (a) let people use it and (b) get feedback
from them which could then be incorporated into
the next release. This is the strategy known as
release early, release often (Raymond, 1999).
Secondly, we anticipated the most common use
of the WordNet to be in checking whether one
word is a hypernym of another. In this case, even
if one word is wrong, it is unlikely that the other
will be, so a small percentage of errors should be
acceptable.
From the practical point of view, the early re-
lease appears to have been a success. The SQL
database proved very popular, and within two
weeks of the first release someone produced a
python API. This was soon followed by inter-
faces in java, ruby, objective C and gauche. We
also received feedback on effective indexing of the
database and some corrections of entries ? these
have been included in the most recent release
(0.91).
The data from the Japanese WordNet has al-
ready been incorporated into other projects. The
first was the Multi-Lingual Semantic Network
(MLSN) (Cook, 2008) a WordNet based net-
work of Arabic, Chinese, English, German and
Japanese. Because both the Japanese WordNet
and MLSN use very open licenses, it is possible
to share entries directly. We have already re-
ceived useful feedback and over a thousand new
entries from MLSN. The second project using our
data is the Asian WordNet (Charoenporn et al,
2008). They have a well developed interface for
collaborative development of linguistic resources,
and we hope to get corrections and additions
from them in the future. Another project us-
ing the Japanese WordNet data is the Language
Grid (Ishida, 2006) which offers the English and
Japanese WordNets as concept dictionaries.
We have also been linked to from other re-
sources. The Japanese-English lexicon project
JMDict (Breen, 2004) now links to the Japanese
WordNet, and members of that project are us-
ing WordNet to suggest new entries. We used
JMDict in the first automatic construction stage,
so it is particularly gratifying to be able to help
JMDict in turn.
Finally, we believe that data about language
should be shared ? language is part of the com-
mon heritage of its speakers. In our case, the
Japanese WordNet was constructed based on the
work that others made available to us and thus we
had a moral obligation to make our results freely
available to others. Further, projects that create
WordNets but do not release them freely hinder
research on lexical semantics in that language ?
people cannot use the unreleased resource, but it
is hard to get funding to duplicate something that
already exists.
In future work, in addition to the planned ex-
tensions listed here, we would like to work on
the following: Explicitly marking lexical variants;
linking to instances in Wikipedia; adding deriva-
tional and antonym links; using the WordNet for
word sense disambiguation.
8 Conclusion
This paper presents the current state of the
Japanese WordNet (157,000 senses, 51,000 con-
cepts and 81,000 unique Japanese words, with
links to SUMO, Goi-Taikei and OCAL) and out-
lined our plans for further work (more words,
links to corpora and other resources). We hope
that WN-Ja will become a useful resource not only
for natural language processing, but also for lan-
guage education/learning and linguistic research.
References
Naoki Asanoma. 2001. Alignment of ontologies:wordnet
and goi-taikei. In NAACL Wokshop on WordNet &
Other Lexical Resources, pages 89?94. Pittsburgh, USA.
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2008. MRD-
based word sense disambiguation: Further extending
Lesk. In Proc. of the 3rd International Joint Conference
on Natural Language Processing (IJCNLP-08), pages
775?780. Hyderabad, India.
Geoff Barnbrook. 2002. Defining Language ? A local
7
grammar of definition sentences. Studies in Corpus Lin-
guistics. John Benjamins.
Luisa Bentivogli, Pamela Forner, and Emanuele Pianta.
2004. Evaluating cross-language annotation transfer in
the MultiSemCor corpus. In 20th International Con-
ference on Computational Linguistics: COLING-2004,
pages 364?370. Geneva.
Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2006. The Hinoki syntactic and semantic treebank of
Japanese. Language Resources and Evaluation, 40(3?
4):253?261. (Special issue on Asian language technol-
ogy).
Francis Bond, Hitoshi Isahara, Kyoko Kanzaki, and Kiy-
otaka Uchimoto. 2008a. Boot-strapping a WordNet
using multiple existing WordNets. In Sixth Interna-
tional conference on Language Resources and Evalua-
tion (LREC 2008). Marrakech.
Francis Bond, Takayuki Kuribayashi, and Chikara
Hashimoto. 2008b. Construction of a free Japanese
treebank based on HPSG. In 14th Annual Meeting of
the Association for Natural Language Processing, pages
241?244. Tokyo. (in Japanese).
James W. Breen. 2003. Word usage examples in an elec-
tronic dictionary. In Papillon (Multi-lingual Dictionary)
Project Workshop. Sapporo.
James W. Breen. 2004. JMDict: a Japanese-multilingual
dictionary. In Coling 2004 Workshop on Multilingual
Linguistic Resources, pages 71?78. Geneva.
Thatsanee Charoenporn, Virach Sornlerlamvanich,
Chumpol Mokarat, and Hitoshi Isahara. 2008. Semi-
automatic compilation of Asian WordNet. In 14th
Annual Meeting of the Association for Natural Lan-
guage Processing, pages 1041?1044. Tokyo.
Darren Cook. 2008. MLSN: A multi-lingual semantic net-
work. In 14th Annual Meeting of the Association for
Natural Language Processing, pages 1136?1139. Tokyo.
EDR. 1990. Concept dictionary. Technical report, Japan
Electronic Dictionary Research Institute, Ltd.
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
W. Nelson Francis and Henry Kucera. 1979. BROWN
CORPUS MANUAL. Brown University, Rhode Island,
third edition.
Gil Francopoulo, Monte George, Nicoletta Calzolari, Mon-
ica Monachini, Nuria Bel, Mandy Pet, and Claudia So-
ria. 2006. Lexical markup framework (LMF). In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006). Genoa,
Italy.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference relations.
In ACL Workshop: Linguistic Annotation Workshop,
pages 132?139. Prague.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and Hiromi
Nakaiwa. 1991. Toward an MT system without pre-
editing ? effects of new methods in ALT-J/E ?. In
Third Machine Translation Summit: MT Summit III,
pages 101?106. Washington DC.
Toru Ishida. 2006. Language grid: An infrastructure for in-
tercultural collaboration. In IEEE/IPSJ Symposium on
Applications and the Internet (SAINT-06), pages 96?
100. (keynote address).
Kow Kuroda, Jae-Ho Lee, Hajime Nozawa, Masaki Mu-
rata, and Kentaro Torisawa. 2009. Manual cleaning of
hypernyms in Torishiki-Kai. In 15th Annual Meeting of
The Association for Natural Language Processing, pages
C1?3. Tottori. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing
system. In Anne Abeille?, editor, Treebanks: Building
and Using Parsed Corpora, chapter 14, pages 249?260.
Kluwer Academic Publishers.
Eric Nichols, Francis Bond, Takaaki Tanaka, Sanae Fu-
jita, and Daniel Flickinger. 2006. Robust ontology ac-
quisition from multiple sources. In Proceedings of the
2nd Workshop on Ontology Learning and Population:
Bridging the Gap between Text and Knowledge, pages
10?17. Sydney.
Ian Niles and Adam Pease. 2001. Towards a standard
upper ontology. In Chris Welty and Barry Smith, edi-
tors, Proceedings of the 2nd International Conference on
Formal Ontology in Information Systems (FOIS-2001).
Maine.
Jonathan Phillips. 2005. Introduction to the open
clip art library. http://rejon.org/media/writings/
ocalintro/ocal_intro_phillips.html. (accessed
2007-11-01).
Eric S. Raymond. 1999. The Cathedral & the Bazaar.
O?Reilly.
Claudia Soria, Monica Monachini, and Piek Vossen. 2009.
Wordnet-LMF: fleshing out a standardized format for
wordnet interoperability. In Second International Work-
shop on Intercultural Collaboration (IWIC-2009). Stan-
ford.
Toshiyuki Takasaki and Yumiko Mori. 2007. Design and
development of a pictogram communication system for
children around the world. In First International Work-
shop on Intercultural Collaboration (IWIC-2007), pages
144?157. Kyoto.
Yasuhito Tanaka. 2001. Compilation of a multilingual par-
allel corpus. In Proceedings of PACLING 2001, pages
265?268. Kyushu.
Kiyotaka Uchimoto, Yujie Zhang, Kiyoshi Sudo, Masaki
Murata, Satoshi Sekine, and Hitoshi Isahara. 2004.
Multilingual aligned parallel treebank corpus reflecting
contextual information and its applications. In Gilles
Se?rasset, editor, COLING 2004 Multilingual Linguistic
Resources, pages 57?64. COLING, Geneva, Switzerland.
P Vossen, E. Agirre, N. Calzolari, C. Fellbaum, S. Hsieh,
C. Huang, H. Isahara, K. Kanzaki, A. Marchetti,
M. Monachini, F. Neri, R. Raffaelli, G. Rigau, and
M. Tescon. 2008. KYOTO: A system for mining,
structuring and distributing knowledge across languages
and cultures. In Proceedings of the Sixth International
Language Resources and Evaluation (LREC?08). Mar-
rakech, Morocco.
Piek Vossen, editor. 1998. Euro WordNet. Kluwer.
8
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 139?144,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Thai WordNet Construction 
 
 
Sareewan Thoongsup1 
Kergrit Robkop1 
Chumpol Mokarat1 
Tan Sinthurahat1 
1 Thai Computational Linguistics Lab. 
NICT Asia Research Center, Thailand 
{sareewan, kergrit,   
Chumpol, tan, thatsanee, 
virach}@tcllab.org 
Thatsanee Charoenporn 1,2 
Virach Sornlertlamvanich 1,2 
Hitoshi Isahara 3 
2National Electronics and Computer 
Technology Center Thailand, Thailand 
3National Institute of Information and 
Communications Technology, Japan 
isahara@nict.go.jp 
 
  
 
Abstract 
This paper describes semi-automatic construc-
tion of Thai WordNet and the applied method 
for Asian wordNet. Based on the Princeton 
WordNet, we develop a method in generating 
a WordNet by using an existing bi-lingual dic-
tionary. We align the PWN synset to a bi-
lingual dictionary through the English equiva-
lent and its part-of-speech (POS), automati-
cally. Manual translation is also employed af-
ter the alignment. We also develop a web-
based collaborative workbench, called KUI 
(Knowledge Unifying Initiator), for revising 
the result of synset assignment and provide a 
framework to create Asian WordNet via the 
linkage through PWN synset.     
1 Introduction 
The Princeton WordNet (PWN) (Fellbuam, 
1998) is one of the most semantically rich Eng-
lish lexical banks widely used as a resource in 
many research and development. WordNet is a 
great inspiration in the extensive development of 
this kind of lexical database in other languages. 
It is not only an important resource in imple-
menting NLP applications in each language, but 
also in inter-linking WordNets of different lan-
guages to develop multi-lingual applications to 
overcome the language barrier. There are some 
efforts in developing WordNets of some lan-
guages (Atserias and et al, 1997; Vossen, 1997; 
Farrers and et al, 1998; Balkova and et al, 2004; 
Isahara and et al, 2008). But the number of lan-
guages that have been successfully developed  
 
 
 
their WordNets is still limited to some active 
research in this area. This paper, however, is the 
one of that attempt.  
This paper describes semi-automatic construc-
tion of Thai WordNet and the applied method for 
Asian WordNet. Based on the Princeton Word-
Net, we develop a method in generating a 
WordNet by using an existing bi-lingual diction-
ary. We align the PWN synset to a bi-lingual 
dictionary through the English equivalent and its 
part-of-speech (POS), automatically. Manual 
translation is also employed after the alignment. 
We also develop a web-based collaborative 
workbench, called KUI (Knowledge Unifying 
Initiator), for revising the result of synset as-
signment and provide a framework to create 
Asian WordNet via the linkage through PWN 
synset. 
The rest of this paper is organized as follows: 
Section 2 describes how we construct the Thai 
WordNet, including approaches, methods, and 
some significant language dependent issues ex-
perienced along the construction. Section 3 pro-
vides the information on Asian WordNet con-
struction and progress. And Section 4 concludes 
our work.  
2 Thai WordNet Construction Proce-
dure 
Different approaches and methods have been 
applied in constructing WordNet of many lan-
guages according to the existing lexical re-
sources. This section describes how Thai Word-
Net is constructed either approach or method. 
139
2.1 Approaches 
To build language WordNet from scratch, two 
approaches were brought up into the discussion: 
the merge approach and the expand approach.  
The merge approach is to build the taxonomies 
of the language; synsets and relations, and then 
map to the PWN by using the English equivalent 
words from existing bilingual dictionaries.  
The expand approach is to map or translate lo-
cal words directly to the PWN's synsets by using 
the existing bilingual dictionaries. 
Employing the merge approach, for Thai as an 
example, we will completely get synsets and re-
lations for the Thai language. But it is time and 
budget consuming task and require a lot of 
skilled lexicographers as well, while less time 
and budget is used when employing the expand 
approach to get a translated version of WordNet. 
But some particular Thai concepts which do not 
occur in PWN will not exist in this lexicon. 
Comparing between these two approaches, the 
Thai WordNet construction intended to follow 
the expand approach by this following reasons; 
 
z Many languages have developed their 
own WordNet using the PWN as a 
model, so we can link Thai lexical data-
base to those languages.  
z The interface for collaboration for other 
languages can be easily developed. 
2.2 Methods 
As presented above, we follow the expand ap-
proach to construct the Thai WordNet by trans-
lating the synsets in the PWN to the Thai lan-
guage. Both automatic and manual methods are 
applied in the process.  
2.2.1 Automatic Synset Alignment  
Following the objective to translate the PWN to 
Thai, we attempted to use the existing lexical 
resources to facilitate the construction. We pro-
posed an automatic method to assign an appro-
priate synset to a lexical entry by considering its 
English equivalent and lexical synonyms which 
are most commonly encoded in a bi-lingual dic-
tionary. (Charoenporn 2008; Sornlertlamvanich, 
2008).    
 
 
 
 
 
 
 WordNet (synset) TE Dict (entry)
 total Assigned total assigned 
Noun 145,103 18,353 
(13%) 
43,072 11,867 
(28%)
Verb 24,884 1,333 
(5%) 
17,669 2,298 
(13%)
Adjective 31,302 4,034 
(13%) 
18,448 3,722 
(20%)
Adverb 5,721 737 
(13%) 
3,008 1,519 
(51%)
Total 207,010 24,457 
(12%) 
82,197 19,406 
(24%)
 
Table 1. Synset assignment to entries in 
Thai-English dictionary 
 
For the result, there is only 12% of the total 
number of the synsets that were able to be as-
signed to Thai lexical entries. And about 24% of 
Thai lexical entries were found with the English 
equivalents that meet one of our criteria. Table 1 
shows the successful rate in assigning synsets to 
the lexical entry in the Thai-English Dictionary.  
Considering the list of unmapped lexical entry, 
the errors can be classified into three groups, as 
the following. 
1. The English equivalent is assigned in a 
compound, especially in case that there 
is no an appropriate translation to repre-
sent exactly the same sense. For exam-
ple, 
L: ??????????? raan3-khaa3-pleek1 
E: retail shop 
2. Some particular words culturally used I 
one language may not be simply trans-
lated into one single word sense in Eng-
lish. In this case, we found it explained 
in a phrase. For example,  
L: ???????? kan0-jeak1 
E: bouquet worn over the ear  
3. Inflected forms i.e. plural, past partici-
ple, are used to express an appropriate 
sense of a lexical entry. This can be 
found in non-inflection languages such 
as Thai and most of Asian languages, 
For example,  
L: ???????? raaw3-ra0-thom0 
E: greived 
By using this method, a little part of PWN has 
been translated into Thai. About 88% of the total 
number of the synsets still cannot be assigned.   
Manual step is therefore applied.   
140
2.2.2 Manual Construction 
Human translation is our next step for synset 
translation. Two important issues were taken into 
discussion, when starting the translation process.  
Those are; 
? How to assign or translate new concepts 
that still do not occur in the Thai lexicon. 
Compound word or phrase is acceptable 
or not. 
? Which equivalent do we need to consider, 
synset-to-synset equivalent or word-to-
word equivalent? 
For the first issue, we actually intend to trans-
late the PWN synsets into single Thai word only. 
But problems occurred when we faced with con-
cept that has not its equivalent word. For exam-
ple, 
  
filly#1 -- (a young female horse under the age 
of four) 
colt2#1 ? (a young male horse under the age 
of four) 
hog2#2, hogget#1, hogg#2 ? (a sheep up to 
the age of one year: one yet to be sheared) 
 
There is not any word that conveys the mean-
ing of the above concepts. That is because of the 
difference of the culture. In this case, phrase or 
compound word will be introduced to use as the 
equivalent word of the concept. This phenome-
non always occurs with cultural dependent con-
cept, technical terms and new concepts.  
As for the second issue, considering between 
(1) synset-to-synset equivalent assignment or (2) 
word-to-word equivalent assignment has to be 
discussed. Let consider the following concept of 
?dog? in the PWN. 
 
dog#1, domestic dog#1, Canis familiaris#1 -- 
(a member of the genus Canis (probably de-
scended from the common wolf) that has been 
domesticated by man since prehistoric times; 
occurs in many breeds; "the dog barked all 
night")  
 
The above synset consists of three words; dog, 
domestic dog, and Canis familiaris. The set of 
Thai synonyms that is equivalent to this English 
synset is the following. 
 
Thai synset of ?dog? 
{T1 ??? maa4 ?dog? (normal word),  
  T2 ????? su1-nak3 ?dog? (polite word),  
  T3 ????????? su1-nak3-baan0 ?domestic dog?,  
  T4 ????? ??????????? kha0-nis3-fae0-mi0-lia0-ris3 
?Canis familiaris?} 
These words have the same concepts but are 
different in usage. How do we choose the right 
Thai word for the right equivalent English word? 
It is a crucial problem. In the paragraph below, 
three English words which represent the concept 
?dog? are used in the different context and can-
not be interchanged. Similarly, T1, T2 and T3 
cannot be used substitutionally. Because it con-
veys different meaning. Therefore, word-to-word 
is our solution.   
 
?...Dog usually means the domestic dog, 
Canis lupus familiaris (or "Canis familiaris" in 
binomial nomenclature)....? 
 
Dog  T1 ??? maa4 ?dog?,  
T2 ????? su1-nak3 ?dog?   
Domestic dog  T3 ????????? 
su1-nak4-baan0  
?domestic dog? 
Canis familiaris T4 ????? ???????????  
kha0-nis3-fae0-mi0-
lia0-ris3 
?Canis familiaris? 
 
    Consequently, word-to-word equivalent is 
very useful for choosing the right synonyms with 
the right context. 
     In conclusion, the main principle for the Eng-
lish to Thai translation includes 
(1) ?Single word? is lexicalized the existence 
of concepts in Thai.  
(2) ?Compound? or ?Phrase? is represented 
some concepts that are not lexicalized in 
Thai. 
(3) Synset-to-synset equivalent is used for 
finding Thai synset that is compatible 
with PWN synset. 
(4) Word-to-word equivalent is used for find-
ing the right Thai word that is compatible 
with PWN word in each synset.  
2.3 Language Issues  
This section describes some significant charac-
teristics of Thai that we have to consider care-
fully during the translation process.    
 
141
2.3.1 Out of concepts in PWN 
There are some Thai words/concepts that do not 
exist in the PWN, especially cultural-related 
words. This is the major problem we have to 
solve during the translation.  
One of our future plans is to add synsets that 
do not exist into the PWN.  
2.3.2 Concept differentiation 
Some concepts in the PWN are not equal to Thai 
concepts. For example, a synset {appear, come 
out} represents one concept ?be issued or pub-
lished? in English, but meanwhile, it represents 
two concepts in Thai, the concept of printed mat-
ter, and the concept of film or movie, respec-
tively.   
2.3.3 Concept Structure differentiation 
In some cases, the level of the concept relation 
between English and Thai is not equal. For ex-
ample, {hair} in the PWN represents a concept 
of ?a covering for the body (or parts of it) con-
sisting of a dense growth of threadlike structures 
(as on the human head); helps to prevent heat 
loss; ?? but in Thai, it is divided into two con-
cepts;  
 
T1 ?? khon4 ?hair? 
= ?hair? that cover the body  
T2 ?? phom4 ?hair? 
= ?hair? that cover on the human head  
 
This shows the nonequivalent of concept. 
Moreover, it also differs in the relation of con-
cept. In PWN ?hair? is a more general concept 
and ?body hair? is more specific concepts. But in 
Thai T1 ?? khon4 ?hair? (hair that covers the 
body) is more general concept and T2 ?? phom5 
?hair? (hair that covers on the human head) is 
more specific one.  
2.3.4 Grammar and usage differentiation  
? Part of speech  
? ?Classifier? is one of Thai POS 
which indicates the semantic 
class to which an item belongs. 
It's widely use in quantitative 
expression. For example, ??? 
knon? used with 'person', ????? 
lang? used with house. 
? Some adjectives in English, such 
as ?beautiful?, 'red' and so on can 
function as the adjective and at-
tribute verb in Thai.  
? Social factors determining language us-
age 
? In Thai, some social factors, 
such as social status, age, or sex 
play an important role to deter-
mine the usage of language. For 
example, these following three 
words ??? kin0, ??? chan4 and ???? 
sa0-waey4, having the same 
meaning ?eat?, are used for dif-
ferent social status of the listener 
or referent. These words cannot 
be grouped in the same synset 
because of their usage. 
3 From Thai to Asian WordNet  
AWN, or Asian WordNet, is the result of the col-
laborative effort in creating an interconnected 
WordNet for Asian languages. Starting with the 
automatic synset assignment as shown in section 
2, we provide KUI (Knowledge Unifying Initia-
tor) (Sornlertlamvanich, 2006), (Sornlertlam-
vanich et al, 2007) to establish an online col-
laborative work in refining the WorNets. KUI is 
community software which allows registered 
members including language experts revise and 
vote for the synset assignment. The system man-
ages the synset assignment according to the pre-
ferred score obtained from the revision process. 
As a result, the community WordNets will be 
accomplished and exported into the original form 
of WordNet database. Via the synset ID assigned 
in the WordNet, the system can generate a cross 
language WordNet result. Through this effort, an 
initial version of Asian WordNet can be fulfilled.  
3.1 Collaboration on Asian WordNet 
Followings are our pilot partners in putting 
things together to make KUI work for AWN. 
? Thai Computational Linguistics Labora-
tory TCL), Thailand  
? National Institute of Information and 
Communications Technology (NICT), 
Japan  
? National Electronics and Computer Tech-
nology Center (NECTEC), Thailand 
? Agency for the Assessment and Applica-
tion of Technology (BPPT), Indonesia 
142
? National University of Mongolia (NUM), 
Mongolia 
? Myanmar Computer Federation (MCF), 
Myanmar 
 
 
 
Figure 1. Collaboration on Asian WordNet 
 
3.2 How words are linked 
In our language WordNet construction, lexical 
entry of each language will be mapped with the 
PWN via its English equivalent. On the process 
of mapping, a unique ID will be generated for 
every lexical entry which contains unique 
sense_key and synset_offset from PWN. Exam-
ples of the generated ID show in Table 2. When 
a word with a unique ID is translated into any 
language, the same unique ID will be attached to 
that word automatically. By this way, the lexicon 
entry in the community can be linked to the each 
other using this unique ID.  
 
 
 
 
 
 
 
 
 
 
 
Table 2. Examples of the unique index with 
sense_key and synset_offset 
3.3 Progress on Thai WordNet and Asian 
WordNet 
This section presents the progress on Asian 
WordNet and Thai WordNet construction. 
3.3.1 Current Asian WordNet 
At present, there are ten Asian languages in the 
community. The amount of the translated synsets 
has been continuously increased. The current 
amount is shown in the table 3. As shown in the 
table, for example, 28,735 senses from 117,659 
senses have been translated into Thai. 
 
Language Synset (s) % of total 
117,659 
senses 
Thai 28,735 24.422 
Korean 23,411 19.897 
Japanese 21,810 18.537 
Myanmar 5,701 4.845 
Vietnamese 3,710 3.153 
Indonesian 3,522 2.993 
Bengali 584 0.496 
Mongolian 424 0.360 
Nepali 13 0.011 
Sudanese 11 0.009 
Assamese 2 0.008 
Khmer 2 0.002 
 
Table 3. Amount of senses translated in  
each language 
3.3.2 Sense Sharing 
Table 4 shows the amount of senses that have 
been conjointly translated in the group of lan-
guage. For example, there are 6 languages that 
found of the same 540 senses.   
 
Language Sense (s) %  
1-Language 27,413 55.598 
2-Language 11,769 23.869 
3-Language 5,903 11.972 
4-Language 2,501 5.072 
5-Language 1,120 2.272 
6-Language 540 1.095 
7-Language 53 0.107 
8-Language 4 0.008 
9-Language 2 0.004 
10-Language 1 0.002 
Total 49,306 100.000 
 
Table 4. Amount of senses translated  
in each language 
3.3.3 Amount of Words in Thai synsets  
From the synset in Thai WordNet, there are the 
minimum of one word (W1) in a synset and the 
maximum of six words (W6) in a synset. The 
percentage shown in Table 5 presents that 
89.78% of Thai synset contain only one word.  
 
 
 
 
143
Amount of word 
in Thai Synset 
Sense (s) %  
W1 19,164 89.78 
W2 1,930 9.04 
W3 211 0.99 
W4 27 0.13  
W5 4 0.02 
W6 8 0.04 
Total 21,344 100.00 
 
Table 5. Amount of Word in Thai synsets 
4 Conclusion  
In this paper we have described the methods of 
Thai WordNet construction. The semi-auto 
alignment method constructed the database by 
using the electronic bilingual dictionary. The 
manual method has constructed by experts and 
the collaborative builders who works on the web 
interface at www.asianwordnet.org.  
References  
Christiane Fellbuam. (ed.). 1998. WordNet: An 
Electronic Lexical Database. MIT Press, 
Cambridge, Mass. 
Xavier Farreres, German Rigau and Horacio 
Rodriguez. 1998. Using WordNet for building 
WordNets. In: Proceedings of the COL-
ING/ACL Workshop on Usage of WordNet in 
Natural Language Processing Systems, Mont-
real. 
Hitoshi Isahara, Francis Bond, Kiyotaka Uchi-
moto, Masao Utiyama and Kyoko Kanzaki. 
2008. Development of the Japanese WordNet. 
In LREC-2008, Marrakech. 
Jordi Atserias, Salvador Climent, Xavier Far-
reres, German Rigau and Horacio Rodriguez. 
1997. Combinding multiple Methods for the 
automatic Construction of Multilingual 
WordNets. In proceedings of International 
Conference ?Recent Advances in Natural 
Language Processing? (RANLP?97). Tzigov 
Chark, Bulgaria. 
Piek Vossen, 1997. EuroWordNet: a multilingual 
database for information retrieval. In proceed-
ings of DELOS workshop on Cross-language 
Information Retrieval, March 5-7, 1997, Zu-
rich. 
Thatsanee Charoenporn, Virach Sornlertlam-
vanich, Chumpol Mokarat, and Hitoshi Isa-
hara. 2008. Semi-automatic Compilation of 
Asian WordNet, In proceedings of the 14th 
NLP2008, University of Tokyo, Komaba 
Campus, Japan, March 18-20, 2008. 
Valenina Balkova, Andrey Suhonogov, Sergey  
Yablonsky. 2004. Rusian WordNet: From 
UML-notation to Internet/Infranet Database 
Implementation. In Porceedings of the Second 
International WordNet Conference (GWC 
2004), pp.31-38. 
Virach Sornlertlamvanich, Thatsanee Charoen-
porn, Chumpol Mokarat, Hitoshi Isahara, 
Hammam Riza, and Purev Jaimai. 2008. 
Synset Assignment for Bi-lingual Dictionary 
with Limited Resource. In proceedings of the 
Third International Joint Conference on Natu-
ral Language Processing (IJCNLP2008), Hy-
derabad, India, January 7-12, 2008. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
144
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 179?186,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Word Segmentation Standard in Chinese, Japanese and Korean 
 
Key-Sun Choi 
KAIST 
Daejeon Korea 
kschoi@kaist.ac.kr 
Hitoshi Isahara 
NICT 
Kyoto Japan 
isahara@nict.go.jp 
Kyoko Kanzaki
NICT 
Kyoto Japan 
kanzaki@nict.go.jp
Hansaem Kim
National Inst. 
Korean Lang.
Seoul  Korea
thesis00@korea.kr
Seok Mun Pak 
Baekseok Univ. 
Cheonan Korea 
smpark@bu.ac.kr 
Maosong Sun 
Tsinghua Univ.
Beijing China 
sms@tsinghua.edu.cn
 
Abstract 
Word segmentation is a process to divide a 
sentence into meaningful units called ?word 
unit? [ISO/DIS 24614-1]. What is a word 
unit is judged by principles for its internal in-
tegrity and external use constraints. A word 
unit?s internal structure is bound by prin-
ciples of lexical integrity, unpredictability 
and so on in order to represent one syntacti-
cally meaningful unit. Principles for external 
use include language economy and frequency 
such that word units could be registered in a 
lexicon or any other storage for practical re-
duction of processing complexity for the fur-
ther syntactic processing after word segmen-
tation. Such principles for word segmentation 
are applied for Chinese, Japanese and Korean, 
and impacts of the standard are discussed. 
1 Introduction 
Word segmentation is the process of dividing of 
sentence into meaningful units. For example, 
?the White House? consists of three words but 
designates one concept for the President?s resi-
dence in USA. ?Pork? in English is translated 
into two words ?pig meat? in Chinese, Korean 
and Japanese. In Japanese and Korean, because 
an auxiliary verb must be followed by main verb, 
they will compose one word unit like ?tabetai? 
and ?meoggo sipda? (want to eat). So the word 
unit is defined by a meaningful unit that could be 
a candidate of lexicon or of any other type of 
storage (or expanded derived lexicon) that is use-
ful for the further syntactic processing. A word 
unit is more or less fixed and there is no syntactic 
interference in the inside of the word unit. In the 
practical sense, it is useful for the further syntac-
tic parsing because it is not decomposable by 
syntactic processing and also frequently occurred 
in corpora.  
There are a series of linguistic annotation 
standards in ISO: MAF (morpho-syntactic anno-
tation framework), SynAF (syntactic annotation 
framework), and others in ISO/TC37/SC4 1 . 
These standards describe annotation methods but 
not for the meaningful units of word segmenta-
tion. In this aspect, MAF and SynAF are to anno-
tate each linguistic layer horizontally in a stan-
dardized way for the further interoperability. 
Word segmentation standard would like to rec-
ommend what word units should be candidates to 
be registered in some storage or lexicon, and 
what type of word sequences called ?word unit? 
should be recognized before syntactic processing. 
In section 2, principles of word segmentation 
will be introduced based on ISO/CD 24614-1. 
Section 3 will describe the problems in word 
segmentation and what should be word units in 
each language of Chinese, Japanese and Korean. 
The conclusion will include what could be 
shared among three languages for word segmen-
tation. 
2 Word Segmentation: Framework and 
Principles 
Word unit is a layered pre-syntactical unit. That 
means that a word unit consists of the smaller 
word units. But the maximal word unit is fre-
quently occurred in corpora under the constraints 
that the syntactic processing will not refer the 
internal structure of the word unit 
Basic atoms of word unit are word form, mor-
pheme including bound morpheme, and non-
lexical items like punctuation mark, numeric 
string, foreign character string and others as 
shown in Figure 1. Usually we say that ?word? is 
lemma or word form. Word form is a form that a 
lexeme takes when used in a sentence. For ex-
ample, strings ?have?, ?has?, and ?having? are 
word forms of the lexeme HAVE, generally dis-
tinguished by the use of capital letters. [ISO/CD 
24614-1] Lemma is a conventional form used to 
represent a lexeme, and lexeme is an abstract 
unit generally associated with a set of word 
forms sharing a common meaning. 
                                                 
1 Refer to http://www.tc37sc4.org/ for documents 
MAF, SynAF and so on. 
179
 
Figure 1. Configuration of Word Unit 
 
BNF of word unit is as follows: 
<word unit> ::= <word form> | <morpheme> | 
<non lexical items> | <word unit>, 
where <word unit> is recursively defined be-
cause a longer word unit contains smaller word 
units. 
Bunsetsu in Japanese is the longest word unit, 
which is an example of layered maximized pre-
syntactical unit. Eojeol in Korean is a spacing 
unit that consists of one content word (noun, 
verb, adjective or adverb) plus a sequence of 
functional elements. Such language-specific 
word units will be described in section 3.  
Principles for word segmentation will set the 
criteria to validate each word unit, to recognize 
its internal structure and non-lexical word unit, to 
be a candidate of lexicon, and other consistency 
to be necessitated by practical applications for 
any text in any language. The meta model of 
word segmentation will be explained in the 
processing point of view, and then their prin-
ciples of word units in the following subsections.  
2.1 Metamodel of Word Segmentation 
A word unit has a practical unit that will be later 
used for syntactic processing. While the word 
segmentation is a process to identify the longest 
word unit and its internal structure such that the 
word unit is not the object to interfere any syn-
tactic operation, ?chunking? is to identify the 
constituents but does not specify their internal 
structure. Syntactic constituent has a syntactic 
role, but the word unit is a subset of syntactic 
constituent. For example, ?blue sky? could be a 
syntactic constituent but not a word unit. Figure 
2 shows the meta model of word segmentation. 
[ISO CD 24614-1]  
2.2 Principles of Word Unit Validation 
Principles for validating a word unit can be ex-
plained by two perspectives: one is linguistic one 
and the other is processing-oriented practical 
perspective.   
In ISO 24614-1, principles from a linguistic 
perspective, there are five principles: principles 
of (1) bound morpheme, (2) lexical integrity, (3) 
unpredictability, (4) idiomatization, and (5) un-
productivity.  
First, bound morpheme is something like ?in? 
of ? inefficient? . The principle of bound mor-
pheme says that each bound morpheme plus 
word will make another word. Second, principle 
of lexical integrity means that any syntactic 
processing cannot refer the internal structure of 
word (or word unit). From the principle, we can 
say that the longest word unit is the maximal 
meaningful unit before syntactic processing. 
Third, another criterion to recognize a word is 
the principle of unpredictability. If we cannot 
infer the real fact from the word, we consider it 
as a word unit. For example, we cannot image 
what is the colour of blackboard because some is 
green, not black. [ISO 24614-1] The fourth prin-
ciple is that the idiom should be one word, which 
could be a subsidiary principle that follows the 
principle of unpredictability. In the last principle, 
unproductivity is a stronger definition of word; 
there is no pattern to be copied to generate an-
other word from this word formation. For exam-
ple, in ????  (white vegetable) in Chinese, 
there is no other coloured vegetable like ?blue 
vegetable.? 
Another set of principles is derived from the 
practical perspective. There are four principles: 
frequency, Gestalt, prototype and language 
economy. Two principles of frequency and lan-
guage economy are to recognize the frequent 
occurrence in corpora. Gestalt and prototype 
principles are the terms in cognitive science 
about what are in our mental lexicon, and what 
are perceivable words.  
Principle of language economy is to say about 
very practical processing efficiency: ?if the in-
clusion of a word (unit) candidate into the lex-
icon can decrease the difficulty of linguistic 
analysis for it, then it is likely to be a word 
(unit).? 
Gestalt principle is to perceive as a whole. ?It 
supports some perceivable phrasal compounds 
into the lexicon even though they seem to be free 
combinations of their perceivable constituent 
parts,? [ISO/CD 24614-1] where the phrasal 
compound is frequently used linguistic unit and 
its meaning is predictable from its constituent 
elements. Similarly, principle of prototype pro-
180
vides a rationale for including some phrasal 
compounds into lexicon, and phrasal compounds 
serve as prototypes in a productive word forma-
tion pattern, like ?apple pie? with the pattern 
?fruit + pie? into the lexicon.  
 
 
Figure 2. Meta model of word segmentation proess 
[ISO/CD 24614-1] 
2.3 Principles for Word Unit Formation 
As a result of word segmentation of sentence, we 
will get word units. These principles will de-
scribe the internal structure of word unit. They 
have four principles: granularity, scope maximi-
zation of affixations, scope maximization of 
compounding, and segmentation for other strings. 
In the principle of granularity, a word unit has its 
internal structure, if necessary for various appli-
cation of word segmentation.  
Principles of scope maximization of affixa-
tions and compounding are to recognize the 
longest word unit as one word unit even if it is 
composed of stem + affixes or compound of 
word units. For example, ?unhappy? or ?happy? 
is one word unit respectively. ?Next generation 
Internet? is one word unit. The principle of seg-
mentation for other strings is to recognize non-
lexical strings as one word unit if it carries some 
syntactic function, for example, 2009 in ?Now 
this year is 2009.?  
3 Word Segmentation for Chinese, Jap-
anese and Korean 
If the word is derived from Chinese characters, 
three languages have common characteristics. If 
their word in noun consists of two or three Chi-
nese characters, they will be one word unit if 
they are ?tightly combined and steadily used.? 
Even if it is longer length, it will be a word unit 
if it is fixed expression or idiom. But if the first 
character is productive with the following num-
eral, unit word or measure word, it will be seg-
mented. If the last character is productive in a 
limited manner, it forms a word unit with the 
preceding word, for example, ????? (Tokyo 
Metropolis), ?8?? (August) or ????? (acce-
lerator). But if it is a productive suffix like plural 
suffix and noun for locality, it is segmented in-
dependently in Chinese word segmentation rule, 
for example, ???|?? (friends), ???|??? 
(north of the Yangtzi River ) or ???|?? (on 
the table) in Chinese. They may have different 
phenomena in each language. 
Negation character of verb and adjective is 
segmented independently in Chinese, but they 
form one word unit in Japanese. For example, 
?yasashikunai? (?????, not kind) is one 
word unit in Japanese, but ??|?? (not to write),  
??| ?? (cannot),  ??|??? (did not research) 
and ?? | ??? (not completed) will be seg-
mented independently in Chinese. In Korean, 
?chinjeolhaji anhta? (???? ??, not kind) 
has one space inserted between two eojeols but it 
could be one word unit. ?ji anhta? makes nega-
tion of adjectival stem ?chinjeolha?.  
We will carefully investigate what principles 
of word units will be applied and what kind of 
conflicts exists. Because the motivation of word 
segmentation standard is to recommend what 
word units should be registered in a type of lex-
icon (where it is not the lexicon in linguistics but 
any kind of practical indexed container for word 
units), it has two possibly conflicting principles. 
For example, principles of unproductivity, fre-
quency, and granularity could cause conflicts 
because they have different perspectives to de-
fine a word unit.  
3.1 Word Segmentation for Chinese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 13 types: noun, verb, adjective, 
pronoun, numeral, measure word, adverb, prepo-
sition, conjunction, auxiliary word, modal word, 
exclamation word, imitative word. 
3.1.1 Noun 
There is word unit specification for common 
nouns as follows: 
- Two-character word or compact two-character 
noun phrase, e.g., ??(beef) ??(steel) 
181
- Noun phrase with compact structure, if violate 
original meaning after segmentation, e.g., ??
?? (Active power) 
- Phrase consisting of adjective with noun, e.g., 
?? (green leave) 
- The meaning changed phrase consisting of ad-
jective, e.g., ???(young wife)  
- Prefix with noun, e.g., ??(elder brother) ?
? (old eagle) ??? (nonmetal)  ???
(ultrasonic) 
- Noun segmentation unit with following postfix 
(e.g. ? ? ? ? ? ? ? ? ?), e.g., ???
(scientist) 
- Noun segmentation unit with prefix and postfix, 
e.g., ???(superconductance) 
- Time noun or phrase, e.g., ??(May), 11 ? 
42 ? 8 ?(forty two minutes and eight seconds 
past eleven), ??(the day before yesterday), 
??(First day of a month in the Chinese lunar 
calendar )   
But the followings are independent word 
units for noun of locality (e.g., ??|? (on the 
table), ??|?? (north of the Yangtzi River)), 
and the ??? suffix referring to from a plural of 
front noun (e.g., ?? ?(friends) ) except ??
??, ?????(pals),  ?????(guys), etc. Prop-
er nouns have similar specification. 
3.1.2 Verb 
The following verb forms will be one word unit 
as: 
- Various forms of reiterative verb, e.g., ??
(look at), ????(come and go) 
- Verb?object structural word, or compact and 
stably used verb phrase, e.g., ??(meeting)  ?
?(dancing) 
- Verb?complement structural word (two-
character), or stably used Verb-complement 
phrase (two-character), e.g., ??(improve) 
- Adjective with noun word, and compact, and 
stably used adjective with noun phrase, e.g., ?
?(make trouble) ,  ??(talk nonsense) 
- Compound directional verb, e.g., ??(go out)  
??(come in). 
But the followings are independent word 
units: 
- ?AAB, ABAB? or ?A? A, A? A, A?? A?, 
e.g., ??|??(have a discuss), ?|?|? (have 
a good chat) 
- Verb delimited by a negative meaning charac-
ters, e.g., ?|?(not to write)   ?|?(cannot)    
?|??(did not research)    ?|??(not com-
pleted) 
- ?Verb + a negative meaning Chinese character 
+  the same verb" structure, e.g., ??|?|?(say 
or not say)?? 
- Incompact or verb?object structural phrase with 
many similar structures, e.g., ? |?(Eat fish)    
?|??(learn skiing) 
- ?2with1? or ?1with2? structural verb- comple-
ment phrase, e.g., ??|?(clean up), ?|??
(speak clearly),  ??|??(explain clearly) 
- Verb-complement word for phrase, if inserted 
with ?? or ??, e.g., ?|?|? (able to knock 
down), and compound directional verb of direc-
tion, e.g., ?|?|?(able to go out) 
- Phrase formed by verb with directional verb, 
e.g., ?|?(send), ?|?|?(run out) 
- Combination of independent single verbs with-
out conjunction, e.g., ?|?(cover with), ?|?,  
?|? (listen, speaking, read and write) 
- Multi-word verb without conjunction, e.g., ?
?|??(investigate and research) 
3.1.3 Adjective 
One word unit shall be recognized in the follow-
ing cases: 
- Adjective in reiterative form of ?AA, AABB, 
ABB, AAB, A+"?"+AB?, e.g., ??(big), ?
???(careless), except the adjectives in rei-
terative form of ?ABAB?, e.g., ?? |??
(snowy white)     
- Adjective phrase in from of ?? A? B??
A? B?? A? B?? A? B?? A? B?, 
e.g., ????(wholeheartedly) 
- Two single-character adjectives with word fea-
tures varied, ?? (long-short)  ?? (deep-
shallow)  ??(big-small) 
- Color adjective word or phrase, e.g., ??(light 
yellow)   ???(olive green) 
But the followings are segmented as indepen-
dent word units: 
- Adjectives in parataxis form and maintaining 
original adjective meaning, e.g., ? |???
(size), ?? |??(glory) 
- Adjective phrase in positive with negative form 
to indicate question, e.g., ??| ?| ??(easy 
or not easy), except the brachylogical one like 
????(easy or not). 
3.1.4 Pronoun 
The followings shall be one word unit: 
182
- Single-character pronoun with ???, e.g.,?? 
(we) 
- ??????? with unit word ??? or ???
????????, e.g., ??(this) 
- Interrogative adjective or phrase, e.g., ??
(how many) 
But, the following will be independent word 
units: 
- ???????  with numeral , unit word or 
noun word segmentation unit, e.g., ? |? ?
(these 10 days), ?| ?(that person) 
- Pronoun of ???????????????, 
etc. shall be segmented from followed measure 
word or noun, e.g., ?| ? (each country), ?| 
?(each type). 
3.1.5 Numeral 
The followings will be one word unit: 
- Chinese digit word, e.g., ?????????
???(180,040,723) 
- ???? percent in fractional number, e.g., ? 
???(third fifth) 
- Paratactic numerals indicating approximate 
number, e.g., ?? ??(eight or nine kg) 
On the other hand, Independent word unit cas-
es are as follows: 
- Numeral shall be segmented from measure 
word, e.g., ?| ?(three) 
- Ordinal number of ???  shall be segmented 
from followed numeral, e.g., ? ? (first) 
- ?????????????, used after adjec-
tive or verb for indicating approximate number. 
3.1.6 Measure word 
Reiterative measure word and compound meas-
ure word or phrase is a word unit, e.g., ??
(every year), ?? man/year. 
3.1.7 Adverb 
Adverb is one word unit. But????????
???, etc. acting as conjunction shall be seg-
mented, e.g., ? ? ? ?(sweet yet savory). 
3.1.8 Preposition 
It is one word unit. For example, ??(be born 
in), and  ????(according to the regulations). 
3.1.9 Conjunction 
Conjunction shall be deemed as segmentation 
unit. 
3.1.10 Auxiliary word 
Structural auxiliary word ????????? 
and tense auxiliary word ??????? are one 
word unit, e.g., ? |? |?  (his book), ? |?
(watched). But the auxiliary word ??? shall be 
segmented from its followed verb, e.g., ?  ?
(what one thinks).  
3.1.11 Modal word 
It is one word unit, e.g., ???? (How are 
you?). 
3.1.12 Exclamation word 
Exclamation word shall be deemed as segmenta-
tion unit. For example, ??????? (How 
beautiful it is!) 
3.1.13 Imitative word 
It is one word unit like ???? (tinkle).  
3.2 Word Segmentation for Japanese 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into mainly 10 types: meishi (noun), 
doushi (verb), keiyoushi (adjective), rentaishi 
(adnominal noun: only used in adnominal usage), 
fukushi (adverb), kandoushi (exclamation), set-
suzoushi (conjunction), setsuji (affix), joshi (par-
ticle), and jodoushi (auxiliary verb). These parts 
of speech are divided into more detailed classes 
in terms of grammatical function. 
The longest "word segmentation" corresponds 
to ?Bunsetsu? in Japanese. 
3.2.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle or auxiliary 
verb (e.g. ???? (neko_ga) which is composed 
from ?Noun + a particle for subject marker?). 
Also, if a word like an adjective or adnominal 
noun modifies a noun, then a modifier (adjec-
tives, adnominal noun, adnominal phrases) and a 
modificand (a noun) are not segmented. 
3.2.2 Verb 
A Japanese verb has an inflectional ending. The 
ending of a verb changes depending on whether 
it is a negation form, an adverbial form, a base 
form, an adnominal form, an assumption form, or 
an imperative form. Japanese verbs are often 
used with auxiliary verbs and/or particles, and a 
verb with auxiliary verbs and/or particles is con-
sidered as a word segmentation unit (e.g. ???
??? ? (aruki_mashi_ta) is composed from 
?Verb + auxiliary verb for politeness + auxiliary 
verb for tense?). 
3.2.3 Adjective 
A Japanese adjective has an inflectional ending. 
Based on the type of inflectional ending, there 
183
are two kinds of adjectives, "i_keiyoushi" and 
"na_keiyoushi". However, both are treated as 
adjectives. 
In terms of traditional Japanese linguistics, 
?keiyoushi? refers to ?i_keiyoushi?(e.g. ???, 
utsukushi_i) and ?keiyoudoushi?(e.g. ??? , 
shizuka_na) refers to ?na_keiyoushi.? In terms of 
inflectional ending of ?na_keiyoushi,? it is some-
times said to be considered as ?Noun + auxiliary 
verb (da)?. 
The ending of an adjective changes depending 
on whether it is a negation form, an adverbial 
form, a base form, an adnominal form, or an as-
sumption form. Japanese adjectives in predica-
tive position are sometimes used with auxiliary 
verbs and/or particles, and they are considered as 
a word segmentation unit. 
3.2.4 Adnominal noun 
An adnominal noun does not have an inflectional 
ending; it is always used as a modifier. An ad-
nominal noun is considered as one segmentation 
unit. 
3.2.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. It is considered as one segmentation unit. 
3.2.6 Conjunction 
A conjunction is considered as one segmentation 
unit. 
3.2.7 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.2.8 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
3.2.9 Particle 
Particles can be divided into two main types. 
One is a case particle which serves as a case 
marker. The other is an auxiliary particle which 
appears at the end of a phrase or a sentence. 
Auxiliary particles represent a mood and a 
tense. 
Particles should not be segmented from a word. 
A particle is always used with a word like a noun, 
a verb, or an adjective, and they are considered 
as one segmentation unit. 
3.2.10 Auxiliary verb 
Auxiliary verbs represent various semantic func-
tions such as a capability, a voice, a tense, an 
aspect and so on. An auxiliary verb appears at 
the end of a phrase or a sentence. Some linguist 
consider ??? (da), which is Japanese copura, as 
a specific category such as ???(hanteishi).   
An auxiliary verb should not be segmented 
from a word. An auxiliary verb is always used 
with a word like a noun, a verb, or an adjective, 
and is considered as one segmentation unit. 
3.2.11 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation. For example: 
Kouin yano  gotoshi 
noun  noun+particle auxiliary verb 
time  arrow  like (flying) 
Time flies fast. 
3.2.12 Abbreviation 
An abbreviation should not be segmented. 
3.3 Word Segmentation for Korean 
For convenience of description, the specification 
in this section follows the convention that classi-
fies words into 12 types: noun, verb, adjective, 
pronoun, numeral, adnominal, adverb, exclama-
tion, particle, auxiliary verb, auxiliary adjective, 
and copula. The basic parts of speech can be di-
vided into more detailed classes in terms of 
grammatical function. Classification in this paper 
is in accord with the top level of MAF. 
In addition, we treat some multi-Eojeol units 
as the word unit for practical purpose. Korean 
Eojeol is a spacing unit that consists of one con-
tent word (like noun, verb) and series of func-
tional elements (particles, word endings). Func-
tional elements are not indispensable. Eojeol is 
similar with Bunsetsu from some points, but an 
Eojeol is recognized by white space in order to 
enhance the readability that enables to use only 
Hangul alphabet in the usual writing.  
3.3.1 Noun 
When a noun is a member constituting a sentence, 
it is usually followed by a particle. (e.g. 
???_?? (saja_ga, ?a lion is?) which is com-
posed from ?Noun + a particle for subject mark-
er?). Noun subsumes common noun, proper noun, 
and bound noun.  
If there are two frequently concatenated Eoje-
ols that consist of modifier (an adjective or an 
adnominal) and modificand (a noun), they can be 
one word unit according to the principle of lan-
guage economy. Other cases of noun word unit 
are as follows: 
1) Compound noun that consists of two more 
nouns can be a word unit. For example, 
184
???? (son_mok, ?wrist?) where son+mok 
= ?hand?+?neck?. 
2) Noun phrase that denotes just one concept 
can be a word unit. For example, ???? 
??? (yesul_ui jeondang, ?sanctuary of the 
arts? that is used for the name of concert 
hall). 
3.3.2 Verb  
A Korean verb has over one inflectional endings. 
The endings of a verb can be changed and at-
tached depending on grammatical function of 
verb (e.g. ??/??/?/?/?/?/?? (break 
[+emphasis] [+polite] [+past] [+conjectural] final 
ending [+polite]). Compound verb (verb+verb, 
noun+verb, adverb+verb) can be a segmentation 
unit by right. For example, ?????? (dola-
ga-da, ?pass away?) is literally translated into 
?go+back? (verb+verb).  ????? (bin-na-da, 
?be shiny?) is derived from ?light + come out? 
(noun+verb). ?????? (baro-jap-da, ?cor-
rect?) is one word unit but it consists of 
?rightly+hold? (adverb+verb).  
3.3.3 Adjective 
A Korean adjective has inflectional endings like 
verb. For example, in ???/?/?/?/?/?? 
(pretty [+polite] [+past] [+conjectural] final end-
ing [+polite]), one adjective has five endings. 
Compound adjective can be a segmentation unit 
by right. (e.g. ????? (geom-buk-da, ?be 
blackish red?)) 
3.3.4 Adnominal  
An adnominal is always used as a modifier for 
noun. Korean adnominal is not treated as noun 
unlike Japanese one. (e.g. ?? ?? (sae jip, ?new 
house?)? which consist of ?adnominal + noun?). 
3.3.5 Adverb 
An adverb does not have an inflectional ending; 
it is always used as a modifier of a sentence or a 
verb. In Korean, adverb includes conjunction. It 
is considered as one segmentation unit. Com-
pound adverb can be a segmentation unit by right. 
Examples are ???? (bam-nat, ?day and night?), 
and ???? (gotgot, ?everywhere? whose literal 
translation is ?where where?). 
3.3.6 Pronoun 
A pronoun is considered as one segmentation 
unit. Typical examples are ??? (na, ?I?), ??? 
(neo, ?you?), and ???? (uri, ?we). Suffix of 
plural ??? (deul, ?-s?) can be attached to some 
of pronouns in Korean. (e.g. ????? (neohui-
deul, ?you+PLURAL?), ???? (geu-deul, ?they? 
= ?he/she+PLURAL?)). 
3.3.7 Numeral 
A numeral is considered as one segmentation 
unit: e.g. ???? (hana, ?one?), ???? (cheojjae, 
?first?). Also figures are treated as one unit like 
?2009?? (the year 2009). 
3.3.8 Exclamation 
An exclamation is considered as one segmenta-
tion unit. 
3.3.9 Particle 
Korean particles can not be segmented from a 
word just like Japanese particles. A particle is 
always used with a word like a noun, a verb, or 
an adjective, but it is considered as one segmen-
tation unit. 
Particles can be divided into two main types. 
One is a case particle that serves as a case marker. 
The other is an auxiliary particle that appears at 
the end of a phrase or a sentence. Auxiliary par-
ticle represents a mood and a tense. 
3.3.10 Auxiliary verb 
A Korean auxiliary verb represents various se-
mantic functions such as a capability, a voice, a 
tense, an aspect and so on.  
Auxiliary verb is only used with a verb plus 
endings with special word ending depending on 
the auxiliary verb. For example, ???? (boda, 
?try to?), an auxiliary verb has the same inflec-
tional endings but it should follow a main verb 
with a connective ending ??? (eo) or ??? (?go?).  
Consider ?try to eat? in English where ?eat? is a 
main verb, and ?try? is an auxiliary verb with 
specialized connective ?to?. In this case, we need 
two Korean Eojeols that corresponds to ?eat + 
to? and ?try?. Because ?to? is a functional ele-
ment that is attached after main verb ?eat?, it 
constitutes one Eojeol. It causes a conflict be-
tween Eojeol and word unit. That means every 
Eojeol cannot be a word unit. What are the word 
units and Eojeols in this case? There are two 
choices: (1) ?eat+to? and ?try?, (2) ?eat?+ ?to 
try?. According to the definition of Eojeol, (1) is 
correct for two concatenated Eojeols. But if the 
syntactic processing is preferable, (2) is more 
likely to be a candidate of word units.  
3.3.11 Auxiliary adjective 
Unlike Japanese, there is auxiliary adjective in 
Korean. Function and usage of it are very similar 
to auxiliary verb. Auxiliary adjective is consi-
dered as one segmentation unit. 
185
Auxiliary verb can be used with a verb or ad-
jective plus endings with special word ending 
depending on the auxiliary adjective. For exam-
ple, in ??? ??? (meokgo sipda, ?want to 
eat?), sipda is an auxiliary adjective whose mean-
ing is ?want? while meok is a main verb ?want? 
and go corresponds to ?to?; so meokgo is ?to eat?.  
3.3.12 Copula 
A copula is always used for changing the func-
tion of noun. After attaching the copula, noun 
can be used like verb. It can be segmented for 
processing. 
3.3.13 Idiom and proverb 
Proverbs, mottos, etc. should be segmented if 
their original meanings are not violated after 
segmentation like Chinese and Japanese. 
3.3.14 Ending 
Ending is attached to the root of verb and adjec-
tive. It means honorific, tense, aspect, modal, etc. 
There are two endings: prefinal ending and fi-
nal ending. They are functional elements to 
represent honorific, past, or future functions in 
prefinal position, and declarative (-da) or con-
cessive (-na)-functions in final ending. Ending is 
not a segmentation unit in Korean. It is just a unit 
for inflection. 
3.3.15 Affix 
A prefix and a suffix used as a constituent of a 
word should not be segmented as a word unit. 
4 Conclusion 
Word segmentation standard is to recommend 
what type of word sequences should be identified 
as one word unit in order to process the syntactic 
parsing. Principles of word segmentation want to 
provide the measure of such word units. But 
principles of frequency and language economy 
are based on a statistical measure, which will be 
decided by some practical purpose.  
Word segmentation in each language is 
somewhat different according to already made 
word segmentation regulation, even violating one 
or more principles of word segmentation. In the 
future, we have to discuss the more synchronized 
word unit concept because we now live in a mul-
ti-lingual environment. For example, consider 
figure 3. Its English literal translation is ?white 
vegetable and pig meat?, where ?white vegeta-
ble? (??) is an unproductive word pattern and 
forms one word unit without component word 
units, and ?pig meat? in Chinese means one Eng-
lish word ?pork?. So ?pig meat? (??) is the 
longest word unit in this case. But in Japanese 
and Korean, ?pig meat? in Chinese characters 
cannot be two word units, because each compo-
nent character is not used independently. 
 
Figure 3. Basic segmentation and word segmenta-
tion [ISO/CD 24614-1] 
What could be shared among three languages 
for word segmentation? The common things are 
not so much among CJK. The Chinese character 
derived nouns are sharable for its word unit 
structure, but not the whole. On the other hand, 
there are many common things between Korean 
and Japanese. Some Korean word endings and 
Japanese auxiliary verbs have the same functions. 
It will be an interesting study to compare for the 
processing purpose.  
The future work will include the role of word 
unit in machine translation. If the corresponding 
word sequences have one word unit in one lan-
guage, it is one symptom to recognize one word 
unit in other languages. It could be ?principle of 
multi-lingual alignment.?   
The concept of ?word unit? is to broaden the 
view about what could be registered in lexicon of 
natural language processing purpose, without 
much linguistic representation. In the result, we 
would like to promote such language resource 
sharing in public, not just dictionaries of words 
in usual manner but of word units. 
Acknowledgement 
This work has been supported by ISO/TC37, 
KATS and Ministry of Knowledge Economy 
(ROKorea), CNIS and SAC (China), JISC (Ja-
pan) and CSK (DPRK) with special contribution 
of Jeniffer DeCamp (ANSI) and Kiyong Lee. 
References 
ISO CD24614-1, Language Resource Management ? 
Word segmentation of written texts for monolin-
gual and multilingual information processing ? Part 
1: Basic concepts and general principles, 2009.  
ISO WD24614-2, ? Part 2: Word segmentation for 
Chinese, Japanese and Korean, 2009. 
186
Automatic error detection in the Japanese learners? English spoken data
Emi IZUMI?? 
emi@crl.go.jp 
Kiyotaka UCHIMOTO? 
uchimoto@crl.go.jp 
Toyomi SAIGA? 
hoshi@karl.tis.co.jp
Thepchai Supnithi* 
thepchai@nectec.or.th
Hitoshi ISAHARA?? 
isahara@crl.go.jp 
Abstract 
This paper describes a method of 
detecting grammatical and lexical errors 
made by Japanese learners of English 
and other techniques that improve the 
accuracy of error detection with a limited 
amount of training data. In this paper, we 
demonstrate to what extent the proposed 
methods hold promise by conducting 
experiments using our learner corpus, 
which contains information on learners? 
errors. 
1 Introduction 
One of the most important things in keeping up 
with our current information-driven society is the 
acquisition of foreign languages, especially 
English for international communications. In 
developing a computer-assisted language teaching 
and learning environment, we have compiled a 
large-scale speech corpus of Japanese learner 
English, which provides a great deal of useful 
information on the construction of a model for the 
developmental stages of Japanese learners? 
speaking abilities.  
In the support system for language learning, 
we have assumed that learners must be informed 
of what kind of errors they have made, and in 
which part of their utterances. To do this, we need 
to have a framework that will allow us to detect 
learners? errors automatically.  
In this paper, we introduce a method of detect-
ing learners? errors, and we examine to what ex-
tent this could be accomplished using our learner 
corpus data including error tags that are labeled 
with the learners? errors.  
2 SST Corpus 
The corpus data was based entirely on audio-
recorded data extracted from an interview test, the 
?Standard Speaking Test (SST)?. The SST is a 
face-to-face interview between an examiner and 
the test-taker. In most cases, the examiner is a 
native speaker of Japanese who is officially 
certified to be an SST examiner. All the 
interviews are audio-recorded, and judged by two 
or three raters based on an SST evaluation scheme 
(SST levels 1 to 9). We recorded 300 hours of 
data, totaling one million words, and transcribed 
this. 
2.1 Error tags 
We designed an original error tagset for 
learners? grammatical and lexical errors, which 
were relatively easy to categorize. Our error tags 
contained three pieces of information, i.e., the part 
of speech, the grammatical/lexical system and the 
corrected form. We prepared special tags for some 
errors that cannot be categorized into any word 
class, such as the misordering of words. Our error 
tagset currently consists of 45 tags. The following 
example is a sentence with an error tag. 
*I lived in <at 
crr="">the</at> New Jersey. 
at indicates that it is an article error, and 
crr=?? means that the corrected form does not 
?Computational Linguistics Group, Communications Research Laboratory, 
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 
?Graduate School of Science and Technology, Kobe University, 1-1 Rokkodai, Nada-ku, Kobe, Japan 
?TIS Inc., 9-1 Toyotsu, Suita, Osaka, Japan 
*National Electronics and Computer Technology Center, 
112 Pahonyothin Road, Klong 1, Klong Luang, Pathumthani, 12120, Thailand 
need an article. By referring to information on the 
corrected form indicated in an error tag, the sys-
tem can convert erroneous parts into corrected 
equivalents. 
3 Error detection method 
In this section, we would like to describe how 
we proceeded with error detection in the learner 
corpus. 
3.1 Types of errors 
We first divided errors into two groups de-
pending on how their surface structures were dif-
ferent from those of the correct ones. The first was 
an ?omission?-type error, where the necessary 
word was missing, and an error tag was inserted to 
interpolate it. The second was a ?replacement?-
type error, where the erroneous word was en-
closed in an error tag to be replaced by the cor-
rected version. We applied different methods to 
detecting these two kinds of errors. 
3.2 Detection of omission-type errors 
Omission-type errors were detected by estimat-
ing whether or not a necessary word string was 
missing in front of each word, including delimit-
ers. We also estimated to which category the error 
belonged during this process. What we call ?error 
categories? here means the 45 error categories that 
are defined in our error tagset. (e.g. article and 
tense errors) These are different from ?error 
types? (omission or replacement). As we can see 
from Fig. 1, when more than one error category is 
given, we have two ways of choosing the best one. 
Method A allows us to estimate whether there is a 
missing word or not for each error category. This 
can be considered the same as deciding which of 
the two labels (E: ?There is a missing word.? or C: 
?There is no missing word.?) should be inserted in 
front of each word. Here, there is an article miss-
ing in front of ?telephone?, so this can be consid-
ered an omission-type error, which is categorized 
as an article error (?at? is a label that indicates that 
this is an article error.). In Method B, if N error 
categories come up, we need to choose the most 
appropriate error category ?k? from among N+1 
categories, which means we have added one more 
category (+1) of ?There is no missing word.? (la-
beled with ?C?) to the N error categories. This can 
be considered the same as putting one of the N+1 
labels in front of each word. If there is more than 
one error tag inserted at the same location, they 
are combined to form a new error tag. 
As we can see from Fig. 2, we referred to 23 
pieces of information to estimate the error cate-
gory: two preceding and following words, their 
word classes, their root forms, three combinations 
of these (one preceding word and one following 
word/two preceding words and one following 
word/one preceding word and two following 
words), and the first and last letter of the word 
immediately following. (In Fig. 2, ?t? and ?e? in 
?telephone?.) The word classes and root forms 
were acquired with ?TreeTagger?. (Shmid 1994) 
3.3 Detection of replacement-type errors 
Replacement-type errors were detected by es-
timating whether or not each word should be de-
leted or replaced with another word string. The 
error category was also estimated during this 
process. As we did in detecting omission-type er-
rors, if more than one error category was given, 
we use two methods of detection. Method C was 
used to estimate whether or not the word should 
be replaced with another word for each error cate-
gory, and if it was to be replaced, the model esti-
mated whether the word was located at the 
beginning, middle or end of the erroneous part. As 
we can see from Fig. 3, this can be considered the Figure 2. Features used for detecting omission-type errors 
Word   POS    Root form 
there     EX       there 
is      VBZ       be 
telephone     NN       telephone 
and      CC       and 
the  DT       the 
books NNS       books 
.  SENT       . t e
?:feature combination      :single feature 
?Erroneous 
part
Figure 1. Detection of omission-type errors when 
there are more than one (N) error categories. 
Method A 
* there is telephone and the books . 
 
E: There is a missing word 
C: There is no missing word (=correct) 
Mehod B 
* there is telephone and the books . 
 
Ek: There is a missing word and the related error 
category is k (1?k?N) 
C: There is no missing word (=correct) 
? 
C 
? 
C 
? 
Ek 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
C 
? 
E 
? 
C 
? 
C 
? 
C 
? 
C 
same as deciding which of the three labels (Eb: 
?The word is at the beginning of the erroneous 
part.?, Ee: ?The word is in the middle or end.? or 
C: ?The word is correct.?) must be applied to each 
word. Method D was used if N error categories 
came up and we chose an appropriate one for the 
word from among 2N+1 categories. ?2N+1 cate-
gories? means that we divided N categories into 
two groups, i.e., where the word was at the begin-
ning of the erroneous part and where the word was 
not at the beginning, and we added one more 
where the word neither needed to be deleted nor 
replaced. This can be considered the same as at-
taching one of the 2N+1 labels to each word. To 
do this, we applied Ramshaw?s IOB scheme 
(Lance 1995). If there was more than one error tag 
attached to the same word, we only referred to the 
tag that covered the highest number of words. 
As Fig. 4 reveals, 32 pieces of information are 
referenced to estimate an error category, i.e., the 
targeted word and the two preceding and follow-
ing words, their word classes, their root forms, 
five combinations of these (the targeted word, the 
one preceding and one following/ the targeted 
word and the one preceding/ the targeted word 
and the one following/ the targeted word and the 
two preceding/ the targeted word and the two fol-
lowing), and the first and last letters of the word. 
3.4 Use of machine learning model 
The Maximum Entropy (ME) model (Jaynes 
1957) is a general technique that is used to esti-
mate the probability distributions of data. The 
over-riding principle in ME is that when nothing 
is known, the distribution should be as uniform as 
possible, i.e., maximum entropy. We calculated 
the distribution of probabilities p(a,b) with this 
method when Eq. 1 was satisfied and Eq. 2 was 
maximized. We then selected the category with 
maximum probability, as calculated from this dis-
tribution of probabilities, to be the correct cate-
gory. 
 
(2)   )),(log(),(             )(             
)1(                                          
(1)          ),(),(~       ),(),(
,
  
, ,
?
? ?
??
?? ??
?=
???
=
BbAa
j
BbAa BbAa
jj
bapbappH
kjffor
bagbapbagbap  
We assumed that the constraint of feature sets 
fi (i?j?k) was defined by Eq. 1. This is where A 
is a set of categories and B is a set of contexts,  
and gj(a,b) is a binary function that returns value 1 
when feature fj exists in context b and the category 
is a. Otherwise, gj(a,b) returns value 0. p~ (a,b) is 
the occurrence rate of the pair (a,b) in the training 
data. 
4 Experiment 
4.1 Targeted error categories 
We selected 13 error categories for detection.  
Table 1. Error categories to be detected 
Noun Number error, Lexical error 
Verb Erroneous subject-verb agreement, Tense error, 
Compliment error 
Adjective Lexical error 
Adverb Lexical error 
Preposition Lexical error on normal and dependent preposition 
Article Lexical error 
Pronoun Lexical error 
Others Collocation error 
 
Figure 4. The features used for detecting replace-
ment-type errors 
?:feature combination      :single feature 
Word     POS         Root form 
there     EX         there 
is      VBZ         be 
telephone     NN         telephone 
and      CC         and 
the      DT         the 
books     NNS         book 
on      IN         on 
the      DT         the 
desk      NN         NN 
.      SENT         . 
t e
?Erroneous
part 
Figure 3. Detection of replacement-type errors 
when there are more than one (N) error categories.
Method C 
* there is telephone and the books on the desk. 
 
 
Eb: The word in the beginning of the part which 
should be replaced. 
Ee: The word in the middle or the end of the part 
which should be replaced. 
C: no need to be replaced (=correct) 
Mehod D 
* there is telephone and the books on the desk. 
 
 
Ebk: The word in the beginning of the part which 
should be replaced and which error category is k. 
Eek: The word in the middle or the end of the part 
which should be replaced and which error category 
is k. (1?k?N) 
C: no need to be replaced (=correct) 
? 
C 
? 
C 
? 
C 
?
Eb
? 
C 
? 
C 
? 
C 
? 
C 
?
C
? 
C 
? 
C 
? 
C 
? 
Ebk 
? 
C 
? 
C 
? 
C 
? 
C 
?
C
4.2 Experiment based on tagged data 
We obtained data from 56 learners? with error 
tags. We used 50 files (5599 sentences) as the 
training data, and 6 files (617 sentences) as the 
test data. 
We tried to detect each error category using the 
methods discussed in Sections 3.2 and 3.3. There 
were some error categories that could not be de-
tected because of the lack of training data, but we 
have obtained the following results for article er-
rors which occurred most frequently. 
Article errors 
Omission- Recall rate 8/71 * 100 = 32.39(%) 
type errors Precision rate 8/11 * 100 = 52.27(%) 
Replacement- Recall rate 0/43 * 100 =  9.30(%) 
type errors Precision rate 0/ 1 * 100 =  22.22(%) 
Results for 13 errors were as follows. 
All errors 
Omission- Recall rate 21/ 93 * 100 = 22.58(%) 
type errors Precision rate 21/ 38 * 100 = 55.26(%) 
Replacement- Recall rate 5/224 * 100 =  2.23(%) 
type errors Precision rate 5/ 56 * 100 =  8.93(%) 
We assumed that the results were inadequate 
because we did not have sufficient training data. 
To overcome this, we added the correct sentences 
to see how this would affect the results. 
4.3 Addition of corrected sentences 
As discussed in Section 2.1, our error tags pro-
vided a corrected form for each error. If the erro-
neous parts were replaced with the corrected 
forms indicated in the error tags one-by-one, ill-
formed sentences could be converted into cor-
rected equivalents. We did this with the 50 items 
of training data to extract the correct sentences 
and then added them to the training data. We also 
added the interviewers? utterances in the entire 
corpus data (totaling 1202 files, excluding 6 that 
were used as the test data) to the training data as 
correct sentences. We added a total of 104925 
correct new sentences. The results we obtained by 
detecting article errors with the new data were as 
follows. 
Article errors 
Omission- Recall rate 8/71 * 100 = 11.27(%) 
type errors Precision rate 8/11 * 100 = 72.73(%) 
Replacement- Recall rate 0/43 * 100 =  0.00(%) 
type errors Precision rate 0/ 1 * 100 =  0.00(%) 
We found that although the recall rate de-
creased, the precision rate went up through adding 
correct sentences to the training data. 
We then determined how we could improve 
the results by adding the artificially made errors to 
the training data. 
4.4 Addition of sentences with artificially 
made errors 
We did this only for article errors. We first ex-
amined what kind of errors had been made with 
articles and found that ?a?, ?an?, ?the? and the 
absence of articles were often confused. We made 
up pseudo-errors just by replacing the correctly 
used articles with one of the others. The results of 
detecting article errors using the new training data, 
including the new corrected sentences described 
in Section 4.2, and 7558 sentences that contained 
artificially made errors were as follows. 
Article errors 
Omission- Recall rate 24/71 * 100 = 33.80(%) 
type errors Precision rate 24/30 * 100 = 80.00(%) 
Replacement- Recall rate 2/43 * 100 =  4.65(%) 
type errors Precision rate 2/ 9 * 100 = 22.22(%) 
We obtained a better recall and precision rate 
for omission-type errors. 
There were no improvements for replacement-
type errors. Since some more detailed context 
might be necessary to decide whether ?a? or ?the? 
must be used, the features we used here might be 
insufficient. 
5 Conclusion 
In this paper, we explained how errors in 
learners? spoken data could be detected and in the 
experiment, using the corpus as it was, the recall 
rate was about 30% and the precision rate was 
about 50%. By adding corrected sentences and 
artificially made errors, the precision rate rose to 
80% while the recall rate remained the same.  
References 
Helmut  Schmid  Probabilistic  part-of-Speech 
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing. pp. 44-49, 1994. 
Lance A. Ramshaw and Mitchell P. Marcus. Text 
chunking using transformation-based learning. In 
Proceedings of the Third ACL Workshop on Very 
Large Corpora, pp. 82-94, 1995. 
Jaynes, E. T. ?Information Theory and Statistical Me-
chanics? Physical Review, 106, pp. 620-630, 1957. 
  
Criterion for Judging Request Intention  
in Response texts of Open-ended Questionnaires 
INUI Hiroko 
Communications Research 
Laboratory 
Graduate School of Science and 
Technology Kobe University 
hinui@crl.go.jp 
UTIYAMA Masao 
 
Communications Research 
Laboratory 
 
mutiyama@crl.go.jp 
ISAHARA Hitoshi  
Communications Research 
Laboratory 
Graduate School of Science and 
Technology Kobe University  
isahara@crl.go.jp 
 
Abstract 
Our general research aim is to extract the 
actual intentions of persons when they 
respond to open-ended questionnaires. 
These intentions include the desire to 
make requests, complaints, expressions of 
resignation and so forth, but here we 
focus on extracting the intention to make 
a request. To do so, we first have to judge 
whether their responses contain the intent 
to make a request. Therefore, as a first 
step, we have developed a criterion for 
judging the existence of request intentions 
in responses. This criterion, which is 
based on paraphrasing, is described in 
detail in this paper. Our assumption is that 
a response with request intentions can be 
paraphrased into a typical request 
expression, e.g., ?I would like to ...?, 
while responses without request are not 
paraphrasable. The criterion is evaluated 
in terms of objectivity, reproducibility and 
effectiveness. Objectivity is demonstrated 
by showing that machine learning 
methods can learn the criterion from a set 
of intention-tagged data, while 
reproducibility, that the judgments of 
three annotators are reasonably consistent, 
and effectiveness, that judgments based 
not on the criterion but on intuition do not 
agree. This means the criterion is 
necessary to achieve reproducibility. 
These experiments indicate that the 
criterion can be used to judge the 
existence of request intentions in 
responses reliably. 
1 Introduction 
In every aspect of society, it is necessary for us 
to ?know what the request is.? This is because 
knowing what the request is plays an important 
role in allowing us to identify and solve problems 
to achieve improvements. 
In recent years, the spread of electronic devices 
such as personal computers and the Internet has 
allowed us to save most requests in machine-
readable texts. On the basis of these texts, research 
and development have been conducted ?to know 
what the request is? as an element technology in 
natural language processing. For example, the 
research includes text mining (Nasukawa, 2001) 
and information extraction (Tateno, 2003) for 
customer claims and inquiries, development of an 
FAQ generation support system to a call center 
(Yanase et al, 2002; Matsuzawa, 2002), an FAQ 
navigation system using Q&A stored a call center 
(Matsui, 2002), and the development of 
requirement capturing methods for extracting 
requests made in meetings for software 
development (Doi, 2003). However, ?to know 
what the request is? means to know the intention of 
various people in society such as residents, users, 
customers and patients, and it is inadequate to 
extract only request expressions expressed literally 
in texts. For this reason, previous works are not 
sufficient to understand intentions. 
Against this background, (Inui et al, 1998; Inui 
et al, 2001; Inui and Isahara, 2002) have been 
studying how to extract and classify request 
intentions of respondents from responses of open-
ended questionnaires (OEQs) which are 
accumulated requests. This paper describes the 
development of a criterion for judging request 
intentions and an evaluation of the criterion in 
terms of objectivity, reproducibility and 
effectiveness. 
  
2 Development of the criterion for 
judging request intentions  
2.1 Problems of an existing theory of 
modality 
Response texts of OEQs are the focus of attention 
as data for text mining. Researchers have tried to 
extract various types of information from those 
texts (Lebart et al, 1998; Li and Yamanishi, 2001; 
Osumi and Lebart, 2000; Takahashi, 2000). 
However, they have mainly used only keywords 
(mostly nouns) as the basic units of extraction. If 
only the characteristic key words are analyzed with 
regard to sentences such as  ?Company A?s beer 
tastes good,? ?Company A?s beer does not seem to 
taste good,? and ?Company B?s beer tastes better 
than company A?s,? the attention is directed 
toward ?company A/company B/beer/tastes/good,? 
and it is not possible to differentiate the meaning 
of the passages. 
Because of this, as (Toyoda, 2002) points out, 
text mining in the future needs to treat modality, 
which often changes the meaning of the sentences 
completely. Two separate studies (Inui et al, 1998; 
Morohashi et al, 1998) have tried to process texts 
using words like auxiliary verbs and auxiliary verb 
equivalents as modality information. The modality 
information focused on in both studies, however, is 
grammatical expressions that have been accepted 
in a previous Japanese language study. Therefore, 
it is not possible to mechanically interpret requests 
and questions displayed by respondents, speakers 
and writers if they don?t contain an auxiliary verb 
or an auxiliary verb equivalent. 
In Japanese language syntax, modality is 
defined as the intention of the writer that is 
represented by grammatical expressions expressed 
grammatically (Nitta and Masuoka ed., 1989) and 
typically appears in the form of particles and 
auxiliary verbs in the sentence structure. Although 
previous text mining has focused on these 
expressions,  modality does not always appear in 
the forms of grammatical expressions, and other 
expressions are more frequently used in real world 
texts. Thus, processing only those grammatical 
expressions listed so far is not sufficient for 
extracting intentions, and it is necessary to have a 
wide coverage of modality that expresses 
intentions. 
2.2 Criterion to judge request intentions 
using paraphrasing  
Surveyors try to know request intentions on the 
respondents through questionnaires, and 
respondents try to convey their request intentions 
to surveyors by responding to questionnaires. 
Therefore, it is important to establish a method that 
can extract the request intentions of the 
respondents based on the expressions given in the 
response texts. In this section, we propose a 
criterion to judge the existence of request 
intentions. 
First, we will analyze the request expressions 
deductively. Native Japanese speakers can 
recognize expressions such as te-hoshii (would like 
you to), te-moraitai (would like you to), te-kudasai 
(please do) and te-kure (do) as request. These are 
linguistically called direct request expressions 
Request? 
1) Whether it can be 
judged to be a request by 
linguistic intuition or not 
Response
2) Whether it can judged
by some criterion to be a
request or not 
Fig. 1 Layers to judge expressions of requests 
YES NO 
Non-Request 
Request? Others
YES NO 
Direct expressions 
of request 
1) Whether it includes 
expressions of direct 
request or not
Response
2) Whether it can be
paraphrased into a sentence 
containing ?te-hoshii? as 
typical request or not 
Fig. 2 Criterion to judge request intentions 
Expressions of
request intention
Others 
YES NO 
YES NO 
Others 
  
(NIJLA, 1960) and able to indicate request 
intentions. Especially, te-hoshii is a typical request 
expression.  
In other words, these direct request expressions 
are a clue to understand that there is a request 
intended. This recognition process is equivalent to 
the first judgment in Fig.1, that is, ?whether a 
response can be judged to be a request by linguistic 
intuition or not.? We regarded this as the first level 
criterion to judge request intentions. It corresponds 
to the first level in Fig.2, the intent of which is 
equal to judge whether the response includes a 
direct request expression or not.  
Second, we consider the case that a response 
does not contain a direct request expression. In this 
case, non-requests in Fig.1 may be judged as 
requests. For example, based on the relation with 
surveyors, respondents and the situation, 
?Guardrails should be built along sidewalks of 
heavily congested roads? and ?Building eco-
friendly roads is important? can be interpreted as 
?We want guardrails along the sidewalks? and 
?We want you to think about the environment.? 
However, the interpretation is due to ?some? 
implicit criterion as shown in the second judgment 
in Fig. 1. As the implicit criterion depends on the 
judges, it is possible that the judgments differ1 . 
This means that the results of the judgment, 
namely request ?  in Fig. 1, are not re-created 
consistently. Therefore, the second judge in Fig.1 
is not reproducible.  
Consequently we attempted to manifest the 
implicit criterion as an explicit criterion to judge 
the existence of request intentions. This 
manifestation is the criterion ?whether a response 
can be paraphrased into a sentence containing te-
hoshii as a typical request expression or not? as the 
second judge in Fig. 2. As this criterion is explicit, 
the judgment of the criterion does not depend on 
the judges and agree consistently. Therefore, the 
second judge in Fig.2, namely the proposed 
criterion is reproducible and the results of the 
judgment, namely the expression of request 
intentions in Fig.2 is re-created consistently2. 
As mentioned above, we propose a criterion for 
judging request intentions by paraphrasing a 
response sentence into a typical request sentence 
                                                          
1  This is demonstrated by the results of the experiment 
described in Section 4.2. 
2  This reproducibility is described in detail in Section 4.1. 
contained te-hoshii. In Section 3, we evaluate the 
proposed criterion by a single judge analytically 
and objectively. In Section 4, we evaluate the 
results of experiments conducted by different 
judges from the viewpoint of reproducibility and 
effectiveness. These evaluations enable to 
demonstrate that the criterion, namely paraphrasing 
is an important method to determine the intentions 
independent of variety of surface expressions and 
differences among individual judgments. 
3 Evaluation by a single judge 
3.1 Analysis of response texts 
Using the proposed criterion described in Section 
2.2, we analyzed and classified response sentences 
manually according to two considerations: (1) if 
they include direct request expressions such as te-
hoshii and te-moraitai; and (2) if it is possible to 
paraphrase them into a sentence ending with te-
hoshii. To make the judgment for (1), we used 
request expressions listed by (Morita and Matsui, 
1989).  
 Expressions of 
direct requests 
Paraphrase Out of 3000 
sentences  
? Included  Possible 547 
? Included Not possible 3 
? Not included  Possible 1190 
? Not included  Not possible 1252 
Table 1 Results of applying criterion  
for judging request intentions3  
The analysis data are part of the response texts 
of OEQs carried out to make the best use of the 
opinions of the citizens in future road planning 
(Voice report, 1996). The original OEQ corpus 
contains a total of 35,674 respondents and 113,316 
opinions. The analysis data comprised 3,000 
sentences sampled at random after separating the 
plural sentences contained in the response text into 
single sentences. The criterion in Section 2.2 was 
used and the results are shown in Table 1. 
Line ? in Table 1 includes sentences with direct 
request expressions such as te-hoshii, te-kudasai 
and te-kure. All of these could be paraphrased into 
te-hoshii and accounted for about 20% of the 3,000 
sentences. Line ?  includes direct request 
expressions that could not be paraphrased because 
they were used in quotations. These examples are 
exceptional. Expressions in line ? correspond to 
                                                          
3 Eight sentences were excluded from Table 1 because they 
were ambiguous out of contexts. 
  
expressions of request intentions in Fig.2 in 
Section 2.2. These expressions are shown in Table 
2. Line ? includes non-request expressions. 
Table 2 shows various forms of expressions 
based on parts of speech (POS), i.e., verbs, nouns 
and adjectives, that have not been considered 
acceptable as modality expressions, even though 
they are paraphrasable by te-hoshii, and thus they 
are request expressions. As described in Section 
2.1, several studies have been made on modality in 
terms of  particles, auxiliary verbs, and auxiliary 
verb equivalents. However, little attention has been 
given to other POS in this regard. This is because 
modality expressions have been primarily 
connected with the grammatical elements such as 
auxiliary verbs in syntax. However, Table 2, which 
lists expressions of request intentions, shows that 
verbs, nouns and adjectives are actually also 
important elements that express modality.  
Previous works that aim to extract requests have 
used pattern matching methods, and patterns that  
mainly consist of the direct request expressions 
corresponding to ?  in Table 1. However, the 
results of manual analysis for paraphrasability 
shown in Table 2 indicate that using the proposed 
criterion enables many expressions of request 
intentions to be extracted from responses. In 
addition, we found a tendency for the number of 
expressions of request intentions direct request 
expressions, as shown in Table 1. In this section, 
we have provided explanation for the coverage of 
the criterion by analyzing response texts.  
3.2 Evaluation of objectivity through 
machine learning methods 
This section shows that the possibility of 
paraphrasing is learnable by machine learning 
methods. The data for the machine learning 
methods were tagged by the expert that analyzed 
the data in Table 1. Our assumption is that if 
machine learning methods can learn the 
paraphrasability from the data, then the data are 
said to have been tagged consistently enough to be 
mechanically learnable. This  indicates that the 
criterion proposed in Section 2 is objectively 
applicable to tag data. 
Machine learning methods 
We use two machine learning methods in this 
section. They are maximum entropy method (ME) 
(Beger et al 96) and support vector machine 
(SVM) (cristianini00)4, both of which have been 
shown to be quite effective in natural language 
processing. 
The task of a machine learning method is to 
make a classifier that can decide whether a 
response is paraphrasable by te-hoshii or not. A 
response X is tagged possible if it is paraphrasable 
                                                          4 We used maxent (http://www.crl.go.jp/jt/a132/ 
members/mutiyama/software.html) for ME learning and 
TinySVM(http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM/) 
for SVM learning.  
Type of POS Types of form of expression Example Sentence 
End-form in 
verbs and 
adjectives 
-??????(make? to do) /-?????
(control)  etc. 
??????????????????????
??(Increase greenbelt and make it easier to see 
signposts) 
Used as noun -??(secure)/-??(equipment) etc. ???????(Secure car parks) 
Predicates 
abbreviated 
-?   etc. ????????????????????(Road 
building from the standpoint of the elderly, 
children, and the disabled) 
Verbs and 
adjectives of 
expectation 
and desire 
-????  (seek) /-?????  (expect) / -??
??  (desire) /-?????  (is desirable) /-??
???  (is desired)/-???  (desire) /-????
?(request) etc.  
???????????????????????
????????(Roads and streets that give 
priority to the disabled, the elderly, children, and 
the weak are desirable) 
<attribute: emergency> 
-??????  (matter of urgency) /-????
?(first priority) /-???????(think that 
the first thing to do) etc.  
???????????????????????
?????????????????(It is all right 
to build expressways in provincial areas, but why 
can?t improving congested places come first?) 
<attribute:importance> 
-????  (is important) /-?????????
?(think that it is also an important matter) /-?
???  (is important) /-??????(should 
be important) /-???  (that is ideal) etc. 
??????????????????????(I 
think that the important matter is to make the 
manner of stopping vehicles thorough ) 
Nouns for 
judging value 
<attribute: necessity> 
-????????  (it may also be necessary) 
/-???????  (feel the necessity for) /-??
???(is indispensable) etc. 
????????????????????
(Cooperation of landowners is indispensable in 
road building) 
Table 2 Expressions of requests and intention obtained by  
using the criterion for judging request intentions 
  
and impossible if not. X is represented by a feature 
vector x = [x1, x2, ??, xl]where  
 
 
Given training data, a machine learning method 
produces a classifier that outputs possible or 
impossible according to a given feature vector. We 
omit the details of ME and SVM. Readers are 
referred to the above references.  
We will compare three sets of features, F1, F2 
and F3, in the experiments below. F1 consists of 
word 1-grams, F2, 1-grams and 2-grams, and F3, 
word 1-grams, 2-grams and 3-grams. For example, 
let X be a response consisting of a word sequence5 
w1, w1,?.., wm where w1 = ?b?and wm = ?e?
are special symbols representing the beginning and 
the ending of a response. Let S1 be the set of 1-
grams in X {wi |2 ? i ?m-1}, S2, 2-grams in X 
{ wiwi+1 |1 ?  i ?m-1} and S3, 3-grams in X 
{ wiwi+1wi+2 |1 ?  i ?m-2}. The F1, F2 and F3 
features contained in X are S1, S1?S2, and S1?S2
?S3, respectively. 
Experiments 
The data used for the experiments consisted of 
3,001 responses6. The numbers of the responses 
tagged possible and impossible were 1,944 and 
1,057, respectively. We used 10-fold cross 
validation to evaluate the accuracies of ME and 
SVM7. For each iteration in the cross validation, 
8/10 of the data was used for training, 1/10, for 
parameter adjustment, and 1/10, for testing. The 
precision, Pi, for iteration i is   
Pi =   
We define P as the mean of the precisions for each 
iterations, i.e., P = ?i Pi /10. We henceforth call P 
precision. The precisions of ME and SVM are in 
Table 3, together with a baseline precision 0.648 
(=1944/3001), which was obtained by tagging all 
the responses possible. In the table, the figures in 
columns ?ME? and ?SVM? are the precisions of 
ME and SVM. Line Fi (i=1,2,3) indicates that the 
precisions in that line were obtained by using Fi as 
                                                          
5 We used ChaSen (http://chasen.aist-nara.ac.jp/) to segment 
an answer into a word sequence. 
6 This data was different from the response text analyzed in 
Section 3.1. 
7 We used the polynomial kernel for SVM. We tried degrees 1 
and 2 d=1,2. Since d=1 outperformed d=2, the results of 
d=1 are in Table 3 
a  feature set. We use one-sided Welch tests to 
measure the differences between precisions and 
say ?statistically significant? or simply 
?significant? when the differences were 
statistically significant at 1% level. 
Table 3 indicates that both ME and SVM 
outperform the baseline by a large margin. The 
differences were, of course, statistically significant. 
Therefore, we can conclude that these methods are 
quite effective in this task.  
 ME SVM Baseline 
F1 0.892 0.887 0.648 
F2 0.912 0.909 0.648 
F3 0.913 0.915 0.648 
Table 3 Precision of ME and SVM  
This table also indicates that ME and SVM are 
comparable in precision. The differences of 
precision were not statistically significant. We next 
compared the highest precisions in lines F1, F2, and 
F3. F1 was significantly outperformed by both F2 
and F3, but there was not a significant difference 
between F2 and F3. Consequently, we can use 
either ME or SVM as a machine learning method 
and F2 or F3 as a feature set. 
Table 3 demonstrates that we can expect about 
91% precision in deciding the paraphrasability by 
using either ME or SVM. This is a reasonably high 
precision. Therefore, we can conclude that the 
criterion proposed in Section 2.2 is sufficiently 
objective and stable. 
4 Evaluation by different judges 
In Section 3, we described the manual analytical 
evaluation by a single judge and the objective 
evaluation by machine learning that uses a corpus 
prepared based on the analytical evaluation. 
Section 4 refers to experiments carried out by 
multiple different judges.  
4.1 Evaluation of reproducibility: judgment 
of paraphrasing by multiple judges 
The subjects of this experiment were three male 
native speakers of Japanese in their twenties who 
were engineering majors. The experiment was 
carried out using a total of 24,000 random 
sentences from the OEQ corpus described in 
Section 3.1 by applying the criterion proposed in 
Section 2.2. If a response text included plural 
sentences, they were separated into single 
sentences as mentioned in Section 3.1. Of the 
xi = 
number of correctly tagged answers
total number of answers in the test data
1   if X has feature i 
0   otherwise 
  
24,000 sentences, the three subjects A, B and C 
were each given 8,000 of them. However, the pairs 
A and B, B and C, and A and C were each given 
4,000 common sentences, so that a variation of 
sentence totaled 12,000. 
As shown in Table 1 in Section 3.1, direct 
request expressions can be paraphrased with te-
hoshii, therefore, we deal only with the judgment 
of the second level in Fig.2, namely the 
paraphrasing into te-hoshii. For the evaluation, we 
prepared a set of work instructions for the subjects, 
part of which is shown below.  
Work instructions  
1) Not only the end expression but also case 
particles, case particle equivalents and those 
containing such expressions or expressions of 
connection are to be paraphrased. 
2) If te-hoshii is to be changed to a negative 
request of shite-hoshiku-nai (do not want), place 
the word negative at the end. 
3) Not only functional words but also content 
words, furthermore, word order may be changed in 
paraphrasing  
#1 S(ource): ???????????? (We  
think that there are not enough 
car parks.) 
? T(arget): ???????????(We want  
car parks to be increased.)  
The experimental results are given in Table 4, 
where P means possible to paraphrase and NP 
means not possible. KC is the kappa coefficient 
between subjects (Cohen 1960).   
 B  
A P NP Total KC 
P 2372 970 3342 0.48 
NP 36 622 658  
Total 2408 1592 4000  
 C  
A P NP Total KC 
P 3123 264 3387 0.61 
NP 171 442 613  
Total 3294 706 4000  
 C  
B P NP Total KC 
P 2119 50 2169 0.49 
NP 934 897 1831  
Total 3053 947 4000  
Table 4 Results of paraphrasability  
using the criterion   
Generally, the closer the kappa coefficient is to 
1, the higher the degree of agreement is obtained. 
There is a complete agreement when it is 1. In 
general, the ranges [0.81-1.00], [0.61-0.80], [0.41-
0.60], [0.21-0.40] and [0.00-0.20] correspond to 
full, practical, medium, low, and no agreement, 
respectively.  
Therefore, as Table 4 indicates, the results of the 
judging and the paraphrasing using the criterion by 
the three subjects showed that there was substantial 
agreement between subject A and C, and medium 
agreement between A and B, and B and C. 
These results indicate that the method based on 
the criterion, whether used by a single judge or by 
different judges(=subjects) for analysis and 
experiment, enables requests and non-requests to 
be distinguished. Therefore, we can conclude that 
using the criterion enables even untrained people 
to reproduce the extraction of requests. 
Sentences such as #2 and #3 below are 
examples of sentences that were agreed to be non-
paraphrasable. These include expressions of 
intentions in which the current situation is accepted 
passively such as #2 ???????(I think that it cannot 
be helped),? or in which the current situation is 
actively accepted such as #3 ?? ? ? ? ? (are 
wonderful)?. Furthermore, #4 is a sentence that 
begins with a clear statement of reason ????
(the reason is).? This indicates that a motive for 
requests exists, and that a response formed by 
multiple sentences often composes request-motive 
adjacency in discourse structure.   
Examples of sentences that could not be 
paraphrased: 
#2 ????????????????????
???(I think that it cannot be 
helped if rise in charges is 
necessary.)  
?3 ???????????????????
?????????????????? (The 
town and roads are wonderful as even 
people in wheelchairs can do 
shopping by themselves here and 
there with ease and wander about.)  
#4 ?????????????????(The 
reason is that overall development 
cannot be hoped for.)  
This analysis shows that paraphrasable sentences 
indicate requests and non-paraphrasable sentences 
indicate the acceptance of the current situations or 
the motives for requests. 
4.2 Evaluation of effectiveness: judging 
intention without using the criterion 
To evaluate whether the proposed criterion 
described in Section 2.2 is effective or not, we 
carried out an experiment to see if a response 
  
shows requests or not without the criterion. The 
two subjects, D and E, who took part in this 
experiment were both native speakers of Japanese. 
Subject D was a male student in his twenties from 
the education department of a university, and  
subject E was a female student also in her twenties 
from the literature department of a university. They 
used the same data of 4,000 sentences that were  
used by the subjects B and C in Section 4.1. The 
subjects D and E did not consult with each other 
and carried out the work separately. We provided 
them with the following instructions before asking 
them to start the work. 
? Each response sentence is context-free. 
? Judge intuitively, and mark 1 if you think the 
sentence shows a request, and mark 0 if you 
do not . 
? Make sure to mark either 1 or 0. 
The results of the experiment are given in Table 
5, where 1 and 0 in the right table correspond to P 
and NP in Table 4. We show the data again 
because subjects B and C used the same data as 
subjects D and E. In Table 5, the kappa coefficient 
(KC), between D and E is lower than that between 
B and C. Moreover, it is the lowest among all those 
given in Tables 4 and 5. The KC of 0.17 means 
there is no agreement between D and E.   
The results indicate the rate of agreement is 
higher for judgments made using the criterion than 
for subjective judgments. That is to say, this proves 
the effectiveness of the criterion.   
 ?  ? 
? 1 0 Total ? P NP Total
1 562 1880 2442 P 2119 50 2169
0 39 1517 1556 NP 934 897 1831
total 601 3397 3998 total 3053 947 4000
KC for D&E 0.17 KC for B&C 0.49 
Table 5 Results for experiment for effectiveness 
4.3 Examination of evaluation results 
We examine here mainly the cases in which no 
agreement was obtained with respect to 
paraphrasing in the experiment described in 
Section 4.1. Table 4 shows the cases where 
disagreement was considerable. The results for 
these cases, shown in Table 6, indicate that 
disagreement is obtained when the sentences are 
paraphrased into the forms including clauses of 
cause and reason indicated by ?node?(because) as 
#5. The clause is underlined in the target sentence 
in #5. 
#5 S:????????????????(A narrow 
road is made even narrower) 
T: ?????????????????(node)?
??????????(Because the narrow 
road is made even narrower, I would 
like to see something done about 
it.)  
The source sentence #5 is a statement showing the 
condition of the road being narrow. This statement 
can be seen as a motive for a request in the target 
sentence of #5. That is to say, the source sentence 
#5 itself shows not the content of a request but the 
?motive for request.? The three subjects disagreed 
in their judgments on whether or not the ?motive 
for request? sentence was paraphrasable as shown 
in the bottom line of Table 6. As the table indicates, 
disagreement rates of 64.4%, 51.5%, and 9.0% 
were obtained between A and B, A and C, and B 
and C. The reason for these high disagreement 
rates was that we did not give clear directions in 
the work instructions. The sentences which the 
paraphrasing includes ?node? are not requests and 
should not be extracted. This means these 
sentences should have been considered to be non-
paraphrasable.  
On the other hand, with regard to ?motive for 
request? sentences, there was an example #1 in 
Section 4.1 in which the work instructions 
requested the subjects to paraphrase such a 
sentence. That is, the work instructions suggested 
that the source sentence #1 ?I think that we do not 
have enough car parks? is a motive for the request 
?I want car parks to be increased.? This kind of 
inadequate instruction led to instability in the work 
done and might have increased the disagreement 
rates obtained in the judgment. 
However, according to the data prepared by the 
expert referred to in Section 3.2, ?motive for 
request? sentences cannot be paraphrased into te-
hoshii, and machine learning has confirmed that 
the data are objective. Therefore, it can be 
considered that the work of removing ?motive for 
requests? sentences can be done stably. This means 
Examinees ?? ?? ??
No. of paraphrase includes node 648 224 89 
A 645 194 --- 
B 3 --- 3 
 
subject 
C --- 30 86 
No. of disagreed paraphrasing  1006 435 984
Rates of node in disagreements (%) 64.4 51.5 9.0
Table 6 Disagreed paraphrase including cause  
and reason clauses ?node?  
  
that if the work instructions give clear directions  
like ?if you are able to add node at the end of a 
sentence, that sentence should be regarded not as a 
content of request, but a motive of request,? then 
the rate of agreement may be improved. 
5 Conclusion 
We have developed a criterion for judging request 
intentions. We evaluated this criterion from three 
points of view. The first evaluation was to analyze 
the data applying the criterion by a single judge. 
From this analysis, it was found that this criterion 
makes it possible to extract requests and that the 
coverage can be guaranteed compared with 
previous studies. Moreover, a corpus was prepared 
based on the analysis and was used for a machine 
learning experiment. From this experiment results, 
we confirmed the criterion using a paraphrase was 
objective. 
Furthermore, by different judges, the second 
evaluation was made from the experiment 
conducted by three subjects. The rate of agreement 
for the paraphrasability was high, which indicated 
that the results of requests extraction were re-
created using the criterion. This proves the 
reproducibility of the criterion.  
In the third experiment, two subjects judged the 
sentences without using the criterion to see 
whether or not there was a request in each response 
sentence. A comparison of the results of the second 
and the third experiments showed that a higher rate 
of agreement was obtained with the method using 
the criterion. This confirmed the effectiveness of 
the criterion. 
In future work, we will analyze ?motives for 
request? sentences found from the examinations, 
and prepare a criterion for distinguishing between 
request motives and the contents of request 
intentions. 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della 
Pietra. 1996. A maximum entropy approach to natural 
language processing. Computaional Linguistics, Vol.22, 
No.1, pp39-71. 
Jacob Cohen. 1960. A Coefficient of Agreement for Nominal 
Scales. Educational and Psychological Measurement. 20, 
37-46.  
Nello Cristianini and John Shawe-Taylor. 2000. An 
Introduction to Support Vector Machines. Cambridge 
University Press. 
Kouichi Doi, Naoyuki Horai, Isamu Watanabe, Yoshinori 
Katayama and Masayuki. Sonobe. 2003. User-oriented 
Requirements Capturing Method in Analyzing 
Requirements Capturing Meeting. Transactions of IPSJ, 
vol.44 No.1, pp48-58. 
The Committee for Roads in the 21st Century Basic Policy 
Board, Road Council. 1996. Voice Report. 
Hiroko Inui, Kiyotaka Ucihmoto and Hitoshi Isahara. 1998. 
Classification of Open-Ended Questionnaires based on 
Analysis of Modality. Proceedings of the 4th Annual 
Meeting of the ANLP, pp540-543. 
Hiroko Inui, Masaki Murata, Kiyotaka Uchimoto and Hitoshi 
Isahara. 2001. Classification of Open-Ended Questionnaires 
based on Surface Information in Sentence Structure. 
Proceedings of the 6th NLPRS2001, pp315-322. 
Hiroko Inui and Hitoshi Isahara. 2002. Proposition for 
?Extended Modality? ?Extraction of Intention in Open-
ended response texts-. Technical Report of EICE, Vol.102 
No.414, NLC2002-43, pp31-36. 
Ludovic Lebart, Andre Salem and Lisette Berry. 1998. 
Exploring Textual Data, Kluwer Academic Publishers, 14-
20. 
Hang Li and Kenji Yamanishi. 2001. Mining from Open 
Answers in Questionnaire Data Using Statistical Learning 
Techniques. Proceedings of the 4 IBIS2001. pp129-134. 
Kunio Matsui and Hozumi Tanaka. 2002. The Navigation to 
the Stored Q&A data using Simple Questions. Technical 
Report of IEICE, Vol.102 No.414, NLC2002-40, pp13-18. 
Hirofumi Matsuzawa. 2002. FAQ Generation Support System 
Using Structured Association Pattern Mining and Natural 
Language Processing. Proceedings of the FIT2002, pp69-70. 
Yoshiyuki Morita and Masae Matsuki. 1989. Expression 
Pattarn of Japanese, ALC 
Masayuki Morohashi, Tetsuya Nasukawa and Touru Nagano. 
1998. Text Mining: Knowledge Acquisition from enormous 
text data ? recognition of intention -. Proceedings of the 
57th Annual Meeting of IPSJ 
Tetsuya Nasukawa. 2001. Text Mining Application for Call 
Centers. Journal of the Japanese Society for Artificial 
Intelligence, Vol.16, No.2, pp219-225. 
Noboru Ohsumi and Ludovic Lebart. 2000. Analyzing Open-
ended Questions: Some Experimental Results for Textual 
Data Analysis Based on InfoMiner. Proceedings of the 
Institute of Statistical Mathematics. Vol.48, No.2, pp339-
376 
The National Institute for Japanese Language. 1960. A 
research for making sentence patterns in colloquial 
Japanese. 1. On materials in conversation. Shuei Publishers. 
Yoshio Nitta and Takashi Masuoka. 1989. Japanese Modality. 
Kurosio Publishers. 
Kazuko Takahashi. 2000. A supporting System for Cording of 
the answers from Open-Ended Question. Sociological 
Theory and Methods, vol.15, No.1. 149-164. 
Masakazu Tateno. 2003. The Method to extract Textual 
?Kansei? Expression in the Custmer?s Voice. IPSJ SIG 
Notes, NL-153-14, pp105-112. 
Yuki Toyoda. 2002. Translation from Text Data to Numeric 
Data ?Points for Attention in Text Mining Preparatory 
Processing as Seen from the Analyst?s. Journal of the 
Japanese Society for Artificial Intelligence, Vol.17 No.6. 
pp738-743. 
Takashi Yanase, Satoko Marumoto, Isao Nanba and Ryo 
Ochitani. 2002. Parsing Question Texts Using the Predicate 
Expressions of the Sentence End. Proceedings of the 8th 
Annual Meeting of the Association for NLP, pp647-650.  
 
Hybrid Neuro and Rule-Based Part of Speech Taggers 
Qing Ma, Masaki  Murata,  Kiyotaka Uch imoto ,  H i tosh i  Isahara 
Communic~t ion s l{esea.rch Labora.tory 
Ministry of Posts a.nd Telecommm~ications 
588-2, lwa,oka,, Nishi-ku, Kobe 6511-2/192, 3a, pa,n 
{qma, murata., uchimoto,  isa.hara}{))crl.go.jp 
Abstract  
A hybrid system R)r tagging part of speech is 
descril)ed that consists of a neuro tagger and 
a rule-based correcter. The neuro tagger is 
an initia.1--state a.nnotator tha.t uses difl'ertnt 
h_,,ngths of contexts based on longe, st context l)ri- 
ority. Its inputs a.re weighted 1)y information 
gains tha.t are obtained by information ma.xi- 
mization. The rule-1)ased correcter is construct- 
ed by a. sol; of trm~sfc)rma.tion rules to xna.ke Ul) 
for the shortcomings o\[' the nou17o tagger. Corn- 
puter experiments show that ahnost 20% of the 
errors ma.de by the neuro tagger a.re correct- 
ed by the, st trans\[orma.tion rules, so tha.t the 
hybrid system ca.n reach a.n a,tcura.cy of 95.5% 
counting only the ambiguous words and 99.1% 
counting all words when a. small Thai corpus 
with 22,311 a mbig;uous words is used t))v tra.in- 
ing. This a(;cu racy is far higher than that using 
an IIMM and is also higher tha.n that using a. 
rule-1)ased model. 
1 Introduct ion 
Many pa.rt of speech (POS) tatters  proposed 
so far (e.g., Brill, 1994; Meria.ldo, 1994; l)aele- 
marls, el. al., 1996; and Schmid, 1994) ha.re 
achieved a. high accura.ey partly because a. very 
large amount of dal,~ was used to 1;rain them 
(e.g., on the order of 1,000,000 words for \]'hl- 
glish). For ma.ny other la.nguages (e.g., Thai, 
which we treat in this paper)~ however, it is not 
as easy to cremate \]a.rge corpora from which lm:ge 
amounts of tr~fining data can be extra.cted. It is 
therefore desirable to construct a practic;d tag- 
ger tha.t needs as little training d a.t;a~ as possible. 
A multi-neuro tagger (Ma a.nd ls~hara, 11998) 
and its slimmed-down version called the ela.s- 
tic neuro tagger (Ma, el; al., 1999), which have 
high genera.lizing ability and therefore are good 
at dealing with the problems of data sp~u:se- 
hess, were proposed to satist~y this requh:ement. 
These taggers perform POS tagging using difl'er- 
ent lengths of corltexts I)~.~sed on longest context 
prk)rity, and each element of tile input is weight- 
ed with information gains (Quinla.n, 1993) for 
retlecting that tile elements of the input h~ve 
different rtlevances in t~Gging. They ha.d a tag- 
ging accuracy of 94.4% (counting only the am- 
biguous words in part of speech) in computer ex- 
periments when a. small 'l'ha.i corpus with 22,311 
am biguous words was u se(l for tr~fi n ing. This ~(:- 
curacy is bu" higher thml t\]lat; USillg tile hidden 
Marker model (IIMM), the main approach to 
\])art o\[ speech tagging, ~nd is ~dso higher t,\]lan 
tha.t using a. rule-based mode\]. 
Neuro taggers, however, htwe several crucial 
shortcomings. First, even in the case where the 
POS of a word is uniquely determined by the 
word on its left, for example, a neural net will 
also try to perlbrnl tagging based on tile com- 
plete context. As a result, even for" when the 
word on tile left; is the same, the tagging result~ 
s will be difl'erent if the complete contexts are 
different, rl'ha, t is~ the neuro tagger carl hard- 
ly acquire the rules with single inputs. Fur- 
thermore, although lexica.l in\[brma.tion is very 
ilnport~ult in t~gging, it is difficult for: neural 
nets to use it becmme doing so would make the 
network enorlnous. That is, the neuro tagger 
ca.nnot acquire (;lit rules with lexical informs> 
tion. Additionally, Imca.use of convergence and 
509 
over-training l)roblems, it is impossible and also 
not advisM)le to train neural nets to an a.ccura,- 
cy of 100%. The training should be stopped at 
an appropriate level of a.ceuracy. Consequently, 
neural nets may not acquire some usefnl rules. 
To make up for these shortcomings of the 
neuro tagger, we introduce in this pa.per a rule- 
based corrector as tile post-processor and con- 
struct a hyl)rid system. The rule-based cot- 
rector is constructed by a set of transforma- 
tion rules, which is acqnired by transforma?ion- 
based error-driven learning (Brill, 1.994:) from 
training corpus using a set of templates. The 
templates are designed to SUl)l)ly the rules that 
the neuro tagger can hardly acquire. Actual- 
ly, by examining the transformation rules ac- 
quired in the computer experiments, the 99.9% 
of them are exactly those that the neuro tagger 
can hardly acquire~ even when using a template 
set including those for generating the'rules that 
the neuro tagger can easily acquire. This rein- 
forces onr expectation that the rule-based ap- 
proach is a well-suited method to cope with the 
shortcomings of the neuro t~gger. Computer ex- 
periments hows thai; about  200/0 of errors made 
by the neuro tagger can be corrected by using 
these rules and that the hybrid system ca.n reach 
an accuracy of 95.5% counting only the aml)ign- 
ous words and 99.1% counting all words in l, he 
testing corpus, when tile same corpus described 
above is used for training. 
2 POS Tagg ing  Prob lems 
In this paper, suppose there is a lexicon V, 
where the POSs that can be served by each word 
are list.ed, and tiler(; is a set of POSs, l?. That is, 
unknown words that do not exist in the lexicon 
are not dealt with. The POS tagging problem 
is thus to find a string of POSs T = T172..-% 
(ri C F, i = 1 , - . . , s )  by following procedure 
~o when sentence W = wlw2...w.~ (wi C V, 
i = 1 , . - . , s )  is given. 
#:W ~ -+ rt, (1) 
where t is the index of the target word (the word 
to be tagged), and W t is a word sequence with 
length l + 1 + r (:entered on the target word: 
l i e  t : wt_  1 . . . .  i o  t ? . . Wt+r~ (2) 
where t - 1 > 1, t + r _< s. 'l'agging ca.n thus be 
regarded as a classification problem 1) 3, replacing 
the POS with (;lass and can therefore be handled 
by using neural nets. 
3 Hybr id  System 
Our hybrid system (Fig. J) consists of a neuro 
tagger, which is used as an initial-state an nota- 
i;or, ~nd a rule-based corrector, which corrects 
the outputs of the neuro tagger. When a word 
seque,~ce W t \[see l~q. (2)\] is given, the neuro 
tagger outl)ut a tagging result rN(Wt) for tile 
target word wt at first. The rule-based correc- 
tor then corrects the output of the neuro tagger 
as a fine tuner and gives tile final tagging result 
Ncuro Tagger Rule-Based 
Corrcctor 
Figure 1: Hybrid neuro and rule-based tagger. 
3.1 Neuro  tagger 
As shown in Fig. 2, the neuro tagger consists 
of a three-layer I)erceptron with elastic input. 
This section mainly descril)es the construction 
of inl)ut and output of the nenro tagger, and 
the elasticity by which it; becomes possible to 
use variable length of context for tagging. For 
details of the architecture of l)erceptron see e.g., 
Haykin, 1994 and for details of the features of 
the neuro tagger see Ma and isahara, 1998 and 
Ma, et aJ., 1999. 
lnl)ut I PT  is constructed fi'om word se- 
quence W t \[Eq. (2)\], which is centered on target 
word wt and has length l + 1 + r: 
I PT  = (iptt_ l , . . ' ,  i ph , . . . ,  iph+.,.), (3) 
provided that input length l+ J+r  has elasticity, 
as described a,t tile end of this section. When 
woM wis given in position x (x = t - l , . . . , t+r ) ,  
510 
OPT 
ip t t_  I ...... i l ) \[t_ I ipl t 
11"1" 
il)lt+l ...... i\]Ht+r 
Figure 2: Neuro tagger. 
element ipt ,  of input I PT  is a. weighted pattern, 
defined as 
ipt.,: = :/.,,. (e,,,,('-,, 'e,." ,q,,~), (4) 
where g;,; is the inIbrmation gain which can be 
obtained using information theory (for details 
see Ma and lsahara., 11998) and 7 is the number 
of tyl)es of POSs. l\[' w is a word that apl)ears 
in the tra.ining data, then ea.ch bit e,,,i can be 
obtained: 
= P,,ot,(/ I , , ,) ,  (s) 
where l>'rob(ril'w) is a prior l>robal)ility of r i 
tha.t (;he woM 'w can take,. It is estitnated from 
the t raining (la,ta: 
C _ (<,  "') 
c' ( ,4  ' 
where C'(r ( ,w)  is the number of lfimes both r: 
at++d w al)pea, r , a,nd C(w)  is the number oi' times 
w appears in the training data. 1\[ w is a word 
that does not at)pear ill (,he training data~ then 
each t)it c,,,,i is obtained: 
~ i\['r i is a. candidate 
(7) e.,,,," = (} otherwise, 
where 7,, is the number of P()Ss that the word 
'w Call ta.ke. Output OPT is defined as 
OFT= ,o+), (s) 
provi<led that the output OI)T is decoded as 
S r/ i fO i= l  &O/=0for j? i  
YN(Wt) Unknown otherwise, 
(.9) 
where rN(W,) is the ?a.gging result obtained by 
the neuro tagger. 
There is more inforlnation available for con- 
structing the input for words on the left, be- 
cause they have already been tagged. In the 
tagging phase, instead of using (4:)-(6), the in- 
put can be constructed simply as 
i / , ,_4 = . oPT( - i ) ,  (1()) 
where i = 1, . . . ,1,  and O I )T ( - i )  means the out- 
put of the tagger for the ith word before the 
target word. ltowever, in the training process, 
the out;put of the tagger is not alway.a correct 
a.nd cannot be ted back to the inputs directly. 
Instead, a weighted awerage of the actual output 
a.nd tlm desired output is used: 
iptt_i = 9t- i  ' (wol ,T " 0 PT  ( -  i) + WlOJ,:s " I) l iS) ,  
(1.1) 
where 1)l':,q' is the desired output, 
o: , : ,5 '  : (& , / )2 , . . . ,  
whose bits are defined as 
\] iI' r i is a desired answer 
I)i = 0 otherwise, (la) 
and WOl,'r and w/)l,\],q' are respecLh:ely de\[(ned as 
1'\]013J 
~,:o1 '~ . . . .  (.14) 
1JACT '  
a,nd 
'w>l,; ,s,  = :1 - wopT ,  (15) 
where \]@,uo and \]'JAC'T are  the objective and 
actual errors. Thus, at the beginning of train- 
ing, the weighting of the desired output is largo. 
It decreases to zero during training. 
Elastic inputs are used in the neuro tagger 
so that the length of COlltext is variable in tag- 
ging based on longest context priority. In (te~ 
tail, (l, r) is initially set as large as possible for 
tagging. If rN('Wi) = Unknown,  then (1, r) is 
reduced by some constant interval. This l)ro- 
cess is repeated until rN(W~) 7 k Unknown or 
(1, r) = (0,0). On the other hand, to nmke the 
same set of connection weights of the neu ro tag- 
ger with the largest (1,'r) ava.ilable a.s lnuch as 
511 
possible when using short inputs for tagging, in 
training phase the neuro tagger is regarded as a 
neural network that has gradually grown fi'om 
small one. The training is therefore performed 
step by step from small networks to large ones 
(for details see Ma, et al 1999). 
3.2 Rule-based eorreetor 
Even when the POS of a word can be deter- 
mined with certainty by only the word on the 
left, for example, tile neuro tagger still tries to 
tag based on the complete context. That is, 
in general, what tile neuro tagger can easily 
acquire by learning is the rules whose condi- 
tional parts are constructed by all inpttts ip tx  
(x = t - l , . . . , t  + r) that are .joined with all 
AND logical operator, i.e., ( ip t t - t  & "'" iptt  & 
? .. iptt+~, -+ OPT) .  In other words, it is (lit: 
ticult for tile neuro tagger to learn rules whose 
conditional parts are constructed by only a sin- 
gle input like ( ipt,.  --+ OPT)  ~). Also, although 
lexical information is very important in tagging, 
it is difficult for the neuro tagger to use it, be- 
cause doing so would make the network euof  
mous. Tha.t is, the neuro tagger cannot acquire 
rules whose conditional parts consist of lexical 
information like (w -4 OPT) ,  (w&r  -4  OPT) ,  
and (w~w2 --+ OPT) ,  where w, Wl, and w2 are 
words and 7- is tile POS. Furthermore, because 
of convergence and over-training 1)rol)lems, it is 
iml)ossible and also not advisable to train net> 
ral nets to all accuracy of 100%. The training 
should be stopped at an apl)rol)riate level of a.c- 
curacy. Thus, neural net may not acquire some 
useful rules. 
The transfbrmation rule-based corrector 
makes up for these crucial shortcomings. 
The rules are acquired Dora a training co l  
pus using a set of transformation templates 
by transformation-based rror-driven learning 
(Brill, 1994). Tile templates are constructed 
using only those that supply the rules that tile 
nenro tagger can hardly acquire, i.e., are those 
1)The neuro tagger can also learn this kind of rules 
because it can tag tile word using only ipt, (the input 
of tile target word), ill the case of reducing tile (I, r) to 
(0,0), as described in Sec. a.l. The rules with single 
input described here, however, are a more general case, 
ill which the input call be ipt,~ (~: = t - 1, . . . ,  t + r). 
for acquiring the rules with single input, with 
lexical information, and with AND logica.1 in- 
put of POSs and lexical information. The set of 
templa,tes i  shown in Table 112). 
According to the learning procedure shown 
in Table 2, an ordered list of transformation 
rules are acquired by applying the template set 
to a training corpus, which had ah'eady been 
tagged by the neuro tagger. After tile trans- 
formation rules are acquired, a corl)us is tagged 
as tbllows. It is first tagged by the neuro tag- 
ger. The tagged corpus is then corrected by 
using the ordered list of transformation rules. 
The correction is a repetitive process applying 
the rules ill order to the corptlS, which is then 
updated, until all rules have been applied. 
4 Exper imenta l  Results  
Data :  For our computer experiments, we used 
tile same Thai corpus used by Ma et al (1999). 
Its 10,d52 sentences were randomly divided in- 
to two sets: one with 8,322 sentences for trail> 
ing and the other with 2,1.30 sentences for test- 
int. The training set contained 12d,331 word- 
s, of which 22,311 were ambiguous; the testing 
set contained a4,5~14 words, of which 6,717 were 
ambiguous. For training tile n euro tagger, only 
the ambiguous wor(ls in the. training set were 
used. For training the HMM, all tilt words in 
the training set were used. In both cases, all the 
words in tile training set were used to estimate 
Prob( r i lw) ,  tim probability of "c i that wor(I w 
can be (for details on the HMM, see Ma, et al, 
1999). In the corpus, 4:7 types of POSs are de- 
fined (Charoenporn et al, 1997); i.e., 7 = 47. 
Neuro tagger: The neuro tagger was con- 
structed by a three-layer perceptron whose 
input-middle-outI)ut layers had p z, 2 7 units, 
respectively, where p = 7 ? (1 + I + r). The 
(l + 1 + r) had tile following elasticity. In train- 
ing, tile (I, r) was increased step by step as (71,1) 
-+ (u,2) (a,2) (a,a) a,d gra,dual 
training fl'om a small to a large network was 
pertbrmed. Ill tagging, on the other hand, the 
2)To see whether this set is suitable, a immloer of ad- 
ditional experiments were conducted using various sets 
of templates. The details are described in Sec. 4. 
512 
r ~ I a,1)lc 1: Set o\[' templa.tes for tra.ns\[orln;~tion rules 
Change t;ag v a to t;ag v ? when: 
(single inlm|;) 
( input ('onsists of a POS) 
1. left (right) word is tagged v. 
2. second left (right) word is tagged r. 
3. third left (right) word is ta.gged r. 
(inI)n|; consist;s of a word) 
4. ta.rget word is ~. 
5. left (right) word is w. 
6. second left, (right) word is w. 
(AND logical inpu? ot" words) 
7. l, arget word is 'UO 1 ~tlld left (right) word is wu. 
8. left; (right) word is u,1 and second lcfl, (,'ight) word is w2. 
9. left, word is w~ a.nd right, word is 'wu. 
(AND logical in.lint; of POS and words) 
10. ta.rget word is uq and left (right) word is llaggod r. 
:11. left (righl;) word is .w~ and left. (right) word is tagged r. 
12. ta.rget word is w~, left (right) word is ,w.,, and left (right) word is tagged r. 
Ta,1)le 2: l)roetdure for learning transi'orma,tion rules 
1. Apply neuro taggtr to training corpus, which is then updated. 
2. Compare tagged results with desired ones and find errors.  
3. Ma.teh templates l'or all errors and obtain set of tra.nsformation rules. 
d. Select rule in corpus with the maximum value of' (cn l , _qood-h .  cnl,_bad), where 
cnZ_qood: number that transforms incorrect ags to correct elliS: 
c'nl._bad: number that transforms correct tags to incorrect ones, 
h: weight to control the strict, hess of generating 1;he rule. 
5. Apply selecttd rule to training corpus, which is then updated. 
6. Append selected rule to ordered list o1" trausl'orma.tion rules. 
7. Ih'4)eal; steps :2 through (j until no such rule can I)e selected, i.e., c 'n , t _good-  
h,. cnl,_bad < O. 
(l, 'r) was inversely reduce(l ste l) by step as (3,3) 
-+ (3,2) (2,2) (2,:1) O,:l) (:l,o) 
(0,0) a.s needed, provided tJlat the number of 
units in the middle layer was kept a.t the ma.xi- 
I l l  l l l l l  vahle. 
Ru le -based  cor re t to r :  The parameter h in 
the tw~Juat;ion function (cnl ,_9ood - h, . c'M._bad) 
used in 1;he learning procedure (Table 2) is a 
weight to control the strictness of generating a. 
rule. IF It is large, the weight of cnt_bad is la.rge 
and the possibility of generating incorrect rules 
is reduced. By regarding the neuro tagger as ~d- 
ready having high accuracy and using tile rule-- 
based correcter as a fine tuner, weight h. was set 
to a. large vahm, 100. Applying |;lit templates 
Co the training corptm, which had already been 
tagged 1) 5, the neuro ta.gger, we obta.ined a.n or- 
dered list; of 520 transfbrmation rules. '.l'~d)le 3 
shows the first 15 transfbrmation rules. 
Results :  Table 4 shows the resull;s of I)()S tag- 
ging for the testing data.. In addition to the 
accuracy o\[" the neuro tagger and hybrid sys- 
tem, the ta.ble also shows tile accuracy of a, bast- 
line model, the IIMM, and a rule-based model 
\['or comparison. The baseline model is one that 
performs tagging without using the contextual 
inlorma.tion; instead, it performs ta.gging using 
only f'requency informa.tion: the proba.bility of 
P()S that; tach word can be. The rule-based 
model, to be exact, is also a hybrid system con- 
513 
'l'a.1)le 3: First 15 transfbrmation rules 
No. F rom To  Cond i t i on  
1 PREL 
2 PREL 
3 Unknown 
4 XVHI4 
5 VATT 
6 Unknown 
7 NCI4N 
8 VATT 
O PREL 
i0 VST~ 
ii VfiTT 
12 NCMN 
13 NCHN 
14 Unknown 
15 NCNN 
RPRE le f t  word is punctuation and r ight  tuord is 5~gu 
RPRE le f t  yard is ~ 
RDVN le f t  ~ord Ls tagged XVfiE 
XVBH le f t  word is II~D 
flDVN le f t  word is  ~ 
VRTT le f t  word is  tagged PREL 
RPRE le f t  word is ua 
VSTfi l e f t  word is ~q~ 
RPRE r ight  word is ~gu and second r ight  word is a~q;J 
RDVN target word is ~t~4 
ADVN target  word is  ~4~ 
RPRE target word is n14 and le f t  word is eentluu 
RPRE le f t  word is ~tt and le f t  ward is tagged NCHN 
fiDVN th i rd  le f t  word is tagged WCT 
CNIT taPget ~ord is nn~ 
where PREL: Relat ive Pronoun, RPRE: Preposit ion,  fiDVN: fidverb with normal form . . . .  
Table d: Results of POS ta,gging for testing data* 
model baseline IIMM rule-based lleuro hybrid 
accuracy 0.836 0.891 0.935 0.944 0.955 
*Accurac9 was determiued only for am lfiguous words. 
sisting of an initial-state annotator and a set of 
transformation rules. As the initial-state anno- 
b~tor, however, the baseline model is used in- 
stea.d of' the neuro tagger. And, its rule set. has 
1,177 transformation rules acquired h'om a more 
general teml)late set, which is described at the 
end of this section. The reason for using a gener- 
al template set is that the sol; of tra.nsibrma.tion 
rules in the rule-based model should be the main 
annotator, not a fine post-processing tuner. For 
the same reason, the parameter to control the 
strictness of generating a rule, h, was set to  a 
small value, \], so that a larger number of rules 
were generated. 
As shown in the table, the accuracy of the 
nenro tagger was far higher than that of the 
HMM and higher than that of the rule-based 
model. The accuracy of the rule-based mod- 
el, on the other hand, was also far higher than 
that of the IIMM, ~lthough it was inferior to 
that of the neuro tagger. The accuracy of the 
hybrid system was 1.1% higher than that of the 
neuro tagger. Actually, the rule-based corrector 
corrected 88.4% and 19.7% of the errors made 
by the neuro tagger for the training and testing 
data, respectively. 
Because the template set shown in Table 1 
was designed only to make up for the short- 
comings of the neuro tagger, tile set is smal- 
l compared to that used by Brill (1994). To 
see whether this set is la.rge enough for our sys- 
tem, we perlbrmed two additional experiments 
in which (\]) a sol; constructed 193' adding the 
templates with OR logical input of words to the 
original set and (2) a, set constructed 1)5' fnrther 
adding the templates with AND and OR logi- 
cal inputs of POSs to the set of case (1) were 
used. The set used in case (2) inclnded the set 
used by Brill (\]994) and all the nets nsed in our 
experiments. It was also used for acquiring the 
transformation rules in the rule-based model. 
The experimental results show that compared 
to the original case, the accuracy in case (1) 
was improved very little and the accuracy in 
case (2) was also improved only 0.03%. These 
results show that the original set is nearly la.rge 
enough for our system. 
To see whether tile set is snitable tbr our 
system, we performed ~tn additional experimen- 
t using the original set in which the templa.tes 
with OR logical inputs were used instead of the 
templates with AND logical inputs. The accu- 
racy dropped by 0.1%. Therefore, tile templates 
with AND logical inputs are more suitable than 
514 
those with O11 logical inputs. 
We also performed an experiment using a 
template set without lexical intbrmation. In this 
case, l;he accuracy dropl)ed by 0.9%, indicating 
that lexical informatioll is important in tagging. 
To determine the effect o1' using a. large h, 
for generating rules, we per\['ormed an experi- 
ment with h = 1. In this case, the accuracy 
dropped by only 0.045%, an insignifica.nt differ- 
ence compared to the case of h, = 100. 
By examining the acquired rules that were 
obtained by al)plying the most COml)lete tem- 
plate set, i.e., the set used in case (2) described 
above, we found that 99.9% of them were those 
that can be obtained by a.pl)lying the original 
set of templates, rl'ha.t is, the acquired rules 
were almost those that are dif\[icult \['or the neu- 
re tagger to acquire. '.l'his rein forced our expec- 
tat;ion that the rule-based al)l)roach is a well- 
suited method to cope with the shortcoming of 
the neuro tagger. 
Finally, il, should 1)e noted that ill the liter- 
atures, tile tagging a.ccuracy is usua.lly delined 
by counting a.ll tile words regardless of whether 
they are a.nlbiguous or not. If we used this dell- 
nil:ion, t\]le accura.cy of our hybrid system would 
be 99.1%. 
5 Conc lus ion  
To collstruct a 1)tactical tagger that needs as 
little training data. a.s possible, neuro taggers, 
which have high generalizing al)ility and there- 
fore a.re good at dealing with the problems ofda~ 
ta. sl)a,rseness, have been proposed so fa.r. Neu- 
re tatters,  however, have crucial shortcomings: 
they ca.nnot utilize lexical information; they 
have trouble learning rules with single inputs; 
and they cannot learn training data to an ac~ 
curacy of 100%. To make up for these short- 
comings, we introduced a rule-based correcter, 
which is constructed by a. set of trans\[brma.tion 
rules obtained by error-driven learning, for post 
1)recessing and constructed a hybrid tagging 
system, l{y examining the transtbrma.tion rules 
acquired in the computer experiments, we found 
that 1;he 99.9% of them were those that; the neu- 
re tagger can hardly acquire, even when using a. 
template set including t;hose for generating the 
rules that the neuro tagger can easily acquire. 
This reinlbrced our expecta.tion that the rule- 
based approach is a well-suited method to cope 
with the shortcoming of the neuro tagger. Com- 
puter experiments showed that 19.7% of the er- 
rors made by the neuro tagger were corrected 
by the tra.nslbrmation rules, so the hybrid sys- 
tem rea.ched an accuracy of 95.5% counting only 
the ambiguous words and 99.\]% counting all the 
words in the testing data, when a small corpus 
with only 22,311 ambiguous words was used tbr 
train int. ~l'h is ind icates thai; ou r tagging ,qystem 
can nearly reach a pra.ctica.l level in terms of tag- 
ging accuracy even when a small Thai corpus is 
used tbr tra.ining. This kind of tagging system 
can be used to constructs multilingua.1 corpora 
that include languages in which large corpora 
have not yet been constructed. 
References  
l~rill, E.: Transfornmtion-based rror-driven lca.rn- 
ing and natural language processing: ~ case s- 
tudy ill 1)art-of-sl)eech tagging, Computational 
Li~g'uistics, Vol. 21, No. 4, pp. 543-565, 199~1. 
Cha.roenporll, T., Sornlertlanlva.nich, V., ~md Isa- 
hara, 11.: Building a la.rge Thai text corpus 
parl; of speech tagged corpus: OI{CIlll), Pro< 
Natural Language Processi~fl Pacific lNm ,5'gn~- 
po.du'm \[997, Phuket, Thailand, pp. 509-5\]2, 
1997. 
I)aelemans, W., Z~wrel, a., Berck, P., and C,i/lis, S.: 
MI3'I': A m<mlory-based pm-t of speech tagger- 
genera.tot, P'roc. /tl.h Workshop on Very Large 
Co,'po~zl, Copenhagen, l)em na.rk, pp. 1-1+1, 99(5. 
l\]aykin, S.: Neural Nchvorlcs, Macmillan College 
Publishing Coral)any, Inc., 199/t. 
Ma, Q. and lsahm'a., H.: A multi-neuro tagger us- 
ing variable lengths of contexts, Prec. COLING- 
ACL'g8, Montreal, pp. 802-806, 1998. 
Ma, Q., Uchimoto, K., Mura.ta, M., and 1sahara H.: 
F, lastic neural networks tbr part of speech tag: 
ging, Prec. IJCNN'99, Washington, \])C., pp. 
2991-2996, 1999. 
Meriaklo, B.: Tagging English text with a proba- 
bilistic model, Computational Linguistics, Vo\]. 
20, No. 2, pp. 1.55-171, 19(.)4. 
Quinla.n, 3.: G'~.5: Programs Jot Machine Learning, 
San Mateo, CA: Morgan Kaufinann, 1993. 
Schmid, 1t.: l'art-of-speech tagging with neural net- 
works, Prec. COLING'94, Kyoto, Japan, pp. 
172-176, 1994. 
515 
Bunsetsu  Ident i f i ca t ion  Us ing  Category -Exc lus ive  Ru les  
Masaki Murata Kiyotaka Uchimoto Qing Ma Hitoshi Isahara 
C()mmunications Research Laboratory, Ministry of Posts and ~I~lecommunications 
588-2, \]waoka, Nishi-ku, Kobe, 651-2d92, Japan 
? - j  ( ' . tel:-k81- 78-969-2 \]81 tax: +81- 78-369-2189 http://www-karc.crl, go.j p/ips/murata 
{ murata,u(:himoto,qma,isahara}(@crl.go.ji) 
Abstract 
This pal>or describes two new bunsetsu identificatkm 
methods using supervised learning. Sin(:e ,Jat)anese 
syntactic analysis ix usnally done after bunsetsu 
identification, lmnsetsu identiiieation is iml)orl;ant 
for analyzing Japanese sentences. In experiments 
comparing the four previously available machine- 
learning methods (decision tree, maximmn-entropy 
method, example-based apI)roaeh and deeiskm list,) 
an(l two new methods llSing categot'y-exclusive rul s~ 
the new method using l;he category-exclusive rules 
with the highest similarity t)erformetl best. 
1 Introduction 
This paper is about machine learning methods for 
identifying bwnsr'ts'~zs, which correspond to English 
phrasal units such as noun phrases and t)rel)ositional 
phrases. Since .Japanes(.' syntactic analysis ix usu- 
ally done after lmnselisu identitication (Uchimot;o el; 
a\].. 1999), i(lentitlying lmnsetsu is important l'or an- 
alyzing ,J;~p~tnese ltt(}tl(:es. The  conventional stud- 
ies on lmnsetsu  identitieation ~ have used hand-made 
rules (Kameda, \]995; Kurohashi, 3998), })ill; bun- 
sel;su identification is not an easy task. Conventional 
studies used many hand-nmde rules develot)ed at the 
cost of many man-hours. Kurohashi, tbr examl)le, 
made 146 rules for lmnsetsu identification (Kuro- 
hashi, 1998). 
Itl }l.tl a . t te t l lp t  to reduce the mnnber of man- 
hours, we used machine-learning methods for bun- 
setsu identitication. Because it; was not clear which 
machine-learning method would 1)e the one most al)- 
propriate for bunsctsu identification, so we tried a 
variety of them. In this paper we rei)orl; ext)er- 
inlel lts comparing tbur inachine-learning me, thods 
(decision tree, maximmn entropy, example-based, 
and decision list; methods) and our new methods us- 
ing category-exclusive rules. 
l lhmsetsu ideni,illcation is a ln'oblem similar to ohm,king 
(lLamshaw and Marcus, 1995; Sang and \h;ellsl;ra, 1999) in 
other l;mguages. 
2 Bunsetsu identification problem 
We conducted experiments (m the following super- 
vised learning methods tbr idel~tiflying }mnsetsu: 
? \])eeision {;l'ee method 
? Max i lnun l  ent ropy  method  
? Examt)le-based method (use of sinfilarity) 
? Decision list (use of probability and frequency) 
? Method 1 (use of exclusive rules) 
? Method 2 (use of exclusive rules with the high- 
est similarity). 
In general, t)misetsu identification is (tone afl;er 
morl)hological and l)efore syntactic analysis. Mor- 
1)hological analysis correst)onds to part-of-st)ee(:h 
tagging ill English. Japanese syntactic structures are 
usually ref)resented by the. relations between lmn- 
setsus, which correspond to l)hrasal units such as a 
noml l)hrase or a t)repositional 1)hrase in \]r, nglish. 
St), 1)unsetsu identification is imi)ortant in .lnpanese 
sentence mmlysis. 
In this paper, we identit\[y a bunsetsu by using 
intbrmation Dora a morl~hological nalysis. Bun- 
setsu identitication is treated as the task of deciding 
whether to insert a "\[" mark to indicate the partition 
between two hunsetsus as in Figure 1. There, fore, 
bunsetsu identilical;ion is done by judging whether a
partition mark should be inserted between two adja- 
cent nlorphemes or not. (We. do not use l;he inserted 
partition mark in the tbllowing analysis ill this paper 
for the sake of simplicity.) 
Our lmnsetsu identification method uses i;t1(} lilOr- 
phok)gk:al intbrmation of the two preceding and two 
succeeding morphemes ofan analyzed space bel;ween 
two adjacent morphemes. We use the following mor- 
phological information: 
(i) Major part-of  speech (POS) category, 2 
(ii) Minor P()S category or intlection tYl)e, 
(iii) Semantic information (the first three-digit nun> 
bet of a category nmnlmr as used ill "BGIt" 
(NLI{,I, 1964:)), 
2Part-of-spec.ch ~ttegories follow those of 3 \[/MAN (Kuro- 
hashi and N~tgao, 1998). 
565 
bohu .qa 
(I) nominative-case particle 
(I identify bunsetsu.) 
\[ bunsetsu wo 
(bunsetsu) objective-case particle 
I matomeagcru 
(identify) 
Figure 1: Example of identified bunsetsus 
Major POS 
Minor POS 
Semantics 
Word 
bun wo ~ugiru 
(sentence) (obj) (divide) 
((I) divide sentences) 
Noun Particle Verb 
Normal Noun Case-Particle Normal Form 
x None 217 
x wo ku.qiru 
Symbol 
Punctuation 
X 
X 
Figure 2: hfformation used in bunsetsu identification 
(iv) Word (lexical iifformation). 
For simplicity we do not use the "Semmltic infor- 
matioif' and "Word" in either of the two outside 
morphemes. 
Figure 2 shows the information used to judge 
whether or not to insert a partition mark in the space 
between two adjacent morphemes, "wo (obj)" and 
"kugiru (divide)," in the sentence "bun wo kugiru. 
((I) divide sentences)." 
3 Bunsetsu  ident i f i ca t ion  process  fo r  
each  mach ine- learn ing  method 
a.1 Deeision-tree method 
In this work we used the program C4.5 (Quinlan, 
1.995) for the decision-tree l arning method. The 
four types of information, (i) major POS, (ii) mi- 
nor POS, (iii) semmltic information, and (iv) word, 
mentioned in the previous section were also used 
as features with the decision-tree l arning method. 
As shown in Figure 3, the number of features is 12 
(2 + 4 + 4 + 2) because we do not use (iii) semantic 
information and (iv) word information from the two 
outside morphemes. 
In Figure 2, for example, the value of the feature 
'the major POS of the far left morpheme' is 'Noun.' 
a.2 Maximum-entropy method 
The maximum-entropy method is useful with sparse 
data conditions and has been used by many re- 
searchers (Berger et al, 1996; Ratnaparkhi, 1996; 
Ratnaparkhi, 1997; Borthwick el; al., 1998; Uchi- 
moto et al, 1999). In our maximuln-entropy exper- 
iment we used Ristad's system (Ristad, 1998). The 
analysis is performed by calculating the probability 
of inserting or not inserting a partition mark, from 
the output of the system. Whichever probability is 
higher is selected as the desired answer. 
In the maximum-entropy method, we use the same 
four types of morI)hological information, (i) major 
POS, (ii) minor POS, (iii) semantic information, and 
(iv) word, as in the decision-tree method. However, 
it, does not consider a combination of features. Un- 
like the decision-tree method, as a result, we had to 
combine features mmmally. 
First we considered a combination of the bits of 
each morphological information. Because there were 
four types of information, the total number of com- 
binations was 2 ~-  1. Since this number is large 
and intractable, we considered that (i) major POS, 
(ii) minor POS, (iii) semantic information, aim (iv) 
word information gradually becolne inore specific in 
this order, and we coml)ined the four types of infor- 
mation in the following way: 
Information A: (i) major POS 
Intbrmation B: (i) major POS and (ii) minor POS 
hfformat, ion C: (i) major POS, (ii) minor POS and 
(iii) semantic information 
Information D: (i) major POS, (ii) minor POS, 
(iii) semantic informa~aion a d (iv) word 
(~) 
We used only Information A and B for the two out- 
side morphemes because we (lid not use semantic 
and word information in the same way it is used in 
the decision-tree inethod. 
Next, we considered the combinations ofeach type 
of information. As shown in Figure 4, the number 
of combinations was 64 (2 x 4 x 4 x 2). 
For data sparseness, in addition to the above com- 
binations, we considered the cases in which frst, one 
of the two outside morphemes was not used, sec- 
ondly, neither of the two outside ones were used, m~d 
thirdly, only one of the two middle ones is used. The 
nmnber of features used in the maximum-entropy 
method is 152, which is obtained as follows: a
3When we extr~,cted features  f rom all of  the  ar t ic les  on  
566 
Far M't mort)henm Left morl)heme 
( Ma.ior POS '} 
~'Major POS'\[ ,J Minor POS 
I, Minor POgJ +/S,mmntic Information + 
( \?ord , 
2 4 
Right morl)hemc Far right mort)heine 
Minor POS f Ma.ior POS ~ 
Semantic Infi)rmation ' + \[ Minor POS j 
Word 
4 2 
Figure 3: Features used in the decision-tree method 
Far left morl)heme 
Information 
hffbrnmtion A } 
2 
Left morpheme Right morl)hcme 
( In format ion!} (Infbrmation A
J hfformation J Intbrmation B
/hfformation & ~ hfformation C
I, Infbrmation 1, hffbrmation D
4 4 
Far right morpheme 
J" Information 
& \[hlformation B A } 
2 
Figure 4: lJ~,atm'es used in the maximunt-entrol)y met.hod. 
No. of t>atures= 2 x 4 x 4 x 2 
+2 x 4 x 4 
+ 4 x 4 x 2 
+ 4 x 4 
+ 4 
+ 4 
= 152 
In Figure 2, (;lie feature that uses Infornultion 
B in the far left morl)heme, Infbrnmtion D in the 
left mort)heine, Information C in the right mor- 
pheme, and Information A in the fa.r right mop 
l/heme is "Noun: Nornml Noun; Particle: Case- 
Particle: none: wo; Verl): Nornml Form: 217; Sym- 
bol". In tim maximmn-entrol)y method we used for 
each space 152 ligatures uch as this ()tie. 
3.3  Example -based  method (use of  
s imi lar i ty)  
An example-based method was t)rollosed t) 3, Nagao 
(Nagao, 1984) in an attempt to solve I)roblenls in 
machine translation. To resolve a. l)rol)h'm, it; uses 
the most similar (;xami)le. In the i)resent work, the 
examt)le-1)ased method imt)artially used the same 
four types of information (see Eli. (1)) as used in 
the maxinmm-entrotly method, 
To use tills method, we must define the similarity 
of an ini)ut to an example. We use the 152 1)atterns 
fl'om the maximum-entropy method to establish the 
level of similarity. We define the similarity S be- 
tween all input and an exmnl)le according to which 
one of these 152 levels is the lnatching level, as fol- 
lows. (The equation reflects the importance of the 
two middle morphemes.) 
January 1, 1995 of a Kyoto University corpus (l;hc mnnber of 
spaces between mrl)henms was 25,81d) by using this method, 
the nunfl)e,r of types of features was 1,534,701. 
S = s(m_t) x s(m-H) x 10,000 
+ s(m_2) x s(.q.~) (2) 
Here m- l ,  m4-\], m-2, and m+2 refer respectively to 
the left;, rigid;, far M't, ;rod far righl; mortflmnms, and 
s(x) is the mort)hological similarity of a ll lOl't)hell le 
x, which is defined as follows: 
s(x) =1 (when no information of x is matched) 
2 (when Information A of x is matdmd) 
3 (when hfl~)rmal:ion B of x is mate, heal) 
4 (when Information C of x is mat;cited) 
5 (when Information D of x is matched) 
(a) 
Figure 5 shows an exmnple of the levels of sim- 
ilarity. When a pattern matches Information A of 
all four lnort)henies , uch as "Noun; Particle; Verb; 
Symbol", its similarity is 40,004 (2 x 2 x 10,000 + 
2 x 2). When a pattern matches a pattern, such as 
" ; Particle: Case-Particle: none: wo; ; ", its 
similarity is 50,001 (5 x 1 x 10,000 + 1 x 1). 
The exmnl)le-1)ased method extracts the exam- 
ple with the highest level of similm'ity and checks 
whether or not that exami)le is marked. A partition 
marl{ is inserted in tile input data only when the ex~ 
amt)le iv marked. When multit)le exalnl)les have the 
same highest level of similarity, the selection of tile 
best example is ambiguous, hi this case, we count 
tile number of nlarked and mlinarked sl)aces in all 
of the examples and choose the larger. 
a .4  Decis ion- l ist  method (use of p robab i l i ty  
and f l ' equeney)  
T i le  decision-list method was proposed by Rivest 
(Rivest, 1987), in which tile rules are not expressed 
as a tree structure like in the decision-tree method, 
567 
No information 
hffornmtion A
Information B
Information C
Information D
bun wo kugiru 
(senWnce) (obj) (divide) 
S(X) ?~\]'- 2 7D,--1 ?/Zq-1 '11~'+2 
1 . . . .  
2 Noun Particle Verb Symbol 
3 Normal Noun Case-Particle Normal Form Punctuation 
4 x None 217 x 
5 x wo kugiru x 
Figure 5: Exmnple of levels of similarity 
but are expanded by combining all the features, and 
are stored in a one-dimensional list. A priority or- 
der is defined in a certain way and all of the rules 
are arranged in this order. The decision-list method 
searches for rules Dora tile top of the list and an- 
alyzes a particular problem by using only the first 
applicable rule. 
In this study we used ill the decision-list method 
the same 152 types of patterns that were used in/;lie 
ma.ximuln-entropy method. 
To determine the priority order of the rules, we re- 
ferred to Yarowsky's method (Yarowsky, 1994) and 
Nishiokwama's method (Nishiokaymna et al, 1998) 
and used the probability a.nd frequency of each rule 
as measures of this priority order. When nnlltiple 
rifles had the same probability, the rules were ar- 
ranged in order of their frequency. 
Suppose, for example, that Pattern A "Noun: 
Normal Noun; Particle: Case-Particle: none: wo; 
Verb: Normal Form: 217; Symhol: Punctuatioif' 
occurs 13 times in a learlfing set and that tell of 
the occurrences include the inserted partition Inal:k. 
Suppose also thai; Pattern B "Noun; Particle; Verb; 
Symbol" occurs 12a times in a learning set and that 
90 of the occurrences include the mark. 
This exmnple is recognized by the following rules: 
Pattern A ~ Partition 76.9% (10/ 13), Freq. 23 
Pattern B => Partition 73.2% (90/123), Freq. 123 
Many similar rules were made and were then listed 
in order of their probabilities and, for any one prob- 
ability, in order of their frequencies. This list was 
searched from tile top ml(:l the answer was obtained 
by using the first, ai)plicable rule. 
3.5 Method  I (use of eategory-exe lus ive  
rules) 
So far, we have described the four existing machine 
learning methods. In the next two sections we de- 
scribe our methods. 
It is reasoimble to consider tile 152 patterns used 
in three of the previous methods. Now, let us sup- 
pose that the 152 patterns fl'om the learning set yield 
the statistics of Figure 6. 
"Partition" means that the rule determines that a 
partition mark should be inserted in the input data 
and "non-t)arl:ition" ineans that tile rule determines 
that a partition mark should not be inserted. 
Suppose that when we solve a hypothetical prob- 
lem Patterns A to G are apt)licable. If we use the 
decision-list inethod, only Rule A is used, which is 
applied first, and this determines that a partition 
mark should not be inserted. For Rules B, C, and 
D, although the fl'equency of each rule is lower thml 
that of Rule A, tile suln of their frequencies of the 
rules is higher, so we think that it is better to use 
Rules B, C, ml(t D than Rule A. Method 1 follows 
this idea, but we do not simply sum up tile frequen- 
cies. Instead, we count the munber of exalnples used 
ill Rules B~ C, and D and judge the category having 
tile largest number of exmnplcs that satisfy the pat- 
tern with the highest probability to be the desired 
ai1swer. 
For exmnple, suppose that in the above examt)le 
the number of examples atis(ying Rules B, C, and 
D is 65. (Because some exmnples overlq) in multi- 
pie rules, the total nunfl)er of exalnples is actually 
smaller than the total number of tile frequencies of 
the three rules.) In this case, among the examples 
used by the rules having 100% probability, tile nmn- 
ber of examples of partition is 65, m~d the number 
of examt)les of non-t)artitioi~ s 34. So, we deternline 
that tile desired answer is to partition. 
A rule having 100% probability is called a 
category-exclusive rule because all the data satist~y- 
ing it belong to one category, which is either parti- 
tion or noi>partition. Because for any given space 
the number of rules used call be as large as 152, 
category-exclusive rules are applie(t often ~. Method 
1 uses all of these category-exclusive rules, so we call 
it tile method using category-exclusive rules. 
Solving problems by using rules whose prol)abili- 
l;ies are nol; 100% may result ill the wrong solutions. 
Almost all of the traditional machine learning meth- 
ods solve problelns by usiug rules whose i)robabilities 
4'l'he ratio of the spaces analyzed by using category- 
exclusive rules is 99.30% (16864/16983) in Experiinent 1 of 
Section d. This indicates that ahnost all of the spaces are 
analyzed by category-exclusive rules. 
568 
\]{,uIe A: 
l{,uh.' B: 
Rule C: 
Rule D: 
Rule E: 
Rule F: 
Rule G: 
l 'attern A 
Pattern \] 
Pattern C 
Patl;crn \]) 
Patt;ern 1'3 
Pal;l;erll F 
Pat tern  G 
-?- I)rol)ability of non-i)al|;ition 
=> probability of partition 
=> i)rolml)ility of partition 
=> probal)ility of 1)artition 
:~ i)robability of partition 
:~ probability of parl:ition 
=> probability of non-partition 
m0% (a4/34)  
100% (,33/a3) 
\]oo% (25/2s)  
J()0(X, ( 1!)/ 19) 
8J.3% (1?)?}/123) 
76.9% ( 10/ ~a) 
57.4% (31{)/540) 
Figm'e 6: ;Ill (',Xaml)h; of rules used in Method :l 
1,?equency 34 
Frequency 33 
Frequency 25 
Frequency 19 
Frequcncy 123 
Fl'o, qucncy \] 3 
lq'equc, ncy 540 
are not J(}0%. By using such methods, we cannot 
hol)e to improve, a,:curacy. If we want to improve ac- 
(;llra(;y~ we nlllst use catt;gory-excltl,qive l'lllCS. The l 'e  
are some eases, however, tbr which, even if we take 
this at)l)r()ach, eategory-exchlsive rules aye rarely al> 
plied. In such cases, we lilllSl; add new feai;ures t() 
I;he mlalysis to create a situation in which many 
c i~tegory -exeh ls ive  ru les  Call \])(; appli(~d. 
I\]owever, il; is not suflieient to use  t ;~/tt~ory- 
exclusive rules. There arc, many nmaningless rules 
which \]la,1)l)ell to  1)e c~tl;egor3~-ex(;lll,qive o l l ly  ill ~t 
learning set. We lllllSt consider how to (~Iimim/te 
such meaningh;ss rule,q. 
3.6  Method  2 (r ising category -exc lus ive  
ru les  w i th  the  h ighest  sint i lar it ;y) 
Method 2 combines the, exami)h>based method and 
Method 1. That  is, it; combines the. method using 
similarity m~d the method usint'~ category-exchlsive 
rules in order to eliminate the meaningless (:art;gory- 
exclusive rules ment;ion(;(l i l lhe 1)revious t;el;ion. 
Mel;ho(1 2 also uses 152 patl(!rus for i(tentillving 
\])llllS(',|;,qll. q~h(!s(, ~ t)~li;l,(!l'll~q ~/l'(? llF,(RI ;IS rules i~ the 
,q;lllle ~,\;;ty }is ill Met\ ] lo( \ ]  \]. l)esir('d HIISWtH',q ill'(; (h;t(; l-  
mined by using the rule. having the high(>t probabil- 
ity. When mull;iple rules have the same 1)rolmbility, 
M(;thod 2 uses the wdue of the similarity described 
in the section of the examl)h>based m(;thod and }lll- 
alyzes the 1)robk:m with the rule having the highest 
simihu'ity. When multiple rules have th(; stone prob- 
nbilil;y and similm'ity, the method takes the exam- 
pies used by the rules having the highest probabil ity 
and the higlmst si,nilarity, and chooses the (:ategory 
with the larger llllllI))CF Of exami)les as t:hc desired 
answer~ in the same way as in Method 1. 
Itowever, when (:ategory-c.xchlsive rules having 
l l lt iro tha l l  Olte fre(l l lel lCy exist, the a})ove t ) roccdl l r ( ;  
is performed after el iminating all of the category- 
exclusive rules having one frequency, in el;her words, 
category-exclusive rules having more than one fl'e- 
quency are giwm a higher priority than category- 
exclusive rules having only one. flTo.qll(;nc.y })lit hav- 
ing ~ high(w similarity. This is 1)c(:ause eategory- 
(;xclusivc rules having only one fl'equen(:y are not so 
reliabh',. 
4 Experiments and discussion 
In our experiments we used a Kyoto  University text 
eorlms (Knrohashi and Nagao, 1997), which is a 
tagged corpus made Ul) of articles fi'om the Mainichi 
newspaper. All exl)eriments reported in this paper 
we.re performed using art, ielcs dated fi'om ,\]mmary 
\] to 5, 1995. We obtained the correct infi)rnmtion 
()n morphoh)gy and }mnse.t;su identiticathm from the 
tagged corpus. 
The following experiments were conducted to de- 
termine which supervised \]earnillg~ lnethod achieves 
the high<'.st a(:Cllra(:y l~tl;e. 
? Exlmriment \] 
\[,(;arninl,; set: ,Janllary 1, 1.995 
~J?t;Sl; st?t: ,\]atlll~/l'y 3, 1.995 
? \]';xt)eriment 2 
Learning set: 3ammry 4, 1995 
Test set: .\]a.nuary 5, 1995 
ltecm>e we used F, xlmriment \] in maki,lg Method 
I and Method 2, \]i;Xl)erinieut 71 is a ch)sc'd data..~et 
for Mel:l~od \] and Method 2. So, we l)crformed Ex- 
lmriment 2. 
The ,'(;suits arc. listed in '12fl)lt;,q I to d. \Ve used 
KNP2.0b4 (Kurohashi, 11997) mM KNP2.0t/6 (Kuro- 
hashi: 1998), which are bmlsetsu identitication and 
syntael;i(" analysis systems using tmmy hand-made 
rules in addition 1;o the six methods des(:ribed in 
Section 3. Be('mtse KNP is not based on a machine 
learning inethod but :many hand-made rules, in the 
KNP results "Learning selY and '~'.Test et" in the ta- 
llies have nt) meanings. In the eXll(wiment of KNP, 
we also uses morphological information in a corpus. 
~\].~hc ';F': ill l;\]le tables indicates the F-measure~ which 
is the. harmonic mean of a recall and a precision. A 
recall is l;he fl'action of correctly identilied partit ions 
out of all the partitions. A t)reeision is the ffae- 
th)n of correctly identitied partit ions out of all the 
SlmCeS which were judged to have a partit ion mark  
inserted. 
Tables I to -/I show the. following results: 
? In the test set I;he dc.cision-tree method was 
a little better thmt the maximmn-entropy 
569 
Table 1: Results of learning set of Exper iment  1 
Method D ~ 
Decision %-ee 99.58% 
Maximum Entropy 99.20% 
Example-Based 99.98% 
Decision List 99.98% 
Method 1 99.98% 
Method 2 99.98% 
KNP 2.0114 99.23% 
KNP 2.0116 99.73% 
Recall Precision 
99.66% 99.51% 
99.35% 99.06% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
100.00% 99.97% 
99.78% 98.69% 
99.77% 99.69% 
The number of spaces between two )nor flmmes is 
25,814. The number of lmrtitions is 9,523. 
Table 3: Results of learning set of Exi)er iment 2 
Method 
Decision Tree 
Maximum Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.01)4 
KNP 2.066 
99.07% I 
99.99% I 
99.99% I 
99.99% I 
99.99% I 
98.94% I 
99.47% I 
RecaU Precision 
99.71% 99.69% 
99.23% 98.92% 
100,00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
100.00% 99.98% 
99.50% 98.39% 
99.47% 99.48% 
The mlmber of spaces between two mor )heines is 
27,665. The number of partitions is 10 143. 
Table 2: Results of test set of Exper iment  1 
Method 
~ ion  Tree 
Maximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.066 
- F Recall 
98.87% 98.67% 99.08% 
98.90% 98.75% 99.06% 
99.02% 198.69% 99.36% 
98.95% i 98.43% ! 99.48% 
98.98% 198.54%! 99.43% 
99.16% I 98.88% ' 99.45% 
99.13% 99.72% ! 98.54% 
99.66% ~_99.68% ! 99.64% 
:)heroes is The lmmber of spaces between two mor 
Precision 
16,983. The mmiber of partitions is 6,166. 
T~ble 4: Results of test set of Exper iment  2 
Method 
Decision Tree 
M aximmn Entropy 
Example-Based 
Decision List 
Method 1 
Method 2 
KNP 2.0b4 
KNP 2.01)6 
P RTcad I Precision 
98.50% 98.51% I 98.49%- 
98.57% 98.55%1 
i 98.82% 98.71%1 
198.75% 98.27%1 
i 98.79% 98.54% I 
i 98.9\[1% 98.65% I 
199.(/7% 99.43%1 
L 99.51% 99.40% ~_ 
98.59% 
98.93% 
99.23% 
99.43% 
99.15% 
98.71% 
99.61% 
The nmnber of" spaces between two morphemes i
32,3o4. The number of partitions is 11,756. 
method.  Al though the maximuln-entropy 
method has a weak point  in that  it, does not 
learn the combinat ions of features, we could 
overcome this weakness by malting almost all of 
the combilmtions of features to produce a higher 
accuracy rate. 
? Tile decision-list n lethod was bet ter  t itan the 
maximum-entropy method in this experinmnt.  
? Tile example-based nlethod obtained the high- 
est accuracy rate among the four exist ing meth- 
ods. 
? Altt lough Method 1, which uses tim category- 
exclusive rule, was worse than the exmnple- 
based method,  it was better  than ti le decision- 
list method.  One reason for this was that  
ti le decision-l ist metl lod chooses rules rmldomly 
when mult iple rules have identical probabi l i t ies 
mid fl'equeneies. 
? Method 2, which uses the category-exchlsive 
rule with the highest similarity, achieved the 
highest accuracy rate among ti le supervised 
learning methods.  
? Tim example-based method,  tim decision-l ist 
inethod, Method 1 and Method 2 obta ined ac- 
curacy rates of about  100% for the leanfing set. 
This indicates that  these methods m:e especial ly 
strong for learning sets. 
? Tile two methods using similarity example-  
based method mid Method 2) were always bet-  
ter than the other methods, indicat ing that  the 
use of s imilar ity is eflective if we can define it 
approl)r iately. 
? We carried out experinmnts by using KNP,  a 
system that  uses ninny ha.nd-made rules. The 
F-measure of KNP was highest in the test set. 
? We used two versions of KNP, KNP 2.0b4 and 
KNP 2.0b6. The lat ter  was mud l  better  t lmn 
tlm former, iudicat ing tha.t the improvements 
made by hand are  effective. But, the mainte-  
nance of rules by hand has a l imit, so the im- 
provements made by hand are not always effec- 
tive. 
Tlle above exper iments indicate that  Method 2 is 
best among the machine learning methods '5. 
In Table 5 we show some cases which were par- 
t i t ioned incorrectly with KNP but correctly with 
51n these experiments, the. differences were very small. 
But, we think that the differences are significant to some ex- 
tent because we performed Experiment 1 and Experiment 2, 
the data we used are a large corplls containing about a few 
ten thousand morphemes and tagged objectively in advance, 
and the difference of about 0.1% is large in the precisions of 
99%. 
570 
Table 5: Cases when KNP was incorrect and Method 
2 wan correct  
ko tsukotsu \[ N H1,,'Tr 9aman.sh i 
(steadily) (lm prurient wit;h) 
L_{"" 1)e patient with ... steadily) 
lyoyuu wo \] motte \]~xrT;~l~ shirizoke 
i(enough Stl'engi;h) obj (have) (1)eat off) 
(... beat off ... having enough sl;rength) 
~aisha wo I gurupu-wake \ [ ~  
,.o.,v,,,,y obj (~r,,,,pi,,~) (do) 
(... 11o grouping companies) 
Method 2. A partition with "NEED" indicates that 
KNP missed inserting the i)artition mark, and a par- 
tition with "WRONG" indicates that KNP inserted 
the partitiol~ mark incorrectly. In the test set of Ex- 
periment 1, the F-measure of KNP2.0b6 was 99.66%. 
The F-measur(. ~ increases to 99.83%, ml(ler the as- 
sumption that when KNP2.0t)6 or Method 2 in cor- 
rect, the answer is correct. Although the accuracy 
rate for KNP2.0b6 was high, there were some cases 
in which KNP t)artitioned incorrectly and Method 
2 partitioned correctly, A combination of Method 
2 with KNP2.0b6 may be able to iml)rove the F- 
lile~lsllrO. 
The only 1)revious research resolving Imnnetsu 
identification by machine learning methods, in the 
work by Zhang (Zhang and Ozeki, 1998). The 
decision-tree, ine, thod was used in this work. But 
this work used only a small mmther of intor- 
l l l;ttion for t)llllsetsll identification" and (lid not 
achieve ll igh accuracy rat;es. (The  recall  rate 
was 97 .6%(=2502/ (2502+62) ) ,  the 1)recision rate 
was 92 .4%(=2502/ (2502+205) ) ,  and F -measure  was 
94.2%.) 
5 Conc lus ion  
To solve tile t)roblem of aecm'ate btmsetsu iden- 
tification, we carried out ext)eriments comt)aring 
tbur existing machine-learning methods (decision- 
tree method, maxilnum-entrol)y method, examI)le- 
based method and decision-list method). We ob- 
tained the following order of acem'acy in bunsetsu 
identification. 
Example-Based > Decision List > 
Maximum Entropy > Decision Tree 
We also described a new method which uses 
category-exclusive rules with the highest similarity. 
This method performed better than the other learn- 
ing methods in ore" exi)eriments. 
(>l'his work used oifly the POS information of the two roof 
phemes of an analyzed space. 
References  
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. \])ella Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Linguistics, 
22(l):ag-rl. 
Andrew Borthwicl% John Sterling, Eugene Agichtein, aim 
Ralph Grishlnan. 1998. Exploiting Diverse Knowledge 
Sources via Maximum I';ntropy in Named Entity ll.ecoglfi- 
tion. In Proceedings of the Sixth Workshop on Very LaT~le 
Corpora, pages 152 -160. 
Masayuki Kameda. 1995. Simple Jalmnese analysis tool q jp.  
The Association for Natural Lan, guage Processing, the Isl 
National Convention, pages 349-352. (ill .)~ti)~tlleSe ). 
Sadao Kurohashi and Makot<) Nagao. 1997. Kyoto University 
text corpus 1)ro./ect. pages 115-118. (in .lapanese). 
Sadao lgurohashi and MM~oto Nagao, 1998. Japanese Mof  
phological Analysis System JUMAN version 3.5. \])ei)art- 
mcnt of Informatics, Kyoto University. (in Japanese). 
Sadao Kurohashi, 1997. Japanese Dependency/Case Struc- 
ture Anahdzer KNI ) version 2.Obj. Department of lnfof  
matics, Kyoto Ulfiversity. (in Japmmse). 
Sadao Kurohashi, 1!)!18. Japanese Dependency/Uase Struc- 
ture Analyzer KNI' version 2.0b6. Del)artment of Infor- 
m~tics, l(yoto University. (in .lapmmse.). 
Makoto Nagao. 1984. A I,'ralneworl( of a Mechanical Transh> 
ti(m between Jai)anese alld English 1)y Analogy lh'incit)le. 
Artificial a\]l(l Iluman hitelligence~ pages 173 q80. 
Shigeyuld Nishiokayama, Takehito Utsuro, and Yu.ii Mat- 
sumoto. 119!18. Extracting preference of dependency be- 
twee/t ,Japanese subordinate clauses from corlms. IE ICE-  
WGNL098- ll, pages 31-38. (in .lapnese). 
NI,I{,\]. 1964. (National Language Resemvh Institute). 
Word List b?l Semantic Principles. Syuei SyuI)pan. (in 
Japanese). 
.\]. 1{.. Quinl;m. 1995. Pro qrams for machine learning. 
Lmme A. I{;mlshaw ;uld Mitchell 1'. Marcus. 1(.)(.15. Text 
clmnking using transformationq)ased l arning. \]11 Proceed- 
ira.IS of th, e Th, ird Workshop on Very La*#e Corpora, l)ages 
82 (.)4. 
Adwait l{.atnat)arklfi. 1996. A Maximum l,;nl;ropy Model tin" 
l'art-OlCSt)eech Tagging. 1)rocecdin9 s of P)mpirieal Method 
for Natural Language l'roeessings, pages 133 1,12. 
Adwait ll,atnaparkhi, 19!17. A l,inear Observed Time Statis- 
ticaI l'm'ser Based ou Maximum \]~ntropy Models. ht t~l'o - 
ceedings of Empirical Method for Natural l, an.quage l'ro- 
cessings. 
Eric Sven Ristad. 1998. Maximum Elltropy Modeling 
Toolkit, Release 1.6 beta. http://www,mnemonic.com/ 
software/metal. 
Ronald L. Rivest. 1987. lmarning l)ecision lasts. Machine 
Learning~ 2:229 -246. 
Erik P. Tjong Kim Sang and .lorn \reenstra. 1999. ll.el)re- 
senting text chunks, in EA CL'99. 
Kiyotaka Uehimoto, Satoshi Sekiim, and IIitoshi Isalmra. 
1999.. Japanese dependency structure analysis based on 
maximmn entrol)y models. In Proceedings of the Ninth 
CoTtference of the 15"ulwpeau Chapter of the Association 
for Computational Linguistics (P)A CL), pages 196-203. 
\])avid Yarowsky. 1!)94. Decision lists fo," lexieal ambiguity 
resolution: Application to accent restoration in Spanish 
and l,Yench. In 22th Alt, Tt~tal Meeting of the Assoeitation 
of the Computational Linguistics, pages 88-95. 
Yujie Zlmng and Kazuhiko Ozeki. 1998. The applica- 
tion of classitieation trees to bunsetsu segmentation of 
Jat)anese sentences. Journal of Natural Language Process- 
ing, 5(4):17-33. 
571 
Backward Beam Search Algorithm 
for Dependency Analysis of Japanese 
Satoshi  Sek ine 
Computer Science Department 
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekine@cs, nyu. edu 
K iyotaka  Uch imoto  H i tosh i  Isahara 
Communications Research Laboratory 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku, 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto,  i sahara \ ]  @crl. go. j p 
Abst rac t  
Backward beam search tbr dependency analy- 
sis of Japanese is proposed. As dependencies 
normally go fl'om left to right in Japanese, it is 
effective to analyze sentences backwards (from 
right to left). The analysis is based on a statisti- 
cal method and employs a bemn search strategy. 
Based on experiments varying the bemn search 
width, we found that the accuracy is not sen- 
sitivc to the bemn width and even the analysis 
with a beam width of 1 gets ahnost he stone de- 
pendency accuracy as the best accuracy using a 
wider bemn width. This suggested a determin- 
istic algorithm for backwards Japanese depen- 
dency analysis, although still the bemn search 
is eitbctive as the N-best sentence accuracy is 
quite high. The time of analysis is observed to 
be quadratic in the sentence l ngth. 
1 In t roduct ion  
Dependency analysis is regarded as one of the 
standard methods of Japanese syntactic anal- 
ysis. The Japanese dependency structure is 
usually represented by the relationship between 
phrasal units called 'bunsetsu'. A bunsetsu nsu- 
ally contains one or more content words, like a 
noun, verb or adjective, and zero or more func- 
tion words, like a postposition (case marker) 
or verb/noun sul~\[ix. The relation between two 
bunsetsu has a direction front a dependent to 
its head. Figure 1 shows examples of 1)unsetsu 
and dependencies. Each bunsetsu is separated 
by "I"" The  first segment "KARE-HA" consists 
of two words, KARE (He) and HA (subject case 
marker). The  numbers  in the "head" line show 
the head ID of the corresponding bunsetsus. 
Note that the last segment does not have a head, 
and it is the head bunsetsu of the sentence. The 
task of the Japanese dependency analysis is to 
find the head ID for each bunsetsu. 
The analysis proposed in this paper has two 
conceptual steps. In the first step, dependency 
likelihoods are calculated for all possible pairs 
of bunsetsus. In the second step, an optimal de- 
pendency set for the entire sentence is retrieved. 
In this paper, we will mainly discuss the second 
step, a method fbr finding an optimal depen- 
dency set. In practice, the method proposed in 
this paper should be able to be combined with 
any systems which calculate dependency likeli- 
hoods. 
It is said that Japanese dependencies have the 
tbllowing characteristics1: 
(1) Dependencies are directed from left to right 
(2) Dependencies don't cross 
(3) Each seglnent except he rightmost one has 
only one head 
(4) In many cases, the left; context is not nec- 
essary to determine a dependency 
The analysis method proposed in this paper as- 
sumed these characteristics and is designed to 
utilize them. Based on these assumptions, we 
can analyze a sentence backwards (from right 
to left) in an efficient manner. There are two 
merits to this approach. Assume that we are 
analyzing the M-th segment of a sentence of 
length N and analysis has already been done 
for the (M + 1)-th to N-th segments (M < N). 
The first merit is that the head of the depen- 
dency of the M-th segment is one of the seg- 
1Of course, there are several exceptions (S.Shirai, 
1998), but the frequencies of such exceptions are neg- 
ligible compared to the current precision of the system. 
We believe those exceptions have to be treated when the 
problems we are facing at the moment are solved. As- 
sumption (4) has not been discussed very much, but our 
investigation with humans showed that it is true in more 
titan 90?./0 of the cases. 
754 
ID i 2 3 4 5 6 
KARE-HA \[ FUTATABI I PAI-W0 \[ TSUKURI, I KANOJO-NI I 0KUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Head 6 4 4 6 6 - 
Translation: He made a pie again and presented it to her. 
Figure 1: Exmnt)le a JaI)anese sentence, 1)unsetsus and det)endencies 
ments between M + 1 and N (because of as- 
sumption 1), which are already analyzed. Be- 
cause of this, we don't have to kce 1) a huge lnlln- 
1)er of possible analyses, i.e. we can avoid some- 
thing like active edges in a chart parser, or mak- 
ing parallel stacks in GLR parsing, as we can 
make a decision at this time. Also, we can use 
the beam search mechanism, 1)y keet)ing only a 
certain nmnl)er of.analysis candidates at (',ach 
segment. The width of the 1)(;am search can 1)c, 
easily tuned and the memory size of the i)ro- 
(:ess is l)rot)ortional to the 1)roduct of the inl)ut 
sentence length and tile boron search width. 
The other merit is that the possit)le heads 
of tile d(~l)en(lency can t)e narrowed down 1)c- 
cause of the ~ssuml)tion of non-crossing det)en- 
(lencies (assumption 2). For exani1)le , if the 
K-th seglll(;nl; dCl)ends on the L-tll segnient 
(A4 < \]~ <~ L), then the \]~J-th segillent (:~l~n't 
depend on any segments between 1~ and L. 
According to our experilnent, this reduced the 
numl)er of heads to consider to less than 50(X~. 
The te(:hnique of backw~trd analysis of 
,lal)anese sentences has 1)een used in rule-based 
methods, for example (Fujita, 1988). How- 
ever, there are several difficulties with rule- 
based methods. First the rules are created by 
hmnans, so it is difficult to have wide cover- 
age and keel) consistency of the rules. Also, it 
is difficult to incorporate a scoring scheme in 
rule-1)ased methods. Many such met;hods used 
hem'isties to make deterministic decisions (and 
backtracking if it; fails in a sear(:hing) rather 
l;han using a scoring scheme. However, the com- 
1)ination of the backward analysis and the sta- 
tistical method has very strong advantages, one 
of which is the 1)emn search. 
2 Stat i s t i c  f ramework  
We. coin|lined tile backward beam search strat- 
egy with a statistical dependency analysis. 'rile 
det~fil of our statistic framework is described 
ill (Uehimoto et al, 1999). There have been 
a lot of prol)OS~fls for statistical analysis, in 
ninny languages, in particular in English and 
Japanese (Magerman, 1995) (Sekine and Grish- 
man, 1995) (Collins, 1997) (I/atnal)arkhi, 1997) 
(K.Shirai et.al, 1998) (Fujio and Matsnlnoto, 
1998) (Itaruno ct.al, 1997)(Ehara, 1998). One 
of the most advance(t systems in English is l)ro- 
posed 1)y I{atnaparkhi. It, uses the Maximum 
Entropy (ME) model and both of the accuracy 
and the speed of the system arc among the best 
ret)ortcd to date. Our  system uses the ME 
model, too. in the ME model, we define a set 
el! \]2~,atlll'eS which arc thought to l)e uscflfl in 
del)ealden(:y analysis, and it: learns the weights 
of the R~atures fl'om training data. Our t~ntttres 
in(:lude part-of-st)eech, inflections, lexical items, 
the existence of a contain or bra(:ket 1)etween 
the segments, and the distmme between the seg- 
ments. Also, confl)inations of those features are 
used as additional fe, atures. The system eal- 
(:ulates the probabilities of dependencies based 
on the model, which is trained using a training 
corpus. The probability of an entire sentence is 
derived from the 1)roduct of tile probal)ilities of 
all the dependencies in the sentence. We choose 
the analysis with the highest probafl)ility to be 
the analysis of the sentence. Although the ac- 
curacy of the analyzer is not the main issue of 
the t)al)er, as any types of models which use de- 
1)endency 1)rol)al)ilities can be iml)lelnented by 
our method, the 1)ertbrmance r t)orted in (Uchi- 
lnoto et al, 1999) is one of the best results re- 
ported by statistic~flly based systems. 
755 
3 A lgor i thm 
In this section, the analysis algorithm will be de- 
scribed. First the algorithm will be illustrated 
using an example, then the algorithm will be 
formally described. The main characteristics of 
the algorithm are the backward analysis and the 
beam search. 
The sentence "KARE-HA FUTATABI PAI-W\[I 
TSUKURI, KANOJ0-NI 0KUTTA. (He made a pie 
again and presented it to her)" is used as an in- 
put. We assume the POS tagging and segmen- 
tation analysis have been done correctly before 
starting the process. The border of each seg- 
ment is shown by "1". In the figures, the head of 
the dependency for each segment is represented 
by the segment number shown at the top of each 
segment. 
<Initial> 
ID 1 2 3 4 5 6 
RARE-HA \[ FUTATABI \[ PAI-WO \[ TSUKURI, \[ KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
................................................................. 
Algorithm 
1. Analyze np to the second segment from the 
end 
The last segment has no dependency, sowe 
don't have to analyze it. The second seg- 
ment fl'om the end always depends on the 
last segment. So the result up to the sec- 
end segment from the end looks like the 
following. 
<Up to the second segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI \[ PAI-WO I TSUKURI, I KANOJO-NI I OKFITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Cand 6 
................................................................. 
. The third segment from the end 
This segment ("TSUKURI," ) has two depen- 
dency candidates. One is the 5th segment 
("KANOJ0-NI") and the other is the 6th seg- 
ment ("0KUTTA"). Now, we use the proba- 
bilities calculated using the ME model in 
order to assign probabilities to the two can- 
didates (Candl and Cand2 in the following 
figure). Let's assume the probabilities 0.1 
and 0.9 respectively as an example. At the 
tail of each analysis, the total probability 
(the product of the probabilities of all de- 
pendencies) is shown. The candidates are 
sorted by the total probability. 
. 
<Up to the third segment from the end> 
ID 1 2 3 4 5 6 
KARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-HI I OKUITA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 0 - (0.9) 
Cand2 5 6 - (0.I) 
................................................................. 
The tburth segment from the end 
For each of the two candidates created at 
the previous tage, the dependencies of the 
fburth segment from the end ("PAI-W0") 
will be analyzed. For Candl, the segment 
can't have a dependency to the fifth seg- 
ment ("KANOJ0-1gI"), because of the non- 
crossing assmnption. So the probabili- 
ties of the dependencies only to the fourth 
(Candi-1) and the sixth (Candi-2) seg- 
ments are calculated. In the example, these 
probabilities are assmned to be 0.6 and 0.4. 
A similar analysis is conducted for Cand2 
(here probabilities are assumed to be 0.5, 
0.1 and 0.4) and three candidates are cre- 
ated (Cand2-1, Cand2-2 and Cand2-3). 
<Up to the fourth segment from the end> 
ID 1 2 3 4 5 6 
RARE-HA I FUTATABI I PAI-WO I TSUKURI, I KANOJO-NI I OKUTTA. 
(He-subj) (again) (pie-obj) (made ,) (to her) (present) 
C~dt - i  4 6 6 - (0.64) 
Candl-2 6 6 6 - (0.30) 
Cand2-1 4 5 6 - (0.05) 
Cand2-2 6 5 6 - (0.04) 
Caud2-3 5 5 6 - (0.01) 
................................................................. 
As tile analysis proceeds, a large number 
(almost L!) of candidates will he created. 
However, by linfiting the number of candi- 
dates at each stage, the total nmnber of 
candidates can be reduced. This is the 
beam search, one of the characteristics of 
the algorithm. By observing the analyses 
in the example, we can e~sily imagine that 
this beam search may not cause a serious 
problem in performance, because the candi- 
dates with low probabilities may be incor- 
rect anyway. For instance, when we set the 
beam search width = 3, then Canal2-2 and 
Cand2-3 in the figure will be discarded at 
this stage, and hence won't be used in the 
following analyses. The relationship of the 
beam search width and the accuracy oh- 
served in our experiments will be reported 
in the next section. 
756 
. Up to the, first segment 
The analyses are conducted in the, same 
way up to the first segment. For example, 
the result of tile analysis tbr the entire sell- 
tence will be shown below. (Appropriate, 
probabilities are used.) 
4.2 Beam search  w idth  and  accuracy  
In this subsection, the relationship between the 
beam width and the accuracy is discussed. In 
principle, the wider the beam search width, the 
more analyses can be retained and the better 
the accuracy cml be expected. However, the re- 
.................................................................. sultis somewhat different froan tile expectation. 
<Up to the first segment> 
ID 1 2 3 4 5 6 
KARE-IIA \[ FUTATABI \[ PAl-W0 { TSUKURI, \[ KANOJ0-NI \[ 0KUTTA. 
(Ile-subj) (again) (pie-obj) (made ,) (to her) (present) 
Candl 6 4 4 6 0 - (0. ii) 
Cand2 4 4 6 6 6 - (0 .09)  
Cand3 6 4 6 5 6 - (0.05) 
................................................................. 
Now, the formal algorithm is described induc- 
tiveJy in Figure 3. The order of the analysis is 
quadratic ill the length of the sentence. 
4 Exper iments  
In this section, experiments and evaluations will 
be reported. We use the Kyoto University Cor- 
pus (version 2) (Kurohashi el.el, 1{)97), a hand 
created Japanese corpus with POS-tags, bun- 
setsu segments and dependency information. 
The sentences in the articles from January 1, 
1994 to January 8, 1994 (7,960 sentences) a.re 
used t'or tim training of the ME model, and 
the sente, nccs in the artMes of Janum'y 9, 1994: 
(1,246 sentences) are used for the ewduation. 
The seid;ences ill the articles of Ja l luary 10, 1994 
are kept for future evaluations. 
4.1 Bas ic  Resu l t  
The evahlation result of our systenl is shown ill 
Table 1. The experiment uses the correctly seg- 
mente(1 and 1)art-oSsl)eet'h tagger1 sentences of 
the Kyoto University corpus. The bealn search 
width is sol; to 1, in other words, the systeln runs 
deterministically. Here, 'dependency accuracy' 
Table 1: lBvaluation 
Dependency accuracy 
Sentence accuracy 
Average analysis time 
87.14% (9814/11263) 
40.60% 0503/1239) 
0.03 sec 
is the percentage of correctly analyzed depen- 
dencies out of all dependencies. 'Sentence accu- 
racy' is the i)ercentage of the sentences in which 
all the dependencies are analyzed correctly. 
Table 2 shows the dependency accuracy and 
sentence accuracy for bemn widths 1 through 
20. The difference is very small, but the best 
Table 2: Relationship between beam width and 
accuracy 
Bemn width Dependency Sentence 
Accuracy Accuracy 
1 
2 
3 
4 
5 
6 
7 
10 
15 
20 
87.14 
87.16 
87.20 
87.1.5 
87.14 
87.16 
87.20 
87.20 
86.21 
86.21 
40.60 
40.76 
40.76 
40.68 
40.60 
40.60 
40.60 
40.60 
40.60 
40.60 
accuracy is obtained when the beain width is 11 
(fbr the dependency accuracy), and 2 and 3 (tbr 
the sentence accuracy). This proves that there 
are cases where the analysis with the highest 
product of probabilities is not correct, but the 
analysis decide(1 at each stage is correct. This is 
a very interesting result of our experiment, and 
it is related to assulnption 4 regarding Japanese 
dependency, lnentioned earlier. 
This suggests that when we analyze a 
.Japanese sentence backwards, we can do it de- 
terministically without great loss of accuracy. 
Table 3 shows where the mlalysis with bemn 
width 1 appears among the analyses with bealn 
width 200. It shows that most deterministic 
analyses appear as tile best analysis in the non- 
deterministic analyses. Also, mnong the deter- 
aninistic analyses which are correct (503 Sell- 
tences), 498 sentences (99.0%) have the same 
mmlysis at the best rank in the 200-beam-width 
analyses. (Followed by 3 sentences at the see-. 
end, 1 sentence ach at the third and fifth rank.) 
It means that in most of the cases, the mmlysis 
757 
<Variable> 
Length: 
W: 
C\[len\]: 
Length of the input sentence in segments 
The beam search width 
Candidate list; C for each segment keeps 
the top W partial analyses from that segment 
to the last segment. 
<Initial Operation> 
The second segment from the end depends on the last segment. 
This analysis is stored in C\[Length-l\]. 
<Inductive Operation> 
Assume the analysis up to the (M+l)-th segment has been finished. 
For each candidate ~c ' in C\[M+i\], do the following operation. 
Compute the possible dependencies of the M-th segment compatible 
with 'c'. For each dependency, create a new candidate Cd~ by 
adding the dependency to 'c'. Calculate the probability of 'd'. 
If C\[M\] has fewer than W entries, add ~d ~ to C\[M\]; 
else if the probability of Cd~ > the probability of the least 
probable entry of C\[M\], replace this entry by 'd'; 
else ignore 'd ' 
When the operation finishes for all candidates in C\[M+i\], 
proceed to the analysis of the (M-l)-th segment. 
Repeat the operation until the first segment is analyzed. 
The best analysis for the sentence is the best candidate in 
C\[1\]. 
Figure 2: Formal Algorithln 
with the highest probability at each stage also 
has the highest probability as a whole. This is 
related to assumption 4. The best analysis with 
the left context and the best analysis without 
tile left context are the same 95% of the time in 
general, and 99% of the time if the analysis is 
correct. These numbers are much higher than 
our human experinmnt mentioned in the ear- 
lier footnote (note that the number here is the 
percentage in terms of sentences, and the num- 
ber in the footnote is the percentage in terms of 
segnmnts.) It means that we may get good ac- 
curacy even without left contexts in analyz ing 
Japanese dependencies. 
4.3 N-Best  accuracy  
As we can generate N-best results, we measured 
N-best sentence accuracy. Figure 3 shows the 
N-best accuracy. N-best accuracy is the per- 
centage of tile sentences which have the correct 
analysis among its top N analyses. By setting 
a large beam width, we can observe N-best ac- 
curacy. The table shows the N-best accuracy 
when the beam width is set, to 20. When we set 
N = 20, 78.5% of the sentences have the cor- 
rect analysis in the top 20 analyses. If we have 
758 
Rank 1 
\]5"e<luc, n y 1175 
(%) (.{},5.8) 
Rank 11 
Frequen(:y 1 
(%) 
Table 3: The rank of the deterministic analysis 
2 3 4: 5 6 7 8 
20 11 8 4 2 1 2 
(1.6) (0.9) (0.6) (0.3) ( I ) .2 ) (0 .1 ) (0 .2 )  
12 j,5 1(i 17 18 
0 o 1 0 J J 
(0.1) ({}.1) (0.1) (0.1) 
9 10 
0 3 
(02) 
19 20 and more 
0 8 
(0.6) 
80 
70 
60 
50 
40 
30 
Sent;once Accuracy 
.53% 
#, 40.60% 
I I I I I I I I I I I - ~ T E  
0 5 10 11.5 20 
N 
Figure 3: N-best sentenc(~ Accuracy 
an ideal sysl;(ml for finding th(~ COl'lCCi; mmlysis 
a,lnOllg? th(;ln~ which maS, 11.%O SCllltl,lll;ic O1" COll- 
l,(;x{; inforlllt~I;io\]\]~ we can have a v(Ty a(:(;Hr~d;e 
an alyzer. 
\~TC Call llltl,l((; two interesting observations 
trom the result. The ac(:uracy of the 1--best 
mmlysis is about 40%, which is more tlm.n half 
of t, he accura(:y of 20-1)est analysis. This shows 
that although the system is not 1)erfb, ct, the 
computation of the 1)rolml)ilities is t)rol)ably 
good in order l;o find the correct mmlysis at the 
top rank. 
The other point is that the accm'aey is sat- 
urated at m'omM 80%. Iml)rovemel,t over 80% 
seelns very dit\[icult even if we use a very large 
bemn width W. (lf we set; W to the number 
of all possible combinations, which means al- 
most L! for sentence length L, we (21M gC{; 100(~0 
N-best accm'aey, lint this is not worth eonsidel'- 
ing.) This suggests tlmt wc h~we missed some- 
thing important. In part;icular, from our inves- 
tigation of the result, we believe that (:oordinate 
structure is one of the most important factors 
to iml)rove the accuracy. This remains one area 
of fllturc work. 
4.4 Speed of  the  analys is  
Based on the f'(n'nml algorithm, the analysis 
tinle can be estimated as t)rot)orl;ional to the 
square, of the inl)ut sentence length. Figure 4: 
shows the relationshi I) between the analysis 
time and the sentence length when wc set the 
beam width to 1. We use a Sun Ultra10 ma- 
chine and the process size is about 8M byte. 
We can see that the actual analyzing time al- 
Analysis time (see.) 
0.3 
0.2 * / "  
0 ~ ~  , r , , , 
0 10 20 30 40 
Sentence length 
\]?igure 4: \]~.elationshi 1) between sentence length 
and mmlyzing time 
most follows the quadratic urve. The ~verage 
amflysis time is 0.03 second and the ~werage sen- 
tence lengl:h is 10 segments. The analysis time 
for the longest sentence (41 segments) is 0.29 
second. W\; have not ot)l;imized the In'ogram in 
terms of speed aim there is room to shrink /;he 
process ize. 
759 
5 Conc lus ion  
In this paper, we proposed astatistical Jttpanese 
dependency analysis method which processes a
sentence backwards. As dependencies normally 
go from left to right in Japanese, it is eflhctive 
to analyze sentences backwards (from right to 
left). In this paper, we proposed a Japanese de- 
pendency analysis which combines a backward 
analysis and a statistical method. It can nat- 
urally incorporate a beam search strategy, an 
effective way of limiting the search space in the 
backwm'd analysis. We observed that the best 
perfbrmances were achieved when the width is 
very small. Actually, 95% of the analyses ob- 
tained with bemn width=l  were the stone as 
the best analyses with beam width=20. The 
analysis time was proportional to the square of 
the sentence length (nmnber of segments), as 
was predicted from the algorithm. The average 
analysis time was 0.03 second (average sentence 
length was 10.0 bunsetsus) and it took 0.29 sec- 
end to analyze the longest sentence, which has 
41 segments. This method can be ~tpplied to 
various languages which haw~ the stone or simi- 
lar characteristics of dependencies, for example 
Koran, Turkish etc. 
References  
Adam Berger and Harry Printz. 1998 : "A 
Comparison of Criteria for Maximum En- 
tropy / Mininmm Divergence Feature Selec- 
tion". Proceedings of the EMNLP-98 97-106 
Michael Collins. 1997 : "Three Generative, 
Lexicalized Models for Statistical Parsing". 
Proceedings of the ACL-97 16-23 
Terumasa Ehara. 1998 : "CMculation of 
Japanese dependency likelihood based on 
Maximmn Entropy model". Proceedings of 
the ANLP, Japan 382-385 
Masakazn t51jio and Yuuji Matsumoto. 1998 
: "Japanese Dependency Structure Analysis 
based on Lexicalized Statistics". Proceedings 
of the EMNLP-98 87-96 
Katsuhiko Fujita. 1988 : "A Trial of determin- 
istic dependency analysis". Proceedings of the 
Japanese Artificial Intelligence Annual meet- 
in9 399-402 
Masahiko Haruno and Satoshi Shirai and Yoshi- 
fumi Ooyama. 1998 : "Using Decision Trees 
to Construct a Practical Parser". Proceedings 
qf the the COLING/A CL-98 505-511 
Sadao Kurohashi and Makoto Nagao. 1994 : 
"KN Parser : ,J~tpanese Dependency/Case 
Structure Analyzer". Proceedings of The In- 
ternational Workshop on Sharable Natural 
Language Resources 48-55 
Sadao Kurohashi and Makoto Nagao. 1997 : 
"Kyoto University text corpus project". Pro- 
ceedings of the ANLP, Japan 115-118 
David Magerman. 1995 : "Statistical Decision- 
Tree Models for Parsiug". Proceedings of the 
ACL-95 276-283 
Adwait I/,atnaparkhi. 1997 : "A Linear Ob- 
served Time Statistical Parser Based on 
Maximum Entropy Models". Proceedings o.f 
EMNLP-97 
Satoshi Sekine and Ralph Grishman. 1995 : "A 
Corpus-based Probabilistic Grammar with 
Only Two Non-terminals". Proceedings of the 
IWPT-95 216-223 
Satoshi Shirai. 1998 : "Heuristics and its lira- 
itation". Jowrnal o\[ the ANLP, Japan Vol.5 
No.l, 1-2 
Kiyoaki Shirai, Kentaro Inui, Takenobu 'lbku- 
naga and Hozunli Tanaka. 1998 : "An Em- 
pirical Evaluation on Statistical Parsing of 
Japanese Sentences Using Lexical Association 
Statistics". P'roceedings ofEMNLP-98 80-86 
Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi 
Isahara. 1999 : "Jat)anese Dependency 
Structm'e Analysis Based on Maximum En- 
tropy Models". P~vceedings o\[ the EACL-99 
pp196-203 
760 
Word Order  Acqu is i t ion  f rom Corpora  
Kiyotaka Uchimoto I, Masaki Murata t, Qing Ma*, 
Satoshi Sekine*, and Hitoshi Isahara t 
tCommunications Research Laboratory 
Ministry of Posts and Telecommunications 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku 
Kobe, Hyogo, 651-2492, Japan 
\[uchimoto ,murata, qma, isahara\] @crl. go. jp 
*New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
sekYne~cs, nyu. edu 
Abstract 
In this paper we describe a method of acquiring word 
order fl'om corpora. Word order is defined as the or- 
der of modifiers, or the order of phrasal milts called 
'bunsetsu' which depend on the stone modifiee. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to de- 
ciding the word order mid which word order tends to 
be selected when several kinds of information con- 
flict. The contribution rate of each piece of informa- 
tion in deciding word order is eiIiciently learned by a 
model within a maximum entropy framework. The 
performance of this traiimd model can be ewfluated 
by checking how many instances of word order st- 
letted by the model agree with those in the original 
text. In this paper, we show t, hat even a raw cor- 
pits that has not been tagged can be used to train 
the model, if it is first analyzed by a parser. This 
is possible because the word order of the text in the 
corpus is correct. 
1 Introduction 
Although it is said tha~ word order is free in 
Japanese, linguistic research shows that there art 
certain word order tendencies - -  adverbs of time, for 
example, tend to t)recede subjects, mM bunsetsus in 
a sentence that are modified by a long modifier tend 
to precede other bunsetsus in the sentence. Knowl- 
edge of these word order tendencies would be useful 
in analyzing and generating sentences. 
Ii1 this paper we define word order as the order of 
nrodifiers, or the order of bunsetsns wlfich depend on 
the same modifiee. There arc several elements which 
contribute to deciding the word order, and they are 
summarized by Saeki (Saeki, 1.998) as basic condi- 
tions that govern word order. When interpreting 
these conditions according to our definition, we era: 
summarize them ,~ tbllows. 
Component la l  eondit lons 
? A bunsetsu having a deep dependency tends 
to precede a bunsetsu having a shallow depen- 
dency. 
When there is a long distance between amodifier 
and its modifiee, the modifier is defined as a bun- 
setsu having a deep dependency. For example, 
the usual word order of modifiers in Japanese 
is tlm following: a bunsetsu which contains an 
interjection, a bunsetsu which contains an ad- 
verb of time, a bunsetsu which contains a sub- 
ject, and a bunsetsu which contains an object. 
Here, the bunsetsu containing an adverb of time 
is defined as a bunsetsu having deeper depen- 
dency than the one containing a subject. We 
call the concept representing the distance be- 
tween a modifier and its modifiee the depth of 
dependency. 
A bunsetsu having wide dependency tends to 
precede a bunsetsu having narrow dependency. 
A bunsetsu having wide dependency is defined 
as a bunsetsu which does not rigidly restrict its 
modifiee. For example, the bunsetsu "~btqlo_c 
(to Tokyo)" often depends on a bunsetsu whicll 
contains a verb of motion such as "ihu (go)" 
while the bunsetsu "watashi_.qa (I)" can depend 
on a bunsetsu which contains any kind of verb. 
Here, the bunsetsu "watashi_ga (I)" is defined as 
a bunsetsu having wider dependency than 1;11o 
tmnsetsu ':Tok~./o_c (to Tokyo)." We call the 
concept of how rigidly a modifier restricts its 
modifiee the width of dependency. 
Syntact i c  condit ions 
? A bunsetsu modified by a long inodifier ton(Is to 
precede a bunsetsu modified by a short lnodifier. 
A long modifier is a long clause, or a clause that 
contains many bunsetsus. 
? A bunsel, su containing a reference pronoun tends 
to precede other bunsetsus in the sentence. 
? A bunsetsu containing a repetition word tends 
to precede other bunsetsus in the sentence. 
A repetition word is a word referring to a word 
in a preceding sentence. For example, Taro 
mid Hanako in the following text are repetition 
words. "Taro and Hanako love each other. Taro 
is a civil servant and Hanako is a doctor." 
? A bunsetsu containing the case marker "wa" 
tends to precede other bunsetsus in the sentence. 
A mnnber of studies have tried to discover the rela- 
tionship between these conditions and word order in 
871 
Japanese. Tokunaga and Tanalca proposed a model 
for estimating JaI)anese word order based on a dic- 
tionary. They focused on the width of dependency 
(Tokunaga and Tanal~a, 1991). Under their model, 
however, word order is restricted to the order of case 
elements of verbs, and it is pointed out that the 
model can deal with only the obligatory case and 
it cmmot deal with contextual information (Saeki, 
1998). An N-gram model fbr detecting word order 
has also been proposed by Maruyama (Maruyama, 
1994), but under this model word order is defined as 
the order of morpheines in a sentence. The problem 
setting of Maruyama's study thus differed fl'om ours, 
and the conditions listed above were not taken into 
account in that study. As for estimating word or- 
der in English, a statistical model has been proposed 
by Shaw and Hatzivassiloglou (Shaw and Hatzivas- 
siloglou, 1999). Under their model, however, word 
order is restricted to the order of premodifiers or 
modifiers depending on nouns, and the model does 
not simultaneously take into account many elements 
that contribute to determining word order. It would 
be difficult to apply the model to estimating word 
order in Japanese when considering the many condi- 
tions as listed above. 
In this paper, we propose a method for acquiring 
from corpora the relationship between the conditions 
itemized above and word order in Japanese. The 
method uses a model which automatically discovers 
what the tendency of the word order in Japanese is 
by using various kinds of information in and around 
the target bunsetsus. This model shows us to what 
extent each piece of information contributes to decid- 
ing the word order and which word order tends to be 
selected when several kinds of information conflict. 
The contribution rate of each piece of information in 
deciding word order is efficiently learned by a model 
within a maximum entrot)y (M.E.) framework. The 
performance of the trained model can be evaluated 
according to how many instances of word order se- 
lected by the model agree with those in the original 
text. Because the word order of the text in the corpus 
is correct, the model can be trained using a raw co> 
pus instead of a tagged corpus, if it is first analyzed 
by a parser. In this paper, we show experimental re- 
sults demonstrating that this is indeed possible even 
when the parser is only 90% accurate. 
This work is a part of the corpus based text gen- 
eration. A whole sentence can be generated in the 
natural order by using the trained model, given de- 
pendencies between bunsetsus. It could be helpful 
for several applications uch as refinement support 
and text generation in machine translation. 
2 Word  Order  Acqu is i t ion  and 
Es t imat ion  
2.1 Word  Order  Mode l  
This section describes a model which estimates the 
likelihood of the appropriate word order. We call 
this model a word order model, and we implemented 
it within an M.E. framework. 
Given tokenization of a test corpus, the problem 
of word order estimation in Japanese can be reduced 
to the problem of assigning one of two tags to each 
relationship between two modifiers. A relationship 
could be tagged with "1" to indicate that the order 
of the two modifiers is appropriate, or with "0" to in- 
dicate that it is not. Ordering all modifiers so as to 
assign the tag "1" to all relationshit)s indicates that 
all modifiers art  in the appropriate word order. The 
two tags form the space of "futures" in the M.E. 
formulation of our estimation problem of word or- 
der between two modifiers. The M.E. model, as well 
as other similar models allows the computation of 
P(flh) for any f in the space of possible futures, F, 
and for every h in the space of possible histories, H. 
A "history" in maximum entropy is all of the condi- 
tioning data that enable us to make a decision in the 
space of futures. In the estimation problem of word 
order, we could reformulate this in terms of finding 
the probability of f associated with the relationship 
at index t in the test cortms as: 
P(flht) = P( f l  hfformation derivable 
from the test corpus 
related to relationship t) 
The computation of P(flh) in any M.E. models is 
dependent on a set of "features" which should be 
hdpful in making a prediction about the flmlre. Like 
most current M.E. models in computational linguis- 
tics, our model is restricted to features which are 
binary functions of the history and future. For in- 
stance, one of our features is 
1. : if has(h,x) = true, 
x = "Mdfl'l - Head-  
g(h,f) = POS(Major) : verb" (1) 
&f= l  
0 : otherwise. 
Here "has(h,x)" is a binary flmction which returns 
true if the history h has feature z. We focus on the 
attributes of a bunsetsu itself and on the features 
occurring between bunsetsus. 
Given a set of features and some training data, 
the maximum entropy estimation process produces a
model ill which every feature .qi has associated with it 
a parameter ai. This allows us to compute the con- 
ditional probability as follows (Berger et al, 1996): 
ag~ (h .f) 
P( / Ih ) -  1L ' (2) 
Z (h) 
ct i . (3) 
Y i 
The maximum entropy estimation technique guaran- 
tees that for every feature gi, the expected value of 
gi according to the M.E. model will equal the empir- 
ical expectation of gi in the training corpus. In other 
words: 
P(h,/). Mh, f) 
h,f 
= (4) 
h / 
Here /5 is an empirical probability and l~ Ie  is the 
872 
Table l: Example of estimating the probabilities of word orders. 
? ) I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. ) "  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?:LX ~ (tennis) / bt:? (played.)" IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?::x,: := 0.4 x 0.8 ? 0.7 0.224 
\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?:o (plowed.)" I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i 
1"9:-':;? ~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.? (played.)" \[13*lll,2<I,u:l. x l: ':.xt, jall x l!).:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036 
\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)" ~ ? H  x l~;.:.xr, ~,~ZI? x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024 
prol)ability assigned by the M.E. model. 
We detine a word order model as a model which 
learns the at)l)ropriate order of each pair of nlodifiers 
which depend on the same modifiee. 'l'his model is 
derived from Eq. (2) as follows? Assmne that there 
are two bunsetsus 231 and 23~ which depend on the 
buusetsu B and that  It is the information derivable 
from the test corpus. \]?lie probability that "B\] B2" is 
the at)propriate order is given by the following equa- 
tion: 
\]iik: :1 ffi( l  ,b) (~'i ,i 
where .qi(1 < i < k) is a fl,atm'e and "1" indicates 
that the order is at)propriate. The terms cq,i and 
(~0,i are estimated fl'oln a eorl)us which is nlorpho- 
logically and syntactically analyzed. When there are 
three or more b\]msetsus that det)end on tit('. S}tlne 
? moditiee, the probability is estimated as follows: I or 
~'t bunsetsus 231, 232, . . . ,  23n which depend on the 
bmtsetsu B and for the information h derivaMe from 
the test corpus, the prot)ability t;hat "23\] 23~ . . .  23," 
is the at)propriate order, or P( l lh) ,  is represented ass 
the probability that every two bunsetsus "Bi ~i-Fj 
(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ 
ate. order, or  P({14~i, i_ l . j  = l \ [ l<  i <. n - -  1, l :< j < 
n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i 
23i-Fj" is the appropriate order. Let us assume that 
every 14Q,~:+j is independent each other. Then 1)(1 Ih,) 
is derived as follows: 
2 ' ( l ib )  = \]1\]. i , . -  \ ] ,  
1 < .i _< , , , -  i}lh) 
n- - |  n - i  
i -- I  j --1 
= l l  1I j), 
i= l  j= l  
where \[Si,i+ j is the information derivable when fb- 
cusing on the bunsetsu 13 m~d its modifiers 13i and 
Bi+j. 
For example, in the sentence "I~ U (kinou, yester- 
day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo, 
tennis) / b t:o (sita., l)layed.)," where a "/" repre- 
sents a bunsetsu boundary, there are three bunset- 
sus that depend on the verb "b  ~: (sita)." We train 
a word order inodel under the assmnl)tion that the 
orders of three t)airs of modifiers -"I~l U" and "~ 
f$1.~," "Net\] " and "?  7-:7, ~ ," and ":kl~l*l~" and "5: 
m :7, ~"  .... are al)ttropriate. We use various ldnds of 
intormation in and around the target bunsetsus as 
features. For example, the information or the feature 
that a noun of time i)recedes a t)rot)er noun is deriv- 
able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro) 
/ b 1=o (pl~\yed.)," and the feature that a case fol- 
lowed by a case marker "w?' precedes a case followed 
by a caqe marker "wo" is derivable from the order ":~ 
fll~ It. ( Taro_wa, Taro) / ? ~ 7. ?k (tennis_wo, tennis) / 
b 2C_o (sita., t)layed.)." 
2.2 Word Order  Es t imat ion  
This section describes the algorithm of estimating 
the word order by using a trained word order model. 
The word order estimation is defined as deciding 
the order of ntoditiers or bunsetsus which depend 
on the same modifiee. The input of this task con- 
sists of modifiers and informat, ion necessary to know 
whether or not features are found. The output is 
the order of the inodifiers. We assume that lexical 
selection in each bunsetsu is already done and all 
del)endencies in a sentence are found. The informa- 
tion necessary to know whether or not features are 
found is morphological, syntactic, semmltic, and ('on- 
textual information, and the locations of bunsetsu 
bonndaries. The features used in our ext)eriments 
are described in Section 3. 
Word order is estimated in the following steps. 
Procedures 
1. All possible orders of modifiers are found. 
2. For each, the probability that it is apt)ropriate 
is estimated by a word order model, or Eq. (6). 
3. The order with the highest probability of 1)eing 
approl)riate is selected. 
l)br example, given the sentence "1~ U (kinou, 
yesterday) /:kfilIl~ (Taro_wa, Taro) /? : :x  ~ (tcn- 
nis_wo, temfis) / b t:o (sita., played.)," tim modi- 
tiers of a verb "b  ?:_ (played)" are three tmnsetsus, 
"l~ U (yesterday)," :k~/i ~ (Taro)," "?  = x ~ (ten- 
nis)." Their apt)ropriate order is estimated in the 
following steps. 
1. The probabilities that the orders of the three 
pairs of modifiers "N- LI " and ":is: BII l~ ," "I~ 
U" and "?~:7 ,~,"  and "~fllIl?" and "?  
c .x  ~" are appropriate are estimated. As- 
sume, for example, ~-H ,;k~l~ta, PrI~ It ,? :-~. ~, and 
P;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7. 
2. As shown in Table 1, probabilities are estimated 
for all six possible orders. The order "I~ U / :k 
fill IS / -7- ~- y. ~ / b \]Co ," which has the highest 
probability, is selected as the most apt)ropriate 
order. 
2.3 Per fo rmance  Eva luat ion  
The pcrformancc of a word order model can be eval- 
uated in the following way. First, extract from a 
test corpus bunsetsus having two or more modifiers. 
Then, using those 1)unsetsus and their modifiers as 
873 
Data 
~Jtlnsetsll lJtlnsetstl nHIYiber Of babel 
nlllllber modifier 
0 1 P 
1 5 
2 3 
3 4 
4 5 P 
5 
Table 2: Example of modifiers extracted fl'om a corpus. 
Modifiers (Bunsetsu number) 
Strings in a bunsetsu 
>kflli ~ ( Taro_to, Taro and) 
~Y'{a ( lIanako_to, llanako) 
-Y- = x q~ (tennis_no, tennis) 
~lc .  (sial_hi, tournament) 
lll'C, (dete,, participate,) 
~@ b t=, (yusyo_sita., won.) 
Moditiers whose modiliee is the bunsetsu 
in the left column. 
~,~ a (0) 
?=x0~ (2) 
~a  (0) ~-r-~a 0) '~ :  (a) 
~e  (o) ~?-~* (1) If'~ (4) 
input, estimate the orders of the modifiers as de- 
scribed in Section 2.2. The percentage of the modi- 
flees whose modifiers' word order agrees with that in 
the original text then gives what we call the agree- 
ment rate. It is a measure of how close the word 
order estimated by the model is to the actual word 
order in the training corpus. 
We use the following two measurements to calcu- 
late the agreement rate. 
Pa i r  of  modi f ie rs  The first measurement is the 
percentage of the pairs of modifiers whose word 
order agrees with that in the test corpus. For 
exmnple, given the sentence in a test corpus "N 
kl (kinou, yesterday) / ~t I la  (Taro_wa, Taro) 
/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita., 
played.)," if the word order estimated by the 
model is "~ H (yesterday) / -7" -- 2. ~ (tennis) / 
~1~ ~:~ (Taro) / b too (played.)," then the or- 
ders of the pairs of modifiers in the original sen- 
tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and 
"~lit:~ / ~--2` ~ ," and those in the estimated 
word order are "~H / -~---2`~," "~H / 
1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ." The agreement 
rate is 67% (2/3) because two of the three orders 
are the same as those in the original sentence. 
Complete  agreement  The second measurement is 
the percentage of the modifiees whose modifiers' 
word order agrees with that in the test corpus. 
3 Exper iments  and  D iscuss ion  
In our experiment, we used the Kyoto University text 
corpus (Version 2) (Kurohashi mid Nagao, 1997), a 
tagged corpus of the Mainichi newspaper. For train- 
ing, we used 17,562 sentences from newspaper arti- 
cles appearing in 1995, from January 1st to Jmmary 
8th and from Jmmary 10th to June 9th. For testing, 
we used 2,394 sentences fl'om articles appearing on 
January 9th and from June 10th to June 30th. 
3.1 Def in i t ion of  Word  Order  in  a Corpus  
In the Kyoto University corpus, each bunsetsu has 
only one modifiee. When a bunsetsu Bm depends on 
a bunsetsu Bd and there is a bunsetsu /3p that de- 
pends on and is coordinate with \])d, Bp has not only 
the information that its modifiee is \]~d but also a la- 
bel indicating a coordination or the information that 
it is coordinate with B d. This information indirectly 
shows that the bunsetsu Bm can depend on both \]3p 
and Bd. In this case, we consider Bm a modifier of 
both Bv and B d. 
Under this condition, modifiers of a bunsetsu B 
are identified in the following steps. 
1. Bunsetsus that depend on a bunsetsu B are clas- 
sifted as modifiers of B. 
2. When B has a label indicating a coordination, 
bunsetsus that are to tile left of 13 and depend on 
the same modifiee as B are classified as modifiers 
of B. 
3. Bunsetsus that depend on a modifier of B and 
have a label indicating a coordination are clas- 
sifted as modifiers of B. The third step is re- 
peated. 
When the above procedure is completed, all bunset- 
sus that coordinate with each other are identified as 
modifiers which depend oi1 the same nmdifiee. For 
example, from the data listed on the left side of To- 
ble 2, the modifiers listed in the right-hand column 
are identified for each bunsetsu. "Nt~I; ~ (Taro_to, 
Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(, 
(dete,, participate,)" are all identified as modifiers 
which depend on the same modifiee "~ b 7=? 
(yusyo_sita., won.)." 
3.2 Exper imenta l  Resu l ts  
The features used in our experiment are listed in Ta- 
bles 3 and 4. Each feature consists of a type and 
a value. The features consist basically of some at- 
tributes of the bunsetsu itself, and syntactic and con- 
textual information. We call the features listed in 
Tables 3 'basic features.' We selected them man- 
ually so that they reflect the basic conditions gov- 
erning word order that were sunmmrized by Saeki 
(Saeki, 1998). The features in Table 4 are combina- 
tions of basic features ('combined features') and were 
also selected manually. They are represented by the 
nmne of the target bunsetsu plus the feature type of 
the basic features. The total number of features was 
about 190,000, and 51,590 of them were observed in 
the training cortms three or more times. These were 
the ones we used in our experiment. 
The following terms are used in these tables: 
Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de- 
scribed in Section 2.1 estimates the probability 
that modifiers are in the appropriate order as 
the product of the probabilities of all pairs of 
modifiers. When estimating the probability tbr 
each pair of modifiers, the model assmnes that 
the two modifiers are in the appropriate order. 
Here we call the left modifier Mdfrl, the right 
modifier Mdfr2, and their modifiee Mdfe. 
Head:  the rightmost word in a bunsetsu other than 
those whose major pro't-of-speech I category is 
1Part-of-speech categories follow those of JUMAN (Kuro- 
hashi and Nagao, 1998). 
874 
Table 3: Basic features. 
Bas ic  features  
Feature values (Number of type) Feature type tegors\] Target 
1)tmsetsus 
1 Mdfrl, Mdfr2, 
Mdfe 
2 Mdfrl, Mdfr2, 
Mdfe 
3 Mdfrl, Mdfr2, 
Mdfe 
4 Mdfrl, Mdh'2, 
Mdfe 
m m  
\[\]ead-POS(Major) 
\[tead-POS(Minor) 
\[lead-hff(Major) 
\[Iead-Inf(Minor) 
\[Iead-SemFeat(110) 
\[1ead-SemFeat(111 ) 
\[Iead-SemFeat(433) 
5 Mdfrl, Mdff2, rype(String) 
Mdfe rype(Major) 
type(Minor) 
6 Mdfrl, Mdfr2, lOSIll l(String) 
Mdfe lOSIIIl(Minor) 
lOSII12(String) 
lOSUI2(Minor) 
7 Mdfrl, Mdfr2, Period 
Mdfe 
8 Mdfrl, Mdfr2 Numl)erOfMdfrs 
Mdfe NumberOfMdfrs 
9 Mdfrl, Mdfr2, 
Mdfe 
10 Mdfl'l, Mdfr2 
Coordination 
(Total : 90) 
Mdfrl-MdfrType-ll )to-Mdfr2-Typc 
Mdfl'2-MdfrTypeql )to-Mdfl-I -q'ype 
Mdfi'l -Md frType-lDto- Md fr2-MdfrType 
11 Mdfrl, Mdfr2, Rel)etition-llead-l,ex 
Mdfe Repetition-Md fr-lleadq,ex 
12 Mdfrl, Mdfr2 ReferencePronoun 
i~efereneePronou  (String) 
',.%066) 
~u (verb), s~u (adjective), ~,'~ (noun) . . . .  (11) 
~'~:~ (common oun), m'~ (quantifier) . . . .  (24) 
~1~ (vowel verb) . . . .  (30) 
'.'~ (stem), t~*~ (fandamental form) . . . .  (60) 
rrue (1) 
true (1) 
true (1) 
:~, :a ,  <-u<, t:u, ~, ~=, t . . . .  (7:3) 
uJ:~ (post-positional particle), . . .  (43) 
?,~JJ'~ (e~use marker), ~*$ (imperative form) . . .  (102) 
'~',5, ~<', a~, ,., l,~. . . . .  (63) 
)ill\], ~$m (ease marker), . . .  (5) 
~, ~, *, ~,*,, ... ((~3) 
~,~;~1 (ease marker) . . . .  (41 
\[nil\], \[exist\] (2) 
A(0), B(1), C(2), 1)(3 or more) (4) 
A(2), B(:3), C(4 or more) (3) 
P(Coordim~te), A(Apposition), I)(otherwise) (3) 
\]'rue, False (2) 
rrue, False (2) 
true, False (2) 
86.65% 73.87% 
\[--0.79%) (--1.54%) 
87.07% 75.03% 
',--O.37%) (--0.38%) 
87.39% 75.20% 
',-0.05%) (-0.m%) 
87.21% 75.20% 
\[--0.23%) (--0.21%) 
84.78% 70.03% 
(-2.66%) (-5.38%) 
87.32% 75.14% 
(-0.12% (--0.27%) 
87.39% 7 ~  
(-0.05%) (+0.13%) 
87.14% 74.86% 
(-0.30%) (-0.55%) 
87.40% 70.30?./o 
(-0.04%) (-0.0~%) 
8~.2~% 73.61% 
(--1.18% (--1.80%) 
87.34% 75.09% 
(--0.10%) (--0.'32%) 
\[nil\],\[exist\] (2) 87.31% 75.id% 
\[nil\], \[exist\] (2) (--0.13%) (--0.27%) 
\[nil\], \[exist\] (2) 87.27% 75.12% 
:~, :a,  :~ .~,~=.~, ,  ~. . . . .  (42) (--0.17%) (--0.29%) 
'%} @ (special marks)," "112 N (1)ost-posi~ioual 
particles)," or "}~N~? (suffixes)." 
Head-Lex :  the fllndalnental forth (unintlected 
forln) of the head word. Only words with a fre- 
quency of tlve or more are used. 
Head- In f :  the inflection type of a head. 
SemFeat :  We use the upper third layers of bunrui 
.qoihyou (NLl/I(National Language Research In- 
stitute), 19641 as semantic features. Bunrui goi- 
hyou is a Japanese thesaurus that has a tree 
structure and consists of seven layers. The tree 
has words in its leaves, and each word has a fig- 
ure indicating its category number. For exam- 
ple, the figure in parenthesis of a feature "Head- 
SemFeat( l l0)" in Table 3 shows the upper three 
digits of the category number of the head word 
or the ancestor node of the head word in the 
third layer in the tree. 
Type:  the rightmost word other than those whose 
major part-of-speech category is "~@ (special 
marks)." If the major category of the word 
is neither "NJN (post-positional particles)" nor 
"}~/~'~ (suffixes)," and the word is inflectable, 2 
then the type is represented by the inflection 
type. 
JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post- 
positional particle in the bunsetsu. And if there 
are two or more post-positional particles in the 
bunsetsu, JOSHI2 is the second-rightmost post- 
positiolml particle. 
NmnberOfMdf rs :  number of modifiers. 
2The inflection types follow those of J UMAN. 
Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types of 
tile modifiers of Mdfi'l and Mdfr2. 
X- IDto -Y :  X is identical to Y. 
Repet i t ion -Head-Lex :  a ret)etition word allpear- 
ing ill a preceding senteuce. 
Re ferencePronour l :  a reference pronoun appear- 
ing in the target bunsetsu or ill its modifiers. 
Categories 1 to 6 ill Table 3 reI)resent attributes 
in a bunsetsu, categories 7 to 10 represent syntac- 
tic information, and categories 11 and 12 represent 
contextual information. 
The results of our experiment are listed in Table 5. 
The first line shows tlle agreement rate when we esti- 
mated word order for 5,278 bunsetsus that have two 
or more modifiers and were extracted from 2,394 sen- 
tences al)pearing on Jmmary 9th and from June 10th 
to June 301tl. \Ve used bunsetsu boundary informa- 
tion and syntactic and contextual information which 
were derivable froln the test corpus and related to 
the input bunsetsus. As syntactic ilffOrlnation we 
used dependency inforlnation, coordinate structure, 
and information on whether the target bunsetsu is at 
the eM of a sentence. As contextual information we 
used the preceding sentence. The values in the row 
labeled Baseline1 in Table 5 are the agreement rates 
obtained when every order of all pairs of modifiers 
was selected randolnly. And values in the B&seline2 
row are the agreement rates obtained when we used 
the following equation instead of Eq. (5): 
freq(w12) 
PMu'(llh) = freq(w12) + frcq(w21)" (7) 
875 
Table 4: Combined features. 
Accuracy without 
the feature 
Pair of Complete 
modifiers :tgreement 
87.23% 74.65% 
(-0.21%) (-0.76%) 
Combined  features  
- -  Twin tbatures 
(Mdfr 1-Type, Mdfr2-Type) ,  
(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,  
(Mdfr  1-Type,  Md fe- t tead-POS) ,  
(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),  
(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc), 
(MdfrE-Type,  Mdfe-Head-Lex) ,  
(Mdfrg-Type,  Mdfe-Head-POS) ,  
(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,  
(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,  
Mdfr  1-Head-Lex,  Mdfe-Per iod) ,  
Mdfr  1 -nead-POS,  Mdfe-Per lod) ,  
Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,  
Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,  
Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,  
Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)  
' t~iplet  tca tures  87.22% i 74.86% 
Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%) 
Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,  
Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,  
MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,  
Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),  
Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,  
Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),  
Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)  
All of  above  combined  features  85.79% 77.67% 
~--1.65%) (--3.74%) 
Table 5: Results of agreement rates. 
Agreement  ra te  
Pair of modifiers Coml)lete agreement 
Our method 87.44%(72,367/14,137) 75.41% (3,980/5,278) 
Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278) 
Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278) 
IIere we assume that B1 and \]32 are modifiers, their 
modifiee is B, the word types of B1 and \]32 are re- 
spectively Wl and we. The values frcq(wr2) and 
frcq(w.27 ) then respectively represent the fl'equencies 
with which w7 and w,2 appeared in the order "WT, we, 
mid w" and "w2, WT, and w" in Malnichi newspaper 
articles fl'om 1991 to 1997. a Equation (7) means 
that given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~, 
(tennis_wo) / b ~-:o (sita.)," one of two possibili- 
ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)" and "#c (wo) 
/ tS (wa) / b ~:o (sita.)," which has the higher fre- 
quency, is selected. 
3.3 Features  and Agreement  Rate  
This section describes how much each feature set con- 
tributes to improving the agreement rate. 
The values listed in the rightmost columns in Ta- 
bles 3 and 4 shows the performance of the word or- 
der estimation without each feature set. The values 
in parentheses are the percentage of improvement or
degradation to the formal experiment. In the exper- 
iments, when a basic feature was deleted, the com- 
bined features that included the basic feature were 
also deleted. The most useful feature is the type of 
3When wl and w2 were the same word, we used the head 
words in Bt  and 132 as Wl and w2. When one offreq(wt2) and 
freq(w21) was zero and the other was five or more, we used 
the f lequencies when they appeared in the order "Wl ws" and 
"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl). 
When both freq(wl.2) and freq(w27) were zero, we instead 
used random figures between 0 and t. 
bunsetsu, which basically signifies the case marker or 
inflection type. This result is close to our expecta- 
tions. 
We selected features that, according to linguistic 
studies, as mudl  as possible reflect the basic condi- 
tions governing word order. The rightmost column 
in Tables 3 and 4 shows the extent o which each con- 
dition contributes to improving the agreement rate. 
However, each category of features might be rougher 
than that which is linguistically interesting. For ex- 
ample, all case markers uch as "wa" and "wo" were 
classified into the same category, and were deleted 
together in the experiment when single categories 
were removed. An experiment that considers each 
of these markers eparately would help us verify the 
importance of these markers separately. If we find 
new features in future linguistic research on word or- 
der, the experiments lacking each feature separately 
would help us verify their importance in the same 
manner .  
3.4 Tra in ing  Corpus  and Agreement  Rate  
The agreement rates for the training corpus and the 
test corpus are shown in Figure 1 as a function of 
the amount of training data (ntunber of sentences). 
The agreement rates in the "pair of modifiers" and 
'?!19~ . . . . .  %:I:: 'i, ~ . . . . . . . . .  ~':z~-,~',:: : : i 
= 
?I 90 90 
!' 
S5 ~5 i ~ - 
E 
75 ~ 75 
7O 7O 
65 . . . . . . . .  (,5 0 . . . .  
0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1 
\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)ala 
Figure 1: Relationship between tile amount of training 
data and the agreement rate. 
"Complete agreement" measurements were respec- 
tiw~ly 82.54% and 68.40%. These values were ob- 
tained with very small training sets (250 sentences). 
These rates m'e considerably higher than those of 
the baselines, indicating that word order in Japanese 
can be acquired fl'om newspaper articles even with a 
small training set. 
With 17,562 training sentences, the agreemenl, 
rate in the "Complete agreement" measurement was 
75.41%. We randomly selected and analyzed 100 
modifiees from 1,298 modifiees whose modifiers' word 
order did not agree with those in the original text. 
We found that 48 of them were in a natural order 
and 52 of them were in an unnatural order. The 
former result shows that the word order was rela- 
tively fl'ee and several orders were acceptable. The 
latter result shows that the word order acquisition 
was not sufficient. To complete the acquisition we 
need more training corpora and features which take 
into account different information than that m Ta- 
bles 3 mid 4. We found many idiomatic expres- 
876 
sions in the uimatural word order results, such as "~ 
ffl\[~il~:5~ (houchi-kokka_ga,  country under the rule 
of law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru, 
to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl- 
,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U 
(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) / 
~'~B (seikon, one's heart and soul) /g~ 6 (homcru, 
to trot somethil,g into soinething)." We think that 
the apt)ropriate word order for these idiomatic ex- 
pressions could be acquired if we had more training 
data. We also found several coordinate structures in 
the Ulnlatural word order results, suggesting that we 
should survey linguistic studies on coordinate struc- 
tures and try to find efllcient features for acquiring 
word order from coordinate structures. 
We (lid not use the results of semantic and con- 
textual analyses as input because corpora with se- 
mantic and contextuM tags were not available. If 
such corpora were available, we could more et\[iciently 
use features dealing with seinantic features, reference 
pronouns, and repetition words. We plan to make 
corpora with semantic and contextual tags and use 
these tags as input. 
3.5 Acqu is i t ion  f rom a Raw Corpus  
In this section, we show that a raw cortms instead of 
a tagged corpus can be used to train the lnodel, if it 
is first analyzed by a parser. We used the lnorl)holog- 
ical analyzer JUMAN and a tmrser KNP (Kurohashi, 
11198) which is based on a det)endency grainlnar, 
it, order to extract iuforumtion from a raw corpus 
for detecting whether or not each feature is found. 
'l?tm accuracy of JUMAN for detecting inorphologi- 
cal boundaries and part-of-speech tags is about 98%, 
and the parsecs dependency accuracy is about 90%. 
These results were obtained from analyzing Mainichi 
newspaper articles. 
We used 217,562 sentences for training. When 
these sel~t, ences were all extracted from a raw corlms , 
the agreement rate was 87.64% for "pair of modifiers" 
and was 75.77% for "Colnplete agreement." When 
the 217,562 training sentences were sentences fl'oln 
the tagged cortms (17,562 sentences) used in our for- 
real exl)eriment aInl froln a raw cortms, the agree- 
" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% and 
for "Complete agreement" was 75.88%. These rates 
were about 0.5% higher than those obtained when we 
used only sentences from a tagged corlms. Thus, we 
can acquire word order by adding inforlnation froln 
a rmv corpus even if we do not have a large tagged 
corpus. The results also indicate that the parser ac- 
curacy is not so significant for word order acquisition 
and that an accuracy of about 90% is sufficient. 
4 Conc lus ion 
This paper described a method of acquiring word or- 
der froln corpora. We defined word order as the order 
of lnodifiers which depend on tile same lnodifiee. The 
lnethod uses a model which estimates the likelihood 
of the apt)ropriate word order. The lnodel automat- 
ically discovers what the tendency of the word order 
in Japanese is by nsing various ldnds of information 
in and arouud the target bunsetsus plus syntactic 
and contextual inforlnation. The contribution rate 
of each piece of inforination in deciding word order 
is efficiently learned by a model implemented within 
an ),,I.E. framework. Comparing results of experi- 
ments controlling for each piece of information, we 
found that the type of inforinatiou having the great~ 
est influence was the case marker or inflection type in 
a bunsetsu. Analyzing the relationship between the 
amount of training data and the agreement rate, we 
fimnd that word order could be acquired even with 
a small set of training data. We also folmd that a 
raw cortms as well as a tagged cortms can be used to 
train the model, if it is first, analyzed by a parser. The 
agreement rate was 75.41% for the Kyoto University 
corpus. We analyzed the lnodifiees whose modifiers' 
word order did not agree with that in the original 
text, and folmd that 48% of theln were in a natural 
order. This shows that, in umny cases, word order 
in Japanese is relatively free and several orders are 
acceptable. 
The text we used were lmwspaper articles, which 
tend to have a standard word order, but we think 
that word orders tend to differ between ditferent 
styles of writing. We would therefore like to carry 
out experiments with other types of texts, such as 
novels, having styles different froln that of newspa- 
pers. 
it has been (lift\]cult o evaluate tile reslflts of text 
generation objectively becmlse there have been no 
good stmldards for ewlllmtion. By using the stan- 
(lard we describe in this paper, however, we can evN- 
uate results objectively, at least for word order esti- 
mation in text, generation. 
We expect hat our lnodel can be used for several 
applications as well as linguistic veritication, such as 
text; refinement silt)port and text generation in nla- 
chine translation. 
References  
Adam L. Better, Stephen A. I)ella Pietra, and Vincent J. Della 
Pietra. 199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn- 
gmtge Processing. Computational Linguistics, 22(11:39-71. 
Sadao Kurohashi and Makol.o Nagao. 1997. Kyoto University 
Text Corl)uS Project. In Proceedings of The Third Annual 
Mectin9 of The Association for Natural Language Process- 
ing, pages 115-118. (in Japanese). 
Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morpho- 
logical Analysis System JUMAN Version 3.6. l)epartment of 
Informatics, Kyoto University. 
Sadao Kurohashi, 11198. Japanese Dependency/Case Structure 
Analyzer KNP Version 2.0b6. Department of Inforln~tties, Ky- 
ore University. 
IIiroshl Maruyama. 1994. Experhnents on V~rord-Order Recovery 
Using N-Cram Models. In YTte \]~9th Annual (;onvention IPS 
Japan. (in Japanese). 
NLRl(National Language Research Institute). 1964. Word List 
by Semantic Pri~ciples. Syuei Syuppan. (in Japmlese). 
Tetsuo Saekl. 1998. Yousetsu nihongo no 9ojun (Survey: Word 
Order in Japanese). Kuroshio Syupl)an. (in Japanese). 
James Shaw and Vasileios Ilatzivassiloglou. 1999. Ordering 
Among l'remodifiers. In Proceedings of the 37th Annual Meet- 
ing of the Association for Computational Linguistics (,4 CL), 
pages 135-143. 
Takenobu Tokunaga and IIozumi Tanaka. 1991. On Estimat- 
ing Japanese Word Order B~used on Valency Information. 
Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5. 
(in Japanese). 
877 
A Statistical Approach to the Processing of Metonymy 
Masao Ut iyama,  Masak i  Murata ,  and H i tosh i  I sahara  
Communicat ions  Research L~boratory, MPT ,  
588-2, Iwaoka, Nishi-ku, Kobe, Hyogo 651-2492 Japa l  
{mut iyam~,murat~, isahara} ~crl.go.j  p
Abst ract  
This paper describes a statistical approach to 
tile interpretation of metonymy. A metonymy 
is received as an input, then its possible inter- 
p retations are ranked by al)t)lying ~ statistical 
measure. The method has been tested experi- 
mentally. It; correctly interpreted 53 out of 75 
metonymies in Jat)anese. 
1 I n t roduct ion  
Metonymy is a figure of st)eech in which tile 
name of one thing is substituted for that of 
something to which it is related. The czplicit 
tc.~m is 'the name of one thing' and the implicit 
t;c~"m is 'the name of something to which it; is 
related'. A typical examt)le of m(;tonymy is
He read Shal(esl)eare. (1) 
'Slmkesl)(~are' is substitut(~d for 'the works of 
Shakespeare'. 'Shakest)eare' is the explicit term 
and 'works' is the implicit term. 
Metonymy is pervasive in natural language. 
The correc~ treatment of lnetonylny is vital tbr 
natural language l)rocessing api)lications , es- 
1)ecially for machine translation (Kamei and 
Wakao, 19!)2; Fass, 1997). A metonymy may be 
aecel)table in a source language but unaccet)t- 
able in a target language. For example, a direct 
translation of 'he read Mao', which is acceptable 
in English an(1 Japanese, is comt)letely unac- 
ceptal)le in Chinese (Kamei and Wakao, 1992). 
In such cases, the machine trmlslation system 
has to interl)ret metonynfies to generate accept- 
able translations. 
Previous approaches to processing lnetonymy 
have used hand-constructed ontologies or se- 
mantic networks (.\]?ass, 1988; Iverson and Hehn- 
reich, 1992; B(maud et al, 1996; Fass, 1997). 1 
1As for metal)her l)rocessing, I 'errari (1996) used t;ex- 
Such al)t)roaches are restricted by the knowl- 
edge bases they use, and may only be applicable 
to domain-specific tasks because the construc- 
tion of large knowledge bases could be very d i f  
ficult. 
The method outlined in this I)apcr, on the 
other hand, uses cortms statistics to interpret 
metonymy, so that ~ variety of metonynfies 
can be handled without using hand-constructed 
knowledge bases. The method is quite t)romis- 
ing as shown by the exl)erimental results given 
in section 5. 
2 Recogn i t ion  and  In terpretat ion  
Two main steps, recognition and i'ntc.'q~vc- 
ration, are involved in the processing of 
metonyn~y (Fass, 1.!)97). in tile recognition st;el), 
metonylnic exl)ressions are labeled. 1111 the in- 
tel'l)r(:tation st;el) , the meanings of those ext)res- 
sions me int, eri)reted. 
Sentence (1), for examl)le, is first recognized 
as a metonymy an(t ~Shakespeare' is identified 
as the explicit term. 't'he interpretation 'works' 
is selected as an implicit term and 'Shakespeare' 
is replaced 1)y 'the works of Shakespeare'. 
A conq)rehensive survey by Fass (\]997) shows 
that the most COllllllOll metho(1 of recogniz- 
ing metonymies i by selection-restriction vio- 
lations. Whether or not statistical approaches 
can recognize metonymy as well as the selection- 
restriction violation method is an interesting 
question. Our concern here, however, is the 
interpretation of metonymy, so we leave that 
question for a future work. 
In interpretation, an implicit term (or terms) 
that is (are) related to the explicit term is (are) 
selected. The method described in this paper 
uses corpus st~tistics for interpretation. 
tual clues obtained through corl)us mmlysis tor detecting 
metal)lmrs. 
885 
This method, as applied to Japanese 
metonymies, receives a metonymy in a phrase 
of the tbnn 'Noun A Case-Marker R Predicate 
V' and returns a list of nouns ranked in or- 
der of the system's estimate of their suitability 
as interpretations of the metonylny, aSSulning 
that noun A is the explicit tenn. For exam- 
ple, given For'a  wo (accusative-case) kau (buy) 
(buy a Ford),  Vay .sya (ear), V .st .sdl  , 
r'uma (vehicle), etc. are returned, in that order. 
Tile method fbllows tile procedure outlined 
below to interpret a inetonymy. 
1. Given a metonymy in the form 'Noun A 
Case-Marker R Predicate V', nouns that 
can 1)e syntactically related to the explicit 
term A are extracted from a corpus. 
2. The extracted nouns are rmlked according 
to their appropriateness a interpretations 
of the metonymy by applying a statistical 
measure. 
The first step is discussed in section 3 and the 
second in section 4. 
3 In fo rmat ion  Source  
\?e use a large corpus to extract nouns which 
can be syntactically related to the exl)licit term 
of a metonylny. A large corpus is vahmble as a 
source of such nouns (Church and Hanks, 1990; 
Brown et al, 1992). 
We used Japanese noun phrases of the fornl 
A no B to extract nouns that were syntactically 
related to A. Nouns in such a syntactic relation 
are usually close semantic relatives of each other 
(Murata et al, 1999), and occur relatively infre- 
quently. We thus also used an A near B rela- 
tion, i.e. identifying tile other nouns within the 
target sentence, to extract nouns that may be 
more loosely related to A, trot occur more fre- 
quently. These two types of syntactic relation 
are treated differently by the statistical nleasure 
which we will discuss in section 4. 
The Japanese noun phrase A no B roughly 
corresponds to the English noun phrase B of A, 
lint it has a nmch broader ange of usage (Kuro- 
hashi and Sakai, 1999). In fact, d no B can ex- 
press most of the possible types of semmltic re- 
lation between two nouns including metonymic 
2~Ford' is spelled qtSdo' ill Japanese. We have used 
English when we spell Japanese loan-words from English 
for the sake of readability. 
concepts uch as that the name of a container 
can represent its contents and the name of an 
artist can imply an art~brnl (conta iner  for 
contents and artist for a r t fo rm below).a Ex- 
amples of these and similar types of metonymic 
concepts (Lakoff and Johnson, 1980; Fass, 1997) 
are given below. 
Container for contents  
? glass no mizu (water) 
? naV  (pot) , y6 i (food) 
Art ist  for artform 
? Beethoven o kyoku (music) 
? Picas.so no e (painting) 
Object  for user 
? ham .sandwich no kyaku (customer) 
? sax no .sO.sya (t)erformer) 
Whole  tbr part 
? kuruma (car) no tirc 
? door" no knob 
These exalnt)les uggest hat we can extract 
semantically related nouns by using tile A no B 
relation. 
4 Stat is t ica l  Measure  
A nletonymy 'Noun A Case-Marker R, Predi- 
cate V' can be regarded as a contraction of 
'Noun A Syntactic-Relation (2 Noun B Case- 
Marker R Predicate V', where A has relation 
Q to B (Yamamoto et al, 1998). For exam- 
ple, Shakc.spcare wo yomu (read) (read Shake- 
speare) is regarded as a contraction of Shake- 
speare no .sakuhin (works) 'wo yomu (read the 
works of Shakespeare), where A=Shake.spcare, 
Q=no, B=.sakuhin, R=wo,  and V=yomu. 
Given a metonymy in the fbrln A R 17, the 
appropriateness of noun B as an interpretation 
of the metonymy under the syntactic relation Q 
is defined by 
LQ(BIA,/~, V) - Pr(BIA, (2, 1~, V), (2) 
ayamamoto et al (\]998) also used A no /3 relation 
to interpret metonymy. 
886 
where Pr( . - . )  represents l)robal/ility and Q is 
either an A no B relation or an A near \]3 re- 
lation. Next;, the appropriateness of noun \]3 is 
defined by 
M(BIA, Ie, V) -nlaxLc~(BIA, l~,V ). (3) 
O 
We rank nouns 1)y at)plying the measure 214. 
Equation (2) can be decomposed as follows: 
LQ(!31A, R,, V) 
= Pr (B IA  , Q, R,, V)  
Pr(A, Q, B, R,, V) 
Pr( A, Q, R, v) 
Pr(A, Q, 13)lh'(R, VIA, Q, Ix) 
Pr(A, Q) Pr(R, VIA, Q) 
Pr(BIA , Q)Pr(R, VIB) 
-~ er(R, v) ' (4) 
where (A, O) and {\]~,, V} are assumed to l)e in- 
del)endent of each other. 
Let f(event)1)e the frequen(:y of an cve'nt and 
Classc.s(\])) be the set of semantic (:lasses to 
which B belongs. 'l'he expressions in Equation 
(4) are then detined t)y 4 
I'r(~lA, Q) - .t'(A, Q, ~x) _ f (A,  Q, ~) 
f (A ,  Q) ~1~ f (A ,  Q, 13)' 
(5) 
Pr(~., riB) 
IU~,I~,v) i' ' *: .1 (U, ~, V) > 0, 
.~- ~,c~cl .......... (10 Pr(l)'l(/)f(C/'R'V) 
J'US) 
otherwise, 
((0 
Pr (B IC  ) - .f(13)/ICI-s.w-.XB)l j ( c )  (r) 
We onfitted Pr(H,, 17) fi'om Equat ion (4) whell 
we calculated Equation (3) in the experiment 
de, scribed in section 5 for the sake of simplicit> 
4Strictly speaking, Equation (6) does not satist\]y 
X',,e,vpr(R, vl/x) -- 1. We h~wc adopted this det- 
inition for the sake of simplicity. This simplifi- 
cation has little effect on the tilml results because 
~--;c'cc~ ........ (m Pr(l~lC)f(C,I~', V) << I will usually 
hohl. More Sol)histieated methods (M;mning ml(t 
Schiitze, 1999) of smoothing f)robability distribution 
m~y I)e I)eneticial. itowever, al)l)lying such methods 
and comparing their effects on the interpretation of
metonymy is beyond the scope of this l)aper. 
This t reatment  does not alter the order of the 
nouns ranked by the syst;em because l?r(H., V) 
is a constant for a given metonymy of the form 
AR V. 
Equations (5) and (6) difl'er in their t reatment  
of zero frequency nouns. In Equat ion (5), a 
noun B such that  f (A ,  Q, B) = 0 will l)e ignored 
(assigned a zero probal)ility) because it is un- 
likely that  such a noml will have a close relation- 
shii / with noun A. In Equation (6), on the other 
hand, a noun B such that  f (B ,  R, V) = 0 is as- 
signed a non-zero probability. These treatments 
reflect the asymmetrical  proper~y of inetonymy, 
i.e. ill a nletonylny of the form A 1{ 1~ an 
implicit term 13 will have a much t ighter rela- 
tionship with the explicit term A than with the 
predicate V. Consequently, a nouil \]3 such that 
f (A ,Q ,  B) >> 0 A f (B ,  JR, V) = 0 may be ap- 
propri~te as an interpretation of the metonymy. 
Therefore, a non-zero t)robat)ility should be as- 
sign(;d to Pr(l~., VI1X ) ev~,n it' I (B ,  2e, V) ; (). ~ 
Equation (7) is the probabil ity that  noun J3 
occurs as a member of (::lass C. This is reduced to 
fU~) if13 is not ambiguous, i.e. IC/a,~,sc.,s,(/3)\[ = f(c) 
1. If it is ambiguous, then f (B )  is distr ibuted 
equally to all classes in Classes(B).  
The frequency of class C is ol)tained simi- 
larly: 
.f(B) (8) 
. f (c )  = ~ ICl(-~c..~(13)1' 11C-.(7 
where 13 is a noun which belongs to the class C. 
Finally we derive 
f(13, ~, v) 
BqC 
(.0) 
In summary,  we use the measure M as de- 
fined in Equat ion (3), and cah:ulated by apply- 
ing Equat ion (4) to Equation (9), to rank nouns 
according to their apl)ropriateness as possible 
interpretat ions of a metonymy. 
Example  Given the statistics below, bottle we 
akeru (open) (open a bottle) will be interpreted 
5The use of Equation (6) takes into account a noun/3 
such that J'(l:~, l{, V) = 0. But, Stlch & llOtlll is usually ig- 
nored if there is another noun B' such that f(13', H., V) > 
0 be~,~,,se. Eo'~ct ....... U~)P, USIO)J'(C,~e.,V) << a < 
J'(lY, H,, V) will usually hokl. This means thai the co- 
occurrence 1)rol)al)iliW between implicit terms and verbs 
are also important in eliminating inapl)rol)riate nomls. 
887 
as described in the fbllowing t)aragraphs, assum- 
ing that cap and rcizSko (refl'igerator) are the 
candidate implicit terms. 
Statistics: 
f(bottlc, no, cap) = 1, 
f(bottlc, no, reizgko) = O, 
f(bottlc, no) = 2, 
f ( bottlc, ncar, cap) = 1, 
f (bottle, near, rciz6ko) = 2, 
f(bottlc, ncar) = 503, 
f(cap) = 478, 
f(rcizSko) = 1521, 
f(cap, wo, akcru) = 8, and 
f(rciz6ko, wo, akcru) = 23. 
f(bottlc, no, rciz6ko) = 0 indicates that bottle 
and rcizSko are not close semantic relatives of 
each other. This shows the effectiveness of us- 
ing A no B relation to filter out loosely related 
words. 
Measure: 
L,o(cap) 
Lncar(Cap) = 
Lno(reizSko) = 
Lncar ( reizS ko ) -~ 
f ( bott:le, no, cap) 
.f ( bottlc, no) 
\](ca,p, wo, a\]~c'ru) 
X 
1 8 
-8 .37?10 -3 , 
2 478 
f (bottle, near, cap) 
f(bottlc, near) 
f ( caI), "wo, a\]~cru) 
X 
.f ( ) 
1 8 
50--3 47-8 = 3.33 ? 10 -5, 
.f ( bottlc, no, rcizSko )
.f ( bottlc, no) 
f ( rcizako, wo, ahcru ) 
? 
.f ( rcizdko 
0 23 
2 1521 
.f ( bottlc, near, rcizSko) 
f (bottlc, near) 
f(rcizSko, wo, akcru) 
X 
f ( rciz~ko ) 
2 23 
503 1521 
- 6.01 x 1() -~,  
M(c p) 
= max{Lno(cap),Lnea.,.(cap)} 
= 8.37 x lO-3, and 
~r ( reizSko )
= 6.01? 10 -5 , 
where L,,o(Cap) = L,~o(Caplbo~tle, wo, akeru), 
M(c p) = M(c pl ot tz , and so o51. 
Since M > M we conclude 
that cap is a more appropriate imt)licit term 
than rcizSho. This conclusion agrees with our 
intuition. 
5 Exper iment  
5 .1  Mater ia l  
Metonymies  Seventy-five lnetonymies were 
used in an ext)erilnent to test tile prol)osed 
lnethod. Sixty-two of them were collected from 
literature oll cognitive linguistics (Yamanashi, 
1988; Yamam~shi, 1995) and psycholinguistics 
(Kusumi, 1995) in Japanese, paying attention 
so that the types of metonymy were sufficiently 
diverse. The remaining 13 metonymies were 
direct translations of the English metonymies 
listed in (Kalnei and Wakao, 1992). These 13 
metonylnies are shown in Table 2, along with 
the results of the experiment. 
Corpus  A corpus which consists of seven 
years of issues of the Mainichi Newspaper (Dora 
1991 to 1997) was used in the experiment. The 
sentences in tlle cortms were mort)hologically 
analyzed by ChaSen version 2.0b6 (Matsumoto 
et al, 1999). The corpus consists of about 153 
million words. 
Semant ic  Class A Japanese thesaurus, Bun- 
rui Goi-tty6 (The N~tional Language Research 
Institute, 1996), was used in the experiment. It 
has a six-layered hierarchy of abstractions and 
contains more than 55,000 nouns. A class was 
defined as a set of nouns which are classified in 
the same abstractions in the top three layers. 
The total nmnber of classes thus obtained was 
43. If a noun was not listed in the thesaurus, it 
was regarded as being in a class of its own. 
888 
5.2 Method 
'.1.11(; method we have dcseril)e,d was applied I;O 
the metonynfie, s (lescril)e,(t ill section 5.1. Tile 
1)r()eedure described 1)clew was followed in in- 
tert)rel;ing a metonynly. 
1. Given a mel,onymy of the, form :Noun A 
Case-Marker R Predicate, V', nouns re- 
\]al;e(l to A 1)y A 'n,o .1:1 relation an(l/or A 
near H relation were extra(:ix'~(l from 1;he, 
corl)us described in Se(:tion 5.\]. 
2. The exl;racted llOllllS @an(lidatcs) were 
ranked acc()rding t() the nw, asure M d(;tined 
in \]{quation (3). 
5.3 Resu l ts  
The r(;sult of at)l)lying the proi)osexl me, thod to 
our sol; of metol~ymies i  summarized in 'l'alfle 
1. A reasonably good result (:an 1)e s(;cn for 
q)oi;h r(,\]ai;ions', i.e. l;he result ot)i;aincd \])y us- 
ing both A no 11 an(t d ncm" 1\] l'elal;ion~; wllen 
extracting nouus fl'onl th(' cOllmS, \[1'1~(', a(:(:u- 
ra(:y of q)ol;h re, l~tions', the ratio ()f lhe nllnil)er 
of (:orrc(:l;ly intcrl)r(;te,(1 ; t()l)-rank(;(l (:an(li(lates 
to l;he, total mmfl)er of m(',l;()nymies in ()it\]' set, 
w,,s 0.7:, (=5' ,V isa+22))  alld ('ol,ti(t(' l,ce 
inWwva.1 estimal;e was t)(;l;ween ().6\] an(t 0.8\].. 
\?e regard this result as quite t)ronfising. 
Since the mc, i;onymies we used wcr(; g(m(u'a\]: 
(lomain-in(lel)(',ndca~t, on(s, l;h(~ (legr(', ~, ()f a(:cu- 
racy achi(;ve, l in this (~xp(;rim(;nt i~; likely t() t)(; 
r(',t)(',al;e(l when our me?hod is ~q)l)lie(l t() oth(;r 
genural sets ()f mel;onymies. 
'.\['~l)l(; l : tt3xl)erimental r('sults. 
I{,elal;ions used Corre(;t \?'rong 
Both relations 53 22 
Only A 'no B 50 25 
Only A near  13 d3 32 
Tal)le 1 also shows that  'both relations' is 
more ae(:ural;e than (',il;her the result obtained 
1)y solely using the A no \]3 relation or the A 
near  B relation. The use of multit)le relations 
in mel, onyn~y int(;rl)retation is I;hus seen to l)e 
1)enefieial. 
aThe correct;hess was judged by the authors. A candi- 
dat(; was judged correct when it; made sense in .Ial)anese. 
For examl)le, we rcgard(;d bet:r, cola, all(l mizu (W;d;el') 
as all (:orr(!c\[; intcrl)r(~l;ations R)r glas.s we nom, u (drink) 
(drink a glass) because lhey llla(le ,q(~llSC in some (:ontcxt. 
Table 2 shows the, results of applying the 
method to the, thirteen directly translated 
metonymies dcscril)ed in sect;ion 5.1.. Aster- 
isks (*) in the tirst (;ohlillll indicate that  direct 
translation of the sentences result in unaccel)t- 
able Japanes(;. The, C's and W's in t;he sec- 
ond eohmm respectively indicate that  the top- 
ranked ('andi(latcs were correct and wrong. The 
s(;nten(:es in the l;hir(t column are the original 
English metonymi(;s adol)tc, d fl'om (Kamci and 
\?akao, t992). The Japanese llletollylllies in 
th(: form h loun  ease-lnarker predi(:ate 7', in the 
fourth column, are the illputs I;o the method. 
In this ('ohunn, we and  9 a mainly r(;present 
I;he ac(:usal;ive-casc and nominative-ease, re- 
Sl)ectively. The nouns listed in the last eolmnn 
m'e the tot) three candidates, in order, according 
to the. measure M that was defined ill Equation 
(3). 
Th(,,se, l'csull;s ( lemonstrate the et\[~(:tiveness of 
lhe m(',thod. '.l>n out of t;11(: 13 m(;tonynfies 
w(u'c intc, rt)rete,(l (:orre, ctly. Moreover, if we 
rcsl;ri(:t our al;l;(',nti()n to the ten nietonylHics 
i}mt m'e a(:(:Cl)tal)le, ill ,/al)anese, all l)ut one 
w(;rc, inl;('rl)r(;te(t (:orrectly. The a(:curacy was 
0.9 ---- (/)/\]0), higher than that  for q)oth rela- 
tions' in Tal)le i. The reason fi)r the higher de- 
gl'ee of ac(:tlra(;y is l;\]lal; the lll(;|;Ollyllli(;s in Tal)le 
2 arc semi,what yi)ical and relativ(;ly easy to 
int(~rl)rel; , while, the lnel;(nlynlics (:olle(:l;c(t fl'()m 
,lal)anese sour(:es included a (liversity of l;yl)es 
and wcr(~ more difficult to intext)let. 
Finally, 1;11(', efl'ecl;iv(umss of using scnlanl;i(: 
classes is discussed. The, l;op candidates ot! six 
out of the 75 metonynfies were assigned their 
al)prot)riatenc, ss by using their semantic lasses, 
i.e. the wducs of 1;11o measure 114 was calculated 
with f (H , /~ ,  V) = 0 in lgquat;ion (6). Of the, se, 
l;hrce were corrccl,. 011 l;hc, other hand, if sc- 
manl;ic class is not use(l, then three of the six 
are still COITeC|;. Here there was no lint)rove- 
merit. However, when we surveyed the results 
of the whole experiment, wc found that  nouns 
for wlfich .f iB, R,, V) -- 0 often lind (:lose re- 
lationship with exl)licit terms ill m(;tonynfics 
and were al)propriate as interpretat ions of the 
metonynfics. We need more research betbre we 
(:an ju(lgc the etl'ectivc, ness of utilizing semantic 
classes. 
rPl'edicatcs are lemmatized. 
889 
Table 2: Results of applying the proposed lnethod to direct translat ions of the metonymies in
(Kanmi and Wakao, 1992). 
Sentences Noun Case-Mm'l~er Pred. Candidates 
C Dave drank the glasses. 
C The .kettle is boiling. 
C Ile bought a Ford. 
C lie has got a Pieasso in his room. 
C Atom read Stcinbeck. 
C 
C 
W 
C 
W 
C 
Ted played J3ach. 
Ite read Mao. 
We need a couple of strong bodies 
tbr our team. 
There a r___q a lot of good heads in the 
university. 
Exxon has raised its price again. 
glass we nomu 
yakan ga waku 
Ford we kau 
Picasso we motu 
Stcinbcck we yomu 
Bach we hiku 
Mao we yomu 
karada ga hituy5 
atama ga iru 
Exxon 9 a agcru 
Washington is insensitive to the 
needs of the people. 
Washington ga musinkci 
C The T.V. said it was very crowded 
at; the festival. 
W The sign said fishing was prohibited 
here .  
T. V. 9a in 
hy&siki ga iu 
beer, cola, mizu (water) 
yu (hot water), 
oyu (hot water), 
nett5 (boiling water) 
zy@Ssya (car), best seller, 
kuruma (vehicle) 
c (painting), image, aizin (love,') 
gensaku (original work), 
mcisaku (fmnous tory), 
daihySsaku (important work) 
mcnuetto (minuet), kyoku (music), 
piano 
si (poem), tyosyo (writings), 
tyosaku (writings) 
carc, ky~tsoku (rest;), 
kaigo (nursing) 
hire (person),tomodati (friend), 
bySnin (sick person) 
Nihon ( Japan) ,ziko (accident), 
kigy5 (company) 
zikanho (assistant vice-minister), 
scikai (political world), 
9ikai (Congress) 
cotn l l lentgto l '~ anl lOl l l lcer  I (:~stel" 
mawari (surrmmding), 
zugara (design) 
.seibi (lnaintclmnce) 
6 Discuss ion  
Semant ic  Re la t ion  The method proposed in 
this pnper identifies implicit terms fbr tile ex- 
plicit term in a metonymy. However, it is not 
concerned with the semantic relation between 
an explicit; term and implicit term, because such 
semantic relations are not directly expressed ill 
corpora, i.e. noun phrases of the form A no 
B can be found in corpora bul; their senmntic 
relations are not. If we need such semantic re- 
lations, we must semantical ly analyze the noun 
phrases (Kurohashi and Sakai, 1999). 
App l i cab i l i ty  to  o ther  languages  Japan- 
ese noun phrases of the form A no B are specitie 
to Japanese. The proposed method, however, 
could easily be extended to other languages. For 
exmnple, in English, noun phrases B of d could 
be used to extract semantical ly related nouns. 
Nouns related by is-a relations or par t -o f  re- 
lations could also be extracted from corpora 
(Hearst, 1992; Berland and Charniak, 1999). If 
such semantical ly related nouns are extracted, 
then they can be ranked according to the mea- 
sure M defined in Equat ion (3). 
Lex ica l ly  based  approaches  Generative 
Lexicon theory (Pustejovsky, 1995) proposed 
the qualia structure which encodes emantic re- 
lations among words explicitly. It is useflfl to 
infer an implicit term of the explicit term in 
a metonymy. The proposed approach, on the 
other hand, uses corpora to infer implicit terms 
and thus sidesteps the construction of qualia 
structure. 8 
7 Conc lus ion  
This paper discussed a statistical approach to 
the interpretat ion of metonymy. The method 
tbllows the procedure described below to inter- 
pret a metonymy in Japanese: 
1. Given a metonymy of the tbrm 'Noun A 
SBriscoe t al. (1990) discusses the use o1" machine- 
readable dictionaries and corpora for acquMng lexical 
semantic information. 
890 
Case-Marker 1{ Predicate V', nouns that 
are syntactically related to the explicit 
terlll A are extracted front a corpus. 
'.2. The extracted nouns are ranked according 
to their degree of appropriateness as inter- 
pretations of the metonymy by applying a 
statistical measure. 
The method has been tested experimentally. 
Fifty-three out of seventy-five metonymies were 
correctly interpreted. This is quite a prolnis- 
ing first; step towm'd the statistical processing 
of metonymy. 
References  
Matthew Berland and Eugene Charniak. 1999. 
Finding parts in very large corpora. In A (7L- 
99, pages 57- 64. 
Jacques Bouaud, Bruno Bachimont, and Pierre 
Zwcigenbaum. 1996. Processing nletonyllly: 
a domain-model heuristic graph travcrsal 3t> 
preach. In COLINC-95, pages 137-142. 
Ted Briscoc, Ann Copestake, and Bran Bogu- 
racy. 1990. Enjoy the paper: L(;xi(:al seman- 
tics via lexicology. In COLING-90, pages 4:2-- 
4:7. 
I)(fi;cr F. l~rown, gincenl; ,l. Delia Pietra, Pe- 
ter V. deSouza, ,\]enifer C. \]~ai, m~d l/.ol)(',rl; I,. 
Mercer. 1992. Class-1)ased n-gram models of 
m~l;ur~l lmlguage. C~o'm,p'u, tat  ioruzl Li'n, guistics, 
1.8(4) :467 479. 
Kelmeth Ward Church and Patrick Hanks. 
1990. Word association orms, mutual in- 
formation, and lexicography. Uomputatio'n, al 
Lin.quistics, 16(1):22 29. 
Dan Fass. 1988. Metonymy and lnel;al)hor: 
What's the difference? In COLING-88, 
pages \]77-181. 
Dan Fass. 1997. Processin9 Mctonymy and 
Me.taph, or, volume 1 of Cont, cm.porar'y Studies 
in Cognitive Science and '\]'cch, nology. Ablcx 
Publishing Corporation. 
Steplmne Fcrrari. 1996. Using textual clues 
to improve metaphor processing. In ACL-95, 
pages 351-354. 
Marl;i A. Hearst. 1992. Automatic acquisition 
of hyponyms fi:om large text corpora. In 
COLING-92, pages 539 545. 
Eric iverson mid Stephen Helmreich. 1992. 
Metallel: An integrated approach to non- 
literal phrase interpretation. Computational 
Intelligence, 8(3):477 493. 
Shin-ichiro I(amei and Takahiro Wakao. 1992. 
Metonymy: Itcassessment, survey of accept- 
ability, and its treatment in a machine trans- 
lation system. In ACL-92, pages 309-311. 
Sadao Kurohashi and Yasuyuki Sakai. 1999. 
Semantic mmlysis of ,Japmmse noun phrases: 
A new approach to dictionary-lmsed under- 
standing. In ACL-99, pages 481 488. 
Takashi Kusumi. 1995. ttiyu-no S'yori-Katci- 
t;o lmi-Kdzfi (Pr'occssin 9 and Semantic Struc- 
ture of "\]'ropes). Kazama Pul)lisher. (in 
Jalmnese). 
George Lakoff and Mm'k Johnson. 1980. 
Meta, phors lye Live By. Chicago University 
Press. 
Christopher D. Mmming and Hinrich Schiitze, 
1999. Fou'ndations of Statistical Nat.ur(d Lan- 
guage \])recessing, chapter 6. The MIT Press. 
Yuji Matsmnoto, Akira Kitauchi, Tatsuo 
Yamashita, and Yoshitalm Hirano. 1999. 
Japanese morphological anMysis system 
ChaScn mmmal. Nara Institute of Science 
and Technology. 
Masaki Murata, Hitoshi Isalmra, and Makoto 
Nagao. 1999. IX.csolut, ion of indirect anal)hera 
in Jal)anese s(;ntcn('es using examples "X no 
Y (X of Y)". In A 6%'99 Work.shop orl, Core/" 
e.'l'(:,ncc and It.s AppIica, tio'ns, 1)ages 31 38. 
,lames l'ustejovsky. 1995. 2Yt, c Generative Lex- 
icon. 'J?he MI'I' Press. 
Tim National Language I/.ese~rch lalstitute. 
1996. Bv, nr',ui Goi-hyO Z~h,o-bav,(Th:l;o'nom, y 
of ,lapo, nc.s'e., e'nla'ulcd cditio@. (in ,Japancse). 
Atsmnu Yammnoto. Masaki Murata, and 
Makoto Nagao. 1998. Example-based 
metonymy interpretation. In \])'roe. of the 
~t,h. Annual \]lgcel;in 9 of th, c Association for 
Natural Language Prwccssing, pages 606 609. 
(in Japanese). 
Masa-aki Yamanashi. 1988. Hiyu-to \]~ikai 
('1;ropes and Understanding). Tokyo Univer- 
sity Publisher. (in Jalmnese ).
Masa-aki Yamalmshi. 1995. Ninti Bunpa-ron 
(Cognitive Linguistics). Hitsuji Publisher. 
(ill Japanese). 
891 
 
	ffText Generation from Keywords
Kiyotaka Uchimoto? Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We describe a method for generating sentences
from ?keywords? or ?headwords?. This method
consists of two main parts, candidate-text con-
struction and evaluation. The construction part
generates text sentences in the form of depen-
dency trees by using complementary informa-
tion to replace information that is missing be-
cause of a ?knowledge gap? and other missing
function words to generate natural text sen-
tences based on a particular monolingual cor-
pus. The evaluation part consists of a model
for generating an appropriate text when given
keywords. This model considers not only word
n-gram information, but also dependency infor-
mation between words. Furthermore, it consid-
ers both string information and morphological
information.
1 Introduction
Text generation is an important technique used
for applications like machine translation, sum-
marization, and human/computer dialogue. In
recent years, many corpora have become avail-
able, and have been used to generate natural
surface sentences. For example, corpora have
been used to generate sentences for language
model estimation in statistical machine trans-
lation. In such translation, given a source lan-
guage text, S, the translated text, T , in the
target language that maximizes the probabil-
ity P (T |S) is selected as the most appropri-
ate translation, T
best
, which is represented as
(Brown et al, 1990)
Tbest = argmaxTP (T |S)
= argmaxT (P (S|T ) ? P (T )) . (1)
In this equation, P (S|T ) represents the model
used to replace words or phrases in a source lan-
guage with those in the target language. It is
called a translation model. P (T ) represents a
language model that is used to reorder trans-
lated words or phrases into a natural order in
the target language. The input of the language
model is a ?bag of words,? and the goal of the
model is basically to reorder the words. At this
point, there is an assumption that natural sen-
tences can be generated by merely reordering
the words given by a translation model. To give
such a complete set of words, however, a trans-
lation model needs a large number of bilingual
corpora. If we could automatically complement
the words needed to generate natural sentences,
we would not have to collect the large number
of bilingual corpora required by a translation
model. In this paper, we assume that the role of
the translation model is not to give a complete
set of words that can be used to generate nat-
ural sentences, but to give a set of headwords
or center words that a speaker might want to
express, and describe a model that can provide
the complementary information needed to gen-
erate natural sentences by using a target lan-
guage corpus when given a set of headwords.
If we denote a set of headwords in a target
language as K, we can express Eq. (1) as
P (T |S) = P (K|S) ? P (T |K). (2)
P (K|S) in this equation represents a model
that gives a set of headwords in the target lan-
guage when given a source-language text sen-
tence. P (T |K) represents a model that gener-
ates text sentence T when given a set of head-
words, K. We call the model represented by
P (T |K) a text-generation model. In this paper,
we describe a text-generation model and a gen-
eration system that uses the model. Given a set
of headwords or keywords, our system outputs
the text sentence that maximizes P (T |K) as an
appropriate text sentence, T
best
:
Tbest = argmaxTP (T |K)
= argmaxT (P (K|T )? P (T )) . (3)
In this equation, we call the model represented
by P (K|T ) a keyword-production model. This
equation is equal to Eq. (1) when a source-
text sentence is replaced with a set of key-
words. Therefore, this model can be regarded
as a model that translates keywords into text
sentences. The model represented by P (T ) in
Eq. (3) is a language model used in statistical
machine translation. The n-gram model is the
most popular one used as a language model.
We assume that there is one extremely proba-
ble ordered set of morphemes and dependencies
between words that produce keywords, and we
express P (K|T ) as
P (K|T ) ? P (K,M,D|T )
= P (K|M,D, T ) ? P (D|M,T )? P (M |T ). (4)
In this equation, M denotes an ordered set of
morphemes and D denotes an ordered set of de-
pendencies in a sentence. P (K|M,D,T ) rep-
resents a keyword-production model. To es-
timate the models represented by P (D|M,T )
and P (M |T ), we use a dependency model and
a morpheme model, respectively, for the depen-
dency analysis and morphological analysis.
Statistical machine translation and example-
based machine translation require numerous
high-quality bilingual corpora. Interlingual ma-
chine translation and transfer-based machine
translation require a parser with high precision.
Therefore, these approaches to translation are
not practical if we do not have enough bilingual
corpora or a good parser. This is especially so if
the source text-sentences are incomplete or have
errors like those often found in OCR and speech-
recognition output. In these cases, however, if
we translate headwords into words in the target
language and generate sentences from the trans-
lated words by using our method, we should be
able to generate natural sentences from which
we can grasp the meaning of the source-text sen-
tences.
The text-generation model represented by
P (T |K) in Eq. (2) can be applied to various
tasks besides machine translation.
? Sentence-generation support system
for people with aphasia: About 300,000
people are reported to suffer from aphasia
in Japan, and 40% of them can select only
a few words to describe a picture. If candi-
date sentences can be generated from these
few words, it would help these people com-
municate with their families and friends.
? Support system for second language
writing: Beginners writing in second lan-
guage usually fined it easy to produce cen-
ter words or headwords, but often have dif-
ficulty generating complete sentences. If
several possible sentences could be gener-
ated from those words, it would help begin-
ners communicate with foreigners or study
second-language writing.
These are just two examples. We believe that
there are many other possible applications.
2 Overview of the Text-Generation
System
In this section, we give an overview of our sys-
tem for generating text sentences from given
keywords. As shown in Fig. 1, this system con-
sists of three parts: generation-rule acquisition,
candidate-text sentence construction, and eval-
uation.
Figure 1: Overview of the text-generation sys-
tem.
Given keywords, text sentences are generated
as follows.
1. During generation-rule acquisition, genera-
tion rules for each keyword are automati-
cally acquired.
2. Candidate-text sentences are constructed
during candidate-text construction by ap-
plying the rules acquired in the first
step. Each candidate-text sentence is rep-
resented by a graph or dependency tree.
3. Candidate-text sentences are ranked ac-
cording to their scores assigned during eval-
uation. The scores are calculated as a
probability estimated by using a keyword-
production model and a language model
that are trained with a corpus.
4. The candidate-text sentence that maxi-
mizes the score or the candidate-text sen-
tences whose scores are over a threshold
are selected as output. The system can
also output candidate-text sentences that
are ranked within the top N sentences.
In this paper, we assume that the target lan-
guage is Japanese. We define a keyword as the
headword of a bunsetsu. A bunsetsu is a phrasal
unit that usually consists of several content and
function words. We define the headword of a
bunsetsu as the rightmost content word in the
bunsetsu, and we define a content word as a
word whose part-of-speech is a verb, adjective,
noun, demonstrative, adverb, conjunction, at-
tribute, interjection, or undefined word. We
define the other words as function words. We
define formal nouns and auxiliary verbs ?SURU
(do)? and ?NARU (become)? as function words,
except when there are no other content words
in the same bunsetsu. Part-of-speech categories
follow those in the Kyoto University text corpus
(Version 3.0) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper.
Figure 2: Example of text generated from key-
words.
For example, given the set of keywords
?kanojo (she),? ?ie (house),? and ?iku (go),? as
shown in Fig. 2, our system retrieves sentences
including each word, and extracts each bunsetsu
that includes each word as a headword of the
bunsetsu. If there is no tagged corpus such
as the Kyoto University text corpus, each bun-
setsu can be extracted by using a morphological-
analysis system and a dependency-analysis sys-
tem such as JUMAN (Kurohashi and Nagao,
1999) and KNP (Kurohashi, 1998). Our system
then acquires generation rules as follows.
? ?kanojo (she)?kanojo (she) no (of)?
? ?kanojo (she)?kanojo (she) ga?
? ?ie (house)?ie (house) ni (to)?
? ?iku (go)?iku (go)?
? ?iku (go)?itta (went)?
The system next generates candidate bunsetsus
for each keyword and candidate-text sentences
in the form of dependency trees, such as ?Can-
didate 1? and ?Candidate 2? in Fig. 2, with
the assumption that there are dependencies be-
tween keywords. Finally, the candidate-text
sentences are ranked by their scores, calculated
by a text-generation model, and transformed
into surface sentences.
In this paper, we focus on the keyword-
production model represented by Eq. (4) and
assume that our system outputs sentences in the
form of dependency trees.
3 Candidate-Text Construction
We automatically acquire generation rules from
a monolingual target corpus at the time of gen-
erating candidate-text sentences. Generation
rules are restricted to those that generate bun-
setsus, and the generated bunsetsus must in-
clude each input keyword as a headword in the
bunsetsu. We then generate candidate-text sen-
tences in the form of dependency trees by simply
combining the bunsetsus generated by the rules.
The simple combination of generated bunsetsus
may produce semantically or grammatically in-
appropriate candidate-text sentences, but our
goal in this work was to generate a variety of
text sentences rather than a few fixed expres-
sions with high precision 1.
3.1 Generation-Rule Acquisition
Let us denote a set of keywords as KS and a
set of rules, each of which generates a bunsetsu
when given keyword k(?KS), as R
k
. We then
restrict r
k
(?R
k
) to those represented as
k ? hkm?. (5)
In this rule, h
k
represents the head morpheme
whose word is equal to keyword k; m? repre-
sents zero, one, or a series of morphemes that
are connected to h
k
in the same bunsetsu. Here,
we define a morpheme as consisting of a word
and its morphological information or grammat-
ical attribute, such as part-of-speech, and we
define a head morpheme as consisting of a head-
word and its grammatical attribute. By apply-
ing these rules, we generate bunsetsus from in-
put keywords.
3.2 Construction of Dependency Trees
Given keywords K = k1k2 . . . kn, candidate bun-
setsus are generated by applying the generation
rules described in Section 3.1. Next, by as-
suming dependency relationships between the
bunsetsus, candidate dependency trees are con-
structed. Dependencies between the bunsetsus
are restricted in that they must have the follow-
ing characteristics of Japanese dependencies:
1Note that 83.33% (3,973/4,768) of the headwords in
the newspaper articles appearing on January 17, 1995
were found in those appearing from January 1st to 16th.
However, only 21.82% (2,295/10,517) of the headword
dependencies in the newspaper articles appearing on
January 17th were found in those appearing from Jan-
uary 1st to 16th.
(i) Dependencies are directed from left to
right.
(ii) Dependencies do not cross.
(iii) All bunsetsus except the rightmost one de-
pend on only one other bunsetsu.
For example, when three keywords are given
and candidate bunsetsus including each keyword
are generated as b1, b2, and b3, the candidate de-
pendency trees are (b1 (b2 b3)) and ((b1 b2) b3)
if we do not reorder keywords, but 16 trees re-
sult if we consider the order of keywords to be
arbitrary.
4 Text-Generation Model
We next describe the model represented by Eq.
(4); that is, a keyword-production model, a
morpheme model that estimates how likely a
string is to be a morpheme, and a dependency
model. The goal of this model is to select
optimal sets of morphemes and dependencies
that can generate natural sentences. We imple-
mented these models within an maximum en-
tropy framework (Berger et al, 1996; Ristad,
1997; Ristad, 1998).
4.1 Keyword-Production Models
This section describes five keyword-production
models which are represented by P (K|M,D,T )
in Eq. (4). In these models, we define the set of
headwords whose frequency in the corpus is over
a certain threshold as a set of keywords, KS,
and we restrict the bunsetsus to those generated
by the generation rules represented in form (5).
We assume that all keywords are independent
and that k
i
corresponds to word w
j
(1 ? j ? m)
when text is given as a series of words w1 . . . wm.
1. trigram model
We assume that k
i
depends only on the two
anterior words w
j?1 and wj?2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj?1, wj?2).(6)
2. posterior trigram model
We assume that k
i
depends only on the two
posterior words w
j+1 and wj+2.
P (K|M,D, T ) =
n
?
i=1
P (ki|wj+1, wj+2).(7)
3. dependency bigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu
including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|wl, wl?1). (8)
Figure 3: Relationship between keywords and
words in bunsetsus.
4. posterior dependency bigram model
We assume that k
i
depends only on the
headword, w
s
, and the word on its right,
w
s+1, in the bunsetsu that is modified by
the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (ki|ws, ws+1). (9)
5. dependency trigram model
We assume that k
i
depends only on the two
rightmost words w
l
and w
l?1 in the right-
most bunsetsu that modifies the bunsetsu,
and on the two rightmost words w
h
and
w
h?1 in the leftmost bunsetsu that modi-
fies the bunsetsu including k
i
(see Fig. 3).
P (K|M,D, T ) =
n
?
i=1
P (k
i
|w
l
, w
l?1
, w
h
, w
h?1
). (10)
4.2 Morpheme Model
Let us assume that there are l grammatical
attributes assigned to morphemes. We call a
model that estimates the likelihood that a given
string is a morpheme and has the grammatical
attribute j(1 ? j ? l) a morpheme model.
Let us also assume that morphemes in the or-
dered set of morphemes M depend on the pre-
ceding morphemes. We can then represent the
probability of M , given text T ; namely, P (M |T )
in Eq. (4):
P (M |T ) =
n
?
i=1
P (mi|m
i?1
1
, T ), (11)
where m
i
can be one of the grammatical at-
tributes assigned to each morpheme.
4.3 Dependency Model
Let us assume that dependencies d
i
(1 ? i ? n)
in the ordered set of dependencies D are inde-
pendent. We can then represent P (D|M,T ) in
Eq. (4) as
P (D|M,T ) =
n
?
i=1
P (di|M,T ). (12)
5 Evaluation
To evaluate our system we made 30 sets of
keywords, with three keywords in each set, as
shown in Table 1. A human subject selected
the sets from headwords that were found ten
Table 1: Input keywords and examples of sys-
tem output.
Input (Keywords) Ex. of system output
??? ?? ?? (???? (??? ????))
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ???) ??)
??? ??? ?? (???? (??? ???))
?? ?? ???
?? ?? ??? ((??? ???) ???)
??? ?? ?? ((???? ???) ??)
??? ?? ?? (???? (??? ?????))
?? ?? ??
? ?? ?? ((?? ???) ????)
?? ?? ??? ((??? ???) ???)
?? ?? ?? ((??? ?????) ??)
?? ?? ?? (??? (??? ????))
?? ??? ?? ((??? ????) ??)
?? ?? ??? (??? (??? ????))
?? ??? ?? (??? (???? ?????))
?? ?? ??
?? ?? ?? ((??? ???) ??????)
??? ?? ? ((???? ??????) ?)
?? ?? ??? ((??? ???) ??????)
?? ?? ???? ((??? ???) ??????)
?? ?? ??? ((??? ???) ???)
?? ?? ??
??? ?? ?? ((???? ????) ??????)
?? ??? ?? ((??? ????) ????)
?? ?? ??? (??? (??? ???))
?? ??? ???? (??? (???? ???????))
?? ?? ??? ((??? ???) ???)
?? ??? ???
?? ?? ???
times or more in the newspaper articles on Jan-
uary 1st in the Kyoto University text corpus
(Version 3.0) without looking at the articles.
We evaluated each model by the percentage
of outputs that were subjectively judged as ap-
propriate by one of the authors. We used two
evaluation standards.
? Standard 1: If the dependency tree ranked
first is semantically and grammatically ap-
propriate, it is judged as appropriate.
? Standard 2: If there is at least one depen-
dency tree that is ranked within the top
ten and is semantically and grammatically
appropriate, it is judged as appropriate.
We used headwords that were found five times
or more in the newspaper articles appearing
from January 1st to 16th in the Kyoto Univer-
sity text corpus and also found in those appear-
ing on January 1st as the set of headwords, KS.
For headwords that were not in KS, we added
their major part-of-speech categories to the set.
We trained our keyword-production models by
using 1,129 sentences (containing 10,201 head-
words) from newspaper articles appearing on
January 1st. We used a morpheme model and a
dependency model identical to those proposed
by Uchimoto et al (Uchimoto et al, 2001; Uchi-
moto et al, 1999; Uchimoto et al, 2000b). To
train the models, we used 8,835 sentences from
newspaper articles appearing from January 1st
to 9th in 1995. Generation rules were acquired
from newspaper articles appearing from Jan-
uary 1st to 16th. The total number of sentences
was 18,435.
First, we evaluated the outputs generated
when the rightmost two keywords, such as ??
? and??,? on each line of Table 1 were input.
Table 2 shows the results. KM1 through KM5
stand for the five keyword-production models
described in Section 4.1, and MM and DM stand
for the morpheme and the dependency models,
respectively. The symbol + indicates a combi-
nation of models. In the models without MM,
DM, or both, P (M |T ) and P (D|M,T ) were as-
sumed to be 1. We carried out additional ex-
periments with models that considered both the
anterior and posterior words, such as the com-
bination of KM1 and KM2 or KM3 and KM4.
The results were at most 16/30 by standard 1
and 24/30 by standard 1.
Table 2: Results of subjective evaluation.
Model Standard 1 Standard 2
KM1 (trigram) 13/30 28/30
KM1 + MM 21/30 28/30
KM1 + DM 12/30 28/30
KM1 + MM + DM 26/30 28/30
KM2 (posterior trigram) 6/30 15/30
KM2 + MM 8/30 20/30
KM2 + DM 10/30 20/30
KM2 + MM + DM 9/30 25/30
KM3 (dependency bigram) 13/30 29/30
KM3 + MM 26/30 29/30
KM3 + DM 14/30 28/30
KM3 + MM + DM 27/30 29/30
KM4 (posterior dependency bigram) 10/30 18/30
KM4 + MM 9/30 26/30
KM4 + DM 9/30 22/30
KM4 + MM + DM 13/30 27/30
KM5 (dependency trigram) 12/30 26/30
KM5 + MM 17/30 28/30
KM5 + DM 12/30 27/30
KM5 + MM + DM 26/30 28/30
The models KM1+MM+DM,
KM3+MM+DM, and KM5+MM+DM
achieved the best results, as shown in Ta-
ble 2. For models KM1, KM3, and KM5, the
results with MM and DM were significantly
better than those without MM and DM in
the evaluation by standard 1. We believe this
was because cases are more tightly connected
with verbs than with nouns, so models KM1,
KM3, and KM5, which learn the connection
between cases and verbs, can better rank the
candidate-text sentences that have a natural
connection between cases and verbs than other
candidates.
Next, we conducted experiments using the
30 sets of keywords shown in Table 1 as in-
puts. We used two keyword-production mod-
els: model KM3+MM+DM, which achieved
the best results in the first experiment, and
model KM5+MM+DM, which considers the
richest information. We assumed that the in-
put keyword order was appropriate and did not
reorder the keywords. The results for both
models were the same: 19/30 in the evalu-
ation by standard 1 and 24/30 in the eval-
uation by standard 2. The right column of
Table 1 shows examples of the system out-
put. For example, for the input ??? (syourai,
in the future), ??? (shin-shin-tou, the New
Frontier Party), and ???? (umareru, to
be born)?, the dependency tree ?(???
[syourai wa] (???? [shin-shin-tou ga] ?
?????? [umareru darou]))? (?The New
Frontier Party will be born in the future.?)
was generated. This output was automati-
cally complemented by the appropriate modal-
ity ????? (darou, will), which agrees with
the word ???? (syourai, in the future), as
well as by post-positional particles such as ?
?? (wa, case marker) and ??? (ga). For
the input ???? (gaikoku-jin, a foreigner), ?
? (kanyuu, to join), and ?? (zouka, to in-
crease)?, the dependency tree ?(( ????
[gaikokujin no] ???? [kanyuu sya ga]) ?
????? [zouka shite iru] )? (?Foreigner
members are increasing in number.?) was
generated. This output was complemented
not only by the modality expression ???
??? (shite iru, the progressive form) and
post-positional particles such as ??? (no, of)
and ??? (ga), but also by the suffix ???
(sya, person), and a compound noun ?????
(kanyuu sya, member) was generated naturally.
In six cases, though, we did not obtain appro-
priate outputs because the candidate-text sen-
tences were not appropriately ranked. Improv-
ing the back-off ability of the model by using
classified words or synonyms as features should
enable us to rank sentences more appropriately.
6 Related Work
Many statistical generation methods have been
proposed. In this section, we describe the differ-
ences between our method and several previous
methods.
Japanese words are often followed by post-
positional particles, such as ?ga? and ?wo?,
to indicate the subject and object of a sen-
tence. There are no corresponding words in
English. Instead, English words are preceded
by articles, ?the? and ?a,? to distinguish def-
inite and indefinite nouns, and so on, and in
this case there are no corresponding words in
Japanese. Knight et al proposed a way to
compensate for missing information caused by
a lack of language-dependent knowledge, or a
?knowledge gap? (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998a; Langkilde
and Knight, 1998b). They use semantic expres-
sions as input, whereas we use keywords. Also,
they construct candidate-text sentences or word
lattices by applying rules, and apply their lan-
guage model, an n-gram model, to select the
most appropriate surface text. While we can-
not use their rules to generate candidate-text
sentences when given keywords, we can apply
their language model to our system to generate
surface-text sentences from candidate-text sen-
tences in the form of dependency trees. We can
also apply the formalism proposed by Langkilde
(Langkilde, 2000) to express the candidate-text
sentences.
Bangalore and Rambow proposed a method
to generate candidate-text sentences in the form
of trees (Bangalore and Rambow, 2000). They
consider dependency information when deriving
trees by using XTAG grammar, but they as-
sume that the input contains dependency infor-
mation. Our system generates candidate-text
sentences without relying on dependency infor-
mation in the input, and our model estimates
the dependencies between keywords.
Ratnaparkhi proposed models to generate
text from semantic attributes (Ratnaparkhi,
2000). The input of these models is semantic
attributes. His models are similar to ours if the
semantic attributes are replaced with keywords.
However, his models need a training corpus in
which certain words are replaced with seman-
tic attributes. Although our model also needs
a training corpus, the corpus can be automati-
cally created by using a morphological analyzer
and a dependency analyzer, both of which are
readily available.
Humphreys et al proposed using mod-
els developed for sentence-structure analysis to
rank candidate-text sentences (Humphreys et
al., 2001). As well as models developed for
sentence-structure analysis, we also use those
developed for morphological analysis and found
that these models contribute to the generation
of appropriate text.
Berger and Lafferty proposed a language
model for information retrieval (Berger and Laf-
ferty, 1999). Their concept is similar to that of
our model, which can be regarded as a model
that translates keywords into text, while their
model can be regarded as one that translates
query words into documents. However, the pur-
pose of their model is different: their goal is to
retrieve text that already exists while ours is to
generate new text.
7 Conclusion
We have described a method for generating sen-
tences from ?keywords? or ?headwords?. This
method consists of two main parts, candidate-
text construction and evaluation.
1. The construction part generates text sen-
tences in the form of dependency trees by
providing complementary information to
replace that missing due to a ?knowledge
gap? and other missing function words, and
thus generates natural text sentences based
on a particular monolingual corpus.
2. The evaluation part consists of a model
for generating an appropriate text sentence
when given keywords. This model consid-
ers the dependency information between
words as well as word n-gram informa-
tion. Furthermore, the model considers
both string and morphological information.
If a language model, such as a word n-gram
model, is applied to the generated-text sen-
tences in the form of dependency trees, an
appropriate surface-text sentence is generated.
The word-order model proposed by Uchimoto et
al. can also generate surface text in a natural
order (Uchimoto et al, 2000a).
There are several possible directions for our
future research. In particular,
? We would like to expand the generation
rules. We restricted the generation rules
automatically acquired from a corpus to
those that generate a bunsetsu. To gener-
ate a greater variety of candidate-text sen-
tences, we would like to expand the rules
that can generate a dependency tree. Ex-
pansion would lead to complementing with
content words as well as function words.
We also would like to prepare default rules
or to classify words into several classes
when no sentences including the keywords
are found in the target corpus.
? Some of the N-best text sentences gener-
ated by our system are semantically and
grammatically unnatural. To remove such
sentences from among the candidate-text
sentences, we must enhance our model so
that it can consider more information, such
as classified words or those in a thesaurus.
? We restricted keywords to the headwords or
rightmost content words in the bunsetsus.
We would like to expand the definition of
keywords to other content words and to
synonyms of the keywords.
Acknowledgments
We thank the Mainichi Newspapers for permis-
sion to use their data. We also thank Kimiko
Ohta, Hiroko Inui, Takehito Utsuro, Man-
abu Okumura, Akira Ushioda, Jun?ichi Tsujii,
Kiyosi Yasuda, and Masahisa Ohta for their
beneficial comments during the progress of this
work.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Probabilis-
tic Hierarchical Model for Generation. In Proceedings of
the COLING, pages 42?48.
A. Berger and J. Lafferty. 1999. Information Retrieval as
Statistical Translation. In Proceedings of the ACM SIGIR,
pages 222?229.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,
F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.
1990. A Statistical Approach to Machine Translation.
Computational Linguistics, 16(2):79?85.
K. Humphreys, M. Calcagno, and D. Weise. 2001. Reusing a
Statistical Language Model for Generation. In Proceedings
of the EWNLG.
K. Knight and V. Hatzivassiloglou. 1995. Two-Level, Many-
Paths Generation. In Proceedings of the ACL, pages 252?
260.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System. In
Proceedings of the NLPRS, pages 451?456.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
S. Kurohashi, 1998. Japanese Dependency/Case Structure
Analyzer KNP Version 2.0b6. Department of Informatics,
Kyoto University.
I. Langkilde and K. Knight. 1998a. Generation that Exploits
Corpus-Based Statistical Knowledge. In Proceedings of the
COLING-ACL, pages 704?710.
I. Langkilde and K. Knight. 1998b. The Practical Value of
N-grams in Generation. In Proceedings of the INLG.
I. Langkilde. 2000. Forest-Based Statistical Sentence Gener-
ation. In Proceedings of the NAACL, pages 170?177.
A. Ratnaparkhi. 2000. Trainable Methods for Surface Natu-
ral Language Generation. In Proceedings of the NAACL,
pages 194?201.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 1999. Japanese De-
pendency Structure Analysis Based on Maximum Entropy
Models. In Proceedings of the EACL, pages 196?203.
K. Uchimoto, M. Murata, Q. Ma, S. Sekine, and H. Isahara.
2000a. Word Order Acquisition from Corpora. In Proceed-
ings of the COLING, pages 871?877.
K. Uchimoto, M. Murata, S. Sekine, and H. Isahara. 2000b.
Dependency Model Using Posterior Context. In Proceed-
ings of the IWPT, pages 321?322.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
Morphological Analysis of The Spontaneous Speech Corpus
Kiyotaka Uchimoto?, Chikashi Nobata?, Atsushi Yamada?,
Satoshi Sekine?, and Hitoshi Isahara?
?Communications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes a project tagging a sponta-
neous speech corpus with morphological infor-
mation such as word segmentation and parts-of-
speech. We use a morphological analysis system
based on a maximum entropy model, which is
independent of the domain of corpora. In this
paper we show the tagging accuracy achieved by
using the model and discuss problems in tagging
the spontaneous speech corpus. We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
1 Introduction
In recent years, systems developed for analyz-
ing written-language texts have become consid-
erably accurate. This accuracy is largely due
to the large amounts of tagged corpora and the
rapid progress in the study of corpus-based nat-
ural language processing. However, the accu-
racy of the systems developed for written lan-
guage is not always high when these same sys-
tems are used to analyze spoken-language texts.
The reason for this remaining inaccuracy is due
to several differences between the two types of
languages. For example, the expressions used
in written language are often quite different
from those in spoken language, and sentence
boundaries are frequently ambiguous in spoken
language. The ?Spontaneous Speech: Corpus
and Processing Technology? project was imple-
mented in 1999 to overcome this problem. Spo-
ken language includes both monologue and dia-
logue texts; the former (e.g. the text of a talk)
was selected as a target of the project because it
was considered to be appropriate to the current
level of study on spoken language.
Tagging the spontaneous speech corpus with
morphological information such as word seg-
mentation and parts-of-speech is one of the
goals of the project. The tagged corpus is help-
ful for us in making a language model in speech
recognition as well as for linguists investigat-
ing distribution of morphemes in spontaneous
speech. For tagging the corpus with morpholog-
ical information, a morphological analysis sys-
tem is needed. Morphological analysis is one of
the basic techniques used in Japanese sentence
analysis. A morpheme is a minimal grammat-
ical unit, such as a word or a suffix, and mor-
phological analysis is the process of segment-
ing a given sentence into a row of morphemes
and assigning to each morpheme grammatical
attributes such as part-of-speech (POS) and in-
flection type. One of the most important prob-
lems in morphological analysis is that posed by
unknown words, which are words found in nei-
ther a dictionary nor a training corpus. Two
statistical approaches have been applied to this
problem. One is to find unknown words from
corpora and put them into a dictionary (e.g.,
(Mori and Nagao, 1996)), and the other is to
estimate a model that can identify unknown
words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both
approaches. They proposed a morphological
analysis method based on a maximum entropy
(M.E.) model (Uchimoto et al, 2001). We used
their method to tag a spontaneous speech cor-
pus. Their method uses a model that can not
only consult a dictionary but can also identify
unknown words by learning certain characteris-
tics. To learn these characteristics, we focused
on such information as whether or not a string
is found in a dictionary and what types of char-
acters are used in a string. The model esti-
mates how likely a string is to be a morpheme.
This model is independent of the domain of cor-
pora; in this paper we demonstrate that this is
true by applying our model to the spontaneous
speech corpus, Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000). We also show
that a dictionary developed for a corpus on a
certain domain is helpful for improving accu-
racy in analyzing a corpus on another domain.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem of
Japanese morphological analysis can be reduced
to the problem of assigning one of two tags to
each string in a sentence. A string is tagged
with a 1 or a 0 to indicate whether or not it is
a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. The 1
tag is thus divided into the number, n, of gram-
matical attributes assigned to morphemes, and
the problem is to assign an attribute (from 0
to n) to every string in a given sentence. The
(n + 1) tags form the space of ?futures? in the
M.E. formulation of our problem of morpholog-
ical analysis. The M.E. model enables the com-
putation of P (f |h) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
The computation of P (f |h) in any M.E. model
is dependent on a set of ?features? which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h, f) =
?
?
?
?
?
1 : if has(h, x) = true,
x = ?POS(?1)(Major) : verb,??
& f = 1
0 : otherwise.
(1)
Here ?has(h,x)? is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and what part-of-speech
the adjacent morpheme is.
Given a set of features and some training
data, the M.E. estimation process produces a
model, which is represented as follows (Berger
et al, 1996; Ristad, 1997; Ristad, 1998):
P (f |h) =
?
i
?
g
i
(h,f)
i
Z
?
(h)
(2)
Z
?
(h) =
?
f
?
i
?
g
i
(h,f)
i
. (3)
We define a model which estimates the like-
lihood that a given string is a morpheme and
has the grammatical attribute i(1 ? i ? n) as a
morpheme model. This model is represented by
Eq. (2), in which f can be one of (n + 1) tags
from 0 to n.
Given a sentence, it is divided into mor-
phemes, and a grammatical attribute is assigned
to each morpheme so as to maximize the sen-
tence probability estimated by our morpheme
model. Sentence probability is defined as the
product of the probabilities estimated for a par-
ticular division of morphemes in a sentence. We
use the Viterbi algorithm to find the optimal set
of morphemes in a sentence.
3 Experiments and Discussion
3.1 Experimental Conditions
We used the spontaneous speech corpus, CSJ,
which is a tagged corpus of transcriptions of
academic presentations and simulated public
speech. Simulated public speech is short speech
spoken specifically for the corpus by paid non-
professional speakers. For training, we used
805,954 morphemes from the corpus, and for
testing, we used 68,315 morphemes from the
corpus. Since there are no boundaries between
sentences in the corpus, we used two types of
boundaries, utterance boundaries, which are au-
tomatically detected at the place where a pause
of 200 ms or longer emerges in the CSJ, and
sentence boundaries assigned by the sentence
boundary identification system, which is based
on hand-crafted rules which use the pauses as
a clue. In the CSJ, fillers and disfluencies are
marked with tags (F) and (D). In the experi-
ments, we did not use those tags. Thus the in-
put sentences for testing are character strings
without any tags. The output is a sequence
of morphemes with grammatical attributes. As
the grammatical attributes, we define the part-
of-speech categories in the CSJ. There are 12
major categories. Therefore, the number of
grammatical attributes is 12, and f in Eq. (2)
can be one of 13 tags from 0 to 12.
Given a sentence, for every string consist-
ing of five or fewer characters and every string
appearing in a dictionary, whether or not the
string is a morpheme was determined and then
the grammatical attribute of each string deter-
mined to be a morpheme was identified and
assigned to that string. We collected all mor-
phemes from the training corpus except dis-
fluencies and used them as dictionary entries.
We denote the entries with a Corpus dictionary.
The maximum length for a morpheme was set
at five because morphemes consisting of six or
more characters are mostly compound words or
words consisting of katakana characters. We as-
sumed that compound words that do not appear
in the dictionary can be divided into strings con-
sisting of five or fewer characters because com-
pound words tend not to appear in dictionar-
ies. Katakana strings that are not found in the
dictionary were assumed to be included in the
dictionary as an entry having the part-of-speech
?Unknown(Major), Katakana(Minor).? An op-
timal set of morphemes in a sentence is searched
for by employing the Viterbi algorithm. The
assigned part-of-speech in the optimal set is se-
lected from all the categories of the M.E. model
except the one in which the string is not a mor-
pheme.
The features used in our experiments are
listed in Table 1. Each feature consists of a
type and a value, which are given in the rows of
the table. The features are basically some at-
tributes of the morpheme itself or attributes of
the morpheme to the left of it. We used the fea-
tures found three or more times in the training
corpus. The notations ?(0)? and ?(-1)? used in
the feature type column in Table 1 respectively
indicate a target string and the morpheme to
the left of it.
The terms used in the table are as follows:
String: Strings appearing as a morpheme three
or more times in the training corpus
Substring: Characters used in a string.
?(Left1)? and ?(Right1)? respectively rep-
resent the leftmost and rightmost charac-
ters of a string. ?(Left2)? and ?(Right2)?
respectively represent the leftmost and
rightmost character bigrams of a string.
Dic: Entries in the Corpus dictionary. As mi-
nor categories we used inflection types such
as a basic form as well as minor part-of-
speech categories. ?Major&Minor? indi-
cates possible combinations between major
and minor part-of-speech categories. When
the target string is in the dictionary, the
part-of-speech attached to the entry corre-
sponding to the string is used as a feature
value. If an entry has two or more parts-
of-speech, the part-of-speech which leads to
the highest probability in a sentence esti-
mated from our model is selected as a fea-
ture value.
Length: Length of a string
TOC: Types of characters used in a string.
?(Beginning)? and ?(End)?, respectively,
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the ?(Begin-
ning)? and ?(End)? are the same character.
?TOC(0)(Transition)? represents the tran-
sition from the leftmost character to the
rightmost character in a string. ?TOC(-
1)(Transition)? represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
character in the target string. For example,
when the adjacent morpheme on the left
is ??? (sensei, teacher)? and the target
string is ?? (ni, case marker),? the feature
value ?Kanji?Hiragana? is selected.
POS: Part-of-speech.
3.2 Results and Discussion
Results of the morphological analysis obtained
by our method are shown in Table 2. Recall
is the percentage of morphemes in the test cor-
pus whose segmentation and major POS tag are
identified correctly. Precision is the percentage
of all morphemes identified by the system that
are identified correctly. The F-measure is de-
fined by the following equation.
F ? measure =
2 ? Recall ? Precision
Recall + Precision
This result shows that there is no significant
difference between accuracies obtained by us-
ing two types of sentence boundaries. However,
we found that the errors that occurred around
utterance boundaries were reduced in the re-
sult obtained with sentence boundaries assigned
by the sentence boundary identification system.
This shows that there is a high possibility that
we can achieve better accuracy if we use bound-
aries assigned by the sentence boundary identi-
fication system as sentence boundaries and if we
use utterance boundaries as features.
In these experiments, we used only the en-
tries with a Corpus dictionary. Next we show
the experimental results with dictionaries de-
veloped for a corpus on a certain domain. We
added to the Corpus dictionary all the approx-
imately 200,000 entries of the JUMAN dictio-
nary (Kurohashi and Nagao, 1999). We also
added the entries of a dictionary developed by
ATR. We call it the ATR dictionary.
Results obtained with each dictionary or each
combination of dictionaries are shown in Ta-
ble 3. In this table, OOV indicates Out-of-
Vocabulary rates. The accuracy obtained with
the JUMAN dictionary or the ATR dictionary
was worse than the accuracy obtained without
those dictionaries. This is because the segmen-
Table 1: Features.
Feature number Feature type Feature value (Number of value)
1 String(0) (223,457)
2 String(-1) (20,769)
3 Substring(0)(Left1) (2,492)
4 Substring(0)(Right1) (2,489)
5 Substring(0)(Left2) (74,046)
6 Substring(0)(Right2) (73,616)
7 Substring(-1)(Left1) (2,237)
8 Substring(-1)(Right1) (2,489)
9 Substring(-1)(Left2) (12,726)
10 Substring(-1)(Right2) (12,241)
11 Dic(0)(Major) Noun, Verb, Adj, . . . Undefined (13)
12 Dic(0)(Minor) Common noun, Topic marker, Basic form. . . (223)
13 Dic(0)(Major&Minor) Noun&Common noun, Verb&Basic form, . . . (239)
14 Length(0) 1, 2, 3, 4, 5, 6 or more (6)
15 Length(-1) 1, 2, 3, 4, 5, 6 or more (6)
16 TOC(0)(Beginning) Kanji, Hiragana, Number, Katakana, Alphabet (5)
17 TOC(0)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
18 TOC(0)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (25)
19 TOC(-1)(End) Kanji, Hiragana, Number, Katakana, Alphabet (5)
20 TOC(-1)(Transition) Kanji?Hiragana, Number?Kanji, Katakana?Kanji, . . . (18)
21 POS(-1) Verb, Adj, Noun, . . . (12)
22 Comb(1,21) Combinations Feature 1 and 21 (142,546)
23 Comb(1,2,21) Combinations Feature 1, 2 and 21 (216,431)
24 Comb(1,13,21) Combinations Feature 1, 13 and 21 (29,876)
25 Comb(1,2,13,21) Combinations Feature 1, 2, 13 and 21 (158,211)
26 Comb(11,21) Combinations Feature 11 and 21 (156)
27 Comb(12,21) Combinations Feature 12 and 21 (1,366)
28 Comb(13,21) Combinations Feature 13 and 21 (1,518)
Table 2: Results of Experiments (Segmentation and major POS tagging).
Boundary Recall Precision F-measure
utterance 93.97% (64,198/68,315) 93.25% (64,198/68,847) 93.61
sentence 93.97% (64,195/68,315) 93.18% (64,195/68,895) 93.57
tation of morphemes and the definition of part-
of-speech categories in the JUMAN and ATR
dictionaries are different from those in the CSJ.
Given a sentence, for every string consisting
of five or fewer characters as well as every string
appearing in a dictionary, whether or not the
string is a morpheme was determined by our
morpheme model. However, we speculate that
we can ignore strings consisting of two or more
characters when they are not found in the dic-
tionary when OOV is low. Therefore, we carried
out the additional experiments ignoring those
strings. In the experiments, given a sentence,
for every string consisting of one character and
every string appearing in a dictionary, whether
or not the string is a morpheme is determined
by our morpheme model. Results obtained un-
der this condition are shown in Table 4. We
compared the accuracies obtained with dictio-
naries including the Corpus dictionary, whose
OOVs are relatively low. The accuracies ob-
tained with the additional dictionaries increased
while those obtained only with the Corpus dic-
tionary decreased. These results show that a
dictionary whose OOV in the test corpus is low
contributes to increasing the accuracy when ig-
noring the possibility that strings that consist
of two or more characters and are not found in
the dictionary become a morpheme.
These results show that a dictionary devel-
oped for a corpus on a certain domain can be
used to improve accuracy in analyzing a corpus
on another domain.
The accuracy in segmentation and major
POS tagging obtained for spontaneous speech
was worse than the approximately 95% obtained
for newspaper articles. We think the main rea-
son for this is the errors and the inconsistency
of the corpus, and the difficulty in recognizing
characteristic expressions often used in spoken
language such as fillers, mispronounced words,
and disfluencies. The inconsistency of the cor-
pus is due to the way the corpus was made, i.e.,
completely by human beings, and it is also due
Table 3: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.64% (63,288/68,315) 91.83% (63,288/68,917) 92.24 1.84%
Corpus sentence 92.61% (63,265/68,315) 91.79% (63,265/68,923) 92.20 1.84%
JUMAN utterance 90.28% (61,676/68,315) 90.07% (61,676/68,478) 90.17 6.13%
JUMAN sentence 90.33% (61,710/68,315) 90.22% (61,710/68,403) 90.27 6.13%
ATR utterance 89.80% (61,348/68,315) 90.12% (61,348/68,073) 89.96 8.14%
ATR sentence 89.96% (61,453/68,315) 90.30% (61,453/68,057) 90.13 8.14%
Corpus+JUMAN utterance 92.03% (62,872/68,315) 91.77% (62,872/68,507) 91.90 0.52%
Corpus+JUMAN sentence 92.09% (62,913/68,315) 91.80% (62,913/68,534) 91.95 0.52%
Corpus+ATR utterance 92.35% (63,086/68,315) 92.03% (63,086/68,547) 92.19 0.64%
Corpus+ATR sentence 92.30% (63,057/68,315) 91.94% (63,057/68,585) 92.12 0.64%
JUMAN+ATR utterance 91.60% (62,579/68,315) 91.57% (62,579/68,339) 91.59 4.61%
JUMAN+ATR sentence 91.66% (62,618/68,315) 91.67% (62,618/68,311) 91.66 4.61%
Corpus+JUMAN+ATR utterance 91.72% (62,658/68,315) 91.66% (62,658/68,357) 91.69 0.47%
Corpus+JUMAN+ATR sentence 91.72% (62,657/68,315) 91.62% (62,657/68,391) 91.67 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
Table 4: Results of Experiments (Segmentation and major POS tagging).
Dictionary Boundary Recall Precision F OOV
Corpus utterance 92.80% (63,395/68,315) 90.47% (63,395/70,075) 91.62 1.84%
Corpus sentence 92.71% (63,333/68,315) 90.48% (63,333/70,000) 91.58 1.84%
Corpus+JUMAN utterance 92.45% (63,154/68,315) 91.60% (63,154/68,942) 92.02 0.52%
Corpus+JUMAN sentence 92.48% (63,179/68,315) 91.71% (63,179/68,893) 92.09 0.52%
Corpus+ATR utterance 92.91% (63,474/68,315) 91.81% (63,474/69,137) 92.36 0.64%
Corpus+ATR sentence 92.75% (63,361/68,315) 91.76% (63,361/69,053) 92.25 0.64%
Corpus+JUMAN+ATR utterance 92.30% (63,055/68,315) 91.57% (63,055/68,858) 91.94 0.47%
Corpus+JUMAN+ATR sentence 92.28% (63,039/68,315) 91.55% (63,039/68,860) 91.91 0.47%
? For training 1/5 of all the training corpus (163,796 morphemes) was used.
to the definition of morphemes. Several incon-
sistencies in the test corpus existed, such as: ?
?? (tokyo, Noun)(Tokyo), ? (to, Other)(the
Metropolis), ? (ritsu, Other)(founded), ?
? (daigaku, Noun)(university),? and ???
(toritsu, Noun)(metropolitan), ?? (daigaku,
Noun)(university).? Both of these are the
names representing the same university. The
???? is partitioned into two in the first one
while it is not partitioned into two in the second
one according to the definition of morphemes.
When such inconsistencies in the corpus exist, it
is difficult for our model to discriminate among
these inconsistencies because we used only bi-
gram information as features. To achieve bet-
ter accuracy, therefore, we need to use trigram
or longer information. To correctly recognize
characteristic expressions often used in spoken
language, we plan to extract typical patterns
used in the expressions, to generalize the pat-
terns manually, and to generate possible expres-
sions using the generalized patterns, and finally,
to add such patterns to the dictionary. We also
plan to expand our model to skip fillers, mispro-
nounced words, and disfluencies because those
expressions are randomly inserted into text and
it is impossible to learn the connectivity be-
tween those randomly inserted expressions and
others.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A Maximum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis without a Dictionary for
Japanese. In Proceedings of the NLPRS, pages 541?544.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analysis System JUMAN Version 3.61. Department of
Informatics, Kyoto University.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proceedings of the
LREC, pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distribu-
tional Analysis. In Proceedings of the COLING, pages
1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method
for Japanese Unknown Words using a Statistical Model
of Morphology and Context. In Proceedings of the ACL,
pages 277?284.
E. S. Ristad. 1997. Maximum Entropy Modeling for Natural
Language. ACL/EACL Tutorial Program, Madrid.
E. S. Ristad. 1998. Maximum Entropy Modeling Toolkit,
Release 1.6 beta. http://www.mnemonic.com/software/
memt.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Us-
ing Maximum Entropy Aided by a Dictionary. In Proceed-
ings of the EMNLP, pages 91?99.
 
		ffA Statistical Model for Domain-Independent Text Segmentation
Masao Utiyama and Hitoshi Isahara
Communications Research Laboratory
2-2-2 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
mutiyama@crl.go.jp and isahara@crl.go.jp
Abstract
We propose a statistical method that
finds the maximum-probability seg-
mentation of a given text. This method
does not require training data because
it estimates probabilities from the given
text. Therefore, it can be applied to
any text in any domain. An experi-
ment showed that the method is more
accurate than or at least as accurate as
a state-of-the-art text segmentation sys-
tem.
1 Introduction
Documents usually include various topics. Identi-
fying and isolating topics by dividing documents,
which is called text segmentation, is important
for many natural language processing tasks, in-
cluding information retrieval (Hearst and Plaunt,
1993; Salton et al, 1996) and summarization
(Kan et al, 1998; Nakao, 2000). In informa-
tion retrieval, users are often interested in par-
ticular topics (parts) of retrieved documents, in-
stead of the documents themselves. To meet such
needs, documents should be segmented into co-
herent topics. Summarization is often used for a
long document that includes multiple topics. A
summary of such a document can be composed
of summaries of the component topics. Identifi-
cation of topics is the task of text segmentation.
A lot of research has been done on text seg-
mentation (Kozima, 1993; Hearst, 1994; Oku-
mura and Honda, 1994; Salton et al, 1996; Yaari,
1997; Kan et al, 1998; Choi, 2000; Nakao, 2000).
A major characteristic of the methods used in this
research is that they do not require training data
to segment given texts. Hearst (1994), for exam-
ple, used only the similarity of word distributions
in a given text to segment the text. Consequently,
these methods can be applied to any text in any
domain, even if training data do not exist. This
property is important when text segmentation is
applied to information retrieval or summarization,
because both tasks deal with domain-independent
documents.
Another application of text segmentation is
the segmentation of a continuous broadcast news
story into individual stories (Allan et al, 1998).
In this application, systems relying on supervised
learning (Yamron et al, 1998; Beeferman et al,
1999) achieve good performance because there
are plenty of training data in the domain. These
systems, however, can not be applied to domains
for which no training data exist.
The text segmentation algorithm described in
this paper is intended to be applied to the sum-
marization of documents or speeches. Therefore,
it should be able to handle domain-independent
texts. The algorithm thus does not use any train-
ing data. It requires only the given documents for
segmentation. It can, however, incorporate train-
ing data when they are available, as discussed in
Section 5.
The algorithm selects the optimum segmen-
tation in terms of the probability defined by a
statistical model. This is a new approach for
domain-independent text segmentation. Previous
approaches usually used lexical cohesion to seg-
ment texts into topics. Kozima (1993), for exam-
ple, used cohesion based on the spreading activa-
tion on a semantic network. Hearst (1994) used
the similarity of word distributions as measured
by the cosine to gauge cohesion. Reynar (1994)
used word repetition as a measure of cohesion.
Choi (2000) used the rank of the cosine, rather
than the cosine itself, to measure the similarity of
sentences.
The statistical model for the algorithm is de-
scribed in Section 2, and the algorithm for ob-
taining the maximum-probability segmentation is
described in Section 3. Experimental results are
presented in Section 4. Further discussion and our
conclusions are given in Sections 5 and 6, respec-
tively.
2 Statistical Model for Text
Segmentation
We first define the probability of a segmentation
of a given text in this section. In the next section,
we then describe the algorithm for selecting the
most likely segmentation.
Let
  
			 be a text consisting of
 words, and let      
			  be a segmen-
tation of
 
consisting of  segments. Then the
probability of the segmentation  is defined by:

Reliable Measures for Aligning Japanese-English News Articles and
Sentences
Masao Utiyama and Hitoshi Isahara
Communications Research Laboratory
3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto 619-0289 Japan
mutiyama@crl.go.jp and isahara@crl.go.jp
Abstract
We have aligned Japanese and English
news articles and sentences to make a
large parallel corpus. We first used a
method based on cross-language informa-
tion retrieval (CLIR) to align the Japanese
and English articles and then used a
method based on dynamic programming
(DP) matching to align the Japanese and
English sentences in these articles. How-
ever, the results included many incorrect
alignments. To remove these, we pro-
pose two measures (scores) that evaluate
the validity of alignments. The measure
for article alignment uses similarities in
sentences aligned by DP matching and
that for sentence alignment uses similar-
ities in articles aligned by CLIR. They
enhance each other to improve the accu-
racy of alignment. Using these measures,
we have successfully constructed a large-
scale article and sentence alignment cor-
pus available to the public.
1 Introduction
A large-scale Japanese-English parallel corpus is an
invaluable resource in the study of natural language
processing (NLP) such as machine translation and
cross-language information retrieval (CLIR). It is
also valuable for language education. However, no
such corpus has been available to the public.
We recently have obtained a noisy parallel cor-
pus of Japanese and English newspapers consisting
of issues published over more than a decade and
have tried to align their articles and sentences. We
first aligned the articles using a method based on
CLIR (Collier et al, 1998; Matsumoto and Tanaka,
2002) and then aligned the sentences in these articles
by using a method based on dynamic programming
(DP) matching (Gale and Church, 1993; Utsuro et
al., 1994). However, the results included many in-
correct alignments due to noise in the corpus.
To remove these, we propose two measures
(scores) that evaluate the validity of article and sen-
tence alignments. Using these, we can selectively
extract valid alignments.
In this paper, we first discuss the basic statistics
on the Japanese and English newspapers. We next
explain methods and measures used for alignment.
We then evaluate the effectiveness of the proposed
measures. Finally, we show that our aligned corpus
has attracted people both inside and outside the NLP
community.
2 Newspapers Aligned
The Japanese and English newspapers used as
source data were the Yomiuri Shimbun and the Daily
Yomiuri. They cover the period from September
1989 to December 2001. The number of Japanese
articles per year ranges from 100,000 to 350,000,
while English articles ranges from 4,000 to 13,000.
The total number of Japanese articles is about
2,000,000 and the total number of English articles is
about 110,000. The number of English articles rep-
resents less than 6 percent that of Japanese articles.
Therefore, we decided to search for the Japanese ar-
ticles corresponding to each of the English articles.
The English articles as of mid-July 1996 have tags
indicating whether they are translated from Japanese
articles or not, though they don?t have explicit links
to the original Japanese articles. Consequently, we
only used the translated English articles for the arti-
cle alignment. The number of English articles used
was 35,318, which is 68 percent of all of the arti-
cles. On the other hand, the English articles before
mid-July 1996 do not have such tags. So we used all
the articles for the period. The number of them was
59,086. We call the set of articles before mid-July
1996 ?1989-1996? and call the set of articles after
mid-July 1996 ?1996-2001.?
If an English article is a translation of a Japanese
article, then the publication date of the Japanese ar-
ticle will be near that of the English article. So we
searched for the original Japanese articles within 2
days before and after the publication of each English
article, i.e., the corresponding article of an English
article was searched for from the Japanese articles of
5 days? issues. The average number of English arti-
cles per day was 24 and that of Japanese articles per
5 days was 1,532 for 1989-1996. For 1996-2001, the
average number of English articles was 18 and that
of Japanese articles was 2,885. As there are many
candidates for alignment with English articles, we
need a reliable measure to estimate the validity of
article alignments to search for appropriate Japanese
articles from these ambiguous matches.
Correct article alignment does not guarantee the
existence of one-to-one correspondence between
English and Japanese sentences in article alignment
because literal translations are exceptional. Original
Japanese articles may be restructured to conform to
the style of English newspapers, additional descrip-
tions may be added to fill cultural gaps, and detailed
descriptions may be omitted. A typical example of a
restructured English and Japanese article pair is:
Part of an English article: ?e1? Two bullet holes were found at
the home of Kengo Tanaka, 65, president of Bungei Shunju, in Ak-
abane, Tokyo, by his wife Kimiko, 64, at around 9 a.m. Monday.
?/e1? ?e2? Police suspect right-wing activists, who have mounted
criticism against articles about the Imperial family appearing in
the Shukan Bunshun, the publisher?s weekly magazine, were re-
sponsible for the shooting. ?/e2? ?e3? Police received an anony-
mous phone call shortly after 1 a.m. Monday by a caller who
reported hearing gunfire near Tanaka?s residence. ?/e3? ?e4? Po-
lice found nothing after investigating the report, but later found a
bullet in the Tanakas? bedroom, where they were sleeping at the
time of the shooting. ?/e4?
Part of a literal translation of a Japanese article: ?j1? At about
8:55 a.m. on the 29th, Kimiko Tanaka, 64, the wife of Bungei
Shunju?s president Kengo Tanaka, 65, found bullet holes on the
eastern wall of their two-story house at 4 Akabane Nishi, Kita-
ku, Tokyo.?/j1? ?j2? As a result of an investigation, the officers of
the Akabane police station found two holes on the exterior wall of
the bedroom and a bullet in the bedroom.?/j2? ?j3? After receiv-
ing an anonymous phone call shortly after 1 a.m. saying that two
or three gunshots were heard near Tanaka?s residence, police offi-
cers hurried to the scene for investigation, but no bullet holes were
found.?/j3? ?j4?When gunshots were heard, Mr. and Mrs. Tanaka
were sleeping in the bedroom.?/j4? ?j5? Since Shukan Bunshun, a
weekly magazine published by Bungei Shunju, recently ran an ar-
ticle criticizing the Imperial family, Akabane police suspect right-
wing activists who have mounted criticism against the recent arti-
cle to be responsible for the shooting and have been investigating
the incident.?/j5?
where there is a three-to-four correspondence be-
tween {e1, e3, e4} and {j1, j2, j3, j4}, together with
a one-to-one correspondence between e2 and j5.
Such sentence matches are of particular interest
to researchers studying human translations and/or
stylistic differences between English and Japanese
newspapers. However, their usefulness as resources
for NLP such as machine translation is limited for
the time being. It is therefore important to extract
sentence alignments that are as literal as possible.
To achieve this, a reliable measure of the validity of
sentence alignments is necessary.
3 Basic Alignment Methods
We adopt a standard strategy to align articles and
sentences. First, we use a method based on CLIR
to align Japanese and English articles (Collier et
al., 1998; Matsumoto and Tanaka, 2002) and then
a method based on DP matching to align Japanese
and English sentences (Gale and Church, 1993; Ut-
suro et al, 1994) in these articles. As each of these
methods uses existing NLP techniques, we describe
them briefly focusing on basic similarity measures,
which we will compare with our proposed measures
in Section 5.
3.1 Article alignment
Translation of words
We first convert each of the Japanese articles into
a set of English words. We use ChaSen1 to seg-
ment each of the Japanese articles into words. We
next extract content words, which are then translated
into English words by looking them up in the EDR
Japanese-English bilingual dictionary,2 EDICT, and
ENAMDICT,3 which have about 230,000, 100,000,
1http://chasen.aist-nara.ac.jp/
2http://www.iijnet.or.jp/edr/
3http://www.csse.monash.edu.au/?jwb/edict.html
and 180,000 entries, respectively. We select two En-
glish words for each of the Japanese words using
simple heuristic rules based on the frequencies of
English words.
Article retrieval
We use each of the English articles as a query and
search for the Japanese article that is most similar
to the query article. The similarity between an En-
glish article and a (word-based English translation
of) Japanese article is measured by BM25 (Robert-
son and Walker, 1994). BM25 and its variants have
been proven to be quite efficient in information re-
trieval. Readers are referred to papers by the Text
REtrieval Conference (TREC)4, for example.
The definition of BM25 is:
BM25(J,E) =
?
T?E
w(1) (k1 + 1)tfK + tf
(k3 + 1)qtf
k3 + qtf
where
J is the set of translated English words of a
Japanese article and E is the set of words of an
English article. The words are stemmed and stop
words are removed.
T is a word contained in E.
w(1) is the weight of T , w(1) = log (N?n+0.5)(n+0.5) .
N is the number of Japanese articles to be searched.
n is the number of articles containing T .
K is k1((1 ? b) + b dlavdl ). k1, b and k3 are pa-
rameters set to 1, 1, and 1000, respectively. dl is
the document length of J and avdl is the average
document length in words.
tf is the frequency of occurrence of T in J . qtf is
the frequency of T in E.
To summarize, we first translate each of the
Japanese articles into a set of English words. We
then use each of the English articles as a query and
search for the most similar Japanese article in terms
of BM25 and assume that it corresponds to the En-
glish article.
3.2 Sentence alignment
The sentences5 in the aligned Japanese and English
articles are aligned by a method based on DP match-
ing (Gale and Church, 1993; Utsuro et al, 1994).
4http://trec.nist.gov/
5We split the Japanese articles into sentences by using sim-
ple heuristics and split the English articles into sentences by
using MXTERMINATOR (Reynar and Ratnaparkhi, 1997).
We allow 1-to-n or n-to-1 (1 ? n ? 6) alignments
when aligning the sentences. Readers are referred
to Utsuro et al (1994) for a concise description of
the algorithm. Here, we only discuss the similarities
between Japanese and English sentences for align-
ment. Let Ji and Ei be the words of Japanese and
English sentences for i-th alignment. The similar-
ity6 between Ji and Ei is:
SIM(Ji, Ei) = co(Ji ?Ei) + 1l(Ji) + l(Ei)? 2co(Ji ? Ei) + 2
where
l(X) =?x?X f(x)
f(x) is the frequency of x in the sentences.
co(Ji ? Ei) =
?
(j,e)?Ji?Ei min(f(j), f(e))
Ji ? Ei = {(j, e)|j ? Ji, e ? Ei} and Ji ? Ei is
a one-to-one correspondence between Japanese and
English words.
Ji and Ei are obtained as follows. We use ChaSen to
morphologically analyze the Japanese sentences and
extract content words, which consists of Ji. We use
Brill?s tagger (Brill, 1992) to POS-tag the English
sentences, extract content words, and use Word-
Net?s library7 to obtain lemmas of the words, which
consists of Ei. We use simple heuristics to obtain
Ji ? Ei, i.e., a one-to-one correspondence between
the words in Ji and Ei, by looking up Japanese-
English and English-Japanese dictionaries made up
by combining entries in the EDR Japanese-English
bilingual dictionary and the EDR English-Japanese
bilingual dictionary. Each of the constructed dictio-
naries has over 300,000 entries.
We evaluated the implemented program against a
corpus consisting of manually aligned Japanese and
English sentences. The source texts were Japanese
white papers (JEIDA, 2000). The style of translation
was generally literal reflecting the nature of govern-
ment documents. We used 12 pairs of texts for eval-
uation. The average number of Japanese sentences
per text was 413 and that of English sentences was
495.
The recall, R, and precision, P , of the program
against this corpus were R = 0.982 and P = 0.986,
respectively, where
6SIM(Ji, Ei) is different from the similarity function used
in Utsuro et al (1994). We use SIM because it performed well
in a preliminary experiment.
7http://www.cogsci.princeton.edu/?wn/
R = number of correctly aligned sentence pairs
total number of sentence pairs aligned in corpus
P = number of correctly aligned sentence pairs
total number of sentence pairs proposed by program
The number of pairs in a one-to-n alignment is n.
For example, if sentences {J1} and {E1, E2, E3}
are aligned, then three pairs ?J1, E1?, ?J1, E2?, and
?J1, E3? are obtained.
This recall and precision are quite good consid-
ering the relatively large differences in the language
structures between Japanese and English.
4 Reliable Measures
We use BM25 and SIM to evaluate the similarity
in articles and sentences, respectively. These mea-
sures, however, cannot be used to reliably discrim-
inate between correct and incorrect alignments as
will be discussed in Section 5. This motivated us to
devise more reliable measures based on basic simi-
larities.
BM25 measures the similarity between two bags
of words. It is not sensitive to differences in the
order of sentences between two articles. To rem-
edy this, we define a measure that uses the similari-
ties in sentence alignments in the article alignment.
We define AVSIM(J,E) as the similarity between
Japanese article, J , and English article, E:
AVSIM(J,E) =
?m
k=1 SIM(Jk, Ek)
m
where (J1, E1), (J2, E2), . . . (Jm, Em) are the sen-
tence alignments obtained by the method described
in Section 3.2. The sentence alignments in a cor-
rectly aligned article alignment should have more
similarity than the ones in an incorrectly aligned ar-
ticle alignment. Consequently, article alignments
with high AVSIM are likely to be correct.
Our sentence alignment program aligns sentences
accurately if the English sentences are literal trans-
lations of the Japanese as discussed in Section 3.2.
However, the relation between English news sen-
tences and Japanese news sentences are not literal
translations. Thus, the results for sentence align-
ments include many incorrect alignments. To dis-
criminate between correct and incorrect alignments,
we take advantage of the similarity in article align-
ments containing sentence alignments so that the
sentence alignments in a similar article alignment
will have a high value. We define
SntScore(Ji, Ei) = AVSIM(J,E)? SIM(Ji, Ei)
SntScore(Ji, Ei) is the similarity in the i-th align-
ment, (Ji, Ei), in article alignment J and E. When
we compare the validity of two sentence alignments
in the same article alignment, the rank order of sen-
tence alignments obtained by applying SntScore is
the same as that of SIM because they share a com-
mon AVSIM. However, when we compare the va-
lidity of two sentence alignments in different article
alignments, SntScore prefers the sentence alignment
with the more similar (high AVSIM) article align-
ment even if their SIM has the same value, while
SIM cannot discriminate between the validity of two
sentence alignments if their SIM has the same value.
Therefore, SntScore is more appropriate than SIM if
we want to compare sentence alignments in different
article alignments, because, in general, a sentence
alignment in a reliable article alignment is more re-
liable than one in an unreliable article alignment.
The next section compares the effectiveness of
AVSIM to that of BM25, and that of SntScore to
that of SIM.
5 Evaluation of Alignment
Here, we discuss the results of evaluating article and
sentence alignments.
5.1 Evaluation of article alignment
We first estimate the precision of article alignments
by using randomly sampled alignments. Next, we
sort them in descending order of BM25 and AVSIM
to see whether these measures can be used to provide
correct alignments with a high ranking. Finally, we
show that the absolute values of AVSIM correspond
well with human judgment.
Randomly sampled article alignments
Each English article was aligned with a Japanese
article with the highest BM25. We sampled 100 ar-
ticle alignments from each of 1996-2001 and 1989-
1996. We then classified the samples into four cate-
gories: ?A?, ?B?, ?C?, and ?D?. ?A? means that there
was more than 50% to 60% overlap in the content of
articles. ?B? means more than 20% to 30% and less
than 50% to 60% overlap. ?D? means that there was
no overlap at all. ?C? means that alignment was not
included in ?A?,?B? or ?D?. We regard alignments
that were judged to be A or B to be suitable for NLP
because of their relatively large overlap.
1996-2001 1989-1996type lower ratio upper lower ratio upper
A 0.49 0.59 0.69 0.20 0.29 0.38
B 0.06 0.12 0.18 0.08 0.15 0.22
C 0.03 0.08 0.13 0.03 0.08 0.13
D 0.13 0.21 0.29 0.38 0.48 0.58
Table 1: Ratio of article alignments
The results of evaluations are in Table 1.8 Here,
?ratio? means the ratio of the number of articles
judged to correspond to the respective category
against the total number of articles. For example,
0.59 in line ?A? of 1996-2001 means that 59 out of
100 samples were evaluated as A. ?Lower? and ?up-
per? mean the lower and upper bounds of the 95%
confidence interval for ratio.
The table shows that the precision (= sum of the
ratios of A and B) for 1996-2001 was higher than
that for 1989-1996. They were 0.71 for 1996-2001
and 0.44 for 1989-1996. This is because the En-
glish articles from 1996-2001 were translations of
Japanese articles, while those from 1989-1996 were
not necessarily translations as explained in Section
2. Although the precision for 1996-2001 was higher
than that for 1989-1996, it is still too low to use them
as NLP resources. In other words, the article align-
ments included many incorrect alignments.
We want to extract alignments which will be eval-
uated as A or B from these noisy alignments. To
do this, we have to sort all alignments according to
some measures that determine their validity and ex-
tract highly ranked ones. To achieve this, AVSIM is
more reliable than BM25 as is explained below.
8The evaluations were done by the authors. We double
checked the sample articles from 1996-2001. Our second
checks are presented in Table 1. The ratio of categories in the
first check were A=0.62, B=0.09, C=0.09, and D=0.20. Com-
paring these figures with those in Table 1, we concluded that
first and second evaluations were consistent.
Sorted alignments: AVSIM vs. BM25
We sorted the same alignments in Table 1 in de-
creasing order of AVSIM and BM25. Alignments
judged to be A or B were regarded as correct. The
number, N, of correct alignments and precision, P,
up to each rank are shown in Table 2.
1996-2001 1989-1996
AVSIM BM25 AVSIM BM25rank
N P N P N P N P
5 5 1.00 5 1.00 5 1.00 2 0.40
10 10 1.00 8 0.80 10 1.00 4 0.40
20 20 1.00 16 0.80 19 0.95 9 0.45
30 30 1.00 25 0.83 28 0.93 16 0.53
40 40 1.00 34 0.85 34 0.85 24 0.60
50 50 1.00 39 0.78 37 0.74 28 0.56
60 60 1.00 47 0.78 42 0.70 30 0.50
70 66 0.94 55 0.79 42 0.60 35 0.50
80 70 0.88 62 0.78 43 0.54 38 0.47
90 71 0.79 68 0.76 43 0.48 40 0.44
100 71 0.71 71 0.71 44 0.44 44 0.44
Table 2: Rank vs. precision
From the table, we can conclude that AVSIM
ranks correct alignments higher than BM25. Its
greater accuracy indicates that it is important to
take similarities in sentence alignments into account
when estimating the validity of article alignments.
AVSIM and human judgment
Table 2 shows that AVSIM is reliable in ranking
correct and incorrect alignments. This section re-
veals that not only rank order but also absolute val-
ues of AVSIM are reliable for discriminating be-
tween correct and incorrect alignments. That is,
they correspond well with human evaluations. This
means that a threshold value is set for each of 1996-
2001 and 1989-1996 so that valid alignments can be
extracted by selecting alignments whose AVSIM is
larger than the threshold.
We used the same data in Table 1 to calculate
statistics on AVSIM. They are shown in Tables 3
and 4 for 1996-2001 and 1989-1996, respectively.
type N lower av. upper th. sig.
A 59 0.176 0.193 0.209 0.168 **
B 12 0.122 0.151 0.179 0.111 **
C 8 0.077 0.094 0.110 0.085 *
D 21 0.065 0.075 0.086
Table 3: Statistics on AVSIM (1996-2001)
In these tables, ?N? means the number of align-
ments against the corresponding human judgment.
type N lower av. upper th. sig.
A 29 0.153 0.175 0.197 0.157 *
B 15 0.113 0.141 0.169 0.131
C 8 0.092 0.123 0.154 0.097 **
D 48 0.076 0.082 0.088
Table 4: Statistics on AVSIM (1989-1996)
?Av.? means the average value of AVSIM. ?Lower?
and ?upper? mean the lower and upper bounds of
the 95% confidence interval for the average. ?Th.?
means the threshold for AVSIM that can be used to
discriminate between the alignments estimated to be
the corresponding evaluations. For example, in Ta-
ble 3, evaluations A and B are separated by 0.168.
These thresholds were identified through linear dis-
criminant analysis. The asterisks ?**? and ?*? in the
?sig.? column mean that the difference in averages
for AVSIM is statistically significant at 1% and 5%
based on a one-sided Welch test.
In these tables, except for the differences in the
averages for B and C in Table 4, all differences
in averages are statistically significant. This indi-
cates that AVSIM can discriminate between differ-
ences in judgment. In other words, the AVSIM val-
ues correspond well with human judgment. We then
tried to determine why B and C in Table 4 were not
separated by inspecting the article alignments and
found that alignments evaluated as C in Table 4 had
relatively large overlaps compared with alignments
judged as C in Table 3. It was more difficult to dis-
tinguish B or C in Table 4 than in Table 3.
We next classified all article alignments in 1996-
2001 and 1989-1996 based on the thresholds in Ta-
bles 3 and 4. The numbers of alignments are in Table
5. It shows that the number of alignments estimated
to be A or B was 46738 (= 31495 + 15243). We
regard about 47,000 article alignments to be suffi-
ciently large to be useful as a resource for NLP such
as bilingual lexicon acquisition and for language ed-
ucation.
1996-2001 1989-1996 total
A 15491 16004 31495
B 9244 5999 15243
C 4944 10258 15202
D 5639 26825 32464
total 35318 59086 94404
Table 5: Number of articles per evaluation
In summary, AVSIM is more reliable than BM25
and corresponds well with human judgment. By us-
ing thresholds, we can extract about 47,000 article
alignments which are estimated to be A or B evalu-
ations.
5.2 Evaluation of sentence alignment
Sentence alignments in article alignments have
many errors even if they have been obtained from
correct article alignments due to free translation as
discussed in Section 2. To extract only correct
alignments, we sorted whole sentence alignments
in whole article alignments in decreasing order of
SntScore and selected only the higher ranked sen-
tence alignments so that the selected alignments
would be sufficiently precise to be useful as NLP
resources.
The number of whole sentence alignments was
about 1,300,000. The most important category for
sentence alignment is one-to-one. Thus, we want
to discard as many errors in this category as pos-
sible. In the first step, we classified whole one-
to-one alignments into two classes: the first con-
sisted of alignments whose Japanese and English
sentences ended with periods, question marks, ex-
clamation marks, or other readily identifiable char-
acteristics. We call this class ?one-to-one?. The
second class consisted of the one-to-one alignments
not belonging to the first class. The alignments
in this class, together with the whole one-to-n
alignments, are called ?one-to-many?. One-to-one
had about 640,000 alignments and one-to-many had
about 660,000 alignments.
We first evaluated the precision of one-to-one
alignments by sorting them in decreasing order of
SntScore. We randomly extracted 100 samples from
each of 10 blocks ranked at the top-300,000 align-
ments. (A block had 30,000 alignments.) We clas-
sified these 1000 samples into two classes: The
first was ?match? (A), the second was ?not match?
(D). We judged a sample as ?A? if the Japanese and
English sentences of the sample shared a common
event (approximately a clause). ?D? consisted of the
samples not belonging to ?A?. The results of evalua-
tion are in Table 6.9
9Evaluations were done by the authors. We double checked
all samples. In the 100 samples, there were a maximum of two
or three where the first and second evaluations were different.
range # of A?s # of D?s
1 - 100 0
30001 - 99 1
60001 - 99 1
90001 - 97 3
120001 - 96 4
150001 - 92 8
180001 - 82 18
210001 - 74 26
240001 - 47 53
270001 - 30 70
Table 6: One-to-one: Rank vs. judgment
This table shows that the number of A?s decreases
rapidly as the rank increases. This means that
SntScore ranks appropriate one-to-one alignments
highly. The table indicates that the top-150,000 one-
to-one alignments are sufficiently reliable.10 The ra-
tio of A?s in these alignments was 0.982.
We then evaluated precision for one-to-many
alignments by sorting them in decreasing order of
SntScore. We classified one-to-many into three cat-
egories: ?1-90000?, ?90001-180000?, and ?180001-
270000?, each of which was covered by the range of
SntScore of one-to-one that was presented in Table
6. We randomly sampled 100 one-to-many align-
ments from these categories and judged them to be A
or D (see Table 7). Table 7 indicates that the 38,090
alignments in the range from ?1-90000? are suffi-
ciently reliable.
range # of one-to-many # of A?s # of D?s
1 - 38090 98 2
90001 - 59228 87 13
180001 - 71711 61 39
Table 7: One-to-many: Rank vs. judgment
Tables 6 and 7 show that we can extract valid
alignments by sorting alignments according to
SntScore and by selecting only higher ranked sen-
tence alignments.
Overall, evaluations between the first and second check were
consistent.
10The notion of ?appropriate (correct) sentence alignment?
depends on applications. Machine translation, for example,
may require more precise (literal) alignment. To get literal
alignments beyond a sharing of a common event, we will select
a set of alignments from the top of the sorted alignments that
satisfies the required literalness. This is because, in general,
higher ranked alignments are more literal translations, because
those alignments tend to have many one-to-one corresponding
words and to be contained in highly similar article alignments.
Comparison with SIM
We compared SntScore with SIM and found that
SntScore is more reliable than SIM in discriminating
between correct and incorrect alignments.
We first sorted the one-to-one alignments in de-
creasing order of SIM and randomly sampled 100
alignments from the top-150,000 alignments. We
classified the samples into A or D. The number of
A?s was 93, and that of D?s was 7. The precision was
0.93. However, in Table 6, the number of A?s was
491 and D?s was 9, for the 500 samples extracted
from the top-150,000 alignments. The precision was
0.982. Thus, the precision of SntScore was higher
than that of SIM and this difference is statistically
significant at 1% based on a one-sided proportional
test.
We then sorted the one-to-many alignments by
SIM and sampled 100 alignments from the top
38,090 and judged them. There were 89 A?s and
11 D?s. The precision was 0.89. However, in Ta-
ble 7, there were 98 A?s and 2 D?s for samples from
the top 38,090 alignments. The precision was 0.98.
This difference is also significant at 1% based on a
one-sided proportional test.
Thus, SntScore is more reliable than SIM. This
high precision in SntScore indicates that it is im-
portant to take the similarities of article alignments
into account when estimating the validity of sen-
tence alignments.
6 Related Work
Much work has been done on article alignment. Col-
lier et al (1998) compared the use of machine trans-
lation (MT) with the use of bilingual dictionary term
lookup (DTL) for news article alignment in Japanese
and English. They revealed that DTL is superior to
MT at high-recall levels. That is, if we want to ob-
tain many article alignments, then DTL is more ap-
propriate than MT. In a preliminary experiment, we
also compared MT and DTL for the data in Table
1 and found that DTL was superior to MT.11 These
11We translated the English articles into Japanese with an MT
system. We then used the translated English articles as queries
and searched the database consisting of Japanese articles. The
direction of translation was opposite to the one described in
Section 3.1. Therefore this comparison is not as objective as
it could be. However, it gives us some idea into a comparison
of MT and DTL.
experimental results indicate that DTL is more ap-
propriate than MT in article alignment.
Matsumoto and Tanaka (2002) attempted to align
Japanese and English news articles in the Nikkei In-
dustrial Daily. Their method achieved a 97% preci-
sion in aligning articles, which is quite high. They
also applied their method to NHK broadcast news.
However, they obtained a lower precision of 69.8%
for the NHK corpus. Thus, the precision of their
method depends on the corpora. Therefore, it is not
clear whether their method would have achieved a
high accuracy in the Yomiuri corpus treated in this
paper.
There are two significant differences between our
work and previous works.
(1) We have proposed AVSIM, which uses sim-
ilarities in sentences aligned by DP matching, as
a reliable measure for article alignment. Previous
works, on the other hand, have used measures based
on bag-of-words.
(2) A more important difference is that we have
actually obtained not only article alignments but also
sentence alignments on a large scale. In addition to
that, we are distributing the alignment data for re-
search and educational purposes. This is the first
attempt at a Japanese-English bilingual corpus.
7 Availability
As of late-October 2002, we have been distributing
the alignment data discussed in this paper for re-
search and educational purposes.12 All the informa-
tion on the article and sentence alignments are nu-
merically encoded so that users who have the Yomi-
uri data can recover the results of alignments. The
data also contains the top-150,000 one-to-one sen-
tence alignments and the top-30,000 one-to-many
sentence alignments as raw sentences. The Yomiuri
Shimbun generously allowed us to distribute them
for research and educational purposes.
We have sent over 30 data sets to organizations
on their request. About half of these were NLP-
related. The other half were linguistics-related. A
few requests were from high-school and junior-high-
school teachers of English. A psycho-linguist was
also included. It is obvious that people from both in-
side and outside the NLP community are interested
12http://www.crl.go.jp/jt/a132/members/mutiyama/jea/index.html
in this Japanese-English alignment data.
8 Conclusion
We have proposed two measures for extracting valid
article and sentence alignments. The measure for ar-
ticle alignment uses similarities in sentences aligned
by DP matching and that for sentence alignment
uses similarities in articles aligned by CLIR. They
enhance each other and allow valid article and sen-
tence alignments to be reliably extracted from an ex-
tremely noisy Japanese-English parallel corpus.
We are distributing the alignment data discussed
in this paper so that it can be used for research and
educational purposes. It has attracted the attention of
people both inside and outside the NLP community.
We have applied our measures to a Japanese and
English bilingual corpus and these are language in-
dependent. It is therefore reasonable to expect that
they can be applied to any language pair and still re-
tain good performance, particularly since their effec-
tiveness has been demonstrated in such a disparate
language pair as Japanese and English.
References
Eric Brill. 1992. A simple rule-based part of speech tagger. In
ANLP-92, pages 152?155.
Nigel Collier, Hideki Hirakawa, and Akira Kumano. 1998. Ma-
chine translation vs. dictionary term translation ? a com-
parison for English-Japanese news article alignment. In
COLING-ACL?98, pages 263?267.
William A. Gale and Kenneth W. Church. 1993. A program
for aligning sentences in bilingual corpora. Computational
Linguistics, 19(1):75?102.
Japan Electronic Industry Development Association JEIDA.
2000. Sizen Gengo Syori-ni Kan-suru Tyousa Houkoku-syo
(Report on natural language processing systems).
Kenji Matsumoto and Hideki Tanaka. 2002. Automatic align-
ment of Japanese and English newspaper articles using an
MT system and a bilingual company name dictionary. In
LREC-2002, pages 480?484.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maxi-
mum entropy approach to identifying sentence boundaries.
In ANLP-97.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-Poisson model for probabilistic
weighted retrieval. In SIGIR?94, pages 232?241.
Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji Mat-
sumoto, and Makoto Nagao. 1994. Bilingual text match-
ing using bilingual dictionary and statistics. In COLING?94,
pages 1076?1082.
Morphological Analysis of a Large Spontaneous Speech Corpus in Japanese
Kiyotaka Uchimoto? Chikashi Nobata? Atsushi Yamada?
Satoshi Sekine? Hitoshi Isahara?
?Communications Research Laboratory
3-5, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
{uchimoto,nova,ark,isahara}@crl.go.jp
?New York University
715 Broadway, 7th floor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
This paper describes two methods for de-
tecting word segments and their morpho-
logical information in a Japanese sponta-
neous speech corpus, and describes how
to tag a large spontaneous speech corpus
accurately by using the two methods. The
first method is used to detect any type of
word segments. The second method is
used when there are several definitions for
word segments and their POS categories,
and when one type of word segments in-
cludes another type of word segments. In
this paper, we show that by using semi-
automatic analysis we achieve a precision
of better than 99% for detecting and tag-
ging short words and 97% for long words;
the two types of words that comprise the
corpus. We also show that better accuracy
is achieved by using both methods than by
using only the first.
1 Introduction
The ?Spontaneous Speech: Corpus and Process-
ing Technology? project is sponsoring the construc-
tion of a large spontaneous Japanese speech corpus,
Corpus of Spontaneous Japanese (CSJ) (Maekawa
et al, 2000). The CSJ is a collection of mono-
logues and dialogues, the majority being mono-
logues such as academic presentations and simu-
lated public speeches. Simulated public speeches
are short speeches presented specifically for the cor-
pus by paid non-professional speakers. The CSJ in-
cludes transcriptions of the speeches as well as audio
recordings of them. One of the goals of the project
is to detect two types of word segments and cor-
responding morphological information in the tran-
scriptions. The two types of word segments were
defined by the members of The National Institute for
Japanese Language and are called short word and
long word. The term short word approximates a dic-
tionary item found in an ordinary Japanese dictio-
nary, and long word represents various compounds.
The length and part-of-speech (POS) of each are dif-
ferent, and every short word is included in a long
word, which is shorter than a Japanese phrasal unit,
a bunsetsu. If all of the short words in the CSJ
were detected, the number of the words would be
approximately seven million. That would be the
largest spontaneous speech corpus in the world. So
far, approximately one tenth of the words have been
manually detected, and morphological information
such as POS category and inflection type have been
assigned to them. Human annotators tagged every
morpheme in the one tenth of the CSJ that has been
tagged, and other annotators checked them. The hu-
man annotators discussed their disagreements and
resolved them. The accuracies of the manual tagging
of short and long words in the one tenth of the CSJ
were greater than 99.8% and 97%, respectively. The
accuracies were evaluated by random sampling. As
it took over two years to tag one tenth of the CSJ ac-
curately, tagging the remainder with morphological
information would take about twenty years. There-
fore, the remaining nine tenths of the CSJ must be
tagged automatically or semi-automatically.
In this paper, we describe methods for detecting
the two types of word segments and corresponding
morphological information. We also describe how
to tag a large spontaneous speech corpus accurately.
Henceforth, we call the two types of word segments
short word and long word respectively, or merely
morphemes. We use the term morphological anal-
ysis for the process of segmenting a given sentence
into a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a POS cate-
gory.
2 Problems and Their Solutions
As we mentioned in Section 1, tagging the whole of
the CSJ manually would be difficult. Therefore, we
are taking a semi-automatic approach. This section
describes major problems in tagging a large sponta-
neous speech corpus with high precision in a semi-
automatic way, and our solutions to those problems.
One of the most important problems in morpho-
logical analysis is that posed by unknown words,
which are words found in neither a dictionary nor
a training corpus. Two statistical approaches have
been applied to this problem. One is to find un-
known words from corpora and put them into a
dictionary (e.g., (Mori and Nagao, 1996)), and the
other is to estimate a model that can identify un-
known words correctly (e.g., (Kashioka et al, 1997;
Nagata, 1999)). Uchimoto et al used both ap-
proaches. They proposed a morphological analysis
method based on a maximum entropy (ME) model
(Uchimoto et al, 2001). Their method uses a model
that estimates how likely a string is to be a mor-
pheme as its probability, and thus it has a potential
to overcome the unknown word problem. Therefore,
we use their method for morphological analysis of
the CSJ. However, Uchimoto et al reported that the
accuracy of automatic word segmentation and POS
tagging was 94 points in F-measure (Uchimoto et
al., 2002). That is much lower than the accuracy ob-
tained by manual tagging. Several problems led to
this inaccuracy. In the following, we describe these
problems and our solutions to them.
? Fillers and disfluencies
Fillers and disfluencies are characteristic ex-
pressions often used in spoken language, but
they are randomly inserted into text, so detect-
ing their segmentation is difficult. In the CSJ,
they are tagged manually. Therefore, we first
delete fillers and disfluencies and then put them
back in their original place after analyzing a
text.
? Accuracy for unknown words
The morpheme model that will be described
in Section 3.1 can detect word segments and
their POS categories even for unknown words.
However, the accuracy for unknown words is
lower than that for known words. One of the
solutions is to use dictionaries developed for a
corpus on another domain to reduce the num-
ber of unknown words, but the improvement
achieved is slight (Uchimoto et al, 2002). We
believe that the reason for this is that defini-
tions of a word segment and its POS category
depend on a particular corpus, and the defi-
nitions from corpus to corpus differ word by
word. Therefore, we need to put only words
extracted from the same corpus into a dictio-
nary. We are manually examining words that
are detected by the morpheme model but that
are not found in a dictionary. We are also
manually examining those words that the mor-
pheme model estimated as having low proba-
bility. During the process of manual exami-
nation, if we find words that are not found in
a dictionary, those words are then put into a
dictionary. Section 4.2.1 will describe the ac-
curacy of detecting unknown words and show
how much those words contribute to improving
the morphological analysis accuracy when they
are detected and put into a dictionary.
? Insufficiency of features
The model currently used for morphological
analysis considers the information of a target
morpheme and that of an adjacent morpheme
on the left. To improve the model, we need to
consider the information of two or more mor-
phemes on the left of the target morpheme.
However, too much information often leads to
overtraining the model. Using all the informa-
tion makes training the model difficult when
there is too much of it. Therefore, the best
way to improve the accuracy of the morpholog-
ical information in the CSJ within the limited
time available to us is to examine and revise
the errors of automatic morphological analysis
and to improve the model. We assume that the
smaller the probability estimated by a model
for an output morpheme is, then the greater
the likelihood is that the output morpheme is
wrong. Therefore, we examine output mor-
phemes in ascending order of their probabili-
ties. The expected improvement of the accu-
racy of the morphological information in the
whole of the CSJ will be described in Sec-
tion 4.2.1
Another problem concerning unknown words
is that the cost of manual examination is high
when there are several definitions for word seg-
ments and their POS categories. Since there
are two types of word definitions in the CSJ,
the cost would double. Therefore, to reduce the
cost, we propose another method for detecting
word segments and their POS categories. The
method will be described in Section 3.2, and
the advantages of the method will be described
in Section 4.2.2
The next problem described here is one that we
have to solve to make a language model for auto-
matic speech recognition.
? Pronunciation
Pronunciation of each word is indispensable for
making a language model for automatic speech
recognition. In the CSJ, pronunciation is tran-
scribed separately from the basic form writ-
ten by using kanji and hiragana characters as
shown in Fig. 1. Text targeted for morpho-
Basic form Pronunciation
0017 00051.425-00052.869 L:
(F??) (F??)
????? ?????????
0018 00053.073-00054.503 L:
???? ????
0019 00054.707-00056.341 L:
???????? ?????????
?Well, I?m going to talk about morphological analysis.?
Figure 1: Example of transcription.
logical analysis is the basic form of the CSJ
and it does not have information on actual pro-
nunciation. The result of morphological anal-
ysis, therefore, is a row of morphemes that
do not have information on actual pronuncia-
tion. To estimate actual pronunciation by using
only the basic form and a dictionary is impossi-
ble. Therefore, actual pronunciation is assigned
to results of morphological analysis by align-
ing the basic form and pronunciation in the
CSJ. First, the results of morphological anal-
ysis, namely, the morphemes, are transliterated
into katakana characters by using a dictionary,
and then they are aligned with pronunciation
in the CSJ by using a dynamic programming
method.
In this paper, we will mainly discuss methods for
detecting word segments and their POS categories in
the whole of the CSJ.
3 Models and Algorithms
This section describes two methods for detecting
word segments and their POS categories. The first
method uses morpheme models and is used to detect
any type of word segment. The second method uses
a chunking model and is only used to detect long
word segments.
3.1 Morpheme Model
Given a tokenized test corpus, namely a set of
strings, the problem of Japanese morphological
analysis can be reduced to the problem of assign-
ing one of two tags to each string in a sentence. A
string is tagged with a 1 or a 0 to indicate whether
it is a morpheme. When a string is a morpheme, a
grammatical attribute is assigned to it. A tag desig-
nated as a 1 is thus assigned one of a number, n, of
grammatical attributes assigned to morphemes, and
the problem becomes to assign an attribute (from 0
to n) to every string in a given sentence.
We define a model that estimates the likelihood
that a given string is a morpheme and has a gram-
matical attribute i(1 ? i ? n) as a morpheme
model. We implemented this model within an ME
modeling framework (Jaynes, 1957; Jaynes, 1979;
Berger et al, 1996). The model is represented by
Eq. (1):
p
?
(a|b) =
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
Z
?
(b)
(1)
Short word Long word
Word Pronunciation POS Others Word Pronunciation POS Others
?? (form) ????(keitai) Noun ????? (morphological
analysis)
????????
?
(keitaisokaiseki) Noun
? (element) ? (so) Suffix
?? (analysis)????(kaiseki) Noun
? ? (ni) PPP case marker ???? (about) ???? (nitsuite) PPP case marker,
compound
word
?? (relate) ?? (tsui) Verb KA-GYO, ADF, eu-
phonic change
? ? (te) PPP conjunctive
? ? (o) Prefix ??????(talk) ??????? (ohanashiitasi) Verb SA-GYO,
ADF
?? (talk) ??? (hanashi) Verb SA-GYO, ADF
???(do) ??? (itashi) Verb SA-GYO, ADF
?? ?? (masu) AUX ending form ?? ?? (masu) AUX ending form
PPP : post-positional particle , AUX : auxiliary verb , ADF : adverbial form
Figure 2: Example of morphological analysis results.
Z
?
(b) =
?
a
exp
(
?
i,j
?
i,j
g
i,j
(a, b)
)
, (2)
where a is one of the categories for classification,
and it can be one of (n+1) tags from 0 to n (This is
called a ?future.?), b is the contextual or condition-
ing information that enables us to make a decision
among the space of futures (This is called a ?his-
tory.?), and Z
?
(b) is a normalizing constant deter-
mined by the requirement that
?
a
p
?
(a|b) = 1 for
all b. The computation of p
?
(a|b) in any ME model
is dependent on a set of ?features? which are binary
functions of the history and future. For instance, one
of our features is
g
i,j
(a, b) =
{
1 : if has(b, f
j
) = 1 & a = a
i
f
j
= ?POS(?1)(Major) : verb,??
0 : otherwise.
(3)
Here ?has(b, f
j
)? is a binary function that returns
1 if the history b has feature f
j
. The features used
in our experiments are described in detail in Sec-
tion 4.1.1.
Given a sentence, probabilities of n tags from 1
to n are estimated for each length of string in that
sentence by using the morpheme model. From all
possible division of morphemes in the sentence, an
optimal one is found by using the Viterbi algorithm.
Each division is represented as a particular division
of morphemes with grammatical attributes in a sen-
tence, and the optimal division is defined as a di-
vision that maximizes the product of the probabil-
ities estimated for each morpheme in the division.
For example, the sentence ???????????
??????? in basic form as shown in Fig. 1 is
analyzed as shown in Fig. 2. ??????? is ana-
lyzed as three morphemes, ??? (noun)?, ?? (suf-
fix)?, and ??? (noun)?, for short words, and as one
morpheme, ?????? (noun)? for long words.
In conventional models (e.g., (Mori and Nagao,
1996; Nagata, 1999)), probabilities were estimated
for candidate morphemes that were found in a dic-
tionary or a corpus and for the remaining strings
obtained by eliminating the candidate morphemes
from a given sentence. Therefore, unknown words
were apt to be either concatenated as one word or di-
vided into both a combination of known words and
a single word that consisted of more than one char-
acter. However, this model has the potential to cor-
rectly detect any length of unknown words.
3.2 Chunking Model
The model described in this section can be applied
when several types of words are defined in a cor-
pus and one type of words consists of compounds of
other types of words. In the CSJ, every long word
consists of one or more short words.
Our method uses two models, a morpheme model
for short words and a chunking model for long
words. After detecting short word segments and
their POS categories by using the former model,
long word segments and their POS categories are de-
tected by using the latter model. We define four la-
bels, as explained below, and extract long word seg-
ments by estimating the appropriate labels for each
short word according to an ME model. The four la-
bels are listed below:
Ba: Beginning of a long word, and the POS cat-
egory of the long word agrees with the short
word.
Ia: Middle or end of a long word, and the POS cat-
egory of the long word agrees with the short
word.
B: Beginning of a long word, and the POS category
of the long word does not agree with the short
word.
I: Middle or end of a long word, and the POS cat-
egory of the long word does not agree with the
short word.
A label assigned to the leftmost constituent of a long
word is ?Ba? or ?B?. Labels assigned to other con-
stituents of a long word are ?Ia?, or ?I?. For exam-
ple, the short words shown in Fig. 2 are labeled as
shown in Fig. 3. The labeling is done deterministi-
cally from the beginning of a given sentence to its
end. The label that has the highest probability as es-
timated by an ME model is assigned to each short
word. The model is represented by Eq. (1). In Eq.
(1), a can be one of four labels. The features used in
our experiments are described in Section 4.1.2.
Short word Long word
Word POS Label Word POS
?? Noun Ba ????? Noun
? Suffix I
?? Noun Ia
? PPP Ba ???? PPP
?? Verb I
? PPP Ia
? Prefix B ?????? Verb
?? Verb Ia
??? Verb Ia
?? AUX Ba ?? AUX
PPP : post-positional particle , AUX : auxiliary verb
Figure 3: Example of labeling.
When a long word that does not include a short
word that has been assigned the label ?Ba? or ?Ia?,
this indicates that the word?s POS category differs
from all of the short words that constitute the long
word. Such a word must be estimated individually.
In this case, we estimate the POS category by us-
ing transformation rules. The transformation rules
are automatically acquired from the training corpus
by extracting long words with constituents, namely
short words, that are labeled only ?B? or ?I?. A rule
is constructed by using the extracted long word and
the adjacent short words on its left and right. For
example, the rule shown in Fig. 4 was acquired in
our experiments. The middle division of the con-
sequent part represents a long word ???? (auxil-
iary verb), and it consists of two short words ???
(post-positional particle) and ??? (verb). If several
different rules have the same antecedent part, only
the rule with the highest frequency is chosen. If no
rules can be applied to a long word segment, rules
are generalized in the following steps.
1. Delete posterior context
2. Delete anterior and posterior contexts
3. Delete anterior and posterior contexts and lexi-
cal entries.
If no rules can be applied to a long word segment in
any step, the POS category noun is assigned to the
long word.
4 Experiments and Discussion
4.1 Experimental Conditions
In our experiments, we used 744,204 short words
and 618,538 long words for training, and 63,037
short words and 51,796 long words for testing.
Those words were extracted from one tenth of the
CSJ that already had been manually tagged. The
training corpus consisted of 319 speeches and the
test corpus consisted of 19 speeches.
Transcription consisted of basic form and pronun-
ciation, as shown in Fig. 1. Speech sounds were
faithfully transcribed as pronunciation, and also rep-
resented as basic forms by using kanji and hiragana
characters. Lines beginning with numerical digits
are time stamps and represent the time it took to
produce the lines between that time stamp and the
next time stamp. Each line other than time stamps
represents a bunsetsu. In our experiments, we used
only the basic forms. Basic forms were tagged with
several types of labels such as fillers, as shown in
Table 1. Strings tagged with those labels were han-
dled according to rules as shown in the rightmost
columns in Table 1.
Since there are no boundaries between sentences
in the corpus, we selected the places in the CSJ that
Anterior context Target words Posterior context
Entry ?? (it, go) ? (te)? (mi, try) ?? (tai, want)
POS Verb PPP Verb AUX
Label Ba B I Ba
Antecedent part
?
Anterior context Long word Posterior context
?? (it, go) ?? (temi, try) ?? (tai, want)
Verb AUX AUX
Consequent part
Figure 4: Example of transformation rules.
Table 1: Type of labels and their handling.
Type of Labels Example Rules
Fillers (F??) delete all
Disfluencies (D?)????? (D2?)? delete all
No confidence in
transcription
(? ?????) leave a candidate
Entirely (?) delete all
Several can- (? ???,????) leave the former
didates exist candidate
Citation on sound or
words
(M?)? (M?)??? leave a candidate
Foreign, archaic, or
dialect words
(O???????) leave a candidate
Personal name, dis-
criminating words,
and slander
???? (R??)??? leave a candidate
Letters and their
pronunciation in
katakana strings
(A????;EU) leave the former
candidate
Strings that cannot
be written in kanji
characters
(K? (F??)??;?) leave the latter can-
didate
are automatically detected as pauses of 500 ms or
longer and then designated them as sentence bound-
aries. In addition to these, we also used utterance
boundaries as sentence boundaries. These are au-
tomatically detected at places where short pauses
(shorter than 200 ms but longer than 50 ms) follow
the typical sentence-ending forms of predicates such
as verbs, adjectives, and copula.
4.1.1 Features Used by Morpheme Models
In the CSJ, bunsetsu boundaries, which are phrase
boundaries in Japanese, were manually detected.
Fillers and disfluencies were marked with the labels
(F) and (D). In the experiments, we eliminated fillers
and disfluencies but we did use their positional infor-
mation as features. We also used as features, bun-
setsu boundaries and the labels (M), (O), (R), and
(A), which were assigned to particular morphemes
such as personal names and foreign words. Thus, the
input sentences for training and testing were charac-
ter strings without fillers and disfluencies, and both
boundary information and various labels were at-
tached to them. Given a sentence, for every string
within a bunsetsu and every string appearing in a
dictionary, the probabilities of a in Eq. (1) were es-
timated by using the morpheme model. The output
was a sequence of morphemes with grammatical at-
tributes, as shown in Fig. 2. We used the POS cate-
gories in the CSJ as grammatical attributes. We ob-
tained 14 major POS categories for short words and
15 major POS categories for long words. Therefore,
a in Eq. (1) can be one of 15 tags from 0 to 14 for
short words, and it can be one of 16 tags from 0 to
15 for long words.
Table 2: Features.
Number Feature Type Feature value
(Number of value) (Short:Long)
1 String(0) (113,474:117,002)
2 String(-1) (17,064:32,037)
3 Substring(0)(Left1) (2,351:2,375)
4 Substring(0)(Right1) (2,148:2,171)
5 Substring(0)(Left2) (30,684:31,456)
6 Substring(0)(Right2) (25,442:25,541)
7 Substring(-1)(Left1) (2,160:2,088)
8 Substring(-1)(Right1) (1,820:1,675)
9 Substring(-1)(Left2) (11,025:12,875)
10 Substring(-1)(Right2) (10,439:13,364)
11 Dic(0)(Major) Noun, Verb, Adjective, . . . Unde-
fined (15:16)
12 Dic(0)(Minor) Common noun, Topic marker, Ba-
sic form. . . (75:71)
13 Dic(0)(Major&Minor) Noun&Common noun,
Verb&Basic form, . . . (246:227)
14 Dic(-1)(Minor) Common noun, Topic marker, Ba-
sic form. . . (16:16)
15 POS(-1) Noun, Verb, Adjective, . . . (14:15)
16 Length(0) 1, 2, 3, 4, 5, 6 or more (6:6)
17 Length(-1) 1, 2, 3, 4, 5, 6 or more (6:6)
18 TOC(0)(Beginning) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
19 TOC(0)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
20 TOC(0)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (25:25)
21 TOC(-1)(End) Kanji, Hiragana, Number,
Katakana, Alphabet (5:5)
22 TOC(-1)(Transition) Kanji?Hiragana,
Number?Kanji,
Katakana?Kanji, . . . (16:15)
23 Boundary Bunsetsu(Beginning), Bun-
setsu(End), Label(Beginning),
Label(End), (4:4)
24 Comb(1,15) (74,602:59,140)
25 Comb(1,2,15) (141,976:136,334)
26 Comb(1,13,15) (78,821:61,813)
27 Comb(1,2,13,15) (156,187:141,442)
28 Comb(11,15) (209:230)
29 Comb(12,15) (733:682)
30 Comb(13,15) (1,549:1,397)
31 Comb(12,14) (730:675)
The features we used with morpheme models in
our experiments are listed in Table 2. Each feature
consists of a type and a value, which are given in the
rows of the table, and it corresponds to j in the func-
tion g
i,j
(a, b) in Eq. (1). The notations ?(0)? and
?(-1)? used in the feature-type column in Table 2 re-
spectively indicate a target string and the morpheme
to the left of it. The terms used in the table are ba-
sically as same as those that Uchimoto et al used
(Uchimoto et al, 2002). The main difference is the
following one:
Boundary: Bunsetsu boundaries and positional in-
formation of labels such as fillers. ?(Begin-
ning)? and ?(End)? in Table 2 respectively indi-
cate whether the left and right side of the target
strings are boundaries.
We used only those features that were found three or
more times in the training corpus.
4.1.2 Features Used by a Chunking Model
We used the following information as features
on the target word: a word and its POS cate-
gory, and the same information for the four clos-
est words, the two on the left and the two on
the right of the target word. Bigram and tri-
gram words that included a target word plus bigram
and trigram POS categories that included the tar-
get word?s POS category were used as features. In
addition, bunsetsu boundaries as described in Sec-
tion 4.1.1 were used. For example, when a target
word was ??? in Fig. 3, ???, ????, ???, ??
??, ???, ?Suffix?, ?Noun?, ?PPP?, ?Verb?, ?PPP?,
???&??, ??&???, ?? &?? &??, ??
&??&??, ?Noun&PPP?, ?PPP&Verb?, ?Suf-
fix&Noun&PPP?, ?PPP&Verb&PPP?, and ?Bun-
setsu(Beginning)? were used as features.
4.2 Results and Discussion
4.2.1 Experiments Using Morpheme Models
Results of the morphological analysis obtained by
using morpheme models are shown in Table 3 and
4. In these tables, OOV indicates Out-of-Vocabulary
rates. Shown in Table 3, OOV was calculated as the
proportion of words not found in a dictionary to all
words in the test corpus. In Table 4, OOV was cal-
culated as the proportion of word and POS category
pairs that were not found in a dictionary to all pairs
in the test corpus. Recall is the percentage of mor-
phemes in the test corpus for which the segmentation
and major POS category were identified correctly.
Precision is the percentage of all morphemes identi-
fied by the system that were identified correctly. The
F-measure is defined by the following equation.
F ? measure =
2? Recall ? Precision
Recall + Precision
Table 3: Accuracies of word segmentation.
Word Recall Precision F OOV
Short 97.47% (61,444
63,037
) 97.62% (61,444
62,945
) 97.54 1.66%
99.23% (62,553
63,037
) 99.11% (62,553
63,114
) 99.17 0%
Long 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21 5.81%
99.05% (51,306
51,796
) 98.58% (51,306
52,047
) 98.81 0%
Table 4: Accuracies of word segmentation and POS
tagging.
Word Recall Precision F OOV
Short 95.72% (60,341
63,037
) 95.86% (60,341
62,945
) 95.79 2.64%
97.57% (61,505
63,037
) 97.45% (61,505
63,114
) 97.51 0%
Long 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21 6.93%
97.30% (50,396
51,796
) 96.83% (50,396
52,047
) 97.06 0%
Tables 3 and 4 show that accuracies would im-
prove significantly if no words were unknown. This
indicates that all morphemes of the CSJ could be an-
alyzed accurately if there were no unknown words.
The improvements that we can expect by detecting
unknown words and putting them into dictionaries
are about 1.5 in F-measure for detecting word seg-
ments of short words and 2.5 for long words. For de-
tecting the word segments and their POS categories,
for short words we expect an improvement of about
2 in F-measure and for long words 3.
Next, we discuss accuracies obtained when un-
known words existed. The OOV for long words
was 4% higher than that for short words. In gen-
eral, the higher the OOV is, the more difficult de-
tecting word segments and their POS categories
is. However, the difference between accuracies
for short and long words was about 1% in recall
and 2% in precision, which is not significant when
we consider that the difference between OOVs for
short and long words was 4%. This result indi-
cates that our morpheme models could detect both
known and unknown words accurately, especially
long words. Therefore, we investigated the recall
of unknown words in the test corpus, and found
that 55.7% (928/1,667) of short word segments and
74.1% (2,660/3,590) of long word segments were
detected correctly. In addition, regarding unknown
words, we also found that 47.5% (791/1,667) of
short word segments plus their POS categories and
67.3% (2,415/3,590) of long word segments plus
their POS categories were detected correctly. The
recall of unknown words was about 20% higher for
long words than for short words. We believe that
this result mainly depended on the difference be-
tween short words and long words in terms of the
definitions of compound words. A compound word
is defined as one word when it is based on the def-
inition of long words; however it is defined as two
or more words when it is based on the definition of
short words. Furthermore, based on the definition of
short words, a division of compound words depends
on its context. More information is needed to pre-
cisely detect short words than is required for long
words. Next, we extracted words that were detected
by the morpheme model but were not found in a dic-
tionary, and investigated the percentage of unknown
words that were completely or partially matched to
the extracted words by their context. This percent-
age was 77.6% (1,293/1,667) for short words, and
80.6% (2,892/3,590) for long words. Most of the re-
maining unknown words that could not be detected
by this method are compound words. We expect that
these compounds can be detected during the manual
examination of those words for which the morpheme
model estimated a low probability, as will be shown
later.
The recall of unknown words was lower than that
of known words, and the accuracy of automatic mor-
phological analysis was lower than that of manual
morphological analysis. As previously stated, to
improve the accuracy of the whole corpus we take
a semi-automatic approach. We assume that the
smaller the probability is for an output morpheme
estimated by a model, the more likely the output
morpheme is wrong, and we examine output mor-
phemes in ascending order of their probabilities. We
investigated how much the accuracy of the whole
corpus would increase. Fig. 5 shows the relation-
ship between the percentage of output morphemes
whose probabilities exceed a threshold and their
93
94
95
96
97
98
99
100
20 30 40 50 60 70 80 90 100
Pr
ec
is
io
n 
(%
)
Output Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 5: Partial analysis.
precision. In this figure, ?short without UKW?,
?long without UKW??, ?short with UKW?, and
?long with UKW? represent the precision for short
words detected assuming there were no unknown
words, precision for long words detected assuming
there were no unknown words, precision of short
words including unknown words, and precision of
long words including unknown words, respectively.
When the output rate in the horizontal axis in-
creases, the number of low-probability morphemes
increases. In all graphs, precisions monotonously
decrease as output rates increase. This means that
tagging errors can be revised effectively when mor-
phemes are examined in ascending order of their
probabilities.
Next, we investigated the relationship between the
percentage of morphemes examined manually and
the precision obtained after detected errors were re-
vised. The result is shown in Fig. 6. Precision
represents the precision of word segmentation and
POS tagging. If unknown words were detected and
put into a dictionary by the method described in the
fourth paragraph of this section, the graph line for
short words would be drawn between the graph lines
?short without UKW? and ?short with UKW?, and
the graph line for long words would be drawn be-
tween the graph lines ?long without UKW? and
?long with UKW?. Based on test results, we can
expect better than 99% precision for short words
and better than 97% precision for long words in the
whole corpus when we examine 10% of output mor-
93
94
95
96
97
98
99
100
0 20 40 60 80 100 120
Pr
ec
is
io
n 
(%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"long_without_UKW"
"short_with_UKW"
"long_with_UKW"
Figure 6: Relationship between the percentage of
morphemes examined manually and precision ob-
tained after revising detected errors (when mor-
phemes with probabilities under threshold and their
adjacent morphemes are examined).
0
10
20
30
40
50
60
0 5 10 15 20 25 30 35 40 45 50
Er
ro
r R
at
es
 in
 E
xa
m
in
ed
 M
or
ph
em
es
 (%
)
Examined Morpheme Rates (%)
"short_without_UKW"
"short_with_UKW"
"long_without_UKW"
"long_with_UKW"
Figure 7: Relationship between percentage of mor-
phemes examined manually and error rate of exam-
ined morphemes.
phemes in ascending order of their probabilities.
Finally, we investigated the relationship between
percentage of morphemes examined manually and
the error rate for all of the examined morphemes.
The result is shown in Fig. 7. We found that about
50% of examined morphemes would be found as er-
rors at the beginning of the examination and about
20% of examined morphemes would be found as
errors when examination of 10% of the whole cor-
pus was completed. When unknown words were de-
tected and put into a dictionary, the error rate de-
creased; even so, over 10% of examined morphemes
would be found as errors.
4.2.2 Experiments Using Chunking Models
Results of the morphological analysis of long
words obtained by using a chunking model are
shown in Table 5 and 6. The first and second lines
Table 5: Accuracies of long word segmentation.
Model Recall Precision F
Morph 96.72% (50,095
51,796
) 95.70% (50,095
52,346
) 96.21
Chunk 97.65% (50,580
51,796
) 97.41% (50,580
51,911
) 97.54
Chunk 98.84% (51,193
51,796
) 98.66% (51,193
51,888
) 98.75
Table 6: Accuracies of long word segmentation and
POS tagging.
Model Recall Precision F
Morph 94.71% (49,058
51,796
) 93.72% (49,058
52,346
) 94.21
Chunk 95.59% (49,513
51,796
) 95.38% (49,513
51,911
) 95.49
Chunk 98.56% (51,051
51,796
) 98.39% (51,051
51,888
) 98.47
Chunk w/o TR 92.61% (47,968
51,796
) 92.40% (47,968
51,911
) 92.51
TR : transformation rules
show the respective accuracies obtained when OOVs
were 5.81% and 6.93%. The third lines show the ac-
curacies obtained when we assumed that the OOV
for short words was 0% and there were no errors in
detecting short word segments and their POS cate-
gories. The fourth line in Table 6 shows the accuracy
obtained when a chunking model without transfor-
mation rules was used.
The accuracy obtained by using the chunking
model was one point higher in F-measure than that
obtained by using the morpheme model, and it was
very close to the accuracy achieved for short words.
This result indicates that errors newly produced by
applying a chunking model to the results obtained
for short words were slight, or errors in the results
obtained for short words were amended by apply-
ing the chunking model. This result also shows that
we can achieve good accuracy for long words by ap-
plying a chunking model even if we do not detect
unknown long words and do not put them into a dic-
tionary. If we could improve the accuracy for short
words, the accuracy for long words would be im-
proved also. The third lines in Tables 5 and 6 show
that the accuracy would improve to over 98 points
in F-measure. The fourth line in Tables 6 shows that
transformation rules significantly contributed to im-
proving the accuracy.
Considering the results obtained in this section
and in Section 4.2.1, we are now detecting short and
long word segments and their POS categories in the
whole corpus by using the following steps:
1. Automatically detect and manually examine
unknown words for short words.
2. Improve the accuracy for short words in the
whole corpus by manually examining short
words in ascending order of their probabilities
estimated by a morpheme model.
3. Apply a chunking model to the short words to
detect long word segments and their POS cate-
gories.
As future work, we are planning to use an active
learning method such as that proposed by Argamon-
Engelson and Dagan (Argamon-Engelson and Da-
gan, 1999) to more effectively improve the accuracy
of the whole corpus.
5 Conclusion
This paper described two methods for detecting
word segments and their POS categories in a
Japanese spontaneous speech corpus, and describes
how to tag a large spontaneous speech corpus accu-
rately by using the two methods. The first method is
used to detect any type of word segments. We found
that about 80% of unknown words could be semi-
automatically detected by using this method. The
second method is used when there are several defi-
nitions for word segments and their POS categories,
and when one type of word segments includes an-
other type of word segments. We found that better
accuracy could be achieved by using both methods
than by using only the first method alone.
Two types of word segments, short words and
long words, are found in a large spontaneous speech
corpus, CSJ. We found that the accuracy of auto-
matic morphological analysis for the short words
was 95.79 in F-measure and for long words, 95.49.
Although the OOV for long words was much higher
than that for short words, almost the same accuracy
was achieved for both types of words by using our
proposed methods. We also found that we can ex-
pect more than 99% of precision for short words,
and 97% for long words found in the whole corpus
when we examined 10% of output morphemes in as-
cending order of their probabilities as estimated by
the proposed models.
In our experiments, only the information con-
tained in the corpus was used; however, more appro-
priate linguistic knowledge than that could be used,
such as morphemic and syntactic rules. We would
like to investigate whether such linguistic knowl-
edge contributes to improved accuracy.
References
S. Argamon-Engelson and I. Dagan. 1999. Committee-Based
Sample Selection For Probabilistic Classifiers. Artificial In-
telligence Research, 11:335?360.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
Maximum Entropy Approach to Natural Language Process-
ing. Computational Linguistics, 22(1):39?71.
E. T. Jaynes. 1957. Information Theory and Statistical Me-
chanics. Physical Review, 106:620?630.
E. T. Jaynes. 1979. Where do we Stand on Maximum Entropy?
In R. D. Levine and M. Tribus, editors, The Maximum En-
tropy Formalism, page 15. M. I. T. Press.
H. Kashioka, S. G. Eubank, and E. W. Black. 1997. Decision-
Tree Morphological Analysis Without a Dictionary for
Japanese. In Proceedings of NLPRS, pages 541?544.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Sponta-
neous Speech Corpus of Japanese. In Proceedings of LREC,
pages 947?952.
S. Mori and M. Nagao. 1996. Word Extraction from Cor-
pora and Its Part-of-Speech Estimation Using Distributional
Analysis. In Proceedings of COLING, pages 1119?1122.
M. Nagata. 1999. A Part of Speech Estimation Method for
Japanese Unknown Words Using a Statistical Model of Mor-
phology and Context. In Proceedings of ACL, pages 277?
284.
K. Uchimoto, S. Sekine, and H. Isahara. 2001. The Unknown
Word Problem: a Morphological Analysis of Japanese Using
Maximum Entropy Aided by a Dictionary. In Proceedings
of EMNLP, pages 91?99.
K. Uchimoto, C. Nobata, A. Yamada, S. Sekine, and H. Isahara.
2002. Morphological Analysis of The Spontaneous Speech
Corpus. In Proceedings of COLING, pages 1298?1302.
A Limited-Domain English to Japanese Medical Speech Translator
Built Using REGULUS 2
Manny Rayner
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
mrayner@riacs.edu
Pierrette Bouillon
University of Geneva
TIM/ISSCO,
40, bvd du Pont-d?Arve,
CH-1211 Geneva 4,
Switzerland
pierrette.bouillon@issco.unige.ch
Vol Van Dalsem III
El Camino Hospital
2500 Grant Road
Mountain View, CA 94040
vvandal3@aol.com
Hitoshi Isahara, Kyoko Kanzaki
Communications Research Laboratory
3-5 Hikaridai
Seika-cho, Soraku-gun
Kyoto, Japan 619-0289
{isahara,kanzaki}@crl.go.jp
Beth Ann Hockey
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
bahockey@riacs.edu
Abstract
We argue that verbal patient diagnosis is a
promising application for limited-domain
speech translation, and describe an ar-
chitecture designed for this type of task
which represents a compromise between
principled linguistics-based processing on
the one hand and efficient phrasal transla-
tion on the other. We propose to demon-
strate a prototype system instantiating this
architecture, which has been built on top
of the Open Source REGULUS 2 platform.
The prototype translates spoken yes-no
questions about headache symptoms from
English to Japanese, using a vocabulary of
about 200 words.
1 Introduction and motivation
Language is crucial to medical diagnosis. Dur-
ing the initial evaluation of a patient in an emer-
gency department, obtaining an accurate history of
the chief complaint is of equal importance to the
physical examination. In many parts of the world
there are large recent immigrant populations that re-
quire medical care but are unable to communicate
fluently in the local language. In the US these im-
migrants are especially likely to use emergency fa-
cilities because of insurance issues. In an emer-
gency setting there is acute need for quick accurate
physician-patient communication but this communi-
cation is made substantially more difficult in cases
where there is a language barrier. Our system is
designed to address this problem using spoken ma-
chine translation.
Designing a spoken translation system to obtain
a detailed medical history would be difficult if not
impossible using the current state of the art. The
reason that the use of spoken translation technol-
ogy is feasible is because what is actually needed in
the emergency setting is more limited. Since medi-
cal histories traditionally are obtained through two-
way physician-patient conversations that are mostly
physician initiative, there is a preestablished limiting
structure that we can follow in designing the trans-
lation system. This structure allows a physician to
sucessfully use one way translation to elicit and re-
strict the range of patient responses while still ob-
taining the necessary information.
Another helpful constraint on the conversational
requirements is that the majority of medical condi-
tions can be initiatlly characterized by a relatively
small number of key questions about quality, quan-
tity and duration of symptoms. For example, key
questions about chest pain include intensity, loca-
tion, duration, quality of pain, and factors that in-
crease or decrease the pain. These answers to these
questions can be sucessfully communicated by a
limited number of one or two word responses (e.g.
yes/no, left/right, numbers) or even gestures (e.g.
pointing to an area of the body). This is clearly a
domain in which the constraints of the task are suf-
ficient for a limited domain, one way spoken trans-
lation system to be a useful tool.
2 An architecture for limited-domain
speech translation
The basic philosophy behind the architecture of the
system is to attempt an intelligent compromise be-
tween fixed-phrase translation on one hand (e.g.
(IntegratedWaveTechnologies, 2002)) and linguisti-
cally motivated grammar-based processing on the
other (e.g. VERBMOBIL (Wahlster, 2000) and Spo-
ken Language Translator (Rayner et al, 2000a)).
At run-time, the system behaves essentially like a
phrasal translator which allows some variation in the
input language. This is close in spirit to the approach
used in most normal phrase-books, which typically
allow ?slots? in at least some phrases (?How much
does ? cost??; ?How do I get to ? ??). However,
in order to minimize the overhead associated with
defining and maintaining large sets of phrasal pat-
terns, these patterns are derived from a single large
linguistically motivated unification grammar; thus
the compile-time architecture is that of a linguisti-
cally motivated system. Phrasal translation at run-
time gives us speed and reliability; the linguistically
motivated compile-time architecture makes the sys-
tem easy to extend and modify.
The runtime system comprises three main mod-
ules. These are respectively responsible for source
language speech recognition, including parsing and
production of semantic representation; transfer and
generation; and synthesis of target language speech.
The speech processing modules (recognition and
synthesis) are implemented on top of the standard
Nuance Toolkit platform (Nuance, 2003). Recogni-
tion is constrained by a CFG language model written
in Nuance Grammar Specification Language (GSL),
which also specifies the semantic representations
produced. This language model is compiled from
a linguistically motivated unification grammar us-
ing the Open Source REGULUS 2 platform (Rayner
et al, 2003; Regulus, 2003); the compilation pro-
cess is driven by a small corpus of examples. The
language processing modules (transfer and genera-
tion) are a suite of simple routines written in SICStus
Prolog. The speech and language processing mod-
ules communicate with each other through a mini-
mal file-based protocol.
The semantic representations on both the source
and target sides are expressed as attribute-value
structures. In accordance with the generally mini-
malistic design philosophy of the project, semantic
representations have been kept as simple as possi-
ble. The basic principle is that the representation of
a clause is a flat list of attribute-value pairs: thus for
example the representation of ?Did your headache
start suddenly?? is the attribute-value list
[[utterance_type,ynq],[tense,past],
[symptom,headache],[state,start],
[manner,suddenly]]
In a broad domain, it is of course trivial to con-
struct examples where this kind of representation
runs into serious problems. In the very narrow do-
main of a phrasebook translator, it has many desir-
able properties. In particular, operations on semantic
representations typically manipulate lists rather than
trees. In a broad domain, we would pay a heavy
price: the lack of structure in the semantic represen-
tations would often make them ambiguous. The very
simple ontology of the phrasebook domain however
means that ambiguity is not a problem; the compo-
nents of a flat list representation can never be de-
rived from more than one functional structure, so
this structure does not need to be explicitly present.
Transfer rules define mappings of sets of attribute-
value pairs to sets of attribute-value pairs; the ma-
jority of the rules map single attribute-value pairs
to single attribute-value pairs. Generation is han-
dled by a small Definite Clause Grammar (DCG),
which converts attribute-value structures into sur-
face strings; its output is passed through a minimal
post-transfer component, which applies a set of rules
which map fixed strings to fixed strings. Speech syn-
thesis is performed either by the Nuance Vocalizer
TTS engine or by concatenation of recorded wave-
files, depending on the output language.
One of the most important questions for a med-
ical translation system is that of reliability; we ad-
dress this issue using the methods of (Rayner and
Bouillon, 2002). The GSL form of the recognition
grammar is run in generation mode using the Nu-
ance generate utility to generate large numbers
of random utterances, all of which are by construc-
tion within system coverage. These utterances are
then processed through the system in batch mode us-
ing all-solutions versions of the relevant processing
algorithms. The results are checked automatically
to find examples where rules are either deficient or
ambiguous. With domains of the complexity under
consideration here, we have found that it is feasible
to refine the rule-sets in this way so that holes and
ambiguities are effectively eliminated.
3 A medical speech translation system
We have built a prototype medical speech transla-
tion system instantiating the functionality outlined
in Section 1 and the architecture of Section 2. The
system permits spoken English input of constrained
yes/no questions about the symptoms of headaches,
using a vocabulary of about 200 words. This is
enough to support most of the standard examina-
tion questions for this subdomain. There are two
versions of the system, producing spoken output in
French and Japanese respectively. Since English ?
Japanese is distinctly the more interesting and chal-
lenging language pair, we will focus on this version.
Speech recognition and source language analy-
sis are performed using REGULUS 2. The grammar
is specialised from the large domain-independent
grammar using the methods sketched in Section 2.
The training corpus has been constructed by hand
from an initial corpus supplied by a medical pro-
fessional; the content of the questions was kept un-
changed, but where necessary the form was revised
to make it more appropriate to a spoken dialogue.
When we felt that it would be difficult to remem-
ber what the canonical form of a question would
be, we added two or three variant forms. For exam-
ple, we permit ?Does bright light make the headache
worse?? as a variant for ?Is the headache aggra-
vated by bright light??, and ?Do you usually have
headaches in the morning?? as a variant for ?Does
the headache usually occur in the morning??. The
current training corpus contains about 200 exam-
ples.
The granularity of the phrasal rules learned by
grammar specialisation has been set so that the con-
stituents in the acquired rules are VBARs, post-
modifier groups, NPs and lexical items. VBARs
may include both inverted subject NPs and adverbs1.
Thus for example the training example ?Are the
headaches usually caused by emotional upset?? in-
duces a top-level rule whose context-free skeleton is
UTT --> VBAR, VBAR, POSTMODS
For the training example, the first VBAR in the in-
duced rule spans the phrase ?are the headaches usu-
ally?, the second VBAR spans the phrase ?caused?,
and the POSTMODS span the phrase ?by emotional
upset?. The same rule could potentially be used to
cover utterances like ?Is the pain sometimes pre-
ceded by nausea?? and ?Is your headache ever as-
sociated with blurred vision??. The same training
example will also induce several lower-level rules,
the least trivial of which are rules for VBAR and
POSTMODS with context-free skeletons
VBAR --> are, NP, ADV
POSTMODS --> P, NP
The grammar specialisation method is described in
full detail in (Rayner et al, 2000b).
With regard to the transfer component, we have
had two main problems to solve. Firstly, it is well-
known that translation from English to Japanese re-
quires major reorganisation of the syntactic form.
Word-order is nearly always completely different,
and category mismatches are very common. It is
mainly for this reason that we chose to use a flat
semantic representation. As long as the domain is
simple enough that the flat representations are un-
ambiguous, transfer can be carried out by mapping
lists of elements into lists of elements. For example,
we translate ?are your headaches caused by fatigue?
as ?tsukare de zutsu ga okorimasu ka? (lit. ?fatigue-
CAUSAL headache-SUBJ occur-PRESENT QUES-
TION?). Here, the source-language representation is
[[utterance_type,ynq],
[tense,present],
[symptom,headache],
[event,cause],
[cause,fatigue]]
and the target-language one is
[[utterance_type,sentence],
[tense,present],
[symptom,zutsu],
1This non-standard definition of VBAR has technical advan-
tages discussed in (Rayner et al, 2000c)
do your headaches often appear at night ?
yoku yoru ni zutsu ga arimasu ka
(often night-AT headache-SUBJ is-PRES-Q)
is the pain in the front of the head ?
itami wa atama no mae no hou desu ka
(pain-TOPIC head-OF front side is-PRES-Q)
did your headache start suddenly ?
zutsu wa totsuzen hajimari mashita ka
(headache-TOPIC sudden start-PRES-Q)
have you had headaches for weeks ?
sushukan zutsu ga tsuzuite imasu ka
(weeks headache-SUBJ have-CONT-PRES-Q)
is the pain usually superficial ?
itsumo itami wa hyomenteki desu ka
(usually pain-SUBJ superficial is-PRES-Q)
is the severity of the headaches increasing ?
zutsu wa hidoku natte imasu ka
(headache-TOPIC severe becoming is-PRES-Q)
Table 1: Examples of utterances covered by the pro-
totype
[event,okoru],[postpos,causal],
[cause,tsukare]]
Each line in the source representation maps into the
corresponding one in the target in the obvious way.
The target-language grammar is constrained enough
that there is only one Japanese sentence which can
be generated from the given representation.
The second major problem for transfer relates to
elliptical utterances. These are very important due
to the one-way character of the interaction: instead
of being able to ask a WH-question (?What does
the pain feel like??), the doctor needs to ask a se-
ries of Y-N questions (?Is the pain dull??, ?Is the
pain burning??, ?Is the pain aching??, etc). We
rapidly found that it was much more natural for
questions after the first one to be phrased ellipti-
cally (?Is the pain dull??, ?Burning??, ?Aching??).
English and Japanese have however different con-
ventions as to what types of phrase can be used
elliptically. Here, for example, it is only pos-
sible to allow some types of Japanese adjectives
to stand alone. Thus we can grammatically and
semantically say ?hageshii desu ka? (lit. ?burn-
ing is-QUESTION?) but not ?*uzukuyona desu
ka? (lit. ?*aching is-QUESTION?). The prob-
lem is that adjectives like ?uzukuyona? must com-
bine adnominally with a noun in this context:
thus we in fact have to generate ?uzukuyona itami
desu ka? (?aching-ADNOMINAL-USAGE pain is-
QUESTION?). Once again, however, the very lim-
ited domain makes it practical to solve the problem
robustly. There are only a handful of transforma-
tions to be implemented, and the extra information
that needs to be added is always clear from the sortal
types of the semantic elements in the target represen-
tation.
Table 1 gives examples of utterances covered by
the system, and the translations produced.
References
IntegratedWaveTechnologies, 2002. http://www.i-w-
t.com/investor.html. As of 15 Mar 2002.
Nuance, 2003. http://www.nuance.com. As of 25 Febru-
ary 2003.
M. Rayner and P. Bouillon. 2002. A phrasebook style
medical speech translator. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors. 2000a. The Spoken Language
Translator. Cambridge University Press.
M. Rayner, D. Carter, and C. Samuelsson. 2000b. Gram-
mar specialisation. In Rayner et al (Rayner et al,
2000a).
M. Rayner, B.A. Hockey, and F. James. 2000c. Compil-
ing language models from a linguistically motivated
unification grammar. In Proceedings of the Eighteenth
International Conference on Computational Linguis-
tics, Saarbrucken, Germany.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2003. http://sourceforge.net/projects/regulus/.
As of 24 April 2003.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
Extraction and Verification of KO-OU Expressions from Large Corpora 
Atsuko kida?,  Eiko Yamamoto?,  Kyoko Kanzaki?,  and  Hitoshi Isahara? 
?The Institute of Behavioral Sciences                       ?Communications Research Laboratory 
2-9 Honmura-cho, Ichigaya, Shinjuku-ku,                 3-5 Hikari-dai, Seika-cho, Souraku-gun, 
Tokyo, 162-0845, Japan                                              Kyoto, 619-0289, Japan 
akida@ibs.or.jp {eiko,kanzaki,isahara}@crl.go.jp
 
 
 
Abstract 
In the Japanese language, as a predicate is 
placed at the end of a sentence, the con-
tent of a sentence cannot be inferred until 
reaching the end. However, when the con-
tent is complicated and the sentence is 
long, people want to know at an earlier 
stage in the sentence whether the content 
is negative, affirmative, or interrogative. 
In Japanese, the grammatical form called 
the KO-OU relation exists. The KO-OU 
relation is a kind of concord. If a KO ele-
ment appears, then an OU element ap-
pears in the latter part of a sentence. It is 
being pointed out that the KO-OU relation 
gives advance notice to the element that 
appears in the latter part of a sentence. In 
this paper, we present the method of ex-
tracting automatically the KO-OU expres-
sion data from large-scale electronic 
corpus and verify the usefulness of the 
KO-OU expression data. 
1 Introduction 
The Japanese language has a grammatical form 
called the KO-OU relation. The KO-OU relation is 
a kind of concord, also referring to a sort of bound 
relation that a KO element appearing in a sentence 
is followed by an OU element in the latter part of 
the same sentence. On the contrary, the cooccur-
rence relation refers to two words appearing in the 
same sentence. 
Because Japanese predicates are usually located 
at the end of sentences, the contents of Japanese 
sentences cannot be decided until reaching the end. 
Furthermore, in Japanese, it is hard to comprehend 
the meaning of the sentence without reading 
through the entire sentence. The KO-OU relation is 
the grammatical form which can be helpful for un-
derstanding the sentence meaning at the early stage. 
While in archaic Japanese, KAKARI-MUSUBI, 
which had morphemic KO-OU relation between 
KAKARI-JOSI1 and the conjugation at the end of a 
sentence, had been used. KAKARI-MUSUBI gave 
advance notice to the elements that would appear 
toward the end of a sentence due to KAKARI-JOSI. 
Today, KAKARI-MUSUBI has dropped out of use. 
However, the KO-OU relation such as "sika-nai 
(only)" or "kessite-nai (never)" is present. In this 
research, we have attempted to collect such ele-
ments to extract KO-OU expression data. In this 
paper, the main points of argument are as follows: 
(1) Method of extracting automatically the KO-OU 
expression data. 
(2) What the KO-OU expression data can be used 
for. 
2 The Previous Works and How to Posi-
tion this Study 
(Ohno, 1993) pointed out that there were expres-
sions that try to give advance notice to whether a 
sentence is affirmative, negative, or interrogative at 
the early stage of a language expression which 
continues timewise. It suggested that there were 
certain adverbs that have replaced KAKARI-JOSI 
in the archaic Japanese words. 
(Masuoka, 1991) described the KO-OU relation 
of sentence elements. According to Masuoka, some 
sentences have the KO-OU expressions as shown 
in Table 1.  
However, this has the following weaknesses. 
The KO and OU elements in a KO-OU relation are 
placed together in the same category, and there is 
                                                          
1 A Japanese particle. 
no description as to the OU element. Furthermore, 
only a limited number of elements are listed. And 
the objectivity of the KO and OU elements is not 
guaranteed. 
The KO-OU expression data is useful as basic 
data to dissolve ambiguity in parsing and to decide 
on the modification relation. However, first of all, 
it is necessary for the data to have a certain length 
for being useful basic data. Secondly, it also needs 
to be objective. Therefore, we have attempted to 
extract KO-OU relations automatically from large-
scale corpus. 
 
Table 1 Masuoka?s KO-OU expression data 
3 Assumed Usage of KO-OU Expression 
Data 
3.1 To Dissolve Ambiguity 
The KO-OU expression data is useful for dissolv-
ing ambiguity of parsing. Furthermore, it is useful 
for deciding the modification relation (Figure 1). 
3.2 Gradual Understanding 
Using the KO-OU expression data will enable the 
reader to guess the end expression midway through 
a sentence. This is because as the KO elements 
appear it is possible to predict the appearance of 
the OU elements (Figure 2). It can be used as a 
basic data for understanding sentences. In addition, 
this technology can be used to guess the point in 
the minutes of a meeting at which the speakers 
change. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 To Dissolve Ambiguity 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2 Gradual Understanding 
 
KO element OU element 
Nee,  oi te-kudasai,  naa 
tabun,  doumo daro-u,  rasii,  you-da 
kessite,  kanarazu-si-mo nai 
conviction  
(If you chew it, you will certainly taste salmon.) 
                                                                          ??? 
 
??????????  ?? ?  ?  ?  ??     ?????  ? 
kamisimereba        kitto    sake  no  aji  ga   suru    ni-chiga-inai 
 
 
 
 
 
 
 
 
 
 
??????????  ?? ?  ?  ?  ??     ?????  ? 
kamisimereba        kitto    sake  no  aji  ga   suru    ni-chiga-inai 
 
To Dissolve 
Ambiguity  refer
KO element  OU element    Similarity score   Distance        Meaning 
 
kitto               ni-chigai-nai    0.004726           6.062697       conviction 
kitto               koto-daro-u      0.00418           11.297666       guess 
kitto hazu 0.003722 12.702345 conviction
KO-OU relation data (image) 
(Works that you see at the open seaside should look attractive.) 
 
???  ?? ? ?? ?? ?  ??? ???  ? ?? ?? ?? 
koudaina umibe de miru sakuhin wa   kitto  miryokuteki ni utsuru hazu  da 
 
 
? ? ? ? ? ? ? ? ?  ? ?  Gradual Understanding 
 
 
 
 
 
 
 
 
KO element  OU element    Similarity score   Distance        Meaning 
 
kitto               ni-chigai-nai    0.004726           6.062697       conviction
kitto               koto-daro-u      0.00418           11.297666       guess 
kitto               hazu               0.003722         12.702345     conviction
refer guess ? conviction ? 
 KO-OU relation data (image) 
4 Extraction of KO-OU Expression Data 
4.1 Method 
(Yamamoto and Umemura, 2002) considered the 
estimation of the one-to-many relation between 
entities in corpus. They carried out experiments 
on extracting one-to-many relation of phenomena 
from corpus using complementary similarity 
measure (CSM) which can cope very well with 
inclusion relation of appearance patterns. The KO-
OU relation in this research can be regarded as a 
type of one-to-many relation.  
4.2 Data Used 
In this paper, we dealt with what is called FUKU-
JOSI2, KAKARI-JOSI, and some adverbs shown 
below. We proceeded on the assumption that these 
are the KO elements in the KO-OU relation. For 
our research, we used newspaper articles from the 
Mainichi Shimbun, Nihon Keizai Shimbun, and 
Yomiuri Shimbun issued between 1991 and 2000. 
 
[Target words] 
koso, sika, sae, ha, mo, bakari, nomi, sura, nara, 
kurai, dake, nannte, kessite, osoraku, tabun, zehi, 
marude, mosi, kitto 
 
Figure 3 Process flow 
4.3 Process Flow 
Process flow is shown in Figure 3. 
(1) We calculated the similarity measure using 
CSM for newspaper articles data that had been 
morphologically analyzed with ChaSen3. 
(2) We extracted pairs containing the target words 
from the results of similarity measure calculation. 
                                                          
2 A Japanese particle. 
3 Morphological Analyzer ChaSen. See http://chasen.aist-
nara.ac.jp/. 
(3) Out of the pairs in (2), we extracted words that 
appeared in the order of KO and OU elements. 
(We judge the pairs based on this word order.) 
(4) We carried out judgment based on reliability. 
As a result of this process, we obtained 14 pairs 
of data which had "kesshite" as KO element, 16 
which had "sae," and 23 which had "wa." Data of 
approximately 20 pairs was obtained per target 
word.  
5 Verification of KO-OU Expression 
Data 
5.1 Necessity to Give Meaning/Information 
If the KO-OU expression data is used for gradual 
understanding of sentences, it was necessary for 
the data to be given meaning/information. When 
the KO element appears, it will be possible to suf-
ficiently grasp or guess the contents of a sentence 
by referring the KO-OU expression data (Figure2). 
However it is difficult to give mean-
ing/information using the data obtained from the 
process in Chapter 4 because the data is broken 
down into each morpheme by the morphological 
analysis, and each element is too short.  
In Japanese sentences, there are many cases in 
which continuation of a particle and an auxiliary 
verb builds a predicate. This continuation plays an 
important role in determining the event of the sen-
tence. Particles and auxiliary verbs are functional 
words. Therefore, it is not possible to determine 
the meaning of some of the particles and auxiliary 
verbs when they appeared independently. Fur-
thermore, there are some cases in which they 
change their meaning when paired with another 
word.  
Table 2 shows the OU elements obtained pursu-
ant to the procedure in Chapter 4 for KO element 
"kitto". "Da" listed in Table 2 has an assertive 
meaning when used in a sentence like "kyou wa 
ame da . (It is raining today.)" On the other hand, it 
has an inferential meaning in the context of "asu 
wa hareru daro-u . (It should be fine tomorrow.)" In 
addition, although "nai" is a negative auxiliary 
verb, when it is paired as in "ka-mo-shire-nai (may 
be)" and "chigai-nai (must be)," the negative mean-
ing disappears. And the overall pairing stands for 
guess and conviction. 
 
 
morphologically  
analyzed data
(1) Calculate similarity 
KO-OU expression data
(2) Extract pairs 
(3) Judgment based on word
(4) Judgment based on reliability. 
Table 2  KO-OU expression data 
KO element OU element KO element OU element 
Kitto u (auxiliary) kitto yo (particle) 
kitto da (auxiliary) kitto chigai (noun) 
kitto to (particle) kitto ka (particle) 
kitto omou (verb) kitto Ne (particle) 
kitto nai (auxiliary) kitto you (noun) 
kitto hazu (noun) : : 
5.2 Verification of OU Element Using 
"Kitto" 
In this section, we carry out an analytical example 
using OU element for KO element "kitto (cer-
tainly)." We can classify the OU elements obtained 
from the procedure in Chapter 4, as follows: 
(a) It can be an OU element by itself,  
(b) It can become an OU element when paired 
with others,  
(c) It does not have the possibility of becoming an 
OU element. 
Words of (c) were not found in the OU ele-
ments obtained for KO element "kitto." In the fol-
lowing, we will describe the details on (a) and (b). 
 (a) OU element by itself 
Out of the OU elements for KO element "kitto" in 
Table 2, "hazu" can be an OU element by itself. 
 
[1] koudaina umibe de miru sakuhin wa kitto miryokuteki 
ni utsuru hazu da . 
(Works that you see at the open seaside should look attractive.) 
 
This is the only sentence with an independent 
OU element for "kitto" in the data obtained from 
the process in Chapter 4. The same can be said of 
data for KO elements other than "kitto." Because 
of morphological analysis, the row of letters has 
been shortened. As a result, there are few ele-
ments that can be regarded as an OU element by 
itself. And just looking at this element does not 
determine the meaning. 
 (b) OU element when paired with others 
When "chigai" is paired with "ni" and "nai" to 
make "ni-chigai-nai (must be)," it becomes an OU 
element. Similarly, pairing "da" with "u" results in 
an OU element "daro-u (perhaps)." "Da" is the 
original form of "daro" and becomes "daro-u" 
when paired with "u." 
 
[2] kitto kintyou suru daro-u . 
(It is certain that one will be nervous.) 
[3] kamisimereba , kitto sake no aji ga suru ni-chiga-inai . 
(If you chew it, you will certainly taste salmon.) 
 
If we look over the entire pairing shown above, 
we can give meaning to such guess and conviction. 
6 Questions for the Future 
As we described in Chapter 5, it is necessary to 
pair multiple elements before giving mean-
ing/information. We currently persuade the issue 
of automatic generation of pairing multiple ele-
ments. Now, we are carrying out experiments on 
calculating the similarity measure of pairing of 
elements. These will give us pairing of automati-
cally generated elements and the similarity meas-
ure of the pairings. This should be useful data for 
resolving ambiguity (Figure 1).  
7 Conclusion 
This paper presented the process of extracting 
KO-OU expression data using CSM and the use-
fulness of the extracted KO-OU expression data. 
We are planning to report on the findings of ex-
periments on automatic generation of OU ele-
ments pairings.  
 
Acknowledgments To compile this paper, we used 
newspaper articles from The Mainichi Newspapers, 
The Yomiuri Shimbun, and Nihon Keizai Shimbun.  
We would like to sincerely thank Dr. M. Utiyama 
of the Communications Research Laboratory for allow-
ing us to use a KWIC tool "tea4." 
References 
A.Kida, E.Yamamoto and H.Isahara. 2002. Analysis of 
expression which projects the following elements 
beforehand. IPSJ SIG Notes NL-152, pp.137-143. 
A.Kida, E.Yamamoto, K.Kanzaki and H.Isahara. 2003. 
The key on the syntax which brings forth a concord 
relation. Proceedings of the 9th Annual Meeting of 
the Association for NLP. pp.23-26.  
T.Masuoka. 1991. Grammar of modality. Kurosio-
syuppan. 
S.Ohno. 1993. Research of a KAKARI-MUSUBI. Iwa-
nami-Shoten. 
E.Yamamoto and K.Umemura. 2002. A similarity 
Measure for Estimation of One-to-Many Relation-
ship in Corpus. Jourmal of Natural Lamguage Proc-
essing. Vol.9 No.2. pp.45-75. 
                                                          
4 See http://www2.crl.go.jp/jt/a132/members/mutiyama/ 
software.html. 
Similarities and Differences among Semantic Behaviors of 
Japanese Adnominal Constituents 
Kyoko Kanzaki and Qing Ma and Hitoshi Isahara 
Communications Research Laboratory 
588-2, Iwaoka, Iwaoka-cho, Nishi-ku, Kobe, 651-2492, Japan 
{kanzakilqma\[isahara} @crl.go.jp 
Abst rac t  
This paper treats the classification of the se- 
mantic functions performed by adnominal con- 
stituents in Japanese, where many parts of 
speech act as adnominal constituents. In order 
to establish a formal treatment of the semantic 
roles, the similarities and differences among ad- 
nominal constituents, i.e. adjectives and "noun 
+ NO (in English "of + noun")" structures, 
which have a broad range of semantic func- 
tions, are discussed. This paper also proposes 
an objective method of classifying these con- 
structs using a large amount of linguistic data. 
The feasibility of this was verified with a self- 
organizing semantic map based on a neural net- 
work model. 
1 In t roduct ion  
Pustejovsky (Pustejovsky, 1995) proposed the 
theory of a generative l xicon as a framework by 
which meanings of words are expressed in one 
unified representation. This kind ofgenerativity 
would be very useful for NLP, especially if it is 
applicable to the complex semantic structures 
represented by various modification relations. 
In our previous research on adjectives (Isahara 
and Kanzaki, 1999) we used Pustejovsky's the- 
ory to classify adjectives in Japanese. In this pa- 
per we take the first steps in a similar classifica- 
tion of the Japanese "noun + NO" construction. 
Bouillon (Bouillon, 1996) applied this theory 
to the adnominal constituent of mental states. 
Saint-Dizier (Saint-Dizier, 1998) discussed ad- 
jectives in French. 
Isahara and Kanzaki (Isahara and Kanzaki, 
1999) treated a much wider range of phenom- 
ena of adnominal constituents. They classified 
the semantic roles of adnominal constituents 
in .Japanese. where many parts of speech act 
as adnominal constituents, and discussed a for- 
mal treatment of their semantic roles. In their 
research, adnominal constituents, mainly ad- 
jectives which function as adverbials, are dis- 
cussed. The present paper describes the sim- 
ilarities and differences among adnominal con- 
stituents, i.e. adjectives and "noun + NO t (in 
English "of + noun")" structures which have 
a broad range of semantic functions. This pa- 
per proposes an objective method for classifying 
these structures using a large amount of linguis- 
tic data. The feasibility of this was verified with 
a self-organizing semantic map based on a neu- 
ral network model. 
In section 2, we explain the semantic func- 
tions performed by "noun + NO." In section 
3, we discuss how we can semi-automatically 
obtain and classify examples of adjectives and 
"noun + NO" structures which have similar se- 
mantic functions. In section 4, we introduce a
self-organizing semantic map to verify the result 
of this classification. In section 5, we discuss 
similarities and differences between adjectives 
and "noun + NO" structures. 
2 The  D ivers i ty  o f  Semant ic  
Re la t ions  between "noun -t- NO"  
and  the i r  Head Nouns  
Among Japanese adnominal constituents, "
noun + NO" represents a wider range of seman- 
tic relations than other adnominal constituents. 
Therefore, "noun + NO" does not always be- 
have like the other adnominal constituents. In 
previous work, some researchers have analyzed 
semantic relations between the noun in the 
"noun + NO" structure and its head noun (Shi- 
mazu et al, 1986). Here, we show several ex- 
amples that demonstrate he diversity of the se- 
l "NO" is a Japanese postpositiona| which can repre- 
sent a wide range of semantic relations. It is similar to 
"of" in English. 
59 
mantic relation between "noun + NO" struc- 
tures and their head nouns shown in their re- 
search. 
DENWA NO SECCHI 
DENSHA NO TUUKIN 
ASHITA NO DEITO 
BILU NO MAE 
KODOMO NO NAMAE 
BAKUHATSU NO GEN'IN 
KAISHI NO JIKOKU 
HEYA NO BANGOU 
KANOJO NO NOUTO 
BENGOSHI NO SMITH SAN 
installation of 
the telephone 
commuting by 
train 
a date for 
tomorrow 
in front of 
the building 
the name of 
the child 
the cause of 
the explosion 
the starting time 
the number of 
the room 
her note 
Mr. Smith, 
the lawyer 
These semantic relations between "noun + 
NO" structures and their head nouns are dif- 
ferent than those between other adnominal con- 
stituents, e.g. adjectives and their head nouns. 
However, some "noun + NO" behavior is sim- 
ilar to the behavior of adjectives and nominal 
adjectivals. In these cases "noun + NO" seems 
not to differ semantically from adjectives and 
nominal adjectivals. Let us consider the English 
examples: 
financial world / world of finance ("ZAIKAI") 
industrial center / center of industry 
("SANGYOU NO CHUUSHIN") 
In this case "noun + NO" need not be dis- 
tinguished from an adjective with respect o se- 
mantic behavior. However, in the following ex- 
amples it is necessary to distinguish them from 
one another. 
global center / center of tile globe 
("SEKAI NO CHUUSHIN 
/ CHIKYUU NO CHUUSHIN") 
We do not have a discrimination criteria that 
automatically recognizes whether a "noun + 
NO" structure is similar in its semantic behav- 
ior to that of adjectives or not. We have at- 
tempted to gather, semi-automatically, nolms in 
the "n(mn + NO" structure which behave like 
adjectives. 
3 The  Exp lorat ion  o f  the  S imi la r i t ies  
o f  Semant ic  Funct ions  o f  "noun + 
NO"  St ructures  and  Ad jec t ives .  
(The  Method  for th is  Research)  
3.1 The Basic Concept  
There is one case in which the meanings of ad- 
nominal constituents are semantically similar 
to the features of the referents of their head 
nouns, e.g. adnominal constituents represent 
the concrete contents of their head nouns. Let 
us consider the Japanese phrase "KANASHII 
KIMOCHI (sad feeling)" and "YOROKOBI NO 
KIMOCHI (feeling of delight)" as examples. 
KANASHII KIMOCHI 
adjective noun 
(sad) (feeling) 
sad feeling 
YOROKOBI NO KIMOCHI 
noun postp, noun 
(delight) (of) (feeling) 
feeling of delight 
NB: The English gloss of the "noun + NO" 
examples hould be read from right to left. 
One meaning of "KIMOCHI (feeling)" repre- 
sents the semantic element <mental state>. In 
the above examples, the adjective, "KANASHII 
(sad)", and "noun + NO", "YOROKOBI NO 
(of delight)", represent he concrete contents 
of their head noun "KIMOCHI (feeling)", i.e. 
they also represent the mental state: "feeling". 
Therefore, even though they belong to different 
parts of speech (adjective/noun), they must be 
classified in the same semantic category since 
both carry the same meaning. Neither the ad- 
jective, "KANASHII (sad)", nor the "noun + 
NO", "YOROKOBI NO (of delight)", can ap- 
pear in predicative position without changing 
their meaning. 
However, if adnominal constituents do not 
share the same semantic oncept as their head 
noun, they cannot represent he contents of 
head nouns. The examples below demonstrate 
this. 
KANASHII KIMOCHI 
adje(:tive noun 
(sad) (feeling) 
JOHN NO KIMOCHI 
noun postp, noun 
(John) (of) (feeling) 
John's feeling 
60 
In the above examples, the noun in "noun + 
NO", "JOHN", does not include the concept, 
<mental state>, so it cannot represent the con- 
tent of "KIMOCHI (feeling)." The adjective, 
"KANASHII (sad)", and the noun in the "noun 
+ NO", "JOHN" do not embody the same con- 
cept and have a different semantic relation with 
their head noun. We cannot find the seman- 
tic similarities between "KANASHII (sad)" and 
"JOHN" that we could between "YOROKOBI 
NO (of delight)" and "KANASHII (sad)." We 
focus on the phenomena where adnominal con- 
stituents represent the concrete contents of their 
head nouns. This makes it possible to identify 
adjectives and "noun + NO" structures which 
are similar in semantic behavior to the referents 
of their head nouns. These expressions are ex- 
tracted semi-automatically from large corpora. 
3.2 How to Ext ract  the Necessary 
In format ion  
When we collect words which have some sim- 
ilarities, it is difficult to select the semantic 
axis for classification by making use of only 
the co-occurring words. In collecting similar 
words, some previous research took not only co- 
occurring words but also the context of these 
words into account (Grefenstette, 1994). One 
of the important points of our analysis is the 
introduction of the distinct semantic elements 
that both "noun + NO" structures and adjecti- 
vals (adjectives and nominals) have in common 
with their head nouns. We wanted to ascertain 
the similarities between "noun + NO" and other 
adnominal constituents based on these common 
semantic elements. For this reason, we used 
the semantic relations, in which adnominal con- 
stituents represent the concrete content of their 
head nouns, as a key to classification. We au- 
tomatically 2 extracted these relations from one 
year of newspaper articles from Mainichi Shim- 
bun (1994), 100 novels from Shincho publishers 
and 100 books covering a variety of topics. We 
used the following procedure to extract he nec- 
essary information. 
Step 1) Extract from the corpora, all nouns 
which are preceded by the Japanese xpression 
"TOIU" which is something like "that" or "of." 
"TOIU + noun (noun that/of ...)" is a typical 
,Japanese xpression which introduces ome in- 
2Only Step 3) is done manually. 
formation about the referent of the noun, such 
as apposition. Therefore, nouns found in this 
pattern may have their content elucidated by 
means of their modifiers. 
Step 2) Extract from the corpora, all "noun 
+ NO" structures, adjectives and nominal ad- 
jectivals which modify the nouns extracted in 
step 1. 
NB, the relationships between adnominal 
constituents and their modified nouns extracted 
here include not only representations of the con- 
tents of the noun, but also other various rela- 
tions. 
Step 3) Extract "noun + NO" structures, ad- 
jectives and nominal adjectivals which represent 
the contents of the referents of the modified 
nouns. Step 3 is done manually. 
Step 4) In order to find the distribution of 
their semantic ategories and analyze the se- 
mantic similarities between "noun + NO" and 
other adnominal constituents in each semantic 
category, we clustered the modified nouns auto- 
matically. This clustering was based on sets of 
similar adnominal constituents which represent 
the content of the referent of the modified noun. 
4 The  Semant ic  Map of  the  
Mod i f ied  Nouns  Const ructed  by  
the Self-Organizing System of the 
Neural Network Model 
We can gather similar modified nouns when we 
classify the modified nouns according to the 
similarities of the adnominal constituents, be- 
cause in our data both adnominal constituents 
and their modified nouns have the same se- 
mantic elements in common that we mentioned 
above. 
We attempted toconstruct the Semantic Map 
of the modified nouns gathered by the above- 
mentioned method by using the self-organizing 
system of the neural network model (Ma et al, 
2000). We suppose that both modified nouns 
and adnominal constituents have common se- 
rnantic elements when adnominal constituents 
represent he concrete content of their head 
nouns. If this is true, nouns with similar mean- 
ings are located near each other oil the semantic 
map, self-organized by the similarities of seman- 
tic elements among the adnominal constituents. 
The result of our experiment verified this sup- 
position (Figure I). The nouns with a similar 
61 
meaning are located near each other on the map 
and we could divide the distribution of the mod- 
ified nouns into seven categories (Figure 2). 
Each group, i.e. the "mental state" 
group, "state/ situation" group, "characteris- 
tics" group, "range/ area" group, "viewpoint/ 
standpoint" group, "aspect" group, and "oth- 
ers," represents a meaning held in common by 
nouns in the group. Mental state can be fur- 
ther divided into the state of emotion, mood 
and intention. As we analyze the adnominal 
constituents in each category of modified nouns, 
we can find the possibility of the co-occurrence 
of an adnominal constituent with a head noun. 
Table 1 shows examples of adjectives and nouns 
in "noun + NO" structures in each group. 
UT~A 
? 
Table 1: List of adjectives and "noun + NO" 
Structures 
<menta l  state: emot ion> 
Adj: KANASHII (sad), URESHII 
(pleasurable) 
noun+no: KANASHIMI (sadness), 
YOROKOBI (delight) 
<state /s i tuat ion> 
Adj: ISOGASHII (busy), 
MUTITUJONA (disorderly) 
noun+no: KURAYAMI (darkness), 
MUISHIKI (unconscious) 
<aspect> 
Adj: YUUMOUNA (brave), 
HIGEKITEKINA (tragic) 
noun+no: KONTON (chaos), TAIHAI 
(decadence) 
<character i s t i c> 
Adj: NONKINA (carefree), 
KISAKUNA (open-hearted) 
noun+no: I J IPPARI (stubbornness), 
GOUMANNA (arrogance) 
<range/area> 
Adj: JOUSHIKITEKINA (comnmnsense), 
KOUTEKINA (official) 
noun+no: GAKUMON (studies), GYOUMU 
(duty) 
<v iewpo int / s tandpo in t> 
Adj: KYOUIKUTEKINA (educational), 
SHOUGYOUTEKINA (economic) 
noun+no: KYOUIKU (education), EISEI 
(hygiene) 
Figure 1: Semantic Map 1 
Figure 2: Semantic Map 2 
In the mental state, state/situation, aspect 
and characteristics groups~ adjectives appear 
more frequently than "noun + NO" construc- 
tions. These are simple adjectives. Ill the 
range/area nd viewpoint/standpoint groups, 
62 
"noun + NO" structures appear more fre- 
quently than simple adjectives. Nominal adjec- 
tivals derived from nouns plus the suffix "TEKI- 
na" appear often with these noun groups. Most 
nouns in the groups "mental state: emotion", 
"state/situation" and "characteristics", contain 
abstract nouns which represent emotions, situa- 
tions or characteristics. There are few concrete 
nouns. However, in the groups "range/area" 
and "viewpoint/standpoint', here are many 
concrete nouns which represent natural phe- 
nomena, organizations or professional domains 
and few abstract nouns. We can find differences 
among "noun + NO" structures, that is, there 
are adjectives which behave like nouns semanti- 
cally and there are nouns which behave seman- 
tically like adjectives. 
5 The  semant ic  behav ior  o f  the  
"noun -t- NO"  s t ruc ture  wh ich  is 
s imi la r  to  that  o f  ad jec t ives  
5.1 Types  of nouns in the "noun -'t- 
NO"  s t ruc ture  
As we mentioned in section 3, we extracted the 
"noun + NO" structures which have the same 
semantic element, along with similar adjectives, 
from large corpora. For example, 
KIKEN_NA JOUTAI 
(dangerous) (situation) 
dangerous ituation 
In this case "dangerous" represents the state 
concretely. 
MIKETTEI NO JOUTAI 
(indecision) (of) (situation) 
a situation of indecision 
In this case, the "MIKETTEI NO (of in- 
decision)" also represents the state concretely. 
Here, both "KIKENN_NA (dangerous)" and 
"MIKETTEI NO (of indecision)" have tile same 
semantic element "state" in common. We find 
that a "situation" can be represented by both 
an adjective and the "noun + NO" structure. 
When "MIKETTEI NO (of indecision)" co- 
occurs with modified nouns other than "situa- 
tion", it mostly represents the semantic notion, 
e.g. "MIKETTEI NO MONDAI (a problem of 
indecision)", and so on. That is,"MIKETTEI 
NO (of indecision)," represents the situation of 
a problem. So we see that "MIKETTEI NO (of 
indecision)" is in itself like an adjective. 
On the other hand, "KUMORI NO (cloudi- 
ness)" behaves ometimes like an adjective and 
sometimes not. 
KUMORI NO JOUTAI 
(cloudiness) (of) (state) 
a state of cloudiness 
The semantic behavior of "KUMORI NO 
(of cloudiness)" is like the behavior of adjec- 
tives in that the cloudiness represents the state 
as "KIKEN_NA (dangerous)," however, "KU- 
MORI NO (of cloudiness)" does not always rep- 
resent the state of the referent of the modified 
noun though "MIKETTEI NO (of indecision)" 
always represents that. "KUMORI (cloudi- 
ness)" is a natural phenomenon which can be 
pointed to concretely. For example, 
KUMORI NO NISSU 
(cloudiness) (of) (amount) 
WA 4 GATU NI SITEWA IJOU DA. 
The amount of cloudiness is unusual for April. 
In this example, "KUMORI NO (of cloudi- 
ness)" modifies "NISSU (the amount)," and 
does not represent a state but the possessor of 
the amount. 
As the examples of "MIKETTEI NO (of 
indecision)" and "KUMORI NO (of cloudi- 
ness)" show, there are nouns which have the 
same properties as adjectives intrinsically (e.g. 
"MIKETTEI (indecision)"), and other nouns 
which intrinsically have different properties 
from adjectives (e.g. "KUMORI (cloudiness)"). 
So, it is important o consider the properties of 
the noun in "noun + NO" when we analyze the 
"noun + NO" which behaves emantically like 
an adjective. Such an analysis enables us to find 
the situation in which they act like adjectives. 
We classified nouns in "noun + NO" structures 
into three types based on what the nouns refer 
to. Nouns from the last category, 3), are similar 
to adjectives emantically. As adjectives do not 
represent concrete objects or verb-like notions, 
nouns from these categories only occasionally 
resemble adjectives. 
63 
Noun Categories: 
1) nouns which refer to concrete objects. (like 
rain, book, science, and so on) 
2) nominalizations (like decision, work, and so 
on) 
3) nouns which belong to neither 1) nor 2), 
e.g. abstract nouns and so on. 
As our corpora contain mainly newspaper ar- 
ticles, many compound nouns appear. Since the 
last word in a compound noun determines the 
properties of the whole word, we focus on the 
last word in classifying them. 
Table 2 contains examples of the noun cate- 
gories. "KOUGYOU TOSHI (industry city)" is 
an example of a compound noun where the last 
word "TOSHI (city)" determines the properties. 
Table 2: Some "noun + NO" constructions with 
"impression" 
1) nouns which refer to concrete objects 
KOUGYOU TOSHI, HINOKI 
(industry city) (cypress) 
2) nominalizations 
SOKUBAKU, KOUTEN 
(restriction) (improvement) 
3) nouns which belong to neither 1) nor 2) 
MUTONTYAKU, JAKUSHOU 
(carelessness) (weakness) 
In the following section, we analyze the sim- 
ilarities and differences of the semantic behav- 
ior of "noun + NO" structures and adjectives. 
Firstly, we describe the case in which the se- 
mantic behavior of "noun + NO" is similar to 
that of adjectives and then we mention the case 
in which the semantic behavior of "noun + NO" 
is different from that of adjectives. Secondly, we 
analyze several types of nouns in "noun + NO" 
which behave like adjectives, ewm though nouns 
in "noun + NO" are not intrinsically similar to 
adjectiw; types. 
5.2 The di f ferences of semant ic  
behav ior  between nouns  in "noun 
-b NO"  and adject ives 
For example, "KANASHII (sad)", "URESHII 
(pleasurable)", "ZANNEN_NA (regrettable)", 
"KANASHIMI NO (of sadness)", "YOROKOBI 
NO (of delight)" and so on, modify nouns 
such as "OMOI (thought)", "KANJI (emo- 
tion)" and so on. Using a set of adnomi- 
nal constituents, such as "KANASHII (sad)", 
"URESHII (pleasurable)", "ZANNEN..NA (re- 
grettable)", as keys for classification, we can 
classify the modified nouns, "OMOI (thought)", 
"KANJI (feeling)" and so on, into the same 
group. Then we can find a semantic relation- 
ship between these adnominal constituents and 
their head nouns, in this case, <emotion>. In 
the following, we describe the similar and dif- 
fering semantic behaviors of "noun ? NO" and 
other adjectives in the same semantic ategory. 
As we described in the previous ection, we ex- 
tract sets of "noun + NO" structures and ad- 
jectives from data which was sorted semanti- 
cally. Words in each set represent he seman- 
tic substance of the similar nouns which they 
modify. Therefore, their semantic categories 
are similar. Examples of modified nouns of a 
similar semantic category and their modifiers 
which have a semantic ategory similar to that 
of the nouns are listed in Table 3. Included are 
some "noun ? NO" examples which though co- 
occurring with <mental state> nouns are not 
classified as such themselves. There are many 
adjectives and nominal adjectivals which can 
modify nouns in Table 3, such as "AWARENA 
(poor)", "IJIRASHII (moving)" and "HOKO- 
RASHII (triumphant)." Some "noun ? NO" 
structures are semantically similar to these ad- 
jectives ince they represent the contents of the 
emotion, e.g. "FUKAI NO KAN (sensation of 
displeasure)" and "YOROKOBI NO KIMOCHI 
(feeling of delight)." Most nouns in these "noun 
+ NO" structures in Table 3 are classified into 
"mental activity by humans" by the "Word List 
Classified by Semantic Principles3. '' "Noun + 
NO" structures, which have this kind of seman- 
tic; category, are similar to adjectives and nom- 
inal adjectivals, as both represent he content 
of the human mind. We call this semantic at- 
'~This list was compiled by The Natural Language Re- 
search Institute, Tokyo. 
64 
Table 3: The modified nouns and adjectives, 
nominal adjectivals, and "noun + NO" 
collected in the semantic ategory, 
<mental state> 
Modi f ied nouns 
KANJI (feeling), KAN (sensation), 
OMOI (thought), KI (intention), 
NEN (inclination), KIMOCHI (mind), 
KIBUN (mood), KANJO (emotion), 
JO (passion) 
Adject ives  and nominal  adject ivals 
AWARE_NA (poor), IJIRASHII (moving), 
HOKORASHII (triumphant), 
KINODOKU_NA (unfortunate), 
SHIAWASE_NA (happy), 
ZANNEN_NA (disappointing), 
URESHII (pleasurable), ...and so on. 
"Nouns"  in the "noun + NO"  s t ruc ture  
a) mental  act iv i ty  
KANASHIMI (sadness), FUKAI (displeasure), 
SHITASHIMI (familiarity), 
ZOUO (abhorrence), GAMAN (endurance), 
KOUKAI (regret), YOROKOBI (joy), 
MANZOKU (satisfaction), 
RAKUTAN (disappointment), 
IGAI (unexpected), ...and so on. 
b) nominal izat ions 
HOSHIN (self-defense), 
CHIKUZAI (moneymaking), 
INTAI (retirement), HIHAN (criticism), 
HIYAKU (rapid progress), ...and so on 
egory created by these adnominal constituents 
and their modified nouns "Feeling." 
On the other hand, some adnominal rela- 
tionships concerning a mental state can only 
be represented by "noun + NO" structures, 
such as "HOSHIN NO KIMOCHI (desire of de- 
fending one's own interest)," "CHIKUZAI NO 
NEN (thought of moneymaking)" and "INTAI 
NO KIMOCHI (idea of retirement)." Event 
nouns are mainly used in these "noun + NO" 
structures. Adnominal modifying relations of 
"nominalization + NO + mental state_noun" 
structures represent an intentional mental state. 
This kind of intentional mental state cannot be 
expressed by adjectives. We call this semantic 
category "Intentional mental state." 
We discussed two types of semantic represen- 
tations above, i.e. Feeling and Intentional men- 
tal state. Feeling can be represented by adjec- 
tives and "noun + NO" structures. However, 
Intentional mental state can be represented 
only by "noun + NO" structures. From the 
standpoint of the characteristics of the modified 
nouns (they represent human mental states), 
these two mental activities (Feeling and Inten- 
tional mental state) are similar, even though 
there are .differences in whether the activity is 
intentional or not. However, from the stand- 
point of the selection of an adnominal relation- 
ship in the surface structure, whether the activ- 
ity has active intention or not will be the decid- 
ing factor for the selection between adjectives 
and "noun + NO" structures. 
5.3 The case where  the semant ic  
behav ior  of "noun + NO"  
structures is similar to that of 
adjectives 
Here we focus on nouns whose properties are 
unlike those of adjectives, i.e. the nouns which 
refer to concrete objects, verbal notions and so 
on.  
(1) In the case where "noun + NO" represents 
characteristics, there is some overlap be- 
tween the semantic behavior of adjectives 
and "noun + NO" structures. 
I) The case where the noun in "noun + NO" 
is a compound noun 
Let us compare "noun + NO" with adjective 
usage. 
MUKUCHI_NA INSHOU 
(reticent) (impression) 
GA TUYOI JOHN-SAN WA "" 
Jotm who makes a reticent impression "-" 
KOUGYOUTOSHI NO INSHOU 
(industry city) (of) (impression) 
GA TUYOI KAWASAKISHI WA... 
65 
KAWASAKI city which gives a strong im- 
pression of an industrial city 4 
b) Modified nouns which represent instances 
of the concrete nouns in compound nouns 
In the previous two examples, the differences 
between "noun + NO" and adjectives depend 
only on whether the nouns they modify rep- 
resent a person or a city where both head 
nouns have characteristics in common. How- 
ever, "KOUGYOUTOSHI (industry city)" does 
not always have the same semantic relation to 
the modified noun, as seen in the following ex- 
ample: 
KOUGYOUTOSHI NO YUKYUTI 
(industry city) (of) (vacant land) 
NI TYAKUMOKU. 
They noticed the vacant land 
in the industrial city. 
In this example, the semantic relation be- 
tween "KOUGYOUTOSHI NO (of industry 
city)" and "YUKYUTI (the vacant land)" indi- 
cate the relation of possession so that it is not a 
semantic relation that adjectives can represent. 
When the modified nouns are abstract nouns 
that represent the property ("INSHOU (impres- 
sion)" or "SEIKAKU (characteristics)" etc.), or 
instances of the concrete nouns in compound 
nouns ("KAWASAKI SHI (KAWASAKI city)"), 
the semantic function of compound nouns in 
"noun + NO" constructions represent the char- 
acteristics of the referent of the modified nouns 
as adjectives do. 
a) Modified nouns which are abstract nouns 
that represent a property. 
KOUGYOUTOSHI NO IMEJI 
(industry city) (of) (image) 
GA OOKII. 
The image of an industrial city is strong. 
KOUKYUUHIN NO INSHOU 
(high quality item) (of) (impression) 
GA TUYOI SHANERU 
(with) CHANNEL the impression of 
a high-quality item is strong. 
4Note that some words which are nouns in Japanese 
(e.g. industry, high quality)must be translated as adjec- 
tiw~ in English (e.g. industrial, high-quality) 
<city-SUZUKA-SHI> 
KOUGYOUTOSHI NO SUZUKA SHI 
(industry city) (of) (SUZUKA city) 
SUZUKA city which is an industrial city 
<item-diamonds> 
KOUKYUUHIN NO DAIYA 
(high quality item) (of) (diamond) 
Diamonds are a high-quality item 
<company-IBM> 
YURYOUGAISHA NO 
(excellent company) (of) 
IBM is an excellent company 
IBM 
When the modified noun is an instance 
of the last word of the modifying com- 
pound noun, the semantic function of the 
whole compound noun is similar to that 
of adjectives because, in this type of com- 
pound, we focus on the adjectival semantic 
element. For example, "KOUGYOU (indus- 
try)" in "KOUGYOUTOSHI (industry city)", 
"KOUKYUU (high-quality)" in "KOUKYU- 
UHIN (high quality item)", and "YUURYOU 
(excellent)" in "YUURYOUGAISHA (excellent 
company)" are adjectival. 
II) the nouns that refer to the concrete object 
in "noun + NO" 
Originally the nouns that refer to a concrete 
object or event do not have the same meaning as 
adjectives, however, they have similar semantic 
behavior to that of adjectives in the following 
case. 
KARE WA OTONASHII KIHUU 
(mild) (disposition) 
NO MOTINUSHI DA. 
He has a mild disposition. 
The "mild" represents he characteristic (dis- 
position). In the following examples the "noun 
+ NO" also indicate the characteristics of some- 
thing. 
66 
KODOMOTACHI WA ... MASSUGU 
NOBIRU 
HINOKI 
(HINOKI-tree) 
These children 
NO INSHOU 
(of) (impression) 
GA ARIMASHITA. 
give the impression of a 
HINOKI-tree which grows straight. 
KAGAKUGAISHA TOIU KOTODE IPPAN 
NO HITO NIWA 
KANKYOUOSEN NO INSHOU 
(environment pollution) (of) (impression) 
impression of environmental pollution 
GA TUYOKATTA. 
Ordinary people have a strong impression of 
environmental pollution from the chemical 
company. 
The impression the children make is of a 
"HINOKI (HINOKI-tree)" and the impression 
the chemical company makes is of "KANKY- 
OUOSEN (environmental pollution)". These 
"noun + NO'structures represent he charac- 
teristics of children and a company in same 
manner that the adjective "mild" indicates his 
characteristic. In these examples, nouns in 
"noun + NO" represent objects and events and 
so on, i.e. "HINOKI-tree" and "environmental 
pollution" these nouns ordinarily do not behave 
like adjectives. That is, the adjective "mild" 
can represent a characteristic directly, however, 
these nouns in "noun + NO" cannot represent 
the characteristics of something directly. We 
cannot say "that children are HINOKI-tree" 
and "the company is the environmental pollu- 
tion" while we can say "He is mild." That is, in 
this case, "noun + NO" cannot appear in the 
predicative position with this meaning. When 
we show the characteristics of something by us- 
ing nouns that refer to concrete objects and 
events, we need to specify the modified nouns 
which indicate the characteristics like "impres- 
sion, .... disposition" and so on. 
(2) "Noun + NO" can represent quantification. 
Some adjectives (:an also represent quantifi- 
cation. 
NIHON NO.HASHIMOTO SHUSHOU NO 
TEIAN WA AIKAWARAZU 
67 
TYUUSHOUTEKI_NA IKI 
(abstract) (level) 
NI TODOMATTA. 
The suggestion of the Japanese prime minis- 
ter, Hashimoto, was still in an abstract state. 
HUSAINO HIRITU GA KAKEI NI TOTTE 
KIKEN_NA IKI NI TASSHITEIRU. 
(dangerous) (level) 
The rate of debt has reached a dangerous 
level for the household budget. 
The suggestion of the Japanese prime min- 
ister is at an "abstract" level on the "concrete- 
abstract" scale and the rate of debt is at a "dan- 
gerous" level on the "safety-dangerous" scale. 
The level of concreteness and safety is repre- 
sented by adjectives. On the other hand, the 
nouns that refer to concrete objects and verbal 
notions also represent a level by inference from 
the context. We can infer the scale from the 
contextual situation. For example, 
KOUNIN KOUHO WA 
UWASA NO DANKAI 
(rumor) (of) (stage) 
the stage of rumor 
DA GA BORUGA SHI 
Though it is completely at the stage of ru- 
mor, the candidate for the succession is Mr. 
Borgar ... 
SHUSHOU GAWA WA "" 
(the prime minister and his staff) 
ENZETU NO IKI 
(speech) (of) (level) 
WO KOERARENAKATTA. 
Though the prime minister and his staff said 
"we will specify the guidelines of the govern- 
ment proposal during the election", after all 
it was still at the level of speech. 
GIJUTUTEKINIWA 
KANSEI NO IKI 
(completeness) (of) (level) 
NI TASSHITEITA. 
It reached a level of completeness, technically. 
In the above case, we do not have a seman- 
tic element of actual "talk" in the "rumor" 
or "speech" meaning nor a semantic element 
"event" in the "completeness" meaning, but we 
have the level of "rumor" on the "truth-rumor" 
scale, the level of "speech" on the "statement- 
speech" scale and the level of "completeness" on 
the "incompleteness-completeness" scale. The 
nouns that refer to concrete objects and verbal 
actions are similar to adjectives when they rep- 
resent a level in context. 
6 Conc lus ion  
In this paper, we discussed the similarities 
and differences among adnominal constituents, 
i.e. adjectives and "noun + NO" structures 
which have a broad range of semantic functions. 
Nouns and adjectives differ in part of speech, 
but they sometimes have similarities when used 
adnominally. In such a case, we need not dis- 
tinguish them from each other semantically. We 
investigated explicit criteria to detect similari- 
ties and differences between nouns and adjec- 
tives in adnominal usage. This research was ver- 
ified by using large corpora and a self-organizing 
mapping system based on the neural network 
model. In future work, we will attempt o sys- 
tematically classify words used adnominally ac- 
cording to the semantic behavior of adnominal 
constituents following the theoretical insights of 
Pustejovsky. 
Acknowledgment 
We would like to thank Catherine Macleod of 
New York University and Kiyotaka Uchimoto 
of the Communications Research Laboratory for 
their invaluable help in writing this paper. 
References 
P. Bouillon. 1996. Mental State Adjectives: the 
Perspective of Generative Lexicon. In Proc. 
of COLING96. 
G. Grefenstette. 1994. Corpus-Derived First, 
Second and Third-Order Word Affinities. In ' 
Proc. off the EURALEX '9~. 
H. Isahara and K. Kanzaki. 1999. Lexical Se- 
mantics to Disambiguate Polysemous Phe- 
nomena of Japanese Adnominal Constituents. 
In Proc. of A CL99. 
Q. Ma, K. Kanzaki, M. Murata, K. Uchi- 
moto, and H. Isahara. 2000. Construction 
of a Japanese Semantic Map using Self- 
Organizing Neural Network Model. In 6th 
Annual Meeting of the Association for Nat- 
ural Language Processing, Japan. (will ap- 
pear). 
J. Pustejovsky. 1995. The Generative Lexicon. 
The MIT Press. 
P. Saint-Dizier. 1998. A Generative Lex- 
icon Perspective for Adjectival Modifica- 
tion. In Proc. of the Conference volume2 
in 36th Annual Meeting of the Associa- 
tion for Computational Linguistics and 17th 
International Conference on Computational 
Linguistics(COLING-A CL '98). 
A. Shimazu, S. Naito, and H. Nomura. 1986. 
Analysis of semantic relations between ouns 
connected by a Japanese particle "no". 
Keiryo Kokugogaku (Mathematical Linguis- 
tics), 15(7). (in Japanese). 
68 
The Unknown Word Problem: a Morphological Analysis of
Japanese Using Maximum Entropy Aided by a Dictionary
Kiyotaka Uchimotoy, Satoshi Sekinez and Hitoshi Isaharay
yCommunications Research Laboratory
2-2-2, Hikari-dai, Seika-cho, Soraku-gun,
Kyoto, 619-0289 Japan
[uchimoto, isahara]@crl.go.jp
zNew York University
715 Broadway, 7th oor
New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
In this paper we describe a morphological analy-
sis method based on a maximum entropy model.
This method uses a model that can not only
consult a dictionary with a large amount of lex-
ical information but can also identify unknown
words by learning certain characteristics. The
model has the potential to overcome the un-
known word problem.
1 Introduction
Morphological analysis is one of the basic tech-
niques used in Japanese sentence analysis. A
morpheme is a minimal grammatical unit, such
as a word or a sux, and morphological analysis
is the process segmenting a given sentence into
a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a part-
of-speech (POS) and an inection type. One of
the most important problems in morphological
analysis is that posed by unknown words, which
are words found in neither a dictionary nor a
training corpus, and there have been two sta-
tistical approaches to this problem. One is to
acquire unknown words from corpora and put
them into a dictionary (e.g., (Mori and Nagao,
1996)), and the other is to estimate a model
that can identify unknown words correctly (e.g.,
(Kashioka et al, 1997; Nagata, 1999)). We
would like to be able to make good use of both
approaches. If words acquired by the former
method could be added to a dictionary and a
model developed by the latter method could
consult the amended dictionary, then the model
could be the best statistical model which has
the potential to overcome the unknown word
problem. Mori and Nagao proposed a statisti-
cal model that can consult a dictionary (Mori
and Nagao, 1998). In their model the proba-
bility that a string of letters or characters is
a morpheme is augmented when the string is
found in a dictionary. The improvement of the
accuracy was slight, however, so we think that
it is dicult to eciently integrate the mecha-
nism for consulting a dictionary into an n-gram
model. In this paper we therefore describe a
morphological analysis method based on a max-
imum entropy (M.E.) model. This method uses
a model that can not only consult a dictionary
but can also identify unknown words by learn-
ing certain characteristics. To learn these char-
acteristics, we focused on such information as
whether or not a string is found in a dictio-
nary and what types of characters are used in a
string. The model estimates how likely a string
is to be a morpheme according to the informa-
tion on hand. When our method was used to
identify morpheme segments in sentences in the
Kyoto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%.
2 A Morpheme Model
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem
of Japanese morphological analysis can be re-
duced to the problem of assigning one of two
tags to each string in a sentence. A string is
tagged with a 1 or a 0 to indicate whether or
not it is a morpheme. When a string is a mor-
pheme, a grammatical attribute is assigned to
it. The 1 tag is thus divided into the num-
ber, n, of grammatical attributes assigned to
morphemes, and the problem is to assign an at-
tribute (from 0 to n) to every string in a given
sentence. The (n+1) tags form the space of \fu-
tures" in the M.E. formulation of our problem
of morphological analysis. The M.E. model, as
well as other similar models, enables the com-
putation of P (f jh) for any future f from the
space of possible futures, F , and for every his-
tory, h, from the space of possible histories, H.
A \history" in M.E. is all of the conditioning
data that enable us to make a decision in the
space of futures. In the problem of morphologi-
cal analysis, we can reformulate this in terms of
nding the probability of f associated with the
relationship at index t in the test corpus:
P (f jh
t
) = P (f jInformation derivable
from the test corpus
related to relationship t)
The computation of P (f jh) in any M.E. models
is dependent on a set of \features" which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
g(h; f) =
8
>
<
>
:
1 : if has(h; x) = true;
x = \POS( 1)(Major) : verb;
00
& f = 1
0 : otherwise:
(1)
Here \has(h,x)" is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and the part-of-speech of
the adjacent morpheme.
Given a set of features and some training
data, the M.E. estimation process produces a
model in which every feature g
i
has an associ-
ated parameter 
i
. This enables us to compute
the conditional probability as follows (Berger et
al., 1996):
P (f jh) =
Q
i

g
i
(h;f)
i
Z

(h)
(2)
Z

(h) =
X
f
Y
i

g
i
(h;f)
i
: (3)
The M.E. estimation process guarantees that for
every feature g
i
, the expected value of g
i
accord-
ing to the M.E. model will equal the empirical
expectation of g
i
in the training corpus. In other
words,
X
h;f
~
P (h; f)  g
i
(h; f)
=
X
h
~
P (h) 
X
f
P
M:E:
(f jh)  g
i
(h; f): (4)
Here
~
P is an empirical probability and P
M:E:
is
the probability assigned by the model.
We dene part-of-speech and bunsetsu
boundaries as grammatical attributes. Here a
bunsetsu is a phrasal unit consisting of one or
more morphemes. When there are m types
of parts-of-speech, and the left-hand side of
each morpheme may or may not be a bunsetsu
boundary, the number, n, of grammatical at-
tributes assigned to morphemes is 2m.
1
We
propose a model which estimates the likelihood
that a given string is a morpheme and has the
grammatical attribute i(1  i  n). We call it
a morpheme model. This model is represented
by Eq. (2), in which f can be one of (n + 1)
tags from 0 to n.
A given sentence is divided into morphemes,
and a grammatical attribute is assigned to each
morpheme so as to maximize the sentence prob-
ability estimated by our morpheme model. Sen-
tence probability is dened as the product of the
probabilities estimated for a particular division
of morphemes in a sentence. We use the Viterbi
algorithm to nd the optimal set of morphemes
in a sentence and we use the method proposed
by Nagata (Nagata, 1994) to search for the N-
best sets.
3 Experiments and Discussion
3.1 Experimental Conditions
The part-of-speech categories that we used fol-
low those of JUMAN (Kurohashi and Nagao,
1999). There are 53 categories covering all pos-
sible combinations of major and minor cate-
gories as dened in JUMAN. The number of
grammatical attributes is 106 if we include the
detection of whether or not the left side of a
morpheme is a bunsetsu boundary. We do not
identify inection types probabilistically since
1
Not only morphemes but also bunsetsus can be iden-
tied by considering the information related to their bun-
setsu boundaries.
they can be almost perfectly identied by check-
ing the spelling of the current morpheme after
a part-of-speech has been assigned to it. There-
fore, f in Eq. (2) can be one of 107 tags from 0
to 106.
We used the Kyoto University text corpus
(Version 2) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper. For
training, we used 7,958 sentences from newspa-
per articles appearing from January 1 to Jan-
uary 8, 1995, and for testing, we used 1,246
sentences from articles appearing on January 9,
1995.
Given a sentence, for every string consisting
of ve or less characters and every string ap-
pearing in the JUMAN dictionary (Kurohashi
and Nagao, 1999), whether or not the string is
a morpheme was determined and then the gram-
matical attribute of each string determined to
be a morpheme was identied and assigned to
that string. The maximum length was set at ve
because morphemes consisting of six or more
characters are mostly compound words or words
consisting of katakana characters. The stipula-
tion that strings consisting of six or more char-
acters appear in the JUMAN dictionary was set
because long strings not present in the JUMAN
dictionary were rarely found to be morphemes
in our training corpus. Here we assume that
compound words that do not appear in the JU-
MAN dictionary can be divided into strings con-
sisting of ve or less characters because com-
pound words tend not to appear in dictionar-
ies, and in fact, compound words which con-
sist of six or more characters and do not ap-
pear in the dictionary were not found in our
training corpus. Katakana strings that are not
found in the JUMAN dictionary were assumed
to be included in the dictionary as an entry
having the part-of-speech \Unknown(Major),
Katakana(Minor)." An optimal set of mor-
phemes in a sentence is searched for by em-
ploying the Viterbi algorithm under the con-
dition that connectivity rules dened between
parts-of-speech in JUMAN must be met. The
assigned part-of-speech in the optimal set is
not always selected from the parts-of-speech at-
tached to entries in the JUMAN dictionary, but
may also be selected from the 53 categories of
the M.E. model. It is dicult to select an appro-
priate category from the 53 when there is little
training data, so we assume that every entry in
the JUMAN dictionary has all possible parts-of-
speech, and the part-of-speech assigned to each
morpheme is selected from those attached to the
entry corresponding to the morpheme string.
The features used in our experiments are
listed in Table 1. Each row in Table 1 contains a
feature type, feature values, and an experimen-
tal result that will be explained later. Each fea-
ture consists of a type and a value. The features
are basically some attributes of the morpheme
itself or those of the morpheme to the left of
it. We used the 31,717 features that were found
three or more times in the training corpus. The
notations \(0)" and \(-1)" used in the feature
type column in Table 1 respectively indicate a
target string and the morpheme on the left of
it.
The terms used in the table are the following:
String: Strings which appeared as a morpheme
ve or more times in the training corpus
Length: Length of a string
POS: Part-of-speech. \Major" and \Minor"
respectively indicate major and minor part-
of-speech categories as dened in JUMAN.
Inf: Inection type as dened in JUMAN
Dic: We use the JUMAN dictionary, which has
about 200,000 entries (Kurohashi and Na-
gao, 1999). \Major&Minor" indicates pos-
sible combinations between major and mi-
nor part-of-speech categories. When the
target string is in the dictionary, the part-
of-speech attached to the entry correspond-
ing to the string is used as a feature value.
If an entry has two or more parts-of-speech,
the part-of-speech which leads to the high-
est probability in a sentence estimated from
our model is selected as a feature value.
JUMAN has another type of dictionary,
which is called a phrase dictionary. Each
entry in the phrase dictionary consists of
one or more morphemes such as \? (to,
case marker), ? (wa, topic marker), ??
(ie, say)." JUMAN uses this dictionary to
detect morphemes which need a longer con-
text to be identied correctly. When the
target string corresponds to the string of
the left most morpheme in the phrase dic-
tionary in JUMAN, the part-of-speech at-
Table 1: Features.
Feature Accuracy without
number Feature type Feature value (Number of value) each feature set
Recall Precision F-measure
1 String(0) (4,331) 93.66% 93.81% 93.73
2 String(-1) (4,331) ( 2.14%) ( 1.28%) ( 1.71)
3 Dic(0)(Major) Verb, Verb&Phrase, Adj, Adj&Phrase, 94.64% 92.87% 93.75
: : : (28)
4 Dic(0)(Minor) Common noun, Common noun&Phrase, ( 1.16%) ( 2.22%) ( 1.69)
Topic marker, : : : (90)
5 Dic(0)(Major&Minor) Noun&Common noun,
Noun&Common noun&Phrase, : : : (103)
6 Length(0) 1, 2, 3, 4, 5, 6 or more (6) 95.52% 94.11% 94.81
7 Length(-1) 1, 2, 3, 4, 5, 6 or more (6) ( 0.28%) ( 0.98%) ( 0.63)
8 TOC(0)(Beginning) Kanji, Hiragana, Symbol, Number, 95.17% 93.89% 94.52
Katakana, Alphabet (6)
9 TOC(0)(End) Kanji, Hiragana, Symbol, Number, ( 0.63%) ( 1.20%) ( 0.92)
Katakana, Alphabet (6)
10 TOC(0)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
11 TOC(-1)(End) Kanji, Hiragana, Symbol, Number,
Katakana, Alphabet (6)
12 TOC(-1)(Transition) Kanji!Hiragana, Number!Kanji,
Katakana!Kanji, : : : (30)
13 POS(-1)(Major) Verb, Adj, Noun, Unknown, : : : (15) 95.60% 95.31% 95.45
14 POS(-1)(Minor) Common noun, Sahen noun, Numeral, ( 0.20%) (+0.22%) (+0.01)
: : : (45)
15 POS(-1)(Major&Minor) [nil], Noun&Common noun,
Noun&Common noun&Phrase, : : : (54)
16 Inf(-1)(Major) Vowel verb, : : : (33) 95.66% 95.00% 95.33
17 Inf(-1)(Minor) Stem, Basic form, Imperative form, : : : (60) ( 0.14%) ( 0.09%) ( 0.11)
18 BB(-1) [nil], [exist] (2) 95.82% 95.25% 95.53
19 BB(-1) & Noun&Common, noun&Bunsetsu boundary, (+0.02%) (+0.16%) (+0.09)
POS(-1)(Major&Minor) Noun&Common, noun&Within a bunsetsu,
: : : (106)
tached to the entry plus the information
that it is in the phrase dictionary (such as
\Verb&Phrase") is used as a feature value.
TOC: Types of characters used in a string.
\(Beginning)" and \(End)" respectively
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the \(Begin-
ning)" and \(End)" are the same charac-
ter. \TOC(0)(Transition)" represents the
transition from the leftmost character to
the rightmost one in a string. \TOC(-
1)(Transition)" represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
one in the target string. For example, when
the adjacent morpheme on the left is \?
? (sensei, teacher)" and the target string
is \? (ni, case marker)," the feature value
\Kanji!Hiragana" is selected.
BB: Indicates whether or not the left side of a
morpheme is a bunsetsu boundary.
3.2 Results and Discussion
Some results of the morphological analysis are
listed in Table 2. Recall is the percentage of
morphemes in the test corpus whose segmen-
tation and major POS tag are identied cor-
rectly. Precision is the percentage of all mor-
phemes identied by the system that are iden-
tied correctly. F represents the F-measure and
is dened by the following equation.
F  measure =
2Recall  Precision
Recall + Precision
Table 2 shows results obtained by using our
method, by using JUMAN, and by using JU-
MAN plus KNP (Kurohashi, 1998). We show
the result obtained using JUMAN plus KNP
because JUMAN alone assigns an \Unknown"
tag to katakana strings when they are not in
the dictionary. All katakana strings not found
Table 2: Results of Experiments (Segmentation and major POS tagging).
Recall Precision F-measure
Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467) 95.44
JUMAN 95.25% (29,814/31,302) 94.90% (29,814/31,417) 95.07
JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417) 98.31
in the dictionary are therefore evaluated as er-
rors. KNP improves on JUMAN by replacing
the \Unknown" tag with a \Noun" tag and dis-
ambiguating part-of-speech ambiguities which
arise during the process of parsing when there is
more than one JUMAN analysis with the same
score.
The accuracy in segmentation and major
POS tagging obtained with our method and
that obtained with JUMAN were about 3%
worse than that obtained with JUMAN plus
KNP. We think the main reason for this was
an insucient amount of training data and fea-
ture sets and the inconsistency of the corpus.
The number of sentences in the training cor-
pus was only about 8,000, and we did not use
as many combined features as were proposed in
Ref. (Uchimoto et al, 1999). We were unable to
use more training data or more feature sets be-
cause every string consisting of ve or less char-
acters in our training corpus was used to train
our model, so the amount of tokenized train-
ing data would have become too large and the
training would not have been completed on the
available machine if we had used more training
data or more feature sets. The inconsistency
of the corpus was due to the way the corpus
was made. The Kyoto University corpus was
made by manually correcting the output of JU-
MAN plus KNP, and it is dicult to manually
correct all of the inconsistencies in the output.
The use of JUMAN plus KNP thus has an ad-
vantage over the use of our method when we
evaluate a system's accuracy by using the Ky-
oto University corpus. For example, the num-
ber of morphemes whose rightmost character is
\?" was 153 in the test corpus, and they were
all the same as those in the output of JUMAN
plus KNP. There were three errors (about 2%)
in the output of our system. There were several
inconsistencies in the test corpus such as \??
(seisan, Noun), ? (sha, Sux)(producer)," and
\??? (shouhi-sha, Noun)(consumer)." They
should have been corrected in the corpus-
making process to \?? (seisan, Noun),? (sha,
Sux)(producer)," and \?? (shouhi, Noun),
? (sha, Sux)(consumer)." It is dicult for
our model to discriminate among these with-
out over-training when there are such incon-
sistencies in the corpus. Other similar incon-
sistencies were, for example, \??? (geijutsu-
ka, Noun)(artist)" and \?? (kougei, Noun),
? (ka, Sux)(craftsman)," \??? (keishi-cho,
Noun)(the Metropolitan Police Board)" and \?
? (kensatsu, Noun), ? (cho, Noun)(the Pub-
lic Prosecutor's Oce)," and \??? (genjitsu-
teki, Adjective)(realistic)" and \?? (risou,
Noun), ? (teki, Sux)(ideal).". If these had
been corrected consistently when making the
corpus, the accuracy obtained by our method
could have been better than that shown in Ta-
ble 2. A study on corpus revision should be un-
dertaken to resolve this issue. We believe it can
be resolved by using our trained model. There
is a high possibility that a morpheme lacks con-
sistency in the training corpus when its proba-
bility, re-estimated by our model, is low. Thus
a method which detects morphemes having a
low probability can identify those lacking con-
sistency in the training corpus. We intend to
try this in the future.
3.3 Features and Accuracy
In our model, dictionary information and cer-
tain characteristics of unknown words are re-
ected as features, as shown in Table 1.
\String" and \Dic" reect the dictionary in-
formation,
2
and \Length" and \TOC"(types
of characters) reect the characteristics of un-
known words. Therefore, our model can not
only consult a dictionary but can also detect un-
known words. Table 1 shows the results of an
2
\String" indicates strings that make up a morpheme
and were found ve or more times in the training corpus.
Using this information as features in our M.E. model
corresponds to consulting a dictionary constructed from
the training corpus.
93
93.5
94
94.5
95
95.5
96
96.5
97
0 1000 2000 3000 4000 5000 6000 7000 8000
F-
m
ea
su
re
Number of Sentences
"training"
"testing"
Figure 1: Relation between accuracy and the number of training sentences.
analysis without the complete feature set. Al-
most all of the feature sets improved accuracy.
The contribution of the dictionary information
was especially signicant.
There were cases, however, in which the use
of dictionary information led to a decrease in
the accuracy. For example, we found these er-
roneous segmentations:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)????? (romanha, the Ro-
mantic school)?" and \??? (aranami, rag-
ing waves)?? (ni, case marker)??? (make,
lose)???? (naishin, one's inmost heart)
?? (to, case marker)?" (Underlined strings
were errors.) when the correct segmentations
were:
\?? (umi, sea)?? (ni, case marker)???
? (kaketa, bet)???? (roman, romance)??
(wa, topic marker)?" and \??? (aranami,
raging waves)?? (ni, case marker)?????
(makenai, not to lose)?? (kokoro, heart)??
(to, case marker)?" (\?" indicates a morpho-
logical boundary.).
These errors were caused by nonstandard en-
tries in the JUMAN dictionary. The dictio-
nary had not only the usual notation using kanji
characters, \????" and \??," but also the
uncommon notation using hiragana strings, \?
???" and \???". To prevent this type of
error, it is necessary to remove nonstandard en-
tries from the dictionary or to investigate the
frequency of such entries in large corpora and
to use it as a feature.
3.4 Accuracy and the Amount of
Training Data
The accuracies (F-measures) for the training
corpus and the test corpus are shown in Figure 1
plotted against the number of sentences used
for training. The learning curve shows that we
can expect improvement if we use more training
data.
3.5 Unknown Words and Accuracy
The strength of our method is that it can iden-
tify morphemes when they are unknown words
and can assign appropriate parts-of-speech to
them. For example, the nouns \?? (Souseki)"
and \?? (Rohan)" are not found in the JU-
Table 3: Accuracy for unknown words (Recall).
Segmentation and Segmentation and
major POS tagging minor POS tagging
For words not found in the dictionary
nor in our training corpus
Our method 69.90% (432/618) 27.51% (170/618)
JUMAN+KNP 79.29% (490/618) 20.55% (127/618)
For words not found in the dictionary
nor in our features
Our method 76.17% (719/944) 32.20% (304/944)
JUMAN+KNP 85.70% (809/944) 27.22% (257/944)
For words not found in the dictionary
Our method 82.40% (1,138/1,381) 49.24% (680/1,381)
JUMAN+KNP 89.79% (1,240/1,381) 38.60% (533/1,381)
MAN dictionary. JUMAN plus KNP analyzes
them simply as \? (Noun)? (Noun)" and \?
(Adverb)? (Noun)," whereas our system ana-
lyzes both of them correctly. Our system cor-
rectly identied them as names of people even
though they were not in the dictionary and did
not appear as features in our M.E. model. Since
these names, or proper nouns, are newly coined
and can be represented by a variety of expres-
sions, no proper nouns can be included in a dic-
tionary, nor can they appear in a training cor-
pus; this means that proper nouns could easily
be unknown words. We investigated the accu-
racy of our method in identifying morphemes
when they are unknown words, and the re-
sults are listed in Table 3. The rst row in
each section shows the recall for the morphemes
that were unknown words. The second row in
each section shows the percentage of morphemes
whose segmentation and \minor" POS tag were
identied correctly. The dierence between the
rst and second lines, the third and fourth lines,
and fth and sixth lines is the denition of un-
known words. Unknown words were dened re-
spectively as words not found in the dictionary
nor in our training corpus, as words not found
in the dictionary nor in our features, and as
words not found in the dictionary. Our accu-
racy, shown as the second rows in Table 3 was
more than 5% better than that of JUMAN plus
KNP for each denition. These results show
that our model can eciently learn the char-
acteristics of unknown words, especially those
of proper nouns such as the names of people,
organizations, and locations.
4 Related Work
Several methods based on statistical models
have been proposed for the morphological anal-
ysis of Japanese sentences. An F-measure of
about 96% was achieved by a method based
on a hidden Markov model (HMM) (Takeuchi
and Matsumoto, 1997) and by one based on
a variable-memory Markov model (Haruno and
Matsumoto, 1997; Kitauchi et al, 1999). Al-
though the accuracy obtained with these meth-
ods was better than that obtained with ours,
their accuracy cannot be compared directly
with that of our method because their part-
of-speech categories dier from ours. And an
advantage of our model is that it can handle
unknown words, whereas their models do not
handle unknown words well. In their models,
unknown words are divided into a combination
of a word consisting of one character and known
words. Haruno and Matsumoto (Haruno and
Matsumoto, 1997) achieved a recall of about
96% when using trigram or greater information,
but achieved a recall of only 94% when using bi-
gram information. This leads us to believe that
we could obtain better accuracy if we use tri-
gram or greater information. We plan to do so
in future work.
Two approaches have been used to deal with
unknown words: acquiring unknown words from
corpora and putting them into a dictionary
(e.g., (Mori and Nagao, 1996)) and develop-
ing a model that can identify unknown words
correctly (e.g., (Kashioka et al, 1997; Nagata,
1999)). Nagata reported a recall of about 40%
for unknown words (Nagata, 1999). As shown
in Table 3, our method achieved a recall of
69.90% for unknown words. Our accuracy was
about 30% better than his. It is dicult to
compare his method with ours directly because
he used a dierent corpus (the EDR corpus),
but the part-of-speech categories and the def-
inition of morphemes he used were similar to
ours. Thus, this comparison is helpful in evalu-
ating our method. There are no spaces between
morphemes in Japanese. In general, therefore,
detecting whether a given string is an unknown
word or is not a morpheme is dicult when it
is not found in the dictionary, nor in the train-
ing corpus. However, our model learns whether
or not a given string is a morpheme and has a
huge amount of data for learning what in a cor-
pus is not a morpheme. Therefore, we believe
that the characteristics of our model led to its
good results for identifying unknown words.
Mori and Nagao proposed a model that can
consult a dictionary (Mori and Nagao, 1998);
they reported an F-measure of about 92 when
using the EDR corpus and of about 95 when
using the Kyoto University corpus. Their slight
improvement in accuracy by using dictionary in-
formation resulted in an F-measure of about 0.2,
while our improvement was about 1.7. Their
accuracy of 95% when using the Kyoto Univer-
sity corpus is similar to ours, but they added
to their dictionary all of the words appearing
in the training corpus. Therefore, their exper-
iment had to deal with fewer unknown words
than ours did.
With regard to the morphological analy-
sis of English sentences, methods for part-of-
speech tagging based on an HMM (Cutting et
al., 1992), a variable-memory Markov model
(Schutze and Singer, 1994), a decision tree
model (Daelemans et al, 1996), an M.E. model
(Ratnaparkhi, 1996), a neural network model
(Schmid, 1994), and a transformation-based
error-driven learning model (Brill, 1995) have
been proposed, as well as a combined method
(Marquez and Padro, 1997; van Halteren et al,
1998). On available machines, however, these
models cannot handle a large amount of lex-
ical information. We think that our model,
which can not only consult a dictionary with
a large amount of lexical information, but can
also identify unknown words by learning cer-
tain characteristics, has the potential to achieve
good accuracy for part-of-speech tagging in En-
glish. We plan to apply our model to English
sentences.
5 Conclusion
This paper described a method for morpho-
logical analysis based on a maximum entropy
(M.E.) model. This method uses a model
that can not only consult a dictionary but can
also identify unknown words by learning cer-
tain characteristics. To learn these characteris-
tics, we focused on such information as whether
or not a string is found in a dictionary and
what types of characters are used in a string.
The model estimates how likely a string is to
be a morpheme according to the information
on hand. When our method was used to iden-
tify morpheme segments in sentences in the Ky-
oto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%. In our experiments without each fea-
ture set shown in Tables 1, we found that dic-
tionary information signicantly contributes to
improving accuracy. We also found that our
model can eciently learn the characteristics of
unknown words, especially proper nouns such
as the names of people, organizations, and lo-
cations.
References
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A Max-
imum Entropy Approach to Natural Lan-
guage Processing. Computational Linguis-
tics, 22(1):39{71.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part-of-Speech Tag-
ging. Computational Linguistics, 21(4):543{
565.
Doung Cutting, Julian Kupiec, Jan Peder-
sen, and Penelope Sibun. 1992. A Practical
Part-of-Speech Tagger. In Proceedings of the
Third Conference on Applied Natural Lan-
guage Processing, pages 133{140.
Walter Daelemans, Jakub Zavrel, Peter Berck,
and Steven Gills. 1996. MBT: A Memory-
Based Part-of-Speech Tagger-Generator. In
Proceedings of the 4th Workshop on Very
Large Corpora, pages 1{14.
Masahiko Haruno and Yuji Matsumoto. 1997.
Mistake-Driven Mixture of Hierarchical-Tag
Context Trees. In Proceedings of the 35th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 230{237.
Hideki Kashioka, Stephen G. Eubank, and
Ezra W. Black. 1997. Decision-Tree Mor-
phological Analysis without a Dictionary for
Japanese. In Proceedings of the Natural Lan-
guage Processing Pacic Rim Symposium,
pages 541{544.
Akira Kitauchi, Takehito Utsuro, and Yuji Mat-
sumoto. 1999. Probabilistic Model Learn-
ing for Japanese Morphological Analysis by
Error-driven Feature Selection. Transactions
of Information Processing Society of Japan,
40(5):2325{2337. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 1997.
Building a Japanese Parsed Corpus while Im-
proving the Parsing System. In Proceedings
of the Natural Language Processing Pacic
Rim Symposium, pages 451{456.
Sadao Kurohashi and Makoto Nagao, 1999.
Japanese Morphological Analysis System JU-
MAN Version 3.61. Department of Informat-
ics, Kyoto University.
Sadao Kurohashi, 1998. Japanese Depen-
dency/Case Structure Analyzer KNP Ver-
sion 2.0b6. Department of Informatics, Ky-
oto University.
Llu

is Marquez and Llu

is Padro. 1997. A Flexi-
ble POS Tagger Using an Automatically Ac-
quired Language Model. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
238{252.
Shinsuke Mori and Makoto Nagao. 1996.
Word Extraction from Corpora and Its Part-
of-Speech Estimation Using Distributional
Analysis. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics (COLING96), pages 1119{1122.
Shinsuke Mori and Makoto Nagao. 1998. An
Improvement of a Morphological Analysis by
a Morpheme Clustering. Journal of Nat-
ural Language Processing, 5(2):75{103. (in
Japanese).
Masaaki Nagata. 1994. A Stochastic Japanese
Morphological Analyzer Using a Forward-DP
Backward-A

N-Best Search Algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics (COL-
ING94), pages 201{207.
Masaaki Nagata. 1999. A Part of Speech Esti-
mation Method for Japanese UnknownWords
using a Statistical Model of Morphology and
Context. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 277{284.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-Of-Speech Tagging. In
Conference on Empirical Methods in Natural
Language Processing, pages 133{142.
Helmut Schmid. 1994. Part-Of-Speech Tagging
with Neural Networks. In Proceedings of the
15th International Conference on Computa-
tional Linguistics (COLING94), pages 172{
176.
Hinrich Schutze and Yoram Singer. 1994. Part-
of-Speech Tagging Using a Variable Memory
Markov Model. In Proceedings of the 32nd
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 181{187.
Koichi Takeuchi and Yuji Matsumoto. 1997.
HMM Parameter Learning for Japanese
Morphological Analyzer. Transactions of
Information Processing Society of Japan,
83(3):500{509. (in Japanese).
Kiyotaka Uchimoto, Satoshi Sekine, and Hi-
toshi Isahara. 1999. Japanese Dependency
Structure Analysis Based on Maximum En-
tropy Models. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL'99), pages 196{203.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. 1998. Improving Data Driven
Wordclass Tagging by System Combination.
In Proceedings of the COLING-ACL '98,
pages 491{497.
 
	ff 	

Evaluation of Features for Sentence Extraction
on Different Types of Corpora
Chikashi Nobata?, Satoshi Sekine? and Hitoshi Isahara?
? Communications Research Laboratory
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{nova, isahara}@crl.go.jp
? Computer Science Department, New York University
715 Broadway, 7th floor, New York, NY 10003, USA
sekine@cs.nyu.edu
Abstract
We report evaluation results for our sum-
marization system and analyze the result-
ing summarization data for three differ-
ent types of corpora. To develop a ro-
bust summarization system, we have cre-
ated a system based on sentence extraction
and applied it to summarize Japanese and
English newspaper articles, obtained some
of the top results at two evaluation work-
shops. We have also created sentence ex-
traction data from Japanese lectures and
evaluated our system with these data. In
addition to the evaluation results, we an-
alyze the relationships between key sen-
tences and the features used in sentence
extraction. We find that discrete combi-
nations of features match distributions of
key sentences better than sequential com-
binations.
1 Introduction
Our ultimate goal is to create a robust summariza-
tion system that can handle different types of docu-
ments in a uniform way. To achieve this goal, we
have developed a summarization system based on
sentence extraction. We have participated in eval-
uation workshops on automatic summarization for
both Japanese and English written corpora. We have
also evaluated the performance of the sentence ex-
traction system for Japanese lectures. At both work-
shops we obtained some of the top results, and for
the speech corpus we obtained results comparable
with those for the written corpora. This means that
the features we use are worth analyzing.
Sentence extraction is one of the main methods
required for a summarization system to reduce the
size of a document. Edmundson (1969) proposed a
method of integrating several features, such as the
positions of sentences and the frequencies of words
in an article, in order to extract sentences. He man-
ually assigned parameter values to integrate features
for estimating the significance scores of sentences.
On the other hand, machine learning methods can
also be applied to integrate features. For sentence
extraction from training data, Kupiec et al (1995)
and Aone et al (1998) used Bayes? rule, Lin (1999)
and Nomoto and Matsumoto (1997) generated a de-
cision tree, and Hirao et al (2002) generated an
SVM.
In this paper, we not only show evaluation results
for our sentence extraction system using combina-
tions of features but also analyze the features for dif-
ferent types of corpora. The analysis gives us some
indication about how to use these features and how
to combine them.
2 Summarization data
The summarization data we used for this research
were prepared from Japanese newspaper articles,
Japanese lectures, and English newspaper articles.
By using these three types of data, we could com-
pare two languages and also two different types of
corpora, a written corpus and a speech corpus.
2.1 Summarization data from Japanese
newspaper articles
Text Summarization Challenge (TSC) is an evalua-
tion workshop for automatic summarization, which
is run by the National Institute of Informatics in
Japan (TSC, 2001). Three tasks were presented at
TSC-2001: extracting important sentences, creating
summaries to be compared with summaries prepared
by humans, and creating summaries for informa-
tion retrieval. We focus on the first task here, i.e.,
the sentence extraction task. At TSC-2001, a dry
run and a formal run were performed. The dry run
data consisted of 30 newspaper articles and manu-
ally created summaries of each. The formal run data
consisted of another 30 pairs of articles and sum-
maries. The average number of sentences per article
was 28.5 (1709 sentences / 60 articles). The news-
paper articles included 15 editorials and 15 news re-
ports in both data sets. The summaries were created
from extracted sentences with three compression ra-
tios (10%, 30%, and 50%). In our analysis, we used
the extraction data for the 10% compression ratio.
In the following sections, we call these summa-
rization data the ?TSC data?. We use the TSC data
as an example of a Japanese written corpus to eval-
uate the performance of sentence extraction.
2.2 Summarization data from Japanese
lectures
The speech corpus we used for this experiment
is part of the Corpus of Spontaneous Japanese
(CSJ) (Maekawa et al, 2000), which is being cre-
ated by NIJLA, TITech, and CRL as an ongoing
joint project. The CSJ is a large collection of mono-
logues, such as lectures, and it includes transcrip-
tions of each speech as well as the voice data. We
selected 60 transcriptions from the CSJ for both sen-
tence segmentation and sentence extraction. Since
these transcription data do not have sentence bound-
aries, sentence segmentation is necessary before
sentence extraction. Three annotators manually gen-
erated sentence segmentation and summarization re-
sults. The target compression ratio was set to 10%.
The results of sentence segmentation were unified
to form the key data, and the average number of
sentences was 68.7 (4123 sentences / 60 speeches).
The results of sentence extraction, however, were
not unified, but were used separately for evaluation.
In the following sections, we call these summa-
rization data the ?CSJ data?. We use the CSJ data as
an example of a Japanese speech corpus to evaluate
the performance of sentence extraction.
2.3 Summarization data from English
newspaper articles
Document Understanding Conference (DUC) is an
evaluation workshop in the U.S. for automatic sum-
marization, which is sponsored by TIDES of the
DARPA program and run by NIST (DUC, 2001).
At DUC-2001, there were two types of tasks:
single-document summarization (SDS) and multi-
document summarization (MDS). The organizers of
DUC-2001 provided 30 sets of documents for a dry
run and another 30 sets for a formal run. These data
were shared by both the SDS and MDS tasks, and
the average number of sentences was 42.5 (25779
sentences / 607 articles). Each document set had a
topic, such as ?Hurricane Andrew? or ?Police Mis-
conduct?, and contained around 10 documents rele-
vant to the topic. We focus on the SDS task here, for
which the size of each summary output was set to
100 words. Model summaries for the articles were
also created by hand and provided. Since these sum-
maries were abstracts, we created sentence extrac-
tion data from the abstracts by word-based compar-
ison.
In the following sections, we call these summa-
rization data the ?DUC data?. We use the DUC data
as an example of an English written corpus to eval-
uate the performance of sentence extraction.
3 Overview of our sentence extraction
system
In this section, we give an overview of our sentence
extraction system, which uses multiple components.
For each sentence, each component outputs a score.
The system then combines these independent scores
by interpolation. Some components have more than
one scoring function, using various features. The
weights and function types used are decided by op-
timizing the performance of the system on training
data.
Our system includes parts that are either common
to the TSC, CSJ, and DUC data or specific to one of
these data sets. We stipulate which parts are specific.
3.1 Features for sentence extraction
3.1.1 Sentence position
We implemented three functions for sentence po-
sition. The first function returns 1 if the position of
the sentence is within a given threshold N from the
beginning, and returns 0 otherwise:
P1. Scorepst(Si)(1 ? i ? n) = 1(if i < N)
= 0(otherwise)
The threshold N is determined by the number of
words in the summary.
The second function is the reciprocal of the po-
sition of the sentence, i.e., the score is highest for
the first sentence, gradually decreases, and goes to a
minimum at the final sentence:
P2. Scorepst(Si) =
1
i
These first two functions are based on the hypoth-
esis that the sentences at the beginning of an article
are more important than those in the remaining part.
The third function is the maximum of the recipro-
cal of the position from either the beginning or the
end of the document:
P3. Scorepst(Si) = max(
1
i ,
1
n? i+ 1)
This method is based on the hypothesis that the sen-
tences at both the beginning and the end of an article
are more important than those in the middle.
3.1.2 Sentence length
The second type of scoring function uses sen-
tence length to determine the significance of sen-
tences. We implemented three scoring functions for
sentence length. The first function only returns the
length of each sentence (Li):
L1. Scorelen(Si) = Li
The second function sets the score to a negative
value as a penalty when the sentence is shorter than
a certain length (C):
L2. Scorelen(Si) = 0 (if Li ? C)
Li ? C (otherwise)
The third function combines the above two ap-
proaches, i.e., it returns the length of a sentence that
has at least a certain length, and otherwise returns a
negative value as a penalty:
L3. Scorelen(Si) = Li (if Li ? C)= Li ? C (otherwise)
The length of a sentence means the number of let-
ters, and based on the results of an experiment with
the training data, we set C to 20 for the TSC and
CSJ data. For the DUC data, the length of a sen-
tence means the number of words, and we set C to
10 during the training stage.
3.1.3 Tf*idf
The third type of scoring function is based on term
frequency (tf) and document frequency (df). We ap-
plied three scoring functions for tf*idf, in which the
term frequencies are calculated differently. The first
function uses the raw term frequencies, while the
other two are two different ways of normalizing the
frequencies, as follows, where DN is the number of
documents given:
T1. tf*idf(w) = tf(w) log DNdf(w)
T2. tf*idf(w) = tf(w)-1
tf(w) log
DN
df(w)
T3. tf*idf(w) = tf(w)
tf(w)+1 log
DN
df(w)
For the TSC and CSJ data, we only used the third
method (T3), which was reported to be effective
for the task of information retrieval (Robertson and
Walker, 1994). The target words for these functions
are nouns (excluding temporal or adverbial nouns).
For each of the nouns in a sentence, the system cal-
culates a Tf*idf score. The total score is the sig-
nificance of the sentence. The word segmentation
was generated by Juman3.61 (Kurohashi and Nagao,
1999). We used articles from the Mainichi newspa-
per in 1994 and 1995 to count document frequen-
cies.
For the DUC data, the raw term frequency (T1)
was selected during the training stage from among
the three tf*idf definitions. A list of stop words were
used to exclude functional words, and articles from
the Wall Street Journal in 1994 and 1995 were used
to count document frequencies.
3.1.4 Headline
We used a similarity measure of the sentence to
the headline as another type of scoring function. The
basic idea is that the more words in the sentence
overlap with the words in the headline, the more im-
portant the sentence is. The function estimates the
relevance between a headline (H) and a sentence
(Si) by using the tf*idf values of the words (w) in
the headline:
Scorehl(Si) =
?
w?H?Si
tf(w)
tf(w)+1 log
DN
df(w)
?
w?H
tf(w)
tf(w)+1 log
DN
df(w)
We also evaluated another method based on this
scoring function by using only named entities (NEs)
instead of words for the TSC data and DUC data.
Only the term frequency was used for NEs, because
we judged that the document frequency for an entity
was usually quite small, thereby making the differ-
ences between entities negligible.
3.1.5 Patterns
For the DUC data, we used dependency patterns
as a type of scoring function. These patterns were
extracted by pattern discovery during information
extraction (Sudo et al, 2001). The details of this ap-
proach are not explained here, because this feature
is not among the features we analyze in Section 5.
The definition of the function appears in (Nobata et
al., 2002).
3.2 Optimal weight
Our system set weights for each scoring function in
order to calculate the total score of a sentence. The
total score (Si) is defined from the scoring functions
(Scorej()) and weights (?j) as follows:
TotalScore(Si) =
?
j
?jScorej(Si) (1)
We estimated the optimal values of these weights
from the training data. After the range of each
weight was set manually, the system changed the
values of the weights within a range and summarized
the training data for each set of weights. Each score
was recorded after the weights were changed, and
the weights with the best scores were stored.
A particular scoring method was also selected in
the cases of features with more than one defined
scoring methods. We used the dry run data from
each workshop as TSC and DUC training data. For
the TSC data, since the 30 articles contained 15 ed-
itorials and 15 news reports, we estimated optimal
values separately for editorials and news reports. For
the CSJ data, we used 50 transcriptions for training
and 10 for testing, as mentioned in Section 2.2.
Table 1: Evaluation results for the TSC data.
Ratio 10% 30% 50% Avg.
System 0.363 (1) 0.435 (5) 0.589 (2) 0.463 (2)
Lead 0.284 0.432 0.586 0.434
4 Evaluation results
In this section, we show our evaluation results on the
three sets of data for the sentence extraction system
described in the previous section.
4.1 Evaluation results for the TSC data
Table 1 shows the evaluation results for our sys-
tem and some baseline systems on the task of sen-
tence extraction at TSC-2001. The figures in Ta-
ble 1 are values of the F-measure1. The ?System?
column shows the performance of our system and its
rank among the nine systems that were applied to the
task, and the ?Lead? column shows the performance
of a baseline system which extracts as many sen-
tences as the threshold from the beginning of a doc-
ument. Since all participants could output as many
sentences as the allowed upper limit, the values of
the recall, precision, and F-measure were the same.
Our system obtained better results than the baseline
systems, especially when the compression ratio was
10%. The average performance was second among
the nine systems.
4.2 Evaluation results for the DUC data
Table 2 shows the results of a subjective evalua-
tion in the SDS task at DUC-2001. In this subjec-
tive evaluation, assessors gave a score to each sys-
tem?s outputs, on a zero-to-four scale (where four is
the best), as compared with summaries made by hu-
mans. The figures shown are the average scores over
all documents. The ?System? column shows the per-
formance of our system and its rank among the 12
systems that were applied to this task. The ?Lead?
1The definitions of each measurement are as follows:
Recall (REC) = COR / GLD
Precision (PRE) = COR / SYS
F-measure = 2 * REC * PRE / (REC + PRE),
where COR is the number of correct sentences marked by the
system, GLD is the total number of correct sentences marked
by humans, and SYS is the total number of sentences marked by
the system. After calculating these scores for each transcription,
the average is calculated as the final score.
Table 2: Evaluation results for the DUC data (sub-
jective evaluation).
System Lead Avg.
Grammaticality 3.711 (5) 3.236 3.580
Cohesion 3.054 (1) 2.926 2.676
Organization 3.215 (1) 3.081 2.870
Total 9.980 (1) 9.243 9.126
Table 3: Evaluation results for the CSJ data.
Annotators
A B C Avg.
REC 0.407 0.331 0.354 0.364
PRE 0.416 0.397 0.322 0.378
F 0.411 0.359 0.334 0.368
column shows the performance of a baseline system
that always outputs the first 100 words of a given
document, while the ?Avg.? column shows the aver-
age for all systems. Our system ranked 5th in gram-
maticality and was ranked at the top for the other
measurements, including the total value.
4.3 Evaluation results for the CSJ data
The evaluation results for sentence extraction with
the CSJ data are shown in Table 3. We compared the
system?s results with each annotator?s key data. As
mentioned previously, we used 50 transcriptions for
training and 10 for testing.
These results are comparable with the perfor-
mance on sentence segmentation for written doc-
uments, because the system?s performance for the
TSC data was 0.363 when the compression ratio was
set to 10%. The results of our experiments thus show
that for transcriptions, sentence extraction achieves
results comparable to those for written documents,
if the are well defined.
4.4 Contributions of features
Table 4 shows the contribution vectors for each set
of training data. The contribution here means the
product of the optimized weight and the standard
deviation of the score for the test data. The vec-
tors were normalized so that the sum of the com-
ponents is equal to 1, and the selected function types
for the features are also shown in the table. Our sys-
tem used the NE-based headline function (HL (N))
for the DUC data and the word-based function (HL
Table 4: Contribution (weight? s.d.) of each feature
for each set of summarization data.
TSC
Features Editorial Report DUC CSJ
Pst. P3. 0.446 P1. 0.254 P1. 0.691 P3. 0.055
Len. L3. 0.000 L3. 0.000 L2. 0.020 L2. 0.881
Tf*idf T3. 0.169 T3. 0.185 T1. 0.239 T3. 0.057
HL (W) 0.171 0.292 - 0.007
HL (N) 0.214 0.269 0.045 -
Pattern - - 0.005 -
(W)) for the CSJ data, and both functions for the
TSC data. The columns for the TSC data show the
contributions when the compression ratio was 10%.
We can see that the feature with the biggest con-
tribution varies among the data sets. While the posi-
tion feature was the most effective for the TSC and
DUC data, the length feature was dominant for the
CSJ data. Most of the short sentences in the lectures
were specific expressions, such as ?This is the result
of the experiment.? or ?Let me summarize my pre-
sentation.?. Since these sentences were not extracted
as key sentences by the annotators, it is believed that
the function giving short sentences a penalty score
matched the manual extraction results.
5 Analysis of the summarization data
In Section 4, we showed how our system, which
combines major features, has performed well as
compared with current summarization systems.
However, the evaluation results alone do not suffi-
ciently explain how such a combination of features
is effective. In this section, we investigate the corre-
lations between each pair of features. We also match
feature pairs with distributions of extracted key sen-
tences as answer summaries to find effective combi-
nation of features for sentence extraction.
5.1 Correlation between features
Table 5 shows Spearman?s rank correlation coeffi-
cients among the four features. Significantly corre-
lated feature pairs are indicated by ???(? = 0.001).
Here, the word-based feature is used as the headline
feature. We see the following tendencies for any of
the data sets:
? ?Position? is relatively independent of the other features.
? ?Length? and ?Tf*idf? have high correlation2.
Table 5: Rank correlation coefficients between fea-
tures.
TSC Report
Features Length Tf*idf Headline
Position 0.019 -0.095 -0.139
Length ? 0.546? 0.338?
Tf*idf ? ? 0.696?
TSC Editorial
Features Length Tf*idf Headline
Position -0.047 -0.099 0.046
Length ? 0.532? 0.289?
Tf*idf ? ? 0.658?
DUC Data
Features Length Tf*idf Headline
Position -0.130? -0.108? -0.134?
Length ? 0.471? 0.293?
Tf*idf ? ? 0.526?
CSJ Data
Features Length Tf*idf Headline
Position -0.092? -0.069? -0.106?
Length ? 0.460? 0.224?
Tf*idf ? ? 0.533?
? ?TF*idf? and ?Headline ? also have high correlation.
These results show that while combinations of these
four features enabled us to obtain good evaluation
results, as shown in Section 4, the features are not
necessarily independent of one another.
5.2 Combination of features
Tables 6 and 7 show the distributions of extracted
key sentences as answer summaries with two pairs
of features: sentence position and the tf*idf value,
and sentence position and the headline information.
In these tables, each sentence is ranked by each of
the two feature values, and the rankings are split ev-
ery 10 percent. For example, if a sentence is ranked
in the first 10 percent by sentence position and the
last 10 percent by the tf*idf feature, the sentence be-
longs to the cell with a position rank of 0.1 and a
tf*idf rank of 1.0 in Table 6.
Each cell thus has two letters. The left letter is the
number of key sentences, and the right letter is the
ratio of key sentences to all sentences in the cell. The
left letter shows how the number of sentences differs
from the average when all the key sentences appear
equally, regardless of the feature values. Let T be
2Here we used equation T1 for the tf*idf feature, and the
score of each sentence was normalized with the sentence length.
Hence, the high correlation between ?Length? and ?Tf*idf? is
not trivial.
the total number of key sentences, M(= T100) be the
average number of key sentences in each range, and
S be the standard deviation of the number of key
sentences among all cells. The number of key sen-
tences for cell Ti,j is then categorized according to
one of the following letters:
A: Ti,j ? M + 2S
B: M + S ? Ti,j < M + 2S
C: M ? S ? Ti,j < M + S
D: M ? 2S ? Ti,j < M ? S
E: Ti,j < M ? 2S
O: Ti,j = 0
-: No sentences exist in the cell.
Similarly, the right letter in a cell shows how the ra-
tio of key sentences differs from the average ratio
when all the key sentences appear equally, regard-
less of feature values. Let N be the total number
of sentences, m(= TN ) be the average ratio of key
sentences, and s be the standard deviation of the ra-
tio among all cells. The ratio of key sentences for
cell ti,j is then categorized according to one of the
following letters:
a: ti,j ? m+ 2s
b: m+ s ? ti,j < m+ 2s
c: m? s ? ti,j < m+ s
d: m? 2s ? ti,j < m? s
e: ti,j < m? 2s
o: ti,j = 0
-: No sentences exist in the cell.
When key sentences appear uniformly regardless of
feature values, every cell is defined as ?Cc?. We
show both the range of the number of key sentences
and the ratio of key sentences, because both are nec-
essary to show how effectively a cell has key sen-
tences. If a cell includes many sentences, the num-
ber of key sentences can be large even though the
ratio is not. On the other hand, when the ratio of key
sentences is large and the number is not, the contri-
bution to key sentence extraction is small.
Table 6 shows the distributions of key sentences
when the features of sentence position and tf*idf
were combined. For the DUC data, both the num-
ber and ratio of key sentences were large when the
sentence position was ranked within the first 20 per-
cent and the value of the tf*idf feature was ranked
in the bottom 50 percent (i.e., Pst. ? 0.2, Tf*idf ?
0.5). On the other hand, both the number and ratio
of key sentences were large for the CSJ data when
the sentence position was ranked in the last 10 per-
cent and the value of the tf*idf feature was ranked
Table 6: Distributions of key sentences based on the combination of the sentence position (Pst.) and tf*idf
features.
DUC data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cb Ba Ba Aa Aa Aa Aa
0.2 Cd Cc Cc Cc Cc Cc Bb Bb Bb Bb
0.3 Cd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.5 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.6 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Cd Dd Cc Cc Cc Cc Cc Cc Cc Cc
0.9 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
1.0 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc
CSJ data
Tf*idf
Pst. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.1 Cc Cc Cc Cc Bc Cc Ab Bb Bb Bb
0.2 Oo Oo Cc Cc Cc Cc Cc Bb Bc Cc
0.3 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.4 Cc Cc Cc Cc Oo Cc Cc Cc Cc Cc
0.5 Oo Cc Oo Oo Cc Oo Cc Cc Cc Cc
0.6 Cc Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.7 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc
0.8 Oo Cc Cc Oo Oo Cc Cc Cc Cc Cc
0.9 Cc Cc Cc Cc Cc Cc Cc Cc Cc Bb
1.0 Cc Cc Ba Bb Bb Aa Aa Bb Aa Aa
after the first 30 percent (i.e., Pst. = 1.0, Tf*idf ?
0.3),. When the tf*idf feature was low, the number
and ratio of key sentences were not large, regardless
of the sentence position values. These results show
that the tf*idf feature is effective when the values
are used as a filter after the sentences are ranked by
sentence position.
Table 7 shows the distributions of key sentences
with the combination of the sentence position and
headline features. About half the sentences did not
share words with the headlines and had a value of
0 for the headline feature. As a result, the cells in
the middle of the table do not have corresponding
sentences. The headline feature cannot be used as
a filter, unlike the tf*idf feature, because many key
sentences are found when the value of the headline
feature is 0. A high value of the headline feature is,
however, a good indicator of key sentences when it
is combined with the position feature. The ratio of
key sentences was large when the headline ranking
was high and the sentence was near the beginning
(at Pst. ? 0.2, Headline ? 0.7) for the DUC data.
For the CSJ data, the ratio of key sentences was also
large when the headline ranking was within the top
10 percent (Pst. = 0.1, Headline = 1.0), as well as
for the sentences near the ends of speeches.
These results indicate that the number and ratio
of key sentences sometimes vary discretely accord-
ing to the changes in feature values when features
are combined for sentence extraction. That is, the
performance of a sentence extraction system can be
improved by categorizing feature values into sev-
eral ranges and then combining ranges. While most
sentence extraction systems use sequential combi-
nations of features, as we do in our system based
on Equation 1, the performance of these systems
can possibly be improved by introducing the cat-
egorization of feature values, without adding any
new features. We have shown that discrete combi-
nations match the distributions of key sentences in
two different corpora, the DUC data and the CSJ
data. This indicates that discrete combinations of
corpora are effective across both different languages
and different types of corpora. Hirao et al (2002)
reported the results of a sentence extraction system
using an SVM, which categorized sequential feature
values into ranges in order to make the features bi-
nary. Some effective combinations of the binary fea-
Table 7: Distributions of key sentences based on
the combination of the sentence position (Pst.) and
headline features.
DUC data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Ab -- -- Ca Ba Ba Aa
0.2 Ac -- -- Cb Cc Ca Ca
0.3 Ac -- -- Cc Cc Cb Cb
0.4 Ac -- -- Cc Cc Cc Cb
0.5 Ac -- -- Cc Cc Cc Cc
0.6 Bc -- -- Cc Cc Cc Cc
0.7 Bc -- -- Cc Cc Cc Cc
0.8 Ac -- -- Cd Cc Cc Cc
0.9 Bd -- -- Cd Cc Cc Cc
1.0 Bd -- -- Cd Cc Cc Cc
CSJ data
Headline
Pst. 0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.0
0.1 Bc -- Cc Cc Bb Cc Aa
0.2 Bc -- Cc Cb Cc Cc Bb
0.3 Cc -- Cc Cc Cc Cc Cc
0.4 Cc -- Oo Cc Cc Cc Cc
0.5 Cc -- Oo Cc Oo Cc Cc
0.6 Cc -- Cc Cc Cc Cc Cc
0.7 Cc -- Oo Cc Cc Cc Cc
0.8 Cc -- Cc Cc Cc Cc Cc
0.9 Ac -- Ca Cc Cc Cc Cb
1.0 Ab -- Ca Aa Ba Ba Ba
tures in that report also indicate the effectiveness of
discrete combinations of features.
6 Conclusion
We have shown evaluation results for our sentence
extraction system and analyzed its features for dif-
ferent types of corpora, which included corpora dif-
fering in both language (Japanese and English) and
type (newspaper articles and lectures). The sys-
tem is based on four major features, and it achieved
some of the top results at evaluation workshops in
2001 for summarizing Japanese newspaper articles
(TSC) and English newspaper articles (DUC). For
Japanese lectures, the sentence extraction system
also obtained comparable results when the sentence
boundary was given.
Our analysis of the features used in this sentence
extraction system has shown that they are not neces-
sarily independent of one another, based on the re-
sults of their rank correlation coefficients. The anal-
ysis also indicated that the categorization of feature
values matches the distribution of key sentences bet-
ter than sequential feature values.
There are several features that were not described
here but are also used in sentence extraction sys-
tems, such as some specific lexical expressions and
syntactic information. In our future work, we will
analyze and use these features to improve the per-
formance of our sentence extraction system.
References
C. Aone, M. E. Okurowski, and J. Gorlinsky. 1998. Train-
able, Scalable Summarization Using Robust NLP and Ma-
chine Learning. In Proc. of COLING-ACL?98, pages 62?66.
DUC. 2001. http://duc.nist.gov. Document Understanding
Conference.
H. Edmundson. 1969. New methods in automatic abstracting.
Journal of ACM, 16(2):264?285.
T. Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto. 2002. Ex-
tracting Important Sentences with Support Vector Machines.
In Proc. of COLING-2002.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A Trainable Docu-
ment Summarizaer. In Proc. of SIGIR?95, pages 68?73.
S. Kurohashi and M. Nagao, 1999. Japanese Morphological
Analyzing System: JUMAN version 3.61. Kyoto University.
Chin-Yew Lin. 1999. Training a selection function for extrac-
tion. In Proc. of the CIKM?99.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000. Spon-
taneous Speech Corpus of Japanese. In Proc. of LREC2000,
pages 947?952.
C. Nobata, S. Sekine, H. Isahara, and R. Grishman. 2002. Sum-
marization System Integrated with Named Entity Tagging
and IE pattern Discovery. In Proceedings of the LREC-2002
Conference, pages 1742?1745, May.
T. Nomoto and Y. Matsumoto. 1997. The Reliability of Human
Coding and Effects on Automatic Abstracting (in Japanese).
In IPSJ-NL 120-11, pages 71?76, July.
S. E. Robertson and S. Walker. 1994. Some simple effec-
tive approximations to the 2-poisson model for probabilistic
weighted retreival. In Proc. of SIGIR?94.
K. Sudo, S. Sekine, and R. Grishman. 2001. Automatic pattern
acquisition for japanese information extraction. In Proc. of
HLT-2001.
TSC. 2001. Proceedings of the Second NTCIR Workshop
on Research in Chinese & Japanese Text Retrieval and Text
Summarization (NTCIR2). National Institute of Informat-
ics.
 
	 ffProceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513?521,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
An Error-Driven Word-Character Hybrid Model
for Joint Chinese Word Segmentation and POS Tagging
Canasai Kruengkrai?? and Kiyotaka Uchimoto? and Jun?ichi Kazama?
Yiou Wang? and Kentaro Torisawa? and Hitoshi Isahara??
?Graduate School of Engineering, Kobe University
1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan
{canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp
Abstract
In this paper, we present a discriminative
word-character hybrid model for joint Chi-
nese word segmentation and POS tagging.
Our word-character hybrid model offers
high performance since it can handle both
known and unknown words. We describe
our strategies that yield good balance for
learning the characteristics of known and
unknown words and propose an error-
driven policy that delivers such balance
by acquiring examples of unknown words
from particular errors in a training cor-
pus. We describe an efficient framework
for training our model based on the Mar-
gin Infused Relaxed Algorithm (MIRA),
evaluate our approach on the Penn Chinese
Treebank, and show that it achieves supe-
rior performance compared to the state-of-
the-art approaches reported in the litera-
ture.
1 Introduction
In Chinese, word segmentation and part-of-speech
(POS) tagging are indispensable steps for higher-
level NLP tasks. Word segmentation and POS tag-
ging results are required as inputs to other NLP
tasks, such as phrase chunking, dependency pars-
ing, and machine translation. Word segmenta-
tion and POS tagging in a joint process have re-
ceived much attention in recent research and have
shown improvements over a pipelined fashion (Ng
and Low, 2004; Nakagawa and Uchimoto, 2007;
Zhang and Clark, 2008; Jiang et al, 2008a; Jiang
et al, 2008b).
In joint word segmentation and the POS tag-
ging process, one serious problem is caused by
unknown words, which are defined as words that
are not found in a training corpus or in a sys-
tem?s word dictionary1. The word boundaries and
the POS tags of unknown words, which are very
difficult to identify, cause numerous errors. The
word-character hybrid model proposed by Naka-
gawa and Uchimoto (Nakagawa, 2004; Nakagawa
and Uchimoto, 2007) shows promising properties
for solving this problem. However, it suffers from
structural complexity. Nakagawa (2004) described
a training method based on a word-based Markov
model and a character-based maximum entropy
model that can be completed in a reasonable time.
However, this training method is limited by the
generatively-trained Markov model in which in-
formative features are hard to exploit.
In this paper, we overcome such limitations
concerning both efficiency and effectiveness. We
propose a new framework for training the word-
character hybrid model based on the Margin
Infused Relaxed Algorithm (MIRA) (Crammer,
2004; Crammer et al, 2005; McDonald, 2006).
We describe k-best decoding for our hybrid model
and design its loss function and the features appro-
priate for our task.
In our word-character hybrid model, allowing
the model to learn the characteristics of both
known and unknown words is crucial to achieve
optimal performance. Here, we describe our
strategies that yield good balance for learning
these two characteristics. We propose an error-
driven policy that delivers this balance by acquir-
ing examples of unknown words from particular
errors in a training corpus. We conducted our ex-
periments on Penn Chinese Treebank (Xia et al,
2000) and compared our approach with the best
previous approaches reported in the literature. Ex-
perimental results indicate that our approach can
achieve state-of-the-art performance.
1A system?s word dictionary usually consists of a word
list, and each word in the list has its own POS category. In
this paper, we constructed the system?s word dictionary from
a training corpus.
513
Figure 1: Lattice used in word-character hybrid model.
Tag Description
B Beginning character in a multi-character word
I Intermediate character in a multi-character word
E End character in a multi-character word
S Single-character word
Table 1: Position-of-character (POC) tags.
The paper proceeds as follows: Section 2 gives
background on the word-character hybrid model,
Section 3 describes our policies for correct path
selection, Section 4 presents our training method
based on MIRA, Section 5 shows our experimen-
tal results, Section 6 discusses related work, and
Section 7 concludes the paper.
2 Background
2.1 Problem formation
In joint word segmentation and the POS tag-
ging process, the task is to predict a path
of word hypotheses y = (y1, . . . , y#y) =
(?w1, p1?, . . . , ?w#y, p#y?) for a given character
sequence x = (c1, . . . , c#x), where w is a word,
p is its POS tag, and a ?#? symbol denotes the
number of elements in each variable. The goal of
our learning algorithm is to learn a mapping from
inputs (unsegmented sentences) x ? X to outputs
(segmented paths) y ? Y based on training sam-
ples of input-output pairs S = {(xt, yt)}Tt=1.
2.2 Search space representation
We represent the search space with a lattice based
on the word-character hybrid model (Nakagawa
and Uchimoto, 2007). In the hybrid model,
given an input sentence, a lattice that consists
of word-level and character-level nodes is con-
structed. Word-level nodes, which correspond to
words found in the system?s word dictionary, have
regular POS tags. Character-level nodes have spe-
cial tags where position-of-character (POC) and
POS tags are combined (Asahara, 2003; Naka-
gawa, 2004). POC tags indicate the word-internal
positions of the characters, as described in Table 1.
Figure 1 shows an example of a lattice for a Chi-
nese sentence: ? ? (Chongming is
China?s third largest island). Note that some nodes
and state transitions are not allowed. For example,
I and E nodes cannot occur at the beginning of the
lattice (marked with dashed boxes), and the transi-
tions from I to B nodes are also forbidden. These
nodes and transitions are ignored during the lattice
construction processing.
In the training phase, since several paths
(marked in bold) can correspond to the correct
analysis in the annotated corpus, we need to se-
lect one correct path yt as a reference for training.2
The next section describes our strategies for deal-
ing with this issue.
With this search space representation, we
can consistently handle unknown words with
character-level nodes. In other words, we use
word-level nodes to identify known words and
character-level nodes to identify unknown words.
In the testing phase, we can use a dynamic pro-
gramming algorithm to search for the most likely
path out of all candidate paths.
2A machine learning problem exists called structured
multi-label classification that allows training from multiple
correct paths. However, in this paper we limit our considera-
tion to structured single-label classification, which is simple
yet provides great performance.
514
3 Policies for correct path selection
In this section, we describe our strategies for se-
lecting the correct path yt in the training phase.
As shown in Figure 1, the paths marked in bold
can represent the correct annotation of the seg-
mented sentence. Ideally, we need to build a word-
character hybrid model that effectively learns the
characteristics of unknown words (with character-
level nodes) as well as those of known words (with
word-level nodes).
We can directly estimate the statistics of known
words from an annotated corpus where a sentence
is already segmented into words and assigned POS
tags. If we select the correct path yt that corre-
sponds to the annotated sentence, it will only con-
sist of word-level nodes that do not allow learning
for unknown words. We therefore need to choose
character-level nodes as correct nodes instead of
word-level nodes for some words. We expect that
those words could reflect unknown words in the
future.
Baayen and Sproat (1996) proposed that the
characteristics of infrequent words in a training
corpus resemble those of unknown words. Their
idea has proven effective for estimating the statis-
tics of unknown words in previous studies (Ratna-
parkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We adopt Baayen and Sproat?s approach as
the baseline policy in our word-character hybrid
model. In the baseline policy, we first count the
frequencies of words3 in the training corpus. We
then collect infrequent words that appear less than
or equal to r times.4 If these infrequent words are
in the correct path, we use character-level nodes
to represent them, and hence the characteristics of
unknown words can be learned. For example, in
Figure 1 we select the character-level nodes of the
word ? ? (Chongming) as the correct nodes. As
a result, the correct path yt can contain both word-
level and character-level nodes (marked with as-
terisks (*)).
To discover more statistics of unknown words,
one might consider just increasing the threshold
value r to obtain more artificial unknown words.
However, our experimental results indicate that
our word-character hybrid model requires an ap-
propriate balance between known and artificial un-
3We consider a word and its POS tag a single entry.
4In our experiments, the optimal threshold value r is se-
lected by evaluating the performance of joint word segmen-
tation and POS tagging on the development set.
known words to achieve optimal performance.
We now describe our new approach to lever-
age additional examples of unknown words. In-
tuition suggests that even though the system can
handle some unknown words, many unidentified
unknown words remain that cannot be recovered
by the system; we wish to learn the characteristics
of such unidentified unknown words. We propose
the following simple scheme:
? Divide the training corpus into ten equal sets
and perform 10-fold cross validation to find
the errors.
? For each trial, train the word-character hybrid
model with the baseline policy (r = 1) us-
ing nine sets and estimate errors using the re-
maining validation set.
? Collect unidentified unknown words from
each validation set.
Several types of errors are produced by the
baseline model, but we only focus on those caused
by unidentified unknown words, which can be eas-
ily collected in the evaluation process. As de-
scribed later in Section 5.2, we measure the recall
on out-of-vocabulary (OOV) words. Here, we de-
fine unidentified unknown words as OOV words
in each validation set that cannot be recovered by
the system. After ten cross validation runs, we
get a list of the unidentified unknown words de-
rived from the whole training corpus. Note that
the unidentified unknown words in the cross val-
idation are not necessary to be infrequent words,
but some overlap may exist. Finally, we obtain the
artificial unknown words that combine the uniden-
tified unknown words in cross validation and in-
frequent words for learning unknown words. We
refer to this approach as the error-driven policy.
4 Training method
4.1 Discriminative online learning
Let Yt = {y1t , . . . , yKt } be a lattice consisting of
candidate paths for a given sentence xt. In the
word-character hybrid model, the lattice Yt can
contain more than 1000 nodes, depending on the
length of the sentence xt and the number of POS
tags in the corpus. Therefore, we require a learn-
ing algorithm that can efficiently handle large and
complex lattice structures.
Online learning is an attractive method for
the hybrid model since it quickly converges
515
Algorithm 1 Generic Online Learning Algorithm
Input: Training set S = {(xt, yt)}Tt=1
Output: Model weight vector w
1: w(0) = 0;v = 0; i = 0
2: for iter = 1 to N do
3: for t = 1 to T do
4: w(i+1) = update w(i) according to (xt, yt)
5: v = v +w(i+1)
6: i = i+ 1
7: end for
8: end for
9: w = v/(N ? T )
within a few iterations (McDonald, 2006). Algo-
rithm 1 outlines the generic online learning algo-
rithm (McDonald, 2006) used in our framework.
4.2 k-best MIRA
We focus on an online learning algorithm called
MIRA (Crammer, 2004), which has the de-
sired accuracy and scalability properties. MIRA
combines the advantages of margin-based and
perceptron-style learning with an optimization
scheme. In particular, we use a generalized ver-
sion of MIRA (Crammer et al, 2005; McDonald,
2006) that can incorporate k-best decoding in the
update procedure. To understand the concept of k-
best MIRA, we begin with a linear score function:
s(x, y;w) = ?w, f(x, y)? , (1)
where w is a weight vector and f is a feature rep-
resentation of an input x and an output y.
Learning a mapping between an input-output
pair corresponds to finding a weight vector w such
that the best scoring path of a given sentence is
the same as (or close to) the correct path. Given
a training example (xt, yt), MIRA tries to estab-
lish a margin between the score of the correct path
s(xt, yt;w) and the score of the best candidate
path s(xt, y?;w) based on the current weight vector
w that is proportional to a loss function L(yt, y?).
In each iteration, MIRA updates the weight vec-
tor w by keeping the norm of the change in the
weight vector as small as possible. With this
framework, we can formulate the optimization
problem as follows (McDonald, 2006):
w(i+1) = argminw?w ?w(i)? (2)
s.t. ?y? ? bestk(xt;w(i)) :
s(xt, yt;w)? s(xt, y?;w) ? L(yt, y?) ,
where bestk(xt;w(i)) ? Yt represents a set of top
k-best paths given the weight vector w(i). The
above quadratic programming (QP) problem can
be solved using Hildreth?s algorithm (Yair Cen-
sor, 1997). Replacing Eq. (2) into line 4 of Al-
gorithm 1, we obtain k-best MIRA.
The next question is how to efficiently gener-
ate bestk(xt;w(i)). In this paper, we apply a dy-
namic programming search (Nagata, 1994) to k-
best MIRA. The algorithm has two main search
steps: forward and backward. For the forward
search, we use Viterbi-style decoding to find the
best partial path and its score up to each node in
the lattice. For the backward search, we use A?-
style decoding to generate the top k-best paths. A
complete path is found when the backward search
reaches the beginning node of the lattice, and the
algorithm terminates when the number of gener-
ated paths equals k.
In summary, we use k-best MIRA to iteratively
update w(i). The final weight vector w is the av-
erage of the weight vectors after each iteration.
As reported in (Collins, 2002; McDonald et al,
2005), parameter averaging can effectively avoid
overfitting. For inference, we can use Viterbi-style
decoding to search for the most likely path y? for
a given sentence x where:
y? = argmax
y?Y
s(x, y;w) . (3)
4.3 Loss function
In conventional sequence labeling where the ob-
servation sequence (word) boundaries are fixed,
one can use the 0/1 loss to measure the errors of
a predicted path with respect to the correct path.
However, in our model, word boundaries vary
based on the considered path, resulting in a dif-
ferent numbers of output tokens. As a result, we
cannot directly use the 0/1 loss.
We instead compute the loss function through
false positives (FP ) and false negatives (FN ).
Here, FP means the number of output nodes that
are not in the correct path, and FN means the
number of nodes in the correct path that cannot
be recognized by the system. We define the loss
function by:
L(yt, y?) = FP + FN . (4)
This loss function can reflect how bad the pre-
dicted path y? is compared to the correct path yt.
A weighted loss function based on FP and FN
can be found in (Ganchev et al, 2007).
516
ID Template Condition
W0 ?w0? for word-level
W1 ?p0? nodes
W2 ?w0, p0?
W3 ?Length(w0), p0?
A0 ?AS(w0)? if w0 is a single-
A1 ?AS(w0), p0? character word
A2 ?AB(w0)? for word-level
A3 ?AB(w0), p0? nodes
A4 ?AE(w0)?
A5 ?AE(w0), p0?
A6 ?AB(w0), AE(w0)?
A7 ?AB(w0), AE(w0), p0?
T0 ?TS(w0)? if w0 is a single-
T1 ?TS(w0), p0? character word
T2 ?TB(w0)? for word-level
T3 ?TB(w0), p0? nodes
T4 ?TE(w0)?
T5 ?TE(w0), p0?
T6 ?TB(w0), TE(w0)?
T7 ?TB(w0), TE(w0), p0?
C0 ?cj?, j ? [?2, 2] ? p0 for character-
C1 ?cj , cj+1?, j ? [?2, 1] ? p0 level nodes
C2 ?c?1, c1? ? p0
C3 ?T (cj)?, j ? [?2, 2] ? p0
C4 ?T (cj), T (cj+1)?, j ? [?2, 1] ? p0
C5 ?T (c?1), T (c1)? ? p0
C6 ?c0, T (c0)? ? p0
Table 2: Unigram features.
4.4 Features
This section discusses the structure of f(x, y). We
broadly classify features into two categories: uni-
gram and bigram features. We design our feature
templates to capture various levels of information
about words and POS tags. Let us introduce some
notation. We write w?1 and w0 for the surface
forms of words, where subscripts ?1 and 0 in-
dicate the previous and current positions, respec-
tively. POS tags p?1 and p0 can be interpreted in
the same way. We denote the characters by cj .
Unigram features: Table 2 shows our unigram
features. Templates W0?W3 are basic word-level
unigram features, where Length(w0) denotes the
length of the word w0. Using just the surface
forms can overfit the training data and lead to poor
predictions on the test data. To alleviate this prob-
lem, we use two generalized features of the sur-
face forms. The first is the beginning and end
characters of the surface (A0?A7). For example,
?AB(w0)? denotes the beginning character of the
current word w0, and ?AB(w0), AE(w0)? denotes
the beginning and end characters in the word. The
second is the types of beginning and end charac-
ters of the surface (T0?T7). We define a set of
general character types, as shown in Table 4.
Templates C0?C6 are basic character-level un-
ID Template Condition
B0 ?w?1, w0? if w?1 and w0
B1 ?p?1, p0? are word-level
B2 ?w?1, p0? nodes
B3 ?p?1, w0?
B4 ?w?1, w0, p0?
B5 ?p?1, w0, p0?
B6 ?w?1, p?1, w0?
B7 ?w?1, p?1, p0?
B8 ?w?1, p?1, w0, p0?
B9 ?Length(w?1), p0?
TB0 ?TE(w?1)?
TB1 ?TE(w?1), p0?
TB2 ?TE(w?1), p?1, p0?
TB3 ?TE(w?1), TB(w0)?
TB4 ?TE(w?1), TB(w0), p0?
TB5 ?TE(w?1), p?1, TB(w0)?
TB6 ?TE(w?1), p?1, TB(w0), p0?
CB0 ?p?1, p0? otherwise
Table 3: Bigram features.
Character type Description
Space Space
Numeral Arabic and Chinese numerals
Symbol Symbols
Alphabet Alphabets
Chinese Chinese characters
Other Others
Table 4: Character types.
igram features taken from (Nakagawa, 2004).
These templates operate over a window of ?2
characters. The features include characters (C0),
pairs of characters (C1?C2), character types (C3),
and pairs of character types (C4?C5). In addi-
tion, we add pairs of characters and character types
(C6).
Bigram features: Table 3 shows our bigram
features. Templates B0-B9 are basic word-
level bigram features. These features aim to
capture all the possible combinations of word
and POS bigrams. Templates TB0-TB6 are the
types of characters for bigrams. For example,
?TE(w?1), TB(w0)? captures the change of char-
acter types from the end character in the previ-
ous word to the beginning character in the current
word.
Note that if one of the adjacent nodes is a
character-level node, we use the template CB0 that
represents POS bigrams. In our preliminary ex-
periments, we found that if we add more features
to non-word-level bigrams, the number of features
grows rapidly due to the dense connections be-
tween non-word-level nodes. However, these fea-
tures only slightly improve performance over us-
ing simple POS bigrams.
517
(a) Experiments on small training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270 3,046 75,169
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 32
OOV (word) 0.0987 (790/8,008)
OOV (word & POS) 0.1140 (913/8,008)
(b) Experiments on large training corpus
Data set CTB chap. IDs # of sent. # of words
Training 1-270, 18,089 493,939
400-931,
1001-1151
Development 301-325 350 6,821
Test 271-300 348 8,008
# of POS tags 35
OOV (word) 0.0347 (278/8,008)
OOV (word & POS) 0.0420 (336/8,008)
Table 5: Training, development, and test data
statistics on CTB 5.0 used in our experiments.
5 Experiments
5.1 Data sets
Previous studies on joint Chinese word segmen-
tation and POS tagging have used Penn Chinese
Treebank (CTB) (Xia et al, 2000) in experiments.
However, versions of CTB and experimental set-
tings vary across different studies.
In this paper, we used CTB 5.0 (LDC2005T01)
as our main corpus, defined the training, develop-
ment and test sets according to (Jiang et al, 2008a;
Jiang et al, 2008b), and designed our experiments
to explore the impact of the training corpus size on
our approach. Table 5 provides the statistics of our
experimental settings on the small and large train-
ing data. The out-of-vocabulary (OOV) is defined
as tokens in the test set that are not in the train-
ing set (Sproat and Emerson, 2003). Note that the
development set was only used for evaluating the
trained model to obtain the optimal values of tun-
able parameters.
5.2 Evaluation
We evaluated both word segmentation (Seg) and
joint word segmentation and POS tagging (Seg
& Tag). We used recall (R), precision (P ), and
F1 as evaluation metrics. Following (Sproat and
Emerson, 2003), we also measured the recall on
OOV (ROOV) tokens and in-vocabulary (RIV) to-
kens. These performance measures can be calcu-
lated as follows:
Recall (R) = # of correct tokens# of tokens in test data
Precision (P ) = # of correct tokens# of tokens in system output
F1 = 2 ?R ? PR+ P
ROOV = # of correct OOV tokens# of OOV tokens in test data
RIV = # of correct IV tokens# of IV tokens in test data
For Seg, a token is considered to be a cor-
rect one if the word boundary is correctly iden-
tified. For Seg & Tag, both the word boundary and
its POS tag have to be correctly identified to be
counted as a correct token.
5.3 Parameter estimation
Our model has three tunable parameters: the num-
ber of training iterations N ; the number of top
k-best paths; and the threshold r for infrequent
words. Since we were interested in finding an
optimal combination of word-level and character-
level nodes for training, we focused on tuning r.
We fixed N = 10 and k = 5 for all experiments.
For the baseline policy, we varied r in the range
of [1, 5] and found that setting r = 3 yielded the
best performance on the development set for both
the small and large training corpus experiments.
For the error-driven policy, we collected unidenti-
fied unknown words using 10-fold cross validation
on the training set, as previously described in Sec-
tion 3.
5.4 Impact of policies for correct path
selection
Table 6 shows the results of our word-character
hybrid model using the error-driven and baseline
policies. The third and fourth columns indicate the
numbers of known and artificial unknown words
in the training phase. The total number of words
is the same, but the different policies yield differ-
ent balances between the known and artificial un-
known words for learning the hybrid model. Op-
timal balances were selected using the develop-
ment set. The error-driven policy provides addi-
tional artificial unknown words in the training set.
The error-driven policy can improve ROOV as well
as maintain good RIV, resulting in overall F1 im-
provements.
518
(a) Experiments on small training corpus
# of words in training (75,169)
Eval type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 63,997 11,172 0.9587 0.9509 0.9548 0.7557 0.9809baseline 64,999 10,170 0.9572 0.9489 0.9530 0.7304 0.9820
Seg & Tag error-driven 63,997 11,172 0.8929 0.8857 0.8892 0.5444 0.9377baseline 64,999 10,170 0.8897 0.8820 0.8859 0.5246 0.9367
(b) Experiments on large training corpus
# of words in training (493,939)
Eval Type Policy kwn. art. unk. R P F1 ROOV RIV
Seg error-driven 442,423 51,516 0.9829 0.9746 0.9787 0.7698 0.9906baseline 449,679 44,260 0.9821 0.9736 0.9779 0.7590 0.9902
Seg & Tag error-driven 442,423 51,516 0.9407 0.9328 0.9367 0.5982 0.9557baseline 449,679 44,260 0.9401 0.9319 0.9360 0.5952 0.9552
Table 6: Results of our word-character hybrid model using error-driven and baseline policies.
Method Seg Seg & Tag
Ours (error-driven) 0.9787 0.9367
Ours (baseline) 0.9779 0.9360
Jiang08a 0.9785 0.9341
Jiang08b 0.9774 0.9337
N&U07 0.9783 0.9332
Table 7: Comparison of F1 results with previous
studies on CTB 5.0.
Seg Seg & Tag
N&U07 Z&C08 Ours N&U07 Z&C08 Ours
Trial (base.) (base.)
1 0.9701 0.9721 0.9732 0.9262 0.9346 0.9358
2 0.9738 0.9762 0.9752 0.9318 0.9385 0.9380
3 0.9571 0.9594 0.9578 0.9023 0.9086 0.9067
4 0.9629 0.9592 0.9655 0.9132 0.9160 0.9223
5 0.9597 0.9606 0.9617 0.9132 0.9172 0.9187
6 0.9473 0.9456 0.9460 0.8823 0.8883 0.8885
7 0.9528 0.9500 0.9562 0.9003 0.9051 0.9076
8 0.9519 0.9512 0.9528 0.9002 0.9030 0.9062
9 0.9566 0.9479 0.9575 0.8996 0.9033 0.9052
10 0.9631 0.9645 0.9659 0.9154 0.9196 0.9225
Avg. 0.9595 0.9590 0.9611 0.9085 0.9134 0.9152
Table 8: Comparison of F1 results of our baseline
model with Nakagawa and Uchimoto (2007) and
Zhang and Clark (2008) on CTB 3.0.
Method Seg Seg & Tag
Ours (baseline) 0.9611 0.9152
Z&C08 0.9590 0.9134
N&U07 0.9595 0.9085
N&L04 0.9520 -
Table 9: Comparison of averaged F1 results (by
10-fold cross validation) with previous studies on
CTB 3.0.
5.5 Comparison with best prior approaches
In this section, we attempt to make meaning-
ful comparison with the best prior approaches re-
ported in the literature. Although most previous
studies used CTB, their versions of CTB and ex-
perimental settings are different, which compli-
cates comparison.
Ng and Low (2004) (N&L04) used CTB 3.0.
However, they just showed POS tagging results
on a per character basis, not on a per word basis.
Zhang and Clark (2008) (Z&C08) generated CTB
3.0 from CTB 4.0. Jiang et al (2008a; 2008b)
(Jiang08a, Jiang08b) used CTB 5.0. Shi and
Wang (2007) used CTB that was distributed in the
SIGHAN Bakeoff. Besides CTB, they also used
HowNet (Dong and Dong, 2006) to obtain seman-
tic class features. Zhang and Clark (2008) indi-
cated that their results cannot directly compare to
the results of Shi and Wang (2007) due to different
experimental settings.
We decided to follow the experimental settings
of Jiang et al (2008a; 2008b) on CTB 5.0 and
Zhang and Clark (2008) on CTB 4.0 since they
reported the best performances on joint word seg-
mentation and POS tagging using the training ma-
terials only derived from the corpora. The perfor-
mance scores of previous studies are directly taken
from their papers. We also conducted experiments
using the system implemented by Nakagawa and
Uchimoto (2007) (N&U07) for comparison.
Our experiment on the large training corpus is
identical to that of Jiang et al (Jiang et al, 2008a;
Jiang et al, 2008b). Table 7 compares the F1 re-
sults with previous studies on CTB 5.0. The result
of our error-driven model is superior to previous
reported results for both Seg and Seg & Tag, and
the result of our baseline model compares favor-
ably to the others.
Following Zhang and Clark (2008), we first
generated CTB 3.0 from CTB 4.0 using sentence
IDs 1?10364. We then divided CTB 3.0 into
ten equal sets and conducted 10-fold cross vali-
519
dation. Unfortunately, Zhang and Clark?s exper-
imental setting did not allow us to use our error-
driven policy since performing 10-fold cross val-
idation again on each main cross validation trial
is computationally too expensive. Therefore, we
used our baseline policy in this setting and fixed
r = 3 for all cross validation runs. Table 8 com-
pares the F1 results of our baseline model with
Nakagawa and Uchimoto (2007) and Zhang and
Clark (2008) on CTB 3.0. Table 9 shows a sum-
mary of averaged F1 results on CTB 3.0. Our
baseline model outperforms all prior approaches
for both Seg and Seg & Tag, and we hope that
our error-driven model can further improve perfor-
mance.
6 Related work
In this section, we discuss related approaches
based on several aspects of learning algorithms
and search space representation methods. Max-
imum entropy models are widely used for word
segmentation and POS tagging tasks (Uchimoto
et al, 2001; Ng and Low, 2004; Nakagawa,
2004; Nakagawa and Uchimoto, 2007) since they
only need moderate training times while they pro-
vide reasonable performance. Conditional random
fields (CRFs) (Lafferty et al, 2001) further im-
prove the performance (Kudo et al, 2004; Shi
and Wang, 2007) by performing whole-sequence
normalization to avoid label-bias and length-bias
problems. However, CRF-based algorithms typ-
ically require longer training times, and we ob-
served an infeasible convergence time for our hy-
brid model.
Online learning has recently gained popularity
for many NLP tasks since it performs comparably
or better than batch learning using shorter train-
ing times (McDonald, 2006). For example, a per-
ceptron algorithm is used for joint Chinese word
segmentation and POS tagging (Zhang and Clark,
2008; Jiang et al, 2008a; Jiang et al, 2008b).
Another potential algorithm is MIRA, which in-
tegrates the notion of the large-margin classifier
(Crammer, 2004). In this paper, we first intro-
duce MIRA to joint word segmentation and POS
tagging and show very encouraging results. With
regard to error-driven learning, Brill (1995) pro-
posed a transformation-based approach that ac-
quires a set of error-correcting rules by comparing
the outputs of an initial tagger with the correct an-
notations on a training corpus. Our approach does
not learn the error-correcting rules. We only aim to
capture the characteristics of unknown words and
augment their representatives.
As for search space representation, Ng and
Low (2004) found that for Chinese, the character-
based model yields better results than the word-
based model. Nakagawa and Uchimoto (2007)
provided empirical evidence that the character-
based model is not always better than the word-
based model. They proposed a hybrid approach
that exploits both the word-based and character-
based models. Our approach overcomes the limi-
tation of the original hybrid model by a discrimi-
native online learning algorithm for training.
7 Conclusion
In this paper, we presented a discriminative word-
character hybrid model for joint Chinese word
segmentation and POS tagging. Our approach
has two important advantages. The first is ro-
bust search space representation based on a hy-
brid model in which word-level and character-
level nodes are used to identify known and un-
known words, respectively. We introduced a sim-
ple scheme based on the error-driven concept to
effectively learn the characteristics of known and
unknown words from the training corpus. The sec-
ond is a discriminative online learning algorithm
based on MIRA that enables us to incorporate ar-
bitrary features to our hybrid model. Based on ex-
tensive comparisons, we showed that our approach
is superior to the existing approaches reported in
the literature. In future work, we plan to apply
our framework to other Asian languages, includ-
ing Thai and Japanese.
Acknowledgments
We would like to thank Tetsuji Nakagawa for his
helpful suggestions about the word-character hy-
brid model, Chen Wenliang for his technical assis-
tance with the Chinese processing, and the anony-
mous reviewers for their insightful comments.
References
Masayuki Asahara. 2003. Corpus-based Japanese
morphological analysis. Nara Institute of Science
and Technology, Doctor?s Thesis.
Harald Baayen and Richard Sproat. 1996. Estimat-
ing lexical priors for low-frequency morphologi-
cally ambiguous forms. Computational Linguistics,
22(2):155?166.
520
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Koby Crammer, Ryan McDonald, and Fernando
Pereira. 2005. Scalable large-margin online learn-
ing for structured classification. In NIPS Workshop
on Learning With Structured Outputs.
Koby Crammer. 2004. Online Learning of Com-
plex Categorial Problems. Hebrew Univeristy of
Jerusalem, PhD Thesis.
Zhendong Dong and Qiang Dong. 2006. Hownet and
the Computation of Meaning. World Scientific.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/umass/chop biocreative ii systems. In Pro-
ceedings of the Second BioCreative Challenge Eval-
uation Workshop.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu?.
2008a. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proceedings of ACL.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for chinese word segmentation and
part-of-speech tagging. In Proceedings of COLING.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Ryan McDonald, Femando Pereira, Kiril Ribarow, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP, pages 523?530.
Ryan McDonald. 2006. Discriminative Training and
Spanning Tree Algorithms for Dependency Parsing.
University of Pennsylvania, PhD Thesis.
Masaki Nagata. 1994. A stochastic japanese mor-
phological analyzer using a forward-DP backward-
A* n-best search algorithm. In Proceedings of
the 15th International Conference on Computational
Linguistics, pages 201?207.
Masaki Nagata. 1999. A part of speech estimation
method for japanese unknown words using a statis-
tical model of morphology and context. In Proceed-
ings of ACL, pages 277?284.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and pos tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sions.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level
information. In Proceedings of COLING, pages
466?472.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, pages 277?284.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
crfs based joint decoding method for cascaded seg-
mentation and labeling tasks. In Proceedings of IJ-
CAI.
Richard Sproat and Thomas Emerson. 2003. The first
international chinese word segmentation bakeoff. In
Proceedings of the 2nd SIGHAN Workshop on Chi-
nese Language Processing, pages 133?143.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The unknown word problem: a morpho-
logical analysis of japanese using maximum entropy
aided by a dictionary. In Proceedings of EMNLP,
pages 91?99.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu dong Chiou, and
Shizhe Huang. 2000. Developing guidelines and
ensuring consistency for chinese text annotation. In
Proceedings of LREC.
Stavros A. Zenios Yair Censor. 1997. Parallel Op-
timization: Theory, Algorithms, and Applications.
Oxford University Press.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging on a single perceptron. In
Proceedings of ACL.
521
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 73?76
Manchester, August 2008
The "Close-Distant" Relation of Adjectival Concepts                
Based on Self-Organizing Map 
Kyoko Kanzaki, Hitoshi Isahara 
National Institute of Information and 
Communications Technology 
3-5, Hikaridai, Seikacho, 
Sorakugun, Kyoto, 619-0289,  
Japan 
{kanzaki,isahara}@nict.go.jp
Noriko Tomuro 
School of Computer Science, Telecom-
munications and Information Systems 
DePaul University 
Chicago, IL 60604 
U.S.A 
tomuro@cs.depaul.edu 
 
Abstract 
In this paper we aim to detect some as-
pects of adjectival meanings. Concepts of 
adjectives are distributed by SOM (Self-
Organizing map) whose feature vectors 
are calculated by MI (Mutual Informa-
tion). For the SOM obtained, we make 
tight clusters from map nodes, calculated 
by cosine. In addition, the number of 
tight clusters obtained by cosine was in-
creased using map nodes and Japanese 
thesaurus. As a result, the number of ex-
tended clusters of concepts was 149 clus-
ters. From the map, we found 8 adjectival 
clusters in super-ordinate level and some 
tendencies of similar and dissimilar clus-
ters. 
1 Introduction 
This paper aims to find a diversity range of ad-
jectival meanings from a coordinate map in 
which  "close-distant" relationships between ad-
jectival classes is reflected. In related research 
over adjectives, Alonge et.al (2000), Solar (2003), 
Marrafa and Mendes (2006) suggested that 
WordNet and EuroWordNet lack sufficient ad-
jectival classes and semantic relations, and  ex-
tended the resources over such relations. 
For the sake of identifying the diversity of ad-
jectival meanings, it is necessary to analyze ad-
jectival semantics via "close-distant" relation-
ships extracted from texts. In our work on ex-
tracting adjective semantics, we consider abstract 
nouns as semantic proxies of adjectives. For the 
clustering method, we utilized a self-organizing 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
map (SOM) based on a neural network model 
(Kohonen, 1997). One of the features of SOM is 
that it assigns words coordinates, allowing for 
the possibility of visualizing word similarity. 
SOM has two advantages for our task. One is 
that we can utilize the map nodes of words to 
locate members of clusters that clustering meth-
ods have failed to classify. The other is that the 
map shows the relative relations of whole clus-
ters of adjectival concepts. By observing such a 
map in which the relations of clusters are re-
flected, we can analyze the diversity of adjectival 
meaning. 
2 Abstract Nouns that Categorize Ad-
jectives 
Collocations between adjectives and nouns in 
?concrete value and its concept? relations can be 
used to represent adjectival semantics. Nemoto 
(1969) indicated that expressions such as ?iro ga 
akai (the color is red)? and ?hayasa ga hayai 
(literally, the speed is fast)? are a kind of 
tautology. Some studies have suggested that 
some abstract nouns collocating with adjectives 
are hypernymic concepts (or concepts) of those 
adjectives, and that some semantic relations 
between abstract nouns and adjectives represent 
a kind of repetition of meaning. 
   This paper defines such abstract nouns as the 
semantic categorization of an adjective (or an 
adjectival concept). 
The data for this study was obtained by ex-
tracting adjectives co-occurring with abstract 
nouns in 100 novels, 100 essays, and 42 years of 
newspaper articles. 
We extracted the abstract nouns according to 
the procedure described by Kanzaki et.al (2006). 
Here, they evaluated the category labels of adjec-
tives obtained by the proposed procedure and 
found that for 63% of the adjectives, the ex-
73
tracted categories were found to be appropriate. 
We constructed a list as follows: 
Abstract Nouns:  
Adjectives modifying abstract nouns   
KIMOCHI (feeling):  
ureshii (glad), kanashii (sad), 
shiawasena (happy) ? 
In this list,  ?KIMOCHI (feeling)? is defined by 
?ureshii (glad), kanashii (sad), and shiawasena 
(happy)?, for example. Here, each abstract noun 
conveys the concept or hypernym of the given 
adjectives. 
Next we classify these abstract nouns based on 
their co-occurring adjectives using SOM. 
3. A Map of Adjective Semantics  
3.1 Input Data 
In our SOM, we use adjectives which occur more 
than four times in our corpus. The number of 
such adjectives was 2374. Then we identified 
361 abstract nouns that co-occurred with four or 
more of the adjectives. The maximum number of 
co-occurring adjectives for a given abstract noun 
in the corpus was 1,594. 
    In the data, each abstract noun was defined by 
a feature vector, in the form of noun co-
occurrences represented by pointwise mutual 
information (Manning and Schutze, 1999). Mu-
tual information (MI) is an information theoric 
measure and has been used in many NLP tasks, 
including clustering words (e.g. Lin and Pantel, 
2002). 
3.2 SOM 
Kohonen?s self-organizing map (SOM) is an un-
supervised learning method, where input in-
stances are projected onto a grid/map of nodes 
arranged in an n-dimensional space. Input in-
stances are usually high-dimensional data, while 
the map is usually two-dimensional (i.e., n = 2). 
Thus, SOM essentially reduces the dimensional-
ity of the data, and can be used as an effective 
tool for data visualization ? projecting complex, 
high-dimensional data onto a low-dimensional 
map. SOM can also be utilized for clustering. 
Each node in a map represents a cluster and is 
associated with a reference vector of m-
dimensions, where m is the dimension of the in-
put instances. During learning, input instances 
are mapped to a map node whose (current) refer-
ence vector is the closest to the instance vector 
(where SOM uses Euclidean distance as the 
measure of similarity by default), and the refer-
ence vectors are gradually smoothed so that the 
differences between the reference vector and the 
instance vectors mapped to the node are mini-
mized. This way, instances mapped to the same 
node form a cluster, and the reference vector es-
sentially corresponds to the centroid of the clus-
ter. 
SOM maps are self-organizing in the sense 
that input instances that are similar are gradually 
pulled closer during learning and assigned to 
nodes that are topographically close to one an-
other on the map. The mapping from input in-
stances to map nodes is one-to-one (i.e., one in-
stance is assigned to exactly one node), but from 
map nodes to instances, the mapping is one-to-
many (i.e., one map node is assigned to zero, one, 
or more instances). 
The input data was the set of 361 abstract 
nouns defined by the 2,374 co-occurring adjec-
tives, as described in the previous section. These 
abstract nouns were distributed visually on the 2-
dimensional map based on co-occurring adjec-
tives. This map is a ?map of adjective semantics? 
because the abstract nouns are identified as prox-
ies for adjective semantics.  
As mentioned before, similar words are lo-
cated in neighboring nodes on the 2-dimensional 
map. The next step is to identify similar clusters 
on the map. 
4. Clusters of Adjective Semantics 
4.1 Tight Clusters from the Map Nodes 
In SOMs, each node represents a cluster, i.e. a set 
of nouns assigned to the same node. These nouns 
are very similar and can be considered to be 
synonyms. However, nouns that are similar 
might map to different nodes because the algo-
rithm?s self-organization is sensitive to the pa-
rameter settings. To account for this, and also to 
obtain a more (coarse-grained) qualitative de-
scription of the map, tight clusters?clusters of 
map nodes whose reference vectors are signifi-
cantly close?were extracted. All groupings of 
map nodes whose average cosine coefficient be-
tween the reference vectors in the group was 
greater than 0.96 were extracted (Salton and 
McGill, 1983).  
4.2 Result  
The total number of clusters was 213. Excluding 
singleton clusters, the number of clustes was 81. 
229 concepts were classified into 81 clusters, 
with 132 concepts not classified into any cluster. 
74
In order to evaluate the quality of the concep-
tual classification, we utilized the ?Bunruigoi-
hyou?  Japanese thesaurus (National Institute of 
Japanese Language, 1964). In ?Bunruigoihyou,? 
each category is assigned a 5-digit category 
number, with close numbers indicating similar 
categories.  
Among the 81 with two or more concepts, the 
number of clusters containing words with the 
same class was 36. That is, for 44% of the clus-
ters, the constituent nouns had the same ?Bun-
ruigoihyou? class label. The ratio of concept 
agreement between "Bunruigiohyou? and our 
obtained clusters was found to be  20.87/81=0.25.  
We also compared tight clusters by performing  
hierarchical clustering with the k-means algo-
rithm. 
The results of the hierarchical clustering were as 
follows: 
1) The rate of clusters agreeing with ?Bunruigoi-
hyou?: 30/96 = 0.31 
2) The average rate of agreement for each tight 
cluster: 21.07/96 = 0.21 
In the case of k-means: 
3)The rate of clusters agreeing with ?Bunruigoi-
hyou?: 33/143 = 0.23 
4) The average rate of agreement for each tight   
cluster: 28.37/143 = 0.198 
From these results, we can observe that clus-
ters obtained with cosine similarity agree more 
with the Japanese thesaurus than the other two 
methods. Therefore, in terms of quality, clusters 
obtained by cosine similarity seem to be superior 
to the others. 
4.3 Using the Position of Map Nodes 
However, even for the result obtained with co-
sine similarity, 132 concepts were not classified 
into any clusters. Additionally, the clusters ap-
pear to be overly fine grained: most tight clusters 
include 1, 2 or 3 concepts. In order to find simi-
lar concepts that cosine similarity failed to clus-
ter together, we used the position information of 
the map nodes.  
After we plotted clusters obtained by cosine 
similarity on the map, we checked for singleton 
concepts located near a cluster which are mem-
bers of the same ?Bunruigoihyou? class.  Also, 
we checked to see if concepts in clusters located 
at neighboring nodes could be clustered together  
using the category numbers of ?Bunruigoihyou. ? 
By extending the clusters, we generated a total 
of 149 clusters, including 68 with two or more 
elements and 81 singleton clusters. 
5. Interpreting the Adjectival Clusters 
In our final map, 361 concepts were distributed 
based on 2374 adjectives into 149 clusters. 
Among the 149 clusters, 68 contained two or 
more concepts.  
5.1 ?Close-Distant? Relations of Clusters and 
Adjectives 
In the final map, clusters at the superordinate 
level are located around the center of the map. 
Upper level concepts tend to agree with clusters 
in ?Bunruigoihyou.?  For examples, ?image and 
impression,? ?situation and state?, ?feeling and 
mood? are located around the center of the map. 
 
 
 
 
 
 
 
 
 
 
Cluster1 (Center of the map): koto (matter), 
in?shou (impression), men (side of some-
thing or someone), and kankaku 
(sense/feeling) 
Cluster2: seishitsu (characteristics of some-
one/something), yousou (aspect)  
Cluster3: kanten (viewpoint), tachiba (stand-
point), bun?ya (domain) 
Cluster4: taido (attitude), yarikata (way of do-
ing) 
Cluster5: gaikan, gaiken, sugata (outlook and 
appearance of someone/something) 
Cluster6:  fun?iki, kuuki, kehai (atmosphere) 
Cluster7:  kimochi, kanji (feeling) 
Cluster8:  joutai (state), joukyou (situation) 
 
In our experiment, at the top level, adjectival 
concepts seem to be divided into 8 basic clusters. 
From the distribution of the map, we find ?close-
distant? relationships between clusters, that is 
clusters located far from each other tend to be 
semantically disparate. In terms of adjective se-
mantics, the semantic relationship between ?ki-
mochi, kanji (feeling)? (Cluster7) and ?seishitsu 
(characteristics of someone/something), yousou 
(aspect)? are distant. 
However, ?kimochi, kanji (feeling)? (Cluster7) 
has a close relation to ?fun?iki, kuuki, kehai 
(atmosphere) ? (Cluster6) and also  ?joutai (state), 
joukyou (situation)? (Cluster8). 
Fig7. Cluster 7 on the map 
1
2 
4 3 
5
Center of a map 6
7
8
75
1. In our experiment, 77 adjectives belonged 
to one or two clusters. Though there is the 
possibility of data sparseness, there is also 
the possibility that the meanings of these 
adjectives are specific. Examples of adjec-
tives belonging to specific clusters are as 
follows: 
 
Adjectives in distant relationships; 
- Clusters 2: keisandakai (seeing everything in 
terms of money), ken?meina 
(wise), ? 
- Cluster 7: akkenai (disappointing/easily), kiya-
sui (feel at home),? 
 
Adjectives in close relationships; 
- Cluster 6: ayashigena (fishy) 
- Cluster7: akkenai (disappointing /easily), kiya-
sui (feel at home) 
- Cluster8: meihakuna (obvious), omoshiroi (in-
teresting), makkurana (dark) 
 
Japanese adjectives are often said to represent 
?kanjou (mental state)?, ?joutai (state),? ?seisitsu 
(characteristics)? and ?teido (degree)?, in addi-
tion to ?positive/negative image.? In our experi-
ment, the SOM unearthed not only these adjecti-
val meanings, but also ?inshou (impression)?, 
?taido (attitude)?, ?kanten (viewpoint)? and 
?sugata (outlook)?, which seem to be discrimina-
tive meanings of adjectives. 
6. Future work 
We classified 361 concepts based on 2374 adjec-
tives using a self-organizing map. Since the 
SOM shows the distribution visually, it provides 
not only clusters of adjectives but also ?close-
distant? relationships between clusters. As a re-
sult, adjectival concepts at the superordinate 
level are divided into 8 main clusters. The results 
not only verify previous work but also suggest 
new discriminative adjective classes. One of the 
advantages of SOM is that it presents its outputs 
visually. As a result, we can explore ?close- dis-
tant? relationships between clusters, and  analyze 
the meaning of each. In addition to increasing the 
range of adjectival classes and improving our 
method, our method provides the means to ana-
lyze concepts which did not agree with those in 
existing thesauri such as ?Bunruigoihyou?, the 
EDR dictionary or Japanese Word Net. 
 
 
 
References 
Alonge, Antonietta., Francesca Bertagna, Nicoletta 
Calzolari, Andriana Roventini and Antonio Zam-
polli. 2000. Encoding Information on Adjectives in 
a Lexical-semantic Net for Computational Applica-
tions, Proceedings of the 1st Conference of the 
North American Chapter of the Association for 
Computational Linguistics(NAACL-00) :42-49 
Kyoko Kanzaki,Qing Ma, Eiko Yamamoto and 
Hitoshi Isahara, 2006, Semantic Analysis of 
Abstract Nouns to Compile a Thesaurus of 
Adjectives, In Proceedings of The Interna-
tional Conference on Language Resources 
and Evaluation (LREC-06) 
Kohonen, Teuvo. 1997. Self-Organizing Maps, Sec-
ond Edition, Springer. 
Lin, Dekang., and Patrick Pantel. 2002. Concept Dis-
covery from Text, Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics(COLING-02): 768-774  
Manning, Christopher D., and Hinrich Sh?tze. 1999. 
Foundations of Statistical Natural language Proc-
essing, The MIT Press. 
Marrafa, Palmira., and Sara Mendes. 2006. Modeling 
Adjectives in Computational Relational Lexica, 
Proceedings of the COLING/ACL2006:555-562 
National Institute for Japanese Language. 1964. Bun-
ruigoihyou (Word List by Semantic Principles). 
Nemoto, Kesao. 1969. The combination of the noun 
with ?ga-Case? and the adjective, Language re-
search 2 for the computer, National Language Re-
search Institute: 63-73 (in Japanese) 
Salton, Gerard., and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval.  McGraw 
Hill. 
Solar, Clara. 2003. Extension of Spanish WordNet, 
Proceedings of the third International WordNet 
Conference(GWC-06):213-219 
 
76
