Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 109?113,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Joint Syntactic and Semantic Dependency Parsing System based on
Maximum Entropy Models
Buzhou Tang1 Lu Li2 Xinxin Li1 Xuan Wang2 Xiaolong Wang2
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen,518055, China
1{tangbuzhou,lixxin2}@gmail.com
2{lli,wangxuan,wangxl}@insun.hit.edu.cn
Abstract
A joint syntactic and semantic dependency
parsing system submitted to the CoNLL-2009
shared task is presented in this paper. The
system is composed of three components: a
syntactic dependency parser, a predicate clas-
sifier and a semantic parser. The first-order
MSTParser is used as our syntactic depen-
dency pasrser. Projective and non-projective
MSTParsers are compared with each other on
seven languages. Predicate classification and
semantic parsing are both recognized as clas-
sification problem, and the Maximum Entropy
Models are used for them in our system. For
semantic parsing and predicate classifying, we
focus on finding optimized features on multi-
ple languages. The average Macro F1 Score
of our system is 73.97 for joint task in closed
challenge.
1 Introduction
The task for CoNLL-2009 is an extension of the
CoNLL-2008 shared task to multiple languages: En-
glish (Surdeanu et al, 2008), Catalan plus Span-
ish (Mariona Taule? et al, 2008), Chinese (Martha
Palmer et al, 2009), Czech (Jan Hajic? et al,
2006), German (Aljoscha Burchardt et al, 2006) and
Japanese (Daisuke Kawahara et al, 2002). Com-
pared to the CoNLL-2008 shared task, the predi-
cates are given for us in semantic dependencies task.
Therefore, we have only need to label the semantic
roles of nouns and verbs, and the frames of predi-
cates.
In this paper, a joint syntactic and semantic de-
pendency parsing system submitted to the CoNLL-
2009 shared task is presented. The system is com-
posed of three components: a syntactic dependency
parser, a predicate classifier and a semantic parser.
The first-order MSTParser is used as our syntactic
dependency parser. Projective and non-projective
MSTParsers are compared with each other on seven
languages. The predicate classifier labeling the
frames of predicates and the semantic parser label-
ing the semantic roles of nouns and verbs for each
predicate are both recognized as classification prob-
lem, and the Maximum Entropy Models (MEs) are
used for them in our system. Among three com-
ponents, we mainly focus on the predicate classifier
and the semantic parser.
For semantic parsing and predicate classifying,
features of different types are selected to our sys-
tem. The effect of them on multiple languages will
be described in the following sections in detail.
2 System Description
Generally Speaking, a syntactic and semantic de-
pendency parsing system is usually divided into four
separate subtasks: syntactic parsing, predicate iden-
tification, predicate classification, and semantic role
labeling. In the CoNLL-2009 shared task, the pred-
icate identification is not required, since the pred-
icates are given for us. Therefore, the system we
present is only composed of three components: a
syntactic dependency parser, a predicate classifier
and a semantic parser. The syntactic dependencies
are processed with the MSTParser 0.4.3b. The pred-
icates identification and semantic role label are pro-
cessed with MEs-based classifier respectively. Un-
like conventional systems, the predicates identifica-
109
tion and the semantic parser are independent with
each other. Figure 1 is the architecture of our sys-
tem.
Figure 1: System Architecture
In our system, we firstly select an appropriate
mode (projective or non-projective) of Graph-based
Parser (MSTParser) for each language, then con-
struct the MEs-based predicates classification and
the MEs-based semantic parser with syntactic de-
pendency relationships and predicate classification
respectively.
2.1 Syntactic Dependency Parsing
MSTParser (McDonald, 2008) is used as our syn-
tactic dependency parser. It is a state-of-the-art de-
pendency parser that searches for maximum span-
ning trees (MST) over directed graph. Both of pro-
jective and non-projective are supported by MST-
Parser. Our system employs the first-order frame-
work with projective and non-projective modes on
seven given languages.
2.2 Predicate Classification
In this phase, we label the sense of each predicate
and the MEs are adopted for classification. Features
of different types are extracted for each predicate,
and an optimized combination of them is adopted in
our final system. Table 1 lists all features. 1-20 are
the features used in Li?s system (Lu Li et al, 2008),
No Features No Features
1 w0 20 Lemma
2 p0 21 DEPREL
3 p?1 22 CHD POS
4 p1 23 CHD POS U
5 p?1p0 24 CHD REL
6 p0p1 25 CHD REL U
7 p?2p0 26 SIB REL
8 p0p2 27 SIB REL U
9 p?3p0 28 SIB POS
10 p0p3 29 SIB POS U
11 p?1p0p1 30 VERB V
12 w0p0 31 4+11
13 w0p?1p0 32 Indegree
14 w0p0p1 33 Outdegree
15 w0p?2p0 34 Degree
16 w0p0p2 35 ARG IN
17 w0p?3p0 36 ARG OUT
18 w0p0p3 37 ARG Degree
19 w0p?1p0p1 38 Span
Table 1: Features for Predicate Classification.
and 21-31 are a part of the optimized features pre-
sented in Che?s system (Wanxiang Che et al, 2008)
In Table 1, ?w? denotes the word and ?p? de-
notes POS of the words. Features in the form of
part1 part2 denote the part2 of the part1, while fea-
tures in the form of part1+part2 denote the combi-
nation of the part1 and part2. ?CHD? and ?SIB? de-
note a sequence of the child and the sibling words
respectively, ?REL? denotes the type of relations,
?U? denotes the result after reducing the adjacent
duplicate tags to one, ?V? denotes whether the part
is a voice, ?In? and ?OUT? denote the in degree and
out degree, which denotes how many dependency
relations coming into this word and going away from
this word,and ?ARG? denotes the semantic roles of
the predicate. The ?Span? denotes the maximum
length between the predicate and its arguments. The
final optimized feature combination is :1-31 and 33-
37.
2.3 Semantic Role Labeling
The semantic role labeling usually contains two sub-
tasks: argument identification and argument classi-
fication. In our system, we perform them in a single
110
stage through one classifier, which specifies a par-
ticular role label to the argument candidates directly
and assigns ?NONE? label to the argument candi-
dates with no role. MEs are also adopted for classifi-
cation. For each word in a sentence, MEs gives each
candidate label (including semantic role labels and
none label) a probability for the predicate. The fea-
tures except for the feature (lemma plus sense num-
ber of the predicate in (Lu Li et al, 2008)) and the
features 32-38 in Table 1 are selected in our system.
3 Experiments and Results
We train the first-order MSTParser 1 with projective
and non-projective modes in terms of default param-
eters respectively. Our maximum entropy classifiers
are implemented with the Maximum Entropy Mod-
eling Toolkit 2 . The default classifier parameters are
used in our system except for iterations. All mod-
els are trained using all training data, and tested on
the whole development data and test data, with 64-
bit 3.00GHz Intel(R) Pentium(R) D CPU and 4.0G
memory.
3.1 Syntactic Dependency Parsing
Table 2 is a performance comparison between pro-
jective parser and non-projective parser on the devel-
opment data of seven languages. In Table 2, ?LAS?,
?ULAS? and ?LCS? denote as Labeled attachment
score, Unlabeled attachment score and Label accu-
racy score respectively.
The experiments show that Catalan, Chinese and
Spanish have projective property and others have
non-projective property.
3.2 Predicate Classification
To get the optimized system, three group features are
used for comparison.
? group 1: features 1-20 in Table 1.
? group 2: features 1-31 in Table 1.
? group 3: all features in Table 1.
The performance of predicate classification on the
development data of the six languages, which con-
tain this subtask, are given in Table 3. The results
1http://sourceforge.net/projects/mstparser.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
LAS(%) ULAS(%) LCS(%)
Catalan 84.18 88.18 91.76
83.69 87.74 91.59
Chinese 72.58 77.06 82.07
62.85 69.47 73.00
Czech 72.79 81.40 80.93
73.18 81.86 81.30
English 86.89 90.29 91.50
86.88 90.34 91.58
German 83.43 86.89 90.24
84.00 87.40 90.61
Japanese 92.23 93.16 98.38
92.23 93.14 98.45
Spanish 83.88 87.93 91.36
83.46 87.46 91.37
Table 2: Performance of Syntactic Dependency
Parsing with different modes. The above line is the
performance of projective mode, while the below
one is the performance of non-projective mode for
each language.
group 1 group 2 group 3
Catalan 75.51 80.90 82.23
Chinese 93.79 94.99 94.75
Czech 91.83 91.77 91.86
English 92.12 92.48 93.20
German 74.49 74.14 75.85
Spanish 74.01 76.22 76.53
Table 3: Performance of predicate classification (F1
scores) for different group features on the develop-
ment data of the six languages.
show that Che?s features and the degrees of the pred-
icate and its arguments are useful for all languages,
the former improves the labeled F1 measure by 0.3%
to 5.4%, and the latter by 0.3% to 1.7%.
3.3 Semantic Role Labeling
In this phase, feature selection and performance lose
caused by P-columns are studied. Firstly, we com-
pare the following two group features:
? group 1: The features except for the lemma
plus sense number of the predicate in (Lu Li
et al, 2008).
111
LF1 ULF1 PF1
Catalan 73.25 92.69 38.41
72.71 91.93 35.22
83.23 100.00 61.88
Chinese 69.60 82.15 28.35
71.49 81.71 29.41
85.44 95.21 58.20
Czech 80.62 92.49 70.04
79.10 91.44 68.34
85.42 96.93 77.78
English 73.91 87.26 33.16
76.10 88.58 36.28
79.35 91.74 43.32
German 64.85 88.05 27.21
65.36 88.63 26.70
72.78 94.54 41.50
Japanese 69.43 82.79 29.27
69.87 83.31 29.69
72.80 87.13 34.96
Spanish 73.49 93.15 39.64
78.18 91.68 33.57
81.96 99.98 59.20
Table 4: Performance of Semantic Role Labeling
(F1 score) with different features.
? group 2: group1+the degrees of the predicate
and its arguments presented in the last section.
Secondly, features extracted from golden-columns
and P-columns are both used for testing.
The performance of them are given in Table 4,
where ?LF1?, ?ULF1? and ?PF1? denote as Labeled
F1 score, Unlabeled F1 score and Proposition F1
score respectively. The above line is the F1 scores of
Semantic Role Labeling with different features. The
uppermost line is the result of group1 features, the
middle line is the result of group2 features extracted
from P-columns, and the downmost one is the result
of group2 features extracted from golden-columns
for each language.
The results show that the features of degree also
improves the labeled F1 measure by 3.4% to 15.8%,
the different labeled F1 between golden-columns
and P-columns is about 2.9%?13.9%.
LAS LF1 M LF1
Catalan 84.18 72.71 81.46
75.68 66.95 71.32
Chinese 72.58 71.49 72.20
63.95 67.06 65.53
Czech 73.18 79.10 76.37
72.60 79.08 75.85
Czech-ood 69.81 79.80 74.81
English 86.88 76.10 82.89
86.61 77.17 81.92
English-ood 80.09 67.21 73.69
German 84.00 65.36 83.06
79.85 61.98 70.93
German-ood 71.86 61.83 66.86
Japanese 92.23 69.87 83.77
91.26 69.58 80.49
Spanish 83.88 71.18 80.74
77.21 66.23 71.72
Table 5: Overall performance of our final joint sys-
tem.
3.4 Overall Performance
In the final system, we select the optimized feature
subset discussed in the former sections. The overall
performance of the system on the development data ,
test data and Out-of-domain data are shown in Table
5 (all features are extracted from P-columns). The
average Macro F1 Scores of our system are 73.97
on test data and 71.79 on Out-of-domain data.
In Table 5, ?LAS?, ?LF1? and ?M LF1? denote
as Labeled accuracy score for Syntactic Dependency
Parsing, Labeled F1 score for Semantic Role Label-
ing, and Overall Macro Labeled F1 score respec-
tively. The topmost line is the result on the devel-
opment data, the middle one is the result on the test
data for each language and the downmost one is the
result on the Out-of-domain data if the data exist.
4 Conclusion and Discussion
We present a joint syntactic and semantic depen-
dency parsing system for CoNLL2009 Shared Task,
which composed of three components: a syntac-
tic dependency parser, a predicate classifier and a
semantic parser. All of them are built with some
state-of-the-art methods. For the predicate classifier
and the semantic parser, a new kind of features?
112
degrees, which reflect the activeness of the words
in a sentence improves their performance. In order
to improve the performance further, we will study
new machine learning methods for semantic depen-
dency parsing, especially the joint learning methods,
which can avoid the information loss problem of our
system.
Acknowledgments
We would like to thank McDonald for providing
the MSTParser program, to Zhang Le for provid-
ing the Maxent program. This research has been
partially supported by the National Natural Science
Foundation of China(No.60703015) and the Na-
tional 863 Program of China (No.2006AA01Z197,
No.2007AA01Z194).
References
Jan Hajic? and Massimiliano Ciaramita and Richard Jo-
hansson and Daisuke Kawahara and Maria Anto`nia
Mart?? and Llu??s Ma`rquez and Adam Meyers and
Joakim Nivre and Sebastian Pado? and Jan S?te?pa?nek
and Pavel Stran?a?k and Miahi Surdeanu and Nianwen
Xue and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5. Boulder, Colorado, USA.
Mariona Taule? and Maria Anto`nia Mart?? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008). Marrakesh, Morroco.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1),pages 143?172.
Jan Hajic? and Jarmila Panevova? and Eva Hajic?ova? and
Petr Sgall and Petr Pajas and Jan S?te?pa?nek and Jir???
Havelka and Marie Mikulova? and Zdene?k Z?abokrtsky?.
2006. Prague Dependency Treebank 2.0. CD-ROM,
Cat. No. LDC2006T01, ISBN 1-58563-370-4. Lin-
guistic Data Consortium, Philadelphia, Pennsylvania,
USA. URL: http://ldc.upenn.edu.
Surdeanu, Mihai and Johansson, Richard and Meyers,
Adam and Ma`rquez, Llu??s and Nivre, Joakim. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning(CoNLL-2008).
Aljoscha Burchardt and Katrin Erk and Anette Frank and
Andrea Kowalski and Sebastian Pado? and Manfred
Pinkal. 2006. The SALSA corpus: a German corpus
resource for lexical semantics. Proceedings of the 5rd
International Conference on Language Resources and
Evaluation (LREC-2006), pages 2008?2013. Genoa,
Italy.
Daisuke Kawahara and Sadao Kurohashi and Ko?iti
Hasida. 2002. Construction of a Japanese Relevance-
tagged Corpus. Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013. Las Palmas, Canary
Islands.
McDonald and Ryan. 2006. Discriminative Learning
and Spanning Tree Algorithms for Dependency Pars-
ing, Ph.D. thesis. University of Pennsylvania.
Lu Li, Shixi Fan, Xuan Wang, XiaolongWang. 2008.
Discriminative Learning of Syntactic and Semantic
Dependencies. CoNLL 2008: Proceedings of the
12th Conference on Computational Natural Language
Learning, pages 218?222. Manchester.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, Sheng Li. 2008. A Cascaded
Syntactic and Semantic Dependency Parsing System.
CoNLL 2008: Proceedings of the 12th Conference
on Computational Natural Language Learning, pages
238?242. Manchester.
113
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 218?222
Manchester, August 2008
Discriminative Learning of Syntactic and Semantic Dependencies
Lu Li
1
, Shixi Fan
2
, Xuan Wang
1
, Xiaolong Wang
1
Shenzhen Graduate School, Harbin Institute of Technology,
Xili, Shenzhen 518055, China
1
{lli,wangxuan,wangxl}@insun.hit.edu.cn
2
fanshixi@hit.edu.cn
Abstract
A Maximum Entropy Model based system
for discriminative learning of syntactic and
semantic dependencies submitted to the
CoNLL-2008 shared task (Surdeanu, et al,
2008) is presented in this paper. The sys-
tem converts the dependency learning task
to classification issues and reconstructs the
dependent relations based on classification
results. Finally F1 scores of 86.69, 69.95
and 78.35 are obtained for syntactic depen-
dencies, semantic dependencies and the
whole system respectively in closed chal-
lenge. For open challenge the correspond-
ing F1 scores are 86.69, 68.99 and 77.84.
1 Introduction
Given sentences and corresponding part-of-speech
of each word, the learning of syntactic and seman-
tic dependency contains two separable goals: (1)
building a dependency tree that defines the syn-
tactic dependency relationships between separated
words; (2) specifying predicates (no matter verbs
or nouns) of the sentences and labeling the seman-
tic dependents for each predicate.
In this paper a discriminative parser is pro-
posed to implement maximum entropy (ME) mod-
els (Berger, et al, 1996) to address the learning
task. The system is divided into two main subsys-
tems: syntactic dependency parsing and semantic
dependency labeling. The former is used to find a
well-formed syntactic dependency tree that occu-
pies all the words in the sentence. If edges are
added between any two words, a full-connected
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
graph is constructed and the dependency tree could
be found using a maximum spanning tree (MST)
algorithm (McDonald, et al, 2005). The latter fo-
cuses on separable predicates whose semantic de-
pendents could be determined using classification
tools, such as ME models
1
etc..
We participated in both closed and open chal-
lenge of the CoNLL-2008 shared task (Surdeanu,
et al, 2008). Results are reported on both develop-
ment and test sets in this paper.
2 System Description
2.1 Syntactic Parsing
The goal of syntactic parsing is to create a la-
beled syntactic dependency parse y for input sen-
tence x including words and their parts of speech
(POS). Inspired by the parsing model that imple-
ments maximum spanning tree (MST) algorithm
to induce the dependency parsing tree (McDonald,
et al, 2005), the system employs the same frame-
work. The incorporated features are defined over
parts of speech of words occurring between and
around a possible head-dependent relation.
Suppose G = (V, E) is a directed graph, where
V is the set of vertices denoting the words in sen-
tence x and E is the set of directed edges between
any two vertices with some scores. The MST al-
gorithm is to find the most probable subgraph of G
that satisfies tree constraints over all vertices. The
score function of the parsing tree y is defined as
s(y) =
?
(i,j)?y
s(i, j) (1)
where (i, j) ? y indicates an edge in y from word
i to word j and s(i, j) denotes its score. Suppose Y
1
http://homepages.inf.ed.ac.uk/s0450736/maxent.html
218
wi
w
j
p
i
p
j
(w
i
, p
i
) (w
j
, p
j
)
(w
i
, w
j
) (p
i
, p
j
)
(w
i
, p
j
) (w
j
, p
i
)
(w
i
, w
j
, p
i
) (w
i
, w
j
, p
j
)
(p
i
, p
j
, w
i
) (p
i
, p
j
, w
j
)
(w
i
, w
j
, p
i
, p
j
) (p
i
, p
k
, p
j
), i < k < j
(p
i
, p
i+1
, p
j?1
, p
j
) (p
i?1
, p
i
, p
j?1
, p
j
)
(p
i
, p
i+1
, p
j
, p
j+1
) (p
i?1
, p
i
, p
j
, p
j+1
)
Table 1: Features for syntactic parsing.
is the set of syntactic dependency labels, the score
function of edges is defined as
s(i, j) = max
l?Y
Pr(l|x, i, j) (2)
ME models are used to calculate the value of
Pr(l|x, i, j), where the features are extracted from
input sentence x. Given i and j as the subscripts
of words in the sentence and word i is the parent
of word j, the features can be illustrated in table
1. w
i
and p
i
are denoted as the ith word and the
ith part of speech respectively in the sentence. The
tuples define integrated features, such as (w
i
, p
i
)
indicates the feature combining the ith word and
ith part of speech. Besides these features, the dis-
tant between word i and word j in sentence x is
considered as a single feature. The distant is also
combined with features in table 1 to produce com-
plex features.
2.2 Semantic Dependency Labeling
Semantic dependencies are always concerning
with specific predicates. Unlike syntactic depen-
dencies, semantic dependency relationships usu-
ally can not be represented as a tree. Thus, the
method used for semantic dependency labeling
is somewhat different from syntactic dependency
parsing. The work of semantic labeling can be di-
vided into two stages: predicate tagging and de-
pendents recognizing.
2.2.1 Predicate Tagging
According to PropBank (Palmer, et al, 2005)
and NomBank (Meyers, et al, 2004), predicates
usually have several rolesets corresponding to dif-
ferent meanings. For example, the verb abandon
has three rolesets marked as ordinal numbers 01,
02 and 03 as described below.
w
i
p
i
p
i?1
p
i+1
(p
i?1
, p
i
) (p
i
, p
i+1
)
(p
i?2
, p
i
) (p
i
, p
i+2
)
(p
i?3
, p
i
) (p
i
, p
i+3
)
(p
i?1
, p
i
, p
i+1
) (w
i
, p
i
)
(w
i
, p
i?1
, p
i
) (w
i
, p
i
, p
i+1
)
(w
i
, p
i?2
, p
i
) (w
i
, p
i
, p
i+2
)
(w
i
, p
i?3
, p
i
) (w
i
, p
i
, p
i+3
)
(w
i
, p
i?1
, p
i
, p
i+1
)
Table 2: Features used for predicate tagging.
<frameset>
<predicate lemma=?abandon?>
<roleset id=?abandon.01? name=?leave
behind? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.02?
name=?exchange? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.03?
name=?surrender, give over? vncls=?-
?>
. . .
</roleset>
</predicate>
</frameset>
The goal of this part is to identify the predicates
in the sentences and to determine the roleset for
each of them. It should be cleared that the ordi-
nal numbers are only used to distinguish different
meanings of a predicate. However, if these num-
bers are treated as tags for predicates, some statisti-
cal properties will be obtained as illustrated in Fig-
ure 1. As can be seen, the distribution of the train
data would be quite informative for representing
the distribution of other three data sets. Based on
this idea, a classification framework is introduced
for predicate tagging.
Suppose the tag set is chosen to be T =
{01, 02, ..., 22} according to the horizontal axis of
Figure 1 and 00 is added to indicate that the ex-
amining word is not a predicate. Suppose t
i
is a
variable indicating the tag of word at position i in
sentences x. ME models are implemented to tag
the predicates.
t
i
= argmax
t? T
Pr(t|x, i) (3)
219
0 5 10 15 200
2
4
6
8
10
12
Ordinal Numbers of Predicates
Log
arith
mic
al N
umb
er o
f Oc
curr
enc
e
traindevelbrownws j
Figure 1: Distribution of the ordinal numbers of
predicates on different data sets. 01 - 21 are at-
tached with the predicates in the corpus and 22
stands for ?SU?.
The features for predicate tagging are listed in ta-
ble 2, where the symbols share the same mean-
ing as in table 1. Experiments show that this pure
statistic processing method is effective for predi-
cate tagging.
2.2.2 Dependents Recognizing
This subtask depends deeply on the results of
syntactic parsing and predicate tagging described
earlier in the system. Predicate tagging identifies
central words and syntactic parsing provides syn-
tactic features for its dependents identification and
classification.
Generally speaking, given a specific predicate in
a sentence, only a few of words are associated as its
semantic dependents. By statistical analysis a list
of part of speech tuples that are appearing to be se-
mantic dependency are collected. All other tuples
are filtered out to improve system performance.
Suppose (p, d) is a couple of predicate and one
of its possible dependents, T is the dependency
tree generated by syntactic parsing, L is the set of
semantic dependency labels. The dependents can
be recognized by using a classification model, ME
models are chosen as before.
l
(p,d)
= argmax
l?L
Pr(l|p, d, T ) (4)
Besides the semantic dependency labels, null is in-
cluded as a special tag to indicate that there is no
semantic dependency between p and d. As a result,
dependents identification (binary classification)
and dependents tagging (multi-classification) can
be solved together within one multi-classification
framework.
The selected features are listed below.
1. Predicate Features
? Lemma and POS of predicate, pred-
icate?s parent in syntactic dependency
tree.
? Voice active or passive.
? Syntactic dependency label of edge be-
tween predicate and its parent.
? POS framework POS list of predicate?s
siblings, POS list of predicate?s children.
? Syntactic dependency framework syn-
tactic dependency label list of the edges
between predicate?s parent and its sib-
lings.
? Parent framework syntactic depen-
dency label list of edges connecting to
predicate?s parent.
2. Dependent Features
? Lemma and POS of dependent, depen-
dent?s parent.
? POS framework POS list of depen-
dent?s siblings.
? Number of children of dependent?s par-
ent.
3. In Between Features
? Position of dependent according to
predicate: before or after.
? POS pair of predicate and dependent.
? Family relation between predicate and
dependent: ancestor or descendant.
? Path length between predicate and de-
pendent.
? Path POS POS list of all words appear-
ing on the path from predicate to depen-
dent.
? Path syntactic dependency label list of
dependency label of edges of path be-
tween predicate and dependent.
3 Experiment results
The classification models were trained using all the
training data. The detailed information are shown
in table 3. All experiments ran on 32-bit Intel(R)
Pentium(R) D CPU 3.00GHz processors with 2.0G
memory.
220
Feature Number Training Time
Syn. 7,488,533 30h
Prd. 1,484,398 8h
Sem. 3,588,514 12h
Table 3: Details of ME models. Syn. is for syntac-
tic parsing, Prd. is for predicate tagging and Sem.
is for semantic dependents recognizing.
Syntactic Semantic Overall
devel 85.29 69.60 77.49
brown 80.80 59.17 70.01
wsj 87.42 71.27 79.38
brown+wsj 86.69 69.95 78.35
(a) Closed Challenge
Syntactic Semantic Overall
devel 85.29 68.45 76.87
brown 80.80 58.22 69.51
wsj 87.42 70.32 78.87
brown+wsj 86.69 68.99 77.84
(b) Open Challenge
Table 4: Scores for joint learning of syntactic and
semantic dependencies.
3.1 Closed Challenge
The system for closed challenge is designed as a
two-stage parser: syntactic parsing and semantic
dependency labeling as described previously. Ta-
ble 4(a) shows the results on different corpus. As
shown in table 4(a), the scores of semantic depen-
dency labeling are quite low, that are influencing
the overall scores. The reason could be inferred
from the description in section 2.2.2 since seman-
tic dependent labeling inherits the errors from the
output of syntactic parsing and predicate tagging.
Following evaluates each part independently.
Besides the multiple classification model de-
scribed in table 3, a binary classification model
was built based on ME for predicate tagging. The
binary model can?t distinguish different rolesets of
predicate, but can identify which words are predi-
cates in sentences. The precision and recall for bi-
nary model are 90.80 and 88.87 respectively, while
for multiple model, the values are 84.60 and 85.60.
For semantic dependent labeling, experiments
were performed under conditions that the gold syn-
tactic dependency tree and predicates list were
given as input. The semantic scores became 80.09,
77.08 and 82.25 for devel, brown and wsj respec-
tively. This implies that the error of syntactic pars-
ing and predicate tagging could be probably aug-
mented in semantic dependent labeling. In order to
improve the performance of the whole system, the
deep dependence between the two stages should be
broken up in future research.
3.2 Open Challenge
In open challenge, the same models are used for
syntactic parsing and predicate tagging as in closed
challenge and two other models are trained for se-
mantic dependent labeling. Suppose M
mst
, M
malt
and M
chunk
are denoted as these three semantic
models, where M
mst
is the model used in closed
challenge, M
malt
is trained on the syntactic de-
pendency tree provided by the open corpus with
the same feature set as M
mst
, and M
chunk
is
trained using features extracted from name entity
and wordnet super senses results provided by the
open corpus.
Considering a possible dependent given a spe-
cific predicate, the feature set used for M
chunk
contains only six elements:
? Whether the dependent is in name entity
chunk: True or False.
? Name entity label of the dependent.
? Whether the dependent is in BBN name entity
chunk: True or False.
? BBN name entity label of the dependent.
? Whether the dependent is in wordnet super
sense chunk: True or False.
? Wordnet super sense label of the dependent.
After implementing these three models on se-
mantic dependents recognizing, the results were
merged to generate the scores described in table
4(b).
The merging strategy is quite simple. Given a
couple of predicate and dependent (p, d), the sys-
tem produces three semantic dependency labels
denoting as l
mst
, l
malt
and l
chunk
, the result la-
bel is chosen to be most frequent semantic label
among the three.
Comparing the scores of open challenge and
closed challenge, it can be found that the score of
the former is less than the latter, which is quite
strange since more resources were used in open
challenge. To examine the influences of differ-
ent semantic dependents recognizing models, each
221
Mmst
M
malt
M
chunk
devel 69.60 64.48 41.72
brown 59.17 56.52 34.04
wsj 71.27 66.40 41.83
Table 5: Semantic scores of different models.
model was implemented in the closed challenge
and the results are shown in table 5. Specially,
model M
chunk
generated too low scores and gave a
heavy negative influence on the final results. Find-
ing a good way to combine several results requires
further research.
4 Conclusions
This paper have presented a simple discriminative
system submitted to the CoNLL-2008 shared task
to address the learning task of syntactic and se-
mantic dependencies. The system was divided into
syntactic parsing and semantic dependents label-
ing. Maximum spanning tree was used to find
a syntactic dependency tree in the full-connected
graph constructed over the words of a sentence.
Maximum entropy models were implemented to
classify syntactic dependency edges, predicates
and their semantic dependents. A brief analysis
has also been given on the results of both closed
challenge and open challenge.
Acknowledgement
This research has been partially supported by the
National Natural Science Foundation of China
(No. 60435020 and No. 90612005), the Goal-
oriented Lessons from the National 863 Program
of China (No.2006AA01Z197) and Project of Mi-
crosoft Research Asia. We would like to thank
Zhixin Hao, Xiao Xin, Languang He and Tao Qian
for their wise suggestion and great help. Thanks
also to Muhammad Waqas Anwar for English im-
provement.
References
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39-71.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young
and Ralph Grishman 2004. The NomBank Project:
An Interim Report HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, 24-31.
Martha Palmer, Daniel Gildea, Paul Kingsbury 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles Computational Linguistics, 31(1):71-
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez and Joakim Nivre 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008)
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP, 523-530.
222
