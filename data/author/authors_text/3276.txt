Answer Extraction 
Towards better Evaluations of NLP  Systems 
Ro l f  Schwi t te r  and D iego  Mo l l~  and Rache l  Fourn ie r  and Michae lHess  
Depar tment  of Informat ion Technology 
Computat iona l  Linguistics Group 
University of Zurich 
CH-8057 Zurich 
\[schwitter, molla, fournier, hess\] @ifi. unizh, ch 
Abst rac t  
We argue that reading comprehension tests are 
not particularly suited for the evaluation of 
NLP systems. Reading comprehension tests are 
specifically designed to evaluate human reading 
skills, and these require vast amounts of world 
knowledge and common-sense r asoning capa- 
bilities. Experience has shown that this kind of 
full-fledged question answering (QA) over texts 
from a wide range of domains is so difficult for 
machines as to be far beyond the present state 
of the art of NLP. To advance the field we pro- 
pose a much more modest evaluation set:up, viz. 
Answer Extraction (AE) over texts from highly 
restricted omains. AE aims at retrieving those 
sentences from documents that contain the ex- 
plicit answer to a user query. AE is less ambi- 
tious than full-fledged QA but has a number of 
important advantages over QA. It relies mainly 
on linguistic knowledge and needs only a very 
limited amount of world knowledge and few in- 
ference rules. However, it requires the solution 
of a number of key linguistic problems. This 
makes AE a suitable task to advance NLP tech- 
niques in a measurable way. Finally, there is a 
real demand for working AE systems in techni: 
cal domains. We outline how evaluation proce- 
dures for AE systems over real world domains 
might look like and discuss their feasibility. 
1 On  the  Des ign  o f  Eva luat ion  
Methods  for  NLP  Systems 
The idea that the systematic and principled 
evaluation of document processing systems is 
crucial for the development of the field as a 
whole has gained wide acceptance in the com- 
munity during the last decade. In a num- 
ber of large-scale projects (among them TREC 
(Voorhees and Harman, 1998) and MUC (MUC- 
7, 1998)), evaluation procedures for specific 
types of systems have been used extensively, and 
refined over the years. Three things were com- 
mon to these evaluations: First, the systems to 
be evaluated were each very closely tied to a par- 
ticular task (document retrieval and information 
extraction, respectively). Second, the evalua- 
tion was of the black box type, i.e. it considered 
only system input-output relations without re- 
gard to the specific mechanisms by which the 
outputs were obtained. Third, the amount of 
data to be processed was enormous (several gi- 
gabytes for TREC). 
There is general agreement that these com- 
petitive evaluations had a striking and bene- 
ficial effect on the performance of the various 
systems tested over the years. However, it is 
also recognized (albeit less generally) that these 
evaluation experiments also had the, less ben- 
eficial, effect that the participating systems fo- 
cussed increasingly more narrowly on those few 
parameters that were measured in the evalua- 
tion, to the detriment of more general prop- 
erties. In some cases this meant that power- 
ful and linguistically interesting but slow sys- 
tems were dropped in favour of shallow but fast 
systems with precious little linguistic content. 
Thus the system with which SRI participated 
in the MUC-3 evaluation in 1991, TACITUS 
(Hobbs et al, 1991), a true text-understanding 
system, was later replaced by FASTUS (Appelt 
et al, 1995; Hobbs et al, 1996), a much sim- 
pler, and vastly faster, information extraction 
system. The reason was that TACITUS was 
spending so much of its time attempting to make 
sense of portions of the text that were irrelevant 
to the task that recall was mediocre. We ar- 
gue that the set-up of these competitive valu- 
ations, and in particular the three parameters 
mentioned above, drove the development of the 
participating systems towards becoming impres- 
20 
sive feats of engineering, fine-tuned to one very 
specific task, but with limited relevance outside 
this task and with little linguistically relevant 
content. We argue that these evaluations there- 
fore did not drive progress in Computational 
Linguistics very much. 
We therefore think it a timely idea to con- 
ceive of evaluation methodologies which mea- 
sure the linguistically relevant functions of NLP 
systems and thus advance Computational Lin- 
guistics as a science rather than as an engineer- 
ing discipline. The suggestion made by the or- 
ganizers of this workshop on how this could be 
achieved has-four comPonents. First, they sug- 
gest to use full-fledged text-based question an- 
swering (QA) as task. Second, they suggest a 
relatively small amount off text (compared with 
the volumes of text used in TREC) as test data. 
Third they (seem to) suggest o .use texts from 
a wide range off domains. Finally they suggest 
to use pre-existing question/answer pairs, de- 
veloped for and tested on humans, as evaluation 
benchmark (Hirschman et al, 1999). 
However, our experience in the field leads us 
to believe that this evaluation set-up will not 
help Computational Linguistics as much as it 
would be needed, mainly because it is way too 
ambitious. We fear that this fact will force de- 
velopers, again, to design all kinds of ad-hoc so- 
lutions and efficiency hacks which will severely 
limit the scientific relevance of the resulting sys- 
tems. We argue that three of the four compo- 
nents of the suggested set-up must be reduced 
considerably in scope to make the test-bed help- 
ful. 
First, we think the task is too difficult. Full- 
fledged QA on the basis of natural language 
texts is far beyond the present state of the 
art. The example of the text-based QA sys- 
tem LILOG (Herzog and Rollinger, 1991) has 
shown that the analysis of texts to the depth 
required for real QA over their contents is so re- 
source intensive as to be unaffordable in any real 
world context.  After an investment of around 65 
person-years of work the LILOG system could 
answer questions over a few (reputedly merely 
three) texts of around one page length each from 
an extremely narrow domain (city guides and 
the like). We think it is fair to say that the situ- 
ation in our field has not changed enough in the 
meantime to invalidate this finding. 
Second, we agree that the volume off data to 
be used should be relatively small. We must 
avoid that the sheer pressure of the volumes of 
texts to be processed forces system developers 
to use shallow methods. 
Third, we think it is very important o restrict 
the domain of the task. We certainly do not ar- 
gue in favour of some toy domain but we get 
the impression that the reading comprehension 
texts under consideration cover a far too wide 
range of topics. We think that technical man- 
uals are a better choice. They cover a narrow 
domain (such as computer operating systems, 
or airplanes), and they also use a relatively re- 
stricted type of language with a reasonably clear 
semantic foundation. 
Fourth, we think that tests that are specif- 
ically designed to evaluate to what extent a 
human being understands a text are intrinsi- 
cally unsuitable for our present purposes. Al- 
though it would admittedly be very convenient 
to have "well written" texts, "good" questions 
about them and the "correct" answers all in one 
package, the texts are not "real world" language 
(in that they were written specifically for these 
tests), and the questions are:just far too difficult, 
primarily because they rely on exactly those 
components of language understanding where 
humans excel and computers are abominably 
poor (inferences over world knowledge). 
In Section 2 we outline what kinds of prob- 
lems would have to be solved by a QA sys- 
tem if it were to answer the test questions 
given in (WRC, 2000). Most of the prob- 
lems would require enormous amounts of world 
knowledge and vast numbers of lexical inference 
rules for a solution, on top of all the "classi- 
cal" linguistic problems our field has been strug- 
gling with (ambiguities, anaphoric references, 
synonymy/hyponymy).  We will then argue in 
Section 3 that a more restricted kind of task, 
Answer Extraction, is better suited as experi- 
mental set-up as it would focus our forces on 
these unsolved but reasonably well-understood 
problems, rather than divert them to the ill- 
understood and fathomless black' hole of world 
knowledge. In Section 4, we will finally outline 
how evaluation procedures in this context might 
look like. 
21 
...k 
2 Why Read ing  Comprehens ion  
Tests  v ia QA are  Too :Difficult 
Reading comprehension tests are designed to 
measure how well human readers understand 
what they read. Each story comes with a set 
of questions about information that is stated 
or implied in the text. The readers demon- 
strate their understanding of the story by an- 
swering the questions about it. Thus, read- 
ing comprehension tests assume a cognitive pro- 
cess of human beings. This process involves ex- 
panding the mental model of a text by using 
its implications and presuppositions, retrieving 
the stored information, performing inferences to 
make implicit information explicit, and generat- 
ing the surface strings that express this infor- 
mation. Many different forms of knowledge take 
part in this process: linguistic, procedural and 
world knowledge. All these forms coalesce in 
the memory of the reader and it is very difficult 
to clearly distinguish and reconstruct them in a 
QA system. At first sight the story published in 
(WRC, 2000) is easy to understand because the 
sentences are short and cohesive. But it turns 
out that a classic QA system would need vast 
amounts of knowledge and inference rules in or- 
der to understand the text and to give sensible 
answers. 
Let us investigate what kind of information 
a full-fledged QA system needs in order to an- 
swer the questions that come with the reading 
comprehension test (Figure 1) and discuss how 
difficult it is to provide this information. 
To answer the first question 
(1) Who collects maple sap? 
the system needs to know that the mass noun 
sap in the text sentence 
Farmers collect the sap. 
is indeed the maple sap mentioned in the 
question. The compound noun maple sap is a se- 
mantically narrower term than the noun sap and 
encodes an implicit relation between the first el- 
ement maple and the head noun sap. This rela- 
tion names the origin of the material. Since no 
explicit information about the relation between 
the two objects is available in the text an ideal 
QA system would have to assume such a relation 
by a form of abductive reasoning. 
How.Maple  Syrup is Made 
Maple syrup comes from sugar maple trees. At 
one time, maple syrup was used to make sugar. 
This is why the tree is called a "sugar" maple 
tree. 
Sugar maple trees make sap. Farmers collect he 
sap. The best time to collect sap is in February 
land March. The nights must be cold and the 
days warm. 
The framer drills a few small holes in each tree. 
He puts a spout in each hole. Then he hangs 
a bucket on the end of each spout. The bucket 
has a cover to keep rain and snow out. The sap 
drips into the bucket. About 10 gallons of sap 
come from each hole. 
1. Who collects maple sap? 
(Farmers) 
2. What does the farmer hang from a spout? 
(A bucket) 
3. When is sap collected? 
(February and March) 
4. Where does the maple sap come from? 
(Sugar maple trees) 
5. Why is the bucket covered? 
(to keep rain and snow out) 
Figure 1: Reading comprehension test 
To answer the second question 
(2) What does the farmer hang from a spout? 
successfully the system would need at least 
three different kinds of knowledge: 
First, it would need discourse knowledge to 
resolve the intersentential co-reference between 
the anaphor he and the antecedent the farmer 
in the following text sequence: 
The farmer drills- a few small holes in each 
tree. \[...\] Then he hangs a bucket ... 
Although locating antecedents has proved to 
be one of the hard problems of natural lan- 
guage processing, the anaphoric reference reso- 
lution can be done easily in this case because the 
antecedent is the most recent preceding noun 
phrase thgt agrees in gender, number and per- 
son. 
22 
Second, the system would require linguistic 
knowledge to deal with the synonymy relation 
between hang on and hang .from, and the at- 
tachment ambiguity of the prepositional phrase 
used in the text sentence and the query. 
Third, the system needs an inference rule that 
makes somehow clear that the noun phrase a 
spout expressed in the query is entailed in the 
more complex noun phrase the end of each spout 
in the text sentence. Additionally, to process 
this relation the system would require an infer- 
ence rule of the form: 
IF X does Y to EACH Z 
THEN X does Y to A Z. 
The third question 
(3) When is sap collected? 
asks for the time point when' ~ap is collected 
but the text gives only a rule-like recommenda- 
tion 
The best time to collect sap is in February 
and March. 
with an additional constraint 
The nights must be cold and the days warm. 
and does not say that the sap is in fact col- 
lected in February and March. The bridging 
inference that the system would need to model 
here is not founded on linguistic knowledge but 
on world knowledge. Solving this problem is 
very hard. It could be argued that default rules 
may solve such problems but it is not clear 
whether formal methods are able to handle the 
sort of default reasoning required for represent- 
ing common-sense reasoning. 
To give an answer for the fourth question 
(4) Where does the maple sap come .from? 
the system needs to know that maple sap 
comes from sugar maple trees. This informa- 
tion is not explicitly available in the text. In- 
stead of saying where maple sap comes from the 
text says where maple syrup comes from: 
Maple syrup comes .from sugar maple trees. 
23 
There exists a metonymy relation between 
these two compound nouns. The compound 
noun maple syrup (i.e. product) can only be 
substituted by maple sap (i.e. material), if the 
system is able to deal with metonymy. Together 
with the information in the sentence 
Sugar maple trees make sap. 
and an additional exical inference rule in 
form of a meaning postulate 
IF X makes Y THEN Y comes from X. 
the system could deduce (in theory) first sap 
and then by abductive reasoning assume that 
the sap found is maple sap. Meaning postulates 
are true by virtue of the meaning they link. Ob- 
servation cannot prove them false. 
To answer the fifth question 
(5) Why is the bucket covered? 
the system needs to know that the syntac- 
tically different expressions has a cover and is 
covered have the same propositional content. 
The system needs an explicit lexical inference 
rule in form of a conditional equivalence 
IF Conditions 
THEN X has a cover ~-> X is covered. 
that converts the verbal phrase with the nom- 
inal expression i to a the corresponding passive 
construction (and vice versa) taking the present 
context into consideration. 
As these concrete xamples how, the task of 
QA over this simple piece of text is frighten- 
ingly difficult. Finding the correct answers to 
the questions requires far more information that 
one would think at first. Apart from linguistic 
knowledge a vast amount of world knowledge 
and a number of bridging inferences are nec- 
essary to answer these seemingly simple ques- 
tions. For human beings bridging inferences 
are automatic and for the most part uncon- 
scious. The hard task consists in reconstructing 
all this information coming from different knowl- 
edge sources and modeling the suitable inference 
rules in a general way so that the system scales 
up. 
3 Answer  Ext ract ion  as an  
A l te rnat ive  Task  
An alternative to QA is answer extraction (AE). 
The general goal of AE is the same as that of 
QA, to find answers to user queries in textual 
documents. But the way to achieve this is differ- 
ent. Instead of generating the answer from the 
information given in the text (possibly in im- 
plicit form only), an AE system will retrieve the 
specific sentence(s) in the text that contain(s) 
the explicit answer to the query. In addition, 
those phrases in the sentence that represent the 
explicit a_nswer to the query may be highlighted. 
For example, let us assume that the following 
sentence is in the text (and we are going to use 
examples from a technical domain, that of the 
Unix user's manual): 
(1) cp copies the contents of filenamel onto 
filename2. 
If the user asks the query 
Which command copies files? 
a QA system will return: 
cp 
However, an AE system will return all the 
sentences in the text that directly answer the 
question, among them (1). 
Obviously, an AE system is far less power- 
ful than a real QA system. Information that 
is not explicit in a text will not be found, let 
alone information that must be derived from 
textual information together with world knowl- 
edge. But AE has a number of important ad- 
vantages over QA as a test paradigm. First, an 
obvious advantage of this approach is that the 
user receives first-hand information, right from 
the text, rather than system-generated replies. 
It is therefore much easier for the user to de- 
termine whether the result is reliable. Second, 
it is a realistic task (as the systems we are de- 
scribing below proves) as there is no need to 
generate natural language output, and there is 
less need to perform complex inferences because 
it merely looks up things in the texts which axe 
explicitly there. It need not use world knowl- 
edge. Third, it requires the solution of a num- 
ber of well-defined and truly important linguistic 
problems and is therefore well suited to measure, 
and advance, progress in these respects. We will 
come to this later. And finally, there is a real 
demand for working AE systems in technical do- 
mains since the standard IR approaches just do 
not work in a satisfactory manner in many appli- 
cations where the user is in pressure to quickly 
find a specific answer to a specific question, and 
not just (potentially long) lists of pointers to 
(potentially large) documents that may (or may 
not) be relevant o the query. Examples of ap- 
plications are on-line software help systems, in- 
terfaces to machine-readable technical manuals, 
help desk systems in large organizations, and 
public enquiry systems accessible over the Web. 
The basic procedure we use in our approach 
to AE is as follows: In an off-line stage, the 
documents are processed and the core mean- 
ing of each sentence is extracted and stored as 
so-called minimal logical forms. In an on-line 
stage, the user query is also processed to pro- 
duce a minimal ogical form. In order to retrieve 
answer sentences from the document collection, 
the minimal logical form of the query is proved, 
by a theorem prover, over the minimal logical 
forms of the entire document collection (Moll~t 
et al, 1998). Note that this method will not re- 
trieve patently wrong answer sentences like bkup 
files all copies on the hard disk in response to 
queries like Which command copies files? This 
is the kind of response we inevitably get if we 
use some variation of the bag-of-words approach 
adopted by IR based systems not performing 
any kind of content analysis. 
We are currently developing two AE sys- 
tems. The first, ExtrAns, uses deep linguis- 
tic analysis to perform AE over the Unix man- 
pages. The prototype of this system uses 500 
Unix manpages, and it can be tested over the 
Web \[http://www.ifi.unizh.ch/cl/extrans\]. In 
the second (new) project, WebExtrAns, we in- 
tend to perform AE-over the "Aircraft Main- 
tenance Manual" of the Airbus 320 (ADRES, 
1996). The larger volume of data (about 900 kg 
of printed paper!) will represent an opportunity 
to test the scalability of an AE system that uses 
deep linguistic analysis. 
There is a number of important areas of re- 
search that ExtrAns and WebExtrAns, and by 
extension any AE system, has to focus on. First 
of all, in order to generate the logical form of the 
24 
sentences, the following must be tackled: Find- 
ing the verb arguments, performing disambigua- 
tion, anaphora resolution, and coping with nom- 
inalizations, passives, ditransitives, compound 
nouns, synonymy, and hyponymy (Moll~t et al, 
1998; Mollh and Hess, 2000). Second, the very 
idea of producing the logical forms of real-world 
text requires the formalization of the logical 
form notation so that it is expressive nough but 
still remaining usable (Schwitter et al, 1999). 
Finally, the goal of producing a practical system 
for a real-world application eeds to address the 
issue of robustness and scalability (Moll~t and 
Hess, 1999).-- 
Note that the fact that AE and QA share the 
same goal makes it possible to start a project 
that initially performs AE, and gradually en- 
hance and extend it with inference and gener- 
ation modules, until we get a full-fledged QA 
system. This is the long-time g0al of our cur- 
rent series of projects on AE. 
4 Eva luat ing  the  Resu l ts  
Instead of using reading comprehension tests 
that are meant for humans, not machines, we 
should produce the specific tests that would 
evaluate the AE capability of machines. Here 
is our proposal. 
Concerning test queries, it is always better to 
use real world queries than queries that were ar- 
tificially constructed to match a portion of text. 
Experience has shown time and again that real 
people tend to come up with questions different 
from those the test designers could think of. By 
using, as we suggest, manuals of real world sys- 
tems, it is possible to tap the interaction of real 
users with this system as a source of real ques- 
tions (we do this by logging the questions ub- 
mitted to our system over the Web). Another 
way of finding queries is to consult he FAQ lists 
concerning a given system sometimes available 
on the Web. In both cases you will have to fil- 
ter out those queries that have no answers in the 
document collection or that are clearly beyond 
the scope of the system to evaluate (for exam- 
ple, if the inference needed to answer a query is 
too complex, even for a human judge). 
Concerning answers, the principal measures 
for the AE task must be recall and precision, 
applied to individual answer sentences. Recall 
is the number of correct answer sentences the 
system retrieved divided by the total number 
of correct answers in the entire document col- 
lection. Precis ion is the number of correct an- 
swer sentences the system retrieved ivided by 
the total number of answers it returned. As is 
known all too well, recall is nearly impossible to 
determine in an exact fashion for all but toy ap- 
plications ince the totality of correct answers in 
the entire document collection has to be found 
mainly by hand. Almost certainly one will have 
to resort to (hopefully) representative samples 
of documents to arrive at a reasonable approxi- 
mation to this value. Precision is easier to deter- 
mine although even this step can become very 
time consuming in real world applications. 
If, on the other hand, one only needs to do 
an approximate evaluation of the AE system, it 
would be possible to find a representative s t of 
correct answers by making a person write the 
ideal answers, and then automatically finding 
the sentences in the documents that are seman- 
tically close to these ideal answers. Semantic 
closeness between a sentence and the ideal an- 
swer can be computed by combining the suc- 
c inctness and correctness of the sentence with 
respect to the ideal answer. Succinctness and 
correctness are the counterparts ofprecision and 
recall, but on the sentence level. These mea- 
sures can be computed by checking the overlap 
of words between the sentence and the ideal an- 
swer (Hirschman et al, 1999), but we suggest a 
more content-based approach. 
Our proposal is to compare not words in a 
sentence, but their logical forms. Of course, this 
comparison can be done only if it is possible to 
agree on how logical forms should look like, to 
compute them, and to perform comparisons be- 
tween them. The second and third conditions 
can be fulfilled if the logical forms are simple 
lists of predicates that contain some minimal se- 
mantic information, as it is the case in ExtrAns 
(Schwitter et al, 1999). In this paper we will 
use a simplification of the minimal ogical forms 
used by ExtrAns. Below are two sentences with 
their logical forms: 
(1) rm removes one or more files. 
remove(x ,y ) ,  rm(x) ,  f i le(y) 
(2) csplit pr ints  the character counts .for each 
file created, and removes any files it creates 
i f  an error occurs. 
25 
print(x,y), csplit(x), character-count(y), 
remove(x ,z ) ,  fi le(z), create(x,z), oc- 
cur(e), error(e) 
As an example of how to compute succinct- 
ness and correctness, take the following ques- 
tion: 
Which command removes files? 
The ideal answer is a full sentence that con- 
tains the information given by the question and 
the information requested. Since rm is the com- 
mand used to remove files, the ideal answer is: 
rm removes  f i les.  
remove(x,y), rm(x), file(y) 
Instead of computing the overlap of words, 
succinctness and correctness ofa sentence can be 
determined by computing the overlap of predi- 
cates. The overlap of the predicates (overlap 
henceforth) of two sentences is the maximum 
set of predicates that can be used as part of the 
logical form in both sentences. The predicates 
in boldface in the two examples above indicate 
the overlap with the ideal answer: 3 for (1), and 
2 for (2). 
Succinctness of a sentence with respect o an 
ideal answer (precision on the sentence level) is 
the ratio between the overlap and the total num- 
ber of predicates in the sentence. Succinctness 
is, therefore, 3/3=1 for (1), and 2/8=0.25 for 
(2). 
Correctness of a sentence with respect o an 
ideal answer (recall on the sentence level) is the 
ratio between the overlap and the number of 
predicates in the ideal answer. In the exam- 
ples above, correctness i 3/3=1 for (1), and 
2/3=0.66 for (2). 
A combined measure of succinctness and cor- 
rectness could be used to determine the seman- 
tic closeness of the sentences to the ideal an- 
swer. By establishing a threshold to the seman- 
tic closeness, one can find the sentences in the 
documents that are answers to the user's query. 
The advantage of using overlap of predicates 
against overlap of words is that the relations be- 
tween the words also affect the measure for suc- 
cinctness and correctness. We can see this in 
the following artificial example. Let us suppose 
that the ideal answer to a query is: 
Madrid defeated Barcelona. 
defeat(x,y), madrid(x), barcelona(y) 
The following candidate sentence produces 
the same predicates: 
Barcelona defeated Madrid. 
defeat(x,y), madr id (y ) ,  barce lona(x)  
However, at most two predicates only can be 
chosen at the same time (in boldface), because 
of the restrictions of the arguments.  In the 
ideal answer, the first argument of "defeat" is 
Madrid and the second argument is Barcelona. 
In the candidate sentence, however, the argu- 
ments are reversed (the name of the variables 
have no effect on this). The overlap is, therefore, 
2. Succinctness and correctness are 2/3=0.66 
and 2/3=0.66, respectively. 
5 Conc lus ion  
We are convinced that reading comprehension 
tests are too difficult for the current state of 
art in natural language processing. Our anal- 
ysis of the Maple Syrup story shows how much 
world knowledge and inference rules are needed 
to actually answer the test questions correctly. 
Therefore, we think that a more restricted kind 
of task that focuses rather on tractable problems 
than on AI-hard problems of question-answering 
(QA) is better suited to take our field a step 
further. Answer Extraction (AE) is an alter- 
native to QA that relies mainly O n linguistic 
knowledge. AE aims at retrieving those exact 
passages of a document hat directly answer a 
given user query. AE is less ambitious than full- 
fledged QA since the answers are not generated 
from a knowledge base but looked up in the doc- 
uments. These documents come from a well- 
defined (technical) domain and consist of a rela- 
tively small volume of data. Our test queries are 
real world queries that express a concrete infor- 
mation need. To evaluate our AE systems, we 
propose besides precision and recall two addi- 
tional measures: succinctness and correctness. 
They measure the quality of answer sentences 
on the sentence level and are computed on the 
basis of the overlap of logical predicates. 
To round out the picture, we address the ques- 
tions in (WRC, 2000) in the view of what we said 
in this paper: 
26 
Q: Can such exams \[reading comprehension 
tests\] be used to evaluate computer-based lan- 
guage understanding effectively and e~ciently? 
A: We think that no language unders tand-  
ing system will currently be able to answer a sig- 
nificant proportion of such questions, which will 
make evaluation results difficult at best, mean- 
ingless at worst. 
Q: Would they provide an impetus and test 
bed for interesting and useful research? 
A: We think that the impetus they might pro- 
vide would drive development in the wrong di- 
rection, viz. towards the creation of (possibly 
impressive) engineering feats without much lin- 
guistically interestingcontent. 
Q: Are they too hard for current technology? 
A: Definitely, and by a long shot. 
Q: Or are they too easy, such that simple 
hacks can score high, although there is clearly 
no understanding involved? ., 
A: "Simple hacks" would almost certainly 
score higher than linguistically interesting meth- 
ods but not because the task is too simple but 
because it is far too difficult. 
References 
ADRES, 1996. A319/A320/A321 Aircraft 
Maintenance Manual. Airbus Industrie, 
Blagnac Cedex, France. Rev. May 1. 
Douglas E. Appelt, Jerry R. Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy 
Kehler, David Martin, Karen Myers, and 
Mabry Tyson. 1995. SRI International FAS- 
TUS system MUC-6 test results and analysis. 
In Proc. Sixth Message Understanding Con- 
\]erence (MUC-6), Columbia, Maryland. 
Otthein Herzog and Claus-Rainer Rollinger, ed- 
itors. 1991. Text Understanding in LILOG: 
Integrating Computational Linguistics and 
Artificial Intelligence - final report on the 
IBM Germany LILOG project, volume 546 of 
Lecture Notes in Computer Science. Springer- 
Verlag, Berlin. 
Lynette Hirschman, Marc Light, Eric Breck, and 
John D. Burger. 1999. Deep Read: A read- 
ing comprehension system. In Proc. A CL '99. 
University of Maryland. 
Jerry Hobbs, Douglas E. Appelt, John S. Bear, 
Mabry Tyson, and David Magerman. 1991. 
The TACITUS system: The MUC-3 experi- 
ence. Technical report, AI Center, SRI Inter- 
national, Menlo Park, CA. 
Jerry R. Hobbs, Douglas E. Appelt, John Bear, 
David Israel, Megumi Kameyama, Mark 
Stickel, and Mabry Tyson. 1996. FASTUS: 
A cascaded finite-state transducer for extract- 
ing information from natural-language t xt. 
In E. Roche and Y. Schabes, editors, Finite 
State Devices for Natural Language Process- 
ing. MIT Press, Cambridge, MA. 
Diego Moll~ and Michael Hess. 1999. On 
the scalability of the answer extraction sys- 
tem "ExtrAns". In Proc. Applications of 
Natural Language to Information Systems 
(NLDB'99), pages 219-224, Klagenfurt, Aus- 
tria. 
Diego Moll~ and Michael Hess. 2000. Deal- 
ing with ambiguities in an answer extrac- 
tion system. In Representation and Treatment 
of Syntactic Ambiguity in Natural Language 
Processing, Paris. ATALA. 
Diego Moll~, Jawad Berri, and Michael Hess. 
1998. A real world implementation f answer 
extraction. In Proc. of the 9th International 
Conference and Workshop on Database and 
Expert Systems. Workshop "Natural Language 
and Information Systems" (NLIS'98), pages 
143-148, Vienna, August. 
MUC-7. 1998. Proc. of the seventh mes- 
sage understanding conference (MUC-7). 
http://www.muc.saic.com. 
Rolf Schwitter, Diego Moll~, and Michael Hess. 
1999. Extrans - -  answer extraction from 
technical documents by minimal ogical forms 
and selective highlighting. In Proc. Third In- 
ternational Tbilisi Symposium on Language, 
Logic and Computation, Batumi, Georgia. 
http://www.ifi.unizh.ch/cl/. 
Ellen M. Voorhees and Donna Harman. 1998. 
Overview of the seventh Text REtrieval Con- 
ference (TREC-7). In Ellen M. Voorhees and 
Donna Harman, editors, The Seventh Text 
REtrieval Conference (TREC-7), number 
500-242 in NIST Special Publication, pages 1- 
24. NIST-DARPA, Government Printing Of- 
rice. 
WRC. 2000. Workshop on reading compre- 
hension texts as evaluation for computer- 
based language understanding systems. 
http://www.gte.com/AboutGTE/gto/anlp- 
naacl2000/comprehension.html. 
27 
Exploiting Paraphrases in a Question Answering System
Fabio Rinaldi, James Dowdall,
Kaarel Kaljurand, Michael Hess
Institute of Computational Linguistics,
University of Zu?rich
Winterthurerstrasse 190
CH-8057 Zu?rich, Switzerland
{rinaldi,dowdall,kalju,hess}
@ifi.unizh.ch
Diego Molla?
Centre for Language Technology,
Macquarie University,
Sydney NSW 2109, Australia
{diego}@ics.mq.edu.au
Abstract
We present a Question Answering system
for technical domains which makes an in-
telligent use of paraphrases to increase the
likelihood of finding the answer to the user?s
question. The system implements a simple
and efficient logic representation of ques-
tions and answers that maps paraphrases
to the same underlying semantic represen-
tation. Further, paraphrases of technical
terminology are dealt with by a separate
process that detects surface variants.
1 Introduction
The problem of paraphrases conceals a number of
different linguistic problems, which in our opinion
need to be treated in separate ways. In fact, para-
phrases can happen at various levels in language. Us-
ing the examples provided in the call for papers for
this workshop, we would like to attempt a simple
classification, without any pretense of being exhaus-
tive:
1. Lexical synonymy.
Example: article, paper, publication
2. Morpho-syntactic variants.
a) Oswald killed Kennedy. / Kennedy was killed
by Oswald.
b) Edison invented the light bulb. / Edison?s
invention of the light bulb.
while (a) is purely syntactical (active vs pas-
sive), (b) involves a nominalisation.
3. PP-attachment.
a plant in Alabama / the Alabama plant
4. Comparatives vs superlatives.
be better than anybody else / be the best
5. Subordinate clauses vs separate sentences linked
by anaphoric pronouns.
The tree healed its wounds by growing new bark.
/ The tree healed its wounds. It grew new bark.
6. Inference.
The stapler costs $10. / The price of the stapler
is $10.
Where is Thimphu located? / Thimphu is capi-
tal of what country?
Of course combinations of the different types are
possible, e.g. Oswald killed Kennedy / Kennedy
was assassinated by Oswald is a combination of (1)
and (2).
Different types of knowledge and different linguis-
tic resources are needed to deal with each of the
above types. While type (1) can be dealt with us-
ing a resource such as WordNet (Fellbaum, 1998),
type (2) needs effective parsing and mapping of syn-
tactic structures into a common deeper structure,
possibly using a repository of nominalisations like
NOMLEX (Meyers et al, 1998). More complex
approaches are needed for the other types, up to
type (6) where generic world knowledge is required,
for instance to know that being a capital of a country
implies being located in that country. 1 Such world
knowledge could be expressed in the form of axioms,
like the following:
(X costs Y) iff (the price of X is Y)
In this paper we focus on the role of paraphrases
in a Question Answering (QA) system targeted at
1Note that the reverse is not true, and therefore this
is not a perfect paraphrase.
technical manuals. Technical documentation is char-
acterised by vast amounts of domain-specific termi-
nology, which needs to be exploited for providing in-
telligent access to the information contained in the
manuals (Rinaldi et al, 2002b). The approach taken
by QA systems is to allow a user to ask a query (for-
mulated in natural language) and have the system
search a background collection of documents in order
to locate an answer. The field of Question Answer-
ing has flourished in recent years2, in part, due to
the QA track of the TREC competitions (Voorhees
and Harman, 2001). These competitions evaluate
systems over a common data set alowing develop-
ers to benchmark performance in relation to other
competitors.
It is a common assumption that technical termi-
nology is subject to strict controls and cannot vary
within a given editing process. However this assump-
tion proves all too often to be incorrect. Unless edi-
tors are making use of a terminology control system
that forces them to use a specific version of a term,
they will naturally tend to use various paraphrases
to refer to the intended domain concept. Besides in
a query a user could use an arbitrary paraphrases of
the target term, which might happen to be one of
those used in the manual itself or might happen to
be a novel one.
We describe some potential solutions to this prob-
lem, taking our Question Answering system as an ex-
ample. We show which benefits our approach based
on paraphrases bring to the system. So far two dif-
ferent domains have been targeted by the system.
An initial application aims at answering questions
about the Unix man pages (Molla? et al, 2000a; Molla?
et al, 2000b). A more complex application targets
the Aircraft Maintenance Manual (AMM) of the Air-
bus A320 (Rinaldi et al, 2002b). Recently we have
started new work, using the Linux HOWTOs as a
new target domain.
In dealing with these domains we have identified
two major obstacles for a QA system, which we can
summarise as follows:
? The Parsing Problem
? The Paraphrase Problem
The Parsing Problem consists in the increased
difficulty of parsing text in a technical domain due to
domain-specific sublanguage. Various types of multi
word expressions characterise these domains, in par-
ticular referring to specific concepts like tools, parts
or procedures. These multi word expressions might
2Although early work in AI already touched upon the
topic, e.g. (Woods, 1977).
include lexical items which are either unknown to
a generic lexicon (e.g. coax cable) or have a spe-
cific meaning unique to this domain. Abbreviations
and acronyms are another common source of incon-
sistencies. In such cases the parser might either
fail to identify the compound as a phrase and con-
sequently fail to parse the sentence including such
items. Alternatively the parser might attempt to
?guess? their lexical category (in the set of open class
categories), leading to an exponential growth of the
number of possible syntactic parses. Not only the in-
ternal structure of the compound can be multi-way
ambiguous, even the boundaries of the compounds
might be difficult to detect and the parsers might
try odd combinations of the tokens belonging to the
compounds with neighbouring tokens.
The Paraphrase Problem resides in the imper-
fect knowledge of users of the systems, who can-
not be expected to be completely familiar with the
domain terminology. Even experienced users, who
know very well the domain, might not remember the
exact wording of a compound and use a paraphrase
to refer to the underlying domain concept. Besides
even in the manual itself, unless the editors have been
forced to use some strict terminology control system,
various paraphrases of the same compound will ap-
pear, and they need to be identified as co-referent.
However, it is not enough to identify all paraphrases
within the manual, novel paraphrases might be cre-
ated by the users each time they query the system.
In the rest of this paper we describe first our Ques-
tion Answering System (in Section 2) and briefly
show how we solved the first of the two problems
described above. Then, in Section 3 we show in de-
tail how the system is capable of coping with the
Paraphrase Problem. Finally in Section 4 we discuss
some related work.
2 A Question Answering System for
Technical Domains
Over the past few years our research group has devel-
oped an Answer Extraction system (ExtrAns) that
works by transforming documents and queries into a
semantic representation called Minimal Logical Form
(MLF) (Molla? et al, 2000a) and derives the answers
by logical proof from the documents. A full linguis-
tic (syntactic and semantic) analysis, complete with
lexical alternations (synonyms and hyponyms) is per-
formed. While documents are processed in an off-line
stage, the query is processed on-line.
Two real world applications have so far been im-
plemented with the same underlying technology. The
original ExtrAns system (Molla? et al, 2000b) is used
///// a.d electrical coax cable.n4 connects.v062 the.d external antenna.n1 to.o the.d ANT connection.n1 /////
-Wd
ff Dsu ff Ss
-
MVp
-Os
ff Ds
-Js
ff Ds
RW
Figure 1: An Example of LG Output
to extract answers to arbitrary user queries over the
Unix documentation files (?man pages?). A set of
500+ unedited man pages has been used for this ap-
plication. An on-line demo of ExtrAns can be found
at the project web page.3
 Knowledge 
Base
Document
Linguistic
Analysis
Term
processing
Figure 2: Off-line
Processing of Docu-
ments
More recently we tackled
a different domain, the Air-
plane Maintenance Manu-
als (AMM) of the Air-
bus A320 (Rinaldi et al,
2002b), which offered the
additional challenges of an
SGML-based format and a
much larger size (120MB).4
Despite being developed
initially for a specific do-
main, ExtrAns has demon-
strated a high level of do-
main independence.
As we work on relatively
small volumes of data we
can afford to process (in
an off-line stage) all the
documents in our collection
rather than just a few se-
lected paragraphs (see Fig-
ure 2). Clearly in some sit-
uations (e.g. processing in-
coming news) such an ap-
proach might not be fea-
sible and paragraph index-
ing techniques would need
to be used. Our current ap-
proach is particularly tar-
geted to small and medium sized collections.
In an initial phase all multi-word expressions
from the domain are collected and structured in
an external resource, which we will refer to as the
TermBase (Rinaldi et al, 2003; Dowdall et al, 2003).
The document sentences (and user queries) are syn-
tactically processed with the Link Grammar (LG)
parser (Sleator and Temperley, 1993) which uses a
3http://www.ifi.unizh.ch/cl/extrans/
4Still considerably smaller than the size of the docu-
ment collections used for TREC
grammar with a wide coverage of English and has
a robust treatment of ungrammatical sentences and
unknown words. The multi-word terms from the the-
saurus are identified and passed to the parser as sin-
gle tokens. This prevents (futile) analysis of the in-
ternal structure of terms (see Figure 1), simplifying
parsing by 46%. This solves the first of the problems
that we have identified in the introduction (?The
Parsing Problem?).
In later stages of processing, a corpus-based ap-
proach (Brill and Resnik, 1994) is used to deal with
ambiguities that cannot be solved with syntactic in-
formation only, in particular attachments of preposi-
tional phrases, gerunds and infinitive constructions.
ExtrAns adopts an anaphora resolution algorithm
(Molla? et al, 2003) that is based on Lappin and Le-
ass? approach (Lappin and Leass, 1994). The original
algorithm, which was applied to the syntactic struc-
tures generated by McCord?s Slot Grammar (Mc-
Cord et al, 1992), has been ported to the output of
Link Grammar. So far the resolution is restricted to
sentence-internal pronouns but the same algorithm
can be applied to sentence-external pronouns too.
A lexicon of nominalisations based on NOMLEX
(Meyers et al, 1998) is used for the most important
cases. The main problem here is that the semantic
relationship between the base words (mostly, but not
exclusively, verbs) and the derived words (mostly,
but not exclusively, nouns) is not sufficiently sys-
tematic to allow a derivation lexicon to be compiled
automatically. Only in relatively rare cases is the
relationship as simple as with to edit <a text> ?
editor of <a text> / <text> editor, as the effort
that went into building resources such as NOMLEX
also shows.
User queries are processed on-line and converted
into MLFs (possibly expanded by synonyms) and
proved by refutation over the document knowledge
base (see Figure 3). Pointers to the original text at-
tached to the retrieved logical forms allow the system
to identify and highlight those words in the retrieved
sentence that contribute most to that particular an-
swer. When the user clicks on one of the answers
provided, the corresponding document will be dis-
played with the relevant passages highlighted.
 Knowledge 
Base
ANSWERSQuery
Document
Linguistic
Analysis
Paraphrase
Identification
Figure 3: On-line Processing of Queries
The meaning of the documents and of the queries
produced by ExtrAns is expressed by means of Mini-
mal Logical Forms (MLFs). The MLFs are designed
so that they can be found for any sentence (using
robust approaches to treat very complex or ungram-
matical sentences), and they are optimized for NLP
tasks that involve the semantic comparison of sen-
tences, such as Answer Extraction.
The expressivity of the MLFs is minimal in the
sense that the main syntactic dependencies between
the words are used to express verb-argument rela-
tions, and modifier and adjunct relations. However,
complex quantification, tense and aspect, temporal
relations, plurality, and modality are not expressed.
One of the effects of this kind of underspecification
is that several natural language queries, although
slightly different in meaning, produce the same logi-
cal form.
The main feature of the MLFs is the use of reifi-
cation (the expression of abstract concepts as con-
crete objects) to achieve flat expressions (Molla? et
al., 2000b). The MLFs are expressed as conjunc-
tions of predicates with all the variables existentially
bound with wide scope. For example, the MLF of
the sentence ?cp will quickly copy the files? is:
(1) holds(e4), object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type cp and of type command,
there is an entity x6 (a file), there is an entity e4,
which represents a copying event where the first ar-
gument is x1 and the second argument is x6, there
is an entity p3 which states that e4 is done quickly,
and the event e4, that is, the copying, holds. The
entities o1, o2, o3, e4, and p3 are the result of reifi-
cation. The reification of the event, e4, has been used
to express that the event is done quickly. The other
entities are not used in this MLF, but other more
complex sentences may need to refer to the reifica-
tion of properties (adjective-modifying adverbs) or
object predicates (non-intersective adjectives such as
the alleged suspect).
ExtrAns finds the answers to the questions by
forming the MLFs of the questions and then run-
ning Prolog?s default resolution mechanism to find
those MLFs that can prove the question. When no
direct proof for the user query is found, the system
is capable of relaxing the proof criteria in a stepwise
manner. First, hyponyms of the query terms will be
added as disjunctions in the logical form of the ques-
tion, thus making it more general but still logically
correct. If that fails, the system will attempt approx-
imate matching, in which the sentence (or sentences)
with the highest overlap of predicates with the query
is retrieved. The (partially) matching sentences are
scored and the best fits are returned. In the case
that this method finds too many answers because
the overlap is too low, the system will attempt key-
word matching, in which syntactic criteria are aban-
doned and only information about word classes is
used. This last step corresponds approximately to a
traditional passage-retrieval methodology with con-
sideration of the POS tags.
3 Dealing with Paraphrases
The system is capable of dealing with paraphrases
at two different levels. On the phrase level, differ-
ent surface realizations (terms) which refer to the
same domain concept will be mapped into a com-
mon identifier (synset identifier). On the sentence
level, paraphrases which involve a (simple) syntactic
transformation will be dealt with by mapping them
into the same logical form. In this section we will
describe these two approaches and discuss ways to
cope with complex types of parapharases.
3.1 Identifying Terminological Paraphrases
During the construction of the MLFs, thesaurus
terms are replaced by their synset identifiers. This
results in an implicit ?terminological normalization?
for the domain. The benefit to the QA process is
an assurance that a query and answer need not in-
volve exactly the same surface realization of a term.
Utilizing the synsets in the semantic representation
means that when the query includes a term, ExtrAns
returns sentences that logically answer the query, in-
Fastr
Term
Extraction
Hyponymy
Thesaurus ExtrAns
Document
Figure 4: Term Processing
volving any known paraphrase of that term.
For example, the logical form of the query Where
are the stowage compartments installed? is trans-
lated internally into the Horn query (2).
(2) evt(install,A,[B,C]),
object(D,E,[B]),
object(s stowage compartment,G,[C])
This means that a term (belonging to the same
synset as stowage compartment) is involved in an in-
stall event with an anonymous object. If there is
an MLF from the document that can match exam-
ple (2), then it is selected as a candidate answer and
the sentence it originates from is shown to the user.
The process of terminological variation is well
investigated (Ibekwe-SanJuan and Dubois, 2002;
Daille et al, 1996; Ibekwe-Sanjuan, 1998). The
primary focus has been to use linguistically based
variation to expand existing term sets through cor-
pus investigation or to produce domain representa-
tions. A subset of such variations identifies terms
which are strictly synonymous. ExtrAns gathers
these morpho-syntactic variations into synsets. The
sets are augmented with terms exhibiting three
weaker synonymy relations described by Hamon &
Nazarenko (2001). These synsets are organized into
a hyponymy (isa) hierarchy, a small example of which
can be seen in Figure 5. Figure 4 shows a schematic
representation of this process.
The first stage is to normalize any terms that con-
tain punctuation by creating a punctuation free ver-
sion and recording the fact that that the two are
strictly synonymous. Further processing is involved
in terms containing brackets to determine if the
bracketed token is an acronym or simply optional. In
the former case an acronym-free term is created and
the acronym is stored as a synonym of the remain-
ing tokens which contain it as a regular expression.
So evac is synonymous with evacuation and ohsc is
synonymous with overhead stowage compartment. In
cases such as emergency (hard landings) the brack-
eted tokens can not be interpreted as acronyms and
so are not removed.
The synonymy relations are identified using the
terminology tool Fastr (Jacquemin, 2001). Every to-
ken of each term is associated with its part-of-speech,
its morphological root, and its synonyms. Phrasal
rules represent the manner in which tokens combine
to form multi-token terms, and feature-value pairs
carry the token specific information. Metarules li-
cense the relation between two terms by constrain-
ing their phrase structures in conjunction with the
morphological and semantic information on the indi-
vidual tokens.
The metarules can identify simple paraphrases
that result from morpho-syntactic variation (cargo
compartment door ?? doors of the cargo compart-
ment), terms with synonymous heads (electrical ca-
ble ?? electrical line), terms with synonymous mod-
ifiers (fastener strip ?? attachment strip) and both
(functional test ?? operational check). For a de-
scription of the frequency and range of types of vari-
ation present in the AMM see Rinaldi et al (2002a).
3.2 Identifying Syntactic Paraphrases
An important effect of using a simplified semantic-
based representation such as the Minimal Logical
Forms is that various types of syntactic variations
are automatically captured by a common representa-
tion. This ensures that many potential paraphrases
in a user query can map to the same answer into the
manual.
For example the question shown in Figure 6 can
be answered thanks to the combination of two fac-
tors. On the lexical level ExtrAns knows that APU
is an abbreviation of Auxiliary Power Unit, while on
the syntactic level the active and passive voices (sup-
plies vs supplied with) map into the same underlying
representation (the same MLF).
Another type of paraphrase which can be detected
at this level is the kind that was classified as type (3)
in the introduction. For example the question: Is
the sensor connected to the APU ECB?, can locate
the answer This sensor is connected to the Elec-
tronic Control Box (ECB) of the APU. This has been
achieved by introducing meaning postulates that op-
erate at the level of the MLFs (such as ?any predicate
that affects an object will also affect the of -modifiers
of that object?).
3.3 Weaker Types of Paraphrases
When the thesaurus definition of terminological syn-
onymy fails to locate an answer from the docu-
ment collection, ExtrAns explores weaker types of
paraphrases, where the equivalence between the two
terms might not be complete.
TERM
doors of the cargo compartment
cargo compartment door
cargo comparment doors
cargo-compartment door
emergency ( hard landings )
emergency hard landings
emergency hard landing
emergency evacuation (evac)
emergency evacuation
evacuation
evac
electrical cable
electrical line
fastner strip
attachment strip
functional test
operational check
door functional test
stowage compartment
overhead stowage compartment
OHSC
1
2
3
5
6
7
10
9
8
11
Figure 5: A Sample of the TermBase
Figure 6: Active vs Passive Voice
First, ExtrAns makes use of the hyponymy rela-
tions, which can be considered as sort of unidirec-
tional paraphrases. Instead of looking for synset
members, the query is reformulated to included hy-
ponyms and hyperonyms of the terms:
(3) (object(s stowage compartment,A,[B]);
object(s overhead stowage compartment,A,[B])),
evt(install,C,[D,B]),
object(E,F,[D|G])
Now the alternative objects are in a logical OR rela-
tion. This query finds the answer in Figure 7 (where
stowage compartment is a hyperonym of overhead
stowage compartment).
We have implemented a very simple ad-hoc algo-
rithm to determine lexical hyponymy between terms.
Term A is a hyponym of term B if (i) A has more to-
kens than B, (ii) all the tokens of B are present in A,
and (iii) both terms have the same head. There are
three provisions. First, ignore terms with dashes and
brackets as cargo compartment is not a hyponym of
cargo - compartment and this relation (synonymy) is
already known from the normalisation process. Sec-
ond, compare lemmatised versions of the terms to
capture that stowage compartment is a hyperonym
of overhead stowage compartments. Finally, the head
of a term is the rightmost non-symbol token (i.e. a
word) which can be determined from the part-of-
speech tags. This hyponymy relation is compara-
ble to the insertion variations defined by Daille et
al. (1996).
The expressivity of the MLF can further be ex-
panded through the use of meaning postulates of the
type: ?If x is installed in y, then x is in y?. This
ensures that the query Where are the equipment and
furnishings? extracts the answer The equipment and
furnishings are installed in the cockpit.
4 Related Work
The importance of detecting paraphrasing in Ques-
tion Answering has been shown dramatically in
TREC9 by the Falcon system (Harabagiu et al,
2001), which made use of an ad-hoc module capable
of caching answers and detecting question similar-
ity. As in that particular evaluation the organisers
deliberately used a set of paraphrases of the same
questions, such approach certainly helped in boost-
ing the performance of the system. In an environ-
ment where the same question (in different formula-
tions) is likely to be repeated a number of times, a
module capable of detecting paraphrases can signif-
icantly improve the performance of a Question An-
Figure 7: Overhead stowage compartment is a Hyponym of Stowage compartment
swering system.
Another example of application of paraphrases for
Question Answering is given in (Murata and Isahara,
2001), which further argues for the importance of
paraphrases for other applications such Summarisa-
tion, error correction and speech generation.
Our approach for the acquisition of terminological
paraphrases might have some points in common with
the approach described in (Terada and Tokunaga,
2001). The motivation that they bring forward for
the necessity of identifying abbreviations is related to
the problem that we have called ?the Parsing Prob-
lem?.
A very different approach to paraphrases is taken
in (Takahashi et al, 2001) where they formulate the
problem as a special case of Machine Translation,
where the source and target language are the same
but special rules, based on different parameters, li-
cense different types of surface realizations.
Hamon & Nazarenko (2001) explore the termino-
logical needs of consulting systems. This type of IR
guides the user in query/keyword expansion or pro-
poses various levels of access into the document base
on the original query. A method of generating three
types of synonymy relations is investigated using gen-
eral language and domain specific dictionaries.
5 Conclusion
Automatic recognition of paraphrases is an effec-
tive technique to ease the information access bur-
den in a technical domain. We have presented some
techniques that we have adopted in a Question An-
swering system for dealing with paraphrases. These
techniques range from the detection of lexical para-
phrases and terminology variants, to the use of a
simplified logical form that provides the same repre-
sentation for morpho-syntactic paraphrases, and the
use of meaning postulates for paraphrases that re-
quire inferences.
References
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
ambiguation. In Proc. COLING ?94, volume 2,
pages 998?1004, Kyoto, Japan.
Beatrice Daille, Benot Habert, Christian Jacquemin,
and Jean Royaute?. 1996. Empirical observation of
term variations and principles for their description.
Terminology, 3(2):197?258.
James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-
SanJuan, and Eric SanJuan. 2003. Complex
structuring of term variants for Question Answer-
ing. In Proc. ACL-2003 Workshop on Multiword
Expressions, Sapporo, Japan.
Christiane Fellbaum 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Thierry Hamon and Adeline Nazarenko. 2001. De-
tection of synonymy links between terms: Experi-
ment and results. In Didier Bourigault, Christian
Jacquemin, and Marie-Claude L?Homme, editors,
Recent Advances in Computational Terminology,
pages 185?208. John Benjamins Publishing Com-
pany.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca,
Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,
Roxana G??rju, Vasile Rus, and Paul Morarescu.
2001. Falcon: Boosting knowledge for answer
engines. In Voorhees and Harman (Voorhees and
Harman, 2001).
Fidelia Ibekwe-SanJuan and Cyrille Dubois. 2002.
Can Syntactic Variations Highlight Semantic
Links Between Domain Topics? In Proceedings
of the 6th International Conference on Terminol-
ogy and Knowledge Engineering (TKE02), pages
57?64, Nancy, August.
Fidelia Ibekwe-Sanjuan. 1998. Terminological Vari-
ation, a Means of Identifying Research Topics from
Texts. In Proceedings of COLING-ACL, pages
571?577, Quebec,Canada, August.
Christian Jacquemin. 2001. Spotting and Discover-
ing Terms through Natural Language Processing.
MIT Press.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Com-
putational Linguistics, 20(4):535?561.
Michael McCord, Arendse Bernth, Shalom Lap-
pin, and Wlodek Zadrozny. 1992. Natural lan-
guage processing within a slot grammar frame-
work. International Journal on Artificial Intelli-
gence Tools, 1(2):229?277.
Adam Meyers, Catherine Macleod, Roman Yangar-
ber, Ralph Grishman, Leslie Barrett, and Ruth
Reeves. 1998. Using NOMLEX to produce
nominalization patterns for information extrac-
tion. In Proceedings: the Computational Treat-
ment of Nominals, Montreal, Canada, (Coling-
ACL98 workshop), August.
Diego Molla?, Gerold Schneider, Rolf Schwitter, and
Michael Hess. 2000a. Answer Extraction using
a Dependency Grammar in ExtrAns. Traitement
Automatique de Langues (T.A.L.), Special Issue
on Dependency Grammar, 41(1):127?156.
Diego Molla?, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000b. Extrans, an answer ex-
traction system. T.A.L. special issue on Informa-
tion Retrieval oriented Natural Language Process-
ing.
Diego Molla?, Rolf Schwitter, Fabio Rinaldi, James
Dowdall, and Michael Hess. 2003. Anaphora res-
olution in ExtrAns. In Proceedings of the Interna-
tional Symposium on Reference Resolution and Its
Applications to Question Answering and Summa-
rization, 23?25 June, Venice, Italy.
Masaki Murata and Hitoshi Isahara. 2001. Univer-
sal model for paraphrasing - using transformation
based on a defined criteria. In Proceedings of the
NLPRS2001 Workshop on Automatic Paraphras-
ing: Theories and Applications.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, Mare Koit, Kadri Vider, and Neeme
Kahusk. 2002a. Terminology as Knowledge in An-
swer Extraction. In Proceedings of the 6th Interna-
tional Conference on Terminology and Knowledge
Engineering (TKE02), pages 107?113, Nancy, 28?
30 August.
Fabio Rinaldi, James Dowdall, Michael Hess, Diego
Molla?, and Rolf Schwitter. 2002b. Towards An-
swer Extraction: an application to Technical Do-
mains. In ECAI2002, European Conference on Ar-
tificial Intelligence, Lyon, 21?26 July.
Fabio Rinaldi, James Dowdall, Michael Hess, Kaarel
Kaljurand, and Magnus Karlsson. 2003. The Role
of Technical Terminology in Question Answering.
In Proceedings of TIA-2003, Terminologie et In-
telligence Artificielle, Strasbourg, April.
Daniel D. Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proc. Third
International Workshop on Parsing Technologies,
pages 277?292.
Tetsuro Takahashi, Tomoya Iwakura, Ryu Iida, and
Kentaro Inui. 2001. Kura: A revision-based
lexico-structural paraphrasing engine. In Proceed-
ings of the NLPRS2001 Workshop on Automatic
Paraphrasing: Theories and Applications.
Akira Terada and Takenobu Tokunaga. 2001. Au-
tomatic disabbreviation by using context informa-
tion. In Proceedings of the NLPRS2001 Workshop
on Automatic Paraphrasing: Theories and Appli-
cations.
Ellen M. Voorhees and Donna Harman, editors.
2001. Proceedings of the Ninth Text REtrieval
Conference (TREC-9), Gaithersburg, Maryland,
November 13-16, 2000.
W.A. Woods. 1977. Lunar rocks in natural English:
Explorations in Natural Language Question An-
swering. In A. Zampolli, editor, Linguistic Struc-
tures Processing, volume 5 of Fundamental Studies
in Computer Science, pages 521?569. North Hol-
land.
Parmenides: an opportunity for ISO TC37 SC4?
Fabio Rinaldi
1
, James Dowdall
1
, Michael Hess
1
, Kaarel Kaljurand
1
, Andreas Persidis
2
,
Babis Theodoulidis
3
, Bill Black
3
, John McNaught
3
, Haralampos Karanikas
3
, Argyris Vasilakopoulos
3
,
Kelly Zervanou
3
, Luc Bernard
3
, Gian Piero Zarri
4
, Hilbert Bruins Slot
5
, Chris van der Touw
5
,
Margaret Daniel-King
6
, Nancy Underwood
6
, Agnes Lisowska
6
, Lonneke van der Plas
6
,
Veronique Sauron
6
, Myra Spiliopoulou
7
, Marko Brunzel
7
, Jeremy Ellman
8
,
Giorgos Orphanos
9
, Thomas Mavroudakis
10
, Spiros Taraviras
10
.
Abstract
Despite the many initiatives in recent years
aimed at creating Language Engineering
standards, it is often the case that dierent
projects use dierent approaches and often
dene their own standards. Even within the
same project it often happens that dierent
tools will require dierent ways to represent
their linguistic data.
In a recently started EU project focusing
on the integration of Information Extrac-
tion and Data Mining techniques, we aim
at avoiding the problem of incompatibility
among dierent tools by dening a Com-
mon Annotation Scheme internal to the
project. However, when the project was
started (Sep 2002) we were unaware of the
standardization eort of ISO TC37/SC4,
and so we commenced once again trying to
dene our own schema. Fortunately, as this
work is still at an early stage (the project
will last till 2005) it is still possible to redi-
rect it in a way that it will be compati-
ble with the standardization work of ISO.
In this paper we describe the status of the
work in the project and explore possible
synergies with the work in ISO TC37 SC4.
1 1
Institute of Computational Linguistics, Uni-
versity of Zurich, Switzerland;
2
Biovista, Athens,
Greece;
3
Centre for Research in Information Manage-
ment, UMIST, Manchester, UK;
4
CNRS, Paris, France;
5
Unilever Research and Development, Vlaardingen,
The Netherlands;
6
TIM/ISSCO, University of Geneva,
Switzerland;
7
Uni Magdeburg, Germany;
8
Wordmap
Ltd., Bath, UK;
9
Neurosoft, Athens, Greece;
10
The
Greek Ministry of National Defense, Athens, Greece
1 Introduction
It is by now widely accepted that some W3C stan-
dards (such as XML and RDF) provide a con-
venient and practical framework for the creation
of eld-specic markup languages (e.g. MathML,
VoiceXML). However XML provides only a common
\alphabet" for interchange among tools, the steps
that need to be taken before there is any real shar-
ing are still many (just as many human languages
share the same alphabets, that does not mean that
they can be mutually intelligible). The necessary
step to achieve mutual understanding in Language
Resources is to create a common data model.
The existence of a standard brings many other
advantages, like the ability to automatically com-
pare the results of dierent tools which provide the
same functionality, from the very basic (e.g. tok-
enization) to the most complex (e.g. discourse rep-
resentation). Some of the NIST-supported competi-
tive evaluations (e.g. MUC) greatly beneted by the
existence of scoring tools, which could automatically
compare the results of each participant against a gold
standard. The creation of such tools (and their ef-
fectiveness) was possible only because the organizing
institute had pre-dened and \imposed" upon the
participants the annotation scheme. However, that
sort of \brute force" approach might not always pro-
duce the best results. It is important to involve the
community in the denition of such standards at an
early stage, so that all the possible concerns can be
met and a wider acceptance can be achieved.
Another clear benet of agreed standards is that
they will increase interoperability among dierent
tools. It is not enough to have publicly available
APIs to ensure that dierent tools can be integrated.
In fact, if their representation languages (their \data
vocabulary") are too divergent, no integration will
be possible (or at least it will require a considerable
mapping eort). For all the above reasons we enthu-
siastically support any concertation work, aimed at
establishing common foundations for the eld.
In a recently started EU project (\Parmenides")
focusing on the integration of Information Extrac-
tion and Data Mining techniques (for Text Mining)
we aim at avoiding the problem of incompatibility
among dierent tools by dening a Common Annota-
tion Scheme internal to the project. However, when
the project was started (Sep 2002) we were unaware
of the standardization eort of ISO TC37 SC4, and
so we commenced once again trying to dene our own
schema. Fortunately, as this work is still at an early
stage (the project will last till 2005) it is still possible
to redirect it in a way that it will be compatible with
the standardization work of ISO.
In this paper we will describe the approach fol-
lowed so far in the denition of the Parmenides Com-
mon Annotation Scheme, even if its relation with ISO
is still only supercial. In the forthcoming months
our intention is to explore possible synergies between
our work and the current initiatives in ISO TC37
SC4, with the aim to get at a Parmenides annota-
tion scheme which is conformant to the approach cur-
rently discussed in the standardization committee.
2 The Parmenides Lingua Franca
In this section we will describe the XML-based anno-
tation scheme proposed for the Parmenides project.
In general terms the project is concerned with or-
ganisational knowledge management, specically, by
developing an ontology driven systematic approach
to integrating the entire process of information gath-
ering, processing and analysis.
The annotation scheme is intended to work as the
projects' lingua franca: all the modules will be re-
quired to be able to accept as input and generate
as output documents conformant to the (agreed) an-
notation scheme. The specication will be used to
create data-level compatibility among all the tools
involved in the project.
Each tool might choose to use or ignore part of
the information dened by the markup: some infor-
mation might not yet be available at a given stage
of processing or might not be required by the next
module. Facilities will need to be provided for lter-
ing annotations according to a simple conguration
le. This is in fact one of the advantages of using
XML: many readily available o-the-shelf tools can
be used for parsing and ltering the XML annota-
tions, according to the needs of each module.
The annotation scheme will be formally dened by
a DTD and an equivalent XML schema denition.
Ideally the schema should remain exible enough to
allow later additional entities when and if they are
needed. However the present document has only an
illustrative purpose, in particular the set of anno-
tation elements introduced needs to be further ex-
panded and the attributes of all elements need to be
veried.
There are a number of simplications which have
been taken in this document with the purpose of
keeping the annotation scheme as simple as possible,
however they might be put into question and more
complex approaches might be required. For instance
we assume that we will be able to identify a unique
set of tags, suitable for all the applications. If this
proves to be incorrect, a possible way to deal with
the problem is the use of XML namespaces. Our
assumptions allow us (for the moment) to keep all
XML elements in the same namespace (and there-
fore ignore the issue altogether).
2.1 Corpus Development
The annotation scheme will be used to create a de-
velopment corpus - a representative sample of the
domain, provided by the users as typical of the doc-
uments they manually process daily. In this phase,
the documents are annotated by domain experts for
the information of interest. This provides the bench-
mark against which algorithms can be developed and
tested to automate extraction as far as possible.
Of primary importance to the annotation process
is the consolidation of the \information of interest",
the text determined as the target of the Information
Extraction modules. Given the projects' goals, this
target will be both diverse and complex necessitating
clarity and consensus.
2.2 Sources Used for this Document
Parmenides aims at using consolidated Information
Extraction techniques, such as Named Entity Ex-
traction, and therefore this work builds upon well-
known approaches, such as the Named Entity anno-
tation scheme from MUC7 (Chinchor, 1997). Cru-
cially, attention will be paid to temporal annota-
tions, with the aim of using extracted temporal in-
formation for detection of trends (using Data Min-
ing techniques). Therefore we have investigated all
the recently developed approaches to such a problem,
and have decided for the adoption of the TERQAS
tagset (Ingria and Pustejovsky, 2002; Pustejovsky et
al., 2002).
Other sources that have been considered include
the GENIA tagset (GENIA, 2003), TEI (TEI Con-
sortium, 2003) and the GDA
1
tagset. The list of
entities introduced so far is by no means complete
1
http://www.i-content.org/GDA/tagset.html
but serves as the starting point, upon which to build
a picture of the domains from information types they
contain. The domain of interests (e.g. Biotechnol-
ogy) are also expected to be terminology-rich and
therefore require proper treatment of terminology.
To supplement the examples presented, a com-
plete document has been annotated according to the
outlined specication.
2
There are currently three
methods of viewing the document which oer dif-
fering ways to visualize the annotations. These
are all based on transformation of the same XML
source document, using XSLT and CSS (and some
Javascript for visualization of attributes). For exam-
ple, the basic view can be seen in gure (1).
3 Levels of Annotation
The set of Parmenides annotations is organized into
three levels:
 Structural Annotations
Used to dene the physical structure of the doc-
ument, it's organization into head and body,
into sections, paragraphs and sentences.
3
 Lexical Annotations
Associated to a short span of text (smaller than
a sentence), and identify lexical units that have
some relevance for the Parmenides project.
 Semantic Annotations
Not associated with any specic piece of text
and as such could be free-oating within the
document, however for the sake of clarity, they
will be grouped into a special unit at the end
of the document. They refer to lexical anno-
tations via co-referential Ids. They (partially)
correspond to what in MUC7 was termed `Tem-
plate Elements' and `Template Relations'.
Structural annotations apply to large text spans,
lexical annotations to smaller text spans (sub-
sentence). Semantic annotations are not directly
linked to a specic text span, however, they are
linked to text units by co-referential identiers.
All annotations are required to have an unique ID
and thus will be individually addressable, this allows
semantic annotations to point to the lexical annota-
tions to which they correspond. Semantic Annota-
tions themselves are given a unique ID, and therefore
can be elements of more complex annotations (\Sce-
nario Template" in MUC parlance).
2
available at http://www.ifi.unizh.ch/Parmenides
3
Apparently the term 'structure' is used with a dif-
ferent meaning in the ISO documentation, referring
to morpho-syntactical structure rather than document
structure.
Structural Annotations The structure of the
documents will be marked using an intuitively appro-
priate scheme which may require further adaptations
to specic documents. For the moment, the root
node is <ParDoc> (Parmenides Document) which
can contain <docinfo>, <body>, <ParAnn>. The
<docinfo> might include a title, abstract or sum-
mary of the documents contents, author informa-
tion and creation/release time. The main body
of the documents (<body>) will be split into sec-
tions (<sec>) which can themselves contain sec-
tions as well as paragraphs (<para>). Within the
paragraphs all sentences will be identied by the
<sentence> tag. The Lexical Annotations will
(normally) be contained within sentences. The -
nal section of all documents will be <ParAnn> (Par-
menides Annotations) where all of the semantic an-
notations that subsume no text are placed. Figure
(2) demonstrates the annotation visualization tool
displaying the documents structure (using nested
boxes).
Lexical Annotations Lexical Annotations are
used to mark any text unit (smaller than a sentence),
which can be of interest in Parmenides. They include
(but are not limited to):
1. Named Entities in the classical MUC sense
2. New domain-specic Named Entities
3. Terms
4. Temporal Expressions
5. Events
6. Descriptive phrases (chunks)
The set of Lexical Annotations described in this
document will need to be further expanded to cover
all the requirements of the project, e.g. names of
products (Acme Arms International's KryoZap (TM)
tear gas riot control gun), including e.g. names of
drugs (Glycocortex's Siderocephalos).
When visualizating the set of Lexical Tags in a
given annotated document, clicking on specic tags
displays the attribute values (see gure (3)).
Semantic Annotations The relations that exist
between lexical entities are expressed through the
semantic annotations. So lexically identied peo-
ple can be linked to their organisation and job ti-
tle, if this information is contained in the document
(see gure (4)). In terms of temporal annotations, it
is the explicit time references and events which are
identied lexically, the temporal relations are then
captured through the range of semantic tags.
Figure 1: Basic Annotation Viewing
3.1 Example
While the structural annotations and lexical annota-
tions should be easy to grasp as they correspond to
accepted notions of document structure and of con-
ventional span-based annotations, an example might
help to illustrate the role of semantic annotations.
(1) The recent ATP award is
<ENAMEX id="e8" type="ORGANIZATION">
Dyax
</ENAMEX>
's second, and follows a
<NUMEX id="n5" type="MONEY">
$4.3 million
</NUMEX>
<ENAMEX id="e9" type="ORGANIZATION">
NIST
</ENAMEX>
grant to
<ENAMEX id="e10" type="ORGANIZATION">
Dyax
</ENAMEX>
and
<ENAMEX id="e11" type="ORGANIZATION">
CropTech Development Corporation
</ENAMEX>
in
<TIMEX3 tid="t4" type="DATE" value="1997">
1997
</TMEX3>
There are two occurrences of Dyax in this short
text: the two Lexical Entities e8 and e10, but clearly
they correspond to the same Semantic Entity. To
capture this equivalence, we could use the syntactic
notion of co-reference (i.e. Identify the two as co-
referent). Another possible approach is to make a
step towards the conceptual level, and create a se-
mantic entity, of which both e8 and e10 are lexical
expressions (which could be dierent, e.g. \Dyax",
\Dyax Corp.", \The Dyax Corporation"). The sec-
ond approach can be implemented using an empty
XML element, created whenever a new entity is men-
tioned in text. For instance, in (2) we can use the tag
Figure 2: Visualization of Structural Annotations
<PEntity> (which stands for Parmenides Entity).
(2) <PEntity peid="obj1" type="ORGANIZATION"
mnem="Dyax" refid="e1 e3 e6 e8 e10 e12"/>
The new element is assigned (as usual) a unique
identication number and a type. The attribute mnem
contains just one of the possible ways to refer to the
semantic entity (a mnemonic name, possibly chosen
randomly). However, it also takes as the value of
the refid attribute as many coreferent ids as are
warranted by the document. In this way all lexical
manifestations of a single entity are identied. All
the lexical entities which refer to this semantic entity,
are possible ways to `name' it (see also g. 4).
Notice that the value of the `type' attribute has
been represented here as a string for readability pur-
poses, in the actual specication it will be a pointer
to a concept in a domain-specic Ontology.
Other semantic entities from (1) are:
(3) <PEntity peid="obj2" type="ORGANIZATION"
mnem="NIST" refid="e2 e4 e7 e9"/>
<PEntity peid="obj3" type="ORGANIZATION"
mnem="CropTech" refid="e11"/>
The newly introduced semantic entities can then
be used to tie together people, titles and organiza-
tions on the semantic level. Consider for example
the text fragment (4), which contains only Lexical
Annotations.
(4) ... said
<ENAMEX id="e17" type="PERSON">
Charles R. Wescott
</ENAMEX>
, Ph.D.,
<ROLE type='x' id="x5">
Senior Scientist
</ROLE>
at
<ENAMEX id="e60" type="ORGANIZATION">
Dyax Corp
</ENAMEX>
The Lexical Entity e17 requires the introduction
of a new semantic entity, which is given the arbitrary
identier `obj5':
(5) <PEntity peid="obj5" type="PERSON"
mnem="Charles R. Wescott" refid="e17"/>
Figure 3: Visualization of Lexical Annotations and their attributes
In turn, this entity is linked to the entity obj1
from (1) by a relation of type `workFor' (PRelation
stands for Parmenides Relation):
(6) <PRelation prid="rel2" source="obj5"
target="obj1" type="worksFor" role="Senior
Scientist" evidence="x5"/>
4 Discussion
As the status of the Parmenides annotation scheme
is still preliminary, we aim in this section to pro-
vide some justication for the choices done so far
and some comparison with existing alternatives.
4.1 Named Entities
One of the purposes of Named Entities is to instanti-
ate frames or templates representing facts involving
these elements. A minor reason to preserve the clas-
sic named entities is so that we can test an IE system
against the MUC evaluation suites and know how
it is doing compared to the competition and where
there may be lacunae. As such, the MUC-7 speci-
cation (Chinchor, 1997) is adopted with the minor
extension of a non-optional identication attribute
on each tag.
4.2 Terminology
A term is a means of referring to a concept of a spe-
cial subject language; it can be a single wordform,
a multiword form or a phrase, this does not matter.
The only thing that matters is that it has special
reference: the term is restricted to refer to its con-
cept of the special domain. The act of (analytically)
dening xes the special reference of a term to a con-
cept. Thus, it makes no sense to talk of a term not
having a denition. A concept is described by den-
ing it (using other certain specialised linguistic forms
(terms) and ordinary words), by relating it to other
concepts, and by assigning a linguistic form (term)
to it.
If we are interested in fact extraction from densely
terminological texts with few named entities apart
from perhaps names of authors, names of laborato-
ries, and probably many instances of amounts and
measures, then we would need to rely much more on
prior identication of terms in the texts, especially
where these are made up of several word forms.
A term can have many variants: even standard-
ised terms have variants e.g. singular, plural forms
of a noun. Thus we should perhaps more correctly
refer to a termform, at least when dealing with text.
Among variants one can also include acronyms and
reduced forms. You therefore nd a set of variants,
typically, all referring to the same concept in a special
domain: they are all terms (or termforms). Again
this problem pinpoints the need for a separation of
the lexical annotations (the surface variants within
the document) and semantic annotations (pointing
abstractly to the underlying concept).
4.3 Approaches to Temporal Annotations
TIDES (Ferro et al, 2001) is a temporal annota-
tion scheme that was developed at the MITRE Cor-
poration and it can be considered as an extension
of the MUC7 Named Entity Recognition (Tempo-
ral Entity Recognition - TIMEX Recognition) (Chin-
chor, 1997). It aims at annotating and normalizing
explicit temporal references. STAG (Setzer, 2001)
is an annotation scheme developed at the University
of She?eld. It has a wider focus than TIDES in
the sense that it combines explicit time annotation,
event annotation and the ability to annotate tempo-
ral relations between events and times.
TimeML (Ingria and Pustejovsky, 2002) stands for
\Time Markup Language" and represents the inte-
gration and consolidation of both TIDES and STAG.
It was created at the TERQAS Workshop
4
and is
designed to combine the advantages of the previous
temporal annotations schemes. It contains a set of
tags which are used to annotate events, time expres-
sions and various types of event-event, event-time
and time-time relations. TimeML is specically tar-
geted at the temporal attributes of events (time of
occurrence, duration etc.).
As the most complete and recent, TimeML should
be adopted for the temporal annotations. Broadly,
its organization follows the Parmenides distinction
between lexical/semantic annotations. Explicit tem-
poral expressions and events receive an appropriate
(text subsuming) lexical tag. The temporal rela-
tions existing between these entities are then cap-
tured through a range of semantic (non-text subsum-
ing) tags.
For example, each event introduces a correspond-
ing semantic tag. There is a distinction be-
tween event \tokens" and event \instances" moti-
vated by predicates that represent more than one
event. Accordingly, each event creates a semantic
<MAKEINSTANCE> tag that subsumes no text. Ei-
ther, one tag for each realised event or a single tag
with the number of events expressed as the value of
the cardinality attribute. The tag is introduced and
the event or to which it refers is determined by the
attributes eventID.
5 Conclusion
We believe that ISO TC37/SC4 provides a very in-
teresting framework within which specic research
concerns can be addressed without the risk of rein-
venting the wheel or creating another totally new
4
http://www.cs.brandeis.edu/~jamesp/arda/time
and incompatible annotation format. The set of an-
notations that we have been targeting so far in Par-
menides is probably a small subset of what is tar-
geted by ISO TC37/SC4. Although we had only lim-
ited access to the documentation available, we think
our approach is compatible with the work being done
in ISO.
It is, we believe, extremely important for a project
like ours, to be involved directly in the ongoing dis-
cussion. Moreover we are at precisely the right stage
for a more direct `exposure' to the ISO TC37/SC4
discussion, as we have completed the exploratory
work but no irrevocable modeling commitment has
so far been taken. Therefore we would hope to be-
come more involved in order to make our proposal
t exactly into that framework. The end result of
this process might be that Parmenides could become
a sort of \Guinea Pig" for at least a subset of ISO
TC37 SC4.
Acknowledgments
The Parmenides project is funded by the European
Commission (contract No. IST-2001-39023) and
by the Swiss Federal O?ce for Education and Sci-
ence (BBW/OFES). All the authors listed have con-
tributed to the (ongoing) work described in this pa-
per. Any remaining errors are the sole responsibility
of the rst author.
References
Nancy Chinchor. 1997. MUC-7 Named Entity Task Denition, Version
3.5. http://www.itl.nist.gov/iaui/894.02/
related projects/muc/proceedings/ne task.html.
Lisa Ferro, Inderjeet Mani, Beth Sundheim, and George Wilson. 2001.
Tides temporal annotation guidelines, version 1.0.2. Technical re-
port, The MITRE Corporation.
GENIA. 2003. Genia project home page. http://www-tsujii.is.s.u-
tokyo.ac.jp/~genia.
Bob Ingria and James Pustejovsky. 2002. TimeML
Specication 1.0 (internal version 3.0.9), July.
http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
James Pustejovsky, Roser Sauri, Andrea Setzer, Rob Giazauskas, and
Bob Ingria. 2002. TimeML Annotation Guideline 1.00 (internal
version 0.4.0), July. http://www.cs.brandeis.edu/%7Ejamesp/arda
/time/documentation/TimeML-Draft3.0.9.html.
Andrea Setzer. 2001. Temporal Information in Newswire Articles: An
Annotation Scheme and Corpus Study. Ph.D. thesis, University of
She?eld.
TEI Consortium. 2003. The text encoding initiative. http://www.tei-
c.org/.
Figure 4: Visualization of Semantic Annotations
Sentence Completion Tests for Training and Assessment in a
Computational Linguistics Curriculum
Cerstin Mahlow, Michael Hess
Institute of Computational Linguistics, University of Zurich
Winterthurerstr. 190
CH-8057 Zurich,
Switzerland,
{mahlow, hess}@cl.unizh.ch
Abstract
This paper presents a novel type of test, halfway
between multiple-choice and free-form text,
used for training and assessment in several
courses in a Computational Linguistics curricu-
lum. We will describe the principles of the test,
the different ways in which it can be used by
learners, and the tools developed for authoring.
Use of this type of test is not limited to the field
of Computational Linguistics. Wherever text
heavy or even picture based topics are taught
use of this type of test is possible.
1 Introduction
Students of Computational Linguistics (CL)
at the University of Zurich come from two
different faculties, viz. the Faculty of Arts
and the Faculty of Economics, Business Man-
agement and Information Technology. Thus
they have a very uneven previous knowledge of
linguistics and programming. The introductory
lectures touch upon most aspects of CL but
cannot compensate for these differences in a
satisfactory way. We are trying to ease the
problem by supplying students with extensive
additional on-line reading material for individ-
ual study. However, until recently students had
no way of testing the knowledge they acquired
through self-study against the requirements
of the courses. For this reason we developed
web-based tools for individual training and
self-assessment.
Most assessments in web-based learning
courses use Multiple Choice (MC) tests. These
tests are easy to create for authors and easy
to use for students. Unfortunately the concept
of MC imposes a very restrictive format on
the tests, and they can basically test only the
presence or absence of small ?knowledge bites?.
More general and abstract types of knowledge
are hard to test by means of MC.
Free-form text tests, i.e. tests allowing replies
in the form of mini-essays, are, of course, far
less restrictive but the costs of assessing them
by hand is, in many institutional contexts,
prohibitively high. Systems for reliable and
consistent automatic assessment of free-form
text are not yet available. Those that exist
either test writing style, or test the presence
or absence in an essay of (explicit) terms or of
(implicit) concepts (example: IEA; (Landauer
et al, 1998, p295-284)), or use a combination
of surface lexical, syntactic, discourse, and
content features (example: e-rater; (Burstein,
2003)). It was shown, by the system developers
themselves, that the most advanced of these
systems, e-rater, can be tricked rather easily
into giving marks that are far too good, by
using some knowledge of the techniques used
by the system (Powers et al, 2001). Since
knowledge of the techniques used by rating
systems can hardly be kept secret for any
length of time all such feature based systems
are open to this kind of tricks.
This is why we developed a new type of test,
called ?Satzerga?nzungstests? (SET) ? SET1,
positioned halfway between multiple-choice
tests and free-form text tests. We use this type
of test for training as well as for assessments,
and it is part of our web-based curriculum.
The development was funded by the University
in view of the implementation of the Bache-
lor/Master/PhD based ?Bologna scheme? in
most European universities (see (European
Ministers of Education, 1999)).
With SETs we are able to create far more
demanding tasks for training and assessment
than we could otherwise. The philosophy
behind SETs will be presented in Section 2.
1?Sentence Completion Tests?.
In Section 3 we will show how the individual
student can use a test. In Section 4 we will
show how to create tests. In section 5, finally,
we will give an overview of the courses in which
we use these tests for teaching Computational
Linguistics (CL), and discuss in which other
contexts they could be used.
2 The philosophy behind SETs
(Ru?tter, 1973) creates an extensive topology for
assessments. He distinguishes between open,
semi-open and closed tasks. The distinction
derives from the type of answer expected
from the learner: There is no certain answer
the author expects (open tasks), the author
expects a certain answer the learner has to
create themselves (semi-open tasks), the learner
has to choose the right answer(s) from given
possibilities (closed tasks). Multiple Choice
tasks (MC) belong to the closed tasks.
The topology presented by Ru?tter is not
restricted to the easy tasks. You will also find
so-called ?Erweiterungswahlaufgaben? in the
class of closed assigns. This task consists of
a piece of information the tested person has
to extend so as to create a coherent piece
of new information. The learner can choose
suitable extensions from a given list. Ru?tter?s
description includes the hint that these tasks
are hard to design but present a very clear
structure for the test person.
Our Sentence Completion Tests can be seen
as an instance of such Erweiterungswahlauf-
gaben. The learner has to answer a complex
question in near-free-form on the basis of
extensive choices of possible answer compo-
nents supplied by the system. There will be
answer components considered indispensable,
some considered correct but optional, others
categorised as outright wrong, and others still
rated as irrelevant to the question at hand.
The required components of the answer will all
have to occur in the answer but not in a fixed
order.
In concrete terms this means that a learner
will author an answer in the form of a complete
sentence, in a piecemeal fashion and under the
guidance of the system, by picking answer frag-
ments from dynamically adapted choice menus
popping up as the answer is growing. At each
step the user input will be checked by the sys-
tem against the answer model that contains all
the expected answer parts, essential relation-
ships between them, and possible restrictions
on the order of these parts. At each step as
well as at the very end the system can generate
feedback for the user which will make him un-
derstand why and in which aspects his answer
is correct or incorrect.
3 How to use a SET
All SETs are presented under a web interface.
The student has to start a browser2 and choose
a single SET3.
3.1 Basic elements of a SET
The student sees four coloured fields4, each la-
beled with a number and a functional descrip-
tion. These fields are:
1. Text up to now
2. Comments/Feedback
3. List of elements to continue
4. Preview
Text up to now contains the question and the
answer in its present state. List of elements
to continue consists of possible continuations
of the answer. Clicking on one of the radio
buttons activates the Preview showing all the
options that will become available once the
learner has committed himself to the given
continuation. That way the user is always
aware of the consequences his choice might
have. The listing field includes two submit
buttons, one for submitting the choice, one for
undoing the last choice. The element list will
show the elements in different order each time
the user reloads or restarts a SET.
The crucial field is the one for Com-
ment/Feedback. The user does not merely get
a ?Right - Wrong? feedback but rather
? If the answer contains the correct compo-
nents but a wrong relationship the feedback
will point this out and invite the user to try
again and find the correct combination.
2At the moment the SET web interface is tested with
Netscape and InternetExplorer.
3Where to find all SETs will be described in sec-
tion 5.2.
4The same colours are used in our ordinary MC-tests
to give the student a familiar feeling for the assessment
situation.
? If the answer consists of correct compo-
nents as well as of wrong ones the feedback
will say so and point out which components
are wrong.
? If the answer is one of the correct ones,
the feedback will approve this solution and
mention the other possible correct answers.
This way for every possible combination of
answer components the user gets a different
optimised feedback.
The text inside the feedback field is displayed
as HTML so that it is possible to include links
to related SETs, back to the lecture notes or
associated material. A feedback text also can
include a link to a new SET, as a followup.
Sometimes it is useful to have the system
generate a comment before a complete an-
swer has been created by the learner. Once
the learner has chosen a certain number of
wrong answer components he will get suitable
feedback before finishing. In this case the
feedback is used to warn the user that he is on
a completely wrong track and that he ought to
undo some of the last choices, or to start again
from scratch.
3.2 A sample SET
See figure 1 for a sample session with SET.
The initial question is: ?Was ist ein Parser??
(What is a parser?).
Here the user chose ?Ein Parser ist eine
Prozedur? (?A parser is a procedure?) as next
element in the third field. This will be the
beginning of his answer. Clicking on the corre-
sponding radio button activated the preview in
the fourth field. Before submitting the choice,
the user can think about the combinations
his choice will allow. The preview shows 4
possibilities to continue with the description of
the aim of this procedure.
If the user is satisfied with his choice, he will
click the submit button ?Auswahl besta?tigen?
(Confirm choice). This will result in reloading
the site with the new information.
Text bisher (Text up to now) will contain
the question, the beginning of the answer and
the fragments added by the learner so far ?Ein
Parser ist eine Prozedur?. The feedback field
will still be empty. Auswahl der Fortsetzungen
(List of elements to continue) will show all
possible continuations. Vorschau (Preview)
will be empty until the user clicks on one of the
radio buttons in the list of elements to continue.
This sequence of actions will be repeated
until the user has created a complete sentence.
He then gets the feedback. If he is not satisfied
with one of his choices before finishing, he can
undo the last choice, or simply restart the SET.
In case the user is on a probably wrong
way he will get feedback before finishing the
SET. See figure 2 for an example. The user
created an answer start ?Ein Parser ist eine
Prozedur, um die syntaktische Korrektheit einer
Sprache...? (?A parser is a prozedure to ... the
syntactical correctness of a language?). The
intervening feedback points to the principle of
correctness concerning certain constructions
of languages and prompts the user to undo
the last decission(s). (?Was wohl soll die
syntaktische Korrektheit einer Sprache sein?!
Nur einzelne Konstruktionen einer Sprache
ko?nnen korrekt oder inkorrekt sein. Einen
Schritt zuru?ck!?
Figure 3 shows the finished SET. The user
followed the hint in the intervening feedback
shown in figure 2. He removed the part ?einer
Sprache? (?of a language?). The answer
created by the user is ?Ein Parser ist eine
Prozedur um die syntaktische Korrektheit eines
Satzes zu ermitteln? (?A parser is a procedure
to detect the syntactical correctnes of a sen-
tence?). Clearly, this answer is not correct. It
describes rather an acceptor than a parser. The
comment says so and then offers a correct def-
inition with a hint to the latin origins of Parser.
3.3 Training mode and assessment
mode
SETs can be used in E-Learning for training as
well as for assessments. Self-assessment can be
seen as an instrument for training ? users get
elaborate feedback for their answers and are
invited to try again.
In the training/self-assessment mode users
get feedback after completing the answer or
while composing it. The feedback always takes
into account all components collected up to
Figure 1: Snapshot of a SET session for answering the SET ?What is a parser??
that point as well as the user?s undo or redo
actions. The user is allowed to undo a decision
as often as he likes. This way finding the right
answer is a question of either knowing it or
following the hints in the feedback.
In the assessment mode the user gets a
number of points credited. The points total is
compiled the same way the feedback is created.
Depending on the answer fragments chosen by
the learner, and on their order, the points total
will be computed. It is also possible to chain
several SETs one after another5, collect the
credits collected in each of them, and present
the grand total at the very end.
The user can be allowed to use the undo but-
ton in different manners. Three settings are
possible:
? The undo button can be used as often as
the learner wants but each use is logged in
the background.
5SETs can be linked in linear or network like fashion
via HTML links or followups in the comments.
? Each use of the undo button results in a
deduction of a certain number of points,
and its use is logged.
? The use of the button is allowed only a pre-
set number of times ? if the user tries to
undo more often, the button is disabled.
That way tutors can track whether the student
arrived at the answer by merely trying out all
possible continuations.
4 How to create a SET
What does an author have to consider when
creating a SET? First, he has to decide which
answer elements the user can choose from at
any given step. Second, he must make sure
that any of the answer components offered as
a choice at a given step will contribute to a
well-formed sentence only. Finally, helpful and
syntactically well-formed comments have to be
defined for any of the possible answers.
What the presentation of a SET ultimately
boils down to is running a Finite State Automa-
ton (FSA), with answer components as states
Figure 2: Snapshot of an intermidate result while answering the SET ?What is a parser??
and user choices as input. This is done by a
Prolog program as the back-end for a single
SET. As input it takes the SET specific Prolog
automaton, the path up to now, and the current
choice of the user. As output it creates the new
current answer, the new list of elements to con-
tinue, the preview, comments, paths and points.
The author of a SET has thus to write
a (potentially large) FSA. This is a tedious
and error-prone task. How can this be done
efficiently and reliably?
4.1 The machinery behind a SET
Developing the automaton normally starts
with the author writing a number of possible
correct and incorrect answers, in a way similar
to the development of an ordinary MC. The
author then marks where these sentences could
be split into fragments. Splitting must allow
the combination of various sentence fragments
from different sentences in a way that only
well-formed passages result. To limit the
number of such combinations the author can
define constraints that explicitly include or
exclude certain combinations.
To increase readability, answer fragments
that are of the same syntactic type can be
collected in boxes. It is, however, advisable
to create distinct boxes for correct fragments,
wrong fragments, and indifferent fragments of
the same syntactic type; this makes the design
of complex automata considerably easier. Each
box has an ID, in-going and outgoing boxes6,
information concerning specific constraints
on allowed combinations, and (positive or
negative) credits the user will collect when
choosing this element. Boxes are linked by
vectored edges to create a number of paths
through the answer fragments, each one of
which will define a complete and syntactically
well-formed sentence.
Splitting answer sentences into fragments
that can be combined freely creates, of course,
a large number of potential answers (in fact,
6Except start boxes ? no in-going box ? and the boxes
at the end of a sentence path ? no outgoing box.
Figure 3: Snapshot of the finished SET session for answering the SET ?What is a parser??
a potentially infinite number). It would
be clearly impossible to write individual
comments for each of these answers. We over-
come this obstacle by generating comments,
semi-automatically in some cases, and fully
automatically in others. The semi-automatical
creation relies on the fact that each answer
fragment can be rated according to its correct-
ness and relevance for a given question. It is
relatively easy to attach, to a limited number
of ?strategically important? answer fragments,
comment fragments specifying in what way
they are (in)correct and (ir)relevant. We then
have SET collect the comment fragments of
all answer fragments chosen by the learner,
and combine them into complete and syntac-
tically well-formed comments that refer to the
individual parts of an answer and point out
superfluous, missing, or wrong bits, in any
degree of detail desired by the author. We
can even generate comments on the basis of
arbitrarily complex logical conditions over an-
swer fragments, thus identifying, among others,
contradictions in answers. That way we can
generate a potentially infinite number of com-
ments on the basis of relatively few comment
fragments. This is the semi-automatic creation
of comments, taking into account the local
properties of an answer path. We also allow
the fully automatic creation of comments that
take into consideration the global properties
of answer paths. Thus the fact that a learner
used the undo button very often in various
places, or took a very circuitous way to arrive
at his answer, may be detected by measuring
global values of the answer path and can then
be commented upon automatically7. For a
detailed documentation see (Brodersen and
Lee, 2003).
4.2 Developing a sample SET
Clearly the author of a SET must be supported
in the design of even moderately complex FSAs.
To this end we developed an authoring tool
called Satzerga?nzungstest-Ersteller-Interface
(SETEI), a Java application with a GUI. It uses
7Resulting in comments like ?You used the undo but-
ton way too often.? or ?Correct but your answer could
have been much shorter?, etc.
a text-based format for saving data and has an
export function to create the FSA. Figure 4
shows the final stages in the development of
the SET ?Was ist ein Parser?? (?What is a
parser??) used as example in section 3.2.
The box in the left upper corner is the
start box, containing the question. Boxes
1, 2, 3, 4, 6, 7, 8, 9, 13 are answer boxes
containing answer fragments. Boxes 10, 11, 12,
14 are comment boxes containing comments
for complete answers or certain combinations
of answer parts (box 14).
One of the boxes, box 14, is selected, and
inside this box the text element 72 is selected.
As the boxes offer limited space the full text of
a selected element is shown at the very bottom
of the window. Here we can also see the box
number, fragment number, and the credits
attached to the selected answer fragment.
These credits can be used, in assessment mode,
to grade the answer. Creating, filling, and
modifying boxes is a matter of a few clicks.
The possible answer paths are represented,
obviously, as vectored edges between boxes.
Each path must end in a comment box.
? Two paths contain three boxes ? 1?8?9
and 1?2?7
? Two paths contain four boxes ? 1?2?3?7
and 1?2?6?13
? One path contains five boxes ?
1?2?3?4?7
Possible answers in the above example may
thus consist of three, four or five parts. Since
each answer box contains at least two text
elements this automaton defines many more
answers than there are paths. On path 1?2, for
instance, the user can combine each element in
box 1 with each element in box 2. Connections
between boxes are created or deleted by simple
dragging or clicking operations. Whenever a
circular connection is created, even an indirect
one, the user is asked whether this is what he
really wanted to do.
The top menu in the window contains the
various tools for the manipulation of boxes.
Thus, to see all text elements in one box plus
all the in-going and out-going boxes and the
constraints for elements, the author may use
the box browser Ansicht (view). The browser
presents a magnified view on the given box
with additional functionalities to edit the
box content. The user can also zoom out
and see the bare structure of the entire FSA,
without box contents, can select sub-parts of
the automaton and try them out in isolation,
etc.
To allow intermediate feedback, comment
boxes may be placed in the middle of the FSA
(such as, in this SET, comment box 14). All
answer paths end with a comment box to give
feedback after creating a complete sentence.
5 Where to use SET
5.1 Where we use SETs
Since winter term 2003/2004 we use SETs at
our institute as a training and self-assessment
tool in introductory courses on CL. They are
often used as final element in learning units in-
tended for self-study by students. These learn-
ing units each cover one particular aspect of
Computational Linguistics that may be unfa-
miliar to part of the audience (such as regu-
lar expression, tokenising, tagging or parsing).
They are organised around Problem-based In-
teractive Learning Applications.8 While simple
skills can be tested with standard MC meth-
ods, for more general and more abstract types
of knowledge SETs turned out to be a much bet-
ter solution. Any type of question that would,
ideally, require a free form answer can be turned
into a SET. These are definitional questions
(?What is a parser??) as well as a questions
requiring comparisons between concepts (?How
does a parser differ from an acceptor??) and the
description of procedures (?What are the pro-
cessing steps of a transfer Machine Translation
system??). It is important that SETs can deter-
mine, and comment upon, non-local properities
of answers. Thus a SET can detect contradic-
tions between different parts of an answer, or a
wrong sequencing in the description of process-
ing steps (say, putting tokenising after parsing),
or repetitions, all of which may occur in parts of
an answer that are arbitrarily far removed from
each other.
8See (Carstensen and Hess, 2003) for more informa-
tion.
Figure 4: SETEI session for creating the SET ?What is a parser??
5.2 Real Examples of SETs
SETs have been developed mainly for the intro-
ductory classes in Computational Linguistics
at Zurich but new tests for more advanced
courses are under development. Since classes
are taught in German, all SETs are in German,
too.
Students can access SETs in two ways:
? As most SETs are used in Learning Units
students will encounter SETs for the first
time when they are working their way
through the Learning Units.
? When preparing for exams students want
to have random access to SETs. For this
reason all SETs ever developed are accessi-
ble via one big collection, our Setcenter.
The Setcenter
www.cl.unizh.ch/ict-open/satztest/setcenter.html
offers a check-box list to create a customised
web page containing a short introduction to
SETs, help for using them, and a list of links
to the chosen SETs. For a first look at SETs
the page www.ifi.unizh.ch/cl/ict-open/satztest/,
with pre-defined examples from outside the
field of Computational Linguistics, may also be
useful.
Most of the SETs we developed ask questions
about the basic concepts and terms of the field.
Some examples are listed in table 1.
In some case we also ?abuse? SETs to
function itself as authoring tool with feedback
facilities. In one case students are asked to
Intro to CL 1 Intro to CL 2
CL Extension / Intension
Linguistics Propositions
Morphology Presuppositions
Semantics Axioms
Parsing Modus Ponens
FSA Lambda Abstracting
Generative Power
Types of Ambiguity
Indexing
Information Extraction
Information Retrieval
Machine Translation
Table 1: SETs in the introductory lectures for
CL
write specific rules for a chunking grammar.
In a SET, they get a set of rule elements
to choose from (pre-terminal and terminal
categories, parentheses, Kleene star, etc.) and
have to combine them, step by step, creating a
grammar rule in the process. If their choice of a
symbol is completely off track (such as a gram-
mar rule beginning with a closing parenthesis)
they are warned right away. Otherwise the
structure of the completed rule is commented
upon. If the rule is not correct, users are sent
back to the beginning. Otherwise they are sent
to a subsequent SET, with a more demanding
task. That way, by chaining SETs, we teach
them to write increasingly complex chunking
rules, under close guidance of the system. This
turned out to be a very promising use of SETs.
5.3 Use of SET in other topics
The question arises whether it would be
possible to use SETs in fields other than CL.
In general, in all fields where short textual
descriptions are the best way to answer ques-
tions, SETs are a good way to automatise
training and testing. SETs are of particular
interest to the Arts and Humanities, but
the Medical Sciences might also be a field
that could benefit form SETs (for instance,
a picture is presented and the user is asked
to describe what seems important or abnormal).
6 Conclusions
In training or assessment situations where
correct answers to questions do not consist of
one (or a few) isolated items (words, numbers,
symbols) but where a complete description in
natural language is required, and when human
tutors are not available, SET is the right tool
to use. It allows to simulate, to some extent,
the detailed comments to individual aspects of
an answer that make human tutors so valuable.
While SETs are great once they have been
written, the process of authoring them is still
painful, demanding, error-prone, and thus
extremely time-consuming. We will need
authoring tools that allow a top-down kind of
design for SETs, with stepwise refinement of
the code and on-the-fly testing of selected parts
of the FSA, instead of the low-level design
process used now. It would also be very useful
to have programs that work, bottom-up, from
possible answers to FSAs, by automatically
identifying common phrases in answers and
collecting them in boxes. We developed such
a system and found it very useful but its
grammatical coverage is too small to make it
viable in practice. The automatic creation of
terminological variations in potential answers,
by accessing on-line lexical resources, will be
another feature that might make life easier for
test developers. We continue work on all of
these lines of research.
7 Acknowledgements
Our thanks go to Sonja Brodersen and David
Lee, who developed the SET and SETEI en-
vironments, to Esther Kaufmann, who created
most of the existing SETs, and to Kai-Uwe
Carstensen for valuable feedback on the results.
References
Sonja Brodersen and David Lee. 2003.
Dynamisches Multiple-Choice mit Satz-
Erga?nzungstests. Dokumentation zum
gesamten Satztestprojekt. unpublished,
December 2003.
Jill Burstein. 2003. The e-rater scoring en-
gine: Automated essay scoring with nat-
ural language processing. In M. D. Sher-
mis and J. Burstein, editors, Automated es-
say scoring: A cross-disciplinary perspective.
Lawrence Erlbaum Associates, Inc., Hillsdale,
NJ.
Kai-Uwe Carstensen and Michael Hess.
2003. Problem-based web-based teaching
in a computational linguistics curriculum.
www.linguistik-online.de, 17(5/2003).
European Ministers of Education. 1999.
The bologna declaration of 19 june 1999.
www.bologna-berlin2003.de/pdf/bologna-
declaration.pdf, June 1999.
T. K. Landauer, P. W. Foltz, and D. Laham.
1998. Introduction to Latent Semantic Anal-
ysis. Discourse Processes.
Donald E. Powers, Jill Burstein, Martin
Chodorow, Mary E. Fowles, and Karen Ku-
kich. 2001. Stumping E-Rater: Challeng-
ing the Validity of Automated Essay Scoring.
GRE Research, GRE Board Professional Re-
port No. 98-08bP, ETS Research Report 01-
03.
Theodor Ru?tter. 1973. Formen der Tes-
taufgabe. Eine Einfu?hrung fu?r didaktische
Zwecke. C.H.Beck.
