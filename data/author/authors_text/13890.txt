Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 799?809,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Latent-Descriptor Clustering for Unsupervised POS Induction 
 
 
Michael Lamar 
Department of Mathematics and Computer Science 
Saint Louis University 
220 N. Grand Blvd. 
St.Louis, MO 63103, USA 
mlamar@slu.edu 
 
Yariv Maron 
Gonda Brain Research Center 
Bar-Ilan University 
Ramat-Gan 52900, Israel 
syarivm@yahoo.com 
Elie Bienenstock 
Division of Applied Mathematics 
and Department of Neuroscience 
Brown University 
Providence, RI 02912, USA 
elie@brown.edu 
 
 
Abstract 
We present a novel approach to distributional-
only, fully unsupervised, POS tagging, based on 
an adaptation of the EM algorithm for the esti-
mation of a Gaussian mixture. In this approach, 
which we call Latent-Descriptor Clustering 
(LDC), word types are clustered using a series 
of progressively more informative descriptor 
vectors. These descriptors, which are computed 
from the immediate left and right context of 
each word in the corpus, are updated based on 
the previous state of the cluster assignments. 
The LDC algorithm is simple and intuitive. Us-
ing standard evaluation criteria for unsupervised 
POS tagging, LDC shows a substantial im-
provement in performance over state-of-the-art 
methods, along with a several-fold reduction in 
computational cost. 
1 Introduction 
Part-of-speech (POS) tagging is a fundamental 
natural-language-processing problem, and POS 
tags are used as input to many important appli-
cations. While state-of-the-art supervised POS 
taggers are more than 97% accurate (Toutanova 
et al, 2003; Tsuruoka and Tsujii, 2005), unsu-
pervised POS taggers continue to lag far behind. 
Several authors addressed this gap using limited 
supervision, such as a dictionary of tags for each 
word (Goldwater and Griffiths, 2007; Ravi and 
Knight, 2009), or a list of word prototypes for 
each tag (Haghighi and Klein, 2006). Even in 
light of all these advancements, there is still in-
terest in a completely unsupervised method for 
POS induction for several reasons. First, most 
languages do not have a tag dictionary. Second, 
the preparation of such resources is error-prone. 
Third, while several widely used tag sets do ex-
ist, researchers do not agree upon any specific 
set of tags across languages or even within one 
language. Since tags are used as basic features 
for many important NLP applications (e.g. 
Headden et al 2008), exploring new, statistically 
motivated, tag sets may also be useful. For these 
reasons, a fully unsupervised induction algo-
rithm has both a practical and a theoretical val-
ue. 
In the past decade, there has been a steady 
improvement on the completely unsupervised 
version of POS induction (Sch?tze, 1995; Clark, 
2001; Clark, 2003; Johnson, 2007; Gao and 
Johnson, 2008; Gra?a et al, 2009; Abend et al, 
2010; Lamar et al, 2010; Reichart et al, 2010; 
Berg-Kirkpatrick et al, 2010). Some of these 
methods use morphological cues (Clark, 2001; 
Clark, 2003; Abend et al, 2010; Reichart et al, 
2010; Berg-Kirkpatrick et al, 2010), but all rely 
heavily on distributional information, i.e., bi-
799
gram statistics. Two recent papers advocate non-
disambiguating models (Abend et al, 2010; 
Lamar et al, 2010): these assign the same tag to 
all tokens of a word type, rather than attempting 
to disambiguate words in context. Lamar et al 
(2010) motivate this choice by showing how 
removing the disambiguation ability from a 
state-of-the-art disambiguating model results in 
increasing its accuracy. 
 
In this paper, we present a novel approach to 
non-disambiguating, distributional-only, fully 
unsupervised, POS tagging. As in all non-
disambiguating distributional approaches, the 
goal, loosely stated, is to assign the same tag to 
words whose contexts in the corpus are similar. 
Our approach, which we call Latent-Descriptor 
Clustering, or LDC, is an iterative algorithm, in 
the spirit of the K-means clustering algorithm 
and of the EM algorithm for the estimation of a 
mixture of Gaussians. 
In conventional K-means clustering, one is 
given a collection of N objects described as N 
data points in an r-dimensional Euclidean space, 
and one seeks a clustering that minimizes the 
sum of intra-cluster squared distances, i.e., the 
sum, over all data points, of the squared distance 
between that point and the centroid of its as-
signed cluster. In LDC, we similarly state our 
goal as one of finding a tagging, i.e., cluster as-
signment, A, that minimizes the sum of intra-
cluster squared distances. However, unlike in 
conventional K-means, the N objects to be clus-
tered are themselves described by vectors?in a 
suitable manifold?that depend on the clustering 
A. We call these vectors latent descriptors. 
Specifically, each object to be clustered, i.e., 
each word type w, is described in terms of its 
left-tag context and right-tag context. These con-
text vectors are the counts of the K different tags 
occurring, under tagging A, to the left and right 
of tokens of word type w in the corpus. We nor-
malize each of these context vectors to unit 
length, producing, for each word type w, two 
points LA(w) and RA(w) on the (K?1)-
dimensional unit sphere. The latent descriptor 
for w consists of the pair (LA(w), RA(w))?more 
details in Section 2. 
A straightforward approach to this latent-
descriptor K-means problem is to adapt the clas-
sical iterative K-means algorithm so as to handle 
the latent descriptors. Specifically, in each itera-
tion, given the assignment A obtained from the 
previous iteration, one first computes the latent 
descriptors for all word types as defined above, 
and then proceeds in the usual way to update 
cluster centroids and to find a new assignment A 
to be used in the next iteration. 
For reasons to be discussed in Section 5, we 
instead prefer a soft-assignment strategy, in-
spired from the EM algorithm for the estimation 
of a mixture of Gaussians. Thus, rather than the 
hard assignment A, we use a soft-assignment 
matrix P. Pwk, interpreted as the probability of 
assigning word w to cluster k, is, essentially, 
proportional to exp{? dwk2/2?2}, where dwk is the 
distance between the latent descriptor for w and 
the centroid, i.e., Gaussian mean, for k. Unlike 
the Gaussian-mixture model however, we use 
the same mixture coefficient and the same Gaus-
sian width for all k. Further, we let the Gaussian 
width ??decrease gradually during the iterative 
process. As is well-known, the EM algorithm for 
Gaussian mixtures reduces in the limit of small ? 
to the simpler K-means clustering algorithm. As 
a result, the last few iterations of LDC effec-
tively implement the hard-assignment K-means-
style algorithm outlined in the previous para-
graph. The soft assignment used earlier in the 
process lends robustness to the algorithm. 
 
The LDC approach is shown to yield substantial 
improvement over state-of-the-art methods for 
the problem of fully unsupervised, distributional 
only, POS tagging. The algorithm is conceptu-
ally simple and easy to implement, requiring less 
than 30 lines of Matlab code. It runs in a few 
seconds of computation time, as opposed to 
hours or days for the training of HMMs. 
2 Notations and Statement of Problem 
The LDC algorithm is best understood in the 
context of the latent-descriptor K-means optimi-
zation problem. In this section, we set up our 
notations and define this problem in detail. For 
simplicity, induced tags are henceforth referred 
to as labels, while tags will be reserved for the 
gold-standard tags, to be used later for evalua-
tion. 
Let W denote the set of word types w1,?,wN, 
and let T denote the set of labels, i.e., induced 
800
tags. The sizes of these sets are |W| = N and |T| = 
K. In the experiments presented in Section 4, N 
is 43,766 and K is either 50 or 17. For any word 
token t in the corpus, we denote the word type of 
t by w(t). The frequency of word type w in the 
corpus is denoted f(w); thus, ?w f(w) = 1. 
For a word type w1, the left-word context of 
w1, L(w1), is defined as the N-dimensional vector 
whose n-th component is the number of bigrams, 
i.e., pairs of consecutive tokens (ti?1, ti) in the 
corpus, such that w(ti) = w1 and w(ti?1) = n. Simi-
larly, we define the right-word context of w1, 
R(w1), as the N-dimensional vector whose n-th 
component is the number of bigrams (ti, ti+1) 
such that w(ti) = w1 and w(ti+1) = n. We let L 
(resp. R) be the N?N matrix whose w-th row is 
L(w) (resp. R(w)). 
 
SK?1 is the unit sphere in the K-dimensional 
Euclidean space ?K. For any x??K, we denote 
by ?(x) the projection of x on SK?1, i.e., ?(x) = 
x/||x||. 
 
A labeling is a map A: W ? T. Given a labeling 
A, we define )(~ 1wLA , the left-label context of 
word type w1, as the K-dimensional vector 
whose k-th component is the number of bigrams 
(ti?1, ti) in the corpus such that w(ti) = w1 and 
A(w(ti?1)) = k. We define the left descriptor of 
word type w as: 
 
))(~()( wLwL AA ?? . 
 
We similarly define the right-label context of w1, 
)(~ 1wRA , as the K-dimensional vector whose k-
th component is the number of bigrams (ti, ti+1) 
such that w(ti) = w1 and A(w(ti+1)) = k, and we 
define the right descriptor of word type w as: 
 
))(~()( wRwR AA ?? . 
 
In short, any labeling A defines two maps, LA 
and RA, each from W to SK?1. 
 
For any function g(w) defined on W, ?g(w)? will 
be used to denote the average of g(w) weighted 
by the frequency of word type w in the corpus: 
?g(w)???? ?w f(w)g(w). 
For any label k, we define: 
 
CL(k) = ?(? LA(w): A(w) = k ?). 
 
Thus, CL(k) is the projection on SK?1 of the 
weighted average of the left descriptors of the 
word types labeled k. We sometimes refer to 
CL(k) as the left centroid of cluster k. Note that 
CL(k) depends on A in two ways, first in that the 
average is taken on words w such that A(w) = k, 
and second through the global dependency of LA 
on A. We similarly define the right centroids: 
 
CR(k)= ?(??RA(w): A(w) = k ?). 
 
Informally, we seek a labeling A such that, for 
any two word types w1 and w2 in W, w1 and w2 
are labeled the same if and only if LA(w1) and 
LA(w2) are close to each other on SK?1 and so are 
RA(w1) and RA(w2). Formally, our goal is to find 
a labeling A that minimizes the objective func-
tion: 
 
F(A)=?||LA(w)?CL(A(w))||2+||RA(w)?CR(A(w))||2?. 
 
Note that, just as in conventional K-means clus-
tering, F(A) is the sum of the intra-cluster 
squared distances. However, unlike conventional 
K-means clustering, the descriptors of the ob-
jects to be clustered depend themselves on the 
clustering. We accordingly refer to LA and RA as 
latent descriptors, and to the method described 
in the next section as Latent-Descriptor Clus-
tering, or LDC.  
 
Note, finally, that we do not seek the global 
minimum of F(A). This global minimum, 0, is 
obtained by the trivial assignment that maps all 
word types to a unique label. Instead, we seek a 
minimum under the constraint that the labeling 
be non-trivial. As we shall see, this constraint 
need not be imposed explicitly: the iterative 
LDC algorithm, when suitably initialized and 
parameterized, converges to non-trivial local 
minima of F(A)?and these are shown to pro-
vide excellent taggers.  
3 Methods 
Recall that a mixture of Gaussians is a genera-
tive model for a random variable taking values 
801
in a Euclidean space ?r. With K Gaussians, the 
model is parameterized by: 
 
? K mixture parameters, i.e., K non-
negative numbers adding up to 1; 
? K means, i.e., K points ?1,?,?K in ?r; 
? K variance-covariance d?d matrices. 
 
The collection of all parameters defining the 
model is denoted by ?. EM is an iterative algo-
rithm used to find a (local) maximizer of the 
likelihood of N observed data points x1,?,xN ? 
?r. Each iteration of the algorithm includes an E 
phase and an M phase. The E phase consists of 
computing, based on the current ?, a probabilis-
tic assignment of each of the N observations to 
the K Gaussian distributions. These probabilistic 
assignments form an N?K stochastic matrix P, 
i.e., a matrix of non-negative numbers in which 
each row sums to 1. The M phase consists of 
updating the model parameters ?, based on the 
current assignments P. For more details, see, 
e.g., Bishop (2006). 
 
The structure of the LDC algorithm is very simi-
lar to that of the EM algorithm. Thus, each itera-
tion of LDC consists of an E phase and an M 
phase. As observations are replaced by latent 
descriptors, an iteration of LDC is best viewed 
as starting with the M phase. The M phase first 
starts by building a pair of latent-descriptor ma-
trices LP and RP, from the soft assignments ob-
tained in the previous iteration. Note that these 
descriptors are now indexed by P, the matrix of 
probabilistic assignments, rather than by hard 
assignments A as in the previous section. 
 
LP and RP are obtained by a straightforward ad-
aptation of the definition given in the previous 
section to the case of probabilistic assignments. 
Thus, the latent descriptors consist of the left-
word and right-word contexts (recall that these 
are given by matrices L and R), mapped into 
left-label and right-label contexts through multi-
plication by the assignment matrix P, and scaled 
to unit length: 
 
LP = ?(LP) 
RP = ?(RP). 
 
With these latent descriptors in hand, we pro-
ceed with the M phase of the algorithm as usual. 
Thus, the left mean ?Lk for Gaussian k is the 
weighted average of the left latent descriptors 
LP(w), scaled to unit length. The weight used in 
this weighted average is Pwk?f(w) (remember 
that f(w) is the frequency of word type w in the 
corpus). Note that the definition of the Gaussian 
mean ?Lk parallels the definition of the cluster 
centroid CL(k) given in the previous section; if 
the assignment P happens to be a hard assign-
ment, ?Lk is actually identical to CL(k). The right 
Gaussian mean ?Rk is computed in a similar 
fashion. As mentioned, we do not estimate any 
mixture coefficients or variance-covariance ma-
trices. 
 
The E phase of the iteration takes the latent de-
scriptors and the Gaussian means, and computes 
a new N?K matrix of probabilistic assignments 
P. These new assignments are given by: 
 
}2/]||)(||||)([||exp{1 222 ??? RkPLkPwk wRwLZP ?????
 
with Z a normalization constant such that 
?k Pwk = 1. ? is a parameter of the model, which, 
as mentioned, is gradually decreased to enforce 
convergence of P to a hard assignment. 
 
The description of the M phase given above 
does not apply to the first iteration, since the M 
phase uses P from the previous iteration. To ini-
tialize the algorithm, i.e., create a set of left and 
right descriptor vectors in the M phase of the 
first iteration, we use the left-word and right-
word contexts L and R. These matrices however 
are of very high dimension (N?N), and thus 
sparse and noisy. We therefore reduce their di-
mensionality, using reduced-rank singular-value 
decomposition. This yields two N?r1 matrices, 
L1 and R1. A natural choice for r1 is r1 = K, and 
this was indeed used for K = 17. For K = 50, we 
also use r1 = 17. The left and right descriptors 
for the first iteration are obtained by scaling 
each row of matrices L1 and R1 to unit length. 
The Gaussian centers ?Lk and ?Rk, k = 1,?,K, are 
set equal to the left and right descriptors of the K 
802
most frequent words in the corpus. This com-
pletes the description of the LDC algorithm.1 
 
While this algorithm is intuitive and simple, it 
does not easily lend itself to mathematical 
analysis; indeed there is no a priori guarantee 
that it will behave as desired. Even for the sim-
pler, hard-assignment, K-means-style version of 
LDC outlined in the previous section, there is no 
equivalent to the statement?valid for the con-
ventional K-means algorithm?that each itera-
tion lowers the intra-cluster sum of squared dis-
tances F(A); this is a mere consequence of the 
fact that the descriptors themselves are updated 
on each iteration. The soft-assignment version of 
LDC does not directly attempt to minimize F(A), 
nor can it be viewed as likelihood maximiza-
tion?as is EM for a Gaussian mixture?since 
the use of latent descriptors precludes the defini-
tion of a generative model for the data. This 
theoretical difficulty is compounded by the use 
of a variable ?. 
 
Empirically however, as shown in the next sec-
tion, we find that the LDC algorithm is very well 
behaved. Two simple tools will be used to aid in 
the description of the behavior of LDC. 
The first tool is an objective function G(P) 
that parallels the definition of F(A) for hard as-
signments. For a probabilistic assignment P, we 
define G(P) to be the weighted average, over all 
w and all k, of ||LP(w) ? ?Lk||2 + ||RP(w) ? ?Rk||2; 
the weight used in this average is Pwk?f(w), just 
as in the computation of the Gaussian means. 
Clearly, G is identical to F on any P that hap-
pens to be a hard assignment. Thus, G is actually 
an extension of the objective function F to soft 
assignments. 
The second tool will allow us to compute a 
tagging accuracy for soft assignments. For this 
purpose, we simply create, for any probabilistic 
assignment P, the obvious labeling A = A*(P) 
that maps w to k with highest Pwk. 
4 Results 
In order to evaluate the performance of LDC, we 
apply it to the Wall Street Journal portion of the 
                                                          
                                                          
1 The LDC code, including tagging accuracy evaluation, is 
available at http://www.dam.brown.edu/people/elie/code/. 
Penn Treebank corpus (1,173,766 tokens, all 
lower-case, resulting in N = 43,766 word types). 
We compare the induced labels with two gold-
standard tagsets: 
 
? PTB45, the standard 45-tag PTB tagset. 
When using PTB45 as the gold standard, 
models induce 50 labels, to allow com-
parison with Gao and Johnson (2008) 
and Lamar et al (2010). 
 
? PTB17, the PTB tagset coarse-grained 
to 17 tags (Smith and Eisner 2005). 
When using PTB17 as the gold standard, 
models induce 17 labels. 
 
In order to compare the labels generated by the 
unsupervised model with the tags of each tagset, 
we use two map-based criteria: 
 
? MTO: many-to-one tagging accuracy, 
i.e., fraction of correctly-tagged tokens 
in the corpus under the so-called many-
to-one mapping, which takes each in-
duced tag to the gold-standard POS tag 
with which it co-occurs most frequently. 
This is the most prevalent metric in use 
for unsupervised POS tagging, and we 
find it the most reliable of all criteria 
currently in use. Accordingly, the study 
presented here emphasizes the use of 
MTO. 
 
? OTO: best tagging accuracy achievable 
under a so-called one-to-one mapping, 
i.e., a mapping such that at most one in-
duced tag is sent to any POS tag. The 
optimal one-to-one mapping is found 
through the Hungarian algorithm2. 
2 Code by Markus Beuhren is available at 
http://www.mathworks.com/matlabcentral/fileexchange/65
43-functions-for-the-rectangular-assignment-problem 
803
 Figures 1 and 2 show the behavior of the LDC 
algorithm for K = 17 and K = 50 respectively. 
From the G curves as well as from the MTO 
scoring curves (using the labeling A*(P) defined 
at the end of Section 3), it is clear that the algo-
rithm converges. The figures show only the first 
15 iterations, as little change is observed after 
that. The schedule of the ? parameter was given 
the simple form ?(t) = ?1exp{?c(t?1)}, t = 
1,2,?, and the parameters ?1 and c were ad-
justed so as to get the best MTO accuracy. With 
the ?-schedules used in these experiments, P 
typically converges to a hard assignment in 
about 45 iterations, ? being then 10?5. 
Figure 1: Convergence of LDC with K = 17. Bottom 
curve: ? -schedule, i.e., sequence of Gaussian widths 
employed. Middle curve: Objective function G(P) 
(see Section 3). Top curve: Many-to-one tagging 
accuracy of labeling A*(P), evaluated against 
PTB17. 
While the objective function G(P) mostly de-
creases, it does show a hump for K = 50 around 
iteration 9. This may be due to the use of latent 
descriptors, or of a variable ?, or both. The 
MTO score sometimes decreases by a small 
fraction of a percent, after having reached its 
peak around the 15th iteration. 
Note that we start ? at 0.4 for K = 17, and at 
0.5 for K = 50. Although we chose two slightly 
different ? schedules for the two tagsets in order 
to achieve optimal performance on each tagset, 
an identical sequence of ? can be used for both 
with only a 1% drop in PTB17 score. 
Figure 2: Same as Figure 1 but with K = 50. Top curve 
shows the MTO accuracy of the labeling evaluated 
against PTB45. 
 
As the width of the Gaussians narrows, each 
vector is steadily pushed toward a single choice 
of cluster. This forced choice, in turn, produces 
more coherent descriptor vectors for all word 
types, and yields a steady increase in tagging 
accuracy. 
 
804
Table 1 compares the tagging accuracy of LDC 
with several recent models of Gao and Johnson 
(2008) and Lamar et al (2010). 
The LDC results shown in the top half of the 
table, which uses the MTO criterion, were ob-
tained with the same ?-schedules as used in Fig-
ures 1 and 2. Note that the LDC algorithm is 
deterministic. However, the randomness in the 
sparse-matrix implementation of reduced-rank 
SVD used in the initialization step causes a 
small variability in performance (the standard 
deviation of the MTO score is 0.0004 for PTB17 
and 0.003 for PTB45). The LDC results reported 
are averages over 20 runs. Each run was halted 
at iteration 15, and the score reported uses the 
labeling A*(P) defined at the end of Section 3. 
The LDC results shown in the bottom half of 
the table, which uses the OTO criterion, were 
obtained with a variant of the LDC algorithm, in 
which the M phase estimates not only the Gaus-
sian means but also the mixture coefficients. 
Also, different ?-schedules were used,3 
For both PTB17 and PTB45, and under both 
criteria, LDC's performance nearly matches or 
exceeds (often by a large margin) the results 
achieved by the other models.  We find the large 
                                                          
3 All details are included in the code available at 
http://www.dam.brown.edu/people/elie/code/. 
increase achieved by LDC in the MTO perform-
ance under the PTB45 tagset particularly com-
pelling. It should be noted that Abend et al 
(2010) report 71.6% MTO accuracy for PTB45, 
but they treat all punctuation tags differently in 
their evaluation and therefore these results can-
not be directly compared. Berg-Kirkpatrick et al 
(2010) report 75.5% MTO accuracy for PTB45 
by incorporating other features such as mor-
phology; Table 1 is limited to distributional-only 
methods. 
 
 Criterion  Model  PTB17 PTB45 
MTO LDC  0.751 0.708 
  SVD2 0.740 0.658 
  HMM-EM 0.647 0.621 
  HMM-VB 0.637 0.605 
  HMM-GS 0.674 0.660 
OTO LDC 0.593 0.483 
 SVD2 0.541 0.473 
  HMM-EM 0.431 0.405 
  HMM-VB 0.514 0.461 
  HMM-GS 0.466 0.499 
Table 1. Tagging accuracy comparison between 
several models for two tagsets and two mapping 
criteria.  Note that LDC significantly outperforms 
all HMMs (Gao and Johnson, 2008) in every case 
except PTB45 under the OTO mapping.  LDC also 
outperforms SVD2 (Lamar et al, 2010) in all 
cases. 
 
 
Figure 3:  Mislabeled words per tag, using the 
PTB17 tagset. Black bars indicate mislabeled words 
when 17 clusters are used.  Gray bars indicate words 
that continue to be mislabeled even when every word 
type is free to choose its own label, as if each type 
were in its own cluster?which defines the theoreti-
cally best possible non-disambiguating model. Top: 
fraction of the corpus mislabeled, broken down by 
gold tags. Bottom: fraction of tokens of each tag that 
are mislabeled.  Many of the infrequent tags are 
100% mislabeled because no induced label is 
mapped to these tags under MTO.  
Figure 3 demonstrates the mistakes made by 
LDC under the MTO mapping.  From the top 
graph, it is clear that the majority of the missed 
tokens are open-class words ? most notably ad-
jectives and adverbs.  Over 8% of the tokens in 
the corpus are mislabeled adjectives ? roughly 
one-third of all total mislabeled tokens (25.8%).  
Furthermore, the corresponding bar in the bot-
tom graph indicates that over half of the adjec-
tives are labeled incorrectly.  Similarly, nearly 
4% of the mislabeled tokens are adverbs, but 
every adverb in the corpus is mislabeled because 
no label is mapped to this tag ? a common oc-
805
currence under MTO, shared by seven of the 
seventeen tags. 
 Figure 4: The confusion matrix for LDC's labeling under PTB17.  The area of a black square indicates the number 
of tokens in each element of the confusion matrix.  The diamonds indicate the induced tag under the MTO map-
ping.  Several labels are mapped to N (Noun), and one of these labels causes appreciable confusion between nouns 
and adjectives.  Because multiple labels are dedicated to a single tag (N, V and PREP), several tags (in this case 7) 
are left with no label. 
 
To further illuminate the errors made by LDC, 
we construct the confusion matrix (figure 4).  
Element (i,j) of this matrix stores the fraction of 
all tokens of POS tag i that are given label j by 
the model.  In a perfect labeling, exactly one 
element of each row and each column would be 
non-zero.  As illustrated in figure 4, the confu-
sion matrices produced by LDC are far from 
perfect.  LDC consistently splits the Nouns into 
several labels and often confuses Nouns and Ad-
jectives under a single label.  These types of 
mistakes have been observed as well in models 
that use supervision (Haghighi and Klein, 2006).  
 
 
 
5 Discussion 
When devising a model for unsupervised POS 
induction, one challenge is to choose a model of 
adequate complexity, this choice being related to 
the bias-variance dilemma ubiquitous in statisti-
cal estimation problems. While large datasets are 
available, they are typically not large enough to 
allow efficient unsupervised learnability in mod-
els that are powerful enough to capture complex 
features of natural languages. Ambiguity is one 
of these features. Here we propose a new ap-
proach to this set of issues: start with a model 
that explicitly entertains ambiguity, and gradu-
ally constrain it so that it eventually converges 
to an unambiguous tagger.  
Thus, although the algorithm uses probabilis-
tic assignments, of Gaussian-mixture type, the 
goal is the construction of hard assignments. By 
806
requiring the Gaussians to be isotropic with uni-
form width and by allowing that width to shrink 
to zero, the algorithm forces the soft assign-
ments to converge to a set of hard assignments. 
Based on its performance, this simulated-
annealing-like approach appears to provide a 
good compromise in the choice of model com-
plexity. 
 
LDC bears some similarities with the algorithm 
of Ney, Essen and Kneser (1994), further im-
plemented, with extensions, by Clark (2003). 
Both models use an iterative approach to mini-
mize an objective function, and both initialize 
with frequent words. However, the model of 
Ney et al is, in essence, an HMM where each 
word type is constrained to belong to a single 
class (i.e., in HMM terminology, be emitted by a 
single hidden state). Accordingly, the objective 
function is the data likelihood under this con-
strained HMM. This takes into account only the 
rightward transition probabilities. Our approach 
is conceptually rather different from an HMM. It 
is more similar to the approach of Sch?tze 
(1995) and Lamar et al (2010), where each 
word type is mapped into a descriptor vector 
derived from its left and right tag contexts. Ac-
cordingly, the objective function is that of the K-
means clustering problem, namely a sum of in-
tra-cluster squared distances. This objective 
function, unlike the likelihood under an HMM, 
takes into account both left and right contexts. It 
also makes use in a crucial way of cluster cen-
troids (or Gaussian means), a notion that has no 
counterpart in the HMM approach. We note that 
LDC achieves much better results (by about 
10%) than a recent implementation of the Ney et 
al. approach (Reichart et al 2010). 
 
The only parameters in LDC are the two pa-
rameters used to define the ? schedule, and r1 
used in the first iteration. Performance was gen-
erally found to degrade gracefully with changes 
in these parameters away from their optimal val-
ues. When ? was made too large in the first few 
iterations, it was found that the algorithm con-
verges to the trivial minimum of the objective 
function F(A), which maps all word types to a 
unique label (see section 2). An alternative 
would be to estimate the variance for each Gaus-
sian separately, as is usually done in EM for 
Gaussian mixtures. This would not necessarily 
preclude the use of an iteration-dependent scal-
ing factor, which would achieve the goal of 
gradually forcing the tagging to become deter-
ministic. Investigating this and related options is 
left for future work. 
 
Reduced-rank SVD is used in the initialization 
of the descriptor vectors, for the optimization to 
get off the ground. The details of this initializa-
tion step do not seem to be too critical, as wit-
nessed by robustness against many parameter 
changes. For instance, using only the 400 most 
frequent words in the corpus?instead of all 
words?in the construction of the left-word and 
right-word context vectors in iteration 1 causes 
no appreciable change in performance. 
 
The probabilistic-assignment algorithm was 
found to be much more robust against parameter 
changes than the hard-assignment version of 
LDC, which parallels the classical K-means 
clustering algorithm (see Section 1). We ex-
perimented with this hard-assignment latent-
descriptor clustering algorithm (data not shown), 
and found that a number of additional devices 
were necessary in order to make it work prop-
erly. In particular, we found it necessary to use 
reduced-rank SVD on each iteration of the algo-
rithm?as opposed to just the first iteration in 
the version presented here?and to gradually 
increase the rank r. Further, we found it neces-
sary to include only the most frequent words at 
the beginning, and only gradually incorporate 
rare words in the algorithm. Both of these de-
vices require fine tuning. Provided they are in-
deed appropriately tuned, the same level of per-
formance as in the probabilistic-assignment ver-
sion could be achieved. However, as mentioned, 
the behavior is much less robust with hard clus-
tering. 
 
Central to the success of LDC is the dynamic 
interplay between the progressively harder clus-
ter assignments and the updated latent descriptor 
vectors.  We operate under the assumption that if 
all word types were labeled optimally, words 
that share a label should have similar descriptor 
vectors arising from this optimal labeling.  
These similar vectors would continue to be clus-
tered together, producing a stable equilibrium in 
807
the dynamic process.  The LDC algorithm dem-
onstrates that, despite starting far from this op-
timal labeling, the alternation between vector 
updates and assignment updates is able to pro-
duce steadily improving clusters, as seen by the 
steady increase of tagging accuracy.   
 
We envision the possibility of extending this 
approach in several ways.  It is a relatively sim-
ple matter to extend the descriptor vectors to 
include context outside the nearest neighbors, 
which may well improve performance. In view 
of the computational efficiency of LDC, which 
runs in under one minute on a desktop PC, the 
added computational burden of working with the 
extended context is not likely to be prohibitive.   
LDC could also be extended to include morpho-
logical or other features, rather than relying ex-
clusively on context.  Again, we would antici-
pate a corresponding increase in accuracy from 
this additional linguistic information.  
 
 
 
 References 
  
Omri Abend, Roi Reichart and Ari Rappoport. Im-
proved Unsupervised POS Induction through Pro-
totype Discovery. 2010. In Proceedings of the 
48th Annual Meeting of the ACL. 
Christopher M. Bishop. 2006. Pattern Recognition 
and Machine Learning. Springer-Verlag, New 
York, LLC. 
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?, 
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In proceedings 
of NAACL 2010. 
Alexander Clark. 2001. The unsupervised induction 
of stochastic context-free grammars using distribu-
tional clustering. In CoNLL. 
Alexander Clark. 2003. Combining distributional and 
morphological information for part of speech in-
duction. In 10th Conference of the European 
Chapter of the Association for Computational Lin-
guistics, pages 59?66.  
Jianfeng Gao and Mark Johnson. 2008. A comparison 
of bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proceedings of the 
2008 Conference on Empirical Methods in Natural 
Language Processing, pages 344?352. 
Sharon Goldwater and Tom Griffiths. 2007. A fully 
Bayesian approach to unsupervised part-of-speech 
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics, pages 744?751. 
Jo?o V. Gra?a, Kuzman Ganchev, Ben Taskar, and 
Fernando Pereira. 2009. Posterior vs. Parameter 
Sparsity in Latent Variable Models. Neural Infor-
mation Processing Systems Conference (NIPS).  
Michael Lamar, Yariv Maron, Mark Johnson, Elie 
Bienenstock. 2010. SVD and Clustering for Unsu-
pervised POS Tagging. In Proceedings of the 48th 
Annual Meeting of the ACL. 
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings of the Human Language Technology Confer-
ence of the NAACL, Main Conference, pages 320?
327, New York City, USA, June. Association for 
Computational Linguistics. 
William P. Headden, David McClosky, and Eugene 
Charniak. 2008. Evaluating unsupervised part-of-
speech tagging for grammar induction. In Pro-
ceedings of the International Conference on Com-
putational Linguistics (COLING ?08). 
Mark Johnson. 2007. Why doesn?t EM find good 
HMM POS-taggers? In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL), pages 
296?305. 
Hermann Ney, Ute Essen, and Reinhard Kneser. 
1994. On structuring probabilistic dependences in 
stochastic language modelling. Computer Speech 
and Language, 8, 1-38. 
Roi Reichart, Raanan Fattal and Ari Rappoport. 
2010. Improved Unsupervised POS Induction Us-
ing Intrinsic Clustering Quality and a Zipfian Con-
straint. CoNLL. 
Sujith Ravi and Kevin Knight. 2009. Minimized 
models for unsupervised part-of-speech tagging. In 
Proceedings of the 47th Annual Meeting of the 
ACL and the 4th IJCNLP of the AFNLP, pages 
504?512. 
Hinrich Sch?tze. 1995. Distributional part-of-speech 
tagging. In Proceedings of the seventh conference 
on European chapter of the Association for Com-
putational Linguistics, pages 141?148. 
Noah A. Smith and Jason Eisner. 2005. Contrastive 
estimation: Training log-linear models on unla-
beled data. In Proceedings of the 43rd Annual 
808
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 354?362. 
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proceedings of HLT-NAACL 2003, pages 
252-259. 
Yoshimasa Tsuruoka and Jun'ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy 
for Tagging Sequence Data. In Proceedings of 
HLT/EMNLP, pp. 467-474. 
 
809
Proceedings of the ACL 2010 Conference Short Papers, pages 215?219,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SVD and Clustering for Unsupervised POS Tagging 
 
Michael Lamar* 
Division of Applied Mathematics 
Brown University 
Providence, RI, USA 
mlamar@dam.brown.edu 
 
Yariv Maron* 
Gonda Brain Research Center 
Bar-Ilan University 
Ramat-Gan, Israel 
syarivm@yahoo.com 
Mark Johnson 
Department of Computing 
Faculty of Science 
Macquarie University 
Sydney, Australia 
mjohnson@science.mq.edu.au 
Elie Bienenstock 
Division of Applied Mathematics 
and Department of Neuroscience 
Brown University 
Providence, RI, USA 
elie@brown.edu 
  
Abstract 
We revisit the algorithm of Sch?tze 
(1995) for unsupervised part-of-speech 
tagging. The algorithm uses reduced-rank 
singular value decomposition followed 
by clustering to extract latent features 
from context distributions. As imple-
mented here, it achieves state-of-the-art 
tagging accuracy at considerably less cost 
than more recent methods. It can also 
produce a range of finer-grained tag-
gings, with potential applications to vari-
ous tasks. 
1 Introduction 
While supervised approaches are able to solve 
the part-of-speech (POS) tagging problem with 
over 97% accuracy (Collins 2002; Toutanova et 
al. 2003), unsupervised algorithms perform con-
siderably less well. These models attempt to tag 
text without resources such as an annotated cor-
pus, a dictionary, etc. The use of singular value 
decomposition (SVD) for this problem was in-
troduced in Sch?tze (1995). Subsequently, a 
number of methods for POS tagging without a 
dictionary were examined, e.g., by Clark (2000), 
Clark (2003), Haghighi and Klein (2006), John-
son (2007), Goldwater and Griffiths (2007), Gao 
and Johnson (2008), and Gra?a et al (2009).  
The latter two, using Hidden Markov Models 
(HMMs), exhibit the highest performances to 
date for fully unsupervised POS tagging.   
The revisited SVD-based approach presented 
here, which we call ?two-step SVD? or SVD2, 
has four important characteristics. First, it 
achieves state-of-the-art tagging accuracy. 
Second, it requires drastically less computational 
effort than the best currently available models. 
Third, it demonstrates that state-of-the-art accu-
racy can be realized without disambiguation, i.e., 
without attempting to assign different tags to dif-
ferent tokens of the same type. Finally, with no 
significant increase in computational cost, SVD2 
can create much finer-grained labelings than typ-
ically produced by other algorithms. When com-
bined with some minimal supervision in post-
processing, this makes the approach useful for 
tagging languages that lack the resources re-
quired by fully supervised models. 
2 Methods 
Following the original work of Sch?tze (1995), 
we begin by constructing a right context matrix, 
R, and a left context matrix, L.  Rij counts the 
number of times in the corpus a token of word 
type i is immediately followed by a token of 
word type j. Similarly, Lij counts the number of 
times a token of type i is preceded by a token of 
type j. We truncate these matrices, including, in 
the right and left contexts, only the w1 most fre-
quent word types. The resulting L and R are of 
dimension Ntypes?w1, where Ntypes is the number 
of word types (spelling forms) in the corpus, and 
w1 is set to 1000. (The full Ntypes? Ntypes context 
matrices satisfy R = LT.) 
* These authors contributed equally. 
215
Next, both context matrices are factored using 
singular value decomposition: 
L = UL SL VL
T 
R = UR SR VR
T. 
The diagonal matrices SL and SR (each of rank 
1000) are reduced down to rank r1 = 100 by re-
placing the 900 smallest singular values in each 
matrix with zeros, yielding SL
* and SR
*.  We then 
form a pair of latent-descriptor matrices defined 
by:   
L* = UL SL
* 
R* = UR SR
*. 
Row i in matrix L* (resp. R*) is the left (resp. 
right) latent descriptor for word type i. We next 
include a normalization step in which each row 
in each of L* and R* is scaled to unit length, 
yielding matrices L** and R**. Finally, we form a 
single descriptor matrix D by concatenating these 
matrices into D = [L** R**].  Row i in matrix D is 
the complete latent descriptor for word type i; 
this latent descriptor sits on the Cartesian product 
of two 100-dimensional unit spheres, hereafter 
the 2-sphere. 
We next categorize these descriptors into 
k1 = 500 groups, using a k-means clustering algo-
rithm. Centroid initialization is done by placing 
the k initial centroids on the descriptors of the k 
most frequent words in the corpus. As the de-
scriptors sit on the 2-sphere, we measure the 
proximity of a descriptor to a centroid by the dot 
product between them; this is equal to the sum of 
the cosines of the angles?computed on the left 
and right parts?between them. We update each 
cluster?s centroid as the weighted average of its 
constituents, the weight being the frequency of 
the word type; the centroids are then scaled, so 
they sit on the 2-sphere. Typically, only a few 
dozen iterations are required for full convergence 
of the clustering algorithm. 
We then apply a second pass of this entire 
SVD-and-clustering procedure. In this second 
pass, we use the k1 = 500 clusters from the first 
iteration to assemble a new pair of context ma-
trices. Now, Rij counts all the cluster-j (j=1? k1) 
words to the right of word i, and Lij counts all the 
cluster-j words to the left of word i. The new ma-
trices L and R have dimension Ntypes ? k1. 
As in the first pass, we perform reduced-rank 
SVD, this time down to rank r2 = 300, and we 
again normalize the descriptors to unit length, 
yielding a new pair of latent descriptor matrices 
L** and R**.  Finally, we concatenate L** and R** 
into a single matrix of descriptors, and cluster 
these descriptors into k2 groups, where k2 is the 
desired number of induced tags. We use the same 
weighted k-means algorithm as in the first pass, 
again placing the k initial centroids on the de-
scriptors of the k most frequent words in the cor-
pus. The final tag of any token in the corpus is 
the cluster number of its type. 
3 Data and Evaluation 
We ran the SVD2 algorithm described above on 
the full Wall Street Journal part of the Penn 
Treebank (1,173,766 tokens). Capitalization was 
ignored, resulting in Ntypes = 43,766, with only a 
minor effect on accuracy. Evaluation was done 
against the POS-tag annotations of the 45-tag 
PTB tagset (hereafter PTB45), and against the 
Smith and Eisner (2005) coarse version of the 
PTB tagset (hereafter PTB17). We selected the 
three evaluation criteria of Gao and Johnson 
(2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to-
1 are the tagging accuracies under the best many-
to-one map and the greedy one-to-one map re-
spectively; VI is a map-free information-
theoretic criterion?see Gao and Johnson (2008) 
for details. Although we find M-to-1 to be the 
most reliable criterion of the three, we include 
the other two criteria for completeness. 
In addition to the best M-to-1 map, we also 
employ here, for large values of k2, a prototype-
based M-to-1 map.  To construct this map, we 
first find, for each induced tag t, the word type 
with which it co-occurs most frequently; we call 
this word type the prototype of t. We then query 
the annotated data for the most common gold tag 
for each prototype, and we map induced tag t to 
this gold tag. This prototype-based M-to-1 map 
produces accuracy scores no greater?typically 
lower?than the best M-to-1 map. We discuss 
the value of this approach as a minimally-
supervised post-processing step in Section 5. 
4 Results 
Low-k performance. Here we present the per-
formance of the SVD2 model when k2, the num-
ber of induced tags, is the same or roughly the 
same as the number of tags in the gold stan-
dard?hence small. Table 1 compares the per-
formance of SVD2 to other leading models. Fol-
lowing Gao and Johnson (2008), the number of 
induced tags is 17 for PTB17 evaluation and 50 
for PTB45 evaluation. Thus, with the exception 
of Gra?a et al (2009) who use 45 induced tags 
for PTB45, the number of induced tags is the 
same across each column of Table 1. 
216
The performance of SVD2 compares favora-
bly to the HMM models. Note that SVD2 is a 
deterministic algorithm. The table shows, in pa-
rentheses, the standard deviations reported in 
Gra?a et al (2009). For the sake of comparison 
with Gra?a et al (2009), we also note that, with 
k2 = 45, SVD2 scores 0.659 on PTB45. The NVI 
scores (Reichart and Rappoport 2009) corres-
ponding to the VI scores for SVD2 are 0.938 for 
PTB17 and 0.885 for PTB45. To examine the 
sensitivity of the algorithm to its four parameters, 
w1, r1, k1, and r2, we changed each of these para-
meters separately by a multiplicative factor of 
either 0.5 or 2; in neither case did M-to-1 accura-
cy drop by more than 0.014. 
This performance was achieved despite the 
fact that the SVD2 tagger is mathematically 
much simpler than the other models. Our MAT-
LAB implementation of SVD2 takes only a few 
minutes to run on a desktop computer, in contrast 
to HMM training times of several hours or days 
(Gao and Johnson 2008; Johnson 2007). 
 
High-k performance.  Not suffering from the 
same computational limitations as other models, 
SVD2 can easily accommodate high numbers of 
induced tags, resulting in fine-grained labelings. 
The value of this flexibility is discussed in the 
next section. Figure 1 shows, as a function of k2, 
the tagging accuracy of SVD2 under both the 
best and the prototype-based M-to-1 maps (see 
Section 3), for both the PTB45 and the PTB17 
tagsets. The horizontal one-tag-per-word-type 
line in each panel is the theoretical upper limit 
for tagging accuracy in non-disambiguating 
models (such as SVD2). This limit is the fraction 
of all tokens in the corpus whose gold tag is the 
most frequent for their type.  
5 Discussion 
At the heart of the algorithm presented here is 
the reduced-rank SVD method of Sch?tze 
(1995), which transforms bigram counts into la-
tent descriptors. In view of the present work, 
which achieves state-of-the-art performance 
when evaluation is done with the criteria now in 
common use, Sch?tze's original work should 
rightly be praised as ahead of its time. The SVD2 
model presented here differs from Sch?tze's 
work in many details of implementation?not all 
of which are explicitly specified in Sch?tze 
(1995). In what follows, we discuss the features 
of SVD2 that are most critical to its performance. 
Failure to incorporate any one of them signifi-
Figure 1. Performance of the SVD2 algo-
rithm as a function of the number of induced 
tags. Top: PTB45; bottom: PTB17.  Each 
plot shows the tagging accuracy under the 
best and the prototype-based M-to-1 maps, as 
well as the upper limit for non-
disambiguating taggers. 
 M-to-1 1-to-1 VI 
Model PTB17 PTB45 PTB17 PTB45 PTB17 PTB45 
SVD2 0.730 0.660 0.513 0.467 3.02 3.84 
HMM-EM  0.647 0.621 0.431 0.405 3.86 4.48 
HMM-VB  0.637 0.605 0.514 0.461 3.44 4.28 
HMM-GS  0.674 0.660 0.466 0.499 3.46 4.04 
HMM-Sparse(32) 0.702(2.2) 0.654(1.0) 0.495 0.445   
VEM (10-1,10-1) 0.682(0.8) 0.546(1.7) 0.528 0.460   
Table 1.  Tagging accuracy under the best M-to-1 map, the greedy 1-to-1 map, and 
VI, for the full PTB45 tagset and  the reduced PTB17 tagset.  HMM-EM, HMM-VB 
and HMM-GS show the best results from Gao and Johnson (2008); HMM-Sparse(32) 
and VEM (10-1,10-1) show the best results from Gra?a et al (2009). 
 
217
cantly reduces the performance of the algorithm 
(M-to-1 reduced by 0.04 to 0.08). 
First, the reduced-rank left-singular vectors 
(for the right and left context matrices) are 
scaled, i.e., multiplied, by the singular values.  
While the resulting descriptors, the rows of L* 
and R*, live in a much lower-dimensional space 
than the original context vectors, they are 
mapped by an angle-preserving map (defined by 
the matrices of right-singular vectors VL and VR) 
into vectors in the original space. These mapped 
vectors best approximate (in the least-squares 
sense) the original context vectors; they have the 
same geometric relationships as their equivalent 
high-dimensional images, making them good 
candidates for the role of word-type descriptors. 
A second important feature of the SVD2 algo-
rithm is the unit-length normalization of the la-
tent descriptors, along with the computation of 
cluster centroids as the weighted averages of 
their constituent vectors. Thanks to this com-
bined device, rare words are treated equally to 
frequent words regarding the length of their de-
scriptor vectors, yet contribute less to the place-
ment of centroids. 
Finally, while the usual drawback of k-means-
clustering algorithms is the dependency of the 
outcome on the initial?usually random?
placement of centroids, our initialization of the k 
centroids as the descriptors of the k most fre-
quent word types in the corpus makes the algo-
rithm fully deterministic, and improves its per-
formance substantially: M-to-1 PTB45 by 0.043, 
M-to-1 PTB17 by 0.063. 
As noted in the Results section, SVD2 is fairly 
robust to changes in all four parameters w1, r1, k1, 
and r2. The values used here were obtained by a  
coarse, greedy strategy, where each parameter 
was optimized independently. It is worth noting 
that dispensing with the second pass altogether, 
i.e., clustering directly the latent descriptor vec-
tors obtained in the first pass into the desired 
number of induced tags, results in a drop of 
Many-to-1 score of only 0.021 for the PTB45 
tagset and 0.009 for the PTB17 tagset. 
 
Disambiguation. An obvious limitation of 
SVD2 is that it is a non-disambiguating tagger, 
assigning the same label to all tokens of a type. 
However, this limitation per se is unlikely to be 
the main obstacle to the improvement of low-k 
performance, since, as is well known, the theo-
retical upper limit for the tagging accuracy of 
non-disambiguating models (shown in Fig. 1) is 
much higher than the current state-of-the-art for 
unsupervised taggers, whether disambiguating or 
not. 
To further gain insight into how successful 
current models are at disambiguating when they 
have the power to do so, we examined a collec-
tion of HMM-VB runs (Gao and Johnson 2008) 
and asked how the accuracy scores would change 
if, after training was completed, the model were 
forced to assign the same label to all tokens of 
the same type. To answer this question, we de-
termined, for each word type, the modal HMM 
state, i.e., the state most frequently assigned by 
the HMM to tokens of that type. We then re-
labeled all words with their modal label. The ef-
fect of thus eliminating the disambiguation ca-
pacity of the model was to slightly increase the 
tagging accuracy under the best M-to-1 map for 
every HMM-VB run (the average increase was 
0.026  for PTB17, and 0.015 for PTB45).  We 
view this as a further indication that, in the cur-
rent state of the art and with regards to tagging 
accuracy, limiting oneself to non-disambiguating 
models may not adversely affect performance.  
To the contrary, this limitation may actually 
benefit an approach such as SVD2. Indeed, on 
difficult learning tasks, simpler models often be-
have better than more powerful ones (Geman et 
al. 1992). HMMs are powerful since they can, in 
theory, induce both a system of tags and a system 
of contextual patterns that allow them to disam-
biguate word types in terms of these tags. How-
ever, carrying out both of these unsupervised 
learning tasks at once is problematic in view of 
the very large number of parameters to be esti-
mated compared to the size of the training data 
set. 
The POS-tagging subtask of disambiguation 
may then be construed as a challenge in its own 
right: demonstrate effective disambiguation in an 
unsupervised model. Specifically, show that tag-
ging accuracy decreases when the model's dis-
ambiguation capacity is removed, by re-labeling 
all tokens with their modal label, defined above. 
We believe that the SVD2 algorithm presented 
here could provide a launching pad for an ap-
proach that would successfully address the dis-
ambiguation challenge. It would do so by allow-
ing a gradual and carefully controlled amount of 
ambiguity into an initially non-disambiguating 
model. This is left for future work. 
 
Fine-grained labeling. An important feature of 
the SVD2 algorithm is its ability to produce a 
fine-grained labeling of the data, using a number 
of clusters much larger than the number of tags 
218
in a syntax-motivated POS-tag system. Such 
fine-grained labelings can capture additional lin-
guistic features. To achieve a fine-grained labe-
ling, only the final clustering step in the SVD2 
algorithm needs to be changed; the computation-
al cost this entails is negligible. A high-quality 
fine-grained labeling, such as achieved by the 
SVD2 approach, may be of practical interest as 
an input to various types of unsupervised gram-
mar-induction algorithms (Headden et al 2008). 
This application is left for future work. 
 
Prototype-based tagging. One potentially im-
portant practical application of a high-quality 
fine-grained labeling is its use for languages 
which lack any kind of annotated data. By first 
applying the SVD2 algorithm, word types are 
grouped together into a few hundred clusters. 
Then, a prototype word is automatically ex-
tracted from each cluster. This produces, in a 
completely unsupervised way, a list of only a 
few hundred words that need to be hand-tagged 
by an expert. The results shown in Fig. 1 indicate 
that these prototype tags can then be used to tag 
the entire corpus with only a minor decrease in 
accuracy compared to the best M-to-1 map?the 
construction of which requires a fully annotated 
corpus. Fig. 1 also indicates that, with only a few 
hundred prototypes, the gap left between the ac-
curacy thus achieved and the upper bound for 
non-disambiguating models is fairly small. 
References 
Alexander Clark. 2000. Inducing syntactic categories 
by context distribution clustering. In The Fourth 
Conference on Natural Language Learning. 
Alexander Clark. 2003. Combining distributional and 
morphological information for part of speech in-
duction. In 10th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 59?66.  
Michael Collins. 2002. Discriminative training me-
thods for hidden markov models: Theory and expe-
riments with perceptron algorithms. In Proceedings of 
the ACL-02 conference on Empirical methods in 
natural language processing ? Volume 10. 
Jianfeng Gao and Mark Johnson. 2008. A comparison 
of bayesian estimators for unsupervised Hidden 
Markov Model POS taggers. In Proceedings of the 
2008 Conference on Empirical Methods in Natural 
Language Processing, pages 344?352. 
Stuart Geman, Elie Bienenstock and Ren? Doursat. 
1992. Neural Networks and the Bias/Variance Di-
lemma. Neural Computation,  4 (1), pages 1?58. 
Sharon Goldwater and Tom Griffiths. 2007. A fully 
Bayesian approach to unsupervised part-of-speech 
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguis-
tics, pages 744?751. 
Jo?o V. Gra?a, Kuzman Ganchev, Ben Taskar, and 
Fernando Pereira. 2009. Posterior vs. Parameter 
Sparsity in Latent Variable Models. In Neural In-
formation Processing Systems Conference (NIPS).  
Aria Haghighi and Dan Klein. 2006. Prototype-driven 
learning for sequence models. In Proceedings of 
the Human Language Technology Conference of 
the NAACL, Main Conference, pages 320?327, 
New York City, USA, June. Association for Com-
putational Linguistics. 
William P. Headden, David McClosky, and Eugene 
Charniak. 2008. Evaluating unsupervised part-of-
speech tagging for grammar induction. In Proceed-
ings of the International Conference on Computa-
tional Linguistics (COLING ?08). 
Mark Johnson. 2007. Why doesn?t EM find good 
HMM POS-taggers? In Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL), pages 296?
305. 
Marina Meil?. 2003. Comparing clusterings by the 
variation of information. In Bernhard Sch?lkopf 
and Manfred K. Warmuth, editors, COLT 2003: 
The Sixteenth Annual Conference on Learning 
Theory, volume 2777 of Lecture Notes in Comput-
er Science, pages 173?187. Springer. 
Roi Reichart and Ari Rappoport. 2009. The NVI 
Clustering Evaluation Measure. In Proceedings of 
the Thirteenth Conference on Computational Natu-
ral Language Learning (CoNLL), pages 165?173. 
Hinrich Sch?tze. 1995. Distributional part-of-speech 
tagging. In Proceedings of the seventh conference 
on European chapter of the Association for Com-
putational Linguistics, pages 141?148. 
Noah A. Smith and Jason Eisner. 2005. Contrastive 
estimation: Training log-linear models on unla-
beled data. In Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 354?362. 
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. 
In Proceedings of HLT-NAACL 2003, pages 252-
259. 
219
