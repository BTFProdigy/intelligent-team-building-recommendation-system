Proceedings of NAACL HLT 2007, pages 332?339,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Multi-document Relationship Fusion via
Constraints on Probabilistic Databases
Gideon Mann
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
gideon.mann@gmail.com
Abstract
Previous multi-document relationship ex-
traction and fusion research has focused
on single relationships. Shifting the fo-
cus to multiple relationships allows for the
use of mutual constraints to aid extrac-
tion. This paper presents a fusion method
which uses a probabilistic database model
to pick relationships which violate few
constraints. This model allows improved
performance on constructing corporate
succession timelines from multiple doc-
uments with respect to a multi-document
fusion baseline.
1 Introduction
Single document information extraction of named
entities and relationships has received much atten-
tion since the MUC evaluations1 in the mid-90s (Ap-
pelt et al, 1993; Grishman and Sundheim, 1996).
Recently, there has been increased interest in the
extraction of named entities and relationships from
multiple documents, since the redundancy of infor-
mation across documents has been shown to be a
powerful resource for obtaining high quality infor-
mation even when the extractors have access to little
or no training data (Etzioni et al, 2004; Hasegawa
et al, 2004). Much of the recent work in multi-
document relationship extraction has focused on
the extraction of isolated relationships (Agichtein,
2005; Pasca et al, 2006), but often the goal, as in
1http://www.itl.nist.gov/iaui/894.02/related projects/muc/
single document tasks like MUC, is to extract a tem-
plate or a relational database composed of related
facts.
With databases containing multiple relationships,
the semantics of the database impose constraints
on possible database configurations. This paper
presents a statistical method which picks relation-
ships which violate few constraints as measured by
a probabilistic database model. The constraints are
hard constraints, and robust estimates are achieved
by accounting for the underlying extraction/fusion
uncertainty.
This method is applied to the problem of con-
structing management succession timelines which
have a rich set of semantic constraints. Using con-
straints on probabilistic databases yields F-Measure
improvements of 5 to 18 points on a per-relationship
basis over a state-of-the-art multi-document extrac-
tion/fusion baseline. The constraints proposed in
this paper are used in a context of minimally super-
vised information extractors and present an alterna-
tive to costly manual annotation.
2 Semantic Constraints on Databases
This paper considers management succession
databases where each record has three fields: a
CEO?s name and the start and end years for that
person?s tenure as CEO (Table 1 Column 1). Each
record is represented by three binary logical pred-
icates: ceo(c,x), start(x,y1), end(x,y2), where c is
a company, x is a CEO?s name, and y1 and y2 are
years.2
2All of the relationships in this paper are defined to be bi-
nary relationships. When extracting relationships of higher ar-
332
Explicit Implicit Logical Constraints
Relationships Relationships (a partial list)
ceo(c, x) precedes(x1,x2) ceo(c,x1), precedes(x1,x2) ? ceo(c,x2), end(x1,y), start(x2, y)
start(x, y) inoffice(x, y) start(x,y1), inoffice(x,y2), end(x, y3) ? y1 ? y2 ? y3
end(x, y) predates(x1, x2) inoffice(x1, y1), inoffice(x2, y2) , y1 < y2 ? predates(x1,x2)
precedes(x1,x2) , inoffice(x1,y1), inoffice(x2,y2) ? y1 ? y2
Table 1: Database semantics can provide 1) a method to augment the explicit relationships in the database
with implicit relationships and 2) logical constraints on these explicit and implicit relationships. In the above
table, c is a company, xi is a person, and yi is a time.
In this setting, database semantics allow for the
derivation of other implicit relationships from the
database: the immediate predecessor of a given CEO
(precedes(x1,x2)), all predecessors of a given CEO
(predates(x1,x2)) and all years the CEO was in of-
fice (inoffice(x,y3)), where x2 is a CEO?s name and
t3 a year (Table 1 Column 2).
These implicit relationships and the original ex-
plicit relationships are governed by a series of se-
mantic relations which impose constraints on the
permissible database configurations (Table 1 Col-
umn 3). For example, it will always be true that a
CEO?s start date precedes their end date:
?x : start(x, y1), end(x, y2) ? y1 ? y2.
Multi-document extraction of single relationships
exploits redundancy and variety of expression to ex-
tract accurate information from across many docu-
ments. However, these are not the only benefits of
extraction from a large document collection. As well
as being rich in redundant information, large docu-
ment collections also contain a wealth of secondary
relationships which are related to the relationship of
interest via constraints as described above. These
secondary relationships can yield benefits to aug-
ment those achieved by redundancy.
3 Multi-Document Database Fusion
There are typically two steps in the extraction of sin-
gle relationships from multiple documents. In the
first step, a relationship extractor goes through the
corpus, finds all possible relationships r in all sen-
tences s and gives them a score p(r|s). Next, the
ity, typically binary relationships are combined (McDonald et
al., 2005).
relationships are fused across sentences to generate
one score ?r for each relationship.
This paper proposes a third step which combines
the fusion scores across relationships. This section
first presents a probabilistic database model gener-
ated from fusion scores and then shows how to use
this model for multi-document fusion.
3.1 A Probabilistic Database Model
A relationship r is defined to be a 3-tuple rt,a,b =
r(t, a, b), where t is the type of the relationship (e.g.
start), and a and b are the arguments of the binary
relationship.3
To construct a probabilistic database for a given
corpus, the weights generated in relationship fusion
are normalized to provide the conditional probability
of a relationship given its type:
p(rt,a,b1 |t1) =
?rt,a,b1?
ri:rti=t
?rt,a,bi
,
where ?r is the fusion score generated by the extrac-
tion/fusion system.4 By applying a prior over types
p(t), a distribution p(r1, t1) can be derived. Given
strong independence assumptions, the probability of
an ordered database configurationR = r1..n of types
t1..n is:
p(r1..n, t1..n) =
n?
i=1
p(ri, ti). (1)
3For readibility in future examples, ?a? and ?b? are replaced
by the types of their arguments. For example, for start the year
in which the CEO starts is referred to as ryear .
4The following fusion method does not depend on a par-
ticular extraction/fusion architecture or training methodology,
merely this conditional probability distribution.
333
As proposed, the model in Equation 1 is faulty
since the relationships in a database are not inde-
pendent. Given a set of database constraints, certain
database configurations are illegal and should be as-
signed zero probability. To address this, the model
in Equation 1 is augmented with constraints that ex-
plicitly set the probability of a database configura-
tion to zero when they are violated.
A database constraint is a logical formula
?(r1..pi(?)), where pi(?) is the arity of the constraint
?. For the constraints presented in this paper, all
constraints ? are modeled with two terms ?? and ??
where:
?(r1..pi(?)) =
(
??(r1..pi(?)) ? ?
?(r1..pi(?))
)
.
For a set of relationships, a constraint holds if ?(?)
is true, and the constraint applies if ??(?) is true. A
constraint ?(?) can only be violated (false) when the
constraint applies, since: (false ? X) = true.
In application to a database, each constraint ? is
quantified over the database to become a quantified
constraint ?r1..n . For example, the constraints that a
person?s start date must come before their end date is
universally quantified over all pairs of relationships
in a configuration R = r1..n:
?r1..n = ?r1,r2 ? R : ?(r1, r2) =
(rt1 = start, r
t
2 = end, r
ceo
1 = r
ceo
2 )
? (ryear1 < r
year
2 ).
This constraint applies to start and end relationships
whose CEO argument matches and is violated when
the years are not in order. If the quantified constraint
?r1..n is true for a given database configuration r1..n
then it holds.
To ensure that only legal database configurations
are assigned positive probabilities, Equation 1 is
augmented with a factor
??r1..n =
{
1 if ?r1..nholds
0 otherwise .
To include a constraint ?, the database model in
Equation 1 is extended to be:
p?(r1..n, t1..n) =
1
Z
(
?
i
p(ri, ti)
)
??r1..n,
where Z is the partition function and corresponds to
the total probability of all database configurations. A
set of constraints ?1..Q = ?1..?Q can be integrated
similarly:
p?1..Q(r1..n, t1..n) =
1
Z
(
?
i
p(ri, ti)
)
?
q
??
q
r1..n
(2)
With these added constraints, the probabilistic
database model assigns non-zero probability only to
databases which don?t violate any constraints.
3.2 Constraints on Probabilistic Databases for
Relationship Rescoring
Though the constrained probabilistic database
model in Equation 2 is theoretically appealing, it
would be infeasible to calculate its partition func-
tion which requires enumeration of all legal 2n
databases. This section proposes two methods for
re-scoring relationships with regards to how likely
they are to be present in a legal database configu-
ration using the model proposed above. The first
method is a confidence estimate based on how likely
it is that ? holds for a given relationship r1:
??(r1, t1) = Ep(r2..n,t2..n)
[
??r1..pi(?)
]
=
?
r2..pi(?)
(?pi(?)
i=2 p(ri, ti)
)
??r1..pi(?)
?
r2..pi(?)
(?pi(?)
i=2 p(ri, ti)
)
=
?
r2..pi(?)
p?(r1..n, t1..n)
?
r2..pi(?)
p(r1..n, t1..n)
,
where the expectation that the constraint holds
is equivalent to the likelihood ratio between the
database probability models with and without con-
straints. In effect, this model measures the expec-
tation that the constraint holds for a finite database
?look-ahead? of size pi(?)? 1.
With this method, for a constraint to reduce the
confidence in a particular relationship by half, half
of all configurations would have to violate the con-
straint.5 Since inconsistencies are relatively rare, for
a given relationship ??(r, t) ? 1 (i.e. almost all
small databases are legal).
5Assuming equal probability for all relationships.
334
To remedy this, another factor ?? is defined simi-
larly to ??, except that it takes a value of 1 only if the
constraint applies to that database configuration. An
applicability probability model is then defined as:
p?(r1..n, t1..n) =
1
Z
(
?
i
p(ri, ti)
)
??r1..n.
The second confidence estimate is based on how
likely it is that the constraint is holds in cases where
it applies (i.e. is not violated):
??,?(r1, t1)
= Ep?(r2..n,t2..n)
[
??r1..pi(?)
]
=
?
r2..pi(?)
(?pi(?)
i=2 p(ri, ti)
)
??r1..pi(?)?
?
r1..pi(?)
?
r2..pi(?)
(?pi(?)
i=2 p(ri, ti)
)
??r1..pi(?)
.
When the constraint doesn?t apply it cannot be vio-
lated, so this confidence estimate ignores those con-
figurations that can?t be affected by the constraint.
Recall that ??(r, t) is the likelihood ratio be-
tween the probability of configurations in which r
holds for constraint ? and all configurations. In con-
trast, ??,?(r, t) is the likelihood ratio between the
database configurations where r applies and holds
for ? and the database configurations where ? ap-
plies. In the later ratio, for confidence in a particular
relationship to be cut in half, only half of the con-
figurations which might actually contain an incon-
sistency would be required to produce a violation.6
As a result, ??,?(r, t) gives a much higher penalty to
relationships which create inconsistencies than does
??(r, t).
In order to apply multiple constraints, indepen-
dent database look-aheads are generated for each
constraint q:
??1..Q,?1..Q(r1, t1) =
?
q
??q ,?q(r1, t1).
For a particular relationship type, these confidence
scores are calculated and then used to rank the rela-
6For example, for a start relationship and the constraint that
a CEO must start before they end, this method would only ex-
amine configurations of one start and one end relationship for
the same CEO. The confidence in a particular start date would
be halved if half of the proposed end dates for a given CEO
occurred before it.
tionships via:
c??1..Q,?1..Q(r1, t1) = p(r1, t1)
?
q
??q ,?q(r1, t1)
(3)
Databases with different precision/recall trade offs
can be selected by descending the ranked list.7
4 Experiments
In order to test the fusion method proposed above,
human annotators manually constructed truth data
of complete chief executive histories for 18 Fortune-
500 companies using online resources. Extraction
from these documents is particularly difficult be-
cause these data have vast differences in genre and
style and are considerably noisy. Furthermore, the
task is complicated to start with.8
A corpus was created for each company by is-
suing a Google query for ?CEO-of-Company OR
Company-CEO?, and collecting the top ranked doc-
uments, generating up to 1000 documents per com-
pany. The data was then split randomly into training,
development and testing sets of 6, 4, and 8 compa-
nies.
Training : Anheuser-Busch, Hewlett-Packard,
Lenner, McGraw-Hill, Pfizer, Raytheon
Dev. : Boeing, Heinz, Staples, Textron
Test : General Electric, General Motors,
Gannett, The Home Depot, IBM,
Kroger, Sears, UPS
Ground truth was created from the entire web, but
since the corpus for each company is only a small
web snapshot, the experimental results are not simi-
lar to extraction tasks like MUC and ACE in that the
corpus is not guaranteed to contain the information
necessary to build the entire database. In particular,
7One thing to note is that since all relationships are given
confidence estimates separately, this process may result ulti-
mately in a database where constraints are violated. A potential
solution, which is not explored here, would be to incrementally
add relationships to the database from the ranked list only if
their addition doesn?t make the database inconsistent.
8For example, in certain companies, the title of the chief
executive has changed over the years, often going from ?Presi-
dent? to ?Chief Executive Officer?. To make things more com-
plicated, after the change, the role of ?President? may still hang
on as a subordinate to the CEO!
335
1) Only one start or end per person.
?r1, r2 : ?(r1, r2) = (r
type
1 = r
type
2 = (start ? end), r
ceo
1 = r
ceo
2 ) ? (r
year
1 = r
year
2 )
2) Only a CEO?s start or end dates belong in the database.
?r1?r2 : ?(r1, r2) = (r
type
1 = start ? end, r
type
2 = ceo) ? (r
ceo
1 = r
ceo
2 )
3) Start dates come before end dates.
?r1, r2 : ?(r1, r2) = (r
type
1 = start, r
type
2 = end, r
ceo
1 = r
ceo
2 ) ? (r
year
1 ? r
year
2 )
4) Can?t be in the middle of someone else?s tenure.
?r1, r2, r3 : ?(r1, r2, r3) = (r
type
1 = start ? inoffice, r
type
2 = end ? inoffice, r
type
3 = start ? inoffice ? end,
rceo1 = r
ceo
2 6= r
ceo
3 , r
year
1 < r
year
2 ) ? (r
year
3 ? r
year
1 ? r
year
3 ? r
year
2 )
5) CEO?s are only in office after their start.
?r1, r2 : ?(r1, r2) = (r
type
1 = start, r
type
2 = inoffice, r
ceo
1 = r
ceo
2 ) ? (r
year
1 ? r
year
2 )
6) CEO?s are only in office before their end.
?r1, r2 : ?(r1, r2) = (r
type
1 = inoffice, r
type
2 = end, r
ceo
1 = r
ceo
2 ) ? (r
year
1 ? r
year
2 )
7) Someone?s end is the same as their successor?s start.
?r1, r2, r3 : ?(r1, r2, r3) =
(rtype1 = end, r
type
2 = start, r
type
3 = precedes, r
ceo
1 = r
first
3 , r
ceo
2 = r
second
3 ) ? (r
year
1 = r
year
2 )
8) All of the someone?s dates (start, inoffice, end) are before their successors.
?r1, r2, r3 : ?(r1, r2, r3) = (r
type
1 = start ? end ? inoffice, r
type
2 = start ? inoffice ? end, r
type
3 = precedes,
rceo1 = r
first
3 , r
ceo
2 = r
second
3 ) ? (r
year
1 ? r
year
2 )
9) Only CEO succession in the database.
?r1?r1, r2 : ?(r1, r2, r3) = (r
type
1 = precedes, r
type
2 = r
type
3 = ceo) ? (r
first
1 = r
ceo
2 , r
second
1 = r
ceo
3 )
Table 2: For a CEO succession database like the one presented in Table 1, the above constraints must hold
if the database is consistent.
many CEOs from pre-Internet years were either in-
frequently mentioned or not mentioned at all in the
database.9 In the following experiments, recall is re-
ported for facts that were retrieved by the extraction
system.
4.1 Relationship Extraction and Fusion
A two-class maximum-entropy classifier was trained
for each relationship type. Each classifier takes a
sentence and two marked entities (e.g. a person and
a year)10 and gives the probability that a relation-
ship between the two entities is supported by the
sentence. For each relationship type, one of the ele-
ments is designated as the ?hook? in order to gener-
ate likely negative examples.11 In training, all entity
pairs are collected from the corpus. The pairs whose
?hook? element doesn?t appear in the database are
thrown out. The remaining pairs are then marked
9Another consequence is that assessing the effectiveness of
the relationships extraction on a per-extraction basis is difficult.
Because there are no training sentences where it is known that
the sentence contains the relationship of interest, grading per-
extraction results can be deceptive.
10The person tagger used is the named-entity tagger from
OpenNLP tools and the year tagger simply finds any four digit
numbers between 1950 and 2010.
11For the CEO relationship, the company was taken to be
the hook. For the other relationships the hook was the primary
CEO.
by exact match to the database. In testing, the rela-
tionship extractor yields the probability p(r|s) of an
entity pair relationship r in a particular sentence s.
The features used in the classifier are: unigrams
between the given information and the target, dis-
tance in words between the given information and
the target, and the exact string between the given in-
formation and the target (if less that 3 words long).
After extraction from individual sentences, the re-
lationships are fused together such that there is one
score for each unique entity pair. In the case of per-
son names, normalization was performed to merge
coreferent but lexically distinct names (e.g. ?Phil
Condit? and ?Philip M. Condit?).
In the following experiments, the baseline fusion
score is:
?r =
?
s
p(r|s) (4)
4.2 Experimental Results
Given the management succession database pro-
posed in Section 2, Table 2 enumerates a set of quan-
tified constraints. Information extraction and fusion
were run separately for each company to create a
probabilistic database. In this section, various con-
straint sets are applied, either individually or jointly,
and evaluated in two ways. The first measures per-
relationship precision/recall using the model pro-
336
Second (8)First Before
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  0.2  0.4  0.6  0.8  1
BaselineOnly One (1)CEOs Only (2)Start Before End (3)Inoffice After Start (5)2,52,3,5,
Figure 1: Precision/Recall curve for start(x,t) rela-
tionships. The joint constraint ?2,3,5,8? is the best
performing, even though constraints ?3? and ?8?
(not pictured) alone don?t perform well.
posed and the second looks at the precision/recall
of a heterogeneous database with many relationship
types. Both evaluations examine the ranked lists of
relationships, where the relationships are ranked by
rescoring via constraints on probabilistic databases
(Equation 3) and compared to the baseline fusion
score (Equation 4). The evaluations use two stan-
dard metrics, interpolated precision at recall level i
(PRi), and MaxF1:
PRi = maxj?i
PRj,
MaxF1 = max
i
2
1
PRi
+ 1Ri
.
Figures 1, 2, and 3 show precision/recall curves
for the application of various sets of constraints. Ta-
ble 3 lists the MaxF1 scores for each of the con-
straint variants. For start and end, the majority of
constraints are beneficial. For precedes, only the
constraint that improved performance constraints
both people in the relationship to be CEOs. Across
all relationships, performance is hurt when using the
constraint that there could only be one relationship
of each type for a given CEO. The reason behind
this is that the confidence estimate based on this
constraint favors relationships with few competitors,
and those relationships are typically for people who
are infrequent in the corpus (and therefore unlikely
to be CEOs).
The best-performing constraint sets yield between
5 and 18 points of improvement on Max F1 (Ta-
ble 3). Surprisingly, the gains from joint con-
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0  0.2  0.4  0.6  0.8  1
BaselineOnly One (1)CEOs Only (2)Inoffice Before End (6)2,6
Figure 2: Precision/Recall curve for end(x,t) rela-
tionships alone. The joint constraint ?2,6? is the best
performing.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0  0.2  0.4  0.6  0.8  1
BaselineEnd is Start (7)First Before Second (8)Only CEO Succession (9)
Figure 3: Precision/Recall for precedes(x,y) rela-
tionships alone. Though the constraint ?First Before
Second (8)? helps performance on start(x,t) relation-
ships, the only constraint which aids here is ?Only
CEOs Succession (9)?.
straints are sometimes more than their additive
gains. ?2,3,5,6,8? is 6 points better for the start rela-
tionship than ?2,3,5,6?, but the gains from ?8? alone
are negligible.
These performance gains on the individual rela-
tionship types also lead to gains when generating an
entire database (Figure 4). The highest performing
constraint is the ?CEOs Only (2)? constraint, which
outperforms the joint constraints of the previous sec-
tion. One reason the joint constraints don?t do as
well here is that each constraint makes the confi-
dence estimate smaller and smaller. This doesn?t
have an effect when judging the relationship types
individually, but when combining the relationships
results, the fused relationships types (start, end) be-
337
Max F1
Constraint Set Start End Pre. DB
? (baseline) 31.2 35.8 34.5 37.9
Only One (1) 10.5 7.2 - 38.1
CEOs Only (2) or (9) 43.3 39.4 39.4 42.9
Start Before End (3) 40.8 32.8 - 40.9
No Overlaps (4) 31.5 35.9 - 36.8
Inoffice After Start (5) 32.5 - - 38.2
Inoffice Before End (6) - 36.5 - 37.4
End is Start (7) 7.3 8.0 20.7 39.2
First before Second (8) 31.4 35.6 26.3 38.1
2,5,6 43.3 40.8 - 42.7
2,3,5,6 43.9 43.3 - 42.2
2,3,5,6,8 49.3 43.9 26.3 40.9
Table 3: Max F1 scores for three relationships
Start(x,t), End(x,t) and Precedes(x,y)) in isolation
and within the context of whole database DB. The
joint constraints perform best for the explicit rela-
tionships in isolation. Using constraints on implicit
derived fields (Inoffice and Precedes) provides ad-
ditional benefit above constraints strictly on explicit
database fields (start, end, ceo).
come artificially lower ranked than the unfused rela-
tionship type (ceo). The best performing contrained
probabilistic database approach beats the baseline
by 5 points.
5 Related Work
Techniques for information extraction from min-
imally supervised data have been explored by
Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002). Those techniques
propose methods for estimating extractors from ex-
ample relationships and a corpus which contains in-
stances of those relationships.
Nahm and Mooney (2002) explore techniques for
extracting multiple relationships in single document
extraction. They learn rules for predicting certain
fields given other extracted fields (i.e. a someone
who lists Windows as a specialty is likely to know
Microsoft Word).
Perhaps the most related work to what is pre-
sented here is previous research which uses database
information as co-occurrence features for informa-
tion extraction in a multi-document setting. Mann
Inoffice Before End (6)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.2  0.4  0.6  0.8  1
BaselineCEOs Only (2)Start Before End (3)2, Inoffice After Start (5)
2,3,5,6
Figure 4: Precision/Recall curve for whole database
reconstruction. Performance curves using con-
straints dominate the baseline.
and Yarowsky (2005) present an incremental ap-
proach where co-occurrence with a known relation-
ship is a feature added in training and test. Cu-
lotta et al (2006) introduce a data mining approach
where discovered relationships from a database are
used as features in extracting new relationships. The
database constraints presented in this paper provide
a more general framework for jointly conditioning
multiple relationships. Additionally, this constraint-
based approach can be applied without special train-
ing of the extraction/fusion system.
In the context of information fusion of single rela-
tionships across multiple documents, Downey et al
(2005) propose a method that models the probabili-
ties of positive and negative extracted classifications.
More distantly related, Sutton andMcCallum (2004)
and Finkel et al (2005) propose graphical models
for combining information about a given entity from
multiple mentions.
In the field of question answering, Prager et al
(2004) answer a question about the list of composi-
tions produced by a given subject by looking for re-
lated information about the subject?s birth and death.
Their method treats supporting information as fixed
hard constraints on the original questions and are ap-
plied in an ad-hoc fashion. This paper proposes a
probabilistic method for using constraints in the con-
text of database extraction and applies this method
over a larger set of relations.
Richardson and Domingos (2006) propose a
method for reasoning about databases and logical
constraints using Markov Random Fields. Their
338
model applies reasoning starting from a known
database. In this paper the database is built from ex-
traction/fusion of relationships from web pages and
contains a significant amount of noise.
6 Conclusion
This paper has presented a probabilistic method for
fusing extracted facts in the context of database ex-
traction when there exist logical constraints between
the fields in the database. The method estimates the
probability than the inclusion of a given relationship
will violate database constraints by taking into ac-
count the uncertainty of the other extracted relation-
ships. Along with the relationships explicitly listed
in the database, constraints are formed over implicit
fields directly recoverable from the explicit listed re-
lationships.
The construction of CEO succession timelines us-
ing minimally trained extractors from web text is a
particularly challenging problem because of noise
resulting from the wide variation in genre in the cor-
pora and errors in extraction. The use of constraints
on probabilistic databases is effective in resolving
many of these errors, leading to improved precision
and recall of retrieved facts, with F-measure gains of
5 to 18 points.
The method presented in this paper combines
symbolic and statistical approaches to natural lan-
guage processing. Logical constraints are made
more robust by taking into account the uncertainty
of the extracted information. An interesting area
of future work is the application of data mining to
search for appropriate constraints to integrate into
this model.
Acknowledgements
This work was supported in part by DoD contract #HM1582-
06-1-2013. Any opinions, findings and conclusions or recom-
mendations expressed in this material belong to the author and
do not necessarily reflect those of the sponsor.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 85?94.
E. Agichtein. 2005. Extracting Relations from Large Text Col-
lections. Ph.D. thesis, Columbia University.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993.
FASTUS: a finite-state processor for information extraction
from real-world text. In Proceedings of IJCAI.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT?98, pages
172?183.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating prob-
abilistic extraction models and data mining to discover rela-
tions and patterns in text. In HLT-NAACL, pages 296?303,
New York, NY, June.
D. Downey, O. Etzioni, and S. Soderland. 2005. A probabilistic
model of redundancy in information extraction. In IJCAI.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A-M. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004. Web-
scale information extraction in knowitall. In WWW.
J. Finkel, T. Grenager, , and C. Manning. 2005. Incorporating
non-local information into information extraction systems by
gibbs sampling. In ACL.
R. Grishman and B. Sundheim. 1996. Message understanding
conference-6: A brief history. In Proceedings of COLING.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering
relations amoung named entities from large corpora. In ACL.
G. Mann and D. Yarowsky. 2005. Multi-field information ex-
traction and cross-document fusion. In ACL.
R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin, and
P. White. 2005. Simple algorithms for complex relationship
extraction with applications to biomedical ie. In Proceed-
ings of ACL.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 60?67.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006.
Organizing and searching the world wide web of facts - step
one: The one-million fact extracion challenge. In AAAI.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings of ACL, pages 574?581.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 41?47.
M. Richardson and P. Domingos. 2006. Markov logic net-
works. Machine Learning, 62:107?136.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information extraction.
Technical Report TR # 04-49, University of Massachusetts,
July. Presented at ICML Workshop on Statistical Relational
Learning and Its Connections to Other Fields.
339
Proceedings of NAACL HLT 2007, Companion Volume, pages 109?112,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Efficient Computation of Entropy Gradient for
Semi-Supervised Conditional Random Fields
Gideon S. Mann and Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
gideon.mann@gmail.com , mccallum@cs.umass.edu
Abstract
Entropy regularization is a straightforward
and successful method of semi-supervised
learning that augments the traditional con-
ditional likelihood objective function with
an additional term that aims to minimize
the predicted label entropy on unlabeled
data. It has previously been demonstrated
to provide positive results in linear-chain
CRFs, but the published method for cal-
culating the entropy gradient requires sig-
nificantly more computation than super-
vised CRF training. This paper presents
a new derivation and dynamic program
for calculating the entropy gradient that
is significantly more efficient?having the
same asymptotic time complexity as su-
pervised CRF training. We also present
efficient generalizations of this method
for calculating the label entropy of all
sub-sequences, which is useful for active
learning, among other applications.
1 Introduction
Semi-supervised learning is of growing importance
in machine learning and NLP (Zhu, 2005). Condi-
tional random fields (CRFs) (Lafferty et al, 2001)
are an appealing target for semi-supervised learning
because they achieve state-of-the-art performance
across a broad spectrum of sequence labeling tasks,
and yet, like many other machine learning methods,
training them by supervised learning typically re-
quires large annotated data sets.
Entropy regularization (ER) is a method of semi-
supervised learning first proposed for classification
tasks (Grandvalet and Bengio, 2004). In addition to
maximizing conditional likelihood of the available
labels, ER also aims to minimize the entropy of the
predicted label distribution on unlabeled data. By in-
sisting on peaked, confident predictions, ER guides
the decision boundary away from dense regions of
input space. It is simple and compelling?no pre-
clustering, no ?auxiliary functions,? tuning of only
one meta-parameter and it is discriminative.
Jiao et al (2006) apply this method to linear-
chain CRFs and demonstrate encouraging accuracy
improvements on a gene-name-tagging task. How-
ever, the method they present for calculating the
gradient of the entropy takes substantially greater
time than the traditional supervised-only gradient.
Whereas supervised training requires only classic
forward/backward, taking time O(ns2) (sequence
length times the square of the number of labels),
their training method takes O(n2s3)?a factor of
O(ns) more. This greatly reduces the practicality
of using large amounts of unlabeled data, which is
exactly the desired use-case.
This paper presents a new, more efficient entropy
gradient derivation and dynamic program that has
the same asymptotic time complexity as the gradient
for traditional CRF training, O(ns2). In order to de-
scribe this calculation, the paper introduces the con-
cept of subsequence constrained entropy?the en-
tropy of a CRF for an observed data sequence when
part of the label sequence is fixed. These meth-
ods will allow training on larger unannotated data
set sizes than previously possible and support active
109
learning.
2 Semi-Supervised CRF Training
Lafferty et al (2001) present linear-chain CRFs, a
discriminative probabilistic model over observation
sequences x and label sequences Y = ?Y1..Yn?,
where |x| = |Y | = n, and each label Yi has s differ-
ent possible discrete values. For a linear-chain CRF
of Markov order one:
p?(Y |x) =
1
Z(x)
exp
(
?
k
?kFk(x, Y )
)
,
where Fk(x, Y ) =
?
i fk(x, Yi, Yi+1, i),
and the partition function Z(x) =
?
Y exp(
?
k ?kFk(x, Y )). Given training
data D = ?d1..dn?, the model is trained by
maximizing the log-likelihood of the data
L(?;D) =
?
d log p?(Y
(d)|x(d)) by gradient
methods (e.g. Limited Memory BFGS), where the
gradient of the likelihood is:
?
??k
L(?;D) =
?
d
Fk(x
(d), Y (d))
?
?
d
?
Y
p?(Y |x
(d))Fk(x
(d), Y ).
The second term (the expected counts of the features
given the model) can be computed in a tractable
amount of time, since according to the Markov as-
sumption, the feature expectations can be rewritten:
?
Y
p?(Y |x)Fk(x, Y ) =
?
i
?
Yi,Yi+1
p?(Yi, Yi+1|x)fk(x, Yi, Yi+1).
A dynamic program (the forward/backward algo-
rithm) then computes in time O(ns2) all the needed
probabilities p?(Yi, Yi+1), where n is the sequence
length, and s is the number of labels.
For semi-supervised training by entropy regular-
ization, we change the objective function by adding
the negative entropy of the unannotated data U =
?u1..un?. (Here Gaussian prior is also shown.)
L(?;D,U) =
?
n
log p?(Y
(d)|x(d)) ?
?
k
?k
2?2
+ ?
?
u
p?(Y
(u)|x(u)) log p?(Y
(u)|x(u)).
This negative entropy term increases as the decision
boundary is moved into sparsely-populated regions
of input space.
3 An Efficient Form of the Entropy
Gradient
In order to maximize the above objective function,
the gradient for the entropy term must be computed.
Jiao et al (2006) perform this computation by:
?
??
? H(Y |x) = covp?(Y |x)[F (x, Y )]?,
where
covp?(Y |x)[Fj(x, Y ), Fk(x, Y )] =
Ep?(Y |x)[Fj(x, Y ), Fk(x, Y )]
? Ep?(Y |x)[Fj(x, Y )]Ep?(Y |x)[Fk(x, Y )].
While the second term of the covariance is easy
to compute, the first term requires calculation of
quadratic feature expectations. The algorithm they
propose to compute this term is O(n2s3) as it re-
quires an extra nested loop in forward/backward.
However, the above form of the gradient is not
the only possibility. We present here an alternative
derivation of the gradient:
?
??k
?H(Y |x) =
?
??k
X
Y
p?(Y |x) log p?(Y |x)
=
X
Y
?
?
??k
p?(Y |x)
?
log p?(Y |x)
+ p?(Y |x)
?
?
??k
log p?(Y |x)
?
=
X
Y
p?(Y |x) log p?(Y |x)
?
 
Fk(x, Y ) ?
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
+
X
Y
p?(Y |x)
 
Fk(x, Y ) ?
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
.
Since
?
Y p?(Y |x)
?
Y ? p?(Y
?|X)Fk(x, Y ?) =?
Y ? p?(Y
?|X)Fk(x, Y ?), the second summand can-
cels, leaving:
?
??
?H(Y |x) =
X
Y
p?(Y |x) log p?(Y |x)Fk(x, Y )
?
 
X
Y
p?(Y |x) log p?(Y |x)
! 
X
Y ?
p?(Y
?|x)Fk(x, Y
?)
!
.
Like the gradient obtained by Jiao et al (2006),
there are two terms, and the second is easily com-
putable given the feature expectations obtained by
110
forward/backward and the entropy for the sequence.
However, unlike the previous method, here the first
term can be efficiently calculated as well. First,
the term must be further factored into a form more
amenable to analysis:
?
Y
p?(Y |x) log p?(Y |x)Fk(x, Y )
=
?
Y
p?(Y |x) log p?(Y |x)
?
i
fk(x, Yi, Yi+1, i)
=
?
i
?
Yi,Yi+1
fk(x, Yi, Yi+1, i)
?
Y?(i..i+1)
p?(Y |x) log p?(Y |x).
Here, Y?(i..i+1) = ?Y1..(i?1)Y(i+2)..n?. In order
to efficiently calculate this term, it is sufficient
to calculate
?
Y?(i..i+1)
p?(Y |x) log p?(Y |x) for all
pairs yi, yi+1. The next section presents a dynamic
program which can perform these computations in
O(ns2).
4 Subsequence Constrained Entropy
We define subsequence constrained entropy as
H?(Y?(a..b)|ya..b, x) =
?
Y?(a..b)
p?(Y |x) log p?(Y |x).
The key to the efficient calculation for all subsets
is to note that the entropy can be factored given a
linear-chain CRF of Markov order 1, since Yi+2 is
independent of Yi given Yi+1.
?
Y?(a..b)
p?(Y?(a..b), ya..b|x) log p?(Y?(a..b), ya..b|x)
=
?
Y?(a..b)
p?(ya..b|x)p?(Y?(a..b)|ya..b, x)?
(
log p?(ya..b|x) + log p?(Y?(a..b)|ya..b, x)
)
=p?(ya..b|x) log p?(ya..b|x)
+ p?(ya..b|x)H
?(Y?(a..b)|ya..b, x)
=p?(ya..b|x) log p?(ya..b|x)
+ p?(ya..b|x)H
?(Y1..(a?1)|ya, x)
+ p?(ya..b|x)H
?(Y(b+1)..n|yb, x).
Given the H?(?) and H?(?) lattices, any sequence
entropy can be computed in constant time. Figure 1
H (0|y6)H (Y6|y5)H (0|y1) H (Y1|y2)
y4
y3
? ? ? ?
Figure 1: Partial lattice shown for com-
puting the subsequence constrained entropy:P
Y p(Y?(3..4), y3, y4) log p(Y?(3..4), y3, y4). Once the
complete H? and H? lattices are constructed (in the direction
of the arrows), the entropy for each label sequence can be
computed in linear time.
illustrates an example in which the constrained se-
quence is of size two, but the method applies to
arbitrary-length contiguous label sequences.
Computing the H?(?) and H?(?) lattices is easily
performed using the probabilities obtained by for-
ward/backward. First recall the decomposition for-
mulas for entropy:
H(X,Y ) = H(X) + H(Y |X)
H(Y |X) =
?
x
P (X = x)H(Y |X = x).
Using this decomposition, we can define a dynamic
program over the entropy lattices similar to for-
ward/backward:
H?(Y1..i|yi+1, x)
=H(Yi|yi+1, x) + H(Y1..(i?1)|Yi, yi+1, x)
=
?
yi
p?(yi|yi+1, x) log p?(yi|yi+1, x)
+
?
yi
p?(yi|yi+1, x)H
?(Y1..(i?1)|yi).
The base case for the dynamic program is
H?(?|y1) = p(y1) log p(y1). The backward entropy
is computed in a similar fashion. The conditional
probabilities p?(yi|yi?1, x) in each of these dynamic
programs are available by marginalizing over the
per-transition marginal probabilities obtained from
forward/backward.
The computational complexity of this calcula-
tion for one label sequence requires one run of for-
ward/backward at O(ns2), and equivalent time to
111
calculate the lattices for H? and H? . To calculate
the gradient requires one final iteration over all label
pairs at each position, which is again time O(ns2),
but no greater, as forward/backward and the en-
tropy calculations need only to be done once. The
complete asymptotic computational cost of calcu-
lating the entropy gradient is O(ns2), which is the
same time as supervised training, and a factor of
O(ns) faster than the method proposed by Jiao et
al. (2006).
Wall clock timing experiments show that this
method takes approximately 1.5 times as long as
traditional supervised training?less than the con-
stant factors would suggest.1 In practice, since the
three extra dynamic programs do not require re-
calculation of the dot-product between parameters
and input features (typically the most expensive part
of inference), they are significantly faster than cal-
culating the original forward/backward lattice.
5 Confidence Estimation
In addition to its merits for computing the entropy
gradient, subsequence constrained entropy has other
uses, including confidence estimation. Kim et al
(2006) propose using entropy as a confidence esti-
mator in active learning in CRFs, where examples
with the most uncertainty are selected for presenta-
tion to humans labelers. In practice, they approxi-
mate the entropy of the labels given the N-best la-
bels. Not only could our method quickly and ex-
actly compute the true entropy, but it could also be
used to find the subsequence that has the highest un-
certainty, which could further reduce the additional
human tagging effort.
6 Related Work
Hernando et al (2005) present a dynamic program
for calculating the entropy of a HMM, which has
some loose similarities to the forward pass of the
algorithm proposed in this paper. Notably, our algo-
rithm allows for efficient calculation of entropy for
any label subsequence.
Semi-supervised learning has been used in many
models, predominantly for classification, as opposed
to structured output models like CRFs. Zhu (2005)
1Reporting experimental results with accuracy is unneces-
sary since we duplicate the training method of Jiao et al (2006).
provides a comprehensive survey of popular semi-
supervised learning techniques.
7 Conclusion
This paper presents two algorithmic advances. First,
it introduces an efficient method for calculating
subsequence constrained entropies in linear-chain
CRFs, (useful for active learning). Second, it
demonstrates how these subsequence constrained
entropies can be used to efficiently calculate the
gradient of the CRF entropy in time O(ns2)?
the same asymptotic time complexity as the for-
ward/backward algorithm, and a O(ns) improve-
ment over previous algorithms?enabling the prac-
tical application of CRF entropy regularization to
large unlabeled data sets.
Acknowledgements
This work was supported in part by DoD contract #HM1582-
06-1-2013, in part by The Central Intelligence Agency, the Na-
tional Security Agency and National Science Foundation under
NSF grant #IIS-0427594, and in part by the Defense Advanced
Research Projects Agency (DARPA), through the Department
of the Interior, NBC, Acquisition Services Division, under con-
tract number NBCHD030010. Any opinions, findings and con-
clusions or recommendations expressed in this material belong
to the author(s) and do not necessarily reflect those of the spon-
sor.
References
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised learning
by entropy minimization. In NIPS.
D. Hernando, V. Crespi, and G. Cybenko. 2005. Efficient com-
putation of the hidden markov model entropy for a given
observation sequence. IEEE Trans. on Information Theory,
51:7:2681?2685.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-
mans. 2006. Semi-supervised conditional random fields
for improved sequence segmentation and labeling. In COL-
ING/ACL.
S. Kim, Y. Song, K. Kim, J.-W. Cha, and G. G. Lee. 2006.
Mmr-based active machine learning for bio named entity
recognition. In HLT/NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, pages 282?
289.
X. Zhu. 2005. Semi-supervised learning literature survey.
Technical Report 1530, Computer Sciences, University of
Wisconsin-Madison.
112
 
	   	

	
ffProceedings of the 43rd Annual Meeting of the ACL, pages 483?490,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multi-Field Information Extraction and Cross-Document Fusion
Gideon S. Mann and David Yarowsky
Department of Computer Science
The Johns Hopkins University
Baltimore, MD 21218 USA
{gsm,yarowsky}@cs.jhu.edu
Abstract
In this paper, we examine the task of extracting a
set of biographic facts about target individuals from
a collection of Web pages. We automatically anno-
tate training text with positive and negative exam-
ples of fact extractions and train Rote, Na??ve Bayes,
and Conditional Random Field extraction models
for fact extraction from individual Web pages. We
then propose and evaluate methods for fusing the
extracted information across documents to return a
consensus answer. A novel cross-field bootstrapping
method leverages data interdependencies to yield
improved performance.
1 Introduction
Much recent statistical information extraction re-
search has applied graphical models to extract in-
formation from one particular document after train-
ing on a large corpus of annotated data (Leek, 1997;
Freitag and McCallum, 1999).1 Such systems are
widely applicable, yet there remain many informa-
tion extraction tasks that are not readily amenable to
these methods. Annotated data required for training
statistical extraction systems is sometimes unavail-
able, while there are examples of the desired infor-
mation. Further, the goal may be to find a few inter-
related pieces of information that are stated multiple
times in a set of documents.
Here, we investigate one task that meets the above
criteria. Given the name of a celebrity such as
1Alternatively, Riloff (1996) trains on in-domain and
out-of-domain texts and then has a human filtering step.
Huffman (1995) proposes a method to train a different type of
extraction system by example.
?Frank Zappa?, our goal is to extract a set of bio-
graphic facts (e.g., birthdate, birth place and occupa-
tion) about that person from documents on the Web.
First, we describe a general method of automatic
annotation for training from positive and negative
examples and use the method to train Rote, Na??ve
Bayes, and Conditional Random Field models (Sec-
tion 2). We then examine how multiple extractions
can be combined to form one consensus answer
(Section 3). We compare fusion methods and show
that frequency voting outperforms the single high-
est confidence answer by an average of 11% across
the various extractors. Increasing the number of re-
trieved documents boosts the overall system accu-
racy as additional documents which mention the in-
dividual in question lead to higher recall. This im-
proved recall more than compensates for a loss in
per-extraction precision from these additional doc-
uments. Next, we present a method for cross-field
bootstrapping (Section 4) which improves per-field
accuracy by 7%. We demonstrate that a small train-
ing set with only the most relevant documents can be
as effective as a larger training set with additional,
less relevant documents (Section 5).
2 Training by Automatic Annotation
Typically, statistical extraction systems (such as
HMMs and CRFs) are trained using hand-annotated
data. Annotating the necessary data by hand is time-
consuming and brittle, since it may require large-
scale re-annotation when the annotation scheme
changes. For the special case of Rote extrac-
tors, a more attractive alternative has been proposed
by Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002).
483
Essentially, for any text snippet of the form
A1pA2qA3, these systems estimate the probability
that a relationship r(p, q) holds between entities p
and q, given the interstitial context, as2
P (r(p, q) | pA2q) = P (r(p, q) | pA2q)
=
?
x,y?T c(xA2y)
?
x c(xA2)
That is, the probability of a relationship r(p, q) is
the number of times that pattern xA2y predicts any
relationship r(x, y) in the training set T . c(.) is the
count. We will refer to x as the hook3 and y as the
target. In this paper, the hook is always an indi-
vidual. Training a Rote extractor is straightforward
given a set T of example relationships r(x, y). For
each hook, download a separate set of relevant doc-
uments (a hook corpus, Dx) from the Web.4 Then
for any particular pattern A2 and an element x, count
how often the pattern xA2 predicts y and how often
it retrieves a spurious y?.5
This annotation method extends to training other
statistical models with positive examples, for exam-
ple a Na??ve Bayes (NB) unigram model. In this
model, instead of looking for an exact A2 pattern
as above, each individual word in the pattern A2 is
used to predict the presence of a relationship.
P (r(p, q) | pA2q)
?P (pA2q | r(p, q))P (r(p, q))
=P (A2 | r(p, q))
=
?
a?A2
P (a | r(p, q))
We perform add-lambda smoothing for out-of-
vocabulary words and thus assign a positive prob-
ability to any sequence. As before, a set of relevant
2The above Rote models also condition on the preceding and
trailing words, for simplicity we only model interstitial words
A2.
3Following (Ravichandran and Hovy, 2002).
4In the following experiments we assume that there is one
main object of interest p, for whom we want to find certain
pieces of information r(p, q), where r denotes the type of re-
lationship (e.g., birthday) and q is a value (e.g., May 20th). We
require one hook corpus for each hook, not a separate one for
each relationship.
5Having a functional constraint ?q? 6= q, r?(p, q?) makes this
estimate much more reliable, but it is possible to use this method
of estimation even when this constraint does not hold.
documents is downloaded for each particular hook.
Then every hook and target is annotated. From that
markup, we can pick out the interstitial A2 patterns
and calculate the necessary probabilities.
Since the NB model assigns a positive probability
to every sequence, we need to pick out likely tar-
gets from those proposed by the NB extractor. We
construct a background model which is a basic un-
igram language model, P (A2) =
?
a?A2 P (a). We
then pick targets chosen by the confidence estimate
CNB(q) = log
P (A2 | r(p, q))
P (A2)
However, this confidence estimate does not work-
well in our dataset.
We propose to use negative examples to estimate
P (A2 | r?(p, q))6 as well as P (A2 | r(p, q)). For
each relationship, we define the target set Er to be
all potential targets and model it using regular ex-
pressions.7 In training, for each relationship r(p, q),
we markup the hook p, the target q, and all spuri-
ous targets (q? ? {Er ? q}) which provide negative
examples. Targets can then be chosen with the fol-
lowing confidence estimate
CNB+E(q) = log
P (A2 | r(p, q))
P (A2 | r?(p, q))
We call this NB+E in the following experiments.
The above process describes a general method for
automatically annotating a corpus with positive and
negative examples, and this corpus can be used to
train statistical models that rely on annotated data.8
In this paper, we test automatic annotation using
Conditional Random Fields (CRFs) (Lafferty et al,
2001) which have achieved high performance for in-
formation extraction. CRFs are undirected graphical
models that estimate the conditional probability of a
state sequence given an output sequence
P (s | o) =
1
Z
exp
( T?
t=1
?
k
?kfk(st?1, st, o, t)
)
6r? stands in for all other possible relationships (including no
relationship) between p and q. P (A2 | r?(p, q)) is estimated as
P (A2 | r(p, q)) is, except with spurious targets.
7e.g., Ebirthyear = {\d\d\d\d}. This is the only source of
human knowledge put into the system and required only around
4 hours of effort, less effort than annotating an entire corpus or
writing information extraction rules.
8This corpus markup gives automatic annotation that yields
noisier training data than manual annotation would.
484
p qA_2
B
p
A_2
A_2
q
B
q
Figure 1: CRF state-transition graphs for extracting a relation-
ship r(p, q) from a sentence pA2q. Left: CRF Extraction with
a background model (B). Right: CRF+E As before but with
spurious target prediction (pA2q?).
We use the Mallet system (McCallum, 2002) for
training and evaluation of the CRFs. In order to ex-
amine the improvement by using negative examples,
we train CRFs with two topologies (Figure 1). The
first, CRF, models the target relationship and back-
ground sequences and is trained on a corpus where
targets (positive examples) are annotated. The sec-
ond, CRF+E, models the target relationship, spu-
rious targets and background sequences, and it is
trained on a corpus where targets (positive exam-
ples) as well as spurious targets (negative examples)
are annotated.
Experimental Results
To test the performance of the different ex-
tractors, we collected a set of 152 semi-
structured mini-biographies from an online site
(www.infoplease.com), and used simple rules to
extract a biographic fact database of birthday and
month (henceforth birthday), birth year, occupation,
birth place, and year of death (when applicable).
An example of the data can be found in Table
1. In our system, we normalized birthdays, and
performed capitalization normalization for the
remaining fields. We did no further normalization,
such as normalizing state names to their two letter
acronyms (e.g., California ? CA). Fifteen names
were set aside as training data, and the rest were
used for testing. For each name, 150 documents
were downloaded from Google to serve as the hook
corpus for either training or testing.9
In training, we automatically annotated docu-
ments using people in the training set as hooks, and
in testing, tried to get targets that exactly matched
what was present in the database. This is a very strict
method of evaluation for three reasons. First, since
the facts were automatically collected, they contain
9Name polyreference, along with ranking errors, result in
the retrieval of undesired documents.
Aaron Neville Frank Zappa
Birthday January 24 December 21
Birth year 1941 1940
Occupation Singer Musician
Birthplace New Orleans Baltimore,Maryland
Year of Death - 1993
Table 1: Two of 152 entries in the Biographic Database. Each
entry contains incomplete information about various celebrities.
Here, Aaron Neville?s birth state is missing, and Frank Zappa
could be equally well described as a guitarist or rock-star.
errors and thus the system is tested against wrong
answers.10 Second, the extractors might have re-
trieved information that was simply not present in
the database but nevertheless correct (e.g., some-
one?s occupation might be listed as writer and the
retrieved occupation might be novelist). Third, since
the retrieved targets were not normalized, there sys-
tem may have retrieved targets that were correct but
were not recognized (e.g., the database birthplace is
New York, and the system retrieves NY).
In testing, we rejected candidate targets that were
not present in our target set models Er. In some
cases, this resulted in the system being unable to find
the correct target for a particular relationship, since
it was not in the target set.
Before fusion (Section 3), we gathered all the
facts extracted by the system and graded them in iso-
lation. We present the per-extraction precision
Pre-Fusion Precision = # Correct Extracted Targets
# Total Extracted Targets
We also present the pseudo-recall, which is the av-
erage number of times per person a correct target
was extracted. It is difficult to calculate true re-
call without manual annotation of the entire corpus,
since it cannot be known for certain how many times
the document set contains the desired information.11
Pre-Fusion Pseudo-Recall = # Correct Extracted Targets
#People
The precision of each of the various extraction
methods is listed in Table 2. The data show that
on average the Rote method has the best precision,
10These deficiencies in testing also have implications for
training, since the models will be trained on annotated data that
has errors. The phenomenon of missing and inaccurate data
was most prevalent for occupation and birthplace relationships,
though it was observed for other relationships as well.
11It is insufficient to count all text matches as instances that
the system should extract. To obtain the true recall, it is nec-
essary to decide whether each sentence contains the desired re-
lationship, even in cases where the information is not what the
biographies have listed.
485
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote .789 .355 .305 .510 .527 .497
NB+E .423 .361 .255 .217 .088 .269
CRF .509 .342 .219 .139 .267 .295
CRF+E .680 .654 .246 .357 .314 .450
Table 2: Pre-Fusion Precision of extracted facts for various extraction systems, trained on 15 people each with 150 documents, and
tested on 137 people each with 150 documents.
Birthday Birth year Occupation Birthplace Year of Death Avg.
Rote 4.8 1.9 1.5 1.0 0.1 1.9
NB+E 9.6 11.5 20.3 11.3 0.7 10.9
CRF 3.0 16.3 31.1 10.7 3.2 12.9
CRF+E 6.8 9.9 3.2 3.6 1.4 5.0
Table 3: Pre-Fusion Pseudo-Recall of extract facts with the identical training/testing set-up as above.
while the NB+E extractor has the worst. Train-
ing the CRF with negative examples (CRF+E) gave
better precision in extracted information then train-
ing it without negative examples. Table 3 lists the
pseudo-recall or average number of correctly ex-
tracted targets per person. The results illustrate that
the Rote has the worst pseudo-recall, and the plain
CRF, trained without negative examples, has the best
pseudo-recall.
To test how the extraction precision changes as
more documents are retrieved from the ranked re-
sults from Google, we created retrieval sets of 1, 5,
15, 30, 75, and 150 documents per person and re-
peated the above experiments with the CRF+E ex-
tractor. The data in Figure 2 suggest that there is a
gradual drop in extraction precision throughout the
corpus, which may be caused by the fact that doc-
uments further down the retrieved list are less rele-
vant, and therefore less likely to contain the relevant
biographic data.
Pre?F
usion P
recisio
n
# Retrieved Documents per Person 80  160 140
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 1
 60 40 20  120 0  100
Birthday
BirthplaceBirthyearOccupationDeathyear
Figure 2: As more documents are retrieved per person, pre-
fusion precision drops.
However, even though the extractor?s precision
drops, the data in Figure 3 indicate that there con-
tinue to be instances of the relevant biographic data.
# Retrieved Documents Per PersonPre
?Fusio
n Pseu
do?Re
call
 1 2 3
 4 5 6
 7 8 9
 10
 0
 0  20  40  60  80  100  120 140
 160
BirthyearBirthdayBirthplaceOccupationDeathyear
Figure 3: Pre-fusion pseudo-recall increases as more documents
are added.
3 Cross-Document Information Fusion
The per-extraction performance was presented in
Section 2, but the final task is to find the single cor-
rect target for each person.12 In this section, we ex-
amine two basic methodologies for combining can-
didate targets. Masterson and Kushmerick (2003)
propose Best which gives each candidate a
score equal to its highest confidence extraction:
Best(x) = argmax
x
C(x).13 We further consider
Voting, which counts the number of times each can-
didate x was extracted: Vote(x) = |C(x) > 0|.
Each of these methods ranks the candidate targets
by score and chooses the top-ranked one.
The experimental setup used in the fusion exper-
iments was the same as before: training on 15 peo-
ple, and testing on 137 people. However, the post-
fusion evaluation differs from the pre-fusion evalua-
tion. After fusion, the system returns one consensus
target for each person and thus the evaluation is on
the accuracy of those targets. That is, missing tar-
12This is a simplifying assumption, since there are many
cases where there might exist multiple possible values, e.g., a
person may be both a writer and a musician.
13C(x) is either the confidence estimate (NB+E) or the prob-
ability score (Rote,CRF,CRF+E).
486
Best Vote
Rote .364 .450
NB+E .385 .588
CRF .513 .624
CRF+E .650 .678
Table 4: Average Accuracy of the Highest Confidence (Best)
and Most Frequent (Vote) across five extraction fields.
gets are graded as wrong.14
Post-Fusion Accuracy = # People with Correct Target
# People
Additionally, since the targets are ranked, we also
calculated the mean reciprocal rank (MRR).15 The
data in Table 4 show the average system perfor-
mance with the different fusion methods. Frequency
voting gave anywhere from a 2% to a 20% improve-
ment over picking the highest confidence candidate.
CRF+E (the CRF trained with negative examples)
was the highest performing system overall.
Birth Day
Fusion Accuracy Fusion MRR
Rote Vote .854 .877
NB+E Vote .854 .889
CRF Vote .650 .703
CRF+E Vote .883 .911
Birth year
Rote Vote .387 .497
NB+E Vote .778 .838
CRF Vote .796 .860
CRF+E Vote .869 .876
Occupation
Rote Vote .299 .405
NB+E Vote .642 .751
CRF Vote .606 .740
CRF+E Vote .423 .553
Birthplace
Rote Vote .321 .338
NB+E Vote .474 .586
CRF Vote .321 .476
CRF+E Vote .467 .560
Year of Death
Rote Vote .389 .389
NB+E Vote .194 .383
CRF .750 .840
CRF+E Vote .750 .827
Table 5: Voting for information fusion, evaluated per person.
CRF+E has best average performance (67.8%).
Table 5 shows the results of using each of these
extractors to extract correct relationships from the
top 150 ranked documents downloaded from the
14For year of death, we only graded cases where the person
had died.
15The reciprocal rank = 1 / the rank of the correct target.
Web. CRF+E was a top performer in 3/5 of the
cases. In the other 2 cases, the NB+E was the most
successful, perhaps because NB+E?s increased re-
call was more useful than CRF+E?s improved pre-
cision.
Retrieval Set Size and Performance
As with pre-fusion, we performed a set of exper-
iments with different retrieval set sizes and used
the CRF+E extraction system trained on 150 docu-
ments per person. The data in Figure 4 show that
performance improves as the retrieval set size in-
creases. Most of the gains come in the first 30 doc-
uments, where average performance increased from
14% (1 document) to 63% (30 documents). Increas-
ing the retrieval set size to 150 documents per person
yielded an additional 5% absolute improvement.
Post?F
usion 
Accur
acy
# Retrieved Documents Per Person 0
 0.1 0.2 0.3
 0.4 0.5 0.6
 0.7 0.8 0.9
 0  20
 40  60  80  100 120 140 160 Occupation
BirthyearBirthdayDeathyearBirthplace
Figure 4: Fusion accuracy increases with more documents per
person
Post-fusion errors come from two major sources.
The first source is the misranking of correct relation-
ships. The second is the case where relevant infor-
mation is not retrieved at all, which we measure as
Post-Fusion Missing = # Missing Targets
# People
The data in Figure 5 suggest that the decrease in
missing targets is a significant contributing factor
to the improvement in performance with increased
document size. Missing targets were a major prob-
lem for Birthplace, constituting more than half the
errors (32% at 150 documents).
4 Cross-Field Bootstrapping
Sections 2 and 3 presented methods for training sep-
arate extractors for particular relationships and for
doing fusion across multiple documents. In this sec-
tion, we leverage data interdependencies to improve
performance.
The method we propose is to bootstrap across
fields and use knowledge of one relationship to im-
prove performance on the extraction of another. For
487
# Retrieved Documents Per PersonPost?F
usion 
Missin
g Targ
ets
 0 0.1 0.2
 0.3 0.4 0.5
 0.6 0.7 0.8
 0.9 1
 20 0  40  60  80  100 120 140 160
BirthplaceOccupationDeathyearBirthdayBirthyear
Figure 5: Additional documents decrease the number of post-
fusion missing targets, targets which are never extracted in any
document.
Birth year
Extraction Precision Fusion Accuracy
CRF .342 .797
+ birthday .472 .861
CRF+E .654 .869
+ birthday .809 .891
Occupation
Extraction Precision Fusion Accuracy
CRF .219 .606
+ birthday .217 .569
+ birth year(f) 21.9 .599
+ all .214 .591
CRF+E .246 .423
+ birthday .325 .577
+ birth year(f) .387 .672
+ all .382 .642
Birthplace
Extraction Precision Fusion Accuracy
CRF .139 .321
+ birthday .158 .372
+ birth year(f) .156 .350
CRF+E .357 .467
+ birthday .350 .474
+ birth year(f) .294 .350
+ occupation(f) .314 .354
+ all .362 .532
Table 6: Performance of Cross-Field Bootstrapping Models.
(f) indicates that the best fused result was taken. birth year(f)
means birth years were annotated using the system that discov-
ered the most accurate birth years.
example, to extract birth year given knowledge of
the birthday, in training we mark up each hook cor-
pus Dx with the known birthday b : birthday(x, b)
and the target birth year y : birthyear(x, y) and
add an additional feature to the CRF that indicates
whether the birthday has been seen in the sentence.16
In testing, for each hook, we first find the birthday
using the methods presented in the previous sec-
tions, annotate the corpus with the extracted birth-
day, and then apply the birth year CRF (see Figure 6
next page).
16The CRF state model doesn?t change. When bootstrapping
from multiple fields, we add the conjunctions of the fields as
features.
Table 6 shows the effect of using this bootstrapped
data to estimate other fields. Based on the relative
performance of each of the individual extraction sys-
tems, we chose the following schedule for perform-
ing the bootstrapping: 1) Birthday, 2) Birth year, 3)
Occupation, 4) Birthplace. We tried adding in all
knowledge available to the system at each point in
the schedule.17 There are gains in accuracy for birth
year, occupation and birthplace by using cross-field
bootstrapping. The performance of the plain CRF+E
averaged across all five fields is 67.4%, while for the
best bootstrapped system it is 74.6%, a gain of 7%.
Doing bootstrapping in this way improves for
people whose information is already partially cor-
rect. As a result, the percentage of people who
have completely correct information improves to
37% from 13.8%, a gain of 24% over the non-
bootstrapped CRF+E system. Additionally, erro-
neous extractions do not hurt accuracy on extraction
of other fields. Performance in the bootstrapped sys-
tem for birthyear, occupation and birth place when
the birthday is wrong is almost the same as perfor-
mance in the non-bootstrapped system.
5 Training Set Size Reduction
One of the results from Section 2 is that lower
ranked documents are less likely to contain the rel-
evant biographic information. While this does not
have an dramatic effect on the post-fusion accuracy
(which improves with more documents), it suggests
that training on a smaller corpus, with more relevant
documents and more sentences with the desired in-
formation, might lead to equivalent or improved per-
formance. In a final set of experiments we looked at
system performance when the extractor is trained on
fewer than 150 documents per person.
The data in Figure 7 show that training on 30 doc-
uments per person yields around the same perfor-
mance as training on 150 documents per person. Av-
erage performance when the system was trained on
30 documents per person is 70%, while average per-
formance when trained on 150 documents per per-
son is 68%. Most of this loss in performance comes
from losses in occupation, but the other relationships
17This system has the extra knowledge of which fused
method is the best for each relationship. This was assessed by
inspection.
488
Frank Zappa was born on December 21.
1. Birthday
Zappa : December 21, 1940.
2. Birthyear1. Birthday
2. Birthyear 3. Birthplace
Zappa was born in 1940 in Baltimore. 
Figure 6: Cross-Field Bootstrapping: In step (1) The birthday,
December 21, is extracted and the text marked. In step 2, cooc-
currences with the discovered birthday make 1940 a better can-
didate for birthyear. In step (3), the discovered birthyear ap-
pears in contexts where the discovered birthday does not and
improves extraction of birth place.
Post?F
usion 
Accur
acy
# Training Documents Per Person 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
 0
 20
 40  60  80  100  120 140 160
BirthdayBirthyearDeathyearOccupationBirthplace
Figure 7: Fusion accuracy doesn?t improve with more than 30
training documents per person.
have either little or no gain from training on addi-
tional documents. There are two possible reasons
why more training data may not help, and even may
hurt performance.
One possibility is that higher ranked retrieved
documents are more likely to contain biographical
facts, while in later documents it is more likely that
automatically annotated training instances are in fact
false positives. That is, higher ranked documents are
cleaner training data. Pre-Fusion precision results
(Figure 8) support this hypothesis since it appears
that later instances are often contaminating earlier
models.
Pre?F
usion 
Precis
ion
# Training Documents Per Person 0
 0.1 0.2 0.3
 0.4 0.5 0.6
 0.7 0.8
 0  20  40  60  80  100 120 140 160
BirthdayBirthyearBirthplace
OccupationDeathyear
Figure 8: Pre-Fusion precision shows slight drops with in-
creased training documents.
The data in Figure 9 suggest an alternate possibil-
ity that later documents also shift the prior toward
a model where it is less likely that a relationship is
observed as fewer targets are extracted.
Pre?F
usion 
Pseud
o?Rec
all
# Training Documents Per Person 0
 1 2 3
 4 5 6
 7 8 9
 10 11
 0
 20  40
 60  80  100  120 140 160
Birthday
BirthplaceDeathyear
Birthyear
Occupation
Figure 9: Pre-Fusion Pseudo-Recall also drops with increased
training documents.
6 Related Work
The closest related work to the task of biographic
fact extraction was done by Cowie et al (2000) and
Schiffman et al (2001), who explore the problem of
biographic summarization.
There has been rather limited published
work in multi-document information extrac-
tion. The closest work to what we present here is
Masterson and Kushmerick (2003), who perform
multi-document information extraction trained on
manually annotated training data and use Best
Confidence to resolve each particular template slot.
In summarizarion, many systems have examined
the multi-document case. Notable systems are
SUMMONS (Radev and McKeown, 1998) and
RIPTIDE (White et al, 2001), which assume per-
fect extracted information and then perform closed
domain summarization. Barzilay et al (1999) does
not explicitly extract facts, but instead picks out
relevant repeated elements and combines them to
obtain a summary which retains the semantics of
the original.
In recent question answering research, informa-
tion fusion has been used to combine multiple
candidate answers to form a consensus answer.
Clarke et al (2001) use frequency of n-gram occur-
rence to pick answers for particular questions. An-
other example of answer fusion comes in (Brill et
al., 2001) which combines the output of multiple
question answering systems in order to rank an-
swers. Dalmas and Webber (2004) use a WordNet
cover heuristic to choose an appropriate location
from a large candidate set of answers.
There has been a considerable amount of work in
training information extraction systems from anno-
tated data since the mid-90s. The initial work in the
field used lexico-syntactic template patterns learned
using a variety of different empirical approaches
(Riloff and Schmelzenbach, 1998; Huffman, 1995;
489
Soderland et al, 1995). Seymore et al (1999) use
HMMs for information extraction and explore ways
to improve the learning process.
Nahm and Mooney (2002) suggest a method to
learn word-to-word relationships across fields by do-
ing data mining on information extraction results.
Prager et al (2004) uses knowledge of birth year to
weed out candidate years of death that are impos-
sible. Using the CRF extractors in our data set,
this heuristic did not yield any improvement. More
distantly related work for multi-field extraction sug-
gests methods for combining information in graphi-
cal models across multiple extraction instances (Sut-
ton et al, 2004; Bunescu and Mooney, 2004) .
7 Conclusion
This paper has presented new experimental method-
ologies and results for cross-document information
fusion, focusing on the task of biographic fact ex-
traction and has proposed a new method for cross-
field bootstrapping. In particular, we have shown
that automatic annotation can be used effectively
to train statistical information extractors such Na??ve
Bayes and CRFs, and that CRF extraction accuracy
can be improved by 5% with a negative example
model. We looked at cross-document fusion and
demonstrated that voting outperforms choosing the
highest confidence extracted information by 2% to
20%. Finally, we introduced a cross-field bootstrap-
ping method that improved average accuracy by 7%.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 85?94.
R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Informa-
tion fusion in the context of multi-document summarization.
In Proceedings of ACL, pages 550?557.
E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. 2001. Data-
intensive question answering. In Proceedings of TREC,
pages 183?189.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT?98, pages
172?183.
R. Bunescu and R. Mooney. 2004. Collective information ex-
traction with relational markov networks. In Proceedings of
ACL, pages 438?445.
C. L. A. Clarke, G. V. Cormack, and T. R. Lynam. 2001. Ex-
ploiting redundancy in question answering. In Proceedings
of SIGIR, pages 358?365.
J. Cowie, S. Nirenburg, and H. Molina-Salgado. 2000. Gener-
ating personal profiles. In The International Conference On
MT And Multilingual NLP.
T. Dalmas and B. Webber. 2004. Information fusion
for answering factoid questions. In Proceedings of 2nd
CoLogNET-ElsNET Symposium. Questions and Answers:
Theoretical Perspectives.
D. Freitag and A. McCallum. 1999. Information extraction
with hmms and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction,
pages 31?36.
S. B. Huffman. 1995. Learning information extraction patterns
from examples. In Working Notes of the IJCAI-95 Workshop
on New Approaches to Learning for Natural Language Pro-
cessing, pages 127?134.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, pages 282?
289.
T. R. Leek. 1997. Information extraction using hidden markov
models. Master?s Thesis, UC San Diego.
D. Masterson and N. Kushmerick. 2003. Information ex-
traction from multi-document threads. In Proceedings of
ECML-2003: Workshop on Adaptive Text Extraction and
Mining, pages 34?41.
A. McCallum. 2002. Mallet: A machine learning for language
toolkit.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 60?67.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings of ACL, pages 574?581.
D. R. Radev and K. R. McKeown. 1998. Generating natural
language summaries from multiple on-line sources. Compu-
tational Linguistics, 24(3):469?500.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical ap-
proach to conceptual case frame acquisition. In Proceedings
of WVLC, pages 49?56.
E. Riloff. 1996. Automatically Generating Extraction Patterns
from Untagged Text. In Proceedings of AAAI, pages 1044?
1049.
B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Produc-
ing biographical summaries: Combining linguistic knowl-
edge with corpus statistics. In Proceedings of ACL, pages
450?457.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999. Learning
hidden markov model structure for information extraction.
In AAAI?99 Workshop on Machine Learning for Information
Extraction, pages 37?42.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995.
CRYSTAL: Inducing a conceptual dictionary. In Proceed-
ings of IJCAI, pages 1314?1319.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004. Dy-
namic conditional random fields: factorize probabilistic
models for labeling and segmenting sequence data. In Pro-
ceedings of ICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce, and
K. Wagstaff. 2001. Multi-document summarization via in-
formation extraction. In Proceedings of HLT.
490
Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
Eric Breck?, Marc Light?, Gideon S. Mann?, Ellen Riloff?,
Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen?
? The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
? Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
? School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
? Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
? Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
? Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
Abstract
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
1 Introduction
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children?s reading levels (Charniak et al,
2000; Hirschman et al, 1999; Ng et al, 2000;
Riloff and Thelen, 2000; Wang et al, 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap1 to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
1Throughout the text, we use ?overlap? to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
2 The data
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al, 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
3 Analyzing the number of answer
opportunities per question
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
?What does the Peugeot company manufacture??
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7
# Answers
%
 Q
ue
st
io
ns
TREC-8
5 0
3 5 2
7.04
3
12.94
CBC
2 1 9
2 7 4
1.25
1
0.61
# Questions
# Answers
Mean
Median
Standard Dev.
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an ?answer? as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 0 2 0 3 0 4 0 5 0 6 0 7 0
# answers occurences per question
%
 o
f s
ys
te
m
s 
w
ith
 a
t l
ea
st
 o
ne
 c
or
re
ct
 re
sp
on
se
Point per question
Mean correct per occurrence #
Figure 2: Answer repetition vs. system response
correctness for TREC-8
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
4 Graphs for analyzing scoring
functions of answer candidates
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=x | correct) and
Pr(overlap=x | incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 2 4 6 8 10 12 14 16 18 20N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
overlap
incorrect
correct
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
overlap
incorrect
correct
Figure 4: Pr(overlap=x|[in]correct) for CBC
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate?s score
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=x | correct) and
Pr(rank=x | incorrect) on the y-axis. The top-
ranked candidate has rank=0.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-
10
00
-
90
0
-
80
0
-
70
0
-
60
0
-
50
0
-
40
0
-
30
0
-
20
0
-
10
0 0
N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
ranked overlap
incorrect
correct
Figure 5: Pr(rank=x | [in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
-45 -40 -35 -30 -25 -20 -15 -10 -5 0No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
ranked overlap
incorrect
correct
Figure 6: Pr(rank=x | [in]correct) for CBC
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al, 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
0 2 4 6 8 10 12 14
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
lo
g-
od
ds
 o
f c
or
re
ct
ne
ss
m
a
ss
overlap
log-odds
mass curve
Figure 7: TREC-8 log odds correct given overlap
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
log Pr(correct|overlap)Pr(incorrect|overlap)
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
?mass,? plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
5 Bounds on scoring functions that use
term overlap
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small ? conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
could differentiate them. In addition, subset rela-
tions often hold between overlaps. A candidate
whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
scheme.7 We formalize these overlap set relations
and then calculate statistics based on them for the
CBC and TREC data.
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
MaxOsets : ( {S2, S4}, {S3} )
Figure 8: Example of Overlap Sets from CBC
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {S2, S4} or
{S3}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {S2, S4} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
7Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
ow,q = {s|s ? q = w}
?q = all unique overlap sets for q
maximal(ow,q) if ?ov,q ? ?q, w 6? v
Mq = {ow,q ? ?q | maximal(ow,q)}
Cq = {s|s correctly answers q}
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the ?best? maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
max = |{q|?o ? Mq,?s ? o s.t. s ? Cq}||Q|
min = |{q|?o ? Mq,?s ? o s ? Cq}||Q|
exp. max = 1|Q| ?
?
q?Q
max
o?Mq
|{s ? o and s ? Cq}|
|o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
Table 1: Maximum overlap analysis of scores
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in ?there is always a chance to get it right?
are also in the class ?there may be a chance to
get it right.?) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
6 Analyzing the effect of multiple
answer type occurrences in a sentence
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(?ow ? Mq, ?s ? ow, s ? Cq)
There is always a chance to get it right 45 7%
(?ow ? Mq, ?s ? ow s.t. s ? Cq)
There may be a chance to get it right 310 48%
(?ow ? Mq s.t. ?s ? ow s.t. s ? Cq)
The wrong answers will always be weighted too highly 137 21%
(?ow ? Mq, ?s ? ow, s 6? Cq)
There are no correct answers with any overlap with Q 66 10%
(?s ? d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(?s ? d, s is incorrect)
Table 2: Maximal Overlap Set Analysis for CBC data
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
?answer confusion? remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
TREC CBC
Answer Type Score Freq Score Freq
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
Figure 9: Sentences with Multiple Items of the
Same Type
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
7 Conclusion
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
References
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
  	
 Fine-Grained Proper Noun Ontologies for Question Answering
Gideon S. Mann
Department of Computer Science
Johns Hopkins University
Baltimore, Maryland 21218
gsm@cs.jhu.edu
Abstract
The WordNet lexical ontology, which is
primarily composed of common nouns,
has been widely used in retrieval tasks.
Here, we explore the notion of a fine-
grained proper noun ontology and argue
for the utility of such an ontology in re-
trieval tasks. To support this claim, we
build a fine-grained proper noun ontol-
ogy from unrestricted news text and use
this ontology to improve performance on
a question answering task.
1 Introduction
The WordNet lexical ontology (Miller, 1990) con-
tains more than 100,000 unique noun forms. Most of
these noun forms are common nouns (nouns describ-
ing non-specific members of a general class, e.g.
?detective?). Only a small percentage1 of the nouns
in WordNet are proper nouns (nouns describing spe-
cific instances, e.g. ?[the detective] Columbo?).
The WordNet ontology has been widely useful,
with applications in information retrieval (Sussna,
1993), text classification (Scott and Matwin, 1998),
and question answering (Pasca and Harabagiu,
2001). These successes have shown that common
noun ontologies have wide applicability and utility.
There exists no ontology with similar coverage
and detail for proper nouns. Prior work in proper
noun identification has focused on ?named entity?
1A random 100 synset sample was composed of 9% proper
nouns.
recognition (Chinchor et al, 1999), stemming from
the MUC evaluations. In this task, each proper noun
is categorized, for example, as a PERSON, a LOCA-
TION, or an ORGANIZATION.
These coarse categorizations are useful, but more
finely grained classification might have additional
advantages. While Bill Clinton is appropriately
identified as a PERSON, this neglects his identity as
a president, a southerner, and a saxophone player.
If an information request identifies the object of the
search not merely as a PERSON, but as a typed
proper noun (e.g. ?a southern president?), this pref-
erence should be used to improve the search.
Unfortunately, building a proper noun ontology
is more difficult than building a common noun on-
tology, since the set of proper nouns grows more
rapidly. New people are born. As people change,
their classification must change as well. A broad-
coverage proper noun ontology must be constantly
updated. Therefore, to propose a viable system, a
method, however limited, must be presented to build
a proper noun ontology.
In this paper, we explore the idea of a fine-grained
proper noun ontology and its use in question answer-
ing. We build a proper noun ontology from unre-
stricted text using simple textual co-occurrence pat-
terns (Section3). This automatically constructed on-
tology is then used on a question answering task to
give preliminary results on the utility of this infor-
mation (Section 4).
2 Ontologies for Question Answering
Modern question answering systems rely heavily on
the fact that questions contain strong preferences for
The 1974 film ?That?s Entertainment!? was made from film clips from what Hollywood studio?
What king of Babylonia reorganized the empire under the Code that bears his name?
What rock ?n? roll musician was born Richard Penniman on Christmas Day?
What is the oldest car company which still exists today?
What was the name of the female Disco singer who scored with the tune ?Dim All the Lights? in 1979?
What was the name of the first Russian astronaut to do a spacewalk?
What was the name of the US helicopter pilot shot down over North Korea?
Which astronaut did Tom Hanks play in ?Apollo 13??
Which former Klu Klux Klan member won an elected office in the U.S.?
Who?s the lead singer of the Led Zeppelin band?
Who is the Greek goddess of retribution or vengeance?
Who is the prophet of the religion of Islam?
Who is the author of the book, ?The Iron Lady: A Biography of Margaret Thatcher??
Who was the lead actress in the movie ?Sleepless in Seattle??
Table 1: Questions Indicating a Typed Proper Noun Preference (Trivia and Trec-8/9 Questions)
the types of answers they expect. Kupiec (1993) ob-
serves that the WH word itself provides preferences
(e.g. ?Who? questions prefer PERSON answers).
He further observes that questions also include type
preferences in other parts of the question. Some-
times these preferences occur within the WH phrase
(?what color?), and sometimes they are embedded
elsewhere within the question (?what is the color
...?). In both, the question indicates a preference for
colors as answers.
Current question answering systems use ontolo-
gies when these type preferences are detected. One
simple method is as follows: when a type preference
is recognized, the preference is located within the
WordNet ontology, and children of that synset are
treated as potential answers. Given the question ?In
pool, what color is the eight ball??, and the ontol-
ogy excerpt shown in Figure 1, the system can nar-
row down the range of choices. This approach has
high precision: if the type preference can be located,
and a candidate answer is found in a child node (in a
suitable corpus context), then the candidate is likely
to be the answer.
Harabagiu et al (2000) proposes another method
for using an ontology: WordNet subtrees are linked
to types recognized by a named entity recognizer.
Their system works as follows: given the question
?What is the wingspan of a condor??, it locates
?wingspan? in the WordNet ontology. It then detects
that ?wingspan? falls into the MAGNITUDE subtree
which is linked to the QUANTITY type. This links
words in the MAGNITUDE subtree to numbers.
While the WordNet ontology is primarily com-
posed of common nouns, it contains some proper
nouns, typically those least likely to be ephemeral
(e.g. countries, cities, and famous figures in his-
tory). These can be used as any other common
nouns are used. Given the question ?Which com-
poser wrote ?The Marriage of Figaro???, the Word-
Net ontology will provide the fact that ?Wolfgang
Amadeus Mozart? is a composer.
Table 1 lists sample questions where a proper
noun ontology would be useful. Some of the proper
noun types are relatively static (Greek gods, kings
of Babylonia). Other categories are more ephemeral
(lead singers, British actresses). WordNet enumer-
ates 70 Greek gods and 80 kings, but no lead singers
and no British actresses.
Ravichandran and Hovy (2002) present an alter-
native ontology for type preference and describe a
method for using this alternative ontology to extract
particular answers using surface text patterns. Their
proposed ontology is orders of magnitude smaller
than WordNet and ontologies considered here, hav-
ing less than 200 nodes.
3 Building a Proper Noun Ontology
In order to better answer the questions in Table 1, we
built a proper noun ontology from approximately 1
gigabyte of AP news wire text. To do so, we tok-
Answer white
WordNet
black grey
chromatic colorachromatic color
red blue pink
color
Preference
Type
Figure 1: Using WordNet to Directly Provide Type
Preferences
distance, length
light time altitude
quantity
Answer 7
wingspan
Named Entity Recognizer
WordNet
size
magnitude
amount
Preference
Type
Figure 2: Linking WordNet subtrees to a Named En-
tity Recognizer
enized and part-of-speech tagged the text, and then
searched for instances of a common noun followed
immediately by a proper noun. This pattern de-
tects phrases of the form ?[the] automaker Mercedes
Benz?, and is ideally suited for proper nouns. In AP
news wire text this is a productive and high preci-
sion pattern, generating nearly 200,000 unique de-
scriptions, with 113,000 different proper nouns and
20,000 different descriptions. In comparison, the
?such as? pattern (Section 5) occurs less than 50,000
times in the same size corpora. Table 2 shows the
descriptions generated for a few proper nouns using
this simple pattern.
To assess the precision of the extractions, we took
a sample of 100 patterns extracted from the AP-news
text. From these 100, 79 of the items classified as
named entities were in fact named entities, and out
of those, 60 (75%) had legitimate descriptions.
Singer
Folk Singer
Emmanuel Charlemagne
Burl Ives
Hou Dejian
Joan Baez
John Denver
Lead Singer
Axel Rose
Marjo Leinonen
John Fogerty
Jim Morrison
Bono
Figure 3: Subset of ?singer? subtree in the Induced
Proper Noun Ontology
To build the complete ontology, first each descrip-
tion and proper noun forms its own synset. Then,
links are added from description to each proper noun
it appears with. Further links are put between de-
scriptions ?X Y? and ?Y? (noun compounds and
their heads). Clearly, this method is problematic in
the cases of polysemous words or complex noun-
noun constructions (?slalom king?) and integrating
this ontology with the WordNet ontology requires
further study.
This proper noun ontology fills many of the holes
in WordNet?s world knowledge. While WordNet has
no lead singer synset, the induced proper noun on-
tology detects 13 distinct lead singers (Figure 3).
WordNet has 2 folk singers; the proper noun ontol-
ogy has 20. In total, WordNet lists 53 proper nouns
as singers, while the induced proper noun ontology
has more than 900. While the induced ontology is
not complete, it is more complete than what was pre-
viously available.
As can be seen from the list of descriptions gener-
ated by this pattern, people are described in a variety
of different ways, and this pattern detects many of
them. Table 3 shows the descriptions generated for
a common proper noun (?Bill Gates?). When the
descriptions are grouped by WordNet synsets and
senses manually resolved, the variety of descriptions
decreases dramatically (Figure 4). ?Bill Gates? can
be described by a few distinct roles, and a distribu-
tion over these descriptions provide an informative
understanding: leader (.48), businessperson (.27),
worker (.05), originator (.05), expert (.05), and rich
billionaire
rich person expert
whizhead boss
leader
mogul entrepreneur
creatorcapitalist worker 
orginatorskilled workerbusinessperson
officialchairman executive founder
officer
presiding officer
person
pioneer
Figure 4: Descriptions of Bill Gates Organized into WordNet, observed descriptions boxed
Proper Noun Count Description
Axel Rose 3 singer
2 lead singer
2 vocalist
Emma Thompson 3 actress
Mercedes-Benz 4 Luxury car maker
4 car maker
3 automaker
2 family
2 luxury
1 gold
1 service
1 subsidiary
Table 2: Proper Noun Descriptions Extracted from
News Corpora
person (.02). Steve Jobs, who has a career path sim-
ilar to Bill Gates, has a similar but distinct signature:
originator (.6), expert (.4).
One immediate observation is that some of the
descriptions may be more relevant than others. Is
Gates? role as an ?office worker? as important as his
role as a ?billionaire?? The current system makes no
decision and treats all descriptions as equally rele-
vant and stores all of them. There is no need to re-
ject descriptions since there is no human cost in su-
perfluous or distracting descriptions (unlike in sum-
marization tasks). It is important that no invalid de-
scriptions are added.
The previous examples have focused on proper
nouns which are people?s names. However, this
method works for many organizations as well, as
Proper Noun Count Description
Bill Gates 15 chairman
9 mogul, tycoon,magnate
2 officer
2 whiz, genius
1 pioneer
1 head
1 founder
1 executive
1 entrepreneur
1 boss
1 billionaire
Table 3: Bill Gates Descriptions in AP Newswire,
grouped by WordNet synset
the data in Table 2 show. However, while descrip-
tion extraction for people is high quality (84% cor-
rect descriptions in a 100 example sample), for non-
people proper names, the quality of extraction is
poorer (47% correct descriptions). This is a trend
which requires further study.
4 Using a Proper Noun Ontology in a
Question Answering Task
We generated the above ontology and used it in a
sentence comprehension task: given a question and
a sentence which answers the question, extract the
minimal short answer to the question from the sen-
tence. The task is motivated by the observation that
extracting short answers is more difficult than ex-
tracting full sentence or passage length ones. Fur-
Ontology Correct Total Precision
Answered
WordNet 127 169 75.1
IPNO 46 67 68.6
WN + IPNO 145 194 74.7
Table 4: Performance on a Test Corpus when an In-
duced Proper Noun Ontology (IPNO) is combined
with Wordnet
thermore, retrieving answers from smaller document
spaces may be more difficult than retrieving answers
from larger ones, if smaller spaces have less redun-
dant coverage of potential answers. In this sen-
tence comprehension task, there is virtually no re-
dundancy. To generate data for this task, we took
trivia games, which, along with the question, had a
full sentence explanation (Mann, 2002).
Baseline experiments used the WordNet ontology
alone. From a semantic type preference stated in
the question, a word was selected from the sentence
as an answer if was a child of the type preference.
?Black? would be picked as an answer for a ?color?
type preference (Figure 1).
To utilize the induced proper noun ontology, we
took the raw data and selected the trailing noun for
each proper noun and for each description. Thus,
for an extraction of the form ?computer mogul Bill
Gates?, we added a pattern of the form ?Gates
mogul?. We created an ontology from these in-
stances completely separate from the WordNet on-
tology.
We put this induced proper noun ontology into
the pipeline as follows: if WordNet failed to find a
match, we used the induced proper noun ontology. If
that ontology failed to find a match, we ignored the
question. In a full system, a named entity recognizer
might be added to resolve the other questions.
We selected 1000 trivia game questions at random
to test out the new two-ontology system. Table 4
shows the results of the experiments. The boost is
clear: improved recall at slightly decreased preci-
sion. Gains made by inducing an ontology from an
unrestricted text corpus (newstext) and applying it to
a unmatched test set (trivia games), suggests that a
broad-coverage general proper noun ontology may
be useful.
It is further surprising that this improvement
comes at such a small cost. The proper noun on-
tology wasn?t trimmed or filtered. The only disad-
vantage of this method is simply that its coverage
is small. Coverage may be increased by using ever
larger corpora. Alternatively, different patterns (for
example, appositives) may increase the number of
words which have descriptions. A rough error anal-
ysis suggests that most of the errors come from mis-
tagging, while few come from correct relationships
in the ontology. This suggests that attempts at noise
reduction might be able to lead to larger gains in per-
formance.
Another potential method for improving coverage
is by bootstrapping descriptions. Our test corpus
contained a question whose answer was ?Mercedes-
Benz?, and whose type preference was ?car com-
pany?. While our proper noun ontology contained
a related link (Mercedes-Benz automaker), it did
not contain the exact link (Mercedes-Benz car com-
pany). However, elsewhere there existed the links
(Opel automaker) and (Opel car company). Poten-
tially these descriptions could be combined to infer
(Mercedes-Benz car company). Formally :
(B Y) and (A Y) and (A Z)   (B Z)
(Mercedes-Benz automaker) and (Opel
automaker) and (Opel car company)  
(Mercedes-Benz car company)
Expanding descriptions using a technique like this
may improve coverage. Still, care must be taken
to ensure that proper inferences are made since this
rule is not always appropriate. Bill Gates is a ten-
billionaire; Steve Jobs isn?t.
5 Prior Work in Building Ontologies
There has been considerable work in the past
decade on building ontologies from unrestricted
text. Hearst (1992) used textual patterns (e.g. ?such
as?) to identify common class members. Cara-
ballo and Charniak (1999) and Caraballo (1999)
augmented these lexical patterns with more gen-
eral lexical co-occurrence statistics (such as rel-
ative entropy). Berland and Charniak (1999) use
Hearst style techniques to learn meronym relation-
ships (part-whole) from corpora. There has also
been work in building ontologies from structured
Correct Answer Question
(Debbie) Reynolds What actress once held the title of ?Miss Burbank??
(Jim) Lovell Which astronaut did Tom Hanks play in ?Apollo 13??
Xerxes Which Persian king moved an invasion force across the
Hellespont on a bridge of ships?
(Donna) Summer What was the name of the female Disco singer
who scored with the tune ?Dim All the Lights? in 1979?
MGM The 1974 film ?That?s Entertainment!? was made from film
clips from what Hollywood studio?
Table 5: Successes of the Proper Noun Ontology for the Question Answering task
text, notably in the AQUILEX project (e.g. Copes-
take, 90) which builds ontologies from machine
readable dictionaries.
The most closely related work is (Girju, 2001),
which describes a method for inducing a domain-
specific ontology using some of the techniques de-
scribed in the previous paragraph. This induced on-
tology is then potential useful for a matched ques-
tion domain. Our paper differs in that it targets
proper nouns, in particular people, which are over-
looked in prior work, have broad applicability, and
can be used in a cross-domain fashion. Furthermore,
we present initial results which attempt to gauge
coverage improvement as a result of the induced on-
tology.
Another related line of work is word clustering.
In these experiments, the attempt is made to cluster
similar nouns, without regard to forming a hierarchy.
Pereira et al (1993) presented initial work, cluster-
ing nouns using their noun-verb co-occurrence in-
formation. Riloff and Lehnert (1993) build seman-
tic lexicons using extraction pattern co-occurrence.
Lin and Pantel (2001) extend these methods by us-
ing many different types of relations and exploiting
corpora of tremendous size.
The important difference for this work between
the hierarchical methods and the clustering meth-
ods is that clusters are unlabelled. The hierarchi-
cal methods can identify that a ?Jeep Cherokee? is a
type of car. In contrast, the clustering methods group
together related nouns, but exactly what the connec-
tion is may be difficult to distinguish (e.g. the clus-
ter ?Sierra Club?, ?Environmental Defense Fund?,
?Natural Resources Defense Council?, ?Public Cit-
izen?, ?National Wildlife Federation?). Generating
labels for proper noun clusters may be another way
to build a proper noun ontology.
The method we use to build the fine-grained
proper name ontology also resembles some of the
work done in coarse-grained named entity recogni-
tion. In particular, Collins and Singer (1999) present
a sophisticated method for using bootstrapping tech-
niques to learn the coarse-classification for a given
proper noun. Riloff and Jones (1999) also present a
method to use bootstrapping to create semantic lexi-
cons of proper nouns. These methods may be appli-
cable for use in fine-grained proper noun ontology
construction as well.
Schiffman et al (2001) describe work on produc-
ing biographical summaries. This work attempts to
synthesize one description of a person from multi-
ple mentions. This summary is an end in itself, as
opposed to general knowledge collected. These de-
scriptions also attempt to be parsimonious in con-
trast to the rather free associations extracted by the
method presented above.
6 Conclusions
In this paper we have motivated the use of a proper
noun ontology for question answering. We de-
scribed a method for inducing pieces of this on-
tology, and then showed preliminary methods can
be useful. Prior work on proper nouns has fo-
cused on classifying them into very coarse cate-
gories (e.g. PERSON, LOCATION). As this paper
has shown, these coarse classifications can be re-
fined fortuitously, especially for the PERSON type.
This paper demonstrates that inducing a gen-
eral ontology improves question answering perfor-
mance. Previous work examined ontology induction
for a specialized domain. It is somewhat surprising
that an ontology built from unrestricted text can lead
to improvement on unmatched questions.
The experiments we performed demonstrated that
though the precision of the ontology is high, the cru-
cial problem is increasing coverage. Tackling this
problem is an important area of future work. Fi-
nally, this work opens up a potential new avenue for
work on inducing proper noun ontologies. There are
doubtlessly many more ways to extract descriptions
and to improve coverage.
References
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, pages 57?64.
S. Caraballo and E. Charniak. 1999. Determining the
specificity of nouns from text.
Sharon Caraballo. 1999. Automatic acquisition of a
hypernym-labeled noun hierarchy from text. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
N. Chinchor, E. Brown, L. Ferro, and P. Robinson. 1999.
1999 named entity recognition task definition. Tech
Report.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing.
Ann Copestake. 1990. An approach to building the hi-
erarchical element of a lexical knowledge base from
a machine readable dictionary. In First International
Workshop on Inheritance in NLP.
Roxana Girju. 2001. Answer fusion with on-line on-
tology development. In Student Research Workshop
Proceedings at The 2nd Meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdea nu, R. Bunescu, R. Girju, V. Rus, and
P. Mor. 2000. Falcon : Boosting knowledge for an-
swer engines. Proc. of TREC-9.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. Proceedings of the Fourteenth
International Conference on Computational Linguis-
tics (COLING-92).
J. Kupiec. 1993. Murax: A robust linguistic approach
for question answering using an on-line encyclopedia.
In ACM-SIGIR?93.
Dekang Lin and Patrick Pantel. 2001. Induction of se-
mantic classes from natural language text. In Proceed-
ings of ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, pages 317?322.
Gideon S. Mann. 2002. Learning how to answer ques-
tions using trivia games. In Proceedings of the Nine-
teenth International Conference on Computational
Linguistics (COLING 2002).
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4):235?312.
Marius Pasca and Sanda Harabagiu. 2001. The informa-
tive role of wordnet in open-domain question answer-
ing. In Proceedings of the NAACL 2001 Workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, pages 138?143. Asso-
ciation for Computational Linguistics.
Fernando C. N. Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Meeting of the Association for Computational Linguis-
tics, pages 183?190.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the Sixteenth National COn-
ference on Artificial Intelligence, pages 1044?1049.
E. Riloff and W. Lehnert. 1993. Automated Dictionary
Construction for Information Extraction from Text. In
Proceedings of the Ninth IEEE Conference on Artifi-
cial Intelligence for Applications, pages 93?99, Los
Alamitos, CA. IEEE Computer Society Press.
Barry Schiffman, Inderjeet Mani, and Kristian J. Concep-
cion. 2001. Producing biographical summaries: Com-
bining linguistic knowledge with corpus statistics. In
Proceedings of the 39th Annual Meeting of the Associ-
ation for Computational Linguistics.
Sam Scott and Stan Matwin. 1998. Text classification
using WordNet hypernyms. In Sanda Harabagiu, ed-
itor, Use of WordNet in Natural Language Processing
Systems: Proceedings of the Conference, pages 38?44.
Association for Computational Linguistics, Somerset,
New Jersey.
M. Sussna. 1993. Word sense disambiguation for free-
text indexing using a massive semantic network. In
Proceedings of CIKM ?93.
Bootstrapping toponym classifiers
David A. Smith and Gideon S. Mann
Center for Language and Speech Processing
Computer Science Department, Johns Hopkins University
Baltimore, MD 21218, USA
{dasmith,gsm}@cs.jhu.edu
Abstract
We present minimally supervised methods for
training and testing geographic name disam-
biguation (GND) systems. We train data-driven
place name classifiers using toponyms already
disambiguated in the training text ? by such
existing cues as ?Nashville, Tenn.? or ?Spring-
field, MA? ? and test the system on texts
where these cues have been stripped out and
on hand-tagged historical texts. We experiment
on three English-language corpora of varying
provenance and complexity: newsfeed from the
1990s, personal narratives from the 19th cen-
tury American west, and memoirs and records
of the U.S. Civil War. Disambiguation accu-
racy ranges from 87% for news to 69% for
some historical collections.
1 Scope and Prior Work
We present minimally supervised methods for training
and testing geographic name disambiguation (GND) sys-
tems. We train data-driven place name classifiers using
toponyms already disambiguated in the training text ?
by such existing cues as ?Nashville, Tenn.? or ?Spring-
field, MA? ? and test the system on text where these
cues have been stripped out and on hand-tagged histori-
cal texts.
As in early work with such named-entity recognition
systems as Nominator (Wacholder et al, 1997), much
previous work in GND has relied on heuristic rules (Ol-
ligschlaeger and Hauptmann, 1999; Kanada, 1999) and
such culturally specific and knowledge intensive tech-
niques as postal codes, addresses, and telephone num-
bers (McCurley, 2001). In previous work, we used the
heuristic technique of calculating weighted centroids of
geographic focus in documents (Smith and Crane, 2001).
Sites closer to the centroid were weighted more heavily
than sites far away unless they had some countervailing
importance such as being a world capital.
News texts offer two principal advantages for boot-
strapping geocoding applications. Just as journalistic
style prefers identifying persons by full name and title on
first mention, place names, when not of major cities, are
often first mentioned followed by the name of their state,
province, or country. Even if a toponym is strictly unam-
biguous, it may still be labelled to provide the reader with
some ?backoff? recognition. Although there is only one
place in the world named ?Wye Mills?, an author would
still usually append ?Maryland? to it so that a reader who
doesn?t recognize the place name can still situate it within
a rough area. In any case, the goal is to generalize from
the kinds of contexts in which writers use a disambiguat-
ing label to one in which they do not.
Since news stories also tend to be relatively short and
focused on a single topic, we can also exploit the heuristic
of ?one sense per discourse?: unless otherwise indicated
? e.g., by a different state label ? subsequent mentions
of the toponym in the story can be identified with the first,
unambiguous reference. News stories often also have to-
ponyms in their datelines that are disambiguated. Our
news training corpus consists of two years (1989-90) of
AP wire and two months (October, November, 1998) of
Topic Detection and Tracking (TDT) data. The test set
is the December, 1998, TDT data. See table 1 for the
numbers of toponyms in the corpora.
In contrast to news texts, historical documents exhibit
a higher density of geographical reference and level of
ambiguity. To test the performance of our minimally-
supervised classifiers in a particularly challenging do-
main, we test it on a corpus of historical documents where
all place names have been marked and disambiguated. As
with news texts, we initially train and test our classifiers
on raw text. The range of geographic reference in these
texts is somewhat similar to American news text: the cor-
pus comprises the Personal Memoirs of Ulysses S. Grant
and two nineteenth-century books of travel about Califor-
nia and Minnesota from the Library of Congress? Amer-
ican Memory project.1 In all, we thus have about 600
pages of tagged historical text.
2 Experimental Setup
Dividing the corpora in training and test data, we train
Naive Bayes classifiers on all examples of disambiguated
toponyms in the training set. Although it is not uncom-
mon for two places in the same state, for example, to
share a name, we define disambiguation for purposes of
these experiments as finding the correct U.S. state or for-
eign country. This asymmetry is reflected in U.S. news
and historical text of the training data, where toponyms
are specified by U.S. states or by foreign countries. We
then run the classifiers on the test text with disambiguat-
ing labels, such as state or country names that immedi-
ately follow the city name, removed.
Since not all toponyms in the test set will have been
seen in training, we also train backoff classifiers to guess
the states and countries related to a story. If, for exam-
ple, we cannot find a classifier for ?Oxford?, but can
tell that a story is about Mississippi, we will still be
able to disambiguate. We use a gazetteer to restrict the
set of candidate states and countries for a given place
name. In trying to disambiguate ?Portland?, we would
thus consider Oregon, Maine, and England, among other
options, but not Maryland. As in the word sense dis-
ambiguation task as usually defined, we are classifying
names and not clustering them. This approach is prac-
tical for geographic names, for which broad-coverage
gazetteers exist, though less so for personal names (Mann
and Yarowsky, 2003). System performance is measured
with reference to the naive baseline where each ambigu-
ous toponym is guessed to be the most commonly oc-
curring place. London, England, would thus always
be guessed rather than London, Ontario. Bootstrapping
methods similar to ours have been shown to be compet-
itive in word sense disambiguation (Yarowsky and Flo-
rian, 2003; Yarowsky, 1995).
3 Difficulty of the Task
Our ability to disambiguate place names should be
weighed against the ease or difficulty of the task. In a
world where most toponyms referred unambiguously to
one place, we would not be impressed by near-perfect
performance.
Before considering how toponyms are used in text, we
can examine the inherent ambiguity of place names in
1Our annotated data also includes disambiguated texts of
Herodotus? Histories and Caesar?s Gallic War, but toponyms in
the ancient (especially Greek) world do not show enough ambi-
guity with personal names or with each other to be interesting.
Corpus Train Test Tagged
News 80,366 1464 0
Am. Mem. 11,877 3782 342
Civ. War 59,994 787 4153
Table 1: Experimental corpora with toponym counts in
unsupervised training and test and hand-tagged test sec-
tions.
Continent % places % names
w/mult. names w/mult. places
N. & Cent. America 11.5 57.1
Oceania 6.9 29.2
South America 11.6 25.0
Asia 32.7 20.3
Africa 27.0 18.2
Europe 18.2 16.6
Table 2: Places with multiple names and names applied
to more than one place in the Getty Thesaurus of Geo-
graphic Names
isolation. The Getty Thesaurus of Geographic Names,
with over a million toponyms, not only synthesizes many
contemporary gazetteers but also contains a wealth of his-
torical names. In table 2, we summarize for each conti-
nent the proportion of places that have multiple names
and of names that can refer to more than one place. Al-
though these proportions are dependent on the names
and places selected for inclusion in this gazetteer, the
relative rankings are suggestive. In areas with more
copious historical records?such as Asia, Africa, and
Europe?a place may be called by many names over
time, but individual names are often distinct. With the
increasing tempo of settlement in modern times, how-
ever, many places may be called by the same name, par-
ticularly by nostalgic colonists in the New World. Other
ambiguities arise when people and places share names.
Very few Greek and Latin place names are also personal
names.2 This is less true of Britain, where surnames
(and surnames used as given names) are often taken from
place names; in America, the confusion grows as numer-
ous towns are named after prominent or obscure peo-
ple. What may be called a lack of imagination in the
many 41 Oxfords, 73 Springfields, 91 Washingtons, and
97 Georgetowns seems to plague the very area ? North
America ? covered by our corpora.
If, however, one Washington or Portland predominates
in actual usage, things are not as bad as they seem. At the
2In Herodotus, for example, the only ambiguities between
people and places are for foreign names such as ?Ninus?, the
name used of Nineveh and of its mythical king.
Corpus H(class) H(class|name) % ambig.
News 6.453 0.241 12.71
Am. Mem. 4.519 0.525 18.81
Civ. War 4.323 0.489 18.49
Table 3: Entropy (H) of the state/country classification
task
very worst, for a baseline system, one can always guess
the most predominant referent. We quantify the level of
uncertainty in our corpora using entropy and average con-
ditional entropy. As stated above, we have simplified the
disambiguation problem to finding the state or country to
which a place belongs. For our training corpora, we can
thus measure the entropy of the classification and the av-
erage conditional entropy of the classification given the
specific place name (table 3). These entropies were cal-
culated using unsmoothed relative frequencies. The con-
ditional entropy, not surprisingly, is fairly low, given that
the percentage of toponyms that refer to more than one
place in the training data is quite low. Since training data
do not perfectly predict test data, however, we have to
smooth these probabilities and entropy goes up.
4 Evaluation
We evaluate our system?s performance on geographic
name disambiguation using two tasks. For the first task,
we use the same sort of untagged raw text used in train-
ing. We simply find the toponyms with disambiguating
labels ? e.g., ?Portland, Maine? ?, remove the labels,
and see if the system can restore them from context. For
the second task, we use texts all of whose toponyms have
been marked and disambiguated. The earlier heuristic
system described in (Smith and Crane, 2001) was run on
the texts and all disambiguation choices were reviewed
by a human editor.
Table 4 shows the results of these experiments. The
baseline accuracy was briefly mentioned above: if a to-
ponym has been seen in training, select the state or coun-
try with which it was most frequently associated. If a site
was not seen, select the most frequent state or country
from among the candidates in the gazetteer. The columns
for ?seen? and ?new? provide separate accuracy rates for
toponyms that were seen in training and for those that
were not. Finally, the overall accuracy of the trained sys-
tem is reported. For the American Memory and Civil War
corpora, we report results on the hand-tagged as well as
the raw text.
Not surprisingly, in light of its lower conditional en-
tropy, disambiguation in news text was the most accurate,
at 87.38%. Not only was the system accurate on news text
overall, but it degraded the least for unseen toponyms.
The relative accuracy on the American Memory and Civil
Corpus Baseline Seen New Overall
News 86.36 87.10 69.72 87.38
Am. Mem. 68.48 74.60 46.34 69.57
(tagged) 80.12 91.74 10.61 77.19
Civ. War 78.27 77.23 33.33 78.65
(tagged) 21.94 71.07 9.38 21.82
Table 4: Disambiguation accuracy (%) on test corpora.
Hand-tagged data were available for the American Mem-
ory and Civil War corpora.
War texts is also consistent with the entropies presented
above. The classifier shows a more marked degradation
when disambiguating toponyms not seen in training.
The accuracy of the classifier on restoring states and
countries in raw text is significantly, but not considerably,
higher than the baseline. It seems that many of toponyms
mentioned in text might be only loosely connected to the
surrounding discourse. An obituary, for example, might
mention that the deceased left a brother, John Doe, of Ar-
lington, Texas. Without tagging our test sets to mark such
tangential statements, it would be hard to weigh errors in
such cases appropriately.
Although accuracy on the hand-tagged data from the
American memory corpus was better than for the raw
text, performance on the Civil War tagged data (Grant?s
Memoirs) was abysmal. Most of this error seems came
from toponyms unseen in training, for with the accuracy
was 9.38%. In both sets of tagged text, moreover, the full
classifier performed below baseline accuracy due to prob-
lems with unseen toponyms. The back-off state models
are clearly inadequate for the minute topographical refer-
ences Grant makes in his descriptions of campaigns. In-
cluding proximity to other places mentioned is probably
the best way to overcome this difficulty. These problems
suggest that we need to more robustly generalize from the
kinds of environments with labelled toponyms to those
without.
5 Conclusions
Lack of labelled training or test data is the bane of many
word sense disambiguation efforts. For geographic name
disambiguation, we can extract training and test instances
from contexts where the toponyms are disambiguated by
the document?s author. Tagging accuracy is quite good,
especially for news texts, which have a lower entropy
in the disambiguation task. In real applications, how-
ever, we do not usually need to disambiguate toponyms
that already have state or country labels; we need to dis-
ambiguate unmarked place names. We investigated the
ability of our classifier to generalize by evaluating on
hand-corrected texts with all toponyms marked and dis-
ambiguated. The mixed results show that more gener-
alization power is needed in our models, particularly the
back-off models that handle toponyms unseen in training.
In future work, we hope to try further methods from
WSD such as decision lists and transformation-based
learning on the GND task. In any event, we hope that
this should improve the accuracy on toponyms seen in
training. As for disambiguating unseen toponyms, incor-
porating our prior work on heuristic proximity-base dis-
ambiguation into the probabilistic framework would be a
natural extension. A fully hand-corrected test corpus of
news text would also provide us with more robust evi-
dence for classifier generalization.
Evidence learned by classifiers to disambiguate to-
ponyms includes the names of prominent people and in-
dustries in a particular place, as well as the topics and
dates of current and historical events, and the titles of
newspapers (see figures 1 and 2). In our news training
corpus, for example, Hawaii was most strongly collo-
cated with ?lava? and Poland with ?solidarity? (case was
ignored). In addition to their use for GND, such associa-
tions should be useful in their own right for event detec-
tion (Smith, 2002), personal name disambiguation, and
augmenting the information in gazetteers.
References
[Kanada1999] Yasusi Kanada. 1999. A method of geo-
graphical name extraction from Japanese text for the-
matic geographical search. In Proceedings of the
Eighth International Conference on Information and
Knowledge Management, pages 46?54, Kansas City,
Missouri, November.
[Mann and Yarowsky2003] Gideon S. Mann and David
Yarowsky. 2003. Unsupervised personal name disam-
biguation. In CoNLL, Edmonton, Alberta. (to appear).
[McCurley2001] Kevin S. McCurley. 2001. Geospatial
mapping and navigation of the web. In Proceedings of
the Tenth International WWW Conference, pages 221?
229, Hong Kong, 1?5 May.
[Olligschlaeger and Hauptmann1999] Andreas M. Ol-
ligschlaeger and Alexander G. Hauptmann. 1999.
Multimodal information systems and GIS: The In-
formedia digital video library. In Proceedings of the
ESRI User Conference, San Diego, California, July.
[Smith and Crane2001] David A. Smith and Gregory
Crane. 2001. Disambiguating geographic names in
a historical digital library. In Proceedings of ECDL,
pages 127?136, Darmstadt, 4-9 September.
[Smith2002] David A. Smith. 2002. Detecting and
browsing events in unstructured text. In Proceedings
of the 25th Annual ACM SIGIR Conference, pages 73?
80, Tampere, Finland, August.
[Wacholder et al1997] Nina Wacholder, Yael Ravin, and
Misook Choi. 1997. Disambiguation of proper names
in text. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 202?
208, Washington, DC, April. Association for Compu-
tational Linguistics.
[Yarowsky and Florian2003] David Yarowsky and Radu
Florian. 2003. Evaluating sense disambiguation per-
formance across diverse parameter spaces. Journal of
Natural Language Engineering, 9(1).
[Yarowsky1995] David Yarowsky. 1995. Unsuper-
vised word sense disambiguation rivaling supervised
mehtods. In Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Linguistics,
pages 189?196.
NASHVILLE , Tenn - Singer Marie Osmond will
receive the 1988 Roy Acuff Community Service
Award from the Country Music Foundation. She will
be honored for her work as national chairwoman of
the Osmond Foundation ... The honor is named for a
Grand Ole Opry star known as ?the king of country
music?.
NASHVILLE , Tenn - The home of country music
is singing the blues after the sale of its last locally
owned music publising company to CBS Records.
Tree International Publishing, ranked as Billboard
magazine ?s No. 1 country music publisher for
the last 16 years, is being sold to New York-based
CBS for a reported $45 million to $50 million, The
Tennessean reported today.
NASHVILLE , Tenn - Country music entertainer
Johnny Cash was scheduled to be released from Bap-
tist Hospital Tuesday, two weeks after undergoing
heart bypass surgery, a hospital spokeswoman said
Monday ...
Figure 1: Documents with Dateline of Nashville, having
strong collocation country music
PORTLAND, Ore - Federal court hearing on whether
to permit logging on timber tracts where northern
spotted owl nests.
GRANTS PASS, Ore - ... ?As more and more federal
lands are set aside for spotted owls and other types
of wildlife and recreation areas, the land available
for perpetual commercial timber management de-
creases?...
SEATTLE - Interior Secretary Manuel Lujan says
federal law should allow economic considerations to
be taken into account in deciding whether to protect
species like the northern spotted owl...
SAN FRANCISCO - Environmental groups can sue
the government to try to stop logging of old-growth fir
near spotted owl nests in western Oregon, a federal
appeals court ruled Tuesday...
PORTLAND, Ore - Environmentalists trying to
protect the northern spotted owl cheered a federal
judge?s decision halting logging on five timber tracts...
WASHINGTON - Are the spotted owls that live in the
ancient forest of the Northwest really endangered or
are they being victimized by the miniature radio trans-
mitters that scientists use to track their movements?
SEATTLE - A federal court extended a ban Thursday
on U.S Forest Servi ce plans to sell nearly 1 billion
board feet of ancient timber from nine nationa l forests
in two states where the northern spotted owl lives.
Figure 2: A sample of new stories with the keyword spot-
ted owl, most are Oregon/Washington
Unsupervised Personal Name Disambiguation
Gideon S. Mann and David Yarowsky
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218 USA
{gsm,yarowsky}@cs.jhu.edu
Abstract
This paper presents a set of algorithms for
distinguishing personal names with mul-
tiple real referents in text, based on little
or no supervision. The approach utilizes
an unsupervised clustering technique over
a rich feature space of biographic facts,
which are automatically extracted via a
language-independent bootstrapping pro-
cess. The induced clustering of named
entities are then partitioned and linked to
their real referents via the automatically
extracted biographic data. Performance is
evaluated based on both a test set of hand-
labeled multi-referent personal names and
via automatically generated pseudonames.
1 Introduction
One open problem in natural language ambiguity
resolution is the task of proper noun disambigua-
tion1. While word senses and translation ambigu-
ities may typically have 2-20 alternative meanings
that must be resolved through context, a personal
name such as ?Jim Clark? may potentially refer to
hundreds or thousands of distinct individuals. Each
different referent typically has some distinct contex-
tual characteristics. These characteristics can help
distinguish, resolve and trace the referents when the
surface names appear in online documents.
A search of Google shows 76,000 web pages
mentioning Jim Clark, of which the first 10 unique
referents are:
1This has been recognized even by the popular press.
Reuters (March 13, 2003) observed the problem of name am-
biguity to be a major stumbling block in personal name web
searches.
1. Jim Clark - Race car driver from Scotland
2. Jim Clark - Clockmaker from Colorado
3. Jim Clark - Film Editor
4. Jim Clark - Netscape Founder
5. Jim Clark - Disaster Survivor
6. Jim Clark - Car Salesman in Kansas
7. Jim Clark - Fishing Instructor in Canada
8. Jim Clark - Computer Science student in Hong Kong
9. Jim Clark - Professor at McGill
10. Jim Clark - Gun Dealer in Louisiana
In this paper, we present a method for dis-
tinguishing the real world referent of a given
name in context. Approaches to this problem
include Wacholder et al (1997), focusing on the
variation of surface name for a given referent,
and Smith and Crane (2002), resolving geographic
name ambiguity. We present preliminary evaluation
on pseudonames: conflations of multiple personal
names, constructed in the same way pseudowords
are used for word sense disambiguation (Gale et al,
1992). We then present corroborating evidence from
real personal name polysemy to show that this tech-
nique works in practice.
Miles Davis
birth day May 26(5), May 25(5)
birth year 1926(82), 1967(18), 1969(9)...
occupation trumpeter(38), artist(10), player(5)...
birth place Alton(7), Illinois(6)
Joerg Haider
birth year 1950(6)
occupation leader(198) politician(93) chairman(6)...
birth place Austria(1)
Table 1: Extracted Biographical Information from
1000 Web Pages
Another topic of recent interest is in producing
biographical summaries from corpora (Schiffman et
al., 2001). Along with disambiguation, our system
simultaneously collects biographic information (Ta-
ble 1). The relevant biographical attributes are de-
picted along with a clustering which shows the dis-
tinct referents (Section 4.1).
2 Robust Extraction of Categorical
Biographic Data
Past work on this task (e.g. Bagga and Baldwin,
1998) has primarily approached personal name dis-
ambiguation using document context profiles or vec-
tors, which recognize and distinguish identical name
instances based on partially indicative words in con-
text such as computer or car in the Clark case. How-
ever, in the specialized case of personal names, there
is more precise information available.
In particular, information extraction techniques
can add high precision, categorical information such
as approximate age/date-of-birth, nationality and
occupation. This categorical data can support or
exclude a candidate name?referent matches with
higher confidence and greater pinpoint accuracy
than via simple context vector-style features alone.
Another major source of disambiguation infor-
mation for proper nouns is the space of associated
names. While these names could be used in a undif-
ferentiated vector-based bag-of-words model, fur-
ther accuracy can be gained by extracting specific
types of association, such as familial relationships
(e.g. son, wife), employment relationships (e.g.
manager of), and nationality as distinct from sim-
ple term co-occurrence in a window. The Jim Clark
married to ?Vickie Parker-Clark? is likely not the
same Jim Clark married to ?Patty Clark?. Addi-
tionally, information about one?s associates can help
predict information about the person in question.
Someone who frequently associates with Egyptians
is likely to be Egyptian, or at the very least, has a
close connection to Egypt.
2.1 Generating Extraction Patterns
One standard method for generating extraction pat-
terns is simply to write them by hand. In this paper,
we have experimented with generating patterns au-
tomatically from data. This has the advantage of be-
ing more flexible, portable and scalable, and poten-
tially having higher precision and recall. It also has
the advantage of being applicable to new languages
for which no developer with sufficient knowledge of
the language is available.
String Patterns with <person> and <birth year>
Extractions where <birth year> is a year
Extraction graded by #correct extractions/#total extractions
<person> ( <birth year> ? \d\d\d\d)
<person> <birthyear>?\d\d\d\d
<person> was born in <birth year>
<person> ( b.<birth year>
Extractions of <person> with potential <birth years>
Web Pages w/<person> and <birth year>
Sentences with <person> and <birth year>
Substrings with <person> and <birth year>
Web Pages with <person>
Sentences with <person>
1642
1685
1869
1770
1899
Ludwig van Beethoven
Humphrey Bogart
Mohandas Gandhi 
John Sebastian Bach
Isaac Newton 
<person>                       <birth year>
Figure 1: Learning Extraction Patterns from Filled
Templates and Web Pages
In the late 90s, there was a substantial body of
research on learning information extraction patterns
from templates (Huffman, 1995; Brin, 1998; Califf
and Mooney, 1998; Freitag and McCallum, 1999;
Yangarber et al, 2000; Ravichandran and Hovy,
2002). These techniques provide a way to bootstrap
information extraction patterns from a set of exam-
ple extractions or seed facts, where a tuple with the
filled roles for the desired pattern are given. For
the task of extracting biographical information, each
example would include the personal name and the
biographic feature. For example, training data for
the pattern born in might be (?Wolfgang Amadeus
Mozart?,1756). Given this set of examples, each
method generates patterns differently.
In this paper, we employ and extend the method
described by Ravichandran and Hovy (2002) shown
in Figure 1. For each seed fact pair for a given tem-
plate (such as (Mozart,1756)), a web query is made
which in turn leads to sentences in which the roles
are observed in nearby association (e.g. ?Mozart
was born in 1756?). All substrings from these sen-
tences are then extracted. The substrings are then
subject to simple generalization, to produce can-
didate patterns: Mozart is replaced by <name>,
1756 is replaced by <birth year>, and all digits
are replaced by #. These substring templates can
English Spanish
Purely Syntactic Patterns
Pattern Template Precision Count
<name> ( <birth year> - #### ) 1 31
<name> ( <birth year> - #### 1 31
- <name> ( <birth year>-#### ) 1 30
- <name> ( <birth year>-#### 1 30
<name> <birth year>-#### 1 27
<name> ( <birth year>-#### ) - 1 26
<name> <name> ( <birth year> 1 18
Syntactic & Lexical
Pattern Template Precision Count
<name> was born in <birth year> 1 19
<name> was born in <birth year> in 1 12
by <name> ( <birth year>-#### ) 1 10
by <name> ( <birth year>-#### 1 10
of <name> ( <birth year>-#### ) 0.933 15
of <name> ( <birth year>-#### 0.933 15
<name> ( <birth year>-#### ) was 0.833 12
Purely Syntactic Patterns
Pattern Template Precision Count
. <name> ( <birth year>- 1 62
. <name> ( <birth year>-## 1 58
. <name> ( <birth year>-#### 1 55
. <name> ( <birth year>-#### ) 1 54
<name> ( <birth year>-#### ) : 1 38
<name> <birth year>-#### , 1 26
<name> , <birth year>-#### 1 25
Syntactic & Lexical
Pattern Template Precision Count
a <name> ( <birth year>-#### 1 30
a <name> ( <birth year>-#### ) 1 29
<birth year> . - Nace <name> 1 21
<birth year> . - Nace <name> , 1 17
<name> ( <birth year>-#### ) , con 1 15
<name> ( <birth year>-#### ) se 1 12
, de <name> ( <birth year>-#### ) 1 12
Table 2: Highest Precision Patterns Extracted for English and Spanish using Suffix Tree Methodology
then serve as extraction patterns for previously un-
known fact pairs, and their precision in fact extrac-
tion can be calculated with respect to a set of cur-
rently known facts.
We examined a subset of the available and desir-
able extracted information. We learned patterns for
birth year and occupation, and hand-coded patterns
for birth location, spouse, birthday, familial relation-
ships, collegiate affiliations and nationality. Other
potential patterns currently under investigation in-
clude employer/employee and place of residence.
2.2 Multilingual Information Extraction
We adapted the information extraction pattern gen-
eration techniques described above to multiple lan-
guages. In particular, the methodology proposed by
Ravichandran and Hovy (2002) requires no parsing
or other language specific resources, so is an ideal
candidate for multilingual use. In this paper, we
conducted an initial test test of the viability of in-
ducing these information extraction patterns across
languages. To test, we constructed a initial database
of 5 people and their birthdays, and used this to in-
duce the English patterns. We then increased the
database to 50 people and birthdays and induced pat-
terns for Spanish, presenting the results above. Fig-
ure 2 shows the top precision patterns extracted for
English and for Spanish.
It can be seen that the Spanish patterns are of
the same length, with similar estimated precision, as
well as similar word and punctuation distribution as
the English ones. In fact, the purely syntactic pat-
terns look identical. The only difference being that
to generate equivalent Spanish data, a database of
training examples an order of magnitude larger was
required. This may be because for each database en-
try more pages were available on English websites
than on Spanish websites.
3 Using Unsupervised Clustering to
Identify the Referents of Personal Names
This section examines clustering of web pages
which containing an ambiguous personal name
(with multiple real referents). The cluster method
we employed is bottom-up centroid agglomerative
clustering. In this method, each document is as-
signed a vector of automatically extracted features.
At each stage of the clustering, the two most similar
vectors are merged, to produce a new cluster, with
a vector equal to the centroid of the vectors in the
cluster. This step is repeated until all documents are
clustered.
To generate the vectors for each document, we ex-
plored a variety of methods:
1. Baseline : All words (plain) or only Proper
Nouns (nnp)
2. Most Relevant words (mi and tf-idf)
3. Basic biographical features (feat)
4. Extended biographical Features (extfeat)
word weight(mi) weight(extfeat)
adderley 5.30 0
snipes 5.16 0
coltrane 5.06 0
montreux 5.01 0
bitches 4.99 0
danson 4.97 0
hemp 4.97 0
mullally 4.95 0
porgy 4.94 0
remastered 4.92 0
actor 3.50 2.40
1926 0 2.20
trumpeter 0 2.20
midland 0 1.39
Table 3: The 10 words with highest mutual infor-
mation with the document collection and all of ex-
tended feature words for DAVIS/HARRELSON pseudon-
ame
3.1 Baseline Models
In our baseline models, we used term vectors com-
posed either of all words (minus a set of closed class
?stop? words) or of only proper nouns. To assess
similarity between vectors we utilized standard co-
sine similarity (cos(a, b) = a?b||a||?||b||).
We experimentally determined that the use of
proper nouns alone led to more pure clustering. As
a result, for the remainder of the experiments, we
used only proper nouns in the vectors, except for
those common words introduced by the various fea-
ture sets.
3.2 Relevant Words (mi and tf-idf)
Selective term weighting has been shown to be
highly effective for information retrieval. For this
study, we investigated both the use of standard TF-
IDF weighting and weighting based on the mutual
information, where given a document collection c,
for each word w, we calculate I(w; c) = p(w|c)p(w) .
From these, we select words which appear more
than ?1 = 20 times in the collection, and have a
I(w; c) greater than ?2 = 10. These words are to
the document?s feature vector with a weight equal to
log(I(w; c)).
3.3 Extracted Biographical Features (feat)
The next set of models use the features extracted
using the methodology described in Section 2. Bi-
ographical information such as birth year, and oc-
cupation, when found, is quite useful in connecting
documents. If a document connects a name with a
birth year, and another document connects the same
name with the same birth year, typically, those two
documents refer to the same person.
Type Extracted Feature
birth place Midland(4), Texas (3), Alton(1),
Illinois(1)
birth year 1926 (9), 1967(3), 1973(2),
1947(1), 1958(1), 1969(1)
occupation actor (11), trumpeter(9),
heavyweight(2) ...
spouse Demi Moore(1)
Table 4: feat: Features Extracted for
DAVIS/HARRELSON pseudoname
These extracted features were used to categori-
cally cluster documents in which they appeared. Be-
cause of their high degree of precision and speci-
ficity, documents which contained similar extracted
features are virtually guaranteed to have the same
referent. By clustering these documents first, large
high quality clusters formed, which then then pro-
vided an anchor for the remaining pages. By ex-
amining the dendrogram in Figure 3, it is clear that
the clusters start with documents with matching fea-
tures, and then the other documents cluster around
this core.
In addition to improving disambiguation perfor-
mance, these extracted features help distinguish the
different clusters, and provide information about the
different people.
3.4 Extended Biographical Features (extfeat)
Another method for using these extracted features is
to give higher weight to words which have ever been
seen as filling a pattern. For example, if 1756 is ex-
tracted as a birth year from a syntactic-based pattern
for the polysemous name, then whenever 1756 is
observed anywhere in context (outside an extraction
pattern), it is given a higher weighting and added to
the document vector as a potential biographic fea-
ture. In our experiments, we did this only for words
which appeared as values for a feature more than a
threshold of 4 times. Then, whenever the word was
seen in a document, it was given a weight equal to
the log of the number of times the word was seen as
an extracted feature.
actor comedy  |     spouse:Demi Moore  |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy starring  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
actor movie  |                        |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson
movie  |             occ:actor  |       Woody Harrelson
actor sleeve  |             occ:actor  |       Woody Harrelson
actor  |             occ:actor  |       Woody Harrelson|                        |       Woody Harrelson|          occ:resident  |       Woody Harrelson
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
Seed1
formats  |                        |       Woody Harrelson
formats  |                        |           Miles Davis|            byear:1926  |           Miles Davis
quintet  |            byear:1926  |           Miles Davis
formats  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|            byear:1973  |           Miles Davis|            byear:1973  |           Miles Davis
click title  |                        |           Miles Davis
click title  |                        |           Miles Davis|         occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis
interviews...  |         occ:trumpeter  |           Miles Davis
biography...  |  byear:1967,occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis|            byear:1967  |           Miles Davis
recordings  |            byear:1967  |           Miles Davis
recordings  |                        |           Miles Davis
quintet trumpet  |      brthloc:Alton,IL  |           Miles Davis|                        |           Miles Davis
trumpet  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
album jazz  |                        |           Miles Davis
album  |                        |           Miles Davis
album music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |           Miles Davis
quintet trumpet  |                        |           Miles Davis
music nbsp  |          occ:musician  |           Miles Davis
album  |          occ:musician  |           Miles Davis|                        |           Miles Davis|            byear:1969  |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
Seed2
0.0 0.05 0.1 0.15 0.2
keywords features referent
Induced Seed 1
Induced Seed 2
Figure 2: nnp+feat+extfeat+mi Clustering Visual-
ization for DAVIS/HARRELSON pseudoname
3.5 Cluster Refactoring
Ideally, the raw unsupervised clustering would yield
a top level distinction between the different refer-
ents. However, this is rarely the case. With this
type of agglomerative clustering, the most similar
pages are clustered first, and outliers are assigned
as stragglers at the top levels of the cluster tree.
This typically leads to a full clustering where the
top-level clusters are significantly less discrimina-
tive than those at the roots. In order to compensate
for this effect, we performed a type of tree refac-
toring, which attempted to pick out and utilize seed
clusters from within the entire clustering.
In the refactoring, the clustering is stopped be-
fore it runs to completion, based on the percentage
of documents clustered and the relative size of the
clusters achieved. At this intermediate stage, rel-
atively large and high-precision clusters are found
(e.g. Figure 2). These automatically-induced clus-
ters are then used as seeds for the next stage, where
the unclustered documents are assigned to the seed
with the closest distance measure (Figure 3).
An alternative to this form of cluster refactoring
would be to initially cluster only pages with ex-
tracted features. This would yield a set of cluster
seeds, divided by features, which could then be used
for further clustering. However, this method relies
on having a number of pages with extracted features
that overlap from each referent. This can only be
actor comedy  |     spouse:Demi Moore  |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy starring  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
actor movie  |                        |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson|             occ:actor  |       Woody Harrelson
movie  |             occ:actor  |       Woody Harrelson
actor sleeve  |             occ:actor  |       Woody Harrelson
actor  |             occ:actor  |       Woody Harrelson|                        |       Woody Harrelson|          occ:resident  |       Woody Harrelson
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
actor star  |                        |       Woody Harrelson
Seed1
hemp  |  brthloc:Midland,Texas  |       Woody Harrelson
hemp  |  brthloc:Midland,Texas  |       Woody Harrelson|         brthloc:Texas  |       Woody Harrelson|                        |       Woody Harrelson
film movie  |                        |       Woody Harrelson
solo  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson
formats killers  |             occ:lover  |       Woody Harrelson|                        |       Woody Harrelson
comedy reviews  |                        |       Woody Harrelson
com  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
actor  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
encoding  |                        |       Woody Harrelson|                        |       Woody Harrelson
marijuana  |                        |       Woody Harrelson
movie  |                        |       Woody Harrelson
film marijuana  |                        |       Woody Harrelson
marijuana  |                        |       Woody Harrelson|                        |       Woody Harrelson
actor marijuana  |                        |       Woody Harrelson
upcoming  |                        |       Woody Harrelson
starring  |                        |       Woody Harrelson|                        |       Woody Harrelson
film movie  |       occ:heavyweight  |       Woody Harrelson
upcoming  |                        |       Woody Harrelson|                        |       Woody Harrelson
biography  |                        |       Woody Harrelson
biography  |                        |       Woody Harrelson|                        |           Miles Davis
movie  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
bowling  |                        |       Woody Harrelson
bowling soundtrack...  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
music  |                        |           Miles Davis
actor album  |                        |       Woody Harrelson|                        |       Woody Harrelson|                        |           Miles Davis|                        |       Woody Harrelson
com musician reviews  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson
marijuana  |            occ:farmer  |       Woody Harrelson
hemp  |                        |       Woody Harrelson
hemp tune  |                        |       Woody Harrelson
hemp  |          occ:activist  |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title ~  |                        |       Woody Harrelson
title  |                        |           Miles Davis|                        |       Woody Harrelson
com  |                        |       Woody Harrelson
com reviews  |                        |       Woody Harrelson
blues  |                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
actors  |                        |       Woody Harrelson
review  |                        |       Woody Harrelson
click  |                        |       Woody Harrelson|                        |           Miles Davis|                        |       Woody Harrelson|                        |       Woody Harrelson|                        |       Woody Harrelson
comedy guest  |                        |       Woody Harrelson
amp  |                        |       Woody Harrelson
w/  |                        |       Woody Harrelson
nbsp w/  |                        |       Woody Harrelson|                        |           Miles Davis
nbsp  |                        |           Miles Davis|                        |           Miles Davis|                        |       Woody Harrelson
arrange celebrity...  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
evil smile smiles  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
formats  |                        |       Woody Harrelson
formats  |                        |           Miles Davis|            byear:1926  |           Miles Davis
quintet  |            byear:1926  |           Miles Davis
formats  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|            byear:1973  |           Miles Davis|            byear:1973  |           Miles Davis
click title  |                        |           Miles Davis
click title  |                        |           Miles Davis|         occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis
interviews...  |         occ:trumpeter  |           Miles Davis
biography...  |  byear:1967,occ:trumpeter  |           Miles Davis|         occ:trumpeter  |           Miles Davis|            byear:1967  |           Miles Davis
recordings  |            byear:1967  |           Miles Davis
recordings  |                        |           Miles Davis
quintet trumpet  |      brthloc:Alton,IL  |           Miles Davis|                        |           Miles Davis
trumpet  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
album jazz  |                        |           Miles Davis
album  |                        |           Miles Davis
album music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
de  |                        |           Miles Davis
quintet trumpet  |                        |           Miles Davis
music nbsp  |          occ:musician  |           Miles Davis
album  |          occ:musician  |           Miles Davis|                        |           Miles Davis|            byear:1969  |           Miles Davis|                        |           Miles Davis
artist  |                        |           Miles Davis|                        |           Miles Davis
Seed2
tracks  |            byear:1958  |           Miles Davis|                        |       Woody Harrelson
formats rehkram  |                        |           Miles Davis|                        |       Woody Harrelson
fusion music...  |                        |           Miles Davis|                        |           Miles Davis
idem  |                        |           Miles Davis
biography videos  |                        |       Woody Harrelson
nbsp  |                        |           Miles Davis|                        |       Woody Harrelson
discography  |                        |           Miles Davis
solos  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
disc  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
music  |                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
music  |                        |           Miles Davis
com poster  |                        |           Miles Davis
poster  |                        |           Miles Davis
poster  |                        |           Miles Davis|                        |           Miles Davis
soundtrack  |                        |           Miles Davis|                        |           Miles Davis
discography  |            byear:1947  |           Miles Davis|                        |           Miles Davis|            occ:artist  |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis|                        |           Miles Davis
bowling  |                        |       Woody Harrelson
musicians recordings...  |            occ:player  |           Miles Davis
music trumpet  |                        |           Miles Davis
discography trumpet  |                        |           Miles Davis
flute saxophone...  |                        |           Miles Davis
musicians  |                        |           Miles Davis
studio  |                        |           Miles Davis|                        |       Woody Harrelson|                        |           Miles Davis|                        |           Miles Davis
0.0 0.2 0.4 0.6 0.8
keywords referentfeatures
Induced
Cluster 1
Induced
Cluster
2
Figure 3: nnp+feat+extfeat+mi Clustering Visual-
ization for DAVIS/HARRELSON pseudoname
assured when the feature set is rich, or a large docu-
ment space is assumed.
4 Experiments
To test these clustering methods, we collected web
pages by making requests to the Google website for
a set of target personal names (up to a maximum
of 1000 pages per name). There was no require-
ment that the web page be focused on that name, nor
was there a minimum number of name occurrences.
As a result, some pages clustered only mentioned
the name in passing, or in a specialized, commercial
context (e.g. Amazon sales product).
The pseudonames were created as follows. The
retrieval results from two different randomly-
selected people were taken, and all references to
either name (in full or part) replaced by a unique,
shared pseudoname. The resulting collection then
consisted of documents which were ambiguous as
to whom they talked about. The aim of the clus-
tering was then to distinguish this artificially con-
flated pseudoname. In addition, a test set of four
naturally occurring polysemous names (such as Jim
Clark), containing an average of 60 instances each,
was manually annotated with distinguishing nameID
numbers and used for a parallel evaluation.
The experiments consist of two parts. The first
output is the clustering visualizations whose utility
can be judged by inspection. The second is a quanti-
tative analysis of the different methodologies. Both
are conducted over test sets of pseudonames and nat-
urally occurring ambiguities.
4.1 Clustering Visualizations
Figures 2/3 and 4 each have two subfigures. The
left/top figure shows the extracted seed sets. The
right/bottom figure shows the final clustering of the
entire document collection. In each figure, there
are three columns of information before the dendro-
gram. The first column contains high weighted doc-
ument content words. The second column contains
the extracted features from the document. The third
column indicates the real referent. This is either the
real name of the conflated pseudoname (e.g. Woody
Harrelson or Miles Davis), or a number indicating
the referent (e.g. 1 - 20 in the case of Jim Clark).
This presentation allows a quick scan of the cluster-
ing to reveal correlations.
In general, the visualizations are informative. Oc-
casionally, the extractions err. One time when the
patterns themselves cannot be syntactically faulted
comes in the case where Woody Harrelson?s wife
is extracted as Demi Moore. The information was
extracted from the sentence: ?Architect Woody Har-
relson and his wife realtor Demi Moore ...? which
appears as a plot description for the movie ?Inde-
driver racing  |            occ:driver  |      1
racing  |           brthyr:1968  |      1
racing  |                        |      1
championship  |                        |      1
championship  |                        |      1|                        |      1
racing  |                        |      1
racing  |                        |      1
road  |                        |      1
statistics  |                        |      1|                        |      1
championship driver  |                        |      1
championship rally  |                        |      1|                        |      1
link  |                        |      1|                        |      1
rally  |                        |      1|                        |      1|                        |      1|                        |      1
rally  |                        |      1
promoter rally  |                        |      1|                        |      1
championship  |                        |      1
Seed1
|                        |      4|                        |      4|                        |      4|                        |      4
founder  |                        |      4
software  |                        |      4|                        |    All|                        |      4|           occ:founder  |      4|           occ:founder  |      4|           occ:founder  |      4|                        |      4
software  |                        |      4|       occ:billionaire  |      4
graphics  |                        |      4|                        |      4|                        |      4|                        |      4|                        |      4|      occ:entrepreneur  |      4|                        |      4|                        |      4
browser graphics  |                        |      4
software  |                        |      4|                        |      9|                        |     11|                        |     11|                        |     11
prevention  |                        |     19|                        |     17|                        |     17
Seed2
0.0 0.1 0.2 0.3 0.4 0.5
Induced Seed 1 (1 = Racing Jim Clark)
Induced Seed 2 (4 = Netscape Jim Clark)
keywords features referent
driver racing  |            occ:driver  |      1
racing  |           brthyr:1968  |      1
racing  |                        |      1
championship  |                        |      1
championship  |                        |      1|                        |      1
racing  |                        |      1
racing  |                        |      1
road  |                        |      1
statistics  |                        |      1|                        |      1
championship driver  |                        |      1
championship rally  |                        |      1|                        |      1
link  |                        |      1|                        |      1
rally  |                        |      1|                        |      1|                        |      1|                        |      1
rally  |                        |      1
promoter rally  |                        |      1|                        |      1
championship  |                        |      1
Seed1
|                        |      4|                        |      4|                        |      4|                        |      4
founder  |                        |      4
software  |                        |      4|                        |    All|                        |      4|           occ:founder  |      4|           occ:founder  |      4|           occ:founder  |      4|                        |      4
software  |                        |      4|       occ:billionaire  |      4
graphics  |                        |      4|                        |      4|                        |      4|                        |      4|                        |      4|      occ:entrepreneur  |      4|                        |      4|                        |      4
browser graphics  |                        |      4
software  |                        |      4|                        |      9|                        |     11|                        |     11|                        |     11
prevention  |                        |     19|                        |     17|                        |     17
Seed2
|                        |     15|                        |     18|                        |     20|                        |     10|                        |      3|                        |      3
links  |                        |     14|                        |     14|                        |      2|                        |      5|                        |      5|                        |     13|                        |      1
software  |                        |      9|                        |      6|                        |     12|                        |      7|                        |     16|                        |      8
0.0 0.2 0.4 0.6 0.8
Induced Cluster 1 
(1 = Racing Jim Clark)
Induced Cluster 2
keywords features referent
(4 = Netscape Jim Clark)
Figure 4: nnp+feat+extfeat+mi Clustering Visual-
ization of Jim Clark Pages: ?1?=Race Car Driver,
?4?=Netscape Founder, ?A?=multiple referents
cent Proposal?. Here, untangling of synecdoche is
needed. For Miles Davis, the incorrectly extracted
birth years refer to record release dates, which take
the same surface form as birth years in some genres.
Figure 4 shows a clustering for a naturally occur-
ing name ambiguity, in particular that of web pages
which refer to ?Jim Clark?. The set was constructed
by retrieving 100 web pages, and then labeling the
pages with respect to their referent. As can be seen,
the clusterings are highly coherent. All of the rele-
vant pages are included in the seed set, and few in-
appropriate pages are added. This type of clustering
would be useful to someone searching for a specific
individual named Jim Clark. Once the clustering had
been performed, a user could scan the output, and
identity the ?Jim Clark? of interest, based both on
extracted features and key words.
4.2 Evaluation on Pseudonames
For automated pseudoname evaluation purposes, we
selected a set of 8 different people for confla-
tion, who we presumed had one vastly predominant
sense. We selected these people giving room for his-
torical figures, figures from pop culture and mod-
ern media culture, as well as ?ordinary? people. We
added people with similar backgrounds (born close
to each other, or having the same profession). The
full list was composed of these 8 individuals:
Haifa Al-Faisal, William Blake, Tom
Cruise, Woody Harrelson, Hermann
Hesse, Wolfgang Amadeus Mozart, Anna
Shusterman, Bryon Tosoff
For each, we submitted Google queries, and re-
trieved up to 1000 pages each. We then took these
hit returns, and subsampled to a maximum of 100
pages per person. The person with the smallest rep-
resentation was Anna Shusterman with 26 pages.
We subsampled by taking the first 100 as ordered
lexically. This may have biased the results some-
what towards unreliable web pages, since pages with
numeric addresses tend to be newer and more tran-
sient.
We evaluated two guanularities of feature extrac-
tion. The small feature set uses high precision
rules to extract occupation (occ), birthday (brthyr),
spouse, birth location (brthloc), and school. The
large feature set adds higher recall (and therefore
noisier) patterns for the previous relationships and
as well as parent/child relationships.
As can be seen from the table, the highest per-
forming system combines proper nouns, relevant
words, and the high precision extracted features
(nnp+feat+mi and nnp+feat+tfidf). The extended
features (nnp+feat+extfeat+mi) do not give addi-
tional benefit to this combination. As can be seen
from the table, the large feature set yields better
overall performance than the smaller feature set.
Clustering Method Disambiguation Accuracy
no extracted features
majority sense 62.5
plain 74.5
tfidf 76.7
nnp 79.7
nnp+tfidf 79.7
nnp+mi 82.9
w/ extracted features feature set size
small large
nnp+feat 82.5 85.1
nnp+feat+extfeat 82.0 84.6
nnp+feat+mi 85.6 85.2
nnp+feat+tfidf 82.9 86.4
Table 5: Disambiguation Accuracy of different
Clustering Methods over 28 pseudonames
This suggests that the increased coverage outweighs
the introduced noise.
For the feat+tfidf system, accuracy at the two-
class disambiguation was above 80% for 25 out of
the 28 pairs. Without these pairs, the average two-
class disambiguation performance over the remain-
ing pairs is 90%. In two of the problematic cases, the
contexts of the names are easily confusable, as the
individuals share the same profession and many of
the same keywords. More complete biographic pro-
files and different clustering biases would be helpful
in fully partitioning these cases. However, in prac-
tice these pseudoname pair situations may be more
difficult than expected for naturally occurring name
pairs. In many occupations that are typically news-
worthy (such as actors, authors, musicians, politi-
cians, etc.), there may be a tendency for individu-
als to avoid using identical names (or entering the
field entirely) to minimize confusion. When people
with identical names do indeed share the same field
one would expect a greater effort to providing dis-
ambiguating contextual features to distinguish them.
We have made some preliminary investigations
into selecting pages according to the number of
mentions, as opposed to by random. The results
have not been conclusive, and continuing work is in-
vestigating the cause.
4.3 Evaluation on Naturally Ambiguous Names
The above results have utilized pseudoname test sets
where high accuracy ground truth is automatically
available in large quantities [O(1000) examples per
name] to better distinguish model performance. Ta-
ble 6 shows the performance on the four O(60) ex-
ample hand-labeled test sets for naturally occurring
polysemous person names. Given that this is an
n-ary classification task, for consistency with the
above experiments the data were assigned to one
of 3 clusters, corresponding to the 2 automatically
derived first-pass majority seed sets and the resid-
ual ?other-use? classification, but evaluated strictly
on performance for the two major senses. While
additional analyses could be accomplished on the
residual sets, this is difficult given their small size
(remaining personal exemplars were mostly single-
tons) and lack of evidence on many single-mention
web pages. Thus the task of accurately partitioning
the two most common uses and clustering the resid-
ual examples for visual exploration may be a natural
and practical use for these classification and visual-
ization technologies.
Weighting Method Precision Recall
TF-IDF .81 .70
Mutual Information .88 .73
Table 6: Classification performance for naturally
occurring name ambiguities on 3-way classification
task (Majority-Use, Secondary-Use, Other-Use).
5 Conclusion
In this paper we have presented a set of algorithms
for finding the real referents for ambiguous per-
sonal names in text using unsupervised clustering
and feature extraction methods. In particular, we
have shown how to learn and use automatically ex-
tracted biographic information to improve clustering
results, and have demonstrated this improvement by
evaluating on pseudonames. We have presented ini-
tial results on learning these patterns to extract bio-
graphic information for multiple languages, and in-
tend to use these techniques for large-scale multilin-
gual polysemous name clustering.
The results presented here support the automatic
clustering of polysemous personal name referents
and visualization of these induced clusters and their
motivating features. These distinct referents can
be verified by inspection both of extracted features
and of the high weighted terms for each document.
These clusterings may be useful in two ways. First
as a useful visualization tool themselves, and second
as seeds for disambiguating further entities.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-document
coreferencing using the vector space model. In Chris-
tian Boitet and Pete Whitelock, editors, Proceedings of the
Thirty-Sixth Annual Meeting of the Association for Compu-
tational Linguistics and Seventeenth International Confer-
ence on Computational Linguistics, pages 79?85, San Fran-
cisco, California. Morgan Kaufmann Publishers.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT?98.
M. E. Califf and R. J. Mooney. 1998. Relational learning
of pattern-match rules for information extraction. In Work-
ing Notes of AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing, pages 6?11, Menlo Park,
CA. AAAI Press.
D. Freitag and A. McCallum. 1999. Information extraction
with hmms and shrinkage. In Proceedings of the AAAI-99
Workshop on Machine Learning for Information Extraction.
B. Gale, K. Church, and D. Yarowsky. 1992. Work on statisti-
cal methods for word sense disambiguation. In AAAI Fall
Symposium on Probabilistic Approaches to Natural Lan-
guage Processing, pages 54?60, Cambridge, MA.
S. B. Huffman. 1995. Learning information extraction patterns
from examples. In Learning for Natural Language Process-
ing, pages 246?260.
D. Ravichandran and E. Hovy. 2002. Learning surface text pat-
terns for a question answering system. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics.
B. Schiffman, I. Mani, and K. J. Concepcion. 2001. Producing
biographical summaries: Combining linguistic knowledge
with corpus statistics. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics.
D. A. Smith and G. Crane. 2002. Disambiguating geographic
names in a historic digital library. In Proceedings of ECDL,
pages 127?136.
N. Wacholder, Y. Ravin, and M. Choi. 1997. Disambiguation
of proper names in text. In Proceedings of Fifth Conference
on Applied Natural Language Processing, pages 202?208.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen.
2000. Unsupervised discovery of scenario-level patterns for
information extraction. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing, (ANLP-
NAACL 2000), pages 282?289.
Proceedings of ACL-08: HLT, pages 870?878,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalized Expectation Criteria for Semi-Supervised Learning of
Conditional Random Fields
Gideon S. Mann
Google Inc.
76 Ninth Avenue
New York, NY 10011
Andrew McCallum
Department of Computer Science
University of Massachusetts
140 Governors Drive
Amherst, MA 01003
Abstract
This paper presents a semi-supervised train-
ing method for linear-chain conditional ran-
dom fields that makes use of labeled features
rather than labeled instances. This is accom-
plished by using generalized expectation cri-
teria to express a preference for parameter set-
tings in which the model?s distribution on un-
labeled data matches a target distribution. We
induce target conditional probability distribu-
tions of labels given features from both anno-
tated feature occurrences in context and ad-
hoc feature majority label assignment. The
use of generalized expectation criteria allows
for a dramatic reduction in annotation time
by shifting from traditional instance-labeling
to feature-labeling, and the methods presented
outperform traditional CRF training and other
semi-supervised methods when limited human
effort is available.
1 Introduction
A significant barrier to applying machine learning
to new real world domains is the cost of obtaining
the necessary training data. To address this prob-
lem, work over the past several years has explored
semi-supervised or unsupervised approaches to the
same problems, seeking to improve accuracy with
the addition of lower cost unlabeled data. Tradi-
tional approaches to semi-supervised learning are
applied to cases in which there is a small amount of
fully labeled data and a much larger amount of un-
labeled data, presumably from the same data source.
For example, EM (Nigam et al, 1998), transduc-
tive SVMs (Joachims, 1999), entropy regularization
(Grandvalet and Bengio, 2004), and graph-based
address          :            *number*           oak            avenue          rent             $
ADDRESS ADDRESS ADDRESS ADDRESS ADDRESS RENT RENT
Traditional Full Instance Labeling
ADDRESS
address : *number* oak avenue rent $ ....
CONTACT
.. ( please include the address of this rental )
ADDRESS
... pm . address : *number* marie street sausalito ...
ADDRESS
.. laundry . address : *number* macarthur blvd ....
Feature Labeling
ConditionalDistribution
of LabelsGiven Word=address
ADDRESS
CONTACT
Figure 1: Top: Traditional instance-labeling in which se-
quences of contiguous tokens are annotated as to their
correct label. Bottom: Feature-labeling in which non-
contiguous feature occurrences in context are labeled for
the purpose of deriving a conditional probability distribu-
tion of labels given a particular feature.
methods (Zhu and Ghahramani, 2002; Szummer and
Jaakkola, 2002) have all been applied to a limited
amount of fully labeled data in conjunction with un-
labeled data to improve the accuracy of a classifier.
In this paper, we explore an alternative approach
in which, instead of fully labeled instances, the
learner has access to labeled features. These fea-
tures can often be labeled at a lower-cost to the hu-
man annotator than labeling entire instances, which
may require annotating the multiple sub-parts of a
sequence structure or tree. Features can be labeled
either by specifying the majority label for a partic-
ular feature or by annotating a few occurrences of
a particular feature in context with the correct label
(Figure 1).
To train models using this information we use
870
generalized expectation (GE) criteria. GE criteria
are terms in a training objective function that as-
sign scores to values of a model expectation. In
particular we use a version of GE that prefers pa-
rameter settings in which certain model expectations
are close to target distributions. Previous work has
shown how to apply GE criteria to maximum en-
tropy classifiers. In section 4, we extend GE crite-
ria to semi-supervised learning of linear-chain con-
ditional random fields, using conditional probability
distributions of labels given features.
To empirically evaluate this method we compare
it with several competing methods for CRF train-
ing, including entropy regularization and expected
gradient, showing that GE provides significant im-
provements. We achieve competitive performance
in comparison to alternate model families, in partic-
ular generative models such as MRFs trained with
EM (Haghighi and Klein, 2006) and HMMs trained
with soft constraints (Chang et al, 2007). Finally, in
Section 5.3 we show that feature-labeling can lead to
dramatic reductions in the annotation time that is re-
quired in order to achieve the same level of accuracy
as traditional instance-labeling.
2 Related Work
There has been a significant amount of work on
semi-supervised learning with small amounts of
fully labeled data (see Zhu (2005)). However there
has been comparatively less work on learning from
alternative forms of labeled resources. One exam-
ple is Schapire et al (2002) who present a method
in which features are annotated with their associated
majority labels and this information is used to boot-
strap a parameterized text classification model. Un-
like the model presented in this paper, they require
some labeled data in order to train their model.
This type of input information (features + major-
ity label) is a powerful and flexible model for spec-
ifying alternative inputs to a classifier, and has been
additionally used by Haghighi and Klein (2006). In
that work, ?prototype? features?words with their
associated labels?are used to train a generative
MRF sequence model. Their probability model can
be formally described as:
p?(x,y) =
1
Z(?)
exp
(
?
k
?kFk(x,y)
)
.
Although the partition function must be computed
over all (x,y) tuples, learning via EM in this model
is possible because of approximations made in com-
puting the partition function.
Another way to gather supervision is by means
of prior label distributions. Mann and McCallum
(2007) introduce a special case of GE, label regular-
ization, and demonstrate its effectiveness for train-
ing maximum entropy classifiers. In label regu-
larization, the model prefers parameter settings in
which the model?s predicted label distribution on the
unsupervised data match a target distribution. Note
that supervision here consists of the the full distribu-
tion over labels (i.e. conditioned on the maximum
entropy ?default feature?), instead of simply the ma-
jority label. Druck et al (2007) also use GE with full
distributions for semi-supervised learning of maxi-
mum entropy models, except here the distributions
are on labels conditioned on features. In Section 4
we describe how GE criteria can be applied to CRFs
given conditional probability distributions of labels
given features.
Another recent method that has been proposed for
training sequence models with constraints is Chang
et al (2007). They use constraints for approximate
EM training of an HMM, incorporating the con-
straints by looking only at the top K most-likely
sequences from a joint model of likelihood and the
constraints. This model can be applied to the combi-
nation of labeled and unlabeled instances, but cannot
be applied in situations where only labeled features
are available. Additionally, our model can be easily
combined with other semi-supervised criteria, such
as entropy regularization. Finally, their model is a
generative HMM which cannot handle the rich, non-
independent feature sets that are available to a CRF.
There have been relatively few different ap-
proaches to CRF semi-supervised training. One ap-
proach has been that proposed in both Miller et al
(2004) and Freitag (2004), uses distributional clus-
tering to induce features from a large corpus, and
then uses these features to augment the feature space
of the labeled data. Since this is an orthogonal
method for improving accuracy it can be combined
with many of the other methods discussed above,
and indeed we have obtained positive preliminary
experimental results with GE criteria (not reported
on here).
871
Another method for semi-supervised CRF train-
ing is entropy regularization, initially proposed by
Grandvalet and Bengio (2004) and extended to
linear-chain CRFs by Jiao et al (2006). In this for-
mulation, the traditional label likelihood (on super-
vised data) is augmented with an additional term that
encourages the model to predict low-entropy label
distributions on the unlabeled data:
O(?;D,U) =
?
d
log p?(y
(d)|x(d))? ?H(y|x).
This method can be quite brittle, since the minimal
entropy solution assigns all of the tokens the same
label.1 In general, entropy regularization is fragile,
and accuracy gains can come only with precise set-
tings of ?. High values of ? fall into the minimal
entropy trap, while low values of ? have no effect on
the model (see (Jiao et al, 2006) for an example).
When some instances have partial labelings (i.e.
labels for some of their tokens), it is possible to train
CRFs via expected gradient methods (Salakhutdinov
et al, 2003). Here a reformulation is presented in
which the gradient is computed for a probability dis-
tribution with a marginalized hidden variable, z, and
observed training labels y:
?L(?) =
?
??
?
z
log p(x, y, z; ?)
=
?
z
p(z|y, x)fk(x, y, z)
?
?
z,y?
p(z, y?|x; ?)fk(x, y, z).
In essence, this resembles the standard gradient for
the CRF, except that there is an additional marginal-
ization in the first term over the hidden variable z.
This type of training has been applied by Quattoni
et al (2007) for hidden-state conditional random
fields, and can be equally applied to semi-supervised
conditional random fields. Note, however, that la-
beling variables of a structured instance (e.g. to-
kens) is different than labeling features?being both
more coarse-grained and applying supervision nar-
rowly only to the individual subpart, not to all places
in the data where the feature occurs.
1In the experiments in this paper, we use ? = 0.001, which
we tuned for best performance on the test set, giving an unfair
advantage to our competitor.
Finally, there are some methods that use auxil-
iary tasks for training sequence models, though they
do not train linear-chain CRFs per se. Ando and
Zhang (2005) include a cluster discovery step into
the supervised training. Smith and Eisner (2005)
use neighborhoods of related instances to figure out
what makes found instances ?good?. Although these
methods can often find good solutions, both are quite
sensitive to the selection of auxiliary information,
and making good selections requires significant in-
sight.2
3 Conditional Random Fields
Linear-chain conditional random fields (CRFs) are a
discriminative probabilistic model over sequences x
of feature vectors and label sequences y = ?y1..yn?,
where |x| = |y| = n, and each label yi has s dif-
ferent possible discrete values. This model is anal-
ogous to maximum entropy models for structured
outputs, where expectations can be efficiently calcu-
lated by dynamic programming. For a linear-chain
CRF of Markov order one:
p?(y|x) =
1
Z(x)
exp
(
?
k
?kFk(x,y)
)
,
where Fk(x,y) =
?
i fk(x, yi, yi+1, i),
and the partition function Z(x) =
?
y exp(
?
k ?kFk(x,y)). Given training data
D =
?
(x(1),y(1))..(x(n),y(n))
?
, the model is tra-
ditionally trained by maximizing the log-likelihood
O(?;D) =
?
d log p?(y
(d)|x(d)) by gradient ascent
where the gradient of the likelihood is:
?
??k
O(?;D) =
?
d
Fk(x
(d),y(d))
?
?
d
?
y
p?(y|x
(d))Fk(x
(d),y).
The second term (the expected counts of the features
given the model) can be computed in a tractable
amount of time, since according to the Markov as-
2Often these are more complicated than picking informative
features as proposed in this paper. One example of the kind of
operator used is the transposition operator proposed by Smith
and Eisner (2005).
872
sumption, the feature expectations can be rewritten:
?
y
p?(y|x)Fk(x,y) =
?
i
?
yi,yi+1
p?(yi, yi+1|x)fk(x, yi, yi+1, i).
A dynamic program (the forward/backward algo-
rithm) then computes in time O(ns2) all the needed
probabilities p?(yi, yi+1), where n is the sequence
length, and s is the number of labels.
4 Generalized Expectation Criteria for
Conditional Random Fields
Prior semi-supervised learning methods have aug-
mented a limited amount of fully labeled data with
either unlabeled data or with constraints (e.g. fea-
tures marked with their majority label). GE crite-
ria can use more information than these previous
methods. In particular GE criteria can take advan-
tage of conditional probability distributions of la-
bels given a feature (p(y|fk(x) = 1)). This in-
formation provides richer constraints to the model
while remaining easily interpretable. People have
good intuitions about the relative predictive strength
of different features. For example, it is clear that
the probability of label PERSON given the feature
WORD=JOHN is high, perhaps around 0.95, where
as for WORD=BROWN it would be lower, perhaps
0.4. These distributions need not be not estimated
with great precision?it is far better to have the free-
dom to express shades of gray than to be force into
a binary supervision signal. Another advantage of
using conditional probability distributions as prob-
abilistic constraints is that they can be easily esti-
mated from data. For the feature INITIAL-CAPITAL,
we identify all tokens with the feature, and then
count the labels with which the feature co-occurs.
GE criteria attempt to match these conditional
probability distributions by model expectations on
unlabeled data, encouraging, for example, the model
to predict that the proportion of the label PERSON
given the word ?john? should be .95 over all of the
unlabeled data.
In general, a GE (generalized expectation) crite-
rion (McCallum et al, 2007) expresses a preference
on the value of a model expectation. One kind of
preference may be expressed by a distance function
?, a target expectation f? , data D, a function f , and
a model distribution p?, the GE criterion objective
function term is ?
(
f? , E[f(x)]
)
. For the purposes
of this paper, we set the functions to be conditional
probability distributions and set ?(p, q) = D(p||q),
the KL-divergence between two distributions.3 For
semi-supervised training of CRFs, we augment the
objective function with the regularization term:
O(?;D,U) =
?
d
log p?(y
(d)|x(d))?
?
k ?k
2?2
? ?D(p?||p??),
where p? is given as a target distribution and
p?? = p??(yj |fm(x, j) = 1)
=
1
Um
?
x?Um
?
j?
p?(y
?
j |x),
with the unnormalized potential
q?? = q??(yj |fm(x, j) = 1) =
?
x?Um
?
j?
p?(y
?
j |x),
where fm(x, j) is a feature that depends only on
the observation sequence x, and j? is defined as
{j : fm(x, j) = 1}, and Um is the set of sequences
where fm(x, j) is present for some j.4
Computing the Gradient
To compute the gradient of the GE criteria,
D(p?||p??), first we drop terms that are constant with
respect to the partial derivative, and we derive the
gradient as follows:
?
??k
?
l
p? log q?? =
?
l
p?
q??
?
??k
q??
=
?
l
p?
q??
?
x?U
?
j?
?
??k
p?(yj? = l|x)
=
?
l
p?
q??
?
x?U
?
j?
?
y?j?
?
??k
p?(yj? = l,y?j? |x),
where y?j = ?y1..(j?1)y(j+1)..n?. The last step fol-
lows from the definition of the marginal probability
3We are actively investigating different choices of distance
functions which may have different generalization properties.
4This formulation assumes binary features.
873
P (yj |x). Now that we have a familiar form in which
we are taking the gradient of a particular label se-
quence, we can continue:
=
?
l
p?
q??
?
x?U
?
j?
?
y?j?
p?(yj? = l,y?j? |x)Fk(x,y)
?
?
l
p?
q??
?
x?U
?
j?
?
y?j?
p?(yj? = l,y?j? |x)
?
y?
p?(y
?|x)Fk(x,y)
=
?
l
p?
q??
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
?
j?
p?(yi, yi+1, yj? = l|x)
?
?
l
p?
q??
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
p?(yi, yi+1|x)
?
j?
p?(yj? = l|x).
After combining terms and rearranging we arrive at
the final form of the gradient:
=
?
x?U
?
i
?
yi,yi+1
fk(x, yi, yi+1, i)
?
l
p?
q??
?
(
?
j?
p?(yi, yi+1, yj? = l|x)?
p?(yi, yi+1|x)
?
j?
p?(yj? = l|x)
)
.
Here, the second term is easily gathered from for-
ward/backward, but obtaining the first term is some-
what more complicated. Computing this term
naively would require multiple runs of constrained
forward/backward. Here we present a more ef-
ficient method that requires only one run of for-
ward/backward.5 First we decompose the prob-
ability into two parts:
?
j? p?(yi, yi+1, yj? =
l|x) =
?i
j=1 p?(yi, yi+1, yj = l|x)I(j ? j
?) +
?J
j=i+1 p?(yi, yi+1, yj = l|x)I(j ? j
?). Next, we
show how to compute these terms efficiently. Simi-
lar to forward/backward, we build a lattice of inter-
mediate results that then can be used to calculate the
5(Kakade et al, 2002) propose a related method that com-
putes p(y1..i = l1..i|yi+1 = l).
quantity of interest:
i?
j=1
p?(yi, yi+1, yj = l|x)I(j ? j
?)
= p(yi, yi+1|x)?(yi, l)I(i ? j?)
+
i?1?
j=1
p?(yi, yi+1, yj = l|x)I(j ? j
?)
= p(yi, yi+1|x)?(yi, l)I(i ? j?)
+
?
?
?
yi?1
i?1?
j=1
p?(yi?1, yi, yj = l|x)I(j ? j
?)
?
?
p?(yi+1|yi,x).
For efficiency,
?
yi?1
?i?1
j=1 p?(yi?1, yi, yj =
l|x)I(j ? j?) is saved at each stage in the lat-
tice.
?J
j=i+1 p?(yi?1, yi, yj = l|x)I(j ? j
?) can
be computed in the same fashion. To compute the
lattices it takes time O(ns2), and one lattice must be
computed for each label so the total time is O(ns3).
5 Experimental Results
We use the CLASSIFIEDS data provided by Grenager
et al (2005) and compare with results reported
by HK06 (Haghighi and Klein, 2006) and CRR07
(Chang et al, 2007). HK06 introduced a set of 33
features along with their majority labels, these are
the primary set of additional constraints (Table 1).
As HK06 notes, these features are selected using
statistics of the labeled data, and here we used sim-
ilar features here in order to compare with previous
results. Though in practice we have found that fea-
ture selection is often intuitive, recent work has ex-
perimented with automatic feature selection using
LDA (Druck et al, 2008). For some of the exper-
iments we also use two sets of 33 additional fea-
tures that we chose by the same method as HK06,
the first 33 of which are also shown in Table 1. We
use the same tokenization of the dataset as HK06,
and training/test/unsupervised sets of 100 instances
each. This data differs slightly from the tokenization
used by CRR07. In particular it lacks the newline
breaks which might be a useful piece of information.
There are three types of supervised/semi-
supervised data used in the experiments. Labeled
instances are the traditional or conventionally
874
Label HK06: 33 Features 33 Added Features
CONTACT *phone* call *time please appointment more
FEATURES kitchen laundry parking room new large
ROOMMATES roommate respectful drama i bit mean
RESTRICTIONS pets smoking dog no sorry cats
UTILITIES utilities pays electricity water garbage included
AVAILABLE immediately begin cheaper *month* now *ordinal*0
SIZE *number*1*1 br sq *number*0*1 bedroom bath
PHOTOS pictures image link *url*long click photos
RENT *number*15*1 $ month deposit lease rent
NEIGHBORHOOD close near shopping located bart downtown
ADDRESS address carlmont ave san *ordinal*5 #
Table 1: Features and their associated majority label.
Features for each label were chosen by the method de-
scribed in HK06 ? top frequency for that label and not
higher frequency for any other label.
+ SVD features
HK06 53.7% 71.5%
CRF + GE/Heuristic 66.9% 68.3%
Table 2: Accuracy of semi-supervised learning methods
with majority labeled features alone. GE outperforms
HK06 when neither model has access to SVD features.
When SVD features are included, HK06 has an edge in
accuracy.
labeled instances used for estimation in traditional
CRF training. Majority labeled features are fea-
tures annotated with their majority label.6 Labeled
features are features m where the distribution
p(yi|fm(x, i)) has been specified. In Section 5.3 we
estimate these distributions from isolated labeled
tokens.
We evaluate the system in two scenarios: (1) with
feature constraints alone and (2) feature constraints
in conjunction with a minimal amount of labeled in-
stances. There is little prior work that demonstrates
the use of both scenarios; CRR07 can only be ap-
plied when there is some labeled data, while HK06
could be applied in both scenarios though there are
no such published experiments.
5.1 Majority Labeled Features Only
When using majority labeled features alone, it can
be seen in Table 2 that GE is the best performing
method. This is important, as it demonstrates that
GE out of the box can be used effectively, without
tuning and extra modifications.
6While HK06 and CRR07 require only majority labeled fea-
tures, GE criteria use conditional probability distributions of la-
bels given features, and so in order to apply GE we must decide
on a particular distribution for each feature constraint. In sec-
tions 5.1 and 5.2 we use a simple heuristic to derive distribu-
tions from majority label information: we assign .99 probabil-
ity to the majority label of the feature and divide the remaining
probability uniformly among the remainder of the labels.
Labeled Instances
10 25 100
supervised HMM 61.6% 70.0% 76.3%
supervised CRF 64.6% 72.9% 79.4%
CRF+ Entropy Reg. 67.3% 73.7% 79.5%
CRR07 70.9% 74.8% 78.6%
+ inference constraints 74.7% 78.5% 81.7%
CRF+GE/Heuristic 72.6% 76.3% 80.1%
Table 3: Accuracy of semi-supervised learning meth-
ods with constraints and limited amounts of training
data. Even though CRR07 uses more constraints and re-
quires additional development data for estimating mix-
ture weights, GE still outperforms CRR07 when that sys-
tem is run without applying constraints during inference.
When these constraints are applied during test-time infer-
ence, CRR07 has an edge over the CRF trained with GE
criteria.
In their original work, HK06 propose a method
for generating additional features given a set of ?pro-
totype? features (the feature constraints in Table 1),
which they demonstrate to be highly effective. In
their method, they collect contexts around all words
in the corpus, then perform a SVD decomposition.
They take the first 50 singular values for all words,
and then if a word is within a thresholded distance
to a prototype feature, they assign that word a new
feature which indicates close similarity to a proto-
type feature. When SVD features such as these are
made available to the systems, HK06 has a higher
accuracy.7 For the remainder of the experiments we
use the SVD feature enhanced data sets.8
We ran additional experiments with expected gra-
dient methods but found them to be ineffective,
reaching around 50% accuracy on the experiments
with the additional SVD features, around 20% less
than the competing methods.
5.2 Majority Labeled Features and Labeled
Instances
Labeled instances are available, the technique de-
scribed in CRR07 can be used. While CRR07 is
run on the same data set as used by HK06, a direct
comparison is problematic. First, they use additional
constraints beyond those used in this paper and those
7We generated our own set of SVD features, so they might
not match exactly the SVD features described in HK06.
8One further experiment HK06 performs which we do not
duplicate here is post-processing the label assignments to better
handle field boundaries. With this addition they realize another
2.5% improvement.
875
used by HK06 (e.g. each contiguous label sequence
must be at least 3 labels long)?so their results can-
not be directly compared. Second, they require addi-
tional training data to estimate weights for their soft
constraints, and do not measure how much of this
additional data is needed. Third, they use a slightly
different tokenization procedure. Fourth, CRR07
uses different subsets of labeled training instances
than used here. For these reasons, the comparison
between the method presented here and CRR07 can-
not be exact.
The technique described in CRR07 can be applied
in two ways: constraints can be applied during learn-
ing, and they can also be applied during inference.
We present comparisons with both of these systems
in Table 3. CRFs trained with GE criteria consis-
tently outperform CRR07 when no constraints are
applied during inference time, even though CRR07
has additional constraints. When the method in
CRR07 is applied with constraints in inference time,
it is able to outperform CRFs trained with GE. We
tried adding the additional constraints described in
CRR07 during test-time inference in our system, but
found no accuracy improvement. After doing error
inspection, those additional constraints weren?t fre-
quently violated by the GE trained method, which
also suggests that adding them wouldn?t have a sig-
nificant effect during training either. It is possible
that for GE training there are alternative inference-
time constraints that would improve performance,
but we didn?t pursue this line of investigation as
there are benefits to operating within a formal prob-
abilistic model, and eschewing constraints applied
during inference time. Without these constraints,
probabilistic models can be combined easily with
one another in order to arrive at a joint model, and
adding in these constraints at inference time compli-
cates the nature of the combination.
5.3 Labeled Features vs. Labeled Instances
In the previous section, the supervision signal was
the majority label of each feature.9 Given a feature
of interest, a human can gather a set of tokens that
have this feature and label them to discover the cor-
9It is not clear how these features would be tagged with ma-
jority label in a real use case. Tagging data to discover the ma-
jority label could potentially require a large number of tagged
instances before the majority label was definitively identified.
Ac
cur
acy
Tokens
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  100  1000  10000  100000
Traditional Instance Labeling33 Labeled Features66 Labeled Features99 Labeled FeaturesCRR07 + inference time constraints
Figure 2: Accuracy of supervised and semi-supervised
learning methods for fixed numbers of labeled tokens.
Training a GE model with only labeled features sig-
nificantly outperforms traditional log-likelihood training
with labeled instances for comparable numbers of labeled
tokens. When training on less than 1500 annotated to-
kens, it also outperforms CRR07 + inference time con-
straints, which uses not only labeled tokens but additional
constraints and development data for estimating mixture
weights.
Labeled Instances
0 10 25 100
HK06 71.5% - - -
GE/Heuristic 68.3% 72.6% 76.3% 80.1%
GE/Sampled 73.0% 74.6% 77.2% 80.5%
Table 4: Accuracy of semi-supervised learning methods
comparing the effects of (1) a heuristic for setting con-
ditional distributions of labels given features and (2) es-
timating this distributions via human annotation. When
GE is given feature distributions are better than the sim-
ple heuristic it is able to realize considerable gains.
relation between the feature and the labels.10 While
the resulting label distribution information could not
be fully utilized by previous methods (HK06 and
CRR07 use only the majority label of the word), it
can, however, be integrated into the GE criteria by
using the distribution from the relative proportions
of labels rather than a the previous heuristic distri-
bution. We present a series of experiments that test
the advantages of this annotation paradigm.
To simulate a human labeler, we randomly sam-
ple (without replacement) tokens with the particu-
lar feature in question, and generate a label using
the human annotations provided in the data. Then
we normalize and smooth the raw counts to obtain a
10In this paper we observe a 10x speed-up by using isolated
labeled tokens instead of a wholly labeled instances?so even
if it takes slightly longer to label isolated tokens, there will still
be a substantial gain.
876
conditional probability distribution over labels given
feature. We experiment with samples of 1, 2,5, 10,
100 tokens per feature, as well as with all available
labeled data. We sample instances for labeling ex-
clusively from the training and development data,
not from the testing data. We train a model using GE
with these estimated conditional probability distri-
butions and compare them with corresponding num-
bers of tokens of traditionally labeled instances.
Training from labeled features significantly out-
performs training from traditional labeled instances
for equivalent numbers of labeled tokens (Figure
2). With 1000 labeled tokens, instance-labeling
achieves accuracy around 65%, while labeling 33
features reaches 72% accuracy.11 To achieve the
same level of performance as traditional instance la-
beling, it can require as much as a factor of ten-fold
fewer annotations of feature occurrences. For exam-
ple, the accuracy achieved after labeling 257 tokens
of 33 features is 71% ? the same accuracy achieved
only after labeling more than 2000 tokens in tradi-
tional instance-labeling.12
Assuming that labeling one token in isolation
takes the same time as labeling one token in a
sequence, these results strongly support a new
paradigm of labeling in which instead of annotat-
ing entire sentences, the human instead selects some
key features of interest and labels tokens that have
this feature. Particularly intriguing is the flexibility
our scenario provides for the selection of ?features
of interest? to be driven by error analysis.
Table 4 compares the heuristic method described
above against sampled conditional probability distri-
butions of labels given features13. Sampled distribu-
tions yield consistent improvements over the heuris-
tic method. The accuracy with no labeled instances
(73.0%) is better than HK06 (71.5%), which demon-
strates that the precisely estimated feature distribu-
tions are helpful for improving accuracy.
Though accuracy begins to level off with distri-
11Labeling 99 features with 1000 tokens reaches nearly 76%.
12Accuracy at one labeled token per feature is much worse
than accuracy with majority label information. This due to the
noise introduced by sampling, as there is the potential for a rel-
atively rare label be sampled and labeled, and thereby train the
system on a non-canonical supervision signal.
13Where the tokens labeled is the total available number in
the data, roughly 2500 tokens.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12
Proba
bility
Label  0
 0.2
 0.4
 0.6
 0.8
 1
 0  2  4  6  8  10  12
Proba
bility
Label
Figure 3: From left to right: distributions (with standard
error) for the feature WORD=ADDRESS obtained from
sampling, using 1 sample per feature and 10 samples per
feature. Labels 1, 2, 3, and 9 are (respectively) FEA-
TURES, CONTACT, SIZE, and ADDRESS. Instead of more
precisely estimating these distributions, it is more benefi-
cial to label a larger set of features.
butions over the original set of 33 labeled features,
we ran additional experiments with 66 and 99 la-
beled features, whose results are also shown in Fig-
ure 2.14 The graph shows that with an increased
number of labeled features, for the same numbers
of labeled tokens, accuracy can be improved. The
reason behind this is clear?while there is some gain
from increased precision of probability estimates (as
they asymptotically approach their ?true? values as
shown in Figure 3), there is more information to be
gained from rougher estimates of a larger set of fea-
tures. One final point about these additional features
is that their distributions are less peaked than the
original feature set. Where the original feature set
distribution has entropy of 8.8, the first 33 added fea-
tures have an entropy of 22.95. Surprisingly, even
ambiguous feature constraints are able to improve
accuracy.
6 Conclusion
We have presented generalized expectation criteria
for linear-chain conditional random fields, a new
semi-supervised training method that makes use of
labeled features rather than labeled instances. Pre-
vious semi-supervised methods have typically used
ad-hoc feature majority label assignments as con-
straints. Our new method uses conditional proba-
bility distributions of labels given features and can
dramatically reduce annotation time. When these
distributions are estimated by means of annotated
feature occurrences in context, there is as much as
a ten-fold reduction in the annotation time that is re-
quired in order to achieve the same level of accuracy
over traditional instance-labeling.
14Also note that for less than 1500 tokens of labeling, the 99
labeled features outperform CRR07 with inference time con-
straints.
877
References
R. K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and unla-
beled data. JMLR, 6.
M.-W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
ACL.
G. Druck, G. Mann, and A. McCallum. 2007. Lever-
aging existing resources using generalized expectation
criteria. In NIPS Workshop on Learning Problem De-
sign.
G. Druck, G. S. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In SIGIR.
D. Freitag. 2004. Trained named entity recognition using
distributional clusters. In EMNLP.
Y. Grandvalet and Y. Bengio. 2004. Semi-supervised
learning by entropy minimization. In NIPS.
T. Grenager, D. Klein, and C. Manning. 2005. Unsuper-
vised learning of field segmentation models for infor-
mation extraction. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-driver learn-
ing for sequence models. In NAACL.
F. Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schu-
urmans. 2006. Semi-supervised conditional random
fields for improved sequence segmentation and label-
ing. In COLING/ACL.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
ICML.
S. Kakade, Y-W. Teg, and S.Roweis. 2002. An alternate
objective function for markovian fields. In ICML.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regular-
ization. In ICML.
A. McCallum, G. S. Mann, and G. Druck. 2007. Gener-
alized expectation criteria. Computer science techni-
cal note, University of Massachusetts, Amherst, MA.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In ACL.
K. Nigam, A. McCallum, S. Thrun, and T. Mitchell.
1998. Learning to classify text from labeled and un-
labeled documents. In AAAI.
A. Quattoni, S. Wang, L-P. Morency, M. Collins, and
T. Darrell. 2007. Hidden-state conditional random
fields. In PAMI.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on both features and instances.
JMLR.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with em and expectation-conjugate-
gradient. In ICML.
R. Schapire, M. Rochery, M. Rahim, and N. Gupta.
2002. Incorporating prior knowledge into boosting.
In ICML.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In ACL.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks. In
NIPS, volume 14.
X. Zhu and Z. Ghahramani. 2002. Learning from labeled
and unlabeled data with label propagation. Technical
Report CMU-CALD-02-107, CMU.
X. Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
878
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360?368,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Semi-supervised Learning of Dependency Parsers
using Generalized Expectation Criteria
Gregory Druck
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
gdruck@cs.umass.edu
Gideon Mann
Google, Inc.
76 9th Ave.
New York, NY 10011
gideon.mann@gmail.com
Andrew McCallum
Dept. of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
In this paper, we propose a novel method
for semi-supervised learning of non-
projective log-linear dependency parsers
using directly expressed linguistic prior
knowledge (e.g. a noun?s parent is often a
verb). Model parameters are estimated us-
ing a generalized expectation (GE) objec-
tive function that penalizes the mismatch
between model predictions and linguistic
expectation constraints. In a comparison
with two prominent ?unsupervised? learn-
ing methods that require indirect biasing
toward the correct syntactic structure, we
show that GE can attain better accuracy
with as few as 20 intuitive constraints. We
also present positive experimental results
on longer sentences in multiple languages.
1 Introduction
Early approaches to parsing assumed a grammar
provided by human experts (Quirk et al, 1985).
Later approaches avoided grammar writing by
learning the grammar from sentences explicitly
annotated with their syntactic structure (Black et
al., 1992). While such supervised approaches have
yielded accurate parsers (Charniak, 2001), the
syntactic annotation of corpora such as the Penn
Treebank is extremely costly, and consequently
there are few treebanks of comparable size.
As a result, there has been recent interest in
unsupervised parsing. However, in order to at-
tain reasonable accuracy, these methods have to
be carefully biased towards the desired syntac-
tic structure. This weak supervision has been
encoded using priors and initializations (Klein
and Manning, 2004; Smith, 2006), specialized
models (Klein and Manning, 2004; Seginer,
2007; Bod, 2006), and implicit negative evi-
dence (Smith, 2006). These indirect methods for
leveraging prior knowledge can be cumbersome
and unintuitive for a non-machine-learning expert.
This paper proposes a method for directly guid-
ing the learning of dependency parsers with nat-
urally encoded linguistic insights. Generalized
expectation (GE) (Mann and McCallum, 2008;
Druck et al, 2008) is a recently proposed frame-
work for incorporating prior knowledge into the
learning of conditional random fields (CRFs) (Laf-
ferty et al, 2001). GE criteria express a preference
on the value of a model expectation. For example,
we know that ?in English, when a determiner is di-
rectly to the left of a noun, the noun is usually the
parent of the determiner?. With GE we may add
a term to the objective function that encourages a
feature-rich CRF to match this expectation on un-
labeled data, and in the process learn about related
features. In this paper we use a non-projective de-
pendency tree CRF (Smith and Smith, 2007).
While a complete exploration of linguistic prior
knowledge for dependency parsing is beyond the
scope of this paper, we provide several promis-
ing demonstrations of the proposed method. On
the English WSJ10 data set, GE training outper-
forms two prominent unsupervised methods using
only 20 constraints either elicited from a human
or provided by an ?oracle? simulating a human.
We also present experiments on longer sentences
in Dutch, Spanish, and Turkish in which we obtain
accuracy comparable to supervised learning with
tens to hundreds of complete parsed sentences.
2 Related Work
This work is closely related to the prototype-
driven grammar induction method of Haghighi
and Klein (2006), which uses prototype phrases
to guide the EM algorithm in learning a PCFG.
Direct comparison with this method is not possi-
ble because we are interested in dependency syn-
tax rather than phrase structure syntax. However,
the approach we advocate has several significant
360
advantages. GE is more general than prototype-
driven learning because GE constraints can be un-
certain. Additionally prototype-driven grammar
induction needs to be used in conjunction with
other unsupervised methods (distributional simi-
larity and CCM (Klein and Manning, 2004)) to
attain reasonable accuracy, and is only evaluated
on length 10 or less sentences with no lexical in-
formation. In contrast, GE uses only the provided
constraints and unparsed sentences, and is used to
train a feature-rich discriminative model.
Conventional semi-supervised learning requires
parsed sentences. Kate and Mooney (2007) and
McClosky et al (2006) both use modified forms
of self-training to bootstrap parsers from limited
labeled data. Wang et al (2008) combine a struc-
tured loss on parsed sentences with a least squares
loss on unlabeled sentences. Koo et al (2008) use
a large unlabeled corpus to estimate cluster fea-
tures which help the parser generalize with fewer
examples. Smith and Eisner (2007) apply entropy
regularization to dependency parsing. The above
methods can be applied to small seed corpora, but
McDonald1 has criticized such methods as work-
ing from an unrealistic premise, as a significant
amount of the effort required to build a treebank
comes in the first 100 sentences (both because of
the time it takes to create an appropriate rubric and
to train annotators).
There are also a number of methods for unsu-
pervised learning of dependency parsers. Klein
and Manning (2004) use a carefully initialized and
structured generative model (DMV) in conjunc-
tion with the EM algorithm to get the first positive
results on unsupervised dependency parsing. As
empirical evidence of the sensitivity of DMV to
initialization, Smith (2006) (pg. 37) uses three dif-
ferent initializations, and only one, the method of
Klein and Manning (2004), gives accuracy higher
than 31% on the WSJ10 corpus (see Section 5).
This initialization encodes the prior knowledge
that long distance attachments are unlikely.
Smith and Eisner (2005) develop contrastive
estimation (CE), in which the model is encour-
aged to move probability mass away from im-
plicit negative examples defined using a care-
fully chosen neighborhood function. For instance,
Smith (2006) (pg. 82) uses eight different neigh-
borhood functions to estimate parameters for the
DMV model. The best performing neighborhood
1R. McDonald, personal communication, 2007
function DEL1ORTRANS1 provides accuracy of
57.6% on WSJ10 (see Section 5). Another neigh-
borhood, DEL1ORTRANS2, provides accuracy of
51.2%. The remaining six neighborhood func-
tions provide accuracy below 50%. This demon-
strates that constructing an appropriate neighbor-
hood function can be delicate and challenging.
Smith and Eisner (2006) propose structural an-
nealing (SA), in which a strong bias for local de-
pendency attachments is enforced early in learn-
ing, and then gradually relaxed. This method is
sensitive to the annealing schedule. Smith (2006)
(pg. 136) use 10 annealing schedules in conjunc-
tion with three initializers. The best performing
combination attains accuracy of 66.7% on WSJ10,
but the worst attains accuracy of 32.5%.
Finally, Seginer (2007) and Bod (2006) ap-
proach unsupervised parsing by constructing
novel syntactic models. The development and tun-
ing of the above methods constitute the encoding
of prior domain knowledge about the desired syn-
tactic structure. In contrast, our framework pro-
vides a straightforward and explicit method for in-
corporating prior knowledge.
Ganchev et al (2009) propose a related method
that uses posterior constrained EM to learn a pro-
jective target language parser using only a source
language parser and word alignments.
3 Generalized Expectation Criteria
Generalized expectation criteria (Mann and Mc-
Callum, 2008; Druck et al, 2008) are terms in
a parameter estimation objective function that ex-
press a preference on the value of a model expec-
tation. Let x represent input variables (i.e. a sen-
tence) and y represent output variables (i.e. a parse
tree). A generalized expectation term G(?) is de-
fined by a constraint function G(y,x) that returns
a non-negative real value given input and output
variables, an empirical distribution p?(x) over in-
put variables (i.e. unlabeled data), a model distri-
bution p?(y|x), and a score function S:
G(?) = S(Ep?(x)[Ep?(y|x)[G(y,x)]]).
In this paper, we use a score function that is the
squared difference of the model expectation of G
and some target expectation G?:
Ssq = ?(G?? Ep?(x)[Ep?(y|x)[G(y,x)]])
2 (1)
We can incorporate prior knowledge into the train-
ing of p?(y|x) by specifying the from of the con-
straint function G and the target expectation G?.
361
Importantly, G does not need to match a particular
feature in the underlying model.
The complete objective function2 includes mul-
tiple GE terms and a prior on parameters3, p(?)
O(?;D) = p(?) +
?
G
G(?)
GE has been applied to logistic regression mod-
els (Mann and McCallum, 2007; Druck et al,
2008) and linear chain CRFs (Mann and McCal-
lum, 2008). In the following sections we apply
GE to non-projective CRF dependency parsing.
3.1 GE in General CRFs
We first consider an arbitrarily structured condi-
tional random field (Lafferty et al, 2001) p?(y|x).
We describe the CRF for non-projective depen-
dency parsing in Section 3.2. The probability of
an output y conditioned on an input x is
p?(y|x) =
1
Zx
exp
(?
j
?jFj(y,x)
)
,
where Fj are feature functions over the cliques
of the graphical model and Z(x) is a normaliz-
ing constant that ensures p?(y|x) sums to 1. We
are interested in the expectation of constraint func-
tion G(x,y) under this model. We abbreviate this
model expectation as:
G? = Ep?(x)[Ep?(y|x)[G(y,x)]]
It can be shown that partial derivative of G(?) us-
ing Ssq4 with respect to model parameter ?j is
?
??j
G(?) = 2(G??G?) (2)
(
Ep?(x)
[
Ep?(y|x) [G(y,x)Fj(y,x)]
?Ep?(y|x) [G(y,x)]Ep?(y|x) [Fj(y,x)]
])
.
Equation 2 has an intuitive interpretation. The first
term (on the first line) is the difference between the
model and target expectations. The second term
2In general, the objective function could also include the
likelihood of available labeled data, but throughout this paper
we assume we have no parsed sentences.
3Throughout this paper we use a Gaussian prior on pa-
rameters with ?2 = 10.
4In previous work, S was the KL-divergence from the tar-
get expectation. The partial derivative of the KL divergence
score function includes the same covariance term as above
but substitutes a different multiplicative term: G?/G?.
(the rest of the equation) is the predicted covari-
ance between the constraint function G and the
model feature function Fj . Therefore, if the con-
straint is not satisfied, GE updates parameters for
features that the model predicts are related to the
constraint function.
If there are constraint functions G for all model
feature functions Fj , and the target expectations
G? are estimated from labeled data, then the glob-
ally optimal parameter setting under the GE objec-
tive function is equivalent to the maximum likeli-
hood solution. However, GE does not require such
a one-to-one correspondence between constraint
functions and model feature functions. This al-
lows bootstrapping of feature-rich models with a
small number of prior expectation constraints.
3.2 Non-Projective Dependency Tree CRFs
We now define a CRF p?(y|x) for unlabeled, non-
projective5 dependency parsing. The tree y is rep-
resented as a vector of the same length as the sen-
tence, where yi is the index of the parent of word
i. The probability of a tree y given sentence x is
p?(y|x) =
1
Zx
exp
( n?
i=1
?
j
?jfj(xi, xyi ,x)
)
,
where fj are edge-factored feature functions that
consider the child input (word, tag, or other fea-
ture), the parent input, and the rest of the sen-
tence. This factorization implies that dependency
decisions are independent conditioned on the in-
put sentence x if y is a tree. ComputingZx and the
edge expectations needed for partial derivatives re-
quires summing over all possible trees for x.
By relating the sum of the scores of all possible
trees to counting the number of spanning trees in a
graph, it can be shown that Zx is the determinant
of the Kirchoff matrixK, which is constructed us-
ing the scores of possible edges. (McDonald and
Satta, 2007; Smith and Smith, 2007). Computing
the determinant takes O(n3) time, where n is the
length of the sentence. To compute the marginal
probability of a particular edge k ? i (i.e. yi=k),
the score of any edge k? ? i such that k? 6= k is
set to 0. The determinant of the resulting modi-
fied Kirchoff matrix Kk?i is then the sum of the
scores of all trees that include the edge k ? i. The
5Note that we could instead define a CRF for projective
dependency parse trees and use a variant of the inside outside
algorithm for inference. We choose non-projective because it
is the more general case.
362
marginal p(yi=k|x; ?) can be computed by divid-
ing this score by Zx (McDonald and Satta, 2007).
Computing all edge expectations with this algo-
rithm takes O(n5) time. Smith and Smith (2007)
describe a more efficient algorithm that can com-
pute all edge expectations in O(n3) time using the
inverse of the Kirchoff matrix K?1.
3.3 GE for Non-Projective Dependency Tree
CRFs
While in general constraint functions G may
consider multiple edges, in this paper we use
edge-factored constraint functions. In this case
Ep?(y|x)[G(y,x)]Ep?(y|x)[Fj(y,x)], the second
term of the covariance in Equation 2, can be
computed using the edge marginal distributions
p?(yi|x). The first term of the covariance
Ep?(y|x)[G(y,x)Fj(y,x)] is more difficult to
compute because it requires the marginal proba-
bility of two edges p?(yi, yi? |x). It is important to
note that the model p? is still edge-factored.
The sum of the scores of all trees that contain
edges k ? i and k? ? i? can be computed by set-
ting the scores of edges j ? i such that j 6= k and
j? ? i? such that j? 6= k? to 0, and computing the
determinant of the resulting modified Kirchoff ma-
trixKk?i,k??i? . There areO(n4) pairs of possible
edges, and the determinant computation takes time
O(n3), so this naive algorithm takes O(n7) time.
An improved algorithm computes, for each pos-
sible edge k ? i, a modified Kirchoff matrix
Kk?i that requires the presence of that edge.
Then, the method of Smith and Smith (2007) can
be used to compute the probability of every pos-
sible edge conditioned on the presence of k ? i,
p?(yi? =k?|yi = k,x), using K
?1
k?i. Multiplying
this probability by p?(yi=k|x) yields the desired
two edge marginal. Because this algorithm pulls
the O(n3) matrix operation out of the inner loop
over edges, the run time is reduced to O(n5).
If it were possible to perform only one O(n3)
matrix operation per sentence, then the gradient
computation would take onlyO(n4) time, the time
required to consider all pairs of edges. Unfortu-
nately, there is no straightforward generalization
of the method of Smith and Smith (2007) to the
two edge marginal problem. Specifically, Laplace
expansion generalizes to second-order matrix mi-
nors, but it is not clear how to compute second-
order cofactors from the inverse Kirchoff matrix
alone (c.f. (Smith and Smith, 2007)).
Consequently, we also propose an approxima-
tion that can be used to speed up GE training at
the expense of a less accurate covariance compu-
tation. We consider different cases of the edges
k ? i, and k? ? i?.
? p?(yi=k, yi?=k?|x)=0 when i=i? and k 6=k?
(different parent for the same word), or when
i=k? and k=i? (cycle), because these pairs of
edges break the tree constraint.
? p?(yi=k, yi? =k?|x)=p?(yi=k|x) when i=
i?, k=k?.
? p?(yi=k, yi? =k?|x)?p?(yi=k|x)p?(yi? =
k?|x) when i 6= i? and i 6= k? or i? 6= k
(different words, do not create a cycle). This
approximation assumes that pairs of edges
that do not fall into one of the above cases
are conditionally independent given x. This
is not true because there are partial trees in
which k ? i and k? ? i? can appear sepa-
rately, but not together (for example if i = k?
and the partial tree contains i? ? k).
Using this approximation, the covariance for one
sentence is approximately equal to
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)g(xi, xyi ,x)]
?
n?
i
Ep?(yi|x)[fj(xi, xyi ,x)]Ep?(yi|x)[g(xi, xyi ,x)]
?
n?
i,k
p?(yi=k|x)p?(yk=i|x)fj(xi, xk,x)g(xk, xi,x).
Intuitively, the first and second terms compute a
covariance over possible parents for a single word,
and the third term accounts for cycles. Computing
the above takes O(n3) time, the time required to
compute single edge marginals. In this paper, we
use the O(n5) exact method, though we find that
the accuracy attained by approximate training is
usually within 5% of the exact method.
If G is not edge-factored, then we need to com-
pute a marginal over three or more edges, making
exact training intractable. An appealing alterna-
tive to a similar approximation to the above would
use loopy belief propagation to efficiently approx-
imate the marginals (Smith and Eisner, 2008).
In this paper g is binary and normalized by its
total count in the corpus. The expectation of g is
then the probability that it indicates a true edge.
363
4 Linguistic Prior Knowledge
Training parsers using GE with the aid of linguists
is an exciting direction for future work. In this pa-
per, we use constraints derived from several basic
types of linguistic knowledge.
One simple form of linguistic knowledge is the
set of possible parent tags for a given child tag.
This type of constraint was used in the devel-
opment of a rule-based dependency parser (De-
busmann et al, 2004). Additional information
can be obtained from small grammar fragments.
Haghighi and Klein (2006) provide a list of proto-
type phrase structure rules that can be augmented
with dependencies and used to define constraints
involving parent and child tags, surrounding or
interposing tags, direction, and distance. Finally
there are well known hypotheses about the direc-
tion and distance of attachments that can be used
to define constraints. Eisner and Smith (2005) use
the fact that short attachments are more common
to improve unsupervised parsing accuracy.
4.1 ?Oracle? constraints
For some experiments that follow we use ?ora-
cle? constraints that are estimated from labeled
data. This involves choosing feature templates
(motivated by the linguistic knowledge described
above) and estimating target expectations. Oracle
methods used in this paper consider three simple
statistics of candidate constraint functions: count
c?(g), edge count c?edge(g), and edge probability
p?(edge|g). Let D be the labeled corpus.
c?(g) =
?
x?D
?
i
?
j
g(xi, xj ,x)
c?edge(g) =
?
(x,y)?D
?
i
g(xi, xyi ,x)
p?(edge|g) =
c?edge(g)
c?(g)
Constraint functions are selected according to
some combination of the above statistics. In
some cases we additionally prune the candidate
set by considering only certain templates. To
compute the target expectation, we simply use
bin(p?(edge|g)), where bin returns the closest
value in the set {0, 0.1, 0.25, 0.5, 0.75, 1}. This
can be viewed as specifying that g is very indica-
tive of edge, somewhat indicative of edge, etc.
5 Experimental Comparison with
Unsupervised Learning
In this section we compare GE training with meth-
ods for unsupervised parsing. We use the WSJ10
corpus (as processed by Smith (2006)), which is
comprised of English sentences of ten words or
fewer (after stripping punctuation) from the WSJ
portion of the Penn Treebank. As in previous work
sentences contain only part-of-speech tags.
We compare GE and supervised training of an
edge-factored CRF with unsupervised learning of
a DMV model (Klein and Manning, 2004) using
EM and contrastive estimation (CE) (Smith and
Eisner, 2005). We also report the accuracy of an
attach-right baseline6. Finally, we report the ac-
curacy of a constraint baseline that assigns a score
to each possible edge that is the sum of the target
expectations for all constraints on that edge. Pos-
sible edges without constraints receive a score of
0. These scores are used as input to the maximum
spanning tree algorithm, which returns the best
tree. Note that this is a strong baseline because it
can handle uncertain constraints, and the tree con-
straint imposed by the MST algorithm helps infor-
mation propagate across edges.
We note that there are considerable differences
between the DMV and CRF models. The DMV
model is more expressive than the CRF because
it can model the arity of a head as well as sib-
ling relationships. Because these features consider
multiple edges, including them in the CRF model
would make exact inference intractable (McDon-
ald and Satta, 2007). However, the CRF may con-
sider the distance between head and child, whereas
DMV does not model distance. The CRF also
models non-projective trees, which when evaluat-
ing on English is likely a disadvantage.
Consequently, we experiment with two sets of
features for the CRF model. The first, restricted
set includes features that consider the head and
child tags of the dependency conjoined with the
direction of the attachment, (parent-POS,child-
POS,direction). With this feature set, the CRF
model is less expressive than DMV. The sec-
ond full set includes standard features for edge-
factored dependency parsers (McDonald et al,
2005), though still unlexicalized. The CRF can-
not consider valency even with the full feature set,
but this is balanced by the ability to use distance.
6The reported accuracies with the DMV model and the
attach-right baseline are taken from (Smith, 2006).
364
feature ex. feature ex.
MD? VB 1.00 NNS? VBD 0.75
POS? NN 0.75 PRP? VBD 0.75
JJ? NNS 0.75 VBD? TO 1.00
NNP? POS 0.75 VBD? VBN 0.75
ROOT?MD 0.75 NNS? VBP 0.75
ROOT? VBD 1.00 PRP? VBP 0.75
ROOT? VBP 0.75 VBP? VBN 0.75
ROOT? VBZ 0.75 PRP? VBZ 0.75
TO? VB 1.00 NN? VBZ 0.75
VBN? IN 0.75 VBZ? VBN 0.75
Table 1: 20 constraints that give 61.3% accuracy
on WSJ10. Tags are grouped according to heads,
and are in the order they appear in the sentence,
with the arrow pointing from head to modifier.
We generate constraints in two ways. First,
we use oracle constraints of the form (parent-
POS,child-POS,direction) such that c?(g) ? 200.
We choose constraints in descending order of
p?(edge|g). The first 20 constraints selected using
this method are displayed in Table 1.
Although the reader can verify that the con-
straints in Table 1 are reasonable, we addition-
ally experiment with human-provided constraints.
We use the prototype phrase-structure constraints
provided by Haghighi and Klein (2006), and
with the aid of head-finding rules, extract 14
(parent-pos,child-pos,direction) constraints.7 We
then estimated target expectations for these con-
straints using our prior knowledge, without look-
ing at the training data. We also created a second
constraint set with an additional six constraints for
tag pairs that were previously underrepresented.
5.1 Results
We present results varying the number of con-
straints in Figures 1 and 2. Figure 1 compares
supervised and GE training of the CRF model, as
well as the feature constraint baseline. First we
note that GE training using the full feature set sub-
stantially outperforms the restricted feature set,
despite the fact that the same set of constraints
is used for both experiments. This result demon-
strates GE?s ability to learn about related but non-
constrained features. GE training also outper-
forms the baseline8.
We compare GE training of the CRF model
7Because the CFG rules in (Haghighi and Klein, 2006)
are ?flattened? and in some cases do not generate appropriate
dependency constraints, we only used a subset.
8The baseline eventually matches the accuracy of the re-
stricted CRF but this is understandable because GE?s ability
to bootstrap is greatly reduced with the restricted feature set.
with unsupervised learning of the DMV model
in Figure 29. Despite the fact that the restricted
CRF is less expressive than DMV, GE training of
this model outperforms EM with 30 constraints
and CE with 50 constraints. GE training of the
full CRF outperforms EM with 10 constraints and
CE with 20 constraints (those displayed in Ta-
ble 1). GE training of the full CRF with the set of
14 constraints from (Haghighi and Klein, 2006),
gives accuracy of 53.8%, which is above the inter-
polated oracle constraints curve (43.5% accuracy
with 10 constraints, 61.3% accuracy with 20 con-
straints). With the 6 additional constraints, we ob-
tain accuracy of 57.7% and match CE.
Recall that CE, EM, and the DMV model in-
corporate prior knowledge indirectly, and that the
reported results are heavily-tuned ideal cases (see
Section 2). In contrast, GE provides a method to
directly encode intuitive linguistic insights.
Finally, note that structural annealing (Smith
and Eisner, 2006) provides 66.7% accuracy on
WSJ10 when choosing the best performing an-
nealing schedule (Smith, 2006). As noted in Sec-
tion 2 other annealing schedules provide accuracy
as low as 32.5%. GE training of the full CRF at-
tains accuracy of 67.0% with 30 constraints.
6 Experimental Comparison with
Supervised Training on Long
Sentences
Unsupervised parsing methods are typically eval-
uated on short sentences, as in Section 5. In this
section we show that GE can be used to train
parsers for longer sentences that provide compa-
rable accuracy to supervised training with tens to
hundreds of parsed sentences.
We use the standard train/test splits of the
Spanish, Dutch, and Turkish data from the 2006
CoNLL Shared Task. We also use standard
edge-factored feature templates (McDonald et al,
2005)10. We experiment with versions of the dat-
9Klein and Manning (2004) report 43.2% accuracy for
DMV with EM on WSJ10. When jointly modeling con-
stituency and dependencies, Klein and Manning (2004) re-
port accuracy of 47.5%. Seginer (2007) and Bod (2006) pro-
pose unsupervised phrase structure parsing methods that give
better unlabeled F-scores than DMV with EM, but they do
not report directed dependency accuracy.
10Typical feature processing uses only supported features,
or those features that occur on at least one true edge in the
training data. Because we assume that the data is unlabeled,
we instead use features on all possible edges. This generates
tens of millions features, so we prune those features that oc-
cur fewer than 10 total times, as in (Smith and Eisner, 2007).
365
10 20 30 40 50 6010
20
30
40
50
60
70
80
90
number of constraints
accu
racy
 
 
constraint baselineCRF restricted supervisedCRF supervisedCRF restricted GECRF GECRF GE human
Figure 1: Comparison of the constraint baseline and
both GE and supervised training of the restricted and
full CRF. Note that supervised training uses 5,301
parsed sentences. GE with human provided con-
straints closely matches the oracle results.
10 20 30 40 50 6010
20
30
40
50
60
70
80
number of constraints
accu
racy
 
 
attach right baselineDMV EMDMV CECRF restricted GECRF GECRF GE human
Figure 2: Comparison of GE training of the re-
stricted and full CRFs with unsupervised learning of
DMV. GE training of the full CRF outperforms CE
with just 20 constraints. GE also matches CE with
20 human provided constraints.
sets in which we remove sentences that are longer
than 20 words and 60 words.
For these experiments, we use an oracle
constraint selection method motivated by the
linguistic prior knowledge described in Section 4.
The first set of constraints specify the most
frequent head tag, attachment direction, and
distance combinations for each child tag. Specif-
ically, we select oracle constraints of the type
(parent-CPOS,child-CPOS,direction,distance)11.
We add constraints for every g such that
c?edge(g) > 100 for max length 60 data sets, and
c?edge(g)>10 times for max length 20 data sets.
In some cases, the possible parent constraints
described above will not be enough to provide
high accuracy, because they do not consider other
tags in the sentence (McDonald et al, 2005).
Consequently, we experiment with adding an
additional 25 sequence constraints (for what are
often called ?between? and ?surrounding? fea-
tures). The oracle feature selection method aims to
choose such constraints that help to reduce uncer-
tainty in the possible parents constraint set. Con-
sequently, we consider sequence features gs with
p?(edge|gs=1) ? 0.75, and whose corresponding
(parent-CPOS,child-CPOS,direction,distance)
constraint g, has edge probability p?(edge|g) ?
0.25. Among these candidates, we sort by
c?(gs=1), and select the top 25.
We compare with the constraint baseline de-
scribed in Section 5. Additionally, we report
11For these experiments we use coarse-grained part-of-
speech tags in constraints.
the number of parsed sentences required for su-
pervised CRF training (averaged over 5 random
splits) to match the accuracy of GE training using
the possible parents + sequence constraint set.
The results are provided in Table 2. We first
observe that GE always beats the baseline, espe-
cially on parent decisions for which there are no
constraints (not reported in Table 2, but for exam-
ple 53.8% vs. 20.5% on Turkish 20). Second, we
note that accuracy is always improved by adding
sequence constraints. Importantly, we observe
that GE gives comparable performance to super-
vised training with tens or hundreds of parsed sen-
tences. These parsed sentences provide a tremen-
dous amount of information to the model, as for
example in 20 Spanish length ? 60 sentences, a
total of 1,630,466 features are observed, 330,856
of them unique. In contrast, the constraint-based
methods are provided at most a few hundred con-
straints. When comparing the human costs of
parsing sentences and specifying constraints, re-
member that parsing sentences requires the devel-
opment of detailed annotation guidelines, which
can be extremely time-consuming (see also the
discussion is Section 2).
Finally, we experiment with iteratively
adding constraints. We sort constraints with
c?(g) > 50 by p?(edge|g), and ensure that 50%
are (parent-CPOS,child-CPOS,direction,distance)
constraints and 50% are sequence constraints.
For lack of space, we only show the results for
Spanish 60. In Figure 3, we see that GE beats
the baseline more soundly than above, and that
366
possible parent constraints + sequence constraints complete trees
baseline GE baseline GE
dutch 20 69.5 70.7 69.8 71.8 80-160
dutch 60 66.5 69.3 66.7 69.8 40-80
spanish 20 70.0 73.2 71.2 75.8 40-80
spanish 60 62.1 66.2 62.7 66.9 20-40
turkish 20 66.3 71.8 67.1 72.9 80-160
turkish 60 62.1 65.5 62.3 66.6 20-40
Table 2: Experiments on Dutch, Spanish, and Turkish with maximum sentence lengths of 20 and 60. Observe that GE
outperforms the baseline, adding sequence constraints improves accuracy, and accuracy with GE training is comparable to
supervised training with tens to hundreds of parsed sentences.
parent tag true predicted
det. 0.005 0.005
adv. 0.018 0.013
conj. 0.012 0.001
pron. 0.011 0.009
verb 0.355 0.405
adj. 0.067 0.075
punc. 0.031 0.013
noun 0.276 0.272
prep. 0.181 0.165
direction true predicted
right 0.621 0.598
left 0.339 0.362
distance true predicted
1 0.495 0.564
2 0.194 0.206
3 0.066 0.050
4 0.042 0.037
5 0.028 0.031
6-10 0.069 0.033
> 10 0.066 0.039
feature (distance) false pos. occ.
verb? punc. (>10) 1183
noun? prep. (1) 1139
adj. ? prep. (1) 855
verb? verb (6-10) 756
verb? verb (>10) 569
noun? punc. (1) 512
verb? punc. (2) 509
prep. ? punc. (1) 476
verb? punc. (4) 427
verb? prep. (1) 422
Table 3: Error analysis for GE training with possible parent + sequence constraints on Spanish 60 data. On the left, the
predicted and true distribution over parent coarse part-of-speech tags. In the middle, the predicted and true distributions over
attachment directions and distances. On the right, common features on false positive edges.
100 200 300 400 500 600 700 8002530
3540
4550
5560
6570
75
number of constraints
accura
cy
Spanish (maximum length 60)
 
 
constraint baselineGE
Figure 3: Comparing GE training of a CRF and constraint
baseline while increasing the number of oracle constraints.
adding constraints continues to increase accuracy.
7 Error Analysis
In this section, we analyze the errors of the model
learned with the possible parent + sequence con-
straints on the Spanish 60 data. In Table 3, we
present four types of analysis. First, we present
the predicted and true distributions over coarse-
grained parent part of speech tags. We can see
that verb is being predicted as a parent tag more
often then it should be, while most other tags are
predicted less often than they should be. Next, we
show the predicted and true distributions over at-
tachment direction and distance. From this we see
that the model is often incorrectly predicting left
attachments, and is predicting too many short at-
tachments. Finally, we show the most common
parent-child tag with direction and distance fea-
tures that occur on false positive edges. From this
table, we see that many errors concern the attach-
ments of punctuation. The second line indicates a
prepositional phrase attachment ambiguity.
This analysis could also be performed by a lin-
guist by looking at predicted trees for selected sen-
tences. Once errors are identified, GE constraints
could be added to address these problems.
8 Conclusions
In this paper, we developed a novel method for
the semi-supervised learning of a non-projective
CRF dependency parser that directly uses linguis-
tic prior knowledge as a training signal. It is our
hope that this method will permit more effective
leveraging of linguistic insight and resources and
enable the construction of parsers in languages and
domains where treebanks are not available.
Acknowledgments
We thank Ryan McDonald, Keith Hall, John Hale, Xiaoyun
Wu, and David Smith for helpful discussions. This work
was completed in part while Gregory Druck was an intern
at Google. This work was supported in part by the Center
for Intelligent Information Retrieval, The Central Intelligence
Agency, the National Security Agency and National Science
Foundation under NSF grant #IIS-0326249, and by the De-
fense Advanced Research Projects Agency (DARPA) under
Contract No. FA8750-07-D-0185/0004. Any opinions, find-
ings and conclusions or recommendations expressed in this
material are the author?s and do not necessarily reflect those
of the sponsor.
367
References
E. Black, J. Lafferty, and S. Roukos. 1992. Development and
evaluation of a broad-coverage probabilistic grammar of
english language computer manuals. In ACL, pages 185?
192.
Rens Bod. 2006. An all-subtrees approach to unsupervised
parsing. In ACL, pages 865?872.
E. Charniak. 2001. Immediate-head parsing for language
models. In ACL.
R. Debusmann, D. Duchier, A. Koller, M. Kuhlmann,
G. Smolka, and S. Thater. 2004. A relational syntax-
semantics interface based on dependency grammar. In
COLING.
G. Druck, G. S. Mann, and A. McCallum. 2008. Learning
from labeled features using generalized expectation crite-
ria. In SIGIR.
J. Eisner and N.A. Smith. 2005. Parsing with soft and hard
constraints on dependency length. In IWPT.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext projec-
tion constraints. In ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven grammar
induction. In COLING.
R. J. Kate and R. J. Mooney. 2007. Semi-supervised learning
for semantic parsing using support vector machines. In
HLT-NAACL (Short Papers).
D. Klein and C. Manning. 2004. Corpus-based induction
of syntactic structure: Models of dependency and con-
stituency. In ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In ICML.
G. Mann and A. McCallum. 2007. Simple, robust, scal-
able semi-supervised learning via expectation regulariza-
tion. In ICML.
G. Mann and A. McCallum. 2008. Generalized expectation
criteria for semi-supervised learning of conditional ran-
dom fields. In ACL.
D. McClosky, E. Charniak, and M. Johnson. 2006. Effective
self-training for parsing. In HLT-NAACL.
Ryan McDonald and Giorgio Satta. 2007. On the complex-
ity of non-projective data-driven dependency parsing. In
Proc. of IWPT, pages 121?132.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL, pages 91?98.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman.
Yoav Seginer. 2007. Fast unsupervised incremental parsing.
In ACL, pages 384?391, Prague, Czech Republic.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: training log-linear models on unlabeled data. In
ACL, pages 354?362.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction. In
COLING-ACL, pages 569?576.
David A. Smith and Jason Eisner. 2007. Bootstrapping
feature-rich dependency parsers with entropic priors. In
EMNLP-CoNLL, pages 667?677.
David A. Smith and Jason Eisner. 2008. Dependency parsing
by belief propagation. In EMNLP.
David A. Smith and Noah A. Smith. 2007. Probabilistic
models of nonprojective dependency trees. In EMNLP-
CoNLL, pages 132?140.
Noah A. Smith. 2006. Novel Estimation Methods for Un-
supervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008.
Semi-supervised convex training for dependency parsing.
In ACL, pages 532?540.
368
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456?464,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Distributed Training Strategies for the Structured Perceptron
Ryan McDonald Keith Hall Gideon Mann
Google, Inc., New York / Zurich
{ryanmcd|kbhall|gmann}@google.com
Abstract
Perceptron training is widely applied in the
natural language processing community for
learning complex structured models. Like all
structured prediction learning frameworks, the
structured perceptron can be costly to train
as training complexity is proportional to in-
ference, which is frequently non-linear in ex-
ample sequence length. In this paper we
investigate distributed training strategies for
the structured perceptron as a means to re-
duce training times when computing clusters
are available. We look at two strategies and
provide convergence bounds for a particu-
lar mode of distributed structured perceptron
training based on iterative parameter mixing
(or averaging). We present experiments on
two structured prediction problems ? named-
entity recognition and dependency parsing ?
to highlight the efficiency of this method.
1 Introduction
One of the most popular training algorithms for
structured prediction problems in natural language
processing is the perceptron (Rosenblatt, 1958;
Collins, 2002). The structured perceptron has many
desirable properties, most notably that there is no
need to calculate a partition function, which is
necessary for other structured prediction paradigms
such as CRFs (Lafferty et al, 2001). Furthermore,
it is robust to approximate inference, which is of-
ten required for problems where the search space
is too large and where strong structural indepen-
dence assumptions are insufficient, such as parsing
(Collins and Roark, 2004; McDonald and Pereira,
2006; Zhang and Clark, 2008) and machine trans-
lation (Liang et al, 2006). However, like all struc-
tured prediction learning frameworks, the structure
perceptron can still be cumbersome to train. This
is both due to the increasing size of available train-
ing sets as well as the fact that training complexity
is proportional to inference, which is frequently non-
linear in sequence length, even with strong structural
independence assumptions.
In this paper we investigate distributed training
strategies for the structured perceptron as a means
of reducing training times when large computing
clusters are available. Traditional machine learning
algorithms are typically designed for a single ma-
chine, and designing an efficient training mechanism
for analogous algorithms on a computing cluster ?
often via a map-reduce framework (Dean and Ghe-
mawat, 2004) ? is an active area of research (Chu
et al, 2007). However, unlike many batch learning
algorithms that can easily be distributed through the
gradient calculation, a distributed training analog for
the perceptron is less clear cut. It employs online up-
dates and its loss function is technically non-convex.
A recent study by Mann et al (2009) has shown
that distributed training through parameter mixing
(or averaging) for maximum entropy models can
be empirically powerful and has strong theoretical
guarantees. A parameter mixing strategy, which can
be applied to any parameterized learning algorithm,
trains separate models in parallel, each on a disjoint
subset of the training data, and then takes an average
of all the parameters as the final model. In this paper,
we provide results which suggest that the percep-
tron is ill-suited for straight-forward parameter mix-
ing, even though it is commonly used for large-scale
structured learning, e.g., Whitelaw et al (2008) for
named-entity recognition. However, a slight mod-
456
ification we call iterative parameter mixing can be
shown to: 1) have similar convergence properties to
the standard perceptron algorithm, 2) find a sepa-
rating hyperplane if the training set is separable, 3)
reduce training times significantly, and 4) produce
models with comparable (or superior) accuracies to
those trained serially on all the data.
2 Related Work
Distributed cluster computation for many batch
training algorithms has previously been examined
by Chu et al (2007), among others. Much of the
relevant prior work on online (or sub-gradient) dis-
tributed training has been focused on asynchronous
optimization via gradient descent. In this sce-
nario, multiple machines run stochastic gradient de-
scent simultaneously as they update and read from
a shared parameter vector asynchronously. Early
work by Tsitsiklis et al (1986) demonstrated that
if the delay between model updates and reads is
bounded, then asynchronous optimization is guaran-
teed to converge. Recently, Zinkevich et al (2009)
performed a similar type of analysis for online learn-
ers with asynchronous updates via stochastic gra-
dient descent. The asynchronous algorithms in
these studies require shared memory between the
distributed computations and are less suitable to
the more common cluster computing environment,
which is what we study here.
While we focus on the perceptron algorithm, there
is a large body of work on training structured pre-
diction classifiers. For batch training the most com-
mon is conditional random fields (CRFs) (Lafferty
et al, 2001), which is the structured analog of maxi-
mum entropy. As such, its training can easily be dis-
tributed through the gradient or sub-gradient com-
putations (Finkel et al, 2008). However, unlike per-
ceptron, CRFs require the computation of a partition
function, which is often expensive and sometimes
intractable. Other batch learning algorithms include
M3Ns (Taskar et al, 2004) and Structured SVMs
(Tsochantaridis et al, 2004). Due to their efficiency,
online learning algorithms have gained attention, es-
pecially for structured prediction tasks in NLP. In
addition to the perceptron (Collins, 2002), others
have looked at stochastic gradient descent (Zhang,
2004), passive aggressive algorithms (McDonald et
Perceptron(T = {(xt,yt)}
|T |
t=1)
1. w(0) = 0; k = 0
2. for n : 1..N
3. for t : 1..T
4. Let y? = argmaxy? w
(k) ? f(xt,y?)
5. if y? 6= yt
6. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
7. k = k + 1
8. return w(k)
Figure 1: The perceptron algorithm.
al., 2005; Crammer et al, 2006), the recently intro-
duced confidence weighted learning (Dredze et al,
2008) and coordinate descent algorithms (Duchi and
Singer, 2009).
3 Structured Perceptron
The structured perceptron was introduced by Collins
(2002) and we adopt much of the notation and pre-
sentation of that study. The structured percetron al-
gorithm ? which is identical to the multi-class per-
ceptron ? is shown in Figure 1. The perceptron is an
online learning algorithm and processes training in-
stances one at a time during each epoch of training.
Lines 4-6 are the core of the algorithm. For a input-
output training instance pair (xt,yt) ? T , the algo-
rithm predicts a structured output y? ? Yt, where Yt
is the space of permissible structured outputs for in-
put xt, e.g., parse trees for an input sentence. This
prediction is determined by a linear classifier based
on the dot product between a high-dimensional fea-
ture representation of a candidate input-output pair
f(x,y) ? RM and a corresponding weight vector
w ? RM , which are the parameters of the model1.
If this prediction is incorrect, then the parameters
are updated to add weight to features for the cor-
responding correct output yt and take weight away
from features for the incorrect output y?. For struc-
tured prediction, the inference step in line 4 is prob-
lem dependent, e.g., CKY for context-free parsing.
A training set T is separable with margin ? >
0 if there exists a vector u ? RM with ?u? = 1
such that u ? f(xt,yt) ? u ? f(xt,y?) ? ?, for all
(xt,yt) ? T , and for all y? ? Yt such that y? 6= yt.
Furthermore, letR ? ||f(xt,yt)?f(xt,y?)||, for all
(xt,yt) ? T and y? ? Yt. A fundamental theorem
1The perceptron can be kernalized for non-linearity.
457
of the perceptron is as follows:
Theorem 1 (Novikoff (1962)). Assume training set
T is separable by margin ?. Let k be the number of
mistakes made training the perceptron (Figure 1) on
T . If training is run indefinitely, then k ? R
2
?2 .
Proof. See Collins (2002) Theorem 1.
Theorem 1 implies that if T is separable then 1) the
perceptron will converge in a finite amount of time,
and 2) will produce a w that separates T . Collins
also proposed a variant of the structured perceptron
where the final weight vector is a weighted average
of all parameters that occur during training, which
he called the averaged perceptron and can be viewed
as an approximation to the voted perceptron algo-
rithm (Freund and Schapire, 1999).
4 Distributed Structured Perceptron
In this section we examine two distributed training
strategies for the perceptron algorithm based on pa-
rameter mixing.
4.1 Parameter Mixing
Distributed training through parameter mixing is a
straight-forward way of training classifiers in paral-
lel. The algorithm is given in Figure 2. The idea is
simple: divide the training data T into S disjoint
shards such that T = {T1, . . . , TS}. Next, train
perceptron models (or any learning algorithm) on
each shard in parallel. After training, set the final
parameters to a weighted mixture of the parameters
of each model using mixture coefficients ?. Note
that we call this strategy parameter mixing as op-
posed to parameter averaging to distinguish it from
the averaged perceptron (see previous section). It is
easy to see how this can be implemented on a cluster
through a map-reduce framework, i.e., the map step
trains the individual models in parallel and the re-
duce step mixes their parameters. The advantages of
parameter mixing are: 1) that it is parallel, making
it possibly to scale to extremely large data sets, and
2) it is resource efficient, in particular with respect
to network usage as parameters are not repeatedly
passed across the network as is often the case for
exact distributed training strategies.
For maximum entropy models, Mann et al (2009)
show it is possible to bound the norm of the dif-
PerceptronParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w(i) = Perceptron(Ti) ?
3. w =
?
i ?iw
(i) ?
4. return w
Figure 2: Distributed perceptron using a parameter mix-
ing strategy. ? Each w(i) is computed in parallel. ? ? =
{?1, . . . , ?S}, ??i ? ? : ?i ? 0 and
?
i ?i = 1.
ference between parameters trained on all the data
serially versus parameters trained with parameter
mixing. However, their analysis requires a stabil-
ity bound on the parameters of a regularized max-
imum entropy model, which is not known to hold
for the perceptron. In Section 5, we present empir-
ical results showing that parameter mixing for dis-
tributed perceptron can be sub-optimal. Addition-
ally, Dredze et al (2008) present negative parame-
ter mixing results for confidence weighted learning,
which is another online learning algorithm. The fol-
lowing theorem may help explain this behavior.
Theorem 2. For a any training set T separable by
margin ?, the perceptron algorithm trained through
a parameter mixing strategy (Figure 2) does not nec-
essarily return a separating weight vector w.
Proof. Consider a binary classification setting
where Y = {0, 1} and T has 4 instances.
We distribute the training set into two shards,
T1 = {(x1,1,y1,1), (x1,2,y1,2)} and T2 =
{(x2,1,y2,1), (x2,2,y2,2)}. Let y1,1 = y2,1 = 0 and
y1,2 = y2,2 = 1. Now, let w, f ? R6 and using
block features, define the feature space as,
f(x1,1, 0) = [1 1 0 0 0 0] f(x1,1, 1) = [0 0 0 1 1 0]
f(x1,2, 0) = [0 0 1 0 0 0] f(x1,2, 1) = [0 0 0 0 0 1]
f(x2,1, 0) = [0 1 1 0 0 0] f(x2,1, 1) = [0 0 0 0 1 1]
f(x2,2, 0) = [1 0 0 0 0 0] f(x2,2, 1) = [0 0 0 1 0 0]
Assuming label 1 tie-breaking, parameter mixing re-
turns w1=[1 1 0 -1 -1 0] and w2=[0 1 1 0 -1 -1]. For
any ?, the mixed weight vector w will not separate
all the points. If both ?1/?2 are non-zero, then all
examples will be classified 0. If ?1=1 and ?2=0,
then (x2,2,y2,2) will be incorrectly classified as 0
and (x1,2,y1,2) when ?1=0 and ?2=1. But there is a
separating weight vector w = [-1 2 -1 1 -2 1].
This counter example does not say that a parameter
mixing strategy will not converge. On the contrary,
458
if T is separable, then each of its subsets is separa-
ble and converge via Theorem 1. What it does say
is that, independent of ?, the mixed weight vector
produced after convergence will not necessarily sep-
arate the entire data, even when T is separable.
4.2 Iterative Parameter Mixing
Consider a slight augmentation to the parameter
mixing strategy. Previously, each parallel percep-
tron was trained to convergence before the parame-
ter mixing step. Instead, shard the data as before, but
train a single epoch of the perceptron algorithm for
each shard (in parallel) and mix the model weights.
This mixed weight vector is then re-sent to each
shard and the perceptrons on those shards reset their
weights to the new mixed weights. Another single
epoch of training is then run (again in parallel over
the shards) and the process repeats. This iterative
parameter mixing algorithm is given in Figure 3.
Again, it is easy to see how this can be imple-
mented as map-reduce, where the map computes the
parameters for each shard for one epoch and the re-
duce mixes and re-sends them. This is analogous
to batch distributed gradient descent methods where
the gradient for each shard is computed in parallel in
the map step and the reduce step sums the gradients
and updates the weight vector. The disadvantage of
iterative parameter mixing, relative to simple param-
eter mixing, is that the amount of information sent
across the network will increase. Thus, if network
latency is a bottleneck, this can become problematic.
However, for many parallel computing frameworks,
including both multi-core computing as well as clus-
ter computing with high rates of connectivity, this is
less of an issue.
Theorem 3. Assume a training set T is separable
by margin ?. Let ki,n be the number of mistakes that
occurred on shard i during the nth epoch of train-
ing. For any N , when training the perceptron with
iterative parameter mixing (Figure 3),
N?
n=1
S?
i=1
?i,nki,n ?
R2
?2
Proof. Let w(i,n) to be the weight vector for the
ith shard after the nth epoch of the main loop and
let w([i,n]?k) be the weight vector that existed on
shard i in the nth epoch k errors before w(i,n). Let
PerceptronIterParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti,w) ?
5. w =
?
i ?i,nw
(i,n) ?
6. return w
OneEpochPerceptron(T , w?)
1. w(0) = w?; k = 0
2. for t : 1..T
3. Let y? = argmaxy? w
(k) ? f(xt,y?)
4. if y? 6= yt
5. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
6. k = k + 1
7. return w(k)
Figure 3: Distributed perceptron using an iterative param-
eter mixing strategy. ? Each w(i,n) is computed in paral-
lel. ? ?n = {?1,n, . . . , ?S,n}, ??i,n ? ?n: ?i,n ? 0 and
?n:
?
i ?i,n = 1.
w(avg,n) be the mixed vector from the weight vec-
tors returned after the nth epoch, i.e.,
w(avg,n) =
S?
i=1
?i,nw(i,n)
Following the analysis from Collins (2002) Theorem
1, by examining line 5 of OneEpochPerceptron in
Figure 3 and the fact that u separates the data by ?:
u ?w(i,n) = u ?w([i,n]?1)
+ u ? (f(xt,yt)? f(xt,y?))
? u ?w([i,n]?1) + ?
? u ?w([i,n]?2) + 2?
. . . ? u ?w(avg,n?1) + ki,n? (A1)
That is, u ? w(i,n) is bounded below by the average
weight vector for the n-1st epoch plus the number
of mistakes made on shard i during the nth epoch
times the margin ?. Next, by OneEpochPerceptron
line 5, the definition ofR, and w([i,n]?1)(f(xt,yt)?
f(xt,y?)) ? 0 when line 5 is called:
?w(i,n)?2 = ?w([i,n]?1)?2
+?f(xt,yt)? f(xt,y?)?2
+ 2w([i,n]?1)(f(xt,yt)? f(xt,y?))
? ?w([i,n]?1)?2 +R2
? ?w([i,n]?2)?2 + 2R2
. . . ? ?w(avg,n?1)?2 + ki,nR2 (A2)
459
That is, the squared L2-norm of a shards weight vec-
tor is bounded above by the same value for the aver-
age weight vector of the n-1st epoch and the number
of mistakes made on that shard during the nth epoch
times R2.
Using A1/A2 we prove two inductive hypotheses:
u ?w(avg,N) ?
N?
n=1
S?
i=1
?i,nki,n? (IH1)
?w(avg,N)?2 ?
N?
n=1
S?
i=1
?i,nki,nR
2 (IH2)
IH1 implies ?w(avg,N)? ?
?N
n=1
?S
i=1 ?i,nki,n?
since u ?w ? ?u??w? and ?u? = 1.
The base case is w(avg,1), where we can observe:
u ?wavg,1 =
S?
i=1
?i,1u ?w(i,1) ?
S?
i=1
?i,1ki,1?
using A1 and the fact that w(avg,0) = 0 for the sec-
ond step. For the IH2 base case we can write:
?w(avg,1)?2 =
?
?
?
?
?
S?
i=1
?i,1w(i,1)
?
?
?
?
?
2
?
S?
i=1
?i,1?w(i,1)?2 ?
S?
i=1
?i,1ki,1R
2
The first inequality is Jensen?s inequality, and the
second is true by A2 and ?w(avg,0)?2 = 0.
Proceeding to the general case, w(avg,N):
u ?w(avg,N) =
S?
i=1
?i,N (u ?w(i,N))
?
S?
i=1
?i,N (u ?w(avg,N?1) + ki,N?)
= u ?w(avg,N?1) +
S?
i=1
?i,Nki,N?
?
[
N?1?
n=1
S?
i=1
?i,nki,n?
]
+
S?
i=1
?i,Nki,N
=
N?
n=1
S?
i=1
?i,nki,n?
The first inequality uses A1, the second step
?
i ?i,N = 1 and the second inequality the induc-
tive hypothesis IH1. For IH2, in the general case,
we can write:
?w(avg,N)?2 ?
S?
i=1
?i,N?w(i,N)?2
?
S?
i=1
?i,N (?w(avg,N?1)?2 + ki,NR2)
= ?w(avg,N?1)?2 +
S?
i=1
?i,Nki,NR
2
?
[
N?1?
n=1
S?
i=1
?i,nki,nR
2
]
+
S?
i=1
?i,Nki,NR
2
=
N?
n=1
S?
i=1
?i,nki,nR
2
The first inequality is Jensen?s, the second A2, and
the third the inductive hypothesis IH2. Putting to-
gether IH1, IH2 and ?w(avg,N)? ? u ?w(avg,N):
[
N?
n=1
S?
i=1
?i,nki,n
]2
?2 ?
[
N?
n=1
S?
i=1
?i,nki,n
]
R2
which yields:
?N
n=1
?S
i=1 ?i,nki,n ?
R2
?2
4.3 Analysis
If we set each ?n to be the uniform mixture, ?i,n =
1/S, then Theorem 3 guarantees convergence to
a separating hyperplane. If
?S
i=1 ?i,nki,n = 0,
then the previous weight vector already separated
the data. Otherwise,
?N
n=1
?S
i=1 ?i,nki,n is still in-
creasing, but is bounded and cannot increase indefi-
nitely. Also note that if S = 1, then ?1,n must equal
1 for all n and this bound is identical to Theorem 1.
However, we are mainly concerned with how fast
convergence occurs, which is directly related to the
number of training epochs each algorithm must run,
i.e., N in Figure 1 and Figure 3. For the non-
distributed variant of the perceptron we can say that
Nnon dist ? R2/?2 since in the worst case a single
mistake happens on each epoch.2 For the distributed
case, consider setting ?i,n = ki,n/kn, where kn =?
i ki,n. That is, we mix parameters proportional to
the number of errors each made during the previous
epoch. Theorem 3 still implies convergence to a sep-
arating hyperplane with this choice. Further, we can
2It is not hard to derive such degenerate cases.
460
bound the required number of epochs Ndist:
Ndist ?
Ndist?
n=1
S?
i=1
[ki,n]
ki,n
kn ?
Ndist?
n=1
S?
i=1
ki,n
kn
ki,n ?
R2
?2
Ignoring when all ki,n are zero (since the algorithm
will have converged), the first inequality is true since
either ki,n ? 1, implying that [ki,n]ki,n/kn ? 1, or
ki,n = 0 and [ki,n]ki,n/kn = 1. The second inequal-
ity is true by the generalized arithmetic-geometric
mean inequality and the final inequality is Theo-
rem 3. Thus, the worst-case number of epochs is
identical for both the regular and distributed percep-
tron ? but the distributed perceptron can theoreti-
cally process each epoch S times faster. This ob-
servation holds only for cases where ?i,n > 0 when
ki,n ? 1 and ?i,n = 0 when ki,n = 0, which does
not include uniform mixing.
5 Experiments
To investigate the distributed perceptron strategies
discussed in Section 4 we look at two structured pre-
diction tasks ? named entity recognition and depen-
dency parsing. We compare up to four systems:
1. Serial (All Data): This is the classifier returned
if trained serially on all the available data.
2. Serial (Sub Sampling): Shard the data, select
one shard randomly and train serially.
3. Parallel (Parameter Mix): Parallel strategy
discussed in Section 4.1 with uniform mixing.
4. Parallel (Iterative Parameter Mix): Parallel
strategy discussed in Section 4.2 with uniform
mixing (Section 5.1 looks at mixing strategies).
For all four systems we compare results for both the
standard perceptron algorithm as well as the aver-
aged perceptron algorithm (Collins, 2002).
We report the final test set metrics of the con-
verged classifiers to determine whether any loss in
accuracy is observed as a consequence of distributed
training strategies. We define convergence as ei-
ther: 1) the training set is separated, or 2) the train-
ing set performance measure (accuracy, f-measure,
etc.) does not change by more than some pre-defined
threshold on three consecutive epochs. As with most
real world data sets, convergence by training set sep-
aration was rarely observed, though in both cases
training set accuracies approached 100%. For both
tasks we also plot test set metrics relative to the user
wall-clock taken to obtain the classifier. The results
were computed by collecting the metrics at the end
of each epoch for every classifier. All experiments
used 10 shards (Section 5.1 looks at convergence rel-
ative to different shard size).
Our first experiment is a named-entity recogni-
tion task using the English data from the CoNLL
2003 shared-task (Tjong Kim Sang and De Meul-
der, 2003). The task is to detect entities in sentences
and label them as one of four types: people, organi-
zations, locations or miscellaneous. For our exper-
iments we used the entire training set (14041 sen-
tences) and evaluated on the official development
set (3250 sentences). We used a straight-forward
IOB label encoding with a 1st order Markov fac-
torization. Our feature set consisted of predicates
extracted over word identities, word affixes, orthog-
raphy, part-of-speech tags and corresponding con-
catenations. The evaluation metric used was micro
f-measure over the four entity class types.
Results are given in Figure 4. There are a num-
ber of things to observe here: 1) training on a single
shard clearly provides inferior performance to train-
ing on all data, 2) the simple parameter mixing strat-
egy improves upon a single shard, but does not meet
the performance of training on all data, 3) iterative
parameter mixing achieves performance as good as
or better than training serially on all the data, and
4) the distributed algorithms return better classifiers
much quicker than training serially on all the data.
This is true regardless of whether the underlying al-
gorithm is the regular or the averaged perceptron.
Point 3 deserves more discussion. In particular, the
iterative parameter mixing strategy has a higher final
f-measure than training on all the data serially than
the standard perceptron (f-measure of 87.9 vs. 85.8).
We suspect this happens for two reasons. First, the
parameter mixing has a bagging like effect which
helps to reduce the variance of the per-shard classi-
fiers (Breiman, 1996). Second, the fact that parame-
ter mixing is just a form of parameter averaging per-
haps has the same effect as the averaged perceptron.
Our second set of experiments looked at the much
more computationally intensive task of dependency
parsing. We used the Prague Dependency Tree-
bank (PDT) (Hajic? et al, 2001), which is a Czech
461
Wall Clock
0.65
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Parameter Mix)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- Parallel (Parameter Mix)Averaged Perceptron -- Parallel (Iterative Parameter Mix)
Reg. Perceptron Avg. Perceptron
F-measure F-measure
Serial (All Data) 85.8 88.2
Serial (Sub Sampling) 75.3 76.6
Parallel (Parameter Mix) 81.5 81.6
Parallel (Iterative Parameter Mix) 87.9 88.1
Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left)
and averaged perceptron (right). Lower table is f-measure for converged models.
language treebank and currently one of the largest
dependency treebanks in existence. We used the
CoNLL-X training (72703 sentences) and testing
splits (365 sentences) of this data (Buchholz and
Marsi, 2006) and dependency parsing models based
on McDonald and Pereira (2006) which factors fea-
tures over pairs of dependency arcs in a tree. To
parse all the sentences in the PDT, one must use a
non-projective parsing algorithm, which is a known
NP-complete inference problem when not assuming
strong independence assumptions. Thus, the use of
approximate inference techniques is common in or-
der to find the highest weighted tree for a sentence.
We use the approximate parsing algorithm given in
McDonald and Pereira (2006), which runs in time
roughly cubic in sentence length. To train such a
model is computationally expensive and can take on
the order of days to train on a single machine.
Unlabeled attachment scores (Buchholz and
Marsi, 2006) are given in Figure 5. The same trends
are seen for dependency parsing that are seen for
named-entity recognition. That is, iterative param-
eter mixing learns classifiers faster and has a final
accuracy as good as or better than training serially
on all data. Again we see that the iterative parame-
ter mixing model returns a more accurate classifier
than the regular perceptron, but at about the same
level as the averaged perceptron.
5.1 Convergence Properties
Section 4.3 suggests that different weighting strate-
gies can lead to different convergence properties,
in particular with respect to the number of epochs.
For the named-entity recognition task we ran four
experiments comparing two different mixing strate-
gies ? uniform mixing (?i,n=1/S) and error mix-
ing (?i,n=ki,n/kn) ? each with two shard sizes ?
S = 10 and S = 100. Figure 6 plots the number
of training errors per epoch for each strategy.
We can make a couple observations. First, the
mixing strategy makes little difference. The rea-
son being that the number of observed errors per
epoch is roughly uniform across shards, making
both strategies ultimately equivalent. The other ob-
servation is that increasing the number of shards
can slow down convergence when viewed relative to
epochs3. Again, this appears in contradiction to the
analysis in Section 4.3, which, at least for the case
of error weighted mixtures, implied that the num-
ber of epochs to convergence was independent of
the number of shards. But that analysis was based
on worst-case scenarios where a single error occurs
on a single shard at each epoch, which is unlikely to
occur in real world data. Instead, consider the uni-
3As opposed to raw wall-clock/CPU time, which benefits
from faster epochs the more shards there are.
462
Wall Clock
0.74
0.76
0.78
0.8
0.82
0.84
Unla
beled
 Atta
chme
nt Sc
ore
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
Unla
beled
 Atta
chme
nt Sc
ore
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- (Iterative Parameter Mix) 
Reg. Perceptron Avg. Perceptron
Unlabeled Attachment Score Unlabeled Attachment Score
Serial (All Data) 81.3 84.7
Serial (Sub Sampling) 77.2 80.1
Parallel (Iterative Parameter Mix) 83.5 84.5
Figure 5: Dependency Parsing experiments. Upper figures plot test data unlabeled attachment score versus wall clock
for both regular perceptron (left) and averaged perceptron (right). Lower table is unlabeled attachment score for
converged models.
0 10 20 30 40 50Training Epochs
0
2000
4000
6000
8000
10000
# Tra
ining
 Mist
akes
Error mixing (10 shards)Uniform mixing (10 shards)Error mixing (100 shards)Uniform mixing (100 shards)
Figure 6: Training errors per epoch for different shard
size and parameter mixing strategies.
form mixture case. Theorem 3 implies:
N?
n=1
S?
i=1
ki,n
S
?
R2
?2
=?
N?
n=1
S?
i=1
ki,n ? S ?
R2
?2
Thus, for cases where training errors are uniformly
distributed across shards, it is possible that, in the
worst-case, convergence may slow proportional the
the number of shards. This implies a trade-off be-
tween slower convergence and quicker epochs when
selecting a large number of shards. In fact, we ob-
served a tipping point for our experiments in which
increasing the number of shards began to have an ad-
verse effect on training times, which for the named-
entity experiments occurred around 25-50 shards.
This is both due to reasons described in this section
as well as the added overhead of maintaining and
summing multiple high-dimensional weight vectors
after each distributed epoch.
It is worth pointing out that a linear term S in
the convergence bound above is similar to conver-
gence/regret bounds for asynchronous distributed
online learning, which typically have bounds lin-
ear in the asynchronous delay (Mesterharm, 2005;
Zinkevich et al, 2009). This delay will be on aver-
age roughly equal to the number of shards S.
6 Conclusions
In this paper we have investigated distributing the
structured perceptron via simple parameter mixing
strategies. Our analysis shows that an iterative pa-
rameter mixing strategy is both guaranteed to sepa-
rate the data (if possible) and significantly reduces
the time required to train high accuracy classifiers.
However, there is a trade-off between increasing
training times through distributed computation and
slower convergence relative to the number of shards.
Finally, we note that using similar proofs to those
given in this paper, it is possible to provide theoreti-
cal guarantees for distributed online passive aggres-
sive learning (Crammer et al, 2006), which is a form
of large-margin perceptron learning. Unfortunately
space limitations prevent exploration here.
Acknowledgements: We thank Mehryar Mohri, Fer-
nando Periera, Mark Dredze and the three anonymous re-
views for their helpful comments on this work.
463
References
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-Reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithm. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Sixth Sym-
posium on Operating System Design and Implementa-
tion.
M. Dredze, K. Crammer, and F. Pereira. 2008.
Confidence-weighted linear classification. In Pro-
ceedings of the International Conference on Machine
learning.
J. Duchi and Y. Singer. 2009. Efficient learning using
forward-backward splitting. In Advances in Neural In-
formation Processing Systems.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Proceedings of the Conference of the Association
for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, B. Vidova Hladka, J. Panevova?, E. Hajic?ova?,
P. Sgall, and P. Pajas. 2001. Prague Dependency Tree-
bank 1.0. LDC, 2001T10.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models. In
Advances in Neural Information Processing Systems.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Conference of the European Chapter
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
C. Mesterharm. 2005. Online learning with delayed la-
bel feedback. In Proceedings of Algorithmic Learning
Theory.
A.B. Novikoff. 1962. On convergence proofs on percep-
trons. In Symposium on the Mathematical Theory of
Automata.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986.
Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE Transactions
on Automatic Control, 31(9):803?812.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the International Conference on Machine learning.
C. Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar.
2008. Web-scale named entity recognition. In Pro-
ceedings of the International Conference on Informa-
tion and Knowledge Management.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
T. Zhang. 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
In Proceedings of the International Conference on Ma-
chine Learning.
M. Zinkevich, A. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems.
464
