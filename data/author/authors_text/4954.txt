Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 505?512
Manchester, August 2008
Tera-Scale Translation Models via Pattern Matching
Adam Lopez
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, United Kingdom
alopez@inf.ed.ac.uk
Abstract
Translation model size is growing at a pace
that outstrips improvements in computing
power, and this hinders research on many
interesting models. We show how an al-
gorithmic scaling technique can be used
to easily handle very large models. Us-
ing this technique, we explore several large
model variants and show an improvement
1.4 BLEU on the NIST 2006 Chinese-
English task. This opens the door for work
on a variety of models that are much less
constrained by computational limitations.
1 Introduction
Translation model size is growing quickly due to
the use of larger training corpora and more com-
plex models. As an example of the growth in avail-
able training data, consider the curated Europarl
corpus (Koehn, 2005), which more than doubled in
size from 20 to 44 million words between 2003 and
2007.1 As an example of model complexity, con-
sider the popular hierarchical phrase-based model
of Chiang (2007), which can translate discontigu-
ous phrases. Under the loosest interpretation of
this capability, any subset of words in a sentence
This research was conducted while I was at the University
of Maryland. I thank David Chiang, Bonnie Dorr, Doug Oard,
Philip Resnik, and the anonymous reviewers for comments,
and especially Chris Dyer for many helpful discussions and
for running the Moses experiments. This research was sup-
ported by the GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-2-001 and
by the EuroMatrix project funded by the European Commis-
sion (6th Framework Programme).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Statistics from http://www.statmt.org/europarl.
can be a phrase. Therefore, the number of rules
that the model can learn is exponential in sentence
length unless strict heuristics are used, which may
limit the model?s effectiveness. Many other mod-
els translate discontiguous phrases, and the size of
their extracted rulesets is such a pervasive problem
that it is a recurring topic in the literature (Chiang,
2007; DeNeefe et al, 2007; Simard et al, 2005).
Most decoder implementations assume that all
model rules and parameters are known in advance.
With very large models, computing all rules and
parameters can be very slow. This is a bottleneck
in experimental settings where we wish to explore
many model variants, and therefore presents a real
impediment to full exploration of their potential.
We present a solution to this problem.
To fully motivate the discussion, we give a con-
crete example of a very large model, which we
generate using simple techniques that are known
to improve translation accuracy. The model takes
77 CPU days to compute and consumes nearly a
terabyte of external storage (?2). We show how
to solve the problem with a previously developed
algorithmic scaling technique that we call transla-
tion by pattern matching (?3). The key idea be-
hind this technique is that rules and parameters are
computed only as needed. Using this technique,
we explore a series of large models, giving experi-
mental results along a variety of scaling axes (?4).
Our results extend previous findings on the use of
long phrases in translation, shed light on the source
of improved performance in hierarchical phrase-
based models, and show that our tera-scale trans-
lation model outperforms a strong baseline.
2 A Tera-Scale Translation Model
We will focus on the hierarchical phrase-based
model of Chiang (2007). It compares favorably
505
with conventional phrase-based translation (Koehn
et al, 2003) on Chinese-English news translation
(Chiang, 2007). We found that a baseline system
trained on 27 million words of news data is already
quite strong, but we suspect that it would be possi-
ble to improve it using some simple techniques.
Add additional training data. Our baseline
already uses much of the available curated news
data, but there is at least three times as much cu-
rated data available in the United Nations proceed-
ings. Adding the UN data gives us a training cor-
pus of 107 million words per language.
Change the word alignments. Our baseline
uses Giza++ alignments (Och and Ney, 2003)
symmetrized with the grow-diag-final-and heuris-
tic (Koehn et al, 2003). We replace these with
the maximum entropy aligments of Ayan and Dorr
(2006b). They reported improvements of 1.6
BLEU in Chinese-English translation, though with
much less training data.
Change the bilingual phrase extraction
heuristic. Our baseline uses a tight heuristic, re-
quiring aligned words at phrase edges. However,
Ayan and Dorr (2006a) showed that a loose heuris-
tic, allowing unaligned words at the phrase edges,
improved accuracy by 3.7 BLEU with some align-
ments, again with much less training data.
Quadrupling the amount of training data pre-
dictably increases model size. The interaction of
the alignment and phrase extraction heuristic in-
creases the model size much more. This is because
the maximum entropy alignments are sparse?
fewer than 70% of the words are aligned. Con-
sider a contiguous phrase chosen at random from a
training sentence. With our sparse alignments, the
chance that both of its edge words are aligned is
less than half. The tight heuristic discards many
possible phrases on this basis alone. The situa-
tion worsens with discontiguous phrases. How-
ever, with the loose heuristic, we see the oppo-
site effect. Not only is a randomly chosen source
phrase with unaligned edge words legal, but it may
have many translations, since its minimal align-
ment in the target is likely to have one or more
adjacent unaligned words, and any combination of
these can be part of a valid target phrase.
To make matters concrete, we estimated the size
of the translation model that would be produced
using these modifications. We did not actually
compute the full model, for reasons that will be-
come apparent. Instead, we modified Chiang?s ex-
tractor to simply report the number of rules that it
would normally extract. We then computed the ra-
tio of extracted rules to that of the baseline system.
Under the rough assumption that the number of
unique rules and representation size grows linearly
in the number of extracted rules, we were then able
to estimate the size of the large model. The results
show that it would be impractical to compute the
model (Table 1). Merely counting all of the rule
occurrences took nearly 5 days on our 17-node re-
search cluster. This does not even include the time
required for sorting and scoring the rules, which
we did not attempt. The resulting model would
be nearly two orders of magnitude larger than the
largest one we could find in the literature (Table 2).
3 Translation by Pattern Matching
Clearly, substantial experimentation with models
this large is impossible unless we have consider-
able resources at our disposal. To get around this
problem, we use an algorithmic scaling technique
that we call translation by pattern matching. In this
approach, the training text and its word alignment
reside in memory. We then translate as follows.
for each input sentence do
for each possible phrase in the sentence do
Find its occurrences in the source text
for each occurrence do
Extract its aligned target phrase (if any)
for each extracted phrase pair do
Score using maximum likelihood
Decode as usual using the scored rules
A similar method is used in example-based
translation (Brown, 2004). It was applied to
phrase-based translation by Callison-Burch et al
(2005) and Zhang and Vogel (2005). The key point
is that the complete translation model is never ac-
tually computed?rules and associated parameters
are computed only as needed. Obviously, the on-
demand computation must be very fast. If we can
achieve this, then the model can in principle be ar-
bitrarily large. Callison-Burch et al (2005) and
Zhang and Vogel (2005) give very similar recipes
for application to phrase-based models.
Fast lookup using pattern matching algo-
rithms. The complexity of the na??ve algorithm to
find all occurrences of a source phrase in a train-
ing text T is linear in the length of the text, O(|T |).
This is much too slow for large texts. They solve
this using an index data structure called a suffix
506
Baseline Large
Rules extracted (millions) 195 19,300
Extract time (CPU hours) 10.8 1,840
Unique rules (millions) 67 6,600*
Extract file size (GB) 9.3 917*
Model size (GB) 6.1 604*
Table 1: Extraction time and model sizes. The
model size reported is the size of the files contain-
ing an external prefix tree representation (Zens and
Ney, 2007). *Denotes estimated quantities.
Citation Millions of rules
Simard et al (2005) (filtered) 4
Chiang (2007) (filtered) 6
DeNeefe et al (2007) 57
Zens and Ney (2007) 225
this paper 6,600
Table 2: Model sizes in the literature.
array (Manber and Myers, 1993). Its size is 4|T |
bytes and it enables lookup of any length-m sub-
string of T in O(m+ log |T |) time.
Fast extraction using sampling. The complex-
ity of extracting target phrases is linear in the num-
ber of source phrase occurrences. For very fre-
quent source phrases, this is expensive. They solve
this problem by extracting only from a sample of
the found source phrases, capping the sample size
at 100. For less frequent source phrases, all possi-
ble targets are extracted.
Fast scoring using maximum likelihood.
Scoring the phrase pairs is linear in the number
of pairs if we use the maximum likelihood esti-
mate p(e|f) for source phrase f and target phrase
e. Since each source phrase only has small number
of targets (up to the sample limit), this step is fast.
However, notice that we cannot easily compute the
target-to-source probability p(f |e) as is commonly
done. We address this in ?4.1.
Callison-Burch et al (2005) and Zhang and Vo-
gel (2005) found that these steps added only tenths
of a second to per-sentence decoding time, which
we independently confirmed (Lopez, 2008, Chap-
ter 3). Their techniques also apply to discontigu-
ous phrases, except for the pattern matching algo-
rithm, which only works for contiguous phrases.
Since our model (and many others) uses discon-
tiguous phrases, we must use a different algorithm.
The most straightforward way to accomplish this
is to use the fast suffix array lookup for the con-
tiguous subphrases of a discontiguous phrase, and
then to combine the results. Suppose that we have
substrings u and v, and a gap character X which
can match any arbitrary sequence of words. Then
to look up the phrase uXv, we first find all occur-
rences of u, and all occurrences of v. We can then
compute all cases where an occurrence of u pre-
cedes and occurrence of v in the same sentence.
The complexity of this last step is linear in the
number of occurrences of u and v. If either u or
v is very frequent, this is too slow. Lopez (2007;
2008) solves this with a series of empirically fast
exact algorithms. We briefly sketch the solution
here; for details see Lopez (2008, Chapter 4).
Lossless pruning. For each phrase, we only
search if we have already successfully found both
its longest suffix and longest prefix. For example,
if a, b, c, and d are all words, then we only search
for phrase abXcd if we have already found occur-
rences of phrases abXc and bXcd.
Precomputation of expensive searches. For
phrases containing multiple very frequent sub-
phrases, we precompute the list of occurrences into
an inverted index. That is, if both u and v are fre-
quent, we simply precompute all locations of uXv
and vXu.
Fast merge algorithm. For phrases pairing a
frequent subphrase with infrequent subphrases, we
use a merge algorithm whose upper bound com-
plexity is logarithmic in the number of occurrences
of the frequent subphrase. That is, if count(u) is
small, and count(v) is big, then we can find uXv
in at most O(count(u) ? log(count(v))) time.
Our implementation is a fast extension to the Hi-
ero decoder (Chiang, 2007), written in Pyrex.2 It
is an order of magnitude faster than the Python im-
plementation of Lopez (2007). Pattern matching,
extraction, and scoring steps add approximately
2 seconds to per-sentence decoding time, slow-
ing decoding by about 50% compared with a con-
ventional exact model representation using exter-
nal prefix trees (Zens and Ney, 2007). See Lopez
(2008, Chapter 4) for analysis.
4 Experiments
Although the algorithmic issues of translation by
pattern matching are largely solved, none of the
previous work has reported any improvements in
2Pyrex combines Python and C code for performance.
http://www.cosc.canterbury.ac.nz/greg.ewing/python/Pyrex/
507
state of the art with very large models.3 In the
remainder of this work, we scratch the surface of
possible uses.
We experimented on Chinese-English newswire
translation. Except where noted, each system was
trained on 27 million words of newswire data,
aligned with GIZA++ (Och and Ney, 2003) and
symmetrized with the grow-diag-final-and heuris-
tic (Koehn et al, 2003). In all experiments that fol-
low, each system configuration was independently
optimized on the NIST 2003 Chinese-English test
set (919 sentences) using minimum error rate train-
ing (Och, 2003) and tested on the NIST 2005
Chinese-English task (1082 sentences). Optimiza-
tion and measurement were done with the NIST
implementation of case-insensitive BLEU 4n4r
(Papineni et al, 2002).4
4.1 Baseline
We compared translation by pattern matching with
a conventional exact model representation using
external prefix trees (Zens and Ney, 2007). To
make model computation efficient for the latter
case, we followed the heuristic limits on phrase ex-
traction used by Chiang (2007).
? Phrases were restricted to five words. Each
gap character counts as a single word regard-
less of how many actual words it spans. Thus
phrase aXb consisting of words a and b sep-
arated by a gap is three words.
? Phrases were restricted to a span of ten words
in the training data.
? Phrases were restricted to two gaps.
? Gaps were required to span at least two words
in the training data.
? Phrases were extracted using a tight heuristic.
Chiang (2007) uses eight features, so we incor-
porate these into the conventional baseline. How-
ever, as discussed previously in ?3, the pattern
matching architecture makes it difficult to compute
the target-to-source translation probability, so this
feature is not included in the pattern matching sys-
tem. This may not be a problem?Och and Ney
3Zhang and Vogel (2005) report improvements, but all of
their results are far below state of the art for the reported task.
This may be because their system was not tuned using mini-
mum error rate training (Och, 2003).
4ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
(2002) observed that this feature could be replaced
by the source-to-target probability without loss of
accuracy. Preliminary experiments suggested that
two other features in Chiang?s model based on rule
counts were not informative, so we considered a
model containing only five features.
1. Sum of logarithms of source-to-target phrase
translation probabilities.
2. Sum of logarithms of source-to-target lexical
weighting (Koehn et al, 2003).
3. Sum of logarithms of target-to-source lexical
weighting.
4. Sum of logarithms of a trigram language
model.
5. A word count feature.
The sample size (see ?3) for the pattern match-
ing system was 300.5 Results show that both trans-
lation by pattern matching and reduced feature set
are harmless to translation accuracy (Table 3).
4.2 Rapid Prototyping via Pattern Matching
Even in this limited experiment, translation by pat-
tern matching improved experimental turnaround.
For the conventional system, it took 10.8 hours to
compute the full model, which required 6.1GB of
space. For the pattern matching system, it took
only 8 minutes to compute all data structures and
indexes. These required only 852MB of space.
There are two tradeoffs. First, memory use is
higher in the pattern matching system, since the
conventional representation resides on disk. Sec-
ond, per-sentence decoding time with the pattern
matching system is slower by about 2 seconds due
to the expense of computing rules and parameters
on demand. Even so, the experimental turnaround
time with the pattern matching system was still
faster. We would need to decode nearly 20,000
sentences with it to equal the computation time of
conventional model construction. We might need
to decode this many times during MERT, but only
because it decodes the same test set many times.
However, it is straightforward to extract all rules
for the development set using pattern matching,
and use them in a conventional system for MERT.
5We use deterministic sampling, which is useful for repro-
ducibility and for minimum error rate training (Och, 2003).
See Lopez (2008, Chapter 3) for details.
508
System BLEU
Conventional (eight features) 30.7
Conventional (five features) 30.6
Pattern matching (five features) 30.9
Table 3: Baseline system results.
Taking our five-feature pattern matching sys-
tem as a starting point, we next considered several
ways in which we might scale up the translation
model. Here the benefits of prototyping become
more apparent. If we were to run the following
experiments in a conventional system, we would
need to compute a new model for each condition.
With translation by pattern matching, nearly ev-
ery variant uses the same underlying representa-
tion, so it was rarely even necessary to recompute
data structures and indexes.
4.3 Relaxing Length Restrictions
Increasing the maximum phrase length in standard
phrase-based translation does not improve BLEU
(Koehn et al, 2003; Zens and Ney, 2007). How-
ever, this effect has not yet been evaluated in hier-
archical phrase-based translation.
We experimented with two analogues to the
maximum phrase length. First, we varied the limit
on source phrase length (counting each gap as a
single word), the closest direct analogue. Chiang
(2007) used a limit of five. We found that accuracy
plateaus at this baseline setting (Figure 1). Second,
we varied the limit on the span of phrases extracted
from the training text. Suppose we are interested in
a source phrase uXv. If u and v are collocated in a
training sentence, but within a span longer than the
limit, then the model is prevented from learning a
rule that translates this discontiguous phrase as a
single unit. Chiang (2007) fixes this limit at ten.
Again, accuracy plateaus near the baseline setting
(Figure 2).
Our results are similar to those for conventional
phrase-based models (Koehn et al, 2003; Zens and
Ney, 2007). Though scaling along these axes is un-
helpful, there is still a large space for exploration.
4.4 Interlude: Hierarchical Phrase-Based
Translation versus Lexical Reordering
In a related line of inquiry, we considered the ef-
fect of increasing the number of gaps in phrases.
Chiang (2007) limits them to two. Although we
consider more than this, we also considered fewer,
2 4 6 8 10
16
.9 2
5.
4 29
.7
30
.4
30
.9
30
.9
30
.8
30
.8
30
.9
30
.7
BLEU
Figure 1: Effect of the maximum phrase length.
1 3 5 7 9 11 13 15
16
.1 2
5.
5
28
.5
30
.3
30
.8
31
.1
31
.2
30
.8
BLEU
Figure 2: Effect of the maximum phrase span.
to gain insight into the hierarchical model.
Hierarchical phrase-based translation is often
reported to be better than conventional phrase-
based translation, but the actual reason for this is
unknown. It is often argued that the ability to trans-
late discontiguous phrases is important to model-
ing translation (Chiang, 2007; Simard et al, 2005;
Quirk and Menezes, 2006), and it may be that this
explains the results. However, there is another hy-
pothesis. The model can also translate phrases in
the form uX or Xu (a single contiguous unit and
a gap). If it learns that uX often translates as
Xu
?
, then in addition to learning that u translates
as u?, it has also learned that u switches places with
a neighboring phrase during translation. This is
similar to lexicalized reordering in conventional
phrase-based models (Tillman, 2004; Al-Onaizan
and Papineni, 2006).6 If this is the real benefit of
the hierarchical model, then the ability to translate
discontiguous phrases may be irrelevant.
To tease apart these claims, we make the follow-
ing distinction. Rules in which both source and tar-
get phrases contain a single contiguous element?
that is, in the form u, Xu, uX , or XuX?
encode lexicalized reordering in hierarchical form.
Rules representing the translation of discontigu-
ous units?minimally uXv?encode translation
knowledge that is strictly outside the purview of
lexical reordering.
We ran experiments varying both the number of
contiguous subphrases and the number of gaps (Ta-
6This hypothesis was suggested independently in personal
communications with several researchers, including Chris
Callison-Burch, Chris Dyer, Alex Fraser, and Franz Och.
509
ble 4). For comparison, we also include results
of the phrase-based system Moses (Koehn et al,
2007) with and without lexicalized reordering.
Our results are consistent with those found else-
where in the literature. The strictest setting allow-
ing no gaps replicates a result in Chiang (2007, Ta-
ble 7), with significantly worse accuracy than all
others. The most striking result is that the accu-
racy of Moses with lexicalized reordering is indis-
tinguishable from the accuracy of the full hierar-
chical system. Both improve over non-lexicalized
Moses by about 1.4 BLEU. The hierarchical emu-
lation owes its performance only partially to lex-
icalized reordering. Additional improvement is
seen when we add discontiguous phrases. That the
effect of lexicalized reordering is weaker in the hi-
erarchical model is unsurprising, since its parame-
terization is much simpler than the one used by the
Moses, which includes several specialized features
for this purpose. This suggests that the hierarchical
model could be improved through better parame-
terization, and still benefit from the translation of
discontiguous phrases.
Finally, we observe that using more gaps does
not improve the hierarchical model.
4.5 The Tera-Scale Model
These are interesting scientific findings, but we
have so far failed to show an improvement over
the baseline. For this, we return to the tera-scale
model of ?2. Recall that in this model, we mod-
ify the baseline by adding 80 million words of
UN data and using sparse maximum entropy align-
ments with a loose phrase extraction heuristic.
To avoid conflating rules learned from in-
domain newswire and out-of-domain UN data, we
treat each corpus independently. We sample from
up to 300 source phrase occurrences from each,
and compute lexical weighting and the source-
to-target phrase translation probabilities separately
for both samples. For the UN corpus, the resulting
probabilities are incorporated into three new fea-
tures. These features receive a value of zero for
any rule computed from the newswire data. Like-
wise, the baseline source-to-target phrase transla-
tion probability and lexical weighting features re-
ceive a value of zero for rules computed from the
UN data.
We make one more modification to the model
that is quite easy with pattern matching. We notice
that it is not always possible to extract a transla-
Subphrases Gaps Example BLEU
1 0 u 26.3
1 1 uX,Xu 30.2*
1 2 XuX 30.0*
2 1 uXv 30.5
2 2 uXvX,XuXv 30.8
3 2 uXvXw 30.9
4 3 uXvXwXy 30.9
5 4 uXvXwXyXz 30.8
Moses without lexicalized reordering 29.4
Moses with lexicalized reordering 30.7
Table 4: Comparison with Moses and effect of the
maximum number of subphrases and gaps. *De-
notes emulation of lexicalized reordering.
tion for a source phrase occurrence, even under the
loose heuristic. This is because there may be no
consistently aligned target phrase according to the
alignment. If a phrase occurs frequently but we
can only rarely extract a translation for it, then our
confidence that it represents a natural unit of trans-
lation should diminish. Conversely, if we usually
extract a translation, then the phrase is probably
a good unit of translation. We call this property
coherence. Conventional offline extraction meth-
ods usually ignore coherence. If a phrase occurs
many times but we can only extract a translation
for it a few times, then those translations tend to
receive very high probabilities, even though they
might simply be the result of noisy alignments.
We can incorporate the notion of coherence di-
rectly into the phrase translation probability. In the
baseline model, the denominator of this probabil-
ity is the sum of the number of rule occurrences
containing the source phrase, following Koehn et
al. (2003).7 We replace this with the number of
attempted extractions. This parameterization may
interact nicely with the loose extraction heuris-
tic, reducing the probability of many greedily ex-
tracted but otherwise noisy phrases.
We compared the baseline with our tera-scale
model. Since we had already performed substan-
tial experimentation with the NIST 2005 set, we
also included the NIST 2006 task as a new held-
out test set. Results including variants produced
by ablating a single modification on the develop-
ment set are given in Table 5.
We also compared our modified system with
an augmented baseline using a 5-gram language
7Or the sample size, whichever is less.
510
NIST 2005 NIST 2006
System BLEU loss BLEU
Tera-Scale Model (all modifications) 32.6 ? 28.4
with grow-diag-final-and instead of maximum entropy alignment 32.1 -0.5
with tight extraction heuristic instead of loose 31.6 -1.0
without UN data 31.6 -1.0
without separate UN features 32.2 -0.4
with standard p(f |e) instead of coherent p(f |e) 31.7 -0.9
Baseline (conventional) 30.7 -1.9
Baseline (pattern matching) 30.9 -1.7 27.0
Table 5: Results of scaling modifications and ablation experiments.
model and rule-based number translation. The ob-
jective of this experiment is to ensure that our im-
provements are complementary to better language
modeling, which often subsumes other improve-
ments. The new baseline achieves a score of 31.9
on the NIST 2005 set, making it nearly the same
as the state-of-the-art results reported by Chiang
(2007). Our modifications increase this to 34.5, a
substantial improvement of 2.6 BLEU.
5 Related Work and Open Problems
There are several other useful approaches to scal-
ing translation models. Zens and Ney (2007) re-
move constraints imposed by the size of main
memory by using an external data structure. John-
son et al (2007) substantially reduce model size
with a filtering method. However, neither of
these approaches addresses the preprocessing bot-
tleneck. To our knowledge, the strand of research
initiated by Callison-Burch et al (2005) and Zhang
and Vogel (2005) and extended here is the first to
do so. Dyer et al (2008) address this bottleneck
with a promising approach based on parallel pro-
cessing, showing reductions in real time that are
linear in the number of CPUs. However, they do
not reduce the overall CPU time. Our techniques
also benefit from parallel processing, but they re-
duce overall CPU time, thus comparing favorably
even in this scenario.8 Moreover, our method
works even with limited parallel processing.
Although we saw success with this approach,
there are some interesting open problems. As dis-
cussed in ?4.2, there are tradeoffs in the form of
slower decoding and increased memory usage. De-
coding speed might be partially addressed using
a mixture of online and offline computation as in
Zhang and Vogel (2005), but faster algorithms are
8All of our reported decoding runs were done in parallel.
still needed. Memory use is important in non-
distributed systems since our data structures will
compete with the language model for memory. It
may be possible to address this problem with a
novel data structure known as a compressed self-
index (Navarro and Ma?kinen, 2007), which sup-
ports fast pattern matching on a representation that
is close in size to the information-theoretic mini-
mum required by the data.
Our approach is currently limited by the require-
ment for very fast parameter estimation. As we
saw, this appears to prevent us from computing the
target-to-source probabilities. It would also appear
to limit our ability to use discriminative training
methods, since these tend to be much slower than
the analytical maximum likelihood estimate. Dis-
criminative methods are desirable for feature-rich
models that we would like to explore with pattern
matching. For example, Chan et al (2007) and
Carpuat and Wu (2007) improve translation ac-
curacy using discriminatively trained models with
contextual features of source phrases. Their fea-
tures are easy to obtain at runtime using our ap-
proach, which finds source phrases in context.
However, to make their experiments tractable, they
trained their discriminative models offline only for
the specific phrases of the test set. Combining dis-
criminative learning with our approach is an open
problem.
6 Conclusion
We showed that very large translation models
present an interesting engineering challenge, and
illustrated a solution to this challenge using pattern
matching algorithms. This enables practical, rapid
exploration of vastly larger models than those cur-
rently in use. We believe that many other improve-
ments are possible when the size of our models is
511
unconstrained by resource limitations.
References
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proc. of ACL-COLING, pages 529?536, Jul.
Necip Fazil Ayan and Bonnie Dorr. 2006a. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proc. of ACL-
COLING, pages 9?16, Jul.
Necip Fazil Ayan and Bonnie J. Dorr. 2006b. A max-
imum entropy approach to combining word align-
ments. In Proc. of HLT-NAACL, pages 96?103, Jun.
Ralf D. Brown. 2004. A modified Burrows-Wheeler
transform for highly-scalable example-based trans-
lation. In Proc. of AMTA, number 3265 in LNCS,
pages 27?36. Springer, Sep.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proc. of ACL, pages 255?262, Jun.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proc. of ACL, pages 61?72, Jun.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statis-
tical machine translation. In Proc. of ACL, pages
33?40, Jun.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proc. of EMNLP-
CoNLL, pages 755?763, Jun.
Chris Dyer, Aaron Cordova, Alex Mont, and Jimmy
Lin. 2008. Fast, easy, and cheap: Construction of
statistical machine translation models with mapre-
duce. In Proc. of the Workshop on Statistical Ma-
chine Translation, pages 199?207, Jun.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proc. of
EMNLP-CoNLL, pages 967?975, Jun.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 127?133, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of ACL Demo and Poster Sessions, pages 177?
180, Jun.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of MT Sum-
mit.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP-CoNLL,
pages 976?985, Jun.
Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. thesis, University of Maryland,
Mar.
Udi Manber and Gene Myers. 1993. Suffix arrays: A
new method for on-line string searches. SIAM Jour-
nal of Computing, 22(5):935?948.
Gonzalo Navarro and Veli Ma?kinen. 2007. Com-
pressed full-text indexes. ACM Computing Surveys,
39(1), Apr.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for ma-
chine translation. In Proc. of ACL, pages 156?163,
Jul.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Jul.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
statistical machine translation. In Proc. of HLT-
NAACL, pages 8?16, Jun.
Michel Simard, Nicola Cancedda, Bruno Cavestro,
Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji
Yamada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Proc. of
HLT-EMNLP, pages 755?762, Oct.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Proc. of HLT-
NAACL: Short Papers, pages 101?104, May.
Richard Zens and Hermann Ney. 2007. Efficient
phrase-table representation for machine translation
with applications to online MT and speech transla-
tion. In Proc. of HLT-NAACL.
Ying Zhang and Stephan Vogel. 2005. An effi-
cient phrase-to-phrase alignment model for arbitrar-
ily long phrase and large corpora. In Proc. of EAMT,
May.
512
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 532?540,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Translation as Weighted Deduction
Adam Lopez
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB
United Kingdom
alopez@inf.ed.ac.uk
Abstract
We present a unified view of many trans-
lation algorithms that synthesizes work on
deductive parsing, semiring parsing, and
efficient approximate search algorithms.
This gives rise to clean analyses and com-
pact descriptions that can serve as the ba-
sis for modular implementations. We illus-
trate this with several examples, showing
how to build search spaces for several dis-
parate phrase-based search strategies, inte-
grate non-local features, and devise novel
models. Although the framework is drawn
from parsing and applied to translation, it
is applicable to many dynamic program-
ming problems arising in natural language
processing and other areas.
1 Introduction
Implementing a large-scale translation system is
a major engineering effort requiring substantial
time and resources, and understanding the trade-
offs involved in model and algorithm design de-
cisions is important for success. As the space of
systems described in the literature becomes more
crowded, identifying their common elements and
isolating their differences becomes crucial to this
understanding. In this work, we present a com-
mon framework for model manipulation and anal-
ysis that accomplishes this, and use it to derive sur-
prising conclusions about phrase-based models.
Most translation algorithms do the same thing:
dynamic programming search over a space of
weighted rules (?2). Fortunately, we need
not search far for modular descriptions of dy-
namic programming algorithms. Deductive logic
(Pereira and Warren, 1983), extended with semir-
ings (Goodman, 1999), is an established formal-
ism used in parsing. It is occasionally used
to describe formally syntactic translation mod-
els, but these treatments tend to be brief (Chiang,
2007; Venugopal et al, 2007; Dyer et al, 2008;
Melamed, 2004). We apply weighted deduction
much more thoroughly, first extending it to phrase-
based models and showing that the set of search
strategies used by these models have surprisingly
different implications for model and search error
(?3, ?4). We then show how it can be used to an-
alyze common translation problems such as non-
local parameterizations (?5), alignment, and novel
model design (?6). Finally, we show that it leads to
a simple analysis of cube pruning (Chiang, 2007),
an important approximate search algorithm (?7).
2 Translation Models
A translation model consists of two distinct ele-
ments: an unweighted ruleset, and a parameteriza-
tion (Lopez, 2008). A ruleset licenses the steps by
which a source string f1...fI may be rewritten as
a target string e1...eJ , thereby defining the finite
set of all possible rewritings of a source string. A
parameterization defines a weight function over
every sequence of rule applications.
In a phrase-based model, the ruleset is simply
the unweighted phrase table, where each phrase
pair fi...fi?/ej ...ej? states that phrase fi...fi? in the
source is rewritten as ej ...ej? in the the target.
The model operates by iteratively applying
rewrites to the source sentence until each source
word has been consumed by exactly one rule. We
call a sequence of rule applications a derivation.
A target string e1...eJ yielded by a derivation D is
obtained by concatenating the target phrases of the
rules in the order in which they were applied. We
define Y (D) to be the target string yielded by D.
532
Now consider the Viterbi approximation to a
noisy channel parameterization of this model,
P (f |D) ? P (D).1 We define P (f |D) in the stan-
dard way.
P (f |D) =
?
fi...fi?/ej ...ej??D
p(fi...fi? |ej ...ej?)
(1)
Note that in the channel model, we can replace any
rule application with any other rule containing the
same source phrase without affecting the partial
score of the rest of the derivation. We call this a
local parameterization.
Now we define a standard n-gram model P (D).
P (D) =
?
ej?Y (D)
p(ej |ej?n+1...ej?1) (2)
This parameterization differs from the channel
model in an important way. If we replace a single
rule in the derivation, the partial score of the rest
of derivation is also affected, because the terms
ej?n+1...ej may come from more than one rule. In
other words, this parameterization encodes a de-
pendency between the steps in a derivation. We
call this a non-local parameterization.
3 Translation As Deduction
For the first part of the discussion that follows, we
consider deductive logics purely over unweighted
rulesets. As a way to introduce deductive logic, we
consider the CKY algorithm for context-free pars-
ing, a common example that we will revisit in ?6.2.
It is also relevant since it can form the basis of a
decoder for inversion transduction grammar (Wu,
1996). In the discussion that follows, we useA,B,
and C to denote arbitrary nonterminal symbols, S
to denote the start nonterminal symbol, and a to
denote a terminal symbol. CKY works on gram-
mars in Chomsky normal form: all rules are either
binary as in A? BC, or unary as in A? a.
The number of possible binary-branching
parses of a sentence is defined by the Catalan num-
ber, an exponential combinatoric function (Church
and Patil, 1982), so dynamic programming is cru-
cial for efficiency. CKY computes all parses in
cubic time by reusing subparses. To parse a sen-
tence a1...aK , we compute a set of items in the
form [A, k, k?], whereA is a nonterminal category,
1The true noisy channel parameterization p(f |e) ? p(e)
would require a marginalization over D, and is intractable
for most models.
k and k? are both integers in the range [0, n]. This
item represents the fact that there is some parse of
span ak+1...ak? rooted at A (span indices are on
the spaces between words). CKY works by creat-
ing items over successively longer spans. First it
creates items [A, k?1, k] for any ruleA? a such
that a = ak. It then considers spans of increasing
length, creating items [A, k, k?] whenever it finds
two items [B, k, k??] and [C, k??, k?] for some gram-
mar ruleA? BC and some midpoint k??. Its goal
is an item [S, 0,K], indicating that there is a parse
of a1...aK rooted at S.
A CKY logic describes its actions as inference
rules, equivalent to Horn clauses. The inference
rule is a list of antecedents, items and rules that
must all be true for the inference to occur; and a
single consequent that is inferred. To denote the
creation of item [A, k, k?] based on existence of
rule A? BC and items [B, k, k??] and [C, k??, k?],
we write an inference rule with antecedents on the
top line and consequent on the second line, follow-
ing Goodman (1999) and Shieber et al (1995).
R(A? BC) [B, k, k??] [C, k??, k?]
[A, k, k?]
We now give the complete Logic CKY.
item form: [A, k, k?] goal: [S, 0,K]
rules:
?
????
????
R(A? ak)
[A, k ? 1, k]
R(A? BC) [B, k, k??] [C, k??, k?]
[A, k, k?]
(Logic CKY)
A benefit of this declarative description is that
complexity can be determined by inspection
(McAllester, 1999). We elaborate on complexity
in ?7, but for now it suffices to point out that the
number of possible items and possible deductions
depends on the product of the domains of the free
variables. For example, the number of possible
CKY items for a grammar with G nonterminals
is O(GK2), because k and k? are both in range
[0,K]. Likewise, the number of possible inference
rules that can fire is O(G3K3).
3.1 A Simple Deductive Decoder
For our first example of a translation logic we con-
sider a simple case: monotone decoding (Marin?o
et al, 2006; Zens and Ney, 2004). Here, rewrite
rules are applied strictly from left to right on the
source sentence. Despite its simplicity, the search
533
space can be very large?in the limit there could
be a translation for every possible segmentation
of the sentence, so there are exponentially many
possible derivations. Fortunately, we know that
monotone decoding can easily be cast as a dy-
namic programming problem. For any position i
in the source sentence f1...fI , we can freely com-
bine any partial derivation covering f1...fi on its
left with any partial derivation covering fi+1...fI
on its right to yield a complete derivation.
In our deductive program for monotone decod-
ing, an item simply encodes the index of the right-
most word that has been rewritten.
item form: [i]
goal: [I]
rule:
[i] R(fi+1...fi?/ej ...ej?)
[i?]
(Logic MONOTONE)
This is the algorithm of Zens and Ney (2004).
With a maximum phrase length of m, i? will range
over [i+1,min(i+m, I)], giving a complexity of
O(Im). In the limit it is O(I2).
3.2 More Complex Decoders
Now we consider phrase-based decoders with
more permissive reordering. In the limit we al-
low arbitrary reordering, so our item must contain
a coverage vector. Let V be a binary vector of
length I; that is, V ? {0, 1}I . Le 0m be a vec-
tor of m 0?s. For example, bit vector 00000 will
be abbreviated 05 and bit vector 000110 will be
abbreviated 031201. Finally, we will need bitwise
AND (?) and OR (?). Note that we impose an ad-
ditional requirement that is not an item in the de-
ductive system as a side condition (we elaborate
on the significance of this in ?4).
item form: [{0, 1}I ] goal: [1I ]
rule:
[V ] R(fi+1...fi?/ej ...ej?)
[V ? 0i1i??i0I?i? ]
V ? 0i1i
??i0I?i
?
= 0I
(Logic PHRASE-BASED)
The runtime complexity is exponential, O(I22I).
Practical decoding strategies are more restrictive,
implementing what is frequently called a distor-
tion limit or reordering limit. We found that these
terms are inexact, used to describe a variety of
quite different strategies.2 Since we did not feel
that the relationship between these various strate-
gies was obvious or well-known, we give logics
2Costa-jussa` and Fonollosa (2006) refer to the reordering
limit and distortion limit as two distinct strategies.
for several of them and a brief analysis of the
implications. Each strategy uses a parameter d,
generically called the distortion limit or reorder-
ing limit.
The Maximum Distortion d strategy (MDd)
requires that the first word of a phrase chosen for
translation be within d words of the the last word
of the most recently translated phrase (Figure 1).3
The effect of this strategy is that, up to the last
word covered in a partial derivation, there must be
a covered word in every d words. Its complexity
is O(I32d).
MDd can produce partial derivations that cannot
be completed by any allowed sequence of jumps.
To prevent this, the Window Length d strategy
(WLd) enforces a tighter restriction that the last
word of a phrase chosen for translation cannot be
more than d words from the leftmost untranslated
word in the source (Figure 1).4 For this logic we
use a bitwise shift operator (), and a predicate
(?1) that counts the number of leading ones in a
bit array.5 Its runtime is exponential in parameter
d, but linear in sentence length, O(d22dI).
The First d Uncovered Words strategy
(FdUW) is described by Tillman and Ney (2003)
and Zens and Ney (2004), who call it the IBM
Constraint.6 It requires at least one of the leftmost
d uncovered words to be covered by a new phrase.
Items in this strategy contain the index i of the
rightmost covered word and a vector U ? [1, I]d
of the d leftmost uncovered words (Figure 1). Its
complexity is O(dI
( I
d+1
)
), which is roughly ex-
ponential in d.
There are additional variants, such as the Maxi-
mum Jump d strategy (MJd), a polynomial-time
strategy described by Kumar and Byrne (2005),
and possibly others. We lack space to describe all
of them, but simply depicting the strategies as log-
ics permits us to make some simple analyses.
First, it should be clear that these reorder-
ing strategies define overlapping but not identical
search spaces: for most values of d it is impossi-
ble to find d? such that any of the other strategies
would be identical (except for degenerate cases
3Moore and Quirk (2007) give a nice description of MDd.
4We do not know if WLd is documented anywhere, but
from inspection it is used in Moses (Koehn et al, 2007). This
was confirmed by Philipp Koehn and Hieu Hoang (p.c.).
5When a phrase covers the first uncovered word in the
source sentence, the new first uncovered word may be further
along in the sentence (if the phrase completely filled a gap),
or just past the end of the phrase (otherwise).
6We could not identify this strategy in the IBM patents.
534
(1)
item form: [i, {0, 1}I ]
goal: [i ? [I ? d, I], 1I ]
rule:
[i??, V ] R(fi+1...fi?/ej ...ej?)
[i?, V ? 0i1i??i0I?i? ]
V ? 0i1i
??i0I?i
?
= 0I , |i? i??| ? d
(2)
item form: [i, {0, 1}d]
goal: [I, 0d]
rules:
?
????
????
[i, C] R(fi+1...fi?/ej ...ej?)
[i??, C  i?? ? i]
C ? 1i
??i0d?i
?+i = 0d, i? ? i ? d,
?1(C ? 1i
??i0d?i
?+i) = i?? ? i
[i, C] R(fi? ...fi??/ej ...ej?)
[i, C ? 0i??i1i???i?0d?i??+i]
C ? 0i
??i1i
???i?0d?i
??+i = 0d, i?? ? i ? d
(3) item form: [i, [1, I + d]d] goal: [I, [I + 1, I + d]]
rules:
?
????
????
[i, U ] R(fi? ...fi??/ej ...ej?)
[i??, U ? [i?, i??] ? [i??, i?? + d? |U ? [i?, i??]|]]
i? > i, fi+1 ? U
[i, U ] R(fi? ...fi??/ej ...ej?)
[i, U ? [i?, i??] ? [max(U ? i) + 1,max(U ? i) + 1 + d? |U ? [i?, i??]|]]
i? < i, [fi? , fi?? ] ? U
Figure 1: Logics (1) MDd, (2) WLd, and (3) FdUW. Note that the goal item of MDd (1) requires that the
last word of the last phrase translated be within d words of the end of the source sentence.
d = 0 and d = I). This has important ramifi-
cations for scientific studies: results reported for
one strategy may not hold for others, and in cases
where the strategy is not clearly described it may
be impossible to replicate results. Furthermore, it
should be clear that the strategy can have signifi-
cant impact on decoding speed and pruning strate-
gies (?7). For example, MDd is more complex
than WLd, and we expect implementations of the
former to require more pruning and suffer from
more search errors, while the latter would suffer
from more model errors since its space of possible
reorderings is smaller.
We emphasize that many other translation mod-
els can be described this way. Logics for the IBM
Models (Brown et al, 1993) would be similar to
our logics for phrase-based models. Syntax-based
translation logics are similar to parsing logics; a
few examples already appear in the literature (Chi-
ang, 2007; Venugopal et al, 2007; Dyer et al,
2008; Melamed, 2004). For simplicity, we will
use the MONOTONE logic for the remainder of our
examples, but all of them generalize to more com-
plex logics.
4 Adding Local Parameterizations via
Semiring-Weighted Deduction
So far we have focused solely on unweighted log-
ics, which correspond to search using only rule-
sets. Now we turn our focus to parameterizations.
As a first step, we consider only local parame-
terizations, which make computing the score of a
derivation quite simple. We are given a set of in-
ferences in the following form (interpreting side
conditions B1...BM as boolean constraints).
A1...AL
C
B1...BM
Now suppose we want to find the highest-scoring
derivation. Each antecedent item A` has a proba-
bility p(A`): if A` is a rule, then the probability is
given, otherwise its probability is computed recur-
sively in the same way that we now compute p(C).
Since C can be the consequent of multiple deduc-
tions, we take the max of its current value (initially
0) and the result of the new deduction.
p(C) = max(p(C), (p(A1)? ...? p(AL))) (3)
If for every A` that is an item, we replace p(A`)
recursively with this expression, we end up with a
maximization over a product of rule probabilities.
Applying this to logic MONOTONE, the result will
be a maximization (over all possible derivations
D) of the algebraic expression in Equation 1.
We might also want to calculate the total prob-
ability of all possible derivations, which is useful
for parameter estimation (Blunsom et al, 2008).
We can do this using the following equation.
p(C) = p(C) + (p(A1)? ...? p(AL)) (4)
535
Equations 3 and 4 are quite similar. This suggests
a useful generalization: semiring-weighted deduc-
tion (Goodman, 1999).7 A semiring ?A,?,??
consists of a domain A, a multiplicative opera-
tor ? and an additive operator ?.8 In Equa-
tion 3 we use the Viterbi semiring ?[0, 1],?,max?,
while in Equation 4 we use the inside semiring
?[0, 1],?,+?. The general form of Equations 3
and 4 can be written for weights w ? A.
w(C)?= w(A1)? ...? w(A`) (5)
Many quantities can be computed simply by us-
ing the appropriate semiring. Goodman (1999) de-
scribes semirings for the Viterbi derivation, k-best
Viterbi derivations, derivation forest, and num-
ber of paths.9 Eisner (2002) describes the expec-
tation semiring for parameter learning. Gimpel
and Smith (2009) describe approximation semir-
ings for approximate summing in (usually in-
tractable) models. In parsing, the boolean semir-
ing ?{>,?},?,?? is used to determine grammati-
cality of an input string. In translation it is relevant
for alignment (?6.1).
5 Adding Non-Local Parameterizations
with the PRODUCT Transform
A problem arises with the semiring-weighted de-
ductive formalism when we add non-local parame-
terizations such as an n-gram model (Equation 2).
Suppose we have a derivation D = (d1, ..., dM ),
where each dm is a rule application. We can view
the language model as a function on D.
P (D) = f(d1, ..., dm, ..., dM ) (6)
The problem is that replacing dm with a lower-
scoring rule d?m may actually improve f due to
the language model dependency. This means that
f is nonmonotonic?it does not display the opti-
mal substructure property on partial derivations,
which is required for dynamic programming (Cor-
men et al, 2001). The logics still work for some
semirings (e.g. boolean), but not others. There-
fore, non-local parameterizations break semiring-
weighted deduction, because we can no longer use
7General weighted deduction subsumes semiring-
weighted deduction (Eisner et al, 2005; Eisner and Blatz,
2006; Nederhof, 2003), but semiring-weighted deduction
covers all translation models we are aware of, so it is a good
first step in applying weighted deduction to translation.
8See Goodman (1999) for additional conditions on semir-
ings used in this framework.
9Eisner and Blatz (2006) give an alternate strategy for the
best derivation.
the same logic under all semirings. We need new
logics; for this we will use a logic programming
transform called the PRODUCT transform (Cohen
et al, 2008).
We first define a logic for the non-local param-
eterization. The logic for an n-gram language
model generates sequence e1...eQ by generating
each new word given the past n? 1 words.10
item form: [eq, ..., eq+n?2] goal: [eQ?n+2, ..., eQ]
rule:
[eq, ..., eq+n?2]R(eq, ..., eq+n?1)
[eq+1, ..., eq+n?1]
(Logic NGRAM)
Now we want to combine NGRAM and MONO-
TONE. To make things easier, we modify MONO-
TONE to encode the idea that once a source phrase
has been recognized, its target words are gener-
ated one at a time. We will use ue and ve to denote
(possibly empty) sequences in ej ...e?j . Borrowing
the notation of Earley (1970), we encode progress
using a dotted phrase ue ? ve.
item form: [i, ue ? ve] goal: [I, ue ? ve]
rules:
[i, ue?] R(fi+1...fi?/ejve)
[i?, ej ? ve]
[i, ue ? ejve]
[i, ueej ? ve]
(Logic MONOTONE-GENERATE)
We combine NGRAM and MONOTONE-
GENERATE using the PRODUCT transform,
which takes two logics as input and essentially
does the following.11
1. Create a new item type from the cross-
product of item types in the input logics.
2. Create inference rules for the new item type
from the cross-product of all inference rules
in the input logics.
3. Constrain the new logic as needed. This is
done by hand, but quite simple, as we will
show by example.
The first two steps give us logic MONOTONE-
GENERATE ? NGRAM (Figure 2). This is close to
what we want, but not quite done. The constraint
we want to apply is that each word written by logic
MONOTONE-GENERATE is equal to the word gen-
erated by logic NGRAM. We accomplish this by
unifying variables eq and en?i in the inference
rules, giving us logic MONOTONE-GENERATE +
NGRAM (Figure 2).
10We ignore start and stop probabilities for simplicity.
11The description of Cohen et al (2008) is much more
complete and includes several examples.
536
(1)
item form: [i, ue ? ve, eq, ..., eq+n?2]
goal: [I, ue?, eQ?n+2, ..., eQ]
rules:
[i, ue?, eq, ..., eq+n?2] R(fi...fi?/ejue) R(eq, ..., eq+n?1)
[i?, ej ? ue, eq+1, ..., eq+n?1]
[i, ue ? ejve, eq, ..., eq+n?2] R(eq, ..., eq+n?1)
[i, ueej ? ve, eq+1, ..., eq+n?1]
(2)
item form: [i, ue ? ve, ej , ..., ej+n?2]
goal: [I, ue?, eJ?n+2, ..., eJ ]
rules:
[i, ue?, ej?n+1, ..., ej?1] R(fi...fi?/ejve) R(ej?n+2...ej)
[i?, ej ? ve, ej?n+2, ..., ej ]
[i, ue ? ei+n?1ve, ei, ..., ei+n?2] R(ej?n+2...ej)
[i+ 1, ueej ? ve, ej?n+2, ..., ej ]
(3) item form: [i, ue ? ve, ei, ..., en?i?2] goal: [I, ue?, eI?n+2, ..., eI ]
rule:
[i, ej?n+1, ..., ej?1] R(fi...fi?/ej ...ej?)R(ej?n+1, ..., ej)...R(ej??n+1...ej?)
[i?, ej??n+2...ej? ]
Figure 2: Logics (1) MONOTONE-GENERATE ? NGRAM, (2) MONOTONE-GENERATE + NGRAM and
(3) MONOTONE-GENERATE + NGRAM SINGLE-SHOT.
This logic restores the optimal subproblem
property and we can apply semiring-weighted de-
duction. Efficient algorithms are given in ?7, but
a brief comment is in order about the new logic.
In most descriptions of phrase-based decoding,
the n-gram language model is applied all at once.
MONOTONE-GENERATE+NGRAM applies the n-
gram language model one word at a time. This
illuminates a space of search strategies that are to
our knowledge unexplored. If a four-word phrase
were proposed as an extension of a partial hypoth-
esis in a typical decoder implementation using a
five-word language model, all four n-grams will
be applied even though the first n-gram might have
a very low score. Viewing each n-gram applica-
tion as producing a new state may yield new strate-
gies for approximate search.
We can derive the more familiar logic by ap-
plying a different transform: unfolding (Eisner
and Blatz, 2006). The idea is to replace an item
with the sequence of antecedents used to pro-
duce it (similar to function inlining). This gives
us MONOTONE-GENERATE+NGRAM SINGLE-
SHOT (Figure 2).
We call the ruleset-based logic the minimal
logic and the logic enhanced with non-local pa-
rameterization the complete logic. Note that the
set of variables in the complete logic is a superset
of the set of variables in the minimal logic. We
can view the minimal logic as a projection of the
complete logic into a smaller dimensional space.
It is important to note that complete logic is sub-
stantially more complex than the minimal logic,
by a factor of O(|VE |n) for a target vocabulary of
VE . Thus, the complexity of non-local parameteri-
zations often makes search spaces large regardless
of the complexity of the minimal logic.
6 Other Uses of the PRODUCT Transform
The PRODUCT transform can also implement
alignment and help derive new models.
6.1 Alignment
In the alignment problem (sometimes called con-
strained decoding or forced decoding), we are
given a reference target sentence r1, ..., rJ , and
we require the translation model to generate only
derivations that produce that sentence. Alignment
is often used in training both generative and dis-
criminative models (Brown et al, 1993; Blunsom
et al, 2008; Liang et al, 2006). Our approach to
alignment is similar to the one for language mod-
eling. First, we implement a logic requiring an
537
input to be identical to the reference.
item form: [j]
goal: [J ]
rule:
[j]
[j + 1]
ej+1 = rj+1
(Logic RECOGNIZE)
The logic only reaches its goal if the input is iden-
tical to the reference. In fact, partial derivations
must produce a prefix of the reference. When we
combine this logic with MONOTONE-GENERATE,
we obtain a logic that only succeeds if the transla-
tion logic generates the reference.
item form: [i, j, ue ? ve] goal: [I, j, ue?]
rules:
?
????
????
[i, j, ue?] R(fi...fi?/ej ...ej?)
[i?, j, ?ej ...ej? ]
[i, j, ue ? ejve]
[i, j + 1, ueej ? ve]
ej+1 = rj+1
(Logic MONOTONE-ALIGN)
Under the boolean semiring, this (minimal) logic
decides if a training example is reachable by the
model, which is required by some discriminative
training regimens (Liang et al, 2006; Blunsom et
al., 2008). We can also compute the Viterbi deriva-
tion or the sum over all derivations of a training
example, needed for some parameter estimation
methods. Cohen et al (2008) derive an alignment
logic for ITG from the product of two CKY logics.
6.2 Translation Model Design
A motivation for many syntax-based translation
models is to use target-side syntax as a language
model (Charniak et al, 2003). Och et al (2004)
showed that simply parsing the N -best outputs
of a phrase-based model did not work; to ob-
tain the full power of a language model, we need
to integrate it into the search process. Most ap-
proaches to this problem focus on synchronous
grammars, but it is possible to integrate the target-
side language model with a phrase-based transla-
tion model. As an exercise, we integrate CKY
with the output of logic MONOTONE-GENERATE.
The constraint is that the indices of the CKY items
unify with the items of the translation logic, which
form a word lattice. Note that this logic retains the
rules in the basic MONOTONE logic, which are not
depicted (Figure 3).
The result is a lattice parser on the output of the
translation model. Lattice parsing is not new to
translation (Dyer et al, 2008), but to our knowl-
edge it has not been used in this way. Viewing
(1)
(2)
Figure 4: Example graphs corresponding to a sim-
ple minimal (1) and complete (2) logic, with cor-
responding nodes in the same color. The state-
splitting induced by non-local features produces in
a large number of arcs which must be evaluated,
which can be reduced by cube pruning.
translation as deduction is helpful for the design
and construction of novel models.
7 Algorithms
Most translation logics are too expensive to ex-
haustively search. However, the logics conve-
niently specify the full search space, which forms
a hypergraph (Klein and Manning, 2001).12 The
equivalence is useful for complexity analysis:
items correspond to nodes and deductions corre-
spond to hyperarcs. These equivalences make it
easy to compute algorithmic bounds.
Cube pruning (Chiang, 2007) is an approxi-
mate search technique for syntax-based translation
models with integrated language models. It op-
erates on two objects: a ?LM graph containing
no language model state, and a +LM hypergraph
containing state. The idea is to generate a fixed
number of nodes in the +LM for each node in
the ?LM graph, using a clever enumeration strat-
egy. We can view cube pruning as arising from
the interaction between a minimal logic and the
state splits induced by non-local features. Figure 4
shows how the added state information can dra-
matically increase the number of deductions that
must be evaluated. Cube pruning works by con-
sidering the most promising states paired with the
most promising extensions. In this way, it easily
fits any search space constructed using the tech-
nique of ?5. Note that the efficiency of cube prun-
ing is limited by the minimal logic.
Stack decoding is a search heuristic that simpli-
fies the complexity of searching a minimal logic.
Each item is associated with a stack whose signa-
12Specifically a B-hypergraph, equivalent to an and-or
graph (Gallo et al, 1993) or context-free grammar (Neder-
hof, 2003). In the degenerate case, this is simply a graph, as
is the case with most phrase-based models.
538
item forms: [i, ue ? ve], [A, i, ue ? ve, i?, u?e ? v?e] goal: [S, 0, ?, I, ue?]
rules:
[i, ue?] R(fi+1...fi?/ejve) R(A? ej)
[A, i, ue?, i?, ej ? ve]
[i, ue ? ejve] R(A? ej)
[A, i, ue ? ejve, i, ueej ? ve]
[B, i, ue ? ve, i??, u??e ? v
??
e ] [C, i
??, u??e ? v
??
e , i
?, u?e ? v
?
e] R(A? BC)
[A, i, ue ? ve, i?, u?e ? v?e]
Figure 3: Logic MONOTONE-GENERATE + CKY
ture is a projection of the item signature (or a pred-
icate on the item signatures)?multiple items are
associated to the same stack. The strength of the
pruning (and resulting complexity improvements)
depending on how much the projection reduces the
search space. In many phrase-based implemen-
tations the stack signature is just the number of
words translated, but other strategies are possible
(Tillman and Ney, 2003).
It is worth noting that logic FdUW (?3.2), de-
pends on stack pruning for speed. Because the
number of stacks is linear in the length of the in-
put, so is the number of unpruned nodes in the
search graph. In contrast, the complexity of logic
WLd is naturally linear in input length. As men-
tioned in ?3.2, this implies a wide divergence in
the model and search errors of these logics, which
to our knowledge has not been investigated.
8 Related Work
We are not the first to observe that phrase-based
models can be represented as logic programs (Eis-
ner et al, 2005; Eisner and Blatz, 2006), but to
our knowledge we are the first to provide explicit
logics for them.13 We also showed that deductive
logic is a useful analytical tool to tackle a variety
of problems in translation algorithm design.
Our work is strongly influenced by Goodman
(1999) and Eisner et al (2005). They describe
many issues not mentioned here, including con-
ditions on semirings, termination conditions, and
strategies for cyclic search graphs. However,
while their weighted deductive formalism is gen-
eral, they focus on concerns relevant to parsing,
such as boolean semirings and cyclicity. Our work
focuses on concerns common for translation, in-
cluding a general view of non-local parameteriza-
tions and cube pruning.
13Huang and Chiang (2007) give an informal example, but
do not elaborate on it.
9 Conclusions and Future Work
We have described a general framework that syn-
thesizes and extends deductive parsing and semir-
ing parsing, and adapts it to translation. Our goal
has been to show that logics make an attractive
shorthand for description, analysis, and construc-
tion of translation models. For instance, we have
shown that it is quite easy to mechanically con-
struct search spaces using non-local features, and
to create exotic models. We showed that differ-
ent flavors of phrase-based models should suffer
from quite different types of error, a problem that
to our knowledge was heretofore unknown. How-
ever, we have only scratched the surface, and we
believe it is possibly to unify a wide variety of
translation algorithms. For example, we believe
that cube pruning can be described as an agenda
discipline in chart parsing (Kay, 1986).
Although the work presented here is abstract,
our motivation is practical. Isolating the errors
in translation systems is a difficult task which can
be made easier by describing and analyzing mod-
els in a modular way (Auli et al, 2009). Fur-
thermore, building large-scale translation systems
from scratch should be unnecessary if existing sys-
tems were built using modular logics and algo-
rithms. We aim to build such systems.
Acknowledgments
This work developed from discussions with Phil
Blunsom, Chris Callison-Burch, Chris Dyer,
Hieu Hoang, Martin Kay, Philipp Koehn, Josh
Schroeder, and Lane Schwartz. Many thanks go to
Chris Dyer, Josh Schroeder, the three anonymous
EACL reviewers, and one anonymous NAACL re-
viewer for very helpful comments on earlier drafts.
This research was supported by the Euromatrix
Project funded by the European Commission (6th
Framework Programme).
539
References
M. Auli, A. Lopez, P. Koehn, and H. Hoang. 2009.
A systematic analysis of translation model search
spaces. In Proc. of WMT, Mar.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL:HLT.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, Jun.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for statistical ma-
chine translation. In Proceedings of MT Summit,
Sept.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
K. Church and R. Patil. 1982. Coping with syntactic
ambiguity or how to put the block in the box on the
table. Computational Linguistics, 8(3?4):139?149,
Jul.
S. B. Cohen, R. J. Simmons, and N. A. Smith. 2008.
Dynamic programming algorithms as products of
weighted logic programs. In Proc. of ICLP.
T. H. Cormen, C. D. Leiserson, R. L. Rivest, and
C. Stein. 2001. Introduction to Algorithms. MIT
Press, 2nd edition.
M. R. Costa-jussa` and J. A. R. Fonollosa. 2006. Statis-
tical machine reordering. In Proc. of EMNLP, pages
70?76, Jul.
C. J. Dyer, S. Muresan, and P. Resnik. 2008. General-
izing word lattice translation. In Proc. of ACL:HLT,
pages 1012?1020.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102,
Feb.
J. Eisner and J. Blatz. 2006. Program transformations
for optimization of parsing algorithms and other
weighted logic programs. In Proc. of Formal Gram-
mar, pages 45?85.
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Compil-
ing comp ling: Weighted dynamic programming and
the Dyna language. In Proc. of HLT-EMNLP, pages
281?290.
J. Eisner. 2002. Parameter estimation for probabilistic
finite-state transducers. In Proc. of ACL, pages 1?8,
Jul.
G. Gallo, G. Longo, and S. Pallottino. 1993. Di-
rected hypergraphs and applications. Discrete Ap-
plied Mathematics, 42(2), Apr.
K. Gimpel and N. A. Smith. 2009. Approximation
semirings: Dynamic programming with non-local
features. In Proc. of EACL, Mar.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25(4):573?605, Dec.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. of ACL, pages 144?151, Jun.
M. Kay. 1986. Algorithm schemata and data structures
in syntactic processing. In B. J. Grosz, K. S. Jones,
and B. L. Webber, editors, Readings in Natural Lan-
guage Processing, pages 35?70. Morgan Kaufmann.
D. Klein and C. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demo and Poster Sessions, pages 177?180, Jun.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proc. of HLT-EMNLP, pages 161?168.
P. Liang, A. Bouchard-Co?te?, B. Taskar, and D. Klein.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL-COLING, pages
761?768, Jul.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3), Aug.
J. B. Marin?o, R. E. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J. A. R. Fonollosa, and M. R. Costa-
jussa`. 2006. N -gram based statistical machine
translation. Computational Linguistics, 32(4):527?
549, Dec.
D. McAllester. 1999. On the complexity analysis of
static analyses. In Proc. of Static Analysis Sympo-
sium, volume 1694/1999 of LNCS. Springer Verlag.
I. D. Melamed. 2004. Statistical machine translation
by parsing. In Proc. of ACL, pages 654?661, Jul.
R. C. Moore and C. Quirk. 2007. Faster beam-search
decoding for phrasal statistical machine translation.
In Proc. of MT Summit.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth?s algorithm. Computational Linguistics,
29(1):135?143, Mar.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In Proc. of HLT-NAACL, pages 161?168, May.
F. C. N. Pereira and D. H. D. Warren. 1983. Parsing as
deduction. In Proc. of ACL, pages 137?144.
S. M. Shieber, Y. Schabes, and F. C. N. Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1?2):3?36, Jul.
C. Tillman and H. Ney. 2003. Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):98?133, Mar.
A. Venugopal, A. Zollmann, and S. Vogel. 2007.
An efficient two-pass approach to synchronous-CFG
driven statistical MT. In Proc. of HLT-NAACL.
D. Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proc. of ACL, pages
152?158, Jun.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of
HLT-NAACL, pages 257?264, May.
540
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 224?232,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
A Systematic Analysis of Translation Model Search Spaces
Michael Auli, Adam Lopez, Hieu Hoang and Philipp Koehn
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB
United Kingdom
m.auli@sms.ed.ac.uk, alopez@inf.ed.ac.uk, h.hoang@sms.ed.ac.uk, pkoehn@inf.ed.ac.uk
Abstract
Translation systems are complex, and
most metrics do little to pinpoint causes of
error or isolate system differences. We use
a simple technique to discover induction
errors, which occur when good transla-
tions are absent from model search spaces.
Our results show that a common prun-
ing heuristic drastically increases induc-
tion error, and also strongly suggest that
the search spaces of phrase-based and hi-
erarchical phrase-based models are highly
overlapping despite the well known struc-
tural differences.
1 Introduction
Most empirical work in translation analyzes mod-
els and algorithms using BLEU (Papineni et al,
2002) and related metrics. Though such met-
rics are useful as sanity checks in iterative sys-
tem development, they are less useful as analyti-
cal tools. The performance of a translation system
depends on the complex interaction of several dif-
ferent components. Since metrics assess only out-
put, they fail to inform us about the consequences
of these interactions, and thus provide no insight
into the errors made by a system, or into the de-
sign tradeoffs of competing systems.
In this work, we show that it is possible to ob-
tain such insights by analyzing translation sys-
tem components in isolation. We focus on model
search spaces (?2), posing a very simple question:
Given a model and a sentence pair, does the search
space contain the sentence pair? Applying this
method to the analysis and comparison of French-
English translation using both phrase-based and
hierarchical phrase-based systems yields surpris-
ing results, which we analyze quantitatively and
qualitatively.
? First, we analyze the induction error of a
model, a measure on the completeness of the
search space. We find that low weight phrase
translations typically discarded by heuristic
pruning nearly triples the number of refer-
ence sentences that can be exactly recon-
structed by either model (?3).
? Second, we find that the high-probability re-
gions in the search spaces of phrase-based
and hierarchical systems are nearly identical
(?4). This means that reported differences be-
tween the models are due to their rankings of
competing hypotheses, rather than structural
differences of the derivations they produce.
2 Models, Search Spaces, and Errors
A translation model consists of two distinct ele-
ments: an unweighted ruleset, and a parameteri-
zation (Lopez, 2008a; 2009). A ruleset licenses
the steps by which a source string f1...fI may be
rewritten as a target string e1...eJ . A parameter-
ization defines a weight function over every se-
quence of rule applications.
In a phrase-based model, the ruleset is simply
the unweighted phrase table, where each phrase
pair fi...fi?/ej ...ej? states that phrase fi...fi? in
the source can be rewritten as ej ...ej? in the tar-
get. The model operates by iteratively apply-
ing rewrites to the source sentence until each
source word has been consumed by exactly one
rule. There are two additional heuristic rules:
The distortion limit dl constrains distances over
which phrases can be reordered, and the transla-
tion option limit tol constrains the number of tar-
get phrases that may be considered for any given
source phrase. Together, these rules completely
determine the finite set of all possible target sen-
tences for a given source sentence. We call this set
of target sentences the model search space.
The parameterization of the model includes all
information needed to score any particular se-
224
quence of rule applications. In our phrase-based
model, it typically includes phrase translation
probabilities, lexical translation probabilities, lan-
guage model probabilities, word counts, and co-
efficients on the linear combination of these. The
combination of large rulesets and complex param-
eterizations typically makes search intractable, re-
quiring the use of approximate search. It is im-
portant to note that, regardless of the parameteri-
zation or search used, the set of all possible output
sentences is still a function of only the ruleset.
Germann et al (2004) identify two types of
translation system error: model error and search
error.1 Model error occurs when the optimal
path through the search space leads to an incorrect
translation. Search error occurs when the approxi-
mate search technique causes the decoder to select
a translation other than the optimum.
Given the decomposition outlined above, it
seems clear that model error depends on param-
eterization, while search error depends on approx-
imate search. However, there is no error type that
clearly depends on the ruleset (Table 1). We there-
fore identify a new type of error on the ruleset: in-
duction error. Induction error occurs when the
search space does not contain the correct target
sentence at all, and is thus a more fundamental
defect than model error. This is difficult to mea-
sure, since there could be many correct transla-
tions and there is no way to see whether they are
all absent from the search space.2 However, if we
assume that a given reference sentence is ground
truth, then as a proxy we can simply ask whether
or not the model search space contains the refer-
ence. This assumption is of course too strong, but
over a sufficiently large test set, it should correlate
with metrics which depend on the reference, since
under most metrics, exactly reproducing the ref-
erence results in a perfect score. More loosely, it
should correlate with translation accuracy?even
if there are many good translations, a model which
is systematically unable to produce any reference
sentences from a sufficiently large test sample is
almost certainly deficient in some way.
3 Does Ruleset Pruning Matter?
The heuristic translation option limit tol controls
the number of translation rules considered per
1They also identify variants within these types.
2It can also be gamed by using a model that can generate
any English word from any French word. However, this is
not a problem for the real models we investigate here.
ruleset induction error
parameterization model error
search search error
Table 1: Translation system components and their
associated error types.
100 101 102 1030
0.2
0.4
0.6
0.8
Translation Options
Phra
se P
roba
bility
 p(e|f)
Figure 1: Distribution p(f |e) of the English trans-
lation options for the French word proble`me.
source span. It plays a major role in keeping the
search space manageable. Ignoring reordering, the
complexity of the search in a phrase-based model
is O(ntol), where n is the number of French spans.
Therefore tol has a major effect on efficiency.
Tight pruning with tol is often assumed without
question to be a worthwhile tradeoff. However,
we wish to examine this assumption more closely.
Consider the French word proble`me. It has 288
different translation options in the phrase table
of our French-English phrase-based system. The
phrase translation probability p(e|f) over these
options is a familiar Zipf distribution (Figure 1).
The most likely candidate translation for the word
is problem with a probability of 0.71, followed by
issue with a much smaller probability of 0.12. Fur-
ther down, we find challenge at rank 25, obsta-
cle at 44 and dilemma at rank 105. Depending on
the context, these might be perfectly good transla-
tions. However, with a typical tol of 20, most of
these options are not considered during decoding.
Table 2 shows that 93.8% of rules are available
during decoding with the standard tol setting and
only about 0.1% of French spans of the entire rule-
set have more than 20 translation options. It seems
as if already most of the information is available
when using the default limit. However, a tol of
20 can clearly exclude good translations as illus-
trated by our example. Therefore we hypothesize
the following: Increasing the translation option
limit gives the decoder a larger vocabulary which
in turn will decrease the induction error. We sup-
225
tol Ruleset Size French Spans
20 93.8 99.9
50 96.8 100.0
100 98.3 100.0
200 99.2 100.0
400 99.7 100.0
800 99.9 100.0
All 100.0 100.0
Table 2: Ruleset size expressed as percentage of
available rules when varying the limit of transla-
tion options tol per English span and percentage
of French spans with up to tol translations.
port this hypothesis experimentally in ?5.4.
4 How Similar are Model Search Spaces?
Most work on hierarchical phrase-based transla-
tion focuses quite intently on its structural differ-
ences from phrase-based translation.
? A hierarchical model can translate discon-
tiguous groups of words as a unit. A phrase-
based model cannot. Lopez (2008b) gives in-
direct experimental evidence that this differ-
ence affects performance.
? A standard phrase-based model can reorder
phrases arbitrarily within the distortion limit,
while the hierarchical model requires some
lexical evidence for movement, resorting to
monotone translation otherwise.
? While both models can indirectly model
word deletion in the context of phrases, the
hierarchical model can delete words using
non-local context due to its use of discontigu-
ous phrases.
The underlying assumption in most discussions
of these models is that these differences in their
generative stories are responsible for differences
in performance. We believe that this assumption
should be investigated empirically.
In an interesting analysis of phrase-based and
hierarchical translation, Zollmann et al (2008)
forced a phrase-based system to produce the trans-
lations generated by a hierarchical system. Unfor-
tunately, their analysis is incomplete; they do not
perform the analysis in both directions. In ?5.5 we
extend their work by requiring each system to gen-
erate the 1-best output of the other. This allows us
to see how their search spaces differ.
5 Experiments
We analyse rulesets in isolation, removing the in-
fluence of the parametrization and heuristics as
much as possible for each system as follows: First,
we disabled beam search to avoid pruning based
on parametrization weights. Second, we require
our decoders to generate the reference via disal-
lowing reference-incompatible hypothesis or chart
entries. This leaves only some search restrictions
such as the distortion limit for the phrase-based
system for which we controlled, or the maximum
number of source words involved in a rule appli-
cation for the hierarchical system.
5.1 Experimental Systems
Our phrase-based system is Moses (Koehn et al,
2007). We set its stack size to 105, disabled the
beam threshold, and varied the translation option
limit tol. Forced translation was implemented by
Schwartz (2008) who ensures that hypothesis are
a prefix of the reference to be generated.
Our hierarchical system is Hiero (Chiang,
2007), modified to construct rules from a small
sample of occurrences of each source phrase in
training as described by Lopez (2008b). The
search parameters restricting the number of rules
or chart entries as well as the minimum threshold
were set to very high values (1050) to prevent prun-
ing. Forced translation was implemented by dis-
carding rules and chart entries which do not match
the reference.
5.2 Experimental Data
We conducted experiments in French-English
translation, attempting to make the experimental
conditions for both systems as equal as possible.
Each system was trained on French-English Eu-
roparl (Koehn, 2005), version 3 (40M words). The
corpus was aligned with GIZA++ (Och and Ney,
2003) and symmetrized with the grow-diag-final-
and heuristic (Koehn et al, 2003). A trigram
language model with modified Kneser-Ney dis-
counting and interpolation was used as produced
by the SRILM toolkit (Stolcke, 2002). Systems
were optimized on the WMT08 French-English
development data (2000 sentences) using mini-
mum error rate training (Och, 2003) and tested
on the WMT08 test data (2000 sentences). Rules
based on unaligned words at the edges of foreign
and source spans were not allowed unless other-
wise stated, this is denoted as the tightness con-
226
20 50 100 200 400 800 All10
15
20
25
30
35
Translation Option Limit
Rea
chab
ility 
(%)
 
 dl=6dl=7dl=8dl=9dl=10dl=11dl=12dl=13dl=14dl=15dl=16
Figure 2: Coverage for phrase-based reference
aligned translation on test data when varying the
translation option and the distortion limits (dl).
straint. Ayan and Dorr (2006) showed that under
certain conditions, this constraint could have sig-
nificant impact on system performance. The max-
imum phrase lengths for both the hierarchical and
phrase-based system were set to 7. The distortion
limit (dl) for the phrase-based system was set to
6 unless otherwise mentioned. All other settings
were left at their default values as described by
Chiang (2007) and Koehn et al (2007).
5.3 Metric: Reference Reachability
We measure system performance in terms of ref-
erence reachability, which is the inverse of in-
duction error: A system is required to be able to
exactly reproduce the reference, otherwise we re-
gard the result as an error.
5.4 Analysis of Ruleset Pruning
In ?3 we outlined the hypothesis that increas-
ing the number of English translation options per
French span can increase performance. Here we
present results for both phrase-based and hierar-
chical systems to support this claim.
5.4.1 Quantitative Results
Figure 2 shows the experimental results when
forcing our phrase-based system to generate un-
seen test data. We observe more than 30% in-
crease in reachability from tol = 20 to tol = 50
for all dl ? 6 which supports our hypothesis that
increasing tol by a small multiple can have a sig-
nificant impact on performance. With no limit on
tol, reachability nearly triples.
French Spans Number of Translations
des 3006
les 2464
la 1582
de 1557
en 1428
de la 1332
fait 1308
une 1303
a` 1291
le 1273
d? 1271
faire 1263
l? 1111
c? est 1109
a` la 1053
, 1035
Table 3: French spans with more than 1000 trans-
lation options.
Notably, the increase stems from the small frac-
tion of French spans (0.1%) which have more than
20 translation options (Table 2). There are only
16 French spans (Table 3) which have more than
1000 translation options, however, utilising these
can still achieve an increase in reachability of up
to 5%. The list shown in Table 3 includes common
articles, interpuncutation, conjunctions, preposi-
tions but also verbs which have unreliable align-
ment points and therefore a very long tail of low
probability translation options. Yet, the largest in-
crease does not stem from using such unreliable
translation options, but rather when increasing tol
by a relatively small amount.
The increases we see in reachability are pro-
portional to the size of the ruleset: The high-
est increases in ruleset size can be seen between
tol = 20 and tol = 200 (Table 2), similarly, reach-
ability performance has then the largest increase.
For higher tol settings both the increases of ruleset
size and reachability are smaller.
Figure 3 plots the average number of words per
sentence for the reachable sentences. The average
sentence length increases by up to six words when
using all translation options. The black line repre-
sents the average number of words per sentence of
the reference set. This shows that longer and more
complex sentences can be generated when using
more translation options.
Similarly, for our hierarchical system (see Fig-
227
20 50 100 200 400 800 All14
16
18
20
22
24
26
28
30
32
Translation Option Limit
Ave
rage
 Num
ber o
f Wo
rds p
er S
ente
nce
 
 dl=6dl=7dl=8dl=9dl=10dl=11dl=12dl=13dl=14dl=15dl=16Reference
Figure 3: Average number of words per sen-
tence for the reachable test data translations of the
phrase-based system (as shown in Figure 2).
25 50 100 200 400 800 1600 3200 6400 12800 Inf5
10
15
20
25
30
35
40
Sample Limit (SL)
Rea
chab
ility 
(%)
 
 
Figure 4: Coverage for hierarchical reference
aligned translation on test data when varying the
number of matching French samples (sl) drawn
from the training data. The baseline setting is
sl = 300.
ure 4) we find that reachability can be more than
doubled when drawing a richer ruleset sample than
in the baseline setting. Those results are not di-
rectly comparable to the phrase-based system due
to the slightly different nature of the parameters
which were varied: In the phrase-based case we
have tol different English spans per French span.
In the hierarchical system it is very likely to have
duplicate French spans in the sample drawn from
training data. Yet, the trend is the same and thus
supports our claim.
5.4.2 Qualitative Results
We were interested how the performance increase
could be achieved and therefore looked into which
kind of translation options were involved when a
translation was generable with a higher tol setting.
One possibility is that the long tail of translation
options includes all kinds of English spans that
match some part of the reference but are simply
an artifact of unreliable alignment points.
We looked at the first twenty translations pro-
duced by our phrase-based system under dl = 10
which could not be generated with tol = 20 but
with tol = 50. The aim was to find out which
translation options made it possible to reach the
reference under tol = 50.
We found that nearly half (9) involved transla-
tion options which used a common or less com-
mon translation of the foreign span. The first four
translations in Table 4 are examples for that. When
allowing unaligned words at the rule edges it turns
out that even 13 out of 20 translations are based on
sound translation options.
The remaining sentences involved translation
options which were an artifact of unreliable align-
ment points. An example rule is la / their, which
erroneously translates a common determiner into
an equally common adjective. The last translation
in Figure 4 involves such a translation option.
This analysis demonstrates that the performance
increase between tol = 20 to tol = 50 is to a
considerable extent based on translation options
which are meaningful.
5.5 Analysis of Mutual Reachability
The aim of this analysis was to find out by how
much the high-probability search spaces of the
phrase-based and hierarchical models differ. The
necessary data was obtained via forcing each sys-
tem to produce the 1-best translation of the other
system denoted as the unconstrained translation.
This unconstrained translation used the standard
setting for the number of translation options.
We controlled for the way unaligned words
were handled during rule extraction: The phrase-
based system allowed unaligned words at the
edges of phrases while the hierarchical system did
not. We varied this condition for the phrase-based
system. The distortion limit of the phrase-based
system was set to 10. This is equal to the maxi-
mum span a rule can be applied within the hierar-
chical system.
We carried out the same experiment for
German-English and English-German translation
which serve as examples for translating into a mor-
228
S: je voterai en faveur du projet de re`glement .
R: i will vote to approve the draft regulation .
O: i shall be voting in favour of the draft regulation .
S: ... il npeut y avoir de de?lai transitoire en matie`re de respect des re`gles de?mocratiques .
R: ... there can be no transitional period for complying with democratic rules .
O: ... there can be no transitional period in the field of democratic rules .
S: je souhaite aux ne?gociateurs la poursuite du succe`s de leur travail dans ce domaine important .
R: i wish the negotiators continued success with their work in this important area .
O: i wish the negotiators the continuation of the success of their work on this important area .
S: mais commencons par les points positifs .
R: but let us begin with the good news .
O: but let us begin with the positive points .
S: ... partage la plupart des conclusions que tire le rapporteur .
R: ... share the majority of conclusions that he draws .
O: ... share most of the conclusions that is the rapporteur .
Table 4: Example translations which could be generated with tol = 50 but not with tol = 20. For each
translation the source (S), reference (R) and the unconstrained output (O) are shown. Bold phrases mark
translation options which were not available under tol = 20.
phologically simpler and more complex language
respectively. The test and training sets for these
languages are similarly sized and are from the
WMT08 shared task.
5.5.1 Quantitative Results
Table 5 shows the mutual reachability perfor-
mance for our phrase-based and hierarchical sys-
tem. The hierarchical system can generate almost
all of the 1-best phrase-based translations, partic-
ularly when unaligned words at rule edges are dis-
allowed which is the most equal condition we ex-
perimented with. The phrase-based reachability
for English-German using tight rulesets is remark-
ably low. We found that this is because the hi-
erarchical model allows unaligned words around
gaps under the tight constraint. This makes it very
hard for the phrase-based system to reach the hi-
erarchical translation. However, the phrase-based
system can overcome this problem when the tight-
ness constraint is loosened (last row in Table 5).
Table 6 shows the translation performance mea-
sured in BLEU for both systems for normal un-
constrained translation. It can be seen that the dif-
ference is rather marginal which is in line with our
reachability results.
We were interested why certain translations of
one system were not reachable by the other sys-
tem. The following two subsections describe
our analysis of these translations for the French-
English language pair.
Translation Direction fr-en de-en en-de
Ht ? Pt 99.40 97.65 98.50
Ht ? Pnt 95.95 93.95 94.30
Pt ? Ht 93.75 92.30 82.95
Pnt ? Ht 97.55 97.55 96.30
Table 5: Mutual reachability performance for
French-English (fr-en), German-English (de-en)
and Enlgish-German (en-de). P? H denotes how
many hierarchical (H) high scoring outputs can be
reached by the phrase-based (P) system. The sub-
scripts nt (non-tight) and t (tight) denote the use
of rules with unaligned words or not.
5.5.2 Qualitative Analysis of Unreachable
Hierarchical Translations
We analysed the first twenty translations within
the set of unreachable hierarchical translations
when disallowing unaligned words at rule edges to
find out why the phrase-based system fails to reach
them. Two aspects were considered in this anal-
ysis: First, the successful hierarchical derivation
and second, the relevant part of the phrase-based
ruleset which was involved in the failed forced
translation i.e. how much of the input and the ref-
erence could be covered by the raw phrase-pairs
available to the phrase-based system.
Within the examined subset, the majority of
sentences (14) involved hierarchical rules which
could not be replicated by the phrase-based sys-
229
System fr-en de-en en-de
Phrase-based 31.96 26.94 19.96
Hierarchical 31.62 27.18 20.20
Difference absolute 0.34 0.24 0.24
Difference (%) 1.06 0.90 1.20
Table 6: Performance for phrase-based and hier-
archical systems in BLEU for French-English (fr-
en), German-English (de-en) and English-German
(en-de).
tem. We described this as the first structural dif-
ference in ?4. Almost all of these translations
(12 out of 14) could not be generated because
of the third structural difference which involved
rule that omits the translation of a word within
the French span. An example is the rule X ?
estX 1 ordinaireX 2 /isX 1 X 2 which omits a trans-
lation for the French word ordinaire in the English
span. For this particular subset the capability of
the hierarchical system to capture long-distance
reorderings did not make the difference, but rather
the ability to drop words within a translation rule.
The phrase-based system cannot learn many
rules which omit the translation of words because
we disallowed unaligned words at phrase edges.
The hierarchical system has the same restriction,
but the constraint does not prohibit rules which
have unaligned words within the rule. This allows
the hierarchical system to learn rules such as the
one presented above. The phrase-based system
can learn similar knowledge, although less gen-
eral, if it is allowed to have unaligned words at
the phrase edges. In fact, without this constraint
13 out of the 20 analysed rules can be generated
by the phrase-based system.
Figure 5 shows a seemingly simple hierarchi-
cal translation which fails to be constructed by the
phrase-based system: The second rule application
involves both the reordering of the translation of
postaux and the omittance of a translation for con-
currence. This translation could be easily captured
by a phrase-pair, however, it requires that the train-
ing data contains exactly such an example which
was not the case. The closest rule the phrase-based
rulestore contains is des services postaux / postal
services which fails since it does not cover all of
the input. This is an example for when the gen-
eralisation of the hierarchical model is superior to
the phrase-based approach.
5.5.3 Qualitative Analysis of Unreachable
Phrase-based Translations
The size of the set of unreachable phrase-based
translations is only 0.6% or 12 sentences. This
means that almost all of the 1-best outputs of the
phrase-based translations can be reached by the hi-
erarchical system. Similarly to above, we analysed
which words of the input as well as which words
of the phrase-based translation can be covered by
the available hierarchical translation rules.
We found that all of the translations were not
generable because of the second structural differ-
ence we identified in ?4. The hierarchical rule-
set did not contain a rule with the necessary lex-
ical evidence to perform the same reordering as
the phrase-based model. Figure 6 shows a phrase-
based translation which could not be reached by
the hierarchical system because a rule of the form
X ? e?lectoralesX 1 /X 1 electoral would be re-
quired to move the translation of e?lectorales (elec-
toral) just before the translation of re?unions (meet-
ings). Inspection of the hierarchical ruleset reveals
that such a rule is not available and so the transla-
tion cannot be generated.
The small size of the set of unreachable phrase-
based translations shows that the lexically in-
formed reordering mechanism of the hierarchical
model is not a large obstacle in generating most of
the phrase-based outputs.
In summary, each system can reproduce nearly
all of the highest-scoring outputs of the other sys-
tem. This shows that the 1-best regions of both
systems are nearly identical despite the differ-
ences discussed in ?4. This means that differences
in observed system performance are probably at-
tributable to the degree of model error and search
error in each system.
6 Related Work and Open Questions
Zhang et al (2008) and Wellington et al (2006)
answer the question: what is the minimal gram-
mar that can be induced to completely describe a
training set? We look at the related question of
what a heuristically induced ruleset can translate
in an unseen test set, considering both phrase- and
grammar-based models. We also extend the work
of Zollmann et al (2008) on Chinese-English, per-
forming the analysis in both directions and provid-
ing a detailed qualitative explanation.
Our focus has been on the induction error of
models, a previously unstudied cause of transla-
230
Source: concurrence des services postaux
Reference: competition between postal services
Hierarchical: postal services
Deviation:
( [0-4: @S -> @X?1 | @X?1 ]
( [0-4: @X -> concurrence @X?1 postaux | postal @X?1 ] postal
( [1-3: @X -> des services | services ] services
)
)
)
Figure 5: Derivation of a hierarchical translation which cannot be generated by the phrase-based system,
in the format of Zollmann et al (2008). The parse tree contains the outputs (shaded) at its leaves in infix
order and each non-leaf node denotes a rule, in the form: [ Source-span: LHS?RHS ].
Source: ceux qui me disaient cela faisaient par exemple re`fe`rence a` certaines des
re?unions e?lectorales auxquelles ils avaient assiste? .
Phrase-based: those who said to me that were for example refer to some of which
they had been electoral meetings .
Reference: they referred to some of the election meetings , for example , that
they had gone to .
Figure 6: Phrase-based translation which cannot be reached by the hierarchical system because no rule to
perform the necessary reordering is available. Marked sections are source and reference spans involved
in the largest possible partial hierarchical derivation.
tion errors. Although the results described here
are striking, our exact match criterion for reach-
ability is surely too strict?for example, we re-
port an error if even a single comma is missing.
One solution is to use a more tolerant criterion
such as WER and measure the amount of devia-
tion from the reference. We could also maximize
BLEU with respect to the reference as in Dreyer et
al. (2007), but it is less interpretable.
7 Conclusion and Future Work
Sparse distributions are common in natural lan-
guage processing, and machine translation is no
exception. We showed that utilizing more of the
entire distribution can dramatically improve the
coverage of translation models, and possibly their
accuracy. Accounting for sparsity explicitly has
achieved significant improvements in other areas
such as in part of speech tagging (Goldwater and
Griffiths, 2007). Considering the entire tail is chal-
lenging, since the search space grows exponen-
tially with the number of translation options. A
first step might be to use features that facilitate
more variety in the top 20 translation options. A
more elaborate aim is to look into alternatives to
maximum likelihood hood estimation such as in
Blunsom and Osborne (2008).
Additionally, our expressiveness analysis shows
clearly that the 1-best region of hierarchical and
phrase-based models is nearly identical. Dis-
counting cases in which systems handle unaligned
words differently, we observe an overlap of be-
tween 96% and 99% across three language pairs.
This implies that the main difference between the
models is in their parameterization, rather than in
the structural differences in the types of transla-
tions they can produce. Our results also suggest
that the search spaces of both models are highly
overlapping: The results for the 1-best region al-
low the conjecture that also other parts of the
search space are behaving similarly since it ap-
pears rather unlikely that spaces are nearly disjoint
with only the 1-best region being nearly identical.
In future work we aim to use n-best lists or lattices
to more precisely measure search space overlap.
We also aim to analyse the effects of the model
and search errors for these systems.
Acknowledgements
This research was supported by the Euromatrix
Project funded by the European Commission (6th
Framework Programme). The experiments were
conducted using the resources provided by the
Edinburgh Compute and Data Facility (ECDF).
Many thanks to the three anonymous reviewers for
very helpful comments on earlier drafts.
231
References
N. F. Ayan and B. Dorr. 2006. Going beyond AER:
An extensive analysis of word alignments and their
impact on MT. In Proc. of ACL-COLING, pages 9?
16, Jul.
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
M. Dreyer, K. B. Hall, and S. P. Khudanpur. 2007.
Comparing reordering constraints for SMT using ef-
ficient BLEU oracle computation. In Proc. of Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 103?110, Apr.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast and optimal decoding for machine
translation. Artificial Intelligence, 154(1?2):127?
143, Apr.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL, pages 744?751, Prague, Czech Re-
public, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL,
pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, Jun.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
A. Lopez. 2008a. Statistical machine translation.
ACM Computing Surveys, 40(3).
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proc. of COLING, pages 505?
512, Aug.
A. Lopez. 2009. Translation as weighted deduction.
In Proc. of EACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167, Morristown, NJ, USA.
K. Papineni, S. Roukos, T. Ward, and W. jing Zhu.
2002. BLEU: A method for automatic evaluation
of machine translation. In Proc. of ACL, pages 311?
318.
L. Schwartz. 2008. Multi-source translation methods.
In Proc. of AMTA, October.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Int. Conf. Spoken Lan-
guage Processing (ICSLP 2002).
B. Wellington, S. Waxmonsky, and I. D. Melamed.
2006. Empirical lower bounds on the complexity
of translational equivalence. In Proc. of ACL, pages
977?984, Morristown, NJ, USA.
H. Zhang, D. Gildea, and D. Chiang. 2008. Extracting
synchronous grammar rules from word-level align-
ments in linear time. In Proc. of COLING, pages
1081?1088, Manchester, UK.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte.
2008. A systematic comparison of phrase-based, hi-
erarchical and syntax-augmented statistical MT. In
Proc. of COLING.
232
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun?
a.arun@sms.ed.ac.uk
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Adam Lopez?
alopez@inf.ed.ac.uk
Barry Haddow?
bhaddow@inf.ed.ac.uk
Philipp Koehn?
pkoehn@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
1 Introduction
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f , find the
translation e? that maximises the conditional
posterior probability p(e|f). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes ? a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al, 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al, 2008; DeNero et
al., 2008; Blunsom et al, 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima?an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e?.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al, 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
102
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
2 A Gibbs sampler for phrase-based
translation models
We begin by assuming a phrase-based translation
model in which the input sentence, f , is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector ?, as described in Koehn et al (2003).
P (e, a|f ;?) = exp [? ? h(e, a, f)]?
?e?,a?? exp [? ? h(e?, a?, f)]
(1)
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter ? that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use ? = 6.
2.1 Gibbs sampling
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, . . . , Xn}. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
1These phrases are not necessarily linguistically motivated.
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y ? X , which the operator updates by sam-
pling from the conditional distribution P (Y |X \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) . . . (eN , aN ), that are drawn from the dis-
tribution P (e, a|f). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
EP (a,e|f)[h] = limN??
1
N
N?
i=1
h(ai, ei, f) (2)
Taking h to be an indicator function
h = ?(a, a?)?(e, e?) provides an estimate of
P (a?, e?|f), and using h = ?(e, e?) marginalises over
all derivations a?, yielding an estimate of P (e?|f).
2.2 Gibbs operators
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
?. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
103
c?est un re?sultat remarquable
it is some result remarkable
(a)
Initial
c?est un re?sultat remarquable
but some result remarkable
(b)
Retrans
c?est un re?sultat remarquable
it is a result remarkable
(c)
Merge
c?est un re?sultat remarquable
it is a remarkable result
(d)
Reorder
1
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c?est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, e? in proportion to the
phrases? conditional probabilities.
P (but|c?est,C) = Ptm(but|c?est) ? Plm(S but some)Z
and
P (it is|c?est,C) = Ptm(it is|c?est) ? Plm(S it is some)Z
where
Z = Ptm(but|c?est) ? Plm(S but some) +
Ptm(it is|c?est) ? Plm(S it is some)
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
2.3 Algorithmic complexity
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
2.4 Experimental verification
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution q?(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
3 Decoding
The task of decoding amounts to finding the single
translation e? that maximises or minimises some cri-
terion given a source sentence f . In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
e? =
?
?
?
argmax(e,a) p(e, a|f) (MaxDeriv)
argmaxe p(e|f) (MaxTrans)
argmine?e? `e?(e)p(e?|f) (MinRisk)
In the minimum risk decoder, `e?(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e?. Our loss is a sentence-level approximation of
(1 ? BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P (a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P (e|f)
104
Iterations
KL d
iverg
ence
l l l
l l
l
l l
l
l
l
l
l l l
l l
10 100 1000 10000 100000 1000000
0.00
1
0.01
0.1
KL Divergence
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
source sentence r}ys wzrA? mAlyzyA yzwr Alflbyn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
3.1 Training data and preparation.
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al, 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
3.2 Translation performance
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
? ? the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al, 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
105
?
20.1
?
20.0
?
19.9
?
19.8
?
19.7
?
19.6
Iterations
Mod
el sc
ore
100 1000 10000
French?English
Initialisationfull
mono
nolm
zero
?
40.6
?
40.4
?
40.2
?
40.0
?
39.8
Iterations
Mod
el sc
ore
100 1000 10000 100000
German?English
Initialisationfull
mono
nolm
zero
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman?s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
3.3 Minimum risk decoding
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Corr
elati
on
100 1000 10000 100000
Language Pairsfr?ende?en
Figure 4: Mean Spearman?s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
106
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
3.4 Discussion
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
4 Minimum risk training
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al, 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e?(e), where e? is the refer-
ence translation and e is a hypothesis translation
L = ?
?e?,f??D
?
e
p(e|f)`e?(e) (3)
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters ? that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to ?:
?L
??k
= ?
?e?,f??D
?
e
p(e|f)`e?(e)
(
hk ? Ep(e|f)[hk]
)
(4)
Equation (4) is slightly more complicated to com-
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al, 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
107
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
loss function we use 4-gram (1 ? BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
4.1 Training experiments
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||~? ? ~?||2/2?2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting ?WP = ?0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
4The ngram precision counts are smoothed by adding 0.01
for n > 1
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
5 Discussion and future work
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by ?sharpening? the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
108
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
6 Related work
Our sampler is similar to the decoder of Germann
et al (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al, 2006; Johnson et al, 2007b).
Most closely related is the work of DeNero
et al (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al, 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al, 2007) use biased k-best approximations.
7 Conclusion
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
Acknowledgments
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161?
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
109
framework of weighted parsing. In Natural Language
Processing ? NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106?117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139?
146, Rochester, New York, April.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335?341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
N. N. Schraudolph, J. Yu, and S. Gu?nter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787?794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524?532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97?105, Columbus, Ohio.
110
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 976?985, Prague, June 2007. c?2007 Association for Computational Linguistics
Hierarchical Phrase-Based Translation with Suffix Arrays
Adam Lopez
Computer Science Department
Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742 USA
alopez@cs.umd.edu
Abstract
A major engineering challenge in statistical
machine translation systems is the efficient
representation of extremely large translation
rulesets. In phrase-based models, this prob-
lem can be addressed by storing the training
data in memory and using a suffix array as
an efficient index to quickly lookup and ex-
tract rules on the fly. Hierarchical phrase-
based translation introduces the added wrin-
kle of source phrases with gaps. Lookup
algorithms used for contiguous phrases no
longer apply and the best approximate pat-
tern matching algorithms are much too slow,
taking several minutes per sentence. We
describe new lookup algorithms for hierar-
chical phrase-based translation that reduce
the empirical computation time by nearly
two orders of magnitude, making on-the-fly
lookup feasible for source phrases with gaps.
1 Introduction
Current statistical machine translation systems rely
on very large rule sets. In phrase-based systems,
rules are extracted from parallel corpora containing
tens or hundreds of millions of words. This can re-
sult in millions of rules using even the most conser-
vative extraction heuristics. Efficient algorithms for
rule storage and access are necessary for practical
decoding algorithms. They are crucial to keeping up
with the ever-increasing size of parallel corpora, as
well as the introduction of new data sources such as
web-mined and comparable corpora.
Until recently, most approaches to this problem
involved substantial tradeoffs. The common prac-
tice of test set filtering renders systems impracti-
cal for all but batch processing. Tight restrictions
on phrase length curtail the power of phrase-based
models. However, some promising engineering so-
lutions are emerging. Zens and Ney (2007) use a
disk-based prefix tree, enabling efficient access to
phrase tables much too large to fit in main memory.
An alternative approach introduced independently
by both Callison-Burch et al (2005) and Zhang and
Vogel (2005) is to store the training data itself in
memory, and use a suffix array as an efficient in-
dex to look up, extract, and score phrase pairs on the
fly. We believe that the latter approach has several
important applications (?7).
So far, these techniques have focused on phrase-
based models using contiguous phrases (Koehn et
al., 2003; Och and Ney, 2004). Some recent models
permit discontiguous phrases (Chiang, 2007; Quirk
et al, 2005; Simard et al, 2005). Of particular in-
terest to us is the hierarchical phrase-based model of
Chiang (2007), which has been shown to be supe-
rior to phrase-based models. The ruleset extracted
by this model is a superset of the ruleset in an equiv-
alent phrase-based model, and it is an order of mag-
nitude larger. This makes efficient rule representa-
tion even more critical. We tackle the problem using
the online rule extraction method of Callison-Burch
et al (2005) and Zhang and Vogel (2005).
The problem statement for our work is: Given
an input sentence, efficiently find all hierarchical
phrase-based translation rules for that sentence in
the training corpus.
976
We first review suffix arrays (?2) and hierarchical
phrase-based translation (?3). We show that the ob-
vious approach using state-of-the-art pattern match-
ing algorithms is hopelessly inefficient (?4). We
then describe a series of algorithms to address this
inefficiency (?5). Our algorithms reduce computa-
tion time by two orders of magnitude, making the
approach feasible (?6). We close with a discussion
that describes several applications of our work (?7).
2 Suffix Arrays
A suffix array is a data structure representing all suf-
fixes of a corpus in lexicographical order (Manber
and Myers, 1993). Formally, for a text T , the ith
suffix of T is the substring of the text beginning at
position i and continuing to the end of T . This suf-
fix can be uniquely identified by the index i of its
first word. The suffix array SAT of T is a permuta-
tion of [1, |T |] arranged by the lexicographical order
of the corresponding suffixes. This representation
enables fast lookup of any contiguous substring us-
ing binary search. Specifically, all occurrences of a
length-m substring can be found in O(m + log |T |)
time (Manber and Myers, 1993). 1
Callison-Burch et al (2005) and Zhang and Vogel
(2005) use suffix arrays as follows.
1. Load the source training text F , the suffix array
SAF , the target training text E, and the align-
ment A into memory.
2. For each input sentence, look up each substring
(phrase) f? of the sentence in the suffix array.
3. For each instance of f? found in F , find its
aligned phrase e? using the phrase extraction
method of Koehn et al (2003).
4. Compute the relative frequency score p(e?|f?) of
each pair using the count of the extracted pair
and the marginal count of f? .
5. Compute the lexical weighting score of the
phrase pair using the alignment that gives the
best score.
1Abouelhoda et al (2004) show that lookup can be done in
optimal O(m) time using some auxiliaray data structures. For
our purposes O(m + log |T |) is practical, since for the 27M-
word corpus used to carry out our experiments, log |T | ? 25.
6. Use the scored rules to translate the input sen-
tence with a standard decoding algorithm.
A difficulty with this approach is step 3, which can
be quite slow. Its complexity is linear in the num-
ber of occurrences of the source phrase f? . Both
Callison-Burch et al (2005) and Zhang and Vogel
(2005) solve this with sampling. If a source phrase
appears more than k times, they sample only k oc-
currences for rule extraction. Both papers report
that translation performance is nearly identical to ex-
tracting all possible phrases when k = 100. 2
3 Hierarchical Phrase-Based Translation
We consider the hierarchical translation model of
Chiang (2007). Formally, this model is a syn-
chronous context-free grammar. The lexicalized
translation rules of the grammar may contain a sin-
gle nonterminal symbol, denoted X . We will use a,
b, c and d to denote terminal symbols, and u, v, and
w to denote (possibly empty) sequences of these ter-
minals. We will additionally use ? and ? to denote
(possibly empty) sequences containing both termi-
nals and nonterminals.
A translation rule is written X ? ?/?. This rule
states that a span of the input matching ? is replaced
by ? in translation. We require that ? and ? con-
tain an equal number (possibly zero) of coindexed
nonterminals. An example rule with coindexes is
X ? uX 1 vX 2w/u
?X 2 v
?X 1w
?. When discussing
only the source side of such rules, we will leave out
the coindexes. For instance, the source side of the
above rule will be written uXvXw. 3
For the purposes of this paper, we adhere to the
restrictions described by Chiang (2007) for rules ex-
tracted from the training data.
? Rules can contain at most two nonterminals.
? Rules can contain at most five terminals.
? Rules can span at most ten words.
2A sample size of 100 is actually quite small for many
phrases, some of which occur tens or hundreds of thousands
of times. It is perhaps surprising that such a small sample size
works as well as the full data. However, recent work by Och
(2005) and Federico and Bertoldi (2006) has shown that the
statistics used by phrase-based systems are not very precise.
3In the canonical representation of the grammar, source-side
coindexes are always in sorted order, making them unambigu-
ous.
977
? Nonterminals must span at least two words.
? Adjacent nonterminals are disallowed in the
source side of a rule.
Expressed more economically, we say that our goal
is to search for source phrases in the form u, uXv,
or uXvXw, where 1 ? |uvw| ? 5, and |v| > 0 in
the final case. Note that the model also allows rules
in the form Xu, uX , XuX , XuXv, and uXvX .
However, these rules are lexically identical to other
rules, and thus will match the same locations in the
source text.
4 The Collocation Problem
On-the-fly lookup using suffix arrays involves an
added complication when the rules are in form uXv
or uXvXw. Binary search enables fast lookup
of contiguous substrings. However, it cannot be
used for discontiguous substrings. Consider the rule
aXbXc. If we search for this rule in the following
logical suffix array fragment, we will find the bold-
faced matches.
...
a c a c b a d c a d ...
a c a d b a a d b d ...
a d d b a a d a b c ...
a d d b d a a b b a ...
a d d b d d c a a a ...
...
Even though these suffixes are in lexicographical
order, matching suffixes are interspersed with non-
matching suffixes. We will need another algorithm
to find the source rules containing at least oneX sur-
rounded by nonempty sequences of terminal sym-
bols.
4.1 Baseline Approach
In the pattern-matching literature, words spanned by
the nonterminal symbols of Chiang?s grammar are
called don?t cares and a nonterminal symbol in a
query pattern that matches a sequence of don?t cares
is called a variable length gap. The search prob-
lem for patterns containing these gaps is a variant of
approximate pattern matching, which has received
substantial attention (Navarro, 2001). The best algo-
rithm for pattern matching with variable-length gaps
in a suffix array is a recent algorithm by Rahman
et al (2006). It works on a pattern w1Xw2X...wI
consisting of I contiguous substrings w1, w2, ...wI ,
each separated by a gap. The algorithm is straight-
forward. After identifying all ni occurrences of
each wi in O(|wi| + log |T |) time, collocations that
meet the gap constraints are computed using an ef-
ficient data structure called a stratified tree (van
Emde Boas et al, 1977). 4 Although we refer the
reader to the source text for a full description of
this data structure, its salient characteristic is that
it implements priority queue operations insert and
next-element in O(log log |T |) time. Therefore, the
total running time for an algorithm to find all con-
tiguous subpatterns and compute their collocations
is O(
?I
i=1 [|wi|+ log|T |+ ni log log |T |]).
We can improve on the algorithm of Rahman et
al. (2006) using a variation on the idea of hashing.
We exploit the fact that our large text is actually a
collection of relatively short sentences, and that col-
located patterns must occur in the same sentence in
order to be considered a rule. Therefore, we can
use the sentence id of each subpattern occurrence
as a kind of hash key. We create a hash table whose
size is exactly the number of sentences in our train-
ing corpus. Each location of the partially matched
pattern w1X...Xwi is inserted into the hash bucket
with the matching sentence id. To find collocated
patterns wi+1, we probe the hash table with each
of the ni+1 locations for that subpattern. When a
match is found, we compare the element with all el-
ements in the bucket to see if it is within the window
imposed by the phrase length constraints. Theoreti-
cally, the worst case for this algorithm occurs when
all elements of both sets resolve to the same hash
bucket, and we must compare all elements of one
set with all elements of the other set. This leads to a
worst case complexity of O(
?I
i=1 [|wi|+ log|T |] +?I
i=1 ni). However, for real language data the per-
formance for sets of any significant size will be
O(
?I
i=1 [|wi|+ log|T |+ ni]), since most patterns
will occur once in any given sentence.
4.2 Analysis
It is instructive to compare this with the complex-
ity for contiguous phrases. In that case, total lookup
time is O(|w| + log|T |) for a contiguous pattern w.
4Often known in the literature as a van Emde Boas tree or
van Emde Boas priority queue.
978
The crucial difference between the contiguous and
discontiguous case is the added term
?I
i=1 ni. For
even moderately frequent subpatterns this term dom-
inates complexity.
To make matters concrete, consider the training
corpus used in our experiments (?6), which contains
27M source words. The three most frequent uni-
grams occur 1.48M, 1.16M and 688K times ? the
first two occur on average more than once per sen-
tence. In the worst case, looking up a contiguous
phrase containing any number and combination of
these unigrams requires no more than 25 compari-
son operations. In contrast, the worst case scenario
for a pattern with a single gap, bookended on either
side by the most frequent word, requires over two
million operations using our baseline algorithm and
over thirteen million using the algorithm of Rahman
et al (2006). A single frequent word in an input
sentence is enough to cause noticeable slowdowns,
since it can appear in up to 530 hierarchical rules.
To analyze the cost empirically, we ran our base-
line algorithm on the first 50 sentences of the NIST
Chinese-English 2003 test set and measured the
CPU time taken to compute collocations. We found
that, on average, it took 2241.25 seconds (?37 min-
utes) per sentence just to compute all of the needed
collocations. By comparison, decoding time per
sentence is roughly 10 seconds with moderately ag-
gressive pruning, using the Python implementation
of Chiang (2007).
5 Solving the Collocation Problem
Clearly, looking up patterns in this way is not prac-
tical. To analyze the problem, we measured the
amount of CPU time per computation. Cumulative
lookup time was dominated by a very small fraction
of the computations (Fig. 1). As expected, further
analysis showed that these expensive computations
all involved one or more very frequent subpatterns.
In the worst cases a single collocation took several
seconds to compute. However, there is a silver lin-
ing. Patterns follow a Zipf distribution, so the num-
ber of pattern types that cause the problem is actu-
ally quite small. The vast majority of patterns are
rare. Therefore, our solution focuses on computa-
tions where one or more of the component patterns
is frequent. Assume that we are computing a collo-
Computations (ranked by time)
C
u
m
u
l
a
t
i
v
e
 
T
i
m
e
 
(
s
)
300K
150K
Figure 1: Ranked computations vs. cumulative time.
A small fraction of all computations account for
most of the computational time.
cation of pattern w1X...Xwi and pattern wi+1, and
we know all locations of each. There are three cases.
? If both patterns are frequent, we resort to a
precomputed intersection (?5.1). We were not
aware of any algorithms to substantially im-
prove the efficiency of this computation when it
is requested on the fly, but precomputation can
be done in a single pass over the text at decoder
startup.
? If one pattern is frequent and the other is rare,
we use an algorithm whose complexity is de-
pendent mainly on the frequency of the rare
pattern (?5.2). It can also be used for pairs
of rare patterns when one pattern is much rarer
than the other.
? If both patterns are rare, no special algorithms
are needed. Any linear algorithm will suffice.
However, for reasons described in ?5.3, our
other collocation algorithms depend on sorted
sets, so we use a merge algorithm.
Finally, in order to cut down on the number of un-
necessary computations, we use an efficient method
to enumerate the phrases to lookup (?5.4). This
method also forms the basis of various caching
strategies for additional speedups. We analyze the
memory use of our algorithms in ?5.5.
5.1 Precomputation
Precomputation of the most expensive collocations
can be done in a single pass over the text. As in-
put, our algorithm requires the identities of the k
979
most frequent contiguous patterns. 5 It then iterates
over the corpus. Whenever a pattern from the list is
seen, we push a tuple consisting of its identity and
current location onto a queue. Whenever the oldest
item on the queue falls outside the maximum phrase
length window with respect to the current position,
we compute that item?s collocation with all succeed-
ing patterns (subject to pattern length constraints)
and pop it from the queue. We repeat this step for
every item that falls outside the window. At the end
of each sentence, we compute collocations for any
remaining items in the queue and then empty it.
Our precomputation includes the most frequent
n-gram subpatterns. Most of these are unigrams,
but in our experiments we found 5-grams among
the 1000 most frequent patterns. We precompute
the locations of source phrase uXv for any pair u
and v that both appear on this list. There is also
a small number of patterns uXv that are very fre-
quent. We cannot easily obtain a list of these in ad-
vance, but we observe that they always consist of a
pair u and v of patterns from near the top of the fre-
quency list. Therefore we also precompute the loca-
tions uXvXw of patterns in which both u and v are
among these super-frequent patterns (all unigrams),
treating this as the collocation of the frequent pattern
uXv and frequent pattern w. We also compute the
analagous case for u and vXw.
5.2 Fast Intersection
For collocations of frequent and rare patterns, we
use a fast set intersection method for sorted sets
called double binary search (Baeza-Yates, 2004). 6
It is based on the intuition that if one set in a pair
of sorted sets is much smaller than the other, then
we can compute their intersection efficiently by per-
forming a binary search in the larger data set D for
each element of the smaller query set Q.
Double binary search takes this idea a step further.
It performs a binary search in D for the median ele-
ment of Q. Whether or not the element is found, the
5These can be identified using a single traversal over a
longest common prefix (LCP) array, an auxiliary data struc-
ture of the suffix array, described by Manber and Myers (1993).
Since we don?t need the LCP array at runtime, we chose to do
this computation once offline.
6Minor modifications are required since we are computing
collocation rather than intersection. Due to space constraints,
details and proof of correctness are available in Lopez (2007a).
search divides both sets into two pairs of smaller sets
that can be processed recursively. Detailed analysis
and empirical results on an information retrieval task
are reported in Baeza-Yates (2004) and Baeza-Yates
and Salinger (2005). If |Q| log |D| < |D| then the
performance is guaranteed to be sublinear. In prac-
tice it is often sublinear even if |Q| log |D| is some-
what larger than |D|. In our implementation we sim-
ply check for the condition ?|Q| log |D| < |D| to
decide whether we should use double binary search
or the merge algorithm. This check is applied in the
recursive cases as well as for the initial inputs. The
variable ? can be adjusted for performance. We de-
termined experimentally that a good value for this
parameter is 0.3.
5.3 Obtaining Sorted Sets
Double binary search requires that its input sets be
in sorted order. However, the suffix array returns
matchings in lexicographical order, not numeric or-
der. The algorithm of Rahman et al (2006) deals
with this problem by inserting the unordered items
into a stratified tree. This requires O(n log log |T |)
time for n items. If we used the same strategy, our
algorithm would no longer be sublinear.
An alternative is to precompute all n-gram occur-
rences in order and store them in an inverted index.
This can be done in one pass over the data. 7 This
approach requires a separate inverted index for each
n, up to the maximum n used by the model. The
memory cost is one length-|T | array per index.
In order to avoid the full n|T | cost in memory,
our implementation uses a mixed strategy. We keep
a precomputed inverted index only for unigrams.
For bigrams and larger n-grams, we generate the in-
dex on the fly using stratified trees. This results in
a superlinear algorithm for intersection. However,
we can exploit the fact that we must compute col-
locations multiple times for each input n-gram by
caching the sorted set after we create it (The caching
strategy is described in ?5.4). Subsequent computa-
tions involving this n-gram can then be done in lin-
ear or sublinear time. Therefore, the cost of building
the inverted index on the fly is amortized over a large
number of computations.
7We combine this step with the other precomputations that
require a pass over the data, thereby removing a redundant
O(|T |) term from the startup cost.
980
5.4 Efficient Enumeration
A major difference between contiguous phrase-
based models and hierarchical phrase-based models
is the number of rules that potentially apply to an
input sentence. To make this concrete, on our data,
with an average of 29 words per sentence, there were
on average 133 contiguous phrases of length 5 or
less that applied. By comparison, there were on av-
erage 7557 hierarchical phrases containing up to 5
words. These patterns are obviously highly overlap-
ping and we employ an algorithm to exploit this fact.
We first describe a baseline algorithm used for con-
tiguous phrases (?5.4.1). We then introduce some
improvements (?5.4.2) and describe a data structure
used by the algorithm (?5.4.3). Finally, we dis-
cuss some special cases for discontiguous phrases
(?5.4.4).
5.4.1 The Zhang-Vogel Algorithm
Zhang and Vogel (2005) present a clever algo-
rithm for contiguous phrase searches in a suffix ar-
ray. It exploits the fact that for eachm-length source
phrase that we want to look up, we will also want to
look up its (m? 1)-length prefix. They observe that
the region of the suffix array containing all suffixes
prefixed by ua is a subset of the region containing
the suffixes prefixed by u. Therefore, if we enumer-
ate the phrases of our sentence in such a way that
we always search for u before searching for ua, we
can restrict the binary search for ua to the range con-
taining the suffixes prefixed by u. If the search for
u fails, we do not need to search for ua at all. They
show that this approach leads to some time savings
for phrase search, although the gains are relatively
modest since the search for contiguous phrases is not
very expensive to begin with. However, the potential
savings in the discontiguous case are much greater.
5.4.2 Improvements and Extensions
We can improve on the Zhang-Vogel algorithm.
An m-length contiguous phrase aub depends not
only on the existence of its prefix au, but also on
the existence of its suffix ub. In the contiguous case,
we cannot use this information to restrict the starting
range of the binary search, but we can check for the
existence of ub to decide whether we even need to
search for aub at all. This can help us avoid searches
that are guaranteed to be fruitless.
Now consider the discontiguous case. As in the
analogous contiguous case, a phrase a?b will only
exist in the text if its maximal prefix a? and maxi-
mal suffix ?b both exist in the corpus and overlap at
specific positions. 8 Searching for a?b is potentially
very expensive, so we put all available information
to work. Before searching, we require that both a?
and ?b exist. Additionally, we compute the loca-
tion of a?b using the locations of both maximal sub-
phrases. To see why the latter optimization is useful,
consider a phrase abXcd. In our baseline algorithm,
we would search for ab and cd, and then perform a
computation to see whether these subphrases were
collocated within an elastic window. However, if we
instead use abXc and bXcd as the basis of the com-
putation, we gain two advantages. First, the number
elements of each set is likely to be smaller then in
the former case. Second, the computation becomes
simpler, because we now only need to check to see
whether the patterns exactly overlap with a starting
offset of one, rather than checking within a window
of locations.
We can improve efficiency even further if we con-
sider cases where the same substring occurs more
than once within the same sentence, or even in mul-
tiple sentences. If the computation required to look
up a phrase is expensive, we would like to perform
the lookup only once. This requires some mecha-
nism for caching. Depending on the situation, we
might want to cache only certain subsets of phrases,
based on their frequency or difficulty to compute.
We would also like the flexibility to combine on-
the-fly lookups with a partially precomputed phrase
table, as in the online/offline mixture of Zhang and
Vogel (2005).
We need a data structure that provides this flex-
ibility, in addition to providing fast access to both
the maximal prefix and maximal suffix of any phrase
that we might consider.
5.4.3 Prefix Trees and Suffix Links
Our search optimizations are easily captured in a
prefix tree data structure augmented with suffix links.
Formally, a prefix tree is an unminimized determin-
istic finite-state automaton that recognizes all of the
patterns in some set. Each node in the tree repre-
8Except when ? = X , in which case a and b must be collo-
cated within a window defined by the phrase length constraints.
981
ab
b
c
cX
X
(1)(2)
(3)
d
(4)
d
a
b
b
c
cX
X
(1)(2)
(3)
d
(4)
d
a
b
b
c
cX
X
(1)(2)
(3)
d
(4)
d
a
b
b
c
cX
X
(1)(2)
(3)
d
(4)
d
X
e
a
c
d
Case 1
Case 2
Figure 2: Illustration of prefix tree construction showing a partial prefix tree, including suffix links. Suppose
we are interested in pattern abXcd, represented by node (1). Its prefix is represented by node (2), and node
(2)?s suffix is represented by node (3). Therefore, node (1)?s suffix is represented by the node pointed to by
the d-edge from node (3), which is node (4). There are two cases. In case 1, node (4) is inactive, so we
can mark node (1) inactive and stop. In case 2, node (4) is active, so we compute the collocation of abXc
and bXcd with information stored at nodes (2) and (4), using either a precomputed intersection, double
binary search, or merge, depending on the size of the sets. If the result is empty, we mark the node inactive.
Otherwise, we store the results at node (1) and add its successor patterns to the frontier for the next iteration.
This includes all patterns containing exactly one more terminal symbol than the current pattern.
sents the prefix of a unique pattern from the set that
is specified by the concatenation of the edge labels
along the path from the root to that node. A suffix
link is a pointer from a node representing path a? to
the node representing path ?. We will use this data
structure to record the set of patterns that we have
searched for and to cache information for those that
were found successfully.
Our algorithm generates the tree breadth-search
along a frontier. In the mth iteration we only search
for patterns containingm terminal symbols. Regard-
less of whether we find a particular pattern, we cre-
ate a node for it in the tree. If the pattern was found
in the corpus, its node is marked active. Otherwise,
it is marked inactive. For found patterns, we store
either the endpoints of the suffix array range con-
taining the phrase (if it is contiguous), or the list of
locations at which the phrase is found (if it is dis-
contiguous). We can also store the extracted rules. 9
Whenever a pattern is successfully found, we add all
patterns with m + 1 terminals that are prefixed by it
9Conveniently, the implementation of Chiang (2007) uses a
prefix tree grammar encoding, as described in Klein and Man-
ning (2001). Our implementation decorates this tree with addi-
tional information required by our algorithms.
to the frontier for processing in the next iteration.
To search for a pattern, we use location infor-
mation from its parent node, which represents its
maximal prefix. Assuming that the node represents
phrase ?b, we find the node representing its max-
imal suffix by following the b-edge from the node
pointed to by its parent node?s suffix link. If the node
pointed to by this suffix link is inactive, we can mark
the node inactive without running a search. When a
node is marked inactive, we discontinue search for
phrases that are prefixed by the path it represents.
The algorithm is illustrated in Figure 2.
5.4.4 Special Cases for Phrases with Gaps
A few subtleties arise in the extraction of hierar-
chical patterns. Gaps are allowed to occur at the be-
ginning or end of a phrase. For instance, we may
have a source phrase Xu or uX or even XuX . Al-
though each of these phrases requires its own path in
the prefix tree, they are lexically identical to phrase
u. An analogous situation occurs with the patterns
XuXv, uXvX , and uXv. There are two cases that
we are concerned with.
The first case consists of all patterns prefixed with
X . The paths to nodes representing these patterns
982
will all contain the X-edge originating at the root
node. All of these paths form the shadow sub-
tree. Path construction in this subtree proceeds dif-
ferently. Because they are lexically identical to their
suffixes, they are automatically extended if their suf-
fix paths are active, and they inherit location infor-
mation of their suffixes.
The second case consists of all patterns suffixed
with X . Whenever we successfully find a new pat-
tern ?, we automatically extend it with an X edge,
provided that ?X is allowed by the model con-
straints. The node pointed to by this edge inherits
its location information from its parent node (repre-
senting the maximal prefix ?).
Note that both special cases occur for patterns in
the form XuX .
5.5 Memory Requirements
As shown in Callison-Burch et al (2005), we must
keep an array for the source text F , its suffix array,
the target text E, and alignment A in memory. As-
suming that A and E are roughly the size of F , the
cost is 4|T |. If we assume that all data use vocabu-
laries that can be represented using 32-bit integers,
then our 27M word corpus can easily be represented
in around 500MB of memory. Adding the inverted
index for unigrams increases this by 20%. The main
additional cost in memory comes from the storage
of the precomputed collocations. This is dependent
both on the corpus size and the number of colloca-
tions that we choose to precompute. Using detailed
timing data from our experiments we were able to
simulate the memory-speed tradeoff (Fig. 3). If we
include a trigram model trained on our bitext and the
Chinese Gigaword corpus, the overall storage costs
for our system are approximately 2GB.
6 Experiments
All of our experiments were performed on Chinese-
English in the news domain. We used a large train-
ing set consisting of over 1 million sentences from
various newswire corpora. This corpus is roughly
the same as the one used for large-scale experiments
by Chiang et al (2005). To generate alignments,
we used GIZA++ (Och and Ney, 2003). We sym-
metrized bidirectional alignments using the grow-
diag-final heuristic (Koehn et al, 2003).
0
0
0
1000
0
Number of frequent subpatterns
Insert text here
41 sec/sent
41 seconds
405 sec/sent
0 MB
725MB
Figure 3: Effect of precomputation on memory use
and processing time. Here we show only the mem-
ory requirements of the precomputed collocations.
We used the first 50 sentences of the NIST 2003
test set to compute timing results. All of our algo-
rithms were implemented in Python 2.4. 10 Timing
results are reported for machines with 8GB of mem-
ory and 4 3GHz Xeon processors running Red Hat
linux 2.6.9. In order to understand the contributions
of various improvements, we also ran the system
with with various ablations. In the default setting,
the prefix tree is constructed for each sentence to
guide phrase lookup, and then discarded. To show
the effect of caching we also ran the algorithm with-
out discarding the prefix tree between sentences, re-
sulting in full inter-sentence caching. The results are
shown in Table 1. 11
It is clear from the results that each of the op-
timizations is needed to sufficiently reduce lookup
time to practical levels. Although this is still rela-
tively slow, it is much closer to the decoding time of
10 seconds per sentence than the baseline.
10Python is an interpreted language and our implementations
do not use any optimization features. It is therefore reasonable
to think that a more efficient reimplementation would result in
across-the-board speedups.
11The results shown here do not include the startup time re-
quired to load the data structures into memory. In our Python
implementation this takes several minutes, which in principle
should be amortized over the cost for each sentence. However,
just as Zens and Ney (2007) do for phrase tables, we could com-
pile our data structures into binary memory-mapped files, which
can be read into memory in a matter of seconds. We are cur-
rently investigating this option in a C reimplementation.
983
Algorithms Secs/Sent Collocations
Baseline 2241.25 325548
Prefix Tree 1578.77 69994
Prefix Tree + precomputation 696.35 69994
Prefix Tree + double binary 405.02 69994
Prefix Tree + precomputation + double binary 40.77 69994
Prefix Tree with full caching + precomputation + double binary 30.70 67712
Table 1: Timing results and number of collocations computed for various combinations of algorithms. The
runs using precomputation use the 1000 most frequent patterns.
7 Conclusions and Future Work
Our work solves a seemingly intractable problem
and opens up a number of intriguing potential ap-
plications. Both Callison-Burch et al (2005) and
Zhang and Vogel (2005) use suffix arrays to relax
the length constraints on phrase-based models. Our
work enables this in hierarchical phrase-based mod-
els. However, we are interested in additional appli-
cations.
Recent work in discriminative learning for many
natural language tasks, such as part-of-speech tag-
ging and information extraction, has shown that fea-
ture engineering plays a critical role in these ap-
proaches. However, in machine translation most fea-
tures can still be traced back to the IBM Models of
15 years ago (Lopez, 2007b). Recently, Lopez and
Resnik (2006) showed that most of the features used
in standard phrase-based models do not help very
much. Our algorithms enable us to look up phrase
pairs in context, which will allow us to compute in-
teresting contextual features that can be used in dis-
criminative learning algorithms to improve transla-
tion accuracy. Essentially, we can use the training
data itself as an indirect representation of whatever
features we might want to compute. This is not pos-
sible with table-based architectures.
Most of the data structures and algorithms dis-
cussed in this paper are widely used in bioinformat-
ics, including suffix arrays, prefix trees, and suf-
fix links (Gusfield, 1997). As discussed in ?4.1,
our problem is a variant of the approximate pattern
matching problem. A major application of approx-
imate pattern matching in bioinformatics is query
processing in protein databases for purposes of se-
quencing, phylogeny, and motif identification.
Current MT models, including hierarchical mod-
els, translate by breaking the input sentence into
small pieces and translating them largely indepen-
dently. Using approximate pattern matching algo-
rithms, we imagine that machine translation could
be treated very much like search in a protein
database. In this scenario, the goal is to select
training sentences that match the input sentence as
closely as possible, under some evaluation function
that accounts for both matching and mismatched
sequences, as well as possibly other data features.
Once we have found the closest sentences we can
translate the matched portions in their entirety, re-
placing mismatches with appropriate word, phrase,
or hierarchical phrase translations as needed. This
model would bring statistical machine translation
closer to convergence with so-called example-based
translation, following current trends (Marcu, 2001;
Och, 2002). We intend to explore these ideas in fu-
ture work.
Acknowledgements
I would like to thank Philip Resnik for encour-
agement, thoughtful discussions and wise counsel;
David Chiang for providing the source code for his
translation system; and Nitin Madnani, Smaranda
Muresan and the anonymous reviewers for very
helpful comments on earlier drafts of this paper.
Any errors are my own. This research was supported
in part by ONR MURI Contract FCPO.810548265
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, findings, conclusions or
recommendations expressed in this paper are those
of the author and do not necessarily reflect the view
of DARPA.
984
References
Mohamed Ibrahim Abouelhoda, Stefan Kurtz, and Enno
Ohlebusch. 2004. Replacing suffix trees with en-
hanced suffix arrays. Journal of Discrete Algorithms,
2(1):53?86, Mar.
Ricardo Baeza-Yates and Alejandro Salinger. 2005. Ex-
perimental analysis of a fast intersection algorithm for
sorted sequences. In M. Consens and G. Navarro, ed-
itors, Proc. of SPIRE, number 3772 in LNCS, pages
13?24, Berlin. Springer-Verlag.
Ricardo Baeza-Yates. 2004. A fast intersection algo-
rithm for sorted sequences. In Proc. of Combinatorial
Pattern Matching, number 3109 in LNCS, pages 400?
408, Berlin. Springer-Verlag.
Chris Callison-Burch, Colin Bannard, and Josh Shroeder.
2005. Scaling phrase-based statistical machine trans-
lation to larger corpora and longer phrases. In Proc. of
ACL, pages 255?262, Jun.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, and Michael Subotin. 2005. The
Hiero machine translation system: Extensions, evalua-
tion, and analysis. In Proc. of HLT-EMLP, pages 779?
786, Oct.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2). In press.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proc. of NAACL Workshop on
Statistical Machine Translation, pages 94?101, Jun.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
Dan Klein and Christopher D. Manning. 2001. Parsing
with treebank grammars: Empirical bounds, theoreti-
cal models, and the structure of the Penn Treebank. In
Proc. of ACL-EACL, pages 330?337, Jul.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 127?133, May.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What?s the link?
In Proc. of AMTA, pages 90?99, Aug.
Adam Lopez. 2007a. Hierarchical phrase-based trans-
lation with suffix arrays. Technical Report 2007-26,
University of Maryland Institute for Advanced Com-
puter Studies, May.
Adam Lopez. 2007b. A survey of statistical machine
translation. Technical Report 2006-47, University of
Maryland Institute for Advanced Computer Studies,
Apr.
Udi Manber and Gene Myers. 1993. Suffix arrays: A
new method for on-line string searches. SIAM Journal
of Computing, 22(5):935?948.
Daniel Marcu. 2001. Towards a unified approach to
memory- and statistical-based machine translation. In
Proc. of ACL-EACL, pages 378?385, Jul.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33(1):31?
88, Mar.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51, Mar.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to machine translation. Computa-
tional Linguistics, 30(4):417?449, Jun.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Oct.
Franz Josef Och. 2005. Statistical machine translation:
The fabulous present and future. In Proc. of ACL
Workshop on Building and Using Parallel Texts, Jun.
Invited talk.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. of ACL, pages 271?279, Jun.
Mohammad Sohel Rahman, Costas S. Iliopoulos, Inbok
Lee, Manal Mohamed, and William F. Smyth. 2006.
Finding patterns with variable length gaps or don?t
cares. In Proc. of COCOON, Aug.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-
mada, Philippe Langlais, and Arne Mauser. 2005.
Translating with non-contiguous phrases. In Proc. of
HLT-EMNLP, pages 755?762, Oct.
Peter van Emde Boas, R. Kaas, and E. Zijlstra. 1977. De-
sign and implementation of an efficient priority queue.
Mathematical Systems Theory, 10(2):99?127.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Proc.
of HLT-NAACL. To appear.
Ying Zhang and Stephan Vogel. 2005. An efficient
phrase-to-phrase alignment model for arbitrarily long
phrase and large corpora. In Proc. of EAMT, May.
985
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 12?13,
Vancouver, October 2005.
Pattern Visualization for Machine Translation Output
Adam Lopez
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
alopez@cs.umd.edu
Philip Resnik
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland
College Park, MD 20742
resnik@umd.edu
Abstract
We describe a method for identifying system-
atic patterns in translation data using part-of-
speech tag sequences. We incorporate this
analysis into a diagnostic tool intended for de-
velopers of machine translation systems, and
demonstrate how our application can be used
by developers to explore patterns in machine
translation output.
1 Introduction
Over the last few years, several automatic metrics for ma-
chine translation (MT) evaluation have been introduced,
largely to reduce the human cost of iterative system evalu-
ation during the development cycle (Papineni et al, 2002;
Melamed et al, 2003). All are predicated on the con-
cept of n-gram matching between the sentence hypoth-
esized by the translation system and one or more ref-
erence translations?that is, human translations for the
test sentence. Although the formulae underlying these
metrics vary, each produces a single number represent-
ing the ?goodness? of the MT system output over a set
of reference documents. We can compare the numbers of
competing systems to get a coarse estimate of their rela-
tive performance. However, this comparison is holistic.
It provides no insight into the specific competencies or
weaknesses of either system.
Ideally, we would like to use automatic methods to pro-
vide immediate diagnostic information about the transla-
tion output?what the system does well, and what it does
poorly. At the most general level, we want to know how
our system performs on the two most basic problems in
translation ? word translation and reordering. Holistic
metrics are at odds with day-to-day hypothesis testing on
these two problems. For instance, during the develop-
ment of a new MT system we may may wish to compare
competing reordering models. We can incorporate each
model into the system in turn, and rank the results on a
test corpus using BLEU (Papineni et al, 2002). We might
then conclude that the model used in the highest-scoring
system is best. However, this is merely an implicit test
of the hypothesis; it does not tell us anything about the
specific strengths and weaknesses of each method, which
may be different from our expectations. Furthermore, if
we understand the relative strengths of each method, we
may be able to devise good ways to combine them, rather
than simply using the best one, or combining strictly by
trial and error. In order to fine-tune MT systems, we need
fine-grained error analysis.
What we would really like to know is how well the
system is able to capture systematic reordering patterns
in the input, which ones it is successful with, and which
ones it has difficulty with. Word n-grams are little help
here: they are too many, too sparse, and it is difficult to
discern general patterns from them.
2 Part-of-Speech Sequence Recall
In developing a new analysis method, we are motivated
in part by recent studies suggesting that word reorder-
ings follow general patterns with respect to syntax, al-
though there remains a high degree of flexibility (Fox,
2002; Hwa et al, 2002). This suggests that in a com-
parative analysis of two MT systems (or two versions of
the same system), it may be useful to look for syntactic
patterns that one system (or version) captures well in the
target language and the other does not, using a syntax-
based, recall-oriented metric.
As an initial step, we would like to summarize reorder-
ing patterns using part-of-speech sequences. Unfortu-
nately, recent work has confirmed the intuition that ap-
plying statistical analyzers trained on well-formed text to
the noisy output of MT systems produces unuseable re-
sults (e.g. (Och et al, 2004)). Therefore, we make the
conservative choice to apply annotation only to the refer-
ence corpus. Word n-gram correspondences with a refer-
ence translation are used to infer the part-of-speech tags
for words in the system output.
The method:
1. Part-of-speech tag the reference corpus. We used
12
Figure 1: Comparing two systems that differ significantly in their recall for POS n-gram JJ NN IN DT NN. The
interface uses color to make examples easy to find.
MXPOST (Ratnaparkhi, 1996), and in order to dis-
cover more general patterns, we map the tag set
down after tagging, e.g. NN, NNP, NNPS and NNS
all map to NN.
2. Compute the frequency freq(ti . . . t j) of every possi-
ble tag sequence ti . . . t j in the reference corpus.
3. Compute the correspondence between each hypoth-
esis sentence and each of its corresponding refer-
ence sentences using an approximation to maximum
matching (Melamed et al, 2003). This algorithm
provides a list of runs or contiguous sequences of
words ei . . .e j in the reference that are also present in
the hypothesis. (Note that runs are order-sensitive.)
4. For each recalled n-gram ei . . .e j, look up the asso-
ciated tag sequence ti . . . t j and increment a counter
recalled(ti . . . t j)
Using this method, we compute the recall of tag pat-
terns, R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
To compare two systems (which could include two ver-
sions of the same system), we identify POS n-grams that
are recalled significantly more frequently by one system
than the other, using a difference-of-proportions test to
assess statistical significance. We have used this method
to analyze the output of two different statistical machine
translation models (Chiang et al, 2005).
3 Visualization
Our demonstration system uses an HTML interface to
summarize the observed pattern recall. Based on frequent
or significantly-different recall, the user can select and
visually inspect color-coded examples of each pattern of
interest in context with both source and reference sen-
tences. An example visualization is shown in Figure 1.
4 Acknowledgements
The authors would like to thank David Chiang, Christof
Monz, and Michael Subotin for helpful commentary on
this work. This research was supported in part by ONR
MURI Contract FCPO.810548265 and Department of
Defense contract RD-02-5700.
References
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz,
Philip Resnik, and Michael Subotin. 2005. The hiero ma-
chine translation system: Extensions, evaluation, and analy-
sis. In Proceedings of HLT/EMNLP 2005, Oct.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proceedings of the 2002 Conference
on EMNLP, pages 304?311, Jul.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak.
2002. Evaluating translational correspondence using annota-
tion projection. In Proceedings of the 40th Annual Meeting
of the ACL, pages 392?399, Jul.
I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003.
Precision and recall of machine translation. In HLT-NAACL
2003 Companion Volume, pages 61?63, May.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin
Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. 2004. A smorgasbord of features for sta-
tistical machine translation. In Proceedings of HLT-NAACL
2004, pages 161?168, May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the ACL, pages 311?318, Jul.
Adwait Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proceedings of the Conference on
EMNLP, pages 133?142, May.
13
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83?86,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Improved HMM Alignment Models for Languages with Scarce Resources
Adam Lopez
Institute for Advanced Computer Studies
Department of Computer Science
University of Maryland
College Park, MD 20742
alopez@cs.umd.edu
Philip Resnik
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland
College Park, MD 20742
resnik@umiacs.umd.edu
Abstract
We introduce improvements to statistical word
alignment based on the Hidden Markov
Model. One improvement incorporates syntac-
tic knowledge. Results on the workshop data
show that alignment performance exceeds that
of a state-of-the art system based on more com-
plex models, resulting in over a 5.5% absolute
reduction in error on Romanian-English.
1 Introduction
The most widely used alignment model is IBM Model 4
(Brown et al, 1993). In empirical evaluations it has out-
performed the other IBM Models and a Hidden Markov
Model (HMM) (Och and Ney, 2003). It was the basis
for a system that performed very well in a comparison
of several alignment systems (Dejean et al, 2003; Mihal-
cea and Pedersen, 2003). Implementations are also freely
available (Al-Onaizan et al, 1999; Och and Ney, 2003).
The IBM Model 4 search space cannot be efficiently
enumerated; therefore it cannot be trained directly using
Expectation Maximization (EM). In practice, a sequence
of simpler models such as IBM Model 1 and an HMM
Model are used to generate initial parameter estimates
and to enumerate a partial search space which can be ex-
panded using hill-climbing heuristics. IBM Model 4 pa-
rameters are then estimated over this partial search space
as an approximation to EM (Brown et al, 1993; Och and
Ney, 2003). This approach yields good results, but it has
been observed that the IBM Model 4 performance is only
slightly better than that of the underlying HMM Model
used in this bootstrapping process (Och and Ney, 2003).
This is illustrated in Figure 1.
Based on this observation, we hypothesize that imple-
mentations of IBM Model 4 derive most of their per-
formance benefits from the underlying HMM Model.
Furthermore, owing to the simplicity of HMM Models,
we believe that they are more conducive to study and
improvement than more complex models such as IBM
Model 4. We illustrate this point by introducing modifi-
cations to the HMM model which improve performance.
.3
.35
.4
.45
.5
.55
.6
.65
.7
Model 1 HMM Model 4
AER
Training Iterations
?
?
? ? ?
? ? ? ? ? ? ? ? ? ? ?
?
?
?
?
? ? ? ? ? ? ? ? ? ?
Figure 1: The improvement in Alignment Error Rate
(AER) is shown for both P(f|e) and P(e|f) alignments on
the Romanian-English development set over several iter-
ations of the IBM Model 1 ? HMM ? IBM Model 4
training sequence.
2 HMMs and Word Alignment
The objective of word alignment is to discover the word-
to-word translational correspondences in a bilingual cor-
pus of S sentence pairs, which we denote {(f(s),e(s)) : s ?
[1,S]}. Each sentence pair (f,e) = ( f M1 ,eN1 ) consists of
a sentence f in one language and its translation e in the
other, with lengths M and N, respectively. By convention
we refer to e as the English sentence and f as the French
sentence. Correspondences in a sentence are represented
by a set of links between words. A link ( f j ,ei) denotes a
correspondence between the ith word ei of e and the jth
word f j of f.
Many alignment models arise from the conditional dis-
tribution P(f|e). We can decompose this by introducing
the hidden alignment variable a = aM1 . Each element of
a takes on a value in the range [1,N]. The value of ai
determines a link between the ith French word fi and
the aith English word eai . This representation introduces
83
an asymmetry into the model because it constrains each
French word to correspond to exactly one English word,
while each English word is permitted to correspond to an
arbitrary number of French words. Although the result-
ing set of links may still be relatively accurate, we can
symmetrize by combining it with the set produced by ap-
plying the complementary model P(e|f) to the same data
(Och and Ney, 2000b). Making a few independence as-
sumptions we arrive at the decomposition in Equation 1. 1
P(f,a|e) =
M
?
i=1
d(ai|ai?1) ? t( fi|eai) (1)
We refer to d(ai|ai?1) as the distortion model and t( fi|eai)
as the translation model. Conveniently, Equation 1 is in
the form of an HMM, so we can apply standard algo-
rithms for HMM parameter estimation and maximization.
This approach was proposed in Vogel et al (1996) and
subsequently improved (Och and Ney, 2000a; Toutanova
et al, 2002).
2.1 The Tree Distortion Model
Equation 1 is adequate in practice, but we can improve
it. Numerous parameterizations have been proposed for
the distortion model. In our surface distortion model, it
depends only on the distance ai ? ai?1 and an automati-
cally determined word class C(eai?1) as shown in Equa-
tion 2. It is similar to (Och and Ney, 2000a). The word
class C(eai?1) is assigned using an unsupervised approach
(Och, 1999).
d(ai|ai?1) = p(ai|ai ?ai?1,C(eai?1)) (2)
The surface distortion model can capture local move-
ment but it cannot capture movement of structures or the
behavior of long-distance dependencies across transla-
tions. The intuitive appeal of capturing richer informa-
tion has inspired numerous alignment models (Wu, 1995;
Yamada and Knight, 2001; Cherry and Lin, 2003). How-
ever, we would like to retain the simplicity and good per-
formance of the HMM Model.
We introduce a distortion model which depends on the
tree distance ?(ei,ek) = (w,x,y) between each pair of En-
glish words ei and ek. Given a dependency parse of eM1 ,
w and x represent the respective number of dependency
links separating ei and ek from their closest common an-
cestor node in the parse tree. 2 The final element y = {1
1We ignore the sentence length probability p(M|N), which
is not relevant to word alignment. We also omit discussion
of HMM start and stop probabilities, and normalization of
t( fi|eai), although we find in practice that attention to these de-
tails can be beneficial.
2The tree distance could easily be adapted to work with
phrase-structure parses or tree-adjoining parses instead of de-
pendency parses.
I1 very2 much3 doubt4 that5
?(I1,very2) = (1,2,0)
?(very2, I1) = (2,1,1)
?(I1,doubt4) = (1,0,0)
?(that5, I1) = (1,1,1)
Figure 2: Example of tree distances in a sentence from
the Romanian-English development set.
if i > k; 0 otherwise} is simply a binary indicator of the
linear relationship of the words within the surface string.
Tree distance is illustrated in Figure 2.
In our tree distortion model, we condition on the tree
distance and the part of speech T (ei?1), giving us Equa-
tion 3.
d(ai|ai?1) = p(ai, |?(eai ,eai?1),T (eai?1)) (3)
Since both the surface distortion and tree distortion
models represent p(ai|ai?1), we can combine them using
linear interpolation as in Equation 4.
d(ai|ai?1) =
?C(eai?1),T (eai?1 )p(ai|?(eai ,eai?1),T (eai?1)) +
(1??C(eai?1),T (eai?1 ))p(ai|ai ?ai?1,C(eai?1))
(4)
The ?C,T parameters can be initialized from a uniform
distribution and trained with the other parameters using
EM. In principle, any number of alternative distortion
models could be combined with this framework.
2.2 Improving Initialization
Our HMM produces reasonable results if we draw our
initial parameter estimates from a uniform distribution.
However, we can do better. We estimate the initial
translation probability t( f j |ei) from the smoothed log-
likelihood ratio LLR(ei, f j)?1 computed over sentence
cooccurrences. Since this method works well, we apply
LLR(ei, f j) in a single reestimation step shown in Equa-
tion 5.
t( f |e) = LLR( f |e)
?2 +n
?e? LLR( f |e?)?2 +n ? |V |
(5)
In reestimation LLR( f |e) is computed from the expected
counts of f and e produced by the EM algorithm. This is
similar to Moore (2004); as in that work, |V | = 100,000,
and ?1, ?2, and n are estimated on development data.
We can also use an improved initial estimate for distor-
tion. Consider a simple distortion model p(ai|ai ?ai?1).
We expect this distribution to have a maximum near
P(ai|0) because we know that words tend to retain their
locality across translation. Rather than wait for this to
occur, we use an initial estimate for the distortion model
given in Equation 6.
84
corpus n ?1 ?2 ? symmetrization n?1 ??11 ??12 ??1
English-Inuktitut 1?4 1.0 1.75 -1.5 ? 5?4 1.0 1.75 -1.5
Romanian-English 5?4 1.5 1.0 -2.5 refined (Och and Ney, 2000b) 5?4 1.5 1.0 -2.5
English-Hindi 1?4 1.5 3.0 -2.5 ? 1?2 1.0 1.0 -1.0
Table 1: Training parameters for the workshop data (see Section 2.2). Parameters n, ?1, ?2, and ? were used in the
initialization of P(f|e) model, while n?1, ??11 , ??12 , and ??1 were used in the initialization of the P(e|f) model.
corpus type HMM limited (Eq. 2) HMM unlimited (Eq. 4) IBM Model 4P R AER P R AER P R AER
English-Inuktitut
P(f|e) .4962 .6894 .4513 ? ? ? .4211 .6519 .5162
P(e|f) .5789 .8635 .3856 ? ? ? .5971 .8089 .3749
? .8916 .6280 .2251 ? ? ? .8682 .5700 .2801
English-Hindi
P(f|e) .5079 .4769 .5081 .5057 .4748 .5102 .5219 .4223 .5332
P(e|f) .5566 .4429 .5067 .5566 .4429 .5067 .5652 .3939 .5358
? .4408 .5649 .5084 .4365 .5614 .5088 .4543 .5401 .5065
Romanian-English
P(f|e) .6876 .6233 .3461 .6876 .6233 .3461 .6828 .5414 .3961
P(e|f) .7168 .6217 .3341 .7155 .6205 .3354 .7520 .5496 .3649
refined .7377 .6169 .3281 .7241 .6215 .3311 .7620 .5134 .3865
Table 2: Results on the workshop data. The systems highlighted in bold are the ones that were used in the shared task.
For each corpus, the last row shown represents the results that were actually submitted. Note that for English-Hindi,
our self-reported results in the unlimited task are slightly lower than the original results submitted for the workshop,
which contained an error.
d(ai|ai?1) =
{
|ai ?ai?1|?/Z,? < 0 if ai 6= ai?1.
1/Z if ai = ai?1.
(6)
We choose Z to normalize the distribution. We must
optimize ? on a development set. This distribution has
a maximum when |ai ? ai?1| ? {?1,0,1}. Although we
could reasonably choose any of these three values as the
maximum for the initial estimate, we found in develop-
ment that the maximum of the surface distortion distribu-
tion varied with C(eai?1), although it was always in the
range [?1,2].
2.3 Does NULL Matter in Asymmetric Alignment?
Och and Ney (2000a) introduce a NULL-alignment ca-
pability to the HMM alignment model. This allows any
word f j to link to a special NULL word ? by conven-
tion denoted e0 ? instead of one of the words eN1 . A link
( f j ,e0) indicates that f j does not correspond to any word
in e. This improved alignment performance in the ab-
sence of symmetrization, presumably because it allows
the model to be conservative when evidence for an align-
ment is lacking.
We hypothesize that NULL alignment is unnecessary
for asymmetric alignment models when we symmetrize
using intersection-based methods (Och and Ney, 2000b).
The intuition is simple: if we don?t permit NULL align-
ments, then we expect to produce a high-recall, low-
precision alignment; the intersection of two such align-
ments should mainly improve precision, resulting in a
high-recall, high-precision alignment. If we allow NULL
alignments, we may be able produce a high-precision,
low-recall asymmetric alignment, but symmetrization by
intersection will not improve recall.
3 Results with the Workshop Data
In our experiments, the dependency parse and parts of
speech are produced by minipar (Lin, 1998). This parser
has been used in a much different alignment model
(Cherry and Lin, 2003). Since we only had parses for
English, we did not use tree distortion in the application
of P(e|f), needed for symmetrization.
The parameter settings that we used in aligning the
workshop data are presented in Table 1. Although our
prior work with English and French indicated that in-
tersection was the best method for symmetrization, we
found in development that this varied depending on the
characteristics of the corpus and the type of annotation
(in particular, whether the annotation set included proba-
ble alignments). The results are summarized in Table 2.
It shows results with our HMM model using both Equa-
tions 2 and 4 as our distortion model, which represent
85
the unlimited and limited resource tracks, respectively.
It also includes a comparison with IBM Model 4, for
which we use a training sequence of IBM Model 1 (5
iterations), HMM (6 iterations), and IBM Model 4 (5 it-
erations). This sequence performed well in an evaluation
of the IBM Models (Och and Ney, 2003).
For comparative purposes, we show results of apply-
ing both P(f|e) and P(e|f) prior to symmetrization, along
with results of symmetrization. Comparison of the asym-
metric and symmetric results largely supports the hypoth-
esis presented in Section 2.3, as our system generally pro-
duces much better recall than IBM Model 4, while of-
fering a competitive precision. Our symmetrized results
usually produced higher recall and precision, and lower
alignment error rate.
We found that the largest gain in performance came
from the improved initialization. The combined distor-
tion model (Equation 4), which provided a small benefit
over the surface distortion model (Equation 2) on the de-
velopment set, performed slightly worse on the test set.
We found that the dependencies on C(eai?1) and
T (eai?1) were harmful to the P(f|e) alignment for Inukti-
tut, and did not submit results for the unlimited resources
configuration. However, we found that alignment was
generally difficult for all models on this particular task,
perhaps due to the agglutinative nature of Inuktitut.
4 Conclusions
We have proposed improvements to the largely over-
looked HMM word alignment model. Our improvements
yield good results on the workshop data. We have addi-
tionally shown that syntactic information can be incorpo-
rated into such a model; although the results are not su-
perior, they are competitive with surface distortion. In fu-
ture work we expect to explore additional parameteriza-
tions of the HMM model, and to perform extrinsic evalu-
ations of the resulting alignments by using them in the pa-
rameter estimation of a phrase-based translation model.
Acknowledgements
This research was supported in part by ONR MURI Con-
tract FCPO.810548265. The authors would like to thank
Bill Byrne, David Chiang, Okan Kolak, and the anony-
mous reviewers for their helpful comments.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz Josef Och,
David Purdy, Noah A. Smith, and David Yarowsky.
1999. Statistical machine translation: Final report. In
Johns Hopkins University 1999 Summer Workshop on
Language Engineering.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311, Jun.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In ACL Proceed-
ings, Jul.
Herve Dejean, Eric Gaussier, Cyril Goutte, and Kenji
Yamada. 2003. Reducing parameter space for word
alignment. In Proceedings of the Workshop on Build-
ing and Using Parallel Texts: Data Driven Machine
Translation and Beyond, pages 23?26, May.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on the Eval-
uation of Parsing Systems, May.
Rada Mihalcea and Ted Pedersen. 2003. An evaluation
exercise for word alignment. In Proceedings of the
Workshop on Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond, pages 1?10,
May.
Robert C. Moore. 2004. Improving IBM word-
alignment model 1. In ACL Proceedings, pages 519?
526, Jul.
Franz Josef Och and Hermann Ney. 2000a. A compari-
son of alignment models for statistical machine trans-
lation. In COLING Proceedings, pages 1086?1090,
Jul.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In ACL Proceedings,
pages 440?447, Oct.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison on various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL Proceedings,
pages 71?76, Jun.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In EMNLP, pages 87?94, Jul.
Stephan Vogel, Hermann Ney, and Christoph Tillman.
1996. Hmm-based word alignment in statistical ma-
chine translation. In COLING Proceedings, pages
836?841, Aug.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 1328?1335, Aug.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In ACL Proceedings.
86
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333?343,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training a Log-Linear Parser with Loss Functions via Softmax-Margin
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
Log-linear parsing models are often trained
by optimizing likelihood, but we would prefer
to optimise for a task-specific metric like F-
measure. Softmax-margin is a convex objec-
tive for such models that minimises a bound
on expected risk for a given loss function, but
its na??ve application requires the loss to de-
compose over the predicted structure, which
is not true of F-measure. We use softmax-
margin to optimise a log-linear CCG parser for
a variety of loss functions, and demonstrate
a novel dynamic programming algorithm that
enables us to use it with F-measure, lead-
ing to substantial gains in accuracy on CCG-
Bank. When we embed our loss-trained parser
into a larger model that includes supertagging
features incorporated via belief propagation,
we obtain further improvements and achieve
a labelled/unlabelled dependency F-measure
of 89.3%/94.0% on gold part-of-speech tags,
and 87.2%/92.8% on automatic part-of-speech
tags, the best reported results for this task.
1 Introduction
Parsing models based on Conditional Random
Fields (CRFs; Lafferty et al, 2001) have been very
successful (Clark and Curran, 2007; Finkel et al,
2008). In practice, they are usually trained by max-
imising the conditional log-likelihood (CLL) of the
training data. However, it is widely appreciated that
optimizing for task-specific metrics often leads to
better performance on those tasks (Goodman, 1996;
Och, 2003).
An especially attractive means of accomplishing
this for CRFs is the softmax-margin (SMM) ob-
jective (Sha and Saul, 2006; Povey and Woodland,
2008; Gimpel and Smith, 2010a) (?2). In addition to
retaining a probabilistic interpretation and optimiz-
ing towards a loss function, it is also convex, mak-
ing it straightforward to optimise. Gimpel and Smith
(2010a) show that it can be easily implemented with
a simple change to standard likelihood-based train-
ing, provided that the loss function decomposes over
the predicted structure.
Unfortunately, the widely-used F-measure met-
ric does not decompose over parses. To solve this,
we introduce a novel dynamic programming algo-
rithm that enables us to compute the exact quanti-
ties needed under the softmax-margin objective us-
ing F-measure as a loss (?3). We experiment with
this and several other metrics, including precision,
recall, and decomposable approximations thereof.
Our ability to optimise towards exact metrics en-
ables us to verify the effectiveness of more effi-
cient approximations. We test the training proce-
dures on the state-of-the-art Combinatory Categorial
Grammar (CCG; Steedman 2000) parser of Clark
and Curran (2007), obtaining substantial improve-
ments under a variety of conditions. We then embed
this model into a more accurate model that incor-
porates additional supertagging features via loopy
belief propagation. The improvements are additive,
obtaining the best reported results on this task (?4).
2 Softmax-Margin Training
The softmax-margin objective modifies the standard
likelihood objective for CRF training by reweighting
333
each possible outcome of a training input according
to its risk, which is simply the loss incurred on a par-
ticular example. This is done by incorporating the
loss function directly into the linear scoring function
of an individual example.
Formally, we are given m training pairs
(x(1), y(1))...(x(m), y(m)), where each x(i) ? X is
drawn from the set of possible inputs, and each
y(i) ? Y(x(i)) is drawn from a set of possible
instance-specific outputs. We want to learn the K
parameters ? of a log-linear model, where each ?k ?
? is the weight of an associated feature hk(x, y).
Function f(x, y) maps input/output pairs to the vec-
tor h1(x, y)...hK(x, y), and our log-linear model as-
signs probabilities in the usual way.
p(y|x) = exp{?
Tf(x, y)}?
y??Y(x) exp{?Tf(x, y?)}
(1)
The conditional log-likelihood objective function is
given by Eq. 2 (Figure 1). Now consider a function
`(y, y?) that returns the loss incurred by choosing to
output y? when the correct output is y. The softmax-
margin objective simply modifies the unnormalised,
unexponentiated score ?Tf(x, y?) by adding `(y, y?)
to it. This yields the objective function (Eq. 3) and
gradient computation (Eq. 4) shown in Figure 1.
This straightforward extension has several desir-
able properties. In addition to having a probabilis-
tic interpretation, it is related to maximum margin
and minimum-risk frameworks, it can be shown to
minimise a bound on expected risk, and it is convex
(Gimpel and Smith, 2010b).
We can also see from Eq. 4 that the only differ-
ence from standard CLL training is that we must
compute feature expectations with respect to the
cost-augmented scoring function. As Gimpel and
Smith (2010a) discuss, if the loss function decom-
poses over the predicted structure, we can treat its
decomposed elements as unweighted features that
fire on the corresponding structures, and compute
expectations in the normal way. In the case of
our parser, where we compute expectations using
the inside-outside algorithm, a loss function decom-
poses if it decomposes over spans or productions of
a CKY chart.
3 Loss Functions for Parsing
Ideally, we would like to optimise our parser towards
a task-based evaluation. Our CCG parser is evalu-
ated on labeled, directed dependency recovery us-
ing F-measure (Clark and Hockenmaier, 2002). Un-
der this evaluation we will represent output y? and
ground truth y as variable-sized sets of dependen-
cies. We can then compute precision P (y, y?), recall
R(y, y?), and F-measure F1(y, y?).
P (y, y?) = |y ? y
?|
|y?| (5)
R(y, y?) = |y ? y
?|
|y| (6)
F1(y, y?) =
2PR
P +R =
2|y ? y?|
|y|+ |y?| (7)
These metrics are positively correlated with perfor-
mance ? they are gain functions. To incorporate
them in the softmax-margin framework we reformu-
late them as loss functions by subtracting from one.
3.1 Computing F-Measure-Augmented
Expectations at the Sentence Level
Unfortunately, none of these metrics decompose
over parses. However, the individual statistics that
are used to compute them do decompose, a fact we
will exploit to devise an algorithm that computes the
necessary expectations. Note that since y is fixed,
F1 is a function of two integers: |y ? y?|, represent-
ing the number of correct dependencies in y?; and
|y?|, representing the total number of dependencies
in y?, which we will denote as n and d, respectively.1
Each pair ?n, d? leads to a different value of F1. Im-
portantly, both n and d decompose over parses.
The key idea will be to treat F1 as a non-local fea-
ture of the parse, dependent on values n and d.2 To
compute expectations we split each span in an oth-
erwise usual inside-outside computation by all pairs
?n, d? incident at that span.
Formally, our goal will be to compute expecta-
tions over the sentence a1...aL. In order to abstract
away from the particulars of CCG we present the al-
gorithm in relatively familiar terms as a variant of
1For numerator and denominator.
2This is essentially the same trick used in the oracle F-measure
algorithm of Huang (2008), and indeed our algorithm is a sum-
product variant of that max-product algorithm.
334
min
?
m?
i=1
?
???Tf(x(i), y(i)) + log
?
y?Y(x(i))
exp{?Tf(x(i), y)}
?
? (2)
min
?
m?
i=1
?
???Tf(x(i), y(i)) + log
?
y?Y(x(i))
exp{?Tf(x(i), y) + `(y(i), y)}
?
? (3)
?
??k
=
m?
i=1
?
??hk(x(i), y(i)) +
?
y?Y(x(i))
exp{?Tf(x(i), y) + `(y(i), y)}?
y??Y(x(i)) exp{?Tf(x(i), y?) + `(y(i), y?)}
hk(x(i), y)
?
? (4)
Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4).
the classic inside-outside algorithm (Baker, 1979).
We use the notation a : A for lexical entries and
BC ? A to indicate that categories B and C com-
bine to form category A via forward or backward
composition or application.3 The weight of a rule
is denoted with w. The classic algorithm associates
inside score I(Ai,j) and outside score O(Ai,j) with
category A spanning sentence positions i through j,
computed via the following recursions.
I(Ai,i+1) =w(ai+1 : A)
I(Ai,j) =
?
k,B,C
I(Bi,k)I(Ck,j)w(BC ? A)
I(GOAL) =I(S0,L)
O(GOAL) =1
O(Ai,j) =
?
k,B,C
O(Ci,k)I(Bj,k)w(AB ? C)+
?
k,B,C
O(Ck,j)I(Bk,i)w(BA? C)
The expectation of A spanning positions i through j
is then I(Ai,j)O(Ai,j)/I(GOAL).
Our algorithm extends these computations to
state-split itemsAi,j,n,d.4 Using functions n+(?) and
d+(?) to respectively represent the number of cor-
rect and total dependencies introduced by a parsing
action, we present our algorithm in Fig. 3. The fi-
nal inside equation and initial outside equation in-
corporate the loss function for all derivations hav-
ing a particular F-score, enabling us to obtain the
3These correspond respectively to unary rules A ? a and bi-
nary rules A ? BC in a Chomsky normal form grammar.
4Here we use state-splitting to refer to splitting an item Ai,j into
many items Ai,j,n,d, one for each ?n, d? pair.
desired expectations. A simple modification of the
goal equations enables us to optimise precision, re-
call or a weighted F-measure.
To analyze the complexity of this algorithm, we
must ask: how many pairs ?n, d? can be incident at
each span? A CCG parser does not necessarily re-
turn one dependency per word (see Figure 2 for an
example), so d is not necessarily equal to the sen-
tence length L as it might be in many dependency
parsers, though it is still bounded by O(L). How-
ever, this behavior is sufficiently uncommon that we
expect all parses of a sentence, good or bad, to have
close to L dependencies, and hence we expect the
range of d to be constant on average. Furthermore,
n will be bounded from below by zero and from
above by min(|y|, |y?|). Hence the set of all possi-
ble F-measures for all possible parses is bounded by
O(L2), but on average it should be closer to O(L).
Following McAllester (1999), we can see from in-
spection of the free variables in Fig. 3 that the algo-
rithm requires worst-case O(L7) and average-case
O(L5) time complexity, and worse-case O(L4) and
average-case O(L3) space complexity.
Note finally that while this algorithm computes
exact sentence-level expectations, it is approximate
at the corpus level, since F-measure does not decom-
pose over sentences. We give the extension to exact
corpus-level expectations in Appendix A.
3.2 Approximate Loss Functions
We will also consider approximate but more effi-
cient alternatives to our exact algorithms. The idea
is to use cost functions which only utilise statistics
335
I(Ai,i+1,n,d) = w(ai+1 : A) iffn = n+(ai+1 : A), d = d+(ai+1 : A)
I(Ai,j,n,d) =
?
k,B,C
?
{n?,n??:n?+n??+n+(BC?A)=n},
{d?,d??:d?+d??+d+(BC?A)=d}
I(Bi,k,n?,d?)I(Ck,j,n??,d??)w(BC ? A)
I(GOAL) =
?
n,d
I(S0,L,n,d)
(
1? 2nd+ |y|
)
O(S0,N,n,d) =
(
1? 2nd+ |y|
)
O(Ai,j,n,d) =
?
k,B,C
?
{n?,n??:n??n???n+(AB?C)=n},
{d?,d??:d??d???d+(AB?C)=d}
O(Ci,k,n?,d?)I(Bj,k,n??,d??)w(AB ? C)+
?
k,B,C
?
{n?,n??:n??n???n+(BA?C)=n},
{d?,d??:d??d???d+(BA?C)=d}
O(Ck,j,n?,d?)I(Bk,i,n??,d??)w(BA? C)
Figure 3: State-split inside and outside recursions for computing softmax-margin with F-measure.
Figure 2: Example of flexible dependency realisation in
CCG: Our parser (Clark and Curran, 2007) creates de-
pendencies arising from coordination once all conjuncts
are found and treats ?and? as the syntactic head of coor-
dinations. The coordination rule (?) does not yet estab-
lish the dependency ?and - pears? (dotted line); it is the
backward application (<) in the larger span, ?apples and
pears?, that establishes it, together with ?and - pears?.
CCG also deals with unbounded dependencies which po-
tentially lead to more dependencies than words (Steed-
man, 2000); in this example a unification mechanism cre-
ates the dependencies ?likes - apples? and ?likes - pears?
in the forward application (>). For further examples and
a more detailed explanation of the mechanism as used in
the C&C parser refer to Clark et al (2002).
available within the current local structure, similar to
those used by Taskar et al (2004) for tracking con-
stituent errors in a context-free parser. We design
three simple losses to approximate precision, recall
and F-measure on CCG dependency structures.
Let T (y) be the set of parsing actions required
to build parse y. Our decomposable approximation
to precision simply counts the number of incorrect
dependencies using the local dependency counts,
n+(?) and d+(?).
DecP (y) =
?
t?T (y)
d+(t)? n+(t) (8)
To compute our approximation to recall we require
the number of gold dependencies, c+(?), which
should have been introduced by a particular parsing
action. A gold dependency is due to be recovered
by a parsing action if its head lies within one child
span and its dependent within the other. This yields a
decomposed approximation to recall that counts the
number of missed dependencies.
DecR(y) =
?
t?T (y)
c+(t)? n+(t) (9)
336
Unfortunately, the flexible handling of dependencies
in CCG complicates our formulation of c+, render-
ing it slightly more approximate. The unification
mechanism of CCG sometimes causes dependencies
to be realised later in the derivation, at a point when
both the head and the dependent are in the same
span, violating the assumption used to compute c+
(see again Figure 2). Exceptions like this can cause
mismatches between n+ and c+. We set c+ = n+
whenever c+ < n+ to account for these occasional
discrepancies.
Finally, we obtain a decomposable approximation
to F-measure.
DecF1(y) = DecP (y) +DecR(y) (10)
4 Experiments
Parsing Strategy. CCG parsers use a pipeline strat-
egy: we first multitag each word of the sentence with
a small subset of its possible lexical categories us-
ing a supertagger, a sequence model over these cat-
egories (Bangalore and Joshi, 1999; Clark, 2002).
Then we parse the sentence under the requirement
that the lexical categories are fixed to those preferred
by the supertagger. In our experiments we used two
variants on this strategy.
First is the adaptive supertagging (AST) approach
of Clark and Curran (2004). It is based on a step
function over supertagger beam widths, relaxing the
pruning threshold for lexical categories only if the
parser fails to find an analysis. The process either
succeeds and returns a parse after some iteration or
gives up after a predefined number of iterations. As
Clark and Curran (2004) show, most sentences can
be parsed with very tight beams.
Reverse adaptive supertagging is a much less ag-
gressive method that seeks only to make sentences
parsable when they otherwise would not be due to an
impractically large search space. Reverse AST starts
with a wide beam, narrowing it at each iteration only
if a maximum chart size is exceeded. Table 1 shows
beam settings for both strategies.
Adaptive supertagging aims for speed via pruning
while the reverse strategy aims for accuracy by ex-
posing the parser to a larger search space. Although
Clark and Curran (2007) found no actual improve-
ments from the latter strategy, we will show that
with our softmax-margin-trained models it can have
a substantial effect.
Parser. We use the C&C parser (Clark and Cur-
ran, 2007) and its supertagger (Clark, 2002). Our
baseline is the hybrid model of Clark and Curran
(2007), which contains features over both normal-
form derivations and CCG dependencies. The parser
relies solely on the supertagger for pruning, using
exact CKY for search over the pruned space. Train-
ing requires calculation of feature expectations over
packed charts of derivations. For training, we lim-
ited the number of items in this chart to 0.3 million,
and for testing, 1 million. We also used a more per-
missive training supertagger beam (Table 2) than in
previous work (Clark and Curran, 2007). Models
were trained with the parser?s L-BFGS trainer.
Evaluation. We evaluated on CCGbank (Hocken-
maier and Steedman, 2007), a right-most normal-
form CCG version of the Penn Treebank. We use
sections 02-21 (39603 sentences) for training, sec-
tion 00 (1913 sentences) for development and sec-
tion 23 (2407 sentences) for testing. We supply
gold-standard part-of-speech tags to the parsers. We
evaluate on labelled and unlabelled predicate argu-
ment structure recovery and supertag accuracy.
4.1 Training with Maximum F-measure Parses
So far we discussed how to optimise towards task-
specific metrics via changing the training objective.
In our first experiment we change the data on which
we optimise CLL. This is a kind of simple base-
line to our later experiments, attempting to achieve
the same effect by simpler means. Specifically, we
use the algorithm of Huang (2008) to generate or-
acle F-measure parses for each sentence. Updating
towards these oracle parses corrects the reachabil-
ity problem in standard CLL training. Since the su-
pertagger is used to prune the training forests, the
correct parse is sometimes pruned away ? reducing
data utilisation to 91%. Clark and Curran (2007)
correct for this by adding the gold tags to the parser
input. While this increases data utilisation, it bi-
ases the model by training in an idealised setting not
available at test time. Using oracle parses corrects
this bias while permitting 99% data utilisation. The
labelled F-score of the oracle parses lies at 98.1%.
Though we expected that this might result in some
improvement, results (Table 3) show that this has no
337
Condition Parameter Iteration 1 2 3 4 5
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
Reverse
? 0.001 0.005 0.01 0.03 0.075
k 150 20 20 20 20
Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.
Parameter ? is a beam threshold while k bounds the number of lexical categories considered for each word.
Condition Parameter Iteration 1 2 3 4 5 6 7
Training ? 0.001 0.001 0.0045 0.0055 0.01 0.05 0.1k 150 20 20 20 20 20 20
C&C ?07 ? 0.0045 0.0055 0.01 0.05 0.1k 20 20 20 20 20
Table 2: Beam step functions used for training: The first row shows the large scale settings used for most experiments
and the standard C&C settings. (cf. Table 1)
LF LP LR UF UP UR Data Util (%)
Baseline 87.40 87.85 86.95 93.11 93.59 92.63 91%
Max-F Parses 87.46 87.95 86.98 93.09 93.61 92.57 99%
CCGbank+Max-F 87.45 87.96 86.94 93.09 93.63 92.55 99%
Table 3: Performance on section 00 of CCGbank when comparing models trained with treebank-parses (Baseline)
and maximum F-score parses (Max-F) using adaptive supertagging as well as a combination of CCGbank and Max-F
parses. Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
effect. However, it does serve as a useful baseline.
4.2 Training with the Exact Algorithm
We first tested our assumptions about the feasibil-
ity of training with our exact algorithm by measur-
ing the amount of state-splitting. Figure 4 plots the
average number of splits per span against the rela-
tive span-frequency; this is based on a typical set of
training forests containing over 600 million states.
The number of splits increases exponentially with
span size but equally so decreases the number of
spans with many splits. Hence the small number of
states with a high number of splits is balanced by a
large number of spans with only a few splits: The
highest number of splits per span observed with our
settings was 4888 but we find that the average num-
ber of splits lies at 44. Encouragingly, this enables
experimentation in all but very large scale settings.
Figure 5 shows the distribution of n and d pairs
across all split-states in the training corpus; since
0%	 ?
2%	 ?
4%	 ?
6%	 ?
8%	 ?
10%	 ?
12%	 ?
1	 ?
10	 ?
100	 ?
1000	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ?
%	 ?o
f	 ?to
tal
	 ?sp
ans
	 ?
Av
era
ge	 ?
nu
mb
er	 ?
of	 ?
spl
its	 ?
span	 ?length	 ?
Average	 ?number	 ?of	 ?splits	 ?
Percentage	 ?of	 ?total	 ?spans	 ?
Figure 4: Average number of state-splits per span length
as introduced by a sentence-level F-measure loss func-
tion. The statistics are averaged over the training forests
generated using the settings described in ?4.
n, the number of correct dependencies, over d, the
number of all recovered dependencies, is precision,
the graph shows that only a minority of states have
either very high or very low precision. The range
of values suggests that the softmax-margin criterion
338
will have an opportunity to substantially modify the
expectations, hopefully to good effect.
!"
#!"
$!"
%!"
&!"
'!"
!" #!" $!" %!" &!" '!" (!" )!"
!"#
$%&
'()'
*(&
&%*+
',%-
%,%
!*.%
/'0!
1'
!"#$%&'()'233',%-%!,%!*.%/'0,1'
!*'!!!!!!" '!!!!!!*#!!!!!!!" #!!!!!!!*#'!!!!!!" #'!!!!!!*$!!!!!!!"
Figure 5: Distribution of states with d dependencies of
which n are correct in the training forests.
We next turn to the question of optimization with
these algorithms. Due to the significant computa-
tional requirements, we used the computationally
less intensive normal-form model of Clark and Cur-
ran (2007) as well as their more restrictive training
beam settings (Table 2). We train on all sentences of
the training set as above and test with AST.
In order to provide greater control over the influ-
ence of the loss function, we introduce a multiplier
? , which simply amends the second term of the ob-
jective function (3) to:
log
?
y?Y (xi)
exp{?T f(xi, y) + ? ? `(yi, y)}
Figure 6 plots performance of the exact loss func-
tions across different settings of ? on various evalu-
ation criteria, for models restricted to at most 3000
items per chart at training time to allow rapid ex-
perimentation with a wide parameter set. Even in
this constrained setting, it is encouraging to see that
each loss function performs best on the criteria it op-
timises. The precision-trained parser also does very
well on F-measure; this is because the parser has a
tendency to perform better in terms of precision than
recall.
4.3 Exact vs. Approximate Loss Functions
With these results in mind, we conducted a compar-
ison of parsers trained using our exact and approxi-
mate loss functions. Table 4 compares their perfor-
mance head to head when restricting training chart
sizes to 100,000 items per sentence, the largest set-
ting our computing resources allowed us to experi-
ment with. The results confirm that the loss-trained
models improve over a likelihood-trained baseline,
and furthermore that the exact loss functions seem
to have the best performance. However, the approx-
imations are extremely competitive with their exact
counterparts. Because they are also efficient, this
makes them attractive for larger-scale experiments.
Training time increases by an order of magnitude
with exact loss functions despite increased theoreti-
cal complexity (?3.1); there is no significant change
with approximate loss functions.
Table 5 shows performance of the approximate
losses with the large scale settings initially outlined
(?4). One striking result is that the softmax-margin
trained models coax more accurate parses from the
larger search space, in contrast to the likelihood-
trained models. Our best loss model improves the
labelled F-measure by over 0.8%.
4.4 Combination with Integrated Parsing and
Supertagging
As a final experiment, we embed our loss-trained
model into an integrated model that incorporates
Markov features over supertags into the parsing
model (Auli and Lopez, 2011). These features have
serious implications on search: even allowing for the
observation of Fowler and Penn (2010) that our CCG
is weakly context-free, the search problem is equiva-
lent to finding the optimal derivation in the weighted
intersection of a regular and context-free language
(Bar-Hillel et al, 1964), making search very expen-
sive. Therefore parsing with this model requires ap-
proximations.
To experiment with this combined model we use
loopy belief propagation (LBP; Pearl et al, 1985),
previously applied to dependency parsing by Smith
and Eisner (2008). A more detailed account of its
application to our combined model can be found in
(2011), but we sketch the idea here. We construct a
graphical model with two factors: one is a distribu-
339
!"# !$#
!%# !&#
'()
'(*+)
'(*,)
'(*-)
'(*.)
'(*/)
,) -) .) /) () +0)
!"#
$%%$
&'(
)*
$"+
,-$
'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
'/*<)
'(*+)
'(*-)
'(*/)
'(*=)
'(*<)
'=*+)
,) -) .) /) () +0)
!"#
$%%$
&'/
-$0
1+12
3'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
'/*/)
'/*=)
'/*<)
'(*+)
'(*-)
'(*/)
,) -) .) /) () +0)
!"#
$%%$
&'4
$0"
%%'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
<-*=)
<-*=/)
<-*')
<-*'/)
<-*<)
<-*</)
,) -) .) /) () +0)
5,6
$-7
"88
138
'90
0,-
"0:
'
.",'
1"234563) 7+)4822)
9:3%52586)4822) ;3%"44)4822)
Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelled
F-measure, (b) precision, (c) recall and (d) supertag accuracy across various settings of ? on the development set.
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
CLL 86.76 87.16 86.36 92.73 93.16 92.30 87.46 87.80 87.12 92.85 93.22 92.49
DecP 87.18 87.93 86.44 92.93 93.73 92.14 87.75 88.34 87.17 93.04 93.66 92.43
DecR 87.31 87.55 87.07 93.00 93.26 92.75 87.57 87.71 87.42 92.92 93.07 92.76
DecF1 87.27 87.78 86.77 93.04 93.58 92.50 87.69 88.10 87.28 93.04 93.48 92.61
P 87.25 87.85 86.66 92.99 93.63 92.36 87.76 88.23 87.30 93.06 93.55 92.57
R 87.34 87.51 87.16 92.98 93.17 92.80 87.57 87.62 87.51 92.92 92.98 92.86
F1 87.34 87.74 86.94 93.05 93.47 92.62 87.71 88.01 87.41 93.02 93.34 92.70
Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposable
precision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).
Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).
340
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
DecP 87.35 92.99 94.25 87.75 93.25 94.22 88.10 93.26 94.51 88.51 93.50 94.39
DecR 87.48 93.00 94.34 87.70 93.16 94.30 87.66 92.83 94.38 87.77 92.91 94.22
DecF1 87.67 93.23 94.39 88.12 93.52 94.46 88.09 93.28 94.50 88.58 93.57 94.53
Table 5: Performance of decomposed loss functions in large-scale training setting. Evaluation is based on labelled and
unlabelled F-measure (LF/UF) and supertag accuracy (ST).
tion over supertag variables defined by a supertag-
ging model, and the other is a distribution over these
variables and a set of span variables defined by our
parsing model.5 The factors communicate by pass-
ing messages across the shared supertag variables
that correspond to their marginal distributions over
those variables. Hence, to compute approximate ex-
pectations across the entire model, we run forward-
backward to obtain posterior supertag assignments.
These marginals are passed as inside values to the
inside-outside algorithm, which returns a new set
of posteriors. The new posteriors are incorporated
into a new iteration of forward-backward, and the
algorithm iterates until convergence, or until a fixed
number of iterations is reached ? we found that a
single iteration is sufficient, corresponding to a trun-
cated version of the algorithm in which posteriors
are simply passed from the supertagger to the parser.
To decode, we use the posteriors in a minimum-risk
parsing algorithm (Goodman, 1996).
Our baseline models are trained separately as be-
fore and combined at test time. For softmax-margin,
we combine a parsing model trained with F1 and
a supertagger trained with Hamming loss. Table 6
shows the results: we observe a gain of up to 1.5%
in labelled F1 and 0.9% in unlabelled F1 on the test
set. The loss functions prove their robustness by im-
proving the more accurate combined models up to
0.4% in labelled F1. Table 7 shows results with au-
tomatic part-of-speech tags and a direct comparison
with the Petrov parser trained on CCGbank (Fowler
and Penn, 2010) which we outpeform on all metrics.
5These complex factors resemble those of Smith and Eisner
(2008) and Dreyer and Eisner (2009); they can be thought of
as case-factor diagrams (McAllester et al, 2008)
5 Conclusion and Future Work
The softmax-margin criterion is a simple and effec-
tive approach to training log-linear parsers. We have
shown that it is possible to compute exact sentence-
level losses under standard parsing metrics, not only
approximations (Taskar et al, 2004). This enables
us to show the effectiveness of these approxima-
tions, and it turns out that they are excellent sub-
stitutes for exact loss functions. Indeed, the approxi-
mate losses are as easy to use as standard conditional
log-likelihood.
Empirically, softmax-margin training improves
parsing performance across the board, beating the
state-of-the-art CCG parsing model of Clark and
Curran (2007) by up to 0.8% labelled F-measure.
It also proves robust, improving a stronger base-
line based on a combined parsing and supertagging
model. Our final result of 89.3%/94.0% labelled
and unlabelled F-measure is the best result reported
for CCG parsing accuracy, beating the original C&C
baseline by up to 1.5%.
In future work we plan to scale our exact loss
functions to larger settings and to explore training
with loss functions within loopy belief propagation.
Although we have focused on CCG parsing in this
work, we expect our methods to be equally appli-
cable to parsing with other grammar formalisms in-
cluding context-free grammar or LTAG.
Acknowledgements
We would like to thank Stephen Clark, Chris-
tos Christodoulopoulos, Mark Granroth-Wilding,
Gholamreza Haffari, Alexandre Klementiev, Tom
Kwiatkowski, Kira Mourao, Matt Post, and Mark
Steedman for helpful discussion related to this
work and comments on previous drafts, and the
341
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
CLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
BP 87.67 93.26 94.43 88.35 93.72 94.73 88.25 93.33 94.60 88.86 93.75 94.84
+DecF1 87.90 93.40 94.52 88.58 93.88 94.79 88.32 93.32 94.66 89.15 93.89 94.98
+SA 87.73 93.28 94.49 88.40 93.71 94.75 88.47 93.48 94.71 89.25 93.98 95.01
Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 as
parser-loss function and supertag-accuracy (SA) as loss in the supertagger.
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
CLL 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75
Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.73 92.34 92.64 92.04
BP 86.45 86.75 86.17 92.60 92.92 92.29 86.84 87.08 86.61 92.57 92.82 92.32
+DecF1 86.73 87.07 86.39 92.79 93.16 92.43 87.08 87.37 86.78 92.68 93.00 92.37
+SA 86.51 86.86 86.16 92.60 92.98 92.23 87.20 87.50 86.90 92.76 93.08 92.44
Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
(2010); evaluation is based on sentences for which all parsers returned an analysis.
anonymous reviewers for helpful comments. We
also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); and the resources provided by
the Edinburgh Compute and Data Facility.
A Computing F-Measure-Augmented
Expectations at the Corpus Level
To compute exact corpus-level expectations for softmax-
margin using F-measure, we add an additional transition
before reaching the GOAL item in our original program.
To reach it, we must parse every sentence in the corpus,
associating statistics of aggregate ?n, d? pairs for the en-
tire training set in intermediate symbols ?(1)...?(m) with
the following inside recursions.
I(?(1)n,d) = I(S
(1)
0,|x(1)|,n,d)
I(?(`)n,d) =
?
n?,n??:n?+n??=n
I(?(`?1)n?,d? )I(S
(`)
0,N,n??,d??)
I(GOAL) =
?
n,d
I(?(m)n,d )
(
1? 2nd+ |y|
)
Outside recursions follow straightforwardly. Implemen-
tation of this algorithm would require substantial dis-
tributed computation or external data structures, so we
did not attempt it.
References
M. Auli and A. Lopez. 2011. A Comparison of Loopy
Belief Propagation and Dual Decomposition for Inte-
grated CCG Supertagging and Parsing. In Proc. of
ACL, June.
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238?265, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In COL-
ING, Morristown, NJ, USA.
S. Clark and J. R. Curran. 2007. Wide-Coverage Ef-
ficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
S. Clark and J. Hockenmaier. 2002. Evaluating a Wide-
Coverage CCG Parser. In Proceedings of the LREC
2002 Beyond Parseval Workshop, pages 60?66, Las
Palmas, Spain.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proc. of ACL.
342
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In TAG+6.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proc. of EMNLP.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Feature-based, conditional random field parsing. In
Proceedings of ACL-HLT.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010a. Softmax-margin
CRFs: training log-linear models with cost functions.
In HLT ?10: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
K. Gimpel and N. A. Smith. 2010b. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity.
J. Goodman. 1996. Parsing algorithms and metrics. In
Proc. of ACL, pages 177?183, Jun.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
L. Huang. 2008. Forest Reranking: Discriminative pars-
ing with Non-Local Features. In Proceedings of ACL-
08: HLT.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML,
pages 282?289.
D. McAllester, M. Collins, and F. Pereira. 2008. Case-
factor diagrams for structured probabilistic modeling.
Journal of Computer and System Sciences, 74(1):84?
96.
D. McAllester. 1999. On the complexity analysis of
static analyses. In Proc. of Static Analysis Symposium,
volume 1694/1999 of LNCS. Springer Verlag.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, Jul.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
D. Povey and P. Woodland. 2008. Minimum phone er-
ror and I-smoothing for improved discrimative train-
ing. In Proc. of ICASSP.
F. Sha and L. K. Saul. 2006. Large margin hidden
Markov models for automatic speech recognition. In
Proc. of NIPS.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP,
pages 1?8, Jul.
343
Proceedings of NAACL-HLT 2013, pages 325?334,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction
for Statistical Machine Translation Using GPUs
Hua He
Dept. of Computer Science
University of Maryland
College Park, Maryland
huah@cs.umd.edu
Jimmy Lin
iSchool and UMIACS
University of Maryland
College Park, Maryland
jimmylin@umd.edu
Adam Lopez
HLTCOE
Johns Hopkins University
Baltimore, Maryland
alopez@cs.jhu.edu
Abstract
Translation models in statistical machine
translation can be scaled to large corpora
and arbitrarily-long phrases by looking up
translations of source phrases ?on the fly?
in an indexed parallel corpus using suffix
arrays. However, this can be slow because
on-demand extraction of phrase tables is
computationally expensive. We address this
problem by developing novel algorithms for
general purpose graphics processing units
(GPUs), which enable suffix array queries
for phrase lookup and phrase extraction to
be massively parallelized. Compared to
a highly-optimized, state-of-the-art serial
CPU-based implementation, our techniques
achieve at least an order of magnitude
improvement in terms of throughput. This
work demonstrates the promise of massively
parallel architectures and the potential
of GPUs for tackling computationally-
demanding problems in statistical machine
translation and language processing.
1 Introduction
Efficiently handling large translation models is a
perennial problem in statistical machine translation.
One particularly promising solution (?2) is to use
the parallel text itself as an implicit representation
of the translation model and extract translation units
?on the fly? when they are needed to decode new
input (Brown, 2004). This idea has been applied
to phrase-based (Callison-Burch et al, 2005; Zhang
and Vogel, 2005), hierarchical (Lopez, 2007; Lopez,
2008b; Lopez, 2008a), and syntax-based (Cromieres
and Kurohashi, 2011) models. A benefit of this
technique is that it scales to arbitrarily large models
with very little pre-processing. For instance, Lopez
(2008b) showed that a translation model trained on
a large corpus with sparse word alignments and
loose extraction heuristics substantially improved
Chinese-English translation. An explicit represen-
tation of the model would have required nearly a
terabyte of memory, but its implicit representation
using the parallel text required only a few gigabytes.
Unfortunately, there is substantial computational
cost in searching a parallel corpus for source
phrases, extracting their translations, and scoring
them on the fly. Since the number of possible
translation units may be quite large (for example,
all substrings of a source sentence) and their
translations are numerous, both phrase lookup and
extraction are performance bottlenecks. Despite
considerable research and the use of efficient
indexes like suffix arrays (Manber and Myers,
1990), this problem remains not fully solved.
We show how to exploit the massive parallelism
offered by modern general purpose graphics pro-
cessing units (GPUs) to eliminate the computational
bottlenecks associated with ?on the fly? phrase ex-
traction. GPUs have previously been applied to
DNA sequence matching using suffix trees (Schatz
et al, 2007) and suffix arrays (Gharaibeh and Ri-
peanu, 2010). Building on this work, we present
two novel contributions: First, we describe improved
GPU algorithms for suffix array queries that achieve
greater parallelism (?3). Second, we propose novel
data structures and algorithms for phrase extraction
(?4) and scoring (?5) that are amenable to GPU par-
325
allelization. The resulting implementation achieves
at least an order of magnitude higher throughput
than a state-of-the-art single-threaded CPU imple-
mentation (?6). Since our experiments verify that
the GPU implementation produces exactly the same
results as a CPU reference implementation on a full
extraction, we can simply replace that component
and reap significant performance advantages with no
impact on translation quality. To the best of our
knowledge, this is the first reported application of
GPU acceleration techniques for statistical machine
translation. We believe these results reveal a promis-
ing yet unexplored future direction in exploiting par-
allelism to tackle perennial performance bottlenecks
in state-of-the-art translation models.
2 Phrase Extraction On Demand
Lopez (2008b) provides the following recipe for
?translation by pattern matching?, which we use as
a guide for the remainder of this paper:
Algorithm 1 Translation by pattern matching
1: for each input sentence do
2: for each possible phrase in the sentence do
3: Find its occurrences in the source text
4: for each occurrence do
5: Extract its aligned target phrase (if any)
6: for each extracted phrase pair do
7: Compute feature values
8: Decode as usual using the scored rules
The computational bottleneck occurs in lines 2?7:
there are vast numbers of query phrases, matching
occurrences, and extracted phrase pairs to process in
the loops. In the next three sections, we attack each
problem in turn.
3 Finding Every Phrase
First, we must find all occurrences of each source
phrase in the input (line 3, Algorithm 1). This
is a classic application of string pattern matching:
given a short query pattern, the task is to find all
occurrences in a much larger text. Solving the
problem efficiently is crucial: for an input sentence
F of length |F |, each of its O(|F |2) substrings is a
potential query pattern.
3.1 Pattern Matching with Suffix Arrays
Although there are many algorithms for pattern
matching, all of the examples that we are aware
of for machine translation rely on suffix arrays.
We briefly review the classic algorithms of Manber
and Myers (1990) here since they form the basis
of our techniques and analysis, but readers who
are familiar with them can safely skip ahead to
additional optimizations (?3.2).
A suffix array represents all suffixes of a corpus
in lexicographical order. Formally, for a text T , the
ith suffix of T is the substring of the text beginning
at position i and continuing to the end of T . Each
suffix can therefore be uniquely identified by the
index i of its first word. A suffix array S(T )
of T is a permutation of these suffix identifiers
[1, |T |] arranged by the lexicographical order of the
corresponding suffixes?in other words, the suffix
array represents a sorted list of all suffixes in T .
With both T and S(T ) in memory, we can find any
query pattern Q in O(|Q| log |T |) time by compar-
ing pattern Q against the first |Q| characters of up to
log |T | different suffixes using binary search.
An inefficiency in this solution is that each com-
parison in the binary search algorithm requires com-
paring all |Q| characters of the query pattern against
some suffix of text T . We can improve on this using
an observation about the longest common prefix
(LCP) of the query pattern and the suffix against
which it is compared. Suppose we search for a query
pattern Q in the span of the suffix array beginning at
suffix L and ending at suffix R. For any suffix M
which falls lexicographically between those at L and
R, the LCP of Q and M will be at least as long as
the LCP of Q and L or Q and R. Hence if we know
the quantity h = MIN(LCP(Q,L), LCP(Q,R)) we
can skip comparisons of the first h symbols between
Q and the suffix M , since they must be the same.
The solution of Manber and Myers (1990) ex-
ploits this fact along with the observation that each
comparison in binary search is carried out accord-
ing to a fixed recursion scheme: a query is only
ever compared against a specific suffix M for a
single range of suffixes bounded by some fixed L
and R. Hence if we know the longest common
prefix between M and each of its corresponding
L and R according to the fixed recursions in the
326
algorithm, we can maintain a bound on h and reduce
the aggregate number of symbol comparisons to
O(|Q| + log |T |). To accomplish this, in addition
to the suffix array, we pre-compute two other arrays
of size |T | for both left and right recursions (called
the LCP arrays).
Memory use is an important consideration, since
GPUs have less memory than CPUs. For the algo-
rithms described here, we require four arrays: the
original text T , the suffix array S(T ), and the two
LCP arrays. We use a representation of T in which
each word has been converted to a unique integer
identifier; with 32-bit integers the total number of
bytes is 16|T |. As we will show, this turns out to be
quite modest, even for large parallel corpora (?6).
3.2 Suffix Array Efficiency Tricks
Previous work on translation by pattern matching
using suffix arrays on serial architectures has pro-
duced a number of efficiency optimizations:
1. Binary search bounds for longer substrings are
initialized to the bounds of their longest prefix.
Substrings are queried only if their longest
prefix string was matched in the text.
2. In addition to conditioning on the longest pre-
fix, Zhang and Vogel (2005) and Lopez (2007)
condition on a successful query for the longest
proper suffix.
3. Lopez (2007) queries each unique substring
of a sentence exactly once, regardless of how
many times it appears in an input sentence.
4. Lopez (2007) directly indexes one-word sub-
strings with a small auxiliary array, so that
their positions in the suffix array can be found
in constant time. For longer substrings, this
optimization reduces the log |T | term of query
complexity to log(count(a)), where a is the
first word of the query string.
Although these efficiency tricks are important in the
serial algorithms that serve as our baseline, not all
of them are applicable to parallel architectures. In
particular, optimizations (1), (2), and (3) introduce
order dependencies between queries; they are disre-
garded in our GPU implementation so that we can
fully exploit parallelization opportunities. We have
not yet fully implemented (4), which is orthogonal
to parallelization: this is left for future work.
3.3 Finding Every Phrase on a GPU
Recent work in computational biology has shown
that suffix arrays are particularly amenable to GPU
acceleration: the suffix-array-based DNA sequence
matching system MummurGPU++ (Gharaibeh and
Ripeanu, 2010) has been reported to outperform the
already fast MummurGPU 2 (Trapnell and Schatz,
2009), based on suffix trees (an alternative indexing
structure). Here, we apply the same ideas to ma-
chine translation, introducing some novel improve-
ments to their algorithms in the process.
A natural approach to parallelism is to perform
all substring queries in parallel (Gharaibeh and Ri-
peanu, 2010). There are no dependencies between
iterations of the loop beginning on line 2 of Algo-
rithm 1, so for input sentence F , we can parallelize
by searching for all O(|F |2) substrings concurrently.
We adopt this approach here.
However, na??ve application of query-level paral-
lelism leads to a large number of wasted threads,
since most long substrings of an input sentence will
not be found in the text. Therefore, we employ
a novel two-pass strategy: in the first pass, we
simply compute, for each position i in the input
sentence, the length j of the longest substring in F
that appears in T . These computations are carried
out concurrently for every position i. During this
pass, we also compute the suffix array bounds of the
one-word substring F [i], to be used as input to the
second pass?a variant of optimizations (1) and (4)
discussed in ?3.2. On the second pass, we search
for all substrings F [i, k] for all k ? [i + 1, i + j].
These computations are carried out concurrently for
all substrings longer than one word.
Even more parallelization is possible. As we saw
in ?3.1, each query in a suffix array actually requires
two binary searches: one each for the first and last
match in S(T ). The abundance of inexpensive
threads on a GPU permits us to perform both queries
concurrently on separate threads. By doing this in
both passes we utilize more of the GPU?s processing
power and obtain further speedups.
As a simple example, consider an input sentence
?The government puts more tax on its citizens?, and
suppose that substrings ?The government?, ?gov-
ernment puts?, and ?puts more tax? are found in
the training text, while none of the words in ?on
327
Initial Word Longest Match Substrings Threads
1st pass 2nd pass
The 2 The, The government 2 2
government 2 government, government puts 2 2
puts 3 puts, puts more, puts more tax 2 4
more 2 more, more tax 2 2
tax 1 tax 2 0
on 0 ? 2 0
its 0 ? 2 0
citizens 0 ? 2 0
Total Threads: 16 10
Table 1: Example of how large numbers of suffix array queries can be factored across two highly parallel passes on a
GPU with a total of 26 threads to perform all queries for this sample input sentence.
its citizens? are found. The number of threads
spawned is shown in Table 1: all threads during a
pass execute in parallel, and each thread performs a
binary search which takes no more than O(|Q| +
log |T |) time. While spawning so many threads
may seem wasteful, this degree of parallelization
still under-utilizes the GPU; the hardware we use
(?6) can manage up to 21,504 concurrent threads
in its resident occupancy. To fully take advantage
of the processing power, we process multiple input
sentences in parallel. Compared with previous
algorithms, our two-pass approach and our strategy
of thread assignment to increase the amount of
parallelism represent novel contributions.
4 Extracting Aligned Target Phrases
The problem at line 5 of Algorithm 1 is to extract the
target phrase aligned to each matching source phrase
instance. Efficiency is crucial since some source
phrases occur hundreds of thousands of times.
Phrase extraction from word alignments typically
uses the consistency check of Och et al (1999). A
consistent phrase is one for which no words inside
the phrase pair are aligned to words outside the
phrase pair. Usually, consistent pairs are computed
offline via dynamic programming over the align-
ment grid, from which we extract all consistent
phrase pairs up to a heuristic bound on phrase length.
The online extraction algorithm of Lopez (2008a)
checks for consistent phrases in a different manner.
Rather than finding all consistent phrase pairs in
a sentence, the algorithm asks: given a specific
source phrase, is there a consistent phrase pair
Figure 1: Source phrase f2f3f4 and target phrase
e2e3e4 are extracted as a consistent pair, since the back-
projection is contained within the original source span.
Figure 2: Source phrase f2f3f4 and target phrase e2e3e4
should not be extracted, since the back-projection is not
contained within the original source span.
of which it is one side? To answer this, it first
computes the projection of the source phrase in the
target sentence: the minimum span containing all
words that are aligned to any word of the source
span. It then computes the projection of the target
span back into the source; if this back-projection
is contained within the original source span, the
phrase pair is consistent, and the target span is
extracted as the translation of the source. Figure 1
shows a ?good? pair for source phrase f2f3f4, since
the back-projection is contained within the original
source span, whereas Figure 2 shows a ?bad? pair
for source phrase f2f3f4 since the back-projection
is not contained within the original source span.
328
4.1 Sampling Consistent Phrases
Regardless of how efficient the extraction of a single
target phrase is made, the fact remains that there
are many phrases to extract. For example, in our
Chinese Xinhua dataset (see ?6), from 8,000 input
query sentences, about 20 million source substrings
can be extracted. The standard solution to this
problem is to sample a set of occurrences of each
source phrase, and only extract translations for those
occurrences (Callison-Burch et al, 2005; Zhang and
Vogel, 2005). As a practical matter, this can be done
by sampling at uniform intervals from the matching
span of a suffix array. Lopez (2008a) reports a
sample size of 300; for phrases occurring fewer than
300 times, all translations are extracted.
4.2 GPU Implementation
We present novel data structures and an algorithm
for efficient phrase extraction, which together are
amenable to massive parallelization on GPUs. The
basic insight is to pre-compute data structures for
the source-to-target algnment projection and back-
projection procedure described by Lopez (2008a)
for checking consistent alignments.
Let us consider a single matching substring (from
the output of the suffix array queries), span [i, j] in
the source text T . For each k, we need to know the
leftmost and rightmost positions that it aligns to in
the target T ?. For this purpose we can define the
target span [i?, j?], along with leftmost and rightmost
arrays L and R as follows:
i? := min
k?[i,j]
L(k)
j? := max
k?[i,j]
R(k)
The arrays L and R are each of length |T |, in-
dexed by absolute corpus position. Each array
element contains the leftmost and rightmost extents
of the source-to-target algnments (in the target),
respectively. Note that in order to save space,
the values stored in the arrays are sentence-relative
positions (e.g., token count from the beginning of
each sentence), so that we only need one byte per
array entry. Thus, i? and j? are sentence-relative
positions (in the target).
Similarly, for the back-projection, we use two
arrays L? and R? on the target side (length |T ?|) to
keep track of the leftmost and rightmost positions
that k? in the target training text align to, as below:
i?? := min
k??[s?+i?,s?+j?]
L?(k?)
j?? := max
k??[s?+i?,s?+j?]
R?(k?)
The arrays L? and R? are indexed by absolute corpus
positions, but their contents are sentence relative
positions (on the source side). To index the arrays
L? and R?, we also need to obtain the corresponding
target sentence start position s?. Note that the back-
projected span [i??, j??] may or may not be the same
as the original span [i, j]. In fact, this is exactly what
we must check for to ensure a consistent alignment.
The suffix array gives us i, which is an ab-
solute corpus position, but we need to know the
sentence-relative position, since the spans computed
by R,L,R?, L? are all sentence relative. To solve
this, we introduce an array P (length |T |) that gives
the relative sentence position of each source word.
We then pack the three source side arrays (R, L,
and P ) into a single RLP array of 32-bit integers
(note that we are actually wasting one byte per array
element). Finally, since the end-of-sentence special
token is not used in any of R, L, or P , its position
in RLP can be used to store an index to the start
of the corresponding target sentence in the target
array T ?. Now, given a source phrase spanning
[i, j] (recall, these are absolute corpus positions), our
phrase extraction algorithm is as follows:
Algorithm 2 Efficient Phrase Extraction Algorithm
1: for each source span [i, j] do
2: Compute [i?, j?]
3: s := i? P [i]? 1
4: s? := RLP [s]
5: i?? := mink??[s?+i?,s?+j?] L
?(k?)
6: j?? := maxk??[s?+i?,s?+j?]R
?(k?)
7: If i? s = i?? and j ? s = j?? then
8: Extract T [i, j] with T ?[s? + i?, s? + j?]
where s is the source sentence start position of a
given source phrase and s? is the target sentence
start position. If the back-projected spans match the
original spans, the phrase pair T [i, j] and T ?[s? +
i?, s? + j?] is extracted.
In total, the data structures RLP , R?, and L?
require 4|T | + 2|T ?| bytes. Not only is this phrase
329
extraction algorithm fast?requiring only a few in-
direct array references?the space requirements for
the auxiliary data structures are quite modest.
Given sufficient resources, we would ideally par-
allelize the phrase table creation process for each
occurrence of the matched source substring. How-
ever, the typical number of source substring matches
for an input sentence is even larger than the number
of threads available on GPUs, so this strategy does
not make sense due to context switching overhead.
Instead, GPU thread blocks (groups of 512 threads)
are used to process each source substring. This
means that for substrings with large numbers of
matches, one thread in the GPU block would process
multiple occurrences. This strategy is widely used,
and according to GPU programming best practices
from NVIDIA, allocating more work to a single
thread maintains high GPU utilization and reduces
the cost of context switches.
5 Computing Every Feature
Finally, we arrive at line 7 in Algorithm 3, where
we must compute feature values for each extracted
phrase pair. Following the implementation of gram-
mar extraction used in cdec (Lopez, 2008a), we
compute several widely-used features:
1. Pair count feature, c(e, f).
2. The joint probability of all target-to-source
phrase translation probabilities, p(e|f)
= c(e, f)/c(f), where e is target phrase, f is
the source phrase.
3. The logarithm of the target-to-source lexical
weighting feature.
4. The logarithm of the source-to-target lexical
weighting feature.
5. The coherence probability, defined as the ratio
between the number of successful extractions
of a source phrase to the total count of the
source phrase in the suffix array.
The output of our phrase extraction is a large
collection of phrase pairs. To extract the above fea-
tures, aggregate statistics need to be computed over
phrase pairs. To make the solution both compact
and efficient, we first sort the unordered collection
of phrases from the GPU into an array, then the
aggregate statistics can be obtained in a single pass
over the array, since identical phrase pairs are now
grouped together.
6 Experimental Setup
We tested our GPU-based grammar extraction im-
plementation under the conditions in which it would
be used for a Chinese-to-English machine transla-
tion task, in particular, replicating the data condi-
tions of Lopez (2008b). Experiments were per-
formed on two data sets. First, we used the source
(Chinese) side of news articles collected from the
Xinhua Agency, with around 27 million words of
Chinese in around one million sentences (totaling
137 MB). Second, we added source-side parallel text
from the United Nations, with around 81 million
words of Chinese in around four million sentences
(totaling 561 MB). In a pre-processing phase, we
mapped every word to a unique integer, with two
special integers representing end-of-sentence and
end-of-corpus, respectively.
Input query data consisted of all sentences from
the NIST 2002?2006 translation campaigns, tok-
enized and integerized identically to the training
data. On average, sentences contained around 29
words. In order to fully stress our GPU algorithms,
we ran tests on batches of 2,000, 4,000, 6,000,
8,000, and 16,000 sentences. Since there are only
around 8,000 test sentences in the NIST data, we
simply duplicated the test data as necessary.
Our experiments used NVIDIA?s Tesla C2050
GPU (Fermi Generation), which has 448 CUDA
cores with a peak memory bandwidth 144 GB/s.
Note that the GPU was released in early 2010
and represents previous generation technology.
NVIDIA?s current GPUs (Kepler) boasts raw
processing power in the 1.3 TFlops (double
precision) range, which is approximately three
times the GPU we used. Our CPU is a 3.33 GHz
Intel Xeon X5260 processor, which has two cores.
As a baseline, we compared against the publicly
available implementation of the CPU-based algo-
rithms described by Lopez (2008a) found in the
pycdec (Chahuneau et al, 2012) extension of the
cdec machine translation system (Dyer et al, 2010).
Note that we only tested grammar extraction for
continuous pairs of phrases, and we did not test the
slower and more complex queries for hierarchical
330
Input Sentences 2,000 4,000 6,000 8,000 16,000
Number of Words 57,868 117,854 161,883 214,246 428,492
Xinhua
With Sampling (s300)
GPU (words/second)
3811
(21.9)
4723
(20.4)
5496
(32.1)
6391
(29.7)
12405
(36.0)
CPU (words/second) 200 (1.5)
Speedup 19? 24? 27? 32? 62?
No Sampling (s?)
GPU (words/second)
1917
(8.5)
2859
(11.1)
3496
(19.9)
4171
(23.2)
8186
(27.6)
CPU (words/second) 1.13 (0.02)
Speedup 1690? 2520? 3082? 3677? 7217?
Xinhua + UN
With Sampling (s300)
GPU (words/second)
2021
(5.3)
2558
(10.7)
2933
(13.9)
3439
(15.2)
6737
(29.0)
CPU (words/second) 157 (1.8)
Speedup 13? 16? 19? 22? 43?
No Sampling (s?)
GPU (words/second)
500.5
(2.5)
770.1
(3.9)
984.6
(5.8)
1243.8
(5.4)
2472.3
(12.0)
CPU (words/second) 0.23 (0.002)
Speedup 2194? 3375? 4315? 5451? 10836?
Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput
is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given
in parentheses, along with relative speedups comparing the two implementations.
(gappy) patterns described by Lopez (2007). Both
our implementation and the baseline are written
primarily in C/C++.1
Our source corpora and test data are the same
as that presented in Lopez (2008b), and using the
CPU implementation as a reference enabled us to
confirm that our extracted grammars and features
are identical (modulo sampling). We timed our
GPU implementation as follows: from the loading
of query sentences, extractions of substrings and
grammar rules, until all grammars for all sentences
are generated in memory. Timing does not include
offline preparations such as the construction of the
suffix array on source texts and the I/O costs for
writing the per-sentence grammar files to disk. This
timing procedure is exactly the same for the CPU
1The Chahuneau et al (2012) implementation is in Cython,
a language for building Python applications with performance-
critical components in C. In particular, all of the suffix array
code that we instrumented for these experiments are compiled
to C/C++. The implementation is a port of the original code
written by Lopez (2008a) in Pyrex, a precursor to Cython.
Much of the code is unchanged from the original version.
baseline. We are confident that our results represent
a fair comparison between the GPU and CPU, and
are not attributed to misconfigurations or other flaws
in experimental procedures. Note that the CPU
implementation runs in a single thread, on the same
machine that hosts the GPU (described above).
7 Results
Table 2 shows performance results comparing our
GPU implementation against the reference CPU
implementation for phrase extraction. In one ex-
perimental condition, the sampling parameter for
frequently-matching phrases is set to 300, per Lopez
(2008a), denoted s300. The experimental condition
without sampling is denoted s?. Following stan-
dard settings, the maximum length of the source
phrase is set to 5 and the maximum length of the
target phrase is set to 15 (same for both GPU
and CPU implementations). The table is divided
into two sections: the top shows results on the
Xinhua data, and the bottom on Xinhua + UN
data. Columns report results for different numbers
331
# Sent. 2000 4000 6000 8000 16000
Speedup 9.6? 14.3? 17.5? 20.9? 40.9?
Phrases 2.1? 1.8? 1.7? 1.6? 1.6?
Table 3: Comparing no sampling on the GPU with sam-
pling on the CPU in terms of performance improvements
(GPU over CPU) and increases in the number of phrase
pairs extracted (GPU over CPU).
of input sentences. Performance is reported in terms
of throughput: the number of processed words per
second on average (i.e., total time divided by the
batch size in words). The results are averaged over
five trials, with 95% confidence intervals shown in
parentheses. Note that as the batch size increases,
we achieve higher throughput on the GPU since
we are better saturating its full processing power.
In contrast, performance is constant on the CPU
regardless of the number of sentences processed.
The CPU throughput on the Xinhua data is 1.13
words per second without sampling and 200 words
per second with sampling. On 16,000 test sentences,
we have mostly saturated the GPU?s processing
power, and observe a 7217? speedup over the CPU
implementation without sampling and 62? speedup
with sampling. On the larger (Xinhua + UN)
corpus, we observe 43? and 10836? speedup with
sampling and no sampling, respectively.
Interestingly, a run without sampling on the GPU
is still substantially faster than a run with sampling
on the CPU. On the Xinhua corpus, we observe
speedups ranging from nine times to forty times, as
shown in Table 3. Without sampling, we are able to
extract up to twice as many phrases.
In previous CPU implementations of on-the-fly
phrase extraction, restrictions were placed on the
maximum length of the source and target phrases
due to computational constraints (in addition to sam-
pling). Given the massive parallelism afforded by
the GPU, might we be able to lift these restrictions
and construct the complete phrase table? To answer
this question, we performed an experiment without
sampling and without any restrictions on the length
of the extracted phrases. The complete phrase
table contained about 0.5% more distinct pairs, with
negligible impact on performance.
When considering these results, an astute reader
might note that we are comparing performance
of a single-threaded implementation with a fully-
saturated GPU. To address this concern, we
conducted an experiment using a multi-threaded
version of the CPU reference implementation to
take full advantage of multiple cores on the CPU (by
specifying the -j option in cdec); we experimented
with up to four threads to fully saturate the
dual-core CPU. In terms of throughput, the CPU
implementation scales linearly, i.e., running on four
threads achieves roughly 4? throughput. Note that
the CPU and GPU implementations take advantage
of parallelism in completely different ways: cdec
can be characterized as embarrassingly parallel, with
different threads processing each complete sentence
in isolation, whereas our GPU implementation
achieves intra-sentential parallelism by exploiting
many threads to concurrently process each sentence.
In terms of absolute performance figures, even
with the 4? throughput improvement from fully
saturating the CPU, our GPU implementation
remains faster by a wide margin. Note that neither
our GPU nor CPU represents state-of-the-art
hardware, and we would expect the performance
advantage of GPUs to be even greater with latest
generation hardware, since the number of available
threads on a GPU is increasing faster than the
number of threads available on a CPU.
Since phrase extraction is only one part of an
end-to-end machine translation system, it makes
sense to examine the overall performance of the
entire translation pipeline. For this experiment, we
used our GPU implementation for phrase extrac-
tion, serialized the grammar files to disk, and used
cdec for decoding (on the CPU). The comparison
condition used cdec for all three stages. We used
standard phrase length constraints (5 on source side,
15 on target side) with sampling of frequent phrases.
Finally, we replicated the data conditions in Lopez
(2008a), where our source corpora was the Xinhua
data set and our development/test sets were the
NIST03/NIST05 data; the NIST05 test set contains
1,082 sentences.
Performance results for end-to-end translation are
shown in Table 4, broken down in terms of total
amount of time for each of the processing stages
for the entire test set under different conditions.
In the decoding stage, we varied the number of
CPU threads (note here we do not observe linear
332
Phrase Extraction I/O Decoding
GPU: 11.0
3.7
1 thread 55.7
2 threads 35.3
CPU: 166.5
3 threads 31.5
4 threads 26.2
Table 4: End-to-end machine translation performance:
time to process the NIST05 test set in seconds, broken
down in terms of the three processing stages.
speedup). In terms of end-to-end results, complete
translation of the test set takes 41 seconds with the
GPU for phrase extraction and CPU for decoding,
compared to 196 seconds using the CPU for both
(with four decoding threads in both cases). This rep-
resents a speedup of 4.8?, which suggests that even
selective optimizations of individual components in
the MT pipeline using GPUs can make a substantial
difference in overall performance.
8 Future Work
There are a number of directions that we have
identified for future work. For computational ef-
ficiency reasons, previous implementations of the
?translation by pattern matching? approach have
had to introduce approximations, e.g., sampling and
constraints on phrase lengths. Our results show that
the massive amounts of parallelism available in the
GPU make these approximations unnecessary, but
it is unclear to what extent they impact translation
quality. For example, Table 3 shows that we extract
up to twice as many phrase pairs without sampling,
but do these pairs actually matter? We have begun to
examine the impact of various settings on translation
quality and have observed small improvements in
some cases (which, note, come for ?free?), but so
far the results have not been conclusive.
The experiments in this paper focus primarily
on throughput, but for large classes of applications
latency is also important. One current limitation of
our work is that large batch sizes are necessary to
fully utilize the available processing power of the
GPU. This and other properties of the GPU, such as
the high latency involved in transferring data from
main memory to GPU memory, make low-latency
processing a challenge, which we hope to address.
Another broad future direction is to ?GPU-ify?
other machine translation models and other com-
ponents in the machine translation pipeline. An
obvious next step is to extend our work to the
hierarchical phrase-based translation model (Chi-
ang, 2007), which would involve extracting ?gappy?
phrases. Lopez (2008a) has tackled this problem
on the CPU, but it is unclear to what extent the
same types of algorithms he proposed can execute
efficiently in the GPU environment. Beyond phrase
extraction, it might be possible to perform decoding
itself in the GPU?not only will this exploit massive
amounts of parallelism, but also reduce costs in
moving data to and from the GPU memory.
9 Conclusion
GPU parallelism offers many promises for practical
and efficient implementations of language process-
ing systems. This promise has been demonstrated
for speech recognition (Chong et al, 2008; Chong
et al, 2009) and parsing (Yi et al, 2011), and we
have demonstrated here that it extends to machine
translation as well. We believe that explorations of
modern parallel hardware architectures is a fertile
area of research: the field has only begun to exam-
ine the possibilities and there remain many more
interesting questions to tackle. Parallelism is critical
not only from the perspective of building real-world
applications, but for overcoming fundamental com-
putational bottlenecks associated with models that
researchers are developing today.
Acknowledgments
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF
under award IIS-1144034. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect views of the sponsors. The second author is
grateful to Esther and Kiri for their loving support
and dedicates this work to Joshua and Jacob. We
would like to thank three anonymous reviewers for
providing helpful suggestions and also acknowledge
Benjamin Van Durme and CLIP labmates for useful
discussions. We also thank UMIACS for providing
hardware resources via the NVIDIA CUDA Center
of Excellence, UMIACS IT staff, especially Joe
Webster, for excellent support.
333
References
R. D. Brown. 2004. A modified Burrows-Wheeler
Transform for highly-scalable example-based transla-
tion. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA 2004), pages 27?36.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL 2005), pages 255?
262.
V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec:
A Python interface to cdec. In Proceedings of the 7th
Machine Translation Marathon (MTM 2012).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer.
2008. Data-parallel large vocabulary continuous
speech recognition on graphics processors. In Pro-
ceedings of the Workshop on Emerging Applications
and Manycore Architectures.
J. Chong, E. Gonina, Y. Yi, and K. Keutzer. 2009. A fully
data parallel WFST-based large vocabulary continuous
speech recognition on a graphics processing unit.
In Proceedings of the 10th Annual Conference of
the International Speech Communication Association
(INTERSPEECH 2009), pages 1183?1186.
F. Cromieres and S. Kurohashi. 2011. Efficient retrieval
of tree translation examples for syntax-based machine
translation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, pages 508?518.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7?12.
A. Gharaibeh and M. Ripeanu. 2010. Size matters:
Space/time tradeoffs to improve GPGPU applications
performance. In Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC 2010),
pages 1?12.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 976?985.
A. Lopez. 2008a. Machine translation by pattern
matching. Ph.D. dissertation, University of Maryland,
College Park, Maryland, USA.
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008), pages 505?512.
U. Manber and G. Myers. 1990. Suffix arrays: a new
method for on-line string searches. In Proceedings of
the First Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA ?90), pages 319?327.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 20?28.
M. Schatz, C. Trapnell, A. Delcher, and A. Varshney.
2007. High-throughput sequence alignment using
graphics processing units. BMC Bioinformatics,
8(1):474.
C. Trapnell and M. C. Schatz. 2009. Optimizing data
intensive GPGPU computations for DNA sequence
alignment. Parallel Computing, 35(8-9):429?440.
Y. Yi, C.-Y. Lai, S. Petrov, and K. Keutzer. 2011.
Efficient parallel CKY parsing on GPUs. In
Proceedings of the 12th International Conference on
Parsing Technologies, pages 175?185.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05).
334
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 470?480,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Comparison of Loopy Belief Propagation and Dual Decomposition for
Integrated CCG Supertagging and Parsing
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
Via an oracle experiment, we show that the
upper bound on accuracy of a CCG parser
is significantly lowered when its search space
is pruned using a supertagger, though the su-
pertagger also prunes many bad parses. In-
spired by this analysis, we design a single
model with both supertagging and parsing fea-
tures, rather than separating them into dis-
tinct models chained together in a pipeline.
To overcome the resulting increase in com-
plexity, we experiment with both belief prop-
agation and dual decomposition approaches to
inference, the first empirical comparison of
these algorithms that we are aware of on a
structured natural language processing prob-
lem. On CCGbank we achieve a labelled de-
pendency F-measure of 88.8% on gold POS
tags, and 86.7% on automatic part-of-speeoch
tags, the best reported results for this task.
1 Introduction
Accurate and efficient parsing of Combinatorial Cat-
egorial Grammar (CCG; Steedman, 2000) is a long-
standing problem in computational linguistics, due
to the complexities associated its mild context sen-
sitivity. Even for practical CCG that are strongly
context-free (Fowler and Penn, 2010), parsing is
much harder than with Penn Treebank-style context-
free grammars, with vast numbers of nonterminal
categories leading to increased grammar constants.
Where a typical Penn Treebank grammar may have
fewer than 100 nonterminals (Hockenmaier and
Steedman, 2002), we found that a CCG grammar
derived from CCGbank contained over 1500. The
same grammar assigns an average of 22 lexical cate-
gories per word (Clark and Curran, 2004a), resulting
in an enormous space of possible derivations.
The most successful approach to CCG parsing is
based on a pipeline strategy (?2). First, we tag (or
multitag) each word of the sentence with a lexical
category using a supertagger, a sequence model over
these categories (Bangalore and Joshi, 1999; Clark,
2002). Second, we parse the sentence under the
requirement that the lexical categories are fixed to
those preferred by the supertagger. Variations on
this approach drive the widely-used, broad coverage
C&C parser (Clark and Curran, 2004a; Clark and
Curran, 2007; Kummerfeld et al, 2010). However,
it fails when the supertagger makes errors. We show
experimentally that this pipeline significantly lowers
the upper bound on parsing accuracy (?3).
The same experiment shows that the supertag-
ger prunes many bad parses. So, while we want to
avoid the error propagation inherent to a pipeline,
ideally we still want to benefit from the key insight
of supertagging: that a sequence model over lexi-
cal categories can be quite accurate. Our solution
is to combine the features of both the supertagger
and the parser into a single, less aggressively pruned
model. The challenge with this model is its pro-
hibitive complexity, which we address with approx-
imate methods: dual decomposition and belief prop-
agation (?4). We present the first side-by-side com-
parison of these algorithms on an NLP task of this
complexity, measuring accuracy, convergence be-
havior, and runtime. In both cases our model signifi-
cantly outperforms the pipeline approach, leading to
the best published results in CCG parsing (?5).
470
2 CCG and Supertagging
CCG is a lexicalized grammar formalism encoding
for each word lexical categories that are either ba-
sic (eg. NN, JJ) or complex. Complex lexical cat-
egories specify the number and directionality of ar-
guments. For example, one lexical category for the
verb like is (S\NP )/NP , specifying the first argu-
ment as an NP to the right and the second as an NP
to the left; there are over 100 lexical categories for
like in our lexicon. In parsing, adjacent spans are
combined using a small number of binary combina-
tory rules like forward application or composition
(Steedman, 2000; Fowler and Penn, 2010). In the
first derivation below, (S\NP )/NP and NP com-
bine to form the spanning category S\NP , which
only requires an NP to its left to form a complete
sentence-spanning S. The second derivation uses
type-raising to change the category type of I.
I like tea
NP (S\NP)/NP NP
>
S\NP
<
S
I like tea
NP (S\NP)/NP NP
>T
S/(S\NP)
>B
S/NP
>
S
As can be inferred from even this small example,
a key difficulty in parsing CCG is that the number
of categories quickly becomes extremely large, and
there are typically many ways to analyze every span
of a sentence.
Supertagging (Bangalore and Joshi, 1999; Clark,
2002) treats the assignment of lexical categories (or
supertags) as a sequence tagging problem. Because
they do this with high accuracy, they are often ex-
ploited to prune the parser?s search space: the parser
only considers lexical categories with high posterior
probability (or other figure of merit) under the su-
pertagging model (Clark and Curran, 2004a). The
posterior probabilities are then discarded; it is the
extensive pruning of lexical categories that leads to
substantially faster parsing times.
Pruning the categories in advance this way has a
specific failure mode: sometimes it is not possible
to produce a sentence-spanning derivation from the
tag sequences preferred by the supertagger, since it
does not enforce grammaticality. A workaround for
this problem is the adaptive supertagging (AST) ap-
proach of Clark and Curran (2004a). It is based on
a step function over supertagger beam widths, re-
laxing the pruning threshold for lexical categories
only if the parser fails to find an analysis. The pro-
cess either succeeds and returns a parse after some
iteration or gives up after a predefined number of it-
erations. As Clark and Curran (2004a) show, most
sentences can be parsed with a very small number of
supertags per word. However, the technique is inher-
ently approximate: it will return a lower probability
parse under the parsing model if a higher probabil-
ity parse can only be constructed from a supertag
sequence returned by a subsequent iteration. In this
way it prioritizes speed over exactness, although the
tradeoff can be modified by adjusting the beam step
function. Regardless, the effect of the approxima-
tion is unbounded.
We will also explore reverse adaptive supertag-
ging, a much less aggressive pruning method that
seeks only to make sentences parseable when they
otherwise would not be due to an impractically large
search space. Reverse AST starts with a wide beam,
narrowing it at each iteration only if a maximum
chart size is exceeded. In this way it prioritizes ex-
actness over speed.
3 Oracle Parsing
What is the effect of these approximations? To
answer this question we computed oracle best and
worst values for labelled dependency F-score using
the algorithm of Huang (2008) on the hybrid model
of Clark and Curran (2007), the best model of their
C&C parser. We computed the oracle on our devel-
opment data, Section 00 of CCGbank (Hockenmaier
and Steedman, 2007), using both AST and Reverse
AST beams settings shown in Table 1.
The results (Table 2) show that the oracle best
accuracy for reverse AST is more than 3% higher
than the aggressive AST pruning.1 In fact, it is al-
most as high as the upper bound oracle accuracy of
97.73% obtained using perfect supertags?in other
words, the search space for reverse AST is theoreti-
cally near-optimal.2 We also observe that the oracle
1The numbers reported here and in later sections differ slightly
from those in a previously circulated draft of this paper, for
two reasons: we evaluate only on sentences for which a parse
was returned instead of all parses, to enable direct comparison
with Clark and Curran (2007); and we use their hybrid model
instead of their normal-form model, except where noted. De-
spite these changes our main findings remained unchanged.
2This idealized oracle reproduces a result from Clark and Cur-
471
Condition Parameter Iteration 1 2 3 4 5
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
Reverse
? 0.001 0.005 0.01 0.03 0.075
k 150 20 20 20 20
Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.
Parameter ? is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for words
seen less than k times.
Viterbi F-score Oracle Max F-score Oracle Min F-score
LF LP LR LF LP LR LF LP LR cat/word
AST 87.38 87.83 86.93 94.35 95.24 93.49 54.31 54.81 53.83 1.3-3.6
Reverse 87.36 87.55 87.17 97.65 98.21 97.09 18.09 17.75 18.43 3.6-1.3
Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle
F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the
the number of lexical categories per word used (from first to last parsing attempt).
88.2	 ?
88.4	 ?
88.6	 ?
88.8	 ?
89.0	 ?
89.2	 ?
89.4	 ?
89.6	 ?
89.8	 ?
85600	 ?
85800	 ?
86000	 ?
86200	 ?
86400	 ?
86600	 ?
86800	 ?
87000	 ?
87200	 ?
87400	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
93.5	 ?
94.0	 ?
94.5	 ?
95.0	 ?
95.5	 ?
96.0	 ?
96.5	 ?
97.0	 ?
97.5	 ?
98.0	 ?
98.5	 ?
82500	 ?
83000	 ?
83500	 ?
84000	 ?
84500	 ?
85000	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
Figure 1: Comparison between model score and Viterbi F-score (left); and between model score and oracle F-score
(right) for different supertagger beams on a subset of CCGbank Section 00.
worst accuracy is much lower in the reverse setting.
It is clear that the supertagger pipeline has two ef-
fects: while it beneficially prunes many bad parses,
it harmfully prunes some very good parses. We can
also see from the scores of the Viterbi parses that
while the reverse condition has access to much better
parses, the model doesn?t actually find them. This
mirrors the result of Clark and Curran (2007) that
they use to justify AST.
Digging deeper, we compared parser model score
against Viterbi F-score and oracle F-score at a va-
ran (2004b). The reason that using the gold-standard supertags
doesn?t result in 100% oracle parsing accuracy is that some
of the development set parses cannot be constructed by the
learned grammar.
riety of fixed beam settings (Figure 1), considering
only the subset of our development set which could
be parsed with all beam settings. The inverse re-
lationship between model score and F-score shows
that the supertagger restricts the parser to mostly
good parses (under F-measure) that the model would
otherwise disprefer. Exactly this effect is exploited
in the pipeline model. However, when the supertag-
ger makes a mistake, the parser cannot recover.
4 Integrated Supertagging and Parsing
The supertagger obviously has good but not perfect
predictive features. An obvious way to exploit this
without being bound by its decisions is to incorpo-
rate these features directly into the parsing model.
472
In our case both the parser and the supertagger are
feature-based models, so from the perspective of a
single parse tree, the change is simple: the tree is
simply scored by the weights corresponding to all
of its active features. However, since the features of
the supertagger are all Markov features on adjacent
supertags, the change has serious implications for
search. If we think of the supertagger as defining a
weighted regular language consisting of all supertag
sequences, and the parser as defining a weighted
mildly context-sensitive language consisting of only
a subset of these sequences, then the search prob-
lem is equivalent to finding the optimal derivation
in the weighted intersection of a regular and mildly
context-sensitive language. Even allowing for the
observation of Fowler and Penn (2010) that our prac-
tical CCG is context-free, this problem still reduces
to the construction of Bar-Hillel et al (1964), mak-
ing search very expensive. Therefore we need ap-
proximations.
Fortunately, recent literature has introduced two
relevant approximations to the NLP community:
loopy belief propagation (Pearl, 1988), applied to
dependency parsing by Smith and Eisner (2008);
and dual decomposition (Dantzig and Wolfe, 1960;
Komodakis et al, 2007; Sontag et al, 2010, inter
alia), applied to dependency parsing by Koo et al
(2010) and lexicalized CFG parsing by Rush et al
(2010). We apply both techniques to our integrated
supertagging and parsing model.
4.1 Loopy Belief Propagation
Belief propagation (BP) is an algorithm for com-
puting marginals (i.e. expectations) on structured
models. These marginals can be used for decoding
(parsing) in a minimum-risk framework (Smith and
Eisner, 2008); or for training using a variety of al-
gorithms (Sutton and McCallum, 2010). We experi-
ment with both uses in ?5. Many researchers in NLP
are familiar with two special cases of belief prop-
agation: the forward-backward and inside-outside
algorithms, used for computing expectations in se-
quence models and context-free grammars, respec-
tively.3 Our use of belief propagation builds directly
on these two familiar algorithms.
3Forward-backward and inside-outside are formally shown to
be special cases of belief propagation by Smyth et al (1997)
and Sato (2007), respectively.
f(T
1
)
f(T
2
)
b(T
0
) b(T
1
)
t
1
T
0
T
1
t
2
T
2
e
0
e
1
e
2
Figure 2: Supertagging factor graph with messages. Cir-
cles are variables and filled squares are factors.
BP is usually understood as an algorithm on bi-
partite factor graphs, which structure a global func-
tion into local functions over subsets of variables
(Kschischang et al, 1998). Variables maintain a be-
lief (expectation) over a distribution of values and
BP passes messages about these beliefs between
variables and factors. The idea is to iteratively up-
date each variable?s beliefs based on the beliefs of
neighboring variables (through a shared factor), us-
ing the sum-product rule.
This results in the following equation for a mes-
sage mx?f (x) from a variable x to a factor f
mx?f (x) =
?
h?n(x)\f
mh?x(x) (1)
where n(x) is the set of all neighbours of x. The
message mf?x from a factor to a variable is
mf?x(x) =
?
?{x}
f(X)
?
y?n(f)\x
my?f (y) (2)
where ? {x} represents all variables other than x,
X = n(f) and f(X) is the set of arguments of the
factor function f .
Making this concrete, our supertagger defines a
distribution over tags T0...TI , based on emission
factors e0...eI and transition factors t1...tI (Fig-
ure 2). The message fi a variable Ti receives from its
neighbor to the left corresponds to the forward prob-
ability, while messages from the right correspond to
backward probability bi.
fi(Ti) =
?
Ti?1
fi?1(Ti?1)ei?1(Ti?1)ti(Ti?1, Ti) (3)
bi(Ti) =
?
Ti+1
bi+1(Ti+1)ei+1(Ti+1)ti+1(Ti, Ti+1) (4)
473
span
(0,2)
span
(1,3)
span
(0,3)
TREE
n(T
0
) o(T
2
)
o(T
0
)
n(T
2
)
T
0
e
0
e
1
e
2
T
1
T
2
f(T
1
)
f(T
2
)
b(T
0
)
b(T
1
)
t
1
t
2
Figure 3: Factor graph for the combined parsing and su-
pertagging model.
The current belief Bx(x) for variable x can be com-
puted by taking the normalized product of all its in-
coming messages.
Bx(x) =
1
Z
?
h?n(x)
mh?x(x) (5)
In the supertagger model, this is just:
p(Ti) =
1
Z
fi(Ti)bi(Ti)ei(Ti) (6)
Our parsing model is also a distribution over vari-
ables Ti, along with an additional quadratic number
of span(i, j) variables. Though difficult to represent
pictorially, a distribution over parses is captured by
an extension to graphical models called case-factor
diagrams (McAllester et al, 2008). We add this
complex distribution to our model as a single fac-
tor (Figure 3). This is a natural extension to the use
of complex factors described by Smith and Eisner
(2008) and Dreyer and Eisner (2009).
When a factor graph is a tree as in Figure 2, BP
converges in a single iteration to the exact marginals.
However, when the model contains cycles, as in Fig-
ure 3, we can iterate message passing. Under certain
assumptions this loopy BP it will converge to ap-
proximate marginals that are bounded under an in-
terpretation from statistical physics (Yedidia et al,
2001; Sutton and McCallum, 2010).
The TREE factor exchanges inside ni and outside
oi messages with the tag and span variables, tak-
ing into account beliefs from the sequence model.
We will omit the unchanged outside recursion for
brevity, but inside messages n(Ci,j) for category
Ci,j in span(i, j) are computed using rule probabil-
ities r as follows:
n(Ci,j) =
?
??
??
fi(Ci,j)bi(Ci,j)ei(Ci,j) if j=i+1
?
k,X,Y
n(Xi,k)n(Yk,j)r(Ci,j , Xi,k, Yk,j)
(7)
Note that the only difference from the classic in-
side algorithm is that the recursive base case of a cat-
egory spanning a single word has been replaced by
a message from the supertag that contains both for-
ward and backward factors, along with a unary emis-
sion factor, which doubles as a unary rule factor and
thus contains the only shared features of the original
models. This difference is also mirrored in the for-
ward and backward messages, which are identical to
Equations 3 and 4, except that they also incorporate
outside messages from the tree factor.
Once all forward-backward and inside-outside
probabilities have been calculated the belief of su-
pertag Ti can be computed as the product of all in-
coming messages. The only difference from Equa-
tion 6 is the addition of the outside message.
p(Ti) =
1
Z
fi(Ti)bi(Ti)ei(Ti)oi(Ti) (8)
The algorithm repeatedly runs forward-backward
and inside-outside, passing their messages back and
forth, until these quantities converge.
4.2 Dual Decomposition
Dual decomposition (Rush et al, 2010; Koo et al,
2010) is a decoding (i.e. search) algorithm for prob-
lems that can be decomposed into exactly solvable
subproblems: in our case, supertagging and parsing.
Formally, given Y as the set of valid parses, Z as the
set of valid supertag sequences, and T as the set of
supertags, we want to solve the following optimiza-
tion for parser f(y) and supertagger g(z).
argmax
y?Y,z?Z
f(y) + g(z) (9)
such that y(i, t) = z(i, t) for all (i, t) ? I (10)
Here y(i, t) is a binary function indicating whether
word i is assigned supertag t by the parser, for the
474
set I = {(i, t) : i ? 1 . . . n, t ? T} denoting
the set of permitted supertags for each word; sim-
ilarly z(i, t) for the supertagger. To enforce the con-
straint that the parser and supertagger agree on a
tag sequence we introduce Lagrangian multipliers
u = {u(i, t) : (i, t) ? I} and construct a dual ob-
jective over variables u(i, t).
L(u) = max
y?Y
(f(y)?
?
i,t
u(i, t)y(i, t)) (11)
+max
z?Z
(f(z) +
?
i,t
u(i, t)z(i, t))
This objective is an upper bound that we want to
make as tight as possible by solving for minu L(u).
We optimize the values of the u(i, t) variables using
the same algorithm as Rush et al (2010) for their
tagging and parsing problem (essentially a percep-
tron update).4 An advantages of DD is that, on con-
vergence, it recovers exact solutions to the combined
problem. However, if it does not converge or we stop
early, an approximation must be returned: following
Rush et al (2010) we used the highest scoring output
of the parsing submodel over all iterations.
5 Experiments
Parser. We use the C&C parser (Clark and Curran,
2007) and its supertagger (Clark, 2002). Our base-
line is the hybrid model of Clark and Curran (2007);
our integrated model simply adds the supertagger
features to this model. The parser relies solely on the
supertagger for pruning, using CKY for search over
the pruned space. Training requires repeated calcu-
lation of feature expectations over packed charts of
derivations. For training, we limited the number of
items in this chart to 0.3 million, and for testing, 1
million. We also used a more permissive training
supertagger beam (Table 3) than in previous work
(Clark and Curran, 2007). Models were trained with
the parser?s L-BFGS trainer.
Evaluation. We evaluated on CCGbank (Hocken-
maier and Steedman, 2007), a right-most normal-
form CCG version of the Penn Treebank. We
use sections 02-21 (39603 sentences) for training,
4The u terms can be interpreted as the messages from factors
to variables (Sontag et al, 2010) and the resulting message
passing algorithms are similar to the max-product algorithm, a
sister algorithm to BP.
section 00 (1913 sentences) for development and
section 23 (2407 sentences) for testing. We sup-
ply gold-standard part-of-speech tags to the parsers.
Evaluation is based on labelled and unlabelled pred-
icate argument structure recovery and supertag ac-
curacy. We only evaluate on sentences for which an
analysis was returned; the coverage for all parsers is
99.22% on section 00, and 99.63% on section 23.
Model combination. We combine the parser and
the supertagger over the search space defined by the
set of supertags within the supertagger beam (see Ta-
ble 1); this avoids having to perform inference over
the prohibitively large set of parses spanned by all
supertags. Hence at each beam setting, the model
operates over the same search space as the baseline;
the difference is that we search with our integrated
model.
5.1 Parsing Accuracy
We first experiment with the separately trained su-
pertagger and parser, which are then combined us-
ing belief propagation (BP) and dual decomposition
(DD). We run the algorithms for many iterations,
and irrespective of convergence, for BP we compute
the minimum risk parse from the current marginals,
and for DD we choose the highest-scoring parse
seen over all iterations. We measured the evolving
accuracy of the models on the development set (Fig-
ure 4). In line with our oracle experiment, these re-
sults demonstrate that we can coax more accurate
parses from the larger search space provided by the
reverse setting; the influence of the supertagger fea-
tures allow us to exploit this advantage.
One behavior we observe in the graph is that the
DD results tend to incrementally improve in accu-
racy while the BP results quickly stabilize, mirroring
the result of Smith and Eisner (2008). This occurs
because DD continues to find higher scoring parses
at each iteration, and hence the results change. How-
ever for BP, even if the marginals have not con-
verged, the minimum risk solution turns out to be
fairly stable across successive iterations.
We next compare the algorithms against the base-
line on our test set (Table 4). We find that the early
stability of BP?s performance generalises to the test
set as does DD?s improvement over several itera-
tions. More importantly, we find that the applying
475
Parameter Iteration 1 2 3 4 5 6 7
Training ? 0.001 0.001 0.0045 0.0055 0.01 0.05 0.1
k 150 20 20 20 20 20 20
Table 3: Beam step function used for training (cf. Table 1).
section 00 (dev) section 23 (test)
AST Reverse AST Reverse
LF UF ST LF UF ST LF UF ST LF UF ST
Baseline 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01
C&C ?07 87.24 93.00 94.16 - - - 87.64 93.00 94.32 - - -
BPk=1 87.70 93.28 94.44 88.35 93.69 94.73 88.20 93.28 94.60 88.78 93.66 94.81
BPk=25 87.70 93.31 94.44 88.33 93.72 94.71 88.19 93.27 94.59 88.80 93.68 94.81
DDk=1 87.40 93.09 94.23 87.38 93.15 94.03 87.74 93.10 94.33 87.67 93.07 94.02
DDk=25 87.71 93.32 94.44 88.29 93.71 94.67 88.14 93.24 94.59 88.80 93.68 94.82
Table 4: Results for individually-trained submodels combined using dual decomposition (DD) or belief propagation
(BP) for k iterations, evaluated by labelled and unlabelled F-score (LF/UF) and supertag accuracy (ST). We compare
against the previous best result of Clark and Curran (2007); our baseline is their model with wider training beams (cf.
Table 3).
87.2	 ?
87.4	 ?
87.6	 ?
87.8	 ?
88.0	 ?
88.2	 ?
88.4	 ?
1	 ? 6	 ? 11	 ? 16	 ? 21	 ? 26	 ? 31	 ? 36	 ? 41	 ? 46	 ?
Lab
elle
d	 ?F
-??sc
ore
	 ?
Itera?ons	 ?
BL	 ?	 ?AST	 ? BL	 ?Rev	 ? BP	 ?AST	 ?
BP	 ?Rev	 ? DD	 ?AST	 ? DD	 ?Rev	 ?
Figure 4: Labelled F-score of baseline (BL), belief prop-
agation (BP), and dual decomposition (DD) on section
00.
our combined model using either algorithm consis-
tently outperforms the baseline after only a few iter-
ations. Overall, we improve the labelled F-measure
by almost 1.1% and unlabelled F-measure by 0.6%
over the baseline. To the best of our knowledge,
the results obtained with BP and DD are the best
reported results on this task using gold POS tags.
Next, we evaluate performance when using au-
tomatic part-of-speech tags as input to our parser
and supertagger (Table 5). This enables us to com-
pare against the results of Fowler and Penn (2010),
who trained the Petrov parser (Petrov et al, 2006)
on CCGbank. We outperform them on all criteria.
Hence our combined model represents the best CCG
parsing results under any setting.
Finally, we revisit the oracle experiment of ?3 us-
ing our combined models (Figure 5). Both show an
improved relationship between model score and F-
measure.
5.2 Algorithmic Convergence
Figure 4 shows that parse accuracy converges af-
ter a few iterations. Do the algorithms converge?
BP converges when the marginals do not change be-
tween iterations, and DD converges when both sub-
models agree on all supertags. We measured the
convergence of each algorithm under these criteria
over 1000 iterations (Figure 6). DD converges much
faster, while BP in the reverse condition converges
quite slowly. This is interesting when contrasted
with its behavior on parse accuracy?its rate of con-
vergence after one iteration is 1.5%, but its accu-
racy is already the highest at this point. Over the
entire 1000 iterations, most sentences converge: all
but 3 for BP (both in AST and reverse) and all but
476
section 00 (dev) section 23 (test)
LF LP LR UF UP UR LF LP LR UF UP UR
Baseline 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75
Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.74 92.34 92.64 92.04
BPk=1 86.44 86.74 86.14 92.54 92.86 92.23 86.73 86.95 86.50 92.45 92.69 92.21
DDk=25 86.35 86.65 86.05 92.52 92.85 92.20 86.68 86.90 86.46 92.44 92.67 92.21
Table 5: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn
(2010); we evaluate on sentences for which all parsers returned an analysis (2323 sentences for section 23 and 1834
sentences for section 00).
89.4	 ?
89.5	 ?
89.6	 ?
89.7	 ?
89.8	 ?
89.9	 ?
90.0	 ?
60000	 ?
80000	 ?
100000	 ?
120000	 ?
140000	 ?
160000	 ?
180000	 ?
200000	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
89.2	 ?
89.3	 ?
89.4	 ?
89.5	 ?
89.6	 ?
89.7	 ?
89.8	 ?
89.9	 ?
85200	 ?
85400	 ?
85600	 ?
85800	 ?
86000	 ?
86200	 ?
86400	 ?
0.07
5	 ? 0.03
	 ?
0.01
	 ?
0.00
5	 ?
0.00
1	 ?
0.00
05	 ?
0.00
01	 ?
0.00
005	 ?
0.00
001	 ?
Lab
elle
ld	 ?F
-??sco
re	 ?
Mo
del
	 ?sco
re	 ?
Supertagger	 ?beam	 ?
Model	 ?score	 ? F-??measure	 ?
Figure 5: Comparison between model score and Viterbi F-score for the integrated model using belief propagation (left)
and dual decomposition (right); the results are based on the same data as Figure 1.
.
!"
#!"
$!"
%!"
&!"
'!"
(!"
)!"
*!"
+!"
#!!"
#" #!" #!!" #!!!"
!"#
$%&'
%#(
%)&*
+%),-
.)
/+%&*0"#1)
,-"./0"
,-"1232452"
66"./0"
66"1232452"
Figure 6: Rate of convergence for belief propagation (BP)
and dual decomposition (DD) with maximum k = 1000.
41 (2.6%) for DD in reverse (6 in AST).
5.3 Parsing Speed
Because the C&C parser with AST is very fast, we
wondered about the effect on speed for our model.
We measured the runtime of the algorithms under
the condition that we stopped at a particular iteration
(Table 6). Although our models improve substan-
tially over C&C, there is a significant cost in speed
for the best result.
5.4 Training the Integrated Model
In the experiments reported so far, the parsing and
supertagging models were trained separately, and
only combined at test time. Although the outcome
of these experiments was successful, we wondered
if we could obtain further improvements by training
the model parameters together.
Since the gradients produced by (loopy) BP
are approximate, for these experiments we used a
stochastic gradient descent (SGD) trainer (Bottou,
2003). We found that the SGD parameters described
by Finkel et al (2008) worked equally well for our
models, and, on the baseline, produced similar re-
sults to L-BFGS. Curiously, however, we found that
the combined model does not perform as well when
477
AST Reverse
sent/sec LF sent/sec LF
Baseline 65.8 87.38 5.9 87.36
BPk=1 60.8 87.70 5.8 88.35
BPk=5 46.7 87.70 4.7 88.34
BPk=25 35.3 87.70 3.5 88.33
DDk=1 64.6 87.40 5.9 87.38
DDk=5 41.9 87.65 3.1 88.09
DDk=25 32.5 87.71 1.9 88.29
Table 6: Parsing time in seconds per sentence (vs. F-
measure) on section 00.
AST Reverse
LF UF ST LF UF ST
Baseline 86.7 92.7 94.0 86.7 92.7 93.9
BP inf 86.8 92.8 94.1 87.2 93.1 94.2
BP train 86.3 92.5 93.8 85.6 92.1 93.2
Table 7: Results of training with SGD on approximate
gradients from LPB on section 00. We test LBP in both
inference and training (train) as well as in inference only
(inf); a maximum number of 10 iterations is used.
the parameters are trained together (Table 7). A pos-
sible reason for this is that we used a stricter su-
pertagger beam setting during training (Clark and
Curran, 2007) to make training on a single machine
practical. This leads to lower performance, particu-
larly in the Reverse condition. Training a model us-
ing DD would require a different optimization algo-
rithm based on Viterbi results (e.g. the perceptron)
which we will pursue in future work.
6 Conclusion and Future Work
Our approach of combining models to avoid the
pipeline problem (Felzenszwalb and McAllester,
2007) is very much in line with much recent work
in NLP. Such diverse topics as machine transla-
tion (Dyer et al, 2008; Dyer and Resnik, 2010;
Mi et al, 2008), part-of-speech tagging (Jiang et
al., 2008), named entity recognition (Finkel and
Manning, 2009) semantic role labelling (Sutton and
McCallum, 2005; Finkel et al, 2006), and oth-
ers have also been improved by combined models.
Our empirical comparison of BP and DD also com-
plements the theoretically-oriented comparison of
marginal- and margin-based variational approxima-
tions for parsing described by Martins et al (2010).
We have shown that the aggressive pruning used
in adaptive supertagging significantly harms the or-
acle performance of the parser, though it mostly
prunes bad parses. Based on these findings, we com-
bined parser and supertagger features into a single
model. Using belief propagation and dual decom-
position, we obtained more principled?and more
accurate?approximations than a pipeline. Mod-
els combined using belief propagation achieve very
good performance immediately, despite an initial
convergence rate just over 1%, while dual decompo-
sition produces comparable results after several iter-
ations, and algorithmically converges more quickly.
Our best result of 88.8% represents the state-of-the
art in CCG parsing accuracy.
In future work we plan to integrate the POS tag-
ger, which is crucial to parsing accuracy (Clark and
Curran, 2004b). We also plan to revisit the idea
of combined training. Though we have focused on
CCG in this work we expect these methods to be
equally useful for other linguistically motivated but
computationally complex formalisms such as lexi-
calized tree adjoining grammar.
Acknowledgements
We would like to thank Phil Blunsom, Prachya
Boonkwan, Christos Christodoulopoulos, Stephen
Clark, Michael Collins, Chris Dyer, Timothy
Fowler, Mark Granroth-Wilding, Philipp Koehn,
Terry Koo, Tom Kwiatkowski, Andre? Martins, Matt
Post, David Smith, David Sontag, Mark Steed-
man, and Charles Sutton for helpful discussion re-
lated to this work and comments on previous drafts,
and the anonymous reviewers for helpful comments.
We also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); the EuroMatrixPlus project
funded by the European Commission, 7th Frame-
work Programme (Lopez); and the resources pro-
vided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk).
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
478
tics, 25(2):238?265, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116?150.
L. Bottou. 2003. Stochastic learning. In Advanced Lec-
tures in Machine Learning, pages 146?168.
S. Clark and J. R. Curran. 2004a. The importance of su-
pertagging for wide-coverage CCG parsing. In COL-
ING, Morristown, NJ, USA.
S. Clark and J. R. Curran. 2004b. Parsing the WSJ using
CCG and log-linear models. In Proc. of ACL, pages
104?111, Barcelona, Spain.
S. Clark and J. R. Curran. 2007. Wide-Coverage Ef-
ficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4):493?552.
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In TAG+6.
G. B. Dantzig and P. Wolfe. 1960. Decomposition
principle for linear programs. Operations Research,
8(1):101?111.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proc. of EMNLP.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of HLT-NAACL.
C. J. Dyer, S. Muresan, and P. Resnik. 2008. Generaliz-
ing word lattice translation. In Proc. of ACL.
P. F. Felzenszwalb and D. McAllester. 2007. The Gener-
alized A* Architecture. In Journal of Artificial Intelli-
gence Research, volume 29, pages 153?190.
J. R. Finkel and C. D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL. Associ-
ation for Computational Linguistics.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008.
Feature-based, conditional random field parsing. In
Proceedings of ACL-HLT.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
J. Hockenmaier and M. Steedman. 2002. Generative
models for statistical parsing with Combinatory Cat-
egorial Grammar. In Proc. of ACL.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
L. Huang. 2008. Forest Reranking: Discriminative pars-
ing with Non-Local Features. In Proceedings of ACL-
08: HLT.
W. Jiang, L. Huang, Q. Liu, and Y. Lu?. 2008. A cas-
caded linear model for joint Chinese word segmen-
tation and part-of-speech tagging. In Proceedings of
ACL-08: HLT.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In Proc. of Int. Conf. on Computer
Vision (ICCV).
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual Decomposition for Parsing with Non-
Projective Head Automata. In In Proc. EMNLP.
F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. 1998.
Factor graphs and the sum-product algorithm. IEEE
Transactions on Information Theory, 47:498?519.
J. K. Kummerfeld, J. Rosener, T. Dawborn, J. Haggerty,
J. R. Curran, and S. Clark. 2010. Faster parsing by
supertagger adaptation. In Proc. of ACL.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
D. McAllester, M. Collins, and F. Pereira. 2008. Case-
factor diagrams for structured probabilistic modeling.
Journal of Computer and System Sciences, 74(1):84?
96.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based trans-
lation. In Proc. of ACL-HLT.
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
A. M. Rush, D. Sontag, M. Collins, and T. Jaakkola.
2010. On dual decomposition and linear program-
ming relaxations for natural language processing. In
In Proc. EMNLP.
T. Sato. 2007. Inside-outside probability computation
for belief propagation. In Proc. of IJCAI.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
P. Smyth, D. Heckerman, and M. Jordan. 1997. Prob-
abilistic independence networks for hidden Markov
probability models. Neural computation, 9(2):227?
269.
D. Sontag, A. Globerson, and T. Jaakkola. 2010. Intro-
duction to dual decomposition. In S. Sra, S. Nowozin,
and S. J. Wright, editors, Optimization for Machine
Learning. MIT Press.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
C. Sutton and A. McCallum. 2005. Joint parsing and
semantic role labelling. In Proc. of CoNLL.
479
C. Sutton and A. McCallum. 2010. An introduction to
conditional random fields. arXiv:stat.ML/1011.4088.
J. Yedidia, W. Freeman, and Y. Weiss. 2001. Generalized
belief propagation. In Proc. of NIPS.
480
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577?1585,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Efficient CCG Parsing: A* versus Adaptive Supertagging
Michael Auli
School of Informatics
University of Edinburgh
m.auli@sms.ed.ac.uk
Adam Lopez
HLTCOE
Johns Hopkins University
alopez@cs.jhu.edu
Abstract
We present a systematic comparison and com-
bination of two orthogonal techniques for
efficient parsing of Combinatory Categorial
Grammar (CCG). First we consider adap-
tive supertagging, a widely used approximate
search technique that prunes most lexical cat-
egories from the parser?s search space using
a separate sequence model. Next we con-
sider several variants on A*, a classic exact
search technique which to our knowledge has
not been applied to more expressive grammar
formalisms like CCG. In addition to standard
hardware-independent measures of parser ef-
fort we also present what we believe is the first
evaluation of A* parsing on the more realistic
but more stringent metric of CPU time. By it-
self, A* substantially reduces parser effort as
measured by the number of edges considered
during parsing, but we show that for CCG this
does not always correspond to improvements
in CPU time over a CKY baseline. Combining
A* with adaptive supertagging decreases CPU
time by 15% for our best model.
1 Introduction
Efficient parsing of Combinatorial Categorial Gram-
mar (CCG; Steedman, 2000) is a longstanding prob-
lem in computational linguistics. Even with practi-
cal CCG that are strongly context-free (Fowler and
Penn, 2010), parsing can be much harder than with
Penn Treebank-style context-free grammars, since
the number of nonterminal categories is generally
much larger, leading to increased grammar con-
stants. Where a typical Penn Treebank grammar
may have fewer than 100 nonterminals (Hocken-
maier and Steedman, 2002), we found that a CCG
grammar derived from CCGbank contained nearly
1600. The same grammar assigns an average of 26
lexical categories per word, resulting in a very large
space of possible derivations.
The most successful strategy to date for efficient
parsing of CCG is to first prune the set of lexi-
cal categories considered for each word, using the
output of a supertagger, a sequence model over
these categories (Bangalore and Joshi, 1999; Clark,
2002). Variations on this approach drive the widely-
used, broad coverage C&C parser (Clark and Cur-
ran, 2004; Clark and Curran, 2007). However, prun-
ing means approximate search: if a lexical category
used by the highest probability derivation is pruned,
the parser will not find that derivation (?2). Since the
supertagger enforces no grammaticality constraints
it may even prefer a sequence of lexical categories
that cannot be combined into any derivation (Fig-
ure 1). Empirically, we show that supertagging im-
proves efficiency by an order of magnitude, but the
tradeoff is a significant loss in accuracy (?3).
Can we improve on this tradeoff? The line of in-
vestigation we pursue in this paper is to consider
more efficient exact algorithms. In particular, we
test different variants of the classical A* algorithm
(Hart et al, 1968), which has met with success in
Penn Treebank parsing with context-free grammars
(Klein and Manning, 2003; Pauls and Klein, 2009a;
Pauls and Klein, 2009b). We can substitute A* for
standard CKY on either the unpruned set of lexi-
cal categories, or the pruned set resulting from su-
1577
Valid supertag-sequences
Valid parses
High scoring 
supertags 
High scoring 
parses
Desirable parses
Attainable parses
Figure 1: The relationship between supertagger and
parser search spaces based on the intersection of their cor-
responding tag sequences.
pertagging. Our empirical results show that on the
unpruned set of lexical categories, heuristics em-
ployed for context-free grammars show substantial
speedups in hardware-independent metrics of parser
effort (?4). To understand how this compares to the
CKY baseline, we conduct a carefully controlled set
of timing experiments. Although our results show
that improvements on hardware-independent met-
rics do not always translate into improvements in
CPU time due to increased processing costs that are
hidden by these metrics, they also show that when
the lexical categories are pruned using the output of
a supertagger, then we can still improve efficiency
by 15% with A* techniques (?5).
2 CCG and Parsing Algorithms
CCG is a lexicalized grammar formalism encoding
for each word lexical categories which are either
basic (eg. NN, JJ) or complex. Complex lexical
categories specify the number and directionality of
arguments. For example, one lexical category (of
over 100 in our model) for the transitive verb like is
(S\NP2)/NP1, specifying the first argument as an
NP to the right and the second as an NP to the left. In
parsing, adjacent spans are combined using a small
number of binary combinatory rules like forward ap-
plication or composition on the spanning categories
(Steedman, 2000; Fowler and Penn, 2010). In the
first derivation below, (S\NP )/NP and NP com-
bine to form the spanning category S\NP , which
only requires an NP to its left to form a complete
sentence-spanning S. The second derivation uses
type-raising to change the category type of I.
I like tea
NP (S\NP)/NP NP
>
S\NP
<
S
I like tea
NP (S\NP)/NP NP
>T
S/(S\NP)
>B
S/NP
>
S
Because of the number of lexical categories and their
complexity, a key difficulty in parsing CCG is that
the number of analyses for each span of the sentence
quickly becomes extremely large, even with efficient
dynamic programming.
2.1 Adaptive Supertagging
Supertagging (Bangalore and Joshi, 1999) treats the
assignment of lexical categories (or supertags) as a
sequence tagging problem. Once the supertagger
has been run, lexical categories that apply to each
word in the input sentence are pruned to contain only
those with high posterior probability (or other figure
of merit) under the supertagging model (Clark and
Curran, 2004). The posterior probabilities are then
discarded; it is the extensive pruning of lexical cate-
gories that leads to substantially faster parsing times.
Pruning the categories in advance this way has a
specific failure mode: sometimes it is not possible
to produce a sentence-spanning derivation from the
tag sequences preferred by the supertagger, since it
does not enforce grammaticality. A workaround for
this problem is the adaptive supertagging (AST) ap-
proach of Clark and Curran (2004). It is based on a
step function over supertagger beam ratios, relaxing
the pruning threshold for lexical categories when-
ever the parser fails to find an analysis. The pro-
cess either succeeds and returns a parse after some
iteration or gives up after a predefined number of
iterations. As Clark and Curran (2004) show, most
sentences can be parsed with a very small number of
supertags per word. However, the technique is inher-
ently approximate: it will return a lower probability
parse under the parsing model if a higher probabil-
ity parse can only be constructed from a supertag
sequence returned by a subsequent iteration. In this
way it prioritizes speed over accuracy, although the
tradeoff can be modified by adjusting the beam step
function.
2.2 A* Parsing
Irrespective of whether lexical categories are pruned
in advance using the output of a supertagger, the
CCG parsers we are aware of all use some vari-
1578
ant of the CKY algorithm. Although CKY is easy
to implement, it is exhaustive: it explores all pos-
sible analyses of all possible spans, irrespective of
whether such analyses are likely to be part of the
highest probability derivation. Hence it seems nat-
ural to consider exact algorithms that are more effi-
cient than CKY.
A* search is an agenda-based best-first graph
search algorithm which finds the lowest cost parse
exactly without necessarily traversing the entire
search space (Klein and Manning, 2003). In contrast
to CKY, items are not processed in topological order
using a simple control loop. Instead, they are pro-
cessed from a priority queue, which orders them by
the product of their inside probability and a heuris-
tic estimate of their outside probability. Provided
that the heuristic never underestimates the true out-
side probability (i.e. it is admissible) the solution is
guaranteed to be exact. Heuristics are model specific
and we consider several variants in our experiments
based on the CFG heuristics developed by Klein and
Manning (2003) and Pauls and Klein (2009a).
3 Adaptive Supertagging Experiments
Parser. For our experiments we used the generative
CCG parser of Hockenmaier and Steedman (2002).
Generative parsers have the property that all edge
weights are non-negative, which is required for A*
techniques.1 Although not quite as accurate as the
discriminative parser of Clark and Curran (2007) in
our preliminary experiments, this parser is still quite
competitive. It is written in Java and implements
the CKY algorithm with a global pruning threshold
of 10?4 for the models we use. We focus on two
parsing models: PCFG, the baseline of Hockenmaier
and Steedman (2002) which treats the grammar as a
PCFG (Table 1); and HWDep, a headword depen-
dency model which is the best performing model of
the parser. The PCFG model simply generates a tree
top down and uses very simple structural probabili-
ties while the HWDep model conditions node expan-
sions on headwords and their lexical categories.
Supertagger. For supertagging we used Den-
nis Mehay?s implementation, which follows Clark
1Indeed, all of the past work on A* parsing that we are aware of
uses generative parsers (Pauls and Klein, 2009b, inter alia).
(2002).2 Due to differences in smoothing of the
supertagging and parsing models, we occasionally
drop supertags returned by the supertagger because
they do not appear in the parsing model 3.
Evaluation. All experiments were conducted on
CCGBank (Hockenmaier and Steedman, 2007), a
right-most normal-form CCG version of the Penn
Treebank. Models were trained on sections 2-21,
tuned on section 00, and tested on section 23. Pars-
ing accuracy is measured using labelled and unla-
belled predicate argument structure recovery (Clark
and Hockenmaier, 2002); we evaluate on all sen-
tences and thus penalise lower coverage. All tim-
ing experiments reported in the paper were run on a
2.5 GHz Xeon machine with 32 GB memory and are
averaged over ten runs4.
3.1 Results
Supertagging has been shown to improve the speed
of a generative parser, although little analysis has
been reported beyond the speedups (Clark, 2002)
We ran experiments to understand the time/accuracy
tradeoff of adaptive supertagging, and to serve as
baselines.
Adaptive supertagging is parametrized by a beam
size ? and a dictionary cutoff k that bounds the
number of lexical categories considered for each
word (Clark and Curran, 2007). Table 3 shows both
the standard beam levels (AST) used for the C&C
parser and looser beam levels: AST-covA, a sim-
ple extension of AST with increased coverage and
AST-covB, also increasing coverage but with bet-
ter performance for the HWDep model.
Parsing results for the AST settings (Tables 4
and 5) confirm that it improves speed by an order of
magnitude over a baseline parser without AST. Per-
haps surprisingly, the number of parse failures de-
creases with AST in some cases. This is because the
parser prunes more aggressively as the search space
increases.5
2http://code.google.com/p/statopenccg
3Less than 2% of supertags are affected by this.
4The timing results reported differ from an earlier draft since
we used a different machine
5Hockenmaier and Steedman (2002) saw a similar effect.
1579
Expansion probability p(exp|P ) exp ? {leaf, unary, left-head, right-head}
Head probability p(H|P, exp) H is the head daughter
Non-head probability p(S|P, exp,H) S is the non-head daughter
Lexical probability p(w|P )
Table 1: Factorisation of the PCFG model. H ,P , and S are categories, and w is a word.
Expansion probability p(exp|P, cP#wP ) exp ? {leaf, unary, left-head, right-head}
Head probability p(H|P, exp, cP#wP ) H is the head daughter
Non-head probability p(S|P, exp,H#cP#wP ) S is the non-head daughter
Lexcat probability p(cS |S#P,H, S) p(cTOP |P=TOP )
Headword probability p(wS |cS#P,H, S,wP ) p(wTOP |cTOP )
Table 2: Headword dependency model factorisation, backoff levels are denoted by ?#? between conditioning variables:
A # B # C indicates that P? (. . . |A,B,C) is interpolated with P? (. . . |A,B), which is an interpolation of P? . . . |A,B)
and P? (. . . |A). Variables cP and wP represent, respectively, the head lexical category and headword of category P .
Condition Parameter Iteration 1 2 3 4 5 6
AST
? (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
AST-covA
? 0.075 0.03 0.01 0.005 0.001 0.0001
k 20 20 20 20 150 150
AST-covB
? 0.03 0.01 0.005 0.001 0.0001 0.0001
k 20 20 20 20 20 150
Table 3: Beam step function used for standard (AST) and high-coverage (AST-covA and AST-covB) supertagging.
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 290 6.6 26.2 5 86.4 86.5 86.5 77.2 77.3 77.3
PCFG (AST) 65 29.5 1.5 14 87.4 85.9 86.6 79.5 78.0 78.8
PCFG (AST-covA) 67 28.6 1.5 6 87.3 86.9 87.1 79.1 78.8 78.9
PCFG (AST-covB) 69 27.7 1.7 5 87.3 86.2 86.7 79.1 78.1 78.6
HWDep 1512 1.3 26.2 5 90.2 90.1 90.2 83.2 83.0 83.1
HWDep (AST) 133 14.4 1.5 16 89.8 88.0 88.9 82.6 80.9 81.8
HWDep (AST-covA) 139 13.7 1.5 9 89.8 88.3 89.0 82.6 81.1 81.9
HWDep (AST-covB) 155 12.3 1.7 7 90.1 88.7 89.4 83.0 81.8 82.4
Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative
CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall
(LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
3.2 Efficiency versus Accuracy
The most interesting result is the effect of the
speedup on accuracy. As shown in Table 6, the
vast majority of sentences are actually parsed with
a very tight supertagger beam, raising the question
of whether many higher-scoring parses are pruned.6
6Similar results are reported by Clark and Curran (2007).
Despite this, labeled F-score improves by up to 1.6
F-measure for the PCFG model, although it harms
accuracy for HWDep as expected.
In order to understand this effect, we filtered sec-
tion 00 to include only sentences of between 18
and 26 words (resulting in 610 sentences) for which
1580
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 326 7.4 25.7 29 85.9 85.4 85.7 76.6 76.2 76.4
PCFG (AST) 82 29.4 1.5 34 86.7 84.8 85.7 78.6 76.9 77.7
PCFG (AST-covA) 85 28.3 1.6 15 86.6 85.5 86.0 78.5 77.5 78.0
PCFG (AST-covB) 86 27.9 1.7 14 86.6 85.6 86.1 78.1 77.3 77.7
HWDep 1754 1.4 25.7 30 90.2 89.3 89.8 83.5 82.7 83.1
HWDep (AST) 167 14.4 1.5 27 89.5 87.6 88.5 82.3 80.6 81.5
HWDep (AST-covA) 177 13.6 1.6 14 89.4 88.1 88.8 82.2 81.1 81.7
HWDep (AST-covB) 188 12.8 1.7 14 89.7 88.5 89.1 82.5 81.4 82.0
Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser.
? Cats/word Parses %
0.075 1.33 1676 87.6
0.03 1.56 114 6.0
0.01 1.97 60 3.1
0.005 2.36 15 0.8
0.001k=150 3.84 32 1.7
Fail 16 0.9
Table 6: Breakdown of the number of sentences parsed
for the HWDep (AST) model (see Table 4) at each of
the supertagger beam levels from the most to the least
restrictive setting.
we can perform exhaustive search without pruning7,
and for which we could parse without failure at all
of the tested beam settings. We then measured the
log probability of the highest probability parse found
under a variety of beam settings, relative to the log
probability of the unpruned exact parse, along with
the labeled F-Score of the Viterbi parse under these
settings (Figure 2). The results show that PCFG ac-
tually finds worse results as it considers more of the
search space. In other words, the supertagger can ac-
tually ?fix? a bad parsing model by restricting it to
a small portion of the search space. With the more
accurate HWDep model, this does not appear to be
a problem and there is a clear opportunity for im-
provement by considering the larger search space.
The next question is whether we can exploit this
larger search space without paying as high a cost in
efficiency.
7The fact that only a subset of short sentences could be exhaus-
tively parsed demonstrates the need for efficient search algo-
rithms.
79	 ?
80	 ?
81	 ?
82	 ?
83	 ?
84	 ?
85	 ?
86	 ?
87	 ?
88	 ?
95.0	 ?
95.5	 ?
96.0	 ?
96.5	 ?
97.0	 ?
97.5	 ?
98.0	 ?
98.5	 ?
99.0	 ?
99.5	 ?
100.0	 ?
0.07
5	 ?
0.03
0	 ?
0.01
0	 ?
0.00
5	 ?
0.00
1	 ?
0.00
1???	 ? 0.00
01	 ?
0.00
01??
?	 ?
exac
t	 ?
Lab
ele
d	 ?F
-??sc
ore
	 ?
%	 ?o
f	 ?o
p?
ma
l	 ?Lo
g	 ?P
rob
abi
lity
	 ?
Supertagger	 ?beam	 ?
PCFG	 ?Log	 ?Probability	 ?
HWDep	 ?Log	 ?Probability	 ?
PCFG	 ?F-??score	 ?
HWDep	 ?F-??score	 ?
Figure 2: Log-probability of parses relative to exact solu-
tion vs. labelled F-score at each supertagging beam-level.
4 A* Parsing Experiments
To compare approaches, we extended our baseline
parser to support A* search. Following (Klein and
Manning, 2003) we restrict our experiments to sen-
tences on which we can perform exact search via us-
ing the same subset of section 00 as in ?3.2. Before
considering CPU time, we first evaluate the amount
of work done by the parser using three hardware-
independent metrics. We measure the number of
edges pushed (Pauls and Klein, 2009a) and edges
popped, corresponding to the insert/decrease-key
operations and remove operation of the priority
queue, respectively. Finally, we measure the num-
ber of traversals, which counts the number of edge
weights computed, regardless of whether the weight
is discarded due to the prior existence of a better
weight. This latter metric seems to be the most ac-
curate account of the work done by the parser.
Due to differences in the PCFG and HWDep mod-
els, we considered different A* variants: for the
PCFG model we use a simple A* with a precom-
1581
puted heuristic, while for the the more complex
HWDep model, we used a hierarchical A* algo-
rithm (Pauls and Klein, 2009a; Felzenszwalb and
McAllester, 2007) based on a simple grammar pro-
jection that we designed.
4.1 Hardware-Independent Results: PCFG
For the PCFG model, we compared three agenda-
based parsers: EXH prioritizes edges by their span
length, thereby simulating the exhaustive CKY algo-
rithm; NULL prioritizes edges by their inside proba-
bility; and SX is an A* parser that prioritizes edges
by their inside probability times an admissible out-
side probability estimate.8 We use the SX estimate
devised by Klein and Manning (2003) for CFG pars-
ing, where they found it offered very good perfor-
mance for relatively little computation. It gives a
bound on the outside probability of a nonterminal P
with i words to the right and j words to the left, and
can be computed from a grammar using a simple dy-
namic program.
The parsers are tested with and without adap-
tive supertagging where the former can be seen as
performing exact search (via A*) over the pruned
search space created by AST.
Table 7 shows that A* with the SX heuristic de-
creases the number of edges pushed by up to 39%
on the unpruned search space. Although encourag-
ing, this is not as impressive as the 95% speedup
obtained by Klein and Manning (2003) with this
heuristic on their CFG. On the other hand, the NULL
heuristic works better for CCG than for CFG, with
speedups of 29% and 11%, respectively. These re-
sults carry over to the AST setting which shows that
A* can improve search even on the highly pruned
search graph. Note that A* only saves work in the
final iteration of AST, since for earlier iterations it
must process the entire agenda to determine that
there is no spanning analysis.
Since there are many more categories in the CCG
grammar we might have expected the SX heuristic to
work better than for a CFG. Why doesn?t it? We can
measure how well a heuristic bounds the true cost in
8The NULL parser is a special case of A*, also called uni-
form cost search, which in the case of parsing corresponds to
Knuth?s algorithm (Knuth, 1977; Klein and Manning, 2001),
the extension of Dijkstra?s algorithm to hypergraphs.
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
1	 ? 4	 ? 7	 ? 10	 ? 13	 ? 16	 ? 19	 ? 22	 ? 25	 ?
Av
era
ge
	 ?Sl
ac
k	 ?
Outside	 ?Span	 ?
Figure 3: Average slack of the SX heuristic. The figure
aggregates the ratio of the difference between the esti-
mated outside cost and true outside costs relative to the
true cost across the development set.
terms of slack: the difference between the true and
estimated outside cost. Lower slack means that the
heuristic bounds the true cost better and guides us to
the exact solution more quickly. Figure 3 plots the
average slack for the SX heuristic against the num-
ber of words in the outside context. Comparing this
with an analysis of the same heuristic when applied
to a CFG by Klein and Manning (2003), we find that
it is less effective in our setting9. There is a steep
increase in slack for outside contexts with size more
than one. The main reason for this is because a sin-
gle word in the outside context is in many cases the
full stop at the end of the sentence, which is very pre-
dictable. However for longer spans the flexibility of
CCG to analyze spans in many different ways means
that the outside estimate for a nonterminal can be
based on many high probability outside derivations
which do not bound the true probability very well.
4.2 Hardware-Independent Results: HWDep
Lexicalization in the HWDep model makes the pre-
computed SX estimate impractical, so for this model
we designed two hierarchical A* (HA*) variants
based on simple grammar projections of the model.
The basic idea of HA* is to compute Viterbi in-
side probabilities using the easier-to-parse projected
9Specifically, we refer to Figure 9 of their paper which uses a
slightly different representation of estimate sharpness
1582
Parser Edges pushed Edges popped Traversals
Std % AST % Std % AST % Std % AST %
EXH 34 100 6.3 100 15.7 100 4.2 100 133.4 100 13.3 100
NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83
SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73
Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges
pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
grammar, use these to compute Viterbi outside prob-
abilities for the simple grammar, and then use these
as outside estimates for the true grammar; all com-
putations are prioritized in a single agenda follow-
ing the algorithm of Felzenszwalb and McAllester
(2007) and Pauls and Klein (2009a). We designed
two simple grammar projections, each simplifying
the HWDep model: PCFGProj completely re-
moves lexicalization and projects the grammar to
a PCFG, while as LexcatProj removes only the
headwords but retains the lexical categories.
Figure 4 compares exhaustive search, A* with no
heuristic (NULL), and HA*. For HA*, parsing ef-
fort is broken down into the different edge types
computed at each stage: We distinguish between the
work carried out to compute the inside and outside
edges of the projection, where the latter represent
the heuristic estimates, and finally, the work to com-
pute the edges of the target grammar. We find that
A* NULL saves about 44% of edges pushed which
makes it slightly more effective than for the PCFG
model. However, the effort to compute the gram-
mar projections outweighs their benefit. We suspect
that this is due to the large difference between the
target grammar and the projection: The PCFG pro-
jection is a simple grammar and so we improve the
probability of a traversal less often than in the target
grammar.
The Lexcat projection performs worst, for two
reasons. First, the projection requires about as much
work to compute as the target grammar without a
heuristic (NULL). Second, the projection itself does
not save a large amount of work as can be seen in
the statistics for the target grammar.
5 CPU Timing Experiments
Hardware-independent metrics are useful for under-
standing agenda-based algorithms, but what we ac-
tually care about is CPU time. We were not aware of
any past work that measures A* parsers in terms of
CPU time, but as this is the real objective we feel that
experiments of this type are important. This is espe-
cially true in real implementations because the sav-
ings in edges processed by an agenda parser come at
a cost: operations on the priority queue data struc-
ture can add significant runtime.
Timing experiments of this type are very
implementation-dependent, so we took care to im-
plement the algorithms as cleanly as possible and
to reuse as much of the existing parser code as we
could. An important implementation decision for
agenda-based algorithms is the data structure used
to implement the priority queue. Preliminary experi-
ments showed that a Fibonacci heap implementation
outperformed several alternatives: Brodal queues
(Brodal, 1996), binary heaps, binomial heaps, and
pairing heaps.10
We carried out timing experiments on the best A*
parsers for each model (SX and NULL for PCFG and
HWDep, respectively), comparing them with our
CKY implementation and the agenda-based CKY
simulation EXH; we used the same data as in ?3.2.
Table 8 presents the cumulative running times with
and without adaptive supertagging average over ten
runs, while Table 9 reports F-scores.
The results (Table 8) are striking. Although the
timing results of the agenda-based parsers track the
hardware-independent metrics, they start at a signif-
icant disadvantage to exhaustive CKY with a sim-
ple control loop. This is most evident when looking
at the timing results for EXH, which in the case of
the full PCFG model requires more than twice the
time than the CKY algorithm that it simulates. A*
10We used the Fibonacci heap implementation at
http://www.jgrapht.org
1583
Figure 4: Comparsion between a CKY simulation (EXH), A* with no heuristic (NULL), hierarchical A* (HA*) using
two grammar projections for standard search (left) and AST (right). The breakdown of the inside/outside edges for the
grammar projection as well as the target grammar shows that the projections, serving as the heuristic estimates for the
target grammar, are costly to compute.
Standard AST
PCFG HWDep PCFG HWDep
CKY 536 24489 34 143
EXH 1251 26889 41 155
A* NULL 1032 21830 36 121
A* SX 889 - 34 -
Table 8: Parsing time in seconds of CKY and agenda-
based parsers with and without adaptive supertagging.
Standard AST
PCFG HWDep PCFG HWDep
CKY 80.4 85.5 81.7 83.8
EXH 79.4 85.5 80.3 83.8
A* NULL 79.6 85.5 80.7 83.8
A* SX 79.4 - 80.4 -
Table 9: Labelled F-score of exact CKY and agenda-
based parsers with/without adaptive supertagging. All
parses have the same probabilities, thus variances are due
to implementation-dependent differences in tiebreaking.
makes modest CPU-time improvements in parsing
the full space of the HWDep model. Although this
decreases the time required to obtain the highest ac-
curacy, it is still a substantial tradeoff in speed com-
pared with AST.
On the other hand, the AST tradeoff improves sig-
nificantly: by combining AST with A* we observe
a decrease in running time of 15% for the A* NULL
parser of the HWDep model over CKY. As the CKY
baseline with AST is very strong, this result shows
that A* holds real promise for CCG parsing.
6 Conclusion and Future Work
Adaptive supertagging is a strong technique for ef-
ficient CCG parsing. Our analysis confirms tremen-
dous speedups, and shows that for weak models, it
can even result in improved accuracy. However, for
better models, the efficiency gains of adaptive su-
pertagging come at the cost of accuracy. One way to
look at this is that the supertagger has good precision
with respect to the parser?s search space, but low re-
call. For instance, we might combine both parsing
and supertagging models in a principled way to ex-
ploit these observations, eg. by making the supertag-
ger output a soft constraint on the parser rather than
a hard constraint. Principled, efficient search algo-
rithms will be crucial to such an approach.
To our knowledge, we are the first to measure
A* parsing speed both in terms of running time and
commonly used hardware-independent metrics. It
is clear from our results that the gains from A* do
not come as easily for CCG as for CFG, and that
agenda-based algorithms like A* must make very
large reductions in the number of edges processed
to result in realtime savings, due to the added ex-
pense of keeping a priority queue. However, we
1584
have shown that A* can yield real improvements
even over the highly optimized technique of adaptive
supertagging: in this pruned search space, a 44%
reduction in the number of edges pushed results in
a 15% speedup in CPU time. Furthermore, just as
A* can be combined with adaptive supertagging, it
should also combine easily with other search-space
pruning methods, such as those of Djordjevic et
al. (2007), Kummerfeld et al (2010), Zhang et al
(2010) and Roark and Hollingshead (2009). In fu-
ture work we plan to examine better A* heuristics
for CCG, and to look at principled approaches to
combine the strengths of A*, adaptive supertagging,
and other techniques to the best advantage.
Acknowledgements
We would like to thank Prachya Boonkwan, Juri
Ganitkevitch, Philipp Koehn, Tom Kwiatkowski,
Matt Post, Mark Steedman, Emily Thomforde, and
Luke Zettlemoyer for helpful discussion related to
this work and comments on previous drafts; Julia
Hockenmaier for furnishing us with her parser; and
the anonymous reviewers for helpful commentary.
We also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); the EuroMatrixPlus project
funded by the European Commission, 7th Frame-
work Programme (Lopez); and the resources pro-
vided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238?265, June.
G. S. Brodal. 1996. Worst-case efficient priority queues.
In Proc. of SODA, pages 52?58.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In Proc.
of COLING.
S. Clark and J. R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493?552.
S. Clark and J. Hockenmaier. 2002. Evaluating a wide-
coverage CCG parser. In Proc. of LREC Beyond Par-
seval Workshop, pages 60?66.
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In Proc. of TAG+6, pages 19?24.
B. Djordjevic, J. R. Curran, and S. Clark. 2007. Improv-
ing the efficiency of a wide-coverage CCG parser. In
Proc. of IWPT.
P. F. Felzenszwalb and D. McAllester. 2007. The Gener-
alized A* Architecture. In Journal of Artificial Intelli-
gence Research, volume 29, pages 153?190.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
paths. Transactions on Systems Science and Cybernet-
ics, 4, Jul.
J. Hockenmaier and M. Steedman. 2002. Generative
models for statistical parsing with Combinatory Cat-
egorial Grammar. In Proc. of ACL, pages 335?342.
Association for Computational Linguistics.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
D. Klein and C. D. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
D. Klein and C. D. Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proc. of HLT-NAACL,
pages 119?126, May.
D. E. Knuth. 1977. A generalization of Dijkstra?s algo-
rithm. Information Processing Letters, 6:1?5.
J. K. Kummerfeld, J. Roesner, T. Dawborn, J. Haggerty,
J. R. Curran, and S. Clark. 2010. Faster parsing by
supertagger adaptation. In Proc. of ACL.
A. Pauls and D. Klein. 2009a. Hierarchical search for
parsing. In Proc. of HLT-NAACL, pages 557?565,
June.
A. Pauls and D. Klein. 2009b. k-best A* Parsing. In
Proc. of ACL-IJCNLP, ACL-IJCNLP ?09, pages 958?
966.
B. Roark and K. Hollingshead. 2009. Linear complexity
context-free parsing pipelines via chart constraints. In
Proc. of HLT-NAACL.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
Y. Zhang, B.-G. Ahn, S. Clark, C. V. Wyk, J. R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. of COLING.
1585
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1374?1383,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Dirt Cheap Web-Scale Parallel Text from the Common Crawl
Jason R. Smith1,2
jsmith@cs.jhu.edu
Philipp Koehn3
pkoehn@inf.ed.ac.uk
Herve Saint-Amand3
herve@saintamh.org
Chris Callison-Burch1,2,5
ccb@cs.jhu.edu ?
Magdalena Plamada4
plamada@cl.uzh.ch
Adam Lopez1,2
alopez@cs.jhu.edu
1Department of Computer Science, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3School of Informatics, University of Edinburgh
4Institute of Computational Linguistics, University of Zurich
5Computer and Information Science Department, University of Pennsylvania
Abstract
Parallel text is the fuel that drives modern
machine translation systems. The Web is a
comprehensive source of preexisting par-
allel text, but crawling the entire web is
impossible for all but the largest compa-
nies. We bring web-scale parallel text to
the masses by mining the Common Crawl,
a public Web crawl hosted on Amazon?s
Elastic Cloud. Starting from nothing more
than a set of common two-letter language
codes, our open-source extension of the
STRAND algorithm mined 32 terabytes of
the crawl in just under a day, at a cost of
about $500. Our large-scale experiment
uncovers large amounts of parallel text in
dozens of language pairs across a variety
of domains and genres, some previously
unavailable in curated datasets. Even with
minimal cleaning and filtering, the result-
ing data boosts translation performance
across the board for five different language
pairs in the news domain, and on open do-
main test sets we see improvements of up
to 5 BLEU. We make our code and data
available for other researchers seeking to
mine this rich new data resource.1
1 Introduction
A key bottleneck in porting statistical machine
translation (SMT) technology to new languages
and domains is the lack of readily available paral-
lel corpora beyond curated datasets. For a handful
of language pairs, large amounts of parallel data
?This research was conducted while Chris Callison-
Burch was at Johns Hopkins University.
1github.com/jrs026/CommonCrawlMiner
are readily available, ordering in the hundreds of
millions of words for Chinese-English and Arabic-
English, and in tens of millions of words for many
European languages (Koehn, 2005). In each case,
much of this data consists of government and news
text. However, for most language pairs and do-
mains there is little to no curated parallel data
available. Hence discovery of parallel data is an
important first step for translation between most
of the world?s languages.
The Web is an important source of parallel
text. Many websites are available in multiple
languages, and unlike other potential sources?
such as multilingual news feeds (Munteanu and
Marcu, 2005) or Wikipedia (Smith et al, 2010)?
it is common to find document pairs that are di-
rect translations of one another. This natural par-
allelism simplifies the mining task, since few re-
sources or existing corpora are needed at the outset
to bootstrap the extraction process.
Parallel text mining from the Web was origi-
nally explored by individuals or small groups of
academic researchers using search engines (Nie
et al, 1999; Chen and Nie, 2000; Resnik, 1999;
Resnik and Smith, 2003). However, anything
more sophisticated generally requires direct access
to web-crawled documents themselves along with
the computing power to process them. For most
researchers, this is prohibitively expensive. As a
consequence, web-mined parallel text has become
the exclusive purview of large companies with the
computational resources to crawl, store, and pro-
cess the entire Web.
To put web-mined parallel text back in the
hands of individual researchers, we mine parallel
text from the Common Crawl, a regularly updated
81-terabyte snapshot of the public internet hosted
1374
on Amazon?s Elastic Cloud (EC2) service.2 Us-
ing the Common Crawl completely removes the
bottleneck of web crawling, and makes it possi-
ble to run algorithms on a substantial portion of
the web at very low cost. Starting from nothing
other than a set of language codes, our extension
of the STRAND algorithm (Resnik and Smith,
2003) identifies potentially parallel documents us-
ing cues from URLs and document content (?2).
We conduct an extensive empirical exploration of
the web-mined data, demonstrating coverage in
a wide variety of languages and domains (?3).
Even without extensive pre-processing, the data
improves translation performance on strong base-
line news translation systems in five different lan-
guage pairs (?4). On general domain and speech
translation tasks where test conditions substan-
tially differ from standard government and news
training text, web-mined training data improves
performance substantially, resulting in improve-
ments of up to 1.5 BLEU on standard test sets, and
5 BLEU on test sets outside of the news domain.
2 Mining the Common Crawl
The Common Crawl corpus is hosted on Ama-
zon?s Simple Storage Service (S3). It can be
downloaded to a local cluster, but the transfer cost
is prohibitive at roughly 10 cents per gigabyte,
making the total over $8000 for the full dataset.3
However, it is unnecessary to obtain a copy of the
data since it can be accessed freely from Amazon?s
Elastic Compute Cloud (EC2) or Elastic MapRe-
duce (EMR) services. In our pipeline, we per-
form the first step of identifying candidate docu-
ment pairs using Amazon EMR, download the re-
sulting document pairs, and perform the remain-
ing steps on our local cluster. We chose EMR be-
cause our candidate matching strategy fit naturally
into the Map-Reduce framework (Dean and Ghe-
mawat, 2004).
Our system is based on the STRAND algorithm
(Resnik and Smith, 2003):
1. Candidate pair selection: Retrieve candidate
document pairs from the CommonCrawl cor-
pus.
2. Structural Filtering:
(a) Convert the HTML of each document
2commoncrawl.org
3http://aws.amazon.com/s3/pricing/
into a sequence of start tags, end tags,
and text chunks.
(b) Align the linearized HTML of candidate
document pairs.
(c) Decide whether to accept or reject each
pair based on features of the alignment.
3. Segmentation: For each text chunk, perform
sentence and word segmentation.
4. Sentence Alignment: For each aligned pair of
text chunks, perform the sentence alignment
method of Gale and Church (1993).
5. Sentence Filtering: Remove sentences that
appear to be boilerplate.
Candidate Pair Selection We adopt a strategy
similar to that of Resnik and Smith (2003) for find-
ing candidate parallel documents, adapted to the
parallel architecture of Map-Reduce.
The mapper operates on each website entry in
the CommonCrawl data. It scans the URL string
for some indicator of its language. Specifically,
we check for:
1. Two/three letter language codes (ISO-639).
2. Language names in English and in the lan-
guage of origin.
If either is present in a URL and surrounded by
non-alphanumeric characters, the URL is identi-
fied as a potential match and the mapper outputs
a key value pair in which the key is the original
URL with the matching string replaced by *, and
the value is the original URL, language name, and
full HTML of the page. For example, if we en-
counter the URL www.website.com/fr/, we
output the following.
? Key: www.website.com/*/
? Value: www.website.com/fr/, French,
(full website entry)
The reducer then receives all websites mapped
to the same ?language independent? URL. If two
or more websites are associated with the same key,
the reducer will output all associated values, as
long as they are not in the same language, as de-
termined by the language identifier in the URL.
This URL-based matching is a simple and in-
expensive solution to the problem of finding can-
didate document pairs. The mapper will discard
1375
most, and neither the mapper nor the reducer do
anything with the HTML of the documents aside
from reading and writing them. This approach is
very simple and likely misses many good potential
candidates, but has the advantage that it requires
no information other than a set of language codes,
and runs in time roughly linear in the size of the
dataset.
Structural Filtering A major component of the
STRAND system is the alignment of HTML docu-
ments. This alignment is used to determine which
document pairs are actually parallel, and if they
are, to align pairs of text blocks within the docu-
ments.
The first step of structural filtering is to lin-
earize the HTML. This means converting its DOM
tree into a sequence of start tags, end tags, and
chunks of text. Some tags (those usually found
within text, such as ?font? and ?a?) are ignored
during this step. Next, the tag/chunk sequences
are aligned using dynamic programming. The ob-
jective of the alignment is to maximize the number
of matching items.
Given this alignment, Resnik and Smith (2003)
define a small set of features which indicate the
alignment quality. They annotated a set of docu-
ment pairs as parallel or non-parallel, and trained
a classifier on this data. We also annotated 101
Spanish-English document pairs in this way and
trained a maximum entropy classifier. However,
even when using the best performing subset of fea-
tures, the classifier only performed as well as a
naive classifier which labeled every document pair
as parallel, in both accuracy and F1. For this rea-
son, we excluded the classifier from our pipeline.
The strong performance of the naive baseline was
likely due to the unbalanced nature of the anno-
tated data? 80% of the document pairs that we
annotated were parallel.
Segmentation The text chunks from the previ-
ous step may contain several sentences, so before
the sentence alignment step we must perform sen-
tence segmentation. We use the Punkt sentence
splitter from NLTK (Loper and Bird, 2002) to
perform both sentence and word segmentation on
each text chunk.
Sentence Alignment For each aligned text
chunk pair, we perform sentence alignment using
the algorithm of Gale and Church (1993).
Sentence Filtering Since we do not perform any
boilerplate removal in earlier steps, there are many
sentence pairs produced by the pipeline which
contain menu items or other bits of text which are
not useful to an SMT system. We avoid perform-
ing any complex boilerplate removal and only re-
move segment pairs where either the source and
target text are identical, or where the source or
target segments appear more than once in the ex-
tracted corpus.
3 Analysis of the Common Crawl Data
We ran our algorithm on the 2009-2010 version
of the crawl, consisting of 32.3 terabytes of data.
Since the full dataset is hosted on EC2, the only
cost to us is CPU time charged by Amazon, which
came to a total of about $400, and data stor-
age/transfer costs for our output, which came to
roughly $100. For practical reasons we split the
run into seven subsets, on which the full algo-
rithm was run independently. This is different
from running a single Map-Reduce job over the
entire dataset, since websites in different subsets
of the data cannot be matched. However, since
the data is stored as it is crawled, it is likely that
matching websites will be found in the same split
of the data. Table 1 shows the amount of raw par-
allel data obtained for a large selection of language
pairs.
As far as we know, ours is the first system built
to mine parallel text from the Common Crawl.
Since the resource is new, we wanted to under-
stand the quantity, quality, and type of data that
we are likely to obtain from it. To this end, we
conducted a number of experiments to measure
these features. Since our mining heuristics are
very simple, these results can be construed as a
lower bound on what is actually possible.
3.1 Recall Estimates
Our first question is about recall: of all the pos-
sible parallel text that is actually available on the
Web, how much does our algorithm actually find
in the Common Crawl? Although this question
is difficult to answer precisely, we can estimate
an answer by comparing our mined URLs against
a large collection of previously mined URLs that
were found using targeted techniques: those in the
French-English Gigaword corpus (Callison-Burch
et al, 2011).
We found that 45% of the URL pairs would
1376
French German Spanish Russian Japanese Chinese
Segments 10.2M 7.50M 5.67M 3.58M 1.70M 1.42M
Source Tokens 128M 79.9M 71.5M 34.7M 9.91M 8.14M
Target Tokens 118M 87.5M 67.6M 36.7M 19.1M 14.8M
Arabic Bulgarian Czech Korean Tamil Urdu
Segments 1.21M 909K 848K 756K 116K 52.1K
Source Tokens 13.1M 8.48M 7.42M 6.56M 1.01M 734K
Target Tokens 13.5M 8.61M 8.20M 7.58M 996K 685K
Bengali Farsi Telugu Somali Kannada Pashto
Segments 59.9K 44.2K 50.6K 52.6K 34.5K 28.0K
Source Tokens 573K 477K 336K 318K 305K 208K
Target Tokens 537K 459K 358K 325K 297K 218K
Table 1: The amount of parallel data mined from CommonCrawl for each language paired with English.
Source tokens are counts of the foreign language tokens, and target tokens are counts of the English
language tokens.
have been discovered by our heuristics, though we
actually only find 3.6% of these URLs in our out-
put.4 If we had included ?f? and ?e? as identi-
fiers for French and English respectively, coverage
of the URL pairs would increase to 74%. How-
ever, we chose not to include single letter identi-
fiers in our experiments due to the high number of
false positives they generated in preliminary ex-
periments.
3.2 Precision Estimates
Since our algorithms rely on cues that are mostly
external to the contents of the extracted data
and have no knowledge of actual languages, we
wanted to evaluate the precision of our algorithm:
how much of the mined data actually consists of
parallel sentences?
To measure this, we conducted a manual anal-
ysis of 200 randomly selected sentence pairs for
each of three language pairs. The texts are het-
erogeneous, covering several topical domains like
tourism, advertising, technical specifications, fi-
nances, e-commerce and medicine. For German-
English, 78% of the extracted data represent per-
fect translations, 4% are paraphrases of each other
(convey a similar meaning, but cannot be used
for SMT training) and 18% represent misalign-
ments. Furthermore, 22% of the true positives
are potentially machine translations (judging by
the quality), whereas in 13% of the cases one of
the sentences contains additional content not ex-
4The difference is likely due to the coverage of the Com-
monCrawl corpus.
pressed in the other. As for the false positives,
13.5% of them have either the source or target
sentence in the wrong language, and the remain-
ing ones representing failures in the alignment
process. Across three languages, our inspection
revealed that around 80% of randomly sampled
data appeared to contain good translations (Table
2). Although this analysis suggests that language
identification and SMT output detection (Venu-
gopal et al, 2011) may be useful additions to the
pipeline, we regard this as reasonably high preci-
sion for our simple algorithm.
Language Precision
Spanish 82%
French 81%
German 78%
Table 2: Manual evaluation of precision (by sen-
tence pair) on the extracted parallel data for Span-
ish, French, and German (paired with English).
In addition to the manual evaluation of preci-
sion, we applied language identification to our
extracted parallel data for several additional lan-
guages. We used the ?langid.py? tool (Lui and
Baldwin, 2012) at the segment level, and report the
percentage of sentence pairs where both sentences
were recognized as the correct language. Table 3
shows our results. Comparing against our man-
ual evaluation from Table 2, it appears that many
sentence pairs are being incorrectly judged as non-
parallel. This is likely because language identifi-
cation tends to perform poorly on short segments.
1377
French German Spanish Arabic
63% 61% 58% 51%
Chinese Japanese Korean Czech
50% 48% 48% 47%
Russian Urdu Bengali Tamil
44% 31% 14% 12%
Kannada Telugu Kurdish
12% 6.3% 2.9%
Table 3: Automatic evaluation of precision
through language identification for several lan-
guages paired with English.
3.3 Domain Name and Topic Analysis
Although the above measures tell us something
about how well our algorithms perform in aggre-
gate for specific language pairs, we also wondered
about the actual contents of the data. A major
difficulty in applying SMT even on languages for
which we have significant quantities of parallel
text is that most of that parallel text is in the news
and government domains. When applied to other
genres, such systems are notoriously brittle. What
kind of genres are represented in the Common
Crawl data?
We first looked at the domain names which con-
tributed the most data. Table 4 gives the top five
domains by the number of tokens. The top two do-
main names are related to travel, and they account
for about 10% of the total data.
We also applied Latent Dirichlet Allocation
(LDA; Blei et al, 2003) to learn a distribution over
latent topics in the extracted data, as this is a pop-
ular exploratory data analysis method. In LDA
a topic is a unigram distribution over words, and
each document is modeled as a distribution over
topics. To create a set of documents from the ex-
tracted CommonCrawl data, we took the English
side of the extracted parallel segments for each
URL in the Spanish-English portion of the data.
This gave us a total of 444, 022 documents. In
our first experiment, we used the MALLET toolkit
(McCallum, 2002) to generate 20 topics, which
are shown in Table 5.
Some of the topics that LDA finds cor-
respond closely with specific domains,
such as topics 1 (blingee.com) and 2
(opensubtitles.org). Several of the topics
correspond to the travel domain. Foreign stop
words appear in a few of the topics. Since our sys-
tem does not include any language identification,
this is not surprising.5 However it does suggest an
avenue for possible improvement.
In our second LDA experiment, we compared
our extracted CommonCrawl data with Europarl.
We created a set of documents from both Com-
monCrawl and Europarl, and again used MAL-
LET to generate 100 topics for this data.6 We then
labeled each document by its most likely topic (as
determined by that topic?s mixture weights), and
counted the number of documents from Europarl
and CommonCrawl for which each topic was most
prominent. While this is very rough, it gives some
idea of where each topic is coming from. Table 6
shows a sample of these topics.
In addition to exploring topics in the datasets,
we also performed additional intrinsic evaluation
at the domain level, choosing top domains for
three language pairs. We specifically classified
sentence pairs as useful or boilerplate (Table 7).
Among our observations, we find that commer-
cial websites tend to contain less boilerplate ma-
terial than encyclopedic websites, and that the ra-
tios tend to be similar across languages in the same
domain.
FR ES DE
www.booking.com 52% 71% 52%
www.hotel.info 34% 44% -
memory-alpha.org 34% 25% 55%
Table 7: Percentage of useful (non-boilerplate)
sentences found by domain and language pair.
hotel.info was not found in our German-
English data.
4 Machine Translation Experiments
For our SMT experiments, we use the Moses
toolkit (Koehn et al, 2007). In these experiments,
a baseline system is trained on an existing parallel
corpus, and the experimental system is trained on
the baseline corpus plus the mined parallel data.
In all experiments we include the target side of the
mined parallel data in the language model, in order
to distinguish whether results are due to influences
from parallel or monolingual data.
5We used MALLET?s stop word removal, but that is only
for English.
6Documents were created from Europarl by taking
?SPEAKER? tags as document boundaries, giving us
208,431 documents total.
1378
Genre Domain Pages Segments Source Tokens Target Tokens
Total 444K 5.67M 71.5M 67.5M
travel www.booking.com 13.4K 424K 5.23M 5.14M
travel www.hotel.info 9.05K 156K 1.93M 2.13M
government www.fao.org 2.47K 60.4K 1.07M 896K
religious scriptures.lds.org 7.04K 47.2K 889K 960K
political www.amnesty.org 4.83K 38.1K 641K 548K
Table 4: The top five domains from the Spanish-English portion of the data. The domains are ranked by
the combined number of source and target tokens.
Index Most Likely Tokens
1 glitter graphics profile comments share love size girl friends happy blingee cute anime twilight sexy emo
2 subtitles online web users files rar movies prg akas dwls xvid dvdrip avi results download eng cd movie
3 miles hotels city search hotel home page list overview select tokyo discount destinations china japan
4 english language students details skype american university school languages words england british college
5 translation japanese english chinese dictionary french german spanish korean russian italian dutch
6 products services ni system power high software design technology control national applications industry
7 en de el instructions amd hyper riv saab kfreebsd poland user fr pln org wikimedia pl commons fran norway
8 information service travel services contact number time account card site credit company business terms
9 people time life day good years work make god give lot long world book today great year end things
10 show km map hotels de hotel beach spain san italy resort del mexico rome portugal home santa berlin la
11 rotary international world club korea foundation district business year global hong kong president ri
12 hotel reviews stay guest rooms service facilities room smoking submitted customers desk score united hour
13 free site blog views video download page google web nero internet http search news links category tv
14 casino game games play domaine ago days music online poker free video film sports golf live world tags bet
15 water food attribution health mango japan massage medical body baby natural yen commons traditional
16 file system windows server linux installation user files set debian version support program install type
17 united kingdom states america house london street park road city inn paris york st france home canada
18 km show map hotels hotel featured search station museum amsterdam airport centre home city rue germany
19 hotel room location staff good breakfast rooms friendly nice clean great excellent comfortable helpful
20 de la en le el hotel es het del und die il est der les des das du para
Table 5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely
tokens.
4.1 News Domain Translation
Our first set of experiments are based on systems
built for the 2012 Workshop on Statistical Ma-
chine Translation (WMT) (Callison-Burch et al,
2012) using all available parallel and monolingual
data for that task, aside from the French-English
Gigaword. In these experiments, we use 5-gram
language models when the target language is En-
glish or German, and 4-gram language models for
French and Spanish. We tune model weights using
minimum error rate training (MERT; Och, 2003)
on the WMT 2008 test data. The results are given
in Table 8. For all language pairs and both test
sets (WMT 2011 and WMT 2012), we show an
improvement of around 0.5 BLEU.
We also included the French-English Gigaword
in separate experiments given in Table 9, and Table
10 compares the sizes of the datasets used. These
results show that even on top of a different, larger
parallel corpus mined from the web, adding Com-
monCrawl data still yields an improvement.
4.2 Open Domain Translation
A substantial appeal of web-mined parallel data
is that it might be suitable to translation of do-
mains other than news, and our topic modeling
analysis (?3.3) suggested that this might indeed be
the case. We therefore performed an additional
set of experiments for Spanish-English, but we
include test sets from outside the news domain.
1379
Europarl CommonCrawl Most Likely Tokens
9 2975 hair body skin products water massage treatment natural oil weight acid plant
2 4383 river mountain tour park tours de day chile valley ski argentina national peru la
8 10377 ford mercury dealer lincoln amsterdam site call responsible affiliates displayed
7048 675 market services european competition small public companies sector internal
9159 1359 time president people fact make case problem clear good put made years situation
13053 849 commission council european parliament member president states mr agreement
1660 5611 international rights human amnesty government death police court number torture
1617 4577 education training people cultural school students culture young information
Table 6: A sample of topics along with the number of Europarl and CommonCrawl documents where
they are the most likely topic in the mixture. We include topics that are mostly found in Europarl or
CommonCrawl, and some that are somewhat prominent in both.
WMT 11 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 30.46 29.96 30.79 32.41 16.12
+Web Data 30.92 30.51 31.05 32.89 16.74
WMT 12 FR-EN EN-FR ES-EN EN-ES EN-DE
Baseline 29.25 27.92 32.80 32.83 16.61
+Web Data 29.82 28.22 33.39 33.41 17.30
Table 8: BLEU scores for several language pairs before and after adding the mined parallel data to
systems trained on data from WMT data.
WMT 11 FR-EN EN-FR
Baseline 30.96 30.69
+Web Data 31.24 31.17
WMT 12 FR-EN EN-FR
Baseline 29.88 28.50
+Web Data 30.08 28.76
Table 9: BLEU scores for French-English and
English-French before and after adding the mined
parallel data to systems trained on data from
WMT data including the French-English Giga-
word (Callison-Burch et al, 2011).
For these experiments, we also include training
data mined from Wikipedia using a simplified ver-
sion of the sentence aligner described by Smith
et al (2010), in order to determine how the ef-
fect of such data compares with the effect of web-
mined data. The baseline system was trained using
only the Europarl corpus (Koehn, 2005) as par-
allel data, and all experiments use the same lan-
guage model trained on the target sides of Eu-
roparl, the English side of all linked Spanish-
English Wikipedia articles, and the English side
of the mined CommonCrawl data. We use a 5-
gram language model and tune using MERT (Och,
Corpus EN-FR EN-ES EN-DE
News Commentary 2.99M 3.43M 3.39M
Europarl 50.3M 49.2M 47.9M
United Nations 316M 281M -
FR-EN Gigaword 668M - -
CommonCrawl 121M 68.8M 88.4M
Table 10: The size (in English tokens) of the train-
ing corpora used in the SMT experiments from Ta-
bles 8 and 9 for each language pair.
2003) on the WMT 2009 test set.
Unfortunately, it is difficult to obtain meaning-
ful results on some open domain test sets such as
the Wikipedia dataset used by Smith et al (2010).
Wikipedia copied across the public internet, and
we did not have a simple way to filter such data
from our mined datasets.
We therefore considered two tests that were
less likely to be problematic. The Tatoeba cor-
pus (Tiedemann, 2009) is a collection of example
sentences translated into many languages by vol-
unteers. The front page of tatoeba.org was
discovered by our URL matching heuristics, but
we excluded any sentence pairs that were found in
the CommonCrawl data from this test set.
1380
The second dataset is a set of crowdsourced
translation of Spanish speech transcriptions from
the Spanish Fisher corpus.7 As part of a re-
search effort on cross-lingual speech applications,
we obtained English translations of the data using
Amazon Mechanical Turk, following a protocol
similar to one described by Zaidan and Callison-
Burch (2011): we provided clear instructions,
employed several quality control measures, and
obtained redundant translations of the complete
dataset (Lopez et al, 2013). The advantage of
this data for our open domain translation test is
twofold. First, the Fisher dataset consists of con-
versations in various Spanish dialects on a wide
variety of prompted topics. Second, because we
obtained the translations ourselves, we could be
absolutely assured that they did not appear in some
form anywhere on the Web, making it an ideal
blind test.
WMT10 Tatoeba Fisher
Europarl 89/72/46/20 94/75/45/18 87/69/39/13
+Wiki 92/78/52/24 96/80/50/21 91/75/44/15
+Web 96/82/56/27 99/88/58/26 96/83/51/19
+Both 96/84/58/29 99/89/60/27 96/83/52/20
Table 11: n-gram coverage percentages (up to 4-
grams) of the source side of our test sets given our
different parallel training corpora computed at the
type level.
WMT10 Tatoeba Fisher
Europarl 27.21 36.13 46.32
+Wiki 28.03 37.82 49.34
+Web 28.50 41.07 51.13
+Both 28.74 41.12 52.23
Table 12: BLEU scores for Spanish-English be-
fore and after adding the mined parallel data to a
baseline Europarl system.
We used 1000 sentences from each of the
Tatoeba and Fisher datasets as test. For com-
parison, we also test on the WMT 2010 test
set (Callison-Burch et al, 2010). Following
Munteanu and Marcu (2005), we show the n-gram
coverage of each corpus (percentage of n-grams
from the test corpus which are also found in the
training corpora) in Table 11. Table 12 gives
end-to-end results, which show a strong improve-
ment on the WMT test set (1.5 BLEU), and larger
7Linguistic Data Consortium LDC2010T04.
improvements on Tatoeba and Fisher (almost 5
BLEU).
5 Discussion
Web-mined parallel texts have been an exclusive
resource of large companies for several years.
However, when web-mined parallel text is avail-
able to everyone at little or no cost, there will
be much greater potential for groundbreaking re-
search to come from all corners. With the advent
of public services such as Amazon Web Services
and the Common Crawl, this may soon be a re-
ality. As we have shown, it is possible to obtain
parallel text for many language pairs in a variety
of domains very cheaply and quickly, and in suf-
ficient quantity and quality to improve statistical
machine translation systems. However, our effort
has merely scratched the surface of what is pos-
sible with this resource. We will make our code
and data available so that others can build on these
results.
Because our system is so simple, we believe that
our results represent lower bounds on the gains
that should be expected in performance of systems
previously trained only on curated datasets. There
are many possible means through which the sys-
tem could be improved, including more sophisti-
cated techniques for identifying matching URLs,
better alignment, better language identification,
better filtering of data, and better exploitation of
resulting cross-domain datasets. Many of the com-
ponents of our pipeline were basic, leaving consid-
erable room for improvement. For example, the
URL matching strategy could easily be improved
for a given language pair by spending a little time
crafting regular expressions tailored to some ma-
jor websites. Callison-Burch et al (2011) gathered
almost 1 trillion tokens of French-English parallel
data this way. Another strategy for mining parallel
webpage pairs is to scan the HTML for links to the
same page in another language (Nie et al, 1999).
Other, more sophisticated techniques may also
be possible. Uszkoreit et al (2010), for ex-
ample, translated all non-English webpages into
English using an existing translation system and
used near-duplicate detection methods to find can-
didate parallel document pairs. Ture and Lin
(2012) had a similar approach for finding paral-
lel Wikipedia documents by using near-duplicate
detection, though they did not need to apply a full
translation system to all non-English documents.
1381
Instead, they represented documents in bag-of-
words vector space, and projected non-English
document vectors into the English vector space us-
ing the translation probabilities of a word align-
ment model. By comparison, one appeal of our
simple approach is that it requires only a table
of language codes. However, with this system
in place, we could obtain enough parallel data to
bootstrap these more sophisticated approaches.
It is also compelling to consider ways in which
web-mined data obtained from scratch could be
used to bootstrap other mining approaches. For
example, Smith et al (2010) mine parallel sen-
tences from comparable documents in Wikipedia,
demonstrating substantial gains on open domain
translation. However, their approach required seed
parallel data to learn models used in a classifier.
We imagine a two-step process, first obtaining par-
allel data from the web, followed by comparable
data from sources such as Wikipedia using mod-
els bootstrapped from the web-mined data. Such a
process could be used to build translation systems
for new language pairs in a very short period of
time, hence fulfilling one of the original promises
of SMT.
Acknowledgements
Thanks to Ann Irvine, Jonathan Weese, and our
anonymous reviewers from NAACL and ACL for
comments on previous drafts. The research lead-
ing to these results has received funding from the
European Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 288487
(MosesCore). This research was partially funded
by the Johns Hopkins University Human Lan-
guage Technology Center of Excellence, and by
gifts from Google and Microsoft.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 17?53. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F. Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ?11, pages 22?64. Associ-
ation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Jiang Chen and Jian-Yun Nie. 2000. Parallel web text
mining for cross-language ir. In IN IN PROC. OF
RIAO, pages 62?77.
J. Dean and S. Ghemawat. 2004. Mapreduce: simpli-
fied data processing on large clusters. In Proceed-
ings of the 6th conference on Symposium on Opeart-
ing Systems Design & Implementation-Volume 6,
pages 10?10. USENIX Association.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Comput. Linguist., 19:75?102, March.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180. Association for Computa-
tional Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT summit, volume 5.
Edward Loper and Steven Bird. 2002. Nltk: the natu-
ral language toolkit. In Proceedings of the ACL-02
Workshop on Effective tools and methodologies for
teaching natural language processing and computa-
tional linguistics - Volume 1, ETMTNLP ?02, pages
63?70. Association for Computational Linguistics.
Adam Lopez, Matt Post, and Chris Callison-Burch.
2013. Parallel speech, transcription, and translation:
The Fisher and Callhome Spanish-English speech
translation corpora. Technical Report 11, Johns
Hopkins University Human Language Technology
Center of Excellence.
Marco Lui and Timothy Baldwin. 2012. langid.py:
an off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
ACL ?12, pages 25?30. Association for Computa-
tional Linguistics.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
1382
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Comput. Linguist.,
31:477?504, December.
Jian-Yun Nie, Michel Simard, Pierre Isabelle, and
Richard Durand. 1999. Cross-language information
retrieval based on parallel texts and automatic min-
ing of parallel texts from the web. In Proceedings of
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?99, pages 74?81, New York, NY,
USA. ACM.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In acl, pages 160?
167, Sapporo, Japan.
P. Resnik and N. A Smith. 2003. The web as a parallel
corpus. Computational Linguistics, 29(3):349?380.
Philip Resnik. 1999. Mining the web for bilingual text.
In Proceedings of the 37th annual meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, ACL ?99, pages 527?534. As-
sociation for Computational Linguistics.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting Parallel Sentences from Compara-
ble Corpora using Document Level Alignment. In
NAACL 2010.
Jo?rg Tiedemann. 2009. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Ferhan Ture and Jimmy Lin. 2012. Why not grab a
free lunch? mining large corpora for parallel sen-
tences to improve translation modeling. In Proceed-
ings of the 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
626?630, Montre?al, Canada, June. Association for
Computational Linguistics.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109. Association for Computational Linguistics.
Ashish Venugopal, Jakob Uszkoreit, David Talbot,
Franz J. Och, and Juri Ganitkevitch. 2011. Water-
marking the outputs of structured prediction with an
application in statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?11, pages
1363?1372. Association for Computational Linguis-
tics.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proc. of ACL.
1383
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor
Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.
1 Introduction
Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al, 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al, 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).
The main focus of this paper is to describe this
past year?s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua?s grammar ex-
1http://github.com/joshua-decoder/joshua
traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.
In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.
2 Thrax: grammar extraction
In modern machine translation systems such as
Joshua (Li et al, 2009) and cdec (Dyer et al, 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple
(N,S, T?, T? , G)
where N is a set of nonterminal symbols of the
grammar, S ? N is the goal symbol, T? and T?
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.
Each rule in G is of the form
X ? ??, ?,??
where X ? N is a nonterminal symbol, ? is a se-
quence of symbols from N ? T?, ? is a sequence of
478
symbols from N ? T? , and ? is a one-to-one cor-
respondence between the nonterminal symbols of ?
and ?.
The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:
w(X ? ??, ??) =
?
i
?i(X ? ??, ??)
?i (1)
where each ?i is a feature function and ?i is the
weight for ?i. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.
2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).
A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e
l
k) is a sub-phrase
of the sentence pair, we say it is consistent with
the pair?s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.
An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent?s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 ? indicating that the
das begr??e ich sehr .
i
very
much
welcome this
.
PRP
NP
S
RB RB
ADVP
VP
VBP DT
NP
.
Figure 1: An aligned sentence pair.
target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.
2.2 System overview
The following were goals in the design of Thrax:
? the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;
? the ability to easily change and extend the fea-
ture sets for each rule
? scalability to arbitrarily large training corpora.
Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google?s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.
The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.
479
Span Hiero SAMT
[1, 3] X ? ?sehr, very much? ADV P ? ?sehr, very much?
[0, 3] X ? ?X sehr, X very much? PRP +ADV P ? ?PRP sehr, PRP very much?
[3, 4] X ? ?begru??e,welcome? V BP ? ?begru??e,welcome?
[0, 6] X ? ?X ich sehr ., i very much X .? S ? ?V P ich sehr ., i very much V P .?
[0, 6] X ? ?X ., X .? S ? ?S/. ., S/. .?
Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .
After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.
Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.
2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.
There are several parameters that can make a sig-
nificant difference in a grammar?s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.
? maximum rule span
? maximum span of consistent phrase pairs
? maximum number of nonterminals
? minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/
? whether to allow adjacent nonterminals on
source side
? whether to allow unaligned words at the edges
of consistent phrase pairs
Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.
When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax?s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.
2.4 Feature functions
Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:
? a binary indicator functions denoting:
? whether the rule is purely abstract (i.e.,
has no terminal symbols)
480
? the rule is purely lexical (i.e., has no non-
terminals)
? the rule is monotonic or has reordering
? the rule has adjacent nonterminals on the
source side
? counters for
? the number of unaligned words in the rule
? the number of terminals on the target side
of the rule
? a constant phrase penalty
In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:
? Phrasal translation probabilities p(?|?) and
p(?|?), calculated with relative frequency:
p(?|?) =
C(?, ?)
C(?)
(2)
(and vice versa), where C(?) is the number of
times a given event was extracted.
? Lexical weighting plex(?|?,A) and
plex(?|?,A). We calculate these weights
as given in (Koehn et al, 2003): let A be the
alignment between ? and ?, so (i, j) ? A if
and only if the ith word of ? is aligned to the
jth word of ?. Then we can define plex(?|?) as
n?
i=1
1
|{j : (i, j) ? A}|
?
(i,j)?A
w(?j |?i) (3)
where ?i is the ith word of ?, ?j is the jth word
of ?, and w(y|x) is the relative frequency of
seeing word y given x.
? Rarity penalty, given by
exp(1? C(X ? ??, ??)) (4)
where again C(?) is a count of the number of
times the rule was extracted.
The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.
It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax?s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.
2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua?s extractor scores each rule with three feature
functions ? lexical probabilities in two directions,
and one phrasal probability score p(?|?).
The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.
Finally, the cdec decoder (Dyer et al, 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.
Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua?s built-in rule ex-
tractor calculates the phrasal probability p(?|?) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(?|?), which would require an-
other sort.
481
Language pair sentences (K) words (M)
cs?en 332 4.7
de?en 279 5.5
en?cs 487 6.9
en?de 359 7.2
en?fr 682 12.5
fr?en 792 14.4
Table 2: Training data size after subsampling.
The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.
The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal?s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.
3 Experiments
We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.
To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Z?abokrtsky?, 2009). The
parallel data was subsampled using Joshua?s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5
Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize
3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with
SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.
5LDC2009T13
pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -
Table 3: Single-reference BLEU-4 scores.
the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al, 2006) with the
provided Treebank-trained grammar.
We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.
4 CachePipe: Cached pipeline runs
Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0?s Make-
file system (Li et al, 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.
6http://www.statmt.org/moses/?n=
FactoredTraining.EMS
482
the
reactor type will be operated with uranium
VBN
DT+NP
GLUE
VP
PP
der reaktortyp , das nicht
angereichert
wird zwar mit uran betrieben
, which is
not
enriched
ist .
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
.
S
VBN
DT+NP
GLUE
VP
PP
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
S
Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.
Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:
cachecmd copy "cp a b" a b
The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.
CachePipe is open-source software, distributed
with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.
5 Future work
Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al, 2004) or more re-
cent tree-to-tree models (Zhang et al, 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.
Acknowledgements
This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors? alone.
7https://github.com/joshua-decoder/
cachepipe
483
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.
Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7?12.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166, January.
Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master?s thesis, Johns Hopkins University, May.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.
484
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 1?9,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Putting Human Assessments of Machine Translation Systems in Order
Adam Lopez
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
Human assessment is often considered the
gold standard in evaluation of translation sys-
tems. But in order for the evaluation to
be meaningful, the rankings obtained from
human assessment must be consistent and
repeatable. Recent analysis by Bojar et
al. (2011) raised several concerns about the
rankings derived from human assessments of
English-Czech translation systems in the 2010
Workshop on Machine Translation. We extend
their analysis to all of the ranking tasks from
2010 and 2011, and show through an exten-
sion of their reasoning that the ranking is nat-
urally cast as an instance of finding the mini-
mum feedback arc set in a tournament, a well-
known NP-complete problem. All instances
of this problem in the workshop data are ef-
ficiently solvable, but in some cases the rank-
ings it produces are surprisingly different from
the ones previously published. This leads to
strong caveats and recommendations for both
producers and consumers of these rankings.
1 Introduction
The value of machine translation depends on its util-
ity to human users, either directly through their use
of it, or indirectly through downstream tasks such
as cross-lingual information extraction or retrieval.
It is therefore essential to assess machine transla-
tion systems according to this utility, but there is a
widespread perception that direct human assessment
is costly, unreproducible, and difficult to interpret.
Automatic metrics that predict human utility have
therefore attracted substantial attention since they
are at least cheap and reproducible given identical
data conditions, though they are frequently and cor-
rectly criticized for low interpretability and correla-
tion with true utility. Their use (and abuse) remains
contentious.
The organizers of the annual Workshop on Ma-
chine Translation (WMT) have taken a strong stance
in this debate, asserting the primacy of human eval-
uation. Every annual report of their findings since
2007 has included a variant of the following state-
ment:
It is our contention that automatic mea-
sures are an imperfect substitute for hu-
man assessment of translation quality.
Therefore, we define the manual evalua-
tion to be primary, and use the human
judgments to validate automatic metrics.
(Callison-Burch et al, 2011)
The workshop?s human evaluation component has
been gradually refined over several years, and as a
consequence it has produced a fantastic collection of
publicly available data consisting primarily of pair-
wise judgements of translation systems made by hu-
man assessors across a wide variety of languages
and tasks. Despite superb effort in the collection of
these assessments, less attention has been focused
on the final product derived from them: a totally-
ordered ranking of translation systems participating
in each task. Many of the official workshop results
depend crucially on this ranking, including the eval-
uation of both machine translation systems and auto-
matic metrics. Considering the enormous costs and
consequences of the ranking, it is important to ask:
is the method of constructing it accurate? The num-
ber of possible rankings is combinatorially large?
with at least ten systems (accounting for more than
1
half the cases we analyzed) there are over three mil-
lion possible rankings, and with at least twenty (oc-
curring a few times), there are over 1018 possible
rankings. Exceptional care is therefore required in
producing the rankings.
Bojar et al (2011) observed a number of discrep-
ancies in the ranking of English-Czech systems from
the 2010 workshop, making these questions ever
more pressing. We extend their analysis in several
ways.
1. We show, through a logical extension of their
reasoning about flaws in the evaluation, that
the final ranking can be naturally cast as an in-
stance of the minimal feedback arc set problem,
a well-known NP-Hard problem.
2. We analyze 25 tasks that were evaluated using
pairwise assessments from human annotators in
2010 and 2011.
3. We produce new rankings for each of the tasks,
which are in some cases surprisingly different
from the published rankings.
4. We identify a new set of concerns about sources
of error and uncertainty in the data.
2 Human Assessment as Pairwise Ranking
The workshop has conducted a variety of different
manual evaluation tasks over the last several years,
but its mainstay has been the relative ranking task.
Assessors are presented with a source sentence fol-
lowed by up to five translations, and are asked to
rank the translations from best to worst, with ties
allowed. Since it is usually infeasible to collect in-
dividual judgements for all sentences for all pairs of
systems on each task, consecutive sequences of three
sentences were randomly sampled from the test data,
with each sentence in each sequence presented to the
same annotator. Some samples were presented mul-
tiple times to the same assessor or to multiple asses-
sors in order to measure intra- and inter-annotator
agreement rates. Since there are often more than
five systems participating in the campaign, the can-
didate translations are likewise sampled from a pool
consisting of the machine translations and a human
reference translation, which is included for quality
JHU 1 JHU?BBN-COMBO
BBN-COMBO 2 JHU?RWTH
RWTH 3 JHU?RWTH-COMBO
RWTH-COMBO 3 JHU?CMU
CMU 4 BBN-COMBO?RWTH
BBN-COMBO?RWTH-COMBO
BBN-COMBO?CMU
RWTH?RWTH-COMBO
RWTH?CMU
RWTH-COMBO?CMU
Figure 1: Example human relative ranking of five sys-
tems (left) and the inferred pairwise rankings (right) on
a single sentence from the WMT 2010 German-English
campaign.
control purposes. It is important to note that the al-
gorithm used to compute the published final rank-
ings included all of this data, including comparisons
against the reference and the redundant assessments
used to compute inter-annotator agreement.
The raw data obtained from this process is a large
set of assessments. Each assessment consists of a
list of up to five systems (including the reference),
and a partial or total ordering of the list. The relative
ranking of each pair of systems contained in the list
is then taken to be their pairwise ranking. Hence a
single assessment of five systems yields ten implicit
pairwise rankings, as illustrated in Figure 1.
3 From Pairwise to Total Ranking
Given these pairwise rankings, the question now be-
comes: how do we decide on a total ordering of
the systems? In the WMT evaluation, this total or-
dering has two critical functions: it is published as
the official ranking of the participating systems; and
it is used as the ground truth against which auto-
matic evaluation metrics are graded, using Spear-
man?s rank correlation coefficient (without ties) as
the measure of accuracy. Choosing a total order is
non-trivial: there are N ! possible orderings of N
systems. Even with relatively small N of the work-
shop, this number can grow extremely large (over
1025 in the worst case of 25 systems).
The method used to generate the published rank-
ings is simple. For each system A among the set
S of ranked systems (which includes the reference),
2
compute the number of times that A is ranked better
than or equivalent to any system B ? S, and then
divide by the total number of comparisons involv-
ing A, yielding the following statistic for system A,
which we call WMT-OFFICAL.
score(A) =
?
B?S count(A  B)?
B?S,3?{?,?,}, count(A3B)
(1)
The systems are ranked according to this statistic,
with higher scores resulting in a better rank.
Bojar et al (2011) raise many concerns about this
method for ranking the systems. While we refer the
reader to their paper for a detailed analysis, we focus
on two issues here:
? Since ties are rewarded, systems may be un-
duly rewarded for merely being similar to oth-
ers, rather than clearly better. This is of particu-
lar concern since there is often a cohort of very
similar systems in the pool, such as those based
on very similar techniques.
? Since the reference is overwhelmingly favored
by the assessors, those systems that are more
frequently compared against the reference in
the random sample will be unfairly penalized.
These observations suggest that the statistic
should be changed to reward only outright wins in
pairwise comparisons, and to lessen the number of
comparisons to the reference. While they do not
recommend a specific sampling rate for comparisons
against the reference, the logical conclusion of their
reasoning is that it should not be sampled at all. This
yields the following statistic similar to one reported
in the appendices of the WMT proceedings, which
we call HEURISTIC 2.
score(A) =
?
B?S?ref count(A ? B)
?
B?S?ref,3?{?,?,}, count(A3B)
(2)
However, the analysis by Bojar et al (2011) goes
further and suggests disregarding the effect of ties
altogether by removing them from the denominator.
This yields their final recommended statistic, which
we call BOJAR.
score(A) =
?
B?S?ref count(A ? B)
?
B?S?ref,3?{?,}, count(A3B)
(3)
Superficially, this appears to be an improve-
ment. However, we observe in the rankings that
two anonymized commercial systems, denoted ON-
LINEA and ONLINEB, consistently appear at or near
the top of the rankings in all tasks. It is natural to
wonder: even if we leave out the reference from
comparisons, couldn?t a system still be penalized
simply by being compared against ONLINEA and
ONLINEB more frequently than its competitors? On
the other hand, couldn?t a system be rewarded sim-
ply by being compared against a bad system more
frequently than its competitors?
There are many possible decisions that we could
make, each leading to a different ranking. However,
there is a more fundamental problem: each of these
heuristic scores is based on statistics aggregated over
completely incomparable sets of data. Any total
ordering of the systems must make a decision be-
tween every pair of systems. When that ranking is
computed using scores computed with any of Equa-
tions 1 through 3, we aggregate over completely dif-
ferent sets of sentences, rates of comparison with
other systems, and even annotators! Deriving sta-
tistical conclusions from such comparisons is at best
suspect. If we want to rank A and B relative to each
other, it would be more reliable to aggregate over
the same set of sentences, same rates of comparison,
and the same annotators. Fortunately, we have this
data in abundance: it is the collection of pairwise
judgements that we started with.
4 Pairwise Ranking as a Tournament
The human assessments are a classic example of a
tournament. A tournament is a graph of N vertices
with exactly
(N
2
)
directed edges?one between each
pair of vertices. The edge connecting each pair of
vertices A and B points to whichever vertex which
is worse in an observed pairwise comparison be-
tween them. Tournaments are a natural represen-
tation of many ranking problems, including search
results, transferable voting systems, and ranking of
sports teams.1
Consider the simple weighted tournament de-
picted in Figure 2. This tournament is acyclic, which
means that we can obtain a total ordering of the ver-
1The original motivating application was modeling the peck-
ing order of chickens (Landau, 1951).
3
AB
C
D
3
2
1
1
1
2
Consistent ranking: A ? B ? C ? D
Ranking according to Eq. 1: A ? C ? B ? D
Figure 2: A weighted tournament and two different rank-
ings of its vertices.
tices that is consistent with all of the pairwise rank-
ings simply by sorting the vertices topologically. We
start by choosing the vertex with no incoming edges
(i.e. the one that wins in all pairwise comparisons),
place it at the top of the ranking, and remove it along
with all of its outgoing edges from the graph. We
then repeat the procedure with the remaining ver-
tices in the graph, placing the next vertex behind
the first one, and so on. The result is a ranking that
preserves all of the pairwise rankings in the original
graph.
This example also highlights a problem in Equa-
tion 1. Imagine an idealized case in which the con-
sistent ranking of the vertices in Figure 2 is their true
ranking, and furthermore that this ranking is unam-
biguous: that is, no matter how many times we sam-
ple the comparison A with B, the result is always
that A ? B, and likewise for all vertices. If the
weights in this example represented the number of
random samples for each system, then Equation 1
will give the inaccurate ranking shown, since it pro-
duces a score of 25 for B and
2
4 for C.
Tournaments can contain cycles, and as we will
show this is often the case in the WMT data. When
this happens, a reasonable solution is to minimize
the discrepancy between the ranking and the ob-
served data. We can do this by reversing a set of
edges in the graph such that (1) the resulting graph
is acyclic, and (2) the summed weights of the re-
versed edges is minimized. A set of edges satisfying
these constraints is called the minimum feedback arc
set (Figure 3).
The feedback arc set problem on general graphs
E
F
G
H
3
2
1
2
1
2
Figure 3: A tournament with a cycle on vertices E, F ,
and G. The dotted edge is the only element of a minimum
feedback arc set: reversing it produces an acyclic graph.
Algorithm 1 Minimum feedback arc set solver
Input: Graph G = (V,E), weights w : E ? R+
Initialize all costs to?
Let cost(?)? 0
Add ? to agenda A
repeat
Let R?? argminR?A cost(R)
Remove R? from A . R? is a partial ranking
Let U ? V \R? . set of unranked vertices
for each vertex v ? U do
Add R? ? v to agenda
Let c?
?
v??U :?v?,v??E w(?v
?, v?)
Let d? cost(R?) + c
Let cost(R??{v})? min(cost(R??{v}), d)
until argminR?A cost(h) = V
is one of the 21 classic problems shown to be
NP-complete by Karp (1972).2 Finding the mini-
mum feedback arc set in a tournament was shown
to be NP-hard by Alon (2006) and Charbit et al
(2007). However, the specific instances exhibited
in the workshop data tend to have only a few cy-
cles, so a relatively straightforward algorithm (for-
malized above for completeness) solves them ex-
actly without much difficulty. The basic idea is to
construct a dynamic program over the possible rank-
ings. Each item in the dynamic program represents
a ranking of some subset of the vertices. An item
is extended by choosing one of the unranked ver-
tices and appending it to the hypothesis, adding to
its cost the weights of all edges from the other un-
ranked vertices to the newly appended vertex (the
2Karp proved NP-completeness of the decision problem that
asks whether there is a feedback arc set of size k; NP-hardness
of the minimization problem follows.
4
Task name #sys #pairs Task name #sys #pairs
2010 Czech-English 12 5375 2011 English-French individual 17 9086
2010 English-Czech 17 13538 2011 English-German syscomb 4 4374
2010 English-French 19 7962 2011 English-German individual 22 12996
2010 English-German 18 13694 2011 English-Spanish syscomb 4 5930
2010 English-Spanish 16 5174 2011 English-Spanish individual 15 11130
2010 French-English 24 8294 2011 French-English syscomb 6 3000
2010 German-English 25 10424 2011 French-English individual 18 6986
2010 Spanish-English 14 11307 2011 German-English syscomb 8 3844
2011 Czech-English syscomb 4 2602 2011 German-English individual 20 9079
2011 Czech-English individual 8 4922 2011 Spanish-English syscomb 6 4156
2011 English-Czech syscomb 2 2686 2011 Spanish-English individual 15 5652
2011 English-Czech individual 10 17875 2011 Urdu-English tunable metrics 8 6257
2011 English-French syscomb 2 880
Table 1: The set of tasks we analyzed, including the number of participating systems (excluding the reference, #sys),
and the number of implicit pairwise judgements collected (including the reference, #pairs).
edges to be reversed). This hypothesis space should
be familiar to most machine translation researchers
since it closely resembles the search space defined
by a phrase-based translation model (Koehn, 2004).
We use Dijkstra?s algorithm (1959) to explore it ef-
ficiently; the complete algorithm is simply a gener-
alization of the simple algorithm for acyclic tourna-
ments described above.
5 Experiments and Analysis
We experimented with 25 relative ranking tasks pro-
duced by WMT 2010 (Callison-Burch et al, 2010)
and WMT 2011 (Callison-Burch et al, 2011); the
full set is shown in Table 1. For each task we con-
sidered four possible methods of ranking the data:
sorting by any of Equation 1 through 3, and sort-
ing consistent with reversal of a minimum feedback
arc set (MFAS). To weight the edges for the latter
approach, we simply used the difference in num-
ber of assessments preferring one system over the
other; that is, an edge from A to B is weighted
count(A ? B)? count(A  B). If this quantity is
negative, there is instead an edge from B to A. The
purpose of this simple weighting is to ensure a so-
lution that minimizes the number of disagreements
with all available evidence, counting each pairwise
comparison as equal.3
3This is not necessarily the best choice of weighting. For
instance, (Bojar et al, 2011) observe that human assessments of
WMT-OFFICIAL MFAS BOJAR
(Eq 1) (Eq 3)
ONLINE-B CU-MARECEK ONLINE-B
CU-BOJAR ONLINE-B CU-BOJAR
CU-MARECEK CU-BOJAR CU-MARECEK
CU-TAMCHYNA CU-TAMCHYNA CU-TAMCHYNA
UEDIN CU-POPEL CU-POPEL
CU-POPEL UEDIN UEDIN
COMMERCIAL2 COMMERCIAL1 COMMERCIAL2
COMMERCIAL1 COMMERCIAL2 COMMERCIAL1
JHU JHU JHU
CU-ZEMAN CU-ZEMAN CU-ZEMAN
38 0 69
Table 2: Different rankings of the 2011 Czech-English
task. Only the MFAS ranking is acyclic with respect to
pairwise judgements. The final row indicates the weight
of the voilated edges.
An MFAS solution written in Python took only a
few minutes to produce rankings for all 25 tasks on a
2.13 GHz Intel Core 2 Duo processor, demonstrating
that it is completely feasible despite being theoreti-
cally intractible. One value of computing this solu-
tion is that it enables us to answer several questions,
shorter sentences tend to be more consistent with each other, so
perhaps they should be weighted more highly. Unfortunately,
it is not clear how to evaluate alternative weighting schemes,
since there is no ground truth for such meta-evaluations.
5
ONLINEB LIUM ? ONLINEB 1 RWTH-COMBO
RWTH-COMBO UPV-COMBO ? CAMBRIDGE 6 CMU-HYPOSEL-COMBO
CMU-HYPOSEL-COMBO JHU ? CAMBRIDGE 1 DCU-COMBO
CAMBRIDGE LIMSI ? UEDIN 1 ONLINEB
LIUM LIMSI ? CMU-HYPOSEL-COMBO 1 LIUM
DCU-COMBO LIUM-COMBO ? CAMBRIDGE 1 CMU-HEAFIELD-COMBO
CMU-HEAFIELD-COMBO LIUM-COMBO ? NRC 3 UPV-COMBO
UPV-COMBO RALI ? UEDIN 1 NRC
NRC RALI ? UPV-COMBO 4 CAMBRIDGE
UEDIN RALI ? JHU 1 UEDIN
JHU RALI ? LIUM 3 JHU-COMBO
LIMSI LIG ? UEDIN 6 LIMSI
JHU-COMBO BBN-COMBO ? NRC 3 RALI
LIUM-COMBO BBN-COMBO ? UEDIN 5 LIUM-COMBO
RALI BBN-COMBO ? UPV-COMBO 5 BBN-COMBO
LIG BBN-COMBO ? JHU 4 JHU
BBN-COMBO RWTH ? UPV-COMBO 3 RWTH
RWTH CMU-STATXFER ? JHU 1 LIG
CMU-STATXFER CMU-STATXFER ? LIG 1 ONLINEA
ONLINEA ONLINEA ? RWTH 1 CMU-STATXFER
HUICONG ONLINEA ? JHU 2 HUICONG
DFKI HUICONG ? LIG 3 DFKI
CU-ZEMAN DFKI ? RWTH 3 GENEVA
GENEVA DFKI ? CMU-STATXFER 1 CU-ZEMAN
Table 3: 2010 French-English reranking with MFAS solver. The left column shows the optimal ranking, while the
center shows the pairwise rankings that are violated by this ranking, along with their edge weights. The right column
shows the ranking under WMT-OFFICIAL (Eq. 1), originally published as two separate tables.
both about the pairwise data itself, and the proposed
heuristic ranking of Bojar et al (2011).
5.1 Cycles in the Pairwise Rankings
Our first experiment checks for cycles in the tourna-
ments. Only nine were acyclic, including all eight
of the system combination tasks, each of which con-
tained only a handful of systems. The most inter-
esting, however, is the 2011 English-Czech individ-
ual task. This task is notable because the heuristic
rankings do not produce a ranking that is consistent
with all of the pairwise judgements, even though one
exists. The three rankings are illustrated side-by-
side in Table 2. One obvious problem is that neither
heuristic score correctly identifies CU-MARECEK as
the best system, even though it wins pairwise com-
parisons against all other systems (the WMT 2011
proceedings do identify it as a winner, despite not
placing it in the highest rank).
On the other hand, the most difficult task to dis-
entangle is the 2010 French-English task (Table 3),
which included 25 systems (individual and system
combinations were evaluated as a group for this task,
despite being reported in separate tables in official
results). Its optimal ranking with MFAS still vio-
lates 61 pairwise ranking samples ? there is sim-
ply no sensible way to put these systems into a to-
tal order. On the other hand, the heuristic rankings
based on Equations 1 through 3 violate even more
comparisons: 107, 108, and 118, respectively. Once
again we see a curious result in the top of the heuris-
tic rankings, with system ONLINEB falling several
spots below the top position in the heurstic ranking,
despite losing out only to LIUM by one vote.
Our major concern, however, is that over half of
the tasks included cycles of one form or another in
the tournaments. This represents a strong inconsis-
6
tency in the data.
5.2 Evaluation of Heuristic Scores
Taking the analysis above further, we find that the
total number of violations of pairwise preferences
across all tasks stands at 396 for the MFAS solution,
and at 1140, 1215, 979 for Equations 1 through 3.
This empirically validates the suggestion by Bojar
et al (2011) to remove ties from both the numera-
tor and denominator of the heuristic measure. On
the other hand, despite the intuitive arguments in its
favor, the empirical evidence does not strongly fa-
vor any of the heuristic measures, all of which are
substantially worse than the MFAS solution.
In fact, HEURISTIC 2 (Eq. 2) fails quite spec-
tacularly in one case: on the ranking of the sys-
tems produced by the tunable metrics task of WMT
2011 (Figure 4). Apart from producing a ranking
very inconsistent with the pairwise judgements, it
achieves a Spearman?s rank correlation coefficent
of 0.43 with the MFAS solution. By comparison,
WMT-OFFICIAL (Eq. 1) produces the best ranking,
with a correlation of 0.93 with the MFAS solution.
The two heuristic measures obtain an even lower
correlation of 0.19 with each other. This difference
in the two rankings was noted in the WMT 2011
report; however comparison with the MFAS ranker
suggests that the published rankings according to the
official metric are about as accurate as those based
on other heuristic metrics.
6 Discussion
Unfortunately, reliably ranking translation systems
based on human assessments appears to be a difficult
task, and it is unclear that WMT has succeeded yet.
Some results presented here, such as the complete
inability to obtain a sensible ordering on the 2010
French-English task?or to produce an acyclic tour-
nament on more than half the tasks?indicate that
further work is needed, and we feel that the pub-
lished results of the human assessment should be re-
garded with a healthy skepticism. There are many
potential sources of uncertainty in the data:
? It is quite rare that one system is uniformly bet-
ter than another. Rather, one system will tend
to perform better in aggregate across many sen-
tences. The number of sentences on which this
MFAS Ranking HEURISTIC 2 Ranking
CMU-BLEU CU-SEMPOS-BLEU
CMU-BLEU-SINGLE NUS-TESLA-F
CU-SEMPOS-BLEU CMU-BLEU
RWTH-CDER CMU-BLEU-SINGLE
CMU-METEOR STANFORD-DCP
STANFORD-DCP CMU-METEOR
NUS-TESLA-F RWTH-CDER
SHEFFIELD-ROSE SHEFFIELD-ROSE
Table 4: Rankings of the WMT 2011 tunable metrics
task. MFAS finds a near-optimal solution, violating only
six judgements with reversals of CMU-METEOR ? CMU-
BLEU and STANFORD-DCP ? CMU-BLEU-SINGLE. In
contrast, the HEURISTIC2 (Eq. 2) solution violates 103
pairwise judgements.
improvement can be reliably observed will vary
greatly. In many cases, it may be less than the
number of samples.
? Individual assessors may be biased or mali-
cious.
? The reliability of pairwise judgements varies
with sentence length, as noted by Bojar et al
(2011).
? The pairwise judgements are not made directly,
but inferred from a larger relative ranking.
? The pairwise judgements are not independent,
since each sample consists of consecutive sen-
tences from the same document. It is likely
that some systems are systematically better or
worse on particular documents.
? The pairwise judgements are not independent,
since many of the assessments are intention-
ally repeated to assess intra- and inter-annotator
agreement.
? Many of the systems will covary, since they are
often based on the same underlying techniques
and software.
How much does any one or all of these factors
affect the final ranking? The technique described
above does not even attempt to address this ques-
tion. Indeed, modeling this kind of data still ap-
pears to be unsolved: a recent paper by Wauthier
7
and Jordan (2011) on modeling latent annotator bias
presents one of the first attempts at solving just one
of the above problems, let alne all of them.
Simple hypothesis testing of the type reported in
the workshop results is simply inadequate to tease
apart the many interacting effects in this type of
data and may lead to many unjustified conclusions.
The tables in the Appendix of Callison-Burch et al
(2011) report p-values of up to 1%, computed for
every pairwise comparison in the dataset. However,
there are over two thousand comparisons in this ap-
pendix, so even at an error rate of 1% we would ex-
pect more than twenty to be wrong. Making matters
worse, many of the p-values are in fact much than
higher than 1%. It is quite reasonable to assume
that hundreds of the pairwise rankings inferred from
these tables are incorrect, or at least meaningless.
Methods for multiple hypothesis testing (Benjamini
and Hochberg, 1995) should be explored.
In short, there is much work to be done. This pa-
per has raised more questions than it answered, but
we offer several recommendations.
? We recommend against using the metric pro-
posed by Bojar et al (2011). While their anal-
ysis is very insightful, their proposed heuristic
metric is not substantially better than the met-
ric used in the official rankings. If anything, an
MFAS-based ranking should be preferred since
it can minimize discrepancies with the pairwise
rankings, but as we have discussed, we believe
this is far from a complete solution.
? Reconsider the use of total ordering, especially
for the evaluation of automatic metrics. As
demonstrated in this paper, there are many pos-
sible ways to generate a total ordering, and the
choice of one may be arbitrary. In some cases
there may not be enough evidence to support a
total ordering, or the evidence is contradictory,
and committing to one may be a source of sub-
stantial noise in the gold standard for evaluating
automatic metrics.
? Consider a pilot study to clearly identify which
sources of uncertainty in the data affect the
rankings and devise methods to account for it,
which may involve redesigning the data collec-
tion protocol. The current approach is designed
to collect data for a variety of different goals,
including intra- and inter-annotator agreement,
pairwise coverage, and maximum throughput.
However, some of goals are at cross-purposes
in that they make it more difficult to make reli-
able statistical inferences about any one aspect
of the data. Additional care should be taken
to minimize dependencies between the samples
used to produce the final ranking.
? Encourage further detailed analysis of the ex-
isting datasets, perhaps through a shared task.
The data that has been amassed so far through
WMT is the best available resource for mak-
ing progress on solving the difficult problem of
producing reliable and repeatable human rank-
ings of machine translation systems. However,
this problem is not solved yet, and it will re-
quire sustained effort to make that progress.
Acknowledgements
Thanks to Ondre?j Bojar, Philipp Koehn, and Mar-
tin Popel for very helpful discussion related to this
work, the anonymous reviewers for detailed and
helpful comments, and Chris Callison-Burch for en-
couraging this investigation and for many explana-
tions and additional data from the workshop.
References
N. Alon. 2006. Ranking tournaments. SIAM Journal on
Discrete Mathematics, 20(1):137?142.
Y. Benjamini and Y. Hochberg. 1995. Controlling the
false discovery rate: a practical and powerful approach
to multiple testing. Journal of the Royal Statistical
Society, 57:289?300.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. F. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, K. Peterson,
M. Przybocki, and O. Zaidan. 2010. Findings of the
2010 joint workshop on statistical machine translation
and metrics for machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
P. Charbit, S. Thomass, and A. Yeo. 2007. The minimum
feedback arc set problem is NP-hard for tournaments.
Combinatorics, Probability and Computing, 16.
8
E. W. Dijkstra. 1959. A note on two problems in connex-
ion with graphs. Numerische Mathematik, 1:269?271.
R. M. Karp. 1972. Reducibility among combinatorial
problems. In Symposium on the Complexity of Com-
puter Computations.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
H. G. Landau. 1951. On dominance relations and
the structure of animal societies: I effect of inher-
ent characteristics. Bulletin of Mathematical Biology,
13(1):1?19.
F. L. Wauthier and M. I. Jordan. 2011. Bayesian bias
mitigation for crowdsourcing. In Proc. of NIPS.
9
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 222?231,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Categorial Grammar to Label Translation Rules
Jonathan Weese and Chris Callison-Burch and Adam Lopez
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
Adding syntactic labels to synchronous
context-free translation rules can improve
performance, but labeling with phrase struc-
ture constituents, as in GHKM (Galley et al,
2004), excludes potentially useful translation
rules. SAMT (Zollmann and Venugopal,
2006) introduces heuristics to create new
non-constituent labels, but these heuristics
introduce many complex labels and tend to
add rarely-applicable rules to the translation
grammar. We introduce a labeling scheme
based on categorial grammar, which allows
syntactic labeling of many rules with a mini-
mal, well-motivated label set. We show that
our labeling scheme performs comparably to
SAMT on an Urdu?English translation task,
yet the label set is an order of magnitude
smaller, and translation is twice as fast.
1 Introduction
The Hiero model of Chiang (2007) popularized
the usage of synchronous context-free grammars
(SCFGs) for machine translation. SCFGs model
translation as a process of isomorphic syntactic
derivation in the source and target language. But the
Hiero model is formally, not linguistically syntactic.
Its derivation trees use only a single non-terminal la-
bel X , carrying no linguistic information. Consider
Rule 1.
X ? ? maison ; house ? (1)
We can add syntactic information to the SCFG
rules by parsing the parallel training data and pro-
jecting parse tree labels onto the spans they yield and
their translations. For example, if house was parsed
as a noun, we could rewrite Rule 1 as
N ? ? maison ; house ?
But we quickly run into trouble: how should we
label a rule that translates pour l?e?tablissement de
into for the establishment of? There is no phrase
structure constituent that corresponds to this English
fragment. This raises a model design question: what
label do we assign to spans that are natural trans-
lations of each other, but have no natural labeling
under a syntactic parse? One possibility would be
to discard such translations from our model as im-
plausible. However, such non-compositional trans-
lations are important in translation (Fox, 2002), and
they have been repeatedly shown to improve trans-
lation performance (Koehn et al, 2003; DeNeefe et
al., 2007).
Syntax-Augmented Machine Translation (SAMT;
Zollmann and Venugopal, 2006) solves this prob-
lem with heuristics that create new labels from the
phrase structure parse: it labels for the establish-
ment of as IN+NP+IN to show that it is the con-
catenation of a noun phrase with a preposition on
either side. While descriptive, this label is unsatis-
fying as a concise description of linguistic function,
fitting uneasily alongside more natural labels in the
phrase structure formalism. SAMT introduces many
thousands of such labels, most of which are seen
very few times. While these heuristics are effective
(Zollmann et al, 2008), they inflate grammar size,
hamper effective parameter estimation due to feature
sparsity, and slow translation speed.
Our objective is to find a syntactic formalism that
222
enables us to label most translation rules without re-
lying on heuristics. Ideally, the label should be small
in order to improve feature estimation and reduce
translation time. Furthering an insight that informs
SAMT, we show that combinatory categorial gram-
mar (CCG) satisfies these requirements.
Under CCG, for the establishment of is labeled
with ((S\NP)\(S\NP))/NP. This seems complex, but
it describes exactly how the fragment should com-
bine with other English words to create a complete
sentence in a linguistically meaningful way. We
show that CCG is a viable formalism to add syntax
to SCFG-based translation.
? We introduce two models for labeling SCFG
rules. One uses labels from a 1-best CCG parse
tree of training data; the second uses the top la-
bels in each cell of a CCG parse chart.
? We show that using 1-best parses performs as
well as a syntactic model using phrase structure
derivations.
? We show that using chart cell labels per-
forms almost as well than SAMT, but the non-
terminal label set is an order of magnitude
smaller and translation is twice as fast.
2 Categorial grammar
Categorial grammar (CG) (Adjukiewicz, 1935; Bar-
Hillel et al, 1964) is a grammar formalism in
which words are assigned grammatical types, or cat-
egories. Once categories are assigned to each word
of a sentence, a small set of universal combinatory
rules uses them to derive a sentence-spanning syn-
tactic structure.
Categories may be either atomic, like N, VP, S,
and other familiar types, or they may be complex
function types. A function type looks like A/B and
takes an argument of type B and returns a type A.
The categories A and B may themselves be either
primitives or functions. A lexical item is assigned a
function category when it takes an argument ? for
example, a verb may be function that needs to be
combined with its subject and object, or an a adjec-
tive may be a function that takes the noun it modifies
as an argument.
Lexical item Category
and conj
cities NP
in (NP\NP)/NP
own (S\NP)/NP
properties NP
they NP
various NP/NP
villages NP
Table 1: An example lexicon, mapping words to cat-
egories.
We can combine two categories with function ap-
plication. Formally, we write
X/Y Y ? X (2)
to show that a function type may be combined with
its argument type to produce the result type. Back-
ward function application also exists, where the ar-
gument occurs to the left of the function.
Combinatory categorial grammar (CCG) is an ex-
tension of CG that includes more combinators (op-
erations that can combine categories). Steedman
and Baldridge (2011) give an excellent overview of
CCG.
As an example, suppose we want to analyze the
sentence ?They own properties in various cities and
villages? using the lexicon shown in Table 1. We as-
sign categories according to the lexicon, then com-
bine the categories using function application and
other combinators to get an analysis of S for the
complete sentence. Figure 1 shows the derivation.
As a practical matter, very efficient CCG parsers
are available (Clark and Curran, 2007). As shown
by Fowler and Penn (2010), in many cases CCG is
context-free, making it an ideal fit for our problem.
2.1 Labels for phrases
Consider the German?English phrase pair der gro?e
Mann ? the tall man. It is easily labeled as an NP
and included in the translation table. By contrast,
der gro?e? the tall, doesn?t typically correspond to
a complete subtree in a phrase structure parse. Yet
translating the tall is likely to be more useful than
translating the tall man, since it is more general?it
can be combined with any other noun translation.
223
They own properties in various cities and villages
NP (S\NP )/NP NP (NP\NP )/NP NP/NP NP conj NP
> <?>
NP NP\NP
<
NP
>
NP\NP
<
NP
>
S\NP
<
S
Figure 1: An example CCG derivation for the sentence ?They own properties in various cities and villages?
using the lexicon from Table 1. ? indicates a conjunction operation; > and < are forward and backward
function application, respectively.
Using CG-style labels with function types, we can
assign the type (for example) NP/N to the tall to
show that it can be combined with a noun on its right
to create a complete noun phrase.1 In general, CG
can produce linguistically meaningful labels of most
spans in a sentence simply as a matter of course.
2.2 Minimal, well-motivated label set
By allowing slashed categories with CG, we in-
crease the number of labels allowed. Despite the in-
crease in the number of labels, CG is advantageous
for two reasons:
1. Our labels are derived from CCG derivations,
so phrases with slashed labels represent well-
motivated, linguistically-informed derivations,
and the categories can be naturally combined.
2. The set of labels is small, relative to SAMT ?
it?s restricted to the labels seen in CCG parses
of the training data.
In short, using CG labels allows us to keep more
linguistically-informed syntactic rules without mak-
ing the set of syntactic labels too big.
3 Translation models
3.1 Extraction from parallel text
To extract SCFG rules, we start with a heuristic to
extract phrases from a word-aligned sentence pair
1We could assign NP/N to the determiner the and N/N to the
adjective tall, then combine those two categories using function
composition to get a category NP/N for the two words together.
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 2: A word-aligned sentence pair fragment,
with a box indicating a consistent phrase pair.
(Tillmann, 2003). Figure 2 shows a such a pair, with
a consistent phrase pair inside the box. A phrase
pair (f, e) is said to be consistent with the alignment
if none of the words of f are aligned outside the
phrase e, and vice versa ? that is, there are no align-
ment points directly above, below, or to the sides of
the box defined by f and e.
Given a consistent phrase pair, we can immedi-
ately extract the rule
X ? ?f, e? (3)
as we would in a phrase-based MT system. How-
ever, whenever we find a consistent phrase pair that
is a sub-phrase of another, we may extract a hierar-
chical rule by treating the inner phrase as a gap in
the larger phrase. For example, we may extract the
rule
X ? ? Pour X ; For X ? (4)
from Figure 3.
224
For
most
people
,
P
o
u
r
l
a
m
a
j
o
r
i
t
?
d
e
s
g
e
n
s
,
Figure 3: A consistent phrase pair with a sub-phrase
that is also consistent. We may extract a hierarchical
SCFG rule from this training example.
The focus of this paper is how to assign labels
to the left-hand non-terminal X and to the non-
terminal gaps on the right-hand side. We discuss five
models below, of which two are novel CG-based la-
beling schemes.
3.2 Baseline: Hiero
Hiero (Chiang, 2007) uses the simplest labeling pos-
sible: there is only one non-terminal symbol, X , for
all rules. Its advantage over phrase-based translation
in its ability to model phrases with gaps in them,
enabling phrases to reorder subphrases. However,
since there?s only one label, there?s no way to in-
clude syntactic information in its translation rules.
3.3 Phrase structure parse tree labeling
One first step for adding syntactic information is to
get syntactic labels from a phrase structure parse
tree. For each word-aligned sentence pair in our
training data, we also include a parse tree of the tar-
get side.
Then we can assign syntactic labels like this: for
each consistent phrase pair (representing either the
left-hand non-terminal or a gap in the right hand
side) we see if the target-language phrase is the exact
span of some subtree of the parse tree.
If a subtree exactly spans the phrase pair, we can
use the root label of that subtree to label the non-
terminal symbol. If there is no such subtree, we
throw away any rules derived from the phrase pair.
As an example, suppose the English side of the
phrase pair in Figure 3 is analyzed as
PP
IN
For
NP
JJ
most
NN
people
Then we can assign syntactic labels to Rule 4 to pro-
duce
PP ? ? Pour NP ; For NP ? (5)
The rules extracted by this scheme are very sim-
ilar to those produced by GHKM (Galley et al,
2004), in particular resulting in the ?composed
rules? of Galley et al (2006), though we use sim-
pler heuristics for handling of unaligned words and
scoring in order to bring the model in line with both
Hiero and SAMT baselines. Under this scheme we
throw away a lot of useful translation rules that don?t
translate exact syntactic constituents. For example,
we can?t label
X ? ? Pour la majorite? des ; For most ? (6)
because no single node exactly spans For most: the
PP node includes people, and the NP node doesn?t
include For.
We can alleviate this problem by changing the
way we get syntactic labels from parse trees.
3.4 SAMT
The Syntax-Augmented Machine Translation
(SAMT) model (Zollmann and Venugopal, 2006)
extracts more rules than the other syntactic model
by allowing different labels for the rules. In SAMT,
we try several different ways to get a label for a
span, stopping the first time we can assign a label:
? As in simple phrase structure labeling, if a sub-
tree of the parse tree exactly spans a phrase, we
assign that phrase the subtree?s root label.
? If a phrase can be covered by two adjacent sub-
trees with labels A and B, we assign their con-
catenation A+B.
? If a phrase spans part of a subtree labeled A that
could be completed with a subtree B to its right,
we assign A/B.
225
? If a phrase spans part of a subtree A but is miss-
ing a B to its left, we assign A\B.
? Finally, if a phrase spans three adjacent sub-
trees with labels A, B, and C, we assign
A+B+C.
Only if all of these assignments fail do we throw
away the potential translation rule.
Under SAMT, we can now label Rule 6. For is
spanned by an IN node, and most is spanned by a JJ
node, so we concatenate the two and label the rule
as
IN+JJ? ? Pour la majorite? des ; For most ? (7)
3.5 CCG 1-best derivation labeling
Our first CG model is similar to the first phrase struc-
ture parse tree model. We start with a word-aligned
sentence pair, but we parse the target sentence using
a CCG parser instead of a phrase structure parser.
When we extract a rule, we see if the consistent
phrase pair is exactly spanned by a category gener-
ated in the 1-best CCG derivation of the target sen-
tence. If there is such a category, we assign that cat-
egory label to the non-terminal. If not, we throw
away the rule.
To continue our extended example, suppose the
English side of Figure 3 was analyzed by a CCG
parser to produce
For most people
(S/S)/N N/N N
>
N
>
S/S
Then just as in the phrase structure model, we
project the syntactic labels down onto the extractable
rule yielding
S/S ? ? Pour N ; For N ? (8)
This does not take advantage of CCG?s ability to
label almost any fragment of language: the frag-
ments with labels in any particular sentence depend
on the order that categories were combined in the
sentence?s 1-best derivation. We can?t label Rule 6,
because no single category spanned For most in the
derivation. In the next model, we increase the num-
ber of spans we can label.
S/S
S/S N
(S/S)/N N/N N
For peoplemost
Figure 4: A portion of the parse chart for a sentence
starting with ?For most people . . . .? Note that the
gray chart cell is not included in the 1-best derivation
of this fragment in Section 3.5.
3.6 CCG parse chart labeling
For this model, we do not use the 1-best CCG deriva-
tion. Instead, when parsing the target sentence, for
each cell in the parse chart, we read the most likely
label according to the parsing model. This lets us as-
sign a label for almost any span of the sentence just
by reading the label from the parse chart.
For example, Figure 4 represents part of a CCG
parse chart for our example fragment of ?For most
people.? Each cell in the chart shows the most prob-
able label for its span. The white cells of the chart
are in fact present in the 1-best derivation, which
means we could extract Rule 8 just as in the previous
model.
But the 1-best derivation model cannot label Rule
6, and this model can. The shaded chart cell in Fig-
ure 4 holds the most likely category for the span For
most. So we assign that label to the X:
S/S ? ? Pour la majorite? des ; For most ? (9)
By including labels from cells that weren?t used
in the 1-best derivation, we can greatly increase the
number of rules we can label.
4 Comparison of resulting grammars
4.1 Effect of grammar size and label set on
parsing efficiency
There are sound theoretical reasons for reducing the
number of non-terminal labels in a grammar. Trans-
lation with a synchronous context-free grammar re-
quires first parsing with the source-language projec-
tion of the grammar, followed by intersection of the
226
target-language projection of the resulting grammar
with a language model. While there are many possi-
ble algorithms for these operations, they all depend
on the size of the grammar.
Consider for example the popular cube pruning
algorithm of Chiang (2007), which is a simple ex-
tension of CKY. It works by first constructing a set
of items of the form ?A, i, j?, where each item corre-
sponds to (possibly many) partial analyses by which
nonterminal A generates the sequence of words from
positions i through j of the source sentence. It then
produces an augmented set of items ?A, i, j, u, v?, in
which items of the first type are augmented with left
and right language model states u and v. In each
pass, the number of items is linear in the number of
nonterminal symbols of the grammar. This observa-
tion has motivated work in grammar transformations
that reduce the size of the nonterminal set, often re-
sulting in substantial gains in parsing or translation
speed (Song et al, 2008; DeNero et al, 2009; Xiao
et al, 2009).
More formally, the upper bound on parsing com-
plexity is always at least linear in the size of the
grammar constant G, where G is often loosely de-
fined as a grammar constant; Iglesias et al (2011)
give a nice analysis of the most common translation
algorithms and their dependence on G. Dunlop et
al. (2010) provide a more fine-grained analysis of G,
showing that for a variety of implementation choices
that it depends on either or both the number of rules
in the grammar and the number of nonterminals in
the grammar. Though these are worst-case analyses,
it should be clear that grammars with fewer rules or
nonterminals can generally be processed more effi-
ciently.
4.2 Number of rules and non-terminals
Table 2 shows the number of rules we can extract
under various labeling schemes. The rules were ex-
tracted from an Urdu?English parallel corpus with
202,019 translations, or almost 2 million words in
each language.
As we described before, moving from the phrase-
structure syntactic model to the extended SAMT
model vastly increases the number of translation
rules ? from about 7 million to 40 million rules.
But the increased rule coverage comes at a cost: the
non-terminal set has increased in size from 70 (the
Model Rules NTs
Hiero 4,171,473 1
Syntax 7,034,393 70
SAMT 40,744,439 18,368
CG derivations 8,042,683 505
CG parse chart 28,961,161 517
Table 2: Number of translation rules and non-
terminal labels in an Urdu?English grammar under
various models.
size of the set of Penn Treebank tags) to over 18,000.
Comparing the phrase structure syntax model to
the 1-best CCG derivation model, we see that the
number of extracted rules increases slightly, and the
grammar uses a set of about 500 non-terminal labels.
This does not seem like a good trade-off; since we
are extracting from the 1-best CCG derivation there
really aren?t many more rules we can label than with
a 1-best phrase structure derivation.
But when we move to the full CCG parse chart
model, we see a significant difference: when read-
ing labels off of the entire parse chart, instead of
the 1-best derivation, we don?t see a significant in-
crease in the non-terminal label set. That is, most
of the labels we see in parse charts of the train-
ing data already show up in the top derivations: the
complete chart doesn?t contain many new labels that
have never been seen before.
But by using the chart cells, we are able to as-
sign syntactic information to many more translation
rules: over 28 million rules, for a grammar about 34
the size of SAMT?s. The parse chart lets us extract
many more rules without significantly increasing the
size of the syntactic label set.
4.3 Sparseness of nonterminals
Examining the histograms in Figure 5 gives us a
different view of the non-terminal label sets in our
models. In each histogram, the horizontal axis mea-
sures label frequency in the corpus. The height of
each bar shows the number of non-terminals with
that frequency.
For the phrase structure syntax model, we see
there are maybe 20 labels out of 70 that show up
on rules less than 1000 times. All the other labels
show up on very many rules.
227
More sparse Less sparse
1 10 102 103 104 105 106
Label Frequency (logscale)
1
10
1
10
1
10
1
10
102
103
N
um
be
r
of
L
ab
el
s
(l
og
sc
al
e)
Phrase structure
CCG 1-best
CCG chart
SAMT
Figure 5: Histograms of label frequency for each model, illustrating the sparsity of each model.
Moving to SAMT, with its heuristically-defined
labels, shows a very different story. Not only does
the model have over 18,000 non-terminal labels, but
thousands of them show up on fewer than 10 rules
apiece. If we look at the rare label types, we see that
a lot of them are improbable three way concatena-
tions A+B+C.
The two CCG models have similar sparseness
profiles. We do see some rare labels occurring only
a few times in the grammars, but the number of
singleton labels is an order of magnitude smaller
than SAMT. Most of the CCG labels show up in
the long tail of very common occurrences. Interest-
ingly, when we move to extracting labels from parse
charts rather than derivations, the number of labels
increases only slightly. However, we also obtain a
great deal more evidence for each observed label,
making estimates more reliable.
5 Experiments
5.1 Data
We tested our models on an Urdu?English transla-
tion task, in which syntax-based systems have been
quite effective (Baker et al, 2009; Zollmann et al,
2008). The training corpus was the National Insti-
tute of Standards and Technology Open Machine
Translation 2009 Evaluation (NIST Open MT09).
According to the MT09 Constrained Training Con-
ditions Resources list2 this data includes NIST Open
MT08 Urdu Resources3 and the NIST Open MT08
Current Test Set Urdu?English4. This gives us
202,019 parallel translations, for approximately 2
million words of training data.
5.2 Experimental design
We used the scripts included with the Moses MT
toolkit (Koehn et al, 2007) to tokenize and nor-
malize the English data. We used a tokenizer and
normalizer developed at the SCALE 2009 workshop
(Baker et al, 2009) to preprocess the Urdu data. We
used GIZA++ (Och and Ney, 2000) to perform word
alignments.
For phrase structure parses of the English data, we
used the Berkeley parser (Petrov and Klein, 2007).
For CCG parses, and for reading labels out of a parse
chart, we used the C&C parser (Clark and Curran,
2007).
After aligning and parsing the training data, we
used the Thrax grammar extractor (Weese et al,
2011) to extract all of the translation grammars.
We used the same feature set in all the transla-
tion grammars. This includes, for each rule C ?
?f ; e?, relative-frequency estimates of the probabil-
2http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3LDC2009E12
4LDC2009E11
228
Model BLEU sec./sent.
Hiero 25.67 (0.9781) 0.05
Syntax 27.06 (0.9703) 3.04
SAMT 28.06 (0.9714) 63.48
CCG derivations 27.3 (0.9770) 5.24
CCG parse chart 27.64 (0.9673) 33.6
Table 3: Results of translation experiments on
Urdu?English. Higher BLEU scores are better.
BLEU?s brevity penalty is reported in parentheses.
ities p(f |A), p(f |e), p(f |e,A), p(e|A), p(e|f), and
p(e|f,A).
The feature set alo includes lexical weighting for
rules as defined by Koehn et al (2003) and various
binary features as well as counters for the number of
unaligned words in each rule.
To train the feature weights we used the Z-MERT
implementation (Zaidan, 2009) of the Minimum
Error-Rate Training algorithm (Och, 2003).
To decode the test sets, we used the Joshua ma-
chine translation decoder (Weese et al, 2011). The
language model is a 5-gram LM trained on English
GigaWord Fourth Edition.5
5.3 Evaluation criteria
We measure machine translation performance using
the BLEU metric (Papineni et al, 2002). We also
report the translation time for the test set in seconds
per sentence. These results are shown in Table 3.
All of the syntactic labeling schemes show an im-
provement over the Hiero model. Indeed, they all
fall in the range of approximately 27?28 BLEU. We
can see that the 1-best derivation CCG model per-
forms slightly better than the phrase structure model,
and the CCG parse chart model performs a little bet-
ter than that. SAMT has the highest BLEU score.
The models with a larger number of rules perform
better; this supports our assertion that we shouldn?t
throw away too many rules.
When it comes to translation time, the three
smaller models (Hiero, phrase structure syntax, and
CCG 1-best derivations) are significantly faster than
the two larger ones. However, even though the CCG
parse chart model is almost 34 the size of SAMT in
terms of number of rules, it doesn?t take 34 of the
5LDC2009T13
time. In fact, it takes only half the time of the SAMT
model, thanks to the smaller rule label set.
6 Discussion and Future Work
Finding an appropriate mechanism to inform phrase-
based translation models and their hierarchical vari-
ants with linguistic syntax is a difficult problem
that has attracted intense interest, with a variety
of promising approaches including unsupervised
clustering (Zollmann and Vogel, 2011), merging
(Hanneman et al, 2011), and selection (Mylonakis
and Sima?an, 2011) of labels derived from phrase-
structure parse trees very much like those used by
our baseline systems. What we find particularly
attractive about CCG is that it naturally assigns
linguistically-motivated labels to most spans of a
sentence using a reasonably concise label set, possi-
bility obviating the need for further refinement. In-
deed, the analytical flexibility of CCG has motivated
its increasing use in MT, from applications in lan-
guage modeling (Birch et al, 2007; Hassan et al,
2007) to more recent proposals to incorporate it into
phrase-based (Mehay, 2010) and hierarchical trans-
lation systems (Auli, 2009).
Our new model builds on these past efforts, rep-
resenting a more fully instantiated model of CCG-
based translation. We have shown that the label
scheme allows us to keep many more translation
rules than labels based on phrase structure syntax,
extracting almost as many rules as the SAMT model,
but keeping the label set an order of magnitude
smaller, which leads to more efficient translation.
This simply scratches the surface of possible uses of
CCG in translation. In future work, we plan to move
from a formally context-free to a formally CCG-
based model of translation, implementing combina-
torial rules such as application, composition, and
type-raising.
Acknowledgements
Thank you to Michael Auli for providing code to
inspect the full chart from the C&C parser. This
research was supported in part by the NSF under
grant IIS-0713448 and in part by the EuroMatrix-
Plus project funded by the European Commission
(7th Framework Programme). Opinions, interpreta-
tions, and conclusions are the authors? alone.
229
References
Kazimierz Adjukiewicz. 1935. Die syntaktische kon-
nexita?t. In Storrs McCall, editor, Polish Logic 1920?
1939, pages 207?231. Oxford University Press.
Michael Auli. 2009. CCG-based models for statisti-
cal machine translation. Ph.D. Proposal, University
of Edinburgh.
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine,
Mike Kayser, Lori Levin, Justin Marinteau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation: Fi-
nal report of the 2009 summer camp for advanced lan-
guage exploration (scale). Technical report, Human
Language Technology Center of Excellence.
Yehoshua Bar-Hillel, Chaim Gaifman, and Eliyahu
Shamir. 1964. On categorial and phrase-structure
grammars. In Yehoshua Bar-Hillel, editor, Language
and Information, pages 99?115. Addison-Wesley.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proc. of WMT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics, 33(4).
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proc. EMNLP.
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In
Proc. NAACL, pages 227?235.
Aaron Dunlop, Nathan Bodenstab, and Brian Roark.
2010. Reducing the grammar constant: an analysis
of CYK parsing efficiency. Technical report CSLU-
2010-02, OHSU.
Timothy A. D. Fowler and Gerald Penn. 2010. Accu-
rate context-free parsing with combinatory categorial
grammar. In Proc. ACL, pages 335?344.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich
syntactic translation models. In In ACL, pages 961?
968.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proc. of WMT.
Hany Hassan, Khalil Sima?an, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proc. of ACL.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adria`
de Gispert, and Michael Riley. 2011. Hierarchi-
cal phrase-based translation representations. In Proc.
EMNLP, pages 1373?1383.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Frederico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demonstration Session.
Dennis Mehay. 2010. Linguistically motivated syntax
for machine translation. Ph.D. Proposal, Ohio State
University.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proc. of ACL-HLT.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. HLT-NAACL.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In Proc.
EMNLP, pages 167?176.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Kersti
Bo?rjars, editors, Non-Transformational Syntax. Wiley-
Blackwell.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proc. of
EMNLP.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proc. of WMT.
Tong Xiao, Mu Li, Dongdong Zhang, Jingbo Zhu, and
Ming Zhou. 2009. Better synchronous binarization
for machine translation. In Proc. EMNLP, pages 362?
370.
230
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91(1):79?88.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings on the Workshop on Statistical Machine
Translation.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proc. of ACL-HLT.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
MT. In Proc. of COLING.
231
