Using Semantic Preferences to Identify Verbal Participation in 
Role Switching Alternations. 
Diana McCar thy  
Cognit ive & Comput ing  Sciences, 
University of Sussex 
Brighton BN1 9QH, UK 
d ianam @cogs.susx. ac. uk 
Abst rac t  
We propose a method for identifying diathesis alter- 
nations where a particular argument type is seen in 
slots which have different grammatical roles in the 
alternating forms. The method uses selectional pref- 
erences acquired as probability distributions over 
WordNet. Preferences for the target slots are com- 
pared using a measure of distributional similarity. 
The method is evaluated on the causative and cona- 
tive alternations, but is generally applicable and 
does not require a priori knowledge specific to the 
alternation. 
1 In t roduct ion  
Diathesis alternations are alternate ways in which 
the arguments of a verb are expressed syntactically. 
The syntactic hanges are sometimes accompanied 
by slight changes in the meaning of the verb. An ex- 
ample of the causative alternation is given in (1) be- 
low. In this alternation, the object of the transitive 
variant can also appear as the subject of the intransi- 
tive variant. In the conative alternation, the transi- 
tive form alternates with a prepositional phrase con- 
struction involving either at or on. An example of 
the conative alternation is given in (2). 
1. The boy broke the window ~-* The window 
broke. 
2. The boy pulled at the rope *-* The boy pulled 
the rope. 
We refer to alternations where a particular seman- 
tic role appears in different grammatical roles in al- 
ternate realisations as "role switching alternations" 
(RSAS). It is these alternations that our method ap- 
plies to. 
Recently, there has been interest in corpus-based 
methods to identify alternations (McCarthy and Ko- 
rhonen, 1998; Lapata, 1999), and associated verb 
classifications (Stevenson and Merlo, 1999). These 
have either elied on a priori knowledge specified for 
the alternations in advance, or are not suitable for 
a wide range of alternations. The fully automatic 
method outlined here is applied to the causative 
and conative alternations, but is applicable to other 
RSAS. 
2 Motivation 
Diathesis alternations have been proposed for a 
number of NLP tasks. Several researchers have sug- 
gested using them for improving lexical acquisition. 
Korhonen (1997) uses them in subcategorization 
frame (SCF) acquisition to improve the performance 
of a statistical filter which determines whether a 
SCF observed for a particular verb is genuine or not. 
They have also been suggested for the recovery of 
predicate argument structure, necessary for SCF ac- 
quisition (Briscoe and Carroll, 1997; Boguraev and 
Briscoe, 1987). And Ribas (1995) showed that selec- 
tional preferences acquired using alternations per- 
formed better on a word sense disambiguation task 
compared to preferences acquired without alterna- 
tions. He used alternations to indicate where the 
argument head data from different slots can be com- 
bined since it occupies the same semantic relation- 
ship with the predicate. 
Different diathesis alternations give different em- 
phasis and nuances of meaning to the same basic 
content. These subtle changes of meaning are impor- 
tant in natural language generation (Stede, 1998). 
Alternations provide a means of reducing redun- 
dancy in the lexicon since the alternating scFs need 
not be enumerated for each individual verb if a 
marker is used to specify which verbs the alterna- 
tion applies to. Alternations also provide a means 
of generalizing patterns of behaviour over groups of 
verbs, typically the group members are semantically 
related. Levin (1993) provides aclassification ofover 
3000 verbs according to their participation i alter- 
nations involving NP and PP constituents. Levin's 
classification is not intended to be exhaustive. Au- 
tomatic identification ofalternations would be a use- 
ful tool for extending the classification with new 
participants. Levin's taxonomy might also be used 
alongside observed behaviour, to predict unseen be- 
haviour. 
Levin's classification has been extended by other 
NLP researchers (Doff and Jones, 1996; Dang et al, 
256 
<Root> 
I  bs,r  t,on  i uoao activ,t e..t 
i /  ~ . . . . . . .  ~ construction time 
car  war  l measure \] /"( time \] ( relation \] migration meal ceremonial 
,, drum 
---0a- t . . . . .  / 
\[time_period\] 
week 
month 
afternoon 
A 
time T 
fo .v,=t,4 
speech 
yelling 
migration 
Figure 1: TCM for the object slot of the transitive frame of start. 
1998). Dang et al (1998) modify it by adding new 
classes which remove the overlap between classes 
from the original scheme. Dorr and Jones (1996) 
extend the classification by using grammatical in- 
formation in LDOCE alongside semantic information 
in WordNet. What is missing is a way of classifying 
verbs when the relevant information is not available 
in a manmade resource. Using corpora by-passes 
reliance on the availability and adequacy of MRDs. 
Additionally, the frequency information in corpora is 
helpful for estimating alternation productivity (La- 
pata, 1999). Estimations of productivity have been 
suggested for controlling the application of alterna- 
tions (Briscoe and Copestake, 1996). We propose a 
method to acquire knowledge of alternation partic- 
ipation directly from corpora, with frequency infor- 
mation available as a by-product. 
3 Method 
We use both syntactic and semantic information for 
identifying participants in RSAs. Firstly, syntactic 
processing is used to find candidates taking the alter- 
nating SeEs. Secondly, selectional preference models 
are acquired for the argument heads associated with 
a specific slot in a specific SCF of a verb. 
We use the SCF acquisition system of Briscoe and 
Carroll (1997), with a probabilistic LR parser (Inui et 
al., 1997) for syntactic processing. The corpus data 
is POS tagged and lemmatised before the LR parser 
is applied. Subcategorization patterns are extracted 
from the parses, these include both the syntactic at- 
egories and the argument heads of the constituents. 
These subcategorization patterns are then classified 
according to a set of 161 SeE classes. The SeE en- 
tries for each verb are then subjected to a statistical 
filter which removes SCFs that have occurred with 
a frequency less than would be expected by chance. 
The resulting SCF lexicon lists each verb with the 
SCFs it takes. Each SCF entry includes a frequency 
count and lists the argument heads at all slots. 
Selectional preferences are automatically acquired 
for the slots involved in the role switching. We refer 
to these as the target slots. For the causative al- 
ternation, the slots are the direct object slot of the 
transitive SCF and the subject slot of the intransi- 
tive. For the conative, the slots are the direct object 
of the transitive and the PP of the np v pp SCF. 
Selectional preferences are acquired using the 
method devised by Li and Abe (1995). The pref- 
erences for a slot are represented as a tree cut model 
(TCM).  This is a set of disjoint classes that partition 
the leaves of the WordNet noun hypernym hierar- 
chy. A conditional probability is attached to each of 
the classes in the set. To ensure the TCM covers all 
the word senses in WordNet, we modify Li and Abe's 
original scheme by creating hyponym leaf classes be- 
low all WordNet's hypernym (internal) classes. Each 
leaf holds the word senses previously held at the in- 
ternal class. The nominal argument heads from a 
target slot are collected and used to populate the 
WordNet hierarchy with frequency information. The 
head lemmas are matched to the classes which con- 
tain them as synonyms. Where a lemma appears as a 
synonym in more than one class, its frequency count 
is divided between all classes for which it has direct 
membership. The frequency counts from hyponym 
classes are added to the count for each hypernym 
class. A root node, created above all the WordNet 
roots, contains the total frequency count for all the 
argument head lemmas found within WordNet. The 
minimum description length principle (MDL) (Rissa- 
nen, 1978) is used to find the best TCM by consid- 
257 
ering the cost (in bits) of describing both the model 
and the argument head data encoded in the model. 
The cost (or description length) for a TCM is cal- 
culated according to equation 1. The number of 
parameters of the model is given by k, this is the 
number of classes in the TCM minus one. S is the 
sample size of the argument head data. The cost of 
describing each argument head (n) is calculated us- 
ing the log of the probability estimate for the classes 
on the TCM that n belongs to (Cn). 
k 
description length = ~ x log IS I -  E logp(cn) (1) 
nES 
A small portion of the TCM for the object slot of 
start in the transitive frame is displayed in figure 1. 
WordNet classes are displayed in boxes with a label 
which best reflects the sense of the class. The prob- 
ability estimates are shown for the classes along the 
TCM. Examples of the argument head data are dis- 
played below the WordNet classes with dotted lines 
indicating membership at a hyponym class beneath 
these classes. 
We assume that verbs which participate will show 
a higher degree of similarity between the preferences 
at the target slots compared with non-participating 
verbs. To compare the preferences we compare the 
probability distributions across WordNet using a 
measure of distributional similarity. Since the prob- 
ability distributions may be at different levels of 
WordNet, we map the TCMs at the target slots to a 
common tree cut, a "base cut". We experiment with 
two different ypes of base cut. The first is simply a 
base cut at the eleven root classes of WordNet. We 
refer to this as the "root base cut" (I~BC). The sec- 
ond is termed the "union base cut" (tJBC). This is 
obtained by taking all classes from the union of the 
tWO TCMs which are not subsumed by another class 
in this union. Duplicates are removed. Probabilities 
are assigned to the classes of a base cut using the 
estimates on the original TCM. The probability esti- 
mate for a hypernym class is obtained by combining 
the probability estimates for all its hyponyms on the 
original cut. Figure 2 exemplifies this process for two 
TOMs (TCM1 and TCM2) in an imaginary hierarchy. 
The UBC is at the classes B, c and D. 
To quantify the similarity between the probability 
distributions for the target slots we use the a-skew 
divergence (aSD) proposed by Lee (1999). 1 This 
measure, defined in equation 2, is a smoothed version 
of the Kulback-Liebler divergence, pl(x)  and p2(x) 
are the two probability distributions which are being 
compared. The ~ constant is a value between 0 and 
1 We also experimented with euclidian distance, the L1 
norm, and cosine measures. The differences in performance 
of these measures were not statistically significant. 
1 which smooths pl(x) with p2(z) so that ~SD is 
always defined. We use the same value (0.99) for 
as Lee. If a is set to 1 then this measure is equivalent 
to the Kulback-Liebler divergence. 
asd(p l (x ) ,p2(x ) )  = x p l ( z ) )  + 
((1 - . )  ? (2) 
4 Exper imenta l  Eva luat ion  
We experiment with a SCF lexicon produced from 
19.3 million words of parsed text from the BNC 
(Leech, 1992). We used the causative and conative 
alternations, ince these have enough candidates in 
our lexicon for experimentation. Evaluation is per- 
formed on verbs already filtered by the syntactic 
processing. The SCF acquisition system has been 
evaluated elsewhere (Briscoe and Carroll, 1997). 
We selected candidate verbs which occurred with 
10 or more nominal argument heads at the target 
slots. The argument heads were restricted to those 
which can be classified in the WordNet hypernym hi- 
erarchy. Candidates were selected by hand so as to 
obtain an even split between candidates which did 
participate in the alternation (positive candidates) 
and those which did not (negative candidates). Four 
human judges were used to determine the "gold stan- 
dard". The judges were asked to specify a yes or 
no decision on participation for each verb. They 
were Mso permitted a don't know verdict. The kappa 
statistic (Siegel and Castellan, 1988) was calculated 
to ensure that there was significant agreement be- 
tween judges for the initial set of candidates. From 
these, verbs were selected which had 75% or more 
agreement, i.e. three or more judges giving the same 
yes or no decision for the verb. 
For the causative alternation we were left with 46 
positives and 53 negatives. For the conative alter- 
nation we had 6 of each. In both cases, we used the 
Mann Whitney U test to see if there was a signifi- 
cant relationship between the similarity measure and 
participation. We then used a threshold on the sim- 
ilarity scores as the decision point for participation 
to determine a level of accuracy. We experimented 
with both the mean and median of the scores as a 
threshold. Seven of the negative causative candi- 
dates were randomly chosen and removed to ensure 
an even split between positive and negative candi- 
dates for determining accuracy using the mean and 
median as thresholds. 
The following subsection describes the results of 
the experiments using the method described in sec- 
tion 3 above. Subsection 4.2 describes an experiment 
on the same data to determine participation using a 
similarity measure based on the intersection of the 
lemmas at the target slots. 
258 
- _ _ 0 .4  ~ g ~  New TCM1 
E F G H I J 
New TCM2 - , ,  
E F G H I J 
Figure 2: New TCMs at the union base cut 
4.1 Using Syntax and Selectional 
Pre ferences  
The results for the causative alternation are dis- 
played in table 1 for both the rt~c and the uBc. The 
relationship between participation and ~SD is highly 
significant in both cases, with values of p well below 
0.01. Accuracy for the mean and median thresholds 
are displayed in the fourth and fifth columns. Both 
thresholds outperform the random baseline of 50%. 
The results for the vl3c are slightly improved, com- 
pared to those for the rtBc, however the improve- 
ment is not significant. 
The numbers of false negative (FN) and false posi- 
tive (FP) errors for the mean and median thresholds 
are displayed in table 2, along with the threshold and 
accuracy. The outcomes for each individual verb for 
the experiment using the RBC and the mean thresh- 
old are as follows: 
? True negatives: 
add admit answer believe borrow cost declare de- 
mand expect feel imagine know notice pay per- 
form practise proclaim read remember sing sur- 
vive understand win write 
? True positives: 
accelerate bang bend boil break burn change 
close cook cool crack decrease drop dry end ex- 
pand fly improve increase match melt open ring 
rip rock roll shatter shut slam smash snap spill 
split spread start stop stretch swing lilt turn 
wake 
? False negatives: 
flood land march repeat terminate 
? False positives: 
ask attack catch choose climb drink eat help kick 
knit miss outline pack paint plan prescribe pull 
remain steal suck warn wash 
The results for the uBc experiment are very similar. 
I f  the median is used, the number of FPs and FNs 
are evenly balanced. This is because the median 
threshold is, by definition, taken midway between 
the test items arranged in order of their similarity 
scores. There are an even number of items on either 
side of the decision point, and an even number of 
positive and negative candidates in our test sample. 
Thus, the errors on either side of the decision point 
are equal in number. 
For both base cuts, there are a larger number of 
false positives than false negatives when the mean 
is used. The mean produces a higher accuracy than 
the median, but gives an increase in false positives. 
Many false positives arise where the preferences at 
both target slots are near neighbours in WordNet. 
For example, this occurred for eat and drink. There 
verbs have a high probability mass (around 0.7) un- 
der the ent i ty  class in both target slots, since both 
people and types of food occur under this class. In 
cases like these, the probability distributions at the 
asc ,  and frequently the UBC, are not sufficiently dis- 
tinctive. 
The polysemy of the verbs may provide another 
explanation for the large quantity of false positives. 
The SCFS and data of different senses should not 
ideally be combined, at least not for coarse grained 
sense distinctions. We tested the false positive and 
true negative candidates to see if there was a re- 
lationship between the polysemy of a verb and its 
misclassification. The number of senses (according 
to WordNet) was used to indicate the polysemy of a 
verb. The Mann Whitney U test was performed on 
259 
RBC 
UBC 
Mann Whitney z 
-4.03 
-4.3 
significance (p) mean median 
0.0003 71 63 
0.00003 73 70 
Table 1: Causative results 
base cut 
UBC 
UBC 
RBC 
RBC 
threshold type 
mean 
median 
mean 
median 
threshold 
0.38 
0.20 
0.32 " 
0.15 
accuracy o~ num FPs 
73 21 
70 14 
71 22 
63 '17 
num FNs 
14 
17 
Table 2: Error analysis for the causative xperiments 
the verbs found to be true negative and false positive 
using the Rat .  A significant relationship was not 
found between participation and misclassification. 
Both groups had an average of 5 senses per verb. 
This is not to say that distinguishing verb senses 
would not improve performance, provided that there 
was sufficient data. However, verb polysemy does 
not appear to be a major source of error, from our 
preliminary analysis. In many eases, such as read 
which was classified both by the judges, and the sys- 
tem as a negative candidate, the predominant sense 
of the verb provides the majority of the data. Alter- 
nate senses, for example, the book reads well, often 
do not contribute nough data so as to give rise to 
a large proportion of errors. Finding an appropriate 
inventory of senses would be difficult, since we would 
not wish to separate related senses which occur as 
alternate variants of one another. The inventory 
would therefore require knowledge of the phenomena 
that we are endeavouring to acquire automatically. 
To show that our method will work for other RSAS, 
we use the conative. Our sample size is rather small 
since we are limited by the number of positive can- 
didates in the corpus having sufficient frequency for 
both sets.  The sparse data problem is acute when 
we look at alternations with specific prepositions. A
sample of 12 verbs (6 positive and 6 negative) re- 
mained after the selection process outlined above. 
For this small sample we obtained a significant re- 
sult (p = 0.02) with a mean accuracy of 67% and 
a median accuracy of 83%. On this occasion, the 
median performed better than the mean. More data 
is required to see if this difference is significant. 
4.2 Us ing  Syntax  and  Lemmas 
This experiment was conducted using the same data 
as that used in the previous subsection. In this ex- 
periment, we used a similarity score on the argument 
heads directly, instead of generalizing the argument 
heads to WordNet classes. The venn diagram in fig- 
ure 3 shows a subset of the lemmas at the transitive 
and intransitive SCFs for the verb break. 
The lemma based similarity measure is termed 
lemmaoverlap (LO) and is given in equation 3, where 
A and B represent the target slots. LO is the size of 
the intersection of the multisets of argument heads 
at the target slots, divided by the size of the smaller 
of the two multisets. The intersection of two mul- 
tisets includes duplicate items only as many times 
as the item is in both sets. For example, if one 
slot contained the argument heads {person, person, 
person, child, man, spokeswoman}, and the other 
slot contained {person, person, child, chair, collec- 
tion}, then the intersection would be {person, per- 
3 son, child}, and LO would be g. This measure ranges 
between zero (no overlap) and I (where one set is a 
proper subset of that at the other slot). 
Lo(A, B) = Imuttiset inlerseetion(A B)I (3) 
Ismallest set(A, B)I 
Using the Mann Whitney U test on the LO scores, 
we obtained a z score of 2.00. This is significant o 
the 95% level, a lower level than that for the class- 
based experiments. The results using the mean and 
median of the LO scores are shown in table 3. Perfor- 
mance is lower than that for the class-based experi- 
ments. The outcome for the individual verbs using 
the mean as a threshold was:- 
? True negatives: 
add admit answer borrow choose climb cost de- 
clare demand drink eat feel imagine notice out- 
line pack paint perform plan practise prescribe 
proclaim read remain sing steal suck survive un- 
derstand wash win write 
? True positives: 
bend boil burn change close cool dry end fly im- 
prove increase match melt open ring roll shut 
slam smash Mart stop tilt wake 
? False negatives: 
accelerate bang break cook crack decrease drop 
expand flood land march repeat rip rock shatter 
260 
_ ~ ~ ~Objects  of Subjects _~ / ~ "~ntransitive 
Intra/~i:e /silence ~ 
/ / \ 
\ war. \ back /ground / 
y weather X dead,oc k /diet / 
Figure 3: Lemmas at the causative target slots of break 
snap spill split spread stretch swing terminate 
turn 
? False positives: 
ask attack believe catch expect help kick knit 
know miss pay pull remember warn 
Interestingly, the errors for the LO measure tend 
to be false negatives, rather than false positives. The 
LO measure is much more conservative than the ap- 
proach using the TCMS. In this case the median 
threshold produces better results. 
For the conative alternation, the lemma based 
method does not show a significant relationship be- 
tween participation and the LO scores. Moreover, 
there is no difference between the sums of the ranks 
of the two groups for the Mann Whitney U test. 
The mean produces an accuracy of 58% whilst the 
median produces an accuracy of 50%. 
5 Re la ted  Work  
There has been some recent interest in observing 
alternations in corpora (McCarthy and Korhonen, 
1998; Lapata, 1999) and predicting related verb 
classifications (Stevenson and Merlo, 1999). Ear- 
lier work by Resnik (1993) demonstrated a link be- 
tween selectional preference strength and participa- 
tion in alternations where the direct object is omit- 
ted. Resnik used syntactic information from the 
bracketing within the Penn Treebank corpus. Re- 
search into the identification of other diathesis al- 
ternations has been advanced by the availability 
of automatic syntactic processing. Most work us- 
ing corpus evidence for verb classification has re- 
lied on a priori knowledge in the form of linguistic 
cues specific to the phenomena being observed (La- 
pata, 1999; Stevenson and Merlo, 1999). Our ap- 
proach, whilst being applicable only to RSAs, does 
not require human input specific to the alternation 
at hand. 
Lapata (1999) identifies participation i the dative 
and benefactive alternations. Lapata's trategy is to 
identify participants using a shallow parser and vari- 
ous linguistic and semantic ues, which are specified 
manually for these two alternations. PP attachments 
are resolved using Hindle and Rooth's (1993) lexical 
association score. Compound nouns, which could be 
mistaken for the double object construction, were 
filtered using the log-likelihood ratio test. The se- 
mantic cues were obtained by manual analysis. The 
relative frequency of a SCF for a verb, compared to 
the total frequency of the verb, was used for filtering 
out erroneous SCFs. 
Lapata does not report recall and precision fig- 
ures against a gold standard. The emphasis is on 
the phenomena actually evident in the corpus data. 
Many of the verbs listed in Levin as taking the al- 
ternation were not observed with this alternation 
in the corpus data. This amounted to 44% of the 
verbs for the benefactive, and 52% for the dative. 
These figures only take into account the verbs for 
which at least one of the SCFS were observed. 54% 
of the verbs listed for the dative and benefactive by 
Levin were not acquired with either of the target 
SCFs. Conversely, many verbs not listed in Levin 
were identified as taking the benefactive or dative 
alternation using Lapata's criteria. Manual analysis 
of these verbs revealed 18 false positives out of 52 
candidates. 
Stevenson and Merlo (1999) use syntactic and lex- 
ical cues for classifying 60 verbs in three verb classes: 
unergative, unaccusative and verbs with an optional 
direct object. These three classes were chosen be- 
261 
threshold type threshold 
mean 
median 
accuracy % num FPS num FNs 
0.26 60 14 23 
0.23 63 17 17 
Table 3: Accuracy and error analysis for lemma based experiments 
cause a few well defined features, specified a pri- 
ori, can distinguish the three groups. Twenty verbs 
from Levin's classification were used in each class. 
They were selected by virtue of having sufficient fre- 
quency in a combined corpus (from the Brown and 
the wsJ) of 65 million words. The verbs were also 
chosen for having one predominant intended sense in 
the corpus. Stevenson and Merlo used four linguisti- 
cally motivated features to distinguish these groups. 
Counts from the corpus data for each of the four fea- 
tures were normalised to give a score on a scale of 1 
to I00. One feature was the causative non-causative 
distinction. For this feature, a measure similar to 
our LO measure was used. The four features were 
identified in the corpus using automatic POS tagging 
and parsing of the data. The data for half of the 
verbs in each class was subject to manual scrutiny, 
after initial automatic processing. The rest of the 
data was produced fully automatically. The verbs 
were classified automatically using the four features. 
The accuracy of automatic lassification was 52% us- 
ing all four features, compared to a baseline of 33%. 
The best result was obtained using a combination of 
three features. This gave an accuracy of 66%. 
McCarthy and Korhonen (1998) proposed a 
method for identifying rtSAS using MDL. This 
method relied on an estimation of the cost of us- 
ing TCMS to encode the argument head data at a 
target slot. The sum of the costs for the two target 
slots was compared to the cost of a TCM for encoding 
the union of the argument head data over the two 
slots. Results are reported for the causative alterna- 
tion with 15 verbs. This method depends on there 
being similar quantities of data at the alternating 
slots, otherwise the data at the more frequent slot 
overwhelms the data at the less frequent slot. How- 
ever, many alternations involve SCFs with substan- 
tially different relative frequencies, especially when 
one SCF is specific to a particular preposition. We 
carried out some experiments using the MDL method 
and our TCMs. For the causative, we used a sample 
of 110 verbs and obtained 63% accuracy. For the 
conative, a sample of 16 verbs was used and this time 
accuracy was only 56%. Notably, only one negative 
decision was made because of the disparate frame 
frequencies, which reduces the cost of combining the 
argument head data. 
6 Conclusion 
We have discovered a significant relationship be- 
tween the similarity of selectional preferences at the 
target slots, and participation in the causative and 
conative alternations. A threshold, such as the mean 
or median can be used to obtain a level of accuracy 
well above the baseline. A lemma based similarity 
score does not always indicate a significant relation- 
ship and generally produces a lower accuracy. 
There are patterns of diathesis behaviour among 
verb groups (Levin, 1993). Accuracy may be im- 
proved by considering severM alternations collec- 
tively, rather than in isolation. Complementary 
techniques to identify alternations, for example 
(Resnik, 1993), might be combined with ours. 
Although we have reported results on only two 
RSAS, our method is applicable to other such alter- 
nations. Furthermore, such application requires no 
human endeavour, apart from that required for eval- 
uation. However, a considerably larger corpus would 
be required to overcome the sparse data problem for 
other RSA alternations. 
7 Acknowledgements 
Some funding for this work was provided by UK EP- 
SRC project GR/L53175 'PSET: Practical Simplifi- 
cation of English Text'. We also acknowledge Gerald 
Gazdar for his helpful comments on this paper. 
References 
Bran Boguraev and Ted Briscoe. 1987. Large lex- 
icons for natural language processing: Utilising 
the grammar coding system of LDOCE. Compu- 
tational Linguistics, 13(3-4):203-218. 
Ted Briscoe and John Carroll. 1997. Automatic 
extraction of subcategorization from corpora. In 
Fifth Applied Natural Language Processing Con- 
ference, pages 356-363. 
Ted Briscoe and Ann Copestake. 1996. Controlling 
the application of lexical rules. In E Viegas, ed- 
itor, SIGLEX Workshop on Lezieal Semantics - 
ACL 96 Workshop. 
Hoa Trang Dang, Karin Kipper, Martha Palmer, 
and Joseph Rosensweig. 1998. Investigating reg- 
ular sense extensions based on intersective Levin 
classes. In Proceedings of the 171h International 
Conference on Computational Linguistics and the 
36th Annual Meeting of the Association for Com- 
putational Linguistics, volume 1, pages 293-299. 
262 
Bonnie J. Dorr and Doug Jones. 1996. Role of word 
sense disambiguation i lexieal acquisition: Pre- 
dicting semantics from syntactic ues. In Proceed- 
ings of the 16lh International Conference on Com- 
putational Linguistics, COLING-96, pages 322- 
327. 
Donald Hindle and Mats Rooth. 1993. Structural 
ambiguity and lexieal relations. Computational 
Linguistics, 19(1):103-120. 
Kentaro Inui, Viraeh Sornlertlamvanich, Hozumi 
Tanaka, and Takenobu Tokunaga. 1997. A new 
formalization of probabilistic glr parsing. In 
5th ACL/SIGPARSE International Workshop on 
Parsing Technologies, pages 123-134, Cambridge, 
MA. 
Anna Korhonen. 1997. Acquiring Subcategorisation 
from Textual Corpora. Master's thesis, University 
of Cambridge. 
Maria Lapata. 1999. Acquiring lexieal generaliza- 
tions from corpora: A case study for diathe- 
sis alternations. In Proceedings of the 37th An- 
nual Meeting of the Association for Computa- 
tional Linguistics, pages 397-404. 
Lillian Lee. 1999. Measures of distributional simi- 
larity. In Proceedings of the 37th Annual Meeting 
of the Association for Computational Linguistics, 
pages 25-32. 
Geoffrey Leech. 1992. 100 million words of English: 
the British National Corpus. Language Research, 
28(1):1-13. 
Beth Levin. 1993. English Verb Classes and Alter- 
nations: a Preliminary Investigation. University 
of Chicago Press, Chicago and London. 
Hang Li and Naoki Abe. 1995. Generalizing case 
frames using a thesaurus and the MDL principle. 
In Proceedings of the International Conference on 
Recent Advances in Natural Language Processing, 
pages 239-248, Bulgaria. 
Diana McCarthy and Anna Korhonen. 1998. De- 
tecting verbal participation in diathesis alterna- 
tions. In Proceedings of the 17th International 
Conference on Computational Linguistics and the 
36th Annual Meeting of the Association for Com- 
putational Linguists., volume 2, pages 1493-1495. 
Philip Resnik. 1993. Selection and Information: 
A Class-Based Approach to Lexical Relationships. 
Ph.D. thesis, University of Pennsylvania. 
Francesc Ribas. 1995. On Acquiring Appropriate 
Selectional Restrictions from Corpora Using a Se- 
mantic Taxonomy. Ph.D. thesis, University of 
Catalonia. 
Jorma. Rissanen. 1978. Modeling by shortest data 
description. Automatiea, 14:465-471. 
Sidney Siegel and N. John Castellan, editors. 1988. 
Non-Parametric Statistics for the Behavioural 
Sciences. McGraw-Hill, New York. 
Manfred Stede. 1998. A generative perspective 
on verb alternations. Computational Linguistics, 
24(3):401-430. 
Suzanne Stevenson and Paola Merlo. 1999. Au- 
tomatic verb classification using distributions of 
grammatical features. In Proceedings of the Ninth 
Conference of the European Chapter of the Associ- 
ation for Computational Linguistics, pages 45-52. 
263 
Characterising Measures of Lexical Distributional Similarity
Julie Weeds, David Weir and Diana McCarthy
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{juliewe, davidw,dianam}@sussex.ac.uk
Abstract
This work investigates the variation in a word?s dis-
tributionally nearest neighbours with respect to the
similarity measure used. We identify one type of
variation as being the relative frequency of the neigh-
bour words with respect to the frequency of the tar-
get word. We then demonstrate a three-way connec-
tion between relative frequency of similar words, a
concept of distributional gnerality and the seman-
tic relation of hyponymy. Finally, we consider the
impact that this has on one application of distribu-
tional similarity methods (judging the composition-
ality of collocations).
1 Introduction
Over recent years, many Natural Language Pro-
cessing (NLP) techniques have been developed
that might benefit from knowledge of distribu-
tionally similar words, i.e., words that occur in
similar contexts. For example, the sparse data
problem can make it difficult to construct lan-
guage models which predict combinations of lex-
ical events. Similarity-based smoothing (Brown
et al, 1992; Dagan et al, 1999) is an intuitively
appealing approach to this problem where prob-
abilities of unseen co-occurrences are estimated
from probabilities of seen co-occurrences of dis-
tributionally similar events.
Other potential applications apply the hy-
pothesised relationship (Harris, 1968) between
distributional similarity and semantic similar-
ity; i.e., similarity in the meaning of words can
be predicted from their distributional similarity.
One advantage of automatically generated the-
sauruses (Grefenstette, 1994; Lin, 1998; Curran
and Moens, 2002) over large-scale manually cre-
ated thesauruses such as WordNet (Fellbaum,
1998) is that they might be tailored to a partic-
ular genre or domain.
However, due to the lack of a tight defini-
tion for the concept of distributional similarity
and the broad range of potential applications, a
large number of measures of distributional sim-
ilarity have been proposed or adopted (see Sec-
tion 2). Previous work on the evaluation of dis-
tributional similarity methods tends to either
compare sets of distributionally similar words
to a manually created semantic resource (Lin,
1998; Curran and Moens, 2002) or be oriented
towards a particular task such as language mod-
elling (Dagan et al, 1999; Lee, 1999). The first
approach is not ideal since it assumes that the
goal of distributional similarity methods is to
predict semantic similarity and that the seman-
tic resource used is a valid gold standard. Fur-
ther, the second approach is clearly advanta-
geous when one wishes to apply distributional
similarity methods in a particular application
area. However, it is not at all obvious that one
universally best measure exists for all applica-
tions (Weeds and Weir, 2003). Thus, applying a
distributional similarity technique to a new ap-
plication necessitates evaluating a large number
of distributional similarity measures in addition
to evaluating the new model or algorithm.
We propose a shift in focus from attempting
to discover the overall best distributional sim-
ilarity measure to analysing the statistical and
linguistic properties of sets of distributionally
similar words returned by different measures.
This will make it possible to predict in advance
of any experimental evaluation which distribu-
tional similarity measures might be most appro-
priate for a particular application.
Further, we explore a problem faced by
the automatic thesaurus generation community,
which is that distributional similarity methods
do not seem to offer any obvious way to dis-
tinguish between the semantic relations of syn-
onymy, antonymy and hyponymy. Previous
work on this problem (Caraballo, 1999; Lin et
al., 2003) involves identifying specific phrasal
patterns within text e.g., ?Xs and other Ys? is
used as evidence that X is a hyponym of Y. Our
work explores the connection between relative
frequency, distributional generality and seman-
tic generality with promising results.
The rest of this paper is organised as follows.
In Section 2, we present ten distributional simi-
larity measures that have been proposed for use
in NLP. In Section 3, we analyse the variation in
neighbour sets returned by these measures. In
Section 4, we take one fundamental statistical
property (word frequency) and analyse correla-
tion between this and the nearest neighbour sets
generated. In Section 5, we relate relative fre-
quency to a concept of distributional generality
and the semantic relation of hyponymy. In Sec-
tion 6, we consider the effects that this has on a
potential application of distributional similarity
techniques, which is judging compositionality of
collocations.
2 Distributional similarity measures
In this section, we introduce some basic con-
cepts and then discuss the ten distributional
similarity measures used in this study.
The co-occurrence types of a target word are
the contexts, c, in which it occurs and these
have associated frequencies which may be used
to form probability estimates. In our work, the
co-occurrence types are always grammatical de-
pendency relations. For example, in Sections 3
to 5, similarity between nouns is derived from
their co-occurrences with verbs in the direct-
object position. In Section 6, similarity between
verbs is derived from their subjects and objects.
The k nearest neighbours of a target word w
are the k words for which similarity with w is
greatest. Our use of the term similarity measure
encompasses measures which should strictly be
referred to as distance, divergence or dissimilar-
ity measures. An increase in distance correlates
with a decrease in similarity. However, either
type of measure can be used to find the k near-
est neighbours of a target word.
Table 1 lists ten distributional similarity mea-
sures. The cosine measure (Salton and McGill,
1983) returns the cosine of the angle between
two vectors.
The Jensen-Shannon (JS) divergence measure
(Rao, 1983) and the ?-skew divergence measure
(Lee, 1999) are based on the Kullback-Leibler
(KL) divergence measure. The KL divergence,
or relative entropy, D(p||q), between two prob-
ability distribution functions p and q is defined
(Cover and Thomas, 1991) as the ?inefficiency
of assuming that the distribution is q when the
true distribution is p?: D(p||q) =
?
c p log
p
q .
However, D(p||q) = ? if there are any con-
texts c for which p(c) > 0 and q(c) = 0. Thus,
this measure cannot be used directly on maxi-
mum likelihood estimate (MLE) probabilities.
One possible solution is to use the JS diver-
gence measure, which measures the cost of using
the average distribution in place of each individ-
ual distribution. Another is the ?-skew diver-
gence measure, which uses the p distribution to
smooth the q distribution. The value of the pa-
rameter ? controls the extent to which the KL
divergence is approximated. We use ? = 0.99
since this provides a close approximation to the
KL divergence and has been shown to provide
good results in previous research (Lee, 2001).
The confusion probability (Sugawara et al,
1985) is an estimate of the probability that one
word can be substituted for another. Words
w1 and w2 are completely confusable if we are
equally as likely to see w2 in a given context as
we are to see w1 in that context.
Jaccard?s coefficient (Salton and McGill,
1983) calculates the proportion of features be-
longing to either word that are shared by both
words. In the simplest case, the features of a
word are defined as the contexts in which it has
been seen to occur. simja+mi is a variant (Lin,
1998) in which the features of a word are those
contexts for which the pointwise mutual infor-
mation (MI) between the word and the context
is positive, where MI can be calculated using
I(c, w) = log P (c|w)P (c) . The related Dice Coeffi-
cient (Frakes and Baeza-Yates, 1992) is omitted
here since it has been shown (van Rijsbergen,
1979) that Dice and Jaccard?s Coefficients are
monotonic in each other.
Lin?s Measure (Lin, 1998) is based on his
information-theoretic similarity theorem, which
states, ?the similarity between A and B is mea-
sured by the ratio between the amount of in-
formation needed to state the commonality of
A and B and the information needed to fully
describe what A and B are.?
The final three measures are settings in
the additive MI-based Co-occurrence Retrieval
Model (AMCRM) (Weeds and Weir, 2003;
Weeds, 2003). We can measure the precision
and the recall of a potential neighbour?s re-
trieval of the co-occurrences of the target word,
where the sets of required and retrieved co-
occurrences (F (w1) and F (w2) respectively) are
those co-occurrences for which MI is positive.
Neighbours with both high precision and high
recall retrieval can be obtained by computing
Measure Function
cosine simcm(w2, w1) =
?
c
P (c|w1).P (c|w2)
??
c
P (c|w1)2
?
c
P (c|w2)2
Jens.-Shan. distjs(w2, w1) = 12
(
D
(
p||p+q2
)
+D
(
q||p+q2
))
where p = P (c|w1) and q = P (c|w2)
?-skew dist?(w2, w1) = D (p||(?.q + (1? ?).p)) where p = P (c|w1) and q = P (c|w2)
conf. prob. simcp(w2|w1) =
?
c
P (w1|c).P (w2|c).P (c)
P (w1)
Jaccard?s simja(w2, w1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : P (c|v) > 0}
Jacc.+MI simja+mi(w2,W1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : I(c, w) > 0}
Lin?s simlin(w2, w1) =
?
F (w1)?F (w2)
(I(c,w1)+I(c,w2))
?
F (w1)
I(c,w1)+
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
precision simP(w2, w1) =
?
F (w1)?F (w2)
I(c,w2)
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
recall simR(w2, w1) =
?
F (w1)?F (w2)
I(c,w1)
?
F (w1)
I(c,w1)
where F (w) = {c : I(c, w) > 0}
harm. mean simhm(w2, w1) =
2.simP (w2,w1).simR(w2,w1)
simP (w2,w1)+simR(w2,w1)
where F (w) = {c : I(c, w) > 0}
Table 1: Ten distributional similarity measures
their harmonic mean (or F-score).
3 Overlap of neighbour sets
We have described a number of ways of calcu-
lating distributional similarity. We now con-
sider whether there is substantial variation in
a word?s distributionally nearest neighbours ac-
cording to the chosen measure. We do this by
calculating the overlap between neighbour sets
for 2000 nouns generated using different mea-
sures from direct-object data extracted from the
British National Corpus (BNC).
3.1 Experimental set-up
The data from which sets of nearest neighbours
are derived is direct-object data for 2000 nouns
extracted from the BNC using a robust accurate
statistical parser (RASP) (Briscoe and Carroll,
2002). For reasons of computational efficiency,
we limit ourselves to 2000 nouns and direct-
object relation data. Given the goal of compar-
ing neighbour sets generated by different mea-
sures, we would not expect these restrictions to
affect our findings. The complete set of 2000
nouns (WScomp) is the union of two sets WShigh
and WSlow for which nouns were selected on the
basis of frequency: WShigh contains the 1000
most frequently occurring nouns (frequency >
500), and WSlow contains the nouns ranked
3001-4000 (frequency ? 100). By excluding
mid-frequency nouns, we obtain a clear sepa-
ration between high and low frequency nouns.
The complete data-set consists of 1,596,798 co-
occurrence tokens distributed over 331,079 co-
occurrence types. From this data, we computed
the similarity between every pair of nouns ac-
cording to each distributional similarity mea-
sure. We then generated ranked sets of nearest
neighbours (of size k = 200 and where a word
is excluded from being a neighbour of itself) for
each word and each measure.
For a given word, we compute the overlap be-
tween neighbour sets using a comparison tech-
nique adapted from Lin (1998). Given a word
w, each word w? in WScomp is assigned a rank
score of k ? rank if it is one of the k near-
est neighbours of w using measure m and zero
otherwise. If NS(w,m) is the vector of such
scores for word w and measure m, then the
overlap, C(NS(w,m1),NS(w,m2)), of two neigh-
bour sets is the cosine between the two vectors:
C(NS(w,m1),NS(w,m2)) =
?
w? rm1(w
?, w)? rm2(w
?, w)
?k
i=1 i2
The overlap score indicates the extent to which
sets share members and the extent to which
they are in the same order. To achieve an over-
lap score of 1, the sets must contain exactly
the same items in exactly the same order. An
overlap score of 0 is obtained if the sets do not
contain any common items. If two sets share
roughly half their items and these shared items
are dispersed throughout the sets in a roughly
similar order, we would expect the overlap be-
tween sets to be around 0.5.
cm js ? cp ja ja+mi lin
cm 1.0(0.0) 0.69(0.12) 0.53(0.15) 0.33(0.09) 0.26(0.12) 0.28(0.15) 0.32(0.15)
js 0.69(0.12) 1.0(0.0) 0.81(0.10) 0.46(0.31) 0.48(0.18) 0.49(0.20) 0.55(0.16)
? 0.53(0.15) 0.81(0.10) 1.0(0.0) 0.61(0.08) 0.4(0.27) 0.39(0.25) 0.48(0.19)
cp 0.33(0.09) 0.46(0.31) 0.61(0.08) 1.0(0.0) 0.24(0.24) 0.20(0.18) 0.29(0.15)
ja 0.26(0.12) 0.48(0.18) 0.4(0.27) 0.24(0.24) 1.0(0.0) 0.81(0.08) 0.69(0.09)
ja+mi 0.28(0.15) 0.49(0.20) 0.39(0.25) 0.20(0.18) 0.81(0.08) 1.0(0.0) 0.81(0.10)
lin 0.32(0.15) 0.55(0.16) 0.48(0.19) 0.29(0.15) 0.69(0.09) 0.81(0.10) 1.0(0.0)
Table 2: Cross-comparison of first seven similarity measures in terms of mean overlap of neighbour
sets and corresponding standard deviations.
P R hm
cm 0.18(0.10) 0.31(0.13) 0.30(0.14)
js 0.19(0.12) 0.55(0.18) 0.51(0.18)
? 0.08(0.08) 0.74(0.14) 0.41(0.23)
cp 0.03(0.04) 0.57(0.10) 0.25(0.18)
ja 0.36(0.30) 0.38(0.30) 0.74(0.14)
ja+mi 0.42(0.30) 0.40(0.31) 0.86(0.07)
lin 0.46(0.25) 0.52(0.22) 0.95(0.039)
Table 3: Mean overlap scores for seven simi-
larity measures with precision, recall and the
harmonic mean in the AMCRM.
3.2 Results
Table 2 shows the mean overlap score between
every pair of the first seven measures in Table 1
calculated over WScomp. Table 3 shows the mean
overlap score between each of these measures
and precision, recall and the harmonic mean in
the AMCRM. In both tables, standard devia-
tions are given in brackets and boldface denotes
the highest levels of overlap for each measure.
For compactness, each measure is denoted by
its subscript from Table 1.
Although overlap between most pairs of
measures is greater than expected if sets of
200 neighbours were generated randomly from
WScomp (in this case, average overlap would be
0.08 and only the overlap between the pairs
(?,P) and (cp,P) is not significantly greater
than this at the 1% level), there are substan-
tial differences between the neighbour sets gen-
erated by different measures. For example, for
many pairs, neighbour sets do not appear to
have even half their members in common.
4 Frequency analysis
We have seen that there is a large variation in
neighbours selected by different similarity mea-
sures. In this section, we analyse how neighbour
sets vary with respect to one fundamental statis-
tical property ? word frequency. To do this, we
measure the bias in neighbour sets towards high
frequency nouns and consider how this varies
depending on whether the target noun is itself
a high frequency noun or low frequency noun.
4.1 Measuring bias
If a measure is biased towards selecting high fre-
quency words as neighbours, then we would ex-
pect that neighbour sets for this measure would
be made up mainly of words from WShigh. Fur-
ther, the more biased the measure is, the more
highly ranked these high frequency words will
tend to be. In other words, there will be high
overlap between neighbour sets generated con-
sidering all 2000 nouns as potential neighbours
and neighbour sets generated considering just
the nouns in WShigh as potential neighbours. In
the extreme case, where all of a noun?s k nearest
neighbours are high frequency nouns, the over-
lap with the high frequency noun neighbour set
will be 1 and the overlap with the low frequency
noun neighbour set will be 0. The inverse is, of
course, true if a measure is biased towards se-
lecting low frequency words as neighbours.
If NSwordset is the vector of neighbours (and
associated rank scores) for a given word, w, and
similarity measure, m, and generated consider-
ing just the words in wordset as potential neigh-
bours, then the overlap between two neighbour
sets can be computed using a cosine (as be-
fore). If Chigh = C(NScomp,NShigh) and Clow =
C(NScomp,NSlow), then we compute the bias to-
wards high frequency neighbours for word w us-
ing measure m as: biashighm(w) =
Chigh
Chigh+Clow
The value of this normalised score lies in the
range [0,1] where 1 indicates a neighbour set
completely made up of high frequency words, 0
indicates a neighbour set completely made up of
low frequency words and 0.5 indicates a neigh-
bour set with no biases towards high or low fre-
quency words. This score is more informative
than simply calculating the proportion of high
high freq. low freq.
target nouns target nouns
cm 0.90 0.87
js 0.94 0.70
? 0.98 0.90
cp 1.00 0.99
ja 0.99 0.21
ja+mi 0.95 0.14
lin 0.85 0.38
P 0.12 0.04
R 0.99 0.98
hm 0.92 0.28
Table 4: Mean value of biashigh according to
measure and frequency of target noun.
and low frequency words in each neighbour set
because it weights the importance of neighbours
by their rank in the set. Thus, a large number
of high frequency words in the positions clos-
est to the target word is considered more biased
than a large number of high frequency words
distributed throughout the neighbour set.
4.2 Results
Table 4 shows the mean value of the biashigh
score for every measure calculated over the set
of high frequency nouns and over the set of low
frequency nouns. The standard deviations (not
shown) all lie in the range [0,0.2]. Any deviation
from 0.5 of greater than 0.0234 is significant at
the 1% level.
For all measures and both sets of target
nouns, there appear to be strong tendencies to
select neighbours of particular frequencies. Fur-
ther, there appears to be three classes of mea-
sures: those that select high frequency nouns
as neighbours regardless of the frequency of the
target noun (cm, js, ?, cp andR); those that se-
lect low frequency nouns as neighbours regard-
less of the frequency of the target noun (P); and
those that select nouns of a similar frequency to
the target noun (ja, ja+mi, lin and hm).
This can also be considered in terms of distri-
butional generality. By definition, recall prefers
words that have occurred in more of the con-
texts that the target noun has, regardless of
whether it occurs in other contexts as well i.e.,
it prefers distributionally more general words.
The probability of this being the case increases
as the frequency of the potential neighbour in-
creases and so, recall tends to select high fre-
quency words. In contrast, precision prefers
words that have occurred in very few contexts
that the target word has not i.e., it prefers dis-
tributionally more specific words. The prob-
ability of this being the case increases as the
frequency of the potential neighbour decreases
and so, precision tends to select low frequency
words. The harmonic mean of precision and re-
call prefers words that have both high precision
and high recall. The probability of this being
the case is highest when the words are of sim-
ilar frequency and so, the harmonic mean will
tend to select words of a similar frequency.
5 Relative frequency and hyponymy
In this section, we consider the observed fre-
quency effects from a semantic perspective.
The concept of distributional generality in-
troduced in the previous section has parallels
with the linguistic relation of hyponymy, where
a hypernym is a semantically more general term
and a hyponym is a semantically more specific
term. For example, animal is an (indirect1) hy-
pernym of dog and conversely dog is an (indi-
rect) hyponym of animal. Although one can
obviously think of counter-examples, we would
generally expect that the more specific term dog
can only be used in contexts where animal can
be used and that the more general term animal
might be used in all of the contexts where dog
is used and possibly others. Thus, we might ex-
pect that distributional generality is correlated
with semantic generality ? a word has high
recall/low precision retrieval of its hyponyms?
co-occurrences and high precision/low recall re-
trieval of its hypernyms? co-occurrences.
Thus, if n1 and n2 are related and P(n2, n1) >
R(n2, n1), we might expect that n2 is a hy-
ponym of n1 and vice versa. However, having
discussed a connection between frequency and
distributional generality, we might also expect
to find that the frequency of the hypernymic
term is greater than that of the hyponymic
term. In order to test these hypotheses, we ex-
tracted all of the possible hyponym-hypernym
pairs (20, 415 pairs in total) from our list of 2000
nouns (using WordNet 1.6). We then calculated
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by the relative values of precision and recall and
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by relative frequency. We found that the direc-
tion of the hyponymy relation is correlated in
the predicted direction with the precision-recall
1There may be other concepts in the hypernym chain
between dog and animal e.g. carnivore and mammal.
values in 71% of cases and correlated in the pre-
dicted direction with relative frequency in 70%
of cases. This supports the idea of a three-way
linking between distributional generality, rela-
tive frequency and semantic generality. We now
consider the impact that this has on a potential
application of distributional similarity methods.
6 Compositionality of collocations
In its most general sense, a collocation is a ha-
bitual or lexicalised word combination. How-
ever, some collocations such as strong tea are
compositional, i.e., their meaning can be de-
termined from their constituents, whereas oth-
ers such as hot dog are not. Both types are
important in language generation since a sys-
tem must choose between alternatives but only
non-compositional ones are of interest in lan-
guage understanding since only these colloca-
tions need to be listed in the dictionary.
Baldwin et al (2003) explore empirical
models of compositionality for noun-noun com-
pounds and verb-particle constructions. Based
on the observation (Haspelmath, 2002) that
compositional collocations tend to be hyponyms
of their head constituent, they propose a model
which considers the semantic similarity between
a collocation and its constituent words.
McCarthy et al (2003) also investigate sev-
eral tests for compositionality including one
(simplexscore) based on the observation that
compositional collocations tend to be similar in
meaning to their constituent parts. They ex-
tract co-occurrence data for 111 phrasal verbs
(e.g. rip off ) and their simplex constituents
(e.g. rip) from the BNC using RASP and cal-
culate the value of simlin between each phrasal
verb and its simplex constituent. The test
simplexscore is used to rank the phrasal verbs
according to their similarity with their simplex
constituent. This ranking is correlated with hu-
man judgements of the compositionality of the
phrasal verbs using Spearman?s rank correlation
coefficient. The value obtained (0.0525) is dis-
appointing since it is not statistically significant
(the probability of this value under the null hy-
pothesis of ?no correlation? is 0.3).2
However, Haspelmath (2002) notes that a
compositional collocation is not just similar to
one of its constituents ? it can be considered to
be a hyponym of its head constituent. For ex-
ample, ?strong tea? is a type of ?tea? and ?to
2Other tests for compositionality investigated by Mc-
Carthy et al (2003) do much better.
Measure rs P (rs) under H0
simlin 0.0525 0.2946
precision -0.160 0.0475
recall 0.219 0.0110
harmonic mean 0.011 0.4562
Table 5: Correlation with compositionality for
different similarity measures
rip up? is a way of ?ripping?.
Thus, we hypothesised that a distributional
measure which tends to select more general
terms as neighbours of the phrasal verb (e.g. re-
call) would do better than measures that tend
to select more specific terms (e.g. precision) or
measures that tend to select terms of a similar
specificity (e.g simlin or the harmonic mean of
precision and recall).
Table 5 shows the results of using different
similarity measures with the simplexscore test
and data of McCarthy et al (2003). We now see
significant correlation between compositionality
judgements and distributional similarity of the
phrasal verb and its head constituent. The cor-
relation using the recall measure is significant
at the 5% level; thus we can conclude that if
the simplex verb has high recall retrieval of the
phrasal verb?s co-occurrences, then the phrasal
is likely to be compositional. The correlation
score using the precision measure is negative
since we would not expect the simplex verb to
be a hyponym of the phrasal verb and thus, if
the simplex verb does have high precision re-
trieval of the phrasal verb?s co-occurrences, it is
less likely to be compositional.
Finally, we obtained a very similar result
(0.217) by ranking phrasals according to their
inverse relative frequency with their simplex
constituent (i.e., freq(simplex)freq(phrasal) ). Thus, it would
seem that the three-way connection between
distributional generality, hyponymy and rela-
tive frequency exists for verbs as well as nouns.
7 Conclusions and further work
We have presented an analysis of a set of dis-
tributional similarity measures. We have seen
that there is a large amount of variation in the
neighbours selected by different measures and
therefore the choice of measure in a given appli-
cation is likely to be important.
We also identified one of the major axes of
variation in neighbour sets as being the fre-
quency of the neighbours selected relative to the
frequency of the target word. There are three
major classes of distributional similarity mea-
sures which can be characterised as 1) higher
frequency selecting or high recall measures; 2)
lower frequency selecting or high precision mea-
sures; and 3) similar frequency selecting or high
precision and recall measures.
A word tends to have high recall similarity
with its hyponyms and high precision similarity
with its hypernyms. Further, in the majority of
cases, it tends to be more frequent than its hy-
ponyms and less frequent than its hypernyms.
Thus, there would seem to a three way corre-
lation between word frequency, distributional
generality and semantic generality.
We have considered the impact of these ob-
servations on a technique which uses a distribu-
tional similarity measure to determine composi-
tionality of collocations. We saw that in this ap-
plication we achieve significantly better results
using a measure that tends to select higher fre-
quency words as neighbours rather than a mea-
sure that tends to select neighbours of a similar
frequency to the target word.
There are a variety of ways in which this work
might be extended. First, we could use the ob-
servations about distributional generality and
relative frequency to aid the process of organ-
ising distributionally similar words into hierar-
chies. Second, we could consider the impact of
frequency characteristics in other applications.
Third, for the general application of distribu-
tional similarity measures, it would be useful
to find other characteristics by which distribu-
tional similarity measures might be classified.
Acknowledgements
This work was funded by a UK EPSRC stu-
dentship to the first author, UK EPSRC project
GR/S26408/01 (NatHab) and UK EPSRC
project GR/N36494/01 (RASP). We would like
to thank Adam Kilgarriff and Bill Keller for use-
ful discussions.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions, pages 89?96, Sapporo, Japan.
Edward Briscoe and John Carroll. 2002. Robust ac-
curate statistical annotation of general text. In
Proceedings of LREC-2002, pages 1499?1504.
P.F. Brown, V.J. DellaPietra, P.V deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguis-
tics, 18(4):467?479.
Sharon Caraballo. 1999. Automatic construction of
a hypernym-labelled noun hierarchy from text. In
Proceedings of ACL-99, pages 120?126.
T.M. Cover and J.A. Thomas. 1991. Elements of
Information Theory. Wiley, New York.
James R. Curran and Marc Moens. 2002. Im-
provements in automatic thesaurus extraction. In
ACL-SIGLEX Workshop on Unsupervised Lexical
Acquisition, Philadelphia.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning Journal, 34.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structures and Algo-
rithms. Prentice Hall.
Gregory Grefenstette. 1994. Corpus-derived first-,
second- and third-order word affinities. In Pro-
ceedings of Euralex, pages 279?290, Amsterdam.
Zelig S. Harris. 1968. Mathematical Structures of
Language. Wiley, New York.
Martin Haspelmath. 2002. Understanding Morphol-
ogy. Arnold Publishers.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of ACL-1999, pages 23?32.
Lillian Lee. 2001. On the effectiveness of the skew
divergence for statistical language analysis. Arti-
ficial Intelligence and Statistics, pages 65?72.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among dis-
tributionally similar words. In Proceedings of
IJCAI-03, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL ?98, pages 768?774, Montreal.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL-2003
Workshop on Multiword Expressions, pages 73?
80, Sapporo, Japan.
C. Radhakrishna Rao. 1983. Diversity: Its measure-
ment, decomposition, apportionment and analy-
sis. Sankyha: The Indian Journal of Statistics,
44(A):1?22.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
K.M. Sugawara, K. Nishimura, K. Toshioka,
M. Okachi, and T. Kaneko. 1985. Isolated word
recognition using hidden markov models. In Pro-
ceedings of the ICASSP-1985, pages 1?4.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, second edition.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of EMNLP-2003, pages 81?88, Sapporo, Japan.
Julie Weeds. 2003. Measures and Applications of
Lexical Distributional Similarity. Ph.D. thesis,
Department of Informatics, University of Sussex.
Automatic Identification of Infrequent Word Senses
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In this paper we show that an unsupervised method for
ranking word senses automatically can be used to iden-
tify infrequently occurring senses. We demonstrate this
using a ranking of noun senses derived from the BNC
and evaluating on the sense-tagged text available in both
SemCor and the SENSEVAL-2 English all-words task.
We show that the method does well at identifying senses
that do not occur in a corpus, and that those that are erro-
neously filtered but do occur typically have a lower fre-
quency than the other senses. This method should be
useful for word sense disambiguation systems, allowing
effort to be concentrated on more frequent senses; it may
also be useful for other tasks such as lexical acquisition.
Whilst the results on balanced corpora are promising, our
chief motivation for the method is for application to do-
main specific text. For text within a particular domain
many senses from a generic inventory will be rare, and
possibly redundant. Since a large domain specific cor-
pus of sense annotated data is not available, we evaluate
our method on domain-specific corpora and demonstrate
that sense types identified for removal are predominantly
senses from outside the domain.
1 Introduction
Much about the behaviour of words is most appro-
priately expressed in terms of word senses rather
than word forms. However, an NLP application
computing over word senses is faced with consid-
erable extra ambiguity. There are systems which
can perform word sense disambiguation (WSD) on
the words in input text, however there is room for
improvement since the best systems on the English
SENSEVAL-2 all-words task obtained at most 69%
for precision and recall. Whilst there are systems
that obtain higher precision (Magnini et al, 2001),
these typically suffer from a low recall. WSD per-
formance is affected by the degree of polysemy, but
even more so by the entropy of the frequency distri-
butions of the words? senses (Kilgarriff and Rosen-
zweig, 2000) since the distribution for many words
is highly skewed. Many of the senses in such an
inventory are rare and WSD and lexical acquisition
systems do best when they take this into account.
There are many ways that the skewed distribution
can be taken into account. One successful approach
is to back-off to the first (predominant) sense (Wilks
and Stevenson, 1998; Hoste et al, 2001). Another
possibility would be concentrate the selection pro-
cess to senses with higher frequency, and filter out
rare senses. This is implicitly done by systems
which rely on hand-tagged training corpora, since
rare senses often do not occur in the available data.
In this paper we use an unsupervised method to rank
word senses from an inventory according to preva-
lence (McCarthy et al, 2004a), and utilise the rank-
ing scores to identify senses which are rare. We use
WordNet for our inventory, since it is widely used
and freely available, but our method could in prin-
ciple be used with another MRD (we comment on
this in the conclusions). We report work with nouns
here, and leave evaluation on other PoS for the fu-
ture.
Our approach exploits automatically acquired
thesauruses which provide ?nearest neighbours? for
a given word entry. The neighbours are ordered
in terms of the distributional similarity that they
share with the target word. The neighbours relate
to different senses of the target word, so for exam-
ple the word competition in such a thesaurus pro-
vided by Lin 1 has neighbours tournament, event,
championship and then further down the ordered list
we see neighbours pertaining to a different sense
competitor,...market...price war. Pantel and Lin
(2002) demonstrate that it is possible to cluster the
neighbours into senses and relate these to WordNet
senses. In contrast, we use the distributional sim-
ilarity scores of the neighbours to rank the various
senses of the target word since we expect that the
quantity and similarity of the neighbours pertain-
ing to different senses will reflect the relative dom-
inance of the senses. This is because there will
1Available from
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
be more data for the more prevalent senses com-
pared to the less frequent senses. We use a measure
of semantic similarity from the WordNet Similarity
package to relate the senses of the target word to the
neighbours in the thesaurus.
The paper is structured as follows. The ranking
method is described elsewhere (McCarthy et al,
2004a), but we summarise in the following section
and describe how ranking scores can be used for fil-
tering word senses. Section 3 describes two exper-
iments using the BNC for acquisition of the sense
rankings with evaluation using the hand-tagged data
in i) SemCor and ii) the English SENSEVAL-2 all-
words task. We demonstrate that the majority of
senses identified by the method do not occur in these
gold-standards, and that for those that do, only a
small percentage of the sense tokens would be re-
moved in error by filtering these senses. In section 4
we use domain labels produced by (Magnini and
Cavaglia`, 2000) to demonstrate differences in the
senses filtered for a sample of words in two domain
specific corpora. We describe some related work in
section 5 and conclude in section 6.
2 Method
McCarthy et al (2004a) describe a method to pro-
duce a ranking over senses and find the predominant
sense of a word just using raw text. We summarise
the method below, and describe how we use it for
identifying candidate senses for filtering.
2.1 Ranking the Senses
In order to rank the senses of a target word (e.g.
plant) we use a thesaurus acquired from automati-
cally parsed text (section 2.2 below). This provides
the  nearest neighbours to each target word (e.g.
factory, refinery, tree etc...) along with the distribu-
tional similarity score between the target word and
its neighbour. We then use the WordNet similar-
ity package (Patwardhan and Pedersen, 2003) (see
section 2.3) to give us a semantic similarity mea-
sure (hereafter referred to as the WordNet similarity
measure) to weight the contribution that each neigh-
bour (e.g. factory) makes to the various senses of
the target word (e.g. flora, industrial, actor etc...).
We take each sense of the target word (  ) in turn
and obtain a score reflecting the prevalence which is
used for ranking. Let 
	Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 369?379, Prague, June 2007. c?2007 Association for Computational Linguistics
Detecting Compositionality of Verb-Object Combinations using Selectional
Preferences
Diana McCarthy
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
dianam@sussex.ac.uk
Sriram Venkatapathy
International Institute
of Information Technology
Hyderabad, India
sriram@research.iiit.ac.in
Aravind K. Joshi
University of Pennsylvania,
Philadelphia
PA, USA.
joshi@linc.cis.upenn.edu
Abstract
In this paper we explore the use of se-
lectional preferences for detecting non-
compositional verb-object combinations. To
characterise the arguments in a given gram-
matical relationship we experiment with
three models of selectional preference. Two
use WordNet and one uses the entries from
a distributional thesaurus as classes for rep-
resentation. In previous work on selectional
preference acquisition, the classes used for
representation are selected according to the
coverage of argument tokens rather than be-
ing selected according to the coverage of
argument types. In our distributional the-
saurus models and one of the methods us-
ing WordNet we select classes for represent-
ing the preferences by virtue of the number
of argument types that they cover, and then
only tokens under these classes which are
representative of the argument head data are
used to estimate the probability distribution
for the selectional preference model. We
demonstrate a highly significant correlation
between measures which use these ?type-
based? selectional preferences and composi-
tionality judgements from a data set used in
previous research. The type-based models
perform better than the models which use to-
kens for selecting the classes. Furthermore,
the models which use the automatically ac-
quired thesaurus entries produced the best
results. The correlation for the thesaurus
models is stronger than any of the individ-
ual features used in previous research on the
same dataset.
1 Introduction
Characterising the semantic behaviour of phrases in
terms of compositionality has particularly attracted
attention in recent years (Lin, 1999; Schone and Ju-
rafsky, 2001; Bannard, 2002; Bannard et al, 2003;
Baldwin et al, 2003; McCarthy et al, 2003; Ban-
nard, 2005; Venkatapathy and Joshi, 2005). Typi-
cally the phrases are putative multiwords and non-
compositionality is viewed as an important feature
of many such ?words with spaces? (Sag et al, 2002).
For applications such as paraphrasing, information
extraction and translation, it is essential to take the
words of non-compositional phrases together as a
unit because the meaning of a phrase cannot be ob-
tained straightforwardly from the constituent words.
In this work we are investigate methods of deter-
mining semantic compositionality of verb-object 1
combinations on a continuum following previous
research in this direction (McCarthy et al, 2003;
Venkatapathy and Joshi, 2005).
Much previous research has used a combination
of statistics and distributional approaches whereby
distributional similarity is used to compare the con-
stituents of the multiword with the multiword itself.
In this paper, we will investigate the use of selec-
tional preferences of verbs. We will use the pref-
erences to find atypical verb-object combinations as
we anticipate that such combinations are more likely
to be non-compositional.
1We use object to refer to direct objects.
369
Selectional preferences of predicates have been
modelled using the man-made thesaurus Word-
Net (Fellbaum, 1998), see for example (Resnik,
1993; Li and Abe, 1998; Abney and Light, 1999;
Clark and Weir, 2002). There are also distribu-
tional approaches which use co-occurrence data to
cluster distributionally similar words together. The
cluster output can then be used as classes for se-
lectional preferences (Pereira et al, 1993), or one
can directly use frequency information from distri-
butionally similar words for smoothing (Grishman
and Sterling, 1994).
We used three different types of probabilistic
models, which vary in the classes selected for rep-
resentation over which the probability distribution of
the argument heads 2 is estimated. Two use WordNet
and the other uses the entries in a thesaurus of distri-
butionally similar words acquired automatically fol-
lowing (Lin, 1998). The first method is due to Li and
Abe (1998). The classes over which the probabil-
ity distribution is calculated are selected according
to the minimum description length principle (MDL)
which uses the argument head tokens for finding the
best classes for representation. This method has pre-
viously been tried for modelling compositionality of
verb-particle constructions (Bannard, 2002).
The other two methods (we refer to them as ?type-
based?) also calculate a probability distribution us-
ing argument head tokens but they select the classes
over which the distribution is calculated using the
number of argument head types (of a verb in a cor-
pus) in a given class, rather than the number of ar-
gument head tokens in contrast to previous WordNet
models (Resnik, 1993; Li and Abe, 1998; Clark and
Weir, 2002). For example, if the object slot of the
verb park contains the argument heads { car, car,
car, car, van, jeep } then the type-based models use
the word type ?car? only once when determining the
classes over which the probability distribution is to
be estimated. Classes are selected which maximise
the number of types that they cover, rather than the
number of tokens. This is done to avoid the selec-
tional preferences being heavily influenced by noise
from highly frequent arguments which may be poly-
semous and some or all of their meanings may not be
2Argument heads are the nouns occurring in the object slot
of the target verb.
semantically related to the ?prototypical? arguments
of the verb. For example car has a gondola sense in
WordNet.
The third method uses entries in a distributional
thesaurus rather than classes from WordNet. The en-
tries used as classes for representation are selected
by virtue of the number of argument types they en-
compass. As with the WordNet models, the tokens
are used to estimate a probability distribution over
these entries.
In the next section, we discuss related work on
identifying compositionality. In section 3, we de-
scribe the methods we are using for acquiring our
models of selectional preference. In section 4, we
test our models on a dataset used in previous re-
search. We compare the three types of models in-
dividually and also investigate the best performing
model when used in combination with other features
used in previous research. We conclude in section 5.
2 Related Work
Most previous work using distributional approaches
to compositionality either contrasts distributional
information of candidate phrases with constituent
words (Schone and Jurafsky, 2001; Bannard et al,
2003; Baldwin et al, 2003; McCarthy et al, 2003)
or uses distributionally similar words to detect non-
productive phrases (Lin, 1999).
Lin (1999) used his method (Lin, 1998) for au-
tomatic thesaurus construction. He identified can-
didate phrases involving several open-class words
output from his parser and filtered these by the log-
likelihood statistic. Lin proposed that if there is a
phrase obtained by substitution of either the head
or modifier in the phrase with a ?nearest neighbour?
from the thesaurus then the mutual information of
this and the original phrase must be significantly dif-
ferent for the original phrase to be considered non-
compositional. He evaluated the output manually.
As well as distributional similarity, researchers
have used a variety of statistics as indicators of
non-compositionality (Blaheta and Johnson, 2001;
Krenn and Evert, 2001). Fazly and Stevenson (2006)
use statistical measures of syntactic behaviour to
gauge whether a verb and noun combination is likely
to be a idiom. Although they are not specifically
detecting compositionality, there is a strong corre-
370
lation between syntactic rigidity and semantic id-
iosyncrasy.
Venkatapathy and Joshi (2005) combine differ-
ent statistical and distributional methods using sup-
port vector machines (SVMs) for identifying non-
compositional verb-object combinations. They ex-
plored seven features as measures of compositional-
ity:
1. frequency
2. pointwise mutual information (Church and
Hanks, 1990),
3. least mutual information difference with simi-
lar collocations, based on (Lin, 1999) and us-
ing Lin?s thesaurus (Lin, 1998) for obtaining
the similar collocations.
4. The distributed frequency of an object, which
takes an average of the frequency of occurrence
with an object over all verbs occurring with the
object above a threshold.
5. distributed frequency of an object, using the
verb, which considers the similarity between
the target verb and the verbs occurring with the
target object above the specified threshold.
6. a latent semantic approach (LSA) based
on (Schu?tze, 1998; Baldwin et al, 2003) and
considering the dissimilarity of the verb-object
pair with its constituent verb
7. the same LSA approach, but considering the
similarity of the verb-object pair with the ver-
bal form of the object (to capture support verb
constructions e.g. give a smile
Venkatapathy and Joshi (2005) produced a dataset
of verb-object pairs with human judgements of com-
positionality. We say more about this dataset and
Venkatapathy and Joshi?s results in section 4 since
we use the dataset for our experiments.
In this paper, we investigate the use of selec-
tional preferences to detect compositionality. Ban-
nard (2002) did some pioneering work to try and
establish a link between the compositionality of
verb particle constructions and the selectional pref-
erences of the multiword and its constituent verb.
His results were hampered by models based on (Li
and Abe, 1998) which involved rather uninforma-
tive models at the roots of WordNet. There are
several reasons for this. The classes for the model
are selected using MDL by compromising between a
simple model with few classes and one which ex-
plains the data well. The models are particularly
affected by the quantity of data available (Wagner,
2002). Also noise from frequent but idiosyncratic or
polysemous arguments weakens the signal. There
is scope for experimenting with other approaches
such as (Clark and Weir, 2002), however, we feel
a type-based approach is worthwhile to avoid the
noise introduced from frequent but polysemous ar-
guments and bias from highly frequent arguments
which might be part of a multiword rather than a pro-
totypical argument of the predicate in question, for
example eat hat. In contrast to Bannard, our experi-
ments are with verb-object combinations rather than
verb particle constructions. We compare Li and Abe
models with WordNet models which use the num-
ber of argument types to obtain the classes for rep-
resentation of the selectional preferences. In addi-
tion to experiments with these WordNet models, we
propose models using entries in distributional the-
sauruses for representing preferences.
3 Three Methods for Acquiring Selectional
Preferences
All models were acquired from verb-object data ex-
tracted using the RASP parser (Briscoe and Carroll,
2002) from the 90 million words of written English
from the BNC (Leech, 1992). We extracted verb and
common noun tuples where the noun is the argu-
ment head of the object relation. The parser was also
used to extract the grammatical relation data used
for acquisition of the thesaurus described below in
section 3.3.
3.1 TCMs
This approach is a reimplementation of Li and Abe
(1998). Each selectional preference model (referred
to as a tree cut model, or TCM) comprises a set of
disjunctive noun classes selected from all the pos-
sibilities in the WordNet hyponym hierarchy 3 us-
ing MDL (Rissanen, 1978). The TCM covers all the
3We use WordNet version 2.1 for the work in this paper.
371
noun senses in the WordNet hierarchy and is associ-
ated with a probability distribution over these noun
senses in the hierarchy reflecting the argument head
data occurring in the given grammatical relationship
with the specified verb. MDL finds the classes in the
TCM by considering the cost measured in bits of de-
scribing both the model and the argument head data
encoded in the model. A compromise is made by
having as simple a model as possible using classes
further up the hierarchy whilst also providing a good
model for the set of argument head tokens (TK).
The classes are selected by recursing from the top
of the WordNet hierarchy comparing the cost (or de-
scription length) of using the mother class to the cost
of using the hyponym daughter classes. In any path,
the mother is preferred unless using the daughters
would reduce the cost. If using the daughters for the
model is less costly than the mother then the recur-
sion continues to compare the cost of the hyponyms
beneath.
The cost (or description length) for a set of classes
is calculated as the model description length (mdl)
and the data description length (ddl) 4 :-
mdl + ddl
k
2 ? log |TK| + ?
?
tk?TK log p(tk) (1)
k, is the number of WordNet classes being cur-
rently considered for the TCM minus one. The MDL
method uses the size of TK on the assumption that
a larger dataset warrants a more detailed model. The
cost of describing the argument head data is calcu-
lated using the log of the probability estimate from
the classes currently being considered for the model.
The probability estimate for a class being considered
for the model is calculated using the cumulative fre-
quency of all the hyponym nouns under that class
that occur in TK , divided by the number of noun
senses that these nouns have, to account for their
polysemy. This cumulative frequency is also divided
by the total number of noun hyponyms under that
class in WordNet to obtain a smoothed estimate for
all nouns under the class. The probability of the
class is obtained by dividing this frequency estimate
by the total frequency of the argument heads. The
algorithm is described fully by Li and Abe (1998).
4See (Li and Abe, 1998) for a full explanation.
0.17
distancestreet
vancar
mile
street car distancelane corner
0.18 0.10 0.17 0.03
entity
physical_
entityentity
abstract_
way gondola
Example nouns
hyponym classes
locationvehicle
tcm
self?propelled
Figure 1: portion of the TCM for the objects of park.
A small portion of the TCM for the object slot of
park is shown in figure 1. WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown for the classes on the TCM. Examples of the
argument head data are displayed below the Word-
Net classes with dotted lines indicating membership
at a hyponym class beneath these classes. We can-
not show the full TCM due to lack of space, but we
show some of the higher probability classes which
cover some typical nouns that occur as objects of
park. Note that probability under the classes ab-
stract entity, way and location arise because of a
systematic parsing error where adverbials such as
distance in park illegally some distance from the
railway station are identified by the parser as ob-
jects. Systematic noise from the parser has an im-
pact on all the selectional preference models de-
scribed in this paper.
3.2 WNPROTOs
We propose a method of acquiring selectional pref-
erences which instead of covering all the noun
senses in WordNet, just gives a probability distribu-
tion over a portion of prototypical classes, we refer
to these models as WNPROTOs. A WNPROTO con-
sists of classes within the noun hierarchy which have
the highest proportion of word types occurring in
the argument head data, rather than using the num-
ber of tokens, or frequency, as is used for the TCMs.
This allows less frequent, but potentially informa-
tive arguments to have some bearing on the models
acquired to reduce the impact of highly frequent but
polysemous arguments. We then used the frequency
data to populate these selected classes.
372
The classes (C) in the WNPROTO are selected
from those which include at least a threshold of 2
argument head types 5 occurring in the training data.
Each argument head in the training data is disam-
biguated according to whichever of the WordNet
classes it occurs at or under which has the highest
?type ratio?. Let TY be the set of argument head
types in the object slot of the verb for which we are
acquiring the preference model. The type ratio for a
class (c) is the ratio of noun types (ty ? TY ) occur-
ring in the training data also listed at or beneath that
class in WordNet to the total number of noun types
listed at or beneath that particular class in WordNet
(wnty ? c). The argument types attested in thetraining data are divided by the number of Word-
Net classes that the noun (classes(ty)) belongs to,
to account for polysemy in the training data.
type ratio(c) =
?
ty?TY ?c
1
|classes(ty)|
|wnty ? c|
(2)
If more than one class has the same type ratio then
the argument is not used for calculating the probabil-
ity of the preference model. In this way, only argu-
ments that can be disambiguated are used for calcu-
lating the probability distribution. The advantage of
using the type ratio to determine the classes used to
represent the model and to disambiguate the argu-
ments is that it prevents high frequency verb noun
combinations from masking the information from
prototypical but low frequency arguments. We wish
to use classes which are as representative of the ar-
gument head types as possible to help detect when
an argument head is not related to these classes and
is therefore more likely to be non-compositional.
For example, the class motor vehicle is selected
for the WNPROTO model of the object slot of park
even though there are 5 meanings of car in WordNet
including elevator car and gondola. There are 174
occurrences of car which overwhelms the frequency
of the other objects (e.g. van 11, vehicle 8) but by
looking for classes with a high proportion of types
(rather than word tokens) car is disambiguated ap-
propriately and the class motor vehicle is selected
for representation.
5We have experimented with a threshold of 3 and obtained
similar results.
0.03
0.04
tanker
boat
pram.61
car
0.05
van
caravan
entity
physical_
entity
Example nouns
hyponym classes
vehicle
self?propelled
transport
vehicle
wheeled
model
motor
vehicle caravan
  classes in
Figure 2: Part of WNPROTO for the object slot of
park
The relative frequency of each class is obtained
from the set of disambiguated argument head tokens
and used to provide the probability distribution over
this set of classes. Note that in WNPROTO, classes
can be subsumed by others in the hyponym hierar-
chy. The probability assigned to a class is appli-
cable to any descendants in the hyponym hierarchy,
except those within any hyponym classes within the
WNPROTO. The algorithm for selecting C and cal-
culating the probability distribution is shown as Al-
gorithm 1. Note that we use brackets for comments.
In figure 2 we show a small portion of the WN-
PROTO for park. Again, WordNet classes are dis-
played in boxes with a label which best reflects the
meaning of the class. The probability estimates are
shown in the boxes for all the classes included in
the WNPROTO. The classes in the WNPROTO model
are shown with dashed lines. Examples of the ar-
gument head data are displayed below the WordNet
classes with dotted lines indicating membership at
a hyponym class beneath these classes. We cannot
show the full WNPROTO due to lack of space, but
we show some of the classes with higher probability
which cover some typical nouns that occur as objects
of park.
373
Algorithm 1 WNPROTO algorithm
C = (){classes in WNPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 ? WordNet
where |ty ? TY occurring in c ? C1| > 1
for all ty ? TY do
find c ? classes(ty) ? C1 where c = argmaxc typeratio(c)
if c & c /? C then
add c to C
add ty ? c to D {Disambiguated ty with c}
end if
end for
for all c ? C do
if |ty ? c ? D| > 1 then
fD = fD + frequency(ty){sum frequencies of types under classes to be used in model}
else
remove c from C {classes with less than two disambiguated nouns are removed}
end if
end for
for all c ? C do
p(c) = frequency-of-all-tys-disambiguated-to-class(c,D)fD {calculating class probabilities}
end for
Algorithm 2 DSPROTO algorithm
C = (){classes in DSPROTO}
D = () {disambiguated ty ? TY }
fD = 0 {frequency of disambiguated items}
TY = argument head types {nouns occurring as objects of verb, with associated frequencies}
C1 = cty ? TY where num-types-in-thesaurus(cty, TY ) > 1
order C1 by num-types-in-thesaurus(cty, TY ) {classes ordered by coverage of argument head types}
for all cty ? ordered C1 do
Dcty = () {disambiguated for this class}
for all ty ? TY where in-thesaurus-entry(cty, ty) do
if ty /? D then
add ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already}
end if
end for
if |Dcty| > 1 then
add cty to C
for all ty ? Dcty do
add ty ? cty to D {Disambiguated ty with cty}
fD = fD + frequency(ty)
end for
end if
end for
for all cty ? C do
p(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D)fD {calculating class probabilities}
end for
374
3.3 DSPROTOs
We use a thesaurus acquired using the method
proposed by Lin (1998). For input we used the
grammatical relation data from automatic parses of
the BNC. For each noun we considered the co-
occurring verbs in the object and subject relation,
the modifying nouns in noun-noun relations and
the modifying adjectives in adjective-noun relations.
Each thesaurus entry consists of the target noun and
the 50 most similar nouns, according to Lin?s mea-
sure of distributional similarity, to the target.
The argument head noun types (TY ) are used
to find the entries in the thesaurus as the ?classes?
(C) of the selectional preference for a given verb.
As with WNPROTOs, we only cover argument types
which form coherent groups with other argument
types since we wish i) to remove noise and ii) to
be able to identify argument types which are not re-
lated with the other types and therefore may be non-
compositional. As our starting point we only con-
sider an argument type as a class for C if its entry in
the thesaurus covers at least a threshold of 2 types. 6
To select C we use a best first search. This method
processes each argument type in TY in order of the
number of the other argument types from TY that it
has in its thesaurus entry of 50 similar nouns. An ar-
gument head is selected as a class for C (cty ? C) 7
if it covers at least 2 of the argument heads that are
not in the thesaurus entries of any of the other classes
already selected for C . Each argument head is dis-
ambiguated by whichever class in C under which it
is listed in the thesaurus and which has the largest
number of the TY in its thesaurus entry. When the
algorithm finishes processing the ordered argument
heads to select C , all argument head types are dis-
ambiguated by C apart from those which after dis-
ambiguation occur in isolation in a class without
other argument types. Finally a probability distri-
bution over C is estimated using the frequency (to-
kens) of argument types that occur in the thesaurus
entries for any cty ? C . If an argument type oc-
curs in the entry of more than one cty then it is as-
signed to whichever of these has the largest number
6As with the WNPROTOs, we experimented with a value of
3 for this threshold and obtained similar results.
7We use cty for the classes of the DSPROTO. These classes
are simply groups of nouns which occur under the entry of a
noun (ty) in the thesaurus.
class (p(c)) disambiguated objects (freq)
van (0.86) car (174) van (11) vehicle (8) . . .
mile (0.05) street (5) distance (4) mile (1) . . .
yard (0.03) corner (4) lane (3) door (1)
backside (0.02) backside (2) bum (1) butt (1) . . .
Figure 3: First four classes of DSPROTO model for
park
of disambiguated argument head types and its token
frequency is attributed to that class. We show the
algorithm as Algorithm 2.
The algorithms for WNPROTO algorithm 1 and
DSPROTO (algorithm 2) differ because of the na-
ture of the inventories of candidate classes (Word-
Net and the distributional thesaurus). There are a
great many candidate classes in WordNet. The WN-
PROTO algorithm selects the classes from all those
that the argument heads belong to directly and indi-
rectly by looping over all argument types to find the
class that disambiguates each by having the largest
type ratio calculated using the undisambiguated ar-
gument heads. The DSPROTO only selects classes
from the fixed set of argument types. The algorithm
loops over the argument types with at least two ar-
gument heads in the thesaurus entry and ordered by
the number of undisambiguated argument heads in
the thesaurus entry. This is a best first search to min-
imise the number of argument heads used in C but
maximise the coverage of argument types.
In figure 3, we show part of a DSPROTO model for
the object of park. 8 Note again that the class mile
arises because of a systematic parsing error where
adverbials such as distance in park illegally some
distance from the railway station are identified by
the parser as objects.
4 Experiments
Venkatapathy and Joshi (2005) produced a dataset of
verb-object pairs with human judgements of com-
positionality. They obtained values of rs between0.111 and 0.300 by individually applying the 7 fea-
tures described above in section 2. The best corre-
lation was given by feature 7 and the second best
was feature 3. They combined all 7 features using
SVMs and splitting their data into test and training
data and achieve a rs of 0.448, which demonstrates
8We cannot show the full model due to lack of space.
375
significantly better correlation with the human gold-
standard than any of the features in isolation
We evaluated our selectional preference models
using the verb-object pairs produced by Venkatapa-
thy and Joshi (2005). 9 This dataset has 765 verb-
object collocations which have been given a rat-
ing between 1 and 6, by two annotators (both flu-
ent speakers of English). Kendall?s Tau (Siegel and
Castellan, 1988) was used to measure agreement,
and a score of 0.61 was obtained which was highly
significant. The ranks of the two annotators gave a
Spearman?s rank-correlation coefficient (rs) of 0.71.
The Verb-Object pairs included some adjectives
(e.g. happy, difficult, popular), pronouns and com-
plements e.g. become director. We used the sub-
set of 638 verb-object pairs that involved common
nouns in the object relationship since our preference
models focused on the object relation for common
nouns. For each verb-object pair we used the pref-
erence models acquired from the RASP parses of the
BNC to obtain the probability of the class that this
object occurs under. Where the object noun is a
member of several classes (classes(noun) ? C)
in the model, the class with the largest probability
is used. Note though that for WNPROTOs we have
the added constraint that a hyponym class from C is
selected in preference to a hypernym in C . Compo-
sitionality of an object noun and verb is computed
as:-
comp(noun, verb) = maxc?classes(noun)?C p(c|verb) (3)
We use the probability of the class, rather than an
estimate of the probability of the object, because we
want to determine how likely any word belonging
to this class might occur with the given verb, rather
than the probability of the specific noun which may
be infrequent, yet typical, of the objects that occur
with this verb. For example, convertible may be
an infrequent object of park, but it is quite likely
given its membership of the class motor vehicle.
We do not want to assume anything about the fre-
quency of non-compositional verb-object combina-
tions, just that they are unlikely to be members of
classes which represent prototypical objects. We
9This verb-object dataset is available from
http://www.cis.upenn.edu/?sriramv/mywork.html.
method rs p < (one tailed)
selectional preferences
TCM 0.090 0.0119
WNPROTO 0.223 0.00003
DSPROTO 0.398 0.00003
features from V&J
frequency (f1) 0.141 0.00023
MI (f2) 0.274 0.00003
Lin99 (f3) 0.139 0.00023
LSA2 (f7) 0.209 0.00003
combination with SVM
f2,3,7 0.413 0.00003
f1,2,3,7 0.419 0.00003
DSPROTO f1,2,3,7 0.454 0.00003
Table 1: Correlation scores for 638 verb object pairs
will contrast these models with a baseline frequency
feature used by Venkatapathy and Joshi.
We use our selectional preference models to pro-
vide the probability that a candidate is represen-
tative of the typical objects of the verb. That is,
if the object might typically occur in such a rela-
tionship then this should lessen the chance that this
verb-object combination is non-compositional. We
used the probability of the classes from our 3 selec-
tional preference models to rank the pairs and then
used Spearman?s rank-correlation coefficient (rs) tocompare these ranks with the ranks from the gold-
standard.
Our results for the three types of preference mod-
els are shown in the first section of table 1. 10 All the
correlation values are significant, but we note that
using the type based selectional preference mod-
els achieves a far greater correlation than using the
TCMs. The DSPROTO models achieve the best re-
sults which is very encouraging given that they only
require raw data and an automatic parser to obtain
the grammatical relations.
We applied 4 of the features used by Venkatapa-
thy and Joshi (2005) 11 and described in section 2
to our subset of 638 items. These features were ob-
10We show absolute values of correlation following (Venkat-
apathy and Joshi, 2005).
11The other 3 features performed less well on this dataset so
we do not report the details here. This seems to be because they
worked particularly well with the adjective and pronoun data in
the full dataset.
376
tained using the same BNC dataset used by Venkat-
apathy and Joshi which was obtained using Bikel?s
parser (Bikel, 2004). We obtained correlation val-
ues for these features as shown in table 1 under
V&J. These features are feature 1 frequency, feature
2 pointwise mutual information, feature 3 based on
(Lin, 1999) and feature 7 LSA feature which consid-
ers the similarity of the verb-object pair with the ver-
bal form of the object. Pointwise mutual informa-
tion did surprisingly well on this 84% subset of the
data, however the DSPROTO preferences still out-
performed this feature. We combined the DSPROTO
and V&J features with an SVM ranking function and
used 10 fold cross validation as Venkatapathy and
Joshi did. We contrast the result with the V&J fea-
tures without the preference models. The results in
the bottom section of table 1 demonstrate that the
preference models can be combined with other fea-
tures to produce optimal results.
5 Conclusions and Directions for Future
Work
We have demonstrated that the selectional prefer-
ences of a verbal predicate can be used to indi-
cate if a specific combination with an object is non-
compositional. We have shown that selectional pref-
erence models which represent prototypical argu-
ments and focus on argument types (rather than to-
kens) do well at the task. Models produced from
distributional thesauruses are the most promising
which is encouraging as the technique could be ap-
plied to a language without a man-made thesaurus.
We find that the probability estimates from our
models show a highly significant correlation, and
are very promising for detecting non-compositional
verb-object pairs, in comparison to individual fea-
tures used previously.
Further comparison of WNPROTOs and
DSPROTOs to other WordNet models are war-
ranted to contrast the effect of our proposal for
disambiguation using word types with iterative
approaches, particularly those of Clark and Weir
(2002). A benefit of the DSPROTOs is that they
do not require a hand-crafted inventory. It would
also be worthwhile comparing the use of raw data
directly, both from the BNC and from google?s
Web 1T corpus (Brants and Franz, 2006) since
web counts have been shown to outperform the
Clark and Weir models on a pseudo-disambiguation
task (Keller and Lapata, 2003).
We believe that preferences should NOT be used
in isolation. Whilst a low preference for a noun
may be indicative of peculiar semantics, this may
not always be the case, for example chew the fat.
Certainly it would be worth combining the prefer-
ences with other measures, such as syntactic fixed-
ness (Fazly and Stevenson, 2006). We also believe it
is worth targeting features to specific types of con-
structions, for example light verb constructions un-
doubtedly warrant special treatment (Stevenson et
al., 2003)
The selectional preference models we have pro-
posed here might also be applied to other tasks. We
hope to use these models in tasks such as diathesis
alternation detection (McCarthy, 2000; Tsang and
Stevenson, 2004) and contrast with WordNet mod-
els previously used for this purpose.
6 Acknowledgements
We acknowledge support from the Royal Society
UK for a Dorothy Hodgkin Fellowship to the first
author. We thank the anonymous reviewers for their
constructive comments on this work.
References
Steven Abney and Marc Light. 1999. Hiding a semantic
class hierarchy in a Markov model. In Proceedings of
the ACL Workshop on Unsupervised Learning in Nat-
ural Language Processing, pages 1?8.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model ofmultiword expression decomposability. In Proceed-
ings of the ACL Workshop on multiword expressions:
analysis, acquisition and treatment, pages 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003. A statistical approach to the semantics ofverb-particles. In Proceedings of the ACL Workshop
on multiword expressions: analysis, acquisition and
treatment, pages 65?72.
Colin. Bannard. 2002. Statistical techniquesfor automatically inferring the semantics of verb-particle constructions. Technical Report WP-2002-
06, University of Edinburgh, School of Informatics.http://lingo.stanford.edu/pubs/WP-2002-06.pdf.
377
Colin Bannard. 2005. Learning about the meaning of
verb-particle constructions from corpora. Computer
Speech and Language, 19(4):467?478.
Daniel M. Bikel. 2004. A distributional analysis of a lex-
icalized statistical parsing model. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), Barcelona, Spain,
July. Association for Computational Linguistics.
Don Blaheta and Mark Johnson. 2001. Unsuper-vised learning of multi-word verbs. In Proceedings
of the ACL Workshop on Collocations, pages 54?60,Toulouse, France.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Edward Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the Third International Conference on Language
Resources and Evaluation (LREC), pages 1499?1504,
Las Palmas, Canary Islands, Spain.
Kenneth Church and Patrick Hanks. 1990. Word asso-ciation norms, mutual information and lexicography.
Computational Linguistics, 19(2):263?312.
Stephen Clark and David Weir. 2002. Class-based prob-ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics (EACL-2006), pages 337?344,
Trento, Italy, April.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA.
Ralph Grishman and John Sterling. 1994. Generalizing
automatically generated selectional patterns. In Pro-
ceedings of the 15th International Conference of Com-
putational Linguistics. COLING-94, volume I, pages742?747.
Frank Keller and Mirella Lapata. 2003. Using the web toobtain frequencies for unseen bigrams. Computational
Linguistics, 29(3):459?484.
Brigitte Krenn and Stefan Evert. 2001. Can we do betterthan frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL Workshop on
Collocations, pages 39?46, Toulouse, France.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,28(1):1?13.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computa-
tional Linguistics, 24(2):217?244.
Dekang Lin. 1998. Automatic retrieval and clusteringof similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Dekang Lin. 1999. Automatic identification of non-compositional phrases. In Proceedings of ACL-99,
pages 317?324, Univeristy of Maryland, College Park,Maryland.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasalverbs. In Proceedings of the ACL 03 Workshop: Multi-
word expressions: analysis, acquisition and treatment,pages 73?80.
Diana McCarthy. 2000. Using semantic preferences toidentify verbal participation in role switching alter-nations. In Proceedings of the First Conference of
the North American Chapter of the Association for
Computational Linguistics. (NAACL), pages 256?263,Seattle,WA.
Fernando Pereira, Nattali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st Annual Meeting of the Association
for Computational Linguistics, pages 183?190.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.Ph.D. thesis, University of Pennsylvania.
Jorma Rissanen. 1978. Modelling by shortest data de-scription. Automatica, 14:465?471.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing
2002), pages 1?15, Mexico City, Mexico.
Patrick Schone and Daniel Jurafsky. 2001. Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem? In Proceedings of the
2001 Conference on Empirical Methods in Natural
Language Processing, pages 100?108, Hong Kong.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Sidney Siegel and N. John Castellan. 1988. Non-
Parametric Statistics for the Behavioral Sciences.
McGraw-Hill, New York.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.2003. Statistical measures of the semi-productivity oflight verb constructions. In Proceedings of the ACL
2004 Workshop on Multiword Expressions: Integrat-
ing Processing, Barcelona, Spain.
378
Vivian Tsang and Suzanne Stevenson. 2004. Using se-
lectional profile distance to detect verb alternations. In
Proceedings of NAACL Workshop on Computational
Lexical Semantics (CLS-04), pages 30?37, Boston,
MA.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-suring the relative compositionality of verb-noun (v-n)collocations by integrating features. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, pages 899?906, Vancouver, B.C., Canada.
Andreas Wagner. 2002. Learning thematic role relations
for wordnets. In Proceedings of ESSLLI-2002 Work-
shop on Machine Learning Approaches in Computa-
tional Linguistics, Trento.
379
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 440?449,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Graded Word Sense Assignment
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Abstract
Word sense disambiguation is typically
phrased as the task of labeling a word in
context with the best-fitting sense from a
sense inventory such as WordNet. While
questions have often been raised over the
choice of sense inventory, computational
linguists have readily accepted the best-
fitting sense methodology despite the fact
that the case for discrete sense bound-
aries is widely disputed by lexical seman-
tics researchers. This paper studies graded
word sense assignment, based on a recent
dataset of graded word sense annotation.
1 Introduction
The task of automatically characterizing word
meaning in text is typically modeled as word sense
disambiguation (WSD): given a list of senses for
target lemma w, the task is to pick the best-fitting
sense for a given occurrence of w. The list of
senses is usually taken from an online dictionary
or thesaurus. However, clear cut sense boundaries
are sometimes hard to define, and the meaning of
words depends strongly on the context in which
they are used (Cruse, 2000; Hanks, 2000). Some
researchers in lexical semantics have suggested
that word meanings lie on a continuum between
i) clear cut cases of ambiguity and ii) vagueness
where clear cut boundaries do not hold (Tuggy,
1993). Certainly, it seems that a more complex
representation of word sense is needed with a
softer, graded representation of meaning rather
than a fixed listing of senses (Cruse, 2000).
A recent annotation study ((Erk et al, 2009),
hereafter GWS) marked a target word in context
with graded ratings (on a scale of 1-5) on senses
from WordNet (Fellbaum, 1998). Table 1 shows
an example of a sentence with the target word
in bold, and with the annotator judgments given
to each sense. The study found that annotators
made ample use of the intermediate ratings on the
scale, and often gave high ratings to more than one
WordNet sense for the same occurrence. It was
found that the annotator ratings could not easily
be transformed to categorial judgments by making
more coarse-grained senses. If human word sense
judgments are best viewed as graded, it makes
sense to explore models of word sense that can
predict graded sense assignments.
In this paper we look at the issue of graded ap-
plicability of word sense from the point of view
of automatic graded word sense assignment, us-
ing the GWS graded word sense dataset. We make
three primary contributions. Firstly, we propose
evaluation metrics that can be used on graded
word sense judgments. Some of these metrics, like
Spearman?s ?, have been used previously (Mc-
Carthy et al, 2003; Mitchell and Lapata, 2008),
but we also introduce new metrics based on the
traditional precision and recall. Secondly, we in-
vestigate how two classes of models perform on
the task of graded word sense assignment: on
the one hand classical WSD models, on the other
hand prototype-based vector space models that
can be viewed as simple one-class classifiers. We
study supervised models, training on traditional
WSD data and evaluating against a graded scale.
Thirdly, the evaluation metrics we use also pro-
vides a novel analysis of annotator performance
on the GWS dataset.
2 Related Work
WSD has to date been a task where word senses are
viewed as having clear cut boundaries. However,
there are indications that word meanings do not
behave in this way (Kilgarriff, 2006). Researchers
in the field of WSD have acknowledged these prob-
lems but have used existing lexical resources in
the hope that useful applications can be built with
them. However, there is no consensus on which
440
Senses
Sentence 1 2 3 4 5 6 7 Annotator
This can be justified thermodynamically in this case, and
this will be done in a separate paper which is being
prepared.
2 3 3 5 5 2 3 Ann. 1
1 3 1 3 5 1 1 Ann. 2
1 5 2 1 5 1 1 Ann. 3
1.3 3.7 2 3 5 1.3 1.7 Avg
Table 1: A sample annotation in the GWS experiment. The senses are: 1 material from cellulose 2 report
3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object
inventory is suitable for which application, other
than cross-lingual applications where the inven-
tory can be determined from parallel data (Carpuat
and Wu, 2007; Chan et al, 2007). For monolin-
gual applications however it is less clear whether
current state-of-the-art WSD systems for tagging
text with dictionary senses are able to have an im-
pact on applications.
One way of addressing the problem of low inter-
annotator agreement and system performance is to
create an inventory that is coarse-grained enough
for humans and computers to do the job reli-
ably (Ide and Wilks, 2006; Hovy et al, 2006;
Palmer et al, 2007). Such coarse-grained invento-
ries can be produced manually from scratch (Hovy
et al, 2006) or by automatically relating (Mc-
Carthy, 2006) or clustering (Navigli, 2006; Nav-
igli et al, 2007) existing word senses. While the
reduction in polysemy makes the task easier, we
do not know which are the right distinctions to re-
tain. In fact, fine-grained distinctions may be more
useful than coarse-grained ones for some applica-
tions (Stokoe, 2005). Furthermore, Hanks (2000)
goes further and argues that while the ability to
distinguish coarse-grained senses is indeed desir-
able, subtler and more complex representations of
word meaning are necessary for text understand-
ing.
In this paper, instead of focusing on issues of
granularity we try to predict graded judgments of
word sense applicability, using a recent dataset
with graded annotation (Erk et al, 2009). Our
hope is that models which can mimic graded hu-
man judgments on the same task should better re-
flect the underlying phenomena of word mean-
ing compared to a system that focuses on mak-
ing clear cut distinctions. Also, we hope that such
models might prove more useful in applications.
There is one existing study of graded sense as-
signment (Ramakrishnan et al, 2004). It tries to
estimate a probability distribution over senses by
converting all of WordNet into a huge Bayesian
Network, and reports improvements in a Question
Answering task. However, it does not test its pre-
diction against human annotator data.
We concentrate on supervised models in this
paper since they generally perform better than
their unsupervised or knowledge-based counter-
parts (Navigli, 2009). We compare them against
a baseline model which simply uses the train-
ing data to obtain a probability distribution over
senses regardless of context, since marginal distri-
butions are highly skewed making a prior distribu-
tion very informative (Chan and Ng, 2005; Lapata
and Brew, 2004).
Along with standard WSD models, we evalu-
ate vector space models that use the training data
to locate a word sense in semantic space. Word
sense and vector space models have been related in
two ways. On the one hand, vector space models
have been used for inducing word senses (Sch?utze,
1998; Pantel and Lin, 2002). The different mean-
ings of a word are obtained by clustering vectors.
The clusters must then be mapped to an inven-
tory if a standard WSD dataset is used for eval-
uation. In contrast, we use sense tagged train-
ing data with the aim of building models of given
word senses, rather than clustering occurrences
into word senses. The second way in which word
sense and vector space models have been related is
to assign disambiguated feature vectors to Word-
Net concepts (Pantel, 2005; Patwardhan and Ped-
ersen, 2006). However those works do not use
sense-tagged data and are not aimed at WSD, rather
the applications are to insert new concepts into an
ontology and to measure the relatedness of con-
cepts.
We are not concerned in this paper with argu-
ing for or against any particular sense inventory.
WordNet has been criticized for being overly fine-
grained (Navigli et al, 2007; Ide and Wilks, 2006),
we are using it here because it is the sense inven-
tory used by Erk et al (2009). That annotation
study used it because it is sufficiently fine-grained
to allow for the examination of subtle distinctions
between usages and because it is publicly available
441
lemma # # training
(PoS) senses SemCor SE-3
add (v) 6 171 238
argument (n) 7 14 195
ask (v) 7 386 236
different (a) 5 106 73
important (a) 5 125 11
interest (n) 7 111 160
paper (n) 7 46 207
win (v) 4 88 53
total training sentences 1047 1173
Table 2: Lemmas used in this study
with various sense-tagged datasets (e.g. (Miller et
al., 1993; Mihalcea et al, 2004)) for comparison.
3 Data
In this paper, we use a subset of the GWS
dataset (Erk et al, 2009) where three annotators
supplied ordinal judgments of the applicability of
WordNet (v3.0) senses on a 5 point scale: 1 ?
completely different, 2 ? mostly different, 3 ? sim-
ilar, 4 ? very similar and 5 ? identical. Table 1
shows a sample annotation. The sentences that
we use from the GWS dataset were originally ex-
tracted from the English SENSEVAL-3 lexical sam-
ple task (Mihalcea et al, 2004) (hereafter SE-3)
and SemCor (Miller et al, 1993).
1
For 8 lem-
mas, 25 sentences were randomly sampled from
SemCor and 25 randomly sampled from SE-3, giv-
ing a total of 50 sentences per lemma. The lem-
mas, their PoS and number of senses from Word-
Net are shown in table 2.
The annotation study found that annotators
made ample use of the intermediate levels of ap-
plicability (2-4), and they often gave positive rat-
ings (3-5) to more than one sense for a single oc-
currence. The example in Table 1 is one such
case. An analysis of the annotator ratings found
that they could not easily be explained in catego-
rial terms by making more coarse-grained senses
because senses that were not positively correlated
often had high ratings for the same instance.
The GWS dataset contains a sequence of judg-
ments for each occurrence of a target word in a
sentence context: one judgment for each Word-
Net sense of the target word. To obtain a sin-
gle judgment for each sense in each sentence we
use the average judgment from the three annota-
tors. As models typically assign values between
1
The GWS data also contains data from the English Lex-
ical Substitution Task (McCarthy and Navigli, 2007) but we
do not use that portion of the data for these experiments.
0 and 1, we normalize the annotator judgments
from the GWS dataset to fall into the same range by
using normalized judgment = (judgment ?
1.0)/4.0. This maps an original judgment of 5 to
a normalized judgment of 1.0, it maps an original
1 to 0.0, and intermediate judgments are mapped
accordingly.
As the GWS dataset is too small to accommodate
both training and testing of a supervised model, we
use all the data from GWS for testing our models,
and train our models on traditional word sense an-
notation data. We use as training data all sentences
from SemCor and the training portion of SE-3 that
are not included in GWS. The quantity of training
data available is shown in the last two columns of
table 2.
4 Evaluating Graded Word Sense
Assignment
This section discusses measures for evaluating
system performance for the case where gold judg-
ments are graded rather than categorial.
Correlation. The standard method for compar-
ing a list of graded gold judgments to a list of
graded predicted judgments is by testing for corre-
lation. In our case, as we cannot assume a normal
distribution of the judgments, a non-parametric
test such as Spearman?s ? will be appropriate.
Spearman?s ? uses the formula of Pearson?s coef-
ficient, defined as
?(X,Y ) =
cov(X,Y )
?
X
?
Y
Pearson?s coefficient computes the correlation of
two random variables X and Y as their covari-
ance divided by the product of their standard devi-
ations. In the computation of Spearman?s ?, values
are transformed to rankings before the formula is
applied.
2
As Spearman?s ? compares the rank-
ings of two sets of judgments, it abstracts from the
absolute values of the judgments. It is useful to
have a measure that abstracts from absolute values
of judgments and magnitude of difference because
the GWS dataset contains annotator judgments on
a fixed scale, and it is quite possible that human
judges will differ in how they use such a scale.
Each judgment in the gold-standard can be
represented as a 4-tuple ?lemma, sense no, sen-
tence no, gold judgment?. For example, ?add.v,
2
Mitchell and Lapata (2008) note that Spearman?s ? tends
to yield smaller coefficients than its parametric counterparts
such as Pearson?s coefficient.
442
1, 1, 0.8? is the first sentence for target add.v, first
WordNet sense, with a (normalized) judgment of
0.8. Likewise, each prediction by the model can
be represented as a 4-tuple ?lemma, sense no, sen-
tence no, predicted judgment?. We writeG for the
set of gold tuples, A for the set of assigned tuples,
L for the set of lemmas, S
`
for the set of sense
numbers that exist for lemma `, and T for the set
of sentence numbers (there are 50 sentences for
each lemma). We writeG|
lemma=`
for the gold set
restricted to those tuples with lemma `, and anal-
ogously for other set restrictions and for A. There
are several possibilities for measuring correlation:
by lemma: for each lemma ` ? L, compute cor-
relation between G|
lemma=`
and A|
lemma=`
by lemma+sense: for each lemma ` and each
sense number i ? S
`
, compute cor-
relation between G|
lemma=`,senseno=i
and
A|
lemma=`,senseno=i
by lemma+sentence: for each lemma ` and sen-
tence number t ? T , compute cor-
relation between G|
lemma=`,sentence=t
and
A|
lemma=`,sentence=t
Comparison by lemma tests for the consis-
tent use of judgments for the same target lemma.
A comparison by lemma+sense ranks all occur-
rences of the same target lemma by how strongly
they evoke a given word sense. A comparison
by lemma+sentence ranks different senses by how
strongly they apply to a given target lemma oc-
currence. In reporting correlation by lemma (by
lemma+sense, by lemma+sentence), we average
over all lemmas (lemma+sense, lemma+sentence
combinations), and we report the percentage of
lemmas (combinations) for which the correlation
was significant. We report averaged correlation by
lemma rather than one overall correlation over all
judgments in order not to give more weight to lem-
mas with more senses.
Divergence. Another possibility for measuring
the performance of a graded sense assignment
model is to use Jensen/Shannon divergence (J/S),
which is a symmetric version of Kullback/Leibler
divergence. Given two probability distributions
p, q, the Kullback/Leibler divergence of q from p
is
D(p||q) =
?
x
p(x) log
p(x)
q(x)
and their J/S is
JS(p, q) =
1
2
(
D(p||
p+ q
2
) +D(q||
p+ q
2
)
We will use J/S for an evaluation by
lemma+sentence: for each lemma ` ? L
and sentence number t ? T , we normalize
G|
lemma=`,sentence=t
, the set of judgments for
senses of ` in t, by the sum of sense judgments for
` and t. We do the same for A|
lemma=`,sentence=t
.
Then we compute J/S. In doing so, we are not
trying to interpret G|
lemma=`,sentence=t
as some
kind of probability distribution over senses, rather
we use J/S as a measure that abstracts from
absolute judgments but not from the magnitude of
differences between judgments.
Precision and Recall. We have discussed a
measure that abstracts from both absolute judg-
ments and magnitude of differences (Spearman?s
?), and a measure that abstracts from absolute
judgments but not the magnitude of differences
(J/S). What is still missing is a measure that tests
to what degree a model conforms to the absolute
judgments given by the human annotators.
To obtain a measure for performance in predict-
ing absolute gold judgments, we generalize preci-
sion and recall. In the categorial case, precision is
defined as P =
true positives
true positives+false positives
, true pos-
itives divided by system-assigned positives, and
recall is R =
true positives
true positives+false negatives
, true posi-
tives divided by gold positives. Writing gold
`,i,t
for the judgment j associated with lemma ` and
sense number i for sentence t in the gold data (i.e.,
?`, i, t, j? ? G), and analogously assigned
`,i,t
, we
extend precision and recall to the graded case as
follows:
P
`
=
?
i?S
`
,t?T
min(gold
`,i,t
, assigned
`,i,t
)
?
i?S
`
,t?T
assigned
`,i,t
and
R
`
=
?
i?S
`
,t?T
min(gold
`,i,t
, assigned
`,i,t
)
?
i?S
`
,t?T
gold
`,i,t
where ` is a lemma. We compute precision and re-
call by lemma, then macro-average them in order
not to give more weight to lemmas that have more
senses. The formula for F-score as the harmonic
mean of precision and recall remains unchanged:
F = 2 P R/(P +R).
If the data is categorial, the graded precision and
recall measures coincide with ?classical? precision
443
Cx/2 until, IN, soft, JJ, remaining, VBG, ingredient,
NNS
Cx/50 for, IN, sweet-sour, NN, sauce, NN, . . . , to, TO,
a, DT, boil, NN
Ch OA, OA/ingredient/NNS
Table 3: Sample features for add in BNC occur-
rence For sweet-sour sauce, cook onion in oil un-
til soft. Add remaining ingredients and bring to
a boil. Cx/2 (Cx/50): context of size 2 (size 50)
either side of the target. Ch: children of target.
and recall, which can be seen as follows. Graded
sense assignment is represented by assigning each
sense a score between 0.0 and 1.0. The categorial
case can be represented in the same way, the dif-
ference being that one single sense will receive a
score of 1.0 while all other senses get a score of
0.0. With this representation for categorial sense
assignment, consider a fixed token t of lemma `.
?
i?S
`
min(assigned
`,i,t
, gold
`,i,t
) will be 1 if the
assigned sense is the gold sense, and 0 otherwise.
5 Models for Graded Word Sense
Assignment
In this section we discuss the computational mod-
els for graded word sense that are tested in this
paper.
Single-best-sense WSD. The first model that we
test is a standard WSD model that assigns, to each
test occurrence of a target word, a single best-
fitting word sense. The system thus attributes a
confidence score of 1 to the assigned sense and a
confidence score of 0 for all other senses for that
sentence. We refer to it as WSD/single. The model
uses standard features: lemma and part of speech
in a narrow context window (2 words either side)
and a wide context window (50 words either side),
as well as dependency labels leading to parent,
children, and siblings of the target word, and lem-
mas and part of speech of parent, child, and sibling
nodes. Table 3 shows sample model features for an
occurrence of add in the British National Corpus
(BNC) (Leech, 1992). The model uses a maxi-
mum entropy learner
3
, training one binary classi-
fier per sense. (With n-ary classifiers, the model?s
performance is slightly worse.) The model is thus
not highly optimized, but fairly standard.
WSD confidence level as judgment. Our second
model is the same WSD system as above, but we
3
http://maxent.sourceforge.net/
use it to predict a judgment for each sense of a
target occurrence, taking the confidence level re-
turned by each sense-specific binary classifier as
the predicted judgment. We refer to this model as
WSD/conf .
Word senses as points in semantic space. The
results of the GWS annotation study raise the ques-
tion of how word senses are best conceptualized,
given that annotators assigned graded judgments
of applicability of word senses, and given that they
often combined high judgments for multiple word
senses. One way of modeling these findings is
to view word senses as prototypes, where some
uses of a word will be typical examples of a given
sense, for some uses the sense will clearly not ap-
ply, and to some uses the sense will be borderline
applicable.
We use a very simple model of word senses as
prototypes, representing them as points in a se-
mantic space. Graded sense applicability judg-
ments can then be modeled using vector similarity.
The dimensions of the vector space are the features
of the WSD system above (including dimensions
like Cx2/until, Cx2/IN, Ch/OA/ingredient/NNS for
the example in Table 3), and the coordinates are
raw feature counts. We compute a single vector
for each sense s, the centroid of all training oc-
currences that have been labeled with s. The pre-
dicted judgment for a test sentence and sense s
is then the similarity of the sentence?s vector to
the centroid vector for s, computed using cosine.
We call this model Prototype. Like instance-based
learners (Daelemans and den Bosch, 2005), the
Prototype model measures the distance between
feature vectors in space. Unlike instance-based
learners, it only uses data from a single category
for training.
As it is to be expected that the vectors in this
space will be very sparse, we also test a variant
of the Prototype model with Sch?utze-style second-
order vectors (Sch?utze, 1998), called Prototype/2.
Given a (first-order) feature vector, we compute
a second-order vector as the centroid of vectors
for all lemma features (omitting stopwords) in the
first-order vector. For the feature vector in Table 3,
this is the centroid of vectors
~
sweet-sour,
~
sauce,
. . . ,
~
boil. We compute the vectors
~
sweet-sour etc.
as dependency vectors (Pad?o and Lapata, 2007)
4
over a Minipar parse (Lin, 1993) of the BNC.
4
We use the DV package, http://www.nlpado.de/
?
sebastian/dv.html, to compute the vector space.
444
We transform raw co-occurrence counts in the
BNC-based vectors using pointwise mutual in-
formation (PMI), a common transformation func-
tion (Mitchell and Lapata, 2008).
5
Another way of motivating the use of vector
space models of word sense is by noting that we
are trying to predict graded sense assignment by
training on traditional word sense annotated data,
where each target word occurrence is typically
marked with a single word sense. Traditional word
sense annotation, when used to predict GWS judg-
ments, will contain spurious negative data: sup-
pose a human annotator is annotating an occur-
rence of target word t and views senses s
1
, s
2
and
s
3
as somewhat applicable, with sense s
1
applying
most clearly. Then if the annotation guidelines ask
for the best-fitting sense, the annotator should only
assign s
1
. The occurrence is recorded as having
sense s
1
, but not senses s
2
and s
3
. This, then, con-
stitutes spurious negative data for senses s
2
and s
3
.
The simple vector space model of word sense that
we use implements a radical solution to this prob-
lem of spurious negative data: it only uses positive
data for a single sense, thus forgoing competition
between categories. It is to be expected that not
using competition between categories will hurt the
vector space model?s performance, but this design
gives us the chance to compare two model classes
that use opposing strategies with respect to spuri-
ous negative data: the WSD models fully trust the
negative data, while the vector space models ig-
nore it.
6 Experiments
This section reports on experiments for the task
of graded word sense assignment. As data, we
use the GWS dataset described in Sec. 3. We test
the models discussed in Sec. 5, evaluating with the
methods described in Sec. 4.
To put the models? performance into perspec-
tive, we first consider the human performance on
the task, shown in Table 4. The first three lines
of the table show the performance of each annota-
tor evaluated against the average of the other two.
The fourth line averages over the previous three
lines to provide an average human ceiling for the
task. In the correlation of rankings by lemma, cor-
relation is statistically significant for all lemmas at
5
We also tested PMI transformation for the first-order vec-
tors, but will not report the results here as they were worse
across the board than without PMI.
p ? 0.01. For correlation by lemma+sense and by
lemma+sentence, the percentage of pairs with sig-
nificant correlation is lower: 73.6 of lemma/sense
pairs and 29.0 of lemma/sentence pairs reach sig-
nificance at p ? 0.05. For p ? 0.01, the per-
centage is 58.3 and 12.2, respectively. The higher
? but lower proportion of significant values for
lemma+sentence pairs compared to lemma+sense
is due to the fact that there are far fewer dat-
apoints (sample size) for each calculation of ?
(#senses for lemma+sentence vs 50 sentences for
lemma+sense).
At 0.131, J/S for Annotator 1 is considerably
lower than for Annotators 2 and 3.
6
In terms
of precision and recall, Annotator 1 again differs
from the other two. At 87.5, her recall is higher
than her precision (50.6), while the other annota-
tors have considerably higher precision (75.5 and
82.4) than recall (62.4 and 52.3). This indicates
that Annotator 1 tended to assign higher ratings
throughout, an impression that is confirmed by Ta-
ble 6. The left two columns show average rat-
ings for each annotator over all senses of all to-
kens (normalized to values between 0.0 and 1.0 as
described in Sec. 3). The three annotators differ
widely in their average ratings, which range from
0.285 for Ann.3 to 0.540 for Ann.1.
Standard WSD. We tested the performance of
the WSD/single model on a standard WSD task,
using the same training and testing data as in
our subsequent experiments, as described in sec-
tion 3.
7
The model?s accuracy when trained and
tested on SemCor was A=77.0%, with a most fre-
quent sense baseline of 63.5%. When trained
and tested on SE-3, the model achieved A=53.0%
against a baseline of 44.0%. When trained and
tested on SemCor plus SE-3, the model reached an
accuracy 58.2%, with a baseline of 56.0%. So on
the combined dataset, the baseline is the average
of the baselines on the individual datasets, while
the model?s performance falls below the average
performance on the individual datasets.
WSD models for graded sense assignment.
Table 5 shows the performance of different mod-
els in the task of graded word sense assignment.
The first line in Table 5 lists results for the maxi-
mum entropy model when used to assign a single
best sense. The second line lists the results for
6
Low J/S implies a closer agreement between two sets of
judgments.
7
Note that this constitutes less training data than in the
SE-3 task.
445
by lemma by lemma+sense by lemma+sentence
Ann ? * ** ? * ** ? * ** J/S P R F
Ann.1 0.517 100.0 100.0 0.407 75.0 58.3 0.482 27.3 11.5 0.131 50.6 87.5 64.1
Ann.2 0.587 100.0 100.0 0.403 68.8 58.3 0.612 38.1 17.2 0.153 75.5 62.4 68.3
Ann.3 0.528 100.0 100.0 0.41 77.1 58.3 0.51 21.8 7.8 0.165 82.4 52.3 64.0
Avg 0.544 100.0 100.0 0.407 73.6 58.3 0.535 29.0 12.2 0.149 69.5 67.4 65.5
Table 4: Human ceiling: one annotator vs. average of the other two annotators. ?, ??: percentage
significant at p ? 0.05, p ? 0.01. Avg: average annotator performance
by lemma by lemma+sense by lemma+sentence
Model ? * ** ? * ** ? * ** J/S P R F
WSD/single 0.267 87.5 75.0 0.053 6.3 4.2 0.28 2.8 1.8 0.39 58.7 25.5 35.5
WSD/conf 0.396 87.5 87.5 0.177 33.3 18.8 0.401 10.8 3.0 0.164 81.8 37.1 51.0
Prototype 0.245 62.5 62.5 0.053 20.8 8.3 0.396 15.3 2.5 0.173 58.4 78.3 66.9
Prototype/2 0.292 87.5 87.5 0.086 14.6 4.2 0.478 22.8 7.5 0.164 68.2 63.3 65.7
Prototype/N 0.396 100.0 100.0 0.137 22.9 14.6 0.396 15.3 2.5 0.173 82.2 29.9 43.9
Prototype/2N 0.465 100.0 100.0 0.168 29.8 23.4 0.478 22.8 7.5 0.164 82.6 30.9 45.0
baseline 0.338 87.5 87.5 0.0 0.0 0.0 0.355 10.3 3.0 0.167 79.9 34.5 48.2
Table 5: Evaluation: computational models, and baseline. ?, ??: percentage significant at p ? 0.05,
p ? 0.01
the same maximum entropy model when classifier
confidence is used as predicted judgment. The last
line shows the baseline, an adaptation of the most
frequent sense baseline to the graded case. For
this baseline, we computed the relative frequency
of each sense in the training corpus and used this
relative frequency as the prediction for each test
sentence and sense combination. The WSD/single
model remains below the baseline in all evalua-
tions except correlation by lemma+sense, where
no rank-based correlation could be computed for
the baseline because it always assigns the same
judgment for a given sense. WSD/conf shows a
performance slightly above the baseline in all eval-
uation measures. Table 6 lists average ratings, av-
eraged over all lemmas, senses, and occurrences,
for each model in the two right-hand columns.
Prototype models. Lines 3-6 in Tables 5 and
6 show results for Prototype variants. While each
Prototype and Prototype/2 model only sees pos-
itive data annotated for a single sense, the vari-
ants with /N (lines 5 and 6) make very limited use
of information coming from all senses of a given
lemma. They normalize judgments for each sen-
tence, with
assigned
norm
`,i,t
=
assigned
`,i,t
?
j?S
`
assigned
`,j,t
Line 3 evaluates the Prototype model with first-
order vectors. Its correlation with the gold data is
somewhat lower than that of WSD/conf in almost
all cases.
8
The Prototype model deviates strongly
8
The reason why the average ? for correlation by
from both WSD/conf and baseline in having a very
good recall, at 78.3, with lower precision at 58.4,
for an overall F-score that is 16 points higher than
that of WSD/conf . Both Prototype and Prototype/2
have average ratings (Table 6) far above those
of the WSD models and of the /N variants. The
second-order vector model Prototype/2 has rela-
tively low correlation by lemma+sense, while cor-
relation by lemma+sentence shows the best per-
formance of all models (along with Prototype/2N).
Its correlation by lemma+sentence is similar to the
lowest correlation by lemma+sentence achieved
by a human annotator. In terms of J/S, this
model also shows the best performance along with
WSD/conf and Prototype/2N. Both /N variants
achieve very high correlation by lemma. Corre-
lation by lemma+sense for the /N models is be-
tween those of Prototype and WSD/conf . The cor-
relation by lemma+sentence is the same with or
without normalization, as normalization does not
change the ranking of senses of an individual sen-
tence. While Prototype has higher recall than pre-
cision, normalization turns it into a model with
even higher precision than WSD/conf but even
lower recall.
Discussion
Human performance. The evaluation of human
annotators in Table 4 provides a novel analysis of
the GWS dataset over and above that by Erk et al
lemma+sense is the same for Prototype and WSD/single
while the significance percentage differs greatly is that the
Prototype shows negative correlation for some of the senses.
446
Ann. avg Model avg
Ann.1 0.540 WSD/single 0.163
Ann.2 0.345 WSD/conf 0.173
Ann.3 0.285 Prototype 0.558
Prototype/N 0.143
Prototype/2 0.375
Prototype/2N 0.143
baseline 0.167
Table 6: Average judgment for individual annota-
tors (transformed) and average rating for models
(2009). Human annotators show very strong cor-
relation of their rankings by lemma. They also had
strong agreement on rankings by lemma+sense,
which ranks occurrences of a lemma by how
strongly they evoke a given sense. The relatively
low precision and recall in Table 4 confirm that
different annotators use the 5-point scale in differ-
ent ways. A comparison of precision and recall
between the annotators reflects the fact that An-
notator 1 tended to give considerably higher rat-
ings than the other two, which is also apparent
in the average ratings in Table 6. Given the rela-
tively low F-score achieved by human annotators,
judgments by additional annotators could make
the GWS dataset more useful, in that the average
judgments would not be influenced so strongly by
idiosyncrasies in the use of the 5-point scale. (Psy-
cholinguistic experiments using fixed scales typi-
cally elicit judgments from 10 or more participants
per item.)
Evaluation measures. Given the degree of dif-
ferences in the absolute values of the human an-
notator judgments (Table 4), a rank-based evalu-
ation of graded sense assignment models, com-
plemented by J/S to evaluate the magnitude of
differences between ratings, seems most appro-
priate to the data. Rankings by lemma+sense
and by lemma+sentence are especially interest-
ing for their potential use in systems that might
use graded sense assignment as part of a larger
pipeline. Still, the new graded precision and re-
call measures allow for a more fine-grained anal-
ysis of the performance of models, showing fun-
damental differences in the behavior of WSD/conf
and the Prototype model. Graded precision and
recall could become even more informative mea-
sures with a gold set containing judgments of more
annotators, since then the absolute gold judgments
would be more reliable.
Standard WSD models and vector space mod-
els. The results in Table 5 reflect the compromise
between the advantage of having competition be-
tween categories and the disadvantage of spurious
negative data: WSD/conf , Prototype/N and Proto-
type/2N achieve the highest correlation by lemma,
and high precision, while Prototype has much bet-
ter recall for an overall higher F-score. However,
as Table 6 shows, Prototype tends to assign high
ratings across the board, leading to high recall.
The much lower average ratings of the /N mod-
els explain their higher precision and lower recall:
they overshoot less and undershoot more. The im-
provement in correlation for the /N models also
indicates that Prototype assigns some sentences
high ratings for all senses, impacting rankings by
lemma and by lemma+sense.
The comparison of Prototype and Prototype/2
gives us a chance to study effects of feature sparse-
ness. Prototype/2, using second-order vectors that
should be much less sparse, yields better rankings
than Prototype. The average ratings of model Pro-
totype/2 (Table 6) are lower than those of Pro-
totype (and closer to human average ratings), re-
sulting in higher precision and lower recall. One
possible reason for the high average ratings of
Prototype is that in sparser (and shorter) vectors,
matches in dimensions for high-frequency, rela-
tively uninformative context items have greater
impact.
It is interesting to see that WSD/conf performs
slightly above the sense frequency baseline in all
evaluations, since this is a very familiar picture
from standard WSD.
Prototype/2N shows the overall most favorable
performance in terms of correlation as it i) pays
minimal attention to the negative data ii) uses nor-
malization to avoid overshooting and iii) compen-
sates for sparse data by using second order vectors.
For J/S, WSD/conf , Prototype/2, Prototype/2N
and the sense frequency baseline just outperform
the score of the lowest-scoring of the three anno-
tators. In terms of F-score, Prototype shows re-
sults very close to human performance. Interest-
ingly, the Prototype model resembles Annotator 1
in its precision and recall, while WSD/conf more
resembles Annotators 2 and 3. None of the mod-
els come close to human performance in ranking
by lemma+sense, which requires an identification
of the ?typical? occurrence of a given sense. The
low ratings in correlation by lemma+sense indi-
cate that the models might be limited by the lack
of training data for many of the rarer senses. In fu-
447
ture work, we will test how the frequency of senses
in the training data affects the different models.
7 Conclusion
In this paper we have done a first study on mod-
eling graded annotator judgments on sense appli-
cability. We have discussed evaluation measures
for models of graded sense assignment, includ-
ing new extensions of precision and recall to the
graded case. A combination of rank-based correla-
tion at the level of lemmas, senses, and sentences,
Jensen/Shannon divergence, and precision and re-
call provided a nuanced picture of the strengths
and weaknesses of different models. We have
tested two types of models: on the one hand a
standard binary WSD model using classifier con-
fidence as predicted judgments, and on the other
hand several vector space models which compute a
prototype vector for each sense in semantic space.
These two types of model differ strongly in their
behavior. The WSD model shows a similar behav-
ior as the baseline, with high precision but low re-
call, while the unnormalized version of the vector
space model has higher recall at lower precision.
The results show both the benefits of having com-
petition between categories, for improved rank-
based correlation and precision, and the problem
of spurious negative data in the training set arising
from the best-sense methodology.
The last two correlation measures, by
lemma+sense and by lemma+sentence, yield
maybe the most insight into the question of the
usability of a computational model for graded
word sense assignment: a graded word sense
assignment model that is a component of a larger
system could provide useful sense information
either by ranking occurrences by how strongly
they evoke a sense, or by ranking senses by how
strongly they apply to a given occurrence. There
is room for improvement however as system
performance is well below that of humans. In
the future we plan to investigate features that are
more informative for making graded judgments.
Second, the vector space model we used was
very simple; it might be worthwhile to test more
sophisticated one-class classifiers (Marsland,
2003; Sch?olkopf et al, 2000).
Acknowledgments. We acknowledge support
from the UK Royal Society for a Dorothy Hodgkin
Fellowship to the second author.
References
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambigua-
tion. In Proceedings of EMNLP-CoNLL 2007, pages
61?72, Prague, Czech Republic, June. Association
for Computational Linguistics.
Y. S. Chan and H. T. Ng. 2005. Word sense disam-
biguation with distribution estimation. In Proceed-
ings of IJCAI 2005, pages 1010?1015, Edinburgh,
Scotland.
Y. S. Chan, H. T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of ACL?07, Prague, Czech Re-
public, June.
D. A. Cruse. 2000. Aspects of the microstructure of
word meanings. In Y. Ravin and C. Leacock, edi-
tors, Polysemy: Theoretical and Computational Ap-
proaches, pages 30?51. OUP, Oxford, UK.
W. Daelemans and A. Van den Bosch. 2005. Memory-
Based Language Processing. Cambridge University
Press, Cambridge, UK.
K. Erk, D. McCarthy, and N. Gaylord. 2009. Inves-
tigations on word senses and word usages. In Pro-
ceedings of ACL-09, Singapore.
C. Fellbaum, editor. 1998. WordNet, An Electronic
Lexical Database. The MIT Press, Cambridge, MA.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. Ontonotes: The 90% solu-
tion. In Proceedings of the HLT-NAACL 2006 work-
shop on Learning word meaning from non-linguistic
data, New York City, USA. Association for Compu-
tational Linguistics.
N. Ide and Y. Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation, Algorithms and Appli-
cations, pages 47?73. Springer.
A. Kilgarriff. 2006. Word senses. In E. Agirre
and P. Edmonds, editors, Word Sense Disambigua-
tion, Algorithms and Applications, pages 29?46.
Springer.
M. Lapata and C. Brew. 2004. Verb class disambigua-
tion using informative priors. Computational Lin-
guistics, 30(1):45?75.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1993. Principle-based parsing without over-
generation. In Proceedings of ACL?93, Columbus,
Ohio, USA.
448
S. Marsland. 2003. Novelty detection in learning sys-
tems. Neural computing surveys, 3:157?195.
D. McCarthy and R. Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Pro-
ceedings of SemEval-2007, pages 48?53, Prague,
Czech Republic.
D. McCarthy, B. Keller, and J. Carroll. 2003. De-
tecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL 03 Workshop:
Multiword expressions: analysis, acquisition and
treatment, pages 73?80.
D. McCarthy. 2006. Relating WordNet senses for
word sense disambiguation. In Proceedings of the
ACL Workshop on Making Sense of Sense: Bring-
ing Psycholinguistics and Computational Linguis-
tics Together, pages 17?24, Trento, Italy.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In Pro-
ceedings of SensEval-3, Barcelona, Spain.
G. A. Miller, C. Leacock, R. Tengi, and R. T Bunker.
1993. A semantic concordance. In Proceedings of
the ARPA Workshop on Human Language Technol-
ogy, pages 303?308. Morgan Kaufman.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL?08
- HLT, pages 236?244, Columbus, Ohio.
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 task 7: Coarse-grained English all-
words task. In Proceedings of SemEval-2007, pages
30?35, Prague, Czech Republic.
R. Navigli. 2006. Meaningful clustering of senses
helps boost word sense disambiguation perfor-
mance. In Proceedings of COLING-ACL 2006,
pages 105?112, Sydney, Australia.
R. Navigli. 2009. Word sense disambiguation: a sur-
vey. ACM Computing Surveys, 41(2):1?69.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
M. Palmer, H. Trang Dang, and C. Fellbaum. 2007.
Making fine-grained and coarse-grained sense dis-
tinctions, both manually and automatically. Natural
Language Engineering, 13:137?163.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Proceedings of KDD?02.
P. Pantel. 2005. Inducing ontological co-occurrence
vectors. In Proceedings of ACL?05, Ann Arbor,
Michigan.
S. Patwardhan and T. Pedersen. 2006. Using wordnet-
based context vectors to estimate the semantic relat-
edness of concepts. In Proceedings of the EACL 06
Workshop: Making Sense of Sense: Bringing Psy-
cholinguistics and Computational Linguistics To-
gether, Trento, Italy.
G. Ramakrishnan, B.P. Prithviraj, A. Deepa, P. Bhat-
tacharyya, and S. Chakrabarti. 2004. Soft word
sense disambiguation. In Proceedings of GWC 04,
Brno, Czech Republic.
B. Sch?olkopf, R. Williamson, A. Smola, J. Shawe-
Taylor, and J. Platt. 2000. Support vector method
for novelty detection. Advances in neural informa-
tion processing systems, 12.
H. Sch?utze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1).
C. Stokoe. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of
HLT/EMNLP-05, pages 403?410, Vancouver, B.C.,
Canada.
D. H. Tuggy. 1993. Ambiguity, polysemy and vague-
ness. Cognitive linguistics, 4(2):273?290.
449
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 419?426, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Domain-Specific Sense Distributions and Predominant Sense Acquisition
Rob Koeling & Diana McCarthy & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
robk,dianam,johnca  @sussex.ac.uk
Abstract
Distributions of the senses of words are
often highly skewed. This fact is exploited
by word sense disambiguation (WSD) sys-
tems which back off to the predominant
sense of a word when contextual clues are
not strong enough. The domain of a doc-
ument has a strong influence on the sense
distribution of words, but it is not feasi-
ble to produce large manually annotated
corpora for every domain of interest. In
this paper we describe the construction of
three sense annotated corpora in different
domains for a sample of English words.
We apply an existing method for acquir-
ing predominant sense information auto-
matically from raw text, and for our sam-
ple demonstrate that (1) acquiring such
information automatically from a mixed-
domain corpus is more accurate than de-
riving it from SemCor, and (2) acquiring
it automatically from text in the same do-
main as the target domain performs best
by a large margin. We also show that
for an all words WSD task this automatic
method is best focussed on words that are
salient to the domain, and on words with
a different acquired predominant sense in
that domain compared to that acquired
from a balanced corpus.
1 Introduction
From analysis of manually sense tagged corpora,
Kilgarriff (2004) has demonstrated that distributions
of the senses of words are often highly skewed. Most
researchers working on word sense disambiguation
(WSD) use manually sense tagged data such as Sem-
Cor (Miller et al, 1993) to train statistical classi-
fiers, but also use the information in SemCor on the
overall sense distribution for each word as a back-
off model. In WSD, the heuristic of just choosing the
most frequent sense of a word is very powerful, es-
pecially for words with highly skewed sense distri-
butions (Yarowsky and Florian, 2002). Indeed, only
5 out of the 26 systems in the recent SENSEVAL-3
English all words task (Snyder and Palmer, 2004)
outperformed the heuristic of choosing the most fre-
quent sense as derived from SemCor (which would
give 61.5% precision and recall1). Furthermore, sys-
tems that did outperform the first sense heuristic did
so only by a small margin (the top score being 65%
precision and recall).
Over a decade ago, Gale et al (1992) observed
the tendency for one sense of a word to prevail in a
given discourse. To take advantage of this, a method
for automatically determining the ?one sense? given
a discourse or document is required. Magnini et al
(2002) have shown that information about the do-
main of a document is very useful for WSD. This is
because many concepts are specific to particular do-
mains, and for many words their most likely mean-
ing in context is strongly correlated to the domain of
the document they appear in. Thus, since word sense
distributions are skewed and depend on the domain
at hand we would like to know for each domain of
application the most likely sense of a word.
However, there are no extant domain-specific
sense tagged corpora to derive such sense distribu-
tion information from. Producing them would be ex-
tremely costly, since a substantial corpus would have
to be annotated by hand for every domain of interest.
In response to this problem, McCarthy et al (2004)
proposed a method for automatically inducing the
1This figure is the mean of two different estimates (Sny-
der and Palmer, 2004), the difference being due to multiword
handling.
419
predominant sense of a word from raw text. They
carried out a limited test of their method on text in
two domains using subject field codes (Magnini and
Cavaglia`, 2000) to assess whether the acquired pre-
dominant sense information was broadly consistent
with the domain of the text it was acquired from.
But they did not evaluate their method on hand-
tagged domain-specific corpora since there was no
such data publicly available.
In this paper, we evaluate the method on domain
specific text by creating a sense-annotated gold stan-
dard2 for a sample of words. We used a lexical sam-
ple because the cost of hand tagging several corpora
for an all-words task would be prohibitive. We show
that the sense distributions of words in this lexical
sample differ depending on domain. We also show
that sense distributions are more skewed in domain-
specific text. Using McCarthy et al?s method, we
automatically acquire predominant sense informa-
tion for the lexical sample from the (raw) corpora,
and evaluate the accuracy of this and predominant
sense information derived from SemCor. We show
that in our domains and for these words, first sense
information automatically acquired from a general
corpus is more accurate than first senses derived
from SemCor. We also show that deriving first sense
information from text in the same domain as the tar-
get data performs best, particularly when focusing
on words which are salient to that domain.
The paper is structured as follows. In section 2
we summarise McCarthy et al?s predominant sense
method. We then (section 3) describe the new gold
standard corpora, and evaluate predominant sense
accuracy (section 4). We discuss the results with
a proposal for applying the method to an all-words
task, and an analysis of our results in terms of this
proposal before concluding with future directions.
2 Finding Predominant Senses
We use the method described in McCarthy et al
(2004) for finding predominant senses from raw
text. The method uses a thesaurus obtained from
the text by parsing, extracting grammatical relations
and then listing each word (  ) with its top  nearest
neighbours, where  is a constant. Like McCarthy
2This resource will be made publicly available for research
purposes in the near future.
et al (2004) we use 	 and obtain our thesaurus
using the distributional similarity metric described
by Lin (1998). We use WordNet (WN) as our sense
inventory. The senses of a word  are each assigned
a ranking score which sums over the distributional
similarity scores of the neighbours and weights each
neighbour?s score by a WN Similarity score (Pat-
wardhan and Pedersen, 2003) between the sense of
 and the sense of the neighbour that maximises the
WN Similarity score. This weight is normalised by
the sum of such WN similarity scores between all
senses of  and and the senses of the neighbour that
maximises this score. We use the WN Similarity jcn
score (Jiang and Conrath, 1997) since this gave rea-
sonable results for McCarthy et al and it is efficient
at run time given precompilation of frequency infor-
mation. The jcn measure needs word frequency in-
formation, which we obtained from the British Na-
tional Corpus (BNC) (Leech, 1992). The distribu-
tional thesaurus was constructed using subject, di-
rect object adjective modifier and noun modifier re-
lations.
3 Creating the Three Gold Standards
In our experiments, we compare for a sample
of nouns the sense rankings created from a bal-
anced corpus (the BNC) with rankings created from
domain-specific corpora (FINANCE and SPORTS)
extracted from the Reuters corpus (Rose et al,
2002). In more detail, the three corpora are:
BNC: The ?written? documents, amounting to 3209
documents (around 89.7M words), and covering a
wide range of topic domains.
FINANCE: 117734 FINANCE documents (around
32.5M words) topic codes: ECAT and MCAT
SPORTS: 35317 SPORTS documents (around 9.1M
words) topic code: GSPO
We computed thesauruses for each of these corpora
using the procedure outlined in section 2.
3.1 Word Selection
In our experiments we used FINANCE and SPORTS
domains. To ensure that a significant number of
the chosen words are relevant for these domains,
we did not choose the words for our experiments
completely randomly. The first selection criterion
we applied used the Subject Field Code (SFC) re-
420
source (Magnini and Cavaglia`, 2000), which assigns
domain labels to synsets in WN version 1.6. We se-
lected all the polysemous nouns in WN 1.6 that have
at least one synset labelled SPORT and one synset
labelled FINANCE. This reduced the set of words
to 38. However, some of these words were fairly
obscure, did not occur frequently enough in one of
the domain corpora or were simply too polysemous.
We narrowed down the set of words using the crite-
ria: (1) frequency in the BNC 
 1000, (2) at most
12 senses, and (3) at least 75 examples in each cor-
pus. Finally a couple of words were removed be-
cause the domain-specific sense was particularly ob-
scure3. The resulting set consists of 17 words4: club,
manager, record, right, bill, check, competition, con-
version, crew, delivery, division, fishing, reserve, re-
turn, score, receiver, running
We refer to this set of words as F&S cds. The first
four words occur in the BNC with high frequency ( 

10000 occurrences), the last two with low frequency
(  2000) and the rest are mid-frequency.
Three further sets of words were selected on the
basis of domain salience. We chose eight words that
are particularly salient in the Sport corpus (referred
to as S sal), eight in the Finance corpus (F sal), and
seven that had equal (not necessarily high) salience
in both, (eq sal). We computed salience as a ratio of
normalised document frequencies, using the formula

	Gloss-Based Semantic Similarity Metrics for Predominant Sense Acquisition
Ryu Iida
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
ryu-i@is.naist.jp
Diana McCarthy and Rob Koeling
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
{dianam,robk}@sussex.ac.uk
Abstract
In recent years there have been various ap-
proaches aimed at automatic acquisition of
predominant senses of words. This infor-
mation can be exploited as a powerful back-
off strategy for word sense disambiguation
given the zipfian distribution of word senses.
Approaches which do not require manually
sense-tagged data have been proposed for
English exploiting lexical resources avail-
able, notably WordNet. In these approaches
distributional similarity is coupled with a se-
mantic similarity measure which ties the dis-
tributionally related words to the sense in-
ventory. The semantic similarity measures
that have been used have all taken advantage
of the hierarchical information in WordNet.
We investigate the applicability to Japanese
and demonstrate the feasibility of a mea-
sure which uses only information in the dic-
tionary definitions, in contrast with previ-
ous work on English which uses hierarchi-
cal information in addition to dictionary def-
initions. We extend the definition based
semantic similarity measure with distribu-
tional similarity applied to the words in dif-
ferent definitions. This increases the recall
of our method and in some cases, precision
as well.
1 Introduction
Word sense disambiguation (WSD) has been an ac-
tive area of research over the last decade because
many researches believe it will be important for
applications which require, or would benefit from,
some degree of semantic interpretation. There has
been considerable skepticism over whether WSD
will actually improve performance of applications,
but we are now starting to see improvement in per-
formance due to WSD in cross-lingual information
retrieval (Clough and Stevenson, 2004; Vossen et
al., 2006) and machine translation (Carpuat and Wu,
2007; Chan et al, 2007) and we hope that other ap-
plications such as question-answering, text simplifi-
cation and summarisation might also benefit as WSD
methods improve.
In addition to contextual evidence, most WSD sys-
tems exploit information on the most likely mean-
ing of a word regardless of context. This is a pow-
erful back-off strategy given the skewed nature of
word sense distributions. For example, in the En-
glish coarse grained all words task (Navigli et al,
2007) at the recent SemEval Workshop the base-
line of choosing the most frequent sense using the
first WordNet sense attained precision and recall of
78.9%which is only a few percent lower than the top
scoring system which obtained 82.5%. This finding
is in line with previous results (Snyder and Palmer,
2004). Systems using a first sense heuristic have
relied on sense-tagged data or lexicographer judg-
ment as to which is the predominant sense of a word.
However sense-tagged data is expensive and further-
more the predominant sense of a word will vary de-
pending on the domain (Koeling et al, 2005; Chan
and Ng, 2007).
One direction of research following McCarthy et
al. (2004) has been to learn the most predominant
561
sense of a word automatically. McCarthy et als
method relies on two methods of similarity. Firstly,
distributional similarity is used to estimate the pre-
dominance of a sense from the number of distribu-
tionally similar words and the strength of their dis-
tributional similarity to the target word. This is done
on the premise that more prevalent meanings have
more evidence in the corpus data used for the distri-
butional similarity calculations and the distribution-
ally similar words (nearest neighbours) to a target
reflect the more predominant meanings as a conse-
quence. Secondly, the senses in the sense inventory
are linked to the nearest neighbours using semantic
similarity which incorporates information from the
sense inventory. It is this semantic similarity mea-
sure which is the focus of our paper in the context of
the method for acquiring predominant senses.
Whilst the McCarthy et al?s method works well
for English, other inventories do not always have
WordNet style resources to tie the nearest neigh-
bours to the sense inventory. WordNet has many se-
mantic relations as well as glosses associated with
its synsets (near synonym sets). While traditional
dictionaries do not organise senses into synsets, they
do typically have sense definitions associated with
the senses. McCarthy et al (2004) suggest that dic-
tionary definitions can be used with their method,
however in the implementation of the measure based
on dictionary definitions that they use, the dictionary
definitions are extended to those of related words us-
ing the hierarchical structure of WordNet (Banerjee
and Pedersen, 2002). This extension to the original
method (Lesk, 1986) was proposed because there is
not always sufficient overlap of the individual words
for which semantic similarity is being computed. In
this paper we refer to the original method (Lesk,
1986) as lesk and the extended measure proposed
by Banerjee and Pedersen as Elesk.
This paper investigates the potential of using
the overlap of dictionary definitions with the Mc-
Carthy et al?s method. We test the method for
obtaining a first sense heuristic using two publicly
available datasets of sense-tagged data in Japanese,
EDR (NICT, 2002) and the SENSEVAL-2 Japanese
dictionary task (Shirai, 2001). We contrast an imple-
mentation of lesk (Lesk, 1986) which uses only dic-
tionary definitions with the Jiang-Conrath measure
(jcn) (Jiang and Conrath, 1997) which uses man-
ually produced hyponym links and was used pre-
viously for this purpose on English datasets (Mc-
Carthy et al, 2004). The jcn measure is only ap-
plicable to the EDR dataset because the dictionary
has hyponymy links which are not available in the
SENSEVAL-2 Japanese dictionary task. We also pro-
pose a new extension to lesk which does not require
hand-crafted hyponym links but instead uses distri-
butional similarity to increase the possibilities for
overlap of the word definitions. We refer to this new
measure as DSlesk. We compare this to the original
lesk on both datasets and show that it increases re-
call, and sometimes precision too whilst not requir-
ing hyponym links.
In the next section we place our contribution in re-
lation to previous work. In section 3 we summarise
the methods we adopt from previous work, and de-
scribe our proposal for a semantic similarity method
that can supplement the information from dictionary
definitions with information from raw text. In sec-
tion 4 we describe the experiments on EDR and the
SENSEVAL-2 Japanese dictionary task and we con-
clude in section 5.
2 Related Work
This work builds upon that of McCarthy et al (2004)
which acquires predominant senses for target words
from a large sample of text using distributional sim-
ilarity (Lin, 1998) to provide evidence for predomi-
nance. The evidence from the distributional similar-
ity is allocated to the senses using semantic similar-
ity fromWordNet (Patwardhan and Pedersen, 2003).
We will describe the method more fully below in
section 3. McCarthy et al (2004) reported results
for English using their automatically acquired first
sense heuristic on SemCor (Miller et al, 1993) and
the SENSEVAL-2 English all words dataset (Sny-
der and Palmer, 2004). The results from this are
promising, given that hand-labelled data is not re-
quired. On polysemous nouns from SemCor they
obtained 48% WSD using their method with Elesk
and 46% with jcn where the random baseline was
24% and the upper-bound was 67% (derived from
the SemCor test data itself). On SENSEVAL-2 all
words dataset using the jcn measure 1 they obtained
63% recall which is encouraging compared to the
1They did not apply lesk to this dataset.
562
SemCor heuristic which obtained 68% but requires
hand-labelled data. The upper-bound on the dataset
was 72% from the test data itself. These results cru-
cially depend on the information in the sense inven-
tory WordNet. WordNet contains hierarchical rela-
tions between word senses which are used in both
jcn and Elesk. There is an issue that such infor-
mation may not be available in other sense invento-
ries, and other inventories will be needed for other
languages. In this paper, we implement the lesk se-
mantic similarity (Lesk, 1986) for the two Japanese
lexicons used in our test datasets, i) the EDR dic-
tionary (NICT, 2002) ii) the Iwanami Kokugo Jiten
Dictionary (Nishio et al, 1994). We investigate the
potential of lesk and jcn, where the latter is applica-
ble. In addition to implementing the original lesk
measure, we propose an extension to the method
inspired by Mihalcea et al (2006). Mihalcea et
al. (2006) used various text based similarity mea-
sures, including WordNet and corpus based similar-
ity methods, to determine if two phrases are para-
phrases. They contrasted this approach with previ-
ous methods which used overlap of the words be-
tween the candidate paraphrases. For each word in
each of the two texts they obtain the maximum sim-
ilarity between the word and any of the words from
the putative paraphrase. The similarity scores for
each word of both phrases contribute to an overall
semantic similarity between 0 and 1 and a threshold
of 0.5 is used to decide if the candidate phrases are
paraphrases. In our work, we compare glosses of
words senses (senses of the target word and senses
of the nearest neighbour) rather than paraphrases. In
this approach we extend the definition overlap by
considering the distributional similarity (Lin, 1998)
rather than identify of the words in the two defini-
tions.
In addition to McCarthy et al (2004) there are
other approaches to finding predominant senses.
Chan and Ng (2005) use parallel data to provide
estimates for sense frequency distributions to feed
into a supervised WSD system. Mohammad and
Hirst (2006) propose an approach to acquiring pre-
dominant senses from corpora which makes use
of the category information in the Macquarie The-
saurus (Barnard, 1986). Lexical chains (Galley and
McKeown, 2003) may also provide a useful first
sense heuristic (Brody et al, 2006) but are produced
usingWordNet relations. We use theMcCarthy et al
approach because this is applicable without aligned
corpus data, semantic category and relation informa-
tion and is applicable to any language assuming the
minimum requirements of i) dictionary definitions
associated with the sense inventory and ii) raw cor-
pus data. We adapt their technique to remove the
reliance on hyponym links.
3 Gloss-based semantic similarity
We first summarise the McCarthy et al method
and the WordNet based semantic similarity func-
tions (jcn and Elesk) that they use for automatic
acquisition of a first sense heuristic applied to dis-
ambiguation of English WordNet datasets. We then
describe the additional semantic similarity method
that we propose for comparison with lesk and jcn.
McCarthy et al use a distributional similarity the-
saurus acquired from corpus data using the method
of Lin (1998) for finding the predominant sense of
a word where the senses are defined by WordNet.
The thesaurus provides the k nearest neighbours to
each target word, along with the distributional sim-
ilarity score between the target word and its neigh-
bour. The WordNet similarity package (Patwardhan
and Pedersen, 2003) is used to weight the contribu-
tion that each neighbour makes to the various senses
of the target word.
Let w be a target word and Nw = {n1,n2...nk}
be the ordered set of the top scoring k
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w,n1),dss(w,n2), ...dss(w,nk)} using (Lin,
1998). Let senses(w) be the set of senses of w
for each sense of w (wsi ? senses(w)) a ranking is
obtained using:
Prevalence Score(wsi) =
?
n j?Nw
dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
(1)
where wnss is the maximum WordNet similarity
score between wsi and the WordNet sense of the
neighbour (n j) that maximises this score. McCarthy
et al compare two different WordNet similarity
scores, jcn and Elesk.
jcn (Jiang and Conrath, 1997) uses corpus data
to estimate a frequency distribution over the classes
563
(synsets) in the WordNet hierarchy. Each synset, is
incremented with the frequency counts from the cor-
pus of all words belonging to that synset, directly or
via the hyponymy relation. The frequency data is
used to calculate the ?information content? (IC) of a
class or sense (s):
IC(s) =?log(p(s))
Jiang and Conrath specify a distance measure be-
tween two senses (s1,s2):
D jcn(s1,s2) = IC(s1)+ IC(s2)?2? IC(s3)
where the third class (s3) is the most informative, or
most specific, superordinate synset of the two senses
s1 and s2. This is transformed from a distance mea-
sure in the WordNet Similarity package by taking
the reciprocal:
jcn(s1,s2) = 1/D jcn(s1,s2)
McCarthy et al use the above measure with wsi
as s1 and whichever sense of the neigbour (n j) that
maximises this WordNet similarity score.
Elesk (Banerjee and Pedersen, 2002) extends the
original lesk algorithm (Lesk, 1986) so we describe
that original algorithm lesk first. This simply cal-
culates the overlap of the content words in the defi-
nitions, frequently referred to as glosses, of the two
word senses.
lesk(s1,s2) =
?
a?g1
member(a,g2)
member(a,g2) =
{
1 if a appears in g2
0 otherwise
where g1 is the gloss of word sense s1, g2 is the gloss
of s2 and a is one of words appearing in g1. In Elesk
which McCarthy et al use the measure is extended
by considering related synsets to s1 and s2, again
where s1 is wsi and s2 is the sense from all senses
of n j that maximises the Elesk WordNet similar-
ity score. Elesk relies heavily on the relationships
that are encoded in WordNet such as hyponymy and
meronymy. Not all languages have resources sup-
plied with these relations, and where they are sup-
plied there may not be as much detail as there is in
WordNet.
In this paper we will examine the use of jcn and
the original lesk in Japanese on the EDR dataset
to see how well the pure definition based measure
fares compared to one using hyponym links. EDR
has hyponym links so we can make this comparison.
The performance of jcn will depend on the coverage
of the hyponym links. For lesk meanwhile there is
an issue that using only overlap of sense definitions
may give poor results because the sense definitions
are usually succinct and the overlap of words may
be low. For example, given the glosses for the words
pigeon and bird:2
pigeon: a fat grey and white bird with
short legs.
bird: a creature that is covered with feath-
ers and has wings and two legs.
If only content words are considered then there
is only one word (leg) which overlaps in the two
glosses, so the resultant lesk score is low (1) even
though the word pigeon is intuitively similar to bird.
The Elesk extension addressed this issue using
WordNet relations to extend the definitions over
which the overlap is calculated for a given pair of
senses. We propose addressing the same issue us-
ing corpus data to supplement the lesk overlap mea-
sure. We propose using distributional similarity (us-
ing (Lin, 1998)) as an approximation of semantic
distance between the words in the two glosses, rather
than requiring an exact match. We refer to this mea-
sure as DSlesk as defined:
DSlesk(s1,s2) = 1
|a ? g1| ?a?g1
max
b?g2
dss(a,b) (2)
where g1 is the gloss of word sense s1, g2 is the gloss
of s2, again s1 is the target word sense wsi in equa-
tion 1 for which we are obtaining the predominance
ranking score and s2 is whichever sense of the neigh-
bour (n j) in equation 1 which maximises this seman-
tic similarity score, as McCarthy et al did with the
wnss in equation 1. a (b) is a word appearing in g1
(g2).
In the calculation of equation (2), we first extract
the most similar word b from g2 to each word (a) in
2These two glosses are defined in OXFORD Advanced
Learner?s Dictionary.
564
dss(bird,creature) = 0.84, dss(bird, f eather) = 0.77,
dss(bird,wing) = 0.55, dss(bird, leg) = 0.43,
dss(leg,creature) = 0.56, dss(leg, f eather) = 0.66,
dss(leg,wing) = 0.74, dss(leg, leg) = 1.00
Figure 1: Examples of distributional similarity
the gloss of s1. We then output the average of the
maximum distributional similarity of all the words
in g1 to any of the words in g2 as the similarity score
between s1 and s2. We acknowledge that DSlesk is
not symmetrical since it depends on the number of
words in the gloss of s1, but not s2. Also our sum-
mation is over these words in s1 and we are not look-
ing for identity but maximum distributional similar-
ity with any of the words in g2 so the summation
will not give the same result as if we did the sum-
mation over the words in g2. It is perfectly reason-
able to have a semantic similarity measure which is
not symmetrical. One may want a measure where
a more specific sense, such as the meat sense of
chicken is closer to the ?animal flesh used as food?
sense of meat than vice versa. We do not believe
that this asymmetry is problematic for our applica-
tion as all the senses of w which we are ranking are
all treated equally with respect to the neighbour n,
and the ranking measure is concerned with finding
evidence for the meaning of w, which we do by fo-
cusing on its definitions, and not the meaning of n.
It would however be worthwhile investigating sym-
metrical versions of the score in the future.
Here is an example given the definitions of bird
and pigeon above and the distributional similarity
scores of all combinations of the two nouns as shown
in Figure 1. In this case, the similarity is estimated
as 1/2(0.84+1.00) = 0.92.
4 Experiments
To investigate how well the McCarthy et al method
ports to other language, we conduct empirical eval-
uation of word sense disambiguation by using the
two available sense-tagged datasets, EDR and the
SENSEVAL-2 Japanese dictionary task. In the ex-
periments, we compare the three semantic similari-
ties, jcn, lesk and DSlesk3, for use in the method to
3Elesk can be used when several semantic relations such as
hypnoymy and meronomy are available. However, we cannot
directly apply Elesk as it was used in (McCarthy et al, 2004) to
find the most likely sense in the set of word senses
defined in each inventory following the approach
of McCarthy et al (2004). For the thesaurus con-
struction we used <verb, case, noun> triplets ex-
tracted from Japanese newspaper articles (9 years of
the Mainichi Shinbun (1991-1999) and 10 years of
the Nihon Keizai Shinbun (1991-2000)) and parsed
by CaboCha (Kudo and Matsumoto, 2002). This re-
sulted in 53 million triplet instances for acquiring
the distributional thesaurus. We adopt the similarity
score proposed by Lin (1998) as the distributional
similarity score and use 50 nearest neighbours in
line with McCarthy et al
For the random baseline we select one word sense
at random for each word token and average the pre-
cision over 100 trials. For contrast with a supervised
approach we show the performance if we use hand-
labelled training data for obtaining the predominant
sense of the test words. This method usually outper-
forms an automatic approach, but crucially relies on
there being hand-labelled data which is expensive to
produce. The method cannot be applied where there
is no hand-labelled training data, it will be unreli-
able for low frequency data and a general dataset
may not be applicable when one moves to domain
specific text (Koeling et al, 2005). Since we are
not using context for disambiguation, but just a first
sense heuristic, we also give the upper-bound which
is the first sense heuristic calculated from the test
data itself.
4.1 EDR
We conduct empirical evaluation using 3,836 poly-
semous nouns in the sense-tagged corpus provided
with EDR (183,502 instances) where the glosses are
defined in the EDR dictionary. We evaluated on this
dataset using WSD precision and recall of this corpus
using only our first-sense heuristic (no context). The
results are shown in Table 1. The WSD performance
of all the automatic methods is much lower than the
supervised method, however, the main point of this
paper is to compare the McCarthy et al method for
finding a first sense in Japanese using jcn, lesk and
our experiments because the meronomy relation is not defined
in the EDR dictionary. In the experiments reported here we fo-
cus on the comparison of the three similarity measures jcn, lesk
and DSlesk for use in the method to determine the predomi-
nant sense of each word. We leave further exploration of other
adaptations of semantic similarity scores for future work.
565
Table 1: Results of EDR
recall precision
baseline 0.402 0.402
jcn 0.495 0.495
lesk 0.474 0.488
DSlesk 0.495 0.495
upper-bound 0.745 0.745
supervised 0.731 0.731
Table 2: Precision on EDR at low frequencies
all freq ? 10 freq ? 5
baseline 0.402 0.405 0.402
jcn 0.495 0.445 0.431
lesk 0.474 0.448 0.426
DSlesk 0.495 0.453 0.433
upper-bound 0.745 0.674 0.639
supervised 0.731 0.519 0.367
DSlesk. Table 1 shows that DSlesk is comparable to
jcn without the requirement for semantic relations
such as hyponymy.
Furthermore, we evaluate precision of each
method at low frequencies of words (? 10, ? 5),
shown in Table 2. Table 2 shows that all methods for
finding a predominant sense outperform the super-
vised one for items with little data (? 5), indicating
that these methods robustly work even for low fre-
quency data where hand-tagged data is unreliable.
Whilst the results are significantly different to the
baseline 4 we note that the difference to the random
baseline is less than for McCarthy et al who ob-
tained 48% for Elesk on polysemous nouns in Sem-
Cor and 46% for jcn against a random baseline of
24%. These differences are probably explained by
differences in the lexical resources. Both Elesk and
jcn rely on semantic relations including hyponymy
with Elesk also using the glosses. jcn in both ap-
proaches use the hyponym links. WordNet 1.6 (used
by McCarthy et al) has 66025 synsets with 66910
hyponym links between these 5. For EDR there are
166868 nodes (word sense groupings) and 53747
4For significance testing we used McNemar?s test ? = 0.05.
5These figures are taken from
http://www.lsi.upc.es/?batalla/wnstats.html#wn16
Table 3: Results of SENSEVAL-2
precision = recall
fine coarse
baseline 0.282 0.399
lesk 0.344 0.501
DSlesk 0.386 0.593
upper-bound 0.747 0.834
supervised 0.742 0.842
hyponym links. So in EDR the ratio of these links
to the nodes is much lower. This and other differ-
ences between EDR and WordNet are likely to be
the reason for the difference in results.
4.2 SENSEVAL-2
We also evaluate the performance using the Japanese
dictionary task in SENSEVAL-2 (Shirai, 2001). In
this experiment, we use 50 nouns (5,000 instances).
For this task, since semantic relations such as hy-
ponym links are not defined, use of jcn is not pos-
sible. Therefore, we just compare lesk and DSlesk
along with our random baseline, the supervised ap-
proach and the upper-bound as before.
The results are evaluated in two ways; one is for
fine-grained senses in the original task definition and
the other is coarse-grained version which is evalu-
ated discarding the finer categorical information of
each definition. The results are shown in Table 3. As
with the EDR results, all unsupervised methods sig-
nificantly outperform the baseline method, though
the supervised methods still outperform the unsu-
pervised ones. In this experiment, DSlesk is also
significantly better than lesk in both fine and coarse-
grained evaluations. It indicates that applying dis-
tributional similarity score to calculating inter-gloss
similarities improves performance.
5 Conclusion
In this paper, we examined different measures of se-
mantic similarity for finding a first sense heuristic
for WSD automatically in Japanese. We defined a
new gloss-based similarity (DSlesk) and evaluated
the performance on two Japanese WSD datasets, out-
performing lesk and achieving a performance com-
parable to the jcn method which relies on hyponym
links which are not always available.
566
There are several issues for future directions of
automatic detection of a first sense heuristic. In this
paper, we proposed an adaptation of the lesk mea-
sure of gloss-based similarity, by using the aver-
age similarity between nouns in the two glosses un-
der comparison in a bag-of-words approach without
recourse to other information. However, it would
be worthwhile exploring other information in the
glosses, such as words of other PoS and predicate
argument relations. We also hope to investigate ap-
plying alignment techniques introduced for entail-
ment recognition (Hickl and Bensley, 2007).
Another important issue in WSD is to group fine-
grained word senses into clusters, making the task
suitable for NLP applications (Ide and Wilks, 2006).
We believe that our gloss-based similarity DSlesk
might be very suitable for this task and we plan to
investigate the possibility.
There are other approaches we would like to ex-
plore in future. Mihalcea (2005) uses dictionary def-
initions alongside graphical algorithms for unsuper-
vised WSD. Whilst the results are not directly com-
parable to ours because we have not included con-
textual evidence in our models, it would be worth-
while exploring if unsupervised graphical models
using only the definitions we have in our lexical re-
sources can perform WSD on a document and give
more reliable first sense heuristics.
Acknowledgements
This work was supported by the UK EPSRC project
EP/C537262 ?Ranking Word Senses for Disam-
biguation: Models and Applications?, and a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would like to thank John Carroll
for several useful discussions on this work.
References
Satanjeev Banerjee and Ted Pedersen. 2002. An adapted
Lesk algorithm for word sense disambiguation using
WordNet. In Proceedings of the Third International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing-02), Mexico City.
J.R.L. Barnard, editor. 1986. Macquaire Thesaurus.
Macquaire Library, Sydney.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), Edinburgh, Scot-
land.
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense disam-
biguation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, Prague, Czech Republic, June. Association for
Computational Linguistics.
Paul Clough and Mark Stevenson. 2004. Evaluating the
contribution of EuroWordNet and word sense disam-
biguation to cross-language retrieval. In Second In-
ternational Global WordNet Conference (GWC-2004),
pages 97?105.
Michel Galley and Kathleen McKeown. 2003. Improv-
ing word sense disambiguation in lexical chaining. In
IJCAI-03, Proceedings of the Eighteenth International
Joint Conference on Artificial Intelligence, Acapulco,
Mexico, August 9-15, 2003, pages 1486?1488. Morgan
Kaufmann.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, pages
171?176.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
567
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the joint confer-
ence on Human Language Technology and Empirical
methods in Natural Language Processing, pages 419?
426, Vancouver, B.C., Canada.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning 2002 (CoNLL), pages 63?69.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from and ice cream cone. In Proceedings of the ACM
SIGDOC Conference, pages 24?26, Toronto, Canada.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,
Montreal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence (AAAI
2006), Boston, MA, July.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the joint conference on Human Language Technology
and Empirical methods in Natural Language Process-
ing, Vancouver, B.C., Canada.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?.308. Morgan Kaufman.
Saif Mohammad and Graeme Hirst. 2006. Determining
word sense dominance using a thesaurus. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), pages 121?128, Trento, Italy, April.
Roberto Navigli, C. Litkowski, Kenneth, and Orin Har-
graves. 2007. SemEval-2007 task 7: Coarse-
grained English all-words task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 30?35, Prague,
Czech Republic.
NICT. 2002. EDR electronic dic-
tionary version 2.0, technical guide.
http://www2.nict.go.jp/kk/e416/EDR/.
Minoru Nishio, Etsutaro Iwabuchi, and ShizuoMitzutani.
1994. Iwanami kokugo jiten dai go han.
Siddharth Patwardhan and Ted Pedersen. 2003.
The CPAN WordNet::Similarity Package.
http://search.cpan.org/author/SID/WordNet-
Similarity-0.03/.
Kiyoaki Shirai. 2001. SENSEVAL-2 Japanese Dictio-
nary Task. In Proceedings of the SENSEVAL-2 work-
shop, pages 33?36.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the ACL SENSEVAL-
3 workshop, pages 41?43, Barcelona, Spain.
Piek Vossen, German Rigau, Inaki Alegria, Eneko Agirre,
David Farwell, and Manuel Fuentes. 2006. Mean-
ingful results for information retrieval in the meaning
project. In Proceedings of the 3rd Global WordNet
Conference. http://nlpweb.kaist.ac.kr/gwc/.
568
c? 2003 Association for Computational Linguistics
Disambiguating Nouns, Verbs, and
Adjectives Using Automatically Acquired
Selectional Preferences
Diana McCarthy? John Carroll?
University of Sussex University of Sussex
Selectional preferences have been used by word sense disambiguation (WSD) systems as one
source of disambiguating information. We evaluate WSD using selectional preferences acquired
for English adjective?noun, subject, and direct object grammatical relationships with respect to
a standard test corpus. The selectional preferences are specific to verb or adjective classes, rather
than individual word forms, so they can be used to disambiguate the co-occurring adjectives and
verbs, rather than just the nominal argument heads. We also investigate use of the one-sense-
per-discourse heuristic to propagate a sense tag for a word to other occurrences of the same word
within the current document in order to increase coverage. Although the preferences perform
well in comparison with other unsupervised WSD systems on the same corpus, the results show
that for many applications, further knowledge sources would be required to achieve an adequate
level of accuracy and coverage. In addition to quantifying performance, we analyze the results to
investigate the situations in which the selectional preferences achieve the best precision and in
which the one-sense-per-discourse heuristic increases performance.
1. Introduction
Although selectional preferences are a possible knowledge source in an automatic
word sense disambiguation (WDS) system, they are not a panacea. One problem is
coverage: Most previous work has focused on acquiring selectional preferences for
verbs and applying them to disambiguate nouns occurring at subject and direct ob-
ject slots (Ribas 1995; McCarthy 1997; Abney and Light 1999; Ciaramita and Johnson
2000; Stevenson and Wilks 2001). In normal running text, however, a large proportion
of word tokens do not fall at these slots. There has been some work looking at other
slots (Resnik 1997), and on using nominal arguments as disambiguators for verbs (Fed-
erici, Montemagni, and Pirrelli 1999; Agirre and Martinez 2001), but the problem of
coverage remains. Selectional preferences can be used for WSD in combination with
other knowledge sources (Stevenson and Wilks 2001), but there is a need to ascer-
tain when they work well so that they can be utilized to their full advantage. This
article is aimed at quantifying the disambiguation performance of automatically ac-
quired selectional preferences in regard to nouns, verbs, and adjectives with respect to
a standard test corpus and evaluation setup (SENSEVAL-2) and to identify strengths
and weaknesses. Although there is clearly a limit to coverage using preferences alone,
because preferences are acquired only with respect to specific grammatical roles, we
show that when dealing with running text, rather than isolated examples, coverage can
be increased at little cost in accuracy by using the one-sense-per-discourse heuristic.
? Department of Informatics, University of Sussex, Brighton BN1 9QH, UK. E-mail: {dianam,
johnca}@sussex.ac.uk
640
Computational Linguistics Volume 29, Number 4
We acquire selectional preferences as probability distributions over the Word-
Net (Fellbaum 1998) noun hyponym hierarchy. The probability distributions are con-
ditioned on a verb or adjective class and a grammatical relationship. A noun is disam-
biguated by using the preferences to give probability estimates for each of its senses
in WordNet, that is, for WordNet synsets. Verbs and adjectives are disambiguated by
using the probability distributions and Bayes? rule to obtain an estimate of the proba-
bility of the adjective or verb class, given the noun and the grammatical relationship.
Previously, we evaluated noun and verb disambiguation on the English all-words task
in the SENSEVAL-2 exercise (Cotton et al 2001). We now present results also using
preferences for adjectives, again evaluated on the SENSEVAL-2 test corpus (but carried
out after the formal evaluation deadline). The results are encouraging, given that this
method does not rely for training on any hand-tagged data or frequency distributions
derived from such data. Although a modest amount of English sense-tagged data is
available, we nevertheless believe it is important to investigate methods that do not
require such data, because there will be languages or texts for which sense-tagged
data for a given word is not available or relevant.
2. Motivation
The goal of this article is to assess the WSD performance of selectional preference
models for adjectives, verbs, and nouns on the SENSEVAL-2 test corpus. There are two
applications for WSD that we have in mind and are directing our research. The first
application is text simplification, as outlined by Carroll, Minnen, Pearce et al (1999).
One subtask in this application involves substituting words with thier more frequent
synonyms, for example, substituting letter for missive. Our motivation for using WSD
is to filter out inappropriate senses of a word token, so that the substituting synonym
is appropriate given the context. For example, in the following sentence we would
like to use strategy, rather than dodge, as a substitute for scheme:
A recent government study singled out the scheme as an example to others.
We are also investigating the disambiguation of verb senses in running text before
subcategorization information for the verbs is acquired, in order to produce a sub-
categorization lexicon specific to sense (Preiss and Korhonen 2002). For example, if
subcategorization were acquired specific to sense, rather than verb form, then distinct
senses of fire could have different subcategorization entries:
fire(1) - sack: NP V NP
fire(2) - shoot: NP V NP, NP V
Selectional preferences could also then be acquired automatically from sense-tagged
data in an iterative approach (McCarthy 2001).
3. Methodology
We acquire selectional preferences from automatically preprocessed and parsed text
during a training phase. The parser is applied to the test data as well in the run-
time phase to identify grammatical relations among nouns, verbs, and adjectives. The
acquired selectional preferences are then applied to the noun-verb and noun-adjective
pairs in these grammatical constructions for disambiguation.
641
McCarthy and Carroll Disambiguating Using Selectional Preferences
0.0005 0.09
<time>
0.080.7
<entity> <measure>
drink suck sipmodel for verbclass
Sense Tagged output
<attribute>
Disambiguator
ParserPreprocessor
Acquisition
Preference
Selectional
a single glass with...
where you can drink
Training data
d04.s12.t09    drink%2:34:00::
d06.s13.t03    man%1:18:00::
instance ID   WordNet sense tag
ncmod  glass_NN1 single_JJ
dobj   drink_VV0    glass_NN1
ncsubj drink_VV0   you_PPY
Grammatical Relations
Test data
The men drink here ...
WordNet
Selectional Preferences
Figure 1
System overview. Solid lines indicate flow of data during training, and broken lines show that
at run time.
The overall structure of the system is illustrated in Figure 1. We describe the
individual components in sections 3.1?3.3 and 4.
3.1 Preprocessing
The preprocessor consists of three modules applied in sequence: a tokenizer, a part-
of-speech (POS) tagger, and a lemmatizer.
The tokenizer comprises a small set of manually developed finite-state rules for
identifying word and sentence boundaries. The tagger (Elworthy 1994) uses a bigram
hidden Markov model augmented with a statistical unknown word guesser. When
applied to the training data for selectional preference acquisition, it produces the sin-
gle highest-ranked POS tag for each word. In the run-time phase, it returns multiple
tag hypotheses, each with an associated forward-backward probability to reduce the
impact of tagging errors. The lemmatizer (Minnen, Carroll, and Pearce 2001) reduces
inflected verbs and nouns to their base forms. It uses a set of finite-state rules express-
ing morphological regularities and subregularities, together with a list of exceptions
for specific (irregular) word forms.
3.2 Parsing
The parser uses a wide-coverage unification-based shallow grammar of English POS
tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using
a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from
extra-grammaticality by returning partial parses. The output of the parser is a set of
grammatical relations (Carroll, Briscoe, and Sanfilippo 1998) specifying the syntactic
dependency between each head and its dependent(s), taken from the phrase structure
tree that is returned from the disambiguation phase.
For selectional preference acquisition we applied the analysis system to the 90
million words of the written portion of the British National Corpus (BNC); the parser
produced complete analyses for around 60% of the sentences and partial analyses
for over 95% of the remainder. Both in the acquisition phase and at run time, we
extract from the analyser output subject?verb, verb?direct object, and noun?adjective
642
Computational Linguistics Volume 29, Number 4
modifier dependencies.1 We did not use the SENSEVAL-2 Penn Treebank?style brack-
etings supplied for the test data.
3.3 Selectional Preference Acquisition
The preferences are acquired for grammatical relations (subject, direct object, and
adjective?noun) involving nouns and grammatically related adjectives or verbs. We
use WordNet synsets to define our sense inventory. Our method exploits the hyponym
links given for nouns (e.g., cheese is a hyponym of food), the troponym links for verbs 2
(e.g., limp is a troponym of walk), and the ?similar-to? relationship given for adjectives
(e.g., one sense of cheap is similar to flimsy).
The preference models are modifications of the tree cut models (TCMs) originally
proposed by Li and Abe (1995, 1998). The main differences between that work and
ours are that we acquire adjective as well as verb models, and also that our models
are with respect to verb and adjective classes, rather than forms. We acquire models
for classes because we are using the models for WSD, whereas Li and Abe used them
for structural disambiguation.
We define a TCM as follows. Let NC be the set of noun synsets (noun classes)
in WordNet: NC = {nc ? WordNet}, and NS be the set of noun senses 3 in Wordnet:
NS = {ns ? WordNet}. A TCM is a set of noun classes that partition NS disjointly.
We use ? to refer to such a set of classes in a TCM. A TCM is defined by ? and a
probability distribution:
?
nc??
p(nc) = 1 (8)
The probability distribution is conditioned by the grammatical context. In this
work, the probability distribution associated with a TCM is conditioned on a verb
class (vc) and either the subject or direct-object relation, or an adjective class (ac)
and the adjective?noun relation. Let VC be the set of verb synsets (verb classes) in
WordNet: VC = {vc ? WordNet}. Let AC be the set of adjective classes (which subsume
WordNet synsets; we elaborate further on this subsequently). Thus, the TCMs define
a probability distribution over NS that is conditioned on a verb class (vc) or adjective
class (ac) and a particular grammatical relation (gr):
?
nc??
p(nc|vc, gr) = 1 (9)
Acquisition of a TCM for a given vc and gr proceeds as follows. The data for
acquiring the preference are obtained from a subset of the tuples involving verbs
in the synset or troponym (subordinate) synsets. Not all verbs that are troponyms
or direct members of the synset are used in training. We take the noun argument
heads occurring with verbs that have no more than 10 senses in WordNet and a
frequency of 20 or more occurrences in the BNC data in the specified grammatical
relationship. The threshold of 10 senses removes some highly polysemous verbs having
many sense distinctions that are rather subtle. Verbs that have more than 10 senses
include very frequent verbs such as be and do that do not select strongly for their
1 In a previous evaluation of grammatical-relation accuracy with in-coverage text, the analyzer returned
subject?verb and verb?direct object dependencies with 84?88% recall and precision (Carroll, Minnen,
and Briscoe 1999).
2 In WordNet, verbs are organized by the troponymy relation, but this is represented with the same
hyponym pointer as is used in the noun hierarchy.
3 We refer to nouns attached to synsets as noun senses.
643
McCarthy and Carroll Disambiguating Using Selectional Preferences
0
20
40
60
80
100
0 50 100 150 200
%
 n
ot
 in
 W
or
dN
et
frequency rank
Figure 2
Verbs not in WordNet by BNC frequency.
arguments. The frequency threshold of 20 is intended to remove noisy data. We set
the threshold by examining a plot of BNC frequency and the percentage of verbs at
particular frequencies that are not listed in WordNet (Figure 2). Using 20 as a threshold
for the subject slot results in only 5% verbs that are not found in WordNet, whereas
73% of verbs with fewer than 20 BNC occurrences are not present in WordNet.4
The selectional-preference models for adjective?noun relations are conditioned on
an ac. Each ac comprises a group of adjective WordNet synsets linked by the ?similar-
to? relation. These groups are formed such that they partition all adjective synsets.
Thus AC = {ac ? WordNet adjective synsets linked by similar-to}. For example, Figure 3
shows the adjective classes that include the adjective fundamental and that are formed
in this way.5 For selectional-preference models conditioned on adjective classes, we
use only those adjectives that have 10 synsets or less in WordNet and have 20 or more
occurrences in the BNC.
The set of ncs in ? are selected from all the possibilities in the hyponym hierarchy
according to the minimum description length (MDL) principle (Rissanen 1978) as used
by Li and Abe (1995, 1998). MDL finds the best TCM by considering the cost (in bits)
of describing both the model and the argument head data encoded in the model. The
cost (or description length) for a TCM is calculated according to equation (10). The
number of parameters of the model is given by k, which is the number of ncs in ?
minus one. N is the sample of the argument head data. The cost of describing each
noun argument head (n) is calculated by the log of the probability estimate for that
noun:
description length = model description length + data description length
= k2 ? log |N| +?
?
n?N
log p(n) (10)
4 These threshold values are somewhat arbitrary, but it turns out that our results are not sensitive to the
exact values.
5 For the sake of brevity, not all adjectives are included in this diagram.
644
Computational Linguistics Volume 29, Number 4
basic
root radical
grassroots
underlying fundamental
rudimentary
primal elementary
basal base
of import important
primal key
central fundamental cardinal
crucial essential
valuable useful
historic
chief principal primary main
measurable
weighty grevious grave
significant important
monumental
profound fundamental
epochal
earthshaking
head
portentous prodigious
evidentiary evidential
noteworthy remarkable
large
Figure 3
Adjective classes that include fundamental.
The probability estimate for each n is obtained using the estimates for all the nss
that n has. Let Cn be the set of ncs that include n as a direct member: Cn = {nc ? NC|n ?
nc}. Let nc? be a hypernym of nc on ? (i.e. nc? ? {?|nc ? nc?}) and let nsnc? = {ns ? nc?}
(i.e., the set of nouns senses at and beneath nc? in the hyponym hierarchy). Then the
estimate p(n) is obtained using the estimates for the hypernym classes on ? for all the
Cn that n belongs to:
p(n) =
?
nc?Cn
p(nc?)
|nsnc? |
(11)
The probability at any particular nc? is divided by nsnc? to give the estimate for each
p(ns) under that nc?.
The probability estimates for the {nc ? ?} ( p(nc|vc, gr) or p(nc|ac, gr)) are obtained
from the tuples from the data of nouns co-occurring with verbs (or adjectives) belong-
ing to the conditioning vc (or ac) in the specified grammatical relationship (< n, v, gr >).
The frequency credit for a tuple is divided by |Cn| for any n, and by the number of
synsets of v, Cv (or Ca if the gr is adjective-noun):
freq(nc|vc, gr) =
?
v?vc
?
n?nc
freq(n|v, gr)
|Cn||Cv|
(12)
A hypernym nc? includes the frequency credit attributed to all its hyponyms ({nc ?
nc?}).
freq(nc?|vc, gr) =
?
nc?nc?
freq(nc|vc, gr) (13)
This ensures that the total frequency credit at any ? across the hyponym hierarchy
equals the credit for the conditioning vc. This will be the sum of the frequency credit
for all verbs that are direct members or troponyms of the vc, divided by the number
of other senses of each of these verbs:
freq(vc|gr) =
?
verb?vc
freq(verb|gr)
|Cv|
(14)
645
McCarthy and Carroll Disambiguating Using Selectional Preferences
group event
Root
human_action
TCM for 
TCM for
control monarchy
throne party
possession
strawmoney
handle
0.010.020.14
0.010.02 0.53
0.06
0.050.07
0.26
entity
seize assume usurp
seize clutch
Figure 4
TCMs for the direct-object slot of two verb classes that include the verb seize.
To ensure that the TCM covers all NS in WordNet, we modify Li and Abe?s original
scheme by creating hyponym leaf classes below all WordNet?s internal classes in the
hyponym hierarchy. Each leaf holds the ns previously held at the internal class.
Figure 4 shows portions of two TCMs. The TCMs are similar, as they both contain
the verb seize, but the TCM for the class that includes clutch has a higher probability
for the entity noun class compared to the class that also includes assume and usurp.
This example includes only top-level WordNet classes, although the TCM may use
more specific noun classes.
4. Disambiguation
Nouns, adjectives and verbs are disambiguated by finding the sense (nc, vc, or ac) with
the maximum probability estimate in the given context. The method disambiguates
nouns and verbs to the WordNet synset level and adjectives to a coarse-grained level
of WordNet synsets linked by the similar-to relation, as described previously.
4.1 Disambiguating Nouns
Nouns are disambiguated when they occur as subjects or direct objects and when
modified by adjectives. We obtain a probability estimate for each nc to which the target
noun belongs, using the distribution of the TCM associated with the co-occurring verb
or adjective and the grammatical relationship.
Li and Abe used TCMs for the task of structural disambiguation. To obtain proba-
bility estimates for noun senses occurring at classes beneath hypernyms on the cut, Li
and Abe used the probability estimate at the nc? on the cut divided by the number of
ns descendants, as we do when finding ? during training, so the probability estimate
is shared equally among all nouns in the nc?, as in equation (15).
p(ns ? nsnc?) =
p(nc?)
|nsnc? |
(15)
One problem with doing this is that in cases in which the TCM is quite high in the
hierarchy, for example, at the entity class, the probability of any ns?s occurring under
this nc? on the TCM will be the same and does not allow us to discriminate among
senses beneath this level.
For the WSD task, we compare the probability estimates at each nc ? Cn, so if
a noun belongs to several synsets, we compare the probability estimates, given the
context, of these synsets. We obtain estimates for each nc by using the probability of
the hypernym nc? on ?. Rather than assume that all synsets under a given nc? on ?
646
Computational Linguistics Volume 29, Number 4
have the same likelihood of occurrence, we multiply the probability estimate for the
hypernym nc? by the ratio of the prior frequency of the nc, that is, p(nc|gr), for which
we seek the estimate divided by the prior frequency of the hypernym nc? (p(nc?|gr)):
p(nc ? hyponyms of nc?|vc, gr) = p(nc?|vc, gr)? p(nc|gr)
p(nc?|gr) (16)
These prior estimates are taken from populating the noun hyponym hierarchy with the
prior frequency data for the gr irrespective of the co-occurring verbs. The probability
at the hypernym nc? will necessarily total the probability at all hyponyms, since the
frequency credit of hyponyms is propagated to hypernyms.
Thus, to disambiguate a noun occurring in a given relationship with a given verb,
the nc ? Cn that gives the largest estimate for p(nc|vc, gr) is taken, where the verb class
(vc) is that which maximizes this estimate from Cv. The TCM acquired for each vc of
the verb in the given gr provides an estimate for p(nc?|vc, gr), and the estimate for nc
is obtained as in equation (16).
For example, one target noun was letter, which occurred as the direct object of
sign in our parses of the SENSEVAL-2 data. The TCM that maximized the probability
estimate for p(nc|vc, direct object) is shown in Figure 5. The noun letter is disambiguated
by comparing the probability estimates on the TCM above the five senses of letter mul-
tiplied by the proportion of that probability mass attributed to that synset. Although
entity has a higher probability on the TCM, compared to matter, which is above the
correct sense of letter,6 the ratio of prior probabilities for the synset containing letter7
under entity is 0.001, whereas that for the synset under matter is 0.226. This gives a
probability of 0.009?0.226 = 0.002 for the noun class probability given the verb class
approval
root
entity
human
capitalist
interpretation
message
communication
abstraction
relation
human_act
symbol
0.16
0.012
0.0329
0.114
0.0060.009
draft
letter
matter
writing
explanation
letter
alphabetic_letter
letter
letter
letter
sign
Figure 5
TCM for the direct-object slot of the verb class including sign and ratify.
6 The gloss is ?a written message addressed to a person or organization; e.g. wrote an indignant letter to
the editor.?
7 The gloss is ?owner who lets another person use something (housing usually) for hire.?
647
McCarthy and Carroll Disambiguating Using Selectional Preferences
(with maximum probability) and grammatical context. This is the highest probability
for any of the synsets of letter, and so in this case the correct sense is selected.
4.2 Disambiguating Verbs and Adjectives
Verbs and adjectives are disambiguated using TCMs to give estimates for p(nc|vc, gr)
and p(nc|ac, gr), respectively. These are combined with prior estimates for p(nc|gr) and
p(vc|gr) (or p(ac|gr)) using Bayes? rule to give:
p(vc|nc, gr) = p(nc|vc, gr)? p(vc|gr)
p(nc|gr) (17)
and for adjective?noun relations:
p(ac|nc, adjnoun) = p(nc|ac, adjnoun)? p(ac|adjnoun)
p(nc|adjnoun) (18)
The prior distributions for p(nc|gr), p(vc|gr) and p(ac|adjnoun) are obtained dur-
ing the training phase. For the prior distribution over NC, the frequency credit of
each noun in the specified gr in the training data is divided by |Cn|. The frequency
credit attached to a hyponym is propagated to the superordinate hypernyms, and the
frequency of a hypernym (nc?) totals the frequency at its hyponyms:
freq(nc?|gr) =
?
nc?nc?
freq(nc|gr) (19)
The distribution over VC is obtained similarly using the troponym relation. For
the distribution over AC, the frequency credit for each adjective is divided by the
number of synsets to which the adjective belongs, and the credit for an ac is the sum
over all the synsets that are members by virtue of the similar-to WordNet link.
To disambiguate a verb occurring with a given noun, the vc from Cv that gives
the largest estimate for p(vc|nc, gr) is taken. The nc for the co-occurring noun is the
nc from Cn that maximizes this estimate. The estimate for p(nc|vc, gr) is taken as in
equation (16) but selecting the vc to maximize the estimate for p(vc|nc, gr) rather than
p(nc|vc, gr). An adjective is likewise disambiguated to the ac from all those to which the
adjective belongs, using the estimate for p(nc|ac, gr) and selecting the nc that maximizes
the p(ac|nc, gr) estimate.
4.3 Increasing Coverage: One Sense per Discourse
There is a significant limitation to the word tokens that can be disambiguated using
selectional preferences, in that they are restricted to those that occur in the specified
grammatical relations and in argument head position. Moreover, we have TCMs only
for adjective and verb classes in which there was at least one adjective or verb mem-
ber that met our criteria for training (having no more than a threshold of 10 senses in
WordNet and a frequency of 20 or more occurrences in the BNC data in the specified
grammatical relationship). We chose not to apply TCMs for disambiguation where we
did not have TCMs for one or more classes for the verb or adjective. To increase cov-
erage, we experimented with applying the one-sense-per-discourse (OSPD) heuristic
(Gale, Church, and Yarowsky 1992). With this heuristic, a sense tag for a given word
is propagated to other occurrences of the same word within the current document in
order to increase coverage. When applying the OSPD heuristic, we simply applied a
tag for a noun, verb, or adjective to all the other instances of the same word type with
the same part of speech in the discourse, provided that only one possible tag for that
word was supplied by the selectional preferences for that discourse.
648
Computational Linguistics Volume 29, Number 4
0
20
40
60
80
100
0 20 40 60 80 100
Re
ca
ll
Precision
sel-ospd-ana sel-ospd
sel
supervised
other unsupervised
first sense heuristic
sel
sel-ospd
sel-ospd-ana
Figure 6
SENSEVAL-2 English all-words task results.
5. Evaluation
We evaluated our system using the SENSEVAL-2 test corpus on the English all-
words task (Cotton et al, 2001). We entered a previous version of this system for
the SENSEVAL-2 exercise, in three variants, under the names ?sussex-sel? (selectional
preferences), ?sussex-sel-ospd? (with the OSPD heuristic), and ?sussex-sel-ospd-ana?
(with anaphora resolution). 8 For SENSEVAL-2 we used only the direct object and
subject slots, since we had not yet dealt with adjectives. In Figure 6 we show how our
system fared at the time of SENSEVAL-2 compared to other unsupervised systems.9
We have also plotted the results of the supervised systems and the precision and recall
achieved by using the most frequent sense (as listed in WordNet).10
In the work reported here, we attempted disambiguation for head nouns and verbs
in subject and direct object relationships, and for adjectives and nouns in adjective-
noun relationships. For each test instance, we applied subject preferences before direct
object preferences, and direct object preferences before adjective?noun preferences. We
also propagated sense tags to test instances not in these relationships by applying the
one-sense-per-discourse heuristic.
We did not use the SENSEVAL-2 coarse-grained classification, as this was not
available at the time when we were acquiring the selectional preferences. We therefore
8 The motivation for using anaphora resolution was increased coverage, but anaphora resolution turned
out not actually to improve performance.
9 We use unsupervised to refer to systems that do not use manually sense-tagged training data, such as
SemCor. Our systems, marked in the figure as sel, sel-ospd, and sel-ospd-ana are unsupervised.
10 We are indebted to Judita Preiss for the most-frequent-sense result. This was obtained using the
frequency data supplied with the WordNet 1.7 version prereleased for SENSEVAL-2.
649
McCarthy and Carroll Disambiguating Using Selectional Preferences
Table 1
Overall results.
With OSPD Without OSPD
Precision 51.1% Precision 52.3%
Recall 23.2% Recall 20.0%
Attempted 45.5% Attempted 38.3%
Table 2
Precision results by part of speech.
Precision (%) Baseline precision (%)
Nouns 58.5 51.7
Polysemous nouns 36.8 25.8
Verbs 40.9 29.7
Polysemous verbs 38.1 25.3
Adjectives 49.8 48.6
Polysemous adjectives 35.5 24.0
Nouns, verbs, and adjectives 51.1 44.9
Polysemous nouns, verbs, and adjectives 36.8 27.3
do not include in the following the coarse-grained results; they are just slightly better
than the fine-grained results, which seems to be typical of other systems.
Our latest overall results are shown in Table 1. In this table we show the results
both with and without the OSPD heuristic. The results for the English SENSEVAL-2
tasks were generally much lower than those for the original SENSEVAL competition.
At the time of the SENSEVAL-2 workshop, this was assumed to be due largely to the
use of WordNet as the inventory, as opposed to HECTOR (Atkins 1993), but Palmer,
Trang Dang, and Fellbaum (forthcoming) have subsequently shown that, at least for the
lexical sample tasks, this was due to a harder selection of words, with a higher average
level of polysemy. For three of the most polysemous verbs that overlapped between
the English lexical sample for SENSEVAL and SENSEVAL-2, the performance was
comparable. Table 2 shows our precision results including use of the OSPD heuristic,
broken down by part of speech. Although the precision for nouns is greater than that
for verbs, the difference is much less when we remove the trivial monosemous cases.
Nouns, verbs, and adjectives all outperform their random baseline for precision, and
the difference is more marked when monosemous instances are dropped.
Table 3 shows the precision results for polysemous words given the slot and the
disambiguation source. Overall, once at least one word token has been disambiguated
by the preferences, the OSPD heuristic seems to perform better than the selectional
preferences. We can see, however, that although this is certainly true for the nouns,
the difference for the adjectives (1.3%) is less marked, and the preferences outperform
OSPD for the verbs. It seems that verbs obey the OSPD principle much less than nouns.
Also, verbs are best disambiguated by their direct objects, whereas nouns appear to
be better disambiguated as subjects and when modified by adjectives.
6. Discussion
6.1 Selectional Preferences
The precision of our system compares well with that of other unsupervised systems on
the SENSEVAL-2 English all-words task, despite the fact that these other systems use a
650
Computational Linguistics Volume 29, Number 4
Table 3
Precision results for polysemous words by part of speech and slot or disambiguation source.
Subject (%) Dobj (%) Adjm (%) OSPD (%)
Polysemous nouns 33.7 26.8 31.0 49.0
Polysemous verbs 33.8 47.3 ? 29.8
Polysemous adjectives ? ? 35.1 36.4
Polysemous nouns, verbs, and adjectives 33.4 36.0 31.6 44.8
number of different sources of information for disambiguation, rather than selectional
preferences in isolation. Light and Greiff (2002) summarize some earlier WSD results
for automatically acquired selectional preferences. These results were obtained for
three systems (Resnik 1997; Abney and Light 1999; Ciaramita and Johnson 2000) on a
training and test data set constructed by Resnik containing nouns occurring as direct
objects of 100 verbs that select strongly for their objects.
Both the test and training sets were extracted from the section of the Brown cor-
pus within the Penn Treebank and used the treebank parses. The test set comprised
the portion of this data within SemCor containing these 100 verbs, and the training
set comprised 800,000 words from the Penn Treebank parses of the Brown corpus not
within SemCor. All three systems obtained higher precision than the results we report
here, with Ciaramita and Johnson?s Bayesian belief networks achieving the best accu-
racy at 51.4%. These results are not comparable with ours, however, for three reasons.
First, our results for the direct-object slot are for all verbs in the English all-words task,
as opposed to just those selecting strongly for their direct objects. We would expect
that WSD results using selectional preferences would be better for the latter class of
verbs. Second, we do not use manually produced parses, but the output from our fully
automatic shallow parser. Third and finally, the baselines reported for Resnik?s test set
were higher than those for the all-words task. For Resnik?s test data, the random base-
line was 28.5%, whereas for the polysemous nouns in the direct-object relation on the
all-words task, it was 23.9%. The distribution of senses was also perhaps more skewed
for Resnik?s test set, since the first sense heuristic was 82.8% (Abney and Light 1999),
whereas it was 53.6% for the polysemous direct objects in the all-words task. Although
our results do show that the precision for the TCMs compares favorably with that of
other unsupervised systems on the English all-words task, it would be worthwhile to
compare other selectional preference models on the same data.
Although the accuracy of our system is encouraging given that it does not use
hand-tagged data, the results are below the level of state-of-the-art supervised systems.
Indeed, a system just assigning to each word its most frequent sense as listed in
WordNet (the ?first-sense heuristic?) would do better than our preference models
(and in fact better than the majority of the SENSEVAL-2 English all-words supervised
systems). The first-sense heuristic, however, assumes the existence of sense-tagged data
that are able to give a definitive first sense. We do not use any first-sense information.
Although a modest amount of sense-tagged data is available for English (Miller et al
1993, Ng and Lee 1996), for other languages with minimal sense-tagged resources, the
heuristic is not applicable. Moreover, for some words the predominant sense varies
depending on the domain and text type.
To quantify this, we carried out an analysis of the polysemous nouns, verbs,
and adjectives in SemCor occurring in more than one SemCor file and found that
a large proportion of words have a different first sense in different files and also
in different genres (see Table 4). For adjectives there seems to be a lot less ambi-
651
McCarthy and Carroll Disambiguating Using Selectional Preferences
Table 4
Percentages of words with a different predominant sense in SemCor, across files and genres.
File Genre
Nouns 70 66
Verbs 79 74
Adjectives 25 21
guity (this has also been noted by Krovetz [1998]; the data in SENSEVAL-2 bear
this out, with many adjectives occurring only in their first sense. For nouns and
verbs, for which the predominant sense is more likely to vary among texts, it would
be worthwhile to try to detect words for which using the predominant sense is
not a reliable strategy, for example, because the word shows ?bursty? topic-related
behavior.
We therefore examined our disambiguation results to see if there was any pattern
in the predicates or arguments that were easily disambiguated themselves or were
good disambiguators of the co-occurring word. No particular patterns were evident
in this respect, perhaps because of the small size of the test data. There were nouns
such as team (precision= 22 ) and cancer (
8
10 ) that did better than average, but whether
or not they did better than the first-sense heuristic depends of course on the sense
in which they are used. For example, all 10 occurrences of cancer are in the first
sense, so the first sense heuristic is impossible to beat in this case. For the test items
that are not in their first sense, we beat the first-sense heuristic, but on the other
hand, we failed to beat the random baseline. (The random baseline is 21.8% and
we obtained 21.4% for these items overall.) Our performance on these items is low
probably because they are lower-frequency senses for which there is less evidence
in the untagged training corpus (the BNC). We believe that selectional preferences
would perform best if they were acquired from similar training data to that for which
disambiguation is required. In the future, we plan to investigate our models for WSD
in specific domains, such as sport and finance. The senses and frequency distribution
of senses for a given domain will in general be quite different from those in a balanced
corpus.
There are individual words that are not used in the first sense on which our TCM
preferences do well, for example sound (precision = 22 ), but there are not enough data
to isolate predicates or arguments that are good disambiguators from those that are
not. We intend to investigate this issue further with the SENSEVAL-2 lexical sample
data, which contains more instances of a smaller number of words.
Performance of selectional preferences depends not just on the actual word being
disambiguated, but the cohesiveness of the tuple <pred, arg, gr>. We have therefore
investigated applying a threshold on the probability of the class (nc, vc, or ac) before
disambiguation. Figure 7 presents a graph of precision against threshold applied to the
probability estimate for the highest-scoring class. We show alongside this the random
baseline and the first-sense heuristic for these items. Selectional preferences appear to
do better on items for which the probability predicted by our model is higher, but the
first-sense heuristic does even better on these. The first sense heuristic, with respect to
SemCor, outperforms the selectional preferences when it is averaged over a given text.
That seems to be the case overall, but there will be some words and texts for which
the first sense from SemCor is not relevant, and use of a threshold on probability, and
perhaps a differential between probability of the top-ranked senses suggested by the
model, should increase precision.
652
Computational Linguistics Volume 29, Number 4
0
0.2
0.4
0.6
0.8
1
0 0.002 0.004 0.006 0.008 0.01
pr
ec
isi
on
threshold
"first sense"
"TCMs"
"random"
Figure 7
Thresholding the probability estimate for the highest-scoring class.
Table 5
Lemma/file combinations in SemCor with more than one sense evident.
Nouns 23%
Verbs 19%
Adjectives 1.6%
6.2 The OSPD Heuristic
In these experiments we applied the OSPD heuristic to increase coverage. One problem
in doing this when using a fine-grained classification like WordNet is that although
the OSPD heuristic works well for homonyms, it is less accurate for related senses
(Krovetz 1998), and this distinction is not made in WordNet. We did, however, find
that in SemCor, for the majority of polysemous11 lemma and file combinations, there
was only one sense exhibited (see Table 5). We refrained from using the OSPD in
situations in which there was conflicting evidence regarding the appropriate sense for
a word type occurring more than once in an individual file. In our experiments the
OSPD heuristic increased coverage by 7% and recall by 3%, at a cost of only a 1%
decrease in precision.
7. Conclusion
We quantified coverage and accuracy of sense disambiguation of verbs, adjectives,
and nouns in the SENSEVAL-2 English all-words test corpus, using automatically
acquired selectional preferences. We improved coverage and recall by applying the
one-sense-per-discourse heuristic. The results show that disambiguation models using
only selectional preferences can perform with accuracy well above the random base-
line, although accuracy would not be high enough for applications in the absence of
11 Krovetz just looked at ?actual ambiguity,? that is, words with more than one sense in SemCor. We
define polysemy as those words having more than one sense in WordNet, since we are using
SENSEVAL-2 data, and not SemCor.
653
McCarthy and Carroll Disambiguating Using Selectional Preferences
other knowledge sources (Stevenson and Wilks 2001). The results compare well with
those for other systems that do not use sense-tagged training data.
Selectional preferences work well for some word combinations and grammatical
relationships, but not well for others. We hope in future work to identify the situations
in which selectional preferences have high precision and to focus on these at the
expense of coverage, on the assumption that other knowledge sources can be used
where there is not strong evidence from the preferences. The first-sense heuristic, based
on sense-tagged data such as that available in SemCor, seems to beat unsupervised
models such as ours. For many words, however, the predominant sense varies across
domains, and so we contend that it is worth concentrating on detecting when the
first sense is not relevant, and where the selectional-preference models provide a high
probability for a secondary sense. In these cases evidence for a sense can be taken from
multiple occurrences of the word in the document, using the one-sense-per-discourse
heuristic.
Acknowledgments
This work was supported by UK EPSRC
project GR/N36493 ?Robust Accurate
Statistical Parsing (RASP)? and EU FW5
project IST-2001-34460 ?MEANING.? We are
grateful to Rob Koeling and three
anonymous reviewers for their helpful
comments on earlier drafts. We would also
like to thank David Weir and Mark
Mclauchlan for useful discussions.
References
Abney, Steven and Marc Light. 1999. Hiding
a semantic class hierarchy in a Markov
model. In Proceedings of the ACL Workshop
on Unsupervised Learning in Natural
Language Processing, pages 1?8.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the Fifth
Workshop on Computational Language
Learning (CoNLL-2001), pages 15?22.
Atkins, Sue. 1993. Tools for computer-aided
lexicography: The Hector project. In
Papers in Computational Lexicography:
COMPLEX 93, Budapest.
Briscoe, Ted and John Carroll. 1993.
Generalised probabilistic LR parsing of
natural language (corpora) with
unification-based grammars.
Computational Linguistics, 19(1):25?59.
Briscoe, Ted and John Carroll. 1995.
Developing and evaluating a probabilistic
LR parser of part-of-speech and
punctuation labels. In fourth
ACL/SIGPARSE International Workshop on
Parsing Technologies, pages 48?58, Prague,
Czech Republic.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: A
survey and a new proposal. In Proceedings
of the International Conference on Language
Resources and Evaluation, pages 447?454.
Carroll, John, Guido Minnen, and Ted
Briscoe. 1999. Corpus annotation for
parser evaluation. In EACL-99
Post-conference Workshop on Linguistically
Interpreted Corpora, pages 35?41, Bergen,
Norway.
Carroll, John, Guido Minnen, Darren Pearce,
Yvonne Canning, Siobhan Devlin, and
John Tait. 1999. Simplifying English text
for language impaired readers. In
Proceedings of the Ninth Conference of the
European Chapter of the Association for
Computational Linguistics, pages 269?270,
Bergen, Norway.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In Proceedings of the
18th International Conference of
Computational Linguistics (COLING-00),
pages 187?193.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
SENSEVAL-2. Available at http://www.
sle.sharp.co.uk/senseval2/.
Elworthy, David. 1994. Does Baum-Welch
re-estimation help taggers? In Proceedings
of the fourth ACL Conference on Applied
Natural Language Processing, pages 53?58,
Stuttgart, Germany.
Federici, Stefano, Simonetta Montemagni,
and Vito Pirrelli. 1999. SENSE: An
analogy-based word sense
disambiguation system. Natural Language
Engineering, 5(2):207?218.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. A method for
disambiguating word senses in a large
corpus. Computers and the Humanities,
654
Computational Linguistics Volume 29, Number 4
26:415?439.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX SENSEVAL Workshop.
Available at http://www.itri.bton.ac.
uk/events/senseval/
ARCHIVE/PROCEEDINGS/.
Li, Hang and Naoki Abe. 1995. Generalizing
case frames using a thesaurus and the
MDL principle. In Proceedings of the
International Conference on Recent Advances
in Natural Language Processing, pages
239?248, Bulgaria.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Light, Marc and Warren Greiff. 2002.
Statistical models for the induction and
use of selectional preferences. Cognitive
Science, 26(3):269?281.
McCarthy, Diana. 1997. Word sense
disambiguation for acquisition of
selectional preferences. In Proceedings of
the ACL/EACL 97 Workshop Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 52?61.
McCarthy, Diana. 2001. Lexical Acquisition at
the Syntax-Semantics Interface: Diathesis
Alternations, Subcategorization Frames and
Selectional Preferences. Ph.D. thesis,
University of Sussex.
Miller, George, A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308. Morgan
Kaufmann.
Minnen, Guido, John Carroll, and Darren
Pearce. 2001. Applied morphological
processing of English. Natural Language
Engineering, 7(3):207?223.
Ng, Hwee Tou and Hian Beng Lee. 1996.
Integrating multiple knowledge sources
to disambiguate word sense: An
exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 40?47.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2003. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Forthcoming, Natural
Language Engineering.
Preiss, Judita and Anna Korhonen. 2002.
Improving subcategorization acquisition
with WSD. In Proceedings of the ACL
Workshop on Word Sense Disambiguation:
Recent Successes and Future Directions,
Philadelphia, PA.
Resnik, Philip. 1997. Selectional preference
and sense disambiguation. In Proceedings
of the SIGLEX Workshop on Tagging Text with
Lexical Semantics: Why, What, and How?
pages 52?57, Washington, DC.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112?118.
Rissanen, Jorma. 1978. Modelling by
shortest data description. Automatica,
14:465?471.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources in
word sense disambiguation. Computational
Linguistics, 17(3):321?349.
Proceedings of NAACL HLT 2009: Short Papers, pages 233?236,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Estimating and Exploiting the Entropy of Sense Distributions
Peng Jin
Institute of Computational Linguistics
Peking University
Beijing China
jandp@pku.edu.cn
Diana McCarthy, Rob Koeling and John Carroll
University of Sussex
Falmer, East Sussex
BN1 9QJ, UK
{dianam,robk,johnca}@sussex.ac.uk
Abstract
Word sense distributions are usually skewed.
Predicting the extent of the skew can help a
word sense disambiguation (WSD) system de-
termine whether to consider evidence from the
local context or apply the simple yet effec-
tive heuristic of using the first (most frequent)
sense. In this paper, we propose a method to
estimate the entropy of a sense distribution to
boost the precision of a first sense heuristic by
restricting its application to words with lower
entropy. We show on two standard datasets
that automatic prediction of entropy can in-
crease the performance of an automatic first
sense heuristic.
1 Introduction
Word sense distributions are typically skewed and
WSD systems do best when they exploit this ten-
dency. This is usually done by estimating the most
frequent sense (MFS) for each word from a training
corpus and using that sense as a back-off strategy for
a word when there is no convincing evidence from
the context. This is known as the MFS heuristic 1
and is very powerful since sense distributions are
usually skewed. The heuristic becomes particularly
hard to beat for words with highly skewed sense dis-
tributions (Yarowsky and Florian, 2002). Although
the MFS can be estimated from tagged corpora, there
are always cases where there is insufficient data, or
where the data is inappropriate, for example because
1It is also referred to as the first sense heuristic in the WSD
literature and in this paper.
it comes from a very different domain. This has mo-
tivated some recent work attempting to estimate the
distributions automatically (McCarthy et al, 2004;
Lapata and Keller, 2007). This paper examines the
case for determining the skew of a word sense distri-
bution by estimating entropy and then using this to
increase the precision of an unsupervised first sense
heuristic by restricting application to those words
where the system can automatically detect that it has
the most chance. We use a method based on that
proposed by McCarthy et al (2004) as this approach
does not require hand-labelled corpora. The method
could easily be adapted to other methods for predic-
ing predominant sense.
2 Method
Given a listing of senses from an inventory, the
method proposed by McCarthy et al (2004) pro-
vides a prevalence ranking score to produce a MFS
heuristic. We make a slight modification to Mc-
Carthy et al?s prevalence score and use it to es-
timate the probability distribution over the senses
of a word. We use the same resources as Mc-
Carthy et al (2004): a distributional similarity the-
saurus and a WordNet semantic similarity measure.
The thesaurus was produced using the metric de-
scribed by Lin (1998) with input from the gram-
matical relation data extracted using the 90 mil-
lion words of written English from the British Na-
tional Corpus (BNC) (Leech, 1992) using the RASP
parser (Briscoe and Carroll, 2002). The thesaurus
consists of entries for each word (w) with the top
50 ?nearest neighbours? to w, where the neighbours
are words ranked by the distributional similarity that
233
they share with w. The WordNet similarity score
is obtained with the jcn measure (Jiang and Con-
rath, 1997) using the WordNet Similarity Package
0.05 (Patwardhan and Pedersen, 2003) and WordNet
version 1.6. The jcn measure needs word frequency
information, which we obtained from the BNC.
2.1 Estimates of Predominance, Probability
and Entropy
Following McCarthy et al (2004), we calculate
prevalence of each sense of the word (w) using a
weighted sum of the distributional similarity scores
of the top 50 neighbours of w. The sense of w that
has the highest value is the automatically detected
MFS (predominant sense). The weights are deter-
mined by the WordNet similarity between the sense
in question and the neighbour. We make a modi-
fication to the original method by multiplying the
weight by the inverse rank of the neighbour from
the list of 50 neighbours. This modification magni-
fies the contribution to each sense depending on the
rank of the neighbour while still allowing a neigh-
bour to contribute to all senses that it relates too.
We verified the effect of this change compared to the
original ranking score by measuring cross-entropy. 2
Let Nw = n1,n2 . . .nk denote the ordered set of the
top k = 50 neighbours of w according to the distri-
butional similarity thesaurus, senses(w) is the set of
senses of w and dss(w,n j) is the distributional sim-
ilarity score of a word w and its jth neighbour. Let
wsi be a sense of w then wnss(wsi,n j) is the maxi-
mum WordNet similarity score between wsi and the
WordNet sense of the neighbour (n j) that maximises
this score. The prevalence score is calculated as fol-
lows with 1rankn j being our modification to McCarthyet al
Prevalence Score(wsi) = ?n j?Nw dss(w,n j)?
wnss(wsi,n j)
?wsi??senses(w)wnss(wsi? ,n j)
? 1rankn j
(1)
To turn this score into a probability estimate we sum
the scores over all senses of a word and the proba-
bility for a sense is the original score divided by this
sum:
2Our modified version of the score gave a lower cross-
entropy with SemCor compared to that in McCarthy et al The
result was highly significant with p < 0.01 on the t-test.
p?(wsi) = prevalence score(wsi)?ws j?w prevalence score(ws j)
(2)
To smooth the data, we evenly distribute 1/10 of the
smallest prevalence score to all senses with a unde-
fined prevalence score values. Entropy is measured
as:
H(senses(w)) =? ?
wsi?senses(w)
p(wsi)log(p(wsi))
using our estimate (p?) for the probability distribu-
tion p over the senses of w.
3 Experiments
We conducted two experiments to evaluate the ben-
efit of using our estimate of entropy to restrict appli-
cation of the MFS heuristic. The two experiments
are conducted on the polysemous nouns in SemCor
and the nouns in the SENSEVAL-2 English all words
task (we will refer to this as SE2-EAW).
3.1 SemCor
For this experiment we used all the polysemous
nouns in Semcor 1.6 (excluding multiwords and
proper nouns). We depart slightly from (McCarthy
et al, 2004) in including all polysemous nouns
whereas they limited the experiment to those with
a frequency in SemCor of 3 or more and where there
is one sense with a higher frequency than the others.
Table 1 shows the precision of finding the predomi-
nant sense using equation 1 with respect to different
entropy thresholds. At each threshold, the MFS in
Semcor provides the upper-bound (UB). The random
baseline (RBL) is computed by selecting one of the
senses of the target word randomly as the predomi-
nant sense. As we hypothesized, precision is higher
when the entropy of the sense distribution is lower,
which is an encouraging result given that the entropy
is automatically estimated. The performance of the
random baseline is higher at lower entropy which
shows that the task is easier and involves a lower de-
gree of polysemy of the target words. However, the
gains over the random baseline are greater at lower
entropy levels indicating that the merits of detect-
ing the skew of the distribution cannot all be due to
lower polysemy levels.
234
H precision #
(?) eq 1 RBL UB tokens
0.5 - - - 0
0.9 80.3 50.0 84.8 466
0.95 85.1 50.0 90.9 1360
1 68.5 50.0 87.4 9874
1.5 67.6 42.6 86.9 11287
2 58.0 36.7 79.5 25997
2.5 55.7 34.4 77.6 31599
3.0 50.2 30.6 73.4 41401
4.0 47.6 28.5 70.8 46987
5.0 (all) 47.3 27.3 70.5 47539
Table 1: First sense heuristic on SemCor
Freq ? P #tokens
1 45.9 1132
5 50.1 5765
10 50.7 10736
100 49.4 39543
1000(all) 47.3 47539
#senses ? P #tokens
2 67.2 10736
5 55.4 31181
8 50.1 41393
12 47.8 46041
30(all) 47.3 47539
Table 2: Precision (P) of equation 1 on SemCor with re-
spect to frequency and polysemy
We also conducted a frequency and polysemy
analysis shown in Table 2 to demonstrate that the
increase in precision is not all due to frequency or
polysemy. This is important, since both frequency
and polysemy level (assuming a predefined sense in-
ventory) could be obtained without the need for au-
tomatic estimation. As we can see, while precision
is higher for lower polysemy, the automatic estimate
of entropy can provide a greater increase in preci-
sion than polysemy, and frequency does not seem to
be strongly correlated with precision.
3.2 SENSEVAL-2 English All Words Dataset
The SE2-EAW task provides a hand-tagged test suite
of 5,000 words of running text from three articles
from the Penn Treebank II (Palmer et al, 2001).
Again, we examine whether precision of the MFS
H precision #
(?) eq 1 RBL SC UB tokens
0.5 - - - - 0
0.9 1 50.0 1 1 7
0.95 94.7 50.0 94.7 1 19
1 69.6 50.0 81.3 94.6 112
1.5 68.0 49.0 81.3 93.8 128
2 69.6 34.7 68.2 87.7 421
2.5 65.0 33.0 65.0 86.5 488
3.0 56.6 27.5 60.8 80.1 687
4.0 52.6 25.6 58.8 79.2 766
5.0 (all) 51.5 25.6 58.5 79.3 769
Table 3: First sense heuristic on SE2-EAW
heuristic can be increased by restricting application
depending on entropy. We use the same resources as
for the SemCor experiment. 3 Table 3 gives the re-
sults. The most frequent sense (MFS) from SE2-EAW
itself provides the upper-bound (UB). We also com-
pare performance with the Semcor MFS (SC). Per-
formance is close to the Semcor MFS while not re-
lying on any manual tagging. As before, precision
increases significantly for words with low estimated
entropy, and the gains over the random baseline are
higher compared to the gains including all words.
4 Related Work
There is promising related work on determining the
predominant sense for a MFS heuristic (Lapata and
Keller, 2007; Mohammad and Hirst, 2006) but our
work is the first to use the ranking score to estimate
entropy and apply it to determine the confidence in
the MFS heuristic. It is likely that these methods
would also have increased precision if the ranking
scores were used to estimate entropy. We leave such
investigations for further work.
Chan and Ng (2005) estimate word sense distri-
butions and demonstrate that sense distribution esti-
mation improves a supervised WSD classifier. They
use three sense distribution methods, including that
of McCarthy et al (2004). While the other two
methods outperform the McCarthy et al method,
3We also used a tool for mapping from WordNet 1.7 to
WordNet 1.6 (Daude? et al, 2000) to map the SE2-EAW noun
data (originally distributed with 1.7 sense numbers) to 1.6 sense
numbers.
235
they rely on parallel training data and are not appli-
cable on 9.6% of the test data for which there are
no training examples. Our method does not require
parallel training data.
Agirre and Mart??nez (2004) show that sense dis-
tribution estimation is very important for both super-
vised and unsupervised WSD. They acquire tagged
examples on a large scale by querying Google with
monosemous synonyms of the word senses in ques-
tion. They show that the method of McCarthy et
al. (2004) can be used to produce a better sampling
technique than relying on the bias from web data
or randomly selecting the same number of exam-
ples for each sense. Our work similarly shows that
the automatic MFS is an unsupervised alternative to
SemCor but our work does not focus on sampling
but on an estimation of confidence in an automatic
MFS heuristic.
5 Conclusions
We demonstrate that our variation of the McCarthy
et al (2004) method for finding a MFS heuristic can
be used for estimating the entropy of a sense dis-
tribution which can be exploited to boost precision.
Words which are estimated as having lower entropy
in general get higher precision. This suggests that
automatic estimation of entropy is a good criterion
for getting higher precision. This is in agreement
with Kilgarriff and Rosenzweig (2000) who demon-
strate that entropy is a good measure of the difficulty
of WSD tasks, though their measure of entropy was
taken from the gold-standard distribution itself.
As future work, we want to compare this approach
of estimating entropy with other methods for es-
timating sense distributions which do not require
hand-labelled data or parallel texts. Currently, we
disregard local context. We wish to couple the con-
fidence in the MFS with contextual evidence and in-
vestigate application on coarse-grained datasets.
Acknowledgements
This work was funded by the China Scholarship Council,
the National Grant Fundamental Research 973 Program
of China: Grant No. 2004CB318102, the UK EPSRC
project EP/C537262 ?Ranking Word Senses for Disam-
biguation?, and a UK Royal Society Dorothy Hodgkin
Fellowship to the second author.
References
E. Agirre and D. Mart??nez. 2004. Unsupervised wsd
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of EMNLP-2004,
pages 25?32, Barcelona, Spain.
E. Briscoe and J. Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of
LREC-2002, pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Y.S. Chan and H.T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proceedings of
IJCAI 2005, pages 1010?1015, Edinburgh, Scotland.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping word-
nets using structural information. In Proceedings of
the 38th Annual Meeting of the Association for Com-
putational Linguistics, Hong Kong.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Interna-
tional Conference on Research in Computational Lin-
guistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for english SENSEVAL. Computers and the
Humanities. Senseval Special Issue, 34(1?2):15?48.
M. Lapata and F. Keller. 2007. An information retrieval
approach to sense ranking. In Proceedings of NAACL-
2007, pages 348?355, Rochester.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of COLING-ACL 98, Mon-
treal, Canada.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of ACL-2004, pages 280?287, Barcelona,
Spain.
S. Mohammad and G. Hirst. 2006. Determining word
sense dominance using a thesauru s. In Proceedings of
EACL-2006, pages 121?128, Trento, Italy.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H. Trang Dang. 2001. English tasks: All-words and
verb lexical sample. In Proceedings of the SENSEVAL-
2 workshop, pages 21?24.
S. Patwardhan and T. Pedersen. 2003. The
wordnet::similarity package. http://wn-
similarity.sourceforge.net/.
D. Yarowsky and R. Florian. 2002. Evaluating sense
disambiguation performance across diverse parame-
ter spaces. Natural Language Engineering, 8(4):293?
310.
236
Finding Predominant Word Senses in Untagged Text
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The problem with using the
predominant, or first sense heuristic, aside from the
fact that it does not take surrounding context into
account, is that it assumes some quantity of hand-
tagged data. Whilst there are a few hand-tagged
corpora available for some languages, one would
expect the frequency distribution of the senses of
words, particularly topical words, to depend on the
genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired
from raw textual corpora and the WordNet similar-
ity package to find predominant noun senses auto-
matically. The acquired predominant senses give a
precision of 64% on the nouns of the SENSEVAL-
2 English all-words task. This is a very promising
result given that our method does not require any
hand-tagged text, such as SemCor. Furthermore,
we demonstrate that our method discovers appropri-
ate predominant senses for words from two domain-
specific corpora.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account. This is shown by the results of
the English all-words task in SENSEVAL-2 (Cot-
ton et al, 1998) in figure 1 below, where the first
sense is that listed in WordNet for the PoS given
by the Penn TreeBank (Palmer et al, 2001). The
senses in WordNet are ordered according to the fre-
quency data in the manually tagged resource Sem-
Cor (Miller et al, 1993). Senses that have not oc-
curred in SemCor are ordered arbitrarily and af-
ter those senses of the word that have occurred.
The figure distinguishes systems which make use
of hand-tagged data (using HTD) such as SemCor,
from those that do not (without HTD). The high per-
formance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where ev-
idence from the context is not sufficient (Hoste et
al., 2001). Whilst a first sense heuristic based on a
sense-tagged corpus such as SemCor is clearly use-
ful, there is a strong case for obtaining a first, or pre-
dominant, sense from untagged corpus data so that
a WSD system can be tuned to the genre or domain
at hand.
SemCor comprises a relatively small sample of
250,000 words. There are words where the first
sense in WordNet is counter-intuitive, because of
the size of the corpus, and because where the fre-
quency data does not indicate a first sense, the or-
dering is arbitrary. For example the first sense of
tiger in WordNet is audacious person whereas one
might expect that carnivorous animal is a more
common usage. There are only a couple of instances
of tiger within SemCor. Another example is em-
bryo, which does not occur at all in SemCor and
the first sense is listed as rudimentary plant rather
than the anticipated fertilised egg meaning. We be-
lieve that an automatic means of finding a predomi-
nant sense would be useful for systems that use it as
a means of backing-off (Wilks and Stevenson, 1998;
Hoste et al, 2001) and for systems that use it in lex-
ical acquisition (McCarthy, 1997; Merlo and Ley-
bold, 2001; Korhonen, 2002) because of the limited
size of hand-tagged resources. More importantly,
when working within a specific domain one would
wish to tune the first sense heuristic to the domain at
hand. The first sense of star in SemCor is celestial
body, however, if one were disambiguating popular
news celebrity would be preferred.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
020
40
60
80
100
0 20 40 60 80 100
re
ca
ll

precision
First Sense
"using HTD" "without HTD" "First Sense"
Figure 1: The first sense heuristic compared with
the SENSEVAL-2 English all-words task results
are therefore investigating a method of automati-
cally ranking WordNet senses from raw text.
Many researchers are developing thesauruses
from automatically parsed data. In these each tar-
get word is entered with an ordered list of ?near-
est neighbours?. The neighbours are words ordered
in terms of the ?distributional similarity? that they
have with the target. Distributional similarity is
a measure indicating the degree that two words, a
word and its neighbour, occur in similar contexts.
From inspection, one can see that the ordered neigh-
bours of such a thesaurus relate to the different
senses of the target word. For example, the neigh-
bours of star in a dependency-based thesaurus pro-
vided by Lin 1 has the ordered list of neighbours:
superstar, player, teammate, actor early in the list,
but one can also see words that are related to another
sense of star e.g. galaxy, sun, world and planet fur-
ther down the list. We expect that the quantity and
similarity of the neighbours pertaining to different
senses will reflect the dominance of the sense to
which they pertain. This is because there will be
more relational data for the more prevalent senses
compared to the less frequent senses. In this pa-
per we describe and evaluate a method for ranking
senses of nouns to obtain the predominant sense of
a word using the neighbours from automatically ac-
quired thesauruses. The neighbours for a word in a
thesaurus are words themselves, rather than senses.
In order to associate the neighbours with senses we
make use of another notion of similarity, ?semantic
similarity?, which exists between senses, rather than
words. We experiment with several WordNet Sim-
ilarity measures (Patwardhan and Pedersen, 2003)
which aim to capture semantic relatedness within
1Available at
http://www.cs.ualberta.ca/?lindek/demos/depsim.htm
the WordNet hierarchy. We use WordNet as our
sense inventory for this work.
The paper is structured as follows. We discuss
our method in the following section. Sections 3 and
4 concern experiments using predominant senses
from the BNC evaluated against the data in SemCor
and the SENSEVAL-2 English all-words task respec-
tively. In section 5 we present results of the method
on two domain specific sections of the Reuters cor-
pus for a sample of words. We describe some re-
lated work in section 6 and conclude in section 7.
2 Method
In order to find the predominant sense of a target
word we use a thesaurus acquired from automati-
cally parsed text based on the method of Lin (1998).
This provides the  nearest neighbours to each tar-
get word, along with the distributional similarity
score between the target word and its neighbour. We
then use the WordNet similarity package (Patward-
han and Pedersen, 2003) to give us a semantic simi-
larity measure (hereafter referred to as the WordNet
similarity measure) to weight the contribution that
each neighbour makes to the various senses of the
target word.
To find the first sense of a word (  ) we
take each sense in turn and obtain a score re-
flecting the prevalence which is used for rank-
ing. Let   	


 be the ordered
set of the top scoring  neighbours of  from
the thesaurus with associated distributional similar-
ity scores 	ffProceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10?18,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Investigations on Word Senses and Word Usages
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Nicholas Gaylord
University of Texas at Austin
nlgaylord@mail.utexas.edu
Abstract
The vast majority of work on word senses
has relied on predefined sense invento-
ries and an annotation schema where each
word instance is tagged with the best fit-
ting sense. This paper examines the case
for a graded notion of word meaning in
two experiments, one which uses WordNet
senses in a graded fashion, contrasted with
the ?winner takes all? annotation, and one
which asks annotators to judge the similar-
ity of two usages. We find that the graded
responses correlate with annotations from
previous datasets, but sense assignments
are used in a way that weakens the case for
clear cut sense boundaries. The responses
from both experiments correlate with the
overlap of paraphrases from the English
lexical substitution task which bodes well
for the use of substitutes as a proxy for
word sense. This paper also provides two
novel datasets which can be used for eval-
uating computational systems.
1 Introduction
The vast majority of work on word sense tag-
ging has assumed that predefined word senses
from a dictionary are an adequate proxy for the
task, although of course there are issues with
this enterprise both in terms of cognitive valid-
ity (Hanks, 2000; Kilgarriff, 1997; Kilgarriff,
2006) and adequacy for computational linguis-
tics applications (Kilgarriff, 2006). Furthermore,
given a predefined list of senses, annotation efforts
and computational approaches to word sense dis-
ambiguation (WSD) have usually assumed that one
best fitting sense should be selected for each us-
age. While there is usually some allowance made
for multiple senses, this is typically not adopted by
annotators or computational systems.
Research on the psychology of concepts (Mur-
phy, 2002; Hampton, 2007) shows that categories
in the human mind are not simply sets with clear-
cut boundaries: Some items are perceived as
more typical than others (Rosch, 1975; Rosch and
Mervis, 1975), and there are borderline cases on
which people disagree more often, and on whose
categorization they are more likely to change their
minds (Hampton, 1979; McCloskey and Glucks-
berg, 1978). Word meanings are certainly related
to mental concepts (Murphy, 2002). This raises
the question of whether there is any such thing as
the one appropriate sense for a given occurrence.
In this paper we will explore using graded re-
sponses for sense tagging within a novel annota-
tion paradigm. Modeling the annotation frame-
work after psycholinguistic experiments, we do
not train annotators to conform to sense distinc-
tions; rather we assess individual differences by
asking annotators to produce graded ratings in-
stead of making a binary choice. We perform two
annotation studies. In the first one, referred to
as WSsim (Word Sense Similarity), annotators
give graded ratings on the applicability of Word-
Net senses. In the second one, Usim (Usage Sim-
ilarity), annotators rate the similarity of pairs of
occurrences (usages) of a common target word.
Both studies explore whether users make use of
a graded scale or persist in making binary deci-
sions even when there is the option for a graded
response. The first study additionally tests to what
extent the judgments on WordNet senses fall into
clear-cut clusters, while the second study allows
us to explore meaning similarity independently of
any lexicon resource.
10
2 Related Work
Manual word sense assignment is difficult for
human annotators (Krishnamurthy and Nicholls,
2000). Reported inter-annotator agreement (ITA)
for fine-grained word sense assignment tasks has
ranged between 69% (Kilgarriff and Rosenzweig,
2000) for a lexical sample using the HECTOR dic-
tionary and 78.6.% using WordNet (Landes et al,
1998) in all-words annotation. The use of more
coarse-grained senses alleviates the problem: In
OntoNotes (Hovy et al, 2006), an ITA of 90% is
used as the criterion for the construction of coarse-
grained sense distinctions. However, intriguingly,
for some high-frequency lemmas such as leave
this ITA threshold is not reached even after mul-
tiple re-partitionings of the semantic space (Chen
and Palmer, 2009). Similarly, the performance
of WSD systems clearly indicates that WSD is not
easy unless one adopts a coarse-grained approach,
and then systems tagging all words at best perform
a few percentage points above the most frequent
sense heuristic (Navigli et al, 2007). Good perfor-
mance on coarse-grained sense distinctions may
be more useful in applications than poor perfor-
mance on fine-grained distinctions (Ide and Wilks,
2006) but we do not know this yet and there is
some evidence to the contrary (Stokoe, 2005).
Rather than focus on the granularity of clus-
ters, the approach we will take in this paper
is to examine the phenomenon of word mean-
ing both with and without recourse to predefined
senses by focusing on the similarity of uses of a
word. Human subjects show excellent agreement
on judging word similarity out of context (Ruben-
stein and Goodenough, 1965; Miller and Charles,
1991), and human judgments have previously been
used successfully to study synonymy and near-
synonymy (Miller and Charles, 1991; Bybee and
Eddington, 2006). We focus on polysemy rather
than synonymy. Our aim will be to use WSsim
to determine to what extent annotations form co-
hesive clusters. In principle, it should be possi-
ble to use existing sense-annotated data to explore
this question: almost all sense annotation efforts
have allowed annotators to assign multiple senses
to a single occurrence, and the distribution of these
sense labels should indicate whether annotators
viewed the senses as disjoint or not. However,
the percentage of markables that received multi-
ple sense labels in existing corpora is small, and it
varies massively between corpora: In the SemCor
corpus (Landes et al, 1998), only 0.3% of all
markables received multiple sense labels. In the
SENSEVAL-3 English lexical task corpus (Mihal-
cea et al, 2004) (hereafter referred to as SE-3), the
ratio is much higher at 8% of all markables1. This
could mean annotators feel that there is usually a
single applicable sense, or it could point to a bias
towards single-sense assignment in the annotation
guidelines and/or the annotation tool. The WSsim
experiment that we report in this paper is designed
to eliminate such bias as far as possible and we
conduct it on data taken from SemCor and SE-3 so
that we can compare the annotations. Although we
use WordNet for the annotation, our study is not a
study of WordNet per se. We choose WordNet be-
cause it is sufficiently fine-grained to examine sub-
tle differences in usage, and because traditionally
annotated datasets exist to which we can compare
our results.
Predefined dictionaries and lexical resources are
not the only possibilities for annotating lexical
items with meaning. In cross-lingual settings, the
actual translations of a word can be taken as the
sense labels (Resnik and Yarowsky, 2000). Re-
cently, McCarthy and Navigli (2007) proposed
the English Lexical Substitution task (hereafter
referred to as LEXSUB) under the auspices of
SemEval-2007. It uses paraphrases for words in
context as a way of annotating meaning. The task
was proposed following a background of discus-
sions in the WSD community as to the adequacy
of predefined word senses. The LEXSUB dataset
comprises open class words (nouns, verbs, adjec-
tives and adverbs) with token instances of each
word appearing in the context of one sentence
taken from the English Internet Corpus (Sharoff,
2006). The methodology can only work where
there are paraphrases, so the dataset only contains
words with more than one meaning where at least
two different meanings have near synonyms. For
meanings without obvious substitutes the annota-
tors were allowed to use multiword paraphrases or
words with slightly more general meanings. This
dataset has been used to evaluate automatic sys-
tems which can find substitutes appropriate for the
context. To the best of our knowledge there has
been no study of how the data collected relates to
word sense annotations or judgments of semantic
similarity. In this paper we examine these relation-
1This is even though both annotation efforts use balanced
corpora, the Brown corpus in the case of SemCor, the British
National Corpus for SE-3.
11
ships by re-using data from LEXSUB in both new
annotation experiments and testing the results for
correlation.
3 Annotation
We conducted two experiments through an on-
line annotation interface. Three annotators partic-
ipated in each experiment; all were native British
English speakers. The first experiment, WSsim,
collected annotator judgments about the applica-
bility of dictionary senses using a 5-point rating
scale. The second, Usim, also utilized a 5-point
scale but collected judgments on the similarity in
meaning between two uses of a word. 2 The scale
was 1 ? completely different, 2 ? mostly different,
3 ? similar, 4 ? very similar and 5 ? identical. In
Usim, this scale rated the similarity of the two uses
of the common target word; in WSsim it rated the
similarity between the use of the target word and
the sense description. In both experiments, the an-
notation interface allowed annotators to revisit and
change previously supplied judgments, and a com-
ment box was provided alongside each item.
WSsim. This experiment contained a total of
430 sentences spanning 11 lemmas (nouns, verbs
and adjectives). For 8 of these lemmas, 50 sen-
tences were included, 25 of them randomly sam-
pled from SemCor 3 and 25 randomly sampled
from SE-3.4 The remaining 3 lemmas in the ex-
periment each had 10 sentences taken from the
LEXSUB data.
WSsim is a word sense annotation task using
WordNet senses.5 Unlike previous word sense an-
notation projects, we asked annotators to provide
judgments on the applicability of every WordNet
sense of the target lemma with the instruction: 6
2Throughout this paper, a target word is assumed to be a
word in a given PoS.
3The SemCor dataset was produced alongside WordNet,
so it can be expected to support the WordNet sense distinc-
tions. The same cannot be said for SE-3.
4Sentence fragments and sentences with 5 or fewer words
were excluded from the sampling. Annotators were given
the sentences, but not the original annotation from these re-
sources.
5WordNet 1.7.1 was used in the annotation of both SE-3
and SemCor; we used the more current WordNet 3.0 after
verifying that the lemmas included in this experiment had the
same senses listed in both versions. Care was taken addition-
ally to ensure that senses were not presented in an order that
reflected their frequency of occurrence.
6The guidelines for both experiments are avail-
able at http://comp.ling.utexas.edu/
people/katrin erk/graded sense and usage
annotation
Your task is to rate, for each of these descriptions,
how well they reflect the meaning of the boldfaced
word in the sentence.
Applicability judgments were not binary, but were
instead collected using the five-point scale given
above which allowed annotators to indicate not
only whether a given sense applied, but to what
degree. Each annotator annotated each of the 430
items. By having multiple annotators per item and
a graded, non-binary annotation scheme we al-
low for and measure differences between annota-
tors, rather than training annotators to conform to
a common sense distinction guideline. By asking
annotators to provide ratings for each individual
sense, we strive to eliminate all bias towards either
single-sense or multiple-sense assignment. In tra-
ditional word sense annotation, such bias could be
introduced directly through annotation guidelines
or indirectly, through tools that make it easier to
assign fewer senses. We focus not on finding the
best fitting sense but collect judgments on the ap-
plicability of all senses.
Usim. This experiment used data from LEXSUB.
For more information on LEXSUB, see McCarthy
and Navigli (2007). 34 lemmas (nouns, verbs, ad-
jectives and adverbs) were manually selected, in-
cluding the 3 lemmas also used in WSsim. We se-
lected lemmas which exhibited a range of mean-
ings and substitutes in the LEXSUB data, with
as few multiword substitutes as possible. Each
lemma is the target in 10 LEXSUB sentences. For
our experiment, we took every possible pairwise
comparison of these 10 sentences for a lemma. We
refer to each such pair of sentences as an SPAIR.
The resulting dataset comprised 45 SPAIRs per
lemma, adding up to 1530 comparisons per anno-
tator overall.
In this annotation experiment, annotators saw
SPAIRs with a common target word and rated the
similarity in meaning between the two uses of the
target word with the instruction:
Your task is to rate, for each pair of sentences, how
similar in meaning the two boldfaced words are on
a five-point scale.
In addition annotators had the ability to respond
with ?Cannot Decide?, indicating that they were
unable to make an effective comparison between
the two contexts, for example because the mean-
ing of one usage was unclear. This occurred in
9 paired occurrences during the course of anno-
tation, and these items (paired occurrences) were
12
excluded from further analysis.
The purpose of Usim was to collect judgments
about degrees of similarity between a word?s
meaning in different contexts. Unlike WSsim,
Usim does not rely upon any dictionary resource
as a basis for the judgments.
4 Analyses
This section reports on analyses on the annotated
data. In all the analyses we use Spearman?s rank
correlation coefficient (?), a nonparametric test,
because the data does not seem to be normally
distributed. We used two-tailed tests in all cases,
rather than assume the direction of the relation-
ship. As noted above, we have three annotators
per task, and each annotator gave judgments for
every sentence (WSsim) or sentence pair (Usim).
Since the annotators may vary as to how they use
the ordinal scale, we do not use the mean of judg-
ments7 but report all individual correlations. All
analyses were done using the R package.8
4.1 WSsim analysis
In the WSsim experiment, annotators rated the ap-
plicability of each WordNet 3.0 sense for a given
target word occurrence. Table 1 shows a sample
annotation for the target argument.n. 9
Pattern of annotation and annotator agree-
ment. Figure 1 shows how often each of the five
judgments on the scale was used, individually and
summed over all annotators. (The y-axis shows
raw counts of each judgment.) We can see from
this figure that the extreme ratings 1 and 5 are used
more often than the intermediate ones, but annota-
tors make use of the full ordinal scale when judg-
ing the applicability of a sense. Also, the figure
shows that annotator 1 used the extreme negative
rating 1 much less than the other two annotators.
Figure 2 shows the percentage of times each judg-
ment was used on senses of three lemmas, differ-
ent.a, interest.n, and win.v. In WordNet, they have
5, 7, and 4 senses, respectively. The pattern for
win.v resembles the overall distribution of judg-
ments, with peaks at the extreme ratings 1 and 5.
The lemma interest.n has a single peak at rating
1, partly due to the fact that senses 5 (financial
7We have also performed several of our calculations us-
ing the mean judgment, and they also gave highly significant
results in all the cases we tested.
8http://www.r-project.org/
9We use word.PoS to denote a target word (lemma).
Annotator 1 Annotator 2 Annotator 3 overall
1
2
3
4
5
0
500
1000
1500
2000
2500
3000
Figure 1: WSsim experiment: number of times
each judgment was used, by annotator and
summed over all annotators. The y-axis shows raw
counts of each judgment.
different.a interest.n win.v
1
2
3
4
5
0.0
0.1
0.2
0.3
0.4
0.5
Figure 2: WSsim experiment: percentage of times
each judgment was used for the lemmas differ-
ent.a, interest.n and win.v. Judgment counts were
summed over all three annotators.
involvement) and 6 (interest group) were rarely
judged to apply. For the lemma different.a, all
judgments have been used with approximately the
same frequency.
We measured the level of agreement between
annotators using Spearman?s ? between the judg-
ments of every pair of annotators. The pairwise
correlations were ? = 0.506, ? = 0.466 and ? =
0.540, all highly significant with p < 2.2e-16.
Agreement with previous annotation in
SemCor and SE-3. 200 of the items in WSsim
had been previously annotated in SemCor, and
200 in SE-3. This lets us compare the annotation
results across annotation efforts. Table 2 shows
the percentage of items where more than one
sense was assigned in the subset of WSsim from
SemCor (first row), from SE-3 (second row), and
13
Senses
Sentence 1 2 3 4 5 6 7 Annotator
This question provoked arguments in America about the
Norton Anthology of Literature by Women, some of the
contents of which were said to have had little value as
literature.
1 4 4 2 1 1 3 Ann. 1
4 5 4 2 1 1 4 Ann. 2
1 4 5 1 1 1 1 Ann. 3
Table 1: A sample annotation in the WSsim experiment. The senses are: 1:statement, 2:controversy,
3:debate, 4:literary argument, 5:parameter, 6:variable, 7:line of reasoning
WSsim judgment
Data Orig. ? 3 ? 4 5
WSsim/SemCor 0.0 80.2 57.5 28.3
WSsim/SE-3 24.0 78.0 58.3 27.1
All WSsim 78.8 57.4 27.7
Table 2: Percentage of items with multiple senses
assigned. Orig: in the original SemCor/SE-3 data.
WSsim judgment: items with judgments at or
above the specified threshold. The percentages for
WSsim are averaged over the three annotators.
all of WSsim (third row). The Orig. column
indicates how many items had multiple labels in
the original annotation (SemCor or SE-3) 10. Note
that no item had more than one sense label in
SemCor. The columns under WSsim judgment
show the percentage of items (averaged over
the three annotators) that had judgments at or
above the specified threshold, starting from rating
3 ? similar. Within WSsim, the percentage of
multiple assignments in the three rows is fairly
constant. WSsim avoids the bias to one sense
by deliberately asking for judgments on the
applicability of each sense rather than asking
annotators to find the best one.
To compute the Spearman?s correlation between
the original sense labels and those given in the
WSsim annotation, we converted SemCor and
SE-3 labels to the format used within WSsim: As-
signed senses were converted to a judgment of 5,
and unassigned senses to a judgment of 1. For the
WSsim/SemCor dataset, the correlation between
original and WSsim annotation was ? = 0.234,
? = 0.448, and ? = 0.390 for the three anno-
tators, each highly significant with p < 2.2e-16.
For the WSsim/SE-3 dataset, the correlations were
? = 0.346, ? = 0.449 and ? = 0.338, each of them
again highly significant at p < 2.2e-16.
Degree of sense grouping. Next we test to what
extent the sense applicability judgments in the
10Overall, 0.3% of tokens in SemCor have multiple labels,
and 8% of tokens in SE-3, so the multiple label assignment in
our sample is not an underestimate.
p < 0.05 p < 0.01
pos neg pos neg
Ann. 1 30.8 11.4 23.2 5.9
Ann. 2 22.2 24.1 19.6 19.6
Ann. 3 12.7 12.0 10.0 6.0
Table 3: Percentage of sense pairs that were sig-
nificantly positively (pos) or negatively (neg) cor-
related at p < 0.05 and p < 0.01, shown by anno-
tator.
j ? 3 j ? 4 j = 5
Ann. 1 71.9 49.1 8.1
Ann. 2 55.3 24.7 8.1
Ann. 3 42.8 24.0 4.9
Table 4: Percentage of sentences in which at least
two uncorrelated (p > 0.05) or negatively corre-
lated senses have been annotated with judgments
at the specified threshold.
WSsim task could be explained by more coarse-
grained, categorial sense assignments. We first
test how many pairs of senses for a given lemma
show similar patterns in the ratings that they re-
ceive. Table 3 shows the percentage of sense pairs
that were significantly correlated for each anno-
tator.11 Significantly positively correlated senses
can possibly be reduced to more coarse-grained
senses. Would annotators have been able to des-
ignate a single appropriate sense given these more
coarse-grained senses? Call two senses groupable
if they are significantly positively correlated; in or-
der not to overlook correlations that are relatively
weak but existent, we use a cutoff of p = 0.05 for
significant correlation. We tested how often anno-
tators gave ratings of at least similar, i.e. ratings
? 3, to senses that were not groupable. Table 4
shows the percentages of items where at least two
non-groupable senses received ratings at or above
the specified threshold. The table shows that re-
gardless of which annotator we look at, over 40%
of all items had two or more non-groupable senses
receive judgments of at least 3 (similar). There
11We exclude senses that received a uniform rating of 1 on
all items. This concerned 4 senses for annotator 2 and 6 for
annotator 3.
14
1) We study the methods and concepts that each writer uses to
defend the cogency of legal, deliberative, or more generally
political prudence against explicit or implicit charges that
practical thinking is merely a knack or form of cleverness.
2) Eleven CIRA members have been convicted of criminal
charges and others are awaiting trial.
Figure 3: An SPAIR for charge.n. Annotator judg-
ments: 2,3,4
were even several items where two or more non-
groupable senses each got a judgment of 5. The
sentence in table 1 is a case where several non-
groupable senses got ratings ? 3. This is most
pronounced for Annotator 2, who along with sense
2 (controversy) assigned senses 1 (statement), 7
(line of reasoning), and 3 (debate), none of which
are groupable with sense 2.
4.2 Usim analysis
In this experiment, ratings between 1 and 5 were
given for every pairwise combination of sentences
for each target lemma. An example of an SPAIR
for charge.n is shown in figure 3. In this case the
verdicts from the annotators were 2, 3 and 4.
Pattern of Annotations and Annotator Agree-
ment Figure 4 gives a bar chart of the judgments
for each annotator and summed over annotators.
We can see from this figure that the annotators
use the full ordinal scale when judging the simi-
larity of a word?s usages, rather than sticking to
the extremes. There is variation across words, de-
pending on the relatedness of each word?s usages.
Figure 5 shows the judgments for the words bar.n,
work.v and raw.a. We see that bar.n has predom-
inantly different usages with a peak for category
1, work.v has more similar judgments (category 5)
compared to any other category and raw.a has a
peak in the middle category (3). 12 There are other
words, like for example fresh.a, where the spread
is more uniform.
To gauge the level of agreement between anno-
tators, we calculated Spearman?s ? between the
judgments of every pair of annotators as in sec-
tion 4.1. The pairwise correlations are all highly
significant (p < 2.2e-16) with Spearman?s ? =
0.502, 0.641 and 0.501 giving an average corre-
lation of 0.548. We also perform leave-one-out re-
sampling following Lapata (2006) which gave us
a Spearman?s correlation of 0.630.
12For figure 5 we sum the judgments over annotators.
Annotator 4 Annotator 5 Annotator 6 overall
12345
0
500
1000
1500
Figure 4: Usim experiment: number of times each
judgment was used, by annotator and summed
over all annotators
bar.n raw.a work.v
12345
0
10
20
30
40
50
60
Figure 5: Usim experiment: number of times each
judgment was used for bar.n, work.v and raw.a
Comparison with LEXSUB substitutions Next
we look at whether the Usim judgments on sen-
tence pairs (SPAIRs) correlate with LEXSUB sub-
stitutes. To do this we use the overlap of substi-
tutes provided by the five LEXSUB annotators be-
tween two sentences in an SPAIR. In LEXSUB the
annotators had to replace each item (a target word
within the context of a sentence) with a substitute
that fitted the context. Each annotator was permit-
ted to supply up to three substitutes provided that
they all fitted the context equally. There were 10
sentences per lemma. For our analyses we take
every SPAIR for a given lemma and calculate the
overlap (inter) of the substitutes provided by the
annotators for the two usages under scrutiny. Let
s1 and s2 be a pair of sentences in an SPAIR and
15
x1 and x2 be the multisets of substitutes for the
respective sentences. Let f req(w,x) be the fre-
quency of a substitute w in a multiset x of sub-
stitutes for a given sentence. 13 INTER(s1,s2) =
?w?x1?x2 min( f req(w,x1), f req(w,x2))
max(|x1|, |x2|)
Using this calculation for each SPAIR we can
now compute the correlation between the Usim
judgments for each annotator and the INTER val-
ues, again using Spearman?s. The figures are
shown in the leftmost block of table 5. The av-
erage correlation for the 3 annotators was 0.488
and the p-values were all < 2.2e-16. This shows
a highly significant correlation of the Usim judg-
ments and the overlap of substitutes.
We also compare the WSsim judgments against
the LEXSUB substitutes, again using the INTER
measure of substitute overlap. For this analysis,
we only use those WSsim sentences that are origi-
nally from LEXSUB. In WSsim, the judgments for
a sentence comprise judgments for each WordNet
sense of that sentence. In order to compare against
INTER, we need to transform these sentence-wise
ratings in WSsim to a WSsim-based judgment of
sentence similarity. To this end, we compute the
Euclidean Distance14 (ED) between two vectors J1
and J2 of judgments for two sentences s1,s2 for the
same lemma `. Each of the n indexes of the vector
represent one of the n different WordNet senses
for `. The value at entry i of the vector J1 is the
judgment that the annotator in question (we do not
average over annotators here) provided for sense i
of ` for sentence s1.
ED(J1,J2) =
?
(
n
?
i=1
(J1[i]? J2[i])
2) (1)
We correlate the Euclidean distances with
INTER. We can only test correlation for the subset
of WSsim that overlaps with the LEXSUB data: the
30 sentences for investigator.n, function.n and or-
der.v, which together give 135 unique SPAIRs. We
refer to this subset as W?U. The results are given
in the third block of table 5. Note that since we are
measuring distance between SPAIRs for WSsim
13The frequency of a substitute in a multiset depends on
the number of LEXSUB annotators that picked the substitute
for this item.
14We use Euclidean Distance rather than a normalizing
measure like Cosine because a sentence where all ratings are
5 should be very different from a sentence where all senses
received a rating of 1.
Usim All Usim W?U WSsim W?U
ann. ? ? ann. ?
4 0.383 0.330 1 -0.520
5 0.498 0.635 2 -0.503
6 0.584 0.631 3 -0.463
Table 5: Annotator correlation with LEXSUB sub-
stitute overlap (inter)
whereas INTER is a measure of similarity, the cor-
relation is negative. The results are highly signif-
icant with individual p-values from < 1.067e-10
to < 1.551e-08 and a mean correlation of -0.495.
The results in the first and third block of table 5 are
not directly comparable, as the results in the first
block are for all Usim data and not the subset of
LEXSUB with WSsim annotations. We therefore
repeated the analysis for Usim on the subset of
data in WSsim and provide the correlation in the
middle section of table 5. The mean correlation
for Usim on this subset of the data is 0.532, which
is a stronger relationship compared to WSsim, al-
though there is more discrepancy between individ-
ual annotators, with the result for annotator 4 giv-
ing a p-value = 9.139e-05 while the other two an-
notators had p-values < 2.2e-16.
The LEXSUB substitute overlaps between dif-
ferent usages correlate well with both Usim and
WSsim judgments, with a slightly stronger rela-
tionship to Usim, perhaps due to the more compli-
cated representation of word meaning in WSsim
which uses the full set of WordNet senses.
4.3 Correlation between WSsim and Usim
As we showed in section 4.1, WSsim correlates
with previous word sense annotations in SemCor
and SE-3 while allowing the user a more graded
response to sense tagging. As we saw in sec-
tion 4.2, Usim and WSsim judgments both have a
highly significant correlation with similarity of us-
ages as measured using the overlap of substitutes
from LEXSUB. Here, we look at the correlation
of WSsim and Usim, considering again the sub-
set of data that is common to both experiments.
We again transform WSsim sense judgments for
individual sentences to distances between SPAIRs
using Euclidean Distance (ED). The Spearman?s
? range between ?0.307 and ?0.671, and all re-
sults are highly significant with p-values between
0.0003 and < 2.2e-16. As above, the correla-
tion is negative because ED is a distance measure
between sentences in an SPAIR, whereas the judg-
16
ments for Usim are similarity judgments. We see
that there is highly significant correlation for every
pairing of annotators from the two experiments.
5 Discussion
Validity of annotation scheme. Annotator rat-
ings show highly significant correlation on both
tasks. This shows that the tasks are well-defined.
In addition, there is a strong correlation between
WSsim and Usim, which indicates that the poten-
tial bias introduced by the use of dictionary senses
in WSsim is not too prominent. However, we note
that WSsim only contained a small portion of 3
lemmas (30 sentences and 135 SPAIRs) in com-
mon with Usim, so more annotation is needed to
be certain of this relationship. Given the differ-
ences between annotator 1 and the other annota-
tors in Fig. 1, it would be interesting to collect
judgments for additional annotators.
Graded judgments of use similarity and sense
applicability. The annotators made use of the
full spectrum of ratings, as shown in Figures 1 and
4. This may be because of a graded perception of
the similarity of uses as well as senses, or because
some uses and senses are very similar. Table 4
shows that for a large number of WSsim items,
multiple senses that were not significantly posi-
tively correlated got high ratings. This seems to
indicate that the ratings we obtained cannot sim-
ply be explained by more coarse-grained senses. It
may hence be reasonable to pursue computational
models of word meaning that are graded, maybe
even models that do not rely on dictionary senses
at all (Erk and Pado, 2008).
Comparison to previous word sense annotation.
Our graded WSsim annotations do correlate with
traditional ?best fitting sense? annotations from
SemCor and SE-3; however, if annotators perceive
similarity between uses and senses as graded, tra-
ditional word sense annotation runs the risk of in-
troducing bias into the annotation.
Comparison to lexical substitutions. There is a
strong correlation between both Usim and WSsim
and the overlap in paraphrases that annotators gen-
erated for LEXSUB. This is very encouraging, and
especially interesting because LEXSUB annotators
freely generated paraphrases rather than selecting
them from a list.
6 Conclusions
We have introduced a novel annotation paradigm
for word sense annotation that allows for graded
judgments and for some variation between anno-
tators. We have used this annotation paradigm
in two experiments, WSsim and Usim, that shed
some light on the question of whether differences
between word usages are perceived as categorial
or graded. Both datasets will be made publicly
available. There was a high correlation between
annotator judgments within and across tasks, as
well as with previous word sense annotation and
with paraphrases proposed in the English Lex-
ical Substitution task. Annotators made ample
use of graded judgments in a way that cannot
be explained through more coarse-grained senses.
These results suggest that it may make sense to
evaluate WSD systems on a task of graded rather
than categorial meaning characterization, either
through dictionary senses or similarity between
uses. In that case, it would be useful to have more
extensive datasets with graded annotation, even
though this annotation paradigm is more time con-
suming and thus more expensive than traditional
word sense annotation.
As a next step, we will automatically cluster the
judgments we obtained in the WSsim and Usim
experiments to further explore the degree to which
the annotation gives rise to sense grouping. We
will also use the ratings in both experiments to
evaluate automatically induced models of word
meaning. The SemEval-2007 word sense induc-
tion task (Agirre and Soroa, 2007) already allows
for evaluation of automatic sense induction sys-
tems, but compares output to gold-standard senses
from OntoNotes. We hope that the Usim dataset
will be particularly useful for evaluating methods
which relate usages without necessarily producing
hard clusters. Also, we will extend the current
dataset using more annotators and exploring ad-
ditional lexicon resources.
Acknowledgments. We acknowledge support
from the UK Royal Society for a Dorothy Hodkin
Fellowship to the second author. We thank Sebas-
tian Pado for many helpful discussions, and An-
drew Young for help with the interface.
References
E. Agirre and A. Soroa. 2007. SemEval-2007
task 2: Evaluating word sense induction and dis-
17
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Repub-
lic.
J. Bybee and D. Eddington. 2006. A usage-based ap-
proach to Spanish verbs of ?becoming?. Language,
82(2):323?355.
J. Chen and M. Palmer. 2009. Improving English
verb sense disambiguation performance with lin-
guistically motivated features and clear sense dis-
tinction boundaries. Journal of Language Resources
and Evaluation, Special Issue on SemEval-2007. in
press.
K. Erk and S. Pado. 2008. A structured vector space
model for word meaning in context. In Proceedings
of EMNLP-08, Waikiki, Hawaii.
J. A. Hampton. 1979. Polymorphous concepts in se-
mantic memory. Journal of Verbal Learning and
Verbal Behavior, 18:441?461.
J. A. Hampton. 2007. Typicality, graded membership,
and vagueness. Cognitive Science, 31:355?384.
P. Hanks. 2000. Do word meanings exist? Computers
and the Humanities, 34(1-2):205?215(11).
E. H. Hovy, M. Marcus, M. Palmer, S. Pradhan,
L. Ramshaw, and R. Weischedel. 2006. OntoNotes:
The 90% solution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the ACL (NAACL-2006), pages
57?60, New York.
N. Ide and Y. Wilks. 2006. Making sense about
sense. In E. Agirre and P. Edmonds, editors,
Word Sense Disambiguation, Algorithms and Appli-
cations, pages 47?73. Springer.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English Senseval. Computers and the
Humanities, 34(1-2):15?48.
A. Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31(2):91?113.
A. Kilgarriff. 2006. Word senses. In E. Agirre
and P. Edmonds, editors, Word Sense Disambigua-
tion, Algorithms and Applications, pages 29?46.
Springer.
R. Krishnamurthy and D. Nicholls. 2000. Peeling
an onion: the lexicographers? experience of man-
ual sense-tagging. Computers and the Humanities,
34(1-2).
S. Landes, C. Leacock, and R. Tengi. 1998. Build-
ing semantic concordances. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
M. Lapata. 2006. Automatic evaluation of information
ordering. Computational Linguistics, 32(4):471?
484.
D. McCarthy and R. Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 48?53,
Prague, Czech Republic.
M. McCloskey and S. Glucksberg. 1978. Natural cat-
egories: Well defined or fuzzy sets? Memory &
Cognition, 6:462?472.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
3rd International Workshop on Semantic Evalua-
tions (SensEval-3) at ACL-2004, Barcelona, Spain.
G. Miller and W. Charles. 1991. Contextual correlates
of semantic similarity. Language and cognitive pro-
cesses, 6(1):1?28.
G. L. Murphy. 2002. The Big Book of Concepts. MIT
Press.
R. Navigli, K. C. Litkowski, and O. Hargraves.
2007. SemEval-2007 task 7: Coarse-grained En-
glish all-words task. In Proceedings of the 4th
International Workshop on Semantic Evaluations
(SemEval-2007), pages 30?35, Prague, Czech Re-
public.
P. Resnik and D. Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural
Language Engineering, 5(3):113?133.
E. Rosch and C. B. Mervis. 1975. Family resem-
blance: Studies in the internal structure of cate-
gories. Cognitive Psychology, 7:573?605.
E. Rosch. 1975. Cognitive representations of seman-
tic categories. Journal of Experimental Psychology:
General, 104:192?233.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Computational Linguistics,
8:627?633.
S. Sharoff. 2006. Open-source corpora: Using the net
to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
C. Stokoe. 2005. Differentiating homonymy and pol-
ysemy in information retrieval. In Proceedings of
HLT/EMNLP-05, pages 403?410, Vancouver, B.C.,
Canada.
18
Statistical Filtering and Subcategorization Frame Acquisition 
Anna Korhonen and  Genev ieve  Gor re l l  
Computer Laboratory, University of Cambridge 
Pembroke Street, Cambridge CB2 3QG, UK 
alk23@cl, cam. ac. uk, genevieve, gorrel l@netdecis ions,  co. uk 
Diana  McCar thy  
School of Cognitive and Computing Sciences 
University of Sussex, Brighton, BN1 9QH, UK 
dianam@cogs, usx. ac.  uk 
Abst rac t  
Research "into the automatic acquisition of 
subcategorization frames (SCFS) from corpora 
is starting to produce large-scale computa- 
tional lexicons which include valuable fre- 
quency information. However, the accuracy 
of the resulting lexicons shows room for im- 
provement. One significant source of error 
lies in the statistical filtering used by some re- 
searchers to remove noise from automatically 
acquired subcategorization frames. In this pa- 
per, we compare three different approaches to 
filtering out spurious hypotheses. Two hy- 
pothesis tests perform poorly, compared to 
filtering frames on the basis of relative fre- 
quency. We discuss reasons for this and con- 
sider directions for future research. 
1 In t roduct ion  
Subcategorization information is vital for suc- 
cessful parsing, however, manual develop- 
ment of large subcategorized lexicons has 
proved difficult because predicates change be- 
haviour between sublanguages, domains and 
over time. Additionally, manually devel- 
oped sucategorization lexicons do not provide 
the relative frequency of different SCFs for a 
given predicate, ssential in a probabilistic ap- 
proach. 
Over the past years acquiring subcatego- 
rization dictionaries from textual corpora has 
become increasingly popular. The different 
approaches (e.g. Brent, !991, 1993; Ushioda 
et al, 1993; Briscoe and Carroll, 1997; Man- 
ning, 1993; Carroll and Rooth, 1998; Gahl, 
1998; Lapata, 1999; Sarkar and Zeman, 2000) 
vary largely according to the methods used 
and the number of SCFS being extracted. Re- 
gardless of this, there is a ceiling on the perfor- 
mance of these systems at around 80% token 
recall 1 
zWhere token recall is the percentage .ofSCF to- 
kens in a sample of manually analysed text that were 
The approaches to extracting SCF informa- 
tion from corpora have frequently employed 
statistical methods for filtering (e.g. Brent, 
1993; Manning 1993; Briscoe and Carroll, 
1997; Lapata, 1999). This has been done to 
remove the noise that arises when dealing with 
naturally occurring data, and from mistakes 
made by the SCF acquisition system, for ex- 
ample, parser errors. 
Filtering is usually done with a hypothe- 
sis test, and frequently with a variation of 
the binomial filter introduced by Brent (1991, 
1993). Hypothesis testing is performed by for- 
mulating a null hypothesis, (H0), which is as- 
sumed true unless there is evidence to the con- 
trary. If there is evidence to the contrary, 
H0 is rejected and the alternative hypothe- 
sis (H1) is accepted. In SCF acquisition, H0 is 
that there is no association between aparticu- 
lar verb (verbj) and a SCF (SCFi), meanwhile 
H1 is that there is such an association. For 
SCF acquisition, the test is one-tailed since H1 
states the direction of the association, a pos- 
itive correlation between verbj and scfi. We 
compare the expected probability of scfi oc- 
curring with verbj if H0 is true, to the ob- 
served probability of co-occurrence obtained 
from the corpus data. If the observed proba- 
bility is greater than the expected probability 
we reject Ho and accept H1, and if not, we 
retain H0. 
Despite the popularity of this method, it 
has been reported as problematic. Accord- 
ing to one account (Briscoe and Carroll, 1997) 
the majority of errors arise because of the sta- 
tistical filtering process, which is reported to 
be particularly unreliable for low frequency 
SCFs (Brent, 1993; Briscoe and Carroll, 1997; 
Manning, 1993; Manning and Schiitze, 1999). 
Lapata (1999) reported that a threshold on 
the relative frequencies produced slightly bet- 
ter results than those achieved with a Brent- 
correctly acquired by the system. 
199 
style binomial filter when establishing SCFs for 
diathesis alternation detection. Lapata deter- 
mined thresholds for each SCF using the fre- 
quency of the SCF in COMLEX Syntax dictio- 
nary (Grishman et al, 1994). 
Adopting the SCF acquisition system of 
Briscoe and Carroll, we have experimented 
with an alternative hypothesis test, the bi- 
nomial log-likelihood ratio (LLR) test (Dun- 
ning, 1993). Sarkar and Zeman (2000) have 
also used this test when filtering SCFs auto- 
matically acquired for Czech. This test has 
been recommended for use in NLP since it 
does not assume a normal distribution, which 
invalidates many other parametric tests for 
use with natural language phenomena. LLR 
can be used in a form (-2logA) which is 
X 2 distributed. Moreover, this asymptote is 
appropriate at quite low frequencies, which 
makes the hypothesis test particularly useful 
when dealing with natural anguage phenom- 
ena, where low frequency events are common- 
place. 
A problem with using hypothesis testing for 
filtering automatically acquired SCFs is ob- 
taining a good estimation of the expected oc- 
currence of scfi with verbj. This is often 
performed using the unconditional distribu- 
tion, that is the probability distribution over 
all SCFS, regardless of the verb. It is as- 
sumed that verbj must occur with scfi sig- 
nificantly more than is expected given this 
estimate. Our paper addresses the problem 
that the conditional distribution, dependent 
on the verb, and unconditional distribution 
are rarely correlated. Therefore statistical fil- 
ters which assume such correlation for H0 will 
be susceptible to error, 
In this paper, we compare the results of 
the Brent style binomial filter of Briscoe and 
Carroll and the LLR filter to a simple method 
which uses a threshold on the relative frequen- 
cies of the verb and SCF combinations. We 
do this within the framework of the Briscoe 
and Carroll SCF acquisition system, which is 
described in section 2.1. The details of the 
two statistical filters are described in section 
2.2, along with the details of the threshold ap- 
plied to the relative frequencies output from 
the SCF acquisition system. The details of the 
experimental evaluation are supplied in sec- 
tion 3. We discuss our findings in section 3.3 
and conclude with directions for future work 
(section 4). 
2 Method  
2.1 F ramework  for SCF  Acquisit ion 
Briscoe and Carroll's (1997) verbal acquisition 
system distinguishes 163 SCFs and returns rel- 
ative frequencies for each SCF found for a given 
predicate. The SCFs are a superset of classes 
found in the Alvey NL Tools (ANLT) dictio- 
nary, Boguraev et al (1987) and the COML~X 
Syntax dictionary, Grishman et al (1994). 
They incorporate information about control 
of predicative arguments, as well as alterna- 
tions such as extraposition and particle move- 
ment. The system employs a shallow parser to 
obtain the subcategorization information. Po- 
tential SCF entries are filtered before the final 
SCF lexicon is produced. The filter is the only 
component of this system which we experi- 
ment with here. The three filtering methods 
which we compare are described below. 
2.2 Fi l ter ing Methods  
2.2.1 B inomia l  Hypothes is  Test 
Briscoe and Carroll (1997) used a binomial 
hypothesis test (BHT) to filter the acquired 
SCFs. They applied BHT as follows. The sys- 
tem recorded the total number of sets of SCF 
cues (n) found for a given predicate, and the 
number of these sets for a given SCF (ra). The 
system estimated the error probability (pe) 
that a cue for a SCF (scfi) occurred with a 
verb which did not take scfi. pe was esti- 
mated in two stages, as shown in equation 1. 
Firstly, the number of verbs which are mem- 
bers of the target SCF in the ANLT dictionary 
were extracted. This number was converted 
to a probability of class membership by divid- 
ing by the total number of verbs in the dic- 
tionary. The complement of this probability 
provided an estimate for the probability of a 
verb not taking scfi. Secondly, this proba- 
bility was multiplied by an estimate for the 
probability of observing the cue for scfi. This 
was estimated using the number of cues for i 
extracted from the Susanue corpus (Sampson, 
1995), divided by the total number of cues. 
pe = (1  - Iverbsl    i  cZass il I eSlc e l, for il (1) 
The probability of an event with probability p
happening exactly rn times out of n attempts 
is given by the following binomial distribution: 
20O 
n~ P(m,n,p) = m!(n-  m)! pro(1 _p)n-m (2) 
The probability of the event happening m or 
more times is: 
= (3) 
k=rn 
Finally, P(m+, n,p e) is the probabil ity that 
m or more occurrences of cues for scfi will oc- 
cur with a verb which is not a member ofscfi, 
given n occurrences of that verb. A threshold 
on this probability, P(m+,n, pe), was set at 
less than or equal to 0.05. This yielded a 95% 
or better confidence that a high enough pro- 
portion of cues for scfi have been observed for 
the verb to be legitimately assigned scfi. 
Other approaches which use a binomial fil- 
ter differ in respect of the calculation of the 
error probability. Brent (1993) estimated the 
error probabilities for each SCF experimen- 
tally from the behaviour of his SCF extrac- 
tor, which detected simple morpho-syntactic 
cues in the corpus data. Manning (1993) in- 
Creased the number of available cues at the ex- 
pense of the reliability of these cues. To main- 
tain high levels of accuracy, Manning applied 
higher bounds on the error probabilities for 
certain cues. These bounds were determined 
experimentally. A similar approach was taken 
by Briscoe, Carroll and Korhonen (1997) in a 
modification to the Briscoe and Carroll sys- 
tem. The overall performance was increased 
by changing the estimates of pe according to 
the performance of the system for the target 
SCF. In the work described here, we use the 
original BHT proposed by Briscoe and Carroll. 
2.2.2 The  B inomia l  Log L ike l ihood  
Rat io  as a S ta t i s t i ca l  F i l te r  
Dunning (1993) demonstrates the benefits of 
the LLR statistic, compared to Pearson's chi- 
squared, on the task of ranking bigram data. 
The binomial log-likelihood ratio test is 
simple to calculate. For each verb and SCF 
combination four counts are required. These 
are the number of times that: 
1. the target verb occurs with the target SCF 
(kl) 
2. the target verb occurs with any other SCF 
(nl - kl) 
3. any other verb occurs with the target SCF 
(k2) 
4. any other verb occurs with any other SCF 
- k2) 
The statistic -21ogA is calculated as follows:- 
log-likelihood = 
where 
2\[logL(pl, kl, nl ) 
+logL(p2, k2, n2) 
-logL(p, kl, nl) 
-logL(p, k2, n2) \] (4) 
logL(p, n, k) = k x logp + (n - k) x log(1 -p )  
and 
kl k2 kl + k2 
P l=- - ,  P2------ ,  P - -  nl n2 nl -4- n2 
The LLR statistic provides a score that re- 
flects the difference in (i) the number of bits 
it takes to describe the observed data, using 
pl = p(SCFIverb ) and p2 = p(SCFl-~verb ), 
and (ii) the number of bits it takes to de- 
scribe the expected ata using the probability 
p = p(scFlany verb). 
The LLR statistic detects differences be- 
tween pl  and p2. The difference could 
potentially be in either direction, but we are 
interested in LLRS where p l  > p2, i.e. where 
there is a positive association between the SCF 
and the verb. For these cases, we compared 
the value of -2logA to the threshold value 
obtained from Pearson's Chi-Squared table, 
to see if it was significant at the 95% level 2. 
2.2.3 Us ing  a Thresho ld  on the  
Re la t ive  Frequenc ies  as a 
Base l ine  
In order to examine the baseline performance 
of this system without employing any notion 
of the significance of the observations, we 
used a threshold on relative frequencies. This 
was done by extracting the SCFS, and rank- 
ing them in the order of the probability of 
their occurrence with the verb. The probabil- 
ities were estimated using a maximum likeli- 
hood estimate (MLE) from the observed rela- 
tive frequencies. A threshold, determined em- 
pirically, was applied to these probability esti- 
mates to filter out the low probability entries 
for each verb. .... 
2See (Gorrell, 1999) for details of this" method. 
201 
3 Eva luat ion  
3.1 Method  
To evaluate the different approaches, we took 
a sample of 10 million words of the BNC cor- 
pus (Leech, 1992). We extracted all sentences 
containing an occurrence of one of fourteen 
verbs 3. The verbs were chosen at random, 
subject to the constraint that they exhibited 
multiple complementation patterns. After the 
extraction process, we retained 3000 citations, 
on average, for each verb. The sentences con- 
taining these verbs were processed by the SCF 
acquisition system, and then we applied the 
three filtering methods described above. We 
also obtained results for a baseline without 
any filtering. 
The results were evaluated against a man- 
ual analysis of corpus data 4. This was ob- 
tained by analysing up to a maximum of 300 
occurrences for each of the 14 test verbs in 
LOB (Garside et al, 1987), Susanne and SEC 
(Taylor and Knowles, 1988) corpora. Follow- 
ing Briscoe and Carroll (1997), we calculated 
precision (percentage of SCFS acquired which 
were also exemplified in the manual analysis) 
and recall (percentage of the SCFs exemplified 
in the manual analysis which were acquired 
automatically). We also combined precision 
and recall into a single measure of overall per- 
formance using the F measure (MA.nniug and 
Schiitze, 1999). 
F = 2.precis ion.  recall (5) 
precision + recall 
3.2 Resu l ts  
Table 1 gives the raw results for the 14 verbs 
using each method. It shows the number of 
true positives (TP), .false positives (FP), and 
.false negatives (FN), as determined accord- 
ing to the manual analysis. The results for 
high frequency SCFs (above 0.01 relative fre- 
quency), medium frequency (between 0.001 
and 0.01) and low frequency (below 0.001) 
SCFs are listed respectively in the second, 
3These verbs were ask, begin, believe, cause, expect, 
find, give, help, like, move, produce, provide, seem, 
swing. 
4The importance of the manual analysis is outlined 
in Briscoe and Carroll (1997). We use the same man- 
ual analysis as Briscoe and Carroll, Le. one from the 
Susanne, LOB, and SEC corpora. A manual analysis of 
the BNC data might produce better results. However, 
since the BNC is a heterogeneous corpus we felt it was 
reasonable to test the data on a different corpus, which 
is also heterogeneous. 
third and fourth columns, and the final col- 
umn includes the total results for all frequency 
ranges. 
Table 2 shows precision and recall for the 14 
verbs and the F measure, which combines pre- 
cision and recall. We also provide the baseline 
results, if all SCFs were accepted. 
From the results given in tables 1 and 2, the 
MLE approach outperformed both hypothesis 
tests. For both BHT and LLR there was an 
increase in FNs at high frequencies, and an 
increase in FPs at medium and low frequen- 
cies, when compared to MLE. The number of 
errors was typically larger for LLR than BHT. 
The hypothesis tests reduced the number of 
FNS at medium and low frequencies, however, 
this was countered by the substantial increase 
in FPs that they gave. While BHT nearly al- 
ways acquired the three most frequent SCFs of 
verbs correctly, LLR tended to reject these. 
While the high number of FNS can be ex- 
plained by reports which have shown LLR to 
be over-conservative (Ribas, 1995; Pedersen, 
1996), the high number of FPs is surprising. 
Although theoretically, the strength of LLR 
lies in its suitability for low frequency data, 
the results displayed in table 1 do not suggest 
that the method performs better than BHT on 
low frequency frames. 
MLE thresholding produced better results 
than the two statistical tests used. Preci- 
sion improved considerably, showing that the 
classes occurring in the data with the high- 
est frequency are often correct. Although MLE 
thresholding clearly makes no attempt to solve 
the sparse data problem, it performs better 
than BHT or LLR overall. MLE is not adept at 
finding low frequency SCFS, however, the other 
methods are problematic in that they wrongly 
accept more than they correctly reject. The 
baseline, of accepting all SCFS, obtained a high 
recall at the expense of precision. 
3.3 D iscuss ion  
Our results indicate that MLE outperforms 
both hypothesis tests. There are two explana- 
tions for this, and these are jointly responsible 
for the results. 
Firstly, the SCF distribution is zipfian, as 
are many distributions concerned with nat- 
ural language (Manning and Schiitze, 1999). 
Figure 1 shows the conditional distribution 
for the verb find. This ~mf~ltered SCF prob- 
ability distribution was obtained from 20 M 
words of BNC data output from the SCF sys- 
202 
High Freq 
TP FP  FN 
BHT 75 29 23 
LLR 66 30 32 
MLE 92 31 6 
Med ium Freq Low Freq 
TP FP  FN TP FP  I FN 
11 37 31 4 23 15 
9 52 33 2 23 17 
0 0 42 0 0 19 
Totals 
TP FP I FN 
m 
90 89 69 
77 105 82 
92 31 67 
Table 1: Raw results for 14 test verbs 
~r31ff.t: Precision % Recall % F measure 
BHT 50.3 56.6 53.3 
LLR 42.3 48.4 45.1 
MLE 74.8 57.8 65.2 
baseline 24.3 83.5 37.6 
Table 2: Precision, Recall, and F measure 
0.1  
0.01 
& 
0.001 
0.0001 
. . . . . . . . .  i . . . . . . . .  
!. 
o 
I , r i i , , i , l  , , i i i i , 
10 100 
rank 
0.1  
0.01 
o.oo~ 
0.01~ 
10  4 
\ 
, , , , t , , r  , i , , i , , ,1  
10 100 
rank 
Figure 1: Hypothesised SCF distribution for 
find 
tern. The unconditional distribution obtained 
from the observed istribution of SCFs in the 
20 M words of BNC is shown in figure 2. The 
figures show SCF rank on the X-axis versus 
SCF frequency on the Y-axis, using logarith- 
mic scales. The line indicates the closest Zipf- 
like power law fit to the data. 
Secondly, the hypothesis tests make the 
false assumption (H0) that the unconditional 
and conditional distributions are correlated. 
The fact that a significant improvement in
performance is made by correcting the prior 
probabilities according to the performance of
the system (Briscoe, Carroll and Korhonen, 
Figure 2: Hypothesised unconditional SCF dis- 
tribution 
1997) suggests the discrepancy between the 
unconditional and the conditional distribu- 
tions. 
We examined the correlation between the 
manual analysis for the 14 verbs, and the 
unconditional distribution of verb types over 
all SCFs estimated from the ANLT using the 
Spearman Rank Correlation Coefficient. The 
results included in table 3 show that only a 
moderate correlation was found averaged over 
all verb types. 
Both LLR and BHT work by comparing the 
observed value of p(scfi\[verbj) to that ex- 
pected by chance. They both use the observed 
203 
\[ Verb Rank  Correlation 
ask 0.10 
begin 0.83 
believe 0.77 
cause 0.19 
expect 
find 
0.45 
0.33 
give 0.06 
help 0.43 
like 0.56 
move 0.53 
produce 0.95 
provide 0.65 
seem 0.16 
swing 
Average 
0.50 
0.47 
Table 3: Rank correlation between the condi- 
tional SCF distributions of the test verbs and 
the unconditional distribution 
value for p(sc.filverbj) from the system's out- 
put, and they both use an estimate for the un- 
conditional probability distribution (p(scfi)) 
for estimating the expected probability. They 
differ in the way that the estimate for the un- 
conditional probability is obtained, and the 
way that it is used in hypothesis testing. 
For  BHT, the null hypothesis i that the ob- 
served value ofp(scfiIverbj) arose by chance, 
because of noise in the data. We estimate the 
probability that the value observed could have 
arisen by chance using p(m+,  n,pe), pe is cal- 
culated using: 
? the SCF acquisition system's raw (until- 
tered) estimate for the unconditional dis- 
tribution, which is obtained from the Su- 
sanne corpus and 
? the ANLT estimate of the unconditional 
distribution of a verb not taking scf~, 
across all SCFs 
For LLR, both the conditional (pl) and un- 
conditional distributions (p2) are estimated 
from the BNC data. The unconditional proba- 
bility distribution uses the occurrence of scfi 
with any verb other than our target. 
The binomial tests look at one point in the 
SCF distribution at a time, for a given verb. 
The expected value is determined using the 
unconditional distribution, on the assumption 
that if the null hypothesis true then this dis- 
tribution will correlate with the conditional 
distribution. However, this is rarely the case. 
Moreover, because of the zipfian nature of 
the distributions, the frequency differences at 
any point can be substantial. In these exper- 
iments, we used one-tailed tests because we 
were looking for cases where there was a pos- 
itive association between the SCF and verb, 
however, in a two-tailed test the null hypoth- 
esis would rarely be accepted, because of the 
substantial differences in the conditional and 
unconditional distributions. 
A large number of false negatives occurred 
for high frequency SCFs because the probabil- 
ity we compared them to was too high. This 
probability was estimated from the combina- 
tion of many verbs genuinely occurring with 
the frame in question, rather than from an es- 
timate of background noise from verbs which 
did not occur with the frame. We did not use 
an estimate from verbs which do not take the 
SCF, since this would require a priori knowl- 
edge about the phenomena that we were en- 
deavouring to acquire automatically. For LLR 
the unconditional probability estimate (p2) 
was high, simply because this SCF was a com- 
mon one, rather than because the data was 
particularly noisy. For BHT, R e was likewise 
too high as the SCF was also common in the 
Susanne data. The ANLT estimate went some- 
way to compensating for this, thus we ob- 
tained fewer false negatives with BHT than 
LLR. 
A large number of false positives occurred 
for low frequency SCFs because the estimate 
for p(scf) was low. This estimate was more 
readily exceeded by the conditional estimate. 
For BHT false positives arose because of the 
low estimate of p(scf) (from Susanne) and 
because the estimate of p(-,SCF) from ANLT 
did not compensate enough for this. For LLR, 
there was no mean~ to compensate for the fact 
that p2 was lower than pl .  
In contrast, MLE did not compare two dis- 
tributions. Simply rejecting the low frequency 
data produced better results overall by avoid- 
ing the false positives with the low frequency 
data, and the false negatives with the high 
frequency data. 
4 Conc lus ion  
This paper explored three possibilities for fil- 
tering out the SCF entries produced by a SCF 
acquisition system. These were (i) a version 
of Brent's binomial filter, commonly used for 
this purpose, (ii) the binomial og-likelihood 
204 
ratio test, recommended for use with low fre- 
quency data and (iii) a simple method using 
a threshold on the MLEs of  the SCFS output 
from the system. Surprisingly, the simple MLE 
thresholding method worked best. The BHT 
and LLR both produced an astounding mlm- 
ber of FPs, particularly at low frequencies. 
Further work on handling low frequency 
data in SCF acquisition is warranted. A non- 
parametric statistical test, such as Fisher's ex- 
act test, recommended by Pedersen (1996), 
might improve on the results obtained using 
parametric tests. However, it seems from our 
experiments hat it would be better to avoid 
hypothesis tests that make use of the uncon- 
ditional distribution. 
One possibility is to put more effort into the 
estimation of pe, and to avoid use of the un- 
conditional distribution for this. In some re- 
cent experiments, we tried optimising the es- 
timates for pe depending on the performance 
of the system for the target SCF, using the 
method proposed by Briscoe, Carroll and Ko- 
rhonen (1997). The estimates of pe were ob- 
tained from a training set separate to the held- 
out BNC data used for testing. Results using 
the new estimates for pe gave an improvement 
of 10% precision and 6% recall, compared to 
the BHT results reported here. Nevertheless, 
the precision result was 14% worse for preci- 
sion than MLE, though there was a 4% im- 
provement in recall, making the overall per- 
formance 3.9 worse than MLE according to the 
F measure. Lapata (1999) also reported that 
a simple relative frequency cut off produced 
slightly better esults than a Brent style BHT. 
If MLE thresholding persistently achieves 
better results, it would be worth investi- 
gating ways of handling the low frequency 
data, such as smoothing, for integration with 
this method. However, more sophisticated 
smoothing methods, which back-off to an un- 
Conditional distribution, will also suffer from 
the lack of correlation between conditional 
and unconditional SCF distributions. Any sta- 
tistical test would work better at low frequen- 
cies than the MLE, since this simply disregards 
all low frequency SCFs. In our experiments, ff 
we had used MLE only for the high frequency 
data, and BHT for medium and low, then over- 
all we would have had 54% precision and 67% 
recall. It certainly seems worth employing hy- 
pothesis tests which do not rely on the un- 
conditional distribution for the low frequency 
SCFS. 
5 Acknowledgements  
We thank Ted Briscoe for many helpful dis- 
cussions and suggestions concerning this work. 
We also acknowledge Yuval Krymolowski for 
useful comments on this paper. 
Re ferences  
Boguraev, B., Briscoe, E., Carroll, J., Carter, 
D. and Grover, C. 1987. The derivation of a 
grammatically-indexed lexicon from the Long- 
man Dictionary of Contemporary English. In 
Proceedings of the 25th Annual Meeting of 
the Association for Computational Linguis- 
tics, Stanford, CA. 193-200. 
Brent, M. 1991. Automatic acquisition of 
subcategorization frames from untagged text. 
In Proceedings of the 29th Annual Meeting 
of the Association for Computational Linguis- 
tics, Berkeley, CA. 209-214. 
Brent, M. 1993. From gra.mmar to lexicon: 
unsupervised learning of lexical syntax. Com- 
putational Linguistics 19.3: 243-262. 
Briscoe, E.J. and J. Carroll 1997. Automatic 
extraction of subcategorization from corpora. 
In Proceedings of the 5th ACL Conf. on Ap- 
plied Nat. Lg. Proc., Washington, DC. 356- 
363. 
Briscoe, E., Carroll, J. and Korhonen, A. 
1997. Automatic extraction of subcategoriza- 
tion frames from corpora - a framework and 
3 experiments. '97 Sparkle WP5 Deliverable, 
available in http://www.ilc.pi.cnr.it/. 
Carroll, G. and Rooth, M. 1998. Valence 
induction with a head-lexicalized PCFG. In 
Proceedings of the 3rd Conference on Empir- 
ical Methods in Natural Language Processing, 
Granada, Spain. 
Dunning, T. 1993. Accurate methods for the 
Statistics of Surprise and Coincidence. Com- 
putational Linguistics 19.1: 61-74. 
Gahl, S. 1998. Automatic extraction of sub- 
corpora based on subcategorization frames 
from a part-of-speech tagged corpus. In Pro- 
ceedings of the COLING-A CL'98, Montreal, 
Canada. 
Garside, R., Leech, G. and Sampson, G. 1987. 
The computational nalysis of English: A 
corpus-based approach. Longman, London. 
Gorrell, G. 1999. Acquiring Subcategorisation 
from Textual Corpora. MPhil dissertation, 
University of Cambridge, UK. 
205 
Grishman, R., Macleod, C. and Meyers, A. 
1994. Comlex syntax: building a computa- 
tional lexicon. In Proceedings of the Interna- 
tional Conference on Computational Linguis- 
tics, COLING-94, Kyoto, Japan. 268-272. 
Lapata, M. 1999. Acquiring lexical gener- 
alizations from corpora: A case study for 
diathesis alternations. In Proceedings of the 
37th Annual Meeting of the Association for 
Computational Linguistics, Maryland. 397- 
404. 
Leech, G. 1992. 100 million words of English: 
the British National Corpus. Language Re- 
search 28(1): 1-13. 
Manning, C. 1993. Automatic acquisition of 
a large subcategorization dictionary from cor- 
pora. In Proceedings of the 31st Annual Meet- 
ing of the Association .for Computational Lin- 
guistics, Columbus, Ohio. 235-242. 
Manning, C. and Schiitze, H. 1999. Founda- 
tions of Statistical Natural Language Process- 
ing. MIT Press, Cambridge MA. 
Pedersen, T. 1996. Fishing for Exactness. In 
Proceedings of the South-Central SAS Users 
Group Conference SCSUG-96, Austin, Texas. 
Ribas, F. 1995. On Acquiring Appropriate Se- 
lectional Restrictions from Corpora Using a 
Semantic Taxonomy. Ph.D thesis, University 
of Catalonia. 
Sampson, G. 1995. English for the computer. 
Oxford University Press, Oxford UK. 
Sarkar, A. and Zeman, D. 2000. Auto- 
matic Extraction of Subcategorization Frames 
for Czech. In Proceedings of the Inter- 
national Conference on Computational Lin- 
guistics, COLING-O0, Saarbrucken, Germany. 
691-697. 
Taylor, L. and Knowles, G. 1988. Manual 
of information to accompany the SEC cor- 
pus: the machine-readable corpus of spoken 
English. University of Lancaster, UK, Ms. 
Ushioda, A., Evans, D., Gibson, T. and 
Waibel, A. 1993. The automatic acquisition of 
frequencies of verb subcategorization frames 
from tagged corpora. In Boguraev, B. and 
Pustejovsky, J. eds. SIGLEX A CL Workshop 
on the Acquisition of Lexieal Knowledge .from 
Text. Columbus, Ohio: 95-106. 
206 
i 
 
	 	

	
Using Automatically Acquired Predominant Senses for Word Sense
Disambiguation
Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
 
dianam,robk,juliewe,johnca  @sussex.ac.uk
Abstract
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The first (or predominant)
sense heuristic assumes the availability of hand-
tagged data. Whilst there are hand-tagged corpora
available for some languages, these are relatively
small in size and many word forms either do not
occur, or occur infrequently. In this paper we in-
vestigate the performance of an unsupervised first
sense heuristic where predominant senses are ac-
quired automatically from raw text. We evaluate on
both the SENSEVAL-2 and SENSEVAL-3 English all-
words data. For accurate WSD the first sense heuris-
tic should be used only as a back-off, where the evi-
dence from the context is not strong enough. In this
paper however, we examine the performance of the
automatically acquired first sense in isolation since
it turned out that the first sense taken from SemCor
outperformed many systems in SENSEVAL-2.
1 Introduction
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account (McCarthy et al, 2004). The high
performance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where evi-
dence from the context is not sufficient (Hoste et al,
2001).
The first sense heuristic is a powerful one. Us-
ing the first sense listed in SemCor on the SENSE-
VAL-2 English all-words data we obtained the re-
sults given in table 1, (where the PoS was given by
the gold-standard data in the SENSEVAL-2 data it-
self). 1 Recall is lower than precision because there
are many words which do not occur in SemCor. Use
1We did not include items which were tagged ?U?
(unassignable) by the human annotators.
PoS precision recall baseline
Noun 70 60 45
Verb 48 44 22
Adjective 71 59 44
Adverb 83 79 59
All PoS 67 59 41
Table 1: The SemCor first sense on the SENSEVAL-
2 English all-words data
of the first sense listed in WordNet gives 65% pre-
cision and recall for all PoS on these items. The
fourth column on table 1 gives the random base-
line which reflects the polysemy of the data. Ta-
ble 2 shows results obtained when we use the most
common sense for an item and PoS using the fre-
quency in the SENSEVAL-2 English all-words data
itself. Recall is lower than precision since we only
use the heuristic on lemmas which have occurred
more than once and where there is one sense which
has a greater frequency than the others, apart from
trivial monosemous cases. 2 Precision is higher in
table 2 than in table 1 reflecting the difference be-
tween an a priori first sense determined by Sem-
Cor, and an upper bound on the performance of this
heuristic for this data. This upper bound is quite
high because of the very skewed sense distributions
in the test data itself. The upper bound for a docu-
ment, or document collection, will depend on how
homogenous the content of that document collec-
tion is, and the skew of the word sense distributions
therein. Indeed, the bias towards one sense for a
given word in a given document or discourse was
observed by Gale et al (1992).
Whilst a first sense heuristic based on a sense-
tagged corpus such as SemCor is clearly useful,
there is a case for obtaining a first, or predomi-
nant, sense from untagged corpus data so that a WSD
2If we include polysemous items that have only occurred
once in the data we obtain a precision of 92% and a recall of
85% over all PoS.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
PoS precision recall baseline
Noun 95 73 45
Verb 79 43 22
Adjective 88 59 44
Adverb 91 72 59
All PoS 90 63 41
Table 2: The SENSEVAL-2 first sense on the SEN-
SEVAL-2 English all-words data
system can be tuned to a given genre or domain
(McCarthy et al, 2004) and also because there will
be words that occur with insufficient frequency in
the hand-tagged resources available. SemCor com-
prises a relatively small sample of 250,000 words.
There are words where the first sense in WordNet is
counter-intuitive, because this is a small sample, and
because where the frequency data does not indicate
a first sense, the ordering is arbitrary. For exam-
ple the first sense of tiger in WordNet is audacious
person whereas one might expect that carnivorous
animal is a more common usage.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
are investigating a method of automatically ranking
WordNet senses from raw text, with no reliance on
manually sense-tagged data such as that in SemCor.
The paper is structured as follows. We discuss
our method in the following section. Section 3 de-
scribes an experiment using predominant senses ac-
quired from the BNC evaluated on the SENSEVAL-2
English all-words task. In section 4 we present our
results on the SENSEVAL-3 English all-words task.
We discuss related work in section 5 and conclude
in section 6.
2 Method
The method is described in (McCarthy et al, 2004),
which we summarise here. We acquire thesauruses
for nouns, verbs, adjectives and adverbs based on
the method proposed by Lin (1998) using grammat-
ical relations output from the RASP parser (Briscoe
and Carroll, 2002). The grammatical contexts used
are listed in table 3, but there is scope for extending
or restricting the contexts for a given PoS.
We use the thesauruses for ranking the senses of
the target words. Each target word (  ) e.g. plant
in the thesaurus is associated with a list of nearest
PoS grammatical contexts
Noun verb in direct object or subject relation
adjective or noun modifier
Verb noun as direct object or subject
Adjective modified noun, modifing adverb
Adverb modified adjective or verb
Table 3: Grammatical contexts used for acquiring
the thesauruses
neighbours ( 	
 ) with distributional similarity
scores (  ) e.g. factory 0.28, refinery 0.17,
tree 0.14 etc... 3 Distributional similarity is a mea-
sure indicating the degree that two words, a word
and its neighbour, occur in similar contexts. The
neighbours reflect the various senses of the word
( The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Relating WordNet Senses for Word Sense Disambiguation
Diana McCarthy
Department of Informatics,
University of Sussex
Brighton BN1 9QH, UK
dianam@sussex.ac.uk
Abstract
The granularity of word senses in current
general purpose sense inventories is of-
ten too fine-grained, with narrow sense
distinctions that are irrelevant for many
NLP applications. This has particularly
been a problem with WordNet which is
widely used for word sense disambigua-
tion (WSD). There have been several at-
tempts to group WordNet senses given a
number of different information sources
in order to reduce granularity. We pro-
pose relating senses as a matter of de-
gree to permit a softer notion of relation-
ships between senses compared to fixed
groupings so that granularity can be var-
ied according to the needs of the applica-
tion. We compare two such approaches
with a gold-standard produced by humans
for this work. We also contrast this gold-
standard and another used in previous re-
search with the automatic methods for re-
lating senses for use with back-off meth-
ods for WSD.
1 Introduction
It is likely that accurate word-level semantic dis-
ambiguation would benefit a number of different
types of NLP application; however it is gener-
ally acknowledged by word sense disambiguation
(WSD) researchers that current levels of accuracy
need to be improved before WSD technology can
usefully be integrated into applications (Ide and
Wilks, in press). There are at least two major prob-
lems facing researchers in this area. One major
problem is the lack of sufficient training data for
supervised WSD systems. One response to this is
WNs# gloss
1 your basis for belief or disbelief; knowledge on
which to base belief; ?the evidence that smoking
causes lung cancer is very compelling?
2 an indication that makes something evident;
?his trembling was evidence of his fear?
3 (law) all the means by which any alleged
matter of fact whose truth is investigated at
judicial trial is established or disproved
Figure 1: The senses of evidence in WordNet
to exploit the natural skew of the data and focus on
finding the first (predominant) sense from a sam-
ple of text (McCarthy et al, 2004). Further con-
textual WSD may be required, but the technique
provides a useful unsupervised back-off method.
The other major problem for WSD is the granu-
larity of the sense inventory since a pre-existing
lexical resource is often too fine-grained, with nar-
row sense distinctions that are irrelevant for the in-
tended application. For example, WordNet (Fell-
baum, 1998) which is widely used and publicly
available, has a great many subtle distinctions that
may in the end not be required. For example, in
figure 1 we show the three senses (WNs#) for ev-
idence from WordNet version 1.7. 1 These are all
clearly related.
One promising approach for improving accu-
racy is to disambiguate to a coarser-grained inven-
tory, which groups together the related senses of
a word. This can be done either by defining the
inventory specifically for the application, which
might be most appropriate for machine translation,
where correspondences across languages could
1We use WordNet 1.7 throughout this paper since the re-
sources we use for evaluation were produced for this version.
17
determine the inventory (Resnik and Yarowsky,
2000). There are however many systems using
man-made resources, particularly WordNet, which
have other purposes in mind, such as entailment
for applications such as question-answering and
information-extraction (Dagan et al, 2005). There
have been several attempts to group WordNet
senses using various different types of information
sources. This paper describes work to automati-
cally relate WordNet word senses using automati-
cally acquired thesauruses (Lin, 1998) and Word-
Net similarity measures (Patwardhan and Peder-
sen, 2003).
This work proposes using graded word sense re-
lationships rather than fixed groupings (clusters).
Previous research has focused on clustering Word-
Net senses into groups. One problem is that to
do this a stopping condition is required such as
the number of clusters required for each word.
This has been done with the numbers determined
by the gold-standard for the purposes of evalu-
ation (Agirre and Lopez de Lacalle, 2003) but
ultimately the right number of classes for each
word cannot usually be predetermined even if one
knows the application, unless only a sample of
words are being handled. In cases where a gold-
standard is provided by humans it is clear that
further relationships could be drawn. For exam-
ple, in the groups (hereafter referred to as SEGR)
made publicly available for the SENSEVAL-2 En-
glish lexical sample (Kilgarriff, 2001) (hereafter
referred to as SEVAL-2 ENG LEX) child is grouped
as shown in table 1. Whilst it is perfectly reason-
able the grouping decision was determined by the
?youth? vs ?descendant? distinction, the relation-
ships between non-grouped senses, notably sense
numbers 1 and 2 are apparent. It is quite possible
that these senses will share contextual cues use-
ful for WSD and distinction between the two might
not be relevant in a given application, for exam-
ple because they are translated in the same way
(nin?o/a in Spanish can mean both young boy/girl
and son/daughter) or have common substitutions
(boy/girl can be used as both offspring or young
person). Instead of clustering senses into groups
we evaluate 2 methods that produce ranked lists of
related senses for each target word sense. We refer
to these as RLISTs. Such listings resemble nearest
neighbour approaches for automatically acquired
thesauruses. They allow for a sense to be related
to others which may not themselves be closely re-
WNs# SEGR gloss
1 1 a young person
2 2 a human offspring
3 1 an immature childish person
4 2 a member of a clan or tribe
Table 1: SEGR for child in SEVAL-2 ENG LEX
lated. Since only a fixed number of senses are de-
fined for each word, the RLISTs include all senses
of the word. A cut-off can then be determined for
any particular application.
Previous research on clustering word senses
has focused on comparison to the SEGR gold-
standard. We evaluate the RLISTs against a new
gold-standard produced by humans for this re-
search since the SEGR does not have documenta-
tion with figures for inter-tagger agreement. As
well as evaluating against a gold-standard, we also
look at the effect of the RLISTs and the gold-
standards themselves on WSD. Since the focus of
this paper is not the WSD system, but the sense
inventory, we use a simple WSD heuristic which
uses the first sense of a word in all contexts, where
the first sense of every word is specified by a re-
source. While contextual evidence is required for
accurate WSD, it is useful to look at this heuris-
tic since it is so widely used as a back-off model
by many systems and is hard to beat on an all-
words task (Snyder and Palmer, 2004). We con-
trast the performance of first sense heuristics i)
from SemCor (Miller et al, 1993) and ii) derived
automatically from the BNC following (McCarthy
et al, 2004) and also iii) an upper-bound first sense
heuristic extracted from the test data.
The paper is organised as follows. In the next
section we describe some related work. In sec-
tion 3 we describe the two methods we will use
to relate senses. Our experiments are described in
section 4. In 4.1 we describe the construction of a
new gold-standard produced using the same sense
inventory used for SEGR, and give inter-annotator
agreement figures for the task. In section 4.2 we
compare our methods to the new gold-standard
and in section 4.3 we investigate how much effect
coarser grained sense distinctions have onWSD us-
ing naive first sense heuristics. We follow this with
a discussion and conclusion.
18
2 Related Work
There is a significant amount of previous work
on grouping WordNet word senses using a num-
ber of different information sources, such as pred-
icate argument structure (Palmer et al, forthcom-
ing), information from WordNet (Mihalcea and
Moldovan, 2001; Tomuro, 2001) 2 and other lex-
ical resources (Peters and Peters, 1998) transla-
tions, system confusability, topic signature and
contextual evidence (Agirre and Lopez de Lacalle,
2003). There is also work on grouping senses
of other inventories using information in the in-
ventory (Dolan, 1994) along with information re-
trieval techniques (Chen and Chang, 1998).
One method presented here (referred to as DIST
and described in section 3) relates most to that
of Agirre and Lopez de Lacalle (2003). They
use contexts of the senses gathered directly from
either manually sense tagged corpora, or using
instances of ?monosemous relatives? which are
monosemous words related to one of the target
word senses in WordNet. We use contexts of
occurrence indirectly. We obtain ?nearest neigh-
bours? which occur in similar contexts to the tar-
get word. A vector is created for each word sense
with a WordNet similarity score between the sense
and each nearest neighbour of the target word. 3
While related senses may not have a lot of shared
contexts directly, because of sparse data, they may
have semantic associations with the same subset
of words that share similar distributional contexts
with the target word. This method avoids re-
liance on sense-tagged data or monosemous rela-
tives because the distributional neighbours can be
obtained automatically from raw text.
Our other method relates to the findings of
Kohomban and Lee (2005). We use the Jiang-
Conrath score (JCN) in the WordNet Similarity
Package. This is a distance measure between
WordNet senses given corpus frequency counts
and the structure of the WordNet hierarchy. It is
described in more detail below. Kohomban and
Lee (2005) get good results on disambiguation of
the SENSEVAL all-words tasks using the 25 unique
beginners from the WordNet hierarchy for train-
ing a coarse-grained WSD system and then using a
first sense heuristic (provided using the frequency
2Mihalcea and Moldovan group WordNet synonym sets
(synsets) rather than word senses.
3We have not tried using these vectors for relating senses
of different words, but leave that for future research.
data in SemCor) to determine the fine-grained out-
put. This shows that the structure of WordNet is
indeed helpful when selecting coarse senses for
WSD. We use the JCN measure to contrast with
our DIST measure which uses a combination of
distributional neighbours and JCN. We have exper-
imented only with nouns to date, although in prin-
ciple our method can be extended for other POS.
3 Methods for producing RLISTs
JCN This is a measure from the WordNet sim-
ilarity package (Patwardhan and Pedersen, 2003)
originally proposed as a distance measure (Jiang
and Conrath, 1997). JCN uses corpus data to pop-
ulate classes (synsets) in the WordNet hierarchy
with frequency counts. Each synset is incremented
with the frequency counts from the corpus of all
words belonging to that synset, directly or via the
hyponymy relation. The frequency data is used to
calculate the ?information content? (IC) of a class
(IC(s) = ?log(p(s))) and with this, Jiang and
Conrath specify a distance measure:
Djcn(s1, s2) = IC(s1)+IC(s2)?2?IC(s3)
where the third class (s3) is the most informative,
or most specific, superordinate synset of the two
senses s1 and s2. This is transformed from a dis-
tance measure in the WN-Similarity package by
taking the reciprocal:
jcn(s1, s2) = 1/Djcn(s1, s2)
We use raw BNC data for calculating IC values.
DIST We use a distributional similarity mea-
sure (Lin, 1998) to obtain a fixed number (50)
of the top ranked nearest neighbours for the tar-
get nouns. For input we used grammatical relation
data extracted using an automatic parser (Briscoe
and Carroll, 2002). We used the 90 million words
of written English from the British National Cor-
pus (BNC) (Leech, 1992). For each noun we
collect co-occurrence triples featuring the noun
in a grammatical relationship with another word.
The words and relationships considered are co-
occurring verbs in the direct object and subject
relation, the modifying nouns in noun-noun rela-
tions and the modifying adjectives in adjective-
noun relations. Using this data, we compute the
distributional similarity proposed by Lin between
each pair of nouns, where the nouns have at least
10 triples. Each noun (w) is then listed with k (=
50) most similar nouns (the nearest neighbours).
The nearest neighbours for a target noun (w)
share distributional contexts and are typically se-
19
..jcn( president)>
Vs4
3 chairperson
4 electric chair
1 seat
chair
2 professorship
senses
nearest neighbours
stool, chairman.......president
= <jcn(V professorship stool), jcn( ...professorship professorshipchairman),
president)>..jcn(chairpersonchairman),...chairpersonstool), jcn(chairpersonV
president)>electric chair..jcn(...chairman),electric chairstool), jcn(electric chair= <jcn(
= <jcn(
= <jcn( seat, stool), jcn( seat,V chairman), .....jcn(seat, president)>s
s
s
1
2
3
Figure 2: Vectors for chair
mantically related to the various senses (Sw) of
w. The relationships between the various senses
are brought out by the shared semantic relation-
ships with the neighbours. For example the top
nearest neighbours of chair include: stool, bench,
chairman, furniture, staff, president. The senses of
chair are 1 seat, 2 professorship, 3 chairperson
and 4 electric chair. The seat and electric chair
senses share semantic relationships with neigh-
bours such as stool, bench, furniture whilst the
professorship and chairperson senses are related
via neighbours such as chairman, president, staff.
The semantic similarity between a neighbour
(n) e.g. stool and a word sense (si ? Sw) e.g.
electric chair is measured using the JCN measure
described above.
To relate the set of senses (Sw) of a word (w)
we produce a vector ~Vsi = (f1...fk) with k fea-
tures for each si ? Sw. The jth feature in ~Vsi
is the highest JCN score between all senses of the
jth neighbour and si. Figure 2 illustrates this
process for chair. In contrast to using JCN be-
tween senses directly, the nearest neighbours per-
mit senses in unrelated areas of WordNet to be re-
lated e.g. painting - activity and painting - ob-
ject since both senses may have neighbours such
as drawing in common. The vectors are used to
produce RLISTs for each si. To produce the RLIST
of a sense si of w we obtain a value for the Spear-
man rank correlation coefficient (r) between the
vector for si and that for each of the other senses
of w (sl ? Sw, where l 6= i). r is calculated by
obtaining rankings for the neighbours on ~Vsi and~Vsl using the JCN values for ranking. We then list
si with the other senses ordered according to the r
value, for example the RLIST for sense 1 of chair
is [4 (0.50), 3 (0.34), 2 (0.20)] where the sense
number is indicated before the bracketed r score.
4 Experiments
For our experiments we use the same set of 20
nouns used by Agirre and Lopez de Lacalle
(2003). The gold standard used in that work was
SEGR. These groupings were released for SEN-
SEVAL-2 but we cannot find any documentation
on how they were produced or on inter-annotator
agreement. 4 We have therefore produced a new
gold-standard (referred to as RS) for these nouns
which we describe in section 4.1. We compare
the results of our methods for relating senses and
SEGR to RS. We then look at the performance of
both the gold-standard groupings (SEGR and RS)
compared to our automatic methods for coarser
grained WSD of SEVAL-2 ENG LEX using some
first sense heuristics.
4.1 Creating a Gold Standard
To create the gold-standard we gave 3 native en-
glish speakers a questionnaire with all possible
pairings of WordNet 1.7 word senses for each of
the 20 nouns in turn. The pairs were derived from
all possible combinations of senses of the given
noun and the judges were asked to indicate a ?re-
lated?, ?unrelated? or don?t know response for
each pair. 5 This task allows a sense to be re-
lated to others which are not themselves related.
The ordering of the senses was randomised and
fake IDs were generated instead of using the sense
numbers provided with WordNet to avoid possi-
ble bias from indications of sense predominance.
The words were presented one at a time and each
combination of senses was presented along with
the WordNet gloss. 6 Table 2 provides the pair-
wise agreement (PWA) figures for each word along
with the overall PWA figure. The number of word
senses for each noun is given in brackets. Overall,
more relationships were identified compared to the
rather fine-grained classes in SEGR, although there
was some variation. The proportion of related
items for our three judges were 52.2%, 56.5% and
22.6% respectively. Given this variation, the last
row gives the pairwise agreement for pairs where
the more lenient judge has said the pair is un-
related. These figures are reasonable given that
humans differ in their tendency to lump or split
4We have asked Agirre and Lopez de Lacalle as well as
those involved with the original SENSEVAL-2 task.
5We are grateful to Adam Kilgarriff for suggesting the
task.
6We will make the questionnaire publicly available with
the gold standard.
20
word (#senses) PWA
art (4) 44.44
authority (7) 52.38
bar (13) 87.07
bum (4) 100.00
chair (4) 43.75
channel (7) 46.03
child (4) 66.67
circuit (6) 46.67
day (10) 64.44
facility (5) 86.67
fatigue (4) 44.44
feeling (6) 42.22
hearth (3) 55.56
mouth (8) 40.48
nation (4) 100.00
nature (5) 73.33
post (8) 92.86
restraint (6) 42.22
sense (5) 73.33
stress (5) 73.33
overall PWA 66.94
given leniency 88.10
Table 2: Pairwise agreement %
senses and the fact that figures for sense annotation
with three judges (as opposed to two, with a third
to break ties) are reported in this region (Koeling
et al, 2005). Again, there are no details on anno-
tation and agreement for SEGR.
4.2 Agreement of automatic methods with RS
Figure 3 shows the PWA of the automatic methods
JCN and DIST when calculated against the RS gold-
standard at various threshold cut-offs. The differ-
ence of the best performance for these two meth-
ods (61.1% DIST and 62.2% for JCN) are not statis-
tically significant (using the chi-squared test). The
baseline which assumes that all pairs are unrelated
is 54.1%. If we compare the SEGR to RS we get
68.9% accuracy. 7 This shows that the SEGR ac-
cords with RS more than the automatic methods.
4.3 Application to SEVAL-2 ENG LEX
We used the same words as in the experiment
above and applied our methods as back-off to
naive WSD heuristics on the SEVAL-2 ENG LEX
7Since these are groupings, there is only one possible an-
swer and no thresholds are applied.
 44
 46
 48
 50
 52
 54
 56
 58
 60
 62
 64
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
ac
cu
ra
cy
threshold
"JCN"
"DISTSIM"
Figure 3: Accuracy of match of RS to JCN and
DIST
test data. 8 Using predominant senses is use-
ful as a back-off method where local context is
not sufficient. Disambiguation is performed us-
ing the first sense heuristic from i) SemCor (Sem-
cor FS) ii) automatic rankings from the BNC pro-
duced using the method proposed by McCarthy et
al. (2004) (Auto FS) and iii) an upper-bound first
sense heuristic from the SEVAL-2 ENG LEX data
itself (SEVAL-2 FS). This represents how well the
method would perform if we knew the first sense.
The results are shown in table 3. The accu-
racy figures are equivalent to both recall and pre-
cision as there were no words in this data with-
out a first sense in either SemCor or the auto-
matic rankings. The fourth row provides a ran-
dom baseline which incorporates the number of
related senses for each instance. Usually this is
calculated as the sum of
?
w?tokens
1
|Sw| over all
word tokens. Since we are evaluating RLISTs,
as well as groups, the number of senses for a
given word is not fixed, but depends in the token
sense. We therefore calculate the random base-
line as
?
ws?tokens
|related senses to ws|
|Sw| , where ws
is a word sense of word w. The columns show the
results for different ways of relating senses; the
senses are in the same group or above the thresh-
old for RLISTs. The second column (fine-grained)
are the results for these first sense heuristics with
the raw WordNet synsets. The third and fourth
columns are the results for the SEGR and RS gold
standards. The final four columns give the results
for RLISTs with JCN and DIST with the threshold
indicated.
8We performed the experiment on both the SENSEVAL-2
English lexical sample training and test data with very similar
results, but just show the results on the test corpus due to lack
of space.
21
groupings thresh on RLISTs
DIST JCN
fine-grained SEGRs RS 0.90 0.20 0.09 0.0585
SEVAL-2 FS 55.6 65.7 87.8 68.0 85.1 68.2 84.7
SemCor FS 47.0 59.1 82.8 55.9 81.7 59.7 79.4
Auto FS 35.5 48.8 82.9 50.2 72.3 53.4 83.3
random BL 17.5 34.8 65.3 32.6 69.7 34.9 63.5
Table 3: Accuracy of Coarse-grained first sense heuristic on SEVAL-2 ENG LEX
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.2  0.4  0.6  0.8  1
ac
cu
ra
cy
threshold
"senseval2"
"SemCor"
"auto"
"random"
Figure 4: Accuracy on SEVAL-2 ENG LEX for
First Sense Heuristics using DIST RLISTs and a
threshold
SemCor FS outperforms Auto FS, and is itself
outperformed by the upper-bound, SEVAL-2 FS.
All methods of relating WordNet synsets increase
the accuracy at the expense of an increased base-
line because the task is easier with less senses to
discriminate between. Both JCN and DIST have
threshold values which improve performance of
the first sense heuristics more than the manually
created SEGR given a comparable or a lower base-
line (smaller classes, and a harder task) e.g. SE-
VAL-2 FS and Auto FS for both types of RLISTs
though SemCor FS only for JCN. RS should be
compared to performance of JCN and DIST at a
similar baseline so we show these in the 6th and
8th columns of the table. In this case the RS seems
to outperform the automatic methods, but the re-
sults for JCN are close enough to be encouraging,
especially considering the baseline 63.5 is lower
than that for RS (65.3).
The RLISTs permit a trade-off between accuracy
and granularity. This can be seen by the graph in
figure 5 which shows the accuracy obtained for the
three first sense heuristics at a range of threshold
values. The random baseline is also shown. The
difference in performance compared to the base-
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  0.05  0.1  0.15  0.2
ac
cu
ra
cy
threshold
"senseval2"
"SemCor"
"auto"
"random"
Figure 5: Accuracy on SEVAL-2 ENG LEX for
First Sense Heuristics using JCN RLISTs and a
threshold
line for a given heuristic is typically better on the
fine-grained task, however the benefits of a coarse-
grained inventory will depend not on this differ-
ence, but on the utility of the relationships and dis-
tinctions made between senses. We return to this
point in the discussion and conclusions.
5 Discussion
The RLISTs show promising results when com-
pared to the human produced gold-standards on a
WSD task and even outperform the SEGR in most
cases. There are other methods proposed in the
literature which also make use of information in
WordNet, particularly looking for senses with re-
lated words in common (Tomuro, 2001; Mihalcea
and Moldovan, 2001). Tomuro does this to find
systematic polysemy, by looking for overlap in
words in different areas of WordNet. Evaluation
is performed using WordNet cousins and inter-
tagger agreement. Mihalcea and Moldovan look
for related words in common between different
senses of words to merge WordNet synsets. They
also use the hand tagged data in SemCor to remove
low frequency synsets. They demonstrate a large
reduction in polysemy of the words in SemCor (up
22
sense JCN RLIST
1 2 (0.11) 3 (0.096) 4 (0.095)
2 4 (0.24) 1 (0.11) 3 (0.099)
3 2 (0.099) 1 (0.096) 4 (0.089)
4 2 (0.24) 1 (0.095) 3 (0.089)
sense DIST RLIST
1 3 (0.88) 4 (0.50) 2 (0.48)
2 4 (0.99) 3 (0.60) 1 (0.48)
3 1 (0.88) 4 (0.60) 2 (0.60)
4 2 (0.99) 3 (0.60) 1 (0.50)
Table 4: RLISTs for child
to 39%) with a small error rate (5.6%) measured
on SemCor. Our DIST approach relates to Agirre
and Lopez de Lacalle (2003) though they pro-
duced groups and evaluated against the SEGR. We
use nearest neighbours and associate these with
word senses, rather than finding occurrences of
word senses in data directly. Nearest neighbours
have been used previously to induce word senses
from raw data (Pantel and Lin, 2002), but not for
relating existing inventories of senses. Measures
of distance in the WordNet hierarchy such as JCN
have been widely used for WSD (Patwardhan et
al., 2003) as well as the information contained in
the structure of the hierarchy (Kohomban and Lee,
2005) which has been used for backing off when
training a supervised system.
Though coarser groupings can improve inter-
tagger agreement and WSD there is also a need to
examine which distinctions are useful since there
are many ways that items can be grouped (Palmer
et al, forthcoming). A major difference to previ-
ous work is our use of RLISTs, allowing for the
level of granularity to be determined for a given
application, and allowing for ?soft relationships?
so that a sense can be related to several others
which are not themselves related. This might also
be done with soft hierarchical clusters, but has not
yet been tried. The idea of relating word sense
as a matter of degree also relates to the methods
of Schu?tze (1998) although his work was evalu-
ated using binary sense distinctions.
The child example in table 1 demonstrate prob-
lems with hard, fixed groupings. Table 4 shows
the RLISTs obtained with our methods, with the
r scores in brackets. While many of the relation-
ships in the SEGR are found, the relationships to
the other senses are apparent. In SEGR no rela-
tionship is retained between the offspring sense
(2) and the young person sense (1). According to
the RS, all paired meanings of child are related. 9
A distance measure, rather than a fixed grouping,
seems appropriate to us because one might want
the young person sense to be related to both hu-
man offspring and immature person, but not have
the latter two senses directly related.
6 Conclusion
We have investigated methods for relating Word-
Net word senses based on distributionally simi-
lar nearest neighbours and using the JCN measure.
Whilst the senses for a given word can be clustered
into sense groups, we propose the use of ranked
lists to relate the senses of a word to each other.
In this way, the granularity can be determined for
a given application and the appropriate number of
senses for a given word is not needed a priori. We
have encouraging results for nouns when compar-
ing RLISTs to manually created gold-standards.
We have produced a new gold-standard for eval-
uation based on the words used in SEVAL-2 ENG
LEX. We did this because there is no available doc-
umentation on inter-annotator agreement for the
SEGR. In future, we hope to produce another gold-
standard resource where the informants indicate a
degree of relatedness, rather than a binary choice
of related or unrelated for each pair.
We would like to see the impact that coarser-
grained WSD has on a task or application. Given
the lack of a plug and play application for feeding
disambiguated data, we hope to examine the ben-
efits on some lexical acquisition tasks that might
feed into an application, for example sense rank-
ing (McCarthy et al, 2004) or selectional prefer-
ence acquisition.
At this stage we have only experimented with
nouns, we hope to go on relating senses in other
parts-of-speech, particularly verbs since they have
very fine-grained distinctions in WordNet and
many of the subtler distinctions are quite proba-
bly not important for some applications. (Palmer
et al, forthcoming) has clearly demonstrated the
necessity for using predicate-argument structure
when grouping verb senses, so we want to exploit
such information for verbs.
We have focused on improving the first sense
heuristic, but we plan to use our groupings with
context-based WSD. To avoid a requirement for
9The two more lenient judges related all the senses of
child.
23
hand-tagged training data, we plan to exploit the
collocates of nearest neighbours.
Acknowledgements
This work was funded by a Royal Society Dorothy Hodgkin
Fellowship and a UK EPSRC project ? Ranking Word Senses
for Disambiguation: Models and Applications.? We thank
Siddharth Patwardhan and Ted Pedersen for making the WN
Similarity package publicly available. We would also like
to thank Adam Kilgarriff for suggesting the creation of the
RS resource and Eneko Agirre, Oier Lopez de Lacalle, John
Carroll and Rob Koeling for helpful discussions.
References
Eneko Agirre and Oier Lopez de Lacalle. 2003. Clustering
wordnet word senses. In Recent Advances in Natural Lan-
guage Processing, Borovets, Bulgaria.
Edward Briscoe and John Carroll. 2002. Robust accurate sta-
tistical annotation of general text. In Proceedings of the
Third International Conference on Language Resources
and Evaluation (LREC), pages 1499?1504, Las Palmas,
Canary Islands, Spain.
Jer Nan Chen and Jason S. Chang. 1998. Topical clustering
of MRD senses based on information retrieval techniques.
Computational Linguistics, 24(1):61?96.
Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005.
The pascal recognising textual entailment challenge. In
Proceedings of the PASCAL First Challenge Workshop,
pages 1?8, Southampton, UK.
William B. Dolan. 1994. Word sense disambiguation :
Clustering related senses. In Proceedings of the 15th
International Conference of Computational Linguistics.
COLING-94, volume II, pages 712?716.
Christiane Fellbaum, editor. 1998. WordNet, An Electronic
Lexical Database. The MIT Press, Cambridge, MA.
Nancy Ide and Yorick Wilks. in press. Making sense
about sense. In Eneko Agirre and Phil Edmonds, edi-
tors, Word Sense Disambiguation, Algorithms and Appli-
cations. Springer.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In Inter-
national Conference on Research in Computational Lin-
guistics, Taiwan.
Adam Kilgarriff. 2001. English lexical sample task descrip-
tion. In Proceedings of the SENSEVAL-2 workshop, pages
17?20.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominant
sense acquisition. In Proceedings of the joint conference
on Human Language Technology and Empirical methods
in Natural Language Processing, pages 419?426, Vancou-
ver, B.C., Canada.
Upali Kohomban and Wee Sun Lee. 2005. Learning seman-
tic classes for word sense disambiguation. In Proceedings
of the 43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?05), pages 34?41, Ann Arbor,
Michigan, June. Association for Computational Linguis-
tics.
Geoffrey Leech. 1992. 100 million words of English: the
British National Corpus. Language Research, 28(1):1?13.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING-ACL 98, Mon-
treal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant senses in untagged text.
In Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, pages 280?287,
Barcelona, Spain.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
generation of a coarse grained WordNet. In Proceedings
of WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, NAACL 2001 Workshop,
Pittsburgh, PA.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Language
Technology, pages 303?308. Morgan Kaufman.
Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.
forthcoming. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, pages
613?619, Edmonton, Canada.
Siddharth Patwardhan and Ted Pedersen.
2003. The cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-Similarity-
0.03/.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. 2003. Using measures of semantic relatedness for
word sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Processing
and Computational Linguistics (CICLing 2003), Mexico
City.
Wim Peters and Ivonne Peters. 1998. Automatic sense clus-
tering in EuroWordNet. In Proceedings of the First Inter-
national Conference on Language Resources and Evalua-
tion (LREC), pages 409?416, Granada, Spain.
Philip Resnik and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
Benjamin Snyder and Martha Palmer. 2004. The English
all-words task. In Proceedings of the ACL SENSEVAL-3
workshop, pages 41?43, Barcelona, Spain.
Noriko Tomuro. 2001. Tree-cut and a lexicon based on sys-
tematic polysemy. In Proceedings of the Second Meet-
ing of the North American Chapter of the Association
for Computational Linguistics. (NAACL 2001), Pittsburgh,
PA.
24
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48?53,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 10: English Lexical Substitution Task
Diana McCarthy
University of Sussex
Falmer, East Sussex
BN1 9QH, UK
dianam@sussex.ac.uk
Roberto Navigli
University of Rome ?La Sapienza?
Via Salaria, 113
00198 Roma, Italy
navigli@di.uniroma1.it
Abstract
In this paper we describe the English Lexical
Substitution task for SemEval. In the task,
annotators and systems find an alternative
substitute word or phrase for a target word in
context. The task involves both finding the
synonyms and disambiguating the context.
Participating systems are free to use any lex-
ical resource. There is a subtask which re-
quires identifying cases where the word is
functioning as part of a multiword in the sen-
tence and detecting what that multiword is.
1 Introduction
Word sense disambiguation (WSD) has been de-
scribed as a task in need of an application. Whilst
researchers believe that it will ultimately prove use-
ful for applications which need some degree of se-
mantic interpretation, the jury is still out on this
point. One problem is that WSD systems have been
tested on fine-grained inventories, rendering the task
harder than it need be for many applications (Ide
and Wilks, 2006). Another significant problem is
that there is no clear choice of inventory for any
given task (other than the use of a parallel corpus
for a specific language pair for a machine translation
application).
The lexical substitution task follows on from
some previous ideas (McCarthy, 2002) to exam-
ine the capabilities of WSD systems built by re-
searchers on a task which has potential for NLP
applications. Finding alternative words that can
occur in given contexts would potentially be use-
ful to many applications such as question answer-
ing, summarisation, paraphrase acquisition (Dagan
et al, 2006), text simplification and lexical acquisi-
tion (McCarthy, 2002). Crucially this task does not
specify the inventory for use beforehand to avoid
bias to one predefined inventory and makes it eas-
ier for those using automatically acquired resources
to enter the arena. Indeed, since the systems in
SemEval did not know the candidate substitutes for
a word before hand, the lexical resource is evaluated
as much as the context based disambiguation com-
ponent.
2 Task set up
The task involves a lexical sample of nouns, verbs,
adjectives and adverbs. Both annotators and sys-
tems select one or more substitutes for the target
word in the context of a sentence. The data was
selected from the English Internet Corpus of En-
glish produced by Sharoff (2006) from the Inter-
net (http://corpus.leeds.ac.uk/internet.html). This is
a balanced corpus similar in flavour to the BNC,
though with less bias to British English, obtained
by sampling data from the web. Annotators are not
provided with the PoS (noun, verb, adjective or ad-
verb) but the systems are. Annotators can provide
up to three substitutes but all should be equally as
good. They are instructed that they can provide a
phrase if they can?t think of a good single word sub-
stitute. They can also use a slightly more general
word if that is close in meaning. There is a ?NAME?
response if the target is part of a proper name and
?NIL? response if annotators cannot think of a good
substitute. The subjects are also asked to identify
48
if they feel the target word is an integral part of
a phrase, and what that phrase was. This option
was envisaged for evaluation of multiword detec-
tion. Annotators did sometimes use it for paraphras-
ing a phrase with another phrase. However, for an
item to be considered a constituent of a multiword,
a majority of at least 2 annotators had to identify the
same multiword.1
The annotators were 5 native English speakers
from the UK. They each annotated the entire dataset.
All annotations were semi-automatically lemma-
tised (substitutes and identified multiwords) unless
the lemmatised version would change the meaning
of the substitute or if it was not obvious what the
canonical version of the multiword should be.
2.1 Data Selection
The data set comprises 2010 sentences, 201 target
words each with 10 sentences. We released 300 for
the trial data and kept the remaining 1710 for the
test release. 298 of the trial, and 1696 of the test
release remained after filtering items with less than
2 non NIL and non NAME responses and a few with
erroneous PoS tags. The words included were se-
lected either manually (70 words) from examination
of a variety of lexical resources and corpora or au-
tomatically (131) using information in these lexical
resources. Words were selected from those having a
number of different meanings, each with at least one
synonym. Since typically the distribution of mean-
ings of a word is strongly skewed (Kilgarriff, 2004),
for the test set we randomly selected 20 words in
each PoS for which we manually selected the sen-
tences 2 (we refer to these words as MAN) whilst for
the remaining words (RAND) the sentences were se-
lected randomly.
2.2 Inter Annotator Agreement
Since we have sets of substitutes for each item and
annotator, pairwise agreement was calculated be-
tween each pair of sets (p1, p2 ? P ) from each pos-
sible pairing (P ) as
?
p1,p2?P
p1?p2
p1?p2
|P |
1Full instructions given to the annotators are posted at
http://www.informatics.susx.ac.uk/research/nlp/mccarthy/files/
instructions.pdf.
2There were only 19 verbs due to an error in automatic se-
lection of one of the verbs picked for manual selection of sen-
tences.
Pairwise inter-annotator agreement was 27.75%.
73.93% had modes, and pairwise agreement with the
mode was 50.67%. Agreement is increased if we re-
move one annotator who typically gave 2 or 3 sub-
stitutes for each item, which increased coverage but
reduced agreement. Without this annotator, inter-
annotator agreement was 31.13% and 64.7% with
mode.
Multiword detection pairwise agreement was
92.30% and agreement on the identification of the
exact form of the actual multiword was 44.13%.
3 Scoring
We have 3 separate subtasks 1) best 2) oot and 3)
mw which we describe below. 3 In the equations
and results tables that follow we use P for precision,
R for recall, and Mode P and Mode R where we
calculate precision and recall against the substitute
chosen by the majority of annotators, provided that
there is a majority.
Let H be the set of annotators, T be the set of test
items with 2 or more responses (non NIL or NAME)
and hi be the set of responses for an item i ? T for
annotator h ? H .
For each i ? T we calculate the mode (mi) i.e.
the most frequent response provided that there is a
response more frequent than the others. The set of
items where there is such a mode is referred to as
TM . Let A (and AM ) be the set of items from T
(or TM ) where the system provides at least one sub-
stitute. Let ai : i ? A (or ai : i ? AM ) be the set
of guesses from the system for item i. For each i
we calculate the multiset union (Hi) for all hi for all
h ? H and for each unique type (res) in Hi will
have an associated frequency (freqres) for the num-
ber of times it appears in Hi.
For example: Given an item (id 9999) for happy;a
supposing the annotators had supplied answers as
follows:
annotator responses
1 glad merry
2 glad
3 cheerful glad
4 merry
5 jovial
3The scoring measures are as described in the doc-
ument at http://nlp.cs.swarthmore.edu/semeval/tasks/task10/
task10documentation.pdf released with our trial data.
49
then Hi would be glad glad glad merry merry
cheerful jovial. The res with associated frequencies
would be glad 3 merry 2 cheerful 1 and jovial 1.
best measures This requires the best file produced
by the system which gives as many guesses as the
system believes are fitting, but where the credit
for each correct guess is divided by the number of
guesses. The first guess in the list is taken as the
best guess (bg).
P =
?
ai:i?A
?
res?ai
freqres
|ai|
|Hi|
|A| (1)
R =
?
ai:i?T
?
res?ai
freqres
|ai|
|Hi|
|T | (2)
Mode P =
?
bgi?AM 1 if bg = mi
|AM | (3)
Mode R =
?
bgi?TM 1 if bg = mi
|TM | (4)
A system is permitted to provide more than one
response, just as the annotators were. They can
do this if they are not sure which response is bet-
ter, however systems will maximise the score if they
guess the most frequent response from the annota-
tors. For P and R the credit is divided by the num-
ber of guesses that a system makes to prevent a sys-
tem simply hedging its bets by providing many re-
sponses. The credit is also divided by the number of
responses from annotators. This gives higher scores
to items with less variation. We want to emphasise
test items with better agreement.
Using the example for happy;a id 9999 above, if
the system?s responses for this item was glad; cheer-
ful the credit for a9999 in the numerator of P and R
would be
3+1
2
7 = .286
For Mode P and Mode R we use the system?s
first guess and compare this to the mode of the anno-
tators responses on items where there was a response
more frequent than the others.
oot measures This allows a system to make up to
10 guesses. The credit for each correct guess is not
divided by the number of guesses. This allows for
the fact that there is a lot of variation for the task and
we only have 5 annotators. With 10 guesses there is
a better chance that the systems find the responses
of these 5 annotators. There is no ordering of the
guesses and the Mode scores give credit where the
mode was found in one of the system?s 10 guesses.
P =
?
ai:i?A
?
res?ai
freqres
|Hi|
|A| (5)
R =
?
ai:i?T
?
res?ai
freqres
|Hi|
|T | (6)
Mode P =
?
ai:i?AM 1 if any guess ? ai = mi
|AM |
(7)
Mode R =
?
ai:i?TM 1 if any guess ? ai = mi
|TM |
(8)
mw measures For this measure, a system must
identify items where the target is part of a multiword
and what the multiword is. The annotators do not all
have linguistics background, they are simply asked
if the target is an integral part of a phrase, and if so
what the phrase is. Sometimes this option is used
by the subjects for paraphrasing a phrase of the sen-
tence, but typically it is used when there is a mul-
tiword. For scoring, a multiword item is one with
a majority vote for the same multiword with more
than 1 annotator identifying the multiword.
Let MW be the subset of T for which there
is such a multiword response from a majority of
at least 2 annotators. Let mwi ? MW be the
multiword identified by majority vote for item i.
Let MWsys be the subset of T for which there is a
multiword response from the system and mwsysi
be a multiword specified by the system for item i.
detection P =
?
mwsysi?MWsys 1 if mwi exists at i
|MWsys| (9)
detection R =
?
mwsysi?MW 1 if mwi exists at i
|MW | (10)
identification P =
?
mwsysi?MWsys 1 if mwsysi = mwi
|MWsys| (11)
50
identification R =
?
mwsysi?MW 1 if mwsysi = mwi
|MW | (12)
3.1 Baselines
We produced baselines using WordNet 2.1 (Miller et
al., 1993a) and a number of distributional similarity
measures. For the WordNet best baseline we found
the best ranked synonym using the criteria 1 to 4
below in order. For WordNet oot we found up to 10
synonyms using criteria 1 to 4 in order until 10 were
found:
1. Synonyms from the first synset of the target
word, and ranked with frequency data obtained
from the BNC (Leech, 1992).
2. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
that first synset, ranked with the frequency data.
3. Synonyms from all synsets of the target word,
and ranked using the BNC frequency data.
4. synonyms from the hypernyms (verbs and
nouns) or closely related classes (adjectives) of
all synsets of the target, ranked with the BNC
frequency data.
We also produced best and oot baselines using
the distributional similarity measures l1, jaccard, co-
sine, lin (Lin, 1998) and ?SD (Lee, 1999) 4. We took
the word with the largest similarity (or smallest dis-
tance for ?SD and l1) for best and the top 10 for oot.
For mw detection and identification we used
WordNet to detect if a multiword in WordNet which
includes the target word occurs within a window of
2 words before and 2 words after the target word.
4 Systems
9 teams registered and 8 participated, and two of
these teams (SWAG and IRST) each entered two sys-
tems, we distinguish the first and second systems
with a 1 and 2 suffix respectively.
The systems all used 1 or more predefined inven-
tories. Most used web queries (HIT, MELB, UNT)
or web data (Brants and Franz, 2006) (IRST2, KU,
4We used 0.99 as the parameter for ? for this measure.
SWAG1, SWAG2, USYD, UNT) to obtain counts for
disambiguation, with some using algorithms to de-
rive domain (IRST1) or co-occurrence (TOR) infor-
mation from the BNC. Most systems did not use
sense tagged data for disambiguation though MELB
did use SemCor (Miller et al, 1993b) for filtering in-
frequent synonyms and UNT used a semi-supervised
word sense disambiguation combined with a host of
other techniques, including machine translation en-
gines.
5 Results
In tables 1 to 3 we have ordered systems accord-
ing to R on the best task, and in tables 4 to 6 ac-
cording to R on oot. We show all scores as per-
centages i.e. we multiply the scores in section 3
by 100. In tables 3 and 6 we show results using
the subset of items which were i) NOT identified as
multiwords (NMWT) ii) scored only on non multi-
word substitutes from both annotators and systems
(i.e. no spaces) (NMWS). Unfortunately we do not
have space to show the analysis for the MAN and
RAND subsets here. Please refer to the task website
for these results. 5 We retain the same ordering for
the further analysis tables when we look at subsets
of the data. Although there are further differences
in the systems which would warrant reranking on an
individual analysis, since we combined the subanal-
yses in one table we keep the order as for 1 and 4
respectively for ease of comparison.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6 KU
is highest ranking on R for best. UNT is best at find-
ing the mode, particularly on oot, though it is the
most complicated system exploiting a great many
knowledge sources and components. IRST2 does
well at finding the mode in best. The IRST2 best
R score is lower because it supplied many answers
for each item however it achieves the best R score
on the oot task. The baselines are outperformed by
most systems. The WordNet baseline outperforms
those derived from distributional methods. The dis-
tributional methods, especially lin, show promising
results given that these methods are automatic and
5The task website is at http://www.informatics.sussex.ac.uk/
research/nlp/mccarthy/task10index.html.
6There is not a big difference between P and R because
systems typically supplied answers for most items.
51
Systems P R Mode P Mode R
KU 12.90 12.90 20.65 20.65
UNT 12.77 12.77 20.73 20.73
MELB 12.68 12.68 20.41 20.41
HIT 11.35 11.35 18.86 18.86
USYD 11.23 10.88 18.22 17.64
IRST1 8.06 8.06 13.09 13.09
IRST2 6.95 6.94 20.33 20.33
TOR 2.98 2.98 4.72 4.72
Table 1: best results
Systems P R Mode P Mode R
WordNet 9.95 9.95 15.28 15.28
lin 8.84 8.53 14.69 14.23
l1 8.11 7.82 13.35 12.93
lee 6.99 6.74 11.34 10.98
jaccard 6.84 6.60 11.17 10.81
cos 5.07 4.89 7.64 7.40
Table 2: best baseline results
don?t require hand-crafted inventories. As yet we
haven?t combined the baselines with disambiguation
methods.
Only HIT attempted the mw task. It outperforms
all baselines from WordNet.
5.1 Post Hoc Analysis
Choosing a lexical substitute for a given word is
not clear cut and there is inherently variation in the
task. Since it is quite likely that there will be syn-
onyms that the five annotators do not think of we
conducted a post hoc analysis to see if the synonyms
selected by the original annotators were better, on
the whole, than those in the systems responses. We
randomly selected 100 sentences from the subset of
items which had more than 2 single word substitutes,
no NAME responses, and where the target word was
NMWT NMWS
Systems P R P R
KU 13.39 13.39 14.33 13.98
UNT 13.46 13.46 13.79 13.79
MELB 13.35 13.35 14.19 13.82
HIT 11.97 11.97 12.55 12.38
USYD 11.68 11.34 12.48 12.10
IRST1 8.44 8.44 8.98 8.92
IRST2 7.25 7.24 7.67 7.66
TOR 3.22 3.22 3.32 3.32
Table 3: Further analysis for best
Systems P R Mode P Mode R
IRST2 69.03 68.90 58.54 58.54
UNT 49.19 49.19 66.26 66.26
KU 46.15 46.15 61.30 61.30
IRST1 41.23 41.20 55.28 55.28
USYD 36.07 34.96 43.66 42.28
SWAG2 37.80 34.66 50.18 46.02
HIT 33.88 33.88 46.91 46.91
SWAG1 35.53 32.83 47.41 43.82
TOR 11.19 11.19 14.63 14.63
Table 4: oot results
Systems P R Mode P Mode R
WordNet 29.70 29.35 40.57 40.57
lin 27.70 26.72 40.47 39.19
l1 24.09 23.23 36.10 34.96
lee 20.09 19.38 29.81 28.86
jaccard 18.23 17.58 26.87 26.02
cos 14.07 13.58 20.82 20.16
Table 5: oot baseline results
NMWT NMWS
Systems P R P R
IRST2 72.04 71.90 76.19 76.06
UNT 51.13 51.13 54.01 54.01
KU 48.43 48.43 49.72 49.72
IRST1 43.11 43.08 45.13 45.11
USYD 37.26 36.17 40.13 38.89
SWAG2 39.95 36.51 40.97 37.75
HIT 35.60 35.60 36.63 36.63
SWAG1 37.49 34.64 38.36 35.67
TOR 11.77 11.77 12.22 12.22
Table 6: Further analysis for oot
HIT WordNet BL
P R P R
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
Table 7: MW results
52
good reasonable bad
sys 9.07 19.08 71.85
origA 37.36 41.01 21.63
Table 8: post hoc results
not one of those identified as a multiword (i.e. a ma-
jority vote by 2 or more annotators for the same mul-
tiword as described in section 2). We then mixed the
substitutes from the human annotators with those of
the systems. Three fresh annotators7 were given the
test sentence and asked to categorise the randomly
ordered substitutes as good, reasonable or bad. We
take the majority verdict for each substitute, but if
there is one reasonable and one good verdict, then
we categorise the substitute as reasonable. The per-
centage of substitutes for systems (sys) and original
annotators (origA) categorised as good, reasonable
and bad by the post hoc annotators are shown in ta-
ble 8. We see the substitutes from the humans have
a higher proportion of good or reasonable responses
by the post hoc annotators compared to the substi-
tutes from the systems.
6 Conclusions and Future Directions
We think this task is an interesting one in which to
evaluate automatic approaches of capturing lexical
meaning. There is an inherent variation in the task
because several substitutes may be possible for a
given context. This makes the task hard and scoring
is less straightforward than a task which has fixed
choices. On the other hand, we believe the task taps
into human understanding of word meaning and we
hope that computers that perform well on this task
will have potential in NLP applications. Since a
pre-defined inventory is not used, the task allows us
to compare lexical resources as well as disambigua-
tion techniques without a bias to any predefined in-
ventory. It is possible for those interested in disam-
biguation to focus on this, rather than the choice of
substitutes, by using the union of responses from the
annotators in future experiments.
7 Acknowledgements
We acknowledge support from the Royal Society UK for fund-
ing the annotation for the project, and for a Dorothy Hodgkin
7Again, these were native English speakers from the UK.
Fellowship to the first author. We also acknowledge support
to the second author from INTEROP NoE (508011, 6th EU
FP). We thank the annotators for their hard work. We thank
Serge Sharoff for the use of his Internet corpus, Julie Weeds for
the software we used for producing the distributional similarity
baselines and Suzanne Stevenson for suggesting the oot task .
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
corpus version 1.1. Technical Report.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the As-
sociation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? In Proceedings of Text, Speech,
Dialogue, Brno, Czech Republic.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 25?32.
Geoffrey Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109?115, Philadelphia, USA.
George Miller, Richard Beckwith, Christine Fellbaum,
David Gross, and Katherine Miller, 1993a. Intro-
duction to WordNet: an On-Line Lexical Database.
ftp://clarity.princeton.edu/pub/WordNet/5papers.ps.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993b. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
53
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 314?317,
Prague, June 2007. c?2007 Association for Computational Linguistics
Sussx: WSD using Automatically Acquired Predominant Senses
Rob Koeling and Diana McCarthy
Department of Informatics
University of Sussex
Brighton BN1 9QJ, UK
robk,dianam@sussex.ac.uk
1 Introduction
We introduced a method for discovering the predom-
inant sense of words automatically using raw (unla-
belled) text in (McCarthy et al, 2004) and partici-
pated with this system in SENSEVAL3. Since then,
we worked on further developing ideas to improve
upon the base method. In the current paper we tar-
get two areas where we believe there is potential for
improvement. In the first one we address the fine-
grained structure of WordNet?s (WN) sense inven-
tory (i.e. the topic of the task in this particular track).
The second issue we address here, deals with topic
domain specilisation of the base method.
Error analysis tought us that the method is sensi-
tive to the fine-grained nature of WN. When two dis-
tinct senses in the WN sense inventory are closely re-
lated, the method often has difficulties discriminat-
ing between the two senses. If, for example, sense 1
and sense 7 for a word are closely related, choosing
sense 7 in stead of sense 1 has serious consequences
if you are using a first-sense heuristic (considering
the highly skewed distribution of word senses). We
expect that applying our method on a coarser grained
sense inventory might help us resolve some of the
more unfortunate errors.
(Magnini et al, 2002) have shown that informa-
tion about the domain of a document is very useful
for WSD. This is because many concepts are spe-
cific to particular domains, and for many words their
most likely meaning in context is strongly correlated
to the domain of the document they appear in. Thus,
since word sense distributions are skewed and de-
pend on the domain at hand we would like to explore
if we can estimate the most likely sense of a word
for each domain of application and exploit this in
a WSD system.
2 Predominant Sense Acquisition
We use the method described in (McCarthy et al,
2004) for finding predominant senses from raw text.
The method uses a thesaurus obtained from the
text by parsing, extracting grammatical relations and
then listing each word (w) with its top k nearest
neighbours, where k is a constant. Like (McCarthy
et al, 2004) we use k = 50 and obtain our thesaurus
using the distributional similarity metric described
by (Lin, 1998) and we use WordNet (WN) as our
sense inventory. The senses of a word w are each
assigned a ranking score which sums over the dis-
tributional similarity scores of the neighbours and
weights each neighbour?s score by a WN Similarity
score (Patwardhan and Pedersen, 2003) between the
sense of w and the sense of the neighbour that max-
imises the WN Similarity score. This weight is nor-
malised by the sum of such WN similarity scores be-
tween all senses of w and the senses of the neighbour
that maximises this score. We use the WN Similarity
jcn score (Jiang and Conrath, 1997) since this gave
reasonable results for (McCarthy et al, 2004) and it
is efficient at run time given precompilation of fre-
quency information. The jcn measure needs word
frequency information, which we obtained from the
British National Corpus (BNC) (Leech, 1992). The
distributional thesaurus was constructed using sub-
ject, direct object adjective modifier and noun mod-
ifier relations.
314
3 Coarse Sense Inventory Adaptation
We contrasted ranking of the original WordNet
senses with ranking produced using the coarse
grained mapping between WordNet senses and the
clusters provided for this task. In the first, which we
refer to as fine-grained training (SUSSX-FR), we use
the original method as described in section 2 using
WordNet 2.1 as our sense inventory. For the second
method which we refer to as coarse-grained train-
ing (SUSSX-CR), we use the clusters of the target
word as our senses. The distributional similarity of
each neighbour is apportioned to these clusters us-
ing the maximum WordNet similarity between any
of the WordNet senses in the cluster and any of the
senses of the neighbour. This WordNet similarity is
normalised as in the original method, but for the de-
nominator we use the sum of the WordNet similarity
scores between this neighbour and all clusters of the
target word.
4 Domain Adaptation
The topic domain of a document has a strong influ-
ence on the sense distribution of words. Unfortu-
nately, it is not feasible to produce large manually
sense-annotated corpora for every domain of inter-
est. Since the method described in section 2 works
with raw text, we can specialize our sense rank-
ings for a particular topic domain, simply by feed-
ing a domain specific corpus to the algorithm. Pre-
vious experiments have shown that unsupervised es-
timation of the predominant sense of certain words
using corpora whose domain has been determined
by hand outperforms estimates based on domain-
independent text for a subset of words and even
outperforms the estimates based on counting oc-
currences in an annotated corpus (Koeling et al,
2005). A later experiment (using SENSEVAL2 and
3 data) showed that using domain specific predomi-
nant senses can slightly improve the results for some
domains (Koeling et al, 2007). However, a firm idea
of when domain specilisation should be considered
could not (yet) be given.
4.1 Creating the Domain Corpora
In order to estimate topic domain specific sense
rankings, we need to specify what we consider ?do-
mains? and we need to collect corpora of texts for
these domains. We decided to use text classifica-
tion for determining the topic domain and adopted
the domain hierarchy as defined for the topic domain
extension for WN (Subject Field Codes or WordNet
Domains (WN-DOMAINS) (Magnini et al, 2002)).
Domains In WN-DOMAINS the Princeton English
WordNet is augmented with domain labels. Ev-
ery synset in WN?s sense inventory is annotated
with at least one domain label, selected from a set
of about 200 labels hierarchically organized (based
on the Dewey Decimal Classification (Diekema, )).
Each synset of Wordnet 1.6 was labeled with one
or more labels. The label ?factotum? was assigned
if any other was inadequate. The first level con-
sists of 5 main categories (e.g. ?doctrines? and ?so-
cial science?) and ?factotum?. ?doctrines?, for exam-
ple, has subcategories such as ?art?, ?religion? and
?psychology?. Some subcategories are divided in
sub-subcategories, e.g. ?dance?, ?music? or ?theatre?
are subcategories of ?art?.
Classifier We extracted bags of domain-specific
words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corre-
sponding glosses associated with a certain domain
label. These bags of words define the domains and
we used them to train a Support Vector Machine
(SVM) text classifier using ?TwentyOne?1.
The classifier distinguishes between 48 classes
(first and second level of the WN-DOMAINS hierar-
chy). When a document is evaluated by the classi-
fier, it returns a list of all the classes (domains) it
recognizes and an associated confidence score re-
flecting the certainty that the document belongs to
that particular domain.
Corpora We used the Gigaword English Corpus
as our data source. This corpus is a comprehen-
sive archive of newswire text data that has been
acquired over several years by the Linguistic Data
Consortium, at the University of Pennsylvania. For
the experiments described in this paper, we use the
first 20 months worth of data of all four sources
(Agence France Press English Service, Associated
Press Worldstream English Service, The New York
Times Newswire Service and The Xinhua News
Agency English Service). There are 4 different types
1TwentyOne Classifier is an Irion Technologies product:
www.irion.ml/products/english/products classify.html
315
Doc.Id. Class Conf. Score
d001 Medicine (Economy) 0.75 (0.75)
d002 Economy (Politics) 0.76 (0.74)
d003 Transport (Biology) 0.75 (0.68)
d004 Comp-Sci (Architecture) 0.81 (0.68)
d005 Psychology (Art) 0.78 (0.74)
Table 1: Output of the classifier for the 5 docu-
ments. The classifiers second choice is given be-
tween brackets.
of documents identified in the corpus. The vast ma-
jority of the documents are of type ?story?. We are
using all the data.
The five documents were fed to the classifier. The
results are given in table 1. Unfortunately, only one
document (d004) was considered to be a clear-cut
example of a particular domain by the classifier (i.e.
a high score is given to the first class and a much
lower score to the following classes).
4.2 Domain rankings
We created domain corpora by feeding the Giga-
Word documents to the classifier and adding each
document to the domain corpus corresponding to
the classifier?s first choice. The five corpora we
needed for these documents were parsed using
RASP (Briscoe and Carroll, 2002) and the result-
ing grammatical relations were used to create a dis-
tributional similarity thesaurus, which in turn was
used for computing the predominant senses (see sec-
tion 2). The only pre-processing we performed
was stripping the XML codes from the documents.
No other filtering was undertaken. This resulted in
five sets of sense inventories with domain-dependent
sense rankings. Each of them has a slightly different
set of words. The words they have in common do
have the same senses, but not necessarily the same
estimated most frequently used sense.
5 Results from Semeval
Coarse Disambiguation of coarse-grained senses is
obviously an easier task than fine grained training.
We had hoped that the coarse-grained training might
show superior performance by removing the noise
created by related but less frequent senses. Since
the mapping between fine-grained senses and clus-
ters is used anyway in the scorer the noise from
related senses does not seem to be an issue. Re-
lated senses are scored correctly. Indeed the per-
formance of the fine-grained training is superior to
that of the coarse-grained training. We believe this
is because predominant meanings have more related
senses. There are therefore more chances that the
distributional similarity of the neighbours will get
apportioned to one of the related senses when there
are more related senses. The coarse grained rank-
ing would have an advantage on occasions when in
the fine-grained ranking the credit between related
senses is split and an unrelated sense ends up with
a higher ranking score. Since the coarse-grained
ranker lumps the credit for related sense together it
would be at an advantage. Clearly this doesn?t hap-
pen enough in the data to outweigh the beneficial
effect of the number of related senses compensating
for other noise in the data.
Doc.Id. Class SUSSX-FR SUSSX-C-WD
d001 Medicine 0.556 0.560
d002 Economy 0.508 0.515
d003 Transport 0.487 0.454
d004 Comp-Sci 0.407 0.424
d005 Psychology 0.356 0.372
Table 2: Impact of domain specialisation for each of
the five documents (F1 scores).
Domain Unfortuately, the system specialised for
domain (SUSSX-C-WD) did not improve the results
over the 5 documents significantly. However, if we
look at the contributions made by each document,
we might learn something about the relation beteen
the output of the classifier and the impact on the
WSDresults. Table 2 shows the per-document results
for the systems SUSSX-FR and SUSSX-C-WD. The
first two documents show very little difference with
the domain independent results. The documents
?d004? and ?d005? show a small but clear improved
performance for the domain results. Unfortunately,
document ?d003? displays a very disappointing drop
of more than 3% in performance, and cancels out all
the gains made by the last two documents.
The output of the classifier seems to be indica-
tive of the results for all documents except ?d003?.
The classifier doesn?t seem to find enough evidence
for a marked preference for a particular domain
316
for documents ?d001? and ?d002?. This could be
an indication that there is no strong domain ef-
fect to be expected. The strong preference for the
?computer science? domain for ?d004? is reflected in
good performance of SUSSX-C-WD and even though
the confidence scores for the first 2 alternatives of
?d005? are fairly close, there is a clear drop in con-
fidence score for the third alternative, which might
indicate that the topic of this document is related to
both first choices of the classifier. It will be interest-
ing to evaluate the results for ?d005? using the ?Art?
sense rankings. One would expect those results to be
similar to the results found here. Finally, the results
for ?d003? are hard to explain. We will need to do an
extensive error analysis as soon as the gold-standard
is available.
6 Conclusions
In this paper we investigated two directions where
we expect potential for improving the performance
of our method for acquiring predominant senses.
In order to fully appreciate what the effects of the
coarse grained sense inventory are (i.e. whether
some of the more unfortunate errors are resolved),
we will have to do an extensive error analysis as
soon as the gold standard becomes available. Con-
sidering the fairly low number of attempted tokens
(only 72.8% of the tokens are attempted), we are at
a disadvantage compared to systems that back-off to
(for example) the first sense in WN. However, we are
well pleased with the high precision (71.7%) of the
method SUSSX-FR, considering this is a completely
unsupervised method. There seems to be potential
gains for domain adaptation, but applying it to each
document does not seem to be advisable. More re-
search needs to be done to identify in which cases a
performance boost can be expected. Five documents
is not enough to fully investigate the matter. At the
moment we are performing a larger scale experiment
with the documents in SemCor. These documents
seem to cover a fairly wide range of domains (ac-
cording to our text classifier) and many domains are
represented by several documents.
Acknowledgements
This work was funded by UK EPSRC project
EP/C537262 ?Ranking Word Senses for Disam-
biguation: Models and Applications?, and by a UK
Royal Society Dorothy Hodgkin Fellowship to the
second author. We would also like to thank Piek
Vossen for giving us access to the Irion Technolo-
gies text categoriser.
References
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of LREC-2002, pages 1499?1504, Las Palmas de GranCanaria.
Anne Diekema. http://www.oclc.org/dewey/.
Jay Jiang and David Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In In-
ternational Conference on Research in Computational
Linguistics, Taiwan.
Rob Koeling, Diana McCarthy, and John Carroll. 2005.
Domain-specific sense distributions and predominantsense acquisition. In Proceedings of the Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing.,pages 419?426, Vancouver, Canada.
Rob Koeling, Diana McCarthy, and John Carroll. 2007.Text categorization for improved priors of word mean-
ing. In Proceedings of the Eighth International Con-
ference on Intelligent Text Processing and Compu-
tational Linguistics (Cicling 2007), pages 241?252,
Mexico City, Mexico.
Geoffrey Leech. 1992. 100 million words of English:the British National Corpus. Language Research,28(1):1?13.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of COLING-ACL 98,Montreal, Canada.
Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll. 2004. Finding predominant senses in un-
tagged text. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,pages 280?287, Barcelona, Spain.
Siddharth Patwardhan and Ted Pedersen.
2003. The cpan wordnet::similarity package.http://search.cpan.org/s?id/WordNet-Similarity/.
317
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution task,
which is based on the English Lexical Substi-
tution task run at SemEval-2007. In the En-
glish version of the task, annotators and sys-
tems had to find an alternative substitute word
or phrase for a target word in context. In this
paper we propose a task where the target word
and contexts will be in English, but the substi-
tutes will be in Spanish. In this paper we pro-
vide background and motivation for the task
and describe how the dataset will differ from
a machine translation task and previous word
sense disambiguation tasks based on parallel
data. We describe the annotation process and
how we anticipate scoring the system output.
We finish with some ideas for participating
systems.
1 Introduction
The Cross-Lingual Lexical Substitution task is
based on the English Lexical Substitution task run at
SemEval-2007. In the 2007 English Lexical Substi-
tution Task, annotators and systems had to find an al-
ternative substitute word or phrase for a target word
in context. In this cross-lingual task the target word
and contexts will be in English, but the substitutes
will be in Spanish.
An automatic system for cross-lingual lexical sub-
stitution would be useful for a number of applica-
tions. For instance, such a system could be used
to assist human translators in their work, by provid-
ing a number of correct translations that the human
translator can choose from. Similarly, the system
could be used to assist language learners, by pro-
viding them with the interpretation of the unknown
words in a text written in the language they are learn-
ing. Last but not least, the output of a cross-lingual
lexical substitution system could be used as input to
existing systems for cross-language information re-
trieval or automatic machine translation.
2 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter re-
ferred to as LEXSUB) was run at SemEval-2007 fol-
lowing earlier ideas on a method of testing WSD
systems without predetermining the inventory (Mc-
Carthy, 2002). The issue of which inventory is ap-
propriate for the task has been a long standing is-
sue for debate, and while there is hope that coarse-
grained inventories will allow for increased system
performance (Ide and Wilks, 2006) we do not yet
know if these will make the distinctions that will
most benefit practical systems (Stokoe, 2005) or re-
flect cognitive processes (Kilgarriff, 2006). LEXSUB
was proposed as a task which, while requiring con-
textual disambiguation, did not presuppose a spe-
cific sense inventory. In fact, it is quite possible to
use alternative representations of meaning (Schu?tze,
1998; Pantel and Lin, 2002).
The motivation for a substitution task was that it
would reflect capabilities that might be useful for
natural language processing tasks such as paraphras-
ing and textual entailment, while only focusing on
one aspect of the problem and therefore not requir-
ing a complete system that might mask system capa-
bilities at a lexical level and at the same time make
76
participation in the task difficult for small research
teams.
The task required systems to produce a substitute
word for a word in context. For example a substitute
of tournament might be given for the second oc-
currence of match (shown in bold) in the following
sentence:
The ideal preparation would be a light meal
about 2-2 1/2 hours pre-match, followed by a
warm-up hit and perhaps a top-up with extra fluid
before the match.
In LEXSUB, the data was collected for 201 words
from open class parts-of-speech (PoS) (i.e. nouns,
verbs, adjectives and adverbs). Words were selected
that have more than one meaning with at least one
near synonym. Ten sentences for each word were
extracted from the English Internet Corpus (Sharoff,
2006). There were five annotators who annotated
each target word as it occurred in the context of a
sentence. The annotators were each allowed to pro-
vide up to three substitutes, though they could also
provide a NIL response if they could not come up
with a substitute. They had to indicate if the target
word was an integral part of a multiword.
A development and test dataset were provided,
but no training data. Any system that relied on train-
ing data, such as sense annotated corpora, had to use
resources available from other sources. The task had
eight participating teams. Teams were allowed to
submit up to two systems and there were a total of
ten different systems. The scoring was conducted
using recall and precision measures using:
? the frequency distribution of responses from
the annotators and
? the mode of the annotators (the most frequent
response).
The systems were scored using their best guess as
well as an out-of-ten score which allowed up to 10
attempts. 1 The results are reported in McCarthy and
Navigli (2007) and in more detail in McCarthy and
Navigli (in press).
1The details are available at
http://nlp.cs.swarthmore.edu/semeval/tasks/
task10/task10documentation.pdf.
3 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained al-
ternative translation records of typical usages of a
test word, also referred to as a translation mem-
ory. Systems could either select the most appropri-
ate translation memory record for each instance and
were scored against a gold-standard set of annota-
tions, or they could provide a translation that was
scored by translation experts after the results were
submitted. In contrast to this work, we propose to
provide actual translations for target instances in ad-
vance, rather than predetermine translations using
lexicographers or rely on post-hoc evaluation, which
does not permit evaluation of new systems after the
competition.
Previous standalone WSD tasks based on parallel
data have obtained distinct translations for senses as
listed in a dictionary (Ng and Chan, 2007). In this
way fine-grained senses with the same translations
can be lumped together, however this does not fully
allow for the fact that some senses for the same
words may have some translations in common but
also others that are not. An example from Resnik
and Yarowsky (2000) (table 4 in that paper) is the
first two senses from WordNet for the noun interest:
WordNet sense Spanish Translation
monetary e.g. on loan intere?s, re?dito
stake/share intere?s,participacio?n
For WSD tasks, a decision can be made to lump
senses with such overlap, or split them using the dis-
tinctive translation and then use the distinctive trans-
lations as a sense inventory. This sense inventory is
then used to collect training from parallel data (Ng
and Chan, 2007). We propose that it would be in-
teresting to collect a dataset where the overlap in
translations for an instance can remain and that this
will depend on the token instance rather than map-
ping to a pre-defined sense inventory. Resnik and
Yarowsky (2000) also conducted their experiments
using words in context, rather than a predefined
77
sense-inventory as in (Ng and Chan, 2007; Chan and
Ng, 2005), however in these experiments the anno-
tators were asked for a single preferred translation.
We intend to allow annotators to supply as many
translations as they feel are equally valid. This will
allow us to examine more subtle relationships be-
tween usages and to allow partial credit to systems
which get a close approximation to the annotators?
translations. Unlike a full blown machine transla-
tion task (Carpuat and Wu, 2007), annotators and
systems will not be required to translate the whole
context but just the target word.
4 The Cross-Lingual Lexical Substitution
Task
Here we discuss our proposal for a Cross-Lingual
Lexical Substitution task. The task will follow LEX-
SUB except that the annotations will be translations
rather than paraphrases.
Given a target word in context, the task is to pro-
vide several correct translations for that word in a
given language. We will use English as the source
language and Spanish as the target language. Mul-
tiwords are ?part and parcel? of natural language.
For this reason, rather than try and filter multiwords,
which is very hard to do without assuming a fixed
inventory, 2 we will ask annotators to indicate where
the target word is part of a multiword and what that
multiword is. This way, we know what the substitute
translation is replacing.
We will provide both development and test sets,
but no training data. As for LEXSUB, any sys-
tems requiring data will need to obtain it from other
sources. We will include nouns, verbs, adjectives
and adverbs in both development and test data. Un-
like LEXSUB, the annotators will be told the PoS of
the current target word.
4.1 Annotation
We are going to use four annotators for our task, all
native Spanish speakers from Mexico, with a high
level of proficiency in English. The annotation in-
terface is shown in figure 1. We will calculate inter-
tagger agreement as pairwise agreement between
2The multiword inventories that do exist are far from com-
plete.
sets of substitutes from annotators, as was done in
LEXSUB.
4.2 An Example
One significant outcome of this task is that there
will not necessarily be clear divisions between us-
ages and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This will mean that there can
be usages that overlap to different extents with each
other but do not have identical translations. An ex-
ample from our preliminary annotation trials is the
target adverb severely. Four sentences are shown in
figure 2 with the translations provided by one an-
notator marked in italics and {} braces. Here, all
the token occurrences seem related to each other in
that they share some translations, but not all. There
are sentences like 1 and 2 that appear not to have
anything in common. However 1, 3, and 4 seem to
be partly related (they share severamente), and 2, 3,
and 4 are also partly related (they share seriamente).
When we look again, sentences 1 and 2, though not
directly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We will adopt the best and out-of-ten precision and
recall scores from LEXSUB. The systems can supply
as many translations as they feel fit the context. The
system translations will be given credit depending
on the number of annotators that picked each trans-
lation. The credit will be divided by the number of
annotator responses for the item and since for the
best score the credit for the system answers for an
item is also divided by the number of answers the
system provides, this allows more credit to be given
to instances where there is less variation. For that
reason, a system is better guessing the translation
that is most frequent unless it really wants to hedge
its bets. Thus if i is an item in the set of instances
I , and Ti is the multiset of gold standard translations
from the human annotators for i, and a system pro-
vides a set of answers Si for i, then the best score
for item i will be:
best score(i) =
?
s?Si frequency(s ? Ti)
|Si| ? |Ti|
(1)
78
Figure 1: The Cross-Lingual Lexical Substitution Interface
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely stressed
by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now. {altamente,
seriamente, exageradamente}
Figure 2: Translations from one annotator for the adverb severely
Precision is calculated by summing the scores for
each item and dividing by the number of items that
the system attempted whereas recall divides the sum
of scores for each item by |I|. Thus:
best precision =
?
i best score(i)
|i ? I : defined(Si)| (2)
best recall =
?
i best score(i)
|I| (3)
The out-of-ten scorer will allow up to ten system
responses and will not divide the credit attributed
to each answer by the number of system responses.
This allows the system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select a
perfectly good translation that the annotators had not
thought of. By allowing up to ten translations in the
out-of-ten task the systems can hedge their bets to
find the translations that the annotators supplied.
oot score(i) =
?
s?Si frequency(s ? Ti)
|Ti|
(4)
oot precision =
?
i oot score(i)
|i ? I : defined(Si)| (5)
79
oot recall =
?
i oot score(i)
|I| (6)
We will refine the scores before June 2009 when
we will release the development data for this cross-
lingual task. We note that there was an issue that the
original LEXSUB out-of-ten scorer allowed dupli-
cates (McCarthy and Navigli, in press). The effect
of duplicates is that systems can get inflated scores
because the credit for each item is not divided by
the number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (in press) describe this oversight, identify
the systems that had included duplicates and explain
the implications. For our task there is an option for
the out-of-ten score. Either:
1. we remove duplicates before scoring or,
2. we allow duplicates so that systems can boost
their scores with duplicates on translations with
higher probability
We will probably allow duplicates but make this
clear to participants.
We may calculate additional best and out-of-ten
scores against the mode from the annotators re-
sponses as was done in LEXSUB, but we have not
decided on this yet. We will not run a multiword
task, but we will use the items identified as multi-
words as an optional filter to the scoring i.e. to see
how systems did without these items.
We will provide baselines and upper-bounds.
5 Systems
In the cross-lingual LEXSUB task, the systems will
have to deal with two parts of the problem, namely:
1. candidate collection
2. candidate selection
The first sub-task, candidate collection, refers to
consulting several resources and coming up with a
list of potential translation candidates for each tar-
get word and part of speech. We do not provide any
inventories, as with the original LEXSUB task, and
thus leave this task of coming up with the most suit-
able translation list (in contrast to the synonym list
required for LEXSUB) to the participants. As was
observed with LEXSUB, it is our intuition that the
quality of this translation list that the systems come
up with will determine to a large extent how well
the final performance of the system will be. Partici-
pants are free to use any ideas. However, a few pos-
sibilities might be to use parallel corpora, bilingual
dictionaries, a translation engine that only translates
the target word, or a machine translation system that
translates the entire sentences. Several of the bilin-
gual dictionaries or even other resources might be
combined together to come up with a comprehen-
sive translation candidate list, if that seems to im-
prove performance.
The second phase, candidate selection, concerns
fitting the translation candidates in context, and thus
coming up with a ranking as to which translations
are the most suitable for each instance. The highest
ranking candidate will be the output for best, and the
list of the top 10 ranking candidates will be the out-
put for out-of-ten. Again, participants are free to use
their creativity in this, while a range of possible al-
gorithms might include using a machine translation
system, using language models, word sense disam-
biguation models, semantic similarity-based tech-
niques, graph-based models etc. Again, combina-
tions of these might be used if they are feasible as
far as time and space are concerned.
We anticipate a minor practical issue to come up
with all participants, and that is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. This is directly re-
lated to the issue of dealing with characters with di-
acritics, and in our experience not all available soft-
ware packages and programs are able to handle dia-
critics and different character encodings in the same
way. This issue is inherent in all cross-lingual tasks,
and we leave it up to the discretion of the partici-
pants to effectively deal with it.
6 Post Hoc Issues
In LEXSUB a post hoc evaluation was conducted us-
ing fresh annotators to ensure that the substitutes
the systems came up with were not typically bet-
ter than those of the original annotators. This was
done as a sanity check because there was no fixed
inventory for the task and there will be a lot of varia-
80
tion in the task and sometimes the systems might do
better than the annotators. The post hoc evaluation
demonstrated that the post hoc annotators typically
preferred the substitutes provided by humans.
We have not yet determined whether we will run
a post hoc evaluation because of the costs of do-
ing this and the time constraints. Another option is
to reannotate a portion of our data using a new set
of annotators but restricting them to the translations
supplied by the initial set of annotations and other
translations from available resources. This would be
worthwhile but it could be done at any stage when
funds permit because we do not intend to supply a
set of candidate translations to the annotators since
we wish to evaluate candidate collection as well as
candidate selection.
7 Conclusions
In this paper we have outlined the cross-lingual lex-
ical substitution task to be run under the auspices
of SemEval-2010. The task will require annotators
and systems to find translations for a target word in
context. Unlike machine translation tasks, the whole
text is not translated and annotators are encouraged
to supply as many translations as fit the context. Un-
like previous WSD tasks based on parallel data, be-
cause we allow multiple translations and because we
do not restrict translations to those that provide clear
cut sense distinctions, we will be able to use the
dataset collected to investigate more subtle represen-
tations of meaning.
8 Acknowledgements
The work of the first and third authors has been partially
supported by a National Science Foundation CAREER
award #0747340. The work of the second author has been
supported by a Royal Society UK Dorothy Hodgkin Fel-
lowship.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), pages 1010?1015,
Edinburgh, Scotland.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Adam Kilgarriff. 2006. Word senses. In Eneko
Agirre and Phil Edmonds, editors, Word Sense Disam-
biguation, Algorithms and Applications, pages 29?46.
Springer.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese transla-
tion task. In Proceedings of the SENSEVAL-2 work-
shop, pages 37?44.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53, Prague,
Czech Republic.
Diana McCarthy and Roberto Navigli. in press. The en-
glish lexical substitution task. Language Resources
and Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and Be-
yond.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109?115, Philadelphia, USA.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-
2007 task 11: English lexical sample task via
English-Chinese parallel text. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 54?58, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Christopher Stokoe. 2005. Differentiating homonymy
and polysemy in information retrieval. In Proceedings
of the joint conference on Human Language Technol-
ogy and Empirical methods in Natural Language Pro-
cessing, pages 403?410, Vancouver, B.C., Canada.
81
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1624?1635, Dublin, Ireland, August 23-29 2014.
Novel Word-sense Identification
Paul Cook
?
, Jey Han Lau
?
, Diana McCarthy
?
and Timothy Baldwin
?
? Department of Computing and Information Systems, The University of Melbourne
? Department of Philosophy, King?s College London
? University of Cambridge
paulcook@unimelb.edu.au, jeyhan.lau@gmail.com,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
Automatic lexical acquisition has been an active area of research in computational linguistics
for over two decades, but the automatic identification of new word-senses has received attention
only very recently. Previous work on this topic has been limited by the availability of appropriate
evaluation resources. In this paper we present the largest corpus-based dataset of diachronic
sense differences to date, which we believe will encourage further work in this area. We then
describe several extensions to a state-of-the-art topic modelling approach for identifying new
word-senses. This adapted method shows superior performance on our dataset of two different
corpus pairs to that of the original method for both: (a) types having taken on a novel sense over
time; and (b) the token instances of such novel senses.
1 Novel word-senses
The meanings of words change over time with, in particular, established words taking on new senses. For
example, the usages of drop, wall, and blow up in the following sentences correspond to relatively-recent
senses of these words that appear to be quite common in text related to popular culture, but are not listed
in many dictionaries; for example, they are all missing from WordNet 3.0 (Fellbaum, 1998).
1. The reissue album drops March 27 and is an extension of Perry?s huge 2010 Teenage Dream. [drops
= ?comes out?, ?is released? ]
2. On Facebook, you can plainly see much of the data the site has on you, because it?s posted to your
wall. [wall = ?Facebook wall?, ?personal electronic noticeboard? ]
3. Why would I give him my number so he can blow up my phone the way he does my inbox. [blow up
= ?overwhelm with messages? ]
Computational lexicons are an essential component of systems for a variety of natural language process-
ing (NLP) tasks. The success of such systems, therefore, depends on the quality of the lexicons they use,
and (semi-)automatic techniques for identifying new word-senses could benefit applied NLP by helping
to keep lexicons up-to-date. In revising dictionaries, lexicographers must identify new word-senses, in
addition to new words themselves; methods which identify new word-senses could therefore also help to
keep dictionaries current.
In this paper, because of the need for lexicon maintenance, we focus on relatively-new word-senses.
Specifically, we consider the identification of word-senses that are not attested in a reference corpus,
taken to represent standard usage, but that are attested in a focus corpus of newer texts.
Lau et al. (2012) introduced the task of novel sense identification. They presented a method for
identifying novel word-senses ? described here in Section 4 ? and evaluated this method on a very
small dataset consisting of just five lemmas having a novel sense in a single corpus pair. Cook et al.
(2013) extended the method of Lau et al. to incorporate knowledge of the expected domains of new word-
senses, but did not conduct a rigorous empirical evaluation. The remainder of this paper is structured
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1624
as follows. After discussing related work in Section 2, we present a substantially-expanded evaluation
dataset in Section 3, that is based on a second corpus pair and consists of many more lemmas with a
novel sense. We describe the models used by Lau et al. and Cook et al., and our new extensions to
them, in Section 4. In Section 5 we analyse the results of novel sense identification, and consider a new
baseline for this task. We demonstrate that the extended methods give an improvement over the original
method of Lau et al. We conclude by discussing some previously-unexplored variations on novel sense
identification, and limitations of the approaches considered.
The primary contributions of this paper are: (1) development of a novel sense detection dataset much
larger than has been used in research to date; (2) development and evaluation of a new baseline for
novel sense detection, reformulations of the method of Lau et al., and a method that incorporates only
the expected domain(s) of novel senses; (3) empirical evaluation of the method of Cook et al.; and (4)
extension of the novel sense detection method of Cook et al. to automatically acquire information about
the expected domain(s) of novel senses.
2 Related work
Identifying diachronic changes in word-sense is a challenge that has only been considered rather recently
in computational linguistics. Sagi et al. (2009) and Cook and Stevenson (2010) propose methods to
identify specific types of semantic change ? widening and narrowing, and amelioration and pejoration,
respectively ? based on specific properties of these phenomena. Gulordava and Baroni (2011) identify
diachronic sense change in an n-gram database, but using a model that is not restricted to any particular
type of semantic change. Cook and Hirst (2011) consider the impact of sense frequency on methods for
identifying novel senses. Crucially, all of the aforementioned approaches are type-based: they are able
to identify words that have undergone a change in meaning, but not the token instances which give rise
to these sense differences.
Bamman and Crane (2011) use a parallel Latin?English corpus to induce word senses and build a
WSD system, which they then apply to study diachronic variation in sense frequency. Rohrdantz et al.
(2011) present a system for visualizing changes in word usage over time. Crucially, in these token-based
approaches there is a clear connection between (induced) word-senses and tokens, making it possible to
identify usages of a specific (new) sense.
Other work has focused on sense differences between dialects and domains. Peirsman et al. (2010)
consider the identification of words that are typical of Belgian and Netherlandic Dutch, due to either
marked frequency or sense. McCarthy et al. (2007) consider the identification of predominant word-
senses in corpora, including differences between domains. However, this approach does not identify new
senses as it relies on a pre-existing sense inventory. Carpuat et al. (2013) identify words in a domain-
specific parallel corpus with novel translations.
The method proposed by Lau et al. (2012), and extended by Cook et al. (2013), identifies novel word-
senses using a state-of-the-art word-sense induction (WSI) system. This token-based approach offers a
natural account of polysemy and not only identifies word types that have a novel sense, but identifies
the token instances of the hypothesized novel senses, without reliance on parallel text or a pre-existing
sense inventory. We therefore adopt this method for evaluation on our new dataset, and propose further
extensions to this method.
3 Datasets
Evaluating approaches to identifying semantic change is a challenge due to the lack of appropriate evalu-
ation resources (i.e., corpora for the appropriate time periods, known to exhibit particular sense changes);
indeed, most previous approaches have used very small datasets (e.g., Sagi et al., 2009; Cook and Steven-
son, 2010; Bamman and Crane, 2011). In this study we consider two datasets of relatively newly-coined
word-senses: (1) an extended version of the dataset based on the BNC (Burnard, 2000) and ukWaC (Fer-
raresi et al., 2008) used by Lau et al. (2012); and (2) a new dataset based on the SiBol/Port Corpus.
1
This
1
http://www3.lingue.unibo.it/blog/clb/?page_id=8
1625
is the largest dataset for evaluating approaches to identifying diachronic semantic change constructed
from corpus evidence to be presented to date.
3.1 BNC?ukWaC
Lau et al. (2012) take the written portion of the BNC (approximately 87 million words of British English
from the late 20th century) as the reference corpus, and a similarly-sized random sample of documents
from the ukWaC (a Web corpus built from the .uk domain in 2007) as the focus corpus. They used
TreeTagger (Schmid, 1994) to tokenise and lemmatise both corpora.
A set of words that has acquired a new sense between the late 20th and early 21st centuries ? the time
periods of the reference and focus corpora ? is required. The Concise Oxford English Dictionary aims
to document contemporary usage, and has been published in numerous editions including Thompson
(1995, COD95) and Soanes and Stevenson (2005, COD08), enabling the identification of new senses
amongst the entries in COD08 relative to COD95. Manually searching these dictionaries for new senses
would be time intensive, but new words often correspond to concepts that are culturally salient (Ayto,
2006), and one can leverage this observation to speed up the process of finding some candidate words
with novel senses.
2
Between the time periods of the reference and focus corpora, computers and the Internet have become
much more mainstream in society. Lau et al. therefore extracted all headwords in COD08 whose entries
contain the word computing. They then carefully annotated these lemmas to identify those that indeed
exhibit the novel sense indicated in the dictionary in the corpora. Here, we expand Lau et al.?s dataset by
extracting all headwords including any of the following words code, computer, internet, network, online,
program, web, and website. We then follow a similar annotation process to Lau et al.
An annotator read the entries for the selected lexical items in COD95 and COD08, and identified those
which have a clear sense related to computers or the Internet in COD08 that is not present in COD95;
such senses are referred to as novel senses. This process, along with all the annotation in this section
(including Section 3.2), is carried out by native English-speaking authors of this paper and graduate
students in computational linguistics.
To ensure that the words identified from the dictionaries do in fact have a new sense in the ukWaC
sample compared to the BNC, we examine word sketches (Kilgarriff et al., 2004)
3
for each of these
lemmas in the BNC and ukWaC for collocates that likely correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel sense in the BNC, or fail to find evidence of the novel
sense in the ukWaC sample.
4
We further examine the usage of these words in the corpora. We extract a random sample of 100
usages of each lemma from the BNC and ukWaC sample, and annotate these usages as to whether they
correspond to the novel sense or not. This binary distinction is easier than fine-grained sense annotation,
and since we do not use these annotations for formal evaluation ? only for selecting items for our dataset
? we do not carry out an inter-annotator agreement study here. We eliminate any lemma for which we
find evidence of the novel sense in the usages from the BNC, or for which we do not find evidence of the
novel sense in the ukWaC sample usages.
5
This process resulted in the identification of two lemmas not in the dataset of Lau et al., with frequency
greater than 1000 in the ukWaC sample, and having a novel sense in the ukWaC compared to the BNC
(feed (n) and visit (v)). Combining these new lemmas with the dataset of Lau et al. gives an expanded
dataset consisting of seven lemmas. For both of the two new lemmas, a second annotator annotated
the sample of 100 usages from the ukWaC. The observed agreement and unweighted Kappa for this
annotation task for all seven lemmas is 97.4% and 0.93, respectively, indicating that this is indeed a
relatively easy annotation task. The annotators discussed the small number of disagreements to reach
2
We access the dictionaries in the same way as Lau et al., namely we use COD08 online via http://oxfordreference.
com, and the paper version of COD95.
3
http://www.sketchengine.co.uk/
4
We examine word sketches for the full ukWaC because this version of the corpus is available through the Sketch Engine.
5
We use the IMS Open Corpus Workbench (http://cwb.sourceforge.net/) to extract usages of our target lemmas
from the corpora. This extraction process fails in a number of cases, and so we also eliminate such items from our dataset.
1626
BNC?ukWaC
Lemma Frequency Novel sense definition
domain (n) 41 Internet domain
export (v) 28 export data
feed (n) 23 data feed
mirror (n) 10 mirror website
poster (n) 4 one who posts online
visit (v) 28 access a website
worm (n) 30 malicious program
SiBol/Port
Lemma Frequency Novel sense definition
cloud (n) 9 Internet-based computational resources
drag (v) 1 move on a computer screen using a mouse
follower (n) 34 Twitter follower
help (n) 1 displayed instructions, e.g., help menu
hit (n) 2 search hit
platform (n) 22 computing platform
poster (n) 5 one who posts online
reader (n) 3 e-reader
rip (v) 1 copy music
site (n) 39 website
text (n) 39 text message
visit (v) 7 access a website
wall (n) 2 Facebook wall
Table 1: Lemmas in the BNC?ukWaC and SiBol/Port datasets. For each lemma, the frequency of its
novel sense in the annotated sample of usages from the focus corpus, and a definition of its novel sense,
are shown.
consensus. The seven lemmas in this dataset are shown in Table 1, along with definitions of their novel
senses, and the frequencies of their novel senses in the focus corpus.
Lau et al. compared the novelty of the lemmas with a novel sense to that of a same-size set of distractor
lemmas not having a novel sense. Here we consider a much larger set of 50 distractors ? 25 nouns and
25 verbs ? randomly sampled from a similar frequency range as the items with a novel sense.
One shortcoming of this dataset (and indeed the subset of it used by Lau et al.) is that text types are
represented to different extents in the BNC and ukWaC, with, for example, texts related to the Internet
being much more common in the ukWaC. Such differences in corpus composition are a noted challenge
for approaches to identifying lexical semantic differences between corpora (Peirsman et al., 2010). In the
following subsection we therefore consider the creation of a new dataset from more-comparable corpora.
3.2 SiBol/Port
The SiBol/Port Corpus consists of texts from several British newspapers for the years 1993, 2005, and
2010; we use the 1993 and 2010 portions of this corpus ? referred to as SP1993 and SP2010 ? as
our reference and focus corpora, respectively. SP1993 and SP2010 contain approximately 93M and 99M
words, respectively. In contrast to BNC?ukWaC, our reference and focus corpora are now comparable,
in that they both consist of texts from British newspapers but they differ with respect to the specific year.
The novel word-senses in the BNC?ukWaC dataset are all related to computers and the Internet, but
there has been recent lexical semantic change unrelated to technology as well (e.g., sick can be used to
mean ?excellent?). In an effort to include such non-technical novel senses in this new dataset, we obtain
a list of headwords for which a sense was added to the Macmillan English Dictionary for Advanced
1627
Learners (MEDAL)
6
since its first edition (Rundell and Fox, 2002), courtesy of Macmillan Dictionaries.
Beginning with these candidates from MEDAL, and the items extracted from COD from Section 3.1, we
discard any lemma whose frequency is less than 1000 in SP1993 or SP2010.
As for the BNC?ukWaC dataset, an annotator examined word sketches for these lemmas. However, it
is possible that the novel sense for a lemma is present in a corpus, but that we fail to find evidence for it in
that lemma?s word sketch. We therefore also obtain judgements from two annotators as to whether each
novel sense is expected to be very infrequent (or unattested) in SP2010. To reduce subsequent annotation
effort, we discard any lemma for which its novel sense is believed to be infrequent in SP2010 by both
judges, and is not found in the word sketch from SP2010.
Annotators then annotate a random sample of 100 usages of each lemma in the reference and focus
corpora as before, and again eliminate any lemma for which we find evidence of its novel sense in the
reference corpus, or fail to find evidence of that sense in the focus corpus. We identify thirteen lemmas
having a novel sense in SP2010 relative to SP1993. These lemmas are also shown in Table 1.
We obtain a second set of annotations for the usages of these lemmas in the sample from SP2010,
with each lemma being annotated by a different annotator than before. The observed agreement and
unweighted Kappa between the two sets of annotations is 96.2% and 0.81, respectively. In cases of
disagreement, a final annotation is again reached through discussion.
We randomly select 164 lemmas (116 nouns and 48 verbs) from a similar frequency range as the
lemmas having a novel sense, to serve as distractors.
Both the BNC?ukWaC and SiBol/Port datasets have been made available.
7
4 The WSI-based approach to novel word-sense detection
In this section we describe the WSI-based method of Lau et al. (2012) for detecting novel senses, and an
extension of this method from Cook et al. (2013). We then present new extensions of this method.
The Lau et al. (2012) WSI model is based on a Hierarchical Dirichlet Process (HDP, Teh et al., 2006),
which is a non-parametric variant of a topic model that, like the commonly-used Latent Dirichlet Allo-
cation (LDA, Blei et al., 2003), learns topics (in the form of multinomial probability distributions over
words) and per-document topic assignments (in the form of multinomial probability distributions over
topics) for a collection of documents; unlike LDA, however, it also optimises the number of topics in an
unsupervised data-driven manner. In the context of WSI, by creating ?documents? that consist of sen-
tences containing a target word, we can view the topics learnt by topic models as the sense representation
of the target word. Indeed, topic models have been previously applied to WSI (e.g., Brody and Lapata,
2009; Yao and Van Durme, 2011).
To generate the input for the topic model, the documents are tokenised (in this case, a ?document? is
a short context, typically 1?3 sentences, containing a target word) into a bag of words. All words are
lemmatised, and stopwords and low frequency terms are removed. Positional word features ? commonly
used in WSI ? for each of the three words to the left and right of the target word are also included.
To induce the senses of a target word w from a given set of usages of w, HDP is run on those usages
(represented according to the features described above) to induce topics; these topics are then interpreted
as representing the senses of w (one topic per sense). To determine the sense assigned to each instance,
the system aggregates over the topic assignments for each word in the context of w, and selects the topic
with the highest aggregated probability, i.e., argmax
z
P (t = z|d), where d is a document and t is a topic.
Recently, Lau et al. (2013a,b) found this method to give the overall best performance on two WSI
shared tasks (Jurgens and Klapaftis, 2013; Navigli and Vannella, 2013), demonstrating that the method
is competitive with the state-of-the-art in WSI, and appropriate as the basis for a method for identifying
novel word-senses.
6
http://www.macmillandictionary.com/
7
http://www.csse.unimelb.edu.au/
~
tim/etc/novel-sense-dataset.tgz
1628
4.1 Novel Sense Detection
Following Lau et al. (2012), to detect novel senses of a target word using this WSI method, we jointly
topic model two corpora: a reference corpus ? taken to represent standard usage ? and a focus corpus
of newer texts potentially containing novel senses. In other words, we extract usages of a target word w
from both corpora, and then topic model the pooled instances of w. Under this approach, the discovered
topics are applicable to both corpora, so there is no need to reconcile two different sets of topics. For the
experiments in this paper, we extract three sentences of context for each usage, one sentence to either
side of the usage of the target word.
As each usage is given a sense assignment, we can identify novel senses ? senses present in the focus
corpus, but unattested in the reference corpus ? based on differences in the sense distribution for a given
word between the two corpora. Lau et al. present a Novelty score which is proportional to the following:
Novelty
Ratio
(s) =
p
f
(s)
p
r
(s)
(1)
where p
f
(s) and p
r
(s) are the proportion of usages of a given word corresponding to sense s in the focus
corpus and reference corpus, respectively, calculated using smoothed maximum likelihood estimates.
The score for a given lemma is the maximum score for any of its induced senses. We refer to the novel
sense for a lemma as the induced sense corresponding to this maximum.
4.2 Alternative Formulations of Novelty
The WSI system underlying the approach of Lau et al. labels each usage of a target lemma with an
induced sense. Therefore, any approach to identifying keywords ? words that are substantially more
frequent in one corpus than another ? can potentially be applied to identify novel senses, by viewing
?words? as (word,sense) tuples. We consider a version of Novelty based on the difference in relative
frequency of an induced sense in the focus and reference corpora, as below:
Novelty
Diff
(s) = p
f
(s)? p
r
(s) (2)
We consider a further new variant of Novelty based on the log-likelihood ratio of an induced sense in the
two corpora, referred to as Novelty
LLR
.
4.3 Incorporating knowledge of expected topics of novel senses
Cook et al. (2013) extended Lau et al.?s method by incorporating the observation that many neologisms
are related to topics that are culturally salient (e.g., Ayto, 2006); nowadays we see many neologisms
related to computing and the Internet. Indeed this observation was used to construct the gold-standard
dataset for this study. Cook et al. identified a set of words, W , related to computing and the Inter-
net, based on manual analysis of keywords for the corpora they considered. They then formulated the
Relevance of an induced sense s for a given word as follows:
Relevance
Manual
(s) =
?
w?W
p(w|s) (3)
For a given lemma, Relevance
Manual
is the maximum of this score for any of its induced senses, similar
to Novelty.
Following Cook et al., we calculate Relevance and Novelty for each induced sense of each lemma,
and then rank all the induced senses by these measures independently. We then compute the rank sum
of each induced sense of each lemma under these two rankings. The final score for a given lemma is
then the rank sum of its highest-ranked sense, and this sense is taken as that lemma?s novel sense. We
refer to this new method as ?Rank Sum?. Cook et al. only considered Novelty and Rank Sum; here we
additionally consider Relevance on its own.
For the keywords, we manually construct a set of words related to computing and the Internet, the
topics for which we expect to observe many novel senses in both of our datasets, in a similar way to
Cook et al. In order to minimize annotation effort, we concentrate on words that are more-frequent in the
1629
focus corpus than the reference corpus. For a given corpus pair, we begin by computing the keywords
for those corpora using Kilgarriff?s (2009) method.
8
Two annotators ? both computational linguists
and not authors of this paper ? independently scanned the top-1000 keywords for the focus corpus, and
selected those that were, based on their intuition, related to computing and the Internet. We then took
the topically-relevant words for a given corpus pair to be those in the intersection of the sets of words
selected by the two annotators. For BNC?ukWaC and SiBol/Port this gives 102 and 30 topically-relevant
words, respectively. This annotation required, on average, 23 minutes per annotator per corpus pair to
complete. Examples of the keywords selected for SiBol/Port include broadband, click, device, online,
and tweet.
4.4 Automatically-extracting keywords
We propose a new fully-automated method for identifying a set of topically-relevant keywords. Because
of the differences in corpus composition, the BNC?ukWaC keywords are often related to computing and
the Internet. To automatically obtain topically-relevant words, we take the top-1000 keywords for the
ukWaC relative to the BNC (i.e., the same keywords annotated for the BNC?ukWaC in Section 4.3).
The keywords for SiBol/Port are less-clearly related to the topics of interest, so we therefore use the
topically-relevant keywords from BNC?ukWaC for both datasets.
5 Results
In the following subsections we consider results at the type and then token level.
5.1 Type-level results
In these experiments we rank all items ? lemmas with a novel sense, and distractors ? by the various
Novelty, Relevance and Rank Sum methods for the BNC?ukWaC and SiBol/Port datasets. When a
lemma takes on a new sense, it might also increase in frequency. We therefore also consider a baseline in
which we rank the lemmas by the ratio of their frequency in the focus corpus and the reference corpus.
This baseline has not been previously considered by Lau et al. (2012) or Cook et al. (2013).
To compare approaches, we examine precision?recall curves in Figures 1 and 2. In an applied setting,
we envision these ranked lists being manually examined; we are therefore primarily interested in the
highly-ranked items, i.e., the left portion of the precision?recall curves.
For BNC?ukWaC (Figure 1), Novelty
Diff
and Novelty
Ratio
perform much better than Novelty
LLR
, but
not better than the frequency ratio baseline, at least for the left-most portion of the precision?recall
curve. Surprisingly, for Relevance, Relevance
Auto
outperforms Relevance
Manual
. This could be because
the focus corpus exhibits a clear topical bias towards computing and the Internet (the expected domain
of many neologisms in the focus corpus), and therefore a larger set of potentially noisy keywords is
more informative than a smaller, hand-selected set. All of the measures including the baseline, except
for Novelty
LLR
, assign higher scores to lemmas with a gold-standard novel sense than the distractors,
according to a one-sided Wilcoxon rank sum test (p < 0.05 in each case).
Turning to SiBol/Port in Figure 2, the frequency ratio baseline is much less effective here; the fre-
quency of the gold-standard novel senses is much lower overall than for BNC?ukWaC. All of the Novelty
and Relevance methods outperform the baseline, and ? with the exception of Novelty
Ratio
? rank the
lemmas with a gold-standard novel sense higher than the distractors (again using a one-sided Wilcoxon
rank sum test and p < 0.05). Furthermore, in this case, Relevance
Manual
outperforms Relevance
Auto
, as
expected.
In terms of the three Novelty measures, only Novelty
Diff
ranked items with a novel sense higher than
the distractors for both datasets. We therefore also show results for the Rank Sum approach combin-
ing Novelty
Diff
and each of Relevance
Manual
and Relevance
Auto
, denoted Rank Sum
Diff,manual
and Rank
Sum
Diff,auto
, respectively, in Figures 1 and 2. For both BNC?ukWaC and SiBol/Port, Rank Sum
Diff,manual
8
Using this method, the keywordness score for a given word is simply the ratio of its frequency per million words, plus a
constant, in two corpora; we set the constant to 100, the value recommended by Kilgarriff.
1630
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 1: Precision?recall curve for the BNC?ukWaC dataset.
gives the best performance, and is a clear improvement over either of the individual methods. As ex-
pected, the performance of Rank Sum
Diff,auto
is not as good, but is nevertheless an improvement over the
frequency ratio baseline for both datasets and provides an alternative to manual scrutiny of the keywords.
To further examine the potential of incorporating knowledge of the expected domains of novel senses
to improve novel sense identification, we consider the case of cloud (n) from the SiBol/Port dataset. The
highest-probability words for the topic with highest Novelty
Diff
are the following: ash, volcanic, flight,
@card@,
9
travel, airline, volcano, airport, air, cloud. This sense appears to be related to the eruption
of the Eyjafjallajo?kull volcano, a major event in 2010 (the year from which the SiBol/Port focus corpus
is taken). Such topical differences, which do not correspond to a novel sense, are a problem for any
approach to identifying lexical semantic differences between two corpora based on differences in the
lexical context of a target word, and indeed observations such as this motivated our use of the methods
incorporating Relevance. The highest probability words for the topic with highest Relevance
Auto
are
the following: cloud, @card@, company, service, business, computing, market, security, datum, need.
This topic appears to correspond to the expected novel sense of Internet-based computational resources,
demonstrating the potential to improve a system for identifying novel word-senses by incorporating
knowledge of the expected domains of neologisms. Moreover, incorporating Relevance is particularly
powerful for avoiding false positives. For example, the distractor clause (n) is the lemma with the
sixth-highest Novelty
Diff
for SiBol/Port. The highest probability words for the corresponding topic are
the following: contract, @card@, club, player, million, england, capello, manager, sign, deal. This
induced sense appears to be related to clauses in Fabio Capello?s contract as manager of the England
national football team, and is not a novel sense of clause. However, none of the induced senses of clause
have high Relevance
Auto
or Relevance
Manual
, and so incorporating information from Relevance can avoid
incorrectly identifying this lemma as having a novel sense.
9
A generic token signifying a cardinal number.
1631
0.0 0.2 0.4 0.6 0.8 1.0
Recall
0.0
0.2
0.4
0.6
0.8
1.0
P
r
e
c
i
s
i
o
n
Freq ratio
Nov: Diff
Nov: LLR
Nov: Ratio
Rel: Man
Rel: Auto
RS: Diff+Man
RS: Diff+Auto
Figure 2: Precision?recall curve for the SiBol/Port dataset.
5.2 Token-level results
In this section, we consider the token-level identification of instances of the gold-standard novel senses.
We compare Novelty, Relevance, and Rank Sum to a baseline that assigns all usages of a lemma to a
single topic which is selected as the novel sense; in this case recall is 1, and precision is proportional to
the frequency of the novel sense. We further consider the theoretical upper-bound of a method which
selects a single topic as the novel sense, based on the output of the HDP-based WSI method; this oracle
selects the best topic in terms of F-score as the novel sense. Results are presented in Table 2.
Each variant of Novelty and Relevance is an improvement over the baseline, although the Relevance
measures don?t perform as well as the Novelty ones, despite this dataset only containing novel senses
related to computing (despite our efforts to include non-technical novel senses). For consistency with
the presentation of the type-level results, we again consider Rank Sum using Novelty
Diff
, even though it
doesn?t perform as well as Novelty
LLR
or Novelty
Ratio
on BNC?ukWaC. Using either automatically- or
manually-obtained keywords, the performance of Rank Sum on BNC?ukWaC is remarkably on par with
the upper-bound, although for SiBol/Port there is little or no improvement over Novelty
Diff
. Neverthe-
less, these findings are further indication that novel sense identification can be improved by incorporating
information about the topics for which we expect to see novel senses. However, this approach is par-
ticularly helpful at the type-level, where information about the expected topics of novel senses prevents
lemmas not having a novel sense (i.e., the distractors) from being assigned high novelty.
6 Discussion and conclusion
The methods considered in this paper could be applied to any corpus pair, and potentially to identify
lexical semantic differences between, for example, domains or language varieties. The focus of this
study is English; sufficiently-large comparable corpora of national varieties of English (e.g., British and
American English), are not readily-available, but could potentially be inexpensively constructed in the
future (Cook and Hirst, 2012). We conducted some preliminary experiments using domain-specific sports
1632
Method
F-score
BNC?ukWaC SiBol/Port
Novelty
Diff
0.57 0.29
Novelty
LLR
0.67 0.28
Novelty
Ratio
0.66 0.28
Relevance
Auto
0.48 0.24
Relevance
Manual
0.45 0.27
Rank Sum
Diff,auto
0.72 0.30
Rank Sum
Diff,manual
0.72 0.29
Upper-bound 0.72 0.42
Baseline 0.36 0.20
Table 2: Token-level F-score for the BNC?ukWaC and SiBol/Port datasets using variants of Novelty,
Relevance, and Rank Sum. The F-score of an oracle upper-bound and baseline are also shown.
and finance corpora (Koeling et al., 2005) and the BNC. However, in these experiments we observed
very high Novelty
Ratio
for many distractors (selected in a similar way to our other experiments). Unlike
the case of time difference, in corpora from different domains, an arbitrarily chosen word will tend to
cooccur with very different words in the corpora, and Novelty
Ratio
will consequently be high. To address
vocabulary differences between corpora, in their experiments on identifying lexical semantic differences
between Dutch dialects, Peirsman et al. (2010) restricted the context words used to represent a target word
to those with moderate frequency in each of the two corpora used. We considered a similar restriction in
experiments on SiBol/Port, but did not see an overall improvement in performance.
We demonstrated that the performance of a method for identifying novel word-senses can be improved
by incorporating information ? acquired manually or automatically ? about the expected topics of
novel senses, which tend to be related to culturally-salient concepts. In future work, we intend to consider
improved approaches for automatically identifying topically-relevant words by incorporating information
about the top keywords of a corpus harvested from the Web for the domain of interest (e.g., PVS et al.,
2012). We also believe that topic models could be useful for identifying emerging or changing domains
themselves given the reference and focus corpus, and related work in this area (e.g., Wang andMcCallum,
2006; Blei and Lafferty, 2007).
To conclude, we have presented the largest type- and token-level dataset of diachronic sense differ-
ences to date, drawing on two pairs of corpora, and have made this dataset available. We applied a
recently-proposed WSI-based method to the task of finding sense differences in this data. We demon-
strated that, while the method shows promise, on a type-based task it is comparable to a a simple fre-
quency baseline, which had not been previously considered for this task. We carried out the first empirical
evaluation of a recently-proposed extension of this method that incorporates manually-acquired knowl-
edge of the expected domains of new senses, and found it to have superior performance at both the type
and token level. We further proposed and evaluated an approach that only uses this domain knowledge,
and a method for automating its acquisition.
Acknowledgments
We thank Michael Rundell and Macmillan Dictionaries for providing the list of headwords added to
MEDAL since its first edition, and Charlotte Taylor for providing us with early access to SiBol/Port. We
also thank Richard Fothergill, Karl Grieser, and Andrew Mackinlay for their help in annotation. This
research was supported in part by funding from the Australian Research Council.
References
John Ayto. 2006. Movers and Shakers: A Chronology of Words that Shaped our Age. Oxford University
Press, Oxford.
1633
David Bamman and Gregory Crane. 2011. Measuring historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Digital Libraries (JCDL 2011), pages 1?10. Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research,
3:993?1022.
David M. Blei and John D. Lafferty. 2007. Latent dirichlet allocation. The Annals of Applied Statistics,
1(1):17?35.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?111. Athens, Greece.
Lou Burnard. 2000. User Reference Guide for the British National Corpus. Technical report, Oxford
University Computing Services.
Marine Carpuat, Hal Daume? III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages
1435?1445. Sofia, Bulgaria.
Paul Cook and Graeme Hirst. 2011. Automatic identification of words with novel but infrequent
senses. In Proceedings of the 25th Pacific Asia Conference on Language Information and Compu-
tation (PACLIC 25), pages 265?274. Singapore.
Paul Cook and Graeme Hirst. 2012. Do Web corpora from top-level domains represent national varieties
of English? In Actes des 11es Journ
?
ees Internationales d?Analyse Statistique des Donn
?
ees Textuelles /
Proceedings of the 11th International Conference on Textual Data Statistical Analysis, pages 281?293.
Lie`ge, Belgium.
Paul Cook, Jey Han Lau, Michael Rundell, Diana McCarthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for detecting new word-senses. In Electronic lexicography
in the 21st century: thinking outside the paper. Proceedings of the eLex 2013 conference, pages 49?65.
Tallinn, Estonia.
Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh International Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34. Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evalu-
ating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54. Marrakech, Morocco.
Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of
semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics, pages 67?71. Edinburgh, Scotland.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-2013 task 13: Word sense induction for graded
and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation
(SemEval 2013), pages 290?299. Atlanta, USA.
Adam Kilgarriff. 2009. Simple maths for keywords. In Proceedings of the Corpus Linguistics Confer-
ence. Liverpool, UK.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David Tugwell. 2004. The Sketch Engine. In Proceed-
ings of the Eleventh EURALEX International Congress (EURALEX 2004), pages 105?116. Lorient,
France.
Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predom-
inant sense acquisition. In Proceedings of Human Language Technology Conference and Conference
1634
on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 419?426. Van-
couver, Canada.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a. unimelb: Topic modelling-based word sense
induction. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013),
pages 307?311. Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b. unimelb: Topic modelling-based word sense
induction for web snippet clustering. In Proceedings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013), pages 217?221. Atlanta, USA.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense
induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter
of the Association for Computational Linguistics (EACL 2012), pages 591?601. Avignon, France.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics, 33(4):553?590.
Roberto Navigli and Daniele Vannella. 2013. SemEval-2013 task 11: Word sense induction and dis-
ambiguation within an end-user application. In Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?201. Atlanta, USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language Engineering, 16(4):469?491.
Avinesh PVS, Diana McCarthy, Dominic Glennon, and Jan Pomika?lek. 2012. Domain specific corpora
from the Web. In Proceedings of the 15th Euralex International Congress, pages 336?342. Oslo,
Norway.
Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Daniel A. Keim, and Frans Plank.
2011. Towards tracking semantic change by visual analytics. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics: Human Language Technologies (ACL 2011),
pages 305?310. Portland, USA.
Michael Rundell and Gwyneth Fox, editors. 2002. Macmillan English Dictionary for Advanced Learn-
ers. Macmillan Education, Oxford, UK.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word
meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?111. Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing, pages 44?49. Manchester, UK.
Catherine Soanes and Angus Stevenson, editors. 2008. The Concise Oxford English Dictionary. Oxford
University Press, Oxford, UK, eleventh (revised) edition. Oxford Reference Online.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Association, 101:1566?1581.
Della Thompson, editor. 1995. The Concise Oxford Dictionary of Current English. Oxford University
Press, Oxford, UK, ninth edition.
Xuerei Wang and Andrew McCallum. 2006. Topics over time: A non-Markov continuous-time model of
topical trends. In Proceedings of the Eleventh International Conference on Knowledge Discovery and
Data Mining, pages 424?433. Philadelphia, USA.
Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10?14.
Portland, USA.
1635
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 591?601,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Word Sense Induction for Novel Sense Detection
Jey Han Lau,?? Paul Cook,? Diana McCarthy, ?
David Newman,? and Timothy Baldwin??
? NICTA Victoria Research Laboratory
? Dept of Computer Science and Software Engineering, University of Melbourne
? Dept of Computer Science, University of California Irvine
? Lexical Computing
jhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, newman@uci.edu, tb@ldwin.net
Abstract
We apply topic modelling to automatically
induce word senses of a target word, and
demonstrate that our word sense induction
method can be used to automatically de-
tect words with emergent novel senses, as
well as token occurrences of those senses.
We start by exploring the utility of stan-
dard topic models for word sense induction
(WSI), with a pre-determined number of
topics (=senses). We next demonstrate that
a non-parametric formulation that learns an
appropriate number of senses per word ac-
tually performs better at the WSI task. We
go on to establish state-of-the-art results
over two WSI datasets, and apply the pro-
posed model to a novel sense detection task.
1 Introduction
Word sense induction (WSI) is the task of auto-
matically inducing the different senses of a given
word, generally in the form of an unsupervised
learning task with senses represented as clusters
of token instances. It contrasts with word sense
disambiguation (WSD), where a fixed sense in-
ventory is assumed to exist, and token instances
of a given word are disambiguated relative to the
sense inventory. While WSI is intuitively appeal-
ing as a task, there have been no real examples of
WSI being successfully deployed in end-user ap-
plications, other than work by Schutze (1998) and
Navigli and Crisafulli (2010) in an information re-
trieval context. A key contribution of this paper
is the successful application of WSI to the lexico-
graphical task of novel sense detection, i.e. identi-
fying words which have taken on new senses over
time.
One of the key challenges in WSI is learning
the appropriate sense granularity for a given word,
i.e. the number of senses that best captures the
token occurrences of that word. Building on the
work of Brody and Lapata (2009) and others, we
approach WSI via topic modelling ? using La-
tent Dirichlet Allocation (LDA: Blei et al(2003))
and derivative approaches ? and use the topic
model to determine the appropriate sense gran-
ularity. Topic modelling is an unsupervised ap-
proach to jointly learn topics ? in the form of
multinomial probability distributions over words
? and per-document topic assignments ? in the
form of multinomial probability distributions over
topics. LDA is appealing for WSI as it both as-
signs senses to words (in the form of topic alloca-
tion), and outputs a representation of each sense
as a weighted list of words. LDA offers a solu-
tion to the question of sense granularity determi-
nation via non-parametric formulations, such as
a Hierarchical Dirichlet Process (HDP: Teh et al
(2006), Yao and Durme (2011)).
Our contributions in this paper are as follows.
We first establish the effectiveness of HDP for
WSI over both the SemEval-2007 and SemEval-
2010WSI datasets (Agirre and Soroa, 2007; Man-
andhar et al 2010), and show that the non-
parametric formulation is superior to a standard
LDA formulation with oracle determination of
sense granularity for a given word. We next
demonstrate that our interpretation of HDP-based
WSI is superior to other topic model-based ap-
proaches to WSI, and indeed, better than the best-
published results for both SemEval datasets. Fi-
nally, we apply our method to the novel sense de-
tection task based on a dataset developed in this
research, and achieve highly encouraging results.
2 Methodology
In topic modelling, documents are assumed to ex-
hibit multiple topics, with each document having
591
its own distribution over topics. Words are gen-
erated in each document by first sampling a topic
from the document?s topic distribution, then sam-
pling a word from that topic. In this work we
use the topic models?s probabilistic assignment of
topics to words for the WSI task.
2.1 Data Representation and Pre-processing
In the context of WSI, topics form our sense rep-
resentation, and words in a sentence are gener-
ated conditioned on a particular sense of the target
word. The ?document? in the WSI case is a sin-
gle sentence or a short document fragment con-
taining the target word, as we would not expect
to be able to generate a full document from the
sense of a single target word.1 In the case of the
SemEval datasets, we use the word contexts pro-
vided in the dataset, while in our novel sense de-
tection experiments, we use a context window of
three sentences, one sentence to either side of the
token occurrence of the target word.
As our baseline representation, we use a bag of
words, where word frequency is kept but not word
order. All words are lemmatised, and stopwords
and low frequency terms are removed.
We also experiment with the addition of po-
sitional context word information, as commonly
used in WSI. That is, we introduce an additional
word feature for each of the three words to the left
and right of the target word.
Pado? and Lapata (2007) demonstrated the im-
portance of syntactic dependency relations in the
construction of semantic space models, e.g. for
WSD. Based on these findings, we include depen-
dency relations as additional features in our topic
models,2 but just for dependency relations that in-
volve the target word.
2.2 Topic Modelling
Topic models learn a probability distribution over
topics for each document, by simply aggregating
the distributions over topics for each word in the
document. In WSI terms, we take this distribu-
tion over topics for each target word (?instance?
in WSI parlance) as our distribution over senses
for that word.
1Notwithstanding the one sense per discourse heuristic
(Gale et al 1992).
2We use the Stanford Parser to do part of speech tagging
and to extract the dependency relations (Klein and Manning,
2003; De Marneffe et al 2006).
In our initial experiments, we use LDA topic
modelling, which requires us to set T , the num-
ber of topics to be learned by the model. The
LDA generative process is: (1) draw a latent
topic z from a document-specific topic distribu-
tion P (t = z|d) then; (2) draw a word w from
the chosen topic P (w|t = z). Thus, the probabil-
ity of producing a single copy of word w given a
document d is given by:
P (w|d) =
T
?
z=1
P (w|t = z)P (t = z|d).
In standard LDA, the user needs to specify the
number of topics T . In non-parametric variants of
LDA, the model dynamically learns the number of
topics as part of the topic modelling. The particu-
lar implementation of non-parametric topic model
we experiment with is Hierarchical Dirichlet Pro-
cess (HDP: Teh et al(2006)),3 where, for each
document, a distribution of mixture components
P (t|d) is sampled from a base distribution G0
as follows: (1) choose a base distribution G0 ?
DP (?,H); (2) for each document d, generate dis-
tribution P (t|d) ? DP (?0, G0); (3) draw a la-
tent topic z from the document?s mixture compo-
nent distribution P (t|d), in the same manner as
for LDA; and (4) draw a word w from the chosen
topic P (w|t = z).4
For both LDA and HDP, we individually topic
model each target word, and determine the sense
assignment z for a given instance by aggregating
over the topic assignments for each word in the
instance and selecting the sense with the highest
aggregated probability, argmaxz P (t = z|d).
3 SemEval Experiments
To facilitate comparison of our proposed method
for WSI with previous approaches, we use the
dataset from the SemEval-2007 and SemEval-
2010 word sense induction tasks (Agirre and
3We use the C++ implementation of HDP
(http://www.cs.princeton.edu/?blei/
topicmodeling.html) in our experiments.
4The two HDP parameters ? and ?0 control the variabil-
ity of senses in the documents. In particular, ? controls the
degree of sharing of topics across documents ? a high ?
value leads to more topics, as topics for different documents
are more dissimilar. ?0, on the other hand, controls the de-
gree of mixing of topics within a document? a high ?0 gen-
erates fewer topics, as topics are less homogeneous within a
document.
592
Soroa, 2007; Manandhar et al 2010). We first
experiment with the SemEval-2010 dataset, as it
includes explicit training and test data for each
target word and utilises a more robust evaluation
methodology. We then return to experiment with
the SemEval-2007 dataset, for comparison pur-
poses with other published results for topic mod-
elling approaches to WSI.
3.1 SemEval-2010
3.1.1 Dataset and Methodology
Our primary WSI evaluation is based on
the dataset provided by the SemEval-2010 WSI
shared task (Manandhar et al 2010). The dataset
contains 100 target words: 50 nouns and 50 verbs.
For each target word, a fixed set of training and
test instances are supplied, typically 1 to 3 sen-
tences in length, each containing the target word.
The default approach to evaluation for the
SemEval-2010 WSI task is in the form of WSD
over the test data, based on the senses that have
been automatically induced from the training
data. Because the induced senses will likely vary
in number and nature between systems, the WSD
evaluation has to incorporate a sense alignment
step, which it performs by splitting the test in-
stances into two sets: a mapping set and an eval-
uation set. The optimal mapping from induced
senses to gold-standard senses is learned from the
mapping set, and the resulting sense alignment is
used to map the predictions of the WSI system to
pre-defined senses for the evaluation set. The par-
ticular split we use to calculate WSD effective-
ness in this paper is 80%/20% (mapping/test), av-
eraged across 5 random splits.5
The SemEval-2010 training data consists of ap-
proximately 163K training instances for the 100
target words, all taken from the web. The test
data is approximately 9K instances taken from a
variety of news sources. Following the standard
approach used by the participating systems in the
SemEval-2010 task, we induce senses only from
the training instances, and use the learned model
to assign senses to the test instances.
5A 60%/40% split is also provided as part of the task
setup, but the results are almost identical to those for the
80%/20% split, and so are omitted from this paper. The orig-
inal task also made use of V-measure and Paired F-score to
evaluate the induced word sense clusters, but have degen-
erate behaviour in correlating strongly with the number of
senses induced by the method (Manandhar et al 2010), and
are hence omitted from this paper.
In our original experiments with LDA, we set
the number of topics (T ) for each target word to
the number of senses represented in the test data
for that word (varying T for each target word).
This is based on the unreasonable assumption that
we will have access to gold-standard information
on sense granularity for each target word, and is
done to establish an upper bound score for LDA.
We then relax the assumption, and use a fixed T
setting for each of sets of nouns (T = 7) and
verbs (T = 3), based on the average number of
senses from the test data in each case. Finally,
we introduce positional context features for LDA,
once again using the fixed T values for nouns and
verbs.
We next apply HDP to the WSI task, using
positional features, but learning the number of
senses automatically for each target word via the
model. Finally, we experiment with adding de-
pendency features to the model.
To summarise, we provide results for the fol-
lowing models:
1. LDA+Variable T : LDA with variable T
for each target word based on the number of
gold-standard senses.
2. LDA+Fixed T : LDA with fixed T for each
of nouns and verbs.
3. LDA+Fixed T+Position: LDA with fixed
T and extra positional word features.
4. HDP+Position: HDP (which automatically
learns T ), with extra positional word fea-
tures.
5. HDP+Position+Dependency: HDP with
both positional word and dependency fea-
tures.
We compare our models with two baselines
from the SemEval-2010 task: (1) Baseline Ran-
dom ? randomly assign each test instance to one
of four senses; (2) Baseline MFS ? most fre-
quent sense baseline, assigning all test instances
to one sense; and also a benchmark system
(UoY), in the form of the University of York sys-
tem (Korkontzelos and Manandhar, 2010), which
achieved the best overall WSD results in the orig-
inal SemEval-2010 task.
3.2 SemEval-2010 Results
The results of our experiments over the SemEval-
2010 dataset are summarised in Table 1.
593
System WSD (80%/20%)All Verbs Nouns
Baselines
Baseline Random 0.57 0.66 0.51
Baseline MFS 0.59 0.67 0.53
LDA
Variable T 0.64 0.69 0.60
Fixed T 0.63 0.68 0.59
Fixed T +Position 0.63 0.68 0.60
HDP
+Position 0.68 0.72 0.65
+Position+Dependency 0.68 0.72 0.65
Benchmark
UoY 0.62 0.67 0.59
Table 1: WSD F-score over the SemEval-2010 dataset
Looking first at the results for LDA, we see
that the first LDA approach (variable T ) is very
competitive, outperforming the benchmark sys-
tem. In this approach, however, we assume per-
fect knowledge of the number of gold senses of
each target word, meaning that the method isn?t
truly unsupervised. When we fixed T for each
of the nouns and verbs, we see a small drop in
F-score, but encouragingly the method still per-
forms above the benchmark. Adding positional
word features improves the results very slightly
for nouns.
When we relax the assumption on the number
of word senses in moving to HDP, we observe a
marked improvement in F-score over LDA. This
is highly encouraging and somewhat surprising,
as in hiding information about sense granularity
from the model, we have actually improved our
results. We return to discuss this effect below.
For the final feature, we add dependency features
to the HDP model (in addition to retaining the
positional word features), but see no movement
in the results.6 While the dependency features
didn?t reduce F-score, their utility is questionable
as the generation of the features from the Stanford
parser is computationally expensive.
To better understand these results, we present
the top-10 terms for each of the senses induced for
the word cheat in Table 2. These senses are learnt
using HDP with both positional word features
(e.g. husband #-1, indicating the lemma husband
to the immediate left of the target word) and de-
pendency features (e.g. cheat#prep on#wife). The
first observation to make is that senses 7, 8 and
9 are ?junk? senses, in that the top-10 terms do
6An identical result was observed for LDA.
not convey a coherent sense. These topics are an
artifact of HDP: they are learnt at a much later
stage of the iterative process of Gibbs sampling
and are often smaller than other topics (i.e. have
more zero-probability terms). We notice that they
are assigned as topics to instances very rarely (al-
though they are certainly used to assign topics to
non-target words in the instances), and as such,
they do not present a real issue when assigning
the sense to an instance, as they are likely to be
overshadowed by the dominant senses.7 This con-
clusion is born out when we experimented with
manually filtering out these topics when assign-
ing instance to senses: there was no perceptible
change in the results, reinforcing our suggestion
that these topics do not impact on target word
sense assignment.
Comparing the results for HDP back to those
for LDA, HDP tends to learn almost double the
number of senses per target word as are in the
gold-standard (and hence are used for the ?Vari-
able T ? version of LDA). Far from hurting our
WSD F-score, however, the extra topics are dom-
inated by junk topics, and boost WSD F-score for
the ?genuine? topics. Based on this insight, we
ran LDA once again with variable T (and posi-
tional and dependency features), but this time set-
ting T to the value learned by HDP, to give LDA
the facility to use junk topics. This resulted in an
F-score of 0.66 across all word classes (verbs =
0.71, nouns = 0.62), demonstrating that, surpris-
ingly, even for the same T setting, HDP achieves
superior results to LDA. I.e., not only does HDP
learn T automatically, but the topic model learned
for a given T is superior to that for LDA.
Looking at the other senses discovered for
cheat, we notice that the model has induced a
myriad of senses: the relationship sense of cheat
(senses 1, 3 and 4, e.g. husband cheats); the exam
usage of cheat (sense 2); the competition/game
usage of cheat (sense 5); and cheating in the po-
litical domain (sense 6). Although the senses are
possibly ?split? a little more than desirable (e.g.
senses 1, 3 and 4 arguably describe the same
sense), the overall quality of the produced senses
7In the WSD evaluation, the alignment of induced senses
to the gold senses is learnt automatically based on the map-
ping instances. E.g. if all instances that are assigned sense
a have gold sense x, then sense a is mapped to gold sense
x. Therefore, if the proportion of junk senses in the map-
ping instances is low, their influence on WSD results will be
negligible.
594
Sense Num Top-10 Terms
1 cheat think want ... love feel tell guy cheat#nsubj#include find
2 cheat student cheating test game school cheat#aux#to teacher exam study
3 husband wife cheat wife #1 tiger husband #-1 cheat#prep on#wife ... woman cheat#nsubj#husband
4 cheat woman relationship cheating partner reason cheat#nsubj#man woman #-1 cheat#aux#to spouse
5 cheat game play player cheating poker cheat#aux#to card cheated money
6 cheat exchange china chinese foreign cheat #-2 cheat #2 china #-1 cheat#aux#to team
7 tina bette kirk walk accuse mon pok symkyn nick star
8 fat jones ashley pen body taste weight expectation parent able
9 euro goal luck fair france irish single 2000 cheat#prep at#point complain
Table 2: The top-10 terms for each of the senses induced for the verb cheat by the HDP model (with positional
word and dependency features)
is encouraging. Also, we observe a spin-off ben-
efit of topic modelling approaches to WSI: the
high-ranking words in each topic can be used to
gist the sense, and anecdotally confirm the impact
of the different feature types (i.e. the positional
word and dependency features).
3.3 Comparison with other Topic Modelling
Approaches to WSI
The idea of applying topic modelling to WSI is
not entirely new. Brody and Lapata (2009) pro-
posed an LDA-based model which assigns differ-
ent weights to different feature sets (e.g. unigram
tokens vs. dependency relations), using a ?lay-
ered? feature representation. They carry out ex-
tensive parameter optimisation of both the (fixed)
number of senses, number of layers, and size of
the context window.
Separately, Yao and Durme (2011) proposed
the use of non-parametric topic models in WSI.
The authors preprocess the instances slightly dif-
ferently, opting to remove the target word from
each instance and stem the tokens. They also
tuned the hyperparameters of the topic model to
optimise the WSI effectiveness over the evalua-
tion set, and didn?t use positional or dependency
features.
Both of these papers were evaluated over
only the SemEval-2007 WSI dataset (Agirre and
Soroa, 2007), so we similarly apply our HDP
method to this dataset for direct comparability. In
the remainder of this section, we refer to Brody
and Lapata (2009) as BL, and Yao and Durme
(2011) as YVD.
The SemEval-2007 dataset consists of roughly
27K instances, for 65 target verbs and 35 target
nouns. BL report on results only over the noun
instances, so we similarly restrict our attention to
System F-Score
BL 0.855
YVD 0.857
SemEval Best (I2R) 0.868
Our method (default parameters) 0.842
Our method (tuned parameters) 0.869
Table 3: F-score for the SemEval-2007 WSI task, for
our HDP method with default and tuned parameter set-
tings, as compared to competitor topic modelling and
other approaches to WSI
the nouns in this paper. Training data was not pro-
vided as part of the original dataset, so we fol-
low the approach of BL and YVD in construct-
ing our own training dataset for each target word
from instances extracted from the British National
Corpus (BNC: Burnard (2000)).8 Both BL and
YVD separately report slightly higher in-domain
results from training on WSJ data (the SemEval-
2007 data was taken from the WSJ). For the pur-
poses of model comparison under identical train-
ing settings, however, it is appropriate to report on
results for only the BNC.
We experiment with both our original method
(with both positional word and dependency fea-
tures, and default parameter settings for HDP)
without any parameter tuning, and the same
method with the tuned parameter settings of
YVD, for direct comparability. We present the re-
sults in Table 3, including the results for the best-
performing system in the original SemEval-2007
task (I2R: Niu et al(2007)).
The results are enlightening: with default pa-
rameter settings, our methodology is slightly be-
low the results of the other three models. Bear
8In creating the training dataset, each instance is made
up of the sentence the target word occurs in, as we as one
sentence to either side of that sentence, i.e. 3 sentences in
total per instance.
595
in mind, however, that the two topic modelling-
based approaches were tuned extensively to the
dataset. When we use the tuned hyperparame-
ter settings of YVD, our results rise around 2.5%
to surpass both topic modelling approaches, and
marginally outperform the I2R system from the
original task. Recall that both BL and YVD report
higher results again using in-domain training data,
so we would expect to see further gains again over
the I2R system in following this path.
Overall, these results agree with our findings
over the SemEval-2010 dataset (Section 3.2), un-
derlining the viability of topic modelling to auto-
mated word sense induction.
3.4 Discussion
As part of our preprocessing, we remove all stop-
words (other than for the positional word and de-
pendency features), as described in Section 2.1.
We separately experimented with not removing
stopwords, based on the intuition that prepositions
such as to and on can be informative in determin-
ing word sense based on local context. The results
were markedly worse, however. We also tried ap-
pending part of speech information to each word
lemma, but the resulting data sparseness meant
that results dropped marginally.
When determining the sense for an instance, we
aggregate the sense assignments for each word in
the instance (not just the target word). An alter-
nate strategy is to use only the target word topic
assignment, but again, the results for this strategy
were inferior to the aggregate method.
In the SemEval-2007 experiments (Sec-
tion 3.3), we found that YVD?s hyperparameter
settings yielded better results than the default
settings. We experimented with parameter tuning
over the SemEval-2010 dataset (including YVD?s
optimal setting on the 2007 dataset), but found
that the default setting achieved the best overall
results: although the WSD F-score improved a
little for nouns, it worsened for verbs. This obser-
vation is not unexpected: as the hyperparameters
were optimised for nouns in their experiments,
the settings might not be appropriate for verbs.
This also suggests that their results may be due in
part to overfitting the SemEval-2007 data.
4 Identifying Novel Senses
Having established the effectiveness of our ap-
proach at WSI, we next turn to an application of
WSI, in identifying words which have taken on
novel senses over time, based on analysis of di-
achronic data. Our topic modelling approach is
particularly attractive for this task as, not only
does it jointly perform type-level WSI, and token-
level WSD based on the induced senses (in as-
signing topics to each instance), but it is possible
to gist the induced senses via the contents of the
topic (typically using the topic words with highest
marginal probability).
The meanings of words can change over time;
in particular, words can take on new senses. Con-
temporary examples of new word-senses include
the meanings of swag and tweet as used below:
1. We all know Frankie is adorable, but does he
have swag? [swag = ?style?]
2. The alleged victim gave a description of the
man on Twitter and tweeted that she thought
she could identify him. [tweet = ?send a mes-
sage on Twitter?]
These senses of swag and tweet are not included
in many dictionaries or computational lexicons ?
e.g., neither of these senses is listed in Wordnet
3.0 (Fellbaum, 1998) ? yet appear to be in regu-
lar usage, particularly in text related to pop culture
and online media.
The manual identification of such new word-
senses is a challenge in lexicography over and
above identifying new words themselves, and
is essential to keeping dictionaries up-to-date.
Moreover, lexicons that better reflect contempo-
rary usage could benefit NLP applications that use
sense inventories.
The challenge of identifying changes in word
sense has only recently been considered in com-
putational linguistics. For example, Sagi et al
(2009), Cook and Stevenson (2010), and Gulor-
dava and Baroni (2011) propose type-based mod-
els of semantic change. Such models do not
account for polysemy, and appear best-suited to
identifying changes in predominant sense. Bam-
man and Crane (2011) use a parallel Latin?
English corpus to induce word senses and build
a WSD system, which they then apply to study
diachronic variation in word senses. Crucially, in
this token-based approach there is a clear connec-
tion between word senses and tokens, making it
possible to identify usages of a specific sense.
Based on the findings in Section 3.2, here we
apply the HDP method for WSI to the task of
596
identifying new word-senses. In contrast to Bam-
man and Crane (2011) our token-based approach
does not require parallel text to induce senses.
4.1 Method
Given two corpora ? a reference corpus which
we take to represent standard usage, and a second
corpus of newer texts ? we identify senses that
are novel to the second corpus compared to the
reference corpus. For a given word w, we pool
all usages of w in the reference corpus and sec-
ond corpus, and run the HDP WSI method on this
super-corpus to induce the senses of w. We then
tag all usages of w in both corpora with their sin-
gle most-likely automatically-induced sense.
Intuitively, if a word w is used in some sense
s in the second corpus, and w is never used in
that sense in the reference corpus, then w has ac-
quired a new sense, namely s. We capture this
intuition into a novelty score (?Nov?) that indi-
cates whether a given word w has a new sense in
the second corpus, s, compared to the reference
corpus, r, as below:
Nov(w) = max
({
ps(ti)? pr(ti)
pr(ti)
: ti ? T
})
(1)
where ps(ti) and pr(ti) are the probability of
sense ti in the second corpus and reference cor-
pus, respectively, calculated using smoothed max-
imum likelihood estimates, and T is the set of
senses induced for w. Novelty is high if there is
some sense t that has much higher relative fre-
quency in s than r and that is also relatively infre-
quent in r.
4.2 Data
Because we are interested in the identification of
novel word-senses for applications such as lexi-
con maintenance, we focus on relatively newly-
coined word-senses. In particular, we take the
written portion of the BNC ? consisting primar-
ily of British English text from the late 20th cen-
tury ? as our reference corpus, and a similarly-
sized random sample of documents from the
ukWaC (Ferraresi et al 2008) ? a Web corpus
built from the .uk domain in 2007 which in-
cludes a wide range of text types ? as our sec-
ond corpus. Text genres are represented to dif-
ferent extents in these corpora with, for example,
text types related to the Internet being much more
common in the ukWaC. Such differences are a
noted challenge for approaches to identifying lex-
ical semantic differences between corpora (Peirs-
man et al 2010), but are difficult to avoid given
the corpora that are available. We use TreeTagger
(Schmid, 1994) to tokenise and lemmatise both
corpora.
Evaluating approaches to identifying seman-
tic change is a challenge, particularly due to the
lack of appropriate evaluation resources; indeed,
most previous approaches have used very small
datasets (Sagi et al 2009; Cook and Stevenson,
2010; Bamman and Crane, 2011). Because this
is a preliminary attempt at applying WSI tech-
niques to identifying new word-senses, our evalu-
ation will also be based on a rather small dataset.
We require a set of words that are known to
have acquired a new sense between the late 20th
and early 21st centuries. The Concise Oxford
English Dictionary aims to document contempo-
rary usage, and has been published in numerous
editions including Thompson (1995, COD95) and
Soanes and Stevenson (2008, COD08). Although
some of the entries have been substantially re-
vised between editions, many have not, enabling
us to easily identify new senses amongst the en-
tries in COD08 relative to COD95. A manual lin-
ear search through the entries in these dictionaries
would be very time consuming, but by exploit-
ing the observation that new words often corre-
spond to concepts that are culturally salient (Ayto,
2006), we can quickly identify some candidates
for words that have taken on a new sense.
Between the time periods of our two corpora,
computers and the Internet have become much
more mainstream in society. We therefore ex-
tracted all entries from COD08 containing the
word computing (which is often used as a topic la-
bel in this dictionary) that have a token frequency
of at least 1000 in the BNC. We then read the
entries for these 87 lexical items in COD95 and
COD08 and identified those which have a clear
computing sense in COD08 that was not present
in COD95. In total we found 22 such items. This
process, along with all the annotation in this sec-
tion, is carried out by a native English-speaking
author of this paper.
To ensure that the words identified from the
dictionaries do in fact have a new sense in the
ukWaC sample compared to the BNC, we exam-
ine the usage of these words in the corpora. We
extract a random sample of 100 usages of each
597
lemma from the BNC and ukWaC sample and
annotate these usages as to whether they corre-
spond to the novel sense or not. This binary dis-
tinction is easier than fine-grained sense annota-
tion, and since we do not use these annotations
for formal evaluation ? only for selecting items
for our dataset ? we do not carry out an inter-
annotator agreement study here. We eliminate any
lemma for which we find evidence of the novel
sense in the BNC, or for which we do not find
evidence of the novel sense in the ukWaC sam-
ple.9 We further check word sketches (Kilgarriff
and Tugwell, 2002)10 for each of these lemmas
in the BNC and ukWaC for collocates that likely
correspond to the novel sense; we exclude any
lemma for which we find evidence of the novel
sense in the BNC, or fail to find evidence of the
novel sense in the ukWaC sample. At the end
of this process we have identified the following
5 lemmas that have the indicated novel senses in
the ukWaC compared to the BNC: domain (n) ?In-
ternet domain?; export (v) ?export data?; mirror
(n) ?mirror website?; poster (n) ?one who posts
online?; and worm (n) ?malicious program?. For
each of the 5 lemmas with novel senses, a sec-
ond annotator ? also a native English-speaking
author of this paper ? annotated the sample of
100 usages from the ukWaC. The observed agree-
ment and unweighted Kappa between the two an-
notators is 97.2% and 0.92, respectively, indicat-
ing that this is indeed a relatively easy annotation
task. The annotators discussed the small number
of disagreements to reach consensus.
For our dataset we also require items that have
not acquired a novel sense in the ukWaC sample.
For each of the above 5 lemmas we identified a
distractor lemma of the same part-of-speech that
has a similar frequency in the BNC, and that has
not undergone sense change between COD95 and
COD08. The 5 distractors are: cinema (n); guess
(v); symptom (n); founder (n); and racism (n).
4.3 Results
We compute novelty (?Nov?, Equation 1) for all
10 items in our dataset, based on the output of the
9We use the IMS Open Corpus Workbench (http://
cwb.sourceforge.net/) to extract the usages of our
target lemmas from the corpora. This extraction process fails
in some cases, and so we also eliminate such items from our
dataset.
10http://www.sketchengine.co.uk/
Lemma Novelty Freq. ratio Novel sense freq.
domain (n) 116.2 2.60 41
worm (n) 68.4 1.04 30
mirror (n) 38.4 0.53 10
guess (v) 16.5 0.93 ?
export (v) 13.8 0.88 28
founder (n) 11.0 1.20 ?
cinema (n) 9.7 1.30 ?
poster (n) 7.9 1.83 4
racism (n) 2.4 0.98 ?
symptom (n) 2.1 1.16 ?
Table 4: Novelty score (?Nov?), ratio of frequency in
the ukWaC sample and BNC, and frequency of the
novel sense in the manually-annotated 100 instances
from the ukWaC sample (where applicable), for all
lemmas in our dataset. Lemmas shown in boldface
have a novel sense in the ukWaC sample compared to
the BNC.
topic modelling. The results are shown in column
?Novelty? in Table 4. The lemmas with a novel
sense have higher novelty scores than the distrac-
tors according to a one-sided Wilcoxon rank sum
test (p < .05).
When a lemma takes on a new sense, it might
also increase in frequency. We therefore also con-
sider a baseline in which we rank the lemmas by
the ratio of their frequency in the second and ref-
erence corpora. These results are shown in col-
umn ?Freq. ratio? in Table 4. The difference be-
tween the frequency ratios for the lemmas with a
novel sense, and the distractors, is not significant
(p > .05).
Examining the frequency of the novel senses?
shown in column ?Novel sense freq.? in Table 4
? we see that the lowest-ranked lemma with a
novel sense, poster, is also the lemma with the
least-frequent novel sense. This result is unsur-
prising as our novelty score will be higher for
higher-frequency novel senses. The identification
of infrequent novel senses remains a challenge.
The top-ranked topic words for the sense cor-
responding to the maximum in Equation 1 for
the highest-ranked distractor, guess, are the fol-
lowing: @card@, post, ..., n?t, comment, think,
subject, forum, view, guess. This sense seems
to correspond to usages of guess in the context
of online forums, which are better represented
in the ukWaC sample than the BNC. Because of
the challenges posed by such differences between
corpora (discussed in Section 4.2) we are unsur-
prised to see such an error, but this could be ad-
dressed in the future by building comparable cor-
598
Lemma
Topic Selection Methodology
Nov Oracle (single topic) Oracle (multiple topics)
Precision Recall F-score Precision Recall F-score Precision Recall F-score
domain (n) 1.00 0.29 0.45 1.00 0.56 0.72 0.97 0.88 0.92
export (v) 0.93 0.96 0.95 0.93 0.96 0.95 0.90 1.00 0.95
mirror (n) 0.67 1.00 0.80 0.67 1.00 0.80 0.67 1.00 0.80
poster (n) 0.00 0.00 0.00 0.44 1.00 0.62 0.44 1.00 0.62
worm (n) 0.93 0.90 0.92 0.93 0.90 0.92 0.86 1.00 0.92
Table 5: Results for identifying the gold-standard novel senses based on the three topic selection methodologies
of: (1) Nov; (2) oracle selection of a single topic; and (3) oracle selection of multiple topics.
pora for use in this application.
Having demonstrated that our method for iden-
tifying novel senses can distinguish lemmas that
have a novel sense in one corpus compared to an-
other from those that do not, we now consider
whether this method can also automatically iden-
tify the usages of the induced novel sense.
For each lemma with a gold-standard novel
sense, we define the automatically-induced novel
sense to be the single sense corresponding to the
maximum in Equation 1. We then compute the
precision, recall, and F-score of this novel sense
with respect to the gold-standard novel sense,
based on the 100 annotated tokens for each of
the 5 lemmas with a novel sense. The results are
shown in the first three numeric columns of Ta-
ble 5.
In the case of export and worm the results are
remarkably good, with precision and recall both
over 0.90. For domain, the low recall is a result of
the majority of usages of the gold-standard novel
sense (?Internet domain?) being split across two
induced senses ? the top-two highest ranked in-
duced senses according to Equation 1. The poor
performance for poster is unsurprising due to the
very low frequency of this lemma?s gold-standard
novel sense.
These results are based on our novelty rank-
ing method (?Nov?), and the assumption that
the novel sense will be represented in a single
topic. To evaluate the theoretical upper-bound
for a topic-ranking method which uses our HDP-
based WSI method and selects a single topic to
capture the novel sense, we next evaluate an op-
timal topic selection approach. In the middle
three numeric columns of Table 5, we present re-
sults for an experimental setup in which the sin-
gle best induced sense ? in terms of F-score ?
is selected as the novel sense by an oracle. We
see big improvements in F-score for domain and
poster. This encouraging result suggests refining
the sense selection heuristic could theoretically
improve our method for identifying novel senses,
and that the topic modelling approach proposed
in this paper has considerable promise for auto-
matic novel sense detection. Of particular note is
the result for poster: although the gold-standard
novel sense of poster is rare, all of its usages are
grouped into a single topic.
Finally, we consider whether an oracle which
can select the best subset of induced senses ? in
terms of F-score ? as the novel sense could of-
fer further improvements. In this case ? results
shown in the final three columns of Table 5 ?
we again see an increase in F-score to 0.92 for
domain. For this lemma the gold-standard novel
sense usages were split across multiple induced
topics, and so we are unsurprised to find that a
method which is able to select multiple topics as
the novel sense performs well. Based on these
findings, in future work we plan to consider alter-
native formulations of novelty.
5 Conclusion
We propose the application of topic modelling
to the task of word sense induction (WSI), start-
ing with a simple LDA-based methodology with
a fixed number of senses, and culminating in
a nonparametric method based on a Hierarchi-
cal Dirichlet Process (HDP), which automatically
learns the number of senses for a given target
word. Our HDP-based method outperforms all
methods over the SemEval-2010WSI dataset, and
is also superior to other topic modelling-based
approaches to WSI based on the SemEval-2007
dataset. We applied the proposed WSI model to
the task of identifying words which have taken on
new senses, including identifying the token oc-
currences of the new word sense. Over a small
dataset developed in this research, we achieved
highly encouraging results.
599
References
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
Task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7?12, Prague, Czech Re-
public.
John Ayto. 2006. Movers and Shakers: A Chronology
of Words that Shaped our Age. Oxford University
Press, Oxford.
David Bamman and Gregory Crane. 2011. Measur-
ing historical word sense variation. In Proceedings
of the 2011 Joint International Conference on Dig-
ital Libraries (JCDL 2011), pages 1?10, Ottawa,
Canada.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
S. Brody and M. Lapata. 2009. Bayesian word sense
induction. pages 103?111, Athens, Greece.
Lou Burnard. 2000. The British National Corpus
Users Reference Guide. Oxford University Com-
puting Services.
Paul Cook and Suzanne Stevenson. 2010. Automat-
ically identifying changes in the semantic orienta-
tion of words. In Proceedings of the Seventh In-
ternational Conference on Language Resources and
Evaluation (LREC 2010), pages 28?34, Valletta,
Malta.
Marie-Catherine De Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
Genoa, Italy.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the 4th Web as Corpus
Workshop: Can we beat Google, pages 47?54, Mar-
rakech, Morocco.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. pages
233?237.
Kristina Gulordava and Marco Baroni. 2011. A dis-
tributional similarity approach to the detection of
semantic change in the Google Books Ngram cor-
pus. In Proceedings of the GEMS 2011 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 67?71, Edinburgh, Scotland.
Adam Kilgarriff and David Tugwell. 2002. Sketch-
ing words. In Marie-He?le`ne Corre?ard, editor, Lex-
icography and Natural Language Processing: A
Festschrift in Honour of B. T. S. Atkins, pages 125?
137. Euralex, Grenoble, France.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10, Whistler, Canada.
Ioannis Korkontzelos and Suresh Manandhar. 2010.
Uoy: Graphs of unambiguous vertices for word
sense induction and disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 355?358, Uppsala, Sweden.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dli-
gach, and Sameer Pradhan. 2010. SemEval-2010
Task 14: Word sense induction & disambiguation.
In Proceedings of the 5th International Workshop
on Semantic Evaluation, pages 63?68, Uppsala,
Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. In-
ducing word senses to improve web search result
clustering. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 116?126, Cambridge, USA.
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan.
2007. I2R: Three systems for word sense discrimi-
nation, chinese word sense disambiguation, and en-
glish word sense disambiguation. In Proceedings
of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 177?182,
Prague, Czech Republic.
Sebastian Pado? and Mirella Lapata. 2007.
Dependency-based construction of semantic
space models. Comput. Linguist., 33:161?199.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49, Manchester, UK.
Hinrich Schutze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Catherine Soanes and Angus Stevenson, editors. 2008.
The Concise Oxford English Dictionary. Oxford
University Press, eleventh (revised) edition. Oxford
Reference Online.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101:1566?
1581.
600
Della Thompson, editor. 1995. The Concise Oxford
Dictionary of Current English. Oxford University
Press, Oxford, ninth edition.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, Oregon.
601
Unsupervised Acquisition of Predominant
Word Senses
Diana McCarthy
University of Sussex
Rob Koeling
University of Sussex
Julie Weeds
University of Sussex
John Carroll?
University of Sussex
There has been a great deal of recent research into word sense disambiguation, particularly
since the inception of the Senseval evaluation exercises. Because a word often has more than
one meaning, resolving word sense ambiguity could benefit applications that need some level
of semantic interpretation of language input. A major problem is that the accuracy of word
sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged
data available, and even the best systems, when tagging every word token in a document,
perform little better than a simple heuristic that guesses the first, or predominant, sense of a
word in all contexts. The success of this heuristic is due to the skewed nature of word sense
distributions. Data for the heuristic can come from either dictionaries or a sample of sense-
tagged data. However, there is a limited supply of the latter, and the sense distributions and
predominant sense of a word can depend on the domain or source of a document. (The first
sense of ?star? for example would be different in the popular press and scientific journals).
In this article, we expand on a previously proposed method for determining the predominant
sense of a word automatically from raw text. We look at a number of different data sources and
parameterizations of the method, using evaluation results and error analyses to identify where
the method performs well and also where it does not. In particular, we find that the method
does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate
predominant sense information than the widely used SemCor corpus for nouns with low coverage
in that corpus. We further show that the method is able to adapt successfully to domains when
using domain specific corpora as input and where the input can either be hand-labeled for domain
or automatically classified.
? Department of Informatics, Brighton BN1 9QH, UK. E-mail: {dianam,robk,juliewe,johnca}@sussex.ac.uk.
Submission received: 16 November 2005; revised submission received: 12 July 2006; accepted for publication
16 February 2007.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 4
1. Introduction
In word sense disambiguation, the ?first sense? heuristic (choosing the first, or predom-
inant sense of a word) is used by most state-of-the-art systems as a back-off method
when information from the context is not sufficient to make a more informed choice.
In this article, we present an in-depth study of a method for automatically acquiring
predominant senses for words from raw text (McCarthy et al 2004a).
The method uses distributionally similar words listed as ?nearest neighbors?
in automatically acquired thesauruses (e.g., Lin 1998a), and takes advantage of the
observation that the more prevalent a sense of a word, the more neighbors will relate
to that sense, and the higher their distributional similarity scores will be. The senses
of a word are defined in a sense inventory. We use WordNet (Fellbaum 1998) because
this is widely used, is publicly available, and has plenty of gold-standard evaluation
data available (Miller et al 1993; Cotton et al 2001; Preiss and Yarowsky 2001; Mihalcea
and Edmonds 2004). The distributional strength of the neighbors is associated with the
senses of a word using a measure of semantic similarity which relies on the relationships
between word senses, such as hyponyms (available in an inventory such as WordNet)
or overlap in the definitions of word senses (available in most dictionaries), or both.
In this article we provide a detailed discussion and quantitative analysis of the
motivation behind the first sense heuristic, and a full description of our method. We
extend previously reported work in a number of different directions:
 We evaluate the method on all parts of speech (PoS) on SemCor (Miller
et al 1993). Previous experiments (McCarthy et al 2004c) evaluated only
nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al 2001)
and Senseval-3 (Mihalcea and Edmonds 2004) data. The evaluation on all
PoS is much more extensive because the SemCor corpus is composed of
220,000 words in contrast to the 6 documents in the Senseval-2 and -3
English all words data (10,000 words).
 We compare two WordNet similarity measures in our evaluation on
nouns, and also contrast performance using two publicly available
thesauruses, both produced from the same NEWSWIRE corpus, but one
derived using a proximity-based approach and the other using
dependency relations from a parser. It turns out that the results from the
proximity-based thesaurus are comparable to those from the dependency-
based thesaurus; this is encouraging for applying the method to languages
without sophisticated analysis tools.
 We manually analyze a sample of errors from the SemCor evaluation. A
small number of errors can be traced back to inherent shortcomings of our
method, but the main source of error is due to noise from related senses.
This is a common problem for all WSD systems (Ide and Wilks 2006) but
one which is only recently starting to be addressed by the WSD
community (Navigli, Litkowski, and Hargraves 2007).
 One motivation for an automatic method for acquiring predominant
senses is that there will always be words for which there are insufficient
data available in manually sense-tagged resources. We compare the
performance of our automatic method with the first sense heuristic
derived from SemCor on nouns in the Senseval-2 data. We find that the
554
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
automatic method outperforms the one obtained from manual annotations
in SemCor for nouns with fewer than five occurrences in SemCor.
 Aside from the lack of coverage of manually annotated data, there is a
need for first sense heuristics to be specific to domain. We explore the
potential for applying the method with domain-specific text for all PoS in
an experiment using a gold-standard domain-specific resource (Magnini
and Cavaglia` 2000) which we have used previously only with nouns. We
show that although there is a little mileage to be had from domain-specific
first sense heuristics for verbs, nouns benefit greatly from domain-specific
training.
 In previous work (Koeling, McCarthy, and Carroll 2005) we produced
manually sense-annotated domain-specific test corpora for a lexical
sample, and demonstrated that predominant senses acquired (from
hand-classified corpora) in the same domain as the test data outperformed
the SemCor first sense. We further this exploration by contrasting with
results from training on automatically categorized text from the English
Gigaword Corpus and show that the results are comparable to those using
hand-classified domain data.
The article is organized as follows. In the next section we motivate the use of pre-
dominant sense information in WSD systems and the need for acquiring this information
automatically. In Section 3 we give an overview of related work in WSD, focusing on the
acquisition of prior sense distributions and domain-specific sense information. Section 4
describes our acquisition method. Section 5 describes the experimental setup for the
work reported in this article. Section 6 describes four experiments. The first evaluates
the first sense heuristic using predominant sense information acquired for all PoS on
SemCor; for nouns we compare two semantic similarity methods and three different
types of distributional thesaurus. We also report an error analysis for all PoS of our
method. The second experiment compares the performance of the automatic method
to the manually produced data in SemCor, on nouns in the Senseval-2 data, looking
particularly at nouns which have a low frequency in SemCor. The third uses corpora in
restricted domains and the subject field code gold standard of Magnini and Cavaglia`
(2000) to investigate the potential for domain-specific rankings for different PoS. The
fourth compares results when we train and test on domain-specific corpora, where
the training data is (1) manually categorized for domain and from the same corpus
as the test data, and (2) where the training data is harvested automatically from another
corpus which is categorized automatically. Finally, we conclude (Section 7) and discuss
directions for future work (Section 8).
2. Motivation
The problem of disambiguating the meanings of words in text has received much
attention recently, particularly since the inception of the Senseval evaluation exercises
(Kilgarriff and Palmer 2000; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004).
One of the standard Senseval tasks (the ?all words? task) is to tag each open class word
with one of its senses, as listed in a dictionary or thesaurus such as WordNet (Fellbaum
1998). The most accurate word sense disambiguation (WSD) systems use supervised
machine learning approaches (Stevenson and Wilks 2001), trained on text which has
been sense tagged by hand. However, the performance of these systems is strongly
555
Computational Linguistics Volume 33, Number 4
dependent on the quantity of training data available (Yarowsky and Florian 2002),
and manually sense-annotated text is extremely costly to produce (Kilgarriff 1998). The
largest all words sense tagged corpus is SemCor, which is 220,000 words taken from
103 passages, each of about 2,000 words, from the Brown corpus (Francis and Kuc?era
1979) and the complete text of a 19th-century American novel, The Red Badge of Courage,
which totals 45,600 words (Landes, Leacock, and Tengi 1998). Approximately half of the
words in this corpus are open-class words (nouns, verbs, adjectives, and adverbs) and
these have been linked to WordNet senses by human taggers using a software interface.
The shortage of training data due to the high costs of tagging texts has motivated
research into unsupervised methods for WSD. But in the English all-words tasks in
Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make
use of hand-tagged data (in some form or other) performed substantially worse than
those that did. Table 1 summarizes the situation. It gives the precision and recall of
the best1 two supervised (S) and unsupervised (U)2 systems for the English all words
and English lexical sample for Senseval-23 and -3, along with the first sense baseline
(FS) reported by the task organizers.4 This is a simple application of the ?first sense?
heuristic?that is, using the most common sense of a word for every instance of it in the
test corpus, regardless of context. Although contextual WSD is of course preferable, the
baseline is a very powerful one and unsupervised systems find it surprisingly hard to
beat (indeed, some of the systems that report themselves as unsupervised actually make
some use of a manually obtained first-sense heuristic). Considering both precision and
recall, only 5 of 26 systems in the Senseval-3 English all-words task beat the first sense
heuristic as derived from SemCor (61.5%5), and then by only a few percentage points
(the top system scoring 65% precision and recall) despite using hand-tagged training
data available from SemCor and previous Senseval data sets, large sets of contextual
features, and sophisticated machine learning algorithms.
The performance of WSD systems, at least for all-words tasks, seems to have
plateaued at a level just above the first sense heuristic (Snyder and Palmer 2004). This is
due to the shortage of training data and the often fine granularity of sense distinctions.
Ide and Wilks (2006) argue that it is best to concentrate effort on distinctions which
are useful for applications and where systems can be confident of high precision. In
cases where systems are less confident, but word senses, rather than words, are needed,
the first sense heuristic is a powerful back-off strategy. This strategy is dependent on
information provided in dictionaries. Two dictionaries that have been used by English
WSD systems are the Longman Dictionary of Contemporary English (LDOCE) (Procter
1 We rank the systems by the recall scores, because this is the accuracy over the entire test set regardless of
how many items were attempted.
2 Note that the classification of systems as unsupervised is not straightforward. Systems reported as
unsupervised in the Senseval proceedings sometimes make use of some manual annotations. For
example, the top scoring system that reported itself unsupervised in the Senseval-3 lexical sample task
used manually sense-tagged training data for constructing glosses.
3 The verb lexical sample was done as a separate exercise for Senseval-2, and for brevity we have not
included the results from this task.
4 The all-words task organizers used the first sense as listed in WordNet. This is based on the SemCor first
sense because WordNet senses are ordered according to the frequency data in SemCor. However, where
senses are not found in WordNet, the ordering is arbitrarily determined as a function of the ?grind?
program (see http://wordnet.princeton.edu/man/grind.1WN.htm). The lexical sample task organizers
state that they use the ?most frequent sense? but do not stipulate if this is taken from WordNet, or
directly from SemCor.
5 This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the difference
being due to the treatment of multiwords.
556
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 1
The best two performing systems of each type (according to fine-grained recall) in Senseval-2
and -3.
All words Lexical sample
Precision (%) Recall (%) Precision (%) Recall (%)
Senseval-2 S 69.0 69.0 64.2 64.2
Senseval-2 S 63.6 63.6 63.8 63.8
Senseval-2 U 45.1 45.1 40.2 40.1
Senseval-2 U 36.0 36.0 58.1 31.9
FS baseline 57.0 57.0 47.6 47.6
Senseval-3 S 65.1 65.1 72.9 72.9
Senseval-3 S 65.1 64.2 72.6 72.6
Senseval-3 U 58.3 58.2 66.1 65.7
Senseval-3 U 55.7 54.6 56.3 56.3
FS baseline 61.5 61.5 55.2 55.2
1978) and WordNet (Fellbaum 1998). These both provide a ranking of senses accord-
ing to their predominance. The sense ordering in LDOCE is based on lexicographer
intuition, whereas in WordNet the senses are ordered according to their frequency in
SemCor (Miller et al 1993).
There are two major problems with deriving a first sense heuristic from these types
of resources. The first is that the predominant sense of a word varies according to
the source of the document (McCarthy and Carroll 2003) and with the domain. For
example, the first sense of star as derived from SemCor is celestial body, but if one were
disambiguating popular news stories then celebrity would be more likely. Domain,
topic, and genre are important in WSD (Martinez and Agirre 2000; Magnini et al 2002)
and the sense-frequency distributions of words depend on all of these factors. Any
dictionary will provide only a single sense ranking, whether this is derived from sense-
tagged data as in WordNet, lexicographer intuition as in LDOCE, or inspection of corpus
data as in the Oxford Advanced Learner?s Dictionary (Hornby 1989). A fixed order of
senses may not reflect the data that an NLP system is dealing with.
The second problem with obtaining predominant sense information applies to the
use of hand-tagged resources, such as SemCor. Such resources are relatively small due
to the cost of manual tagging (Kilgarriff 1998). Many words will simply not be covered,
or occur only a few times. For many words in WordNet the ordering of word senses is
based on a very small number of occurrences in SemCor. For example, the first sense
of tiger is an audacious person whereas most people would assume the carnivorous
animal sense is more prevalent. This is because the two senses each occur exactly once
in SemCor, and when there is no frequency information to break the tie the WordNet
sense ordering is assigned arbitrarily. There are many fairly common words (such as
the noun crane) which do not occur at all in SemCor. Table 2 gives the number and
percentage of words6 in WordNet and the BNC which do not occur in SemCor. As one
would expect from Zipf?s law, a substantial number of words do not occur in SemCor,
even when we do not consider multiwords. Many of these words are extremely rare, but
6 Here and elsewhere in this article we give figures only for words without embedded spaces, that is, not
multiwords.
557
Computational Linguistics Volume 33, Number 4
Table 2
Words (excluding multiwords) in WordNet 1.7.1 and the BNC without any data in SemCor.
WordNet types BNC types
PoS No. % No. %
noun 43,781 81.9 360,535 97.5
verb 4,741 56.4 25,292 87.6
adjective 14,991 72.3 95,908 95.4
adverb 2,405 64.4 10,223 89.2
Table 3
Polysemous word types in the Senseval-2 and -3 English all-words tasks test documents with no
data in SemCor (0 columns), or with very little data (? 1 and ? 5 occurrences). Note that there
are no annotations for adverbs in the Senseval-3 documents.
Senseval-2 Senseval-3
0 ? 1 ? 5 0 ? 1 ? 5
PoS No. % No. % No. % No. % No. % No. %
noun 12 3.2 28 7.4 49 12.9 13 3.1 26 6.3 69 16.7
verb 7 2.1 11 3.4 28 8.6 3 0.9 10 2.9 36 10.4
adjective 9 4.2 16 7.4 50 23.1 8 4.7 15 8.9 33 19.5
adverb 1 0.9 1 0.9 2 1.8 ? ? ? ? ? ?
in any given document it is likely that there will be at least some words without SemCor
data. Table 3 quantifies this, for the Senseval-2 and -3 all-words tasks test data, showing
the percentage of polysemous word types with no frequency information in SemCor, the
percentage with zero or one occurrences, and the percentage with up to five occurrences.
(For example, the table indicates that 12.9% of nouns in the Senseval-2 data, and 16.7%
in Senseval-3, have five or fewer occurrences in SemCor.) Thus, although SemCor may
cover many frequently occurring word types in a given document, there are likely to be
a substantial proportion for which there is very little or no information available.
Tables 4 and 5 present an analysis of the actual ambiguity of polysemous words
within the six documents making up the Senseval-2 and -3 all-words test data. They
show the extent to which these words are used in a predominant sense, within a
document, and the extent to which this is the same as that given by SemCor. The two
tables share a common format: columns 2?5 give percentages over all ?document/word
type? combinations. The second column shows the percentage of the ?document/word
type? combinations where the word is used in the document in only one of its senses.
The fourth column shows the same percentage but for ?document/word type? combi-
nations where the word is used in more than one sense in the document. The third and
fifth columns give the percentage of the words in the preceding columns (second and
fourth, respectively) where the first sense for the word in the document is the same as in
SemCor (FS = SC FS). For the third column, this is the only sense that this word appears
in within the document. (Note that for any row, columns 2 and 4 account for all possibil-
ities so will always add up to 100.) The sixth column gives the mean degree of polysemy,
according to WordNet, for the set of words that these figures are calculated for.
558
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 4
Most frequent sense analysis for Senseval-2 and -3 polysemous lemmas occurring more than
once in a document (adverb data is only from Senseval-2).
1 sense > 1 sense
PoS % FS = SC FS % % FS = SC FS % Mean polysemy
noun 72.2 52.2 27.8 7.3 5.9
verb 45.6 25.1 54.4 16.9 12.7
adjective 62.9 40.5 37.1 10.3 4.8
adverb 64.7 50.0 35.3 17.6 4.7
The figures in Table 4 are for words occurring more than once in a given Senseval
test document. The tendency for words to be used in only one sense in any given
document7 is strongest for nouns, although adverbs and adjectives also tend towards
one sense. Verbs are on average much more polysemous than the other parts of speech
yet still 45.6% of polysemous verbs which occur more than once are used in only a single
sense. However, because verbs are in general more polysemous, it makes it less likely
that if a verb occurs in only one sense in a document then it will be the one indicated by
SemCor.
The figures in Table 5 are for all words in the Senseval documents (not just those oc-
curring more than once), showing the accuracy of a SemCor-derived first-sense heuristic
for words with a frequency below a specified threshold (column 1) in SemCor. The table
shows that although having a first sense from SemCor is certainly useful, when looking
at figures for all the words in the Senseval documents a good proportion have first
senses other than the one indicated by SemCor. Furthermore, the lower the frequency
in SemCor the more likely that the first sense indicated by SemCor is wrong. (However,
the situation is slightly different for adverbs because there are not many with low
frequency in SemCor and they are on average not very polysemous, so for them a first
sense derived from a resource like SemCor?where one exists?is possibly sufficient.)
These results show that although SemCor is a useful resource, there will always be
words for which its coverage is inadequate. In addition, few languages have extensive
hand-tagged resources or sense orderings produced by lexicographers. Moreover, gen-
eral resources containing word sense information are not likely to be appropriate when
processing language for a wider variety of domains, topics, and genres. What is needed
is a means to find predominant senses automatically.
3. Related Work
Most research in WSD to date has concentrated on using contextual features, typically
neighboring words, to help infer the correct sense of a target word. In contrast, our
work is aimed at discovering the predominant sense of a word from raw text because
7 The tendency for words to be used in only one sense in a given discourse is weaker for fine-grained
distinctions (Krovetz 1998) compared to coarse-grained distinctions (Gale, Church, and Yarowsky 1992).
Nevertheless, even with a fine-grained inventory the first sense heuristic is certainly powerful, as shown
in Table 1.
559
Computational Linguistics Volume 33, Number 4
Table 5
Most frequent sense analysis for all polysemous lemmas in the Senseval-2 and -3 test data,
broken down by their frequencies of occurrence in SemCor (adverb data is only from
Senseval-2).
1 sense > 1 sense
Frequency % FS = SC FS % % FS = SC FS % Mean polysemy
noun
? 1 (54) 96.3 24.1 3.7 0.0 2.8
? 5 (118) 96.6 43.2 3.4 0.0 3.2
? 10 (191) 96.9 48.7 3.1 0.0 3.3
all (792) 88.8 51.6 11.2 2.5 5.5
verb
? 1 (21) 100.0 33.3 0.0 0.0 2.4
? 5 (64) 98.4 35.9 1.6 1.6 3.2
? 10 (110) 98.2 38.2 1.8 1.8 3.5
all (671) 82.6 39.3 17.4 5.1 9.0
adjective
? 1 (31) 93.5 19.4 6.5 0.0 2.5
? 5 (83) 95.2 34.9 4.8 1.2 2.7
? 10 (120) 90.8 40.8 9.2 1.7 2.8
all (385) 82.6 46.2 17.4 3.6 5.1
adverb
? 1 (1) 0.0 0.0 100.0 0.0 2.0
? 5 (2) 50.0 50.0 50.0 0.0 2.0
? 10 (8) 87.5 62.5 12.5 0.0 2.3
all (111) 82.9 62.2 17.1 5.4 4.0
the first sense heuristic is so powerful, and because manually sense-tagged data is not
always available.
Lapata and Brew (2004) highlighted the importance of a good prior in WSD. They
used syntactic evidence to find a prior distribution for Levin (1993) verb classes, and
incorporated this in a WSD system. Lapata and Brew obtained their priors for verb
classes directly from subcategorization evidence in a parsed corpus, whereas we use
parsed data to find distributionally similar words (nearest neighbors) to the target
word which reflect the different senses of the word and have associated distributional
similarity scores which can be used for ranking the senses according to prevalence.
We would, however, agree that subcategorization evidence should be very useful for
disambiguating verbs, and would hope to combine such evidence with our ranking
models for context-based WSD.
A major benefit of our work is that this method permits us to produce predominant
senses for any desired domain and text type. Buitelaar and Sacaleanu (2001) explored
ranking and selection of synsets in GermaNet for specific domains using the words
in a given synset, and those related by hyponymy, and a term relevance measure
taken from information retrieval. Buitelaar and Sacaleanu evaluated their method on
identifying domain-specific concepts using human judgments on 100 items. We evaluate
560
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
our method using publicly available resources for balanced text, and, for domain-
specific investigations, resources we have developed ourselves (Koeling, McCarthy,
and Carroll 2005). Magnini and Cavaglia` (2000) associated WordNet word senses with
particular domains, and this has proved useful for high precision WSD (Magnini et
al. 2001); indeed, we have used their domain labels (or subject field codes, SFCs) for
evaluation (Section 6.3). Identification of these SFCs for word senses was semi-automatic
and required a considerable amount of hand-labeling. Our approach requires only raw
text from the given domain and because of this it can easily be applied to a new domain
or sense inventory, as long as there is enough appropriate text.
There are other approaches aimed at gleaning domain-specific information from
raw data. Gliozzo, Giuliano, and Strapparava (2005) induced domain models from raw
data using unsupervised latent semantic models and then fed this into a supervised
WSD model and evaluated on Senseval-3 lexical sample data in four languages. Chan
and Ng (2005) obtained probability distributions to feed into their supervised WSD mod-
els. They used multilingual parallel corpus data to provide probability estimates for a
subset of 22 nouns from the lexical sample task. They then fed this into a supervised WSD
model and verified that the estimates for prior distributions improved performance for
supervised WSD. We intend eventually to use our prevalence scores to feed into un-
supervised WSD models. Although unsupervised models seem to be beaten whenever
there is training data to be had, we anticipate that unsupervised models with improved
priors from the ranking might outperform supervised systems in situations where there
is little training data available. Whereas this article is about finding predominant senses
for back-off in a WSD system, the method could be applied to finding a prior distribution
over all word senses of each target word. It is our intention that the back-off models pro-
duced by our prevalence ranking, either as predominant senses or prior distributions
over word senses, could be combined with contextual information for WSD.
Mohammad and Hirst (2006) describe an approach to acquiring predominant senses
from corpora which makes use of the category information in the Macquarie Thesaurus.
Evaluation is performed on an artificially constructed test set from unambiguous words
in the same category as the 27 test words (nouns, verbs, and adjectives). The senses of
the words are the categories of the thesaurus and the experiment uses only two senses
of each word, the two most predominant ones. The predominance of the two senses is
altered systematically. The results are encouraging because a much smaller amount of
corpus data is needed compared to our approach. However, their method has only been
applied to an artificially constructed test set, rather than a publicly available corpus, and
has yet to be applied in a domain-specific setting, which is the chief motivation of our
work.
The work of Pantel and Lin (2002) is probably the most closely related study
that predates ours, although their ultimate goal is different. Pantel and Lin devised
a method called CBC (clustering by committee) where the 10 nearest neighbors of
a word in a distributional thesaurus are clustered to identify the various senses of
the word. Pantel and Lin use a measure of semantic similarity (Lin 1997) to evaluate
the discovered classes with respect to WordNet as a gold standard. The CBC method
obtained a precision of 61% (the percentage of senses discovered that did exist in
WordNet) and a recall of 51% (the percentage of senses discovered from the union of
those discovered with different clustering algorithms that they tried).8
8 The calculation of recall was over the union of senses discovered automatically, rather than over the
senses in WordNet, because senses in WordNet may be unattested in the data.
561
Computational Linguistics Volume 33, Number 4
Pantel and Lin?s approach is related to ours in that, in their sense discovery pro-
cedure, predominant senses have more of a chance of being found than other senses,
although their algorithm is specifically tailored to look for senses regardless of fre-
quency. To do this the algorithm removes neighbors of the target word once they
are assigned to a cluster so that less frequent senses can be discovered. Our method,
described in detail in Section 4, associates the nearest neighbors to the senses of the
target in a predefined inventory (we use WordNet). We rank the senses using a measure
which sums over the distributional similarity of neighbors weighted by the strength of
the association between the neighbors and the sense. This is done on the assumption
that more prevalent senses will have strong associations with more nearest neighbors
because they have occurred in more contexts in the corpus used for producing the
thesaurus. Both the number and the distributional similarity of the neighbors are used
in our prevalence ranking measure. Pantel and Lin process the possible clusters in order
of their average distributional similarity and number of neighbors but do not take the
number of neighbors into account in the scores given for the clusters. The measures
that Pantel and Lin associate with their clusters are determined by the cohesiveness
of the cluster with the target word because their aim is one of sense discovery. Their
measure is the similarity between the cluster and the target word and does not retain
the distributional similarity of the neighbors within the cluster. It is quite possible that
there is a low frequency sense of a target word with synonyms that form a nice cohesive
group.
Although the number of neighbors assigned to a cluster may correlate with our
ranking score, intuition suggests that a combination of the quantity and distributional
similarity of neighbors to the target word sense is best for determining the relative
predominance of senses. In Section 6 we test this hypothesis using a simplified version
of our method which only uses the number of neighbors, and assigns each to one
sense. Comparisons with the CBC algorithm as it stands would be difficult because
in order to evaluate acquisition of predominance information we have used publicly
available gold-standard sense-tagged corpora, and these have WordNet senses. CBC
will not always find WordNet senses. For example, using the on-line demonstration of
CBC,9 several common senses from nouns from the Senseval-2 lexical sample are not
discovered, including the upright object sense of post, the block of something sense
of bar, the daytime sense of day and the meaning of the word sense of the word sense.
Automatic acquisition of sense inventories is an important endeavor, and we hope to
look at ways of combining our method for detecting predominance with automatically
induced inventories such as those produced by CBC. Evaluation of induced inventories
should be done in the context of an application, because the senses will be keyed to the
acquisition corpus and not to WordNet.
Induction of senses allows coverage of senses appearing in the data that are not
present in a predefined inventory. Although we could adapt our method for use with
an automatically induced inventory, our method which uses WordNet might also be
combined with one that can automatically find new senses from text and then relate
these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with
unknown nouns.
9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the
option to include all corpora (TREC-2002, TREC-9, and COSMOS).
562
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
4. Method
In our method, the predominant sense for a target word is determined from a preva-
lence ranking of the possible senses for that word. The senses come from a predefined
inventory (which might be a dictionary or WordNet-like resource). The ranking is
derived using a distributional thesaurus automatically produced from a large corpus,
and a semantic similarity measure defined over the sense inventory. The distributional
thesaurus contains a set of words that are ?nearest neighbors? to the target word with
respect to similarity of the way in which they are distributed. (Distributional similarity
is based on the hypothesis of Harris, 1968, that words which occur in similar contexts
have related meanings.) The thesaurus assigns a distributional similarity score to each
neighbor word, indicating its closeness to the target word. For example, the nearest10
neighbors of sandwich might be:
salad, pizza, bread, soup...
and the nearest neighbors of the polysemous noun star11 might be:
actor, footballer, planet, circle...
These neighbors reflect the various senses of the word, which for star might be:
 a celebrity
 a celestial body
 a shape
 a sign of the zodiac12
We assume that the number and distributional similarity scores of neighbors pertaining
to a given sense of a target word will reflect the prevalence of that sense in the corpus
from which the thesaurus was derived. This is because the more prevalent senses of the
word will appear more frequently and in more contexts than other, less prevalent senses.
The neighbors of the target word relate to its senses, but are themselves word forms
rather than senses. The senses of the target word are predefined in a sense inventory
and we use a semantic similarity score defined over the sense inventory to relate the
neighbors to the various senses of the target word. The two semantic similarity scores
that we use in this article are implemented in the WordNet similarity package. One uses
the overlap in definitions of word senses, based on Lesk (1986), and the other uses a
combination of corpus statistics and the WordNet hyponym hierarchy, based on Jiang
and Conrath (1997). We describe these fully in Section 4.2. We now describe intuitively
10 In this and other examples we restrict ourselves to four neighbors for brevity.
11 In this example we assume that the sense inventory assigns four senses to star, but the inventory could
assign fewer or more depending on its level of granularity and level of detail.
12 Note that this zodiac or horoscope sense of star usually occurs as part of the multiword star sign (e.g.,
your star sign secrets revealed) or in plural form (your stars today?free online).
563
Computational Linguistics Volume 33, Number 4
the measure for ranking the senses according to predominance, and then give a more
formal definition.
The measure uses the sum total of the distributional similarity scores of the k nearest
neighbors. This total is divided between the senses of the target word by apportioning
the distributional similarity of each neighbor to the senses. The contribution of each
neighbor is measured in terms of its distributional similarity score so that ?nearer?
neighbors count for more. The distributional similarity score of each neighbor is divided
between the various senses rather than attributing the neighbor to only one sense. This
is done because neighbors can relate to more than one sense due to relationships such
as systematic polysemy. For example, in the thesaurus we describe subsequently in
Section 4.1 acquired from the BNC, chicken has neighbors duck and goose which relate to
both the meat and animal senses. We apportion the contribution of a neighbor to each
of the word senses according to a weight which is the normalized semantic similarity
score between the sense and the neighbor. We normalize the semantic similarity scores
because some of the semantic similarity scores that we use, described in Section 4.2,
can get disproportionately large. Because we normalize the semantic similarity scores,
the sum of the ranking scores for a word equals the sum of the distributional similarity
scores. To summarize, we rank the senses of the target word, such as star, by apportion-
ing the distributional similarity scores of the top k neighbors between the senses. Each
distributional similarity score (dss) is weighted by a normalized semantic similarity
score (sss) between the sense and the neighbor. This process is illustrated in Figure 1.
More formally, to find the predominant sense of a word (w) we take each sense
in turn and obtain a prevalence score. Let Nw = {n1, n2...nk} be the ordered set of the
top scoring k neighbors of w from the distributional thesaurus with associated scores
{dss(w, n1), dss(w, n2), ...dss(w, nk)}. Let senses(w) be the set of senses of w in the sense
inventory. For each sense of w (si ? senses(w)) we obtain a prevalence score by summing
over the dss(w, nj) of each neighbor (nj ? Nw) multiplied by a weight. This weight is the
sss between the target sense (si) and nj divided by the sum of all sss scores for senses(w)
Figure 1
The prevalence ranking process for the noun star.
564
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
and nj. sss is the maximum WordNet similarity score (sss?) between si and the senses of
nj (sx ? senses(nj)).13 Each sense si ? senses(w) is therefore assigned a score as follows:
Prevalence Score(w, si) =
?
nj?Nw
dss(w, nj) ?
sss(si, nj)
?
si??senses(w) sss(si? , nj)
(1)
where
sss(si, nj) = max
sx?senses(nj )
sss?(si, sx) (2)
We describe dss and sss? in Sections 4.1 and 4.2. Note that the dss for a given neighbor
is shared between the different senses of w depending on the weight given by the
normalized sss.
4.1 The Distributional Similarity Score
Measures of distributional similarity take into account the shared contexts of the two
words. Several measures of distributional similarity have been described in the litera-
ture. In our experiments, dss is computed using Lin?s similarity measure (Lin 1998a).
We set the number of nearest neighbors to equal 50.14 We use three different sources of
data for our first two experiments, resulting in three distributional thesauruses. These
are described in the next section. We use domain-specific data for our third and fourth
experiments. The data sources for these are described in Sections 6.3 and 6.4.
A word, w, is described by a set of features, f , each with an associated frequency,
where each feature is a pair ?r, x? consisting of a grammatical relation name and the
other word in the relation. We computed distributional similarity scores for every pair of
words of the same PoS where each word?s total feature frequency was at least 10. A the-
saurus entry of size k for a target word w is then defined as the k most similar words to w.
A large number of distributional similarity measures have been proposed in the
literature (see Weeds 2003 for a review) and comparing them is outside the scope of this
work. However, the study of Weeds and Weir (2005) provides interesting insights into
what makes a ?good? distributional similarity measure in the contexts of semantic simi-
larity prediction and language modeling. In particular, weighting features by pointwise
mutual information (Church and Hanks 1989) appears to be beneficial. The pointwise
mutual information (I(w, f )) between a word and a feature is calculated as
I(w, f ) = log
P( f |w)
P( f )
(3)
Intuitively, this means that the occurrence of a less-common feature is more important
in describing a word than a more-common feature. For example, the verb eat is more
selective and tells us more about the meaning of its arguments than the verb be.
13 We use sss for the semantic similarity between a WordNet sense and another word, the neighbor. We use
sss? for the semantic similarity between two WordNet senses, si and a sense of the neighbor (sx).
14 From previous work (McCarthy et al 2004b), the value of k has a minimal effect on finding the
predominant sense; however, we will continue experimentation with this in the future for using our
ranking score for estimating probability distributions of senses, because a sufficiently large value of k will
be needed to include neighbors for rarer senses.
565
Computational Linguistics Volume 33, Number 4
We chose to use the distributional similarity score described by Lin (1998a) because
it is an unparameterized measure which uses pointwise mutual information to weight
features and which has been shown (Weeds 2003) to be highly competitive in making
predictions of semantic similarity. This measure is based on Lin?s information-theoretic
similarity theorem (Lin 1997):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
In our application, if T(w) is the set of features f such that I(w, f ) is positive, then the
similarity between two words, w and n, is
dss(w, n) =
?
f?T(w)?T(n)
(
I(w, f ) + I(n, f )
)
?
f?T(w) I(w, f ) +
?
f?T(n) I(n, f )
(4)
However, due to this choice of dss and the openness of the domain, we restrict ourselves
to only considering words with a total feature frequency of at least 10. Weeds et al (2005)
do show that distributional similarity can be computed for lower frequency words but
this is using a highly specialized corpus of 400,000 words from the biomedical domain.
Further, it has been shown (Weeds et al 2005; Weeds and Weir 2005) that performance
of Lin?s distributional similarity score decreases more significantly than other measures
for low frequency nouns. We leave the investigation of other distributional similarity
scores and the application to smaller corpora as areas for further study.
4.2 The Semantic Similarity Scores
WordNet is widely used for research in WSD because it is publicly available and there
are a number of associated sense-tagged corpora (Miller et al 1993; Cotton et al 2001;
Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes.
Several semantic similarity scores have been proposed that leverage the structure of
WordNet; for sss? we experiment with two of these, as implemented in the WordNet
Similarity Package (Patwardhan and Pedersen 2003).
The WordNet Similarity Package implements a range of similarity scores. McCarthy
et al (2004b) experimented with six of these for the sss? used in the prevalence score,
Equation (2). In the experiments reported here we use the two scores that performed
best in that previous work. We briefly summarize them here; Patwardhan, Banerjee,
and Pedersen (2003) give a more detailed discussion. The scores measure the similarity
between two WordNet senses (s1 and s2).
lesk This measure (Banerjee and Pedersen 2002) maximizes the number of overlap-
ping words in the gloss, or definition, of the senses. It uses the glosses of semanti-
cally related (according to WordNet) senses too. We use the default version of the
measure in the package with no normalizing for gloss length, and the default set
of relations:
lesk(s1, s2) = |{W1 ? definition(s1)}| ? |{W2 ? definition(s2)}| (5)
where definitions(s) is the gloss definition of sense s concatenated with the gloss
definitions of the senses related to s where the relationships are defined by the de-
566
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
fault set of relations in the relations.dat file supplied with the WordNet Similarity
package. W ? definition(s) is the set of words from the concatenated definitions.
jcn This measure (Jiang and Conrath 1997) uses corpus data to populate classes
(synsets) in the WordNet hierarchy with frequency counts. Each synset is incre-
mented with the frequency counts (from the corpus) of all words belonging to
that synset, directly or via the hyponymy relation. The frequency data is used to
calculate the ?information content? (IC; Resnik 1995) of a class as follows:
IC(s) = ?log(p(s)) (6)
Jiang and Conrath specify a distance measure:
Djcn(s1, s2) = IC(s1) + IC(s2) ? 2 ? IC(s3) (7)
where the third class (s3) is the most informative, or most specific, superordinate
synset of the two senses s1 and s2. This is converted to a similarity measure
in the WordNet Similarity package by taking the reciprocal as in Equation (8)
(which follows). For this reason, the jcn values can get very large indeed when
the distances are negligible, for example where the neighbor has a sense which is
a synonym. This is a motivation for our normalizing the sss in Equation (1).
jcn(s1, s2) = 1/Djcn(s1, s2) (8)
The IC data required for the jcn measure can be acquired automatically from raw
text. We used raw data from the BNC to create the IC files. There are various parameters
that can be set in the WordNet Similarity Package when creating these files; we used
the RESNIK method of counting frequencies in WordNet (Resnik 1995), the stop words
provided with the package, and no smoothing.
The lesk score is applicable to all parts of speech, whereas the jcn is applicable
only to nouns and verbs because it relies on IC counts which are obtained using the
hyponym links and these only exist for nouns and verbs.15 However, we did not use
jcn for verbs because in previous experiments (McCarthy et al 2004c) the lesk measure
outperformed jcn because the structure of the hyponym hierarchy is very shallow for
verbs and the measure is therefore considerably less informative for verbs than it is for
nouns.
4.3 An Example
We illustrate the application of our measure with an example. For star, if we set16 k = 4
and have the dss for the previously given neighbors as in the first row of Table 6, and
15 For verbs these pointers actually encode troponymy, which is a particular kind of entailment relation,
rather than hyponymy.
16 In this example, as before, we set k to 4 for the sake of brevity.
567
Computational Linguistics Volume 33, Number 4
Table 6
Example dss and sss scores for star and its neighbors.
Neighbors of star (dss)
Senses actor (0.22) footballer (0.12) planet (0.08) circle (0.03)
celebrity 0.42 0.53 0.02 0.01
celestial body 0.01 0.01 0.68 0.10
shape 0.0 0.0 0.02 0.78
zodiac 0.03 0.03 0.21 0.01
Total 0.46 0.57 0.93 0.90
the sss between the senses and the neighbors as in the remaining rows, the prevalence
score for celebrity would be:
= 0.22 ? 0.420.46 + 0.12 ? 0.530.57 + 0.08 ? 0.020.93 + 0.03 ? 0.010.90
= 0.2009 + 0.1116 + 0.0017 + 0.0003
= 0.3145
The prevalence score for each of the senses would be:
prevalence score(celebrity) = 0.3145
prevalence score(celestial body) = 0.0687
prevalence score(shape) = 0.0277
prevalence score(zodiac) = 0.0390
so the method would select celebrity as the predominant sense.
5. Experimental Setup
5.1 The Distributional Thesauruses
The three thesauruses used in our first two experiments were all created automatically
from raw corpus data, based either on grammatical relations between words computed
by syntactic parsers or alternatively on word proximity relations.
We created the first thesaurus, which we call BNC, from grammatical relation output
produced by the RASP system (Briscoe and Carroll 2002) applied to the 90M words of
the ?written? portion of the British National Corpus (Leech 1992), for all polysemous
nouns, verbs, adjectives, and adverbs in WordNet. For each word we considered co-
occurring words in the grammatical contexts listed in Table 7.
In the first two experiments, we also use two further automatically computed
distributional thesauruses, produced by Dekang Lin from 125M words of text from the
Wall Street Journal, San Jose Mercury News, and AP Newswire, using the same similarity
measure. The thesauruses are publicly available.17 One was constructed based on word
17 The thesauruses are available for download from http://www.cs.ualberta.ca/~lindek/
downloads.htm.
568
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 7
Grammatical contexts used for acquiring the BNC thesaurus.
PoS Grammatical contexts
noun verb in direct object or subject relation, adjective or noun modifier
verb noun as direct object or subject
adjective modified noun, modifying adverb
adverb modified adjective or verb
Table 8
Thesaurus coverage of polysemous words (excluding multiwords) in WordNet 1.6.
PoS Thesaurus types NISC NITH
noun BNC 7,090 2,436 115
noun DEP 6,583 2,176 217
noun PROX 6,582 2,176 217
verb BNC 2,958 553 45
adjective BNC 3,659 1,208 123
adverb BNC 505 132 38
similarities computed from syntactic dependencies produced by MINIPAR (Lin 1998b),
and the other was constructed based on textual proximity relationships between words.
We refer below to the original corpus as NEWSWIRE, and these two thesauruses as DEP
and PROX, respectively. We restricted our experiments to the nouns in these thesauruses.
Table 8 contains details of the numbers of polysemous (according to WordNet 1.6)
words contained in these thesauruses, the number of words in SemCor that were not
found in these thesauruses (NITH) and the number of words in the thesauruses that
were not in SemCor (NISC).
For the experiments described in Sections 6.3 and 6.4 we use exactly the same
method as that proposed for the BNC thesaurus, however the data source is different
and is described in those sections.
5.2 The Sense Inventory
We use WordNet version 1.6 as the sense inventory for our first three experiments, and
1.7.1 for our last experiment.18
For sss? we use the WordNet Similarity Package version 0.05 (Patwardhan and
Pedersen 2003).
18 We use 1.6 which is a rather old version of WordNet so that we can directly evaluate on the SemCor data
released with this version; we also use it to enable comparison with the results of McCarthy et al (2004a).
We use WordNet 1.7.1 for the fourth experiment, because this is the version that was used for annotating
the test data in that experiment. We plan to move to more recent versions of WordNet and experiment
with other sense inventories in the future.
569
Computational Linguistics Volume 33, Number 4
6. Experiments
In this section we describe four experiments using our method for acquiring predomi-
nant sense information.
The first experiment evaluates automatically acquired predominant senses for all
parts of speech, using SemCor as the test corpus. This extends previous work which
had only evaluated all PoS on Senseval-2 (Cotton et al 2001) and Senseval-3 (Mihalcea
and Edmonds 2004) data. The SemCor corpus is composed of 220,000 words, in contrast
to the 6 documents in the Senseval-2 and -3 English all-words data (10,000 words). We
examine the effects of using the two different semantic similarity scores that performed
well in previous work: jcn is quick to compute but lesk has the advantage that it is
applicable to all PoS and can be implemented for any dictionary with sense defini-
tions. We compare three thesauruses: one is derived from the BNC and two from the
NEWSWIRE corpus. The two from the NEWSWIRE corpus examine the requirement for
a parser by contrasting results obtained when the thesaurus is built using parsed data
compared to a proximity approach. We contrast the results of the BNC thesaurus with
a simplified version of the prevalence score which uses the number of the k neighbors
closest to a sense for ranking without using the dss and without sharing the credit for
a neighbor between senses. We also perform an error analysis on a random sample
of words for which a predominant sense was found that differed from that given by
SemCor, identifying and giving an indication of the frequencies of the main sources of
error.
The second experiment is on nouns in the Senseval-2 all-words data, again using
predominant senses acquired using each of the three distributional thesauruses, but
in this experiment we explore the benefits of an automatic first sense heuristic when
there is inadequate data in available resources. Although McCarthy et al (2004c) show
that on Senseval-2 and Senseval-3 test data a first sense heuristic derived from SemCor
outperforms the automatic method, we look at whether the method?s performance is
relatively stronger on words for which there is little data in SemCor. This is important
because, as we have shown in Table 5, low frequency words are used often in senses
other than the sense that is ranked first according to SemCor.
In addition to the issue of lack of coverage of manually annotated resources, sense
frequency will depend on the domain of the data. In the third experiment, we revisit
some previous work on noun senses and domain (McCarthy et al 2004a) using corpora
of news text about sports and finance. Using distributional thesauruses computed from
these corpora and a gold standard domain labeling of word senses we look at the
potential for computing domain-specific predominant senses for parts of speech other
than nouns.
Continuing the line of research on automatic acquisition of domain-specific pre-
dominant senses, the fourth experiment compares results when we train and test on
domain-specific corpora, where the training data is (1) manually categorized for domain
and from the same corpus as the gold-standard test data, and (2) where the training data
is harvested automatically from another corpus which is categorized automatically.
6.1 Experiment 1: All Parts of Speech
In this experiment, we evaluate the accuracy of automatically acquired predominant
senses for all open class parts of speech, taking SemCor as the gold standard. For nouns
we use the semantic similarity measures lesk and jcn, and for other parts of speech, lesk.
We use the three distributional thesauruses BNC, DEP, and PROX.
570
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The gold standard is derived from the Brown Corpus files publicly released as part
of SemCor, rather than the processed data provided in the cntlist file in the WordNet
distribution. The released SemCor files contain only the tagged data from the Brown
Corpus and do not include data from The Red Badge of Courage. We use the released data
rather than that in cntlist because this includes the actual tagged examples which are
marked for genre by the Brown files. We envisage the possibility of further experiments
with these genre markers. We only evaluate on instances where a single, unique sense
is supplied by the annotators. So, for example, we ignore instances like the following
with multiple wnsn values:
<wf cmd=done pos=NN lemma=tooth wnsn=3;1 lexsn=1:05:02::;1:08:00::>tooth</wf>
We also only evaluate on polysemous words (according to WordNet) having one sense
in SemCor which is more frequent than any other, and for which both SemCor and our
thesauruses have at least a minimal amount of data. Specifically, a word must occur
three or more times in SemCor; it must also occur in ten or more grammatical relations
in the parsed version of the BNC and have neighbors in the distributional thesaurus, or
be present in Dekang Lin?s thesaurus.19
We evaluate on nouns, verbs, adjectives, and adverbs separately, computing a num-
ber of accuracy measures, both type-based and token-based. PSacc is calculated over
word types in SemCor which have one sense which occurs more than any other. It is the
accuracy of identifying the predominant sense in SemCor. If the automatic ranking has
a tie for the top ranked sense then we score that word as incorrect.20 So we have
PSacc =
|correcttyp|
|typesmf |
? 100 (9)
where typesmf are the types in SemCor such that one sense is more frequent than
any other, the word has occurred at least three times in SemCor and has an entry
in the thesaurus. |correcttyp| is the number of these where the automatically acquired
predominant sense matches the first sense in SemCor.
PSaccBL is the predominant sense random baseline, obtained as follows:
PSaccBL =
?
w?typesmf
1
|senses(w)|
|typesmf |
? 100 (10)
WSDsc is a token-based measure. It is the WSD accuracy that would be obtained
by using the first sense heuristic with the automatically acquired predominant sense
information, in cases where there was a unique automatic top ranked sense:
WSDsc =
|correcttok|
|SCtokensafs|
? 100 (11)
19 Although we do not evaluate words for which there were no neighbors in the thesaurus, we could extend
the thesaurus to include some of these by widening the range of grammatical relations covered and
compensating for some systematic PoS tagging errors.
20 If we exclude these words with joint top ranking from the automatic method (precision rather than recall)
then we obtain marginally higher accuracy for the jcn measure but no difference for lesk.
571
Computational Linguistics Volume 33, Number 4
where |correcttok| is the number of tokens disambiguated correctly out of the tokens in
SemCor having an automatically acquired first sense (SCtokensafs).
SC FS is the WSD accuracy of the SemCor first sense heuristic on the same set of
tokens (SCtokensafs), which is the upper bound because the information it uses is derived
from the test data itself. RBL is the random baseline for the WSD task, calculated by
splitting the credit for each token to be tagged in the test data evenly between all of the
word?s senses.
RBL =
?
w?SCtokensafs
1
|senses(w)|
|SCtokensafs|
? 100 (12)
The results are shown in Table 9. We examined differences between the semantic
similarity measures (lesk and jcn), the BNC and DEP thesauruses, and the DEP and
PROX thesauruses using the ?2 test of significance with one degree of freedom (Siegel
and Castellan 1988). None of the differences between the different combinations of
similarity measures and thesauruses for the type-based measure PSacc are significant.
The differences between lesk and jcn are significant for the token-based measure WSDsc
for both the BNC and PROX thesauruses (both p < .001), however not when comparing
lesk and jcn for the DEP thesaurus. Although lesk is more accurate than jcn, at least on
the WSD task, jcn is much faster because of the precompilation of IC in the WordNet
similarity package; however, lesk has the additional benefit of being applicable to other
parts of speech. The method gives particularly good results for adjectives, given that
they have a similar random baseline to nouns. It does not do so well for adverbs and
verbs, but still performs well above the random baseline which is low for verbs due
to their high degree of polysemy. Given that the first sense heuristic from SemCor is
particularly strong for adverbs, it is disappointing that the automatic method does not
perform as well as it does on adjectives. One possible reason for this might be that
adverbs are often less strongly associated to the verbs that they modify than adjectives
are to the nouns that they modify, so the distributional thesaurus information is less
reliable. Another reason may be that less data are available for adverbs, both in the
thesaurus and also in WordNet.
Table 9
Evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 54.5 32.3 53,468 48.7 68.6 24.7
noun lesk DEP 2,437 56.3 32.1 52,158 49.2 68.4 24.6
noun lesk PROX 2,437 55.9 32.1 52,158 49.0 68.4 24.6
noun jcn BNC 2,555 54.0 32.3 53,429 46.1 68.6 24.7
noun jcn DEP 2,436 56.4 32.1 52,122 48.8 68.4 24.6
noun jcn PROX 2,436 55.9 32.1 52,117 47.7 68.4 24.6
verb lesk BNC 1,149 45.6 27.1 31,182 36.1 57.1 17.1
adjective lesk BNC 1,154 60.4 32.8 18,216 56.8 73.8 24.9
adverb lesk BNC 230 52.2 39.9 8,810 43.2 76.1 33.0
572
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Comparing the results for the DEP and the PROX thesauruses, we see that although
there is no significant difference in PSacc (with either lesk or jcn), there is for WSDsc
when using jcn (p < .001), but not when comparing the lesk values for these thesauruses.
Even though the differences between jcn DEP and jcn PROX are significant, the absolute
differences are nevertheless relatively small; this bodes well for applying the automatic
predominant sense method to languages less well resourced than English, because
the PROX thesaurus was produced without using a parser. The differences in results
between jcn BNC and jcn DEP for nouns are statistically significant (p < .001).21 The
better accuracy with DEP may be because the NEWSWIRE corpus is larger than the
BNC. We intend to investigate the effects of corpus size in the future. The differences in
results between lesk BNC and lesk DEP for nouns are not significant.
6.1.1 Results Using Simplified Prevalence Score. A simple variation of our method is just to
associate each neighbor with just one sense and use the number of neighbors associated
with a sense for the prevalence score. This gives a modified version of Equation (1)
where each sense si ? senses(w) is assigned a score as follows:
Simplified Prevalence Score(w, si) = |{nj ? Nw} : arg max
sk?senses(w)
(sss(sk, nj)) = si| (13)
where
sss(sk, nj) = max
sx?senses(nj )
sss?(sk, sx) (14)
For the example in Table 6, celebrity would get the top score of 2 (due to it having
the highest sss for actor and footballer), celestial body would get a score of 1 (due to its
sss with planet), shape would get 1 (due to circle), and zodiac would obtain a Simplified
Prevalence Score of 0 because it does not have the highest sss for any of the neighbors.
As the results from Table 10 show, we do not get such good results with this score.
This supports our intuition that a combination of both the number of neighbors and
their distributional similarity scores is important for determining predominance. The
rest of the article gives results and analysis for our original prevalence score as given in
Equation (1).
6.1.2 Error Analysis. We took a random sample of 80 words that occurred more than five
times in SemCor, 20 words for each PoS, from those where the automatically identified
predominant sense was different from the SemCor first sense when using the lesk sss
and BNC thesaurus and our ranking score as defined in Equation (1) (i.e., the data
represented by the first result line and the last three result lines of Table 9). Herein, we
call the automatically identified sense AUTO FS, and the SemCor sense SemCor FS. We
21 The coverage of the SemCor data by the DEP and PROX thesauruses is slightly lower than that of the
BNC-derived thesaurus due to mismatches in spelling and capitalization and also probably because the
NEWSWIRE corpus is narrower in genre and domain than the BNC.
573
Computational Linguistics Volume 33, Number 4
Table 10
Simplified prevalence score, evaluation on SemCor, polysemous words only.
Type Token
PoS Settings No. PSacc PSaccBL No. WSDsc SC FS RBL
noun lesk BNC 2,555 52.9 32.3 53,175 47.2 68.6 24.7
noun jcn BNC 2,555 50.1 32.3 52,033 46.7 69.2 24.8
verb lesk BNC 1,149 45.1 27.1 30,364 36.7 58.0 17.4
adjective lesk BNC 1,154 58.3 32.8 18,136 56.0 73.7 24.8
adverb lesk BNC 230 50.0 39.9 8,802 42.2 76.1 33.0
manually inspected the data for each of the words to find the source of the problem.
We did not have the (substantial) resources that would be required to sense tag all
occurrences of these words in the BNC to see what their actual first senses were. Instead,
we examined the parses, grammatical relations, and sense definitions for the words to
see why the AUTO FS was ranked above the SemCor FS. We found the following main
types of error:22
corpora The difference appears to be due to genuine divergence between the BNC
and SemCor. For this error type we looked at the BNC parses to see if the acquired
predominant sense was clearly due to differences in the corpus data. There may
be other errors that should have been assigned this category, but without access
to sense tagged BNC data we could not be sure of this, so we used this category
conservatively. An example of this error is the adjective solid which has the good
quality first sense in the Brown files in SemCor, but the firm sense according to
our BNC automatic ranking.
related The automatic predominant sense is closely related to the SemCor first sense.
Although many word senses are related to some extent, the category was picked
where a close relationship seemed to be the main cause of the error. An example
is the noun straw which has two senses in WordNet 1.6, fibre used for hats and
fodder and plant material. The SemCor FS was the former whereas our AUTO FS
was the latter.
competing Two or more related senses are ranked highly but they are overtaken by
an unrelated sense. For example, the ranking and scores for the noun transmission
are:
WordNet sense Description Prevalence score
5 gears 1.79
2 communication 1.20
1 act of sending a message 1.19
3 fraction of radiant energy 0.48
4 infection 0.15
22 There were a few other, less numerous types of error, for example systematic PoS mis-tagging of particles
(such as down) as adverbs.
574
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
The act of sending a message sense is overtaken by the gears sense because the
credit from shared distributional neighbors is split between it and the communi-
cation sense.
neighbors There are not many neighbors related to the sense. There can be various
reasons for this, such as the sense having restricted contexts of occurrence or only
a small number of near synonyms existing for the sense. An example of this is
the adjective live where the SemCor FS unrecorded sense seems to occur in the
BNC corpus more than the alive sense; there are plenty of grammatical relations
pertaining to this sense, but there are few distributional neighbors near in meaning
to unrecorded.23
spurious similarity The WordNet similarity scores were misled by spurious relation-
ships to neighbors; this can occur in dense areas such as the ?physical object?
region of the noun hyponym hierarchy. An example of this is the verb tap which
has neighbors push and press which are related to the AUTO FS (solicit) as well as
the SemCor FS (strike lightly).
The results of the error analysis are shown in Table 11. The analysis shows that
differences between the training (BNC) and testing (SemCor) corpora are not a major
source of error. Although SemCor itself (the released files from the Brown corpus
comprising only 200,000 words) is not large enough to build a thesaurus with entries
for a reasonable portion of the words, we did build a thesaurus from the entire Brown
corpus (1 million words) to see the effect of corpus data. The results are compared
to those from the BNC in Table 12 on the set of words which had thesaurus entries
in the Brown data (to make the results more comparable, because the corpora are of
such different sizes). We also show the average results for 10 random selections of a
1 million word random sample of the BNC. To do this we randomly selected 190 th of the
tuples.24 The differences in the WSDsc for the BNC 190 sample and the Brown corpus are
significant (p < .01 on the ?2), but the differences in PSacc are not significant. Although
the entire BNC produced better results than the Brown data, this is undoubtedly due to
the difference in size of the corpus. Taking a comparably sized sample, the results are
slightly better from Brown which is the corpus from which SemCor is taken.
For nouns, it was apparent that in two cases less-prevalent senses were receiving a
higher ranking simply because the credit for some neighbors associated with another
meaning was split between related senses (error type competing). This was not ob-
served for other parts of speech, possibly because the AUTO FS was rarely unrelated to
the SemCor FS.
There were some problems arising from spurious similarity. One possible source
of such problems is due to the ambiguity of the neighbor; in the future we will look
at reducing this source of error by removing neighbors which have a value for sx in
Equation (2) which is not the same as that preferred by the other senses of the target
word (w). For adverbs, all the cases that were categorized as spurious similarity were
also noted to be related to the SemCor FS, though they were not categorized as related
as this was not considered the primary cause of the error.
The analysis was hardest for verbs. Verbs are on average highly polysemous, and
often have senses that are related. Furthermore, the structure of the WordNet verb tro-
ponym hierarchy is very shallow compared to the noun hyponymy hierarchy, so there
23 The closest neighbors to the adjective live are adult, forthcoming, lively, solo, excellent, stuffed, living, dead,
and australian weekly.
24 The variance for the 190 sample for PSacc was 0.46 and for WSDsc it was 0.49.
575
Computational Linguistics Volume 33, Number 4
Table 11
Results of the error analysis for the sample of 80 words.
PoS
noun verb adjective adverb All PoS
corpora 1 2 1 1 5
related 8 12 13 8 41
competing 2 0 0 0 2
neighbors 4 3 2 2 11
spurious similarity 5 3 4 9 21
Table 12
SemCor results for Nouns using jcn.
Thesaurus PSacc% WSDsc %
full BNC 53.8 44.9
1
90 BNC 46.6 40.8
Brown 47.2 41.7
are more possibilities for spurious similarities from overlap of glosses. So, although we
tried to identify the main problem source, for verbs the problems usually arose from a
combination of factors and the relatedness of the senses was usually one of these.
Relatedness of senses and fine-grained distinctions are major sources of error. There
have been various attempts to group WordNet senses both manually and automati-
cally (Agirre and Lopez de Lacalle 2003; McCarthy 2006; Palmer, Dang, and Fellbaum
2007). Indeed, McCarthy demonstrated that distributional and semantic similarity can
be used for relating word senses and that such methods increase accuracy of first
sense heuristics, including the automatic method proposed here. WSD is improved with
coarser-grained inventories but ultimately, performance depends on the application.
For example, the noun bar has 11 senses in WordNet 1.6. These include the pub sense
as well as the counter sense and these are related to a certain extent. One might want
to group them when acquiring predominant senses, but there may be situations where
they should be distinguished. For example, if one were to ask a robot to ?go to the bar?
one would hope it could use the context to go get the drinks rather than replying that it
is already there! Even in cases where fine-grained distinctions are ultimately required, it
may be helpful to have a coarse-grained prior and then use contextual features to tease
apart subtle sense distinctions.
From our error analysis, the problem of semantically isolated senses (identified as
neighbors) was not a major source of error, but still causes some problems. One possible
remedy might be to identify these cases by looking for neighbors which relate strongly
to a sense which none of the other neighbors relate to and weighting the contribution
from these neighbors more. This may however give rise to further errors because of the
noise introduced by focusing on individual neighbors. We will explore such directions
in future work.
576
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
In this experiment we did not assign any credit for near misses. In many cases
of error the SemCor FS nonetheless received a high prevalence score. In the future
we hope to use the score for probability estimation, and combine this with contextual
information for WSD as in related work by Lapata and Brew (2004) and Chan and Ng
(2005).
6.2 Experiment 2: Frequency and the SemCor First Sense Heuristic
In the previous section we described an evaluation of the accuracy of automatically
acquired predominant sense information. We carried out the evaluation with respect to
SemCor in order to have as much test data as possible. To obtain reasonably reliable
gold-standard first-sense data and first-sense heuristic upper bounds, we limited the
evaluation to words occurring at least three times in SemCor. Clearly this scenario is
unrealistic. For many words, and particularly for nouns, there is very little or no data in
SemCor; Table 2 shows that 81.9% of nouns (excluding multiwords) listed in WordNet
do not occur at all in SemCor. Thus, even for English, which has substantial manually
sense-tagged resources, coverage is severely limited for many words.
For a more realistic comparison of automatic and manual heuristics, we therefore
now change to a different test corpus, the Senseval-2 English all-words task data set. We
focus on nouns and evaluate using all words regardless of their frequencies in SemCor.
We examine the effect of frequency in SemCor on performance of a SemCor-derived
heuristic in comparison to results from our automatic method on the same words. Our
hypothesis is that although automatically acquired predominant sense information may
not outperform first-sense data obtained from a hand-tagged resource over all words in
a text, the information may well be more accurate for low frequency items.
We use a mapping between different WordNet versions25 (Daude?, Padro?, and Rigau
2000) to obtain the Senseval-2 all words noun data (originally distributed with 1.7 sense
numbers) with 1.6 sense numbers. As well as examining the performance of our method
in contrast to the SemCor heuristic, we calculate an upper bound for this using the
first sense heuristic from the Senseval-2 all-words data itself. This is obtained for nouns
with two or more occurrences in the Senseval-2 data and where one sense occurs more
than any of the others. We calculate type, precision, and recall, using this Senseval-2
first-sense as the gold standard. The recall measure is the same as PSacc described
previously, except that we include items which do not have entries in the thesaurus,
scoring them incorrect. Precision only includes items where there is a sense ranked
higher than any other for that word with the prevalence score, that is, it does not include
items with a joint automatic ranking. We also calculate token precision and recall (WSD).
These measures relate to WSDsc, but again, recall includes words not in the thesaurus
which are scored incorrect, and precision does not include items with a joint automatic
ranking. We also separately compute WSD precision for words not in SemCor (NISC).
The results are shown in Table 13.26
The automatically acquired predominant sense results (the first six lines of results
in the table) are approaching the SemCor-derived results (third line from the bottom of
the table). The NISC results are particularly encouraging, but with the caveat that there
are only 17 such words in the data. The precision for these items is higher than the
25 This mapping is available at http://www.lsi.upc.es/~nlp/tools/mapping.html.
26 Note that these figures are lower than those of McCarthy et al (2004a) in a similar experiment because
the evaluation here is only on polysemous words.
577
Computational Linguistics Volume 33, Number 4
Table 13
Evaluating predominant sense information for polysemous nouns on the Senseval-2 all-words
task data.
Type WSD/token
Settings Precision (%) Recall (%) Precision (%) Recall (%) Precision NISC (%)
lesk BNC 56.3 53.7 54.6 53.4 58.3
lesk DEP 52.0 47.2 52.6 48.7 58.3
lesk PROX 52.0 47.2 52.3 48.5 58.3
jcn BNC 52.4 50.0 51.8 50.6 66.7
jcn DEP 52.0 47.2 58.0 53.7 83.3
jcn PROX 53.1 48.1 57.3 53.1 83.3
SemCor 64.8 63.0 58.5 57.3 0.0
Senseval-2 ? ? 90.8 60.1 100.0
RBL 26.5 26.5 26.0 26.0 50.0
overall figure. This is because the nouns involved are less frequent so tend to be less
polysemous and consequently have a higher random baseline. There are a few nouns
that are not in the automatic ranking, but this is due to the fact that neighbors were
not collected for these nouns in the thesaurus because of tagging or parser errors or
the particular set of grammatical relations used. It should be possible to extend the
range of grammatical relations, or use proximity-based relations, so that neighbors can
be obtained in these cases. It would also be possible to assign some credit in the case of
joint top ranked senses to increase coverage.
Looking at Table 13 in more detail, it seems to be the case that although the BNC
thesaurus does well in identifying the first sense of a word (the type results), the PROX
and DEP thesauruses from the NEWSWIRE corpus return better WSD results when
used with the jcn measure. This is possibly because jcn works well for more frequent
items due to its incorporation of frequency information, and the NEWSWIRE corpus
has more data for frequent words, although coverage is not as good as the BNC as
seen by the bigger differences in precision and recall and the figures in Table 8. The
lower coverage may be due to the narrower domain and genre of the NEWSWIRE
corpus, though spelling and capitalization differences probably also account for some
differences.
Table 14 shows results on the Senseval-2 nouns for the best similarity measure
and thesaurus combinations in Table 13 for nouns at or below various frequencies
in SemCor. (The differences between the DEP and PROX thesauruses are negligible at
frequencies of 10 or below, so for those we report only the results for DEP.) As we
anticipated, for low frequency words the automatic methods do give more accurate
predominant sense information than SemCor. The low number of test items at frequency
five or less means that results for jcn with the BNC thesaurus are not significantly better
when compared with SemCor (p = .05); however the lesk WSD results are significantly
better (p < .01 for the ? 1 threshold and p < .05 for the ? 5 threshold). On the whole, we
see that the automatic method, using either jcn or lesk and any of the three thesauruses,
tend to give better results than SemCor on nouns which have low coverage in SemCor.
Figures 2 and 3 show the precision for type and token (WSD) evaluation where the
items have a frequency at or below given thresholds in SemCor. Although the manually
578
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 14
Senseval-2 results, polysemous nouns only, broken down by their frequencies of occurrence in
SemCor.
Type WSD/token
No. of occurrences in Precision Recall Precision Recall
SemCor (no. of words) Settings (%) (%) (%) (%)
jcn BNC 100.0 33.3 66.7 47.1
lesk BNC 100.0 33.3 58.3 41.2
0 (17) jcn DEP 100.0 33.3 83.3 58.8
lesk DEP 100.0 33.3 58.3 41.2
SemCor 0.0 0.0 0.0 0.0
Senseval-2 ? ? 100.0 52.9
RBL 38.9 38.9 46.1 46.1
jcn BNC 66.7 44.4 54.1 45.5
lesk BNC 83.3 55.6 67.6 56.8
jcn DEP 50.0 22.2 51.7 34.1
? 1 (44) lesk DEP 75.0 33.3 69.0 45.5
SemCor 50.0 33.3 33.3 20.5
Senseval-2 ? ? 93.3 63.6
RBL 40.7 40.7 42.8 42.8
jcn BNC 80.0 61.5 63.0 57.5
lesk BNC 90.0 69.2 71.2 65.0
? 5 (80) jcn DEP 71.4 38.5 56.7 42.5
lesk DEP 85.7 46.2 70.0 52.5
SemCor 60.0 46.2 54.0 42.5
Senseval-2 ? ? 95.9 58.8
RBL 38.1 38.1 39.1 39.1
jcn BNC 75.0 63.2 59.3 55.8
lesk BNC 68.8 57.9 62.8 59.2
? 10 (120) jcn DEP 66.7 42.1 56.8 45.0
lesk DEP 58.3 36.8 58.9 46.7
SemCor 68.8 57.9 57.3 49.2
Senseval-2 ? ? 96.8 50.8
RBL 37.5 37.5 38.0 38.0
jcn BNC 76.0 67.9 66.7 64.8
lesk BNC 64.0 57.1 71.6 69.6
? 15 (250) jcn DEP 60.0 42.9 68.8 55.6
lesk DEP 55.0 39.3 67.3 54.4
jcn PROX 70.0 50.0 72.3 58.4
lesk PROX 55.0 39.3 66.8 54.0
SemCor 64.0 57.1 66.5 62.0
Senseval-2 ? ? 98.8 68.4
RBL 32.9 32.9 30.4 30.4
jcn BNC 52.4 50.0 51.8 50.6
lesk BNC 56.3 53.7 54.6 53.4
all (786) jcn DEP 52.0 47.2 58.0 53.7
lesk DEP 52.0 47.2 52.6 48.7
jcn PROX 53.1 48.1 57.3 53.1
lesk PROX 52.0 47.2 52.3 48.5
SemCor 64.8 63.0 58.5 57.3
Senseval-2 ? ? 90.8 60.1
RBL 26.5 26.5 26.0 26.0
579
Computational Linguistics Volume 33, Number 4
Figure 2
?TYPE? precision on finding the predominant sense for the Senseval-2 English all-words test
data for nouns having a frequency less than or equal to various thresholds.
produced SemCor first-sense heuristic outperforms the automatic methods over all the
test items (see the ?all? results in Table 14), when items are below a frequency threshold
of five the automatic methods give better results. Indeed, as the threshold is moved
up to 20 and even 30, more nouns are covered, and the automatic methods are still
comparable and in some cases competitive with the SemCor heuristic.
6.3 Experiment 3: The Influence of Domain
In this experiment, we investigate the potential of the automatic ranking method for
computing predominant senses with respect to particular domains. We have previ-
ously demonstrated that the method produces intuitive domain-specific models for
nouns (McCarthy et al 2004a), and that these can be more accurate than first senses de-
rived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005).
Here we investigate the behavior for other parts of speech, using a similar experimental
setup to that of McCarthy et al That work used the subject field codes (SFC) (Magnini
and Cavaglia` 2000)27 as a gold standard. In SFC the Princeton English WordNet is
augmented with some domain labels. Every synset in WordNet?s sense inventory is
annotated with at least one domain label, selected from a set of about 200 labels. These
labels are organized in a tree structure. Each synset of WordNet 1.6 is labeled with one
or more labels. The label factotum is assigned if any other is inadequate. The first level
consists of five main categories (e.g., doctrines and social science) and factotum.doctrines
27 More recently referred to as WordNet Domains (WN-DOMAINS).
580
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Figure 3
WSD precision on the Senseval-2 English all-words test data for nouns having a frequency less
than or equal to various thresholds.
has subcategories such as art, religion, and psychology. Some subcategories are further
divided in subcategories (e.g., dance, music, and theatre are subcategories of art).
McCarthy et al (2004a) used two domain-specific corpora for input to the method
for finding predominant senses. The corpora were obtained from the Reuters Corpus,
Volume 1 (RCV1; Rose, Stevenson, and Whitehead 2002) using the Reuters topic codes.
The two domain-specific corpora were:
SPORTS (Reuters topic code GSPO), 9.1 million words
FINANCE (Reuters topic codes ECAT and MCAT), 32.5 million words
In that work we produced sense rankings for a set of 38 nouns which have at
least one synset with an economy SFC label and one with a sport SFC label. We then
demonstrated that there were more sport labels assigned to the predominant senses
acquired from the SPORTS corpus and more economy labels assigned to those from the
FINANCE corpus. The predominant senses from both domains had a similarly high
percentage of factotum (domain-independent) labels. We reproduce the results here (in
Figure 4) for ease of reference, and for comparison with other results presented in this
section. The y-axis in this figure shows the percentage of the predominant sense labels
for these 38 nouns that have the SFC label indicated by the x-axis.
We envisaged running the same experiment with verbs, adjectives, and adverbs,
although we suspected that these would show less domain-specific tendencies and
there would be fewer candidate words to work with. The SFC labels for all senses of
polysemous words (excluding multiwords) in the various parts of speech are shown in
Table 15. We see from the distribution of factotum labels across the parts of speech that
nouns are certainly the PoS most likely to be influenced by domain.
To produce results like Figure 4 for each PoS, we needed words having at least one
synset with a sport label and one with an economy label. There were 20 such verbs but
581
Computational Linguistics Volume 33, Number 4
Figure 4
Distribution of domain labels of predominant senses for 38 polysemous nouns ranked using the
SPORTS and FINANCE corpora.
only two adjectives and no adverbs meeting this condition. We therefore performed
the experiment only with verbs. To do this we used the SPORTS and FINANCE corpora
as before, computing thesauruses for verbs using the grammatical relations specified
in Table 7. The results for the distribution of domain labels of the predominant senses
Table 15
Most frequent SFC labels for all senses of polysemous words in WordNet, by part of speech.
Domain % Domain %
noun biology 29.3 verb factotum 67.0
factotum 20.7 psychology 3.5
art 6.2 sport 2.9
sport 3.1 art 2.5
medicine 3.1 biology 2.5
other 37.6 other 21.6
adjective factotum 67.8 adverb factotum 81.4
biology 6.5 psychology 7.5
art 3.2 art 1.8
psychology 2.7 physics 1.1
physics 1.9 economy 1.1
other 17.9 other 7.1
582
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
acquired from the SPORTS and FINANCE corpora are shown in Figure 5. We see the same
tendency for sport labels for predominant senses from the SPORTS corpus and economy
labels for the predominant senses from the FINANCE corpus, but the relationship is
less marked compared with nouns because of the high proportions of factotum senses
in both corpora for verbs. We believe that acquisition of domain-specific predominant
senses should be focused on those words which show domain-specific tendencies. We
hope to put more work into automatic detection of these tendencies using indicators
such as domain salience and words that have different sense rankings in a given domain
compared to the BNC (as discussed by Koeling, McCarthy, and Carroll 2005).
6.4 Experiment 4: Domain-Specific Predominant Sense Acquisition
In the final set of experiments we evaluate the acquired predominant senses for domain-
specific corpora. The first of the two experiments was reported by Koeling, McCarthy,
and Carroll (2005), but we extend it by the second experiment reported subsequently.
Because there are no publicly available domain-specific manually sense-tagged corpora,
we created our own gold standard. The two chosen domains (SPORTS and FINANCE) and
the domain-neutral corpus (BNC) are the same as we used in the previous experiment.
We selected 40 words and we sampled (randomly) sentences containing these words
from the three corpora and asked annotators to choose the correct sense for the target
words. The set consists of 17 words which have at least one sense assigned an economy
domain label and at least one sense assigned a sports label: club, manager, record, right,
bill, check, competition, conversion, crew, delivery, division, fishing, reserve, return, score,
receiver, running; eight words that are particular salient in the SPORTS domain: fan,
star, transfer, striker, goal, title, tie, coach; eight words that are particular salient in the
Figure 5
Distribution of domain labels of predominant senses for 20 polysemous verbs ranked using the
SPORTS and FINANCE corpora.
583
Computational Linguistics Volume 33, Number 4
Table 16
WSD using predominant senses, training, and testing on all domain combinations
(hand-classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 39.1 49.9 24.0
SPORTS 25.7 19.7 43.7
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
FINANCE domain: package, chip, bond, market, strike, bank, share, target; and seven words
that are equally salient in both domains: will, phase, half, top, performance, level, country.
Koeling, McCarthy, and Carroll (2005) give further details of the construction of the gold
standard.
In the first experiment, we train on a corpus of documents with manually assigned
domain labels (i.e., sub-corpora of the Reuters corpus, see Section 6.3), and we test on
data from the same source. In a second experiment we build a text classifier, use the
text classifier to obtain SPORTS and FINANCE corpora (using general newswire text from
the English Gigaword Corpus; Graff 2003) and test on the gold-standard data from the
Reuters corpus. The second experiment eliminates issues about dependencies between
training and test data and will shed light on the question of how robust the acquired
predominant sense method is with respect to noise in the input data. At the same time,
the second experiment paves the way towards creating predominant sense inventories
for any conceivable domain.
6.4.1 Experiment Using Hand-Labeled Data. In this section we focus on the predominant
sense evaluation of the experiments described by Koeling, McCarthy, and Carroll (2005).
After running the predominant sense finding algorithms on the raw text of the two do-
main corpora (SPORTS and FINANCE) and the domain-neutral corpus (BNC), we evaluate
the accuracy of performing WSD on the sample of 40 words purely with the first sense
heuristic using all nine combinations of training and test corpora. The results (as given
in Table 16) are compared with a random baseline (?Random BL?)28 and the accuracy
using the first sense heuristic from SemCor (?SemCor FS?).29
The results in Table 16 show that the best results are obtained when the predominant
senses are acquired using the appropriate domain (i.e., test and training data from the
same domain). Moreover, when trained on the domain-relevant corpora, the random
baseline as well as the baseline provided by SemCor are comfortably beaten. It can be
observed from these results that apparently the BNC is more similar to the FINANCE
corpus than it is to the SPORTS corpus. The results for the SPORTS domain lag behind the
results for the FINANCE domain by almost 6 percentage points. This could be because
28 The random baseline is
?
i?tokens
1
#senses(i) .
29 The precision is given alongside in brackets because a predominant sense for the word striker is not
supplied by SemCor. The automatic method proposes a predominant sense in every case.
584
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Table 17
WSD using predominant senses, training, and testing on all domain combinations (automatically
classified corpora).
Testing
Training BNC FINANCE SPORTS
BNC 40.7 43.3 33.2
FINANCE 38.2 44.0 29.0
SPORTS 27.0 23.4 45.0
Random BL 19.8 19.6 19.4
SemCor FS 32.0 (32.9) 33.9 (35.0) 16.3 (16.8)
of the smaller amount of training data available (32M words versus 9M words), but it
could also be an artifact of this particular selection of words.
6.4.2 Experiment Using Automatically Classified Data. Although the previous experiment
shows that it is possible to acquire domain-specific predominant senses successfully, the
usefulness of doing this will be far greater if there is no need to classify corpora with
respect to domain by hand. There is no such thing as a standard domain specification
because the definition of a domain depends on user and application. It would be
advantageous if we could automatically obtain a user-/application-specific corpus from
which to acquire predominant senses.
In this section we describe an experiment where we build a text classifier using
WordNet as a sense inventory and the SFC domain extension (see Section 6.3). We
extracted bags of domain-specific words from WordNet for all the defined domains by
collecting all the word senses (synsets) and corresponding glosses associated with each
domain label. These bags of words are the fingerprints for the domains and we used
them to train a Support Vector Machine (SVM) text classifier using TwentyOne.30
The classifier distinguishes between 48 classes (the first and second levels of the
SFC hierarchy). When a document is evaluated by the classifier, it returns a list of
all the classes (domains) it recognizes and an associated confidence score reflecting the
certainty that the document belongs to that particular domain. We classified 10 months?
worth of data from the English Gigaword Corpus using this classifier and assigned each
document to the corpus belonging to the highest scoring class of the classifier?s output.
The level of confidence was ignored at this stage.
This resulted in a SPORTS corpus comprising about 11M words and a FINANCE
corpus of about 27M words. The predominant sense finding algorithm was run on the
raw text of these two corpora and we followed exactly the same evaluation strategy as
in the previous section. The results are summarized in Table 17 and are very similar
to those based on hand-labeled corpora. Again, the best results are obtained when test
and training data are derived from the same domain. The FINANCE?FINANCE result
is slightly worse, but is still well above both Random and the SemCor baseline. The
SPORTS?SPORTS result has slightly improved over the result reported in the previous
30 TwentyOne Classifier is an Irion Technologies product: www.irion.ml/products/english/
products classify.html.
585
Computational Linguistics Volume 33, Number 4
section. The reason for these differences may well be because the FINANCE corpus used
for this experiment is smaller and the SPORTS corpus is slightly larger than those used in
the hand-labeled experiment.
Automatically classifying documents inherently introduces noise in the training
corpora. This experiment to test the robustness of our method for finding predominant
senses suggests that it deals well with the noise. Further experiments that take the
confidence levels of the classifier into account will allow us to create corpora with less
noise and will allow us to find the right balance between corpus size and corpus quality.
7. Conclusions
In this article we have argued that information on the predominant sense of words is
important, and that it is desirable to be able to infer this automatically from unlabeled
text. We presented a number of evaluations investigating various facets of a previously
proposed method for automatically acquiring this information (McCarthy et al 2004a).
The evaluations extend ones in previous publications in a number of ways: they use
larger, balanced test data sets, and they compare alternative semantic similarity scores
and distributional thesauruses derived from different corpora and based on different
kinds of relations. We also looked in detail at areas where the method performs well
and also where it does not, and carried out a manual error analysis to identify the types
of mistakes it makes.
Our main results are:
 The predominant sense acquisition method produces promising results
overall for all open class parts of speech, when evaluated on SemCor, a
large balanced corpus.
 The highest accuracies are for nouns and adjectives; overall accuracy for
verbs is lower, but they have the lowest random baseline; adverbs have the
lowest average polysemy but gains over the random baseline are lower
than for other PoS.
 Using a thesaurus computed from proximity-based relations produces
almost as good results as using an otherwise identical one computed from
syntactic dependency-based relations.
 Lesk?s semantic similarity score (Banerjee and Pedersen 2002, lesk)
produces particularly good results for nouns which have low corpus
frequencies; Jiang and Conrath?s (1997, jcn) score does well on higher
frequency words.31
 For low frequency nouns in SemCor, the method, using any combination
of automatically acquired thesaurus and semantic similarity score that we
tried, produces more accurate predominant sense information than
SemCor. In particular, for nouns with a frequency of five or less (12.9% of
the polysemous nouns in the Senseval-2 data) it outperforms the SemCor
first sense heuristic. As the threshold is increased, the SemCor first sense
31 The lesk score has wider applicability than jcn since it can be applied to all parts of speech. It can also be
used with any sense inventory which has textual definitions for its senses even if the inventory does not
contain WordNet-like semantic relations.
586
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
heuristic becomes more competitive, but some of the automatic methods
are still outperforming it for nouns occurring 20 or fewer times in SemCor.
 Nouns show a stronger tendency for domain-specific meanings than other
parts of speech, but predominant senses for verbs acquired automatically
with respect to domain-specific corpora also correlate with the appropriate
domain labeling for those senses.
 Predominant senses acquired using domain-specific corpora outperform
those from SemCor in a WSD task, for a selection of nouns, using corpora
consisting of either hand-classified or automatically-classified documents.
8. Further Work
We are continuing to work on automatic ranking of word senses for WSD. Our next step
will be to use the numeric values of sense prevalence scores to compare the skews in
the distributions of word senses across different corpora and see if this enables us to
detect automatically words for which a domain- or genre-specific ranking is warranted.
Looking at skews should also help in predicting words for which contextual WSD is
likely to be particularly powerful, for example when more than one sense is scored
as being highly prevalent. In such situations we will combine our method with an
approach to unsupervised context-based WSD which uses the collocates of the distri-
butional neighbors associated with each of the senses as contextual features.
Our error analysis shows that many errors in identifying predominant senses are
caused by the sense distinctions in WordNet being particularly fine-grained. We have
recently (Koeling and McCarthy 2007) evaluated our method on the coarse-grained
English all words task at SemEval (Navigli, Litkowski, and Hargraves 2007). We will fol-
low work on finding relationships between WordNet senses to induce coarser-grained
classes (McCarthy 2006), and on automatic induction of senses (Pantel and Lin 2002)
and adapt our method to acquire prevalence rankings for these. The granularity of the
inventory will depend on the application and we plan to apply rankings over such
inventories for WSD within the context of a task, such as lexical substitution (McCarthy
and Navigli 2007).
To date we have only applied our methods to English. We plan to apply our
approach to other languages for which sense tagged resources of the size of SemCor are
not available. Given the good results with Lin?s proximity based thesaurus we believe
our method should work even for languages which do not have high quality parsers
available.
Acknowledgments
This work was supported by the UK EPSRC
project EP/C537262 ?Ranking Word Senses
for Disambiguation: Models and
Applications,? and a UK Royal Society
Dorothy Hodgkin Fellowship to the first
author. We are grateful to Dekang Lin for
making his thesaurus data publicly available
and to Siddharth Patwardhan and Ted
Pedersen for the WordNet Similarity
Package. We thank the anonymous reviewers
for the many helpful comments and
suggestions they made.
References
Agirre, Eneko and Oier Lopez de Lacalle.
2003. Clustering WordNet word senses. In
Recent Advances in Natural Language
Processing, pages 121?130, Borovets,
Bulgaria.
Banerjee, Satanjeev and Ted Pedersen. 2002.
An adapted Lesk algorithm for word sense
disambiguation using WordNet. In
Proceedings of the Third International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-02),
pages 136?145, Mexico City.
587
Computational Linguistics Volume 33, Number 4
Briscoe, Edward and John Carroll. 2002.
Robust accurate statistical annotation of
general text. In Proceedings of the Third
International Conference on Language
Resources and Evaluation (LREC),
pages 1499?1504, Las Palmas, Canary
Islands, Spain.
Buitelaar, Paul and Bogdan Sacaleanu.
2001. Ranking and selecting synsets by
domain relevance. In Proceedings of
WordNet and Other Lexical Resources:
Applications, Extensions and Customizations,
NAACL 2001 Workshop, pages 119?124,
Pittsburgh, PA.
Chan, Yee Seng and Hwee Tou Ng. 2005.
Word sense disambiguation with
distribution estimation. In Proceedings of
the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005),
pages 1010?1015, Edinburgh, UK.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics
(ACL-89), pages 76?82, Vancouver, British
Columbia, Canada.
Ciaramita, Massimiliano and Mark Johnson.
2003. Supersense tagging of unknown
nouns in WordNet. In Proceedings of the
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2003),
pages 168?175, Sapporo, Japan.
Cotton, Scott, Phil Edmonds, Adam
Kilgarriff, and Martha Palmer. 2001.
Senseval-2. http://www.sle.sharp.
co.uk/senseval2.
Curran, James. 2005. Supersense tagging
of unknown nouns using semantic
similarity. In Proceedings of the 43rd
Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages
26?33, Ann Arbor, MI.
Daude?, Jordi, Lluis Padro?, and German
Rigau. 2000. Mapping WordNets using
structural information. In Proceedings of the
38th Annual Meeting of the Association for
Computational Linguistics, pages 504?511,
Hong Kong.
Fellbaum, Christiane, editor. 1998. WordNet,
An Electronic Lexical Database. The MIT
Press, Cambridge, MA.
Francis, W. Nelson and Henry Kuc?era, 1979.
Manual of Information to Accompany a
Standard Corpus of Present-Day Edited
American English, for Use with Digital
Computers. Department of Linguistics,
Brown University, Providence, RI. Revised
and amplified ed.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. One sense per discourse.
In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, pages 233?237,
Harriman, NY.
Gliozzo, Alfio, Claudio Giuliano, and
Carlo Strapparava. 2005. Domain kernels
for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics,
pages 403?410, Ann Arbor, MI.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium, Philadelphia,
PA.
Harris, Zellig S. 1968. Mathematical Structures
of Languages. Wiley, New York, NY.
Hornby, A. S. 1989. Oxford Advanced Learner?s
Dictionary of Current English. Oxford
University Press, Oxford, UK.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Phil Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, The Netherlands,
pages 47?73.
Jiang, Jay and David Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In 10th
International Conference on Research in
Computational Linguistics, pages 19?33,
Taiwan.
Kilgarriff, Adam. 1998. Gold standard
datasets for evaluating word sense
disambiguation programs. Computer
Speech and Language, 12(3):453?472.
Kilgarriff, Adam and Martha Palmer, editors.
2000. Senseval: Special Issue of the Journal
Computers and the Humanities, volume
34(1?2). Kluwer, Dordrecht, The
Netherlands.
Koeling, Rob and Diana McCarthy. 2007.
Sussx: WSD using automatically acquired
predominant senses. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 314?317,
Prague, Czech Republic.
Koeling, Rob, Diana McCarthy, and John
Carroll. 2005. Domain-specific sense
distributions and predominant sense
acquisition. In Proceedings of the Human
Language Technology Conference and
EMNLP, pages 419?426, Vancouver, British
Columbia, Canada.
Krovetz, Robert. 1998. More than one sense
per discourse. In Proceedings of the
ACL-SIGLEX Senseval Workshop.
http://www.itri.bton.ac.uk/events/
senseval/ARCHIVE/PROCEEDINGS/.
Landes, Shari, Claudia Leacock, and
Randee I. Tengi, editors. 1998. Building
588
McCarthy, Koeling, Weeds, and Carroll Acquisition of Predominant Word Senses
Semantic Concordances. The MIT Press,
Cambridge, MA.
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?75.
Leech, Geoffrey. 1992. 100 million words of
English: The British National Corpus.
Language Research, 28(1):1?13.
Lesk, Michael. 1986. Automatic sense
disambiguation using machine readable
dictionaries: How to tell a pine cone from
an ice cream cone. In Proceedings of the
ACM SIGDOC Conference, pages 24?26,
Toronto, Canada.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago and
London.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association
for Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval and
clustering of similar words. In Proceedings
of COLING-ACL?98, pages 768?774,
Montreal, Canada.
Lin, Dekang. 1998b. Dependency-based
evaluation of MINIPAR. In Proceedings of
the Workshop on the Evaluation of Parsing
Systems, pages 48?56, Granada, Spain.
http://www.cs.ualberta.ca/~lindek/
minipar.htm.
Magnini, Bernardo and Gabriela Cavaglia`.
2000. Integrating subject field codes into
WordNet. In Proceedings of LREC-2000,
pages 1413?1418, Athens, Greece.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzuli, and Alfio Gliozzo. 2001.
Using domain information for word sense
disambiguation. In Proceedings of the
Senseval-2 Workshop, pages 111?114,
Toulouse, France.
Magnini, Bernardo, Carlo Strapparava,
Giovanni Pezzulo, and Alfio Gliozzo. 2002.
The role of domain information in word
sense disambiguation. Natural Language
Engineering, 8(4):359?373.
Martinez, David and Eneko Agirre. 2000.
One sense per collocation and genre/topic
variations. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 207?215, Hong Kong.
McCarthy, Diana. 2006. Relating WordNet
senses for word sense disambiguation. In
Proceedings of the EACL 06 Workshop:
Making Sense of Sense: Bringing
Psycholinguistics and Computational
Linguistics Together, pages 17?24, Trento,
Italy.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004a. Finding
predominant senses in untagged text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 280?287, Barcelona, Spain.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004b. Ranking WordNet
senses automatically. CSRP 569,
Department of Informatics, University of
Sussex, January.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004c. Using
automatically acquired predominant
senses for word sense disambiguation. In
Proceedings of the ACL Senseval-3 Workshop,
pages 151?154, Barcelona, Spain.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of
ACL/SIGLEX SemEval-2007, pages 48?53,
Prague, Czech Republic.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings Senseval-3 3rd
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Barcelona, Spain.
Miller, George A., Claudia Leacock, Randee
Tengi, and Ross T. Bunker. 1993. A
semantic concordance. In Proceedings of the
ARPA Workshop on Human Language
Technology, pages 303?308, San Francisco,
CA.
Mohammad, Saif and Graeme Hirst.
2006. Determining word sense dominance
using a thesaurus. In Proceedings of
the 11th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL-2006), pages 121?128,
Trento, Italy.
Navigli, Roberto, Ken Litkowski, and
Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of ACL/SIGLEX
SemEval-2007, pages 30?35, Prague,
Czech Republic.
589
Computational Linguistics Volume 33, Number 4
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(02):137?163.
Pantel, Patrick and Dekang Lin.
2002. Discovering word senses from
text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and
Data Mining, pages 613?619, Edmonton,
Alberta, Canada.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing 2003), pages 241?257, Mexico
City, Mexico.
Patwardhan, Siddharth and Ted Pedersen.
2003. The CPAN WordNet::Similarity
Package. http://search.cpan.org/~sid/
WordNet-Similarity-0.05/.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems. ACL,
Toulouse, France.
Procter, Paul, editor. 1978. Longman
Dictionary of Contemporary English.
Longman Group Ltd., Harlow, UK.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity
in a taxonomy. In 14th International
Joint Conference on Artificial Intelligence,
pages 448?453, Montreal, Canada.
Rose, Tony G., Mary Stevenson, and Miles
Whitehead. 2002. The Reuters Corpus
volume 1?From yesterday?s news to
tomorrow?s language resources. In
Proceedings of the 3rd International
Conference on Language Resources and
Evaluation, pages 827?833, Las Palmas,
Canary Islands, Spain.
Siegel, Sidney and N. John Castellan.
1988. Non-Parametric Statistics for the
Behavioral Sciences. McGraw-Hill,
New York, NY.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task.
In Proceedings of the ACL Senseval-3
Workshop, pages 41?43, Barcelona,
Spain.
Stevenson, Mark and Yorick Wilks. 2001.
The interaction of knowledge sources for
word sense disambiguation. Computational
Linguistics, 27(3):321?350.
Weeds, Julie. 2003. Measures and
Applications of Lexical Distributional
Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex,
Brighton, UK.
Weeds, Julie, James Dowdall, Gerold
Schneider, Bill Keller, and David Weir.
2005. Using distributional similarity to
organise biomedical terminology.
Terminology, 11(1):107?141.
Weeds, Julie and David Weir. 2005.
Co-occurrence Retrieval: A flexible
framework for lexical distributional
similarity. Computational Linguistics,
31(4):439?476.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation
performance across diverse parameter
spaces. Natural Language Engineering,
8(4):293?310.
590
Measuring Word Meaning in Context
Katrin Erk?
University of Texas at Austin
Diana McCarthy??
University of Cambridge
Nicholas Gaylord?
University of Texas at Austin
Word sense disambiguation (WSD) is an old and important task in computational linguistics
that still remains challenging, to machines as well as to human annotators. Recently there have
been several proposals for representing word meaning in context that diverge from the traditional
use of a single best sense for each occurrence. They represent word meaning in context through
multiple paraphrases, as points in vector space, or as distributions over latent senses. New
methods of evaluating and comparing these different representations are needed.
In this paper we propose two novel annotation schemes that characterize word meaning in
context in a graded fashion. In WSsim annotation, the applicability of each dictionary sense
is rated on an ordinal scale. Usim annotation directly rates the similarity of pairs of usages of
the same lemma, again on a scale. We find that the novel annotation schemes show good inter-
annotator agreement, as well as a strong correlation with traditional single-sense annotation and
with annotation of multiple lexical paraphrases. Annotators make use of the whole ordinal scale,
and give very fine-grained judgments that ?mix and match? senses for each individual usage.
We also find that the Usim ratings obey the triangle inequality, justifying models that treat usage
similarity as metric.
There has recently been much work on grouping senses into coarse-grained groups. We
demonstrate that graded WSsim and Usim ratings can be used to analyze existing coarse-grained
sense groupings to identify sense groups that may not match intuitions of untrained native
speakers. In the course of the comparison, we also show that the WSsim ratings are not subsumed
by any static sense grouping.
? Linguistics Department. CLA Liberal Arts Building, 305 E. 23rd St. B5100, Austin, TX, USA 78712.
E-mail: katrin.erk@mail.utexas.edu, nlgaylord@utexas.edu.
?? Visiting Scholar, Department of Theoretical and Applied Linguistics, University of Cambridge,
Sidgwick Avenue, Cambridge, CB3 9DA, UK. E-mail: diana@dianamccarthy.co.uk.
Submission received: 3 November 2011; revised version received: 30 April 2012; accepted for publication:
25 June 2012.
doi:10.1162/COLI a 000142
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
1. Introduction
Word sense disambiguation (WSD) is a task that has attracted much work in computa-
tional linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview),
including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss and
Yarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, Ma`rquez, and
Wicentowski 2007; Erk and Strapparava 2010), which were originally organized
expressly as a forum for shared tasks in WSD. In WSD, polysemy is typically modeled
through a dictionary, where the senses of a word are understood to be mutually disjoint.
The meaning of an occurrence of a word is then characterized through the best-fitting
among its dictionary senses.
The assumption of senses that are mutually disjoint and that have clear bound-
aries has been drawn into doubt by lexicographers (Kilgarriff 1997; Hanks 2000), lin-
guists (Tuggy 1993; Cruse 1995), and psychologists (Kintsch 2007). Hanks (2000) argues
that word senses have uses where they clearly fit, and borderline uses where only a
few of a sense?s identifying features apply. This notion matches results in psychol-
ogy on human concept representation: Mental categories show ?fuzzy boundaries,?
and category members differ in typicality and degree of membership (Rosch 1975;
Rosch and Mervis 1975; Hampton 2007). This raises the question of annotation: Is it
possible to collect word meaning annotation that captures degrees to which a sense
applies?
Recently, there have been several proposals for modeling word meaning in context
that can represent different degrees of similarity to a word sense, as well as different
degrees of similarity between occurrences of a word. The SemEval Lexical Substitu-
tion task (McCarthy and Navigli 2009) represents each occurrence through multiple
weighted paraphrases. Other approaches represent meaning in context through a vector
space model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, and
Pinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010). Again,
this raises the question of annotation: Can human annotators give fine-grained judg-
ments about degrees of similarity between word occurrences, like these computational
models predict?
The question that we explore in this paper is: Can word meaning be described
through annotation in the form of graded judgments? We want to know whether an-
notators can provide graded meaning annotation in a consistent fashion. Also, we
want to know whether annotators will use the whole graded scale, or whether
they will fall back on binary ratings of either ?identical? or ?different.? Our ques-
tion, however, is not whether annotators can be trained to do this. Rather, our
aim is to describe word meaning as language users perceive it. We want to tap into
the annotators? intuitive notions of word meaning. As a consequence, we use un-
trained annotators. We view it as an important aim on its own to capture lan-
guage users? intuitions on word meaning, but it is also instrumental in answering
our first question, of whether word meaning can be described through graded
annotator judgments: Training annotators in depth on how to distinguish pre-
defined hand-crafted senses could influence them to assign those senses in a binary
fashion.
We introduce two novel annotation tasks in which human annotators characterize
word meaning in context. In the first task, they rate the applicability of dictionary
senses on a graded scale. In the second task, they rate the similarity between pairs of
usages of the same word, also on a graded scale. In designing the annotation tasks, we
utilize techniques from psycholinguistic experimentation: Annotators give ratings on a
512
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
scale, rather than selecting a single label; we also use multiple annotators for each item,
retaining all annotator judgments.1
The result of this graded annotation can then be used to evaluate computational
models of word meaning: either to evaluate graded models of word meaning, or to
evaluate traditional WSD systems in a graded fashion. They can also be used to ana-
lyze existing word sense inventories, in particular to identify sense distinctions worth
revisiting?we say more on this latter use subsequently.
Our aim is not to improve inter-annotator agreement over traditional sense an-
notation. It is highly unlikely that ratings on a scale would ever achieve higher exact
agreement than binary annotation. Our aim is also not to maximize exact agreement, as
we expect to see individual differences in perceived meaning, and want to capture those
differences. Still it is desirable to have an end product of the annotation that is robust
against such individual differences. In order to achieve this, we average judgments over
multiple annotators after first inspecting pairwise correlations between annotators to
ensure that they are all doing their work diligently and with similar outcomes.
Analyzing the annotation results, we find that the annotators make use of inter-
mediate points on the graded scale and do not treat the task as inherently binary. We
find that there is good inter-annotator agreement, measured as correlation. There is
also a highly significant correlation across tasks and with traditional WSD and lexical
substitution tasks. This indicates that the annotators performed these tasks in a con-
sistent fashion. It also indicates that diverse ways of representing word meaning in
context?single best sense, weighted senses, multiple paraphrases, usage similarity?
yield similar characterizations. We find that annotators frequently give high scores to
more than one sense, in a way that is not remedied by a more coarse-grained sense
inventory. In fact, the annotations are often inconsistent with disjoint sense partitions.
The work reported here is based on our earlier work reported in Erk, McCarthy, and
Gaylord (2009). The current paper extends the previous work in three ways.
1. We add extensive new annotation to corroborate our findings from
the previous, smaller study. In this new, second round of annotation,
annotators do the two graded ratings tasks as well as traditional
single-sense annotation and annotation with paraphrases (lexical
substitutes), all on the same data. Each item is rated by eight annotators
in parallel. This setting, with four different types of word meaning
annotation on the same data, allows us to compare annotation results
across tasks more directly than before.2
2. We test whether the similarity ratings on pairs of usages obey the triangle
inequality, and find that they do. This point is interesting for psychological
reasons. Tversky and Gati (Tversky 1977; Tversky and Gati 1982) found
that similarity ratings on words did not obey the triangle inequality?
although, unlike our study, they were dealing with words out of context.
The fact that usage similarity ratings obey the triangle inequality is also
important for modeling and annotation purposes.
1 We do not use as many raters per item as is usual in psycholinguistics, however, as our aim is to cover a
sizeable amount of corpus data.
2 The annotation data from this second round are available at http://www.dianamccarthy.co.uk/
downloads/WordMeaningAnno2012/.
513
Computational Linguistics Volume 39, Number 3
3. We examine the extent to which our graded annotation accords with two
existing coarse-grained sense groupings, and we demonstrate that our
graded annotations can be used to double-check on sense groupings and
find potentially problematic groupings.
2. Background
In this section, we offer an overview of previous word sense annotation efforts, and then
discuss alternative approaches to the annotation and modeling of word meaning.
2.1 Word Sense Annotation
Inter-annotator agreement (also called inter-tagger agreement, or ITA) is one indicator
of the difficulty of the task of manually assigning word senses (Krishnamurthy and
Nicholls 2000). With WordNet, the sense inventory currently most widely used in
word sense annotation, ITA ranges from 67% to 78% (Landes, Leacock, and Tengi 1998;
Mihalcea, Chklovski, and Kilgarriff 2004; Snyder and Palmer 2004), depending on
factors such as degree of polysemy and inter-relatedness of the senses. This issue is
not specific to WordNet. Annotation efforts based on other dictionaries have achieved
similar ITA levels, as shown in Table 1. The first group in that table shows two corpora
in which all open-class words are annotated for word sense, in both cases using
WordNet. The second group consists of two English lexical sample corpora, in which
only some target words are annotated. One of them uses WordSmyth senses for verbs
and WordNet for all other parts of speech, and the other uses HECTOR, with similar
ITA, so the choice of dictionary does not seem to make much difference in this case.3
Next is SALSA, a German corpus using FrameNet frames as senses, then OntoNotes,
again an English lexical sample corpus. Inter-annotator agreement is listed in the last
column of the table; agreement is in general relatively low for the first four corpora,
which use fine-grained sense distinctions, and higher for SALSA and OntoNotes, which
have more coarse-grained senses.
Sense granularity has a clear impact upon levels of inter-annotator agreement
(Palmer, Dang, and Fellbaum 2007). ITA is substantially improved by using coarser-
grained senses, as seen in OntoNotes (Hovy et al 2006), which uses an ITA of 90% as the
criterion for constructing coarse-grained sense distinctions. Although this strategy does
improve ITA, it does not eliminate the issues seen with more fine-grained annotation
efforts: For some lemmas, such as leave, 90% ITA is not reached even after multiple
re-partitionings of the semantic space (Chen and Palmer 2009). This suggests that the
meanings of at least some words may not be separable into senses distinct enough
for consistent annotation.4 Moreover, sense granularity does not appear to be the only
question influencing ITA differences between lemmas. Passonneau et al (2010) found
three main factors: sense concreteness, specificity of the context in which the target word
occurs, and similarity between senses. It is worth noting that of these factors, only the
third can be directly addressed by a change in the dictionary.
3 HECTOR senses are described in richer detail than WordNet senses and the resource is strongly
corpus-based. We use WordNet in our work due to its high popularity and free availability.
4 Examples such as this indicate that there is at times a problem with clearly defining consistently
separable senses of a word. There is no clear measure of exactly how frequent such cases are, however.
This is due in part to the fact that this question depends so heavily on the data being considered and the
distinctions being posited.
514
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 1
Word sense-annotated data, with inter-annotator agreement (ITA).
Corpus Dictionary Corpus reference ITA
SemCor WordNet Landes, Leacock, and Tengi
(1998)
78.6%
SensEval-3 WordNet Snyder and Palmer (2004) 72.5%
SensEval-1 lex. sample HECTOR Kilgarriff and Rosenzweig
(2000)
66.5%
SensEval-3 lex. sample WordNet, WordSmyth Mihalcea, Chklovski, and
Kilgarriff (2004)
67.3%
SALSA FrameNet Burchardt et al (2006) 86%
OntoNotes OntoNotes Hovy et al (2006) most > 90%
Table 2
Best word sense disambiguation performance in SensEval/SemEval English lexical sample
tasks.
Shared task Shared task overview Best precision Baseline
SensEval-1 Kilgarriff and Rosenzweig (2000) 77% 69%
SensEval-2 Senseval-2 (2001) 64% 51%
SensEval-3 Mihalcea, Chklovski, and Kilgarriff (2004) 73% 55%
SemEval-1 Pradhan et al (2007) 89% (not given)
ITA levels in word sense annotation tasks are mirrored in the performance of WSD
systems trained on the annotated data. Table 2 shows results for the best systems that
participated at four English lexical sample tasks. With fine-grained sense inventories,
the top-ranking WSD systems participating in the event achieved precision scores of 73%
to 77% (Edmonds and Cotton 2001; Mihalcea, Chklovski, and Kilgarriff 2004). Current
state-of-the-art systems have made modest improvements on this; for example, the
system described by Zhong and Ng (2010) achieves 65.3% on the English lexical sample
at SENSEVAL-2, though the same system obtains 72.6%, just below Mihalcea, Chklovski,
and Kilgarriff (2004), on the English lexical sample at SENSEVAL-3. Nevertheless, the pic-
ture remains the same with systems getting around three out of four word occurrences
correct. Under a coarse-grained approach, system performance improves considerably
(Palmer, Dang, and Fellbaum 2007; Pradhan et al 2007), with the best participating
system achieving a precision close to 90%.5 The merits of a coarser-grained approach
are still a matter of debate (Stokoe 2005; Ide and Wilks 2006; Navigli, Litkowski, and
Hargraves 2007; Brown 2010), however.
Although identifying the proper level of granularity for sense repositories has im-
portant implications for improving WSD, we do not focus on this question here. Rather,
we propose novel annotation tasks that allow us to probe the relatedness between
dictionary senses in a flexible fashion, and to explore word meaning in context without
presupposing hard boundaries between usages. The resulting data sets can be used
to compare different inventories, coarse or otherwise. In addition, we hope that they
will prove useful for the evaluation of alternative representations of ambiguity in word
5 Zhong, Ng, and Chan (2008) report similar results (89.1%) with their state-of-the-art system when
evaluating on the OntoNotes corpus, which is larger than the SENSEVAL data sets.
515
Computational Linguistics Volume 39, Number 3
meaning (Erk and Pado 2008; Mitchell and Lapata 2008; Reisinger and Mooney 2010;
Thater, Fu?rstenau, and Pinkal 2010; Reddy et al 2011; Van de Cruys, Poibeau, and
Korhonen 2011).
2.2 Representation of Word Meaning in Word Sense Inventories
One possible factor contributing to the difficulty of manual and automatic word sense
assignment is the design of word sense inventories themselves. As we have seen, such
difficulties are encountered across dictionaries, and it has been argued that there are
problems with the characterization of word meanings as sets of discrete and mutually
exclusive senses (Tuggy 1993; Cruse 1995; Kilgarriff 1997; Hanks 2000; Kintsch 2007).
2.2.1 Criticisms of Enumerative Approaches to Meaning. Dictionaries are practical resources
and the nature of the finished product depends upon the needs of the target audience, as
well as budgetary and related constraints (cf. Hanks 2000). Consequently, dictionaries
differ in the words that they cover, and also in the word meanings that they distinguish.
Dictionary senses are generalizations over the meanings that a word can take, and these
generalizations themselves are abstractions over collected occurrences of the word in
different contexts (Kilgarriff 1992, 1997, 2006). Regardless of a dictionary?s granularity,
the possibility exists for some amount of detail to be lost as a result of this process.
Kilgarriff (1997) calls into question the possibility of general, all-purpose senses of
a word and argues that sense distinction only makes sense with respect to a given task.
For example, in machine translation, the senses to be distinguished should be those
that lead to different translations in the target language. It has since been demonstrated
that this is in fact the case (Carpuat and Wu 2007a, 2007b). Hanks (2000) questions the
view of senses as disjoint classes defined by necessary and sufficient conditions. He
shows that even with a classic homonym like ?bank,? some occurrences are more typical
examples of a particular sense than others. This notion of typicality is also important in
theories of concept representation in psychology (Murphy 2002). Theoretical treatments
of word meaning such as the Generative Lexicon (Pustejovsky 1991) also draw attention
to the subtle, yet reliable, fluctuations of meaning-in-context, and work in this paradigm
also provides evidence that two senses which may appear to be quite distinct can in
fact be quite difficult to distinguish in certain contexts (Copestake and Briscoe 1995,
page 53).
2.2.2 Psychological Research on Lexical and Conceptual Knowledge. Not all members of a
mental category are equal. Some are perceived as more typical than others (Rosch 1975;
Rosch and Mervis 1975; and many others), and even category membership itself is
clearer in some cases than in others (Hampton 1979). These results are about mental
concepts, however, rather than word meanings per se, which raises the question of
the relation between word meanings and conceptual knowledge. Murphy (1991, 2002)
argues that although not every concept is associated with a word, word meanings show
many of the same phenomena as concepts in general?word meaning is ?made up of
pieces of conceptual structure? (Murphy 2002, page 391). A body of work in cognitive
linguistics also discusses the relation between word meaning and conceptual structure
(Coleman and Kay 1981; Taylor 2003).
Psycholinguistic studies on word meaning offer insight into the question of the
mental representation of word senses. Unlike homonym meanings, the senses of a
polysemous word are thought to be related, suggesting that the mental representations
of these senses may overlap as well. The psycholinguistic literature on this question
516
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
is not wholly clear-cut, but by and large does support the position that polysemous
senses are not entirely discrete in the mental lexicon. Whereas Klein and Murphy (2001,
2002) do provide evidence for discreteness of mental sense representations, it appears
as though these findings may be due in part to the particular senses included in their
studies (Klepousniotou, Titone, and Romero 2008).
Moreover, many psycholinguistic studies have indeed found evidence for process-
ing differences between homonyms and polysemous words, using a variety of experi-
mental designs, including eye movements and reading times (Frazier and Rayner 1990;
Pickering and Frisson 2001) as well as response times in sensicality and lexical decision
tasks (Williams 1992; Klepousniotou 2002). Brown (2008, 2010) takes the question of
shared vs. separate meaning representations one step further in a semantic priming
study6 in which she shows that intuitive meaning-in-context similarity judgments have
a processing correlate in on-line sentence comprehension. Response time to the target
is a negative linear function of its similarity in meaning to the prime, and response
accuracy is a positive linear function of this similarity. In other words, the more similar
in meaning a prime?target pair was judged to be, the faster and more accurately sub-
jects responded. This provides empirical support for a processing correlate of graded
similarity-in-meaning judgments.
In our work reported here, we take inspiration from work in psychology and look
at ways to model word meaning more continuously. Even though there is still some
controversy, the majority of studies support the view that senses of polysemous words
are linked in their mental representations. In our work we do not make an explicit
distinction between homonymy and polysemy, but the data sets we have produced may
be useful for a future exploration of this distinction.
2.3 Alternative Approaches to Word Meaning
Earlier we suggested that word meaning may be better described without positing
disjoint senses. We now describe some alternatives to word sense inventory approaches
to word meaning, most of which do not rely on disjoint senses.
2.3.1 Substitution-Based Approaches. McCarthy and Navigli (2007) explore the use of
synonym or near-synonym lexical substitutions to characterize the meaning of word
occurrences. In contrast to dictionary senses, substitutes are not taken to partition
a word?s meaning into distinct senses. McCarthy and Navigli gathered their lexical
substitution data using multiple annotators. Annotators were allowed to provide up to
three paraphrases for each item. Data were gathered for 10 sentences per lemma for 210
lemmas, spanning verbs, nouns, adjectives, and adverbs. The annotation took the form
of each occurrence being associated with a multiset of supplied paraphrases, weighted
by the frequency with which each paraphrase was supplied. We make extensive use of
the LEXSUB dataset in our work reported here. An example sentence with substitutes
from the LEXSUB dataset (sentence 451) is given in Table 3.
A related approach also characterizes meaning through equivalent terms, but terms
in another language. Resnik and Yarowsky (2000, page 10) suggest ?to restrict a word
sense inventory to distinctions that are typically lexicalized cross-linguistically? [emphasis
in original]. They argue that such an approach will avoid being too fine-grained, and
that the distinctions that are made will be independently motivated by crosslinguistic
6 See McNamara (2005) for more information on priming studies.
517
Computational Linguistics Volume 39, Number 3
Table 3
An example of annotation from the lexical substitution data set: sentence 451.
Sentence: My interest in Europe?s defence policy is nothing new.
Annotation: original 2; recent 2; novel 2; different 1; additional 1
trends. Although substitution and translation methods are not without their own issues
(Kilgarriff 1992, page 48), they constitute an approach to word meaning that avoids
many of the drawbacks of more traditional sense distinction and annotation. Some
cross-linguistic approaches group translations into disjoint senses (Lefever and Hoste
2010), whereas others do not (Mihalcea, Sinha, and McCarthy 2010).
2.3.2 Distributional Approaches. Recently there have been a growing number of distri-
butional approaches to representing word meaning in context. These models offer an
opportunity to model subtle distinctions in meaning between two occurrences of a word
in different contexts. In particular, they allow comparisons between two occurrences of
a word without having to classify them as having the same sense or different senses.
Some of these approaches compute a distributional representation for a word across all
its meanings, and then adapt this to a given sentence context (Landauer and Dumais
1997; Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, and Pinkal 2010;
Van de Cruys, Poibeau, and Korhonen 2011). Others group distributional contexts into
senses. This can be done on the fly for a given occurrence (Erk and Pado 2010; Reddy
et al 2011), or beforehand (Dinu and Lapata 2010; Reisinger and Mooney 2010). The
latter two approaches then represent an occurrence through weights over those senses.
A third group of approaches is based on language models (Deschacht and Moens 2009;
Washtell 2010; Moon and Erk 2012): They infer other words that could be used in the
position of the target word.7
3. Two Novel Annotation Tasks
In this section we introduce two novel annotation schemes that draw on methods
common in psycholinguistic experiments, but uncommon in corpus annotation. Tra-
ditional word sense annotation usually assumes that there is a single correct label
for each markable. Annotators are trained to identify the correct labels consistently,
often with highly specific a priori guidelines. Multiple annotators are often used, but
despite the frequently low ITA in word sense annotation, differences between annotator
responses are often treated as the result of annotator error and are not retained in the
final annotation data.
In these respects, traditional word sense annotation tasks differ in design from
many psycholinguistic experiments, such as the ones discussed in the previous section.
Psycholinguistic experiments frequently do not make strong assumptions about how
participants will respond, and in fact are designed to gather data on that very ques-
tion. Participants are given general guidelines for completing the experiment but these
7 Distributional models for phrases have recently received much attention, even more so than models for
word meaning in context (Baroni and Zamparelli 2010; Coecke, Sadrzadeh, and Clark 2010; Mitchell and
Lapata 2010; Grefenstette and Sadrzadeh 2011; Socher et al 2011). They are less directly relevant to the
current paper, however, as we focus on eliciting judgments for individual words in sentence contexts,
rather than whole phrases.
518
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 4
Interpretation of the five-point scale given to the annotators. This interpretation is the same for
the Usim and WSsim tasks.
1 completely different
2 mostly different
3 similar
4 very similar
5 identical
guidelines generally stop short of precise procedural detail, to avoid undue influence
over participant responses. All of the psycholinguistic studies discussed earlier used
participants na??ve as to the purpose of the experiment, and who were minimally trained.
Responses are often graded in nature, involving ratings on an ordinal scale or in some
cases even a continuously valued dimension (e.g., as in Magnitude Estimation). Mul-
tiple participants respond to each stimulus, but all participant responses are typically
retained, as there are often meaningful discrepancies in participant responses that are
not ascribable to error. All of the psycholinguistic studies discussed previously collected
data from multiple participants (up to 80 in the case of one experiment by Williams
[1992]).
The annotation tasks we present subsequently draw upon these principles of exper-
imental design. We collected responses using a scale, rather than binary judgments; we
designed the annotation tasks to be accomplishable without prior training and with
minimal guidelines, and we used multiple annotators (up to eight) and retained all
responses in an effort to capture individual differences. In the following, we describe
two different annotation tasks, one with and one without the use of dictionary senses.
Graded Ratings for Dictionary Senses. In our first annotation task, dubbed WSsim (for
Word Sense Similarity), annotators rated the applicability of WordNet dictionary senses,
using a five-point ordinal scale.8 Annotators rated the applicability of every single
WordNet sense for the target lemma, where a rating of 1 indicated that the sense
in question did not apply at all, and a rating of 5 indicated that the sense applied
completely to that occurrence of the lemma. Table 4 shows the descriptions of the five
points on the scale that the annotators were given. By asking annotators to provide
ratings for each individual sense, we strive to eliminate all bias toward either single-
sense or multiple-sense annotation. By asking annotators to provide ratings on a scale,
we allow for the fact that senses may not be perceived in a binary fashion.
Graded Ratings for Usage Similarity. In our second annotation task, dubbed Usim (for
Usage Similarity), we collected annotations of word usages without recourse to dic-
tionary senses, by asking annotators to judge the similarity in meaning of one usage
of a lemma to other usages. Annotators were presented with pairs of contexts that
share a word in common, and were asked to rate how similar in meaning they perceive
those two occurrences to be. Ratings are again on a five-point ordinal scale; a rating of
1 indicated that the two occurrences of the target lemma were completely dissimilar in
meaning, and a rating of 5 indicated that the two occurrences of the target lemma were
identical in meaning. The descriptions of the five points on the scale, shown in Table 4,
8 The use of a five-point scale is a common choice when collecting ordinal ratings, as it allows more
detailed responses than the ?yes/no/maybe? provided by a three-point scale.
519
Computational Linguistics Volume 39, Number 3
were identical to those used in the WSsim task. Annotators were able to respond ?I don?t
know? if they were unable to gauge the similarity in meaning of the two occurrences.9
Annotation Procedure. All annotation for this project was conducted over the Internet
in specially designed interfaces. In both tasks, all annotator responses were retained,
without resolution of disagreement between annotators. We do not focus on obtaining
a single ?correct? annotation, but rather view all responses as valuable sources of
information, even when they diverge.
For each item presented, annotators additionally were provided a comment field
should they desire to include a more detailed response regarding the item in question.
They could use this, for example, to comment on problems understanding the sentence.
The annotators were able to revisit previous items in the task. Annotators were not able
to skip forward in the task without rating the current item. If an annotator attempted to
submit an incomplete annotation they were prompted to provide a complete response
before proceeding. They were free to log out and resume later at any point, however,
and also could access the instructions whenever they wanted.
Two Rounds of Annotation. We performed two rounds of the annotation experiments,
hereafter referred to as R1 and R2.10 Both annotation rounds included both a WSsim and
a Usim task, labeled in the subsequent discussion as WSsim-1 and Usim-1 for R1, and
WSsim-2 and Usim-2 for R2. An important part of the data analysis is to compare the
new, graded annotation to other types of annotation. We compare it to both traditional
word sense annotation, with a single best sense for each occurrence, and lexical
substitution, which characterizes each occurrence through paraphrases. In R1, we chose
annotation data that had previously been labeled with either traditional single sense
annotation or with lexical substitutions. R2 included two additional annotation tasks,
one involving traditional WSD methodology (WSbest) and a lexical substitution task
(SYNbest). In the SYNbest task, annotators provided a single best lexical substitution,
in contrast to the multiple substitutes annotators provided in the original LEXSUB data.11
Three annotators participated in each task in the R1, and eight annotators partici-
pated in R2. In R1, separate groups of annotators participated in WSsim and Usim an-
notation, whereas in R2 the same group of annotators was used for all annotation, so as
to allow comparison across tasks for the same annotator as well as across annotators. In
R2, therefore, the same annotators did both traditional word sense annotation (WSbest)
and the graded word sense annotation of the WSsim task. This raises the question of
whether their experience on one task will influence their annotation choice on the other
task. We tested this by varying the order in which annotators did WSsim and WSbest.
R2 annotators were divided into two groups of four annotators with the order of tasks
as follows:
group 1: Usim-2 SYNbest WSsim-2 WSbest
group 2: Usim-2 SYNbest WSbest WSsim-2
Another difference between the two rounds of annotation was that in R2 we per-
mitted the annotators to see one more sentence of context on either side of the target
9 The ?I don?t know? option was present only in the Usim interface, and was not available in WSsim.
10 The annotation was conducted in two separate rounds due to funding.
11 Annotation guidelines for R1 are at http://www.katrinerk.com/graded-sense-and-usage-annotation
and guidelines for R2 tasks are at http://www.dianamccarthy.co.uk/downloads/
WordMeaningAnno2012/.
520
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 5
Abbreviations used in the text for annotation tasks and rounds.
WSsim Task: graded annotation of WordNet senses on a five-point scale
Usim Task: graded annotation of usage similarity on a five-point scale
WSbest Task: traditional single-sense annotation
SYNbest Task: lexical substitution
R1 Annotation round 1
R2 Annotation round 2
sentence. In R1 each item was given only one sentence as context. We added more
context in order to reduce the chance that the sentence would be unclear. Table 5
summarizes up the annotation tasks and annotation rounds on which we report.
Data Annotated. The data to be annotated in WSsim-1 were taken primarily from
Semcor (Miller et al 1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea,
Chklovski, and Kilgarriff 2004). This experiment contained a total of 430 sentences span-
ning 11 lemmas (nouns, verbs, and adjectives). For eight of these lemmas, 50 sentences
were included, 25 randomly sampled from Semcor and 25 randomly sampled from SE-3.
The remaining three lemmas in the experiment had 10 sentences each, from the LEXSUB
data. Each of the three annotators annotated each of the 430 items, providing a response
for each WordNet sense for that lemma. Usim-1 used data from LEXSUB. Thirty-four
lemmas were manually selected, including the three lemmas also used in WSsim-1. We
selected lemmas which exhibited a range of meanings and substitutes in the LEXSUB
data, with as few multiword substitutes as possible. Each lemma is the target in 10
LEXSUB sentences except there were only nine sentences for the lemma bar.n because
of a part-of-speech tagging error in the LEXSUB trial data. For each lemma, annotators
were presented with every pairwise comparison of these 10 sentences. We refer to each
such pair as an SPAIR. There were 45 SPAIRs per lemma (36 for bar.n), adding up to 1,521
comparisons per annotator in Usim-1.
In R1, only 30 sentences were included in both WSsim and Usim. Because compar-
ison of annotator responses on this subset of the two tasks yielded promising results,
R2 used the same set of sentences for both Usim and WSsim so as to better compare
these tasks. All data in the second round were taken from LEXSUB, and contained 26
lemmas with 10 sentences for each. We produced the SYNbest annotation, rather than
use the existing LEXSUB annotation, so that we could ensure the same conditions as
with the other annotation tasks, that is, using the same annotators and providing the
extra sentence of context on either side of the original LEXSUB context. We also only
required that the annotators provide one substitute. As such, there were 260 target
lemma occurrences that received graded word sense applicability ratings in WSsim-2,
and 1,170 SPAIRs (pairs of occurrences) to be annotated in Usim-2.
4. Analysis of the Annotation
In this section we present our analysis of the annotated data. We test inter-annotator
agreement, and we test to what extent annotators make use of the added flexibility
of the graded annotation. We also compare the outcome of our graded annotation to
traditional word sense annotation and lexical substitutions for the same data.
521
Computational Linguistics Volume 39, Number 3
4.1 Evaluation Measures
Because both graded annotation tasks, WSsim and Usim, use ratings on five-point
scales rather than binary ratings, we measure agreement in terms of correlation. Because
ratings were not normally distributed, we choose a non-parametric test which uses
ranks rather than absolute values: We use Spearmans rank correlation coefficient (rho),
following Mitchell and Lapata (2008). For assessing inter-tagger agreement on the R2
WSbest task we adopt the standard WSD measure of average pairwise agreement, and
for R2 SYNbest, we use the same pairwise agreement calculation used in LEXSUB.
When comparing graded ratings with single-sense or lexical substitution annota-
tion, we use the mean of all annotator ratings in the WSsim or Usim annotation. This
is justified because the inter-annotator agreement is highly significant, with respectable
rho compared with previous work (Mitchell and Lapata 2008).
As the annotation schemes differ between R1 and R2 (as mentioned previously, the
number of annotators and the amount of visible context are different, and R2 annotators
did traditional word sense annotation in the WSbest task in addition to the graded
tasks) we report the results of R1 and R2 separately.12
4.2 WSsim: Graded Ratings for WordNet Senses
In the WSsim task, annotators rated the applicability of each sense of the target word on
a five-point scale. We first do a qualitative analysis, then turn to a quantitative analysis
of annotation results.
4.2.1 Qualitative Analysis. Table 6 shows an example of WSsim annotation. The target
is the verb dismiss, which was annotated in R2. The first column gives the WordNet
sense number (sn).13 Note that in the task, the annotators were given the synonyms
and full description but in this figure we only supply part of the description for the
sake of space. As can be seen, three of the annotators chose a single-sense annotation
by giving a rating of 5 to one sense and ratings of 1 to all others. Two annotators gave
ratings of 1 and 2 to all but one sense. The other three annotators gave positive ratings
(ratings of at least 3 [similar], see Table 4) to at least two of the senses. All annotators
agree that the first sense fits the usage perfectly, and all annotators agree that senses
3 and 5 do not apply. The second sense, on the other hand, has an interestingly wide
distribution of judgments, ranging from 1 to 4. This is the judicial sense of the verb, as
in ?this case is dismissed.? Some annotators consider this sense to be completely distinct
from sense 1, whereas others see a connection. There is disagreement among annotators,
about sense 6. This is the sense ?dismiss, dissolve,? as in ?the president dissolved the
parliament.? Six of the annotators consider this sense completely unrelated to ?dismiss
our actions as irrelevant,? whereas two annotators view it as highly related (though
not completely identical). It is noteworthy that each of the two opinions, a rating of 1
12 It is known that when responses are collected on an ordinal scale, the possibility exists for different
individuals to use the scale differently. As such, it is common practice to standardize responses using a
z-score, which maps a response X to z = X??? . The calculation of z-scores makes reference to the mean
and the standard deviation of an annotator?s responses. Because responses were not normally distributed
in our task, a transformation that relies on measures of central tendency is not appropriate. So we do not
use z-scores in this paper. We repeated all analyses with z-score transform anyway, and found the results
to be basically the same as those we report here with the raw values. Overall, using z-scores slightly
strengthened most findings, but there were no differences in statistical significance anywhere.
13 We use WordNet 3.0 for our annotation.
522
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 6
WSsim example, R2: Annotator judgments for the different senses of dismiss.
If we see ourselves as separate from the world, it is easy to dismiss our actions as irrelevant
or unlikely to make any difference. (902)
sn Description Ratings By Annotator Mean
1 bar from attention or consideration 5 5 5 5 5 5 5 5 5
2 cease to consider 1 4 1 3 2 2 1 3 2.125
3 stop associating with 1 2 1 1 1 2 1 1 1.25
4 terminate the employment of 1 4 1 2 1 1 1 1 1.5
5 cause or permit a person to leave 1 2 1 1 1 1 1 2 1.25
6 declare void 1 1 1 4 1 1 1 4 1.75
and a rating of 4, was chosen by multiple annotators. Because multiple annotators give
each judgment, these data seem to reflect a genuine difference in perceived sense. We
discuss inter-annotator agreement, both overall and considering individual annotators,
subsequently.
Table 7 gives an example sentence from R1, where the annotated target is the noun
paper. All annotators agree that sense 5, ?scholarly article,? applies fully. Sense 2 (?essay?)
also gets ratings of ? 3 from all annotators. The first annotator seems also to have
perceived the ?physical object? connotation to apply strongly to this example, and has
expressed this quite consistently by giving high marks to sense 1 as well as 7.
Table 8 shows a sample annotated sentence with an adjective target, neat, annotated
in R2. In this case, only one annotator chose single-sense annotation by marking exclu-
sively sense 4. One annotator gave ratings ? 3 (similar) to all senses of the lemma. All
other annotators saw at least two senses as applying (with ratings ? 3) and at least one
sense as not applying at all (with a rating of 1). Sense 4 has received positive ratings (that
is, ratings ? 3) throughout. Senses 1, 2, and 6 have mixed ratings, and senses 3 and 5
have positive ratings only from the one annotator who marked everything as applying.
Interestingly, ratings for senses 1, 2, and 6 diverge sharply, with some annotators seeing
them as not applying at all, and some giving them ratings in the 3?5 range. Note that the
Table 7
WSsim example, R1: Annotator judgments for the different senses of paper.
This can be justified thermodynamically in this case, and this will be done in a separate
paper which is being prepared. (br-j03, sent. 4)
sn Description Ratings Mean
1 a material made of cellulose pulp 4 1 1 1.3
2 an essay (especially one written as an assignment) 3 3 5 3.7
3 a daily or weekly publication on folded sheets; contains
news and articles and advertisements
2 1 3 2
4 a medium for written communication 5 3 1 3
5 a scholarly article describing the results of observations
or stating hypotheses
5 5 5 5
6 a business firm that publishes newspapers 2 1 1 1.3
7 the physical object that is the product of a newspaper
publisher
4 1 1 1.7
523
Computational Linguistics Volume 39, Number 3
Table 8
WSsim example, R2: Annotator judgments for the different senses of neat.
Over the course of the 20th century scholars have learned that such images tried to make
messy reality neater than it really is (103)
sn Description Ratings By Annotator Mean
1 free from clumsiness; precisely or
deftly executed
1 5 1 4 5 5 5 5 3.375
2 refined and tasteful in appearance or
behavior or style
3 4 1 4 4 3 1 3 2.875
3 having desirable or positive qualities
especially those suitable for a
thing specified
1 3 1 1 1 1 1 1 1.25
4 marked by order and cleanliness in
appearance or habits
4 5 5 3 4 5 5 5 4.5
5 not diluted 1 4 1 1 1 1 1 1 1.375
6 showing care in execution 1 4 1 3 4 1 3 3 2.5
Table 9
Correlation matrix for pairwise correlation agreement for WSsim-1. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A B C
A 1.00 0.47 0.51
B 0.47 1.00 0.54
C 0.51 0.54 1.00
against avg 0.56 0.58 0.61
annotators who give ratings of 1 are not the same for these three ratings, pointing to dif-
ferent, but quite nuanced, judgments of the ?make reality neater? usage in this sentence.
4.2.2 Inter-annotator Agreement. We now turn to a quantitative analysis, starting with
inter-annotator agreement. For the graded WSsim annotation, it does not make sense
to compute the percentage of perfect agreement. As discussed earlier, we report inter-
annotator agreement in terms of correlation, using Spearman?s rho. We calculate pair-
wise agreements and report the average over all pairs. The pairwise correlations are
shown in the matrix in Table 9. We have used capital letters to represent the individ-
uals, preserving the same letter for the same person across tasks. In the last row we
show agreement of each annotator?s judgments against the average judgment from the
other annotators. The pairwise correlations range from 0.47 to 0.54 and all pairwise
correlations were highly significant (p  0.001), with an average of rho = 0.504. This
is a very reasonable result given that Mitchell and Lapata (2008) report a rho of 0.40
on a graded semantic similarity task.14 The lowest correlation against the average
14 Direct comparison across tasks is not appropriate, but we wish to point out that for graded semantic
judgments this level of correlation is perfectly reasonable. The Mitchell and Lapata (2008) data
set has been used in an evaluation exercise (GEMS-2011, https://sites.google.com/site/
geometricalmodels/shared-evaluation). Mitchell and Lapata point out that Spearman?s rho
tends to yield lower coefficients compared with parametric alternatives such as Pearson?s.
524
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 10
Correlation matrix for pairwise correlation agreement for WSsim-2. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A C D F G H I J
A 1.00 0.55 0.58 0.60 0.61 0.63 0.61 0.59
C 0.55 1.00 0.54 0.66 0.57 0.55 0.65 0.52
D 0.58 0.54 1.00 0.55 0.58 0.52 0.56 0.54
F 0.60 0.66 0.55 1.00 0.62 0.62 0.72 0.59
G 0.61 0.57 0.58 0.62 1.00 0.63 0.62 0.62
H 0.63 0.55 0.52 0.62 0.63 1.00 0.64 0.64
I 0.61 0.65 0.56 0.72 0.62 0.64 1.00 0.58
J 0.59 0.52 0.54 0.59 0.62 0.64 0.58 1.00
against avg 0.70 0.58 0.62 0.64 0.70 0.71 0.66 0.71
from the other annotators was 0.56. We discuss the annotations of individuals in Sec-
tion 4.6, including our decision to retain the judgments of all annotators for our gold
standard.
From the correlation matrix in Table 10 we see that for WSsim-2, pairwise corre-
lations ranged from 0.52 to 0.72. The average value of the pairwise correlations was
rho = 0.60, and again every pair was highly significant (p  0.001). The lowest correla-
tion against the average from all the other annotators was 0.58.
4.2.3 Choice of Single Sense Versus Multiple Senses. In traditional word sense annotation,
annotators can mark more than one sense as applicable, but annotation guidelines often
encourage them to view the choice of a single sense as the norm. In WSsim, annotators
gave ratings for all senses of the target. So we would expect that in WSsim, there would
be a higher proportion of senses selected as applicable. Indeed we find this to be the
case: Table 11 shows the proportion of sentences where some annotator has assigned
more than one sense with a judgment of 5, the highest value. Both WSsim-1 and WSsim-
2 have a much higher proportion of sentences with multiple senses chosen than the
traditional sense-annotated data sets SemCor and SE-3. Interestingly, we notice that
the percentage for WSsim-1 is considerably higher than for WSsim-2. In principle, this
could be due to differences in the lemmas that were annotated, or differences in the
sense perception of the annotators between R1 and R2. Another potential influencing
Table 11
WSsim annotation: Proportion of sentences where multiple senses received a rating of 5 (highest
judgment) from the same annotator.
Proportion
WSsim-1 46%
WSsim-2 30%
WSsim-2, WSsim first 36%
WSsim-2, WSbest first 23%
SemCor 0.3%
SE-3 8%
525
Computational Linguistics Volume 39, Number 3
factor is the order of annotation experiments: As described earlier, half of the R2 anno-
tators did WSbest annotation before doing WSsim-2, and half did the two experiments
in the opposite order. As Table 11 shows, those doing the graded task WSsim-2 before
the binary task WSbest had a greater proportion of multiple senses annotated with
the highest response. This demonstrates that annotators in a word meaning task can
be influenced by factors outside of the current annotation task, in this case another
annotation task that they have done previously. We take this as an argument in favor of
using as many annotators as possible in order to counteract factors that contribute noise.
In our case, we counter the influence of previous annotation tasks somewhat by using
multiple annotators and altering the order of the WSsim and WSbest tasks. Another
option would have been to use different annotators for different tasks; by using the
same set of annotators for all four tasks, however, we can better control for individual
variation.
4.2.4 Use of the Graded Scale. We next ask whether annotators in WSsim made use of the
whole five-point scale, or whether they mostly chose the extreme ratings of 1 and 5.
If the latter were the case, this could indicate that they viewed the task of word sense
assignment as binary. Figure 1a shows the relative frequency distribution of responses
from all annotators over the five scores for both R1 and R2. Figures 2a and 3a show the
same but for each individual annotator. In both rounds the annotators chose the rating
of 1 (?completely different,? see Table 4) most often. This is understandable because each
item is a sentence and sense combination and there will typically be several irrelevant
senses for a given sentence. The second most frequent choice was 5 (?identical?). Both
rounds had plenty of judgments somewhere between the two poles, so the annotators
do not seem to view the task of assigning word sense as completely binary. Although
the annotators vary, they all use the intermediate categories to some extent and certainly
the intermediate category judgments do not originate from a minority of annotators.
We notice that R2 annotators tended to give more judgments of 1 (?completely
different?) than the R1 annotators. One possible reason is again that half our annota-
tors did WSbest before WSsim-2. If this were the cause for the lower judgments, we
would expect more ratings of 1 for the annotators who did the traditional word sense
annotation (WSbest) first. In Table 12 we list the relative frequency of each rating for the
different groups of annotators. We certainly see an increase in the judgments of 1 where
Figure 1
WSsim and Usim R1 and R2 ratings.
526
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Figure 2
WSsim and Usim R1 individual ratings.
Figure 3
WSsim and Usim R2 individual ratings.
WSbest is performed before WSsim-2. Again, this may indicate that annotators were
leaning more towards finding a single exact match because they were influenced by
the WSbest task they had done before. Annotators in that group were also slightly less
inclined to take the middle ground, but this was true of both groups of R2 annotators
compared with the R1 annotators. We think that this difference between the two rounds
may well be due to the lemmas and data.
In Table 18, we show the average range15 and average variance of the judgments
per item for each of the graded annotation tasks. WSsim naturally has less variation
15 As an example, the first two senses (1 and 2) in Table 6 have ranges of 0 and 3, respectively.
527
Computational Linguistics Volume 39, Number 3
Table 12
The relative frequency of the annotations at each judgment from all annotators.
Judgment
Exp 1 2 3 4 5
WSsim-1 0.43 0.106 0.139 0.143 0.181
WSsim-2 0.696 0.081 0.067 0.048 0.109
WSsim-2, WSsim first 0.664 0.099 0.069 0.048 0.12
WSsim-2, WSbest first 0.727 0.063 0.065 0.048 0.097
Usim-1 0.360 0.202 0.165 0.150 0.123
Usim-2 0.316 0.150 0.126 0.112 0.296
compared with Usim because, for any sentence, there are inevitably many WordNet
senses which are irrelevant to the context at hand and which will obtain a judgment
of 1 from everyone. This is particularly the case for WSsim-2 where the annotators
gave more judgments of 1, as discussed previously. The majority of items have a range
of less than two for WSsim. We discuss the Usim figures further in the following
section.
4.3 Usim: Graded Ratings for Usage Similarity
In Usim annotation, annotators compared pairs of usages of a target word (SPAIRs) and
rated their similarity on the five-point scale given in Table 4. The annotators were also
permitted a response of ?don?t know.? Such responses were rare but were used when
the annotators really could not judge usage similarity, perhaps because the meaning
of one sentence was not clear. We removed any pairs where one of the annotators had
given a ?don?t know? verdict (9 in R1, 28 in R2). For R1 this meant that we were left with
a total of 1,512 SPAIRs and in R2 we had a resultant 1,142 SPAIRs.
4.3.1 Qualitative Analysis. We again start by inspecting examples of Usim annotation.
Table 13 shows the annotation for an SPAIR of the verb dismiss. The first of the two
sentences talks about ?dismissing actions as irrelevant,? the second is about dismissing
a person. Interestingly, the second usage could be argued to carry both a connotation
of ?ushering out? and a connotation of ?disregarding.? Annotator opinions on this SPAIR
vary from a 1 (completely different) to a 5 (identical), but most annotators seem to view
the two usages as related to an intermediate degree. This is adequately reflected in the
average rating of 3.125. Table 14 compares the sentence from Table 8 to another sentence
Table 13
Usim example: Annotator judgments for a pair of usages of dismiss.
Sentences Ratings
If we see ourselves as separate from the world, it is easy to dismiss
our actions as irrelevant or unlikely to make any difference.
1, 2, 3, 3,
3, 4, 4, 5
Simply thank your Gremlin for his or her opinion, dismiss him or
her, and ask your true inner voice to turn up its volume.
528
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 14
Usim example: Annotator judgments for a pair of usages of neat.
Sentences Ratings
Over the course of the 20th century scholars have learned that such
images tried to make messy reality neater than it really is.
3, 3, 4, 4,
4, 4, 5, 5
Strong field patterns created by hedgerows give the landscape a
neat, well structured appearance.
Table 15
Usim example: Annotator judgments for a pair of usages of account.
Sentences Ratings
Samba-3 permits use of multiple account data base backends. 1, 2, 3, 3,
3, 4, 4, 4
Within a week, Scotiabank said that it had frozen some accounts
linked to Washington?s hit list.
with the target neat. The first sentence is a metaphorical use (making reality neater),
the second is literal (landscape with neat appearance), but still the SPAIR gets high
ratings of 3?5 throughout for an average of 4.0. Note that the WordNet senses, shown
in Table 8, do not distinguish the literal and metaphorical uses of the adjective, either.
Table 15 shows two uses of the noun account. The first pertains to accounts on a software
system, the second to bank accounts. The spread of annotator ratings shows that these
two uses are not the same, but that some relation exists. The average rating for this
SPAIR is 3.0.
4.3.2 Inter-annotator Agreement. We again calculate inter-annotator agreement as the
average over pairwise Spearman?s correlations. The pairwise correlations are shown
in the matrix in Table 16. In the last row we show agreement of each annotator?s
judgments against the average judgment from the other annotators. For Usim-1 the
range of correlation coefficients is between 0.50 and 0.64 with an average correlation
of rho = 0.548. All the pairs are highly significantly correlated (p  0.001). The smallest
correlation for any individual against the average is 0.55. The correlation matrix for
Usim-2 is provided in Table 17; the range of correlation coefficients is between 0.42 and
Table 16
Correlation matrix for pairwise correlation agreement for Usim-1. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A D E
A 1.00 0.50 0.64
D 0.50 1.00 0.50
E 0.64 0.50 1.00
against avg 0.67 0.55 0.67
529
Computational Linguistics Volume 39, Number 3
Table 17
Correlation matrix for pairwise correlation agreement for Usim-2. The last row provides the
agreement of the annotator in that column against the average from the other annotators.
A C D F G H I J
A 1.00 0.70 0.52 0.70 0.69 0.72 0.73 0.67
C 0.70 1.00 0.48 0.72 0.60 0.66 0.71 0.69
D 0.52 0.48 1.00 0.48 0.49 0.51 0.50 0.42
F 0.70 0.72 0.48 1.00 0.66 0.71 0.74 0.68
G 0.69 0.60 0.49 0.66 1.00 0.71 0.65 0.62
H 0.72 0.66 0.51 0.71 0.71 1.00 0.70 0.65
I 0.73 0.71 0.50 0.74 0.65 0.70 1.00 0.72
J 0.67 0.69 0.42 0.68 0.62 0.65 0.72 1.00
against avg 0.82 0.78 0.58 0.80 0.76 0.80 0.81 0.76
0.73. All these correlations are highly significant (p 0.001) with an average correlation
of rho = 0.62. The lowest agreement between any individual and the average judgment
of the others is 0.58. Again, we note that these are all respectable values for tasks
involving semantic similarity ratings.
Use of the graded scale. Figure 1b shows how annotators made use of the graded scale
in Usim-1 and Usim-2. It graphs the relative frequency of each of the judgments on the
five-point scale. Figures 2b and 3b show the same but for each individual annotator. In
both annotation rounds, the rating 1 (completely different) was chosen most frequently.
There are also in both annotation rounds many ratings in the middle points of the
scale, indeed we see a larger proportion of mid-range scores for Usim than for WSsim
in general, as shown in Table 12. Figures 2b and 3b show that although individuals
differ, all use the mid points to some extent and it is certainly not the case that these
mid-range judgments come from a minority of annotators. In Usim, annotators com-
pared pairs of usages, whereas in WSsim, they compared usages with sense defini-
tions. The sense definitions suggest a categorization that may bias annotators towards
categorical choices. Comparing the two annotation rounds for Usim, we see that in
Usim-2 there seem to be many more judgments at 5 than in Usim-1. This is similar
to our findings for WSsim, where we also obtained more polar judgments for R2 than
for R1.
There is a larger range on average for Usim-2 compared with the other tasks as
shown earlier by Table 18. This is understandable given that there are eight annotators
Table 18
Average range and average variance of judgments for each of the graded experiments.
avg range avg variance
WSsim-1 1.78 1.44
WSsim-2 1.55 0.71
Usim-1 1.41 0.92
Usim-2 2.50 1.12
530
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
for R2 compared with R1,16 and so a greater chance of a larger range per item. There is
substantial variation by lemma. In Usim-2, fire.v, rough.a, and coach.n have an average
range of 1.33, 1.76, and 1.93, respectively, whereas suffer.v, neat.a, and function.n have
average ranges of 3.14, 3.16, and 3.58, respectively. The variation in range appears to
depend on the lemma rather than POS. This variation can be viewed as a gauge of how
difficult the lemma is. Although the range is larger in Usim-2, however, the average
variance per item (i.e., the variance considering the eight annotators) is 1.12 and lower
than that for WSsim-1.
Usim and the triangle inequality. In Euclidean space, the lengths of two sides of a triangle,
taken together, must always be greater than the length of the third side. This is the
triangle inequality:
length(longest) < length(second longest) + length(shortest)
We now ask whether the triangle inequality holds for Usim ratings. If Usim similarities
are metric, that is, if we can view the ratings as proximity in a Euclidean ?meaning
space,? then the triangle inequality would have to hold. This question is interesting for
what it says about the psychology of usage similarity judgments. Classic results due
to Tversky and colleagues (Tversky 1977; Tversky and Gati 1982) show that human
judgments of similarity are not always metric. Tversky (1977), varying an example
by William James, gives the following example, which involves words, but explicitly
ignores context:
Consider the similarity between countries: Jamaica is similar to Cuba (because of
geographical proximity); Cuba is similar to Russia (because of their political affinity);
but Jamaica and Russia are not similar at all. [. . . ] the perceived distance of Jamaica to
Russia exceeds the perceived distance of Jamaica to Cuba, plus that of Cuba to
Russia?contrary to the triangle inequality.
Note, however, that Tversky was considering similarity judgments for different words,
whereas we look at different usages of the same word. The question of whether the
triangle inequality holds for Usim ratings is also interesting for modeling reasons.
Several recent approaches model word meaning in context through points in vector
space (Erk and Pado 2008; Mitchell and Lapata 2008; Dinu and Lapata 2010; Reisinger
and Mooney 2010; Thater, Fu?rstenau, and Pinkal 2010; Washtell 2010; Van de Cruys,
Poibeau, and Korhonen 2011). They work on the tacit assumption that similarity of
word usages is metric?an assumption that we can directly test here. Third, the triangle
inequality question is also relevant for future annotation; we will discuss this in more
detail subsequently.
To test whether Usim ratings obey the triangle inequality, we first convert the
similarity ratings that the annotators gave to dissimilarity ratings: Let savg be the mean
similarity rating over all annotators, then we use the dissimilarity rating d = 6 ? savg
(as 5 was the highest possible similarity score).
We examine the proportion of sentence triples where the triangle inequality holds
(that is, we consider every triple of sentences that share the same target lemma). In those
16 A likely reason for the larger range in WSsim-1 compared with WSsim-2 is that in WSsim-2 half the
annotators had performed WSbest before WSsim-2 and produced more judgments of 1 compared with
WSsim-1.
531
Computational Linguistics Volume 39, Number 3
cases where the triangle inequality is violated, we also assess the degree to which it is
violated, calculated as the average distance that is missed: Let Tmiss be the set of triples
for which the triangle inequality does not hold, then we compute
m = 1|Tmiss|
?
t?Tmiss
length(longestt) ? (length(second longestt) + length(shortestt))
This is the average amount by which the longest side is ?too long.?
For the first round of annotation, Usim-1, we found that 99.2% of the sentence
triples obey the triangle inequality. For the triples that miss it, the average amount
by which the longest side is too long is m = 0.520. This is half a point on the five-
point rating scale, a low amount. In R2, all sentence triples obey the triangle inequality.
One potential reason for this is that we have eight annotators for R2, and a larger
sample of annotators reduces the variation from individuals. Another reason may
be that the annotators in R2 could view two more sentences of context than those
in R1.
Tables 19 and 20 show results of the triangle inequality analysis, but by individual
annotator. Every annotator has at least 93% of sentence triples obeying the principle. For
the triples that miss it, they tend to miss it by between one and two points. The results
for individuals accord with the triangle inequality principle, though to a lesser extent
compared with the analysis using the average, which reduces the impact of variation
from individuals.
As discussed previously, this result (that the triangle inequality holds for Usim
annotation triples) is interesting because it contrasts with Tversky?s findings (Tversky
1977; Tversky and Gati 1982) that similarity ratings between different words are not
metric. And although we consider similarity ratings for usages of the same word, not
different words, we would argue that our findings point to the importance of consider-
ing the context in which a word is used. It would be interesting to test whether similarity
ratings for different words, when used in context, obey the triangle inequality. To
reference the Tversky example, and borrowing some terminology from Cruse, evoking
the ISLAND facet of Jamaica and Cuba versus the COMMUNIST STATE facet of Cuba and
Russia would account for the non-metricality of the similarity judgments as Tversky
Table 19
Triangle inequality analysis by annotator, Usim-1.
average A D E
perc obey 93.8 97.2 97.3
missed by 1.267 1.221 1.167
Table 20
Triangle inequality analysis by annotator, Usim-2.
A C D F G H I J
perc obey 94.1 97.5 98.4 97.2 93.6 97.0 97.4 97.4
missed by 1.508 1.405 1.122 1.824 1.477 1.281 1.759 1.338
532
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 21
WSbest annotations.
sense selected Proportion with
no yes multiple choice
WSbest 19,599 2,401 0.13
WSbest, WSsim-2 first 9,779 1,221 0.15
WSbest, WSbest first 9,820 1,180 0.11
points out, and moreover highlight the lack of an apt comparison between Jamaica and
Russia at all. There is some motivation for this idea in the psychological literature on
structural alignment and alignable differences (Gentner and Markman 1997; Gentner
and Gunn 2001).
In addition, our finding that the triangle inequality holds for Usim annotation
will be useful for future Usim annotation. Usage similarity annotation is costly (and
somewhat tedious) as annotators give ratings for each pair of sentences for a given
target lemma. Given that we can expect usage similarity to be metric, we can eliminate
the need for some of the ratings. Once annotators have rated two usage pairs out of a
triple, their ratings set an upper limit on the similarity of the third pair. In the best case, if
usages s1 and s2 have a distance of 1 (i.e., a similarity of 5), and s1 and s3 have a distance
of 1, then the distance of s1 and s3 can be at most 2. For all usage triples where two
pairs have been judged highly similar, we can thus omit obtaining a rating for the third
pair. A second option for obtaining more Usim annotation is to use crowdsourcing. In
crowdsourcing annotation, quality control is always an issue, and again we can make
use of the triangle inequality to detect spurious annotation: Ratings that grossly violate
the triangle inequality can be safely discarded.
4.4 WSbest
The WSbest task reflects the traditional methodology in word sense annotation where
words are annotated with the best fitting sense. The guidelines17 allow for selecting
more than one sense provided all fit the example equally well. Table 21 shows that,
as one would expect given the number of senses in WordNet, there are more unse-
lected senses than selected. We again find an influence of task order: When annota-
tors did the graded annotation (WSsim-2) before WSbest, there were more multiple
assignments (see the last column) and therefore more senses selected. This difference
is statistically significant (?2 test, p = 0.02). Regardless of the order of tasks, we no-
tice that the proportion of multiple sense choice is far lower than the equivalent for
WSsim (see Table 11), as is expected due to the different annotation schemes and
guidelines.
We calculated inter-annotator agreement using pairwise agreement, as is standard
in WSD. There are several ways to calculate pairwise agreement in cases of multiple
selection, though these details are not typically given in WSD papers. We use the size
of the intersection of selections divided by the maximum number of selections from
17 See http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/wsbest.html.
533
Computational Linguistics Volume 39, Number 3
Table 22
Inter-annotator agreement without one individual for WSbest and SYNbest R2.
average A C D F G H I J
WSbest 0.574 0.579 0.564 0.605 0.560 0.582 0.566 0.566 0.568
SYNbest 0.261 0.261 0.259 0.285 0.254 0.256 0.245 0.260 0.267
either annotator. This is equivalent to 1 for agreement and 0 for disagreement in cases
where both annotators have selected only one sense. Formally, let i ? I be one annotated
sentence. Let A be the set of annotators and let PA = {{a, a?} | a, a? ? A} be the set of
annotator pairs. Let ai be the set of senses that annotator a ? A has chosen for sentence
i. Then pairwise agreement between annotators is calculated as:
ITA WSbest =
?
i?I
?
{a,a?}?PA
|ai?a?i |
max(|ai|,|a?i |)
|PA| ? |I|
(1)
The average ITA was calculated as 0.574.18 If we restrict the calculation to items
where each annotator only selected one sense (not multiple), the average is 0.626.
For SE-3, ITA was 0.628 on the English Lexical Sample task, not including the
multiword data (Mihalcea, Chklovski, and Kilgarriff 2004). This annotation exercise
used volunteers from the Web (Mihalcea and Chklovski 2003). Like our study, it had
taggers without lexicography background and gave a comparable ITA to our 0.626. We
calculated pairwise agreement for eight annotators. To carry out the experiment under
maximally similar conditions to previous studies, we also calculated ITA for items with
only one response and use only the four annotators who performed WSbest first. This
resulted in an average ITA of 0.638.
We also calculated the agreement for WSbest in R2 as in Equation 1 but with each
individual removed to see the change in agreement. The results are in the first row of
Table 22.
4.5 SYNbest
The SYNbest task is a repetition of the LEXSUB task (McCarthy and Navigli 2007, 2009)
except that annotators were asked to provide one synonym at most. As in LEXSUB,
agreement between a pair of annotators was counted as the proportion of all the
sentences for which the two annotators had given the same response.
As in WSbest, let A be the set of annotators. I is the set of test items, but as in
LEXSUB we only include those where at least two annotators have provided at least
one substitute: If only one annotator can think of a substitute then it is likely to be a
problematic item. As in WSbest, let ai be the set19 of responses (substitutes) for an item
18 Although there is a statistical difference in the number of multiple assignments depending upon whether
WSsim-2 is completed before or after WSbest, ITA on the WSbest task does not significantly differ
between the two sets.
19 Though, in fact, unlike LEXSUB and WSbest, we only collect one substitute per annotator for SYNbest.
534
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
i ? I for annotator a ? A. Let PA again be the set of pairs of annotators from A. Pairwise
agreement between annotators is calculated as in LEXSUB as:
PA =
?
i?I
?
{a,a?}?PA
|ai?a?i |
|ai?a?i |
|PA| ? |I|
(2)
Note that in contrast to pairwise agreement for traditional word sense annotation
(WSbest), the credit for each item (the intersection of annotations from the annotator
pair) is divided by the union of the responses. For traditional WSD evaluation, it is
divided by the number of responses from either annotator, which is usually one. For
lexical substitution this is important as the annotations are not collected over a predeter-
mined inventory. In LEXSUB, the PA figure was 0.278, whereas we obtain PA = 0.261 on
SYNbest. There were differences in the experimental set-up. We had eight annotators,
compared with five, and for SYNbest each annotator only provided one substitute.
Additionally, our experiment involved only a subset of the data used in LEXSUB. The
figures are not directly comparable, but are reasonably in line.
In our task, out of eight annotators we had at most three people who could not find
a substitute for any given item, so there were always at least five substitutes per item.
In LEXSUB there were 16 items excluded from testing in the full data set of 2010 because
there was only one token substitute provided by the set of annotators.
We also calculated the agreement for SYNbest as in Equation 2 but with each
individual removed to see the change in agreement. The results are in the second row
of Table 22.
4.6 Discussion of the Annotations of Individuals
We do not pose these annotation tasks as having ?correct responses.? We wish instead
to obtain the annotators? opinions, and accept the fact that the judgments will vary.
Nevertheless, we would not wish to conduct our analysis using annotators who were
not taking the task seriously. In the analyses that follow in subsequent sections, we
use the average judgment from our annotators to reduce variation from individuals.
Nevertheless, before doing so, in this subsection we briefly discuss the analysis of the
individual annotations provided earlier in this section in support of our decision to use
all annotators for the gold standard.
Although there was variation in the profile of annotations for individuals, all of the
annotators showed reasonable correlation on the graded task and at a level in excess
of that achieved on other graded semantic tasks (Mitchell and Lapata 2008). There will
inevitably be one annotator that has the lowest correlation with the others on any given
task, but we found that this was not the same annotator on every task. For example,
C on WSsim-2 has the lowest correlation with the average, yet concurs with others
much more on Usim-2 and leaving C out would reduce agreement on WSbest and on
SYNbest. D has lower correlation with others on several tasks, though higher than C on
WSsim-2. When we redo the triangle inequality analysis in Section 4.3 individually we
see from Tables 19 and 20 that annotator D is the highest performing annotator in terms
of meeting the triangle inequality principle in R2 and is a close runner-up in R1. These
results indicate that although annotators may use the graded scale in different ways,
their annotations tally to a reasonable extent. We therefore used all annotators for the
gold standard.
535
Computational Linguistics Volume 39, Number 3
4.7 Agreement Between Annotations in Different Frameworks
In this paper we are considering various different annotations of the same underly-
ing phenomenon: word meaning as it appears in context. In doing so, we contrast
traditional WSD methodology (SE-3, SemCor, and WSbest) with graded judgments of
sense applicability (WSsim), usage similarity (Usim), and lexical substitution as in
LEXSUB and SYNbest. In this section we compare the annotations from these different
paradigms where the annotations are performed on the same underlying data. For
WSsim and Usim, we use average ratings as the point of comparison.
4.7.1 Agreement Between WSsim and Traditional Sense Assignment. To compare WSsim
ratings on a five-point scale with traditional sense assignment on the same data, we
convert the traditional word sense assignments to ratings on a five-point scale: Any
sense that is assigned is given a score of 5, and any sense that is not assigned is
given a score of 1. If multiple senses are chosen in the gold standard, then they are
all given scores of 5. We then correlate the converted ratings of the traditional word
sense assignment with the average WSsim ratings using Spearman?s rho.
As described earlier, most of the sentences annotated in WSsim-1 were taken from
either SE-3 or SemCor. The correlation of WSsim-1 and SE-3 is rho = 0.416, and the cor-
relation of WSsim-1 with SemCor is rho = 0.425. Both are highly significant (p  0.001).
For R2 we can directly contrast WSsim with the traditional sense annotation in
WSbest on the same data. This allows a fuller comparison of traditional and graded
tagging because we have a data set annotated with both methodologies, under the same
conditions, and with the same set of annotators. We use the mode (most common) sense
tag from our eight annotators as the traditional gold standard label for WSbest and
assign a rating of 5 to that sense, and a rating of 1 elsewhere. We again used Spearman?s
rho to measure correlation between WSbest and WSsim and obtained rho = 0.483
(p  0.001).
4.7.2 Agreement Between WSsim and Usim. WSsim and Usim provide two graded an-
notations of word usage in context. To compare the two, we convert WSsim scores
to usage similarity ratings as in Usim. In WSsim, each sense has a rating (aver-
aged over annotators), so a sentence has a vector of ratings with a ?dimension? for
each sense. For example, the vector of average ratings for the sentence in Table 6 is
?5, 2.125, 1.25, 1.5, 1.25, 1.75?. All sentences with the same target will have vectors in the
same space, as they share the same sense list. Accordingly, we can compare a pair u,u?
of sentences that share a target using Euclidean distance:
d(u, u?) =
?
?
i
(ui ? u?i)2
where ui is the ith dimension of the vector u of ratings for sentence u. Note that this gives
us a dissimilarity rating for u,u?. We can now compare these sentence pair dissimilarities
to the similarity ratings of the Usim annotation.
In R1 we found a correlation of rho = ?0.596 between WSsim and Usim ratings.20
The basis of this comparison is small, at three lemmas with 10 sentences each, giving
20 The negative correlation is due to the comparison of dissimilarity ratings with similarity ratings.
536
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 23
Spearman?s correlation between lexical paraphrase overlap on the one hand, and Usim
similarity or WSsim dissimilarity on the other hand.
tasks rho
Usim-1 vs. LEXSUB 0.590
Usim-2 vs. SYNbest 0.764
WSsim-1 vs. LEXSUB ?0.495
WSsim-2 vs. SYNbest ?0.749
135 sentence pairs in total, because that is all the data available annotated in both
paradigms. For R2 we can perform the analysis on the whole Usim-2 and WSsim-2
data, which gives us 26 lemmas, with 1,142 sentence pairs.21 Correlation on R2 data is
rho = ?0.816. The degree of correlation is striking. We conclude that there is a very
strong relationship between the annotations for Usim and WSsim. This bodes well for
using Usim as a resource for evaluating sense inventories, an idea that we will pursue
further in Section 6: It reflects word meaning but is not tied to any given sense inventory.
4.7.3 Agreement of WSsim and Usim with Lexical Substitution. Lexical paraphrases (sub-
stitutes) have been used as a means of evaluating WSD systems in a task where the
inventory is not predefined (McCarthy and Navigli 2007, 2009). Because the R1 an-
notation was done in part on data that had previously been annotated with lexical
substitutions, and R2 included lexical substitution annotation, we can compare para-
phrase annotation with the results of WSsim and Usim. Again, we need to transform
annotations to make the comparison feasible. We convert all annotations to a Usim-
like format using sentence pair similarity or dissimilarity ratings. For WSsim, we use
the transformation described previously, using Euclidean distance between sense rating
vectors. We transform lexical substitution annotation using multiset intersection, as
the lexical substitution annotation of a sentence is a multiset of substitutes22 from all
annotators. If sentences s1, s2 have substitute multisets subs1 and subs2, respectively, and
freqi(w) is the frequency of substitute w in multiset subsi, then we calculate multiset
intersection as
INTER(s1, s2) =
1
max(|subs1|, |subs2|)
?
w?subs1?subs2
min( freq1(w), freq2(w))
Again, as before and in LEXSUB, we only keep sentences for which at least two
annotators could come up with a substitute. We also did not include any items that
were tagged with the wrong POS in LEXSUB.23
Table 23 shows correlation, in terms of Spearman?s rho, of Usim and WSsim
annotation with lexical substitution annotation. The values of Usim and WSsim are
based on mean scores averaged over all annotators. The INTER values computed for the
21 This is the number of pairs remaining after we exclude any pairs where one of the annotators provided a
?do not know? response.
22 The frequency of a substitute in a multiset depends on the number of annotators that picked the
substitute for the particular data point.
23 This was relevant only for the trial portion of LEXSUB, as the test portion was manually verified.
537
Computational Linguistics Volume 39, Number 3
lexical substitution annotation yield similarity ratings for sentence pairs; accordingly,
correlations of transformed lexical substitution with Usim are positive, and correlations
of transformed lexical substitution with the WSsim-based sentence dissimilarity ratings
are negative. All correlations are highly significant (p  0.001). We anticipated a higher
correlation of SYNbest with R2 annotation compared with that obtained using LEXSUB
and R1 annotation: In R2 the set of annotators is larger, the same set of annotators do
all experiments, and the SYNbest annotation focuses on obtaining one substitute per
annotator (whereas LEXSUB allowed annotators to supply up to three paraphrases). This
turned out to be in fact the case, as a comparison of rows 1 and 2 of Table 23 shows,
and likewise a comparison of rows 3 and 4. We notice that the correlation is slightly
stronger for Usim compared with WSsim, for both annotation rounds. One possible
reason for this is that the comparison of lexical substitution data with Usim involves
only one transformation of annotation data (the INTER calculation), whereas the com-
parison with WSsim involves two (INTER and also the Euclidean distance transforma-
tion). We can expect each transformation of annotation data to be ?lossy? in the sense
of introducing additional variance. Furthermore, WSsim relies on WordNet, which
may add a layer of structure that does not reflect the overlap in semantic similarity
between usages.
4.7.4 Summary. The Usim framework enables us to compare different annotation
schemes for word meaning, as it is relatively straightforward to map all annotations
to sentence pair (dis-)similarity ratings. We found strong relationships between WSsim
and Usim annotation, and between both graded annotation frameworks on the one
hand and traditional word sense annotation or lexical substitutions on the other hand.
This provides some validation for the novel annotation frameworks. Also, if all labeling
schemes provide comparable results, that opens up opportunities for choosing the best-
fitting labeling scheme for each situation. All these tasks pursue the same endeavor,
although the graded annotations and substitutions strive to capture the more subtle
nuances of meaning that are not adequately represented by the winner takes all ap-
proach of traditional methodology. WSsim is closest to the traditional methodology and
would suit systems needing to output WordNet sense labels, for example because they
want to exploit the semantic relations in WordNet. Usim is application-independent. It
allows for evaluation of systems that relate usages, whether into clusters or simply on
a continuum. It could, for example, be used as a resource-independent gold standard
for word sense induction. Lexical substitution tasks are particularly useful where the
application being considered would benefit from lexical paraphrasing, for example, text
simplification, summarization, or query expansion in information retrieval.
5. Examining Sense Groupings Emerging from WSsim Annotation
Recently there has been considerable work on grouping fine-grained senses, often from
WordNet, into more coarse-grained sense groups (Palmer, Dang, and Fellbaum 2007).
The use of coarse-grained sense groups has been shown to yield considerable improve-
ments in inter-annotator agreement in manual annotation, as well as in the accuracy of
WSD systems (Palmer, Dang, and Fellbaum 2007; Pradhan et al 2007). In our WSsim
annotation, we have used fine-grained WordNet senses, but we want to check that our
results are not an artifact of this fine-grained inventory. Furthermore, the annotation
results might be useful for identifying senses that could be grouped or for identifying
senses where grouping is not straightforward.
538
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
In WSsim, annotators gave ratings for each sense of a target word. If an annotator
perceives two senses of some target word as very similar, they will probably give them
similar ratings, and not just for a single sentence but across all the sentences featuring
the target word in question. So by looking for pairs of senses that tended to receive
similar ratings across all sentences, we can identify sense descriptions that according
to our annotators describe similar senses. Conversely, we expect that unrelated senses
would have dissimilar ratings. If there were many senses that the WSsim annotators
implicitly ?grouped? by giving them similar ratings throughout, we would have to
revise our finding that WSsim annotators often perceived more than one sense to be
applicable, as they would have perceived only what could be described as one implicit
sense group.
If a coarse-grained sense grouping is designed with the aim of reflecting sense
distinctions that would be intuitively plausible to an untrained speaker of the language,
then senses in a common group should also be similar according to WSsim annotation.
So when WSsim annotators give very different ratings to senses that are in the same
coarse-grained group, or very similar ratings to senses that are in different groups, this
can point to problems in a coarse-grained sense group.
In this section, first we describe two existing sense groupings (Hovy et al 2006;
Navigli, Litkowski, and Hargraves 2007). Then we test the extent that the annotations
accord with sense groupings by:
1. comparing judgments against the existing groupings, and re-examining
the question of how often WSsim annotators found multiple different
WordNet senses highly applicable.
2. using the WSsim data to examine the extent that the annotations could be
used to induce sense groupings.
5.1 Existing Sense Grouping Efforts
OntoNotes. The OntoNotes project (Hovy et al 2006; Chen and Palmer 2009) annotates
word sense, along with coreference and semantic roles. The senses that it uses for verbs
are WordNet 2.1 and 2.0, manually grouped based on both syntactic and semantic
criteria. Examples of these criteria include the causative/inchoative distinction, and
semantic features of particular argument positions, like animacy. Once the sense groups
for a lemma are constructed manually, they are used in trial annotation. If an inter-
annotator agreement of approximately 90% is reached, the lemma?s sense groups are
used for annotation; otherwise they are revised. Chen and Palmer report that the sense
groups used in OntoNotes have resulted in a rise in inter-annotator agreement as well
as annotator productivity. The third column of Table 24 shows OntoNotes groups for
the noun account.
5.1.1 The SemEval-2007 English All Words Task (EAW). For the English All Words task
at SemEval-2007, WordNet 2.1 senses were grouped by mapping them to the more
coarse-grained Oxford Dictionary of English senses. For the training data, this mapping
was done automatically; for the test data, the mapping was done by hand (Navigli,
Litkowski, and Hargraves 2007). For our analysis, we used only lemmas that were
included in the test data where the mapping had been produced manually.
539
Computational Linguistics Volume 39, Number 3
Table 24
WordNet 2.1 senses of the noun account, and their groups in OntoNotes (ON) and EAW.
WordNet sense WordNet ON EAW
sense no. group group
business relationship: ?he asked to see the executive
who handled his account?
3 1.1 5
report: ?by all accounts they were a happy couple? 8 1.2 2
explanation: ?I expected a brief account? 4 1.2 2
history, story: ?he gave an inaccurate account of
the plot [...]?
1 1.3 2
report, story: ?the account of his speech [...] made
the governor furious?
2 1.3 2
account statement: ?they send me an accounting
every month?
7 1.4 4
bill: ?send me an account of what I owe? 9 1.4 4
score: ?don?t do it on my account? 5 1.5 3
importance: ?a person of considerable account? 6 1.6 3
the quality of taking advantage: ?she turned her
writing skills to good account?
10 1.7 1
Column (4) of Table 24 shows EAW groups for the noun account.24 The two resources
largely agree in the groupings for account. But whereas EAW groups senses 1, 2, 4, and
8 together, OntoNotes splits those senses into two groups.
5.2 Does WSsim Annotation Conform to Existing Sense Groups?
In the WSsim annotation, we have used the fine-grained senses of WordNet 3.0. But
annotators were free to give high ratings for a sentence to more than one sense. So
it is possible that they implicitly used more coarse-grained sense distinctions. In this
and the following section, we will explore the question of whether, and to what extent,
WSsim annotators used implicit coarse-grained sense groups. In this section, we will
first ask whether their annotation matched the sense groups of either OntoNotes or
EAW. OntoNotes and EAW differ in the lemmas they cover. Also, as we saw earlier,
when they both cover a lemma, they do not always agree in the sense groups that they
propose. So we study the agreement of WSsim annotation with the two sense groupings
separately. We only study the lemmas which are in both the WSsim data and in either
OntoNotes or the EAW test data, listed in Table 25.
Tables 26 and 27 show the results. Table 26 looks at the number of sentences where
two senses both had high ratings, but are in different groupings in either OntoNotes or
EAW. The first row shows how many sentences there were where two senses received
a judgment of ? 3, but the two senses were not in a common OntoNotes/EAW group.
The second row shows the same for judgments ? 4, and the last row for judgments
of 5 only. In general, the percentages are higher for EAW than for OntoNotes. This is
not due to any difference in granularity between the two resources. The EAW sense
groups encompass on average 2.3 fine-grained senses for the R1 lemmas and 2.6 for the
24 The table shows the EAW groups of the WordNet senses, but the group numbering is our own for ease of
reference as no labels are given in EAW.
540
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 25
Lemmas in R1 and R2 WSsim that have coarse-grained mappings in OntoNotes and SemEval
2007 EAW.
R1 R2
lemma ON EAW ON EAW
account.n
? ?
add.v
?
ask.v
? ?
call.v
? ?
coach.n
?
different.a
?
dismiss.v
? ?
fire.v
?
fix.v
?
hold.v
? ?
lead.n
?
new.a
?
order.v
? ?
paper.n
?
rich.a
?
shed.v
?
suffer.v
? ?
win.v
? ?
Table 26
Sentences that have positive judgments for senses in different coarse groupings: percentage, and
absolute number in parentheses. J. = WSsim judgment, averaged over annotators.
OntoNotes EAW
J. Rd. 1 Rd. 2 Rd. 1 Rd. 2
? 3 28% (42) 52% (52) 78% (157) 62% (50)
? 4 13% (19) 16% (16) 41% (82) 22% (18)
5 3% (5) 3% (3) 8% (17) 6% (5)
Table 27
Sentences that have widely different judgments for pairs of senses in the same coarse grouping:
percentage, and absolute number in parentheses. J1 = WSsim judgment for the sense with the
lower rating, averaged over annotators; J2 = averaged WSsim judgment for the higher-rated of
the two senses.
OntoNotes EAW
J1 J2 Rd. 1 Rd. 2 Rd. 1 Rd. 2
? 2 ? 4 35% (52) 30% (30) 20% (39) 60% (48)
? 2 5 11% (16) 4% (4) 2% (4) 15% (12)
541
Computational Linguistics Volume 39, Number 3
R2 lemmas, and for OntoNotes the mean group sizes are 2.3 (R1) and 2.4 (R2). More
likely it is due to the individual lemmas. We observe that ratings of ?similar? or higher
are frequent. In all conditions except WSsim-1/OntoNotes, we find percentages over
50%. On the other hand, there are many fewer sentences where two senses received
judgments of ?very similar? or ?identical? but were not in the same OntoNotes or
EAW group, but these cases do exist. For example, there were five sentences with the
target dismiss.v which in WSsim received an average judgment of 4 or 5 for senses from
two different OntoNotes groups, 1.1 and 1.2. As dismiss is an R2 lemma, for which
only 10 sentences were annotated, this means that this phenomenon was found in
half the sentences annotated. The two sense groups are related: One is a literal, the
other a metaphorical, use of the verb. OntoNotes group 1.1 is defined as ?refuse to give
consideration to something or someone,? and group 1.2 is ?discharge, let go, persuade
to leave, send away.? One such sentence was the second sentence in Table 13.
Table 27 lists the number of sentences where two senses in the same OntoNotes
or EAW grouping received widely different ratings in the WSsim annotation. The first
row shows how many sentences there were where one sense received a rating of ? 2
and another sense from the same OntoNotes or EAW group had a rating of ? 4. The
second row shows the same for sense pairs in the same coarse-grained group where
one received a rating of ? 2 and the other the highest possible rating of 5. (Note that
the table considers judgments averaged over all annotators, so this row counts only
sentences where all annotators agreed on the highest rating.) An example of such a case
is Rich people manage their money well. In WSsim the first sense in WordNet (possessing
material wealth) received an average score of 5 (i.e. a unanimous verdict), whereas all
other senses received a score of less than 2. This included the third sense (of great worth
or quality; ?a rich collection of antiques?), which had an average of 1.625, and sense 8
(suggestive of or characterized by great expense; ?a rich display?) with an average of 1.125.
Both senses 3 and 8 are in the same group as sense 1 in EAW.
These are sentences where the WSsim annotation suggests a more fine-grained
analysis than the OntoNotes and EAW groups offer. The percentages are substantial:
For the more inclusive analysis in the first row, the numbers are between 20% and 60%
of all sentences, and between 2% and 15% of sentences even fall into the more restrictive
case in the second row. There is no clear trend in whether we see more of this type of
disagreement for OntoNotes or for EAW, or for the first or the second round of WSsim
annotation.
We see that there are a considerable number of sentences where either two senses
from the same OntoNotes or EAW group have received diverging WSsim ratings, or
two senses from different groups have received high ratings. In this way, the WSsim
annotation can be used to scrutinize sense groupings: If one aim of the sense groupings
is to form groups that would match intuitive sense judgments by untrained subjects,
then WSsim annotation would suggest that the senses of dismiss.v that correspond to
?dismiss a person? and ?dismiss an idea? may be too close together to be placed in
different groups.
5.3 Inducing Sense Relatedness from WSsim Annotation
In the WSsim annotation, annotators have annotated each occurrence of a target word
with a rating for each of the WordNet senses for the target, as illustrated in Tables 6?8.
This allows us, conversely, to chart the ratings that a WordNet sense received across
all sentences. Table 28 shows this chart for two senses of the noun account. In the
table, ratings are averaged across all annotators. In this case, the averaged ratings are
542
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Table 28
WSsim ratings for two senses of the noun account for 10 annotated sentences (averaged over
annotators).
WordNet Sentence
sense 1 2 3 4 5 6 7 8 9 10
1 1.00 2.25 1.13 4.25 1.13 1.0 1.13 1.13 1.13 4.25
4 1.50 3.00 1.25 2.88 1.50 1.50 1.63 1.00 1.38 3.88
Figure 4
Correlation between sense pairs: Distribution of rho values (Spearman?s rho).
similar for the two senses: They tend to be high for the same sentences, and low for the
same sentences. In general, senses that are closely related should tend to receive similar
ratings: high on the same sentences, and low on the same sentences, as illustrated for
the two senses in Table 28.
This then means that we can test the correlation on the ratings for two senses to see
if the WSsim annotators perceived them to be similar. We compute correlation for any
pair of senses for a common lemma, again using Spearman?s rho.25 Figure 4 shows the
distribution of rho values obtained for all the sense pairs, as histograms for R1 (left)
and R2 (right). When two senses are strongly positively correlated, this means that
the annotators likely viewed them as similar. When two senses are strongly negatively
correlated, this means they are probably so different that they tend never to be assigned
high ratings for the same sentences. We see that in both rounds, there were roughly as
many positive correlations as negative correlations. In R1, the rho values seem more or
less equally distributed over the range from ?1 to 1. In R2, there were more annotators
and the distribution is closer to a normal distribution with more rho values close to 0.
25 We exclude senses that received a uniform rating of 1 on all items. For R1 there were no such cases and
for R2 there were only 14 out of a total of 275 senses.
543
Computational Linguistics Volume 39, Number 3
We have shown the OntoNotes and EAW sense groups for the noun account. We can
now look at the WSsim-derived correlations for the same lemma, shown in Figure 5. The
first row in each box shows the WordNet sense number, and the second row shows the
OntoNotes and EAW sense groups. All three labels are those used in Table 24. Each edge
represents a correlation in the WSsim annotation. To avoid clutter, only correlations
with rho ? 0.5 are included, and a sense is only shown if it is correlated with any other
sense. Edge thickness corresponds to the value of the correlation coefficient rho between
each two senses; rho is also annotated on the edges. The first thing to note is that WSsim-
based correlation does not give us sense groups. Correlations are of different strengths,
and different cutoffs would result in different link graphs. Even for the chosen cutoff
of rho = 0.5, the correlations do not induce cliques (in the graph-theoretic sense). For
example, the sixth sense of account shows a correlation of rho ? 0.5 with the eighth
sense, but not with any of the other senses to which the eighth sense is linked. The
figure also shows that there are some senses that are strongly correlated in their annota-
tion but are not grouped in one or the other of the existing groupings. For example,
senses 3 (the executive who handles his account) and 7 (account statement) are strongly
correlated, but are in different groups in OntoNotes as well as in EAW. There are also
senses that share the same group in one of the coarse grained inventories, but have
a weak or even a negative correlation based on the WSsim annotation. For example,
Figure 5
Sense correlation in the WSsim annotation for the noun account. Showing all correlations with
rho ? 0.5. Upper row in each box: WordNet sense number. Lower row: OntoNotes and EAW
sense groups. Edge thickness expresses strength of correlation.
544
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Figure 6
Overall correlation versus annotation in single sentences: Number of sentences in which two
senses with an overall correlation ? ? have both been annotated with a judgment of ? j, for
j = 3, 4, 5. (Judgments averaged over annotators.)
for the lemma paper, senses 1 (a material made from cellulose pulp) and 4 (a medium for
written communication) are in the same EAW group, but have a correlation in WSsim of
rho = ?0.52.
In Section 5.2 we asked whether the many cases where WSsim annotators gave
high ratings to more than a single sense could possibly be explained by them im-
plicitly using more coarse-grained senses. We answered this question by comparing
the WSsim annotation to OntoNotes and EAW sense groups, finding a considerable
number of sentences where two senses received a high rating but were not from the
same sense group. Now we can repeat the question, but try to answer it using the
WSsim sense relations obtained from correlation: Is it possible that WSsim annotators
implicitly used more coarse-grained senses, but just not the OntoNotes or EAW sense
groups?
We tested how often annotators gave ratings of at least similar (i.e., ratings ? 3) to
senses that were related at a level ? rho, for rho ranging from ?1 to 1. The question
that we want to answer is: If annotators give high ratings to multiple senses on the
same sentence, is it always to senses that are strongly positively correlated, or do they
sometimes pick multiple senses that are not strongly correlated, or even senses that
are negatively correlated? The results are shown in Figure 6. First, we can see that
there is a sizeable number of sentences where two senses that are negatively correlated
have both received a positive judgment. For R1, the numbers for negatively correlated
senses are 135 ( j ? 3), 29 ( j ? 4), and 2 ( j = 5). For R2, the numbers of sentences are
lower absolutely and in proportion, with 29 ( j ? 3), 7 ( j ? 4), and 0 ( j = 5). It is also
interesting to look at a less stringent threshold than rho ? 0; we can use the significance
levels p ? 0.05 and p ? 0.01 for this. If we look at sense pairs that were not positively
correlated at p ? 0.05 (p ? 0.01), there were 185 (205) sentences in R1 and 54 (88)
sentences in R2 where two such senses both received judgments of 3 or higher. Note
that the significance levels of p ? 0.05, p ? 0.01 are here just arbitrary thresholds at
which to inspect the data; they are not thresholds that determine the significance of
545
Computational Linguistics Volume 39, Number 3
some hypothesis.26 This brings us back to the question asked above of whether the
WSsim annotators implicitly used more coarse-grained senses. If they had implicitly
used more coarse-grained senses, we would have expected to see very few cases where
unrelated senses got a high rating on the same sentence. What we found instead was
that such cases were relatively frequent, which implies that WSsim annotators in both
rounds ?mix and match? senses specifically for each sentence that they evaluate. For
example, the senses 1 (she dismissed his advances) and 5 (I was dismissed after I gave my
report) of dismiss are negatively correlated (rho = ?0.61) yet have average judgments of
3.25 and 4.125 on the second example in Table 13.
5.3.1 Summary. In this section we have analyzed the WSsim annotation in comparison
with more coarse-grained sense repositories. One aim was to find out whether anno-
tators really used the fine granularity that the WSsim task offered or whether they
implicitly used more coarse-grained senses. Both by comparing the WSsim annotation
to coarse-grained OntoNotes and EAW sense groups, and by comparing the WSsim
annotation to the sense relations implied by WSsim, we find that annotators did make
use of the ability to combine sense ratings in a way that was particular to each sentence
they annotated. We also conclude that WSsim annotation can be used to evaluate
OntoNotes and EAW groupings with respect to the level to which senses are intuitively
distinguishable to untrained subjects. Here, WSsim annotation can uncover senses in
different groups that WSsim annotators often conflate, or senses in a single coarse group
that WSsim annotators treat differently.
6. Usim and Sense Groupings
One of the major motivations for the Usim task is that it allows us to examine the
meanings of words without recourse to a predefined inventory. We have demonstrated
in this paper that the data from this task can be compared directly to paraphrase
data as well as to data annotated for word sense. In the previous section we have
focused on using our WSsim data to examine existing sense groupings. WSsim is useful
precisely because it has sense annotations from an existing inventory, WordNet, so we
can use the graded annotations to see how these senses relate, and also relationships
between coarser grained inventories with mappings to WordNet. Usim does not capture
this information, nevertheless it might be useful as a resource for examining sense
groupings. We can use it to examine the extent to which sense groupings keep usages
together that have a high usage similarity according to Usim, and keep sentences
with low usage similarity apart. In this analysis, we use the data from R2 because
this has Usim judgments for sentences alongside traditional word sense annotations
(WSbest). As WSbest annotation, we use the mode of the chosen senses27 (as in the
analysis in Section 4.7) for each sentence in R2, and map it to its coarse-grained sense in
26 We are performing multiple tests on the same senses, which increases the likelihood of falsely assuming
two senses to be significantly correlated at some significance level (Type I errors). The significance levels
are only arbitrary thresholds in our case, however. In addition, our analysis focuses on sense pairs that
are not significantly positively correlated. For that reason, Type I errors actually reduce our estimate
of the number of sentences in which two non-related senses both received high ratings. Conversely,
correcting for multiple testing makes our estimate less conservative: If we count sentences with positive
ratings for sense pairs that are not positively correlated at p ? 0.05 with Bonferroni correction, the number
of sentences rises from 185 to 207 for judgments of 3 or higher.
27 We perform this analysis only on sentences where there was one sense found as mode and where this had
a coarse-grained mapping in either the EAW or OntoNotes resources.
546
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
EAW and/or OntoNotes. We then compute the average Usim similarity for all pairs of
sentences with the same coarse-grained sense, and compare it with the average Usim
similarity for sentence pairs with different coarse-grained senses. The results are shown
in the first row of figures in Table 29. We see that the OntoNotes and EAW sense groups
do indeed partition the sentences such that pairs within the same group have high usage
similarity (4 or above) and those in different groups have low usage similarity (2 or
below).
The second part of Table 29 performs the same analysis on the basis of individual
lemmas. A dash (?) means that either there was no coarse mapping, or there were no
sentence pairs in this category. For example, there were no sentence pairs identified
as having different OntoNotes groups or EAW groups for the lemma suffer.v. For the
lemmas call.v and dismiss.v, the two sense inventories give rise to the same groupings of
sentence pairs.
In the table, we see many lemmas where the groupings concur with the Usim
judgments. One example is account.n, where the sentence pairs in the same coarse group
get high average Usim values, whereas sentence pairs with different coarse groups have
low average Usim values. There are, however, a few lemmas where the average Usim
values indicate that either the coarse groupings might benefit from another inspection,
or that the lemma has meanings with subtle relationships where grouping is not a
straightforward exercise. One example is new.a, which has the same high Usim values
for both same and different categories in EAW. Another is shed.v, where the sentences
annotated with the same OntoNotes groups actually have a lower average Usim value
than those with different groups.
We can also use Usim judgments to analyze individual sense groups. This could
be useful in determining specific groups that might warrant further revision, or that
represent meanings which are simply difficult to distinguish. To demonstrate this, we
analyzed all coarse-grained sense groups with at least one sentence pair in R2, that is, all
groups that had at least two R2 sentences whose WSbest mode mapped to that coarse
Table 29
Average Usim rating for R2 where WSbest annotations suggested the same or different coarse
grouping.
OntoNotes EAW
same different same different
4.0 1.9 4.1 2.0
by lemma
account.n 4.0 1.6 4.0 1.5
call.v 4.3 1.4 4.3 1.4
coach.n 4.6 2.3 ? ?
dismiss.v 3.8 2.6 3.8 2.6
fire.v 4.6 1.2 ? ?
fix.v 4.2 1.1 ? ?
hold.v 4.5 2.0 3.8 1.9
lead.v ? ? 2.9 1.5
new.a ? ? 4.6 4.6
order.v 4.3 1.7 ? ?
rich.a ? ? 4.6 2.0
shed.v 2.9 3.3 ? ?
suffer.v 4.2 ? 4.2 ?
547
Computational Linguistics Volume 39, Number 3
group. (Naturally, due to the skewed nature of sense distributions and the fact that
we only have ten sentences for each lemma, some groups do not meet this criterion.)
We find that the majority of groups that were analyzed have an average Usim rating
of over 4. This is the case for 75% of the analyzed EAW groups and 76% of OntoNotes
groups. There were, however, groups with very low values. One example was group 1.1
of shed.v in OntoNotes, with an average Usim rating of 2.9. This group includes both
literal senses (trees shed their leaves) and metaphorical senses (he shed his image as a
pushy boss) of the verb shed. Another example is group 7 of lead.n in EAW, also with
an average Usim of 2.9. This group includes taking the lead as well as lead actor, so quite
a diverse collection of usages. Two example sentences annotated with these two senses
are shown here. This pair had an average Usim value of 1.25.
My students perform a wide variety of music and they can be found singing leading
roles in their high school and college musical productions, singing lead in rock and
wedding bands, winning classical music competitions, singing at the summer
conservatory of The Papermill Playhouse, and learning to sing so they can sing with
local choirs.
And as a result of President Bush?s initiative, which he took as part of the G-8
Presidency, and also the other changes in which the US, UK has been in the lead, not
least in Afghanistan and Iraq, you can now feel the winds of change blowing through
the Arab world.
In the future we hope to obtain more Usim data. When we have more data, we will
investigate whether the groupings that Usim identifies as problematic tend to be the
same ones that require more iterations in inventory construction (Hovy et al 2006). We
also plan to test whether groupings with low Usim ratings tend to have lower inter-
tagger agreement on traditional WSD annotation.
7. Computational Modeling
The graded meaning annotation data from Usim and WSsim annotation can be used to
evaluate computational models of word meaning. In this section we summarize existing
work on modeling the R1 data, which has already been made publicly available.
The WSsim data can be used to evaluate graded word meaning models as well
as traditional WSD systems. Instead of evaluating only the highest-confidence sense of a
WSD model, we can take a more nuanced look at a model?s predictions, and give credit if
it proposes multiple appropriate senses. In Erk and McCarthy (2009) we take advantage
of this fact to evaluate and compare two supervised models on the WSsim data: a
traditional WSD model, and a distributional model that forms one prototype vector for
each sense of a given lemma. Both are trained on traditional single-sense annotation, but
the prototype model does not see any negative data during training in order to avoid
spurious negative data. For training, each word occurrence is represented as either a
first-order or a second-order bag-of-words vector of its sentence. In an evaluation using
weighted variants of precision and recall, we find that when the traditional WSD model
is credited for all the senses that it proposes, rather than only the single sense with
the highest confidence value, it does much better on both measures. This shows that
the model does propose multiple appropriate senses, such that its performance may be
underestimated in traditional evaluation. As was to be expected, the prototype models
that do not see negative data during training have much higher recall at lower precision,
for an overall better F-score (again using weighted variants of the evaluation measures).
548
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
Thater, Fu?rstenau, and Pinkal (2010) address the WSsim data with an unsupervised
model. It represents a word sense as the sum of the vectors for all synonyms in its synset,
plus the vectors for all hypernyms scaled down by a factor of 10. They also use a more
complex, syntax-based model to derive occurrence representations. Unfortunately their
results are not directly comparable to Erk and McCarthy (2009) because they evaluate
on a subset of the data (verb lemmas only).
The Usim data, which directly describes the similarity of pairs of usages, can be
used to evaluate distributional models of word meaning in context. So far, only one type
of model has been evaluated on this data to the best of our knowledge: the clustering-
based approach of Reisinger and Mooney (2010). They use the Usim data to test to what
extent their clusters correspond to human intuitions on a word?s senses. Their result is
negative, as a low correlation of human judgments and predictions suggests to them
that the induced clusters are not a good match for human senses. The Usim data is
particularly interesting for a different way of evaluating distributional and vector space
approaches for word meaning in context. These have been evaluated on the tasks of
lexical substitution (Erk and Pado 2008; Dinu and Lapata 2010; Thater, Fu?rstenau, and
Pinkal 2010; Van de Cruys, Poibeau, and Korhonen 2011), information retrieval, and
word sense disambiguation (Schu?tze 1998), but Usim, in contrast, offers a different and
more direct evaluation perspective.
8. Conclusion
In this paper we have explored the question of whether word meaning can be described
in a graded fashion. Our aim has been to use annotation with graded ratings to capture
untrained speakers? intuitions on word meaning. Our motivation has been two-fold. On
the one hand we are drawing on current theories of cognition, which hold that mental
concepts have ?fuzzy boundaries.? On the other hand we wanted to give a basis to
current computational models for word meaning in context that predicts degrees of
similarity between word occurrences. We have addressed this question through two
novel types of graded annotations of word meaning in context that draws on methods
from psycholinguistic experimentation. WSsim obtains word sense annotations from a
given sense inventory but uses graded judgments for each sense. Usim judges similarity
of pairs of usages of the same lemma.
The analysis of annotation results lets us answer our main question in the affir-
mative. Annotators can describe word meaning through graded ratings with good
inter-annotator agreement, measured through pairwise correlation. Even though no in-
depth training on sense distinctions was provided, the pairwise correlations were good
in every single case, indicating that all annotators did the tasks in a similar fashion.
In both tasks, all annotators made use of the full graded scale, and did not treat the
task as binary. The Usim annotation provides us with a means of comparing different
word meaning annotation paradigms. We have used it to demonstrate that there is
strong correlation of these new annotations with both traditional WSD labels, and with
overlap of lexical paraphrases. This is as we anticipated, as all of these annotations are
describing the same phenomenon of word meaning in context through different means.
In additional analysis of the WSsim annotation, we found a high proportion of
sentences (between 23% and 46%) in which multiple senses received high positive
judgments from the same annotators. At the same time, annotators used the WSsim
ratings in a nuanced and fine-grained fashion, sometimes assigning high ratings on the
same sentence to two senses that overall patterned very differently. Analyzing Usim
annotation, we found that all annotators? ratings obey the triangle inequality in almost
549
Computational Linguistics Volume 39, Number 3
all cases. This can be taken as a measure of intra-annotator consistency on the task.
It also means that current distributional approaches to word meaning in context are
justified in viewing usage similarity as metric. Triangle inequality can be used to check
the validity of future Usim annotation.
We do not propose that either one of our annotations is a panacea for evaluation
of systems that represent word meaning in context, but we argue that they provide
data sets that better reflect the fluid nature of word meaning and allow us to evaluate
questions related to word meaning in a new fashion. In this paper, we have used
both WSsim and Usim data to analyze existing coarse-grained sense inventories. We
have demonstrated that it is often not straightforward to group sentences into disjoint
senses, depending on the lemma. We have also shown how both WSsim and Usim style
judgments can be used to identify problematic lemmas, as well as sense groupings that
may warrant another inspection to check whether they match naive speakers? intuitive
judgments. The graded annotation can also be used to identify lemmas whose usages
are difficult to group into clear distinct senses. This information can in the future be
used to handle such lemmas differently when making sense inventories, in annotation,
and in computational systems.
An important next question to consider is the use of WSsim and Usim data to eval-
uate computational models of word meaning. As we have shown (Erk and McCarthy
2009), WSsim data can be used to evaluate traditional WSD systems in a graded fashion.
We plan to do a more large-scale evaluation to assess to what extent the performance
of current WSD systems is underestimated. Also, fine-grained WSsim annotation can be
used for a comparison of fine-grained and coarse-grained traditional WSD systems. We
have also shown (Erk and McCarthy 2009) that WSsim can be used to evaluate graded
word sense assignment systems. Although we used a supervised setting, however, we
trained on traditional sense annotation. We plan to collect more WSsim annotation in
order to be able to train word sense assignment systems on graded data, for example,
using a regression model.
In the same vein, we will extend the available Usim data to cover many more
sentences by using crowdsourcing. The use of Usim for supervised training of word
meaning models is particularly interesting as all existing usage similarity models are
unsupervised; given previous results in WSD, we can expect that supervision will
improve the performance of models of usage meaning. One way of using Usim data
in training is to learn a similarity metric. Metric learning (see, e.g., Davis et al 2007)
induces a distance measure from given constraints stating similarity or dissimilarity of
items.
Our novel graded annotation frameworks, WSsim and Usim, are validated both
through good agreement between those data sets themselves, as well as good agreement
between those data sets and traditional word sense annotation and lexical substitutions.
Because all labeling schemes provide comparable results, this allows different ways of
evaluating systems providing different perspectives on system output. Furthermore,
the different paradigms may suit different types of systems. Lexical substitution tasks
(McCarthy 2002; McCarthy and Navigli 2009) are particularly useful where the ap-
plication being considered would benefit from lexical paraphrasing, for example, text
simplification, summarization, or query expansion in information retrieval. WSsim is
closest to the traditional methodology (WSbest) and would suit systems needing to
output WordNet sense labels, for example, because they want to exploit the semantic
relations in WordNet for tasks such as inferencing or producing lexical chains. Unlike
WSbest, it avoids a winner-takes-all approach and allows for more nuanced sense
tagging. Usim is application-independent. It allows for evaluation of systems that relate
550
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
usages, whether into clusters or simply on a continuum. It could, for example, be used
as a resource-independent gold standard for word sense induction by calculating the
within and across class similarities. Aside from its use as an enabling technology within
a natural language processing application, a system that performs well at the Usim task
may be useful in its own right. For example, it could be used to enable lexicographers
to work on groups of examples that reflect similar meanings, or find further examples
close to the one being scrutinized.
Acknowledgments
The annotation was funded by a UK Royal
Society Dorothy Hodgkin Fellowship to
Diana McCarthy. This work was supported
by National Science Foundation grant
IIS-0845925 for Katrin Erk. We are grateful
to Huw McCarthy for implementing the
interface for round 2 of the annotation. We
thank the anonymous reviewers for many
helpful comments and suggestions.
References
Agirre, Eneko and Philip Edmonds,
editors. 2007. Word Sense Disambiguation:
Algorithms and Applications. Springer,
Dordrecht.
Agirre, Eneko, Llu??s Ma`rquez, and Richard
Wicentowski, editors. 2007. Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007).
Prague.
Baroni, Marco and Roberto Zamparelli. 2010.
Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions
in semantic space. In Proceedings of the 2010
Conference on Empirical Methods in Natural
Language Processing, pages 1,183?1,193,
Cambridge, MA.
Brown, Susan. 2008. Choosing sense
distinctions for WSD: Psycholinguistic
evidence. In Proceedings of ACL-08: HLT,
Short Papers (Companion Volume),
pages 249?252, Columbus, OH.
Brown, Susan. 2010. Finding Meaning:
Sense Inventories for Improved Word Sense
Disambiguation. Ph.D. thesis, University
of Colorado at Boulder.
Burchardt, Aljoscha, Katrin Erk, Annette
Frank, Andrea Kowalski, Sebastian Pado,
and Manfred Pinkal. 2006. The SALSA
corpus: A German resource for lexical
semantics. In Proceedings of the Fifth
International Conference on Language
Resources and Evaluation (LREC 2006),
pages 969?974, Genoa.
Carpuat, Marine and Dekai Wu. 2007a. How
phrase sense disambiguation outperforms
word sense disambiguation for statistical
machine translation. In Proceedings
of the 11th Conference on Theoretical and
Methodological Issues in Machine Translation
(TMI 2007), pages 43?52, Skovde.
Carpuat, Marine and Dekai Wu. 2007b.
Improving statistical machine translation
using word sense disambiguation. In
Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007),
pages 61?72, Prague.
Chen, Jinying and Martha Palmer. 2009.
Improving English verb sense
disambiguation performance with
linguistically motivated features and clear
sense distinction boundaries. Journal of
Language Resources and Evaluation (Special
Issue on SemEval-2007), 43:181?208.
Coecke, Bob, Mehrnoosh Sadrzadeh, and
Stephen Clark. 2010. Mathematical
foundations for a compositional
distributed model of meaning. Lambek
Festschrift, Linguistic Analysis, 36:345?384.
Coleman, Linda and Paul Kay. 1981.
Prototype semantics: The English word
?lie.? Language, 57:26?44.
Copestake, Ann and Ted Briscoe. 1995.
Semi-productive polysemy and sense
extension. Journal of Semantics, 12:15?67.
Cruse, D. A. 1995. Polysemy and related
phenomena from a cognitive linguistic
viewpoint. In Philip Saint-Dizier and
Evelyne Viegas, editors, Computational
Lexical Semantics. Cambridge University
Press, pages 33?49.
Davis, Jason, Brian Kulis, Prateek Jain,
Suvrit Sra, and Inderjit Dhillon. 2007.
Information-theoretic metric learning.
In Proceedings of the 24th International
Conference on Machine Learning,
pages 209?216, Corvallis, OR.
Deschacht, Koen and Marie-Francine Moens.
2009. Semi-supervised semantic role
labeling using the Latent Words Language
Model. In Proceedings of EMNLP,
pages 21?29, Singapore.
Dinu, Georgiana and Mirella Lapata. 2010.
Measuring distributional similarity in
context. In Proceedings of the 2010
551
Computational Linguistics Volume 39, Number 3
Conference on Empirical Methods in Natural
Language Processing, pages 1,162?1,172,
Cambridge, MA.
Edmonds, Philip and Scott Cotton, editors.
2001. Proceedings of the SensEval-2
Workshop. Toulouse. See http://www.sle.
sharp.co.uk/senseval.
Erk, Katrin and Diana McCarthy. 2009.
Graded word sense assignment. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 440?449, Singapore.
Erk, Katrin, Diana McCarthy, and Nicholas
Gaylord. 2009. Investigations on word
senses and word usages. In Proceedings
of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 10?18,
Suntec.
Erk, Katrin and Sebastian Pado. 2008. A
structured vector space model for word
meaning in context. In Proceedings of
EMNLP-08, pages 897?906, Waikiki, HI.
Erk, Katrin and Sebastian Pado. 2010.
Exemplar-based models for word meaning
in context. In Proceedings of the ACL 2010
Conference Short Papers, pages 92?97,
Uppsala.
Erk, Katrin and Carlo Strapparava, editors.
2010. Proceedings of the 5th International
Workshop on Semantic Evaluation(SemEval).
Uppsala.
Frazier, Lyn and Keith Rayner. 1990. Taking
on semantic commitments: Processing
multiple meanings vs. multiple senses.
Journal of Memory and Language,
29:181?200.
Gentner, Dedre and Virginia Gunn. 2001.
Structural alignment facilitates the
noticing of differences. Memory and
Cognition, 21:565?577.
Gentner, Dedre and Arthur Markman. 1997.
Structural alignment in analogy and
similarity. American Psychologist, 52:45?56.
Grefenstette, Edward and Mehrnoosh
Sadrzadeh. 2011. Experimental
support for a categorical compositional
distributional model of meaning.
In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language
Processing, pages 1,394?1,404, Edinburgh.
Hampton, James A. 1979. Polymorphous
concepts in semantic memory. Journal
of Verbal Learning and Verbal Behavior,
18:441?461.
Hampton, James A. 2007. Typicality, graded
membership, and vagueness. Cognitive
Science, 31:355?384.
Hanks, Patrick. 2000. Do word meanings
exist? Computers and the Humanities,
34(1?2):205?215.
Hovy, Eduard H., Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. OntoNotes: The 90%
solution. In Proceedings of the Human
Language Technology Conference of the North
American Chapter of the ACL (NAACL-2006),
pages 57?60, New York.
Ide, Nancy and Yorick Wilks. 2006. Making
sense about sense. In Eneko Agirre and
Philip Edmonds, editors, Word Sense
Disambiguation, Algorithms and Applications.
Springer, Dordrecht, pages 47?73.
Kilgarriff, Adam. 1992. Polysemy. Ph.D.
thesis, University of Sussex.
Kilgarriff, Adam. 1997. I don?t believe in
word senses. Computers and the Humanities,
31(2):91?113.
Kilgarriff, Adam. 2006. Word senses.
In Eneko Agirre and Philip Edmonds,
editors, Word Sense Disambiguation:
Algorithms and Applications. Springer,
Dordrecht, pages 29?46.
Kilgarriff, Adam and Martha Palmer,
editors. 2000. Senseval: Special Issue of the
Journal Computers and the Humanities,
volume 34(1?2). Kluwer, Dordrecht.
Kilgarriff, Adam and Joseph Rosenzweig.
2000. Framework and results for English
Senseval. Computers and the Humanities,
34(1-2):15?48.
Kintsch, Walter. 2007. Meaning in context. In
T. K. Landauer, D. McNamara, S. Dennis,
and W. Kintsch, editors, Handbook of Latent
Semantic Analysis. Erlbaum, Mahwah, NJ,
pages 89?105.
Klein, Devorah and Gregory Murphy. 2001.
The representation of polysemous words.
Journal of Memory and Language,
45:259?282.
Klein, Devorah and Gregory Murphy. 2002.
Paper has been my ruin: Conceptual
relations of polysemous senses. Journal of
Memory and Language, 47:548?570.
Klepousniotou, Ekaterini. 2002. The
processing of lexical ambiguity:
Homonymy and polysemy in the mental
lexicon. Brain and Language, 81:205?223.
Klepousniotou, Ekaterini, Debra Titone, and
Caroline Romero. 2008. Making sense of
word senses: The comprehension of
polysemy depends on sense overlap.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 34(6):1,534?1,543.
Krishnamurthy, Ramesh and Diane
Nicholls. 2000. Peeling an onion: The
lexicographers? experience of manual
552
Erk, McCarthy, and Gaylord Measuring Word Meaning in Context
sense-tagging. Computers and the
Humanities, 34(1-2):85?97.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The Latent
Semantic Analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104:211?240.
Landes, Shari, Claudia Leacock, and
Randee Tengi. 1998. Building semantic
concordances. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical
Database. The MIT Press, Cambridge, MA.
Lefever, Els and Ve?ronique Hoste. 2010.
Semeval-2010 task 3: Cross-lingual word
sense disambiguation. In Proceedings of the
5th International Workshop on Semantic
Evaluation, pages 15?20, Uppsala.
McCarthy, Diana. 2002. Lexical substitution
as a task for wsd evaluation. In Proceedings
of the ACL Workshop on Word Sense
Disambiguation: Recent Successes and
Future Directions, pages 109?115,
Philadelphia, PA.
McCarthy, Diana and Roberto Navigli. 2007.
SemEval-2007 task 10: English lexical
substitution task. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 48?53,
Prague.
McCarthy, Diana and Roberto Navigli. 2009.
The English lexical substitution task.
Language Resources and Evaluation Special
Issue on Computational Semantic Analysis
of Language: SemEval-2007 and Beyond,
43(2):139?159.
McNamara, Timothy P. 2005. Semantic
Priming: Perspectives from Memory and Word
Recognition. Psychology Press, New York.
Mihalcea, Rada and Timothy Chklovski.
2003. Open Mind Word Expert: Creating
large annotated data collections with web
users? help. In Proceedings of the EACL 2003
Workshop on Linguistically Annotated
Corpora (LINC 2003), pages 53?60,
Budapest.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The SENSEVAL-3
English lexical sample task. In Rada
Mihalcea and Phil Edmonds, editors,
Proceedings SENSEVAL-3 Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
pages 25?28, Barcelona.
Mihalcea, Rada and Phil Edmonds, editors.
2004. Proceedings SENSEVAL-3 Second
International Workshop on Evaluating
Word Sense Disambiguation Systems,
Barcelona.
Mihalcea, Rada, Ravi Sinha, and Diana
McCarthy. 2010. Semeval-2010 task 2:
Cross-lingual lexical substitution. In
Proceedings of the 5th International
Workshop on Semantic Evaluation,
pages 9?14, Uppsala.
Miller, George A., Claudia Leacock,
Randee Tengi, and Ross T Bunker.
1993. A semantic concordance. In
Proceedings of the ARPA Workshop
on Human Language Technology,
pages 303?308, Plainsboro, NJ.
Mitchell, Jeff and Mirella Lapata. 2008.
Vector-based models of semantic
composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, OH.
Mitchell, Jeff and Mirella Lapata. 2010.
Composition in distributional models
of semantics. Cognitive Science,
34(8):1388?1429.
Moon, Taesun and Katrin Erk. In press.
An inference-based model of word
meaning in context as a paraphrase
distribution. ACM Transactions on
Intelligent Systems and Technology
special issue on paraphrasing.
Murphy, Gregory L. 1991. Meaning and
concepts. In Paula Schwanenflugel, editor,
The Psychology of Word Meanings. Lawrence
Erlbaum Associates, Mahwah, NJ,
pages 11?35.
Murphy, Gregory L. 2002. The Big Book of
Concepts. MIT Press, Cambridge, MA.
Navigli, Roberto. 2009. Word sense
disambiguation: a survey. ACM
Computing Surveys, 41(2):1?69.
Navigli, Roberto, Kenneth C. Litkowski,
and Orin Hargraves. 2007. SemEval-2007
task 7: Coarse-grained English all-words
task. In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 30?35, Prague.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13:137?163.
Passonneau, Rebecca, Ansaf Salleb-Aouissi,
Vikas Bhardwaj, and Nancy Ide. 2010.
Word sense annotation of polysemous
words by multiple annotators. In
Proceedings of LREC-7, pages 3,244?3,249,
Valleta.
Pickering, Martin and Steven Frisson. 2001.
Processing ambiguous verbs: Evidence
from eye movements. Journal of
Experimental Psychology: Learning,
Memory, and Cognition, 27:556?573.
553
Computational Linguistics Volume 39, Number 3
Pradhan, Sameer, Edward Loper, Dmitriy
Dligach, and Martha Palmer. 2007.
Semeval-2007 task 17: English lexical
sample, SRL and all words. In 4th
International Workshop on Semantic
Evaluations (SemEval-4) at ACL-2007,
pages 87?92, Prague.
Preiss, Judita and David Yarowsky, editors.
2001. Proceedings of Senseval-2 Second
International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse.
Pustejovsky, James. 1991. The generative
lexicon. Computational Linguistics,
17(4):409?441.
Reddy, Siva, Ioannis P. Klapaftis, Diana
McCarthy, and Suresh Manandhar. 2011.
Dynamic and static prototype vectors for
semantic composition. In Proceedings of The
5th International Joint Conference on Natural
Language Processing 2011 (IJCNLP 2011),
pages 210?218, Chiang Mai.
Reisinger, Joseph and Raymond J. Mooney.
2010. Multi-prototype vector-space models
of word meaning. In Proceedings of Human
Language Technologies: The 11th Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 109?117, Los Angeles, CA.
Resnik, Philip and David Yarowsky. 2000.
Distinguishing systems and distinguishing
senses: New evaluation methods for word
sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Rosch, Eleanor. 1975. Cognitive
representations of semantic categories.
Journal of Experimental Psychology: General,
104:192?233.
Rosch, Eleanor and Carolyn B. Mervis. 1975.
Family resemblance: Studies in the internal
structure of categories. Cognitive
Psychology, 7:573?605.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?123.
Senseval-2. 2001. Web page:
http://www.sle.sharp.co.uk/senseval2.
Snyder, Benjamin and Martha Palmer.
2004. The English all-words task. In
3rd International Workshop on Semantic
Evaluations (SensEval-3) at ACL-2004,
pages 41?43, Barcelona.
Socher, Richard, Eric H. Huang, Jeffrey
Pennin, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for
paraphrase detection. In Advances in
Neural Information Processing Systems 24,
pages 801?809, Grenada.
Stokoe, Christopher. 2005. Differentiating
homonymy and polysemy in information
retrieval. In Proceedings of HLT/EMNLP-05,
pages 403?410, Vancouver.
Taylor, John R. 2003. Linguistic Categorization.
Oxford University Press, New York.
Thater, Stefan, Hagen Fu?rstenau, and
Manfred Pinkal. 2010. Contextualizing
semantic representations using
syntactically enriched vector models.
In Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 948?957, Uppsala.
Tuggy, David H. 1993. Ambiguity, polysemy
and vagueness. Cognitive Linguistics,
4(2):273?290.
Tversky, Amos. 1977. Features of similarity.
Psychological Review, 84(4):327?352.
Tversky, Amos and Itamar Gati. 1982.
Similarity, separability, and the triangle
inequality. Psychological Review,
89(2):123?154.
Van de Cruys, Tim, Thierry Poibeau, and
Anna Korhonen. 2011. Latent vector
weighting for word meaning in context.
In Proceedings of the 2011 Conference
on Empirical Methods in Natural
Language Processing, pages 1,012?1,022,
Edinburgh.
Washtell, Justin. 2010. Expectation vectors:
A semiotics inspired approach to
geometric lexical-semantic representation.
In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language
Semantics, pages 45?50, Uppsala.
Williams, John. 1992. Processing polysemous
words in context: Evidence for interrelated
meanings. Journal of Psycholinguistic
Research, 21:193?218.
Zhong, Zhi and Hwee Tou Ng. 2010. It
makes sense: A wide-coverage word
sense disambiguation system for free
text. In Proceedings of the ACL 2010
System Demonstrations, pages 78?83,
Uppsala.
Zhong, Zhi, Hwee Tou Ng, and Yee Seng
Chan. 2008. Word sense disambiguation
using OntoNotes: An empirical study.
In Proceedings of the 2008 Conference
on Empirical Methods in Natural
Language Processing, pages 1,002?1,010,
Honolulu, HI.
554
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 736?741,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Diathesis alternation approximation for verb clustering
Lin Sun
Greedy Intelligence Ltd
Hangzhou, China
lin.sun@greedyint.com
Diana McCarthy and Anna Korhonen
DTAL and Computer Laboratory
University of Cambridge
Cambridge, UK
diana@dianamccarthy.co.uk
alk23@cam.ac.uk
Abstract
Although diathesis alternations have been
used as features for manual verb clas-
sification, and there is recent work on
incorporating such features in computa-
tional models of human language acquisi-
tion, work on large scale verb classifica-
tion has yet to examine the potential for
using diathesis alternations as input fea-
tures to the clustering process. This pa-
per proposes a method for approximating
diathesis alternation behaviour in corpus
data and shows, using a state-of-the-art
verb clustering system, that features based
on alternation approximation outperform
those based on independent subcategoriza-
tion frames. Our alternation-based ap-
proach is particularly adept at leveraging
information from less frequent data.
1 Introduction
Diathesis alternations (DAs) are regular alterna-
tions of the syntactic expression of verbal argu-
ments, sometimes accompanied by a change in
meaning. For example, The man broke the win-
dow ? The window broke. The syntactic phe-
nomena are triggered by the underlying semantics
of the participating verbs. Levin (1993)?s seminal
book provides a manual inventory both of DAs and
verb classes where membership is determined ac-
cording to participation in these alternations. For
example, most of the COOK verbs (e.g. bake,
cook, fry . . . ) can all take various DAs, such as
the causative alternation, middle alternation and
instrument subject alternation.
In computational linguistics, work inspired by
Levin?s classification has exploited the link be-
tween syntax and semantics for producing clas-
sifications of verbs. Such classifications are use-
ful for a wide variety of purposes such as se-
mantic role labelling (Gildea and Jurafsky, 2002),
predicting unseen syntax (Parisien and Steven-
son, 2010), argument zoning (Guo et al, 2011)
and metaphor identification (Shutova et al, 2010).
While Levin?s classification can be extended man-
ually (Kipper-Schuler, 2005), a large body of re-
search has developed methods for automatic verb
classification since such methods can be applied
easily to other domains and languages.
Existing work on automatic classification relies
largely on syntactic features such as subcatego-
rization frames (SCF)s (Schulte im Walde, 2006;
Sun and Korhonen, 2011; Vlachos et al, 2009;
Brew and Schulte im Walde, 2002). There has also
been some success incorporating selectional pref-
erences (Sun and Korhonen, 2009).
Few have attempted to use, or approximate,
diathesis features directly for verb classification
although manual classifications have relied on
them heavily, and there has been related work on
identifying the DAs themselves automatically us-
ing SCF and semantic information (Resnik, 1993;
McCarthy and Korhonen, 1998; Lapata, 1999;
McCarthy, 2000; Tsang and Stevenson, 2004).
Exceptions to this include Merlo and Stevenson
(2001), Joanis et al (2008) and Parisien and
Stevenson (2010, 2011). Merlo and Stevenson
(2001) used cues such as passive voice, animacy
and syntactic frames coupled with the overlap
of lexical fillers between the alternating slots to
predict a 3-way classification (unergative, unac-
cusative and object-drop). Joanis et al (2008)
used similar features to classify verbs on a much
larger scale. They classify up to 496 verbs us-
ing 11 different classifications each having be-
tween 2 and 14 classes. Parisien and Steven-
son (2010, 2011) used hierarchical Bayesian mod-
els on slot frequency data obtained from child-
directed speech parsed with a dependency parser
to model acquisition of SCF, alternations and ul-
timately verb classes which provided predictions
for unseen syntactic behaviour of class members.
736
Frame Example sentence Freq
NP+PPon Jessica sprayed paint on the wall 40
NP+PPwith Jessica sprayed the wall with paint 30
PPwith *The wall sprayed with paint 0
PPon Jessica sprayed paint on the wall 30
Table 1: Example frames for verb spray
In this paper, like Sun and Korhonen (2009);
Joanis et al (2008) we seek to automatically clas-
sify verbs into a broad range of classes. Like Joa-
nis et al, we include evidence of DA, but we do
not manually select features attributed to specific
alternations but rather experiment with syntactic
evidence for alternation approximation. We use
the verb clustering system presented in Sun and
Korhonen (2009) because it achieves state-of-the-
art results on several datasets, including those of
Joanis et al, even without the additional boost in
performance from the selectional preference data.
We are interested in the improvement that can be
achieved to verb clustering using approximations
for DAs, rather than the DA per se. As such we
make the simple assumption that if a pair of SCFs
tends to occur with the same verbs, we have a po-
tential occurrence of DA. Although this approx-
imation can give rise to false positives (pairs of
frames that co-occur frequently but are not DA)
we are nevertheless interested in investigating its
potential usefulness for verb classification. One
attractive aspect of this method is that it does not
require a pre-defined list of possible alternations.
2 Diathesis Alternation Approximation
A DA can be approximated by a pair of SCFs.
We parameterize frames involving prepositional
phrases with the preposition. Example SCFs for
the verb ?spray? are shown in Table 1. The feature
value of a single frame feature is the frequency
of the SCF. Given two frames fv(i), fv(j) of a
verb v, they can be transformed into a feature pair
(fv(i), fv(j)) as an approximation to a DA. The
feature value of the DA feature (fv(i), fv(j)) is ap-
proximated by the joint probability of the pair of
frames p(fv(i), fv(j)|v), obtained by integrating
all the possible DAs. The key assumption is that
the joint probability of two SCFs has a strong cor-
relation with a DA on the grounds that the DA gives
rise to both SCFs in the pair. We use the DA feature
(fv(i), fv(j)) with its value p(fv(i), fv(j)|v) as a
new feature for verb clustering. As a comparison
point, we can ignore the DA and make a frame in-
dependence assumption. The joint probability is
decomposed as:
p(fv(i), fv(j)|v)? , p(fv(i)|v) ? p(fv(j)|v) (1)
We assume that SCFs are dependent as they are
generated by the underlying meaning components
(Levin and Hovav, 2006). The frame dependency
is represented by a simple graphical model in fig-
ure 1.
Figure 1: Graphical model for the joint probability of pairs of
frames. v represents a verb, a represents a DA and f repre-
sents a specific frame in total of M possible frames
In the data, the verb (v) and frames (f ) are ob-
served, and any underlying alternation (a) is hid-
den. The aim is to approximate but not to detect a
DA, so a is summed out:
p(fv(i), fv(j)|v) =
?
a
p(fv(i), fv(j)|a) ? p(a|v)
(2)
In order to evaluate this sum, we use a relaxation
1: the sum in equation 1 is replaced with the max-
imum (max). This is a reasonable relaxation, as a
pair of frames rarely participates in more than one
type of a DA.
p(fv(i), fv(j)|v) ? max(p(fv(i), fv(j)|a)?p(a|v))
(3)
The second relaxation further relaxes the first one
by replacing the max with the least upper bound
(sup): If fv(i) occurs a times, fv(j) occurs b times
and b < a, the number of times that a DA occurs
between fv(i) and fv(j) must be smaller or equal
to b.
p(fv(i), fv(j)|v) ? sup{p(fv(i), fv(j)|a)} ? sup{p(a|v)}
(4)
sup{p(fv(i), fv(j)|a)} = Z?1 ?min(fv(i), fv(j))
sup{p(a|v)} = 1
Z =
?
m
?
n
min(fv(m), fv(n))
1A relaxation is used in mathematical optimization for re-
laxing the strict requirement, by either substituting it with an
easier requirement or dropping it completely.
737
Frame pair Possible DA Frequency
NP+PPon NP+PPwith Locative 30
NP+PPon PPwith Causative(with) 0
NP+PPon PPon Causative(on) 30
NP+PPwith PPwith ? 0
NP+PPwith PPon ? 30
PPwith PPon ? 0
NP+PPon NP+PPon - 40
NP+PPwith NP+PPwith - 30
PPwith PPwith - 0
PPon PPon - 30
Table 2: Example frame pair features for spray
So we end up with a simple form:
p(fv(i), fv(j)|v) ? Z?1 ?min(fv(i), fv(j)) (5)
The equation is intuitive: If fv(i) occurs 40 times
and fv(j) 30 times, the DA between fv(i) and
fv(j) ? 30 times. This upper bound value is used
as the feature value of the DA feature. The original
feature vector f of dimension M is transformed
into M2 dimensions feature vector f? . Table 2
shows the transformed feature space for spray.
The feature space matches our expectation well:
valid DAs have a value greater than 0 and invalid
DAs have a value of 0.
3 Experiments
We evaluated this model by performing verb clus-
tering experiments using three feature sets:
F1: SCF parameterized with preposition. Exam-
ples are shown in Table 1.
F2: The frame pair features built from F1 with
the frame independence assumption (equa-
tion 1). This feature is not a DA feature as
it ignores the inter-dependency of the frames.
F3: The frame pair features (DAs) built from
F1 with the frame dependency assumption
(equation 4). This is the DA feature which
considers the correlation of the two frames
which are generated from the alternation.
F3 implicitly includes F1, as a frame can pair
with itself. 2 In the example in Table 2, the frame
pair ?PP(on) PP(on)? will always have the same
value as the ?PP(on)? frame in F1.
We extracted the SCFs using the system of
Preiss et al (2007) which classifies each corpus
2We did this so that F3 included the SCF features as well
as the DA approximation features. It would be possible in
future work to exclude the pairs involving identical frames,
thereby relying solely on the DA approximations, and com-
pare performance with the results obtained here.
occurrence of a verb as a member of one of the 168
SCFs on the basis of grammatical relations iden-
tified by the RASP (Briscoe et al, 2006) parser.
We experimented with two datasets that have been
used in prior work on verb clustering: the test sets
7-11 (3-14 classes) in Joanis et al (2008), and the
17 classes set in Sun et al (2008).
We used the spectral clustering (SPEC) method
and settings as in Sun and Korhonen (2009) but
adopted the Bhattacharyya kernel (Jebara and
Kondor, 2003) to improve the computational effi-
ciency of the approach given the high dimension-
ality of the quadratic feature space.
wb(v, v?) =
D?
d=1
(vdv?d)1/2 (6)
The mean-filed bound of the Bhattacharyya kernel
is very similar to the KL divergence kernel (Jebara
et al, 2004) which is frequently used in verb clus-
tering experiments (Korhonen et al, 2003; Sun
and Korhonen, 2009).
To further reduce computational complexity, we
restricted our scope to the more frequent features.
In the experiment described in this section we used
the 50 most frequent features for the 3-6 way clas-
sifications (Joanis et al?s test set 7-9) and 100 fea-
tures for the 7-17 way classifications. In the next
section, we will demonstrate that F3 outperforms
F1 regardless of the feature number setting. The
features are normalized to sum 1.
The clustering results are evaluated using F-
Measure as in Sun and Korhonen (2009) which
provides the harmonic mean of precision (P ) and
recall (R)
P is calculated using modified purity ? a global
measure which evaluates the mean precision of
clusters. Each cluster (ki ? K) is associated
with the gold-standard class to which the major-
ity of its members belong. The number of verbs
in a cluster (ki) that take this class is denoted by
nprevalent(ki).
P =
?
ki?K:nprevalent(ki)>2
nprevalent(ki)
|verbs|
R is calculated using weighted class accuracy:
the proportion of members of the dominant cluster
DOM-CLUSTi within each of the gold-standard
classes ci ? C.
738
Datasets
Joanis et al Sun et al7 8 9 10 11
F1 54.54 49.97 35.77 46.61 38.81 60.03
F2 50.00 49.50 32.79 54.13 40.61 64.00
F3 56.36 53.79 52.90 66.32 50.97 69.62
Table 3: Results when using F3 (DA), F2 (pair of independent
frames) and F1 (single frame) features with Bhattacharyya
kernel on Joanis et al and Sun et al datasets
R =
?|C|
i=1 |verbs in DOM-CLUSTi|
|verbs|
The results are shown in Table 3. The result of
F2 is lower than that of F3, and even lower than
that of F1 for 3-6 way classification. This indi-
cates that the frame independence assumption is
a poor assumption. F3 yields substantially better
result than F2 and F1. The result of F3 is 6.4%
higher than the result (F=63.28) reported in Sun
and Korhonen (2009) using the F1 feature.
This experiment shows, on two datasets, that DA
features are clearly more effective than the frame
features for verb clustering, even when relaxations
are used.
4 Analysis of Feature Frequency
A further experiment was carried out using F1 and
F3 on Joanis et al (2008)?s test sets 10 and 11.
The frequency ranked features were added to the
clustering one at a time, starting from the most
frequent one. The results are shown in figure 2.
F3 outperforms F1 clearly on all the feature num-
ber settings. After adding some highly frequent
frames (22 for test set 10 and 67 for test set 11),
the performance for F1 is not further improved.
The performance of F3, in contrast, is improved
for almost all (including the mid-range frequency)
frames, although to a lesser degree for low fre-
quency frames.
5 Related work
Parisien and Stevenson (2010) introduced a hier-
archical Bayesian model capable of learning verb
alternations and constructions from syntactic in-
put. The focus was on modelling and explaining
the child alternation acquisition rather than on au-
tomatic verb classification. Therefore, no quanti-
tative evaluation of the clustering is reported, and
the number of verbs under the novel verb gen-
eralization test is relatively small. Parisien and
1 22 1000.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
1 20 1.53416
347
3476
34?
34?6
34.
34.6
346
3466
??? ? ?? ? ????? ?1?1??
Figure 2: Comparison between frame features (F1) and DA
features (F3) with different feature number settings. DA fea-
tures clearly outperform frame features. The top figure is the
result on test set 10 (8 ways). The bottom figure is the result
on test set 11 (14 ways). The x axis is the number of features.
The y axis is the F-Measure result.
Stevenson (2011) extended this work by adding
semantic features.
Parisien and Stevenson?s (2010) model 2 has a
similar structure to the graphic model in figure 1.
A fundamental difference is that we explicitly use
a probability distribution over alternations (pair of
frames) to represent a verb, whereas they represent
a verb by a distribution over the observed frames
similar to Vlachos et al (2009) ?s approach. Also
the parameters in their model were inferred by
Gibbs sampling whereas we avoided this inference
step by using relaxation.
6 Conclusion and Future work
We have demonstrated the merits of using DAs for
verb clustering compared to the SCF data from
which they are derived on standard verb classi-
fication datasets and when integrated in a state-
of-the-art verb clustering system. We have also
demonstrated that the performance of frame fea-
tures is dominated by the high frequency frames.
In contrast, the DA features enable the mid-range
frequency frames to further improve the perfor-
mance.
739
In the future, we plan to evaluate the perfor-
mance of DA features in a larger scale experiment.
Due to the high dimensionality of the transformed
feature space (quadratic of the original feature
space), we will need to improve the computational
efficiency further, e.g. via use of an unsupervised
dimensionality reduction technique Zhao and Liu
(2007). Moreover, we plan to use Bayesian in-
ference as in Vlachos et al (2009); Parisien and
Stevenson (2010, 2011) to infer the actual param-
eter values and avoid the relaxation.
Finally, we plan to supplement the DA feature
with evidence from the slot fillers of the alternat-
ing slots, in the spirit of earlier work (McCarthy,
2000; Merlo and Stevenson, 2001; Joanis et al,
2008). Unlike these previous works, we will use
selectional preferences to generalize the argument
heads but will do so using preferences from dis-
tributional data (Sun and Korhonen, 2009) rather
than WordNet, and use all argument head data in
all frames. We envisage using maximum average
distributional similarity of the argument heads in
any potentially alternating slots in a pair of co-
occurring frames as a feature, just as we currently
use the frequency of the less frequent co-occurring
frame.
Acknowledgement
Our work was funded by the Royal Society
University Research Fellowship (AK) and the
Dorothy Hodgkin Postgraduate Award (LS).
References
C. Brew and S. Schulte im Walde. Spectral clus-
tering for German verbs. In Proceedings of
EMNLP, 2002.
E. Briscoe, J. Carroll, and R. Watson. The second
release of the RASP system. In Proceedings
of the COLING/ACL on Interactive presentation
sessions, pages 77?80, 2006.
D. Gildea and D. Jurafsky. Automatic labeling
of semantic roles. Computational Linguistics,
28(3):245?288, 2002.
Y. Guo, A. Korhonen, and T. Poibeau. A
weakly-supervised approach to argumentative
zoning of scientific documents. In Proceedings
of EMNLP, pages 273?283, Stroudsburg, PA,
USA, 2011. ACL.
T. Jebara and R. Kondor. Bhattacharyya and ex-
pected likelihood kernels. In Learning Theory
and Kernel Machines: 16th Annual Conference
on Learning Theory and 7th Kernel Workshop,
page 57. Springer, 2003.
T. Jebara, R. Kondor, and A. Howard. Probability
product kernels. The Journal of Machine Learn-
ing Research, 5:819?844, 2004.
E. Joanis, S. Stevenson, and D. James. A general
feature space for automatic verb classification.
Natural Language Engineering, 2008.
K. Kipper-Schuler. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, Com-
puter and Information Science Dept., University
of Pennsylvania, Philadelphia, PA, June 2005.
A. Korhonen, Y. Krymolowski, and Z. Marx.
Clustering polysemic subcategorization frame
distributions semantically. In Proceedings of
ACL, pages 64?71, Morristown, NJ, USA,
2003. ACL.
M. Lapata. Acquiring lexical generalizations from
corpora: A case study for diathesis alternations.
In Proceedings of ACL, pages 397?404. ACL
Morristown, NJ, USA, 1999.
B. Levin and M. Hovav. Argument realiza-
tion. Computational Linguistics, 32(3):447?
450, 2006.
B. Levin. English Verb Classes and Alterna-
tions: a preliminary investigation. University
of Chicago Press, Chicago and London, 1993.
D. McCarthy and A. Korhonen. Detecting verbal
participation in diathesis alternations. In Pro-
ceedings of ACL, volume 36, pages 1493?1495.
ACL, 1998.
D. McCarthy. Using semantic preferences to iden-
tify verbal participation in role switching alter-
nations. In Proceedings of NAACL, pages 256?
263. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA, 2000.
P. Merlo and S. Stevenson. Automatic verb clas-
sification based on statistical distributions of ar-
gument structure. Computational Linguistics,
27(3):373?408, 2001.
C. Parisien and S. Stevenson. Learning verb al-
ternations in a usage-based Bayesian model. In
Proceedings of the 32nd annual meeting of the
Cognitive Science Society, 2010.
C. Parisien and S. Stevenson. Generalizing be-
tween form and meaning using learned verb
classes. In Proceedings of the 33rd Annual
Meeting of the Cognitive Science Society, 2011.
740
J. Preiss, T. Briscoe, and A. Korhonen. A system
for large-scale acquisition of verbal, nominal
and adjectival subcategorization frames from
corpora. In Proceedings of ACL, volume 45,
page 912, 2007.
P. Resnik. Selection and Information: A Class-
Based Approach to Lexical Relationships. PhD
thesis, University of Pennsylvania, 1993.
S. Schulte im Walde. Experiments on the au-
tomatic induction of German semantic verb
classes. Computational Linguistics, 32(2):159?
194, 2006.
E. Shutova, L. Sun, and A. Korhonen. Metaphor
identification using verb and noun clustering.
In Proceedings of COLING, pages 1002?1010.
ACL, 2010.
L. Sun and A. Korhonen. Improving verb clus-
tering with automatically acquired selectional
preferences. In Proceedings of EMNLP, pages
638?647, 2009.
L. Sun and A. Korhonen. Hierarchical verb clus-
tering using graph factorization. In Proceedings
of EMNLP, pages 1023?1033, Edinburgh, Scot-
land, UK., July 2011. ACL.
L. Sun, A. Korhonen, and Y. Krymolowski. Verb
class discovery from rich syntactic data. Lecture
Notes in Computer Science, 4919:16, 2008.
V. Tsang and S. Stevenson. Using selectional
profile distance to detect verb alternations. In
HLT/NAACL 2004 Workshop on Computational
Lexical Semantics, 2004.
A. Vlachos, A. Korhonen, and Z. Ghahramani.
Unsupervised and constrained dirichlet process
mixture models for verb clustering. In Proceed-
ings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 74?82,
2009.
Z. Zhao and H. Liu. Spectral feature selection
for supervised and unsupervised learning. In
Proceedings of ICML, pages 1151?1157, New
York, NY, USA, 2007. ACM.
741
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259?270,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
Jey Han Lau,
?
Paul Cook,
?
Diana McCarthy,
?
Spandana Gella,
?
and Timothy Baldwin
?
? Dept of Philosophy, King?s College London
? Dept of Computing and Information Systems, The University of Melbourne
? University of Cambridge
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
Abstract
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren?t attested
in the corpus, and identifying novel senses
in the corpus which aren?t captured in the
sense inventory.
1 Introduction
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al, 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions ? and predominant senses too ?
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al, 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al, 2007; Li et
al., 2010; Lau et al, 2012; Preiss and Stevenson,
2013; Cai et al, 2007; Knopp et al, 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses ? i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
259
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses ? i.e. senses that are
attested in the corpus but not in the sense inven-
tory ? would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al, 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
2 Background and Related Work
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al, 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al, 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al, 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al, 2007). Mc-
Carthy et al (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al, 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (?corroborators?), each of which, in turn,
depends on the document?s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al
260
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al, 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al, 2012). In a similar vein,
Peirsman et al (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
3 Methodology
Our methodology is based on the WSI system
described in Lau et al (2012),
1
which has been
shown (Lau et al, 2012; Lau et al, 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al, 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.
2
Note that in
their original work, Lau et al (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
1
Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2
This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
marginal, we make no use of parser-based features
in this paper.
3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.
4
To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.
5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic?
sense alignment methodology with portability in
mind ? it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.
6
We then calculate the Jensen?
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense s
i
and topic t
j
is:
sim(s
i
, t
j
) = 1? JS(S?T ) (1)
where S and T are the multinomial distributions
3
For hyper-parameters ? and ?, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al (2006).
4
To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5
The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6
Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al, 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
261
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
over words for sense s
i
and topic t
j
, respectively,
and JS(X?Y ) is the Jensen?Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(s
i
, t
j
)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense s
i
is given as follows:
prevalence(s
i
) =
T
?
j
(sim(s
i
, t
j
)? P (t
j
)) (2)
=
T
?
j
(
sim(s
i
, t
j
)?
f(t
j
)
?
T
k
f(t
k
)
)
where f(t
j
) is the frequency of topic t
j
(i.e. the
number of usages assigned to topic t
j
), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
4 WordNet Experiments
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) Acc
UB
, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);
7
and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (Acc
UB
), calculated as follows:
ERR = 1?
Acc
UB
? Acc
Acc
UB
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar?s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p < 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p > 0.1). Note
that there is still much room for improvement with
7
The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
262
Dataset
FS
CORPUS
MKWC HDP-WSI
Acc
UB
Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
Table 2: WSD accuracy for MKWC and HDP-WSI
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FS
CORPUS
upper bound] is indicated in boldface).
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
Table 3: Sense distribution evaluation of MKWC
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al, 2009). To this
end, we introduce a second evaluation metric:
the Jensen?Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p < 0.05) but the results
for the other two datasets are not (p > 0.1 in each
case).
Dataset
FS
CORPUS
FS
DICT
HDP-WSI
Acc
UB
Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FS
CORPUS
upper bound] is indicated in boldface).
Dataset FS
CORPUS
FS
DICT
HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
Table 5: Sense distribution evaluation of HDP-
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.
8
The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al, 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
?flat? sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al, 2007) com-
pared to a dependency one,
9
it is not stated how
8
McCarthy et al (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
9
The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/
?
lindek/downloads.htm.
263
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
5 Macmillan Experiments
In our second set of experiments, we move to a
new dataset (Gella et al, to appear) based on text
from ukWaC (Ferraresi et al, 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary
10
(henceforth ?Macmillan?). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.
11
In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al, 2004; Navigli et al, 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3?Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al, 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
?Other? in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
10
http://www.macmillandictionary.com/
11
Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly ?Other?), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
5.1 Learning Sense Distributions
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (?FS
CORPUS
?), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (?FS
DICT
?), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FS
DICT
) over both datasets (McNe-
mar?s test; p < 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.
12
This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12
The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al (2005) is comparable to UKWAC.
264
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),
13
making
the task slightly easier in the Macmillan case.
5.2 Identification of Unattested Senses
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren?t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren?t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
st-affinity(s
i
) =
?
T
j
sim(s
i
, t
j
)
?
S
k
?
T
l
sim(s
k
, t
l
)
(3)
where sim(s
i
, t
j
) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13
Note that the set of lemmas differs between the respec-
tive datasets, so this isn?t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,
14
evaluated using
sense-level precision (P ), recall (R) and F-score
(F ) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092? 0.044 over UKWAC
and 0.125?0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
5.3 Identification of Novel Senses
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn?t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these ?novel senses?, and define ?novel
sense identification? to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren?t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn?t ob-
served in the corpus. Also, while we have annota-
tions of ?Other? usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can?t use the
?Other? annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
14
We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
265
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
20 0.0?0.2 0.052?0.009 0.35 0.42 0.36
9 0.2?0.4 0.089?0.024 0.24 0.59 0.29
6 0.4?0.6 0.061?0.004 0.63 0.64 0.63
Table 7: Classification of usages with novel sense for all target lemmas.
No. Lemmas with Relative Freq Threshold
P R F
a Removed Sense of Removed Sense Mean?stdev
9 0.2?0.4 0.093?0.023 0.50 0.66 0.52
6 0.4?0.6 0.099?0.018 0.73 0.90 0.80
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0?0.2; medium = 0.2?
0.4; and high = 0.4?0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if > 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al, 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
ts-affinity(t
j
) =
?
S
i
sim(s
i
, t
j
)
?
T
l
?
S
k
sim(s
k
, t
l
)
(4)
where, once again, sim(s
i
, t
j
) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances ? instances from target lem-
mas that have a sense artificially removed and
those that do not ? impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
266
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0?0.2 0.4543
9 11 0.2?0.4 0.0391
6 14 0.4?0.6 0.0247
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
novelty(w) = min
t
j
(
max
s
i
sim(s
i
, t
j
)
f(t
j
)
)
(5)
where f(t
j
) is the frequency of topic t
j
in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as a low-probability topic for
a high-frequency word, or a high-probability topic
for a low-frequency word) are indicative of a novel
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of a one-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
novel sense) are statistically different.
15
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p < 0.05), but the results are less promising for
the low-frequency novel senses.
6 Discussion
Our methodologies for the two proposed tasks of
identifying unused and novel senses are simple
15
Note that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas in the dataset.
extensions to demonstrate the flexibility and ro-
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of sim(s
i
, t
j
) for com-
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
A natural next step for this research would be to
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised WSD of individual token
occurrences of a given word.
In summary, we have proposed a topic
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory.
Acknowledgements
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian Re-
search Council.
267
References
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25?32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7?12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33?41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356?364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136?145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277?281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024?1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103?
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249?252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum?e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435?1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010?
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89?96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28?34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49?65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128?135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47?54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67?71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
268
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57?60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561?568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics ? Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233?
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290?299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V?olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97?103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419?
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45?75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591?601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307?311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217?221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24?26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138?1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768?774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25?30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63?68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220?1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280?287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553?590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303?308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
269
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121?128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193?
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075?1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30?35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49?56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469?491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680?684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga?etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257?282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104?
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566?1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10?14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78?83, Uppsala, Sweden.
270
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 9?14,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
Lexical Computing Ltd.
diana@dianamccarthy.co.uk
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution
task, where given an English target word
in context, participating systems had to
find an alternative substitute word or
phrase in Spanish. The task is based on
the English Lexical Substitution task run
at SemEval-2007. In this paper we pro-
vide background and motivation for the
task, we describe the data annotation pro-
cess and the scoring system, and present
the results of the participating systems.
1 Introduction
In the Cross-Lingual Lexical Substitution task, an-
notators and systems had to find an alternative
substitute word or phrase in Spanish for an En-
glish target word in context. The task is based
on the English Lexical Substitution task run at
SemEval-2007, where both target words and sub-
stitutes were in English.
An automatic system for cross-lingual lexical
substitution would be useful for a number of ap-
plications. For instance, such a system could be
used to assist human translators in their work, by
providing a number of correct translations that the
human translator can choose from. Similarly, the
system could be used to assist language learners,
by providing them with the interpretation of the
unknown words in a text written in the language
they are learning. Last but not least, the output
of a cross-lingual lexical substitution system could
be used as input to existing systems for cross-
language information retrieval or automatic ma-
chine translation.
2 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained
alternative translation records of typical usages of
a test word, also referred to as a translation mem-
ory. Systems could either select the most appro-
priate translation memory record for each instance
and were scored against a gold-standard set of an-
notations, or they could provide a translation that
was scored by translation experts after the results
were submitted. In contrast to this work, in our
task we provided actual translations for target in-
stances in advance, rather than predetermine trans-
lations using lexicographers or rely on post-hoc
evaluation, which does not permit evaluation of
new systems after the competition.
Previous standalone WSD tasks based on par-
allel data have obtained distinct translations for
senses as listed in a dictionary (Ng and Chan,
2007). In this way fine-grained senses with the
same translations can be lumped together, how-
ever this does not fully allow for the fact that some
senses for the same words may have some transla-
tions in common but also others that are not (Sinha
et al, 2009).
In our task, we collected a dataset which al-
lows instances of the same word to have some
translations in common, while not necessitating
a clustering of translations from a specific re-
source into senses (in comparison to Lefever and
Hoste (2010)). 1 Resnik and Yarowsky (2000) also
1Though in that task note that it is possible for a transla-
tion to occur in more than one cluster. It will be interesting to
9
conducted experiments using words in context,
rather than a predefined sense-inventory however
in these experiments the annotators were asked for
a single preferred translation. In our case, we al-
lowed annotators to supply as many translations
as they felt were equally valid. This allows us
to examine more subtle relationships between us-
ages and to allow partial credit to systems that
get a close approximation to the annotators? trans-
lations. Unlike a full blown machine translation
task (Carpuat and Wu, 2007), annotators and sys-
tems are not required to translate the whole context
but just the target word.
3 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter
referred to as LEXSUB) was run at SemEval-
2007 (McCarthy and Navigli, 2007; McCarthy and
Navigli, 2009). LEXSUB was proposed as a task
which, while requiring contextual disambiguation,
did not presuppose a specific sense inventory. In
fact, it is quite possible to use alternative rep-
resentations of meaning, such as those proposed
by Schu?tze (1998) and Pantel and Lin (2002).
The motivation for a substitution task was that
it would reflect capabilities that might be useful
for natural language processing tasks such as para-
phrasing and textual entailment, while not requir-
ing a complete system that might mask system ca-
pabilities at a lexical level and make participation
in the task difficult for small research teams.
The task required systems to produce a substi-
tute word for a word in context. The data was
collected for 201 words from open class parts-of-
speech (PoS) (i.e. nouns, verbs, adjectives and ad-
verbs). Words were selected that have more than
one meaning with at least one near synonym. Ten
sentences for each word were extracted from the
English Internet Corpus (Sharoff, 2006). There
were five annotators who annotated each target
word as it occurred in the context of a sentence.
The annotators were each allowed to provide up to
three substitutes, though they could also provide
a NIL response if they could not come up with a
substitute. They had to indicate if the target word
was an integral part of a multiword.
see the extent that this actually occurred in their data and the
extent that the translations that our annotators provided might
be clustered.
4 The Cross-Lingual Lexical
Substitution Task
The Cross-Lingual Lexical Substitution task fol-
lows LEXSUB except that the annotations are
translations rather than paraphrases. Given a tar-
get word in context, the task is to provide several
correct translations for that word in a given lan-
guage. We used English as the source language
and Spanish as the target language.
We provided both development and test sets, but
no training data. As for LEXSUB, any systems re-
quiring training data had to obtain it from other
sources. We included nouns, verbs, adjectives and
adverbs in both development and test data. We
used the same set of 30 development words as in
LEXSUB, and a subset of 100 words from the LEX-
SUB test set, selected so that they exhibit a wide
variety of substitutes. For each word, the same ex-
ample sentences were used as in LEXSUB.
4.1 Annotation
We used four annotators for the task, all native
Spanish speakers from Mexico, with a high level
of proficiency in English. As in LEXSUB, the an-
notators were allowed to use any resources they
wanted to, and were required to provide as many
substitutes as they could think of.
The inter-tagger agreement (ITA) was calcu-
lated as pairwise agreement between sets of sub-
stitutes from annotators, as done in LEXSUB. The
ITA without mode was determined as 0.2777,
which is comparable with the ITA of 0.2775 de-
termined for LEXSUB.
4.2 An Example
One significant outcome of this task is that there
are not necessarily clear divisions between usages
and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This means that there can be
usages that overlap to different extents with each
other but do not have identical translations. An
example is the target adverb severely. Four sen-
tences are shown in Figure 1 with the translations
provided by one annotator marked in italics and
{} braces. Here, all the token occurrences seem
related to each other in that they share some trans-
lations, but not all. There are sentences like 1
and 2 that appear not to have anything in com-
mon. However 1, 3, and 4 seem to be partly re-
lated (they share severamente), and 2, 3, and 4 are
also partly related (they share seriamente). When
10
we look again, sentences 1 and 2, though not di-
rectly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We adopted the best and out-of-ten precision and
recall scores from LEXSUB (oot in the equations
below). The systems were allowed to supply as
many translations as they feel fit the context. The
system translations are then given credit depend-
ing on the number of annotators that picked each
translation. The credit is divided by the number
of annotator responses for the item and since for
the best score the credit for the system answers
for an item is also divided by the number of an-
swers the system provides, this allows more credit
to be given to instances where there is less varia-
tion. For that reason, a system is better guessing
the translation that is most frequent unless it re-
ally wants to hedge its bets. Thus if i is an item
in the set of instances I , and T
i
is the multiset of
gold standard translations from the human annota-
tors for i, and a system provides a set of answers
S
i
for i, then the best score for item i is2:
best score(i) =
?
s?S
i
frequency(s ? T
i
)
|S
i
| ? |T
i
|
(1)
Precision is calculated by summing the scores
for each item and dividing by the number of items
that the system attempted whereas recall divides
the sum of scores for each item by |I|. Thus:
best precision =
?
i
best score(i)
|i ? I : defined(S
i
)|
(2)
best recall =
?
i
best score(i)
|I|
(3)
The out-of-ten scorer allows up to ten system
responses and does not divide the credit attributed
to each answer by the number of system responses.
This allows a system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select
a perfectly good translation that the annotators had
not thought of. By allowing up to ten translations
in the out-of-ten task the systems can hedge their
bets to find the translations that the annotators sup-
plied.
2NB scores are multiplied by 100, though for out-of-ten
this is not strictly a percentage.
oot score(i) =
?
s?S
i
frequency(s ? T
i
)
|T
i
|
(4)
oot precision =
?
i
oot score(i)
|i ? I : defined(S
i
)|
(5)
oot recall =
?
i
oot score(i)
|I|
(6)
We note that there was an issue that the origi-
nal LEXSUB out-of-ten scorer allowed duplicates
(McCarthy and Navigli, 2009). The effect of du-
plicates is that systems can get inflated scores be-
cause the credit for each item is not divided by the
number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (2009) describe this oversight, identify the
systems that had included duplicates and explain
the implications. For our task, we decided to con-
tinue to allow for duplicates, so that systems can
boost their scores with duplicates on translations
with higher probability.
For both the best and out-of-ten measures, we
also report a mode score, which is calculated
against the mode from the annotators responses as
was done in LEXSUB. Unlike the LEXSUB task,
we did not run a separate multi-word subtask and
evaluation.
5 Baselines and Upper bound
To place results in perspective, several baselines as
well as the upper bound were calculated.
5.1 Baselines
We calculated two baselines, one dictionary-based
and one dictionary and corpus-based. The base-
lines were produced with the help of an on-
line Spanish-English dictionary3 and the Spanish
Wikipedia. For the first baseline, denoted by DICT,
for all target words, we collected all the Spanish
translations provided by the dictionary, in the or-
der returned on the online query page. The best
baseline was produced by taking the first transla-
tion provided by the online dictionary, while the
out-of-ten baseline was produced by taking the
first 10 translations provided.
The second baseline, DICTCORP, also ac-
counted for the frequency of the translations
within a Spanish dictionary. All the translations
3www.spanishdict.com
11
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely
stressed by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now.
{altamente, seriamente, exageradamente}
Figure 1: Translations from one annotator for the adverb severely
provided by the online dictionary for a given target
word were ranked according to their frequencies in
the Spanish Wikipedia, producing the DICTCORP
baseline.
5.2 Upper bound
The results for the best task reflect the inherent
variability as less credit is given where annotators
express differences. The theoretical upper bound
for the best recall (and precision if all items are
attempted) score is calculated as:
best
ub
=
?
i?I
freq
most freq substitute
i
|T
i
|
|I|
? 100
= 40.57 (7)
Note of course that this upper bound is theoretical
and assumes a human could find the most frequent
substitute selected by all annotators. Performance
of annotators will undoubtedly be lower than the
theoretical upper bound because of human vari-
ability on this task. Since we allow for duplicates,
the out-of-ten upper bound assumes the most fre-
quent word type in T
i
is selected for all ten an-
swers. Thus we would obtain ten times the best
upper bound (equation 7).
oot
ub
=
?
i?I
freq
most freq substitute
i
?10
|T
i
|
|I|
? 100
= 405.78 (8)
If we had not allowed duplicates then the out-
of-ten upper bound would have been just less than
100% (99.97). This is calculated by assuming the
top 10 most frequent responses from the annota-
tors are picked in every case. There are only a cou-
ple of cases where there are more than 10 transla-
tions from the annotators.
6 Systems
Nine teams participated in the task, and several
of them entered two systems. The systems used
various resources, including bilingual dictionar-
ies, parallel corpora such as Europarl or corpora
built from Wikipedia, monolingual corpora such
as Web1T or newswire collections, and transla-
tion software such as Moses, GIZA or Google.
Some systems attempted to select the substitutes
on the English side, using a lexical substitu-
tion framework or word sense disambiguation,
whereas some systems made the selection on the
Spanish side using lexical substitution in Spanish.
In the following, we briefly describe each par-
ticipating system.
CU-SMT relies on a phrase-based statistical ma-
chine translation system, trained on the Europarl
English-Spanish parallel corpora.
The UvT-v and UvT-g systems make use of k-
nearest neighbour classifiers to build one word ex-
pert for each target word, and select translations
on the basis of a GIZA alignment of the Europarl
parallel corpus.
The UBA-T and UBA-W systems both use can-
didates from Google dictionary, SpanishDict.com
and Babylon, which are then confirmed using par-
allel texts. UBA-T relies on the automatic trans-
lation of the source sentence using the Google
Translation API, combined with several heuristics.
The UBA-W system uses a parallel corpus auto-
matically constructed from DBpedia.
SWAT-E and SWAT-S use a lexical substitution
framework applied to either English or Spanish.
The SWAT-E system first performs lexical sub-
12
stitution in English, and then each substitute is
translated into Spanish. SWAT-S translates the
source sentences into Spanish, identifies the Span-
ish word corresponding to the target word, and
then it performs lexical substitution in Spanish.
TYO uses an English monolingual substitution
module, and then it translates the substitution can-
didates into Spanish using the Freedict and the
Google English-Spanish dictionary.
FCC-LS uses the probability of a word to be
translated into a candidate based on estimates ob-
tained from the GIZA alignment of the Europarl
corpus. These translations are subsequently fil-
tered to include only those that appear in a trans-
lation of the target word using Google translate.
WLVUSP determines candidates using the best
N translations of the test sentences obtained with
the Moses system, which are further filtered us-
ing an English-Spanish dictionary. USPWLV uses
candidates from an alignment of Europarl, which
are then selected using various features and a clas-
sifier tuned on the development data.
IRST-1 generates the best substitute using a PoS
constrained alignment of Moses translations of the
source sentences, with a back-off to a bilingual
dictionary. For out-of-ten, dictionary translations
are filtered using the LSA similarity between can-
didates and the sentence translation into Spanish.
IRSTbs is intended as a baseline, and it uses only
the PoS constrained Moses translation for best,
and the dictionary translations for out-of-ten.
ColEur and ColSlm use a supervised word sense
disambiguation algorithm to distinguish between
senses in the English source sentences. Trans-
lations are then assigned by using GIZA align-
ments from a parallel corpus, collected for the
word senses of interest.
7 Results
Tables 1 and 2 show the precision P and recall
R for the best and out-of-ten tasks respectively,
for normal and mode. The rows are ordered by
R. The out-of-ten systems were allowed to pro-
vide up to 10 substitutes and did not have any ad-
vantage by providing less. Since duplicates were
allowed so that a system can put more emphasis
on items it is more confident of, this means that
out-of-ten R and P scores might exceed 100%
because the credit for each of the human answers
is used for each of the duplicates (McCarthy and
Navigli, 2009). Duplicates will not help the mode
scores, and can be detrimental as valuable guesses
which would not be penalised are taken up with
Systems R P Mode R Mode P
UBA-T 27.15 27.15 57.20 57.20
USPWLV 26.81 26.81 58.85 58.85
ColSlm 25.99 27.59 56.24 59.16
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 20.56 21.62 44.58 45.01
UBA-W 19.68 19.68 39.09 39.09
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 18.15 19.47 37.72 40.03
IRST-1 15.38 22.16 33.47 45.95
IRSTbs 13.21 22.51 28.26 45.27
TYO 8.39 8.62 14.95 15.31
DICT 24.34 24.34 50.34 50.34
DICTCORP 15.09 15.09 29.22 29.22
Table 1: best results
duplicates. In table 2, in the column marked dups,
we display the number of test items for which at
least one duplicate answer was provided. 4 Al-
though systems were perfectly free to use dupli-
cates, some may not have realised this. 5 Dupli-
cates help when a system is fairly confident of a
subset of its 10 answers.
We had anticipated a practical issue to come up
with all participants, which is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. While we were
counting on the participants to clean their files and
provide us with clean characters only, we ended up
with result files following different encodings (e.g,
UTF-8, ANSI), some of them including diacrit-
ics, and some of them containing malformed char-
acters. We were able to perform a basic cleaning
of the files, and transform the diacritics into their
diacriticless counterparts, however it was not pos-
sible to clean all the malformed characters without
a significant manual effort that was not possible
due to time constraints. As a result, a few of the
participants ended up losing a few points because
their translations, while being correct, contained
an invalid, malformed character that was not rec-
ognized as correct by the scorer.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6
4Please note that any residual character encoding issues
were not considered by the scorer and so the number of du-
plicates may be slightly higher than if diacritics/different en-
codings had been considered.
5Also, note that some systems did not supply 10 transla-
tions. Their scores would possibly have improved if they had
done so.
6There is not a big difference between P and R because
13
Systems R P Mode R Mode P dups
SWAT-E 174.59 174.59 66.94 66.94 968
SWAT-S 97.98 97.98 79.01 79.01 872
UvT-v 58.91 58.91 62.96 62.96 345
UvT-g 55.29 55.29 73.94 73.94 146
UBA-W 52.75 52.75 83.54 83.54 -
WLVUSP 48.48 48.48 77.91 77.91 64
UBA-T 47.99 47.99 81.07 81.07 -
USPWLV 47.60 47.60 79.84 79.84 30
ColSlm 43.91 46.61 65.98 69.41 509
ColEur 41.72 44.77 67.35 71.47 125
TYO 34.54 35.46 58.02 59.16 -
IRST-1 31.48 33.14 55.42 58.30 -
FCC-LS 23.90 23.90 31.96 31.96 308
IRSTbs 8.33 29.74 19.89 64.44 -
DICT 44.04 44.04 73.53 73.53 30
DICTCORP 42.65 42.65 71.60 71.60 -
Table 2: out-of-ten results
UBA-T has the highest ranking on R for best. US-
PWLV is best at finding the mode, for best how-
ever the UBA-W and UBA-T systems (particularly
the former) both have exceptional performance for
finding the mode in the out-of-ten task, though
note that SWAT-S performs competitively given
that its duplicate responses will reduce its chances
on this metric. SWAT-E is the best system for out-
of-ten, as several of the items that were empha-
sized through duplication were also correct.
The results are much higher than for LEX-
SUB (McCarthy and Navigli, 2007). There are sev-
eral possible causes for this. It is perhaps easier
for humans, and machines to come up with trans-
lations compared to paraphrases. Though the ITA
figures are comparable on both tasks, our task con-
tained only a subset of the data in LEXSUB and we
specifically avoided data where the LEXSUB an-
notators had not been able to come up with a sub-
stitute or had labelled the instance as a name e.g.
measurements such as pound, yard or terms such
as mad in mad cow disease. Another reason for
this difference may be that there are many parallel
corpora available for training a system for this task
whereas that was not the case for LEXSUB.
8 Conclusions
In this paper we described the SemEval-2010
cross-lingual lexical substitution task, including
the motivation behind the task, the annotation pro-
cess and the scoring system, as well as the partic-
ipating systems. Nine different teams with a total
systems typically supplied answers for most items. However,
IRST-1 and IRSTbs did considerably better on precision com-
pared to recall since they did not cover all test items.
of 15 different systems participated in the task, us-
ing a variety of resources and approaches. Com-
parative evaluations using different metrics helped
determine what works well for the selection of
cross-lingual lexical substitutes.
9 Acknowledgements
The work of the first and second authors has been partially
supported by a National Science Foundation CAREER award
#0747340. The work of the third author has been supported
by a Royal Society UK Dorothy Hodgkin Fellowship. The
authors are grateful to Samer Hassan for his help with the
annotation interface.
References
Marine Carpuat and Dekai Wu. 2007. Improving statis-
tical machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL
2007), pages 61?72, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese translation
task. In Proceedings of the SENSEVAL-2 workshop, pages
37?44.
Els Lefever and Veronique Hoste. 2010. SemEval-2007 task
3: Cross-lingual word sense disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic Eval-
uations (SemEval-2010), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 48?53, Prague, Czech Re-
public.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Eval-
uation Special Issue on Computational Semantic Analysis
of Language: SemEval-2007 and Beyond, 43(2):139?159.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-2007 task
11: English lexical sample task via English-Chinese paral-
lel text. In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), pages 54?58,
Prague, Czech Republic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, pages
613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea. 2009.
Semeval-2010 task 2: Cross-lingual lexical substitution.
In Proceedings of the NAACL-HLT Workshop SEW-2009
- Semantic Evaluations: Recent Achievements and Future
Directions, Boulder, Colorado, USA.
14
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 387?391,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
IIITH: Domain Specific Word Sense Disambiguation
Siva Reddy
IIIT Hyderabad
India
gvsreddy@students.iiit.ac.in
Diana McCarthy
Lexical Computing Ltd.
United Kingdom
diana@dianamccarthy.co.uk
Abhilash Inumella
IIIT Hyderabad
India
abhilashi@students.iiit.ac.in
Mark Stevenson
University of Sheffield
United Kingdom
m.stevenson@dcs.shef.ac.uk
Abstract
We describe two systems that participated
in SemEval-2010 task 17 (All-words Word
Sense Disambiguation on a Specific Do-
main) and were ranked in the third and
fourth positions in the formal evaluation.
Domain adaptation techniques using the
background documents released in the
task were used to assign ranking scores to
the words and their senses. The test data
was disambiguated using the Personalized
PageRank algorithm which was applied
to a graph constructed from the whole of
WordNet in which nodes are initialized
with ranking scores of words and their
senses. In the competition, our systems
achieved comparable accuracy of 53.4 and
52.2, which outperforms the most frequent
sense baseline (50.5).
1 Introduction
The senses in WordNet are ordered according to
their frequency in a manually tagged corpus, Sem-
Cor (Miller et al, 1993). Senses that do not oc-
cur in SemCor are ordered arbitrarily after those
senses of the word that have occurred. It is known
from the results of SENSEVAL2 (Cotton et al,
2001) and SENSEVAL3 (Mihalcea and Edmonds,
2004) that first sense heuristic outperforms many
WSD systems (see McCarthy et al (2007)). The
first sense baseline?s strong performance is due to
the skewed frequency distribution of word senses.
WordNet sense distributions based on SemCor are
clearly useful, however in a given domain these
distributions may not hold true. For example, the
first sense for ?bank? in WordNet refers to ?slop-
ing land beside a body of river? and the second
to ?financial institution?, but in the domain of ?fi-
nance? the ?financial institution? sense would be
expected to be more likely than the ?sloping land
beside a body of river? sense. Unfortunately, it
is not feasible to produce large manually sense-
annotated corpora for every domain of interest.
McCarthy et al (2004) propose a method to pre-
dict sense distributions from raw corpora and use
this as a first sense heuristic for tagging text with
the predominant sense. Rather than assigning pre-
dominant sense in every case, our approach aims
to use these sense distributions collected from do-
main specific corpora as a knowledge source and
combine this with information from the context.
Our approach focuses on the strong influence of
domain for WSD (Buitelaar et al, 2006) and the
benefits of focusing on words salient to the do-
main (Koeling et al, 2005). Words are assigned
a ranking score based on its keyness (salience) in
the given domain. We use these word scores as
another knowledge source.
Graph based methods have been shown to
produce state-of-the-art performance for unsu-
pervised word sense disambiguation (Agirre and
Soroa, 2009; Sinha and Mihalcea, 2007). These
approaches use well-known graph-based tech-
niques to find and exploit the structural properties
of the graph underlying a particular lexical knowl-
edge base (LKB), such as WordNet. These graph-
based algorithms are appealing because they take
into account information drawn from the entire
graph as well as from the given context, making
them superior to other approaches that rely only
on local information individually derived for each
word.
Our approach uses the Personalized PageRank
algorithm (Agirre and Soroa, 2009) over a graph
387
representing WordNet to disambiguate ambigu-
ous words by taking their context into consider-
ation. We also combine domain-specific informa-
tion from the knowledge sources, like sense distri-
bution scores and keyword ranking scores, into the
graph thus personalizing the graph for the given
domain.
In section 2, we describe domain sense ranking.
Domain keyword ranking is described in Section
3. Graph construction and personalized page rank
are described in Section 4. Evaluation results over
the SemEval data are provided in Section 5.
2 Domain Sense Ranking
McCarthy et al (2004) propose a method for
finding predominant senses from raw text. The
method uses a thesaurus acquired from automat-
ically parsed text based on the method described
by Lin (1998). This provides the top k nearest
neighbours for each target word w, along with the
distributional similarity score between the target
word and each neighbour. The senses of a word
w are each assigned a score by summing over the
distributional similarity scores of its neighbours.
These are weighted by a semantic similarity score
(using WordNet Similarity score (Pedersen et al,
2004) between the sense of w and the sense of the
neighbour that maximizes the semantic similarity
score.
More formally, let N
w
= {n
1
, n
2
, . . . n
k
}
be the ordered set of the top k scoring
neighbours of w from the thesaurus with
associated distributional similarity scores
{dss(w, n
1
), dss(w, n
2
), . . . dss(w, n
k
)}. Let
senses(w) be the set of senses of w. For each
sense of w (ws
i
? senses(w)) a ranking score is
obtained by summing over the dss(w, n
j
) of each
neighbour (n
j
? N
w
) multiplied by a weight.
This weight is the WordNet similarity score
(wnss) between the target sense (ws
i
) and the
sense of n
j
(ns
x
? senses(n
j
)) that maximizes
this score, divided by the sum of all such WordNet
similarity scores for senses(w) and n
j
. Each
sense ws
i
? senses(w) is given a sense ranking
score srs(ws
i
) using
srs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
where wnss(ws
i
, n
j
) =
max
ns
x
?senses(n
j
)
(wnss(ws
i
, ns
x
))
Since this approach requires only raw text,
sense rankings for a particular domain can be gen-
erated by simply training the algorithm using a
corpus representing that domain. We used the
background documents provided to the partici-
pants in this task as a domain specific corpus. In
general, a domain specific corpus can be obtained
using domain-specific keywords (Kilgarriff et al,
2010). A thesaurus is acquired from automatically
parsed background documents using the Stanford
Parser (Klein and Manning, 2003). We used k = 5
to built the thesaurus. As we increased k we found
the number of non-domain specific words occur-
ring in the thesaurus increased and negatively af-
fected the sense distributions. To counter this, one
of our systems IIITH2 used a slightly modified
ranking score by multiplying the effect of each
neighbour with its domain keyword ranking score.
The modified sense ranking msrs(ws
j
) score of
sense ws
i
is
msrs(ws
i
) =
?
n
j
N
w
dss(w, n
j
)?
wnss(ws
i
, n
j
)
?
ws
i
senses(w)
wnss(ws
i
, n
j
)
?krs(n
j
)
where krs(n
j
) is the keyword ranking score of
the neighbour n
j
in the domain specific corpus. In
the next section we describe the way in which we
compute krs(n
j
).
WordNet::Similarity::lesk (Pedersen et al,
2004) was used to compute word similarity wnss.
IIITH1 and IIITH2 systems differ in the way
senses are ranked. IIITH1 uses srs(ws
j
) whereas
IIITH2 system uses msrs(ws
j
) for computing
sense ranking scores in the given domain.
3 Domain Keyword Ranking
We extracted keywords in the domain by compar-
ing the frequency lists of domain corpora (back-
ground documents) and a very large general cor-
pus, ukWaC (Ferraresi et al, 2008), using the
method described by Rayson and Garside (2000).
For each word in the frequency list of the domain
corpora, words(domain), we calculated the log-
likelihood (LL) statistic as described in Rayson
and Garside (2000). We then normalized LL to
compute keyword ranking score krs(w) of word
w words(domain) using
388
krs(w) =
LL(w)
?
w
i
?words(domain)
LL(w
i
)
The above score represents the keyness of the
word in the given domain. Top ten keywords (in
descending order of krs) in the corpora provided
for this task are species, biodiversity, life, habitat,
natura
1
, EU, forest, conservation, years, amp
2
.
4 Personalized PageRank
Our approach uses the Personalized PageRank al-
gorithm (Agirre and Soroa, 2009) with WordNet
as the lexical knowledge base (LKB) to perform
WSD. WordNet is converted to a graph by repre-
senting each synset as a node (synset node) and the
relationships in WordNet (hypernymy, hyponymy
etc.) as edges between synset nodes. The graph is
initialized by adding a node (word node) for each
context word of the target word (including itself)
thus creating a context dependent graph (person-
alized graph). The popular PageRank (Page et al,
1999) algorithm is employed to analyze this per-
sonalized graph (thus the algorithm is referred as
personalized PageRank algorithm) and the sense
for each disambiguous word is chosen by choos-
ing the synset node which gets the highest weight
after a certain number of iterations of PageRank
algorithm.
We capture domain information in the personal-
ized graph by using sense ranking scores and key-
word ranking scores of the domain to assign initial
weights to the word nodes and their edges (word-
synset edge). This way we personalize the graph
for the given domain.
4.1 Graph Initialization Methods
We experimented with different ways of initial-
izing the graph, described below, which are de-
signed to capture domain specific information.
Personalized Page rank (PPR): In this method,
the graph is initialized by allocating equal prob-
ability mass to all the word nodes in the context
including the target word itself, thus making the
graph context sensitive. This does not include do-
main specific information.
1
In background documents this word occurs in reports de-
scribing Natura 2000 networking programme.
2
This new word ?amp? is created by our programs while
extracting body text from background documents. The
HTML code ?&amp;? which represents the symbol?&? is
converted into this word.
Keyword Ranking scores with PPR (KRS +
PPR): This is same as PPR except that context
words are initialized with krs.
Sense Ranking scores with PPR (SRS + PPR):
Edges connecting words and their synsets are as-
signed weights equal to srs. The initialization of
word nodes is same as in PPR.
KRS + SRS + PPR: Word nodes are initialized
with krs and edges are assigned weights equal to
srs.
In addition to the above methods of unsuper-
vised graph initialization, we also initialized the
graph in a semi-supervised manner. WordNet (ver-
sion 1.7 and above) have a field tag cnt for each
synset (in the file index.sense) which represents
the number of times the synset is tagged in vari-
ous semantic concordance texts. We used this in-
formation, concordance score (cs) of each synset,
with the above methods of graph initialization as
described below.
Concordance scores with PPR (CS + PPR): The
graph initialization is similar to PPR initialization
additionally with concordance score of synsets on
the edges joining words and their synsets.
CS + KRS + PPR: The initialization graph of
KRS + PPR is further initialized by assigning con-
cordance scores to the edges connecting words and
their synsets.
CS + SRS + PPR: Edges connecting words and
their synsets are assigned weights equal to sum of
the concordance scores and sense ranking scores
i.e. cs + srs. The initialization of word nodes is
same as in PPR.
CS + KRS + SRS + PPR: Word nodes are ini-
tialized with krs and edges are assigned weights
equal to cs + srs.
PageRank was applied to all the above graphs to
disambiguate a target word.
4.2 Experimental details of PageRank
Tool: We used UKB tool
3
(Agirre and Soroa,
2009) which provides an implementation of per-
sonalized PageRank. We modified it to incorpo-
rate our methods of graph initialization. The LKB
used in our experiments is WordNet3.0 + Gloss
which is provided in the tool. More details of the
tools used can be found in the Appendix.
Normalizations: Sense ranking scores (srs) and
keyword ranking scores (krs) have diverse ranges.
We found srs generally in the range between 0 to
3
http://ixa2.si.ehu.es/ukb/
389
Precision Recall
Unsupervised Graph Initialization
PPR 37.3 36.8
KRS + PPR 38.1 37.6
SRS + PPR 48.4 47.8
KRS + SRS + PPR 48.0 47.4
Semi-supervised Graph Initialization
CS + PPR 50.2 49.6
CS + KRS + PPR 50.1 49.5
* CS + SRS + PPR 53.4 52.8
CS + KRS + SRS + PPR 53.6 52.9
Others
1
st
sense 50.5 50.5
PSH 49.8 43.2
Table 1: Evaluation results on English test data of SemEval-2010 Task-17. * represents the system which
we submitted to SemEval and is ranked 3rd in public evaluation.
1 and krs in the range 0 to 0.02. Since these scores
are used to assign initial weights in the graph,
these ranges are scaled to fall in a common range
of [0, 100]. Using any other scaling method should
not effect the performance much since PageRank
(and UKB tool) has its own internal mechanisms
to normalize the weights.
5 Evaluation Results
Test data released for this task is disambiguated
using IIITH1 and IIITH2 systems. As described
in Section 2, IIITH1 and IIITH2 systems differ in
the way the sense ranking scores are computed.
Here we project only the results of IIITH1 since
IIITH1 performed slightly better than IIITH2 in all
the above settings. Results of 1
st
sense system pro-
vided by the organizers which assigns first sense
computed from the annotations in hand-labeled
corpora is also presented. Additionally, we also
present the results of Predominant Sense Heuristic
(PSH) which assigns every word w with the sense
ws
j
(ws
j
? senses(w)) which has the highest
value of srs(ws
j
) computed in Section 2 similar
to (McCarthy et al, 2004).
Table 1 presents the evaluation results. We used
TreeTagger
4
to Part of Speech tag the test data.
POS information was used to discard irrelevant
senses. Due to POS tagging errors, our precision
values were not equal to recall values. In the com-
petition, we submitted IIITH1 and IIITH2 systems
with CS + SRS + PPR graph initialization. IIITH1
4
http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
and IIIH2 gave performances of 53.4 % and 52.2
% precision respectively. In our later experiments,
we found CS + KRS + SRS + PPR has given the
best performance of 53.6 % precision.
From the results, it can be seen when srs in-
formation is incorporated in the graph, precision
improved by 11.1% compared to PPR in unsuper-
vised graph initialization and by 3.19% compared
to CS + PPR in semi-supervised graph initializa-
tion. Also little improvements are seen when krs
information is added. This shows that domain
specific information like sense ranking scores and
keyword ranking scores play a major role in do-
main specific WSD.
The difference between the results in unsu-
pervised and semi-supervised graph initializations
may be attributed to the additional information the
semi-supervised graph is having i.e. the sense dis-
tribution knowledge of non-domain specific words
(common words).
6 Conclusion
This paper proposes a method for domain specific
WSD. Our method is based on a graph-based al-
gorithm (Personalized Page Rank) which is mod-
ified to include information representing the do-
main (sense ranking and key word ranking scores).
Experiments show that exploiting this domain spe-
cific information within the graph based methods
produces better results than when this information
is used individually.
390
Acknowledgements
The authors are grateful to Ted Pedersen for his
helpful advice on the WordNet Similarity Pack-
age. We also thank Rajeev Sangal for supporting
the authors Siva Reddy and Abhilash Inumella.
References
Eneko Agirre and Aitor Soroa. 2009. Personaliz-
ing pagerank for word sense disambiguation. In
EACL ?09: Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 33?41, Morristown, NJ,
USA. Association for Computational Linguistics.
Paul Buitelaar, Bernardo Magnini, Carlo Strapparava,
and Piek Vossen. 2006. Domain-specific wsd. In
Word Sense Disambiguation. Algorithms and Appli-
cations, Editors: Eneko Agirre and Philip Edmonds.
Springer.
Scott Cotton, Phil Edmonds, Adam Kilgarriff, and
Martha Palmer. 2001. Senseval-2. http://www.
sle.sharp.co.uk/senseval2.
A. Ferraresi, E. Zanchetta, M. Baroni, and S. Bernar-
dini. 2008. Introducing and evaluating ukwac,
a very large web-derived corpus of english. In
Proceed-ings of the WAC4 Workshop at LREC 2008,
Marrakesh, Morocco.
Adam Kilgarriff, Siva Reddy, Jan Pomik?alek, and Avi-
nesh PVS. 2010. A corpus factory for many lan-
guages. In LREC 2010, Malta.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 423?430, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In HLT ?05: Proceed-
ings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 419?426, Morristown, NJ, USA.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of the 17th
international conference on Computational linguis-
tics, pages 768?774, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590.
Rada Mihalcea and Phil Edmonds, editors. 2004.
Proceedings Senseval-3 3rd International Workshop
on Evaluating Word Sense Disambiguation Systems.
ACL, Barcelona, Spain.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kauf-
man.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In HLT-NAACL ?04: Demon-
stration Papers at HLT-NAACL 2004 on XX, pages
38?41, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Paul Rayson and Roger Garside. 2000. Comparing
corpora using frequency profiling. In Proceedings
of the workshop on Comparing corpora, pages 1?
6, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-basedword sense disambiguation using mea-
sures of word semantic similarity. In ICSC ?07: Pro-
ceedings of the International Conference on Seman-
tic Computing, pages 363?369, Washington, DC,
USA. IEEE Computer Society.
Appendix
Domain Specific Thesaurus, Sense Ranking
Scores and Keyword Ranking Scores are accessi-
ble at
http://web.iiit.ac.in/
?
gvsreddy/
SemEval2010/
Tools Used:
? UKB is used with options ?ppr ?dict weight. Dictio-
nary files which UKB uses are automatically generated
using sense ranking scores srs.
? Background document words are canonicalized using
KSTEM, a morphological analyzer
? The Stanford Parser is used to parse background docu-
ments to build thesaurus
? Test data is part of speech tagged using TreeTagger.
391
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 228?236,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Effects of Semantic Annotations on Precision Parse Ranking
Andrew MacKinlay??, Rebecca Dridan??, Diana McCarthy?? and Timothy Baldwin??
? Dept. of Computing and Information Systems, University of Melbourne, Australia
? NICTA Victoria Research Laboratories, University of Melbourne, Australia
? Department of Informatics, University of Oslo, Norway
? Computational Linguistics and Phonetics, Saarland University, Germany
amack@csse.unimelb.edu.au, rdridan@ifi.uio.no,
diana@dianamccarthy.co.uk, tb@ldwin.net
Abstract
We investigate the effects of adding semantic
annotations including word sense hypernyms
to the source text for use as an extra source
of information in HPSG parse ranking for the
English Resource Grammar. The semantic an-
notations are coarse semantic categories or en-
tries from a distributional thesaurus, assigned
either heuristically or by a pre-trained tagger.
We test this using two test corpora in different
domains with various sources of training data.
The best reduces error rate in dependency F-
score by 1% on average, while some methods
produce substantial decreases in performance.
1 Introduction
Most start-of-the-art natural language parsers (Char-
niak, 2000; Clark and Curran, 2004; Collins, 1997)
use lexicalised features for parse ranking. These are
important to achieve optimal parsing accuracy, and
yet these are also the features which by their nature
suffer from data-sparseness problems in the training
data. In the absence of reliable fine-grained statis-
tics for a given token, various strategies are possible.
There will often be statistics available for coarser
categories, such as the POS of the particular token.
However, it is possible that these coarser represen-
tations discard too much, missing out information
which could be valuable to the parse ranking. An
intermediate level of representation could provide
valuable additional information here. For example,
?This research was conducted while the second author was
a postdoctoral researcher within NICTA VRL.
?The third author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
assume we wish to correctly attach the prepositional
phrases in the following examples:
(1) I saw a tree with my telescope
(2) I saw a tree with no leaves
The most obvious interpretation in each case has the
prepositional phrase headed by with attaching in dif-
ferent places: to the verb phrase in the first example,
and to the noun tree in the second. Such distinctions
are difficult for a parser to make when the training
data is sparse, but imagine we had seen examples
such as the following in the training corpus:
(3) Kim saw a eucalypt with his binoculars
(4) Sandy observed a willow with plentiful foliage
There are few lexical items in common, but in each
case the prepositional phrase attachment follows the
same pattern: in (3) it attaches to the verb, and in
(4) to the noun. A conventional lexicalised parser
would have no knowledge of the semantic similarity
between eucalypt and tree, willow and tree, binoc-
ulars and telescope, or foliage and leaves, so would
not be able to make any conclusions about the earlier
examples on the basis of this training data. However
if the parse ranker has also been supplied with in-
formation about synonyms or hypernyms of the lex-
emes in the training data, it could possibly have gen-
eralised, to learn that PPs containing nouns related
to seeing instruments often modify verbs relating to
observation (in preference to nouns denoting inani-
mate objects), while plant flora can often be modi-
fied by PPs relating to appendages of plants such as
leaves. This is not necessarily applicable only to PP
attachment, but may help in a range of other syntac-
tic phenomena, such as distinguishing between com-
plements and modifiers of verbs.
228
The synonyms or hypernyms could take the form
of any grouping which relates word forms with se-
mantic or syntactic commonality ? such as a label
from the WordNet (Miller, 1995) hierarchy, a sub-
categorisation frame (for verbs) or closely related
terms from a distributional thesaurus (Lin, 1998).
We present work here on using various levels
of semantic generalisation as an attempt to im-
prove parse selection accuracy with the English Re-
source Grammar (ERG: Flickinger (2000)), a preci-
sion HPSG-based grammar of English.
2 Related Work
2.1 Parse Selection for Precision Grammars
The focus of this work is on parsing using hand-
crafted precision HPSG-based grammars, and in
particular the ERG. While these grammars are care-
fully crafted to avoid overgeneration, the ambiguity
of natural languages means that there will unavoid-
ably be multiple candidate parses licensed by the
grammar for any non-trivial sentence. For the ERG,
the number of parses postulated for a given sentence
can be anywhere from zero to tens of thousands. It
is the job of the parse selection model to select the
best parse from all of these candidates as accurately
as possible, for some definition of ?best?, as we dis-
cuss in Section 3.2.
Parse selection is usually performed by training
discriminative parse selection models, which ?dis-
criminate? between the set of all candidate parses.
A widely-used method to achieve this is outlined
in Velldal (2007). We feed both correct and incor-
rect parses licensed by the grammar to the TADM
toolkit (Malouf, 2002), and learn a maximum en-
tropy model. This method is used by Zhang et al
(2007) and MacKinlay et al (2011) inter alia. One
important implementation detail is that rather than
exhaustively ranking all candidates out of possibly
many thousands of trees, Zhang et al (2007) showed
that it was possible to use ?selective unpacking?,
which means that the exhaustive parse forest can be
represented compactly as a ?packed forest?, and the
top-ranked trees can be successively reconstructed,
enabling faster parsing using less memory.
2.2 Semantic Generalisation for parse ranking
Above, we outlined a number of reasons why
semantic generalisation of lexemes could enable
parsers to make more efficient use of training data,
and indeed, there has been some prior work investi-
gating this possibility. Agirre et al (2008) applied
two state-of-the-art treebank parsers to the sense-
tagged subset of the Brown corpus version of the
Penn Treebank (Marcus et al, 1993), and added
sense annotation to the training data to evaluate their
impact on parse selection and specifically on PP-
attachment. The annotations they used were oracle
sense annotations, automatic sense recognition and
the first sense heuristic, and it was this last method
which was the best performer in general. The sense
annotations were either the WordNet synset ID or
the coarse semantic file, which we explain in more
detail below, and replaced the original tokens in
the training data. The largest improvement in pars-
ing F-score was a 6.9% reduction in error rate for
the Bikel parser (Bikel, 2002), boosting the F-score
from 0.841 to 0.852, using the noun supersense only.
More recently, Agirre et al (2011) largely repro-
duced these results with a dependency parser.
Fujita et al (2007) add sense information to im-
prove parse ranking with JaCy (Siegel and Bender,
2002), an HPSG-based grammar which uses simi-
lar machinery to the ERG. They use baseline syn-
tactic features, and also add semantic features based
on dependency triples extracted from the semantic
representations of the sentence trees output by the
parser. The dataset they use has human-assigned
sense tags from a Japanese lexical hierarchy, which
they use as a source of annotations. The dependency
triples are modified in each feature set by replacing
elements of the semantic triples with corresponding
senses or hypernyms. In the best-performing con-
figuration, they use both syntactic and semantic fea-
tures with multiple levels of the the semantic hier-
archy from combined feature sets. They achieve a
5.6% improvement in exact match parsing accuracy.
3 Methodology
We performed experiments in HPSG parse rank-
ing using the ERG, evaluating the impact on parse
selection of semantic annotations such as coarse
sense labels or synonyms from a distributional the-
229
WESCIENCE LOGON
Total Sentences 9632 9410
Parseable Sentences 9249 8799
Validated Sentences 7631 8550
Train/Test Sentences 6149/1482 6823/1727
Tokens/sentence 15.0 13.6
Training Tokens 92.5k 92.8k
Table 1: Corpora used in our experiments, with total sen-
tences, how many of those can be parsed, how many of
the parseable sentences have a single gold parse (and are
used in these experiments), and average sentence length
saurus. Our work here differs from the aforemen-
tioned work of Fujita et al (2007) in a number of
ways. Firstly, we use purely syntactic parse selec-
tion features based on the derivation tree of the sen-
tence (see Section 3.4.3), rather than ranking using
dependency triples, meaning that our method is in
principle able to be integrated into a parser more eas-
ily, where the final set of dependencies would not be
known in advance. Secondly, we do not use human-
created sense annotations, instead relying on heuris-
tics or trained sense-taggers, which is closer to the
reality of real-world parsing tasks.
3.1 Corpora
Following MacKinlay et al (2011), we use two pri-
mary training corpora. First, we use the LOGON
corpus (Oepen et al, 2004), a collection of En-
glish translations of Norwegian hiking texts. The
LOGON corpus contains 8550 sentences with ex-
actly one gold parse, which we partitioned ran-
domly by sentence into 10 approximately equal sec-
tions, reserving two sections as test data, and us-
ing the remainder as our training corpus. These
sentences were randomly divided into training and
development data. Secondly, we use the We-
Science (Ytrest?l et al, 2009) corpus, a collection
of Wikipedia articles related to computational lin-
guistics. The corpus contains 11558 sentences, from
which we randomly chose 9632, preserving the re-
mainder for future work. This left 7631 sentences
with a single gold tree, which we divided into a
training set and a development set in the same way.
The corpora are summarised in Table 1.
With these corpora, we are able to investigate in-
domain and cross-domain effects, by testing on a
different corpus to the training corpus, so we can
examine whether sense-tagging alleviates the cross-
domain performance penalty noted in MacKinlay et
al. (2011). We can also use a subset of each training
corpus to simulate the common situation of sparse
training data, so we can investigate whether sense-
tagging enables the learner to make better use of a
limited quantity of training data.
3.2 Evaluation
Our primary evaluation metric is Elementary De-
pendency Match (Dridan and Oepen, 2011). This
converts the semantic output of the ERG into a set
of dependency-like triples, and scores these triples
using precision, recall and F-score as is conven-
tional for other dependency evaluation. Following
MacKinlay et al (2011), we use the EDMNA mode
of evaluation, which provides a good level of com-
parability while still reflecting most the semantically
salient information from the grammar.
Other work on the ERG and related grammars has
tended to focus on exact tree match, but the granu-
lar EDM metric is a better fit for our needs here ?
among other reasons, it is more sensitive in terms
of error rate reduction to changes in parse selection
models (MacKinlay et al, 2011). Additionally, it is
desirable to be able to choose between two different
parses which do not match the gold standard exactly
but when one of the parses is a closer match than the
other; this is not possible with exact match accuracy.
3.3 Reranking for parse selection
The features we are adding to the parse selection
procedure could all in principle be applied by the
parser during the selective unpacking stage, since
they all depend on information which can be pre-
computed. However, we wish to avoid the need for
multiple expensive parsing runs, and more impor-
tantly the need to modify the relatively complex in-
ternals of the parse ranking machinery in the PET
parser (Callmeier, 2000). So instead of performing
the parse ranking in conjunction with parsing, as is
the usual practice, we use a pre-parsed forest of the
top-500 trees for each corpus, and rerank the forest
afterwards for each configuration shown.
The pre-parsed forests use the same models which
were used in treebanking. Using reranking means
that the set of candidate trees is held constant, which
230
means that parse selection models never get the
chance to introduce a new tree which was not in
the original parse forest from which the gold tree
was annotated, which may provide a very small per-
formance boost (although when the parse selection
models are similar as is the case for most of the mod-
els here, this effect is likely to be very small).
3.4 Word Sense Annotations
3.4.1 Using the WordNet Hierarchy
Most experiments we report on here make some
use of the WordNet sense inventory. Obviously we
need to determine the best sense and corresponding
WordNet synset for a given token. We return to this
in Section 3.4.2, but for now assume that the sense
disambiguation is done.
As we are concerned primarily with making
commonalities between lemmas with different base
forms apparent to the parse selection model, the fine-
grained synset ID will do relatively little to provide
a coarser identifier for the token ? indeed, if two
tokens with identical forms were assigned different
synset IDs, we would be obscuring the similarity.1
We can of course make use of the WordNet hier-
archy, and use hypernyms from the hierarchy to tag
each candidate token, but there are a large number
of ways this can be achieved, particularly when it
is possibly to assign multiple labels per token as is
the case here (which we discuss in Section 3.4.3).
We apply two relatively simple strategies. We noted
in Section 2.2 that Agirre et al (2008) found that
the semantic file was useful. This is the coarse lex-
icographic category label, elsewhere denoted super-
sense (Ciaramita and Altun, 2006), which is the
terminology we use. Nouns are divided into 26
coarse categories such as ?animal?, ?quantity? or
?phenomenon?, and verbs into 15 categories such as
?perception? or ?consumption?. In some configura-
tions, denoted SS, we tag each open-class token with
one of the supersense labels.
Another configuration attempts to avoid making
assumptions about which level of the hierarchy will
be most useful for parse disambiguation, instead
leaving it the MaxEnt parse ranker to pick those la-
bels from the hierarchy which are most useful. Each
1This could be useful for verbs since senses interact strongly
subcategorisation frames, but that is not our focus here.
open class token is labelled with multiple synsets,
starting with the assigned leaf synset and travelling
as high as possible up the hierarchy, with no distinc-
tion made between the different levels in the hier-
archy. Configurations using this are designated HP,
for ?hypernym path?.
3.4.2 Disambiguating senses
We return now to the question of determination
of the synset for a given token. One frequently-
used and robust strategy is to lemmatise and POS-
tag each token, and assign it the first-listed sense
from WordNet (which may or may not be based on
actual frequency counts). We POS-tag using TnT
(Brants, 2000) and lemmatise using WordNet?s na-
tive lemmatiser. This yields a leaf-level synset, mak-
ing it suitable as a source of annotations for both SS
and HP. We denote this ?WNF? for ?WordNet First?
(shown in parentheses after SS or HP).
Secondly, to evaluate whether a more informed
approach to sense-tagging helps beyond the naive
WNF method, in the ?SST? method, we use the out-
puts of SuperSense Tagger (Ciaramita and Altun,
2006), which is optimised for assigning the super-
senses described above, and can outperform a WNF-
style baseline on at least some datasets. Since this
only gives us coarse supersense labels, it can only
provide SS annotations, as we do not get the leaf
synsets needed for HP. The input we feed in is POS-
tagged with TnT as above, for comparability with
the WNF method, and to ensure that it is compati-
ble with the configuration in which the corpora were
parsed ? specifically, the unknown-word handling
uses a version of the sentences tagged with TnT. We
ignore multi-token named entity outputs from Su-
perSense Tagger, as these would introduce a con-
founding factor in our experiments and also reduce
comparability of the results with the WNF method.
3.4.3 A distributional thesaurus method
A final configuration attempts to avoid the need
for curated resources such as WordNet, instead us-
ing an automatically-constructed distributional the-
saurus (Lin, 1998). We use the thesaurus from
McCarthy et al (2004), constructed along these
lines using the grammatical relations from RASP
(Briscoe and Carroll, 2002) applied to 90 millions
words of text from the British National Corpus.
231
root_frag
np_frg_c
hdn_bnp_c
aj-hdn_norm_c
legal_a1
"legal"
n_pl_olr
issue_n1
"issues"
Figure 1: ERG derivation tree for the phrase Legal issues
[n_-_c_le "issues"]
[n_pl_olr n_-_c_le "issues"]
[aj-hdn_norm_c n_pl_olr n_-_c_le "issues"]
(a) Original features
[n_-_c_le noun.cognition]
[n_pl_olr n_-_c_le noun.cognition]
[aj-hdn_norm_c n_pl_olr n_-_c_le noun.cognition]
(b) Additional features in leaf mode, which augment the original
features
[noun.cognition "issues"]
[n_pl_olr noun.cognition "issues"]
[aj-hdn_norm_c n_pl_olr noun.cognition "issues"]
(c) Additional features in leaf-parent (?P?) mode, which augment
the original features
Figure 2: Examples of features extracted from for
"issues" node in Figure 1 with grandparenting level
of 2 or less
To apply the mapping, we POS-tag the text with
TnT as usual, and for each noun, verb and adjec-
tive we lemmatise the token (with WordNet again,
falling back to the surface form if this fails), and
look up the corresponding entry in the thesaurus. If
there is a match, we select the top five most simi-
lar entries (or fewer if there are less than five), and
use these new entries to create additional features,
as well as adding a feature for the lemma itself in all
cases. This method is denoted LDT for ?Lin Distri-
butional Thesaurus?. We note that many other meth-
ods could be used to select these, such as different
numbers of synonyms, or dynamically changing the
number of synonyms based on a threshold against
the top similarity score, but this is not something we
evaluate in this preliminary investigation.
Adding Word Sense to Parse Selection Models
We noted above that parse selection using the
methodology established by Velldal (2007) uses
human-annotated incorrect and correct derivation
trees to train a maximum entropy parse selection
model. More specifically, the model is trained using
features extracted from the candidate HPSG deriva-
tion trees, using the labels of each node (which are
the rule names from the grammar) and those of a
limited number of ancestor nodes.
As an example, we examine the noun phrase Le-
gal issues from the WESCIENCE corpus, for which
the correct ERG derivation tree is shown in Figure 1.
Features are created by examining each node in the
tree and at least its parent, with the feature name set
to the concatenation of the node labels. We also gen-
erally make used of grandparenting features, where
we examine earlier ancestors in the derivation tree.
A grandparenting level of one means we would also
use the label of the grandparent (i.e. the parent?s par-
ent) of the node, a level of two means we would add
in the great-grandparent label, and so on. Our exper-
iments here use a maximum grandparenting level of
three. There is also an additional transformation ap-
plied to the tree ? the immediate parent of each leaf
is, which is usually a lexeme, is replaced with the
corresponding lexical type, which is a broader par-
ent category from the type hierarchy of the grammar,
although the details of this are not relevant here.
For the node labelled "issues" in Figure 1 with
grandparenting levels from zero to two, we would
extract the features as shown in Figure 2(a) (where
the parent node issue_n1 has already been re-
placed with its lexical type n_-c_le).
In this work here, we create variants of these fea-
tures. A preprocessing script runs over the training
or test data, and for each sentence lists variants of
each token using standoff markup indexed by char-
acter span, which are created from the set of addi-
tional semantic tags assigned to each token by the
word sense configuration (from those described in
Section 3.4) which is currently in use. These sets of
semantic tags for a given word could be a single su-
persense tag, as in SS, a set of synset IDs as in HP
or a set of replacement lemmas in LDT. In all cases,
the set of semantic tags could also be empty ? if ei-
ther the word has a part of speech which we are not
232
Test Train SS (WNF) SSp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 85.09/82.33/83.69 +0.09 84.81/82.20/83.48 ?0.11
WESC (92k) 86.56/83.58/85.05 86.83/84.04/85.41 +0.36 87.03/83.96/85.47 +0.42
LOG (23k) 88.60/87.23/87.91 88.72/87.20/87.95 +0.04 88.43/87.00/87.71 ?0.21
LOG (92k) 91.74/90.15/90.94 91.82/90.07/90.94 ?0.00 91.90/90.13/91.01 +0.07
WESC
WESC (23k) 86.80/84.43/85.60 87.12/84.44/85.76 +0.16 87.18/84.50/85.82 +0.22
WESC (92k) 89.34/86.81/88.06 89.54/86.76/88.13 +0.07 89.43/87.23/88.32 +0.26
LOG (23k) 83.74/81.41/82.56 84.02/81.43/82.71 +0.15 84.10/81.67/82.86 +0.31
LOG (92k) 85.98/82.93/84.43 86.02/82.69/84.32 ?0.11 85.89/82.76/84.30 ?0.13
Table 2: Results for SS (WNF) (supersense from first WordNet sense), evaluated on 23k tokens (approx 1500
sentences) of either WESCIENCE or LOGON, and trained on various sizes of in-domain and cross-domain training
data. Subscript ?p? indicates mappings were applied to leaf parents rather than leaves.
Test Train SS (SST) SSp(SST)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.97/82.38/83.65 +0.06 85.32/82.66/83.97 +0.37
WESC (92k) 86.56/83.58/85.05 87.05/84.47/85.74 +0.70 86.98/83.87/85.40 +0.35
LOG (23k) 88.60/87.23/87.91 88.93/87.50/88.21 +0.29 88.84/87.40/88.11 +0.20
LOG (92k) 91.74/90.15/90.94 91.67/90.02/90.83 ?0.10 91.47/89.96/90.71 ?0.23
WESC
WESC (23k) 86.80/84.43/85.60 86.88/84.29/85.56 ?0.04 87.32/84.48/85.88 +0.27
WESC (92k) 89.34/86.81/88.06 89.53/86.54/88.01 ?0.05 89.50/86.56/88.00 ?0.05
LOG (23k) 83.74/81.41/82.56 84.06/81.30/82.66 +0.10 83.96/81.64/82.78 +0.23
LOG (92k) 85.98/82.93/84.43 86.13/82.96/84.51 +0.08 85.76/82.84/84.28 ?0.16
Table 3: Results for SS (SST) (supersense from SuperSense Tagger)
Test Train HPWNF HPp(WNF)
P/ R/ F P/ R/ F ?F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.56/82.03/83.28 ?0.32 84.74/82.20/83.45 ?0.15
WESC (92k) 86.56/83.58/85.05 86.65/84.22/85.42 +0.37 86.41/83.65/85.01 ?0.04
LOG (23k) 88.60/87.23/87.91 88.58/87.26/87.92 +0.00 88.58/87.35/87.96 +0.05
LOG (92k) 91.74/90.15/90.94 91.68/90.19/90.93 ?0.01 91.66/89.85/90.75 ?0.19
WESC
WESC (23k) 86.80/84.43/85.60 86.89/84.19/85.52 ?0.08 87.18/84.43/85.78 +0.18
WESC (92k) 89.34/86.81/88.06 89.74/86.96/88.33 +0.27 89.23/86.88/88.04 ?0.01
LOG (23k) 83.74/81.41/82.56 83.87/81.20/82.51 ?0.04 83.47/81.00/82.22 ?0.33
LOG (92k) 85.98/82.93/84.43 85.89/82.38/84.10 ?0.33 85.75/83.03/84.37 ?0.06
Table 4: Results for HPWNF (hypernym path from first WordNet sense)
Test Train LDTp(5)
P/ R/ F P/ R/ F ?F
LOG
WESC (23k) 85.02/82.22/83.60 84.48/82.18/83.31 ?0.28
WESC (92k) 86.56/83.58/85.05 86.36/84.14/85.23 +0.19
LOG (23k) 88.60/87.23/87.91 88.28/86.99/87.63 ?0.28
LOG (92k) 91.74/90.15/90.94 91.01/89.25/90.12 ?0.82
WESC
WESC (23k) 86.80/84.43/85.60 86.17/83.51/84.82 ?0.78
WESC (92k) 89.34/86.81/88.06 88.31/85.61/86.94 ?1.12
LOG (23k) 83.74/81.41/82.56 83.60/81.18/82.37 ?0.19
LOG (92k) 85.98/82.93/84.43 85.74/82.96/84.33 ?0.11
Table 5: Results for LDT (5) (Lin-style distributional thesaurus, expanding each term with the top-5 most similar)
233
attempting to tag semantically, or if our method has
no knowledge of the particular word.
The mapping is applied at the point of feature ex-
traction from the set of derivation trees ? at model
construction time for the training set and at rerank-
ing time for the development set. If a given leaf to-
ken has some set of corresponding semantic tags, we
add a set of variant features for each semantic tag,
duplicated and modified from the matching ?core?
features described above. There are two ways these
mappings can be applied, since it is not immedi-
ately apparent where the extra lexical generalisation
would be most useful. The ?leaf? variant applies to
the leaf node itself, so that in each feature involving
the leaf node, add a variant where the leaf node sur-
face string has been replaced with the new seman-
tic tag. The ?parent? variant, which has a subscript
?P? (e.g. SSp(WNF) ) applies the mapping to the
immediate parent of the leaf node, leaving the leaf
itself unchanged, but creating variant features with
the parent nodes replaced with the tag.
For our example here, we assume that we have
an SS mapping for Figure 2(a), and that this has
mapped the token for "issues" to the WordNet
supersense noun.cognition. For the leaf vari-
ant, the extra features that would be added (either for
considering inclusion in the model, or for scoring a
sentence when reranking) are shown in Figure 2(b),
while those for the parent variant are in Figure 2(c).
3.4.4 Evaluating the contribution of sense
annotations
Wewish to evaluate whether adding sense annota-
tions improve parser accuracy against the baseline of
training a model in the conventional way using only
syntactic features. As noted above, we suspect that
this semantic generalisation may help in cases where
appropriate training data is sparse ? that is, where
the training data is from a different domain or only
a small amount exists. So to evaluate the various
methods in these conditions, we train models from
small (23k token) training sets and large (96k token)
training sets created from subsets of each corpus
(WESCIENCE and LOGON). For the baseline, we
train these models without modification. For each
of the various methods of adding semantic tags, we
then re-use each of these training sets to create new
models after adding the appropriate additional fea-
tures as described above, to evaluate whether these
additional features improve parsing accuracy
4 Results
We present an extensive summary of the results ob-
tained using the various methods in Tables 2, 3, 4
and 5. In each case we show results for applying
to the leaf and to the parent. Aggregating the re-
sults for each method, the differences range between
substantially negative and modestly positive, with a
large number of fluctuations due to statistical noise.
LDT is the least promising performer, with only
one very modest improvement, and the largest de-
creases in performance, of around 1%. The HP-
WNF and HPp(WNF) methods make changes in
either direction ? on average, over all four train-
ing/test combinations, there are very small drops
in F-score of 0.02% for HPWNF, and 0.06% for
HPp(WNF), which indicates that neither of the
methods is likely to be useful in reliably improving
parser performance.
The SS methods are more promising. SS (WNF)
and SSp(WNF) methods yield an average im-
provement of 0.10% each, while SS (SST) and
SSp(SST) give average improvements of 0.12%
and 0.13% respectively (representing an error rate
reduction of around 1%). Interestingly, the increase
in tagging accuracy we might expect using Super-
Sense Tagger only translates to a modest (and prob-
ably not significant) increase in parser performance,
possibly because the tagger is not optimised for the
domains in question. Amongst the statistical noise
it is hard to discern overall trends; surprisingly, it
seems that the size of the training corpus has rela-
tively little to do with the success of adding these su-
persense annotations, and that the corpus being from
an unmatched domain doesn?t necessarily mean that
sense-tagging will improve accuracy either. There
may be a slight trend for sense annotations to be
more useful when WESCIENCE is the training cor-
pus (either in the small or the large size).
To gain a better insight into how the effects
change as the size of the training corpus changes for
the different domains, we created learning curves for
the best-performing method, SSp(SST) (although
as noted above, all SS methods give similar levels
of improvement), shown in Figure 3. Overall, these
234
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on LOGON Corpus
Test Corpus
LOGON*
LOGON* +SS
WeSc
WeSc +SS
(a) LOGON
0 20 40 60 80 100
Training Tokens (thousands)
0.76
0.78
0.80
0.82
0.84
0.86
0.88
0.90
E
D
M
N
A
 
F
-
s
c
o
r
e
Trained on WeScience Corpus
Test Corpus
LOGON
LOGON +SS
WeSc*
WeSc* +SS
(b) WESCIENCE
Figure 3: EDMNA learning curves for SS (SST) (supersense from SuperSense Tagger). ?*? denotes in-domain
training corpus.
graphs support the same conclusions as the tables
? the gains we see are very modest and there is a
slight tendency for WESCIENCE models to benefit
more from the semantic generalisation, but no strong
tendencies for this to work better for cross-domain
training data or small training sets.
5 Conclusion
We have presented an initial study evaluat-
ing whether a fairly simple approach to using
automatically-created coarse semantic annotations
can improve HPSG parse selection accuracy using
the English Resource Grammar. We have provided
some weak evidence that adding features based on
semantic annotations, and in particular word super-
sense, can provide modest improvements in parse
selection performance in terms of dependency F-
score, with the best-performing method SSp(SST)
providing an average reduction in error rate over 4
training/test corpus combinations of 1%. Other ap-
proaches were less promising. In all configurations,
there were instances of F-score decreases, some-
times substantial.
It is somewhat surprising that we did not achieve
reliable performance gains which were seen in the
related work described above. One possible expla-
nation is that the model training parameters were
suboptimal for this data set since the characteris-
tics of the data are somewhat different than with-
out sense annotations. The failure to improve some-
what mirrors the results of Clark (2001), who was at-
tempting to improve the parse ranking performance
of the unification-based based probabilistic parser of
Carroll and Briscoe (1996). Clark (2001) used de-
pendencies to rank parses, and WordNet-based tech-
niques to generalise this model and learn selectional
preferences, but failed to improve performance over
the structural (i.e. non-dependency) ranking in the
original parser. Additionally, perhaps the changes
we applied in this work to the parse ranking could
possibly have been more effective with features
based on semantic dependences as used by Fujita
et al (2007), although we outlined reasons why we
wished to avoid this approach.
This work is preliminary and there is room for
more exploration in this space. There is scope for
much more feature engineering on the semantic an-
notations, such as using different levels of the se-
mantic hierarchy, or replacing the purely lexical fea-
tures instead of augmenting them. Additionally,
more error analysis would reveal whether this ap-
proach was more useful for avoiding certain kinds
of parser errors (such as PP-attachment).
Acknowledgements
NICTA is funded by the Australian Government as
represented by the Department of Broadband, Com-
munications and the Digital Economy and the Aus-
tralian Research Council through the ICT Centre of
Excellence program.
235
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL-08: HLT, pages
317?325, Columbus, Ohio, June.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing
with semantic classes. In Proceedings of the 49th An-
nual Meeting of the Association of Computational Lin-
guistics, ACL-HLT 2011 Short Paper, Portland, Ore-
gon?.
D. M. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceed-
ings of the second international conference on Human
Language Technology Research, pages 178?182, San
Francisco, CA, USA.
T. Brants. 2000. Tnt ? a statistical part-of-speech tag-
ger. In Proceedings of the Sixth Conference on Ap-
plied Natural Language Processing, pages 224?231,
Seattle, Washington, USA, April.
T. Briscoe and J. Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proceedings of the
3rd International Conference on Language Resources
and Evaluation, pages 1499?1504.
U. Callmeier. 2000. Pet ? a platform for experimenta-
tion with efficient HPSG processing techniques. Nat.
Lang. Eng., 6(1):99?107.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr pars- ing system
through evaluation. In Proceedings of the SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92?100, Philadelphia, PA.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st North American chapter of
the Association for Computational Linguistics confer-
ence, pages 132?139.
M. Ciaramita and Y. Altun. 2006. Broad-coverage sense
disambiguation and information extraction with a su-
persense sequence tagger. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 594?602, Sydney, Australia,
July.
S. Clark and J.R. Curran. 2004. Parsing the WSJ us-
ing CCG and log-linear models. In Proceedings of the
42nd Meeting of the ACL, pages 104?111.
S. Clark. 2001. Class-based Statistical Models for Lex-
ical Knowledge Acquisition. Ph.D. thesis, University
of Sussex.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23, Madrid, Spain, July.
R. Dridan and S. Oepen. 2011. Parser evaluation us-
ing elementary dependency matching. In Proceedings
of the 12th International Conference on Parsing Tech-
nologies, pages 225?230, Dublin, Ireland, October.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Nat. Lang. Eng., 6
(1):15?28.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for HPSG parse selec-
tion. In ACL 2007 Workshop on Deep Linguistic Pro-
cessing, pages 25?32, Prague, Czech Republic, June.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2,
pages 768?774.
A. MacKinlay, R. Dridan, D. Flickinger, and T. Baldwin.
2011. Cross-domain effects on parse selection for pre-
cision grammars. Research on Language & Computa-
tion, 8(4):299?340.
R. Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of
the Sixth Conference on Natural Language Learning
(CoNLL-2002), pages 49?55.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of english:
the penn treebank. Comput. Linguist., 19(2):313?330.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant word senses in untagged text. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 279?es.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish. Communications of the ACM, 38(11):39?41.
S. Oepen, D. Flickinger, K. Toutanova, and C.D. Man-
ning. 2004. LinGO Redwoods: A rich and dynamic
treebank for HPSG. Research on Language & Com-
putation, 2(4):575?596.
M. Siegel and E.M. Bender. 2002. Efficient deep pro-
cessing of japanese. In Proceedings of the 3rd work-
shop on Asian language resources and international
standardization-Volume 12, pages 1?8.
E. Velldal. 2007. Empirical Realization Ranking. Ph.D.
thesis, University of Oslo Department of Informatics.
G. Ytrest?l, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and annotating Wikipedia sub-domains ? to-
wards a new eScience community resourc. In Pro-
ceedings of the Seventh International Workshop on
Treebanks and Linguistic Theories, Groeningen, The
Netherlands, January.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in
unification-based n-best parsing. In IWPT ?07: Pro-
ceedings of the 10th International Conference on Pars-
ing Technologies, pages 48?59, Morristown, NJ, USA.
236
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 557?564,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DSS: Text Similarity Using Lexical Alignments of Form, Distributional
Semantics and Grammatical Relations
Diana McCarthy
Saarland University?
diana@dianamccarthy.co.uk
Spandana Gella
University of Malta
spandanagella@gmail.com
Siva Reddy
Lexical Computing Ltd.
siva@sivareddy.in
Abstract
In this paper we present our systems for the
STS task. Our systems are all based on a
simple process of identifying the components
that correspond between two sentences. Cur-
rently we use words (that is word forms), lem-
mas, distributional similar words and gram-
matical relations identified with a dependency
parser. We submitted three systems. All sys-
tems only use open class words. Our first sys-
tem (alignheuristic) tries to obtain a map-
ping between every open class token using all
the above sources of information. Our second
system (wordsim) uses a different algorithm
and unlike alignheuristic, it does not use
the dependency information. The third sys-
tem (average) simply takes the average of
the scores for each item from the other two
systems to take advantage of the merits of
both systems. For this reason we only pro-
vide a brief description of that. The results
are promising, with Pearson?s coefficients on
each individual dataset ranging from .3765
to .7761 for our relatively simple heuristics
based systems that do not require training on
different datasets. We provide some analy-
sis of the results and also provide results for
our data using Spearman?s, which as a non-
parametric measure which we argue is better
able to reflect the merits of the different sys-
tems (average is ranked between the others).
1 Introduction
Our motivation for the systems entered in the STS
task (Agirre et al, 2012) was to model the contribu-
? The first author is a visiting scholar on the Erasmus
Mundus Masters Program in ?Language and Communication
Technologies? (LCT, 2007?0060).
tion of each linguistic component of both sentences
to the similarity of the texts by finding an align-
ment. Ultimately such a system could be exploited
for ranking candidate paraphrases of a chunk of text
of any length. We envisage a system as outlined in
the future work section. The systems reported are
simple baselines to such a system. We have two
main systems (alignheuristic and wordsim) and
also a system which simply uses the average score
for each item from the two main systems (average).
In our systems we:
? only deal with open class words as to-
kens i.e. nouns, verbs, adjectives, adverbs.
alignheuristic and average also use num-
bers
? assume that tokens have a 1:1 mapping
? match:
? word forms
? lemmas
? distributionally similar lemmas
? (alignheuristic and average only) ar-
gument or head in a matched grammatical
relation with a word that already has a lex-
ical mapping
? score the sentence pair based on the size of the
overlap. Different formulations of the score are
used by our methods
The paper is structured as follows. In the next
section we make a brief mention of related work
though of course there will be more pertinent related
work presented and published at SemEval 2012. In
section 3 we give a detailed account of the systems
557
and in section 4 we provide the results obtained on
the training data on developing our systems. In sec-
tion 5 we present the results on the test data, along
with a little analysis using the gold standard data. In
section 6 we conclude our findings and discuss our
ideas for future work.
2 Related Work
Semantic textual similarity relates to textual entail-
ment (Dagan et al, 2005), lexical substitution (Mc-
Carthy and Navigli, 2009) and paraphrasing (Hirst,
2003). The key issue for semantic textual similarity
is that the task is to determine similarity, where sim-
ilarity is cast as meaning equivalence. 1 In textual
entailment the relation under question is the more
specific relation of entailment, where the meaning
of one sentence is entailed by another and a sys-
tem needs to determine the direction of the entail-
ment. Lexical substitution relates to semantic tex-
tual similarity though the task involves a lemma in
the context of a sentence, candidate substitutes are
not provided, and the relation at question in the task
is one of substitutability. 2 Paraphrase recognition
is a highly related task, for example using compa-
rable corpora (Barzilay and Elhadad, 2003), and it
is likely that semantic textual similarity measures
might be useful for ranking candidates in paraphrase
acquisition.
In addition to various works related to textual
entailment, lexical substitution and paraphrasing,
there has been some prior work explicitly on se-
mantic text similarity. Semantic textual similarity
has been explored in various works. Mihalcea et al
(2006) extend earlier work on word similarity us-
ing various WordNet similarity measures (Patward-
han et al, 2003) and a couple of corpus-based dis-
tributional measures: PMI-IR (Turney, 2002) and
LSA (Berry, 1992). They use a measure which
takes a summation over all tokens in both sen-
tences. For each token they find the maximum sim-
ilarity (WordNet or distributional) weighted by the
inverse document frequency of that word. The dis-
1See the guidelines given to the annotators at
http://www.cs.columbia.edu/?weiwei/workshop/
instructions.pdf
2This is more or less semantic equivalence since the an-
notators were instructed to focus on meaning http://www.
dianamccarthy.co.uk/files/instructions.pdf.
tributional similarity measures perform at a simi-
lar level to the knowledge-based measures that use
WordNet. Mohler and Mihalcea (2009) adapt this
work for automatic short answer grading, that is
matching a candidate answer to one supplied by
the tutor. Mohler et al (2011) take this applica-
tion forward, combining lexical semantic similarity
measures with a graph-alignment which considers
dependency graphs using the Stanford dependency
parser (de Marneffe et al, 2006) in terms of lexical,
semantic and syntactic features. A score is then pro-
vided for each node in the graph. The features are
combined using machine learning.
The systems we propose likewise use lexical sim-
ilarity and dependency relations, but in a simple
heuristic formulation without a man-made thesaurus
such as WordNet and without machine learning.
3 Systems
We lemmatize and part-of-speech tag the data using
TreeTagger (Schmid, 1994). We process the tagged
data with default settings of the Malt Parser (Nivre
et al, 2007) to dependency parse the data. All sys-
tems make use of a distributional thesaurus which
lists distributionally similar lemmas (?neighbours?)
for a given lemma. This is a thesaurus constructed
using log-dice (Rychly?, 2008) and UkWaC (Fer-
raresi et al, 2008). 3 Note that we use only the
top 20 neighbours for any word in all the methods
described below. We have not experimented with
varying this threshold.
In the following descriptions, we refer to our sen-
tences as s1 and s2 and these open classed tokens
within those sentences as ti ? s1 and t j ? s2 where
each token in either sentence is represented by a
word (w), lemma (l), part-of-speech (p) and gram-
matical relation (gr), identified by the Malt parser,
to its dependency head at a given position (hp) in
the same sentence.
3.1 alignheuristic
This method uses nouns, verbs, adjectives, adverbs
and numbers. The algorithm aligns words (w), or
lemmas (l) from left to right from s1 to s2 and vice
3This is the ukWaC distributional thesaurus avail-
able in Sketch Engine (Kilgarriff et al, 2004) at
http://the.sketchengine.co.uk/bonito/run.cgi/
first\_form?corpname=preloaded/ukwac2
558
versa (wmtch). If there is no alignment for words or
lemmas then it does the same matching process (s1
given s2 and vice versa) for distributionally similar
neighbours using the distributional thesaurus men-
tioned above (tmtch) and also another matching pro-
cess looking for a corresponding grammatical rela-
tion identified with the Malt parser in the other sen-
tence where the head (or argument) already has a
match in both sentences (rmtch).
A fuller and more formal description of the algo-
rithm follows:
1. retain nouns, verbs (not be), adjectives, adverbs
and numbers in both sentences s1 and s2.
2. wmtch:
(a) look for word matches
? wi ? s1 to w j ? s2, left to right i.e. the
first matching w j ? s2 is selected as a
match for wi.
? w j ? s2 to wi ? s1, left to right i.e. the
first matching wi ? s1 is selected as a
match for w j
(b) and then lemma matches for any ti ? s1
and t j ? s1 not matched in steps 2a
? li ? s1 to l j ? s2 , left to right i.e. the
first matching l j ? s2 is selected as a
match for li.
? l j ? s2 to li ? s1 , left to right i.e. the
first matching li ? s1 is selected as a
match for l j
3. using only ti ? s1 and t j ? s2 not matched in
the above steps:
? tmtch: match lemma and PoS (l + p) with
the distributional thesaurus against the top
20 most similar lemma-pos entries. That
is:
(a) For l + pi ? s1, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + p j ? s2, left to
right i.e. the first matching l + p j ? s2
to any of the most similar words to
l + pi according to the thesaurus is se-
lected as a match for l + pi ? s1.
(b) For l + p j ? s2, if not already matched
at step 2 above, find the most similar
words in the thesaurus, and match if
one of these is in l + pi ? s1, left to
right
? rmtch: match the tokens, if not already
matched at step 2 above, by looking for
a head or argument relation with a token
that has been matched at step 2 to a token
with the inverse relation. That is:
i For ti ? s1, if not already matched at
step 2 above, if hpi ? s1 (the pointer
to the head, i.e. parent, of ti) refers to
a token tx ? s1 which has wmtch at tk
in s2, and there exists a tq ? s2 with
grq = gri and hpq = tk, then match ti
with tq
ii For ti ? s1 , if not already matched
at step 2 or the preceding step (rmtch
3i) and if there exists another tx ? s1
with a hpx which refers to ti (i.e. ti is
the parent, or head, of tx) with a match
between tx and tk ? s2 from step 2, 4
and where tk has grk = grx with hpk
which refers to tq in s2, then match ti
with tq 5
iii we do likewise in reverse for s2 to s1
and then check all matches are recip-
rocated with the same 1:1 mapping
Finally, we calculate the score sim(s1, s2):
|wmtch| + (wt ? |tmtch + rmtch|)
|s1| + |s2|
? 5 (1)
where wt is a weight of 0.5 or less (see below).
The sim score gives a score of 5 where two
sentences have the same open class tokens, since
matches in both directions are included and the de-
nominator is the number of open class tokens in both
sentences. The score is 0 if there are no matches.
The thesaurus and grammatical relation matches are
counted equally and are considered less important
4In the example illustrated in figure 1 and discussed below,
ti could be rose in the upper sentence (s1) and Nasdaq would be
tx and tk.
5So in our example, from figure 1, ti (rose) is matched with tq
(climb) as climb is the counterpart head to rose for the matched
arguments (Nasdaq).
559
NasdaqThe tech?loaded composite rose 20.96 points to 1595.91, ending at its highest level for 12 months.
thesaurus
malt
malt
points, or 1.2 percent, to 1,615.02.The technology?laced climbed 19.11 Index <.IXIC>CompositeNasdaq
Figure 1: Example of matching with alignheuristic
for the score as the exact matches. We set wt to 0.4
for the official run, though we could improve perfor-
mance by perhaps setting a bit lower as shown below
in section 4.1.
Figure 1 shows an example pair of sentences from
the training data in MSRpar. The solid lines show
alignments between words. Composite and compos-
ite are not matched because the lemmatizer assumes
that the former is a proper noun and does not decap-
italise; we could decapitalise all proper nouns. The
dotted arcs show parallel dependency relations in the
sentences where the argument (Nasdaq) is matched
by wmtch. The rmtch process therefore assumes the
corresponding heads (rise and climb) align. In addi-
tion, tmtch finds a match from climb to rise as rise is
in the top 20 most similar words (neighbours) in the
distributional thesaurus. climb is not however in the
top 20 for rise and so a link is not found in the other
direction. We have not yet experimented with val-
idating the thesaurus and grammatical relation pro-
cesses together, though that would be worthwhile in
future.
3.2 wordsim
In this method, we first choose the shortest sentence
based on the number of open words. Let s1 and s2
be the shortest and longest sentences respectively.
For every lemma li ? s1, we find its best matching
lemma l j ? s2 using the following heuristics and
assigning an alignment score as follows:
1. if li=l j, then the alignment score of li
(algnscr(li)) is one.
2. else li ? s1 is matched with a lemma l j ? s2
with which it has the highest distributional sim-
ilarity. 6 The alignment score, algnscr(li) is
the distributional similarity between li and l j
(which is always less than one).
The final sentence similarity score between the
pair s1 and s2 is computed as
sim(s1, s2) =
?
li?s1 algnscr(li)
|s1|
(2)
3.3 average
This system simple uses the average score for each
item from alignheuristic and wordsim. This is
simply so we can make a compromise between the
merits of the two systems.
4 Experiments on the Training Data
Table 1 displays the results on training data for the
system settings as they were for the final test run. We
conducted further analysis for the alignheuristic
system and that is reported in the following subsec-
tion. We can see that while the alignheuristic
is better on the MSRpar and SMT-eur datasets, the
wordsim outperforms it on the MSRvid dataset,
which contains shorter, simpler sentences. One rea-
son might be that the wordsim credits alignments
in one direction only and this works well when sen-
tences are of a similar length but can loose out on the
longer paraphrase and SMT data. This behaviour is
6Provided this is within the top 20 most similar words in the
thesaurus.
560
MSRpar MSRvid SMT-eur
alignheuristic 0.6015 0.6994 0.5222
wordsim 0.4134 0.7658 0.4479
average 0.5337 0.7535 0.5061
Table 1: Results on training data
confirmed by the results on the test data reported be-
low in section 5, though we cannot rule out that other
factors play a part.
4.1 alignheuristic
We developed the system on the training data for the
purpose of finding bugs, and setting the weight in
equation 1. During development we found the opti-
mal weight for wt to be 0.4. 7 Unfortunately we did
not leave ourselves sufficient time to set the weights
after resolving the bugs. In table 1 we report the
results on the training data for the system that we
uploaded, however in table 2 we report more recent
results for the final system but with different values
of wt. From table 2 it seems that results may have
been improved if we had determined the final value
of wt after debugging our system fully, however this
depends on the type of data as 0.4 was optimal for
the datasets with more complex sentences (MSRpar
and SMT-eur).
In table 3, we report results for alignheuristic
with and without the distributional similarity
thesaurus (tmtch) and the dependency relations
(rmtch). In table 4 we show the total number of to-
ken alignments made by the different matching pro-
cesses on the training data. We see, from table 4
that the MSRvid data relies on the thesaurus and de-
pendency relations to a far greater extent than the
other datasets, presumably because of the shorter
sentences where many have a few contrasting words
in similar syntactic relations, for example s1 Some-
one is drawing. s2 Someone is dancing. 8 We see
from table 3 that the use of these matching processes
is less accurate for MSRvid and that while tmtch
improves performance, rmtch seems to degrade per-
formance. From table 2 it would seem that on this
type of data we would get the best results by reduc-
7We have not yet attempted setting the weight on alignment
by relation and alignment by distributional similarity separately.
8Note that the alignheuristic algorithm is symmetrical
with respect to s1 and s2 so it does not matter which is which.
wt MSRpar MSRvid SMT-eur
0.5 0.5998 0.6518 0.5290
0.4 0.6015 0.6994 0.5222
0.3 0.6020 0.7352 0.5146
0.2 0.6016 0.7577 0.5059
0.1 0.6003 0.7673 0.4964
0 0.5981 0.7661 0.4863
Table 2: Results for the alignheuristic algorithm on
the training data: varying wt
MSR MSR SMT
par vid -eur
-tmtch+rmtch 0.6008 0.7245 0.5129
+tmtch-rmtch 0.5989 0.7699 0.4959
-tmtch-rmtch 0.5981 0.7661 0.4863
+tmtch+rmtch 0.6015 0.6994 0.5222
Table 3: Results for the alignheuristic algorithm on
the training data: with and without tmtch and rmtch
ing wt to a minimum, and perhaps it would make
sense to drop rmtch. Meanwhile, on the longer more
complex MSRpar and SMT-eur data, the less precise
rmtch and tmtch are used less frequently (relative to
the wmtch) but can be seen from table 3 to improve
performance on both training datasets. Moreover, as
we mention above, from table 2 the parameter set-
ting of 0.4 used for our final test run was optimal for
these datasets.
MSRpar MSRvid SMT-eur
wmtch 10960 2349 12155
tmtch 378 1221 964
rmtch 1008 965 1755
Table 4: Number of token alignments for the different
matching processes
561
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news
alignheuristic .5253 (60) .5735 (24) .7123 (53) .4781 (25) .6984 (7) .4177 (38)
average .5490 (58) .5020 (48) .7645 (41) .4875 (16) .6677(14) .4324 (31)
wordsim .5130 (61) .3765 (75) .7761 (34) .4161 (58) .5728 (59) .3964 (48)
Table 5: Official results: Rank (out of 89) is shown in brackets
run ALL MSRpar MSRvid SMT-eur On-WN SMT-news average ?
alignheuristic 0.5216 0.5539 0.7125 0.5404 0.6928 0.3655 0.5645
average 0.5087 0.4818 0.7653 0.5415 0.6302 0.3835 0.5518
wordsim 0.4279 0.3608 0.7799 0.4487 0.4976 0.3388 0.4756
Table 7: Spearman?s ? for the 5 datasets, ?all? and the average coefficient across the datasets
run mean Allnrm
alignheuristic 0.6030 (21) 0.7962 (42)
average 0.5943 (26) 0.8047 (35)
wordsim 0.5287 (55) 0.7895 (49)
Table 6: Official results: Further metrics suggested in dis-
cussion. Rank (out of 89) is shown in brackets
5 Results
Table 5 provides the official results for our submitted
systems, along with the rank on each dataset. The re-
sults in the ?all? column which combine all datasets
together are at odds with our intuitions. Our sys-
tems were ranked higher in every individual dataset
compared to the ?all? ranking, with the exception of
wordsim and the MSRpar dataset. This ?all? met-
ric is anticipated to impact systems that have dif-
ferent settings for different types of data however
we did not train our systems to run differently on
different data. We used exactly the same parame-
ter settings for each system on every dataset. We
believe Pearson?s measure has a significant impact
on results because it is a parametric measure and
as such the shape of the distribution (the distribu-
tion of scores) is assumed to be normal. We present
the results in table 6 from new metrics proposed by
participants during the post-results discussion: All-
nrm (normalised) and mean (this score is weighted
by the number of sentence pairs in each dataset). 9
The Allnrm score, proposed by a participant during
the discussion phase to try and combat issues with
9Post-results discussion is archived at http://groups.
google.com/group/sts-semeval/topics
the ?all? score, also does not accord with our intu-
ition given the ranks of our systems on the individ-
ual datasets. The mean score, proposed by another
participant, however does reflect performance on the
individual datasets. Our average system is ranked
between alignheuristic and wordsim which is
in line with our expectations given results on the
training data and individual datasets.
As mentioned above, an issue with the use of
Pearson?s correlation coefficient is that it is para-
metric and assumes that the scores are normally dis-
tributed. We calculated Spearman?s ? which is the
non-parametric equivalent of Pearson?s and uses the
ranks of the scores, rather than the actual scores. 10
The results are presented in table 7. We cannot cal-
culate the results for other systems, and therefore the
ranks for our system, since we do not have the other
system?s outputs but we do see that the relative per-
formance of our system on ?all? is more in line with
our expectations: average, which simply uses the
average of the other two systems for each item, is
usually ranked between the other two systems, de-
pending on the dataset. Spearman?s ?all? gives a sim-
ilar ranking of our three systems as the mean score.
We also show average ?. This is a macro average
of the Spearman?s value for the 5 datasets without
weighting by the number of sentence pairs. 11
10Note that Spearman?s ? is often a little lower than Pear-
son?s (Mitchell and Lapata, 2008)
11We do recognise the difficulty in determining metrics on a
new pilot study. The task organisers are making every effort to
make it clear that this enterprise is a pilot, not a competition and
that they welcome feedback.
562
6 Conclusions
The systems were developed in less than a week
including the time with the test data. There are
many trivial fixes that may improve the basic algo-
rithm, such as decapitalising proper nouns. There
are many things we would like to try, such as val-
idating the dependency matching process with the
thesaurus matching. We would like to match larger
units rather than tokens, with preferences towards
the longer matching blocks. In parallel to the devel-
opment of alignheuristic, we developed a sys-
tem which measures the similarity between a node
in the dependency tree of s1 and a node in the de-
pendency tree of s2 as the sum of the lexical sim-
ilarity of the lemmas at the nodes and the simi-
larity of its children nodes. We did not submit a
run for the system as it did not perform as well as
alignheuristic, probably because the score fo-
cused on structure too much. We hope to spend time
developing this system in future.
Ultimately, we envisage a system that:
? can have non 1:1 mappings between tokens, i.e.
a phrase may be paraphrased as a word for ex-
ample blow up may be paraphrased as explode
? can map between sequences of non-contiguous
words for example the words in the phrase blow
up may be separated but mapped to the word
explode as in the bomb exploded ? They blew
it up
? has categories (such as equivalence, entailment,
negation, omission . . . ) associated with each
mapping. Speculation, modality and sentiment
should be indicated on any relevant chunk so
that differences can be detected between candi-
date and referent
? scores the candidate using a function of the
scores of the mapped units (as in the systems
described above) but alters the score to reflect
the category as well as the source of the map-
ping, for example entailment without equiva-
lence should reduce the similarity score, in con-
trast to equivalence, and negation should re-
duce this still further
Crucially we would welcome a task where anno-
tators would also provide a score on sub chunks of
the sentences (or arbitrary blocks of text) that align
along with a category for the mapping (equivalence,
entailment, negation etc..). This would allow us to
look under the hood at the text similarity task and de-
termine the reason behind the similarity judgments.
7 Acknowledgements
We thank the task organisers for their efforts in or-
ganising the task and their readiness to take on board
discussions on this as a pilot. We also thank the
SemEval-2012 co-ordinators.
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First
Joint Conference on Lexical and Computational
Semantics (*SEM 2012).
Barzilay, R. and Elhadad, N. (2003). Sentence align-
ment for monolingual comparable corpora. In
Collins, M. and Steedman, M., editors, Proceed-
ings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 25?
32.
Berry, M. (1992). Large scale singular value compu-
tations. International Journal of Supercomputer
Applications, 6(1):13?49.
Dagan, I., Glickman, O., and Magnini, B. (2005).
The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL First Chal-
lenge Workshop, pages 1?8, Southampton, UK.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed dependency
parses from phrase structure parses. In To appear
at LREC-06.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the Sixth International
Conference on Language Resources and Evalua-
tion (LREC 2008), Marrakech, Morocco.
Hirst, G. (2003). Paraphrasing paraphrased. In-
vited talk at the Second International Workshop
563
on Paraphrasing, 41st Annual Meeting of the As-
sociation for Computational Linguistics.
Kilgarriff, A., Rychly?, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of Eu-
ralex, pages 105?116, Lorient, France. Reprinted
in Patrick Hanks (ed.). 2007. Lexicology: Critical
concepts in Linguistics. London: Routledge.
McCarthy, D. and Navigli, R. (2009). The English
lexical substitution task. Language Resources and
Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and
Beyond, 43(2):139?159.
Mihalcea, R., Corley, C., and Strapparava, C.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of the American Association for Artificial Intelli-
gence (AAAI 2006), Boston, MA.
Mitchell, J. and Lapata, M. (2008). Vector-based
models of semantic composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Mohler, M., Bunescu, R., and Mihalcea, R. (2011).
Learning to grade short answer questions us-
ing semantic similarity measures and dependency
graph alignments. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, pages 752?762, Portland, Oregon, USA. As-
sociation for Computational Linguistics.
Mohler, M. and Mihalcea, R. (2009). Text-to-text se-
mantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009),
pages 567?575, Athens, Greece. Association for
Computational Linguistics.
Nivre, J., Hall, J., Nilsson, J., Chanev, A., Eryigit,
G., Ku?bler, S., Marinov, S., and Marsi, E. (2007).
Maltparser: A language-independent system for
data-driven dependency parsing. Natural Lan-
guage Engineering, 13(2):95?135.
Patwardhan, S., Banerjee, S., and Pedersen, T.
(2003). Using measures of semantic relatedness
for word sense disambiguation. In Proceedings
of the Fourth International Conference on Intelli-
gent Text Processing and Computational Linguis-
tics (CICLing 2003), Mexico City.
Rychly?, P. (2008). A lexicographer-friendly associ-
ation score. In Proceedings of 2nd Workshop on
Recent Advances in Slavonic Natural Languages
Processing, RASLAN 2008, Brno.
Schmid, H. (1994). Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49, Manchester,
UK.
Turney, P. D. (2002). Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR, cs.LG/0212033.
564
From Predicting Predominant
Senses to Local Context for
Word Sense Disambiguation
Rob Koeling
Diana McCarthy
University of Sussex (UK)
email: robk@sussex.ac.uk
Abstract
Recent work on automatically predicting the predominant sense of a word
has proven to be promising (McCarthy et al, 2004). It can be applied (as a
first sense heuristic) to Word Sense Disambiguation (WSD) tasks, without
needing expensive hand-annotated data sets. Due to the big skew in the
sense distribution of many words (Yarowsky and Florian, 2002), the First
Sense heuristic for WSD is often hard to beat. However, the local context
of an ambiguous word can give important clues to which of its senses was
intended. The sense ranking method proposed by McCarthy et al (2004)
uses a distributional similarity thesaurus. The k nearest neighbours in the
thesaurus are used to establish the predominant sense of a word. In this
paper we report on a first investigation on how to use the grammatical
relations the target word is involved with, in order to select a subset of
the neighbours from the automatically created thesaurus, to take the local
context into account. This unsupervised method is quantitatively evalu-
ated on SemCor. We found a slight improvement in precision over using
the predicted first sense. Finally, we discuss strengths and weaknesses of
the method and suggest ways to improve the results in the future.
129
130 Koeling and McCarthy
1 Introduction
In recent years, a lot of research was done on establishing the predominant sense of
ambiguous words automatically using untagged texts (McCarthy et al, 2004, 2007).
The motivation for that work is twofold: on the one hand it builds on the strength
of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of
choosing themost commonly used sense of a word, irrespective of the context in which
the word occurs) and on the other hand it recognizes that manually created resources
for establishing word sense distributions are expensive to create and therefore hard
to find. The one resource that is used most widely, SemCor (Miller et al, 1993), is
only available for English and only representative for ?general? (non domain specific)
text. McCarthy et als method was successfully applied to a corpus of modern English
text (the BNC (Leech, 1992)) and the predicted predominant senses compared well
with the gold standard given by SemCor. Other experiments showed that the method
can successfully be adapted to domain specific text (Koeling et al, 2005) and other
languages (for example, Japanese (Iida et al, 2008)).
Even though the first sense heuristic is powerful, it would be preferable to only
use it for WSD, when either the sense distribution is so skewed that the most com-
monly used sense is by far the most dominant, or as a back-off when few other clues
are available to decide otherwise. The use of local context is ultimately necessary to
find evidence for the intended sense of an ambiguous word. In this paper we inves-
tigate how we can exploit results from intermediate steps taken when calculating the
predominant senses to this end.
The work on automatically finding predominant senses1 was partly inspired by the
observation that you can identify word senses by looking at the nearest neighbours of a
target word in a distributional thesaurus. For example, consider the following (simpli-
fied) entry for the word plant in such a thesaurus (omitting the scores for distributional
similarity):
(9) plant : factory, industry, facility, business, company, species, tree, crop, en-
gine, flower, farm, leaf, market, garden, field, seed, shrub...
Just by looking at the neighbours you can identify two main groups of neighbours,
each pointing at separate senses of the word. First there is the set of words consist-
ing of factory, industry, facility, business, company, engine that hint at the ?industrial
plant? sense of the word and then there is the set consisting of tree, crop, flower, leaf,
species, garden, field, seed, shrub that are more closely related to the ?flora? sense of
the word. A few words, like farm and possibly market could be associated equally
strongly with either sense. The idea behind ?sense ranking? is, that the right mix of
1. number of neighbours with a strong associations with one or more of the senses,
2. the strength of the association (semantic similarity) between neighbour and
sense and
1McCarthy et al (2004) concentrates on evaluating the predominant sense, but the method does in fact
rank all the senses in order of frequency of use.
From Predicting Predominant Senses to Local Context for WSD 131
3. the strength of the distributional similarity of the contributing neighbour and the
target word, will allow us to estimate the relative importance (i.e. frequency of
use) of each sense.
What we want to explore here, is how we can use the local context of an occurrence
of the target word, to select a subset of these neighbours. This subset should consist
of words that are related more strongly to the sense of the word in the target sentence.
For example, consider the word plant in a sentence like:
(10) The gardener grows plants from vegetable seeds.
Plant is used in this sentence as the ?subject of grow?. A simple way of zooming
in on potentially relevant neighbours is by using the most informative contexts shared
between neighbours and the word in the target sentence. This is implemented by
selecting just those words that occur in the same grammatical context (i.e. as subject
of the verb ?grow?) in a reference corpus2. If we apply that to the example in 9, we
end up with the following subset: business, industry, species, tree, crop, flower, seed,
shrub. Even though the first two words are still associated with the ?industrial plant?
sense, we can see that the majority of the words in this subset is strongly associated
with the intended sense.
In the next section we first give a quick introduction to the sense ranking algorithm
introduced in McCarthy et al (2004). Then we explain howwe can use the database of
grammatical relations that we used for creating the thesaurus, for selecting a subset of
neighbours in the thesaurus. The following section describes an evaluation performed
on the SemCor data. In the last two sections we discuss the results and especially
why both recall and precision are lower than we had hoped and what can be done to
improve the results.
2 Predominant Senses and Local Context
For a full review of McCarthy et als ranking method, we refer to McCarthy et al
(2004) or McCarthy et al (2007). Here we give a short description of the method.
Since we need the grammatical relations used for building the thesaurus, for selecting
a subset of the neighbours, we explain the procedure for building the thesaurus in 2.2.
In the last part of this section we explain how we exploit local context for SD.
2.1 Finding Predominant Senses
We use the method described inMcCarthy et al (2004) for finding predominant senses
from raw text. It can be applied to all parts of speech, but the experiments in this pa-
per all focus on nouns only. The method uses a thesaurus obtained from the text by
parsing, extracting grammatical relations and then listing each word (w) with its top k
nearest neighbours, where k is a constant. Like McCarthy et al (2004) we use k = 50
and obtain our thesaurus using the distributional similarity metric described by Lin
(1998). We use WordNet (WN) as our sense inventory. The senses of a word w are
each assigned a ranking score which sums over the distributional similarity scores of
2We use the same corpus used for generating the thesaurus as for the reference corpus (in all our
experiments).
132 Koeling and McCarthy
the neighbours and weights each neighbour?s score by a WN Similarity score (Pat-
wardhan and Pedersen, 2003) between the sense of w and the sense of the neighbour
that maximises the WN Similarity score. This weight is normalised by the sum of
such WN similarity scores between all senses of w and and the senses of the neigh-
bour that maximises this score. We use the WN Similarity jcn score on nouns (Jiang
and Conrath, 1997) since this gave reasonable results for McCarthy et al and it is
efficient at run time given precompilation of frequency information. The jcn measure
needs word frequency information, which we obtained from the British National Cor-
pus (BNC) (Leech, 1992). The distributional thesaurus was constructed using subject,
direct object adjective modifier and noun modifier relations.
Thus we rank each sense wsi ?WSw using Prevalence Score wsi =
(11) ?
n j?Nw
dssn j ?
wnss(wsi,n j)
?wsi??WSw wnss(wsi? ,n j)
where the WordNet similarity score (wnss) is defined as:
wnss(wsi,n j) = max
nsx?NSn j
(wnss(wsi,nsx))
2.2 Building the Thesaurus
The thesaurus was acquired using the method described by Lin (1998). For input we
used grammatical relation data extracted using an automatic parser (Briscoe and Car-
roll, 2002). For the experiments in this paper we used the 90 million words of written
English from the BNC. For each noun we considered the co-occurring verbs in the
direct object and subject relation, the modifying nouns in noun-noun relations and the
modifying adjectives in adjective-noun relations. This limited set of grammatical rela-
tions was chosen since accuracy of the parser is particularly high for these 4 relations.
We could easily extend the set of relations to more in the future. A noun,w, is thus de-
scribed by a set of co-occurrence triples < w,r,x > and associated frequencies, where
r is a grammatical relation and x is a possible co-occurrence with w in that relation.
For every pair of nouns, where each noun had a total frequency in the triple data of 10
or more, we computed their distributional similarity using the measure given by Lin
(1998). If T (w) is the set of co-occurrence types (r,x) such that I(w,r,x) is positive
then the similarity between two nouns, w and n, can be computed as:
(12)
?(r,x)?T(w)?T (n) (I(w,r,x)+ I(n,r,x))
?(r,x)?T (w) I(w,r,x)+ ?(r,x)?T(n) I(n,r,x)
where I(w,r,x) = log P(x|w?r)P(x|r)
A thesaurus entry of size k for a target nounw can then be defined as the k most similar
nouns to noun w.
2.3 Local Context
The basis for building the distributional similarity thesaurus, is the set of grammatical
relations that the target word shares with other words. For example, if we look at
the thesaurus entry for the noun bike, then we see that the closest neighbours are (the
synonym) bicycle and the closely related motorbike (and motorcycle). The next 10
closest neighbours are all other vehicles (car, van, boat, bus, etc.). This is something
From Predicting Predominant Senses to Local Context for WSD 133
we would expect to see, since all these words do occur in similar grammatical contexts.
We travel by bike, as well as by motorcycle, car and bus. We park them, drive_off
with them, hire them, abandon them and repair them. Many of these relations can be
applied to a wide range of vehicles (or even a wider range of objects). However, some
relations are more specific to two-wheeled vehicles. For example, it is quite common
to mount a bike or a motorbike, whereas it is less common to mount a car or a van.
(Motor)bikes are chained to stop people from stealing them and it is probably more
common to ride a (motor)bike as opposed to driving a car or truck. Of course there
are many other more general things you can do with these vehicles: buy, sell, steal
them; there are yellow bikes, cars and boats, just like other objects. Therefore, we can
see many other types of objects lower in the list of neighbours that share these more
general grammatical relations, but not those that are specific to, say, vehicles or even
the sub-category of two-wheeled vehicles.
Consider the following sentence containing the ambiguous noun body:
(13) ?Regular exercise keeps the body healthy.?
(14) ?The funding body approved the final report.?
We would like our algorithm to be able to recognize that Wordnet?s first sense of
the word body (the entire physical structure of an organism (especially an animal or
human being)) is the most appropriate for sentence 13 and the third sense (a group
of persons associated by some common tie or occupation and regarded as an entity)
for sentence 14. If we calculate the most likely sense using all of the first 50 nearest
neighbours in the thesaurus, we predict that sense 4 is the most frequently used sense
(the body excluding the head and neck and limbs).
However, the two uses of the target word in 13 and 14 appear each in a very spe-
cific grammatical context. How can we exploit this local context to single out a certain
subset of the 50 nearest neighbours, containing those words that are particularly rel-
evant for (or more closely related to) the grammatical relation that the target word is
involved in this particular sentence. The idea we pursue here is to look at those neigh-
bours in the thesaurus that occur in the same grammatical relation as our target word
and share a high mutual information (i.e. word and grammatical relation do not only
occur frequently together, but also when you see one, there is a high probability that
you see the other).
While creating the thesaurus we consider all the words that co-occur with a certain
target word (where co-occur means that it appears in the same grammatical relation).
We also calculate the mutual information of both the target word and the co-occurring
word and the grammatical relation. Instead of throwing this information away after
finishing an entry in the thesaurus, we now store this information in the grammatical
relation database.
Since this database grows to enormous proportions (in the order of 200GB for the
one built up while processing the BNC), we need to reduce its size to be able to
work with it. If we only keep those entries in the database that involve the words
in the thesaurus and their 50 neighbours, we can reduce the database to manageable
proportions. We experimented with reducing the number of entries in the database
even further by limiting the number of entries per grammatical relations to the ones
134 Koeling and McCarthy
with the highest mutual information scores, but this only had a negative effect on the
recall, without improving the precision. As we will see later, data sparseness is a
serious issue and it is therefore not advisable to cut-out any usable information that
we have at our disposal. The word sense disambiguation procedure that uses the local
context is then straightforward:
1. Parse the sentence with the target word (the word to be disambiguated).
2. If the target word is not involved with any of the 4 grammatical relations we
considered for building up the thesaurus, local context can not be used.
3. Otherwise, consult the database to retrieve the co-occurring words:
? Let GR be the set of triples < w,r,x > from equation 12 in Section 2.2 for
target word w.
? Let NGR be the set of triples < n j,r,x > from equation 12 for any neigh-
bour n j ? Nw
? For all w ? T and all top 50 n ? Nw, keep entries with < ?,r,x > in
database.
? Let SGR be the set of relations < r,x > in the target sentence, where I <
w,r,x > and I < n,r,x > are both positive (i.e. r,x are both in the target
sentence and have high MI in BNC for both w and n.)
4. Compute the ranking score for each sense by applying to a modified version of
the ranking equation (15) (compared to the original given in (11)), where the k
nearest neighbours are replaced by the subset found in the step 3.
(15) Prevalence Score ws_lci = ?n j?Nw MI?dssn j ?
wnss(wsi,n j)
?wsi? ?WSw wnss(wsi? ,n j)
where the WordNet similarity score (wnss) is defined as before and let MI be I <
n,r,x >, i.e. the Mutual Information given by the events of seeing the grammatical
relation in question and seeing the neighbour.
2.4 An example
The fact that a subset of the neighbours in the thesaurus share some specific relations
with the target word in a particular sentence is something that we wish to exploit for
Word Sense Disambiguation. Let us have a closer look at the two example sentences
13 and 14 that we introduced in the previous section.
The grammatical relations that our target word body is involved with are (from
sentences 13 and 14 respectively):3
(16) ?body? object of ?keep? for sentence 13 and
(17) ?body? subject of ?approved? and ?body? modified by the noun ?funding? for
sentence 14
3At the moment we only take 4 grammatical relations into account: Verb-Subject, Verb-Object, Adj-
Noun and Noun-Noun modifier.
From Predicting Predominant Senses to Local Context for WSD 135
Table 1: Results of evaluation on the nouns in SemCor
Method Attempted Correct Wrong Precision Recall
Local Context 23,235 11,904 11,331 0.512 0.161
First sense 23,235 11,795 11,440 0.508 ?
Since keep is a fairly general verb, it is not surprising that quite a few of the neighbours
occur as object of keep. As a matter of fact, 28 of the first 50 neighbours share this
relation. However, the good news is, that pretty much all the words associated with
body-parts (such as arm, hand, leg, face and head) are among them.
The two grammatical relations that body is involved with in sentence 14, are more
specific. There are just 6 neighbours that share the ?subject of approve? relation with
body and another 5 that are used to modify the noun body. Among these words are the
highly relevant words organisation, institution and board.
3 Evaluation on SemCor
The example in the last section shows that in certain cases the method performs the
way we envisaged. However, we need a quantitative evaluation to get a proper picture
of the method?s usefulness. We performed a full evaluation on SemCor. In this experi-
ment we limited our attention to nouns only. We further eliminated Proper Names and
multi-word units from the test set. Since the nouns in both these categories are mostly
monosemous, they are less interesting as test material and apart from that, they intro-
duce problems (mostly parser related) that have little to do with the proposed method.
A total of 73,918 words were left to evaluate. Table 1 summarizes the results. The fig-
ure for recall for the ?First Sense? method is not given, because we want to contrast the
local context method with the first sense method. Whilst the first sense method will
return an answer in most cases, the local context method proposed in this paper will
not. Here we want to focus on how we can improve on using the first sense heuristic
by taking local context into account, rather than give complete results for a WSD task.
There are several things to say about these results. First of all, even though the
results for ?local context? are slightly better than for ?first sense?, we expected more
from it. We had identified quite a few cases like 13 and 14 above, where the local
context seemed to be able to help to identify the right neigbours in order to make
the difference. Below, we will discuss a few cases where the grammatical relations
involved are so general, that the subset of neighbours is large and most importantly,
not discriminative enough. It seems to be reasonable to expect that the latter cases will
not influence the precision too much (i.e. a smaller group of neighbourswill often give
a different result, but some better, some worse).
The recall is also lower than expected. The first thought was that data sparseness
was the main problem here, but additional experiments showed us that that is unlike to
be the case. In one experiment we took a part of the GigaWord corpus (Graff, 2003),
similar in size to the written part of the BNC (used in our original experiment) and
built our grammatical relation database using the combined corpus. The recall went
up a little, but at the price of a slightly lower precision.
136 Koeling and McCarthy
4 Discussion
The main problem causing the low recall seems to be the small number of grammatical
relations that we use for building the thesaurus. The four relations used (verb-subject,
verb-object, noun-noun-modifier and adjective-noun-modifier) were chosen because
of the parsers? high accuracy for these. For building the thesaurus, these grammatical
relations suffice, since every word will occur in one of these relations sooner or later.
However, whenever in a sentence the target word occurs outside these four relations,
we are not able to look it up in our database. Nouns within prepositional phrases seem
to be a major victim here. It should be straightforward to experiment with including
prepositional phrase related grammatical relations. We will have to evaluate the influ-
ence of the introduced noise on creating the thesaurus. Alternatively, it is possible to
use the four relations as before for creating the thesaurus and store the extra relations
in our database just for look-up.
A second cause for missing target words is parser errors. Even though RASP will
produce partial parses whenever a full parse of a sentence is not available, some loss
is inevitable. This is a harder problem to solve. One way of solving this problem
might be by using a proximity thesaurus instead of a thesaurus build using grammat-
ical relatons. McCarthy et al (2007) reported promising results for using proximity
based thesaurus for predicting predominant senses, with accuracy figures closely be-
hind those achieved with a dependency based thesaurus.
One plausible reason why the method is not working in many cases, is the fact that
the word to be disambiguated in the target sentence often occurs in a very general
grammatical relation. For example, ?subject of? or ?direct object of? a verb like have.
In these cases, most of the neighbors in the thesaurus will be selected. Even though it
is clear that that would minimize the positive effect, it is not immediately obvious that
this would have a negative effect. It might therefore be the case that the number of
cases where the grammatical relation is a good selection criterion, is just lower than
we thought (although this is not the impression that you get when you look at the
data). We will need to establish a way of quantitatively evaluating this.
The Mutual Information score gives us a measure of the dependence between the
grammatical relation and the word (neighbour of the target word) we are interested
it. It gives us a handle on ?generality? of the combination of seeing both events. This
means that for a very common grammatical relation, many words will be expected to
co-occur with a frequency comparable to their general frequency in texts. The contrast
with relation/word combinations for which this is not the case might be usable for
identifying the cases that we want to exclude here.
5 Conclusions
In this paper we propose a completely unsupervised method for Word Sense Disam-
biguation that takes the local context of the target word into account. The starting
point for this method is a method for automatically predicting the predominant senses
of words. The grammatical relations that were used to create the distributional simi-
larity thesaurus is exploited to select a subset of the k neighbours in the thesaurus, to
focus on those neighbours that are used in the same grammatical context as the word
we want to disambiguate in the target sentence.
From Predicting Predominant Senses to Local Context for WSD 137
Even though the precision of our proposed method is slightly higher than for the
predominant sense method, we are disappointed by the current results. We do believe
that there is moremileage to be had from the methodwe suggest. Improvement of both
recall and precision is on the agenda for future research. As we stated in the previous
section, we believe that the lower than expected recall can be addressed fairly easily,
by considering more grammatical relations. This is straightforward to implement and
results can be expected in the near future.
A second approach, involving a thesaurus built on proximity, rather than grammat-
ical relations will also be investigated. Considering the expected lower precision for
this approach, we plan to use the proximity-based thesaurus as a ?back off? solution
in case we fail to produce an answer with the dependency-based thesaurus. When
the proximity-based thesaurus is in place, we plan to perform a full evaluation of the
dependency versus the proximity approach.
Before we can deal with improving the local context method?s precision, we need
to have a better idea of the circumstances in which the method gets it wrong. We have
identified a large group of examples, where it is unlikely that the method will be suc-
cessful. A first step will be to develop a method to identify these cases automatically
and eliminate those from the targets that we are attempting to try. In the previous
section, we sketched how we think that we can achieve this by applying a Pointwise
Mutual Information threshold. If we are successful, this will at least give us the op-
portunity to focus on the strengths and weaknesses of the method. At the moment, the
virtues of the method seem to be obscured too much by dealing with cases that should
not be considered.
More insight in the method can also be gained from trying to identify in which
situations the method is more likely to get it right. At the moment we haven?t broken
down the results yet in terms of the target word?s polysemy and/or frequency of use.
Some grammatical relations might be more useful for identifying the intended sense
than other. A detailed analysis could give us these insights.
We do believe there is a strong case to be made for using unsupervised methods
for Word Sense Disambiguation (apart from McCarthy et al (2004)?s predominant
sense method, other approaches include e.g. Basili et al (2006)). The predominant
sense method has proven to be successful. However, applying the first sense heuristic
should be limited to certain cases. We can think of the cases where the dominance
of the predominant sense is so strong, that there is little to gain from doing a proper
attempt to disambiguation or to the cases where ?everything else fails?. Ultimately,
our goal is to find a balance between the dominance of the predominant sense and the
strength of the evidence from the supporting context. If we are able to recognize the
correct clues from the local context and use these clues to focus on those words with
a high distributional similarity to the target word in the context in which the word is
actually used, we can build on work on predicting predominant senses, to rely less on
the first sense heuristic. This would be a good step forward for unsupervised WSD.
Acknowledgments This workwas funded byUK EPSRC project EP/C537262 ?Rank-
ing Word Senses for Disambiguation: Models and Applications?, and by a UK Royal
Society Dorothy Hodgkin Fellowship to the second author. We would like to thank
Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package avail-
able and Julie Weeds for the thesaurus software.
138 Koeling and McCarthy
References
Basili, R., M. Cammisa, and A. Gliozzo (2006). Integrating domain and paradig-
matic similarity for unsupervised sense tagging. In Proceedings of 7th European
Conference on Artificial Intelligence (ECAI06).
Briscoe, E. and J. Carroll (2002). Robust accurate statistical annotation of general
text. In Proceedings of the Third International Conference on Language Resources
and Evaluation (LREC), Las Palmas, Canary Islands, Spain, pp. 1499?1504.
Graff, D. (2003). English gigaword. Linguistic Data Consortium, Philadelphia.
Iida, R., D. McCarthy, and R. Koeling (2008). Gloss-based semantic similarity met-
rics for predominant sense acquisition. In Proceedings of the Third International
Joint Conference on Natural Language Processing, Hyderabad, India, pp. 561?568.
Jiang, J. and D. Conrath (1997). Semantic similarity based on corpus statistics and
lexical taxonomy. In 10th International Conference on Research in Computational
Linguistics, Taiwan, pp. 19?33.
Koeling, R., D. McCarthy, , and J. Carroll (2005). Domain-specific sense distribu-
tions and predominant sense acquisition. In Proceedings of the Human Language
Technology Conference and EMNLP, Vancouver, Canada, pp. 419?426.
Leech, G. (1992). 100 million words of English: the British National Corpus. Lan-
guage Research 28(1), 1?13.
Lin, D. (1998). Automatic retrieval and clustering of similar words. In Proceedings
of COLING-ACL?98, Montreal, Canada, pp. 768?774.
McCarthy, D., R. Koeling, J. Weeds, and J. Carroll (2004). Finding predominant
senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, Barcelona, Spain, pp. 280?287.
McCarthy, D., R. Koeling, J. Weeds, and J. Carroll (2007). Unsupervised acquisition
of predominant word senses. Computational Linguistics 33(4), 553?590.
Miller, G. A., C. Leacock, R. Tengi, and R. T. Bunker (1993). A semantic concor-
dance. In Proceedings of the ARPA Workshop on Human Language Technology,
pp. 303?308. Morgan Kaufman.
Patwardhan, S. and T. Pedersen (2003). The CPAN WordNet::Similarity Package.
http://search.cpan.org/?sid/WordNet-Similarity-0.05/.
Yarowsky, D. and R. Florian (2002). Evaluating sense disambiguation performance
across diverse parameter spaces. Natural Language Engineering 8(4), 293?310.
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, page 1,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Alternative Annotations of Word Usage
Diana McCarthy,
Department of Informatics,
University of Sussex
Falmer BN1 9QJ, UK
dianam@sussex.ac.uk
Abstract
Right from Senseval?s inception there have been questions over the choice of sense inventory for
word sense disambiguation (Kilgarriff, 1998). While researchers usually acknowledge the issues with
predefined listings produced by lexicographers, such lexical resources have been a major catalyst to
work on annotating words with meaning. As well as the heavy reliance on manually produced sense
inventories, the work on word sense disambiguation has focused on the task of selecting the single best
sense from the predefined inventory for each given token instance. There is little evidence that the
state-of-the-art level of success is sufficient to benefit applications. We also have no evidence that the
systems we build are interpreting words in context in the way that humans do. One direction that has
been explored for practical reasons is that of finding a level of granularity where annotators and systems
can do the task with a high level of agreement (Navigli et al, 2007; Hovy et al, 2006). In this talk I will
discuss some alternative annotations using synonyms (McCarthy and Navigli, 2007), translations (Sinha
et al, 2009) and WordNet senses with graded judgments (Erk et al, to appear) which are not proposed
as a panacea to the issue of semantic representation but will allow us to look at word usages in a more
graded fashion and which are arguably better placed to reflect the phenomena we wish to capture than
the ?winner takes all? strategy.
References
Katrin Erk, Diana McCarthy, and Nick Gaylord. Investigations on word senses and word usages. In Proceedings of
ACL-IJCNLP 2009, to appear.
Eduard Hovy, Mitch Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes: The 90% solution.
In Proceedings of the HLT-NAACL 2006 workshop on Learning word meaning from non-linguistic data, New York
City, USA, 2006. Association for Computational Linguistics.
Adam Kilgarriff. Gold standard datasets for evaluating word sense disambiguation programs. Computer Speech and
Language, 12(3):453?472, 1998.
Diana McCarthy and Roberto Navigli. SemEval-2007 task 10: English lexical substitution task. In Proceedings of
(SemEval-2007), pages 48?53, Prague, Czech Republic, 2007.
Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. SemEval-2007 task 7: Coarse-grained English all-words
task? In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30?35,
Prague, Czech Republic, 2007.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea. Semeval-2010 task 2: Cross-lingual lexical substitution. In Pro-
ceedings of the NAACL-HLT Workshop SEW-2009, Boulder, Colorado, USA, 2009.
1
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 54?60,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Exemplar-based Word-Space Model for Compositionality Detection: Shared
task system description
Siva Reddy
University of York, UK
siva@cs.york.ac.uk
Suresh Manandhar
University of York, UK
suresh@cs.york.ac.uk
Diana McCarthy
Lexical Computing Ltd, UK
diana@dianamccarthy.co.uk
Spandana Gella
University of York, UK
spandana@cs.york.ac.uk
Abstract
In this paper, we highlight the problems of
polysemy in word space models of compo-
sitionality detection. Most models represent
each word as a single prototype-based vec-
tor without addressing polysemy. We propose
an exemplar-based model which is designed
to handle polysemy. This model is tested for
compositionality detection and it is found to
outperform existing prototype-based models.
We have participated in the shared task (Bie-
mann and Giesbrecht, 2011) and our best per-
forming exemplar-model is ranked first in two
types of evaluations and second in two other
evaluations.
1 Introduction
In the field of computational semantics, to represent
the meaning of a compound word, two mechanisms
are commonly used. One is based on the distribu-
tional hypothesis (Harris, 1954) and the other is on
the principle of semantic compositionality (Partee,
1995, p. 313).
The distributional hypothesis (DH) states that
words that occur in similar contexts tend to have
similar meanings. Using this hypothesis, distribu-
tional models like the Word-space model (WSM,
Sahlgren, 2006) represent a target word?s meaning
as a context vector (location in space). The simi-
larity between two meanings is the closeness (prox-
imity) between the vectors. The context vector of a
target word is built from its distributional behaviour
observed in a corpus. Similarly, the context vector of
a compound word can be built by treating the com-
pound as a single word. We refer to such a vector as
a DH-based vector.
The other mechanism is based on the principle of
semantic compositionality (PSC) which states that
the meaning of a compound word is a function of,
and only of, the meaning of its parts and the way
in which the parts are combined. If the meaning of
a part is represented in a WSM using the distribu-
tional hypothesis, then the principle can be applied
to compose the distributional behaviour of a com-
pound word from its parts without actually using the
corpus instances of the compound. We refer to this
as a PSC-based vector. So a PSC-based is composed
of component DH-based vectors.
Both of these two mechanisms are capable of de-
termining the meaning vector of a compound word.
For a given compound, if a DH-based vector and
a PSC-based vector of the compound are projected
into an identical space, one would expect the vec-
tors to occupy the same location i.e. both the vectors
should be nearly the same. However the principle
of semantic compositionality does not hold for non-
compositional compounds, which is actually what
the existing WSMs of compositionality detection ex-
ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;
Schone and Jurafsky, 2001). The DH-based and
PSC-based vectors are expected to have high simi-
larity when a compound is compositional and low
similarity for non-compositional compounds.
Most methods in WSM (Turney and Pantel, 2010)
represent a word as a single context vector built from
merging all its corpus instances. Such a representa-
tion is called the prototype-based modelling (Mur-
phy, 2002). These prototype-based vectors do not
54
distinguish the instances according to the senses of
a target word. Since most compounds are less am-
biguous than single words, there is less need for dis-
tinguishing instances in a DH-based prototype vec-
tor of a compound and we do not address that here
but leave ambiguity of compounds for future work.
However the constituent words of the compound are
more ambiguous. When DH-based vectors of the
constituent words are used for composing the PSC-
based vector of the compound, the resulting vec-
tor may contain instances, and therefore contexts,
that are not relevant for the given compound. These
noisy contexts effect the similarity between the PSC-
based vector and the DH-based vector of the com-
pound. Basing compositionality judgements on a
such a noisy similarity value is no longer reliable.
In this paper, we address this problem of pol-
ysemy of constituent words of a compound using
an exemplar-based modelling (Smith and Medin,
1981). In exemplar-based modelling of WSM (Erk
and Pado?, 2010), each word is represented by all its
corpus instances (exemplars) without merging them
into a single vector. Depending upon the purpose,
only relevant exemplars of the target word are acti-
vated and then these are merged to form a refined
prototype-vector which is less-noisy compared to
the original prototype-vector. Exemplar-based mod-
els are more powerful than prototype-based ones be-
cause they retain specific instance information.
We have evaluated our models on the validation
data released in the shared task (Biemann and Gies-
brecht, 2011). Based on the validation results, we
have chosen three systems for public evaluation and
participated in the shared task (Biemann and Gies-
brecht, 2011).
2 Word Space Model
In this section, construction of WSM for all our ex-
periments is described. We use Sketch Engine1 (Kil-
garriff et al, 2004) to retrieve all the exemplars for
a target word or a pattern using corpus query lan-
guage. Let w1 w2 be a compound word with con-
stituent words w1 and w2. Ew denotes the set of
exemplars of w. Vw is the prototype vector of the
word w, which is built by merging all the exemplars
in Ew
1Sketch Engine http://www.sketchengine.co.uk
For the purposes of producing a PSC-based vector
for a compound, a vector of a constituent word is
built using only the exemplars which do not contain
the compound. Note that the vectors are sensitive
to a compound?s word-order since the exemplars of
w1 w2 are not the same as w2 w1.
We use other WSM settings following Mitchell
and Lapata (2008). The dimensions of the WSM
are the top 2000 content words in the given corpus
(along with their coarse-grained part-of-speech in-
formation). Cosine similarity (sim) is used to mea-
sure the similarity between two vectors. Values at
the specific positions in the vector representing con-
text words are set to the ratio of the probability of
the context word given the target word to the overall
probability of the context word. The context window
of a target word?s exemplar is the whole sentence of
the target word excluding the target word. Our lan-
guage of interest is English. We use the ukWaC cor-
pus (Ferraresi et al, 2008) for producing out WSMs.
3 Related Work
As described in Section 1, most WSM models for
compositionality detection measure the similarity
between the true distributional vector Vw1w2 of the
compound and the composed vector Vw1?w2 , where
? denotes a compositionality function. If the simi-
larity is high, the compound is treated as composi-
tional or else non-compositional.
Giesbrecht (2009); Katz and Giesbrecht (2006);
Schone and Jurafsky (2001) obtained the compo-
sitionality vector of w1 w2 using vector addition
Vw1?w2 = aVw1 + bVw2 . In this approach, if
sim(Vw1?w2 , Vw1w2) > ?, the compound is clas-
sified as compositional, where ? is a threshold for
deciding compositionality. Global values of a and b
were chosen by optimizing the performance on the
development set. It was found that no single thresh-
old value ? held for all compounds. Changing the
threshold alters performance arbitrarily. This might
be due to the polysemous nature of the constituent
words which makes the composed vector Vw1?w2
filled with noisy contexts and thus making the judge-
ment unpredictable.
In the above model, if a=0 and b=1, the result-
ing model is similar to that of Baldwin et al (2003).
They also observe similar behaviour of the thresh-
55
old ?. We try to address this problem by addressing
the polysemy in WSMs using exemplar-based mod-
elling.
The above models use a simple addition based
compositionality function. Mitchell and Lapata
(2008) observed that a simple multiplication func-
tion modelled compositionality better than addi-
tion. Contrary to that, Guevara (2011) observed
additive models worked well for building composi-
tional vectors. In our work, we try using evidence
from both compositionality functions, simple addi-
tion and simple multiplication.
Bannard et al (2003); McCarthy et al (2003) ob-
served that methods based on distributional similar-
ities between a phrase and its constituent words help
when determining the compositionality behaviour of
phrases. We therefore also use evidence from the
similarities between each constituent word and the
compound.
4 Our Approach: Exemplar-based Model
Our approach works as follows. Firstly, given a
compound w1 w2, we build its DH-based proto-
type vector Vw1w2 from all its exemplars Ew1w2 .
Secondly, we remove irrelevant exemplars in Ew1
and Ew2 of constituent words and build the refined
prototype vectors Vwr1 and Vwr2 of the constituent
words w1 and w2 respectively. These refined vec-
tors are used to compose the PSC-based vectors 2 of
the compound. Related work to ours is (Reisinger
and Mooney, 2010) where exemplars of a word are
first clustered and then prototype vectors are built.
This work does not relate to compositionality but to
measuring semantic similarity of single words. As
such, their clusters are not influenced by other words
whereas in our approach for detecting composition-
ality, the other constituent word plays a major role.
We use the compositionality functions, sim-
ple addition and simple multiplication to build
Vwr1+wr2 and Vwr1?wr2 respectively. Based on
the similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),
sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1?wr2), we
decide if the compound is compositional or non-
compositional. These steps are described in a little
more detail below.
2Note that we use two PSC-based vectors for representing a
compound.
4.1 Building Refined Prototype Vectors
We aim to remove irrelevant exemplars of one con-
stituent word with the help of the other constituent
word?s distributional behaviour. For example, let
us take the compound traffic light. Light occurs
in many contexts such as quantum theory, optics,
lamps and spiritual theory. In ukWaC, light has
316,126 instances. Not all these exemplars are rel-
evant to compose the PSC-based vector of traffic
light. These irrelevant exemplars increases the se-
mantic differences between traffic light and light and
thus increase the differences between Vtraffic?light
and Vtraffic light. sim(Vlight, Vtraffic light) is found to be
0.27.
Our intuition and motivation for exemplar re-
moval is that it is beneficiary to choose only the
exemplars of light which share similar contexts of
traffic since traffic light should have contexts sim-
ilar to both traffic and light if it is compositional.
We rank each exemplar of light based on common
co-occurrences of traffic and also words which are
distributionally similar to traffic. Co-occurrences of
traffic are the context words which frequently occur
with traffic, e.g. car, road etc. Using these, the
exemplar from a sentence such as ?Cameras capture
cars running red lights . . .? will be ranked higher
than one which does not have contexts related to
traffic. The distributionally similar words to traffic
are the words (like synonyms, antonyms) which are
similar to traffic in that they occur in similar con-
texts, e.g. transport, flow etc. Using these distri-
butionally similar words helps reduce the impact of
data sparseness and helps prioritise contexts of traf-
fic which are semantically related. We use Sketch
Engine to compute the scores of a word observed
in a given corpus. Sketch Engine scores the co-
occurrences (collocations) using logDice motivated
by (Curran, 2003) and distributionally related words
using (Rychly? and Kilgarriff, 2007; Lexical Com-
puting Ltd., 2007). For a given word, both of these
scores are normalised in the range (0,1)
All the exemplars of light are ranked based on
the co-occurrences of these collocations and distri-
butionally related words of traffic using
strafficE ? Elight =
?
c ? E
xEc ? y
traffic
c (1)
where strafficE ? Elight stands for the relevance score of the
56
exemplar E w.r.t. traffic, c for context word in the
exemplar E, xEc is the coordinate value (contextual
score) of the context word c in the exemplar E and
ytrafficc is the score of the context word c w.r.t. traffic.
A refined prototype vector of light is then built by
merging the top n exemplars of light
Vlightr =
n?
ei?Etrafficlight ;i=0
ei (2)
where Etrafficlight are the set of exemplars of light
ranked using co-occurrence information from the
other constituent word traffic. n is chosen such that
sim(Vlightr , Vtraffic light) is maximised. This similar-
ity is observed to be greatest using just 2286 (less
than 1%) of the total exemplars of light. After ex-
emplar removal, sim(Vlightr , Vtraffic light) increased to
0.47 from the initial value of 0.27. Though n is cho-
sen by maximising similarity, which is not desirable
for non-compositional compounds, the lack of simi-
larity will give the strongest possible indication that
a compound is not compositional.
4.2 Building Compositional Vectors
We use the compositionality functions, simple ad-
dition and simple multiplication to build composi-
tional vectors Vwr1+wr2 and Vwr1?wr2 . These are as de-
scribed in (Mitchell and Lapata, 2008). In model ad-
dition, Vw1?w2 = aVw1 + bVw2 , all the previous ap-
proaches use static values of a and b. Instead, we use
dynamic weights computed from the participating
vectors using a =
sim(Vw1w2 ,Vw1 )
sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )
and b = 1?a. These weights differ from compound
to compound.
4.3 Compositionality Judgement
To judge if a compound is compositional or non-
compositional, previous approaches (see Section 3)
base their judgement on a single similarity value. As
discussed, we base our judgement based on the col-
lective evidences from all the similarity values using
a linear equation of the form
?(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)
+ a2.sim(Vw1w2 , Vwr2) (3)
+ a3.sim(Vw1w2 , Vwr1+wr2)
+ a4.sim(Vw1w2 , Vwr1?wr2)
Model APD Acc.
Exm-Best 13.09 88.0
Pro-Addn 15.42 76.0
Pro-Mult 17.52 80.0
Pro-Best 15.12 80.0
Table 1: Average Point Difference (APD) and Av-
erage Accuracy (Acc.) of Compositionality Judge-
ments
where the value of ? denotes the compositionality
score. The range of ? is in between 0-100. If ? ?
34, the compound is treated as non-compositional,
34 < ? < 67 as medium compositional and ? ?
67 as highly compositional. The parameters ai?s
are estimated using ordinary least square regression
by training over the training data released in the
shared task (Biemann and Giesbrecht, 2011). For
the three categories ? adjective-noun, verb-object
and subject-verb ? the parameters are estimated sep-
arately.
Note that if a1 = a2 = a4 = 0, the model bases
its judgement only on addition. Similarly if a1 =
a2 = a3 = 0, the model bases its judgement only on
multiplication.
We also experimented with combinations such as
?(Vwr1 , Vw2) and ?(Vw1 , Vwr2) i.e. using refined vec-
tor for one of the constituent word and the unrefined
prototype vector for the other constituent word.
4.4 Selecting the best model
To participate in the shared task, we have selected
the best performing model by evaluating the mod-
els on the validation data released in the shared task
(Biemann and Giesbrecht, 2011). Table 1 displays
the results on the validation data. The average point
difference is calculated by taking the average of the
difference in a model?s score ? and the gold score
annotated by humans, over all compounds. Table 1
also displays the overall accuracy of coarse grained
labels ? low, medium and high.
Best performance for verb(v)-object(o) com-
pounds is found for the combination ?(Vvr , Vor) of
Equation 3. For subject(s)-verb(v) compounds, it is
for ?(Vsr , Vvr) and a3 = a4 = 0. For adjective(j)-
noun(n) compounds, it is ?(Vjr , Vn). We are not
certain of the reason for this difference, perhaps
there may be less ambiguity of words within specific
grammatical relationships or it may be simply due to
57
TotPrd Spearman ? Kendalls ?
Rand-Base 174 0.02 0.02
Exm-Best 169 0.35 0.24
Pro-Best 169 0.33 0.23
Exm 169 0.26 0.18
SharedTaskNextBest 174 0.33 0.23
Table 2: Correlation Scores
the actual compounds in those categories. We leave
analysis of this for future work. We combined the
outputs of these category-specific models to build
the best model Exm-Best.
For comparison, results of standard mod-
els prototype addition (Pro-Addn) and prototype-
multiplication (Pro-Mult) are also displayed in Table
1. Pro-Addn can be represented as ?(Vw1 , Vw2) with
a1 = a2 = a4 = 0. Pro-Mult can be represented as
?(Vw1 , Vw2) with a1 = a2 = a3 = 0. Pro-Best is
the best performing model in prototype-based mod-
elling. It is found to be ?(Vw1 , Vw2). (Note: De-
pending upon the compound type, some of the ai?s
in Pro-Best may be 0).
Overall, exemplar-based modelling excelled in
both the evaluations, average point difference and
coarse-grained label accuracies. The systems Exm-
Best, Pro-Best and Exm ?(Vwr1 , Vwr2) were submit-
ted for the public evaluation in the shared task. All
the model parameters were estimated by regression
on the task?s training data separately for the 3 com-
pound types as described in Section 4.3. We perform
the regression separately for these classes to max-
imise performance. In the future, we will investigate
whether these settings gave us better results on the
test data compared to setting the values the same re-
gardless of the category of compound.
5 Shared Task Results
Table 2 displays Spearman ? and Kendalls ? corre-
lation scores of all the models. TotPrd stands for
the total number of predictions. Rand-Base is the
baseline system which randomly assigns a compo-
sitionality score for a compound. Our model Exm-
Best was the best performing system compared to
all other systems in this evaluation criteria. Shared-
TaskNextBest is the next best performing system
apart from our models. Due to lemmatization er-
rors in the test data, our models could only predict
judgements for 169 out of 174 compounds.
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 32.82 34.57 29.83 32.34
Zero-Base 23.42 24.67 17.03 25.47
Exm-Best 16.51 15.19 15.72 18.6
Pro-Best 16.79 14.62 18.89 18.31
Exm 17.28 15.82 18.18 18.6
SharedTaskBest 16.19 14.93 21.64 14.66
Table 3: Average Point Difference Scores
All ADJ-NN V-SUBJ V-OBJ
Rand-Base 0.297 0.288 0.308 0.30
Zero-Base 0.356 0.288 0.654 0.25
Most-Freq-Base 0.593 0.673 0.346 0.65
Exm-Best 0.576 0.692 0.5 0.475
Pro-Best 0.567 0.731 0.346 0.5
Exm 0.542 0.692 0.346 0.475
SharedTaskBest 0.585 0.654 0.385 0.625
Table 4: Coarse Grained Accuracy
Table 3 displays average point difference scores.
Zero-Base is a baseline system which assigns a score
of 50 to all compounds. SharedTaskBest is the over-
all best performing system. Exm-Best was ranked
second best among all the systems. For ADJ-NN
and V-SUBJ compounds, the best performing sys-
tems in the shared task are Pro-Best and Exm-Best
respectively. Our models did less well on V-OBJ
compounds and we will explore the reasons for this
in future work.
Table 4 displays coarse grained scores. As above,
similar behaviour is observed for coarse grained ac-
curacies. Most-Freq-Base is the baseline system
which assigns the most frequent coarse-grained la-
bel for a compound based on its type (ADJ-NN, V-
SUBJ, V-OBJ) as observed in training data. Most-
Freq-Base outperforms all other systems.
6 Conclusions
In this paper, we examined the effect of polysemy
in word space models for compositionality detec-
tion. We showed exemplar-based WSM is effective
in dealing with polysemy. Also, we use multiple
evidences for compositionality detection rather than
basing our judgement on a single evidence. Over-
all, performance of the Exemplar-based models of
compositionality detection is found to be superior to
prototype-based models.
58
References
Baldwin, T., Bannard, C., Tanaka, T., and Widdows,
D. (2003). An empirical model of multiword ex-
pression decomposability. In Proceedings of the
ACL 2003 workshop on Multiword expressions:
analysis, acquisition and treatment - Volume 18,
MWE ?03, pages 89?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL 2003 work-
shop on Multiword expressions: analysis, ac-
quisition and treatment - Volume 18, MWE ?03,
pages 65?72, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Biemann, C. and Giesbrecht, E. (2011). Distri-
butional semantics and compositionality 2011:
Shared task description and results. In Pro-
ceedings of DISCo-2011 in conjunction with ACL
2011.
Curran, J. R. (2003). From distributional to semantic
similarity. Technical report, PhD Thesis, Univer-
sity of Edinburgh.
Erk, K. and Pado?, S. (2010). Exemplar-based mod-
els for word meaning in context. In Proceed-
ings of the ACL 2010 Conference Short Papers,
ACLShort ?10, pages 92?97, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-
dini, S. (2008). Introducing and evaluating
ukwac, a very large web-derived corpus of en-
glish. In Proceedings of the WAC4 Workshop at
LREC 2008, Marrakesh, Morocco.
Giesbrecht, E. (2009). In search of semantic com-
positionality in vector spaces. In Proceedings
of the 17th International Conference on Concep-
tual Structures: Conceptual Structures: Leverag-
ing Semantic Technologies, ICCS ?09, pages 173?
184, Berlin, Heidelberg. Springer-Verlag.
Guevara, E. R. (2011). Computing semantic com-
positionality in distributional semantics. In Pro-
ceedings of the Ninth International Conference on
Computational Semantics, IWCS ?2011.
Harris, Z. S. (1954). Distributional structure. Word,
10:146?162.
Katz, G. and Giesbrecht, E. (2006). Automatic
identification of non-compositional multi-word
expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underly-
ing Properties, MWE ?06, pages 12?19, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.
(2004). The sketch engine. In Proceedings of EU-
RALEX.
Lexical Computing Ltd. (2007). Statistics used in
the sketch engine.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis,
acquisition and treatment - Volume 18, MWE ?03,
pages 73?80, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mitchell, J. and Lapata, M. (2008). Vector-based
Models of Semantic Composition. In Proceed-
ings of ACL-08: HLT, pages 236?244, Columbus,
Ohio. Association for Computational Linguistics.
Murphy, G. L. (2002). The Big Book of Concepts.
The MIT Press.
Partee, B. (1995). Lexical semantics and compo-
sitionality. L. Gleitman and M. Liberman (eds.)
Language, which is Volume 1 of D. Osherson (ed.)
An Invitation to Cognitive Science (2nd Edition),
pages 311?360.
Reisinger, J. and Mooney, R. J. (2010). Multi-
prototype vector-space models of word mean-
ing. In Proceedings of the 11th Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-
2010), pages 109?117.
Rychly?, P. and Kilgarriff, A. (2007). An efficient
algorithm for building a distributional thesaurus
(and other sketch engine developments). In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Ses-
sions, ACL ?07, pages 41?44, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Sahlgren, M. (2006). The Word-Space Model: Us-
ing distributional analysis to represent syntag-
59
matic and paradigmatic relations between words
in high-dimensional vector spaces. PhD thesis,
Stockholm University.
Schone, P. and Jurafsky, D. (2001). Is knowledge-
free induction of multiword unit dictionary head-
words a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?01.
Smith, E. E. and Medin, D. L. (1981). Categories
and concepts / Edward E. Smith and Douglas L.
Medin. Harvard University Press, Cambridge,
Mass. :.
Turney, P. D. and Pantel, P. (2010). From frequency
to meaning: vector space models of semantics. J.
Artif. Int. Res., 37:141?188.
60
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 13?17,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
SemLink+: FrameNet, VerbNet and Event Ontologies 
 Martha Palmer, Claire Bonial Department of Linguistics University of Colorado  Boulder, CO  mpalmer/Claire.Bonial@colorado.edu 
 Diana McCarthy Department of Theoretical and Applied Linguistics (DTAL)  University of Cambridge  diana@dianamccarthy.co.uk     Abstract This paper reviews the significant contributions FrameNet has made to our understanding of lexical resources, semantic roles and event relations. 1 Introduction  One of the great challenges of Natural Language Processing (NLP) is the multitude of choices that language gives us for expressing the same thing in different ways. This is obviously true when taking other languages into consideration - the same thought can be expressed in English, French, Chinese or Russian, with widely varying results. However, it is also true when considering a single language such as English. Light verb constructions, nominalizations, idioms, slang, paraphrases, and synonyms all give us myriads of alternatives for ?coining a phrase.?  This causes immense difficulty for NLP systems.  No one has made greater contributions to advancing the state of the art of lexical semantics, and its applications to NLP, than Chuck Fillmore.  In this paper we focus on the central role that FrameNet has played in our development of SemLink+ and in our current explorations into event ontologies that can play a practical role in accurate automatic event extraction. 2 Detecting events An elusive goal of current NLP systems is the accurate detection of events ? recognizing the meaningful relations among the topics, people, 
places   and  events   buried  within text. These relations can be very complex, and are not always explicit, requiring subtle semantic interpretation of the data.  For instance, NLP systems must be able to automatically recognize that Stock prices sank and The stock market is falling can be describing the same event. Such an interpretation relies upon a  recognition of the similarity between sinking and falling, as well as noting the connection between stock prices and the stock market, and, finally, acknowledgment that they are playing the same role. A key element in event extraction is the identification of the participants of an event, such as the initiator of an action and any parties affected by it.  Basically who did what to whom, when, where, why and how? Many systems today rely on semantic role labeling to help identify participants, and lexical resources that provide an inventory of possible predicate argument structures for individual lexical items are crucial to the success of semantic role labeling (Palmer,et al., 2010).  3 SemLink+ and  Semantic Roles SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2004), and the recently added OntoNotes (ON) sense groupings (Weischedel, et al., 2011). They all associate semantic information with the propositions in a sentence.  Each was created independently with somewhat differing goals, and they vary in the level and nature of semantic detail represented. FrameNet is the 
13
most fine-grained with the richest semantics, VerbNet     focuses    on     syntactically-based generalizations that carry semantic implications, and the relatively coarse-grained PropBank has been shown to provide the most effective training data for supervised Machine Learning techniques.  Nonetheless, they can be seen as complementary rather than conflicting, and together comprise a whole that is greater than the sum of its parts. SemLink serves as a platform to unify these resources.  The recent addition of ON sense groupings, which can be thought of as a more coarse-grained view of WordNet (Fellbaum, 1998), provides even broader coverage for verbs, and a level of representation that is appropriate for linking between VerbNet class members and FrameNet lexical units, as described below.    SemLink unifies these lexical resources at several different levels.  First by providing type-to-type mappings between the lexical units for each framework.  For PropBank these are the very coarse-grained rolesets, for VerbNet  they are verbs that are members of VerbNet classes, and for FrameNet they are the lexical units associated with each Frame.  The same lemma can have multiple PropBank rolesets and can be in several VerbNet classes and FrameNet frames, but always with different meanings. In general, the mappings from PropBank to VerbNet or FrameNet tend to be 1-many, while the mappings between VerbNet and FrameNet are more likely to be 1-1.  For example, the verb hear has just one coarse-grained sense in PropBank, with the following roleset:  Arg0: hearer Arg1: utterance, sound Arg2: speaker, source of sound  This roleset maps to both the Discover and See classes of VerbNet, and the Hear and Perception_experience frames of FrameNet.      Then, for each lexical unit, SemLink also supplies a mapping between the semantic roles of PropBank and VerbNet, as well as the roles of  VerbNet and FrameNet. PropBank uses very generic labels such as Arg0 and Arg1, which correspond to Dowty?s Prototypical Agent and Patient, respectively (Dowty, 1991).  PropBank has up to six numbered arguments 
for core verb specific roles and for adjuncts it has several generally applicable ArgModifiers that have function tag labels such as: MaNneR, TeMPoral, LOCation, DIRection, GOaL, etc. VerbNet uses more traditional linguistic thematic role labels, with about 30 in total, and assumes adjuncts (ArgM?s) will be supplied by PropBank based semantic role labelers.  FrameNet is even more fine-grained and has frame-specific core and peripheral roles called Frame Elements for each frame, amounting to over 2000 individual Frame Element types.  For example, He talked about politics would receive the following semantic role labels from each framework.1   PropBank (talk.01) HeArg0 talkedRELATION about politicsArg1    VerbNet (Talk-37.5):  HeAGENT talkedRELATION about politicsTOPIC  FrameNet (Statement frame):  HeSPEAKER talkedRELATION about politicsTOPIC      Thanks to Chuck Fillmore?s careful guidance, the rich, meticulously crafted Frames in FrameNet, with their detailed descriptions of all possible arguments and their relations to each other, offer the potential of providing a foundation for inferencing about events and their consequences.  In addition FrameNet has from the beginning been inclusive in its addition of nominal and adjectival forms to the Frames, which greatly increases our coverage of all predicating elements (Bonial, et al., 2014).  There is also a comprehensive FrameNet Constructicon that painstakingly lists many phrasal constructions, such as ?the Xer, the Yer? that cannot be found anywhere else (Fillmore, et al., 2012). Many of these frames, including the constructions, apply equally well to other languages,  as evidenced by the various efforts to develop FrameNets in other languages2 promising a likely benefit to multilingual information 
                                                            1 Arg0 maps to Agent maps to Speaker.  Arg1 maps to Topic maps to Topic. 2 See FrameNet projects in other languages listed at https://framenet.icsi.berkeley.edu/fndrupal/framenets_in_other_languages 
14
processing as well.  Given the close theoretical ties between PropBank, VerbNet and FrameNet, it should be possible to bootstrap from the successful PropBank-based automatic semantic role labelers to equally accurate FrameNet and VerbNet annotators, and to improve overall semantic role labeling performance (Bauer & Rambow, 2011; Dipanjan, et al., 2010; Giuglea & Moschitti, 2006; Merlo & der Plas, 2009; Yi, et al., 2007).  That is one of the primary goals of SemLink.     The first release of SemLink (1.1) contained mappings between these three lexical resources as well as a set of PropBank instances from the Wall Street Journal data with mappings to VerbNet classes and thematic roles (Palmer, 2009).  Our most recent release, SemLink 1.2,3 now includes mappings to FrameNet frames and Frame Elements wherever they are available (FN version 1.5), as well as ON sense groupings (Bonial, et al., 2013). The mapping files between PropBank and VerbNet (version 3.2), and FrameNet have also been checked for consistency and updated to more accurately reflect the current relations between these resources.    This annotated corpus can now be used to train and evaluate VerbNet Class and FrameNet Frame classifiers, to explore clusters of Frame Elements that map to the same VerbNet and PropBank semantic roles, and to evaluate approaches to semantic role labeling that use the type-to-type mappings to bootstrap VerbNet and FrameNet role labels from automatic PropBank semantic role labels. 4 Events, Event Types and Subevents Accurate and informative semantic role labels are an essential component of event extraction, but, although necessary, they are not sufficient. Automatic event detection also requires the ability to distinguish between events which are truly separate, such as Yesterday, John was throwing a ball to Mary and Bill was flying a kite, as opposed to related events such as John was washing the dishes and Mary was drying them.  The second pair could be seen as temporally related subevents of an overall doing the dishes or cleaning up                                                             3 available for download here: http://verbs.colorado.edu/semlink/ 
the kitchen event. It can sometimes be quite challenging to determine the relationship between two events. For instance, earthquakes are quite often associated with the collapse of buildings, as in the following example, The quake destroyed parts of Sausalito.  All tall buildings were demolished.  Many readers might agree that the earthquakes CAUSED the demolishment of the buildings. However, are the building collapses also SUBEVENTs of the earthquakes?  Sometimes they happen a few days later, or immediately, simultaneously with the earthquake. Are they both subevents? In general, for accurate event detection, it would be very useful to know which events must precede, must follow, or cannot be simultaneous with, which other events.   As discussed in the 2013 NAACL Events workshop and this year?s ACL Events workshop, clear, consistent annotation of events and their coreference and causal and temporal relations is a much desired but very challenging goal (Ikuta & Palmer, 2014).  Any assistance that can be provided by lexical resources is welcome. Another very important contribution that FrameNet has made is in the realm of defining these kinds of relations, and others, between frames.  Parent-Child Frame to Frame relations can include Inheritance, Subframe, Perspective On, Using, Causative Of, Inchoative of, and there is also a Precedes temporal ordering relation.   The DEFT working group in Richer Event Descriptions has recently been exploring expanding the ACE and ERE event types, and how they can be mapped onto a broader ontological context.  Exploring the FrameNet relations that the relevant lexical items participate in has been most informative. We first examined the simple LDC ERE classification of Conflict events, which has demonstrations and attacks as siblings (ERE guidelines). We find FrameNet?s classification of attacks as Hostile-Encounters quite useful, and have no argument with it having an Inheritance relation with Intentionally_act, and a Using relation with Taking_sides. Demonstrations, on the other hand, come under the Protest Frame, which has a Using relation with Taking_sides. The FrameNet 
15
organization of demonstrations and attacks, although perfectly justifiable, doesn?t map neatly onto the LDC organization since, although they are close, they are not siblings.  However, by also considering SUMO (Niles & Pease, 2001), the Predicate Matrix (de Lacalle , et al., 2014), WordNet and VerbNet, we were able to develop the upper level partial Event Ontology given in Figure 1, which comfortably incorporates the ERE and FrameNet relations within a broader framework, preserving the key aspects of each.   We are now discussing the ERE Life events, birth, death, injury, marriage, divorce, etc., and FrameNet is again proving to be inspirational.  SemLink+ will encompass our growing Event Ontology, as well as the mappings between the resources and the multiple layers of annotation on the same data.  
 Figure 1 ? SemLink+ Event Ontology, partial  5 Conclusion Since computers do not interact with and experience the world the same way humans do, how could they ever interpret language describing the world the same way humans do?  That NLP has made as much progress as it has is truly phenomenal, and there is much more still that can be done.  Rich, detailed, lexical resources like FrameNet are major stepping stones that will enable continued improvements in the automatic representation of sentences in context. FrameNet, and WordNet, PropBank, VerbNet and SemLink+, provide priceless, invaluable information about myriads of different types of events and the creative ways in which they can be expressed, 
as well as rich details about all of their possible participants.  If we can harness the power of distributional semantics to help us dynamically extend and enrich what has already been manually created, we may find our computers to be much smarter than we ever imagined them to be. Acknowledgments This work has benefited immensely from comments and suggestions during the discussions of the RED working group on Event Ontologies, especially from Teruko Mitamura, Annie Zaenen, Ann Bies, and German Rigau.  We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing, DARPA FA-8750-13-2-0045, subaward 560215 (via LDC) DEFT: Deep Exploration and Filtering of Text, DARPA Machine Reading (via BBN), and NIH: 1 R01 LM010090-01A1, THYME, (via Harvard).  The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA, NSF or NIH. References Daniel Bauer & Owen Rambow, 2011, Increasing Coverage of Syntactic Subcategorization Patterns in FrameNet Using VerbNet, In the Proceedings of the IEEE Fifth International Conference on Semantic Computing. Claire Bonial, Julia Bonn, Kathryn Conger, Jena Hwang and Martha Palmer, 2014.  PropBank: Semantics of New Predicate Types. The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Claire Bonial,  Kevin, Stowe, and Martha Palmer, 2013. Renewing and Revising SemLink. The GenLex Workshop on Linked Data in Linguistics, held with GenLex-13.  Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith, 2010. Probabilistic Frame-Semantic Parsing. In Proceedings of the NAACL 2010.  David Dowty, 1991. Thematic Proto-Roles and Argument Selection. Language, 67:547-619 Christiane Fellbaum, 1998. WordNet: An Electronic Lexical Data-base. Language, Speech and Communications. MIT Press 
16
Charles. J. Fillmore; Collin F. Baker, and H. Sato, 2004. FrameNet as a ``Net".  In Proceedings of LREC 2004, 4, pages 1091-1094 Charles J. Fillmore, Russell R. Lee-Goldman, and Russell Rhodes. 2012. ?The FrameNet Constructicon? Boas, H.C. and Sag, I.A. (Eds.) Sign-based Construction Grammar, CSLI Publications. Ana-Maria Guiglea and Alessandro Moschitti. 2006. Semantic role labeling via FrameNet, VerbNet and PropBank. In Proceedings of Coling-ACL 2006, pages 929?936. Rei Ikuta and Martha Palmer (2014) Challenges of Adding Causation to Richer Event Descriptions, In the Proceedings of 2nd Events Workshop, held with ACL 2014, Baltimore, MD. Karin Kipper, Anna Korhonen, Neville Ryant and Martha Palmer. 2008. A Large-Scale Classification of English Verbs. Language Resources and Evaluation Journal, 42(1):21?40 Maddalen Lopez de Lacalle, Egoitz Laparra, German Rigau, 2014, Predicate Matrix: extending SemLink throughWordNet mappings, The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Ian Niles and Adam Pease, 2001. Towards a Standard Upper Ontology. In Proceedings of the 2nd International Conference on Formal 
Ontology in Information Systems (FOIS-2001), Chris Welty and Barry Smith, eds, Ogunquit, Maine, October 17-19, 2001. Paola Merlo and Lonneke van der Plas. 2009. Abstraction and generalization in semantic role labels: PropBank, VerbNet or both?, In the Proceedings of  ACL 2009. Martha Palmer, 2009. SemLink: Linking PropBank, VerbNet and FrameNet, In the Proceedings of the Generative Lexicon Conference, GenLex-09. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106 Martha Palmer, Daniel Gildea and Nianwen Xue. Semantic Role Labeling. 2010. Synthesis Lectures on Human Language Technology Series, ed. Graeme Hirst, Morgan and Claypoole. Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradan, Lance Ramshaw and Nianwen Xue. OntoNotes: A Large Training Corpus for Enhanced Processing, included in Part 1 : Data Acquisition and Linguistic Resources of the Handbook of Natural Language Processing and Machine Translation: Global Automatic Language Exploitation Editors: Joseph Olive, Caitlin Christianson, John McCary, Springer Verglag, pp 54-63, 201 
 
17
