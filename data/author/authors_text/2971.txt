Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 809?816
Manchester, August 2008
Acquiring Sense Tagged Examples using Relevance Feedback
Mark Stevenson, Yikun Guo and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
inital.surname@dcs.shef.ac.uk
Abstract
Supervised approaches to Word Sense Dis-
ambiguation (WSD) have been shown to
outperform other approaches but are ham-
pered by reliance on labeled training ex-
amples (the data acquisition bottleneck).
This paper presents a novel approach to the
automatic acquisition of labeled examples
for WSD which makes use of the Informa-
tion Retrieval technique of relevance feed-
back. This semi-supervised method gener-
ates additional labeled examples based on
existing annotated data. Our approach is
applied to a set of ambiguous terms from
biomedical journal articles and found to
significantly improve the performance of a
state-of-the-art WSD system.
1 Introduction
The resolution of lexical ambiguities has long been
considered an important part of the process of
understanding natural language. Supervised ap-
proaches to Word Sense Disambiguation (WSD)
have been shown to perform better than unsuper-
vised ones (Agirre and Edmonds, 2007) but require
examples of ambiguous words used in context an-
notated with the appropriate sense (labeled exam-
ples). However these often prove difficult to obtain
since manual sense annotation of text is a complex
and time consuming process. In fact, Ng (1997)
estimated that 16 person years of manual effort
would be required to create enough labeled exam-
ples to train a wide-coverage WSD system. This
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
limitation is commonly referred to as the data ac-
quisition bottleneck. It is particularly acute in spe-
cific domains, such as biomedicine, where terms
may have technical usages which only domain ex-
perts are likely to be aware of. For example, pos-
sible meanings of the term ?ganglion? in UMLS
(Humphreys et al, 1998) include ?neural structure?
or ?benign mucinous tumour?, although only the
first meaning is listed in WordNet. These domain-
specific semantic distinctions make manual sense
annotation all the more difficult.
One approach to the data acquisition bottleneck
is to generate labeled training examples automat-
ically. Others, such as Leacock et al (1998) and
Agirre and Mart??nez (2004b), used information
from WordNet to construct queries which were
used to retrieve training examples. This paper
presents a novel approach to this problem. Rele-
vance feedback, a technique used in Information
Retrieval (IR) to improve search results, is adapted
to identify further examples for each sense of am-
biguous terms. These examples are then used
to train a semi-supervised WSD system either by
combining them with existing annotated data or
using them alone. The approach is applied to a set
of ambiguous terms in biomedical texts, a domain
for which existing resources containing labeled ex-
amples, such as the NLM-WSD data set (Weeber
et al, 2001), are limited.
The next section outlines previous techniques
which have been used to avoid the data acquisi-
tion bottleneck. Section 3 describes our approach
based on relevance feedback. The WSD system we
use is described in Section 4. Section 5 describes
experiments carried out to determine the useful-
ness of the automatically retrieved examples. The
final section summarises conclusions which can be
drawn from this work and outlines future work.
809
2 Previous Approaches
A variety of approaches to the data acquisition bot-
tleneck have been proposed. One is to use un-
supervised algorithms, which do not require la-
beled training data. Examples include Lesk (1986)
who disambiguated ambiguous words by examin-
ing their dictionary definitions and selecting the
sense whose definition overlapped most with def-
initions of words in the ambiguous word?s con-
text. Leroy and Rindflesch (2005) presented an
unsupervised approach to WSD in the biomedi-
cal domain using information derived from UMLS
(Humphreys et al, 1998).
However, results from SemEval (Agirre et al,
2007) and its predecessors have shown that su-
pervised approaches to WSD generally outperform
unsupervised ones. It has also been shown that re-
sults obtained from supervised methods improve
with access to additional labeled data for training
(Ng, 1997). Consequently various techniques for
automatically generating training data have been
developed.
One approach makes use of the fact that differ-
ent senses of ambiguous words often have different
translations (e.g. Ng et al (2003)). Parallel text is
used as training data with the alternative transla-
tions serving as sense labels. However, disadvan-
tages of this approach are that the alternative trans-
lations do not always correspond to the sense dis-
tinctions in the original language and parallel text
is not always available.
Another approach, developed by Leacock et
al. (1998) and extended by Agirre and Mart??nez
(2004b), is to examine a lexical resource, Word-
Net in both cases, to identify unambiguous terms
which are closely related to each of the senses of an
ambiguous term. These ?monosemous relatives?
are used to as query terms for a search engine and
the examples returned used as additional training
data.
In the biomedical domain, Humphrey et al
(2006) use journal descriptors to train models
based on the terms which are likely to co-occur
with each sense. Liu et al (2002) used informa-
tion in UMLS to disambiguate automatically re-
trieved examples which were then used as labeled
training data. The meanings of 35 ambiguous ab-
breviations were identified by examining the close-
ness of concepts in the same abstract in UMLS.
Widdows et al (2003) employ a similar approach,
although their method also makes use of parallel
corpora when available.
All of these approaches rely on the existence of
an external resource (e.g. parallel text or a domain
ontology). In this paper we present a novel ap-
proach, inspired by the relevance feedback tech-
nique used in IR, which automatically identifes ad-
ditional training examples using existing labeled
data.
3 Generating Examples using Relevance
Feedback
The aim of relevance feedback is to generate im-
proved search queries based on manual analysis of
a set of retrieved documents which has been shown
to improve search precision (Salton, 1971; Robert-
son and Spark Jones, 1976). Variations of rele-
vance feedback have been developed for a range of
IR models including Vector Space and probabilis-
tic models. The formulation of relevance feedback
for the Vector Space Model is most pertinent to our
approach.
Given a collection of documents, C, containing
a set of terms, C
terms
, a basic premise of the Vec-
tor Space Model is that documents and queries can
be represented by vectors whose dimensions repre-
sent the C
terms
. Relevance feedback assumes that
a retrieval system returns a set of documents, D,
for some query, q. It is also assumed that a user
has examined D and identified some of the docu-
ments as relevant to q and others as not relevant.
Relevant documents are denoted by D
+q
and the
irrelevant as D
?q
, where D
+q
? D, D
?q
? D
and D
+q
?D
?q
= ?. This information is used to
create a modified query, q
m
, which should be more
accurate than q. A standard approach to construct-
ing q
m
was described by Rocchio (1971):
q
m
= ?q+
?
|D
+q
|
?
?d?D
+q
d ?
?
|D
?q
|
?
?d?D
?q
d (1)
where the parameters ?, ? and ? are set for partic-
ular applications. Rocchio (1971) set ? to 1.
Our scenario is similar to the relevance feedback
problem since the sense tagged examples provide
information about the documents in which a par-
ticular meaning of an ambiguous term is likely to
be found. By identifying the features which dis-
tinguish the documents containing one sense from
the others we can create queries which can then be
used to retrieve further examples of the ambiguous
words used in the same sense. However, unlike
810
score(t, s) = idf(t)?
?
?
?
|D
+s
|
?
?d?D
+s
count(t, d)?
?
|D
?s
|
?
?d?D
?s
count(t, d)
?
?
(2)
the relevance feedback scenario there is no origi-
nal query to modify. Consequently we start with
a query containing just the ambiguous term and
use relevance feedback to generate queries which
aim to retrieve documents where that term is being
used in a particular sense.
The remainder of this section describes how this
approach is applied in more detail.
3.1 Corpus Analysis
The first stage of our process is to analyse the la-
beled examples and identify good search terms.
For each sense of an ambiguous term, s, the la-
beled examples are divided into two sets: those
annotated with the sense in question and the re-
mainder (annotated with another sense). In rele-
vance feedback terminology the documents anno-
tated with the sense in question are considered to
be relevant and the remainder irrelevant. These ex-
amples are denoted by D
+s
and D
?s
respectively.
At its core relevance feedback, as outlined
above, aims to discover how accurately each term
in the collection discriminates between relevant
and irrelevant documents. This approach was used
to inspire a technique for identifying terms which
are likely to indicate the sense in which an am-
biguous word is being used. We compute a single
score for each term, reflecting its indicativeness of
that sense, using the formula in equation 2, where
count(t, d) is the number of times term t occurs in
document d and idf(t) is the inverse document fre-
quency term weighting function commonly used in
IR. We compute idf as follows:
idf(t) = log
|C|
df(t)
(3)
where D is the set of all annotated examples (i.e.
D = D
+s
? D
?s
) and df(t) the number of docu-
ments in C which contain t.
1
In our experiments the ? and ? parameters in
equation 2 are set to 1. Documents are lemma-
tised and stopwords removed before computing
relevance scores.
1
Our computation of idf(t) is based on only information
from the labeled examples, i.e. we assume C = D
+s
?D
?s
.
Alternatively idf could be computed over a larger corpus of
labeled and unlabeled examples.
Table 1 shows the ten terms with the highest
relevance score for two senses of the term ?cul-
ture? in UMLS: ?laboratory culture? (?In periph-
eral blood mononuclear cell culture streptococcal
erythrogenic toxins are able to stimulate trypto-
phan degradation in humans?) and ?anthropolog-
ical culture? (?The aim of this paper is to de-
scribe the origins, initial steps and strategy, cur-
rent progress and main accomplishments of intro-
ducing a quality management culture within the
healthcare system in Poland.?).
?anthropological culture? ?laboratory culture?
cultural 26.17 suggest 6.32
recommendation 14.82 protein 6.13
force 14.80 presence 5.86
ethnic 14.79 demonstrate 5.86
practice 14.76 analysis 5.78
man 14.76 gene 5.58
problem 13.04 compare 5.47
assessment 12.94 level 5.36
experience 11.60 response 5.35
consider 11.58 data 5.35
Table 1: Relevant terms for two senses of ?culture?
3.2 Query Generation
Unlike the traditional formulation of relevance
feedback there is no initial query. To create a
query designed to retrieve examples of each sense
we simply combine the ambiguous term and the
n terms with the highest relevance scores. We
found that using the three highest ranked terms
provided good results. So, for example, the queries
generated for the two senses of culture shown
in Table 1 would be ?culture cultural
recommendation force? and ?culture
suggest protein presence?.
3.3 Example Collection
The next stage is to collect a set of examples using
the generated queries. We use the Entrez retrieval
system (http://www.ncbi.nlm.nih.gov/
sites/gquery) which provides an online in-
terface for carrying out boolean queries over the
PubMed database of biomedical journal abstracts.
Agirre and Mart??nez (2004b) showed that it is
important to preserve the bias of the original cor-
pus when automatically retrieving examples and
811
consequently the number retrieved for each sense
is kept in proportion to the original corpus. For
example, if our existing labeled examples contain
75 usages of ?culture? in the ?laboratoy culture?
sense and 25 meaning ?anthropological culture? we
would ensure that 75% of the examples returned
would refer to the first sense and 25% to the sec-
ond.
Unsurprisingly, we found that the most useful
abstracts for a particular sense are the ones which
contain more of the relevant terms identified using
the process in Section 3.1. However, if too many
terms are included Entrez may not return any ab-
stracts. To ensure that a sufficient number of ab-
stracts are returned we implemented a process of
query relaxation which begins by querying Entrez
with the most specific query for set of terms. If that
query matches enough abstracts these are retrieved
and the search for labeled examples for the rele-
vant sense considered complete. However, if that
query does not match enough abstracts it is relaxed
and Entrez queried again. This process is repeated
until enough examples can be retrieved for a par-
ticular sense.
The process of relaxing queries is carried out as
follows. Assume we have an ambiguous term, a,
and a set of terms T identified using the process
in Section 3.1. The first, most specific query,
is formed from the conjunction of all terms in
a ? T , i.e. ?a and t
1
AND t
2
AND ... t
|T |
?.
This is referred to as the level |T | query. If
this query does not return enough abstracts the
more relaxed level |T | ? 1 query is formed.
This query returns documents which include the
ambiguous word and all but one of the terms in T :
?a AND ((t
1
AND t
2
AND ... AND t
n?1
) OR
(t
1
AND t
2
AND ... t
n?2
AND t
n
) OR ... OR
(t
2
AND t
3
... AND t
n
))?. Similarly, level
|T | ? 2 queries return documents containing the
ambiguous term and all but two of the terms
in T . Level 1 queries, the most relaxed, return
documents containing the ambiguous term and
one of the terms in T . We do not use just the
ambiguous term as the query since this does not
contain any information which could discriminate
between the possible meanings. Figure 1 shows
the queries which are formed for the ambigu-
ous term ?culture? and the three most salient
terms identified for the ?anthropological culture?
sense. The ?matches? column lists the number
of PubMed abstracts the query matches. It can
be seen that there are no matches for the level 3
query and 83 for the more relaxed level 2 query.
For this sense, abstracts returned by the level 2
query would be used if 83 or fewer examples were
required, otherwise abstracts returned by the level
1 query would be used.
Note that the queries submitted to Entrez are re-
stricted so the terms only match against the title
and abstract of the PubMed articles. This avoids
spurious matches against other parts of the records
including metadata and authors? names.
4 WSD System
The basis of our WSD system was developed by
Agirre and Mart??nez (2004a) and participated in
the Senseval-3 challenge (Mihalcea et al, 2004)
with a performance which was close to the best
system for the English and Basque lexical sample
tasks. The system has been adapted to the biomed-
ical domain (Stevenson et al, 2008) and has the
best reported results over the NLM-WSD corpus
(Weeber et al, 2001), a standard data set for eval-
uation of WSD algorithms in this domain.
The system uses a wide range of features which
are commonly employed for WSD:
Local collocations: A total of 41 features which
extensively describe the context of the ambiguous
word and fall into two main types: (1) bigrams
and trigrams containing the ambiguous word con-
structed from lemmas, word forms or PoS tags,
and (2) preceding/following lemma/word-form of
the content words (adjective, adverb, noun and
verb) in the same sentence with the target word.
Syntactic Dependencies: This feature mod-
els longer-distance dependencies of the ambiguous
words than can be represented by the local colloca-
tions. Five relations are extracted: object, subject,
noun-modifier, preposition and sibling. These are
identified using heuristic patterns and regular ex-
pressions applied to PoS tag sequences around the
ambiguous word (Agirre and Mart??nez, 2004a).
Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as described
by Pedersen (2001).
Unigrams: Lemmas of all content words
(nouns, verbs, adjectives, adverbs) in the target
word?s sentence and, as a separate feature, lem-
mas of all content words within a 4-word window
around the target word, excluding those in a list
of corpus-specific stopwords (e.g. ?ABSTRACT?,
?CONCLUSION?). In addition, the lemmas of any
812
Level Matches Query
3 0 culture AND (cultural AND recommendation AND force)
2 83 culture AND ((cultural AND recommendation) OR (cultural AND force) OR
(recommendation AND force))
1 6,358 culture AND (cultural OR recommendation OR force)
Figure 1: Examples of various query levels
unigrams which appear at least twice in the en-
tire corpus which are found in the abstract are also
included as features. This feature was not used
by Agirre and Mart??nez (2004a), but Joshi et al
(2005) found them to be useful for this task.
Features are combined using the Vector Space
Model, a memory-based learning algorithm (see
Agirre and Mart??nez (2004a)). Each occurrence
of an ambiguous word is represented as a binary
vector in which each position indicates the oc-
currence/absence of a feature. A single centroid
vector is generated for each sense during training.
These centroids are compared with the vectors that
represent new examples using the cosine metric to
compute similarity. The sense assigned to a new
example is that of the closest centroid.
5 Experiments
5.1 Setup
The NLM-WSD corpus Weeber et al (2001) was
used for evaluation. It contains 100 examples of 50
ambiguous terms which occur frequently in MED-
LINE. Each example consists of the abstract from
a biomedical journal article which contains an in-
stance of the ambiguous terms which has been
manually annotated with a UMLS concept.
The 50 ambiguous terms which form the NLM-
WSD data set represent a range of challenges for
WSD systems. Various researchers (Liu et al,
2004; Leroy and Rindflesch, 2005; Joshi et al,
2005; McInnes et al, 2007) chose to exclude some
of the terms (generally those with highly skewed
sense distributions or low inter-annotator agree-
ment) and evaluated their systems against a subset
of the terms. The number of terms in these subsets
range between 9 and 28. The Most Frequent Sense
(MFS) heuristic has become a standard baseline in
WSD (McCarthy et al, 2004) and is simply the ac-
curacy which would be obtained by assigning the
most common meaning of a term to all of its in-
stances in a corpus. The MFS for the whole NLM-
WSD corpus is 78% and ranges between 69.9%
and 54.9% for the various subsets. We report re-
sults across the NLM-WSD corpus and four sub-
sets from the literature for completeness.
The approach described in Section 3 was ap-
plied to the NLM-WSD data set. 10-fold cross
validation is used for all experiments. Conse-
quently 10 instances of each ambiguous term were
held back for testing during each fold and addi-
tional examples generated by examining the 90 re-
maining instances. Three sets of labeled examples
were generated for each fold, containing 90, 180
and 270 examples for each ambiguous term. The
NLM-WSD corpus represents the only reliably la-
beled data to which we have access and is used to
evaluate all approaches (that is, systems trained on
combinations of the NLM-WSD corpus and/or the
automatically generated examples).
5.2 Results
Various WSD systems were created. The ?basic?
system was trained using only the NLM-WSD data
set and was used as a benchmark. Three systems,
?+90?, ?+180? and ?+270? were trained using the
combination of the NLM-WSD data set and, re-
spectively, the 90, 180 and 270 automatically re-
trieved examples for each term. A further three
systems, ?90?, ?180? and ?270? were trained us-
ing only the automatically retrieved examples.
The performance of our system is shown in Ta-
ble 2. The part of the table labeled ?Subsets prop-
erties? lists the number of terms in each subset of
the NLM-WSD corpus and the relevant MFS base-
line.
Adding the first 90 automatically retrieved ex-
amples (?+90? column) significantly improves per-
formance of our system from 87.2%, over all
words, to 88.5% (Wilcoxon Signed Ranks Test,
p < 0.01). Improvements are observed over
all subsets of the NLM-WSD corpus. Although
the improvements may seem modest they should
be understood in the context of the WSD system
we are using which has exceeded previously re-
ported performance figures and therefore repre-
sents a high baseline.
Table 2 also shows that adding more auto-
matically retrieved examples (?+180? and ?+270?
columns) causes a drop in performance and re-
813
Subset Properties Combined New only
Subset Terms MFS
basic
+90 +180 +270 90 180 270
All words 50 78.0 87.2 88.5 87.0 86.1 85.6 84.5 82.7
Joshi et. al. 28 66.9 82.3 83.8 81.6 80.9 79.8 78.0 76.3
Liu et. al. 22 69.9 77.8 79.6 76.9 76.1 74.9 72.0 70.9
Leroy 15 55.3 84.3 85.9 84.4 83.6 81.2 80.0 78.0
McInnes et. al. 9 54.9 79.6 81.8 80.4 79.4 75.2 73.0 71.4
Table 2: Performance of system using a variety of combinations of training examples
sults using these examples are worse than using the
NLM-WSD corpus alone. The query relaxation
process, outlined in Section 3.3, uses less discrim-
inating queries when more examples are required
and it is likely that this is leading to noise in the
training examples.
The rightmost portion of Table 2 shows perfor-
mance when the system is trained using only the
automatically generated examples which is con-
sistently worse than using the NLM-WSD corpus
alone. Performance also decreases as more exam-
ples are added. However, results obtained using
only the automatically generated training exam-
ples are consistently better than the relevant base-
line.
Table 3 shows the performance of the sys-
tem trained on the NLM-WSD data set compared
against training using only the 90 automatically
generated examples for each ambiguous term in
the NLM-WSD corpus. It can be seen that there
is a wide variation between the performance of
the additional examples compared with the origi-
nal corpus. For 11 terms training using the addi-
tional examples alone is more effective than using
the NLM-WSD corpus. However, there are several
words for which the performance using the auto-
matically acquired examples is considerably worse
than using the NLM-WSD corpus.
Information about the performance of a system
trained using only the 90 automatically acquired
examples can be used to boost WSD performance
further. In this scenario, which we refer to as ex-
ample filtering, the system has a choice whether
to make use of the additional training data or not.
For each word, performance of the WSD system
trained using only the 90 automatically acquired
examples is compared against the one trained on
the NLM-WSD data set (i.e. results shown in Ta-
ble 3). If the performance is as good, or better,
then the additional examples are used, otherwise
only examples in the NLM-WSD corpus are used
as training data. Since the annotated examples in
the NLM-WSD corpus have already been exam-
ined to generate the additional examples, example
filtering does not require any more labeled data.
Results obtained when example filtering is used
are shown in Table 4. The columns ?+90(f)?,
?+180(f)? and ?+270(f)? show performance when
the relevant set of examples is filtered. (Note
that all three sets of examples are filtered against
the performance of the first 90 examples, i.e. re-
sults shown in Table 3.) This table shows that
example filtering improves performance when the
WSD system is trained using the automatically re-
trieved examples. Performance using the first 90
filtered examples (?+90(f)? column) is 89%, over
all words, compared with 88.5% when filtering is
not used. While performance decreases as larger
sets of examples are used, results using each of the
three sets of filtered examples is signifcantly bet-
ter than the basic system (Wilcoxon Signed Ranks
Test, p < 0.01 for ?+90(f)? and ?+180(f)?, p <
0.05 for ?+270(f)?).
6 Conclusion and Future Work
This paper has presented a novel approach to the
data acquisition bottleneck for WSD. Our tech-
nique is inspired by the relevance feedback tech-
nique from IR. This is a semi-supervised approach
which generates labeled examples using available
sense annotated data and, unlike previously pub-
lished approaches, does not rely on external re-
sources such as parallel text or an ontology. Eval-
uation was carried out on a WSD task from the
biomedical domain for which the number of la-
beled examples available for each ambiguous term
is limited. The automatically acquired examples
improve the performance of a WSD system which
has already been shown to exceed previously pub-
lished results.
The approach presented in this paper could be
extended in several ways. Our experiments focus
814
basic +90(f) +180(f) +270(f)
All words 87.2 89.0 88.2 87.9
Joshi et. al. 82.3 84.6 83.5 83.3
Liu et. al. 84.3 86.6 85.7 85.5
Leroy 77.8 80.3 79.1 78.5
McInnes et. al. 79.6 82.4 81.6 80.8
Table 4: Performance using example filtering
on the biomedical domain. The relevance feedback
approach could be applied to other lexical ambi-
guities found in biomedical texts, such as abbre-
viations with multiple expansions (e.g. Liu et al
(2002)), or to WSD of general text, possibly using
the SemEval data for evaluation.
Future work will explore alternative methods for
generating query terms including other types of
relevance feedback and lexical association mea-
sures (e.g. Chi-squared and mutual information).
Experiments described here rely on a boolean IR
engine (Entrez). It is possible that an IR sys-
tem which takes term weights into account could
lead to the retrieval of more useful MEDLINE ab-
stracts. Finally, it would be interesting to explore
the relation between query relaxation and the use-
fulness of the retrieved abstracts.
Acknowledgments
The authors are grateful to David Martinez for the
use of his WSD system for these experiments and
to feedback provided by three anonymous review-
ers. This work was funded by the UK Engineer-
ing and Physical Sciences Research Council, grant
number EP/E004350/1.
References
E. Agirre and P. Edmonds, editors. 2007. Word
Sense Disambiguation: Algorithms and Applica-
tions. Text, Speech and Language Technology.
Springer.
E. Agirre and D. Mart??nez. 2004a. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text, pages
44?48, Barcelona, Spain, July.
E. Agirre and D. Mart??nez. 2004b. Unsupervised WSD
based on automatically retrieved examples: The im-
portance of bias. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-04), Barcelona, Spain.
E. Agirre, L. Marquez, and R. Wicentowski, editors.
2007. SemEval 2007: Proceedings of the 4th
International Workshop on Semantic Evaluations,
Prague, Czech Republic.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense
Disambiguation by selecting the best semantic type
based on Journal Descriptor Indexing: Preliminary
experiment. Journal of the American Society for In-
formation Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and
G. Barnett. 1998. The Unified Medical Language
System: An Informatics Research Collaboration.
Journal of the American Medical Informatics Asso-
ciation, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Com-
parative Study of Support Vector Machines Applied
to the Word Sense Disambiguation Problem for the
Medical Domain. In Proceedings of the Second In-
dian Conference on Artificial Intelligence (IICAI-
05), pages 3449?3468, Pune, India.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(1):147?165.
G. Leroy and T. Rindflesch. 2005. Effects of Infor-
mation and Machine Learning algorithms on Word
Sense Disambiguation with small datasets. Interna-
tional Journal of Medical Informatics, 74(7-8):573?
585.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of
ACM SIGDOC Conference, pages 24?26, Toronto,
Canada.
H. Liu, S. Johnson, and C. Friedman. 2002. Automatic
Resolution of Ambiguous Terms Based on Machine
Learning and Conceptual Relations in the UMLS.
Journal of the American Medical Informatics Asso-
ciation, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-
aspect Comparison Study of Supervised Word Sense
Disambiguation. Journal of the American Medical
Informatics Association, 11(4):320?331.
815
word basic 90 ?
adjustment 71 70 -1
association 100 100 0
blood pressure 48 50 2
cold 88 86 -2
condition 89 90 1
culture 96 91 -5
degree 96 86 -10
depression 88 85 -3
determination 87 82 -5
discharge 95 92 -3
energy 98 99 1
evaluation 76 75 -1
extraction 85 82 -3
failure 66 71 5
fat 85 83 -2
fit 87 85 -2
fluid 100 100 0
frequency 95 94 -1
ganglion 97 95 -2
glucose 91 92 1
growth 70 67 -3
immunosuppression 79 79 0
implantation 90 88 -2
inhibition 98 98 0
japanese 73 75 2
lead 91 90 -1
man 87 82 -5
mole 95 84 -11
mosaic 87 83 -4
nutrition 53 43 -10
pathology 85 85 0
pressure 94 96 2
radiation 84 82 -2
reduction 89 90 1
repair 87 86 -1
resistance 98 97 -1
scale 86 79 -7
secretion 99 99 0
sensitivity 93 91 -2
sex 87 84 -3
single 99 99 0
strains 92 92 0
support 86 89 3
surgery 97 98 1
transient 99 99 0
transport 93 93 0
ultrasound 87 85 -2
variation 94 89 -5
weight 77 77 0
white 73 74 1
Average 87.2 85.6 -1.58
Table 3: Comparison of performance using orig-
inal training data and 90 automatically generated
examples
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding Predominant Senses in Untagged
Text. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Lingusitics (ACL-
2004), pages 280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Us-
ing UMLS Concept Unique Identifiers (CUIs) for
Word Sense Disambiguation in the Biomedical Do-
main. In Proceedings of the Annual Symposium
of the American Medical Informatics Association,
pages 533?537, Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of Senseval-3: The Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, Barcelona, Spain.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Paral-
lel Texts for Word Sense Disambiguation: an Empir-
ical Study. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-03), pages 455?462, Sapporo, Japan.
H. Ng. 1997. Getting serious about Word Sense Dis-
ambiguation. In Proceedings of the SIGLEX Work-
shop ?Tagging Text with Lexical Semantics: What,
why and how??, pages 1?7, Washington, DC.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
S. Robertson and K. Spark Jones. 1976. Relevance
weighting of search terms. Journal of the Ameri-
can Society for Information Science and Technology,
27(3):129?146.
J. Rocchio. 1971. Relevance feedback in Information
Retrieval. In G. Salton, editor, The SMART Retrieval
System ? Experiments in Automatic Document Pro-
cessing. Prentice Hall, Englewood Cliffs, NJ.
G. Salton. 1971. The SMART Retrieval System ? Ex-
periments in Automatic Document Processing. Pren-
tice Hall Inc., Englewood Cliffs, NJ.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Knowledge Sources for Word Sense Disam-
biguation of Biomedical Text. In Proceedings of the
Workshop on Current Trends in Biomedical Natural
Language Processing at ACL 2008, pages 80?87.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMAI Sympo-
sium, pages 746?50, Washington, DC.
D. Widdows, S. Peters, S. Cedernerg, C. Chan, D. Stef-
fen, and P. Buitelaar. 2003. Unsupervised Mono-
lingual and Bilingual Word-sense Disambiguation of
Medical Documents using UMLS. In Workshop on
?Natural Langauge Processing in Biomedicine? at
ACL 2003, pages 9?16, Sapporo, Japan.
816
A Large Scale Terminology Resource for Biomedical Text Processing
Henk Harkema, Robert Gaizauskas, Mark Hepple, Angus Roberts,
Ian Roberts, Neil Davis, Yikun Guo
Department of Computer Science, University of Sheffield, UK
biomed@dcs.shef.ac.uk
Abstract
In this paper we discuss the design, implemen-
tation, and use of Termino, a large scale termi-
nological resource for text processing. Dealing
with terminology is a difficult but unavoidable
task for language processing applications, such
as Information Extraction in technical domains.
Complex, heterogeneous information must be
stored about large numbers of terms. At the
same time term recognition must be performed
in realistic times. Termino attempts to recon-
cile this tension by maintaining a flexible, ex-
tensible relational database for storing termino-
logical information and compiling finite state
machines from this database to do term look-
up. While Termino has been developed for
biomedical applications, its general design al-
lows it to be used for term processing in any
domain.
1 Introduction
It has been widely recognized that the biomedical litera-
ture is now so large, and growing so quickly, that it is be-
coming increasingly difficult for researchers to access the
published results that are relevant to their research. Con-
sequently, any technology that can facilitate this access
should help to increase research productivity. This has
led to an increased interest in the application of natural
language processing techniques for the automatic capture
of biomedical content from journal abstracts, complete
papers, and other textual documents (Gaizauskas et al,
2003; Hahn et al, 2002; Pustejovsky et al, 2002; Rind-
flesch et al, 2000).
An essential processing step in these applications is
the identification and semantic classification of techni-
cal terms in text, since these terms often point to enti-
ties about which information should be extracted. Proper
semantic classification of terms also helps in resolving
anaphora and extracting relations whose arguments are
restricted semantically.
1.1 Challenge
Any technical domain generates very large numbers of
terms ? single or multiword expressions that have some
specialised use or meaning in that domain. For exam-
ple, the UMLS Metathesaurus (Humphreys et al, 1998),
which provides a semantic classification of terms from a
wide range of vocabularies in the clinical and biomedical
domain, currently contains well over 2 million distinct
English terms.
For a variety of reasons, recognizing these terms in
text is not a trivial task. First of all, terms are often
long multi-token sequences, e.g. 3-methyladenine-DNA
glycosylase I. Moreover, since terms are referred to re-
peatedly in discourses there is a benefit in their being
short and unambiguous, so they are frequently abbre-
viated and acronymized, e.g. CvL for chromobacterium
viscosum lipase. However, abbreviations may not al-
ways occur together with their full forms in a text, the
method of abbreviation is not predictable in all cases, and
many three letter abbreviations are highly overloaded.
Terms are also subject to a high degree of orthographic
variation as a result of the representation of non-Latin
characters, e.g. a-helix vs. alpha-helix, capitalization,
e.g. DNA vs. dna, hyphenation, e.g. anti-histamine vs. an-
tihistamine, and British and American spelling variants,
e.g. tumour vs. tumor. Furthermore, biomedical science
is a dynamic field: new terms are constantly being in-
troduced while old ones fall into disuse. Finally, certain
classes of biomedical terms exhibit metonomy, e.g. when
a protein is referred to by the gene that expresses it.
To begin to address these issues in term recognition, we
are building a large-scale resource for storing and recog-
nizing technical terminology, called Termino. This re-
source must store complex, heterogeneous information
about large numbers of terms. At the same time term
recognition must be performed in realistic times. Ter-
mino attempts to reconcile this tension by maintaining a
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 53-60.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
flexible, extensible relational database for storing termi-
nological information and compiling finite state machines
from this database to do term look-up.
1.2 Context
Termino is being developed in the context of two ongoing
projects: CLEF, for Clinical E-Science Framework (Rec-
tor et al, 2003) and myGrid (Goble et al, 2003). Both
these projects involve an Information Extraction compo-
nent. Information Extraction is the activity of identifying
pre-defined classes of entities and relationships in natural
language texts and storing this information in a structured
format enabling rapid and effective access to the informa-
tion, e.g. Gaizauskas and Wilks (1998), Grishman (1997).
The goal of the CLEF project is to extract information
from patient records regarding the treatment of cancer.
The treatment of cancer patients may extend over several
years and the resulting clinical record may include many
documents, such as clinic letters, case notes, lab reports,
discharge summaries, etc. These documents are gener-
ally full of medical terms naming entities such as body
parts, drugs, problems (i.e. symptoms and diseases), in-
vestigations and interventions. Some of these terms are
particular to the hospital from which the document origi-
nates. We aim to identify these classes of entities, as well
as relationships between such entities, e.g. that an investi-
gation has indicated a particular problem, which, in turn,
has been treated with a particular intervention. The infor-
mation extracted from the patient records is potentially of
value for immediate patient care, but can also be used to
support longitudinal and epidemiological medical stud-
ies, and to assist policy makers and health care managers
in regard to planning and clinical governance.
The myGrid project aims to present research biolo-
gists with a unified workbench through which component
bioinformatic services can be accessed using a workflow
model. These services may be remotely located from the
user and will be exploited via grid or web-service chan-
nels. A text extraction service will form one of these ser-
vices and will facilitate access to information in the sci-
entific literature. This text service comprises an off-line
and an on-line component. The off-line component in-
volves pre-processing a large biological sciences corpus,
in this case the contents of Medline, in order to identify
various biological entities such as genes, enzymes, and
proteins, and relationships between them such as struc-
tural and locative relations. These entities and relation-
ships are referred to in Medline abstracts by a very large
number of technical terms and expressions, which con-
tributes to the complexity of processing these texts. The
on-line component supports access to the extracted infor-
mation, as well as to the raw texts, via a SOAP interface
to an SQL database.
Despite the different objectives for text extraction
within the CLEF and myGrid projects, many of the tech-
nical challenges they face are the same, such as the
need for extensive capabilities to recognize and classify
biomedical entities as described using complex techni-
cal terminology in text. As a consequence we are con-
structing a general framework for the extraction of infor-
mation from biomedical text: AMBIT, a system for ac-
quiring medical and biological information from text. An
overview of the AMBIT logical architecture is shown in
figure 1.
The AMBIT system contains several engines, of which
Termino is one. The Information Extraction Engine pulls
selected information out of natural language text and
pushes this information into a set of pre-defined tem-
plates. These are structured objects which consists of one
or more slots for holding the extracted entities and rela-
tions. The Query Engine allows users to access informa-
tion through traditional free text search and search based
on the structured information produced by the Informa-
tion Extraction Engine, so that queries may refer to spe-
cific entities and classes of entities, and specific kinds of
relations that are recognised to hold between them. The
Text Indexing Engine is used to index text and extracted,
structured information for the purposes of information re-
trieval. The AMBIT system contains two further compo-
nents: an interface layer, which provides a web or grid
channel to allow user and program access to the system;
and a database which holds free text and structured infor-
mation that can be searched through the Query Engine.
Termino interacts with the Query Engine and the Text
Indexing Engine to provide terminological support for
query formulation and text indexation. It also provides
knowledge for the Information Extraction Engine to use
in identifying and classifying biomedical entities in text.
The Terminology Engine can furthermore be called by
users and remote programs to access information from
the various lexical resources that are integrated in the ter-
minological database.
2 Related Work
Since identification and classification of technical terms
in biomedical text is an essential step in information
extraction and other natural language processing tasks,
most natural language processing systems contain a
terminological resource of some sort. Some systems
make use of existing terminological resources, notably
the UMLS Metathesaurus, e.g. Rindflesch et al (2000),
Pustejovski et al (2002); other systems rely on re-
sources that have been specifically built for the applica-
tion, e.g. Humphreys et al (2000), Thomas et al (2000).
The UMLS Metathesaurus provides a semantic classi-
fication of terms drawn from a wide range of vocabularies
in the clinical and biomedical domain (Humphreys et al,
1998). It does so by grouping strings from the source vo-
from Hospital 1
Clinical Records
Journals
On?line
Abstracts
Medline
Literature
Biomedical
Engine
Indexing
Text
Engine
Extraction
...
(Termino)
Engine
Terminology
Ambit
from Hospital 2
Clinical Records
Web GRID
Interface layer
Raw text
(entities / relations)
Structured InfoFree text
  search
Engine
Query
Information
SOAP /
     HTTP
& Annotations
Structured Info
Figure 1: AMBIT Architecture
cabularies that are judged to have the same meaning into
concepts, and mapping these concepts onto nodes or se-
mantic types in a semantic network. Although the UMLS
Metathesaurus is used in a number of biomedical natural
language processing applications, we have decided not to
adopt the UMLS Metathesaurus as the primary terminol-
ogy resource in AMBIT for a variety of reasons.
One of the reasons for this decision is that the Metathe-
saurus is a closed system: strings are classified in terms
of the concepts and the semantic types that are present
in the Metathesaurus and the semantic network, whereas
we would like to be able to link our terms into multi-
ple ontologies, including in-house ontologies that do not
figure in any of the Metathesaurus? source vocabularies
and hence are not available through the Metathesaurus.
Moreover, we would also like to be able to have access to
additional terminological information that is not present
in the Metathesaurus, such as, for example, the annota-
tions in the Gene Ontology (The Gene Ontology Con-
sortium, 2001) assigned to a given human protein term.
While the terms making up the the tripartite Gene On-
tology are present in the UMLS Metathesaurus, assign-
ments of these terms to gene products are not recorded
in the Metathesaurus. Furthermore, as new terms appear
constantly in the biomedical field we would like to be
able to instantly add these to our terminological resource
and not have to wait until they have been included in the
UMLS Metathesaurus. Additionally, some medical terms
appearing in patient notes are hospital-specific and are
unlikely to be included in the Metathesaurus at all.
With regard to systems that do not use the UMLS
Metathesaurus, but rather depend on terminological re-
sources that have been specifically built for an applica-
tion, we note that these terminological resources tend to
be limited in the following two respects. First, the struc-
ture of these resources is often fixed and in some cases
amounts to simple gazetteer lists. Secondly, because of
their fixed structure, these resources are usually popu-
lated with content from just a few sources, leaving out
many other potentially interesting sources of terminolog-
ical information.
Instead, we intend for Termino to be an exten-
sible resource that can hold diverse kinds of termi-
nological information. The information in Termino
is either imported from existing, outside knowledge
sources, e.g. the Enzyme Nomenclature (http://www.
chem.qmw.ac.uk/iubmb/enzyme/), the Structural Classi-
fication of Proteins database (Murzin et al, 1995), and
the UMLS Metathesaurus, or it is induced from on-line
raw text resources, e.g. Medline abstracts. Termino thus
provides uniform access to terminological information
aggregated across many sources. Using Termino re-
moves the need for multiple, source-specific terminolog-
ical components within text processing systems that em-
ploy multiple terminological resources.
3 Architecture
Termino consists of two components: a database holding
terminological information and a compiler for generating
term recognizers from the contents of the database. These
two components will be discussed in the following two
sections.
STRINGS
string str id
. . . . . .
neurofibromin str728
abdomen str056
mammectomy str176
mastectomy str183
. . . . . .
TERMOID STRINGS
trm id str id
. . . . . .
trm023 str056
trm656 str056
trm924 str728
trm369 str728
trm278 str176
trm627 str183
. . . . . .
PART OF SPEECH
trm id pos
. . . . . .
trm023 N
. . . . . .
SYNONYMY
syn id trm id scl id
. . . . . . . . .
syn866 trm278 syn006
syn435 trm627 syn006
. . . . . . . . .
GO ANNOTATIONS
trm id annotation version
. . . . . . . . .
trm924 GO:0004857 9/2003
trm369 GO:0008285 9/2003
. . . . . . . . .
UMLS
trm id cui lui sui version
. . . . . . . . . . . . . . .
trm278 C0024881 L0024669 S0059711 2003AC
trm656 C0000726 L0000726 S0414154 2003AC
. . . . . . . . . . . . . . .
Figure 2: Structure of the terminological database
3.1 Terminological Database
The terminological database is designed to meet three re-
quirements. First of all, it must be capable of storing large
numbers of terms. As we have seen, the UMLS Metathe-
saurus contains over 2 million distinct terms. However,
as UMLS is just one of many resources whose terms may
need to be stored, many millions of terms may need to
be stored in total. Secondly, Termino?s database must
also be flexible enough to hold a variety of information
about terms, including information of a morpho-syntactic
nature, such as part of speech and morphological class;
information of a semantic nature, such as quasi-logical
form and links to concepts in ontologies; and provenance
information, such as the sources of the information in the
database. The database will also contain links to connect
synonyms and morphological and orthographic variants
to one another and to connect abbreviations and acronyms
to their full forms. Finally, the database must be orga-
nized in such a way that it allows for fast and efficient
recognition of terms in text.
As mentioned above, the information in Termino?s
database is either imported from existing, outside knowl-
edge sources or induced from text corpora. Since these
sources are heterogeneous in both information content
and format, Termino?s database is ?extensional?: it stores
strings and information about strings. Higher-order con-
cepts such as ?term? emerge as the result of interconnec-
tions between strings and information in the database.
The database is organized as a set of relational tables,
each storing one of the types of information mentioned
above. In this way, new information can easily be in-
cluded in the database without any global changes to the
structure of the database.
Terminological information about any given string is
usually gathered from multiple sources. As information
about a string accumulates in the database, we must make
sure that co-dependencies between various pieces of in-
formation about the string are preserved. This considera-
tion leads to the fundamental element of the terminologi-
cal database, a termoid. A termoid consists of a string to-
gether with associated information of various kinds about
the string. Information in one termoid holds conjunc-
tively for the termoid?s string, while multiple termoids
for the same string express disjunctive alternatives.
For instance, taking an example from UMLS, we may
learn from one source that the string cold as an adjective
refers to a temperature, whereas another source may tell
us that cold as a noun refers to a disease. This informa-
tion is stored in the database as two termoids: abstractly,
?cold, adjective, temperature? and ?cold, noun, disease?.
A single termoid ?cold, adjective, noun, temperature, dis-
ease? would not capture the co-dependency between the
part of speech and the ?meaning? of cold.1 This example
illustrates that a string can be in more than one termoid.
1Note that the UMLS Metathesaurus has no mechanism for
storing this co-dependency between grammatical and semantic
information.
Each termoid, however, has one and only one string.
Figure 2 provides a detailed example of part of the
structure of the terminological database. In the table
STRINGS every unique string is assigned a string iden-
tifier (str id). In the table TERMOID STRINGS each string
identifier is associated with one or more termoid iden-
tifiers (trm id). These termoid identifiers then serve as
keys into the tables holding terminological information.
Thus, in this particular example, the database includes
the information that in the Gene Ontology the string
neurofibromin has been assigned the terms with identi-
fiers GO:0004857 and GO:0008285. Furthermore, in the
UMLS Metathesaurus version 2003AC, the string mam-
mectomy has been assigned the concept-unique identifier
C0024881 (CUI), the lemma-unique identifier L0024669
(LUI), and the string-unique identifier S0059711 (SUI).
Connections between termoids such as those arising
from synonymy and orthographic variation are recorded
in another set of tables. For example, the table SYN-
ONYMY in figure 2 indicates that termoids 278 and
627 are synonymous, since they have the same syn-
onymy class identifier (scl id).2 The synonymy identifier
(syn id) identifies the assignment of a termoid to a partic-
ular synonymy class. This identifier is used to record the
source on which the assignment is based. This can be a
reference to a knowledge source from which synonymy
information has been imported into Termino, or a refer-
ence to both an algorithm by which and a corpus from
which synonyms have been extracted. Similarly there are
tables containing provenance information for strings, in-
dexed by str id, and termoids, indexed by trm id. These
tables are not shown in he example.
With regard to the first requirement for the design of
the terminological database mentioned at the beginning
of this section ? scalability ?, an implementation of Ter-
mino in MySQL has been loaded with 427,000 termoids
for 363,000 strings (see section 4 for more details). In it
the largest table, STRINGS, measures just 16MB, which is
nowhere near the default limit of 4GB that MySQL im-
poses on the size of tables. Hence, storing a large num-
ber of terms in Termino is not a problem size-wise. The
second requirement, flexibility of the database, is met by
distributing terminological information over a set of rela-
tively small tables and linking the contents of these tables
to strings via termoid identifiers. In this way we avoid the
strictures of any one fixed representational scheme, thus
making it possible for the database to hold information
from disparate sources. The third requirement on the de-
sign of the database, efficient recognition of terms, will
2The function of synonymy class identifiers in Termino is
similar to the function of CUIs in the UMLS Metathesaurus.
However, since we are not bound to a classification into UMLS
CUIs, we can assert synonymy between terms coming from ar-
bitrary sources.
be addressed in the next section.
3.2 Term Recognition
To ensure fast term recognition with Termino?s vast ter-
minological database, the system comes equipped with
a compiler for generating finite state machines from the
strings in the terminological database discussed in the
previous section. Direct look-up of strings in the database
is not an option, because it is unknown in advance at
which positions in a text terms will start and end. In order
to be complete, one would have to look up all sequences
of words or tokens in the text, which is very inefficient.
Compilation of a finite state recognizer proceeds in
the following way. First, each string in the database is
broken into tokens, where a token is either a contigu-
ous sequence of alpha-numeric characters or a punctu-
ation symbol. Next, starting from a single initial state, a
path through the machine is constructed, using the tokens
of the string to label transitions. For example, for the
string Graves? disease the machine will include a path
with transitions on Graves, ?, and disease. New states are
only created when necessary. The state reached on the fi-
nal token of a string will be labeled final and is associated
with the identifiers of the termoids for that string.
To recognize terms in text, the text is tokenized and the
finite state machine is run over the text, starting from the
initial state at each token in the text. For each sequence
of tokens leading to a final state, the termoid identifiers
associated with that state are returned. These identifiers
are then used to access the terminological database and
retrieve the information contained in the termoids. Where
appropriate the machine will produce multiple termoid
identifiers for strings. It will also recognize overlapping
and embedded strings.
Figure 3 shows a small terminological database and a
finite state recognizer derived from it. Running this rec-
ognizer over the phrase . . . thyroid dysfunction, such as
Graves? disease . . . produces four annotations: thyroid
is assigned the termoid identifiers trm1 and trm2; thyroid
dysfunction, trm3; and Graves? disease, trm4.
It should be emphasised at this point that term recog-
nition as performed by Termino is in fact term look-up
and not the end point of term processing. Term look-up
might return multiple possible terms for a given string,
or for overlapping strings, and subsequent processes may
apply to filter these alternatives down to the single option
that seems most likely to be correct in the given context.
Furthermore, more flexible processes of term recognition
might apply over the results of look-up. For example, a
term grammar might be provided for a given domain, al-
lowing longer terms to be built from shorter terms that
have been identified by term look-up.
The compiler can be parameterized to produce finite
state machines that match exact strings only, or that ab-
STRINGS
string str id
thyroid str12
thyroid disfunction str15
Graves? disease str25
TERMOID STRINGS
trm id str id
trm1 str12
trm2 str12
trm3 str15
trm4 str25
? trm4disease
thyroid
Graves
trm3
trm2
trm1
disfunction
Figure 3: Sample terminological database and finite state term recognizer
stract away from morphological and orthographical vari-
ation. At the moment, morphological information about
strings is supplied by a component outside Termino. In
our current term recognition system, this component ap-
plies to a text before the recognition process and asso-
ciates all verbs and nouns with their base form. Similarly,
the morphological component applies to the strings in the
terminological database before the compilation process.
The set-up in which term recognizers are compiled
from the contents of the terminological database turns
Termino into a general terminological resource which is
not restricted to any single domain or application. The
database can be loaded with terms from multiple domains
and compilation can be restricted to particular subsets of
strings by selecting termoids from the database based on
their source, for example. In this way one can produce
term recognizers that are tailored towards specific do-
mains or specific applications within domains.
4 Implementation & Performance
A first version of Termino has been implemented. It uses
a database implemented in MySQL and currently con-
tains over 427,000 termoids for around 363,000 strings.
Content has been imported from various sources by
means of source-specific scripts for extracting relevant
information from sources and a general script for load-
ing this extracted information into Termino. More specif-
ically, to support information extraction from patient
records, we have included in Termino strings from the
UMLS Metathesaurus falling under the following seman-
tic types: pharmacologic substances, anatomical struc-
tures, therapeutic procedure, diagnostic procedure, and
several others. We have also loaded a list of hu-
man proteins and their assignments to the Gene Ontol-
ogy as produced by the European Bioinformatics Insti-
tute (http://www.ebi.ac.uk/GOA/) into Termino. Further-
more, we have included several gazetteer lists containing
terms in the fields of molecular biology and pharmacol-
ogy that were assembled for previous information extrac-
tion projects in our NLP group. A web services (SOAP)
API to the database is under development. We plan to
make the resource available to researchers as a web ser-
vice or in downloadable form.3
The compiler to construct finite state recognizers from
the database is fully implemented, tested, and integrated
into AMBIT. The compiled recognizer for the 363,000
strings of Termino has 1.2 million states and an on-disk
size of around 80MB. Loading the matcher from disk
into memory requires about 70 seconds (on an UltraSparc
900MHz), but once loaded recognition is a very fast pro-
cess. We have been able to annotate a corpus of 114,200
documents, drawn from electronic patient records from
the Royal Marsden NHS Trust in London and each ap-
proximately 1kB of text, in approximately 44 hours ? an
average rate of 1.4 seconds per document, or 42 docu-
ments per minute. On average, about 30 terms falling un-
der the UMLS ?clinical? semantic types mentioned above
were recognized in each document. We are currently an-
notating a bench-mark corpus in order to obtain precision
and recall figures. We are also planning to compile rec-
ognizers for differently sized subsets of the terminologi-
cal database and measure their recognition speed over a
given collection of texts. This will provide some indica-
tion as to the scalability of the system.
Since Termino currently contains many terms imported
from the UMLS Metathesaurus, it is interesting to com-
pare its term recognition performance against the per-
formance of MetaMap. MetaMap is a program avail-
able from at the National Library of Medicine ? the de-
velopers of UMLS ? specifically designed to discover
UMLS Metathesaurus concepts referred to in text (Aron-
son, 2001). An impressionistic comparison of the per-
formance of Termino and MetaMap on the CLEF patient
records shows that the results differ in two ways. First,
MetaMap recognizes more terms than Termino. This
is simply because MetaMap draws on a comprehensive
version of UMLS, whereas Termino just contains a se-
lected subset of the strings in the Metathesaurus. Sec-
ondly, MetaMap is able to recognize variants of terms,
e.g. it will map the verb to treat and its inflectional forms
onto the term treatment, whereas Termino currently does
not do this. To recognize term variants MetaMap re-
lies on UMLS?s SPECIALIST lexicon, which provides
3Users may have to sign license agreements with third par-
ties in order to be able to use restricted resources that have been
integrated into Termino.
syntactic, morphological, and orthographic information
for many of the terms occurring in the Metathesaurus.
While the performance of both systems differs in favor
of MetaMap, it is important to note that the source of
these differences is unrelated to the actual design of Ter-
mino?s terminological database or Termino?s use of fi-
nite state machines to do term recognition. Rather, the
divergence in performance follows from a difference in
breadth of content of both systems at the moment. With
regard to practical matters, the comparison showed that
term recognition with Termino is much faster than with
MetaMap. Also, compiling a finite state recognizer from
the terminological database in Termino is a matter of min-
utes, whereas setting up MetaMap can take several hours.
However, since MetaMap?s processing is more involved
than Termino?s, e.g. MetaMap parses the input first, and
hence requires more resources, these remarks should be
backed up with a more rigorous comparison between Ter-
mino and MetaMap, which is currently underway.
The advantage of term recognition with Termino over
MetaMap and UMLS or any other recognizer with a sin-
gle source, is that it provides immediate entry points
into a variety of outside ontologies and other knowledge
sources, making the information in these sources avail-
able to processing steps subsequent to term recognition.
For example, for a gene or protein name recognized in a
text, Termino will return the database identifiers of this
term in the HUGO Nomenclature database (Wain et al,
2002) and the OMIM database (Online Mendelian Inher-
itance in Man, OMIM (TM), 2000). These identifiers
give access to the information stored in these databases
about the gene or protein, including alternative names,
gene map locus, related disorders, and references to rele-
vant papers.
5 Conclusions & Future Work
Dealing with terminology is an essential step in natural
language processing in technical domains. In this paper
we have described the design, implementation, and use of
Termino, a large scale terminology resource for biomedi-
cal language processing.
Termino includes a relational database which is de-
signed to store a large number of terms together with
complex, heterogeneous information about these terms,
such as morpho-syntactic information, links to concepts
in ontologies, and other kinds of annotations. The
database is also designed to be extensible: it is easy to
include terms and information about terms found in out-
side biological databases and ontologies. Term look-up
in text is done via finite state machines that are compiled
from the contents of the database. This approach allows
the database to be very rich without sacrificing speed at
look-up time. These three features make Termino a flexi-
ble tool for inclusion in a biomedical text processing sys-
tem.
As noted in section 3.2, Termino has not been designed
to be used as a stand-alone term recognition system but
rather as the first component, the lexical look-up com-
ponent, in a multi-component term processing system.
Since Termino may return multiple terms for a given
string, or for overlapping strings, some post-filtering of
these alternatives is necessary. Secondly, it is likely that
better term recognition performance will be obtained by
supplementing Termino look-up with a term parser which
uses a grammar to give a term recognizer the generative
capacity to recognize previously unseen terms. For ex-
ample, many terms for chemical compounds conform to
grammars that allow complex terms to be built out of sim-
pler terms prefixed or suffixed with numerals separated
from the simpler term with hyphens. It does not make
sense to attempt to store in Termino all of these variants.
Termino provides a firm basis on which to build large-
scale biomedical text processing applications. However,
there are a number of directions where further work can
be done. First, as noted in 3.2, morphological informa-
tion is currently not held in Termino, but rather resides
in an external morphological analyzer. We are working
to extend the Termino data model to enable information
about morphological variation to be stored in Termino,
so that Termino serves as a single source of information
for the terms it contains. Secondly, we are working to
build term induction modules to allow Termino content
to be automatically acquired from corpora, in addition
to deriving it from manually created resources such as
UMLS. Finally, while we have already incorporated Ter-
mino into the AMBIT system where it collaborates with
a term parser to perform more complete term recogni-
tion, more work can be done to with respect to such an
integration. For example, probabilities could be incorpo-
rated into Termino to assist with probabilistic parsing of
terms; or, issues of trade-off between what should be in
the term lexicon versus the term grammar could be fur-
ther explored by looking to see which compound terms
in the lexicon contain other terms as substrings and at-
tempt to abstract away from these to grammar rules. For
example, in the example thyroid disfunction above, both
thyroid and disfunction are terms, the first of class ?body
part?, the second of class ?problem?. Their combination
thyroid disfunction is a term of class ?problem?, suggest-
ing a rule of the form ?problem?   ?body part? ?problem?.
References
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of the American Medical Infor-
matics Association Symposium, pages 17?21.
R. Gaizauskas and Y. Wilks. 1998. Information extrac-
tion: Beyond document retrieval. Journal of Docu-
mentation, 54(1):70?105.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Protein structures and information extrac-
tion from biological texts: The PASTA system. Jour-
nal of Bioinformatics, 19(1):135?143.
C.A. Goble, C.J. Wroe, R. Stevens, and the my-
Grid consortium. 2003. The myGrid project:
Services, architecture and demonstrator. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In Maria Teresa Pazienza, editor, In-
formation Extraction, pages 10?27. Springer Verlag.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: the
medSynDiKATe text mining system. In Proceedings
of the Pacific Symposium on Biocomputing, pages 338?
349.
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and
G.O. Barnett. 1998. The Unified Medical Language
System: An informatics research collaboration. Jour-
nal of the American Medical Informatics Association,
1(5):1?13.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to biolog-
ical science journal articles: Enzyme interactions and
protein structures. In Proceedings of the Pacific Sym-
posium on Biocomputing, pages 505?516.
A.G. Murzin, S.E. Brenner, T. Hubbard, and C. Chothia.
1995. SCOP: A structural classification of proteins
database for the investigation of sequences and struc-
tures. Journal of Molecular Biology, (247):536?540.
(http://scop.mrc-lmb.cam.ac.uk/scop/).
Online Mendelian Inheritance in Man, OMIM (TM).
2000. McKusick-Nathans Institute for Genetic
Medicine, Johns Hopkins University (Baltimore, MD)
and National Center for Biotechnology Informa-
tion, National Library of Medicine (Bethesda, MD).
http://www.ncbi.nlm.nih.gov/omim/.
J. Pustejovsky, J. Castan?o, R. Saur??, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain, As-
sociation for Computational Linguistics 40th Anniver-
sary Meeting (ACL-02), pages 85?92.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, R. Gaizauskas, M. Hepple, D. Scott,
and R. Power. 2003. Joining up health care
with clinical and post-genomic research. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
C.T. Rindflesch, J.V. Rajan, and L. Hunter. 2000. Ex-
tracting molecular binding relationships from biomed-
ical text. In Proceedings of the 6th Applied Natu-
ral Language Processing conference / North American
chapter of the Association for Computational Linguis-
tics annual meeting, pages 188?915.
The Gene Ontology Consortium. 2001. Creating the
gene ontology resource: design and implementation.
Genome Research, 11(8):1425?1433.
J. Thomas, D. Milward, C. Ouzounis, and S. Pulman.
2000. Automatic extraction of protein interactions
from scientific abstracts. In Proceedings of the Pacific
Symposium on Biocomputing, pages 538?549.
H.M. Wain, M. Lush, F. Ducluzeau, and S. Povey.
2002. Genew: The human nomenclature
database. Nucleic Acids Research, 30(1):169?171.
(http://www.gene.ucl.ac.uk/nomenclature/).
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 80?87,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Knowledge Sources for Word Sense
Disambiguation of Biomedical Text
Mark Stevenson, Yikun Guo
and Robert Gaizauskas
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello Street
Sheffield, S1 4DP
United Kingdom
{inital.surname}@dcs.shef.ac.uk
David Martinez
Department of Computer Science
& Software Engineering
University of Melbourne
Victoria 3010
Australia
davidm@csse.unimelb.edu.au
Abstract
Like text in other domains, biomedical doc-
uments contain a range of terms with more
than one possible meaning. These ambigu-
ities form a significant obstacle to the auto-
matic processing of biomedical texts. Previ-
ous approaches to resolving this problem have
made use of a variety of knowledge sources in-
cluding linguistic information (from the con-
text in which the ambiguous term is used) and
domain-specific resources (such as UMLS). In
this paper we compare a range of knowledge
sources which have been previously used and
introduce a novel one: MeSH terms. The best
performance is obtained using linguistic fea-
tures in combination with MeSH terms. Re-
sults from our system outperform published
results for previously reported systems on a
standard test set (the NLM-WSD corpus).
1 Introduction
The number of documents discussing biomedical
science is growing at an ever increasing rate, making
it difficult to keep track of recent developments. Au-
tomated methods for cataloging, searching and nav-
igating these documents would be of great benefit
to researchers working in this area, as well as hav-
ing potential benefits to medicine and other branches
of science. Lexical ambiguity, the linguistic phe-
nomena where a word or phrase has more than
one potential meaning, makes the automatic pro-
cessing of text difficult. For example, ?cold? has
six possible meanings in the Unified Medical Lan-
guage System (UMLS) Metathesaurus (Humphreys
et al, 1998) including ?common cold?, ?cold sen-
sation? and ?Chronic Obstructive Airway Disease
(COLD)?. The NLM Indexing Initiative (Aronson et
al., 2000) attempted to automatically index biomedi-
cal journals with concepts from the UMLS Metathe-
saurus and concluded that lexical ambiguity was the
biggest challenge in the automation of the indexing
process. Weeber et al (2001) analysed MEDLINE
abstracts and found that 11.7% of phrases were am-
biguous relative to the UMLS Metathesaurus.
Word Sense Disambiguation (WSD) is the pro-
cess of resolving lexical ambiguities. Previous re-
searchers have used a variety of approaches for
WSD of biomedical text. Some of them have taken
techniques proven to be effective for WSD of gen-
eral text and applied them to ambiguities in the
biomedical domain, while others have created sys-
tems using domain-specific biomedical resources.
However, there has been no direct comparison of
which knowledge sources are the most useful or
whether combining a variety of knowledge sources,
a strategy which has been shown to be successful for
WSD in the general domain (Stevenson and Wilks,
2001), improves results.
This paper compares the effectiveness of a vari-
ety of knowledge sources for WSD in the biomed-
ical domain. These include features which have
been commonly used for WSD of general text as
well as information derived from domain-specific
resources. One of these features is MeSH terms,
which we find to be particularly effective when com-
bined with generic features.
The next section provides an overview of various
approaches to WSD in the biomedical domain. Sec-
80
tion 3 outlines our approach, paying particular atten-
tion to the range of knowledge sources used by our
system. An evaluation of this system is presented
in Section 4. Section 5 summarises this paper and
provides suggestions for future work.
2 Previous Work
WSD has been actively researched since the 1950s
and is regarded as an important part of the process
of understanding natural language texts.
2.1 The NLM-WSD data set
Research on WSD for general text in the last decade
has been driven by the SemEval evaluation frame-
works1 which provide a set of standard evaluation
materials for a variety of semantic evaluation tasks.
At this point there is no specific collection for the
biomedical domain in SemEval, but a test collection
for WSD in biomedicine was developed by Wee-
ber et al (2001), and has been used as a benchmark
by many independent groups. The UMLS Metathe-
saurus was used to provide a set of possible mean-
ings for terms in biomedical text. 50 ambiguous
terms which occur frequently in MEDLINE were
chosen for inclusion in the test set. 100 instances
of each term were selected from citations added to
the MEDLINE database in 1998 and manually dis-
ambiguated by 11 annotators. Twelve terms were
flagged as ?problematic? due to substantial disagree-
ment between the annotators. There are an average
of 2.64 possible meanings per ambiguous term and
the most ambiguous term, ?cold? has five possible
meanings. In addition to the meanings defined in
UMLS, annotators had the option of assigning a spe-
cial tag (?none?) when none of the UMLS meanings
seemed appropriate.
Various researchers have chosen to evaluate their
systems against subsets of this data set. Liu et al
(2004) excluded the 12 terms identified as problem-
atic by Weeber et al (2001) in addition to 16 for
which the majority (most frequent) sense accounted
for more than 90% of the instances, leaving 22 terms
against which their system was evaluated. Leroy and
Rindflesch (2005) used a set of 15 terms for which
the majority sense accounted for less than 65% of
the instances. Joshi et al (2005) evaluated against
1http://www.senseval.org
the set union of those two sets, providing 28 am-
biguous terms. McInnes et al (2007) used the set
intersection of the two sets (dubbed the ?common
subset?) which contained 9 terms. The terms which
form these various subsets are shown in Figure 1.
The 50 terms which form the NLM-WSD data set
represent a range of challenges for WSD systems.
The Most Frequent Sense (MFS) heuristic has be-
come a standard baseline in WSD (McCarthy et al,
2004) and is simply the accuracy which would be
obtained by assigning the most common meaning of
a term to all of its instances in a corpus. Despite its
simplicity, the MFS heuristic is a hard baseline to
beat, particularly for unsupervised systems, because
it uses hand-tagged data to determine which sense
is the most frequent. Analysis of the NLM-WSD
data set showed that the MFS over all 50 ambigu-
ous terms is 78%. The different subsets have lower
MFS, indicating that the terms they contain are more
difficult to disambiguate. The 22 terms used by (Liu
et al, 2004) have a MFS of 69.9% while the set
used by (Leroy and Rindflesch, 2005) has an MFS
of 55.3%. The union and intersection of these sets
have MFS of 66.9% and 54.9% respectively.
adjustment
blood pressure
evaluation
immunosuppression
radiation
sensitivity
degree
growth
man
mosaic
nutrition
cold
depression
discharge
extraction
fat
implantation
association
condition
culture
determination
energy
failure
fit
fluid
frequency
ganglion
glucose
inhibition 
pressure 
resistance
secretion
single
strains
support
surgery
transient
transport
variation
repair
scale
weight
white
japanese
lead
mole
pathology
reduction
sex
ultrasound
NLM-WSD data set
Liu et. al. (2004)
Leroy and Rindflesch (2005)
Figure 1: The NLM-WSD test set and some of its sub-
sets. Note that the test set used by (Joshi et al, 2005)
comprises the set union of the terms used by (Liu et al,
2004) and (Leroy and Rindflesch, 2005) while the ?com-
mon subset? is formed from their intersection.
2.2 WSD of Biomedical Text
A standard approach to WSD is to make use of
supervised machine learning systems which are
trained on examples of ambiguous words in con-
text along with the correct sense for that usage. The
81
models created are then applied to new examples of
that word to determine the sense being used.
Approaches which are adapted from WSD of gen-
eral text include Liu et al (2004). Their technique
uses a supervised learning algorithm with a vari-
ety of features consisting of a range of collocations
of the ambiguous word and all words in the ab-
stract. They compared a variety of supervised ma-
chine learning algorithms and found that a decision
list worked best. Their best system correctly dis-
ambiguated 78% the occurrences of 22 ambiguous
terms in the NLM-WSD data set (see Section 2.1).
Joshi et al (2005) also use collocations as features
and experimented with five supervised learning al-
gorithms: Support Vector Machines, Naive Bayes,
decision trees, decision lists and boosting. The Sup-
port Vector Machine performed scoring 82.5% on
a set of 28 words (see Section 2.1) and 84.9% on
the 22 terms used by Liu et al (2004). Performance
of the Naive Bayes classifier was comparable to the
Support Vector Machine, while the other algorithms
were hampered by the large number of features.
Examples of approaches which have made use of
knowledge sources specific to the biomedical do-
main include Leroy and Rindflesch (2005), who re-
lied on information from the UMLS Metathesaurus
assigned by MetaMap (Aronson, 2001). Their sys-
tem used information about whether the ambigu-
ous word is the head word of a phrase identified by
MetaMap, the ambiguous word?s part of speech, se-
mantic relations between the ambiguous words and
surrounding words from UMLS as well as semantic
types of the ambiguous word and surrounding word.
Naive Bayes was used as a learning algorithm. This
approach correctly disambiguated 65.6% of word in-
stances from a set of 15 terms (see Section 2.1).
Humphrey et al (2006) presented an unsupervised
system that also used semantic types. They con-
structed semantic type vectors for each word from
a large collection of MEDLINE abstracts. This al-
lowed their method to perform disambiguation at a
coarser level, without the need for labeled training
examples. In most cases the semantic types can be
mapped to the UMLS concepts but not for five of the
terms in the NLM-WSD data set. Humphrey et al
(2006) reported 78.6% accuracy over the remaining
45. However, their approach could not be applied
to all instances of ambiguous terms and, in particu-
lar, is unable to model the ?none? tag. Their system
could only assign senses to an average of 54% of the
instances of each ambiguous term.
McInnes et al (2007) made use of Concept
Unique Identifiers (CUIs) from UMLS which are
also assigned by MetaMap. The information con-
tained in CUIs is more specific than in the semantic
types applied by Leroy and Rindflesch (2005). For
example, there are two CUIs for the term ?culture?
in UMLS: ?C0010453: Anthropological Culture?
and ?C0430400: Laboratory Culture?. The seman-
tic type for the first of these is ?Idea or Concept? and
?Laboratory Procedure? for the second. McInnes et
al. (2007) were interested in exploring whether the
more specific information contained in CUIs was
more effective than UMLS semantic types. Their
best result was reported for a system which repre-
sented each sense by all CUIs which occurred at
least twice in the abstract surrounding the ambigu-
ous word. They used a Naive Bayes classifier as the
learning algorithm. McInnes et al (2007) reported
an accuracy of 74.5% on the set of ambiguous terms
tested by Leroy and Rindflesch (2005) and 80.0% on
the set used by Joshi et al (2005). They concluded
that CUIs are more useful for WSD than UMLS se-
mantic types but that they are not as robust as fea-
tures which are known to work in general English,
such as unigrams and bigrams.
3 Approach
Our approach is to adapt a state-of-the-art WSD sys-
tem to the biomedical domain by augmenting it with
additional domain-specific and domain-independent
knowledge sources. Our basic system (Agirre and
Mart??nez, 2004) participated in the Senseval-3 chal-
lenge (Mihalcea et al, 2004) with a performance
close to the best system for the English and Basque
lexical sample tasks. The system is based on a su-
pervised learning approach. The features used by
Agirre and Mart??nez (2004) are derived from text
around the ambiguous word and are domain inde-
pendent. We refer to these as linguistic features.
This feature set has been adapted for the disam-
biguation of biomedical text by adding further lin-
guistic features and two different types of domain-
specific features: CUIs (as used by (McInnes et al,
2007)) and Medical Subject Heading (MeSH) terms.
82
3.1 Features
Our feature set contains a number of parameters
which were set empirically (e.g. threshold for un-
igram frequency in the linguistic features). In addi-
tion, we use the entire abstract as the context of the
ambiguous term for relevant features rather than just
the sentence containing the term. Effects of varying
these parameters are consistent with previous results
(Liu et al, 2004; Joshi et al, 2005; McInnes et al,
2007) and are not reported in this paper.
Linguistic features: The system uses a wide
range of domain-independent features which are
commonly used for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags2 and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence with the target word. For example, con-
sider the sentence below with the target word
adjustment.
?Body surface area adjustments of
initial heparin dosing...?
The features would include the following: left-
content-word-lemma ?area adjustment?, right-
function-word-lemma ?adjustment of ?, left-
POS ?NN NNS?, right-POS ?NNS IN?, left-
content-word-form ?area adjustments?, right-
function-word-form ?adjustment of ?, etc.
? Syntactic Dependencies: These features model
longer-distance dependencies of the ambigu-
ous words than can be represented by the lo-
cal collocations. Five relations are extracted:
object, subject, noun-modifier, preposition and
sibling. These are identified using heuristic pat-
terns and regular expressions applied to PoS tag
sequences around the ambiguous word. In the
above example, ?heparin? is noun-modifier fea-
ture of ?adjustment?.
2A maximum-entropy-based part of speech tagger was used
(Ratnaparkhi, 1996) without the adaptation to the biomedical
domain.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of unigrams which appear
more frequently than a predefined threshold in
the entire corpus, excluding those in a list of
stopwords. We empirically set the threshold
to 1. This feature was not used by Agirre and
Mart??nez (2004), but Joshi et al (2005) found
them to be useful for this task.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates all the possible com-
binations of the concept names found. For exam-
ple, MetaMap will segment the phrase ?Body sur-
face area adjustments of initial heparin dosing ...?
into two chunks: ?Body surface area adjustments?
and ?of initial heparin dosing?. The first chunk
will be mapped onto four CUIs with the concept
name ?Body Surface Area?: ?C0005902: Diag-
nostic Procedure? and ?C1261466: Organism At-
tribute? and a further pair with the name ?Adjust-
ments?: ?C0456081: Health Care Activity? and
?C0871291: Individual Adjustment?. The final re-
sults from MetaMap for the first chunk will be eight
combinations of those concept names, e.g. first four
by second two concept names. CUIs which occur
more than three times in the abstract containing the
ambiguous word are included as features.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH contains over 24,000 terms organised into an
11 level hierarchy.
The terms assigned to the abstract in which
each ambiguous word occurs are used as fea-
tures. For example, the abstract containing our
example phrase has been assigned 16 MeSH
83
terms including ?M01.060.116.100: Aged?,
?M01.060.116.100.080: Aged, 80 and over?,
?D27.505.954.502.119: Anticoagulants? and
?G09.188.261.560.150: Blood Coagulation?. To
our knowledge MeSH terms have not been pre-
viously used as a feature for WSD of biomedical
documents.
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model is a memory-based
learning algorithm which was used by (Agirre and
Mart??nez, 2004). Each occurrence of an ambiguous
word is represented as a binary vector in which each
position indicates the occurrence/absence of a fea-
ture. A single centroid vector is generated for each
sense during training. These centroids are compared
with the vectors that represent new examples using
the cosine metric to compute similarity. The sense
assigned to a new example is that of the closest cen-
troid.
The Naive Bayes classifier is based on a proba-
bilistic model which assumes conditional indepen-
dence of features given the target classification. It
calculates the posterior probability that an instance
belongs to a particular class given the prior proba-
bilities of the class and the conditional probability
of each feature given the target class.
Support Vector Machines have been widely
used in classification tasks. SVMs map feature vec-
tors onto a high dimensional space and construct a
classifier by searching for the hyperplane that gives
the greatest separation between the classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Results
This system was applied to the NLM-WSD data set.
Experiments were carried out using each of the three
types of features (linguistic, CUI and MeSH) both
alone and in combination. Ten-fold cross valida-
tion was used, and the figures we report are averaged
across all ten runs.
Results from this experiment are shown in Table
1 which lists the performance using combinations of
learning algorithm and features. The figure shown
for each configuration represents the percentage of
instances of ambiguous terms which are correctly
disambiguated.
These results show that each of the three types
of knowledge (linguistic, CUIs and MeSH) can be
used to create a classifier which achieves a reason-
able level of disambiguation since performance ex-
ceeds the relevant baseline score. This suggests that
each of the knowledge sources can contribute to the
disambiguation of ambiguous terms in biomedical
text.
The best performance is obtained using a combi-
nation of the linguistic and MeSH features, a pattern
observed across all test sets and machine learning
algorithms. Although the increase in performance
gained from using both the linguistic and MeSH
features compared to only the linguistic features is
modest it is statistically significant, as is the differ-
ence between using both linguistic and MeSH fea-
tures compared with using the MeSH features alone
(Wilcoxon Signed Ranks Test, p < 0.01).
Combining MeSH terms with other features gen-
erally improves performance, suggesting that the
information contained in MeSH terms is distinct
from the other knowledge sources. However, the
inclusion of CUIs as features does not always im-
prove performance and, in several cases, causes it to
fall. This is consistent with McInnes et al (2007)
who concluded that CUIs were a useful informa-
tion source for disambiguation of biomedical text
but that they were not as robust as a linguistic knowl-
edge source (unigrams) which they had used for a
previous system. The most likely reason for this is
that our approach relies on automatically assigned
CUIs, provided by MetaMap, while the MeSH terms
are assigned manually. We do not have access to a
reliable assignment of CUIs to text; if we had WSD
would not be necessary. On the other hand, reli-
ably assigned MeSH terms are readily available in
Medline. The CUIs assigned by MetaMap are noisy
while the MeSH terms are more reliable and prove
to be a more useful knowledge source for WSD.
The Vector Space Model learning algorithm per-
forms significantly better than both Support Vector
Machine and Naive Bayes (Wilcoxon Signed Ranks
Test, p < 0.01). This pattern is observed regardless
84
Features
CUI+ Linguistic Linguistic Linguistic+Data sets Linguistic CUI MeSH
MeSH +MeSH +CUI MeSH+CUI
Vector space model
All words 87.2 85.8 81.9 86.9 87.8 87.3 87.6
Joshi subset 82.3 79.6 76.6 81.4 83.3 82.4 82.6
Leroy subset 77.8 74.4 70.4 75.8 79.0 78.0 77.8
Liu subset 84.3 81.3 78.3 83.4 85.1 84.3 84.5
Common subset 79.6 75.1 70.4 76.9 80.8 79.6 79.2
Naive Bayes
All words 86.2 81.2 85.7 81.1 86.4 81.4 81.5
Joshi subset 80.6 73.4 80.1 73.3 80.9 73.7 73.8
Leroy subset 76.4 66.1 74.6 65.9 76.8 66.3 66.3
Liu subset 81.9 75.4 81.7 75.3 82.2 75.5 75.6
Common subset 76.7 66.1 74.7 65.8 77.2 65.9 65.9
Support Vector Machine
All words 85.6 83.5 85.3 84.5 86.1 85.3 85.6
Joshi subset 79.8 76.4 79.5 78.0 80.6 79.1 79.8
Leroy subset 75.1 69.7 72.6 72.0 76.3 74.2 74.9
Liu subset 81.3 78.2 81.0 80.0 82.0 80.6 81.2
Common subset 75.7 69.8 71.6 73.0 76.8 74.7 75.2
Previous Approaches
MFS Liu et. al. Leroy and Joshi et. McInnes et.
baseline (2004) Rindflesch (2005) al. (2005) al. (2007)
All words 78.0 ? ? ? 85.3
Joshi subset 66.9 ? ? 82.5 80.0
Leroy subset 55.3 ? 65.5 77.4 74.5
Liu subset 69.9 78.0 ? 84.9 82.0
Common subset 54.9 ? 68.8 79.8 75.7
Table 1: Results from WSD system applied to various sections of the NLM-WSD data set using a variety of fea-
tures and machine learning algorithms. Results from baseline and previously published approaches are included for
comparison.
of which set of features are used, and it is consis-
tent of the results in Senseval data from (Agirre and
Mart??nez, 2004).
4.1 Per-Word Analysis
Table 2 shows the results of our best performing sys-
tem (combination of linguistic and MeSH features
using the Vector Space Model learning algorithm).
Comparable results for previous supervised systems
are also reported where available.3 The MFS base-
line for each term is shown in the leftmost column.
The performance of Leroy and Rindflesch?s sys-
3It is not possible to directly compare our results with Liu
et al (2004) or Humphrey et al (2006). The first report only
optimal configuration for each term (combination of feature sets
and learning algorithm) while the second do not assign senses
to all of the instances of each ambiguous term (see Section 2).
tem is always lower than the best result for each
word. The systems reported by Joshi et al (2005)
and McInnes et al (2007) are better than, or the
same as, all other systems for 14 and 12 words re-
spectively. The system reported here achieves re-
sults equal to or better than previously reported sys-
tems for 33 terms.
There are seven terms for which the performance
of our approach is actually lower than the MFS base-
line (shown in italics) in Table 2. (In fact, the base-
line outperforms all systems for four of these terms.)
The performance of our system is within 1% of the
baseline for five of these terms. The remaining pair,
?blood pressure? and ?failure?, are included in the
set of problematic words identified by (Weeber et
al., 2001). Examination of the possible senses show
that they include pairs with similar meanings. For
85
MFS Leroy and Joshi et. McInnes et. Reported
baseline Rindflesch (2005) al. (2005) al. (2007) system
adjustment 62 57 71 70 74
association 100 - - 97 100
blood pressure 54 46 53 46 46
cold 86 - 90 89 88
condition 90 - - 89 89
culture 89 - - 94 95
degree 63 68 89 79 95
depression 85 - 86 81 88
determination 79 - - 81 87
discharge 74 - 95 96 95
energy 99 - - 99 98
evaluation 50 57 69 73 81
extraction 82 - 84 86 85
failure 71 - - 73 67
fat 71 - 84 77 84
fit 82 - - 87 88
fluid 100 - - 99 100
frequency 94 - - 94 94
ganglion 93 - - 94 96
glucose 91 - - 90 91
growth 63 62 71 69 68
immunosuppression 59 61 80 75 80
implantation 81 - 94 92 93
inhibition 98 - - 98 98
japanese 73 - 77 76 75
lead 71 - 89 90 94
man 58 80 89 80 90
mole 83 - 95 87 93
mosaic 52 66 87 75 87
nutrition 45 48 52 49 54
pathology 85 - 85 84 85
pressure 96 - - 93 95
radiation 61 72 82 81 84
reduction 89 - 91 92 89
repair 52 81 87 93 88
resistance 97 - - 96 98
scale 65 84 81 83 88
secretion 99 - - 99 99
sensitivity 49 70 88 92 93
sex 80 - 88 87 87
single 99 - - 98 99
strains 92 - - 92 93
support 90 - - 91 89
surgery 98 - - 94 97
transient 99 - - 98 99
transport 93 - - 93 93
ultrasound 84 - 92 85 90
variation 80 - - 91 95
weight 47 68 83 79 81
white 49 62 79 74 76
Table 2: Per-word performance of best reported systems.
example, the two senses which account for 98% of
the instances of ?blood pressure?, which refer to the
blood pressure within an organism and the result ob-
tained from measuring this quantity, are very closely
related semantically.
5 Conclusion
This paper has compared a variety of knowledge
sources for WSD of ambiguous biomedical terms
and reported results which exceed the performance
of previously published approaches. We found that
accurate results can be achieved using a combina-
tion of linguistic features commonly used for WSD
86
of general text and manually assigned MeSH terms.
While CUIs are a useful source of information for
disambiguation, they do not improve the perfor-
mance of other features when used in combination
with them. Our approach uses manually assigned
MeSH terms while the CUIs are obtained automati-
cally using MetaMap.
The linguistic knowledge sources used in this pa-
per comprise a wide variety of features including
n-grams and syntactic dependencies. We have not
explored the effectiveness of these individually and
this is a topic for further work.
In addition, our approach does not make use of
the fact that MeSH terms are organised into a hierar-
chy. It would be interesting to discover whether this
information could be used to improve WSD perfor-
mance. Others have developed techniques to make
use of hierarchical information in WordNet for WSD
(see Budanitsky and Hirst (2006)) which could be
adapted to MeSH.
References
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson, O. Bodenreider, H. Chang, S. Humphrey,
J. Mork, S. Nelson, T. Rindflesch, and W. Wilbur.
2000. The NLM Indexing Initiative. In Proceedings
of the AMIA Symposium.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13?47.
S. Humphrey, W. Rogers, H. Kilicoglu, D. Demner-
Fushman, and T. Rindflesch. 2006. Word Sense Dis-
ambiguation by selecting the best semantic type based
on Journal Descriptor Indexing: Preliminary experi-
ment. Journal of the American Society for Information
Science and Technology, 57(5):96?113.
L. Humphreys, D. Lindberg, H. Schoolman, and G. Bar-
nett. 1998. The Unified Medical Language System:
An Informatics Research Collaboration. Journal of the
American Medical Informatics Association, 1(5):1?11.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
G. Leroy and T. Rindflesch. 2005. Effects of Information
and Machine Learning algorithms on Word Sense Dis-
ambiguation with small datasets. International Jour-
nal of Medical Informatics, 74(7-8):573?585.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004.
Finding predominant senses in untagged text. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Lingusitics (ACL-2004), pages
280?287, Barcelona, Spain.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA., June.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 133?142.
M. Stevenson and Y. Wilks. 2001. The Interaction of
Knowledge Sources in Word Sense Disambiguation.
Computational Linguistics, 27(3):321?350.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
87
Proceedings of the Workshop on BioNLP, pages 71?79,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Disambiguation of Biomedical Abbreviations
Mark Stevenson1, Yikun Guo2, Abdulaziz Al Amri3 and Robert Gaizauskas4
Department of Computer Science
University of Sheffield
Regent Court, 211 Portobello
Sheffield, S1 4DP
United Kingdom
1,2,4{initial.surname}@dcs.shef.ac.uk, 3abdulazizmail@gmail.com
Abstract
Abbreviations are common in biomedical doc-
uments and many are ambiguous in the sense
that they have several potential expansions.
Identifying the correct expansion is necessary
for language understanding and important for
applications such as document retrieval. Iden-
tifying the correct expansion can be viewed as
a Word Sense Disambiguation (WSD) prob-
lem. A WSD system that uses a variety of
knowledge sources, including two types of in-
formation specific to the biomedical domain,
is also described. This system was tested on a
corpus of ambiguous abbreviations, created by
automatically identifying the correct expan-
sion in Medline abstracts, and found to iden-
tify the correct expansion with up to 99% ac-
curacy.
1 Introduction
Many abbreviations are ambiguous in the sense that
they have more than one possible expansion. For
example, expansions for ?NLP? include ?Neuro-
linguistic Programming? as well as ?Natural Lan-
guage Processing?. Ambiguous abbreviations form
a challenge to language understanding since iden-
tification of the correct expansion is often impor-
tant. The query ?NLP?, for example, returns pages
which refer to ?Neuro-linguistic programming? for
most web search engines, pages which are of lim-
ited value to those interested in Natural Language
Processing. In some cases this problem could be
obviated by altering the query terms, for example
including ?Natural?, ?Language? and ?Processing?.
However, this will not help when the abbreviation?s
expansion does not occur within the document. Fred
and Cheng (1999) point out that this is often the case
in biomedical documents, in this domain ubiquitous
abbreviations (such as DNA and mRNA) often ap-
pear without an expansion.
It has been reported that misinterpretation of ab-
breviations in biomedical documents has lead to
medical practitioners making fatal errors (Fred and
Cheng, 1999). However, identifying the correct ex-
pansion is not a straightforward task since an ab-
breviation may have several possible expansions.
Chang et al (2002) reported that abbreviations in
biomedical journal articles consisting of six charac-
ters or less have an average of 4.61 possible mean-
ings and Pustejovsky et al (2002) mention that the
simple abbreviation ?AC? is associated with at least
10 strings in different biomedical documents includ-
ing ?atrioventricular connection?, ?anterior colpor-
rhaphy procedure?, ?auditory cortex? and ?atypical
carcinoid?.
The problem of identifying the correct expansion
of an ambiguous abbreviation can be viewed as a
Word Sense Disambiguation (WSD) task where the
various expansions are the ?senses? of the abbrevia-
tion. In this paper we approach the problem in this
way by applying a WSD system which has previ-
ously been applied to biomedical text (Stevenson et
al., 2008). The WSD system uses a variety of infor-
mation sources, including those traditionally applied
to the WSD problem in addition to two knowledge
sources that are specific to the biomedical domain.
Evaluation of systems for disambiguating am-
biguous abbreviations has been hindered by the fact
71
that there is no freely available benchmark corpus
against which approaches can be compared. We de-
scribe a process whereby such a corpus can be cre-
ated by automatically mining abstracts from Med-
line. This corpus is being made publicly available
to encourage comparative research in this area. Our
abbreviation disambiguation system was evaluated
against this corpus and found to identify the correct
abbreviation with up to 99% accuracy.
The remainder of this paper is organised as fol-
lows. The next section describes relevant previous
work on disambiguation of abbreviations. Section
3 describes a supervised learning WSD system tai-
lored specifically to the biomedical domain. Section
4 describes the automatic creation of a corpus of am-
biguous abbreviations designed specifically for the
training and evaluation of abbreviation disambigua-
tion systems. Section 5 describes the evaluation of
our system on this corpus. Our conclusions are pre-
sented in Section 6.
2 Previous Work
Gaudan et al (2005) distinguish two types of abbre-
viations: global and local. Global abbreviations are
those found in documents without the expansion ex-
plicitly stated, while local abbreviations are defined
in the same document in which the abbreviation oc-
curs. Our work is concerned with the problem of
disambiguating global abbreviations. Gaudan et al
(2005) point out that global abbreviations are often
ambiguous.
Various researchers have explored the problem
of disambiguating global abbreviations in biomed-
ical documents. Liu et al (2001)(2002) used sev-
eral domain-specific knowledge sources to identify
terms which are semantically related to each possi-
ble expansion but which have only one sense them-
selves. Instances of these terms were identified in
a corpus of biomedical journal abstracts and used
as training data. Their learning algorithm uses a
variety of features including all words in the ab-
stract and collocations of the ambiguous abbrevia-
tion. They report an accuracy of 97% on a small set
of abbreviations. Liu et al (2004) present a fully
supervised approach. They compared a variety of
supervised machine learning algorithms and found
that the best performance over a set of 15 ambigu-
ous abbreviations, 98.6%, was obtained using Naive
Bayes. Gaudan et al (2005) use a Support Vector
Machine trained on a bag-of-words model and re-
port an accuracy of 98.5%. Yu et al (2006) exper-
imented with two supervised learning algorithms:
Naive Bayes and Support Vector Machines. They
extracted a corpus containing examples of 60 ab-
breviations from a set of biomedical journal articles
which was split so that abstracts in which the abbre-
viations were defined were used as training data and
those in which no definition is found as test data.
Abbreviations in the test portion were manually dis-
ambiguated. They report 79% coverage and 80%
precision using a Naive Bayes classifier. Pakho-
mov (2002) applied a maximum entropy model to
identify the meanings of ambiguous abbreviations in
10,000 rheumatology notes with around 89% accu-
racy. Joshi et al (2006) disambiguated abbreviations
in clinical notes using three supervised learning al-
gorithms (Naive Bayes, decision trees and Support
Vector Machines). They used a range of features and
found that the best performance was obtained when
these were combined. Unfortunately direct compari-
son of these methods is made difficult by the fact that
various researchers have evaluated their approaches
on different data sets.
A variety of approaches have also been proposed
for the problem of disambiguating local abbrevia-
tions in biomedical documents. This task is equiv-
alent to identifying the abbreviation?s expansion in
the document. The problem is relatively straight-
forward for abbreviations which are created by se-
lecting the first character from each word in the ex-
pansion, such as ?angiotensin converting enzyme
(ACE)?, but is more difficult when this convention
is not followed, for example ?acetylchlinesterase
(ACE)?, ?antisocial personality (ASP)? and ?cata-
lase (CAT)?. Okazaki et al (2008) recently pro-
posed an approach to this problem based on dis-
criminative alignment that has been shown to per-
form well. However, the most common solutions
are based on heuristic approaches, for example
Adar (2004) and Zhou et al (2006). Pustejovsky
et al (2002) used hand-built regular expressions.
Schwartz and Hearst (2003) describe an approach
which starts by identifying the set of candidate ex-
pansions in the same sentence as an abbreviation.
The most likely one is identified by searching for the
72
shortest candidate which contains all the characters
in the abbreviation in the correct order.
3 Abbreviation Disambiguation System
Our abbreviation disambiguation system is based on
a state-of-the-art WSD system that has been adapted
to the biomedical domain by augmenting it with ad-
ditional knowledge sources. The system on which
our approach is based (Agirre and Mart??nez, 2004)
participated in the Senseval-3 challenge (Mihalcea
et al, 2004) with a performance close to the best
system for the lexical sample tasks in two languages
while the version adapted to the biomedical domain
has achieved the best recorded results (Stevenson et
al., 2008) on a standard test set consisting of am-
biguous terms (Weeber et al, 2001).
This system is based on a supervised learning ap-
proach with features derived from text around the
ambiguous word that are domain independent. We
refer to these as general features. This feature set
has been adapted for the disambiguation of biomed-
ical text by adding further linguistic features and two
different types of domain-specific features: CUIs (as
used by McInnes et al (2007)) and Medical Sub-
ject Heading (MeSH) terms. This set of features is
more diverse than have been explored by previous
approaches to abbreviation disambiguation.
3.1 Features
Our feature set contains a number of parameters
(e.g. thresholds for unigram and CUI frequencies).
These parameters were set to the same values that
were used when the system was applied to gen-
eral biomedical terms (Stevenson et al, 2008) since
these were found to perform well. We also use the
entire abstract as the context of the ambiguous term
for relevant features rather than just the sentence
containing the term. Effects of altering these vari-
ables are consistent with previous results (Liu et al,
2004; Joshi et al, 2005; McInnes et al, 2007) and
are not reported here.
General features: The system uses a wide range
of domain-independent features that are commonly
employed for WSD.
? Local collocations: A total of 41 features which
extensively describe the context of the am-
biguous word and fall into two main types:
(1) bigrams and trigrams containing the am-
biguous word constructed from lemmas, word
forms or PoS tags and (2) preceding/following
lemma/word-form of the content words (adjec-
tive, adverb, noun and verb) in the same sen-
tence as the ambiguous abbreviation. For ex-
ample, consider the sentence below with the
target abreviation BSA.
?Lean BSA was obtained from height
and lean body weight ...?
The features would include the following:
left-content-word-lemma ?lean BSA?, right-
function-word-lemma ?BSA be?, left-POS ?JJ
NNP?, right-POS ?NNP VBD?, left-content-
word-form ?Lean BSA?, right-function-word-
form ?BSA was?, etc.
? Salient bigrams: Salient bigrams within the ab-
stract with high log-likelihood scores, as de-
scribed by Pedersen (2001).
? Unigrams: Lemmas of all content words in the
abstract and words within a ?4-word window
around the target word, excluding those in a list
of stopwords. In addition, the lemmas of any
unigrams appearing at least twice in the entire
corpus and which are found in the abstract are
also included as features.
Concept Unique Identifiers (CUIs): We follow
the approach presented by McInnes et al (2007) to
generate features based on UMLS Concept Unique
Identifiers (CUIs). The MetaMap program (Aron-
son, 2001) identifies all words and terms in a
text which could be mapped onto a UMLS CUI.
MetaMap does not disambiguate the senses of the
concepts, instead it enumerates likely candidate con-
cepts. For example, MetaMap will segment the
phrase ?Lean BSA was obtained from height and
lean body weight ...? into four chunks: ?Lean
BSA?, ?obtained?, ?from height? and ?lean body
weight?. The first chunk will be mapped onto
three CUIs: ?C1261466: BSA (Body surface area)?,
?C1511233: BSA (NCI Board of Scientific Ad-
visors)? and ?C0036774: BSA (Serum Albumin,
Bovine)?. The chunk ?lean body weight? is mapped
onto two concepts: ?C0005910: Body Weight?
73
and ?C1305866: Body Weight (Weighing patient)?1.
CUIs occurring more than twice in an abstract are in-
cluded as features. CUIs have been used for various
disambiguation tasks in the biomedical domain, in-
cluding disambiguation of ambiguous general terms
(McInnes et al, 2007) and gene symbol disambigua-
tion (Xu et al, 2007), but not, to our knowledge, for
abbreviation disambiguation.
Medical Subject Headings (MeSH): The fi-
nal feature is also specific to the biomedical do-
main. Medical Subject Headings (MeSH) (Nelson
et al, 2002) is a controlled vocabulary for index-
ing biomedical and health-related information and
documents. MeSH terms are manually assigned to
abstracts by human indexers. The latest version of
MeSH (2009) contains over 25,000 terms organised
into an 11 level hierarchy.
The MeSH terms assigned to the abstract in which
each ambiguous word occurs are used as features.
For example, the abstract containing our example
phrase has been assigned 16 terms including ?Body
Surface Area?, ?Body Weight?, ?Humans? and ?Or-
gan Size? . MeSH terms have previously been used
for abbreviation disambiguation by Yu et al (2006).
3.2 Learning Algorithms
We compared three machine leaning algorithms
which have previously been shown to be effective
for WSD tasks.
The Vector Space Model (VSM) is a memory-
based learning algorithm which was used by Agirre
and Mart??nez (2004). Each occurrence of an
ambiguous word is represented as a binary vec-
tor in which each position indicates the occur-
rence/absence of a feature. A single centroid vector
is generated for each sense during training. These
centroids are compared with the vectors that repre-
sent new examples using the cosine metric to com-
pute similarity. The sense assigned to a new example
is that of the closest centroid.
The Naive Bayes (NB) classifier is based on a
probabilistic model which assumes conditional in-
dependence of features given the target classifica-
tion. It calculates the posterior probability that an
1The first of these, C0005910, refers to the weight of
a patient as a property of that individual while the second,
C1305866, refers to the process of weighing a patient as part
of a diagnostic procedure.
instance belongs to a particular class given the prior
probabilities of the class and the conditional proba-
bility of each feature given the target class.
Support Vector Machines (SVM) have been
widely used in classification tasks. SVMs map
feature vectors onto a high dimensional space and
construct a classifier by searching for the hyper-
plane that gives the greatest separation between the
classes.
We used our own implementation of the Vector
Space Model and Weka implementations (Witten
and Frank, 2005) of the other two algorithms.
4 Evaluation Corpus
The most common method for generating corpora
to train and test WSD systems is to manually an-
notate instances of ambiguous terms found in text
with the appropriate meaning. However, this process
is both time-consuming and difficult (Artstein and
Poesio, 2008). An alternative to manual tagging is
to find a way of automatically creating sense tagged
corpora. For the translation of ambiguous English
words Ng et al (2003) made use of the fact that the
various senses are often translated differently. For
example when ?bank? is used in the ?financial insti-
tution? sense it is translated to French as ?banque?
and ?bord? when it is used to mean ?edge of river?.
However, a disadvantage of this approach is that it
relies on the existence of parallel text which may
not be available. In the biomedical domain Liu et al
(2001)(2002) created a corpus using unambiguous
related terms (see Section 2) although they found
that it was not always possible to identify suitable
related terms.
4.1 Corpus Creation
Liu et al (2001) also made use of the fact that
when abbreviations are introduced they are often ac-
companied by their expansion, for example ?BSA
(bovine serum albumin)?. This phenomenon was
exploited to automatically generate a corpus of ab-
breviations and associated definitions by replacing
the abbreviation and expansion with the abbrevia-
tion alone. For example, the sentence ?The adsorp-
tion behavior of bovine serum albumin (BSA) on
a Sepharose based hydrophobic interaction support
has been studied.? becomes ?The adsorption behav-
74
?BSA? AND ?body surface area? NOT ?bovine serum albumin?
?BSA? AND ?bovine serum albumin? NOT ?body surface area?
Figure 1: Example queries for abbreviation ?BSA?
ior of BSA on a Sepharose based hydrophobic inter-
action support has been studied.?
We used this approach to create a corpus of sense
tagged abbreviations in biomedical documents using
a set of 21 three letter abbreviations used in previ-
ous research on abbreviation disambiguation (Liu et
al., 2001; Liu et al, 2002; Liu et al, 2004). Pos-
sible expansions for the majority of these abbrevi-
ations were listed in these papers. For the few re-
maining ones possible expansions were taken from
the Medstract database (Pustejovsky et al, 2002).
We searched for instances of these abbreviations in
Medline, a database containing more than 18 mil-
lion abstracts from publications in biomedicine and
the life sciences. For each abbreviation we queried
Medline, using the Entrez interface, to identify doc-
uments containing one of its meanings. For exam-
ple the abbreviation ?BSA? has two possible expan-
sions: ?body surface area? and ?bovine serum alu-
min?. Medline is searched to identify documents
that contain each possible expansion of the abbre-
viation using the queries shown in Figure 1. Each
query matches documents containing the abbrevia-
tion and relevant expansion and no mentions of the
other possible expansion(s).
The retrieved documents are then processed to
remove the expansions of each abbreviation. The
Schwartz and Hearst (2003) algorithm for identi-
fying abbreviations and the relevant expansion (see
Section 2) is then run over each of the retrieved ab-
stracts to identify the correct expansion. The expan-
sion is removed from the document and stored sep-
arately, effectively creating a sense tagged corpus.
For convenience the abstracts are converted into a
format similar to the one used for the NLM-WSD
corpus (Weeber et al, 2001).
The resulting corpus consists of 55,655 docu-
ments. For each abbreviation Table 1 shows the
number of abstracts retrieved from Medline (in the
column labeled ?Abstracts?) and the number of ex-
pansions (?Count? column). The column labelled
?Rare? lists the number of expansions that account
for fewer than 1% of the occurrences of an abbre-
viation and ?Frequent? lists the percentage of occu-
rances represented by the most frequent expansion.
It can be seen that there is a wide variation between
the number of abstracts retrieved for each abbrevi-
ation. CSF occurs in 14,871 abstracts and ASP in
just 71. There is also a wide variation between the
frequency of the most common expansion with over
99% of the occurrences of ?CSF? representing one
expansion (?cerebrospinal fluid?) while for ?ASP?
two of the five possible expansions (?antisocial per-
sonality? and ?aspartate?) each account for almost
34% of the documents. In addition, several abbrevi-
ations have expansions which occur only rarely. For
example, two of the expansions of ?APC? (?atrial
pressure complexes? and ?aphidicholin?) each have
only a single document and account for just 0.03%
of the instances of that abbreviation.
4.2 Corpus Reduction
Given the diversity of the abbreviations which were
downloaded from Medline, both in terms of num-
ber of documents and distribution of senses, sub-
sets of this corpus that are more suitable for WSD
experiments were created. Corpora containing 100,
200 and 300 randomly selected examples of each ab-
breviation were generated and these are referred to
as Corpus.100, Corpus.200 and Corpus.300 respec-
tively.
Some of the 21 abbreviations were not suitable
for inclusion in these corpora. Abbreviations were
not included in the relevant corpus if an insufficient
number of examples were retrieved from Medline.
For example, only 71 abstracts containing ?ASP?
were retrieved and it is is not included in any of the
three corpora. Similarly, ?ANA? and ?FDP? are not
included in Corpus.200 or Corpus.300 and ?DIP?
not included in Corpus.300. In addition, rare senses,
those which represent fewer than 1% of the occur-
rences of an abbreviation in all retrieved abstracts,
were discarded. Finally, two abbreviations (?ACE?
and ?CSF?) have only one sense that is not ?Rare?
75
Expansions
Abstracts Count Rare Frequent
ACE 3105 3 2 98.7
ANA 100 3 0 58.0
APC 3146 5 2 39.4
ASP 71 5 0 33.8
BPD 1841 3 0 46.7
BSA 5373 2 0 86.4
CAT 4636 3 1 55.2
CML 2234 4 2 91.7
CMV 7665 2 0 96.7
CSF 14871 3 2 99.1
DIP 209 2 0 75.1
EMG 2052 2 0 88.4
FDP 130 4 0 78.5
LAM 325 4 1 48.3
MAC 955 5 1 64.3
MCP 815 5 1 50.2
PCA 2442 5 1 68.9
PCP 1642 2 0 57.8
PEG 607 2 0 94.1
PVC 234 2 2 78.2
RSV 3202 2 0 76.7
Average 2650 3.2 0.6 70.8
Table 1: Properties of abbreviations corpus retrieved
from Medline
(see Table 1) and these were also excluded from the
reduced corpora.
Consequently, Corpus.100 contains 18 abbrevia-
tions (?ACE?, ?ASP? and ?CSF? are excluded), Cor-
pus.200 contains 16 (?ANA? and ?FDP? are also
excluded) and Corpus.300 contains 14 (?DIP? and
?PVC? also excluded). Where an abbreviation is in-
cluded in more than one corpus, all the examples in
the smaller corpus are included in the larger one(s).
For example, the 100 examples of ?APC? in Cor-
pus.100 are also included in Corpus.200 and Cor-
pus.300.
5 Experiments
Various combinations of learning algorithms and
features were applied to the three reduced corpora
described in Section 4.2. Performance of the WSD
system is measured in terms of the proportion of ab-
breviation instances for which the correct expansion
is identified. 10-fold cross validation was used for
all experiments and all quoted results refer to the av-
erage performance across the 10 folds. Results are
shown in Table 2. The baseline figures, based on
selecting the most frequent expansion for each ab-
breviation, are shown for each corpus. Note that
these figures vary slightly across the three corpora
because of the different abbreviations each contains
(see Section 4.2).
A first observation is that performance of the
WSD system is consistently better than the base-
line for the relevant corpus and, with a few excep-
tions, above 90%. As might be expected, perfor-
mance improves as additional training examples are
added. However, even when the number of exam-
ples is relatively low, just 100, performance of the
best configuration (VSM learning algorithm with all
three types of feature) is 97.4%.
The best result, 99% (300 training examples,
VSM learning algorithm with all feature types), ex-
ceeds reported performance of previous abbreviation
disambiguation systems (see Section 2). Although
these results are not directly comparable, since these
studies used different evaluation corpora, the set
of ambiguous abbreviations used in this study and
methodology for corpus creation are similar to those
used by Liu et al (2001)(2002)(2004).
The best performance for each learning algorithm
is obtained when all three types of features are com-
bined. The difference between performance ob-
tained using all three feature types and using only
the MeSH or CUI features is statistically significant
(Wilcoxon Signed Ranks test, p < 0.01) although
the difference between this and performance using
just the linguistic features is not.
The VSM learning algorithm generally performs
better than either the SVM or Naive Bayes learning
algorithms. The difference between performance of
VSM and the other algorithms is statistically signif-
icant for Corpus.100 but not for the other two, sug-
gesting that this learning algorithm is better able to
cope with small number of training examples than
Naive Bayes and Support Vector Machines. Strong
performance of the VSM algorithm is consistent
with previous work which has shown that this algo-
rithm performs well on the disambiguation of am-
biguous terms in both biomedical and general text
(Agirre and Mart??nez, 2004; Stevenson et al, 2008).
76
Features
Algorithm Linguistic Linguistic CUI+ Linguistic+Linguistic CUI MeSH +CUI +MeSH MeSH MeSH+CUI
Corpus.100 (Baseline = 69.0%)
SVM 0.934 0.900 0.949 0.947 0.946 0.938 0.954
NB 0.940 0.917 0.949 0.951 0.947 0.944 0.958
VSM 0.968 0.937 0.888 0.970 0.971 0.939 0.974
Corpus.200 (Baseline = 69.1%)
SVM 0.957 0.911 0.964 0.964 0.964 0.947 0.965
NB 0.966 0.926 0.962 0.969 0.971 0.955 0.972
VSM 0.979 0.930 0.894 0.982 0.981 0.947 0.984
Corpus.300 (Baseline = 68.7%)
SVM 0.966 0.914 0.970 0.968 0.974 0.954 0.975
NB 0.971 0.933 0.960 0.971 0.976 0.960 0.978
VSM 0.981 0.938 0.894 0.987 0.985 0.957 0.990
Table 2: Performance of WSD system using various combinations of learning algorithms and features.
Performance of our system on this task is higher
than would be expected for most WSD tasks sug-
gesting that the problem of abbreviation disam-
biguation is simpler than the disambiguation of gen-
eral terms. The most probable reason for this is that
the various expansions of abbreviations in our cor-
pus are more distinct and better defined than senses
for general terms. For example, the three possi-
ble expansions for ?ANA? in our corpus are a pro-
fessional body (?American Nurses Association?), a
type of medical test (?antinuclear?) and a neuro-
transmitter (?Anandamide?). It is likely that these
diverse meanings will tend to occur in very differ-
ent contexts and in documents with different topics.
On the other hand it is widely accepted that distinc-
tions between possible meanings of words in natu-
ral language are often vague (Kilgarriff, 1993). It
is likely that clearer distinctions between possible
expansions of abbreviations make the task of iden-
tifying the correct one more straightforward than
identifying meanings of ambiguous words. In ad-
dition, the creation of annotated data for WSD is of-
ten hampered by the difficulty in obtaining sufficient
agreement between annotators (Artstein and Poesio,
2008; Weeber et al, 2001) and this problem does not
apply to our automatically-generated corpus.
Results in Table 2 indicate that CUIs are use-
ful features in the disambiguation of abbreviations.
This is in contrast with previous experiments on am-
biguous terms in biomedical documents (Stevenson
et al, 2008) in which it was found that the best
performance as obtained using only linguistic and
MeSH features. It is likely that the clear distinction
between expansions of abbreviations is the reason
behind this difference. CUIs are assigned automat-
ically by the MetaMap program (Aronson, 2001).
However, this assignment is very noisy. It is likely
that the various expansions of abbreviations are dis-
tinct enough for this noise to be tolerated by the
learning algorithms while it causes problems when
the meanings are closer together, such as in the case
of ambiguous terms.
5.1 Performance of Individual Abbreviations
Table 3 shows the performance of the best WSD sys-
tem (VSM learning algorithm with all features) for
each abbreviation in the three subsets of our corpus.
Our system performs well for all abbreviations. Ac-
curacy is no lower than 92% for any abbreviation
using Corpus.100 and no lower than 97% for Cor-
pus.300, demonstrating that the approach is robust.
In fact, the approach still performs well for abbre-
viations with low baseline scores, such as ?APC?,
?BPD? and ?LAM?.
It is interesting to note that the abbreviations with
the lowest performance tend to have expansions that
are closely related. For example, the two expansions
of ?EMG? are ?electromyography? and ?electromyo-
77
Corpus
100 200 300
ANA 0.980 - -
APC 0.980 1.000 1.000
BPD 1.000 1.000 1.000
BSA 0.970 0.970 0.982
CAT 0.990 0.990 1.000
CML 0.960 0.963 0.978
CMV 0.970 0.970 0.970
DIP 1.000 1.000 -
EMG 0.920 0.960 0.980
FDP 0.970 - -
LAM 0.960 0.980 0.980
MAC 0.970 0.990 0.989
MCP 0.980 0.978 1.000
PCA 0.960 0.987 0.992
PCP 0.990 1.000 1.000
PEG 0.980 0.982 1.000
PVC 0.990 1.000 -
RSV 0.960 0.972 0.978
Overall 0.974 0.984 0.990
Table 3: Performance of WSD system over individual ab-
breviations in three reduced corpora
gram? while for ?LAM? one expansion (?Lymphan-
gioleiomyomatosis?) is a rare lung disease and the
other (?Lipoarabinomannan?) a molecule associated
with another lung disease (tuberculosis). On the
other hand, abbreviations that are more accurately
disambiguated tend to have expansions with more
distinct meanings. For example, ?BPD? can be an
acronym for ?borderline personality disorder? (a psy-
chiatric diagnosis), ?bronchopulmonary dysplasia?
(a lung disease) or ?biparietal diameter? (diameter of
a foetus? head in an ultrasound) and the expansions
of ?DIP? are ?desquamative interstitial pneumonia?
(a lung disease) and ?distal interphalangeal joints?
(types of joints in the human hand and foot).
6 Conclusions
This paper has presented an approach to the disam-
biguation of ambiguous abbreviations in biomedi-
cal documents. We treat this problem as a form
of WSD and apply a system that combines a wider
range of features than have been previously applied,
including those which are commonly used within
WSD systems in addition to information from two
domain-specific knowledge sources. The approach
is evaluated using a corpus of abbreviations auto-
matically mined from Medline and found to iden-
tify the correct expansion with accuracy of up to
99%. This figure is higher than previously reported
results for abbreviation disambiguation systems, al-
though direct comparison is difficult due to the use
of different data sets. It was also found that best per-
formance could be obtained using a simple machine
learning algorithm and a diverse range of knowledge
sources. Performance of our system is higher than is
normally achieved by WSD systems when applied
to general terms and we suggest that the reason for
this is that the various expansions of abbreviations
are better defined and more distinct than the senses
of ambiguous words.
This study has been limited to the disambiguation
of abbreviations consisting of exactly three letters.
Possibilities for future work include experimenting
with abbreviations of various lengths.
Data
The corpus described in Section 4 has been
made freely available for research and may
be obtained from http://nlp.shef.ac.uk/
BioWSD/downloads/abbreviationdata/.
Acknowledgments
We are grateful to the anonymous reviewers of this
paper for their valuable feedback.
References
E. Adar. 2004. SaRAD: A simple and robust abbrevia-
tion dictionary. Bioinformatics, 20(4):527?533.
E. Agirre and D. Mart??nez. 2004. The Basque Coun-
try University system: English and Basque tasks. In
Rada Mihalcea and Phil Edmonds, editors, Senseval-
3: Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 44?
48, Barcelona, Spain, July.
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: the MetaMap program.
In Proceedings of the American Medical Informatics
Association (AMIA), pages 17?21.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
78
J. Chang, H. Schu?tze, and R. Altman. 2002. Creating an
Online Dictionary of Abbreviations from MEDLINE.
The Journal of the American Medical Informatics As-
sociation, 9(6):612?620.
H. Fred and T. Cheng. 1999. Acronymesis: the explod-
ing misuse of acronyms. Texas Heart Institute Jour-
nal, 30:255?257.
S. Gaudan, H. Kirsch, and D. Rebholz-Schuhmann.
2005. Resolving abbreviations to their senses in Med-
line. Bioinformatics, 21(18):3658?3664.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A Compara-
tive Study of Support Vector Machines Applied to the
Word Sense Disambiguation Problem for the Medical
Domain. In Proceedings of the Second Indian Confer-
ence on Artificial Intelligence (IICAI-05), pages 3449?
3468, Pune, India.
M. Joshi, S. Pakhomov, T. Pedersen, and C. Chute. 2006.
A comparative study of supervised learning as applied
to acronym expansion in clinical reports. In Proceed-
ings of the Annual Symposium of the American Medi-
cal Informatics Association, pages 399?403, Washing-
ton, DC.
A. Kilgarriff. 1993. Dictionary word sense distinctions:
An enquiry into their nature. Computers and the Hu-
manities, 26:356?387.
H. Liu, Y. Lussier, and C. Friedman. 2001. Disam-
biguating ambiguous biomedical terms in biomedical
narrative text: An unsupervised method. Journal of
Biomedical Informatics, 34:249?261.
H. Liu, S. Johnson, and C. Friedman. 2002. Au-
tomatic Resolution of Ambiguous Terms Based on
Machine Learning and Conceptual Relations in the
UMLS. Journal of the American Medical Informatics
Association, 9(6):621?636.
H. Liu, V. Teller, and C. Friedman. 2004. A Multi-aspect
Comparison Study of Supervised Word Sense Disam-
biguation. Journal of the American Medical Informat-
ics Association, 11(4):320?331.
B. McInnes, T. Pedersen, and J. Carlis. 2007. Using
UMLS Concept Unique Identifiers (CUIs) for Word
Sense Disambiguation in the Biomedical Domain. In
Proceedings of the Annual Symposium of the Ameri-
can Medical Informatics Association, pages 533?537,
Chicago, IL.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004. The
Senseval-3 English lexical sample task. In Proceed-
ings of Senseval-3: The Third International Workshop
on the Evaluation of Systems for the Semantic Analysis
of Text, Barcelona, Spain.
S. Nelson, T. Powell, and B. Humphreys. 2002. The
Unified Medical Language System (UMLS) Project.
In Allen Kent and Carolyn M. Hall, editors, Ency-
clopedia of Library and Information Science. Marcel
Dekker, Inc.
H. Ng, B. Wang, and S. Chan. 2003. Exploiting Parallel
Texts for Word Sense Disambiguation: an Empirical
Study. In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics (ACL-
03), pages 455?462, Sapporo, Japan.
N. Okazaki, S. Ananiadou, and J. Tsujii. 2008. A dis-
criminative alignment model for abbreviation recogni-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 657?664, Manchester, UK.
S. Pakhomov. 2002. Semi-supervised maximum entropy
based approach to acronym and abbreviation normal-
ization in medical texts. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 160?167, Philadelphia, PA.
T. Pedersen. 2001. A Decision Tree of Bigrams is an
Accurate Predictor of Word Sense. In Proceedings
of the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL-01), pages 79?86, Pittsburgh, PA.
J. Pustejovsky, J. Castano, R. Saur, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creating
Large-scale Information Servers for Biomedical Li-
braries. In ACL 2002 Workshop on Natural Language
Processing in the Biomedical Domain.
A. Schwartz and M. Hearst. 2003. A simple algorithm
for identifying abbreviation definitions in biomedical
text. In Proceedings of the Pacific Symposium on Bio-
computing, Kauai.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Developing
a Test Collection for Biomedical Word Sense Disam-
biguation. In Proceedings of AMAI Symposium, pages
746?50, Washington, DC.
I. Witten and E. Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kauf-
mann, San Francisco.
H. Xu, J. Fan, G. Hripcsak, E. Mendonc?a, Markatou M.,
and Friedman C. 2007. Gene symbol disambigua-
tion using knowledge-based profiles. Bioinformatics,
23(8):1015?22.
H. Yu, W. Kim, V. Hatzivassiloglou, and J. Wilbur. 2006.
A large scale, corpus-based approach for automati-
cally disambigutaing biomedical abbreviations. ACM
Transactions on Information Systems, 24(3):380?404.
W. Zhou, I. Vetle, and N. Smalheiser. 2006. ADAM: an-
other database of abbreviations in MEDLINE. Bioin-
formatics, 22(22):2813?2818.
79
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 353?356,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
The Effect of Ambiguity on the Automated Acquisition of WSD Examples
Mark Stevenson and Yikun Guo
Department of Computer Science,
University of Sheffield,
Regent Court, 211 Portobello,
Sheffield, S1 4DP
United Kingdom
m.stevenson@dcs.shef.ac.uk and g.yikun@dcs.shef.ac.uk
Abstract
Several methods for automatically gen-
erating labeled examples that can be
used as training data for WSD systems
have been proposed, including a semi-
supervised approach based on relevance
feedback (Stevenson et al, 2008a). This
approach was shown to generate examples
that improved the performance of a WSD
system for a set of ambiguous terms from
the biomedical domain. However, we find
that this approach does not perform as well
on other data sets. The levels of ambigu-
ity in these data sets are analysed and we
suggest this is the reason for this negative
result.
1 Introduction
Several studies, for example (Mihalcea et al,
2004; Pradhan et al, 2007), have shown that su-
pervised approaches to Word Sense Disambigua-
tion (WSD) outperform unsupervised ones. But
these rely on labeled training data which is diffi-
cult to create and not always available (e.g. (Wee-
ber et al, 2001)). Various techniques for creating
labeled training data automatically have been sug-
gested in the literature. Stevenson et al (2008a)
describe a semi-supervised approach that used rel-
evance feedback (Rocchio, 1971) to analyse ex-
isting labeled examples and use the information
produced to generate further ones. The approach
was tested on the biomedical domain and the addi-
tional examples found to improve performance of
a WSD system. However, biomedical documents
represent a restricted domain. In this paper the
same approach is tested against two data sets that
are not limited to a single domain.
2 Application to a Range of Data Sets
In this paper the relevance feedback approach de-
scribed by Stevenson et al (2008a) is evaluated us-
ing three data sets: the NLM-WSD corpus (Wee-
ber et al, 2001) which Stevenson et al (2008a)
used for their experiments, the Senseval-3 lexical
sample task (Mihalcea et al, 2004) and the coarse-
grained version of the SemEval English lexical
sample task (Pradhan et al, 2007).
2.1 Generating Examples
To generate examples for a particular sense of an
ambiguous term all of the examples where the
term is used in that sense are considered to be
?relevant documents? while the examples in which
any other sense of the term is used are considered
to be ?irrelevant documents?. Relevance feed-
back (Rocchio, 1971) is used to generate a set of
query terms designed to identify relevant docu-
ments, and therefore instances of the sense. The
top five query terms are used to retrieve docu-
ments and these are used as labeled examples of
the sense. Further details of this process are de-
scribed by Stevenson et al (2008a).
This process requires a collection of documents
that can be queried to generate the additional
examples. For the NLM-WSD data set we
used PubMed, a database of biomedical journal
abstracts queried using the Entrez retrieval sys-
tem (http://www.ncbi.nlm.nih.gov/
sites/gquery). The British National Corpus
(BNC) was used for Senseval-3 and SemEval.1
Lucene (http://lucene.apache.org) was
used to index the BNC and retrieve examples.
1We also experimented with the English WaCky corpus
(Baroni et al, 2009) which contains nearly 2 billion words
automatically retrieved from the web. However, results were
not as good as when the BNC was used.
353
2.2 WSD System
We use a WSD system that has been shown to
perform well when evaluated against ambiguities
found in both general text and the biomedical do-
main (Stevenson et al, 2008b). Medical Subject
Headings (MeSH), a controlled vocabulary used
for document indexing, are obtained from PubMed
and used as additional features for the NLM-WSD
data set since they have been shown to improve
performance. The features are combined using
the Vector Space Model, a simple memory-based
learning algorithm.
2.3 Experiment
Experiments were carried out comparing perfor-
mance when the WSD system was trained using
either the examples in the original data set (orig-
inal), the examples generated from these using
the relevance feedback approach (additional) or a
combination of these (combined). The Senseval-
3 and SemEval corpora are split into training and
test portions so the training portion is used as the
original data set and the WSD system evaluated
against the held-back data. As there is no such
recognised standard split for the NLM-WSD cor-
pus, 10-fold cross-validation was used. For each
fold the training portion is used as the original data
set and automatically generated examples created
by examining just that part of the data. Evaluation
is carried out against the fold?s test data and the
average result across the 10 folds reported.
Table 1 shows the results of this experiment.2
Examples generated using the relevance feedback
approach only improve results for one data set, the
NLM-WSD corpus. In this case there is a sig-
nificant improvement (Mann-Whitney, p < 0.01)
when the original and automatically generated ex-
amples are combined. There is no such improve-
ment for the other two data sets: WSD results us-
ing the additional data are noticeably worse than
when the original data is used alone and, although
performance improves when these examples are
combined with the original data, results are still
lower than using the original data. When exam-
ples are combined there is a drop in performance
of 1.2% and 2.9% for SemEval and Senseval-3 re-
2Results reported here for the NLM-WSD corpus are
slightly different from those reported by (Stevenson et al,
2008a). We used an additional feature (MeSH headings),
which improved the baseline performance, and more query
terms which improved the quality of the additional examples
for all three data sets.
spectively.
Corpus Original Additional Combined
NLM-WSD 87.9 87.6 89.2
SemEval 83.7 74.6 82.5
Senseval-3 68.8 56.3 65.9
Table 1: Results of relevance feedback approach
applied to three data sets
These results indicate that the relevance feed-
back approach described by Stevenson et al
(2008a) is not able to generate useful examples for
the Senseval-3 and SemEval data sets, although it
can for the NLM-WSD data set. We hypothesise
that these corpora contain different levels of ambi-
guity which effect suitability of the approach.
3 Analysis of Ambiguities
The three data sets are compared using measures
designed to determine the level of ambiguity they
contain. Section 3.1 reports results using various
widely used measures based on the distribution of
senses. Section 3.2 introduces a measure based
on the semantic similarity between the possible
senses of ambiguous terms.
3.1 Sense Distributions
Three measures for characterising the difficulty of
WSD data sets based on their sense distribution
were used. The first is the widely applied most
frequent sense (MFS) baseline (McCarthy et al,
2004), i.e. the proportion of examples for an am-
biguous term that are labeled with the commonest
sense. The second is number of senses per am-
biguous term. The final measure, the entropy of
the sense distribution, has been shown to be a good
indication of disambiguation difficulty (Kilgarriff
and Rosenzweig, 2000). For two of these mea-
sures (number of senses and entropy) a higher fig-
ure indicates greater ambiguity while for the MFS
measure a lower figure indicates a more difficult
data set.
Table 2 shows the results of computing these
measures averaged across all terms in the cor-
pus. For two measures (number of senses and en-
tropy) the NLM-WSD corpus is least ambiguous,
Senseval-3 the most ambiguous with SemEval be-
tween them. The MFS scores are very similar for
two data sets (NLM-WSD and SemEval), both of
which are much higher than for Senseval-3.
354
These measures suggest that the NLM-WSD
corpus is less ambiguous than the other two and
also that the Senseval-3 corpus is the most am-
biguous of the three.
Corpus MFS Senses Entropy
NLM-WSD 78.0 2.63 0.73
SemEval 78.4 3.60 0.91
Senseval-3 53.8 6.43 1.75
Table 2: Properties of Data Sets using sense distri-
bution measures
3.2 Semantic Similarity
We also developed a measure that takes into ac-
count the similarity in meaning between the possi-
ble senses for an ambiguous term. This measure is
similar to the one used by Passoneau et al (2009)
to analyse levels of inter-annotator agreement in
word sense annotation. Our measure is shown in
equation 1 where Senses is the set of possible
senses for an ambiguous term, |Senses| = n and
(Senses
2
)
is the set of all subsets of Senses contain-
ing two of its members (i.e the set of unordered
pairs). The similarity between a pair of senses,
sim(x, y), can be computed using any lexical sim-
ilarity measure, see Pedersen et al (2004). Essen-
tially this measure computes the mean of the sim-
ilarities between each pair of senses for the term.
sim measure =
?
{x,y}(Senses2 )
sim(x, y)
(n
2
) (1)
One problem with comparing the data sets used
here is that they use a range of sense invento-
ries. Although lexical similarity measures have
been applied to WordNet (Pedersen et al, 2004)
and UMLS (Pedersen et al, 2007), it is not clear
that the scores they produce can be meaningfully
compared. To avoid this problem we mapped the
sense inventories onto a single resource: WordNet
version 3.0.
The mapping was most straightforward for
Senseval-3 which uses WordNet 1.7.1 and could
be automatically mapped onto WordNet 3.0 senses
using publicly available mappings (Daude? et al,
2000). The SemEval data contains a mapping
from the OntoNotes senses to groups of WordNet
2.1 senses. The first sense from this group was
mapped to WordNet 3.0 using the same mappings.
Mapping the NLM-WSD corpus was more
problematic and had to be carried out manually by
comparing sense definitions in UMLS and Word-
Net 3.0. We had expected this process to be diffi-
cult but found clear mappings for the majority of
senses. There were even found cases in which the
sense definitions were identical in both resources.
(The most likely reason for this is that some of
the resources that are included in the UMLS were
used to compile WordNet.) Another, more serious,
problem is related to the annotation scheme used
in the NLM-WSD corpus. If none of the possi-
ble senses in UMLS were judged to be appropri-
ate the annotators could label the sense as ?None?.
We did not map these senses since it would require
examining each instance to determine the most ap-
propriate sense or senses in WordNet and we ex-
pected this to be error prone. In addition, there is
no guarantee that all of the instances of a particular
term labeled with ?None? refer to the same mean-
ing. All of the ?None? senses were removed from
the NLM-WSD data set and any terms where there
were more than ten instances marked as ?None?
were also rejected from the similarity analysis.
This allowed us to compute the similarity score
for just 20 examples (40% of the total) although
we felt that this was a large enough sample to pro-
vide insight into the data set.
The WordNet::Similarity package (Ped-
ersen et al, 2004) was used to compute similar-
ity scores. Results are reported for three of the
measures in this package. (Other measures pro-
duced similar results.) The simple path measure
computes the similarity between a pair of nodes in
WordNet as the reciprocal of the number of edges
in the shortest path between them, the LCh mea-
sure (Leacock et al, 1998) also uses information
about the length of the shortest path between a pair
of nodes and combines this with information about
the maximum depth in WordNet and the JCn mea-
sure (Jaing and Conrath, 1997) makes use of in-
formation theory to assign probabilities to each of
the nodes in the WordNet hierarchy and computes
similarity based on these scores.
Table 3 shows the values of equation 1 for
the three similarity measures with scores averaged
across terms. These results indicate that for all
measures the Senseval-3 data set contains the most
ambiguity and NLM-WSD the least. This analysis
is consistent with the one carried out using mea-
sures based on sense distributions (Section 3.1)
355
MeasureCorpus
Path JCn LCh
NLM-WSD 0.074 0.032 1.027
SemEval 0.136 0.061 1.292
Senseval-3 0.159 0.063 1.500
Table 3: Semantic similarity for each data set us-
ing a variety of measures
and suggest that the senses in the NLM-WSD data
set are more clearly distinguished than the other
two.
4 Conclusion
This paper has explored a semi-supervised ap-
proach to the generation of labeled training data
for WSD that is based on relevance feedback
(Stevenson et al, 2008a). It was tested on three
data sets but was only found to generate examples
that were accurate enough to improve WSD per-
formance for one of these. The data set in which
a performance improvement was observed repre-
sented a limited domain (biomedicine) while the
other two were not restricted in this way. Measures
designed to quantify the level of ambiguity were
applied to these data sets including ones based on
the distribution of senses and another designed to
quantify similarities between senses. These mea-
sures provided evidence that the corpus for which
the relevance feedback approach was successful
contained less ambiguity than the other two and
this suggests that the relevance feedback approach
is most appropriate when the level of ambiguity is
low.
The experiments described in this paper high-
light the importance of the level of ambiguity on
the relevance feedback approach?s ability to gen-
erate useful labeled examples. Since it is semi-
supervised the ambiguity level can be checked us-
ing the measures used in this paper (Section 3)
and the performance of any automatically gener-
ated examples can be compared with the manu-
ally labeled ones (see Section 2.3) before deciding
whether or not they should be applied.
References
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209?226.
J. Daude?, L. Padro?, and G. Rigau. 2000. Mapping
wordnets using structural information. In Proceed-
ings of ACL ?00, pages 504?511, Hong Kong.
J. Jaing and D. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers and
the Humanities, 34(1-2):15?48.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(1):147?165.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in untagged
text. In Proceedings of ACL?04, pages 279?286,
Barcelona, Spain.
R. Mihalcea, T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
Proceedings of Senseval-3, pages 25?28, Barcelona,
Spain.
R. Passoneau, A. Salleb-Aouissi, and N. Ide. 2009.
Making sense of word sense variation. In Proceed-
ings of SEW-2009, pages 2?9, Boulder, Colorado.
T. Pedersen, S. Patwardhan, and Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proceedings of AAAI-04, pages 1024?
1025, San Jose, CA.
T. Pedersen, S. Pakhomov, S. Patwardhan, and
C. Chute. 2007. Measures of semantic similarity
and relateness in the biomedical domain. Journal of
Biomedical Informatics, 40(3):288?299.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007. SemEval-2007 Task-17: English Lexical
Sample, SRL and All Words. In Proceedings of
SemEval-2007, pages 87?92, Prague, Czech Repub-
lic.
J. Rocchio. 1971. Relevance feedback in Informa-
tion Retrieval. In G. Salton, editor, The SMART
Retrieval System ? Experiments in Automatic Doc-
ument Processing. Prentice Hall, Englewood Cliffs,
NJ.
M. Stevenson, Y. Guo, and R. Gaizauskas. 2008a.
Acquiring Sense Tagged Examples using Relevance
Feedback. In Proceedings of the Coling 2008, pages
809?816, Manchester, UK, August.
M. Stevenson, Y. Guo, R. Gaizauskas, and D. Martinez.
2008b. Disambiguation of biomedical text using di-
verse sources of information. BMC Bioinformatics,
9(Suppl 11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
oping a Test Collection for Biomedical Word Sense
Disambiguation. In Proceedings of AMIA Sympo-
sium, pages 746?50, Washington, DC.
356
