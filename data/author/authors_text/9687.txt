Proceedings of NAACL HLT 2009: Short Papers, pages 53?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Natural Language Understanding of Partial Speech Recognition
Results in Dialogue Systems
Kenji Sagae and Gwen Christian and David DeVault and David R. Traum
Institute for Creative Technologies, University of Southern California
13274 Fiji Way, Marina del Rey, CA 90292
{sagae,gchristian,devault,traum}@ict.usc.edu
Abstract
We investigate natural language understand-
ing of partial speech recognition results to
equip a dialogue system with incremental lan-
guage processing capabilities for more realis-
tic human-computer conversations. We show
that relatively high accuracy can be achieved
in understanding of spontaneous utterances
before utterances are completed.
1 Introduction
Most spoken dialogue systems wait until the user
stops speaking before trying to understand and re-
act to what the user is saying. In particular, in a
typical dialogue system pipeline, it is only once the
user?s spoken utterance is complete that the results
of automatic speech recognition (ASR) are sent on
to natural language understanding (NLU) and dia-
logue management, which then triggers generation
and synthesis of the next system prompt. While
this style of interaction is adequate for some appli-
cations, it enforces a rigid pacing that can be un-
natural and inefficient for mixed-initiative dialogue.
To achieve more flexible turn-taking with human
users, for whom turn-taking and feedback at the sub-
utterance level is natural and common, the system
needs to engage in incremental processing, in which
interpretation components are activated, and in some
cases decisions are made, before the user utterance
is complete.
There is a growing body of work on incremen-
tal processing in dialogue systems. Some of this
work has demonstrated overall improvements in sys-
tem responsiveness and user satisfaction; e.g. (Aist
et al, 2007; Skantze and Schlangen, 2009). Several
research groups, inspired by psycholinguistic mod-
els of human processing, have also been exploring
technical frameworks that allow diverse contextual
information to be brought to bear during incremen-
tal processing; e.g. (Kruijff et al, 2007; Aist et al,
2007).
While this work often assumes or suggests it is
possible for systems to understand partial user ut-
terances, this premise has generally not been given
detailed quantitative study. The contribution of this
paper is to demonstrate and explore quantitatively
the extent to which one specific dialogue system can
anticipate what an utterance means, on the basis of
partial ASR results, before the utterance is complete.
2 NLU for spontaneous spoken utterances
in a dialogue system
For this initial effort, we chose to look at incremental
processing of natural language understanding in the
SASO-EN system (Traum et al, 2008), a complex
spoken dialog system for which we have a corpus
of user data that includes recorded speech files that
have been transcribed and annotated with a semantic
representation. The domain of this system is a nego-
tiation scenario involving the location of a medical
clinic in a foreign country. The system is intended as
a negotiation training tool, where users learn about
negotiation tactics in the context of the culture and
social norms of a particular community.
2.1 The natural language understanding task
The NLU module must take the output of ASR as
input, and produce domain-specific semantic frames
as output. These frames are intended to capture
much of the meaning of the utterance, although a
53
dialogue manager further enriches the frame rep-
resentations with pragmatic information (Traum,
2003). NLU output frames are attribute-value ma-
trices, where the attributes and values are linked to a
domain-specific ontology and task model.
Complicating the NLU task of is the relatively
high word error rate (0.54) in ASR of user speech
input, given conversational speech in a complex do-
main and an untrained broad user population.
The following example, where the user attempts
to address complaints about lack of power in the pro-
posed location for the clinic, illustrates an utterance-
frame pair.
? Utterance (speech): we are prepared to give
you guys generators for electricity downtown
? ASR (NLU input): we up apparently give you
guys generators for a letter city don town
? Frame (NLU output):
<s>.mood declarative
<s>.sem.agent kirk
<s>.sem.event deliver
<s>.sem.modal.possibility can
<s>.sem.speechact.type offer
<s>.sem.theme power-generator
<s>.sem.type event
The original NLU component for this system was
described in (Leuski and Traum, 2008). For the pur-
poses of this experiment, we have developed a new
NLU module and tested on several different data
sets as described in the next section. Our approach
is to use maximum entropy models (Berger et al,
1996) to learn a suitable mapping from features de-
rived from the words in the ASR output to semantic
frames. Given a set of examples of semantic frames
with corresponding ASR output, a classifier should
learn, for example, that when ?generators? appears
in the output of ASR, the value power-generators is
likely to be present in the output frame. The specific
features used by the classifier are: each word in the
input string (bag-of-words representation of the in-
put), each bigram (consecutive words), each pair of
any two words in the input, and the number of words
in the input string.
0102030405060 1
23
45
67
89
1011
1213
1415
1617
1819
2021
2223
24
Length
 
n (word
s)
Number of utterances (bars)
050100150200250300350400450
Cumulative number of 
utterances (line)
Exact
ly n wo
rds
At mo
st n w
ords
Figure 1: Length of utterances in the development set.
2.2 Data
Our corpus consists of 4,500 user utterances spread
across a number of different dialogue sessions. Ut-
terances that were out-of-domain (13.7% of the cor-
pus) were assigned a ?garbage? frame, with no se-
mantic content. Approximately 10% of the utter-
ances were set aside for final testing, and another
10% was designated the development corpus for the
NLU module. The development and test sets were
chosen so that all the utterances in a session were
kept in the same set, but sessions were chosen at ran-
dom for inclusion in the development and test sets.
The training set contains 136 distinct frames,
each of which is composed of several attribute-value
pairs, called frame elements. Figure 1 shows the ut-
terance length distribution in the development set.
2.3 NLU results on complete ASR output
To evaluate NLU results, we look at precision, re-
call and f-score of frame elements. When the NLU
module is trained on complete ASR utterances in
the training set, and tested on complete ASR utter-
ances in the development set, f-score of frame ele-
ments is 0.76, with precision at 0.78 and recall at
0.74. To gain insight on what the upperbound on
the accuracy of the NLU module might be, we also
trained the classifier using features extracted from
gold-standard manual transcription (instead of ASR
output), and tested the accuracy of analyses of gold-
standard transcriptions (which would not be avail-
able at run-time in the dialogue system). Under
these ideal conditions, NLU f-score is 0.87. Training
on gold-standard transcriptions and testing on ASR
output produces results with a lower f-score, 0.74.
54
3 NLU on partial ASR results
Roughly half of the utterances in our training data
contain six words or more, and the average utter-
ance length is 5.9 words. Since the ASR module is
capable of sending partial results to the NLU mod-
ule even before the user has finished an utterance, in
principle the dialogue system can start understand-
ing and even responding to user input as soon as
enough words have been uttered to give the system
some indication of what the user means, or even
what the user will have said once the utterance is
completed. To measure the extent to which our NLU
module can predict the frame for an input utterance
when it sees only a partial ASR result with the first
n words, we examine two aspects of NLU with par-
tial ASR results. The first is correctness of the NLU
output with partial ASR results of varying lengths, if
we take the gold-standard manual annotation for the
entire utterance as the correct frame for any of the
partial ASR results for that utterance. The second is
stability: how similar the NLU output with partial
ASR results of varying lengths is to what the NLU
result would have been for the entire utterance.
3.1 Training the NLU module for analysis of
partial ASR results
The simplest way to performNLU of partial ASR re-
sults is simply to process the partial utterances using
the NLU module trained on complete ASR output.
However, better results may be obtained by train-
ing separate NLU models for analysis of partial ut-
terances of different lengths. To train these sepa-
rate NLU models, we first ran the audio of the utter-
ances in the training data through our ASR module,
recording all partial results for each utterance. Then,
to train a model to analyze partial utterances con-
taining n words, we used only partial utterances in
the training set containing n words (unless the entire
utterance contained less than n words, in which case
we simply used the complete utterance). In some
cases, multiple partial ASR results for a single utter-
ance contained the same number of words, and we
used the last partial result with the appropriate num-
ber of words 1. We trained separate NLU models for
1At run-time, this can be closely approximated by taking
the partial utterance immediately preceding the first partial ut-
terance of length n+ 1.
01020304050607080
1
2
3
4
5
6
7
8
9
10
all
Length
 
n (word
s)
F-score
Traine
d on a
ll data
Traine
d on p
artials
 up to
length
 
n
Traine
d on p
artials
 up to
length
 
n + c
ontex
t
Figure 2: Correctness for three NLU models on partial
ASR results up to n words.
n varying from one to ten.
3.2 Results
Figure 2 shows the f-score for frames obtained by
processing partial ASR results up to length n using
three NLU models. The dashed line is our baseline
NLU model, trained on complete utterances only
(model 1). The solid line shows the results obtained
with length-specific NLU models (model 2), and the
dotted line shows results for length-specific models
that also use features that capture dialogue context
(model 3). Models 1 and 2 are described in the previ-
ous sections. The additional features used in model
3 are unigram and bigram word features extracted
from the most recent system utterance.
As seen in Figure 2, there is a clear benefit to
training NLU models specifically tailored for partial
ASR results. Training a model on partial utterances
with four or five words allows for relatively high f-
score of frame elements (0.67 and 0.71, respectively,
compared to 0.58 and 0.66 when the same partial
ASR results are analyzed using model 1). Consider-
ing that half of the utterances are expected to have
more than five words (based on the length of the ut-
terances in the training set), allowing the system to
start processing user input when four or five-word
partial ASR results are available provides interesting
opportunities. Targeting partial results with seven
words or more is less productive, since the time sav-
ings are reduced, and the gain in accuracy is modest.
The context features used in model 3 did not pro-
vide substantial benefits in NLU accuracy. It is pos-
55
0102030405060708090100
1
2
3
4
5
6
7
8
9
10
Length
 
n of pa
rtial A
SR ou
tput us
ed in m
odel 2
Stability F-score
Figure 3: Stability of NLU results for partial ASR results
up to length n.
sible that other ways of representing context or di-
alogue state may be more effective. This is an area
we are currently investigating.
Finally, figure 3 shows the stability of NLU re-
sults produced by model 2 for partial ASR utter-
ances of varying lengths. This is intended to be an
indication of how much the frame assigned to a par-
tial utterance differs from the ultimate NLU output
for the entire utterance. This ultimate NLU output
is the frame assigned by model 1 for the complete
utterance. Stability is then measured as the F-score
between the output of model 2 for a particular partial
utterance, and the output of model 1 for the corre-
sponding complete utterance. A stability F-score of
1.0 would mean that the frame produced for the par-
tial utterance is identical to the frame produced for
the entire utterance. Lower values indicate that the
frame assigned to a partial utterance is revised sig-
nificantly when the entire input is available. As ex-
pected, the frames produced by model 2 for partial
utterances with at least eight words match closely
the frames produced by model 1 for the complete ut-
terances. Although the frames for partial utterances
of length six are almost as accurate as the frames for
the complete utterances (figure 2), figure 3 indicates
that these frames are still often revised once the en-
tire input utterance is available.
4 Conclusion
We have presented experiments that show that it
is possible to obtain domain-specific semantic rep-
resentations of spontaneous speech utterances with
reasonable accuracy before automatic speech recog-
nition of the utterances is completed. This allows for
interesting opportunities in dialogue systems, such
as agents that can interrupt the user, or even finish
the user?s sentence. Having an estimate of the cor-
rectness and stability of NLU results obtained with
partial utterances allows the dialogue system to es-
timate how likely its initial interpretation of an user
utterance is to be correct, or at least agree with its
ultimate interpretation. We are currently working on
the extensions to the NLU model that will allow for
the use of different types of context features, and in-
vestigating interesting ways in which agents can take
advantage of early interpretations.
Acknowledgments
The work described here has been sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM). Statements and opin-
ions expressed do not necessarily reflect the position
or the policy of the United States Government, and
no official endorsement should be inferred.
References
G. Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,
M. Swift, and M. K. Tanenhaus. 2007. Incremental
dialogue system faster than and preferred to its non-
incremental counterpart. In Proc. of the 29th Annual
Conference of the Cognitive Science Society.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
G. J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and
N. Hawes. 2007. Incremental, multi-level processing
for comprehending situated dialogue in human-robot
interaction. In Language and Robots: Proc. from the
Symposium (LangRo?2007). University of Aveiro, 12.
A. Leuski and D. Traum. 2008. A statistical approach
for text processing in virtual humans. In 26th Army
Science Conference.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proc. of the
12th Conference of the European Chapter of the ACL.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of Intelli-
gent Virtual Agents Conference IVA-2008.
D. Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In Proc. of the
International Workshop on Computational Semantics,
pages 380?394, January.
56
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 80?86,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Zero to Spoken Dialogue System in One Quarter: Teaching Computational
Linguistics to Linguists Using Regulus
Beth Ann Hockey
Department of Linguistics,
UARC
UC Santa Cruz
Mail-Stop 19-26, NASA Ames
Moffett Field, CA 94035-1000
bahockey@ucsc.edu
Gwen Christian
Department of Linguistics
UCSC
Santa Cruz, CA 95064, USA
jchristi@ucsc.edu
Abstract
This paper describes a Computational Lin-
guistics course designed for Linguistics stu-
dents. The course is structured around the ar-
chitecture of a Spoken Dialogue System and
makes extensive use of the dialogue system
tools and examples available in the Regu-
lus Open Source Project. Although only a
quarter long course, students learn Computa-
tional Linguistics and programming sufficient
to build their own Spoken Dialogue System as
a course project.
1 Introduction
Spoken Dialogue Systems model end-to-end ex-
ecution of conversation and consequently require
knowledge of many areas of computational linguis-
tics, speech technology and linguistics. The struc-
ture of Spoken Dialogue Systems offers a ready-
made structure for teaching a computational linguis-
tics course. One can work through the components
and cover a broad range of material in a grounded
and motivating way. The course described in this
paper was designed for linguistics students, upper-
division undergraduate and graduate, many having
limited experience with programming or computer
science. By the end of a quarter long course, stu-
dents were able to build a working spoken dialogue
systems and had a good introductory level under-
standing of the related computational linguistics top-
ics.
When this course was first being contemplated,
it became apparent that there were a number of
somewhat unusual properties that it should have,
and a number of useful goals for it to accomplish.
The Linguistics Department in which this course is
given had only sporadic offerings of computational
courses, due in part to having no faculty with a pri-
mary focus in Computational Linguistics. linguis-
tics students are very interested in having courses in
this area, but even in the University as a whole avail-
ability is limited. A course on information extraction
is offered in the Engineering School and while some
linguistics students are equipped to take that course,
many do not have sufficient computer science back-
ground or programming experience to make that a
viable option.
This course, in the Linguistics department,
needed to be for linguistics students, who might not
have well-developed computer skills. It needed to
fit into a single quarter, be self-contained, depend
only on linguistics courses as prerequisites, and give
students at least an overview of a number of areas
of CL. These students are also interested in con-
nections with industry; now that there are industry
jobs available for linguists, students are eager for in-
ternships and jobs where they can apply the skills
learned in their linguistics courses. Given this, it
was also important that the students learn to program
during the course, both to make engineering courses
more accessible, and to attract potential employers.
In addition, since the department was interested
in finding ways to expand computational linguistics
offerings, it clearly would be good if the course ap-
pealed to the students, the department?s faculty and
to higher levels of the University administration.
80
2 Class Demographics
Students in the course are a mix of graduates and
upper-division undergraduates with a solid back-
ground in syntax and semantics but are not expected
to have much in the way of programming experi-
ence. Familiarity with Windows, Unix and some
minimal experience with shell scripting are recom-
mended but not required. Students have been very
successful in the course starting with no program-
ming experience at all. Because the Linguistics de-
partment is especially strong in formal linguistics,
and the courses typically require extensive problem
sets, linguistics students have good aptitude for and
experience working with formal systems and this ap-
titude and skill set seems to transfer quite readily to
programming.
3 Regulus Open Source Platform
The Regulus Open Source Platform is a major re-
source for the course. Regulus is designed for
corpus-based derivation of efficient domain-specific
speech recognisers from general linguistically-
motivated unification grammars. The process of
creating an application-specific Regulus recogniser
starts with a general unification grammar (UG), to-
gether with a supplementary lexicon containing ex-
tra domain-specific vocabulary. An application-
specific UG is then automatically derived using Ex-
planation Based Learning (EBL) specialisation tech-
niques (van Harmelen and Bundy, 1988). This
corpus-based EBL method is parameterised by 1) a
small domain-specific training corpus, from which
the system learns the vocabulary and types of
phrases that should be kept in the specialised gram-
mar, and 2) a set of ?operationality criteria?, which
control the specialised grammar?s generality. The
application-specific UG is then compiled into a
Nuance-compatible CFG. Processing up to this point
is all carried out using Open Source Regulus tools.
Two Nuance utilities then transform the output CFG
into a recogniser. One of these uses the training cor-
pus a second time to convert the CFG into a PCFG;
the second performs the PCFG-to-recogniser com-
pilation step. This platform has been used the base
for an number of applications including The Clarissa
Procedure Browser (Clarissa, 2006) and MedSLT
(Bouillon et al, 2005)
The Regulus website (Regulus, 2008) makes
available a number of resources, including compil-
ers, an integrated development environment, a Reg-
ulus resource grammar for English, online docu-
mentation and a set of example dialogue and trans-
lation systems. These examples range from com-
pletely basic to quite complex. This material is all
described in detail in the Regulus book (Rayner et
al., 2006), which documents the system and pro-
vides a tutorial. As noted in reviews of the book,
(Roark, 2007) (Bos, 2008) it is very detailed. To
quote Roark, ?the tutorial format is terrifically ex-
plicit which will make this volume appropriate for
undergraduate courses looking to provide students
with hands-on exercises in building spoken dialog
systems.? Not only does the Regulus-based dia-
logue architecture supply an organizing principle for
the course but a large proportion of the homework
comes from the exercises in the book. The exam-
ples serve as starting points for the students projects,
give good illustrations of the various dialogue com-
ponents and are nice clean programming examples.
The more research-oriented material in the Regulus
book also provides opportunities for discussion of
topics such as unification, feature grammars, ellip-
sis processing, dialogue-state update, Chomsky hi-
erarchy and compilers. Reviewers of the book have
noted a potential problem: although Regulus itself
is open source it is currently dependent on two com-
mercial pieces of software, SICStus Prolog and Nu-
ance speech recognition platform (8.5). Nuance 8.5
is a speech recognition developer platform that is
widely used for build telephone call centers. This
developer kit supplies the acoustic models which
model the sounds of the language, the user supplies
a language model which defines the range of lan-
guage that will be recognized for a particular appli-
cation. This dependance on these commercial prod-
ucts has turned out not to be a serious problem for
us since we were able to get a research license from
Nuance and purchase a site license for SICStus Pro-
log. However, beyond the fact that we were able to
get licenses, we are not convinced that eliminating
the commercial software would be an educational
win. While, for example, SWI Prolog might work
as well in the course the commercial SISCtus Pro-
log given a suitable port of Regulus, we think that
having the students work with a widely used com-
81
mercial speech recognition product such as Nuance,
is beneficial training for students looking for jobs
or internships. Using Nuance also avoids frustration
because its performance is dramatically better than
the free alternatives.
4 Other Materials
The course uses a variety of materials in addition to
the Regulus platform and book. For historical and
current views of research in dialogue and speech,
course sessions typically begin with an example
project or system, usually with a video or a runnable
version. Examples of system web materials
that we use include: (Resurrected)SHRDLU
(http://www.semaphorecorp.com/
misc/shrdlu.html), TRIPS and TRAINS
(http://www.cs.rochester.edu/
research/cisd/projects/trips/
movies/TRIPS\ Overview/), Galaxy
(http://groups.csail.mit.edu/sls/
/applications/jupiter.shtml), Vo-
calJoyStick (http://ssli.ee.washington.
edu/vj/), and ProjectListen (http://
www.cs.cmu.edu/?listen/mm.html)and
NASA?s Clarissa Procedure Browser (http://
ti.arc.nasa.gov/projects/clarissa/
gallery.php?ta\=\&gid\=\&pid\=).
Jurasfsky and Martin (Jurafsky and Martin, 2000)
is used as an additional text and various research pa-
pers are given as reading in addition to the Regulus
material. Jurafsky and Martin is also good source
for exercises. The Jurafsky and Martin material and
the Regulus material are fairly complementary and
fit together well in the context of this type of course.
Various other exercises are used, including two stu-
dent favorites: a classic ?construct your own ELIZA?
task, and a exercise in reverse engineering a tele-
phone call center, which is an original created for
this course.
5 Programming languages
Prolog is used as the primary language in the course
for several reasons. First, Prolog was built for pro-
cessing language and consequently has a natural fit
to language processing tasks. Second, as a high-
level language, Prolog allows students to stay on a
fairly conceptual level and does not require them to
spend time learning how to handle low-level tasks.
Prolog is good for rapid prototyping; a small amount
of Prolog code can do a lot of work and in a one
quarter class this is an important advantage. Also,
Prolog is very close to predicate logic, which the lin-
guistics students already know from their semantics
classes. When the students look at Prolog and see
something familiar, it builds confidence and helps
make the task of learning to program seem less
daunting. The declarative nature of Prolog, which
often frustrates computer science students who were
well trained in procedural programming, feels natu-
ral for the linguists. And finally, the Regulus Open
Source System is written mainly in Prolog, so using
Prolog for the course makes the Regulus examples
maximally accessible.
Note that Regulus does support development of
Java dialogue processing components, and provides
Java examples. However, the Java based examples
are two to three times longer, more complicated and
less transparent than their Prolog counterparts, for
the same functionality. We believe that the Java
based materials would be very good for a more ad-
vanced course on multimodal applications, where
the advantages of Java would be evident, but in a
beginning course for linguists, we find Prolog peda-
gogically superior.
A potential downside to using Prolog is that it
is not a particularly mainstream programming lan-
guage. If the course was solely about technical train-
ing for immediate employment, Java or C++ would
probably be better. However, because most students
enter the course with limited programming expe-
rience, the most important programming outcomes
for the course are that they end up with evidence
that they can complete a non-trivial programming
project, that they gain the experience of debugging
and structuring code and that they end up better
able to learn additional computer science subsequent
to the course. The alternative these students have
for learning programming is to take traditional pro-
gramming courses, starting with an extremely basic
introduction to computers course and taking 1-2 ad-
ditional quarter long courses to reach the level of
programming sophistication that they reach in one
quarter in this course. In addition, taking the alterna-
tive route, they would learn no Computational Lin-
guistics, and would likely find those courses much
82
less engaging.
6 Course Content
Figure 6, depicts relationships between the dialogue
system components and related topics both in Lin-
guistics and in Computational Linguistics and/or
Computer Science. The course follows the flow of
the Dialogue System processing through the various
components, discussing topics related to each com-
ponent. The first two weeks of the course are used
as an overview. Spoken Dialogue Systems are put
in the context of Computational Linguistics, Speech
Technology, NLP and current commercial and re-
search state of the art. General CL tools and tech-
niques are introduced and a quick tour is made of the
various dialogue system components. In addition to
giving the students background about the field, we
want them to be functioning at a basic level with the
software at the end of two weeks so that they can be-
gin work on their projects. Following the two week
introduction, about two weeks are devoted to each
component.
The speech recognition discussion is focused
mainly on language modeling. This is an area of par-
ticular strength for Regulus and the grammar-based
modeling is an easy place for linguists to start. Cov-
ering the details of speech recognition algorithms in
addition to the other material being covered would
be too much for a ten week course. In addition, the
department has recently added a course on speech
recognition and text-to-speech, so this is an obvi-
ous thing to omit from this course. With the Nu-
ance speech recognition platform, there is plenty for
the students to learn as users rather than as speech
recognition implementers. In practice, it is not un-
usual for a Spoken Dialogue System implementer to
use a speech recognition platform rather than build-
ing their own, so the students are getting a realistic
experience.
For the Input Management, Regulus has imple-
mented several types of semantic representations,
from a simple linear list representation that can be
used with the Alterf robust semantics tool, to one
that handles complex embedding. So the Input Man-
ager related component can explore the trade offs in
processing and representation, using Regulus exam-
ples.
The Dialogue Managment section looks at simple
finite state dialogue management as well as the dia-
logue state update approach that has typically been
used in Regulus based applications. Many other top-
ics are possible depending on the available time.
The Output Management unit looks at various as-
pects of generation, timing of actions and could also
discuss paraphrase generation or prosodic mark up.
Other topics of a system wide nature such as N-
best processing or help systems can be discussed at
the end of the course if time allows.
7 Improvements for ?08
The course is currently being taught for Spring quar-
ter and a number of changes have been implemented
to address what we felt were the weak points of the
course as previously taught. It was generally agreed
that the first version of the course was quite success-
ful and had many of the desired properties. Students
learned Computational Linguistics and they learned
how to program. The demo session of the students?
projects held at the end of the course was attended by
much of the linguistics department, plus a few out-
side visitors. Attendees were impressed with how
much the students had accomplished. In building on
that success, we wanted to improve the following ar-
eas: enrollment, limiting distractions from the spo-
ken dialogue material, building software engineer-
ing skills, making connections with industry and/or
research, and visibility.
The first time the course was given, enrollment
was six students. This level of enrollment was no
doubt in part related to the fact that the course was
announced relatively late and students did not have
enough lead time to work it into their schedules. The
small size was intimate, but it seemed as though it
would be better for more students to be able to ben-
efit from the course. For the current course, students
knew a year and a half in advance that it would be
offered. We also had an information session about
the course as part of an internship workshop, and
apparently word of mouth was good. With addition
of a course assistant the maximum we felt we could
handle without compromising the hands-on experi-
ence was twenty. Demand greatly exceeded supply
and we ended up with twenty two students initially
enrolled. As of the deadline for dropping without
83
Figure 1: Course Schematic: Architecture of Dialogue System with associated linguistic areas/topic at above and
Computational Linguistics and/or Computer Science topics below
penalty course enrollment is 16. The course is cur-
rently scheduled to be taught every other year but we
are considering offering it in summer school in the
non-scheduled years.
Two activities in the first incarnation of the course
were time-consuming without contributing directly
to learning about CL and Dialogue Systems. First,
students spent considerable time getting the soft-
ware, particularly the Nuance speech recognition
software and the Nuance text-to-speech, installed on
their personal machines. The variability across their
machines and fact we did not at that time have a
good way to run the software on Machintoshes con-
tributed to the problem. This made the speech as-
pects seem more daunting than they should have,
and delayed some of the topics and exercises.
For the current course, we arranged to have all
of the software up and running for them on day
one, in an instructional lab on campus. Mandatory
lab sessions were scheduled for the course in the
instructional lab, starting on the first day of class,
so that we could make sure that students were able
to run the software from the very beginning of the
course. These arrangements did not work out quite
as smoothly as we had hoped, but was still an im-
provement over the first time the course was taught.
Rather than being completely dependent on stu-
dents? personal machines, the labs, combined with
a strategy we worked out for running the software
from external USB drives, provide students with a
way to do their assignments even if they have un-
suitable personal machines. In the labs, students are
able to see how the software should behave if prop-
erly installed, and this is very helpful to them when
installing on their personal machines. We refined the
installation instructions considerably, which seemed
to improve installation speed. The Macintosh prob-
lem has been solved, at least for Intel Macs, since we
84
have been successful in running the software with
BootCamp. The twice weekly lab sessions also give
students a chance do installation and practical lab
exercises in an environment in which the course as-
sistant is able see what they are doing, and give them
assistance. Observing and getting help from more
computationally savy classmates is also common in
the labs. Athough the measures taken to reduce the
software installation burden still leave some room
for improvement, students were able to use Regulus
and Nuance successfully, on average, in less than
half the time required the first time the course was
taught.
The other distracting activity was building the
backend for the course projects. Spoken Dialogue
Systems are usually an interface technology, but the
students in the first offering of the course had to
build their projects end to end. While this was not
a complete loss, since they did get useful program-
ming experience, it seemed as though it would be
an improvement if students could focus more on the
spoken dialogue aspects. The approach for doing
this in the current course is to recruit Project Part-
ners from industry, government, academic research
projects and other university courses. Our students
build the spoken dialogue system components and
then work with their Project Partner to connect to
the Project Partner?s system. The students will then
demonstrate the project as a whole, that is, their di-
alogue system working with the Project Partner?s
material/system, at the course end demo sessions.
We have project partners working in areas such as:
robotics, telephone based services, automotive in-
dustry, and virtual environments. There are a num-
ber of potential benefits to this approach. Students
are able to spend most of their time on the spoken
dialogue system and yet have something interest-
ing to connect to. In fact, they have access to sys-
tems that are real research projects, and real com-
mercial products that are beyond what our students
would be capable of producing on their own. Stu-
dents gain the experience of doing a fairly realistic
software collaboration, in which they are the spo-
ken dialogue experts. Project partners are enthu-
siastic because they get to try projects they might
not have time or resources to do. Industry partners
get to check out potential interns and research part-
ners may find potential collaborators. In the previ-
ous version of the course, half of the students who
finished the course subsequently worked on Spoken
Dialogue oriented research projects connected with
the department. One of the students had a successful
summer internship with Ford Motors as a result of
having taken the course. The research and industry
connection was already there, but the Project Partner
program strengthens it and expands the opportuni-
ties beyond projects connected with the department.
One enhancement to students? software engineer-
ing skills in the current version of the course is that
students are using version control from day one.
Each student in the course is being provided with
a Subversion repository with a Track ticket system
hosted by Freepository.com. Part of the incentive
for doing this was to protect Project Partners? IP, so
that materials provided by (particularly commercial)
Project Partners would not be housed at the Univer-
sity, and would only be accessible to relevant stu-
dent(s), the Project Partner, the instructor and the
course assistant. The repositories also support re-
mote collaboration making a wider range of orga-
nizations workable as project partners. With the
repositories the students gain experience with ver-
sion control and bug-tracking. Having the version
control and ticket system should also make the de-
velopment of their projects easier. Another way we
are hoping to enhance the students software skills is
through simply having more assistance available for
students in this area. We have added the previously
mentioned lab sections in the instructional labs, we
have arranged for the course assistant to have sub-
stantial time available for tutoring, and we are post-
ing tutorials as needed on the course website.
The final area of improvement that we wanted to
address is visibility. This is a matter of some prac-
tical importance for the course, the addition of CL
to the department?s offerings, and the students. Vis-
ibility among students has improved with word of
mouth and with the strategically timed information
session held the quarter prior to holding the course.
The course end demo session in the first offering of
the course did a good job of bringing it to the at-
tention of the students and faculty in the Linguis-
tics Department. For the current course, the Project
Partner program provides considerable visibility for
students, the department, and the University, among
industry, government and other Universities. We are
85
also expanding the demo session at the end of the
course. This time the demo session will be held as a
University wide event, and will be held at the main
UC Santa Cruz campus and a second time at the Uni-
versity?s satellite Silicon Valley Center, in order to
tap into different potential audiences. The session
at the Silicon Valley Center has potential for giving
students good exposure to potential employers, and
both sessions have good potential for highlighting
the Linguistics department.
8 Summary and Conclusion
The course presented in this paper has three key fea-
tures. First it is designed for linguistics students.
This means having linguistics and not computer sci-
ence as prerequisites and necessitates teaching stu-
dents programming and computer science when they
may start with little or no background. Second,
the course takes the architecture of a Spoken Dia-
logue System as the structure of the course, working
through the components and discussing CL topics
as they relate to the components. The third feature is
the extensive use of the Regulus Open Source plat-
form as key resource for the course. Regulus ma-
terial is used for exercises, as a base for construc-
tion of students? course projects, and for introducing
topics such as unification, feature grammars, Chom-
sky hierarchy, and dialogue management. We have
found this combination excellent for teaching CL to
linguistics students. The grammar-based language
modeling in Regulus, the use of Prolog and relat-
ing linguistic topics as well as computational ones to
the various dialogue system components, gives lin-
guistics students familiar material to build on. The
medium vocabulary type of Spoken Dialogue sys-
tem supported by the Regulus platform, makes a
very motivating course project and students are able
to program by the end of the course.
We discuss a number of innovations we have in-
troduced in the latest version of the course, such as
the Project Partner program, use of instructional labs
and subversion repositories, and expanded course
demo session. Since we are teaching the course for
the second time during Spring Quarter, we will be
able to report on the outcome of these innovations at
the workshop.
Acknowledgments
We would like to thank Nuance, for giving us the
research licenses for Nuance 8.5 and Vocalizer that
helped make this course and this paper possible.
References
Johan Bos. 2008. A review of putting linguistics into
speech recognition. the regulus grammar compiler.
Natural Language Engineering, 14(1).
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
Clarissa, 2006. http://www.ic.arc.nasa.gov/projects/clarissa/.
As of 1 Jan 2006.
D. Jurafsky and J. H. Martin. 2000. Speech and
Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics and
Speech Recognition. Prentice Hall Inc, New Jersey.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2008. http://www.issco.unige.ch/projects/regulus/,
http://sourceforge.net/projects/regulus/. As of 1 Jan
2008.
Brian Roark. 2007. A review of putting linguistics into
speech recognition: The regulus grammar compiler.
Computational Linguistics, 33(2).
T. van Harmelen and A. Bundy. 1988. Explanation-
based generalization = partial evaluation (research
note). Artificial Intelligence, 36:401?412.
86
