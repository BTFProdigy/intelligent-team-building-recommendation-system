Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does
Michelle L Gregory
Brown University
mgregory@cog.brown.edu
Mark Johnson
Brown University
Mark Johnson@Brown.edu
Eugene Charniak
Brown University
ec@cs.brown.edu
Abstract
This paper investigates the usefulness of
sentence-internal prosodic cues in syntac-
tic parsing of transcribed speech. Intu-
itively, prosodic cues would seem to pro-
vide much the same information in speech
as punctuation does in text, so we tried to
incorporate them into our parser in much
the same way as punctuation is. We com-
pared the accuracy of a statistical parser
on the LDC Switchboard treebank corpus
of transcribed sentence-segmented speech
using various combinations of punctua-
tion and sentence-internal prosodic infor-
mation (duration, pausing, and f0 cues).
With no prosodic or punctuation informa-
tion the parser?s accuracy (as measured by
F-score) is 86.9%, and adding punctuation
increases its F-score to 88.2%. However,
all of the ways we have tried of adding
prosodic information decrease the parser?s
F-score to between 84.8% to 86.8%, de-
pending on exactly which prosodic infor-
mation is added. This suggests that for
sentence-internal prosodic information to
improve speech transcript parsing, either
different prosodic cues will have to used
or they will have be exploited in the parser
in a way different to that used currently.
1 Introduction
Acoustic cues, generally duration, pausing, and
f0, have been demonstrated to be useful for auto-
S
INTJ
UH
Oh
,
,
NP
PRP
I
VP
VBD
loved
NP
PRP
it
.
.
Figure 1: A treebank style tree in which punctuation
is coded with terminal and preterminal nodes.
matic segmentation of natural speech (Baron et al,
2002; Hirschberg and Nakatani, 1998; Neiman et
al., 1998). In fact, it is generally accepted that
prosodic information is a reliable tool in predict-
ing topic shifts and sentence boundaries (Shriberg
et al, 2000). Sentences are generally demarcated
by a major fall (or rise) in f0, lengthening of the
final syllable, and following pauses. However,
the usefulness of prosodic information in sentence-
internal parsing is less clear. While assumed not
to be a one-to-one mapping, there is evidence
that there is a strong correlation between prosodic
boundaries and sentence-internal syntactic bound-
aries (Altenberg, 1987; Croft, 1995). For exam-
ple, Schepman and Rodway (2000) have shown that
prosodic cues reliably predict ambiguous attach-
ment of relative clauses within coordination con-
structions. Jansen et al (2001) have demonstrated
that prosodic breaks and an increase in pitch range
can distinguish direct quotes from indirect quotes in
a corpus of natural speech.
This paper evaluates the accuracy of a statistical
parser whose input includes prosodic cues. The pur-
pose of this study to determine if prosodic cues im-
prove parsing accuracy in the same way that punc-
tuation does. Punctuation is represented in the vari-
ous Penn treebank corpora as independent word-like
tokens, with corresponding terminal and pretermi-
nal nodes, as shown in Figure 1 (Bies et al, 1995).
Even though this seems linguistically highly un-
natural (e.g., punctuation might indicate supraseg-
mental prosodic properties), statistical parsers gen-
erally perform significantly better when their train-
ing and test data contains punctuation represented
in this way than if the punctuation is stripped out
of the training and test data (Charniak, 2000; En-
gel et al, 2002; Johnson, 1998). On the Switch-
board treebank data set using the experimental setup
described below we obtained an F-score of 0.882
when using punctuation and 0.869 when punctua-
tion was stripped out, replicating previous experi-
ments demonstrating the importance of punctuation.
(F-score is a standard measure of parse accuracy, see
e.g., Manning and Schu?tze (1999) for details).
This paper investigates how prosodic cues, when
encoded in the parser?s input in a manner similar to
the way the Penn treebanks encode punctuation, af-
fect parser accuracy. Our starting point is the ob-
servation that the Penn treebank annotation of punc-
tuation does significantly improve parsing accuracy.
Coupled with the assumption that punctuation and
prosody are encoding similar information, this led
us to try to encode prosodic information in a man-
ner that was as similar as possible to the way that
punctuation is encoded in the Penn treebanks.
For example, commas in text and pauses in speech
seem to convey similar information. In fact, when
transcribing speech, commas are often used to de-
note a pause. Thus, given the correlation between
the two, and the fact that sentence-internal punctu-
ation tends to be commas, we expected that pause
duration, coded in a way similar to punctuation,
would improve parsing accuracy in the same way
that punctuation does.
While it may be the case that the encoding of
prosodic information used in the experiments be-
low is perhaps not optimal and the parser has not
been tuned to use this information, note that exactly
the same objections could be made to the way that
punctuation is encoded and used in modern statis-
tical parsers, and punctuation does in fact dramati-
cally improve parsing accuracy.
We focus in this paper on parsing accuracy in a
modern statistical parsing framework, but it is im-
portant to remember that prosodic cues might help
parsing in other ways as well, even if they do not im-
prove parsing accuracy. No?th et al (2000) point out
that prosodic cues reduce parsing time and increase
recognition accuracy when parsing speech lattices
with the hand-crafted Verbmobil grammar. Page 266
of Kompe (1997) discusses the effect that incorpo-
rating prosodic information has on parse quality in
the Verbmobil system using the TUG unification
grammar parser: out of the 54 parses affected by
the addition of prosodic information, 33 were judged
?better with prosody?, 14 were judged ?better with-
out prosody? and 7 were judged ?unclear?. Our
experiments below differ from the experiments of
No?th and Kompe in many ways. First, we used
speech transcripts rather than speech recognizer lat-
tices. Second, we used a general-purpose broad-
coverage statistical parser rather than a unification
grammar parser with a hand-constructed grammar.
2 Method
The data used for this study is the transcribed ver-
sion of the Switchboard Corpus as released by
the Linguistic Data Consortium. The Switchboard
Corpus is a corpus of telephone conversations be-
tween adult speakers of varying dialects. The cor-
pus was split into training and test data as de-
scribed in Charniak and Johnson (2001). The train-
ing data consisted of all files in sections 2 and 3 of
the Switchboard treebank. The testing corpus con-
sists of files sw4004.mrg to sw4153.mrg, while files
sw4519.mrg to sw4936.mrg were used as develop-
ment corpus.
2.1 Prosodic variables
Prosodic information for the corpus was ob-
tained from forced alignments provided by
Hamaker et al (2003) and Ferrer et al (2002).
Hamaker et al (2003) provided word alignments
between the LDC parsed corpus and new alignments
of the Switchboard Coprus. Most of the differences
between the two alignments were individual lexical
items. In cases of differences, we kept the lexical
item from the LDC version. Ferrer et al (2002)
provided very rich prosodic information including
duration, pausing, f0 information, and individual
speaker statistics for each word in the corpus. The
information obtained from this corpus was aligned
to the LDC corpus.
It is not known exactly which prosodic vari-
ables convey the information about syntactic bound-
aries that is most useful to a modern syntactic
parser, so we investigated many different com-
binations of these variables. We looked for
changes in pitch and duration that we expected
would correspond to syntactic boundaries. While
we tested many combinations of variables, they
were mainly based on the variables PAU DUR N,
NORM LAST RHYME DUR, FOK WRD DIFF MNMN N,
FOK LR MEAN KBASELN and SLOPE MEAN DIFF N in
the data provided by Ferrer et al (2002).
While Ferrer (2002) should be consulted for full
details, PAU DUR N is pause duration normalized by
the speaker?s mean sentence-internal pause dura-
tion, NORM LAST RHYME DUR is the duration of the
phone minus the mean phone duration normalized
by the standard deviation of the phone duration for
each phone in the rhyme, FOK WRD DIFF MNMN NG
is the log of the mean f0 of the current word,
divided by the log mean f0 of the following
word, normalized by the speakers mean range,
FOK LR MEAN KBASELN is the log of the mean f0
of the word normalized by speaker?s baseline, and
SLOPE MEAN DIFF N is the difference in the f0 slope
normalized by the speaker?s mean f0 slope.
These variables all range over continuous values.
Modern statistical parsing technology has been de-
veloped assuming that all of the input variables are
categorical, and currently our parser can only use
categorical inputs. Given the complexity of the dy-
namic programming algorithms used by the parser,
it would be a major research undertaking to develop
a statistical parser of the same quality as the one
used here that is capable of using both categorical
and continuous variables as input.
In the experiments below we binned the contin-
uous prosodic variables to produce the actual cate-
gorical values used in our experiments. Binning in-
volves a trade-off, as fewer bins involve a loss of
information, whereas a large number of bins splits
the data so finely that the statistical models used in
the parser fail to generalize. We binned by first con-
structing a histogram of each feature?s values, and
divided these values into bins in such a way that each
bin contained the same number of samples. In runs
in which a single feature is the sole prosodic feature
we divided that feature?s values into 10 bins, while
runs in which two or more prosodic features were
conjoined we divided each feature into 5 bins.
While not reported here, we experimented with a
wide variety of different binning strategies, includ-
ing using the bins proposed by Ferrer et al (2002).
In fact the number of bins used does not affect the
results markedly; we obtained virtually the same re-
sults with only two bins.
We generated and inserted ?pseudo-punctuation?
symbols based on these binned values that were in-
serted into the parse input as described below. In
general, a pseudo-punctuation symbol is the con-
junction of the binned values of all of the prosodic
features used in a particular run. When map-
ping from binned prosodic variables to pseudo-
punctuation symbols, some of the binned values
can be represented by the absence of a pseudo-
punctuation symbol.
Because we intend these pseudo-punctuation
symbols to be as similar as possible to normal punc-
tuation, we generated pseudo-punctuation symbols
only when the corresponding prosodic variable falls
outside of its typical values. The ranges are given
below, and were chosen so that they align with
bin boundaries and result in each type of pseudo-
punctuation symbol occuring on 40% of words.
Thus when a prosodic feature is used alone only 4 of
its 10 bins are represented by a pseudo-punctuation
symbol.
However, when two or more types of the prosodic
pseudo-punctuation symbols are used at once there
is a larger number of different pseudo-punctuation
symbols and a greater number of words appear-
ing with a following pseudo-punctuation symbol.
For example, when P, R and S prosodic annota-
tions are used together there are 89 distinct types
of prosodic pseudo-punctuation symbols in our cor-
pus, and 54% of words are followed by a prosodic
pseudo-punctuation symbol.
The experiments below make use of the following
types of pseudo-punctuation symbols, either alone
or concatenated in combination. See Figure 2 for
an example tree with pseudo-punctuation symbols
inserted.
Pb This is based on the bin b of the binned
PAU DUR N value, and is only generated when
the PAU DUR N value is greater than 0.285.
Rb This is based on the bin b of the binned
NORM LAST RHYME DUR value, and is only
generated that value is greater than -0.061.
Wb This is based on the bin b of the binned
FOK WRD DIFF MNMN N value, and is only gen-
erated when that value is less than -0.071 or
greater than 0.0814.
Lb This is based on the bin b of the
FOK LR MEAN KBASELN value, and is only
generated when that value is less than 0.157 or
greater than 0.391.
Sb This is based on the bin b of the
SLOPE MEAN DIFF N value, and is only
generated whenever that value is non-zero.
In addition, we also created a binary version of
the P feature in order to evaluate the effect of bina-
rization.
NP This is based on the PAU DUR N value, and is
only generated when that value is greater than
0.285.
We actually experimented with a much wider
range of binned variables, but they all produced re-
sults similar to those described below.
2.2 Parse corpus construction
We tried to incorporate the binned prosodic informa-
tion described in the previous subsection in a manner
that corresponds as closely as possible to the way
that punctuation is represented in this corpus, be-
cause previous experiments have shown that punc-
tuation improves parser performance (Charniak and
Johnson, 2001; Engel et al, 2002). We deleted dis-
fluency tags and EDITED subtrees from our training
and test corpora.
We investigated several combinations of prosodic
pseudo-punctuation symbols. For each of these we
generated a training and test corpus. The pseudo-
punctuation symbols are dominated by a new preter-
minal PROSODY to produce a well-formed tree.
These prosodic local trees are introduced into the
tree following the word they described, and are at-
tached as high as possible in the tree, just as punc-
tuation is in the Penn treebank. Figure 2 depicts
a typical tree that contains P R S prosodic pseudo-
punctuation symbols inserted following the word
they describe.
We experimented with several other ways of in-
corporating prosody into parse trees, none of which
greatly affected the results. For example, we also ex-
perimented with a ?raised? representation in which
the prosodic pseudo-punctuation symbol also serves
as the preterminal label. The corresponding ?raised?
version of the example tree is depicted in Figure 3.
The motivation for raising is as follows. The sta-
tistical parser used for this research generates the
siblings of a head in a sequential fashion, first pre-
dicting the category label of a sibling and later con-
ditioning on that label to predict the remaining sib-
lings. ?Raising? should permit the generative model
to condition not just on the presence of a prosodic
pseudo-punctuation symbol but also on its actual
identity. If some but not all of the prosodic pseudo-
punctuation symbols were especially indicative of
some aspect of phrase structure, then the ?raising?
structures should permit the parsing model to detect
this and condition on just those symbols. Note that
in the Penn treebank annotation scheme, different
types of punctuation are given different preterminal
categories, so punctuation is encoded in the treebank
using a ?raised? representation.
The resulting corpora contain both prosodic and
punctuation information. We prepared our actual
training and testing corpora by selectively remov-
ing subtrees from these corpora. By removing all
punctuation subtrees we obtain corpora that contain
prosodic information but no punctuation, by remov-
ing all prosodic information we obtain the original
treebank data, and by removing both prosodic and
punctuation subtrees we obtain corpora that contain
neither type of information.
2.3 Evaluation
We trained and evaluated the parser on the various
types of corpora described in the previous section.
S
INTJ
UH
Uh
PROSODY
*R4*
,
,
NP
PRP
I
PROSODY
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
PROSODY
*R3*S2*
NN
house
PROSODY
*S4*
,
,
Figure 2: A tree with P R S prosodic pseudo-punctuation symbols inserted following the words they corre-
spond to. (No P prosodic features occured in this utterance).
S
INTJ
UH
Uh
*R4*
*R4*
,
,
NP
PRP
I
*R4*
*R4*
VP
VBP
do
RB
nt
VP
VB
live
PP
IN
in
NP
DT
a
*R3*S2*
*R3*S2*
NN
house
*S4*
*S4*
,
,
Figure 3: The same sentence as in Figure 2, but with prosodic pseudo-punctuation raised to the preterminal
level.
Annotation unraised raised
punctuation 88.212
none 86.891
L 85.632 85.361
NP 86.633 86.633
P 86.754 86.594
R 86.407 86.288
S 86.424 85.75
W 86.031 85.681
P R 86.405 86.282
P W 86.175 85.713
P S 86.328 85.922
P R S 85.64 84.832
Table 1: The F-score of the parser?s output when
trained and tested on corpora with varying prosodic
pseudo-punctuation symbols. The entry ?punc-
tuation? gives the parser?s performance on input
with standard punctuation, while ?none? gives the
parser?s performance on input without any punctua-
tion or prosodic pseudo-punctuation whatsoever.
(We always tested on the type of corpora that corre-
sponded to the training data). We evaluated parser
performance using the methodology described in
Engel et al (2002), which is a simple adaptation of
the well-known PARSEVAL measures in which punc-
tuation and prosody preterminals are ignored. This
evaluation yields precision, recall and F-score values
for each type of training and test corpora.
3 Results
Table 1 presents the results of our experiments. The
RAISED prosody entry corresponds to the raised ver-
sion of the COMBINED corpora, as described above.
We replicated previous results and showed that
punctuation information does help parsing. How-
ever, none of the experiments with prosodic infor-
mation resulted in improved parsing performance;
indeed, adding prosodic information reduced perfor-
mance by 2 percentage points in some cases. This is
a very large amount by the standards of modern sta-
tistical parsers. Notice that the general trend is that
performance decreases as the amount and complex-
ity of the prosodic annotation increased.
4 Discussion and Conclusion
Simple statistical tests show that there is in fact
a significant correlation between the location of
opening and closing phrase boundaries and all of
the prosodic pseudo-punctuation symbols described
above, so there is no doubt that these do con-
vey information about syntactic structure. How-
ever, adding the prosodic pseudo-punctuation sym-
bols uniformly decreased parsing accuracy relative
to input with no prosodic information. There are a
number of reasons why this might be the case.
While we investigated a wide range of prosodic
features, it is possible that different prosodic features
might improve parsing performance, and it would be
interesting to see if improved prosodic feature ex-
traction would improve parsing accuracy.
We suspect that the decrease in accuracy is due
to the fact that the addition of prosodic pseudo-
punctuation symbols effectively excluded other
sources of information from the parser?s statisti-
cal models. For example, as mentioned earlier the
parser uses a mixture of n-gram models to predict
the sequence of categories on the right-hand side
of syntactic rules, backing off ultimately to a dis-
tribution that includes just the head and the preced-
ing sibling?s category. Consider the effect of insert-
ing a prosodic pseudo-punctuation symbol on such
a model. The prosodic pseudo-punctuation symbol
would replace the true preceding sibling?s category
in the model, thus possibly resulting in poorer over-
all performance (note however that the parser also
includes a higher-order backoff distribution in which
the next category is predicted using the preceding
two sibling?s categories, so the true sibling?s cate-
gory would still have some predictive value).
The basic point is that inserting additional in-
formation into the parse tree effectively splits the
conditioning contexts, exacerbating the sparse data
problems that are arguably the bane of all statisti-
cal parsers. Additional information only improves
parsing accuracy if the information it conveys is suf-
ficient to overcome the loss in accuracy incurred by
the increase in data sparseness. It seems that punctu-
ation carries sufficient information to overcome this
loss, but that the prosodic categories we introduced
do not.
It could be that our results reflect the fact that we
are parsing speech transcripts in which the words
(and hence their parts of speech) are very reliably
identified, whereas our prosodic features were auto-
matically extracted directly from the speech signal
and hence might be noisier. If the explanation pro-
posed above is correct, it is perhaps not surprising
that an accurate part of speech label would prove
more useful in a conditioning context used by the
parser than a noisy prosodic feature. Note that this
would not be the case when parsing from speech rec-
ognizer output (since word identity would itself be
uncertain), and it is possible that in such applications
prosodic information would be more useful.
Of course, there are many other ways prosodic in-
formation might be exploited in a parser, and one
of those may yield improved parser performance.
We chose to incorporate prosodic information into
our parser in a way that was similar to the way
that punctuation is annotated in the Penn treebanks
because we assumed that punctuation carries infor-
mation similar to prosody, and it had already been
demonstrated that punctuation annotated in the Penn
treebank fashion does systematically improve pars-
ing accuracy.
But the assumption that prosody conveys infor-
mation about syntactic structure in the same way
that punctuation does could be false. It could also be
that even though prosody encodes information about
syntactic structure, this information is encoded in
a manner that is too complicated for our parser to
utilize. For example, even though commas are of-
ten used to indicate pauses, pauses have many other
functions in fluent speech. Pauses of greater than
200 ms are often associated with planning problems,
which might be correlated with syntactic structure
in ways too complex for the parser to exploit. While
not reported here, we tried various techniques to iso-
late different functions of pauses, such as exclud-
ing pauses of greater than 200 ms. However, all of
these experiments produced results similar to those
reported here.
Finally, there is another possible reason why our
assumption that prosody and punctuation are similar
in their information content could be wrong. Our
prosodic information was automatically extracted
from the speech stream, while punctuation was pro-
duced by human annotators who presumably com-
prehended the utterances being annotated. Given
this, it is perhaps no surprise that our automatically
extracted prosodic annotations proved less useful
than human-produced punctuation.
References
Bengt Altenberg. 1987. Prosodic patterns in spoken En-
glish: studies in the correlation between prosody and
grammar. Lund University Press, Lund.
Don Baron, Elizabeth Shriberg, and Andreas Stolcke.
2002. Automatic punctuation and disfluency detec-
tion in multi-party meetings using prosodic and lex-
ical cues. In Proc. Intl. Conf. on Spoken Language
Processing, volume 2, pages 949?952, Denver.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre, 1995. Bracketting Guideliness for Treebank II
style Penn Treebank Project. Linguistic Data Consor-
tium.
Eugene Charniak and Mark Johnson. 2001. Edit detec-
tion and parsing for transcribed speech. In Proceed-
ings of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In The Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics, pages 132?139.
William Croft. 1995. Intonation units and grammatical
structure. Linguistics, 33:839?882.
Donald Engel, Eugene Charniak, and Mark Johnson.
2002. Parsing and disfluency placement. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 49?54.
Luciana Ferrer, Elizabeth Shriberg, and Andreas Stol-
cke. 2002. Is the speaker done yet? faster and more
accurate end-of-utterance detection using prosody in
human-computer dialog. In Proc. Intl. Conf. on Spo-
ken Language Processing, volume 3, pages 2061?
2064, Denver.
Luciana Ferrer. 2002. Prosodic features for the switch-
board database. Technical report, SRI International,
Menlo Park.
Jon Hamaker, Dan Harkins, and Joe Picone. 2003. Man-
ually corrected switchboard word alignments.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proc. Intl. Conf.
on Spoken Language Processing, volume 4, pages
1255?1258, Philadelphia.
Wouter Jansen, Michelle L. Gregory, and Jason M. Bre-
nier. 2001. Prosodic correlates of directly reported
speech: Evidence from conversational speech. In Pro-
ceedings of the ISCA Workshop on Prosody in Speech
Recognition and Understanding, pages 77?80, Red
Banks, NJ.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Ralf Kompe. 1997. Prosody in speech understanding
systems. Springer, Berlin.
Chris Manning and Hinrich Schu?tze. 1999. Foundations
of Statistical Natural Language Processing. The MIT
Press, Cambridge, Massachusetts.
Heinrich Neiman, Elmar Noth, Anton Batliner, Jan
Buckow, Florian Gallwitz, Richard Huber, and Volkar
Warnke. 1998. Using prosodic cues in spoken dialog
systems. In Proceedings of the International Work-
shop on Speech and Computer, pages 17?28, St. Pe-
tersburg.
Elmar No?th, Anton Batliner, Andreas Kie?ling, Ralf
Kompe, and Heinrich Niemann. 2000. Verbmobil:
The use of prosody in the linguistic components of a
speech understanding system. IEEE Transactions on
Speech and Auditory Processing, 8(5):519?532.
Astrid Schepman and Paul Rodway. 2000. Prosody
and on-line parsing in coordination structures. The
Quarterly Journal of Experimental Psychology: A,
53(2):377?396.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tur, and Gorkhan Tur. 2000. Prosody-based auto-
matic segmentation of speech into sentences and top-
ics. Speech Communication, 32(1-2):127?154.
Using Conditional Random Fields to Predict Pitch Accents in
Conversational Speech
Michelle L. Gregory
Linguistics Department
University at Buffalo
Buffalo, NY 14260
mgregory@buffalo.edu
Yasemin Altun
Department of Computer Science
Brown University
Providence, RI 02912
altun@cs.brown.edu
Abstract
The detection of prosodic characteristics is an im-
portant aspect of both speech synthesis and speech
recognition. Correct placement of pitch accents aids
in more natural sounding speech, while automatic
detection of accents can contribute to better word-
level recognition and better textual understanding.
In this paper we investigate probabilistic, contex-
tual, and phonological factors that influence pitch
accent placement in natural, conversational speech
in a sequence labeling setting. We introduce Con-
ditional Random Fields (CRFs) to pitch accent pre-
diction task in order to incorporate these factors ef-
ficiently in a sequence model. We demonstrate the
usefulness and the incremental effect of these fac-
tors in a sequence model by performing experiments
on hand labeled data from the Switchboard Corpus.
Our model outperforms the baseline and previous
models of pitch accent prediction on the Switch-
board Corpus.
1 Introduction
The suprasegmental features of speech relay critical
information in conversation. Yet, one of the ma-
jor roadblocks to natural sounding speech synthe-
sis has been the identification and implementation
of prosodic characteristics. The difficulty with this
task lies in the fact that prosodic cues are never ab-
solute; they are relative to individual speakers, gen-
der, dialect, discourse context, local context, phono-
logical environment, and many other factors. This is
especially true of pitch accent, the acoustic cues that
make one word more prominent than others in an
utterance. For example, a word with a fundamen-
tal frequency (f0) of 120 Hz would likely be quite
prominent in a male speaker, but not for a typical fe-
male speaker. Likewise, the accent on the utterance
?Jon?s leaving.? is critical in determining whether
it is the answer to the question ?Who is leaving??
(?JON?s leaving.?) or ?What is Jon doing?? (?Jon?s
LEAVING.?). Accurate pitch accent prediction lies
in the successful combination of as many of the con-
textual variables as possible. Syntactic information
such as part of speech has proven to be a success-
ful predictor of accentuation (Hirschberg, 1993; Pan
and Hirschberg, 2001). In general, function words
are not accented, while content words are. Vari-
ous measures of a word?s informativeness, such as
the information content (IC) of a word (Pan and
McKeown, 1999) and its collocational strength in a
given context (Pan and Hirschberg, 2001) have also
proven to be useful models of pitch accent. How-
ever, in open topic conversational speech, accent is
very unpredictable. Part of speech and the infor-
mativeness of a word do not capture all aspects of
accentuation, as we see in this example taken from
Switchboard, where a function word gets accented
(accented words are in uppercase):
I, I have STRONG OBJECTIONS to THAT.
Accent is also influenced by aspects of rhythm
and timing. The length of words, in both number
of phones and normalized duration, affect its likeli-
hood of being accented. Additionally, whether the
immediately surrounding words bear pitch accent
also affect the likelihood of accentuation. In other
words, a word that might typically be accented may
be unaccented because the surrounding words also
bear pitch accent. Phrase boundaries seem to play
a role in accentuation as well. The first word of in-
tonational phrases (IP) is less likely to be accented
while the last word of an IP tends be accented. In
short, accented words within the same IP are not in-
dependent of each other.
Previous work on pitch accent prediction, how-
ever, neglected the dependency between labels. Dif-
ferent machine learning techniques, such as deci-
sion trees (Hirschberg, 1993), rule induction sys-
tems (Pan and McKeown, 1999), bagging (Sun,
2002), boosting (Sun, 2002) have been used in a
scenario where the accent of each word is pre-
dicted independently. One exception to this line
of research is the use of Hidden Markov Models
(HMM) for pitch accent prediction (Pan and McK-
eown, 1999; Conkie et al, 1999). Pan and McKe-
own (1999) demonstrate the effectiveness of a se-
quence model over a rule induction system, RIP-
PER, that treats each label independently by show-
ing that HMMs outperform RIPPER when the same
variables are used.
Until recently, HMMs were the predominant for-
malism to model label sequences. However, they
have two major shortcomings. They are trained
non-discriminatively using maximum likelihood es-
timation to model the joint probability of the ob-
servation and label sequences. Also, they require
questionable independence assumptions to achieve
efficient inference and learning. Therefore, vari-
ables used in Hidden Markov models of pitch ac-
cent prediction have been very limited, e.g. part of
speech and frequency (Pan and McKeown, 1999).
Discriminative learning methods, such as Maximum
Entropy Markov Models (McCallum et al, 2000),
Projection Based Markov Models (Punyakanok and
Roth, 2000), Conditional Random Fields (Lafferty
et al, 2001), Sequence AdaBoost (Altun et al,
2003a), Sequence Perceptron (Collins, 2002), Hid-
den Markov Support Vector Machines (Altun et
al., 2003b) and Maximum-Margin Markov Net-
works (Taskar et al, 2004), overcome the limita-
tions of HMMs. Among these methods, CRFs is
the most common technique used in NLP and has
been successfully applied to Part-of-Speech Tag-
ging (Lafferty et al, 2001), Named-Entity Recog-
nition (Collins, 2002) and shallow parsing (Sha and
Pereira, 2003; McCallum, 2003).
The goal of this study is to better identify which
words in a string of text will bear pitch accent.
Our contribution is two-fold: employing new pre-
dictors and utilizing a discriminative model. We
combine the advantages of probabilistic, syntactic,
and phonological predictors with the advantages of
modeling pitch accent in a sequence labeling setting
using CRFs (Lafferty et al, 2001).
The rest of the paper is organized as follows: In
Section 2, we introduce CRFs. Then, we describe
our corpus and the variables in Section 3 and Sec-
tion 4. We present the experimental setup and report
results in Section 5. Finally, we discuss our results
(Section 6) and conclude (Section 7).
2 Conditional Random Fields
CRFs can be considered as a generalization of lo-
gistic regression to label sequences. They define
a conditional probability distribution of a label se-
quence y given an observation sequence x. In this
paper, x = (x1, x2, . . . , xn) denotes a sentence of
length n and y = (y1, y2, . . . , yn) denotes the la-
bel sequence corresponding to x. In pitch accent
prediction, xt is a word and yt is a binary label de-
noting whether xt is accented or not.
CRFs specify a linear discriminative function F
parameterized by ? over a feature representation of
the observation and label sequence ?(x,y). The
model is assumed to be stationary, thus the feature
representation can be partitioned with respect to po-
sitions t in the sequence and linearly combined with
respect to the importance of each feature ?k, de-
noted by ?k. Then the discriminative function can
be stated as in Equation 1:
F (x,y; ?) =
?
t
??,?t(x,y)? (1)
Then, the conditional probability is given by
p(y|x; ?) = 1Z(x,?)F (x,y; ?) (2)
where Z(x,?) = ?y? F (x, y?; ?) is a normaliza-
tion constant which is computed by summing over
all possible label sequences y? of the observation se-
quence x.
We extract two types of features from a sequence
pair:
1. Current label and information about the obser-
vation sequence, such as part-of-speech tag of
a word that is within a window centered at the
word currently labeled, e.g. Is the current word
pitch accented and the part-of-speech tag of
the previous word=Noun?
2. Current label and the neighbors of that label,
i.e. features that capture the inter-label depen-
dencies, e.g. Is the current word pitch accented
and the previous word not accented?
Since CRFs condition on the observation se-
quence, they can efficiently employ feature repre-
sentations that incorporate overlapping features, i.e.
multiple interacting features or long-range depen-
dencies of the observations, as opposed to HMMs
which generate observation sequences.
In this paper, we limit ourselves to 1-order
Markov model features to encode inter-label de-
pendencies. The information used to encode the
observation-label dependencies is explained in de-
tail in Section 4.
In CRFs, the objective function is the log-loss of
the model with ? parameters with respect to a train-
ing set D. This function is defined as the negative
sum of the conditional probabilities of each training
label sequence yi, given the observation sequence
xi, where D ? {(xi,yi) : i = 1, . . . ,m}. CRFs are
known to overfit, especially with noisy data if not
regularized. To overcome this problem, we penalize
the objective function by adding a Gaussian prior
(a term proportional to the squared norm ||?||2) as
suggested in (Johnson et al, 1999). Then the loss
function is given as:
L(?;D) = ?
m
?
i
log p(yi|xi; ?) +
1
2c||?||
2
= ?
m
?
i
F (xi,yi; ?) + logZ(xi,?)
+ 12c||?||
2 (3)
where c is a constant.
Lafferty et al (2001), proposed a modification
of improved iterative scaling for parameter estima-
tion in CRFs. However, gradient-based methods
have often found to be more efficient for minimizing
Equation 3 (Minka, 2001; Sha and Pereira, 2003).
In this paper, we use the conjugate gradient descent
method to optimize the above objective function.
The gradients are computed as in Equation 4:
??L =
m
?
i
?
t
Ep[?t(xi,y)] ? ?t(xi,yi)
+ c? (4)
where the expectation is with respect to all possi-
ble label sequences of the observation sequence xi
and can be computed using the forward backward
algorithm.
Given an observation sequence x, the best label
sequence is given by:
y? = arg max
y
F (x,y; ??) (5)
where ?? is the parameter vector that minimizes
L(?;D). The best label sequence can be identified
by performing the Viterbi algorithm.
3 Corpus
The data for this study were taken from the Switch-
board Corpus (Godfrey et al, 1992), which con-
sists of 2430 telephone conversations between adult
speakers (approximately 2.4 million words). Partic-
ipants were both male and female and represented
all major dialects of American English. We used a
portion of this corpus that was phonetically hand-
transcribed (Greenberg et al, 1996) and segmented
into speech boundaries at turn boundaries or pauses
of more than 500 ms on both sides. Fragments con-
tained seven words on average. Additionally, each
word was coded for probabilistic and contextual
information, such as word frequency, conditional
probabilities, the rate of speech, and the canonical
pronunciation (Fosler-Lussier and Morgan, 1999).
The dataset used in all analysis in this study con-
sists of only the first hour of the database, comprised
of 1,824 utterances with 13,190 words. These utter-
ances were hand coded for pitch accent and intona-
tional phrase brakes.
3.1 Pitch Accent Coding
The utterances were hand labeled for accents and
boundaries according to the Tilt Intonational Model
(Taylor, 2000). This model is characterized by a
series of intonational events: accents and bound-
aries. Labelers were instructed to use duration, am-
plitude, pausing information, and changes in f0 to
identify events. In general, labelers followed the ba-
sic conventions of EToBI for coding (Taylor, 2000).
However, the Tilt coding scheme was simplified.
Accents were coded as either major or minor (and
some rare level accents) and breaks were either ris-
ing or falling. Agreement for the Tilt coding was
reported at 86%. The CU coding also used a simpli-
fied EToBI coding scheme, with accent types con-
flated and only major breaks coded. Accent and
break coding pair-wise agreement was between 85-
95% between coders, with a kappa ? of 71%-74%
where ? is the difference between expected agree-
ment and actual agreement.
4 Variables
The label we were predicting was a binary distinc-
tion of accented or not. The variables we used for
prediction fall into three main categories: syntac-
tic, probabilistic variables, which include word fre-
quency and collocation measures, and phonological
variables, which capture aspects of rhythm and tim-
ing that affect accentuation.
4.1 Syntactic variables
The only syntactic category we used was a four-
way classification for hand-generated part of speech
(POS): Function, Noun, Verb, Other, where Other
includes all adjectives and adverbs1 . Table 1 gives
the percentage of accented and unaccented items by
POS.
1We also tested a categorization of 14 distinct part of speech
classes, but the results did not improve, so we only report on the
four-way classification.
Accented Unaccented
Function 21% 79%
Verb 59% 41%
Noun 30% 70%
Other 49% 51%
Table 1: Percentage of accented and unaccented
items by POS.
Variable Definition Example
Unigram log p(wi) and, I
Bigram log p(wi|wi?1) roughing it
Rev Bigram log p(wi|wi+1) rid of
Joint log p(wi?1, wi) and I
Rev Joint log p(wi, wi+1) and I
Table 2: Definition of probabilistic variables.
4.2 Probabilistic variables
Following a line of research that incorporates the
information content of a word as well as collo-
cation measures (Pan and McKeown, 1999; Pan
and Hirschberg, 2001) we have included a number
of probabilistic variables. The probabilistic vari-
ables we used were the unigram frequency, the pre-
dictability of a word given the preceding word (bi-
gram), the predictability of a word given the follow-
ing word (reverse bigram), the joint probability of a
word with the preceding (joint), and the joint prob-
ability of a word with the following word (reverse
joint). Table 2 provides the definition for these,
as well as high probability examples from the cor-
pus (the emphasized word being the current target).
Note all probabilistic variables were in log scale.
The values for these probabilities were obtained
using the entire 2.4 million words of SWBD2. Table
3 presents the Spearman?s rank correlation coeffi-
cient between the probabilistic measures and accent
(Conover, 1980). These values indicate the strong
correlation of accents to the probabilistic variables.
As the probability increases, the chance of an accent
decreases. Note that all values are significant at the
p < .001 level.
We also created a combined part of speech and
unigram frequency variable in order to have a vari-
able that corresponds to the variable used in (Pan
2Our current implementation of CRF only takes categorical
variables, thus for the experiments, all probabilistic variables
were binned into 5 equal categories. We also tried more bins
and produced similar results, so we only report on the 5-binned
categories. We computed correlations between pitch accent and
the original 5 variables as well as the binned variables and they
are very similar.
Variables Spearman?s ?
Unigram -.451
Bigram -.309
Reverse Bigram -.383
Joint -.207
Reverse joint -.265
Table 3: Spearman?s correlation values for the prob-
abilistic measures.
and McKeown, 1999).
4.3 Phonological variables
The last category of predictors, phonological vari-
ables, concern aspects of rhythm and timing of an
utterance. We have two main sources for these vari-
ables: those that can be computed solely from a
string of text (textual), and those that require some
sort of acoustic information (acoustic). Sun (2002)
demonstrated that the number of phones in a syl-
lable, the number of syllables in a word, and the
position of a word in a sentence are useful predic-
tors of which syllables get accented. While Sun was
concerned with predicting accented syllables, some
of the same variables apply to word level targets as
well. For our textual phonological features, we in-
cluded the number of syllables in a word and the
number of phones (both in citation form as well as
transcribed form). Instead of position in a sentence,
we used the position of the word in an utterance
since the fragments do not necessarily correspond
to sentences in the database we used. We also made
use of the utterance length. Below is the list of our
textual features:
? Number of canonical syllables
? Number of canonical phones
? Number of transcribed phones
? The length of the utterance in number of words
? The position of the word in the utterance
The main purpose of this study is to better pre-
dict which words in a string of text receive accent.
So far, all of our predictors are ones easily com-
puted from a string of text. However, we have in-
cluded a few variables that affect the likelihood of
a word being accented that require some acoustic
data. To the best of our knowledge, these features
have not been used in acoustic models of pitch ac-
cent prediction. These features include the duration
of the word, speech rate, and following intonational
phrase boundaries. Given the nature of the SWBD
corpus, there are many disfluencies. Thus, we also
Feature ?2 Sig
canonical syllables 1636 p < .001
canonical phones 2430 p < .001
transcribed phones 2741 p < .001
utt length 80 p < .005
utt position 295 p < .001
duration 3073 p < .001
speech rate 101 p < .001
following pause 27 p < .001
foll filled pause 328 p < .001
foll IP boundary 1047 p < .001
Table 4: Significance of phonological features on
pitch accent prediction.
included following pauses and filled pauses as pre-
dictors. Below is the list of our acoustic features:
? Log of duration in milliseconds normalized
by number of canonical phones binned into 5
equal categories.
? Log Speech Rate; calculated on strings of
speech bounded on either side by pauses of
300 ms or greater and binned into 5 equal cat-
egories.
? Following pause; a binary distinction of
whether a word is followed by a period of si-
lence or not.
? Following filled pause; a binary distinction of
whether a word was followed by a filled pause
(uh, um) or not.
? Following IP boundary
Table 4 indicates that each of these features sig-
nificantly affect the presence of pitch accent. While
certainly all of these variables are not independent
of on another, using CRFs, one can incorporate all
of these variables into the pitch accent prediction
model with the advantage of making use of the de-
pendencies among the labels.
4.4 Surrounding Information
Sun (2002) has shown that the values immediately
preceding and following the target are good predic-
tors for the value of the target. We also experi-
mented with the effects of the surrounding values
by varying the window size of the observation-label
feature extraction described in Section 2. When the
window size is 1, only values of the word that is la-
belled are incorporated in the model. When the win-
dow size is 3, the values of the previous and the fol-
lowing words as well as the current word are incor-
porated in the model. Window size 5 captures the
values of the current word, the two previous words
and the two following words.
5 Experiments and Results
All experiments were run using 10 fold cross-
validation. We used Viterbi decoding to find the
most likely sequence and report the performance in
terms of label accuracy. We ran all experiments with
varying window sizes (w ? {1, 3, 5}). The baseline
which simply assigns the most common label, un-
accented, achieves 60.53 ? 1.50%.
Previous research has demonstrated that part of
speech and frequency, or a combination of these
two, are very reliable predictors of pitch accent.
Thus, to test the worthiness of using a CRF model,
the first experiment we ran was a comparison of an
HMM to a CRF using just the combination of part of
speech and unigram. The HMM score (referred as
HMM:POS, Unigram in Table 5) was 68.62 ? 1.78,
while the CRF model (referred as CRF:POS, Uni-
gram in Table 5) performed significantly better at
72.56 ? 1.86. Note that Pan and McKeown (1999)
reported 74% accuracy with their HMM model.
The difference is due to the different corpora used
in each case. While they also used spontaneous
speech, it was a limited domain in the sense that
it was speech from discharge orders from doctors
at one medical facility. The SWDB corpus is open
domain conversational speech.
In order to capture some aspects of the IC and
collocational strength of a word, in the second ex-
periment we ran part of speech plus all of the prob-
abilistic variables (referred as CRF:POS, Prob in
Table 5). The model accuracy was 73.94%, thus
improved over the model using POS and unigram
values by 1.38%.
In the third experiment we wanted to know if TTS
applications that made use of purely textual input
could be aided by the addition of timing and rhythm
variables that can be gleaned from a text string.
Thus, we included the textual features described in
Section 4.3 in addition to the probabilistic and syn-
tactic features (referred as CRF:POS, Prob, Txt in
Table 5). The accuracy was improved by 1.73%.
For the final experiment, we added the acoustic
variable, resulting in the use of all the variables de-
scribed in Section 4 (referred as CRF:All in Table
5). We get about 0.5% increase in accuracy, 76.1%
with a window of size w = 1.
Using larger windows resulted in minor increases
in the performance of the model, as summarized in
Table 5. Our best accuracy was 76.36% using all
features in a w = 5 window size.
Model:Variables w = 1 w = 3 w = 5
Baseline 60.53
HMM: POS,Unigram 68.62
CRF: POS, Unigram 72.56
CRF: POS, Prob 73.94 74.19 74.51
CRF: POS, Prob, Txt 75.67 75.74 75.89
CRF: All 76.1 76.23 76.36
Table 5: Test accuracy of pitch accent prediction on
SWDB using various variables and window sizes.
6 Discussion
Pitch accent prediction is a difficult task, in that, the
number of different speakers, topics, utterance frag-
ments and disfluent production of the SWBD corpus
only increase this difficulty. The fact that 21% of
the function words are accented indicates that mod-
els of pitch accent that mostly rely on part of speech
and unigram frequency would not fair well with this
corpus. We have presented a model of pitch accent
that captures some of the other factors that influence
accentuation. In addition to adding more probabilis-
tic variables and phonological factors, we have used
a sequence model that captures the interdependence
of accents within a phrase.
Given the distinct natures of corpora used, it is
difficult to compare these results with earlier mod-
els. However, in experiment 1 (HMM: POS, Uni-
gram vs CRF: POS, Unigram) we have shown that
a CRF model achieves a better performance than an
HMM model using the same features. However,
the real strength of CRFs comes from their ability
to incorporate different sources of information effi-
ciently, as is demonstrated in our experiments.
We did not test directly the probabilistic measures
(or collocation measures) that have been used before
for this task, namely information content (IC) (Pan
and McKeown, 1999) and mutual information (Pan
and Hirschberg, 2001). However, the measures we
have used encompass similar information. For ex-
ample, IC is only the additive inverse of our unigram
measure:
IC(w) = ? log p(w) (6)
Rather than using mutual information as a measure
of collocational strength, we used unigram, bigram
and joint probabilities. A model that includes both
joint probability and the unigram probabilities of wi
and wi?1 is comparable to one that includes mutual
information.
Just as the likelihood of a word being accented
is influenced by a following silence or IP bound-
ary, the collocational strength of the target word
with the following word (captured by reverse bi-
gram and reverse joint) is also a factor. With the
use of POS, unigram, and all bigram and joint prob-
abilities, we have shown that (a) CRFs outperform
HMMs, and (b) our probabilistic variables increase
accuracy from a model that include POS + unigram
(73.94% compared to 72.56%).
For tasks in which pitch accent is predicted solely
based on a string of text, without the addition of
acoustic data, we have shown that adding aspects
of rhythm and timing aids in the identification of
accent targets. We used the number of words in
an utterance, where in the utterance a word falls,
how long in both number of syllables and number
of phones all affect accentuation. The addition of
these variables improved the model by nearly 2%.
These results suggest that Accent prediction models
that only make use of textual information could be
improved with the addition of these variables.
While not trying to provide a complete model
of accentuation from acoustic information, in this
study we tested a few acoustic variables that have
not yet been tested. The nature of the SWBD cor-
pus allowed us to investigate the role of disfluencies
and widely variable durations and speech rate on ac-
centuation. Especially speech rate, duration and sur-
rounding silence are good predictors of pitch accent.
The addition of these predictors only slightly im-
proved the model (about .5%). Acoustic features are
very sensitive to individual speakers. In the corpus,
there are many different speakers of varying ages
and dialects. These variables might become more
useful if one controls for individual speaker differ-
ences. To really test the usefulness of these vari-
ables, one would have to combine them with acous-
tic features that have been demonstrated to be good
predictors of pitch accent (Sun, 2002; Conkie et al,
1999; Wightman et al, 2000).
7 Conclusion
We used CRFs with new measures of collocational
strength and new phonological factors that capture
aspects of rhythm and timing to model pitch accent
prediction. CRFs have the theoretical advantage of
incorporating all these factors in a principled and ef-
ficient way. We demonstrated that CRFs outperform
HMMs also experimentally. We also demonstrated
the usefulness of some new probabilistic variables
and phonological variables. Our results mainly have
implications for the textual prediction of accents in
TTS applications, but might also be useful in au-
tomatic speech recognition tasks such as automatic
transcription of multi-speaker meetings. In the near
future we would like to incorporate reliable acoustic
information, controlling for individual speaker dif-
ference and also apply different discriminative se-
quence labeling techniques to pitch accent predic-
tion task.
8 Acknowledgements
This work was partially funded by CAREER award
#IIS 9733067 IGERT. We would also like to thank
Mark Johnson for the idea of this project, Dan Ju-
rafsky, Alan Bell, Cynthia Girand, and Jason Bre-
nier for their helpful comments and help with the
database.
References
Y. Altun, T. Hofmann, and M. Johnson. 2003a.
Discriminative learning for label sequences via
boosting. In Proc. of Advances in Neural Infor-
mation Processing Systems.
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003b.
Hidden markov support vector machines. In
Proc. of 20th International Conference on Ma-
chine Learning.
M. Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In Proc.
of Empirical Methods of Natural Language Pro-
cessing.
A. Conkie, G. Riccardi, and R. Rose. 1999.
Prosody recognition from speech utterances us-
ing acoustic and linguistic based models of
prosodic events. In Proc. of EUROSPEECH?99.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
E. Fosler-Lussier and N. Morgan. 1999. Effects of
speaking rate and word frequency on conversa-
tional pronunci ations. In Speech Communica-
tion.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and develo pment. In Proc. of the Inter-
national Conference on Acoustics, Speech, and
Signal Processing.
S. Greenberg, D. Ellis, and J. Hollenback. 1996. In-
sights into spoken language gleaned from pho-
netic transcripti on of the Switchboard corpus.
In Proc. of International Conference on Spoken
Language Processsing.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2):305?340.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
unification-based grammars. In Proc. of ACL?99
Association for Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proc. of 18th International Conference on Ma-
chine Learning.
A. McCallum, D. Freitag, and F. Pereira. 2000.
Maximum Entropy Markov Models for Infor-
mation Extraction and Segmentation. In Proc.
of 17th International Conference on Machine
Learning.
A. McCallum. 2003. Efficiently inducing features
of Conditional Random Fields. In Proc. of Un-
certainty in Articifical Intelligence.
T. Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Technical report,
CMU, Department of Statistics, TR 758.
S. Pan and J. Hirschberg. 2001. Modeling local
context for pitch accent prediction. In Proc. of
ACL?01, Association for Computational Linguis-
tics.
S. Pan and K. McKeown. 1999. Word informa-
tiveness and automatic pitch accent modeling.
In Proc. of the Joint SIGDAT Conference on
EMNLP and VLC.
V. Punyakanok and D. Roth. 2000. The use of
classifiers in sequential inference. In Proc. of
Advances in Neural Information Processing Sys-
tems.
F. Sha and F. Pereira. 2003. Shallow parsing with
conditional random fields. In Proc. of Human
Language Technology.
Xuejing Sun. 2002. Pitch accent prediction using
ensemble machine learning. In Proc. of the In-
ternational Conference on Spoken Language Pro-
cessing.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-
margin markov networks. In Proc. of Advances
in Neural Information Processing Systems.
P. Taylor. 2000. Analysis and synthesis of intona-
tion using the Tilt model. Journal of the Acousti-
cal Society of America.
C. W. Wightman, A. K. Syrdal, G. Stemmer,
A. Conkie, and M. Beutnagel. 2000. Percep-
tually Based Automatic Prosody Labeling and
Prosodically Enriched Unit Selection Improve
Concatenative Text-To-Speech Synthesis. vol-
ume 2, pages 71?74.
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 2?3,
Vancouver, October 2005.
Bridging the Gap between Technology and Users: Leveraging Machine 
Translation in a Visual Data Triage Tool 
Thomas Hoeft Nick Cramer M. L. Gregory Elizabeth Hetzler
Pacific Northwest  
National Laboratory 
Pacific Northwest  
National Laboratory 
Pacific Northwest  
National Laboratory 
Pacific Northwest  
National Laboratory 
902 Battelle Blvd. 902 Battelle Blvd. 902 Battelle Blvd. 902 Battelle Blvd. 
Richland, WA 99354 Richland, WA 99354 Richland, WA 99354 Richland, WA 99354 
{thomas.hoeft;nick.cramer;michelle.gregory;beth.hetzler}@pnl.gov 
 
  
1 Introduction 
While one of the oldest pursuits in computational 
linguistics (see Bar-Hillel, 1951), machine transla-
tion (MT) remains an unsolved problem. While 
current research has progressed a great deal, tech-
nology transfer to end users is limited. In this 
demo, we present a visualization tool for manipu-
lating foreign language data. Using software de-
veloped for the exploration and understanding of 
large amounts of text data, IN-SPIRE (Hetzler & 
Turner 2004), we have developed a novel approach 
to mining and triaging large amounts of foreign 
language texts. By clustering documents in their 
native language and only using translations in the 
data triage phase, our system avoids the major pit-
falls that plague modern machine translation. More 
generally, the visualization environment we have 
developed allows users to take advantage of cur-
rent NLP technologies, including MT. We will 
demonstrate use of this tool to triage a corpus of 
foreign text. 
2 IN-SPIRE 
IN-SPIRE (Hetzler et al, 2004) is a visual ana-
lytics tool developed by Pacific Northwest Na-
tional Laboratory to facilitate the collection and 
rapid understanding of large textual corpora. IN-
SPIRE generates a compiled document set from 
mathematical signatures for each document in a 
set. Document signatures are clustered according 
to common themes to enable information retrieval 
and visualizations.  Information is presented to the 
user using several visual metaphors to expose dif-
ferent facets of the textual data. The central visual 
metaphor is a galaxy view of the corpus that allows 
users to intuitively interact with thousands of 
documents, examining them by theme. 
 Context vectors for documents such as LSA 
(Deerwester  et al, 1990) provide a powerful foun-
dation for information retrieval and natural lan-
guage processing techniques.  IN-SPIRE leverages 
such representations for clustering, projection and 
queries-by-example (QBE). In addition to standard 
Boolean word queries, QBE is a process in which a 
user document query is converted into a mathe-
matical signature and compared to the multi-
dimensional mathematical representation of the 
document corpus.  A spherical distance threshold 
adjustable by the end user controls a query result 
set.  Using IN-SPIRE?s group functionality, sub-
sets of the corpus are identified for more detailed 
analyses.  Information analysts can isolate mean-
ingful document subsets into groups for hypothesis 
testing and the identification of trends.  Depending 
on the corpus, one or more clusters may be less 
interesting to users. Removal of these documents, 
called ?outliers?, enables the investigator to more 
clearly understand the relationships between re-
maining documents.  These tools expose various 
facets of document text and document inter-
relationships. 
3 Foreign Language Triage Capabilities  
Information analysts need to sift through large 
datasets quickly and efficiently to identify relevant 
information for knowledge discovery. The need to 
sift through foreign language data complicates the 
task immensely. The addition of foreign language 
capabilities to IN-SPIRE addresses this need.  We 
have integrated third party translators for over 40 
languages and  third party software for language 
identification. Datasets compiled with language 
detection allow IN-SPIRE to automatically select 
the most appropriate translator for each document.  
To triage a foreign language dataset, the sys-
tem clusters the documents in their native language 
2
(with no pre-translation required). A user can then 
view the cluster labels, or peak terms, in the native 
language, or have them translated via Systran 
(Senellart et al, 2003) or CyberTrans (not publicly 
available). The user can then explore the clusters to 
get a general sense of the thematic coverage of the 
dataset. They identify clusters relevant to their in-
terests and the tool reclusters to show more subtle 
themes differentiating the remaining documents. If 
they search for particular words, the clusters and 
translated labels help them distinguish the various 
contexts in which those words appear. Finding a 
cluster of document of interest, a particular docu-
ment or set of documents can be viewed and trans-
lated on demand. This avoids the need to translate 
the entire document set, so that only the documents 
of interest are translated. The native text is dis-
played alongside the translation at all stages.   
4 Evaluation 
Since this is a prototype visualization tool we 
have yet to conduct formal user evaluations. We 
have begun field testing this tool with users who 
report successful data triage in foreign languages 
with which they are not familiar. We have also 
begun evaluations involving parallel corpora.  Us-
ing Arabic English Parallel News Text (LDC 
2004), which contains over 8,000 human translated 
documents from various Arabic new sources, we 
processed the English version in IN-SPIRE to view 
the document clusters and their labels. We also 
processed the Arabic version in Arabic according 
to the description above. The two screenshots be-
low demonstrate that the documents clustered in 
similar manners (note that cluster labels have been 
translated in the Arabic data). 
 
    
Figure 1: Galaxy view of the Arabic and English 
clusters and labels  
 
To demonstrate that our clustering algorithm on 
the native language is an efficient and reliable 
method for data triage on foreign language data, 
we also pre-translated the data with CyberTrans 
and clustered on the output. Figure 3, demonstrates 
that similar clusters arise out of this methodology. 
However, the processing time was increase
 
d 15-
ld with no clear advantage for data triage. 
 
fo
 
Figure 3: Galaxy view of the pre-translated Ara-
bic to English clusters and labels 
lue from 
existing machine translation capabilities.  
nslation. American Documenta-
n 2 (4),  pp.229-237. 
 
omputer Graphics and Applications, 24(5):22-26. 
f the Society for Information 
cience, 41(6):391-407. 
.edu/Catalog/catalogEntry.jsp?cata
gId=LDC2004T18 
chnology. MT Summit 
IX. New Orleans, Louisianna.  
 
Initial user reports and comparisons with a par-
allel corpus demonstrate that our visualization en-
vironment enables users to search through and 
cluster massive amounts of data without native 
speaker competence or dependence on a machine 
translation system. Users can identify clusters of 
potential interest with this tool and translate (by 
human or machine) only those documents of rele-
vance. We have demonstrated that this visualiza-
tion tool allows users to derive high va
References  
Bar-Hillel, Yehoshua, 1951. The present state of re-
search on mechanical tra
tio
  
Hetzler, Elizabeth and Alan Turner. 2004. ?Analysis 
Experiences Using Information Visualization,? IEEE
C
 
Deerwester, S., S.T. Dumais, T.K. Landauer, G.W. 
Furnas, R.A. Harshman. 1990. Indexing by Latent Se-
mantic Analysis. Journal o
S
 
Linquistic Data Consortium. 2004. 
http://www.ldc.upenn
lo
 
Senellart, Jean; Jin Yang, and Anabel Rebollo. 2003. 
SYSTRAN Intuitive Coding Te
3
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 141?144,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Domain Disambiguation via Word Sense Disambiguation 
 
Antonio Sanfilippo, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Stephen.Tratz, Michelle.Gregory}@pnl.gov 
 
 
  
 
Abstract 
Word subject domains have been 
widely used to improve the perform-
ance of word sense disambiguation al-
gorithms. However, comparatively little 
effort has been devoted so far to the 
disambiguation of word subject do-
mains. The few existing approaches 
have focused on the development of al-
gorithms specific to word domain dis-
ambiguation. In this paper we explore 
an alternative approach where word 
domain disambiguation is achieved via 
word sense disambiguation. Our study 
shows that this approach yields very 
strong results, suggesting that word 
domain disambiguation can be ad-
dressed in terms of word sense disam-
biguation with no need for special 
purpose algorithms.  
1 Introduction 
Word subject domains have been ubiquitously 
used in dictionaries to help human readers pin-
point the specific sense of a word by specifying 
technical usage, e.g. see ?subject field codes? in 
Procter (1978). In computational linguistics, 
word subject domains have been widely used to 
improve the performance of machine translation 
systems. For example, in a review of commonly 
used features in automated translation, Mowatt 
(1999) reports that most of the machine transla-
tion systems surveyed made use of word subject 
domains. Word subject domains have also been 
used in information systems. For example, San-
filippo (1998) describes a summarization system 
where subject domains provide users with useful 
conceptual parameters to tailor summary re-
quests to a user?s interest.  
Successful usage of word domains in applica-
tions such as machine translation and summari-
zation is strongly dependent on the ability to 
assign the appropriate subject domain to a word 
in its context. Such an assignment requires a 
process of Word Domain Disambiguation 
(WDD) because the same word can often be as-
signed different subject domains out of context 
(e.g. the word partner can potentially be re-
lated to FINANCE or MARRIAGE).  
Interestingly enough, word subject domains 
have been widely used to improve the perform-
ance of Word Sense Disambiguation (WSD) 
algorithms (Wilks and Stevenson 1998, Magnini 
et al 2001; Gliozzo et al 2004). However, com-
paratively little effort has been devoted so far to 
the word domain disambiguation itself. The 
most notable exceptions are the work of Magnini 
and Strapparava (2000) and Suarez & Palomar 
(2002). Both studies propose algorithms specific 
to the WDD task and have focused on the dis-
ambiguation of noun domains.  
In this paper we explore an alternative ap-
proach where word domain disambiguation is 
achieved via word sense disambiguation. More-
over, we extend the treatment of WDD to verbs 
and adjectives. Initial results show that this ap-
proach yield very strong results, suggesting that 
WDD can be addressed in terms of word sense 
disambiguation with no need of special purpose 
algorithms.  
141
  
Figure 1: Senses and domains for the word bank in WordNet Domains, with number of occurrences in SemCor, 
adapted from Magnini et al (2002). 
2 WDD via WSD 
Our approach relies on the use of WordNet Do-
mains (Bagnini and Cavagli? 2000) and can be 
outlined in the following two steps:  
1. use a WordNet-based WSD algorithm to 
assign a sense to each word in the input 
text, e.g. doctor  doctor#n#1 
2. use WordNet Domains to map disam-
biguated words into the subject domain 
associated with the word, e.g. doc-
tor#n#1doctor#n#1#MEDICINE. 
2.1 WordNet Domains 
WordNet Domains is an extension of WordNet 
(http://wordnet.princeton.edu/) where synonym 
sets have been annotated with one or more sub-
ject domain labels, as shown in Figure 1. Subject 
domains provide an interesting and useful classi-
fication which cuts across part of speech and 
WordNet sub-hierarchies. For example, doc-
tor#n#1 and operate#n#1 both have sub-
ject domain MEDICINE, and SPORT includes both 
athlete#n#1 with top hypernym life-
form#n#1 and sport#n#1 with  top hy-
pernym act#n#2.  
2.2 Word Sense Disambiguation 
To assign a sense to each word in the input text, 
we used the WSD algorithm presented in San-
filippo et al (2006). This WSD algorithm is 
based on a supervised classification approach 
that uses SemCor1 as training corpus. The algo-
rithm employs the OpenNLP MaxEnt imple-
mentation of the maximum entropy 
classification algorithm (Berger et al 1996) to 
develop word sense recognition signatures for 
each lemma which predicts the most likely sense 
for the lemma according to the context in which 
the lemma occurs. 
Following Dang & Palmer (2005) and Ko-
homban & Lee (2005), Sanfilippo et al (2006) 
use contextual, syntactic and semantic informa-
tion to inform our verb class disambiguation 
system.  
? Contextual information includes the verb 
under analysis plus three tokens found on 
each side of the verb, within sentence 
boundaries. Tokens included word as well 
as punctuation. 
? Syntactic information includes grammatical 
dependencies (e.g. subject, object) and mor-
pho-syntactic features such as part of 
speech, case, number and tense.  
? Semantic information includes named entity 
types (e.g. person, location, organization) 
and hypernyms. 
We chose this WSD algorithm as it provides 
some of the best published results to date, as the 
comparison with top performing WSD systems 
in Senseval3 presented in Table 1 shows---see 
http://www.senseval.org and Snyder & Palmer 
(2004) for terms of reference on Senseval3. 
                                                          
1
 http://www.cs.unt.edu/~rada/downloads.html. 
142
System Precision Fraction of 
Recall 
Sanfilippo et al  2006 61% 22% 
GAMBL 59.0% 21.3% 
SenseLearner 56.1% 20.2% 
Baseline 52.9% 19.1% 
Table 1: Results for verb sense disambiguation on 
Senseval3 data, adapted from Sanfilippo et al (2006). 
3 Evaluation 
To evaluate our WDD approach, we used both 
the SemCor and Senseval3 data sets. Both cor-
pora were stripped of their sense annotations and 
processed with an extension of the WSD algo-
rithm of Sanfilippo et al (2006) to assign a 
WordNet sense to each noun, verb and adjective. 
The extension consisted in extending the train-
ing data set so as to include a selection of 
WordNet examples (full sentences containing a 
main verb) and the Open Mind Word Expert 
corpus (Chklovski and Mihalcea 2002).  
The original hand-coded word sense annota-
tions of the SemCor and Senseval3 corpora and 
the word sense annotations assigned by the 
WSD algorithm used in this study were mapped 
into subject domain annotations using WordNet 
Domains, as described in the opening paragraph 
of section 2 above. The version of the SemCor 
and Senseval3 corpora where subject domain 
annotations were generated from hand-coded 
word senses served as gold standard.  A baseline 
for both corpora was obtained by assigning to 
each lemma the subject domain corresponding to 
sense 1 of the lemma.  
WDD results of a tenfold cross-validation for 
the SemCor data set are given in Table 2. Accu-
racy is high across nouns, verbs and adjectives.2 
To verify the statistical significance of these re-
sults against the baseline, we used a standard 
proportions comparison test (see Fleiss 1981, p. 
30). According to this test, the accuracy of our 
system is significantly better than the baseline.  
The high accuracy of our WDD algorithm is 
corroborated by the results for the Senseval3 
data set in Table 3. Such corroboration is impor-
tant as the Senseval3 corpus was not part of the 
data set used to train the WSD algorithm which 
provided the basis for subject domain assign-
                                                          
2
 We have not worked on adverbs yet, but we expect com-
parable results. 
ment. The standard comparison test for the Sen-
seval3 is not as conclusive as with SemCor. This 
is probably due to the comparatively smaller size 
of the Senseval3 corpus. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.874 0.933 0.942 0.912 
Baseline 0.848 0.927 0.932 0.897 
p-value 4.6e-54 1.4e-07 5.5e-08 1.4e-58 
Table 2: SemCor WDD results. 
 
 Nouns Verbs Adj.s Overall 
Accuracy 0.797 0.908 0.888 0.848 
Baseline 0.783 0.893 0.862 0.829 
p-value 0.227 0.169 0.151 0.048 
Table 3: Senseval3 WDD results. 
4 Comparison with Previous WDD 
Work 
Our WDD algorithm compares favorably with 
the approach explored in Bagnini and Strap-
parava (2000), who report 0.82 p/r in the WDD 
tasks for a subset of nouns in SemCor.  
Suarez and Palomar (2002) report WDD re-
sults of 78.7% accuracy for nouns against a 
baseline of 68.7% accuracy for the same data 
set. As in the present study, Suarez and Palomar 
derive the baseline by assigning to each lemma 
the subject domain corresponding to sense 1 of 
the lemma. Unfortunately, a meaningful com-
parison with Suarez and Palomar (2002) is not 
possible as they use a different data set, the DSO 
corpus.3 We are currently working on repeating 
our study with the DSO corpus and will include 
the results of this evaluation in the final version 
of the paper to achieve commensurability with 
the results reported by Suarez and Palomar. 
5 Conclusions and Further Work 
Current approaches to WDD have assumed that 
special purpose algorithms are needed to model 
the WDD task. We have shown that very com-
petitive and perhaps unrivaled results (pending 
on evaluation of our WDD algorithm with the 
DSO corpus) can be obtained using WSD as the 
basis for subject domain assignment. This im-
provement in WDD performance can be used to 
                                                          
3
 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?cata 
logId=LDC97T12.  
143
obtain further gains in WSD accuracy, following 
Wilks and Stevenson (1998), Magnini et al 
(2001) and Gliozzo et al (2004). A more accu-
rate WSD model will in turn yield yet better 
WDD results, as demonstrated in this paper. 
Consequently, further improvements in accuracy 
for both WSD and WDD can be expected 
through a bootstrapping cycle where WDD re-
sults are fed as input to the WSD process, and 
the resulting improved WSD model is then used 
to achieve better WDD results. We intend to 
explore this possibility in future extensions of 
this work. 
Acknowledgements 
We would like to thank Paul Whitney for help 
with the evaluation of the results presented in 
Section 3.  
References  
Berger, A., S. Della Pietra and V. Della Pietra (1996) 
A Maximum Entropy Approach to Natural Lan-
guage Processing. Computational Linguistics, vol-
ume 22, number 1, pages 39-71. 
Chklovski, T. and R. Mihalcea (2002) Building a 
Sense Tagged Corpus with Open Mind Word Ex-
pert. Proceedings of the ACL 2002 Workshop on 
"Word Sense Disambiguation: Recent Successes 
and Future Directions, Philadelphia, July 2002, pp. 
116-122. 
Dang, H. T. and M. Palmer (2005) The Role of Se-
mantic Roles in Disambiguating Verb Senses. In 
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Ar-
bor MI, June 26-28, 2005.  
Fleiss, J. L. (1981) Statistical Methods for Rates and 
Proportions. 2nd edition. New York: John Wiley 
& Sons. 
Gliozzo, A., C. Strapparava, I. Dagan (2004) Unsu-
pervised and Supervised Exploitation of Semantic 
Domains in Lexical Disambiguation. Computer 
Speech and Language,18(3), Pages 275-299. 
Kohomban, U. and  W. Lee (2005) Learning seman-
tic classes for word sense disambiguation. In Pro-
ceedings of the 43rd Annual meeting of the 
Association for Computational Linguistics, Ann 
Arbor, MI.   
Magnini, B., Cavagli?, G. (2000) Integrating Subject 
Field Codes into WordNet. Proceedings of LREC-
2000, Second International Conference on Lan-
guage Resources and Evaluation, Athens, Greece, 
31 MAY- 2 JUNE 2000, pp. 1413-1418. 
Magnini, B., Strapparava C. (2000) Experiments in 
Word Domain Disambiguation for Parallel Texts. 
Proceedings of the ACL Workshop on Word 
Senses and Multilinguality, Hong-Kong, October 
7, 2000, pp. 27-33 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2001) Using Domain Information for 
Word Sense Disambiguation. In Proceeding of 
SENSEVAL-2: Second International Workshop on 
Evaluating Word Sense Disambiguation Systems, 
pp. 111-114, 5-6 July 2001, Toulouse, France. 
Magnini, B., C. Strapparava, G. Pezzulo and A. 
Gliozzo (2002) The Role of Domain Information 
in Word Sense Disambiguation. Natural Language 
Engineering, 8(4):359?373. 
Mowatt, D. (1999) Types of Semantic Information 
Necessary in a Machine Translation Lexicon. Con-
f?rence TALN, Carg?se, pp. 12-17. 
Procter, Paul (Ed.) (1978) Longman Dictionary o 
Contemporary English. Longman Group Ltd., Es-
sex, UK. 
Sanfilippo, A. (1998) Ranking Text Units According 
to Textual Saliency, Connectivity and Topic Apt-
ness. COLING-ACL 1998: 1157-1163. 
Sanfilippo, A., S. Tratz, M. Gregory, A.Chappell, P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. 
Hohimer, A. White. (2006) Automating Ontologi-
cal Annotation with WordNet. Proceedings of the 
3rd Global WordNet Conference, Jeju Island, 
South Korea, Jan 19-26 2006.  
Snyder, B.  and M. Palmer. 2004. The English all-
words task. SENSEVAL-3: Third International 
Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.  
Su?rez, A., Palomar, M. (2002) Word sense vs. word 
domain disambiguation: a maximum entropy ap-
proach. In Sojka P., Kopecek I., Pala K., eds.: 
Text, Speech and Dialogue (TSD 2002). Volume 
2448 of Lecture Notes in Artificial Intelligence, 
Springer, (2002) 131?138. 
Wilks, Y. and Stevenson, M. (1998) Word sense dis-
ambiguation using optimised combinations of 
knowledge sources. Proceedings of the 17th inter-
national conference on Computational Linguistics, 
pp. 1398?1402. 
144
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 23?30,
Sydney, July 2006. c?2006 Association for Computational Linguistics
User-directed Sentiment Analysis: Visualizing the Affective Content of 
Documents  
Michelle L. Gregory 
PNNL 
902 Battelle Blvd.  
Richland Wa. 99354 
michelle.gregory@pnl.gov 
Nancy Chinchor 
Consultant 
chinchor@earthlink.net 
Paul Whitney 
PNNL 
902 Battelle Blvd.  
Richland Wa. 99354 
paul.whitney@pnl.gov 
Richard Carter 
PNNL 
902 Battelle Blvd.  
Richland Wa. 99354 
richard.carter@pnl.gov 
 
Elizabeth Hetzler 
PNNL 
902 Battelle Blvd.  
Richland Wa. 99354 
beth.hetzler@pnl.gov 
Alan Turner 
PNNL 
902 Battelle Blvd.  
Richland Wa. 99354 
alan.turner@pnl.gov 
 
Abstract 
Recent advances in text analysis have led 
to finer-grained semantic analysis, in-
cluding automatic sentiment analysis?
the task of measuring documents, or 
chunks of text, based on emotive catego-
ries, such as positive or negative. How-
ever, considerably less progress has been 
made on efficient ways of exploring 
these measurements. This paper discusses 
approaches for visualizing the affective 
content of documents and describes an 
interactive capability for exploring emo-
tion in a large document collection. 
1 Introduction 
Recent advances in text analysis have led to 
finer-grained semantic classification, which en-
ables the automatic exploration of subtle areas of 
meaning.  One area that has received a lot of at-
tention is automatic sentiment analysis?the task 
of classifying documents, or chunks of text, into 
emotive categories, such as positive or negative. 
Sentiment analysis is generally used for tracking 
people?s attitudes about particular individuals or 
items. For example, corporations use sentiment 
analysis to determine employee attitude and cus-
tomer satisfaction with their products. Given the 
plethora of data in digital form, the ability to ac-
curately and efficiently measure the emotional 
content of documents is paramount.  
The focus of much of the automatic sentiment 
analysis research is on identifying the affect 
bearing words (words with emotional content) 
and on measurement approaches for sentiment 
(Turney & Littman, 2003; Pang & Lee, 2004; 
Wilson et al, 2005). While identifying related 
content is an essential component for automatic 
sentiment analysis, it only provides half the 
story. A useful area of research that has received 
much less attention is how these measurements 
might be presented to the users for exploration 
and added value.  
This paper discusses approaches for visualiz-
ing affect and describes an interactive capability 
for exploring emotion in a large document col-
lection. In Section 2 we review current ap-
proaches to identifying the affective content of 
documents, as well as possible ways of visualiz-
ing it. In Section 3 we describe our approach: 
The combination of a lexical scoring method to 
determine the affective content of documents and 
a visual analytics tool for visualizing it. We pro-
vide a detailed case study in Section 4, followed 
by a discussion of possible evaluations.  
2 Background  
At the AAAI Symposium on Attitude and Affect 
held at Stanford in 2004 (Qu et al, 2005), it was 
clear that the lexical approach to capturing affect 
was adequate for broad brush results, but there 
were no production quality visualizations for 
presenting those results analytically. Thus, we 
began exploring methods and tools for the visu-
alization of lexically-based approaches for meas-
uring affect which could facilitate the exploration 
of affect within a text collection. 
 
2.1 Affect Extraction 
Following the general methodology of informa-
tional retrieval, there are two pre-dominant 
methods for identifying sentiment in text: Text 
classification models and lexical approaches. 
Classification models require that a set of docu-
ments are hand labeled for affect, and a system is 
23
trained on the feature vectors associated with 
labels. New text is automatically classified by 
comparing the feature vectors with the training 
set. (Pang & Lee, 2004; Aue & Gamon, 2005). 
This methodology generally requires a large 
amount of training data and is domain dependent.  
In the lexical approach, documents (Turney & 
Littman, 2003), phrases (see Wilson et al, 2005), 
or sentences (Weibe & Riloff, 2005) are catego-
rized as positive or negative, for example, based 
on the number of words in them that match a 
lexicon of sentiment bearing terms. Major draw-
backs of this approach include the contextual 
variability of sentiment (what is positive in one 
domain may not be in another) and incomplete 
coverage of the lexicon. This latter drawback is 
often circumvented by employing bootstrapping 
(Turney & Littman, 2003; Weibe & Riloff, 2005) 
which allows one to create a larger lexicon from 
a small number of seed words, and potentially 
one specific to a particular domain. 
2.2 Affect Visualization 
The uses of automatic sentiment classification 
are clear (public opinion, customer reviews, 
product analysis, etc.). However, there has not 
been a great deal of research into ways of visual-
izing affective content in ways that might aid 
data exploration and the analytic process.   
There are a number of visualizations designed 
to reveal the emotional content of text, in par-
ticular, text that is thought to be highly emotively 
charged such as conversational transcripts and 
chat room transcripts (see DiMicco et al, 2002; 
Tat & Carpendale, 2002; Lieberman et al, 2004;  
Wang et al, 2004, for example).  Aside from 
using color and emoticons to explore individual 
documents (Liu et al, 2003) or email inboxes 
(Mandic & Kerne, 2004), there are very few 
visualizations suitable for exploring the affect of 
large collections of text. One exception is the 
work of Liu et al (2005) in which they provide a 
visualization tool to compare reviews of prod-
ucts,using a bar graph metaphor. Their system 
automatically extracts product features (with as-
sociated affect) through parsing and pos tagging, 
having to handle exceptional cases individually. 
Their Opinion Observer is a powerful tool de-
signed for a single purpose: comparing customer 
reviews.  
In this paper, we introduce a visual analytic 
tool designed to explore the emotional content of 
large collections of open domain documents. The 
tools described here work with document collec-
tions of all sizes, structures (html, xml, .doc, 
email, etc), sources (private collections, web, 
etc.), and types of document collections. The 
visualization tool is a mature tool that supports 
the analytical process by enabling users to ex-
plore the thematic content of the collection, use 
natural language to query the collection, make 
groups, view documents by time, etc. The ability 
to explore the emotional content of an entire col-
lection of documents not only enables users to 
compare the range of affect in documents within 
the collection, but also allows them to relate af-
fect to other dimensions in the collection, such as 
major topics and themes, time, and source.   
3 The Approach 
Our methodology combines a traditional lexical 
approach to scoring documents for affect with a 
mature visualization tool. We first automatically 
identify affect by comparing each document 
against a lexicon of affect-bearing words and 
obtain an affect score for each document. We 
provide a number of visual metaphors to repre-
sent the affect in the collection and a number of 
tools that can be used to interactively explore the 
affective content of the data. 
3.1 Lexicon and Measurement 
We use a lexicon of affect-bearing words to iden-
tify the distribution of affect in the documents. 
Our lexicon authoring system allows affect-
bearing terms, and their associated strengths, to 
be bulk loaded, declared manually, or algo-
rithmically suggested. In this paper, we use a 
lexicon derived from the General Inquirer (GI) 
and supplemented with lexical items derived 
from a semi-supervised bootstrapping task. The 
GI tool is a computer-assisted approach for con-
tent analyses of textual data (Stone, 1977). It in-
cludes an extensive lexicon of over 11,000 hand-
coded word stems and 182 categories.  
We used this lexicon, specifically the positive 
and negative axes, to create a larger lexicon by 
bootstrapping. Lexical bootstrapping is a method 
used to help expand dictionaries of semantic 
categories (Riloff & Jones, 1999) in the context 
of a document set of interest. The approach we 
have adopted begins with a lexicon of affect 
bearing words (POS and NEG) and a corpus. 
Each document in the corpus receives an affect 
score by counting the number of words from the 
seed lexicon that occur in the document; a sepa-
rate score is given for each affect axis. Words in 
the corpus are scored for affect potential by 
comparing their distribution (using an L1 Distri-
24
bution metric) of occurrence over the set if 
documents to the distribution of affect bearing 
words. Words that compare favorably with affect 
are hypothesized as affect bearing words. Results 
are then manually culled to determine if in fact 
they should be included in the lexicon. 
Here we report on results using a lexicon built 
from 8 affect categories, comprising 4 concept 
pairs:   
? Positive (n=2236)-Negative (n=2708) 
? Virtue (n=638)-Vice (n=649) 
? Pleasure (n=151)-Pain (n=220) 
? Power Cooperative (n=103)-Power Con-
flict (n=194)   
 
Each document in the collection is compared 
against all 8 affect categories and receives a 
score for each. Scores are based on the summa-
tion of each affect axis in the document, normal-
ized by the number of words in the documents. 
This provides an overall proportion of positive 
words, for example, per document. Scores can 
also be calculated as the summation of each axis, 
normalized by the total number of affect words 
for all axes. This allows one to quickly estimate 
the balance of affect in the documents. For ex-
ample, using this measurement, one could see 
that a particular document contains as many 
positive as negative terms, or if it is heavily 
skewed towards one or the other. 
While the results reported here are based on a 
predefined lexicon, our system does include a 
Lexicon Editor in which a user can manually en-
ter their own lexicon or add strengths to lexical 
items. Included in the editor is a Lexicon Boot-
strapping Utility which the user can use to help 
create a specialized lexicon of their own. This 
utility runs as described above. Note that while 
we enable the capability of strength, we have not 
experimented with that variable here. All words 
for all axes have a default strength of .5.  
3.2 Visualization 
To visualize the affective content of a collection 
of documents, we combined a variety of visual 
metaphors with a tool designed for visual ana-
lytics of documents, IN-SPIRE. 
3.2.1 The IN-SPIRE System 
IN-SPIRE (Hetzler and Turner, 2004) is a visual 
analytics tool designed to facilitate rapid under-
standing of large textual corpora. IN-SPIRE gen-
erates a compiled document set from mathemati-
cal signatures for each document in a set. 
Document signatures are clustered according to 
common themes to enable information explora-
tion and visualizations. Information is presented 
to the user using several visual metaphors to ex-
pose different facets of the textual data. The cen-
tral visual metaphor is a Galaxy view of the cor-
pus that allows users to intuitively interact with 
thousands of documents, examining them by 
theme (see Figure 4, below). IN-SPIRE leverages 
the use of context vectors such as LSA (Deer-
wester et al, 1990) for document clustering and 
projection. Additional analytic tools allow explo-
ration of temporal trends, thematic distribution 
by source or other metadata, and query relation-
ships and overlaps. IN-SPIRE was recently en-
hanced to support visual analysis of sentiment.  
3.2.2 Visual Metaphors 
In selecting metaphors to represent the affect 
scores of documents, we started by identifying 
the kinds of questions that users would want to 
explore. Consider, as a guiding example, a set of 
customer reviews for several commercial prod-
ucts (Hu & Liu, 2004). A user reviewing this 
data might be interested in a number of ques-
tions, such as: 
 
? What is the range of affect overall?   
? Which products are viewed most posi-
tively? Most negatively? 
? What is the range of affect for a particular 
product? 
? How does the affect in the reviews deviate 
from the norm? Which are more negative 
or positive than would be expected from 
the averages? 
? How does the feedback of one product 
compare to that of another? 
? Can we isolate the affect as it pertains to 
different features of the products? 
 
In selecting a base metaphor for affect, we 
wanted to be able to address these kinds of ques-
tions. We wanted a metaphor that would support 
viewing affect axes individually as well as in 
pairs. In addition to representing the most com-
mon axes, negative and positive, we wanted to 
provide more flexibility by incorporating the 
ability to portray multiple pairs because we sus-
pect that additional axes will help the user ex-
plore nuances of emotion in the data. For our 
current metaphor, we drew inspiration from the 
Rose plot used by Florence Nightingale (Wainer, 
1997). This metaphor is appealing in that it is 
easily interpreted, that larger scores draw more 
25
attention, and that measures are shown in consis-
tent relative location, making it easier to compare 
measures across document groups. We use a 
modified version of this metaphor in which each 
axis is represented individually but is also paired 
with its opposite to aid in direct comparisons. To 
this end, we vary the spacing between the rose 
petals to reinforce the pairing. We also use color; 
each pair has a common hue, with the more posi-
tive of the pair shown in a lighter shade and the 
more negative one in a darker shade (see Figure 
1). 
To address how much the range of affect var-
ies across a set of documents, we adapted the 
concept of a box plot to the rose petal. For each 
axis, we show the median and quartile values as 
shown in the figure below. The dark line indi-
cates the median value and the color band por-
trays the quartiles. In the plot in Figure 1, for 
example, the scores vary quite a bit. 
  
 
Figure 1. Rose plot adapted to show median and 
quartile variation. 
 
Another variation we made on the base meta-
phor was to address a more subtle set of ques-
tions. It may happen that the affect scores within 
a dataset are largely driven by document mem-
bership in particular groups. For example, in our 
customer data, it may be that all documents 
about Product A are relatively positive while 
those about Product B are relatively negative. A 
user wanting to understand customer complaints 
may have a subtle need. It is not sufficient to just 
look at the most negative documents in the data-
set, because none of the Product A documents 
may pass this threshold. What may also help is to 
look at all documents that are more negative than 
one would expect, given the product they dis-
cuss. To carry out this calculation, we use a sta-
tistical technique to calculate the Main (or ex-
pected) affect value for each group and the Re-
sidual (or deviation) affect value for each docu-
ment with respect to its group (Scheffe, 1999).  
To convey the Residual concept, we needed a 
representation of deviation from expected value. 
We also wanted this portrayal to be similar to the 
base metaphor. We use a unit circle to portray 
the expected value and show deviation by draw-
ing the appropriate rose petals either outside 
(larger than expected) or inside (smaller than 
expected) the unit circle, with the color amount 
showing the amount of deviation from expected. 
In the figures below, the dotted circle represents 
expected value. The glyph on the left shows a 
cluster with scores slightly higher than expected 
for Positive and for Cooperation affect. The 
glyph on the right shows a cluster with scores 
slightly higher than expected for the Negative 
and Vice affect axes (Figure 2).    
 
Figure 2. Rose plot adapted to show deviation 
from expected values. 
3.2.3 Visual Interaction 
IN-SPIRE includes a variety of analytic tools 
that allow exploration of temporal trends, the-
matic distribution by source or other metadata, 
and query relationships and overlaps. We have 
incorporated several interaction capabilities for 
further exploration of the affect. Our analysis 
system allows users to group documents in nu-
merous ways, such as by query results, by meta-
data (such as the product), by time frame, and by 
similarity in themes. A user can select one or 
more of these groups and see a summary of af-
fect and its variation in those groups. In addition, 
the group members are clustered by their affect 
scores and glyphs of the residual, or variation 
from expected value, are shown for each of these 
sub-group clusters.   
Below each rose we display a small histogram 
showing the number of documents represented 
by that glyph (see Figure 3). These allow com-
parison of affect to cluster or group size. For ex-
ample, we find that extreme affect scores are 
typically found in the smaller clusters, while lar-
ger ones often show more mid-range scores. As 
the user selects document groups or clusters, we 
show the proportion of documents selected.  
 
26
 
Figure 3. Clusters by affect score, with one rose 
plot per cluster. 
 
The interaction may also be driven from the 
affect size. If a given clustering of affect charac-
teristics is selected, the user can see the themes 
they represent, how they correlate to metadata, or 
the time distribution. We illustrate how the affect 
visualization and interaction fit into a larger 
analysis with a brief case study. 
4 Case study 
The IN-SPIRE visualization tool is a non-data 
specific tool, designed to explore large amounts 
of textual data for a variety of genres and docu-
ment types (doc, xml,  etc). Many users of the 
system have their own data sets they wish to ex-
plore (company internal documents), or data can 
be harvested directly from the web, either in a 
single web harvest, or dynamically. The case 
study and dataset presented here is intended as an 
example only, it does not represent the full range 
of exploration capabilities of the affective con-
tent of datasets.  
We explore a set of customer reviews, com-
prising a collection of Amazon reviews for five 
products (Hu & Liu, 2004). While a customer 
may not want to explore reviews for 5 different 
product types at once, the dataset is realistic in 
that a web harvest of one review site will contain 
reviews of multiple products. This allows us to 
demonstrate how the tool enables users to focus 
on the data and comparisons that they are inter-
ested in exploring. The 5 products in this dataset 
are: 
? Canon G3; digital camera 
? Nikon coolpix 4300; digital camera 
? Nokia 6610; cell phone 
? Creative Labs Nomad Jukebox Zen Xtra 
40GB; mp3 player 
? Apex AD2600 Progressive-scan DVD 
player 
 
We begin by clustering the reviews, based on 
overall thematic content. The labels are auto-
matically generated and indicate some of the 
stronger theme combinations in this dataset. 
These clusters are driven largely by product vo-
cabulary. The two cameras cluster in the lower 
portion; the Zen shows up in the upper right clus-
ters, with the phone in the middle and the Apex 
DVD player in the upper left and upper middle. 
In this image, the pink dots are the Apex DVD 
reviews. 
 
 
Figure 4. Thematic clustering of product review 
 
The affect measurements on these documents 
generate five clusters in our system, each of 
which is summarized with a rose plot showing 
affect variation. This gives us information on the 
range and distribution of affect overall in this 
data. We can select one of these plots, either to 
review the documents or to interact further. Se-
lection is indicated with a green border, as shown 
in the upper middle plot of Figure 5.  
 
 
Figure 5. Clusters by affect, with one cluster 
glyph selected. 
 
 
The selected documents are relatively positive; 
they have higher scores in the Positive and Vir-
tue axes and lower scores in the Negative axis. 
We may want to see how the documents in this 
27
affect cluster distribute over the five products. 
This question is answered by the correlation tool, 
shown in Figure 6; the positive affect cluster 
contains more reviews on the Zen MP3 player 
than any of the other products. 
 
 
Figure 6. Products represented in one of the posi-
tive affect clusters. 
 
Alternatively we could get a summary of af-
fect per product.  Figure 7 shows the affect for 
the Apex DVD player and the Nokia cell phone. 
While both are positive, the Apex has stronger 
negative ratings than the Nokia. 
 
 
Figure 7. Comparison of Affect Scores of Nokia 
to Apex 
 
More detail is apparent by looking at the clus-
ters within one or more groups and examining 
the deviations. Figure 8 shows the sub-clusters 
within the Apex group. We include the summary 
for the group as a whole (directly beneath the 
Apex label), and then show the four sub-clusters 
by illustrating how they deviate from expected 
value. We see that two of these tend to be more 
positive than expected and two are more negative 
than expected. 
 
 
Figure 8. Summary of Apex products with sub-
clusters showing deviations. 
 
 
 
Figure 9. Thematic distribution of reviews for 
one product (Apex). 
 
Looking at the thematic distribution among 
the Apex documents shows topics that dominate 
its reviews (Figure 9). 
We can examine the affect across these vari-
ous clusters. Figure 10 shows the comparison of 
the ?service? cluster to the ?dvd player picture? 
cluster. This graphic demonstrates that docu-
ments with ?service? as a main theme tend to be 
much more negative, while documents with ?pic-
ture? as a main theme are much more positive.  
 
28
 
Figure 10. Affect summary and variation for 
?service? cluster and ?picture? cluster. 
 
The visualization tool includes a document 
viewer so that any selection of documents can be 
reviewed. For example, a user may be interested 
in why the ?service? documents tend to be nega-
tive, in which case they can review the original 
reviews. The doc viewer, shown in Figure 11, 
can be used at any stage in the process with any 
number of documents selected. Individual docu-
ments can be viewed by clicking on a document 
title in the upper portion of the doc viewer.  
 
Figure 11: The Doc Viewer. 
 
In this case study, we have illustrated the use-
fulness of visualizing the emotional content of a 
document collection. Using the tools presented 
here, we can summarize the dataset by saying 
that in general, the customer reviews are positive 
(Figure 5), but reviews for some products are 
more positive than others (Figures 6 and 7). In 
addition to the general content of the reviews, we 
can narrow our focus to the features contained in 
the reviews. We saw that while reviews for Apex 
are generally positive (Figure 8), reviews about 
Apex ?service? tend to be much more negative 
than reviews about Apex ?picture? (Figure 10).  
5 Evaluation 
IN-SPIRE is a document visualization tool that is 
designed to explore the thematic content of a 
large collection of documents. In this paper, we 
have described the added functionality of explor-
ing affect as one of the possible dimensions. As 
an exploratory system, it is difficult to define 
appropriate evaluation metric. Because the goal 
of our system is not to discretely bin the docu-
ments into affect categories, traditional metrics 
such as precision are not applicable. However, to 
get a sense of the coverage of our lexicon, we did 
compare our measurements to the hand annota-
tions provided for the customer review dataset.  
The dataset had hand scores (-3-3) for each 
feature contained in each review. We summed 
these scores to discretely bin them into positive 
(>0) or negative (<0). We did this both at the 
feature level and the review level (by looking at 
the cumulative score for all the features in the 
review). We compared these categorizations to 
the scores output by our measurement tool. If a 
document had a higher proportion of positive 
words than negative, we classified it as positive, 
and negative if it had a higher proportion of 
negative words. Using a chi-square, we found 
that the categorizations from our system were 
related with the hand annotations for both the 
whole reviews (chi-square=33.02, df=4, 
p<0.0001) and the individual features (chi-
square=150.6, df=4, p<0.0001), with actual 
agreement around 71% for both datasets. While 
this number is not in itself impressive, recall that 
our lexicon was built independently of the data 
for which is was applied. W also expect some 
agreement to be lost by conflating all scores into 
discrete bins, we expect that if we compared the 
numeric values of the hand annotations and our 
scores, we would have stronger correlations. 
These scores only provide an indication that 
the lexicon we used correlates with the hand an-
notations for the same data. As an exploratory 
system, however, a better evaluation metric 
would be a user study in which we get feedback 
on the usefulness of this capability in accom-
plishing a variety of analytical tasks. IN-SPIRE 
is currently deployed in a number of settings, 
both commercial and government. The added 
capabilities for interactively exploring affect 
have recently been deployed. We plan to conduct 
a variety of user evaluations in-situ that focus on 
its utility in a number of different tasks. Results 
of these studies will help steer the further devel-
opment of this methodology. 
29
6 Conclusion 
We have developed a measurement and visuali-
zation approach to affect that we expect to be 
useful in the context of the IN-SPIRE text analy-
sis toolkit. Our innovations include the flexibility 
of the lexicons used, the measurement options, 
the bootstrapping method and utility for lexicon 
development, and the visualization of affect us-
ing rose plots and interactive exploration in the 
context of an established text analysis toolkit. 
While the case study presented here was con-
ducted in English, all tools described are lan-
guage independent and we have begun exploring 
and creating lexicons of affect bearing words in 
multiple languages.  
 
References 
A. Aue.  & M. Gamon. 2005. Customizing Senti-
ment Classifiers to New Domains: a Case Study. 
Submitted  RANLP.  
S. Deerwester, S.T. Dumais, T.K. Landauer, G.W. 
Furnas, and R.A. Harshman. 1990. Indexing by La-
tent Semantic Analysis. Journal of the Society for 
Information Science, 41(6):391?407.  
 J. M. DiMicco, V. Lakshmipathy, A. T. Fiore. 2002. 
Conductive Chat: Instant Messaging With a Skin 
Conductivity Channel. In Proceedings of Confer-
ence on  Computer Supported Cooperative Work. 
D. G. Feitelson. 2003. Comparing Partitions with Spie 
Charts. Technical Report 2003-87, School of Com-
puter Science and Engineering, The Hebrew Uni-
versity of Jerusalem.  
E. Hetzler and A. Turner. 2004. Analysis Experiences 
Using Information Visualization. IEEE Computer 
Graphics and Applications, 24(5):22-26, 2004. 
M. Hu and B. Liu. 2004. Mining Opinion Features in 
Customer Reviews. In Proceedings of Nineteenth 
National Conference on Artificial Intelligence 
(AAAI-2004). 
H. Lieberman, H. Liu, P. Singh and B. Barry. 2004. 
Beating Common Sense into Interactive Applica-
tions. AI Magazine 25(4): Winter 2004, 63-76. 
B. Liu, M. Hu and J. Cheng. 2005. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th international World Wide 
Web conference (WWW-2005), May 10-14, 2005: 
Chiba, Japan. 
H. Liu, T. Selker, H. Lieberman. 2003. Visualizing 
the Affective Structure of a Text Document. Com-
puter Human Interaction, April 5-10, 2003: Fort 
Lauderdale. 
M. Mandic and A. Kerne. 2004. faMailiar?Intimacy-
based Email Visualization. In Proceedings of IEEE 
Information Visualization 2004, Austin Texas, 31-
32. 
B. Pang and L. Lee. 2004. A Sentimental Education: 
Sentiment Analysis Using Subjectivity Summariza-
tion Based on Minimum Cuts. In Proceedings of 
the 42nd ACL, pp. 271-278, 2004.  
Y. Qu,, J. Shanahan, and J. Weibe. 2004. Exploring 
Attitude and Affect in Text: Theories and Applica-
tions. Technical Report SS-04-07.  
E. Riloff and R. Jones. 1999.  Learning Dictionaries 
for Information Extraction by Multi-Level Boot-
strapping. Proceedings of the Sixteenth National 
Conference on Artificial Intelligence (AAAI-99) 
pp. 474-479. 
H. Scheff?. 1999. The Analysis of Variance, Wiley-
Interscience. 
P. Stone. 1977. Thematic Text Analysis: New Agen-
das for Analyzing Text Content. In Text Analysis 
for the Social Sciences, ed. Carl Roberts, Lawrence 
Erlbaum Associates. 
A. Tat and S. Carpendale. 2002. Visualizing Human 
Dialog. In Proceedings of IEEE Conference on In-
formation Visualization, IV'02, p.16-24, London, 
UK.  
P. Turney and M. Littman. 2003. Measuring Praise 
and Criticism: Inference of Semantic Orientation 
from Association. ACM Transactions on Informa-
tion Systems (TOIS) 21:315-346. 
H. Wainer. 1997. A Rose by Another Name.? Visual 
Revelations, Copernicus Books, New York. 
H. Wang, H. Prendinger, and T. Igarashi. 2004. Com-
municating Emotions in Online Chat Using Physio-
logical Sensors and Animated Text.? In Proceed-
ings of ACM SIGCHI Conference on Human Fac-
tors in Computing Systems (CHI'04), Vienna, Aus-
tria, April 24-29. 
J. Wiebe and Ellen Riloff. 2005. Creating Subjective 
and Objective Sentence Classifiers from Unanno-
tated Texts.? In Proceedings of Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics. 
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis.?  In Proceeding of HLT-EMNLP-
2005.  
 
 
30
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 25?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
 
Integrating Ontological Knowledge and Textual Evidence in Estimating 
Gene and Gene Product Similarity 
 
Antonio Sanfilippo, Christian Posse, Banu Gopalan, Stephen Tratz, Michelle Gregory 
Pacific Northwest National Laboratory 
Richland, WA 99352 
{Antonio.Sanfilippo, Christian.Posse, Banu.Gopalan, Stephen.Tratz, 
Michelle.Gregory}@pnl.gov  
 
 
  
 
Abstract 
With the rising influence of the Gene On-
tology, new approaches have emerged 
where the similarity between genes or 
gene products is obtained by comparing 
Gene Ontology code annotations associ-
ated with them. So far, these approaches 
have solely relied on the knowledge en-
coded in the Gene Ontology and the gene 
annotations associated with the Gene On-
tology database. The goal of this paper is 
to demonstrate that improvements to these 
approaches can be obtained by integrating 
textual evidence extracted from relevant 
biomedical literature. 
1 Introduction 
The establishment of similarity between genes and 
gene products through homology searches has be-
come an important discovery procedure that biolo-
gists use to infer structural and functional 
properties of genes and gene products?see Chang 
et al (2001) and references therein. With the rising 
influence of the Gene Ontology1 (GO), new ap-
proaches have emerged where the similarity be-
tween genes or gene products is obtained by 
comparing GO code annotations associated with 
them. The Gene Ontology provides three orthogo-
nal networks of functional genomic concepts struc-
                                                          
1 http://www.geneontology.org. 
tured in terms of semantic relationships such as 
inheritance and meronymy, which encode biologi-
cal process (BP), molecular function (MF) and cel-
lular component (CC) properties of genes and gene 
products. GO code annotations explicitly relate 
genes and gene products in terms of participation 
in the same/similar biological processes, presence 
in the same/similar cellular components and ex-
pression of the same/similar molecular functions. 
Therefore, the use of GO code annotations in es-
tablishing gene and gene product similarity pro-
vides significant added functionality to methods 
such as BLAST (Altschul et al 1997) and FASTA 
(Pearson and Lipman 1988) where gene and gene 
product similarity is calculated using string-based 
heuristics to select maximal segment pair align-
ments across gene and gene product sequences to 
approximate the Smith-Waterman algorithm 
(Smith and Waterman 1981). 
Three main GO-based approaches have emerged 
so far to compute gene and gene product similarity. 
One approach assesses GO code similarity in terms 
of shared hierarchical relations within each gene 
ontology (BP, MF, or CC) (Lord et al 2002, 2003; 
Couto et al 2003; Azuaje et al 2005).  For exam-
ple, the relative semantic closeness of two biologi-
cal processes would be determined by the 
informational specificity of the most immediate 
parent that the two biological processes share in 
the BP ontology. The second approach establishes 
GO code similarity by leveraging associative rela-
tions across the three gene ontologies (Bodenreider 
et al 2005). Such associative relations make pre-
dictions such as which cellular component is most 
likely to be the location of a given biological proc-
25
ess and which molecular function is most likely to 
be involved in a given biological process. The third 
approach computes GO code similarity by combin-
ing hierarchical and associative relations (Posse et 
al. 2006). 
Several studies within the last few years 
(Andrade et al 1997, Andrade 1999, MacCallum et 
al. 2000, Chang at al. 2001) have shown that the 
inclusion of evidence from relevant scientific lit-
erature improves homology search. It is therefore 
highly plausible that literature evidence can also 
help improve GO-based approaches to gene and 
gene product similarity. Sanfilippo et al (2004) 
propose a method for integrating literature evi-
dence within an early version of the GO-based 
similarity algorithm presented in Posse et al 
(2006). However, no effort has been made so far in 
evaluating the potential contribution of textual evi-
dence extracted from relevant biomedical literature 
for GO-based approaches to the computation of 
gene and gene product similarity. The goal of this 
paper is to address this gap with specific reference 
to the assessment of protein similarity. 
2 Background 
GO-based similarity methods that focus on meas-
uring intra-ontological relations have adopted the 
information theoretic treatment of semantic simi-
larity developed in Natural Language Process-
ing?see Budanitsky (1999) for an extensive 
survey. An example of such a treatment is given by 
Resnik (1995), who defines semantic similarity 
between two concept nodes c1 c2 in a graph as the 
information content of the least common su-
perordinate (lcs) of c1 and c2, as shown in (1). The 
information content of a concept node c, IC(c), is 
computed as -log p(c) where p(c) indicates the 
probability of encountering instances of c in a spe-
cific corpus. 
(1)     
)),c p(lcs(c
)),c IC(lcs(c) ,csim(c
21log
2121
?=
==
Jiang and Conrath (1997) provide a refinement of 
Resnik?s measure by factoring in the distance from 
each concept to the least common superordinate, as 
shown in (2).2
                                                          
2 Jiang and Conrath (1997) actually define the distance be-
tween two concepts nodes c1 c2, e.g.     
 )), c IC(lcs(c ) -  IC(c)  IC(c) , cdist(c 2122121 ?+=
(2)
)),cIC(lcs(c) -IC(c)IC(c ) ,csim(c 21221
121 ?+=  
Lin (1998) provides a slight variant of Jiang?s and 
Conrath?s measure, as indicated in (3).  
(3) 
)  IC(c) IC(c
)), c IC(lcs(c 
 ) ,csim(c
21
212
21 +
?=  
The information theoretic approach is very well 
suited to assess GO code similarity since each gene 
subontology is formalized as a directed acyclic 
graph. In addition, the GO database3 includes nu-
merous curated GO annotations which can be used 
to calculate the information content of each GO 
code with high reliability. Evaluations of this 
methodology have yielded promising results. For 
example, Lord et al (2002, 2003) demonstrate that 
there is strong correlation between GO-based simi-
larity judgments for human proteins and similarity 
judgments obtained through BLAST searches for 
the same proteins. Azuaje et al (2005) show that 
there is a strong connection between the degree of 
GO-based similarity and the expression correlation 
of gene products. 
As Bodenreider et al (2005) remark, the main 
problem with the information theoretic approach to 
GO code similarity is that it does not take into ac-
count associative relations across the gene ontolo-
gies. For example, the two GO codes 0050909 
(sensory perception of taste) and 0008527 (taste 
receptor activity) belong to different gene ontolo-
gies (BP and MF), but they are undeniably very 
closely related. The information theoretic approach 
would simply miss associations of this kind as it is 
not designed to capture inter-ontological relations.  
Bodenreider et al (2005) propose to recover as-
sociative relations across the gene ontologies using 
a variety of statistical techniques which estimate 
the similarity of two GO codes inter-ontologically 
in terms of the distribution of the gene product an-
notations associated with the two GO codes in the 
GO database. One such technique is an adaptation 
of the vector space model frequently used in In-
formation Retrieval (Salton et al 1975), where 
                                                                                           
For ease of exposition, we have converted Jiang?s and Con-
rath?s semantic distance measure to semantic similarity by 
taking its inverse, following Pedersen et al (2005). 
3 http://www.godatabase.org/dev/database.  
26
each GO code is represented as a vector of gene-
based features weighted according to their distribu-
tion in the GO annotation database, and the simi-
larity between two GO codes is computed as the 
cosine of the vectors for the two codes. 
The ability to measure associative relations 
across the gene ontologies can significantly aug-
ment the functionality of the information theoretic 
approach so as to provide a more comprehensive 
assessment of gene and gene product similarity. 
However, in spite of their complementarities, the 
two GO code similarity measures are not easily 
integrated. This is because the two measures are 
obtained through different methods, express dis-
tinct senses of similarity (i.e. intra- and inter-
ontological) and are thus incomparable.  
Posse et al (2006) develop a GO-based similar-
ity algorithm?XOA, short for Cross-Ontological 
Analytics?capable of combining intra- and inter-
ontological relations by ?translating? each associa-
tive relation across the gene ontologies into a hier-
archical relation within a single ontology. More 
precisely, let c1 denote a GO code in the gene on-
tology O1 and c2 a GO code in the gene ontology 
O2. The XOA similarity between c1 and c2 is de-
fined as shown in (4), where4
? cos(ci,cj) denotes the cosine associative meas-
ure proposed by Bodenreider et al (2005) 
? sim(ci,cj) denotes any of the three intra-
ontological semantic similarities described 
above, see (1)-(3) 
? maxci in Oj {f(ci)} denotes the maximum of the 
function f() over all GO codes ci in the gene 
ontology Oj.  
The major innovation of the XOA approach is to 
allow the comparison of two nodes c1, c2 across 
distinct ontologies O1, O2 by mapping c1 into its 
closest node c4 in O2 and c2 into its closest node 
c3 in O1. The inter-ontological semantic similarity 
between c1 and c2 can be then estimated from the 
intra-ontological semantic similarities between c1-
                                                          
4 If c1 and c2 are in the same ontology, i.e. O1=O2, then 
xoa(c1,c2) is still computed as in (4). In most cases, the 
maximum in (4) would be obtained with c3 = c2 and c4 = c1 
so that  XOA(c1,c2) would simply be computed as sim(c1,c2). 
However, there are situations where there exists a GO code c3 
(c4) in the same ontology which 
? is highly associated with c1 (c2),  
?  is semantically close to c2 (c1), and  
?  leads to a value for  sim(c1,c3) x cos(c2,c3)  ((sim(c2,c4) 
x cos(c1,c4)) that is higher than sim(c1,c2). 
c3 and c2-c4, using multiplication with the associa-
tive relations between c2-c3 and c1-c4 as a score 
enrichment device. 
 
(4)  
??
?
?
??
?
?
?
??
?
?
??
?
?
?
?
?
??
??
?
??
??
?
??
??
?
??
??
?
=
), c(c
), c(c
), c(c
), c(c
Oinc
Oinc
,
), c(c
41cos
42sim
32cos
31sim
XOA
24
13
max
max
max21  
 
Posse et al (2006) show that the XOA similarity 
measure provides substantial advantages. For ex-
ample, a comparative evaluation of protein similar-
ity, following the benchmark study of Lord et al 
(2002, 2003), reveals that XOA provides the basis 
for a better correlation with protein sequence simi-
larities as measured by BLAST bit score than any 
intra-ontological semantic similarity measure. The 
XOA similarity between genes/gene products de-
rives from the XOA similarity between GO codes. 
Let GP1 and GP2 be two genes/gene products. Let 
c11,c12,?, c1n denote the set of GO codes associ-
ated with GP1 and c21, c22,?., c2m the set of GO 
codes associated with GP2. The XOA similarity 
between GP1 and GP2 is defined as in (5), where 
i=1,?,n and j=1,?,m.  
 
(5) XOA(GP1,GP2) = max {XOA(c1i, c2j)} 
 
The results of the study by Posse et al (2006) are 
shown in Table 1. Note that the correlation be-
tween protein similarities based on intra-
ontological similarity measures and BLAST bit 
scores in Table 1 is given for each choice of gene 
ontology (MF, BP, CC). This is because intra-
ontological similarity methods only take into ac-
count GO codes that are in the same ontology and 
can therefore only assess protein similarity from a 
single ontology viewpoint. By contrast, the XOA-
based protein similarity measure makes use of GO 
codes that can belong to any of the three gene on-
tologies and needs not be broken down by single 
ontologies, although the contribution of each gene 
ontology or even single GO codes can still be 
fleshed out, if so desired. 
Is it possible to improve on these XOA results 
by factoring in textual evidence? We will address 
this question in the remaining part of the paper. 
27
 
Semantic Similarity 
Measures 
Resnik Lin Jiang &  
Conrath 
Intra-ontological    
Molecular Function 0.307 0.301 0.296 
Biological Process 0.195 0.202 0.203 
Cellular Component 0.229 0.234 0.233 
XOA 0.405 0.393 0.368 
Table 1: Spearman rank order correlation coeffi-
cients between BLAST bit score and semantic 
similarities, calculated using a set of 255,502 pro-
tein pairs?adapted from Posse et al (2006). 
3 Textual Evidence Selection 
Our first step in integrating textual evidence into 
the XOA algorithm is to select salient information 
from biomedical literature germane to the problem. 
Several approaches can be used to carry out this 
prerequisite. For example, one possibility is to col-
lect documents relevant to the task at hand, e.g. 
through PubMed queries, and use feature weight-
ing and selection techniques from the Information 
Retrieval literature?e.g. tf*idf (Buckley 1985) and 
Information Gain (e.g. Yang and Pedersen 
1997)?to distill the most relevant information. An-
other possibility is to use Information Extraction 
algorithms tailored to the biomedical domain such 
as Medstract (http://www.medstract.org, Puste-
jovsky et al 2002) to extract entity-relationship 
structures of relevance. Yet another possibility is to 
use specialized tools such as GoPubMed (Doms 
and Schroeder 2005) where traditional keyword-
based capabilities are coupled with term extraction 
and ontological annotation techniques.  
In our study, we opted for the latter solution, us-
ing generic Information Retrieval techniques to 
normalize and weigh the textual evidence ex-
tracted. The main advantage of this choice is that 
tools such as GoPubMed provide very high quality 
term extraction at no cost. Less appealing is the 
fact that the textual evidence provided is GO-based 
and therefore does not offer information which is 
orthogonal to the gene ontology. It is reasonable to 
expect better results than those reported in this pa-
per if more GO-independent textual evidence were 
brought to bear. We are currently working on using 
Medstract as a source of additional textual evi-
dence. 
GoPubMed is a web server which allows users 
to explore PubMed search results using the Gene 
Ontology for categorization and navigation pur-
poses (available at http://www.gopubmed.org). As 
shown in Figure 1 below, the system offers the 
following functionality: 
? It provides an overview of PubMed search re-
sults by categorizing abstracts according to the 
Gene Ontology 
? It verifies its classification by providing an 
accuracy percentage for each 
? It shows definitions of Gene Ontology terms 
? It allows users to navigate PubMed search re-
sults by GO categories 
? It automatically shows GO terms related to the 
original query for each result  
? It shows query terms (e.g. ?Rab5? in the mid-
dle windowpane of Figure 1) 
? It automatically extracts terms from search 
results which map to GO categories (e.g. high-
lighted terms other than ?Rab5?  in the middle 
windowpane of Figure 1). 
In integrating textual evidence with the XOA al-
gorithm, we utilized the last functionality (auto-
matic extraction of terms) as an Information 
Extraction capability. Details about the term ex-
traction algorithm used in GoPubMed are given in 
Delfs et al (2004). In short, the GoPubMed term 
extraction algorithm uses word alignment strate-
gies in combination with stemming to match word 
sequences from PubMed abstracts with GO terms. 
In doing so, partial and discontinuous matches are 
allowed. Partial and discontinuous matches are 
weighted according to closeness of fit. This is indi-
cated by the accuracy percentages associated with 
GO in Figure 1 (right side). In this study we did 
not make use of these accuracy percentages, but 
plan to do so in the future. 
28
 
Figure 1: GoPubMed sample query for the ?rab5? protein. The abstracts shown are automatically proposed by the 
system after the user issues the protein query and then selects the GO term ?late endosome? (bottom left) as the 
discriminating parameter. 
  
Our data set consists of 2360 human protein 
pairs containing 1783 distinct human proteins. This 
data set was obtained as a 1% random sample of 
the human proteins used in the benchmark study of 
Posse et al (2006)?see Table 1.5 For each of the 
1783 human proteins, we made a GoPubMed query 
and retrieved up to 100 abstracts. We then col-
lected all the terms extracted by GoPubMed for 
each protein across the abstracts retrieved. Table 2 
provides an example of the output of this process. 
 
nutrient, uptake, carbohydrate, metabolism, affect-
ing, cathepsin, activity, protein, lipid, growth, rate, 
habitually, signal, transduction, fat, protein, cad-
herin, chromosomal, responses, exogenous, lactat-
ing, exchanges, affects, mammary, gland, ?. 
Table 2: Sample output of the GoPubMed term extrac-
tion process for the Cadherin-related tumor suppressor 
protein. 
                                                          
5 We chose such a small sample to facilitate the collection of 
evidence from GoPubMed, which is not yet fully automated. 
Our XOA approach is very scalable, and we do not anticipate 
any problem running the full protein data set of 255,502 pairs, 
once we fully automate the GoPubMed extraction process. 
4 Integrating Textual Evidence in XOA 
Using the output of the GoPubMed term extraction 
process, we created vector-based signatures for 
each of the 1783 proteins, where  
? features are obtained by stemming the terms 
provided by GoPubMed  
? the value for each feature is derived as the 
tf*idf  for the feature. 
We then calculated the similarity between each of 
the 2360 protein pairs as the cosine value of the 
two vector-based signatures associated with the 
protein pair. 
We tried two different strategies to augment the 
XOA score for protein similarity using the protein 
similarity values obtained as the cosine of the 
GoPubMed term-based signatures. The first strat-
egy adopts a fusion approach in which the two 
similarity measures are first normalized to be 
commensurable and then combined to provide an 
interpretable integrated model. A simple normali-
zation is obtained by observing that the Resnik?s 
information content measure is commensurable to 
29
the log of the text based cosine (LC). This leads us 
to the fusion model shown in (5) for XOA, based 
on Resnik?s semantic similarity measure (XOAR). 
(5)      Fusion(Resnik) = XOAR + LC 
We then observe that the XOA measures based on 
Resnik, Lin (XOAL) and Jiang & Conrath (XOAJC) 
are highly correlated (correlations exceed 0.95 on 
the large benchmarking dataset discussed in sec-
tion 2, see Table 1). This suggests the fusion model 
shown in (6), where the averages of the XOA 
scores are computed from the benchmarking data 
set. 
(6)      Fusion(Lin) =  
                XOAL + LC*Ave( XOAL)/Ave(XOAR) 
          Fusion(Jiang & Conrath) =  
               XOAJC + LC*Ave(XOAJC)/Ave(XOAR) 
The second strategy consists in building a predic-
tion model for BLAST bit score (BBS) using the 
XOA score and the log-cosine LC as predictors 
without the constraint of remaining interpretable. 
As in the previous strategy, a different model was 
sought for each of the three XOA variants. In each 
case, we restrict ourselves to cubic polynomial re-
gression models as such models are quite efficient 
at capturing complex nonlinear relationships be-
tween target and predictors (e.g. Weisberg 2005). 
More precisely, for each of the semantic similarity 
measures, we fit the regression model to BBS 
shown in (7), where the subscript x denotes either 
R, L or JC, and the coefficients a to h are found by 
maximizing the Spearman rank order correlations 
between BBS and the regression model. This 
maximization is automatically carried out by using 
a random walk optimization approach (Romeijn 
1992). The coefficients used in this study for each 
semantic similarity measure are shown in Table 3. 
 
(7)    a*XOAx + b*XOAx2 + c*XOAx +  d*LC 
        + e*LC2 + f*LC3 +  g*XOAx*LC 
5 Evaluation 
Table 4 summarizes the results for both strategies, 
comparing Spearman rank correlations between 
BBS and the models from the fusion and regres-
sion approaches with Spearman rank correlations 
between BBS and XOA alone. Note that the latter 
correlations are lower than the one reported in Ta-
ble 2 due to the small size of our sample (1% of the 
original data set, as pointed out above). P-values 
associated with the changes in the correlation val-
ues are also reported, enclosed in parentheses.  
 
 Resnik Lin Jiang & Conrath 
a -10684.43 2.83453e-05 0.2025174 
b 1.786986 -31318.0 -1.93974 
c 503.3746 45388.66 0.08461453 
d -3.952441 208.5917 4.939535e-06 
e 0.0034074 1.55518e-04 0.0033902 
f 1.4036e-05 9.972911e-05 -0.000838812 
g 713.769 -1.10477e-06 2.461781 
Table 3: Coefficients of the regression model maximiz-
ing Spearman rank correlation between BBS and the 
regression model for each of the three semantic similar-
ity measures. 
 
 XOA Fusion Regression 
Resnik 0.295 0.325 (>0.20) 0.388 (0.0008) 
Lin 0.274 0.301 (>0.20) 0.372 (0.0005) 
Jiang & 
Conrath 0.273 0.285 (>0.20) 0.348 (0.008) 
Table 4: Spearman rank order correlation coefficients 
between BLAST bit score BBS and XOA, BBS and the 
fusion model, and BBS and the regression model. P-
values for the differences between the augmented mod-
els and XOA alone are given in parentheses. 
 
An important finding from Table 4 is that inte-
grating text-based evidence in the semantic simi-
larity measures systematically improves the 
relationships between BLAST and XOA. Not sur-
prisingly, the fusion models yield smaller im-
provements. However, these improvements in the 
order of 3% for the Resnik and Lin variants are 
very encouraging, even though they are not statis-
tically significant. The regression models, on the 
other hand, provide larger and statistically signifi-
cant improvements, reinforcing our hypothesis that 
textual evidence complements the GO-based simi-
larity measures. We expect that a more sophisti-
cated NLP treatment of textual evidence will yield 
significant improvements even for the more inter-
pretable fusion models.   
Conclusions and Further Work 
Our early results show that literature evidence pro-
vides a significant contribution, even using very 
simple Information Extraction and integration 
methods such as those described in this paper. The 
employment of more sophisticated Information 
30
Extraction tools and integration techniques is 
therefore likely to bring higher gains.  
Further work using GoPubMed involves factor-
ing in the accuracy percentage which related ex-
tracted terms to their induced GO categories and 
capturing complex phrases (e.g. signal transduc-
tion, fat protein). We also intend to compare the 
advantages provided by the GoPubMed term ex-
traction process with Information Extraction tools 
created for the biomedical domain such as Med-
stract (Pustejovsky et al 2002), and develop a 
methodology for integrating a variety of Informa-
tion Extraction processes into XOA. 
References  
Altschul, S.F., T. L. Madden, A. A. Schaffer, J. Zhang, 
Z. Anang, W. Miller and D.J. Lipman (1997) Gapped 
BLAST and PSI-BLST: a new generation of protein 
database search programs.  Nucl. Acids Res. 25:3389-
3402. 
Andrade, M.A. (1999) Position-specific annotation of 
protein function based on multiple homologs. ISMB 
28-33. 
Andrade, M.A. and A. Valencia (1997) Automatic an-
notation for biological sequences by extraction of 
keywords from MEDLINE abstracts. Development 
of a prototype system. ISMB 25-32. 
Azuaje F., H. Wang and O. Bodenreider (2005) Ontol-
ogy-driven similarity approaches to supporting gene 
functional assessment. In Proceedings of the 
ISMB'2005 SIG meeting on Bio-ontologies 2005, 
pages 9-10.  
Bodenreider, O., M. Aubry and A. Burgun (2005) Non-
lexical approaches to identifying associative relations 
in the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 104-115. 
Buckley, C. (1985) Implementation of the SMART in-
formation retrieval system. Technical Report 85-686, 
Cornell University. 
Budanitsky, A. (1999) Lexical semantic relatedness and 
its application in natural language processing. Tech-
nical report CSRG-390, Department of Computer 
Science, University of Toronto. 
Chang, J.T., S. Raychaudhuri, and R.B. Altman (2001) 
Including biological literature improves homology 
search. In Proc. Pacific Symposium on Biocomput-
ing, pages 374?383. 
Couto, F. M., M. J. Silva and P. Coutinho (2003) Im-
plementation of a functional semantic similarity 
measure between gene-products. Technical Report, 
Department of Informatics, University of Lisbon, 
http://www.di.fc.ul.pt/tech-reports/03-29.pdf.  
Delfs, R., A. Doms, A. Kozlenkov, and M. Schroeder. 
(2004) GoPubMed: ontology based literature search 
applied to Gene Ontology and PubMed. In Proc. of 
German Bioinformatics Conference, Bielefeld, Ger-
many. LNBI Springer. 
Doms, A. and M. Schroeder (2005) GoPubMed: Explor-
ing PubMed with the GeneOntology. Nucleic Acids 
Research. 33: W783-W786; doi:10.1093/nar/gki470. 
Jiang J. and D. Conrath (1997) Semantic similarity 
based on corpus statistics and lexical taxonomy. In 
Proceedings of International Conference on Re-
search in Computational Linguistics, Taiwan. 
Romeijn, E.H. (1992) Global Optimization by Random 
Walk Sampling Methods. Tinbergen Institute Re-
search Series, Volume 32. Thesis Publishers, Am-
sterdam. 
Lin, D. (1998) An information-theoretic definition of 
similarity. In Proceedings of the 15th International 
Conference on Machine Learning, Madison, WI. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2002) Investigating semantic similarity measures 
across the Gene Ontology: the relationship between 
sequence and annotation. Bioinformatics 
19(10):1275-1283. 
Lord P.W., R.D. Stevens, A. Brass, and C.A. Goble 
(2003) Semantic similarity measures as tools for ex-
ploring the Gene Ontology. In Proceedings of Pacific 
Symposium on Biocomputing, pages 601-612. 
MacCallum, R. M., L. A. Kelley and Sternberg, M. J. 
(2000) SAWTED: structure assignment with text de-
scription--enhanced detection of remote homologues 
with automated SWISS-PROT annotation compari-
sons. Bioinformatics 16, 125-9. 
Pearson, W. R. and D. J. Lipman (1988) Improved tools 
for biological sequence analysis. In Proceedings of 
the National Academy of Sciences 85:2444-2448.  
Pedersen, T., S. Banerjee and S. Patwardhan (2005) 
Maximizing Semantic Relatedness to Perform Word 
Sense Disambiguation. University of Minnesota Su-
percomputing Institute Research Report UMSI 
2005/25, March. Available at http://www.msi.umn. 
edu/general/Reports/rptfiles/2005-25.pdf.  
Posse, C., A. Sanfilippo, B. Gopalan, R. Riensche, N. 
Beagley, and B. Baddeley (2006) Cross-Ontological 
Analytics: Combining associative and hierarchical re-
lations in the Gene Ontologies to assess gene product 
similarity. To appear in Proceedings of International 
31
Workshop on Bioinformatics Research and Applica-
tions. Reading, U.K. 
Pustejovsky, J., J. Casta?o, R. Saur?, A. Rumshisky, J. 
Zhang, W. Luo (2002) Medstract: Creating large-
scale information servers for biomedical libraries. 
ACL 2002 Workshop on Natural Language Process-
ing in the Biomedical Domain. Philadelphia, PA. 
Resnik, P. (1995) Using information content to evaluate 
semantic similarity. In Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence, 
pages 448?453, Montreal. 
Sanfilippo A., C. Posse and B. Gopalan (2004) Aligning 
the Gene Ontologies. In Proceedings of the Stan-
dards and Ontologies for Functional Genomics Con-
ference 2, Philadelphia, PA, http://www.sofg.org/ 
meetings/sofg2004/Sanfilippo.ppt.  
Salton, G., A. Wong and C. S. Yang (1975) A Vector 
space model for automatic indexing, CACM 
18(11):613-620. 
Smith, T. and M. S. Waterman (1981) Identification of 
common molecular subsequences. J. Mol. Biol. 
147:195-197. 
Weisberg, S. (2005) Applied linear regression. Wiley, 
New York. 
Yang, Y. and J.O. Pedersen (1997) A comparative 
Study on feature selection in text categorization. In 
Proceedings of the 14th International Conference on 
Machine Learning (ICML), pages 412-420, Nash-
ville.  
 
32
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 50?57,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
ChAT: A Time-Linked System for Conversational Analysis 
 
Michelle L. Gregory Douglas Love Stuart Rose Anne Schur 
Pacific Northwest National Laboratory 
609 Battelle Blvd 
Richland, WA 99354 
{michelle.gregory;douglas.love;stuart.rose;anne.schur}@pnl.gov 
 
Abstract 
We present a system for analyzing conver-
sational data. The system includes state-of-
the-art natural language processing compo-
nents that have been modified to accom-
modate the unique nature of conversational 
data. In addition, we leverage the added 
richness of conversational data by analyz-
ing various aspects of the participants and 
their relationships to each other. Our tool 
provides users with the ability to easily 
identify topics or persons of interest, in-
cluding who talked to whom, when, entities 
that were discussed, etc. Using this tool, 
one can also isolate more complex net-
works of information: individuals who may 
have discussed the same topics but never 
talked to each other. The tool includes a UI 
that plots information over time, and a se-
mantic graph that highlights relationships 
of interest.  
1 Introduction 
The ability to extract and summarize content from 
data is a fundamental goal of computational lin-
guistics. As such, a number of tools exist to auto-
matically categorize, cluster, and extract 
information from documents. However, these tools 
do not transfer well to data sources that are more 
conversational in nature, such as multi-party meet-
ings, telephone conversations, email, chat rooms, 
etc. Given the plethora of these data sources, there 
is a need to be able to quickly and accurately ex-
tract and process pertinent information from these 
sources without having to cull them manually.  
Much of the work on computational analysis of 
dialogue has focused on automatic topic segmenta-
tion of conversational data, and in particular, using 
features of the discourse to aid in segmentation 
(Galley et al 2003; Stolcke et al, 1999; 
Hirschberg & Hakatani, 1996.). Detailed discourse 
and conversational analytics have been the focus of 
much linguistic research and have been used by the 
computational community for creating models of 
dialogue to aid in natural language understanding 
and generation (Allen & Core, 1997; Carletta et al, 
1997; van Deemter et al, 2005; Walker et al, 
1996). However, there has been much less focus on 
computational tools that can aid in either the analy-
sis of conversations themselves, or in rendering 
conversational data in ways such that it can be 
used with traditional data mining techniques that 
have been successful for document understanding.  
This current work is most similar to the NITE 
XML Toolkit (Carletta & Kilgour, 2005) which 
was designed for annotating conversational data. 
NITE XML is system in which transcripts of con-
versations are viewable and time aligned with their 
audio transcripts. It is especially useful for adding 
annotations to multi-modal data formats. NITE 
XML is not analysis tool, however. Annotations 
are generally manually added. In this paper, we 
present a Conversational Analysis Tool (ChAT) 
which integrates several language processing tools 
(topic segmentation, affect scoring, named entity 
extraction) that can be used to automatically anno-
tate conversational data. The processing compo-
nents have been specially adapted to deal with 
conversational data.  
ChAT is not an annotation tool, however, it is 
analysis tool. It includes a UI that combines a vari-
ety of data sources onto one screen that enables 
users to progressively explore conversational data. 
For example, one can explore who was present in a 
50
given conversation, what they talked about, and the 
emotional content of the data. The data can be 
viewed by time slice or in a semantic graph. The 
language processing components in ChAT are ver-
satile in that they were developed in modular, open 
designs so that they can be used independently or 
be integrated into other analytics tools. We present 
ChAT architecture and processing components in 
Section 2. In section 3 we present the UI , with a 
discussion following in section 4.  
2 ChAT Architecture 
ChAT is a text processing tool designed to aid in 
the analysis of any kind of threaded dialogue, in-
cluding meeting transcripts, telephone transcripts, 
usenet groups, chat room, email or blogs. Figure 1 
illustrates the data processing flow in ChAT. 
 
 
 
Figure 1: ChAT Architecture. 
 
 Data is ingested via an ingest engine, then the 
central processing engine normalizes the format 
(time stamp, speaker ID, utterance; one utterance 
per line). Processing components are called by the 
central processing engine which provides the input 
to each component, and collects the output to send 
to the UI. 
We designed the system to be general enough to 
handle multiple data types. Thus, with the excep-
tion of the ingest engine, the processing compo-
nents are domain and source independent. For 
example, we did not want the topic segmentation 
to rely on features specific to a dataset, such as 
acoustic information from transcripts. Addition-
ally, all processing components have been built as 
independent plug-ins to the processing engine: The 
input of one does not rely on the output of the oth-
ers. This allows for a great deal of flexibility in 
that a user can choose to include or exclude vari-
ous processes to suit their needs, or even exchange 
the components with new tools. We describe each 
of the processing components in the next section. 
2.1 Ingest Engine 
The ingest engine is designed to input multiple 
data sources and transform them into a uniform 
structure which includes one utterance per line, 
including time stamp and participant information. 
So far, we have ingested three data sources. The 
ICSI meeting corpus (Janin et al, 2003) is a corpus 
of text transcripts of research meetings. There are 
75 meetings in the corpus, lasting 30 minutes to 
1.5 hours in duration, with 5-8 participants in each 
meeting. A subset of these meetings were hand 
coded for topic segments (Galley, et al, 2003). We 
also used telephone transcripts from the August 14, 
2003 power grid failure that resulted in a regional 
blackout1. These data consist of files containing 
transcripts of multiple telephone conversations be-
tween multiple parties. Lastly, we employed a chat 
room dataset that was built in-house by summer 
interns who were instructed to play a murder mys-
tery game over chat. Participants took on a charac-
ter persona as their login and content was based on 
a predefined scenario, but all interactions were un-
scripted beyond that. 
                                                          
1http://energycommerce.house.gov/108/Hearings/09032003hearing1061/hearing
.htm 
Ingest Engine User Interface 
Central Processing Engine 
Processing Components 
          
 
           
Topic Segmentation 
Participant 
Information 
Named Entity 
Extraction 
Affect 
51
  
Figure 2. Plot of WindowDiff evaluation metric for LCseg and WLM on meeting corpus. p-value = 
0.032121 for two-sample equal variance t-test. 
2.2 Topic Segmentation 
The output of the ingest process is a list of utter-
ance that include a time (or sequence) stamp, a 
participant name, and an utterance. Topic segmen-
tation is then performed on the utterances to chunk 
them into topically cohesive units. Traditionally, 
algorithms for segmentation have relied on textual 
cues (Hearst, 1997; Miller et al 1998; Beeferman 
et al 1999; Choi, 2000). These techniques have 
proved useful in segmenting single authored 
documents that are rich in content and where there 
is a great deal of topic continuity. Topic segmenta-
tion of conversational data is much more difficult 
due to often sparse content, intertwined topics, and 
lack of topic continuity. 
Topic segmentation algorithms generally rely on 
a lexical cohesion signal that requires smoothing in 
order to eliminate noise from changes of word 
choices in adjoining statements that do not indicate 
topic shifts (Hearst, 1997; Barzilay and Elhadad, 
1997). Many state of the art techniques use a slid-
ing window for smoothing (Hearst, 1997; Miller et 
al. 1998; Galley et al, 2003). We employ a win-
dowless method (WLM) for calculating a suitable 
cohesion signal which does not rely on a sliding 
window to achieve the requisite smoothing for an 
effective segmentation. Instead, WLM employs a 
constrained minimal-spanning tree (MST) algo-
rithm to find and join pairs of elements in a se-
quence. In most applications, the nearest-neighbor 
search used by an MST involves an exhaustive, 
O(N2), search throughout all pairs of elements. 
However since WLM only requires information on 
the distance between adjoining elements in the se-
quence the search space for finding the two closest 
adjoining elements is linear, O(N), where N is the 
number of elements in the sequence. We can there-
fore take advantage of the hierarchical summary 
structure that an MST algorithm affords while not 
incurring the performance penalty.  
Of particular interest for our research was the 
success of WLM on threaded dialogue. We evalu-
ated WLM?s performance on the ICSI meeting 
corpus (Janin et al 2003) by comparing our seg-
mentation results to the results obtained by imple-
menting LCSeg (Galley et al, 2003). Using the 25 
hand segmented meetings, our algorithm achieved 
a significantly better segmentation for 20 out of 25 
documents. Figure 2 shows the hypothesized seg-
ments from the two algorithms on the ICSI Meet-
ing Corpus. 
Topic segmentation of conversational data can 
be aided by employing features of the discourse or 
speech environment, such as acoustic cues, etc. 
(Stolcke et al, 1999; Galley et al, 2003). In this 
work, we have avoided using data dependent (the 
52
integration of acoustic cues for speech transcripts, 
for example) features to aid in segmentation be-
cause we wanted our system to be as versatile as 
possible. This approach provides the best segmen-
tation possible for a variety of data sources, regard-
less of data type.  
 
2.3 Named Entity Extraction  
In addition to topics, ChAT also has integrated 
software to extract the named entities. We use 
Cicero Lite from the Language Computer Corpora-
tion (LCC) for our entity detection (for a product 
description and evaluation, see Harabagiu et al, 
2003). Using a combination of semantic represen-
tations and statistical approaches, Cicero Lite iso-
lates approximately 80 entity types. ChAT 
currently makes use of only a handful of these 
categories, but can easily be modified to include 
more. Because named entity extraction relies on 
cross-utterance dependencies, the main processing 
engine sends all utterance from a conversation at 
once rather than an utterance at a time.  
2.4 Sentiment Analysis 
In addition to topic and entity extraction, conversa-
tions can also be analyzed by who participated in 
them and their relationship to one another and their 
attitude toward topics they discuss. In an initial 
attempt to capture participant attitude, we have 
included a sentiment analysis, or affect, compo-
nent. Sentiment analysis is conducted via a lexical 
approach. The lexicon we employed is the General 
Inquirer (GI) lexicon developed for content analy-
ses of textual data (Stone, 1977). It includes an 
extensive lexicon of over 11,000 hand coded word 
stems, and 182 categories, but our implementation 
is limited to positive (POS) and negative (NEG) 
axes. In ChAT, every utterance is scored for the 
number of positive and negative words it contains. 
We make use of this data by keeping track of the 
affect of topics in general, as well as the general 
mood of the participants.  
 
2.5 Participant Roles 
Analyzing conversations consists of more than 
analyzing the topics within them. Inherent to the 
nature of conversational data are the participants. 
Using textual cues, one can gain insight into the 
relationships of participants to each other and the 
topics. In ChAT we have integrated several simple 
metrics as indicators of social dynamics amongst 
the participants. Using simple speaker statistics, 
such as number of utterances, number of words, 
etc., we can gain insight to the level of engagement 
of participants in the conversation. Features we use 
include: 
? The number of utterance 
? Proportion of questions versus state-
ments 
? Proportion of ?unsolicited? statements 
(ones not preceded by a question mark) 
Additionally, we use the same lexical resources 
as we use for sentiment analysis for indications of 
personality type. We make use of the lexical cate-
gories of strong, weak, power cooperative, and 
power conflict as indicators of participant roles in 
the conversational setting.  Thus far, we have not 
conducted any formal evaluation on the sentiment 
analysis with this data, but our initial studies of our 
pos and neg categories show a 73% agreement 
with hand tagged positive and negative segments 
on a different data set.  
3 User Interface 
As described in Section 2 on ChAT architecture, 
the processing components are independent of the 
UI, but we do have a built-in UI that incorporates 
the processing components that is designed to aid 
in analyzing conversations.  
53
  
Figure 3. Screen shot of the main UI for ChAT  
 
The components of the system are all linked 
through the X-axis, representing time, as seen in 
Figure 3. Depending on the dataset, positions along 
the time axis  are based on either the time stamp or 
sequential position of the utterance. The default 
time range is the whole conversation or chat room 
session, but a narrower range can be selected by 
dragging in the interval panel at the top of the UI. 
Note that all of the values for each of the compo-
nents are recalculated based on the selected time 
interval. Figure 4 shows that a time selection re-
sults in a finer grained subset of the data, allowing 
one to drill down to specific topics of inter-
est.
 
Figure 4: Time Selection.  
 
54
The number of utterance for a given time frame 
is indicated by the number inside the box corre-
sponding to the time frame. The number is recalcu-
lated as different time frames are selected. 
3.1.1 Topics 
The central organizing unit in the UI is topics. The 
topic panel, detailed in Figure 5, consists of three 
parts: the color key, affect scores, and topic labels. 
Once a data file is imported into the UI, topic seg-
mentation is performed on the dataset according to 
the processes outline in Section 3.2. Topic labels 
are assigned to each topic chunk. Currently, we use 
the most prevalent word tokens as the label, and 
the user can control the number of words per label. 
Each topic segment is assigned a color, which is 
indicated by the color key. The persistence of a 
color throughout the time axis indicates which 
topic is being discussed at any given time. This 
allows a user to quickly see the distribution of top-
ics of a meeting, for example. It also allows a user 
to quickly see the participants who discussed a 
given topic. 
 
 
Figure 5. Topic Labels in the Topic Panel. 
 
3.1.2 Affect 
Affect scores are computed for each topic by 
counting the number of POS or NEG affect words 
in each utterance that comprises a topic within the 
selected time interval. Affect is measured by the 
proportion of POS to NEG words in the selected 
time frame. If the proportion is greater than 0, the 
score is POS (represented by a +), if it is less than 
0, it is NEG (-). The degree of sentiment is indi-
cated by varying shades of color on the + or ? 
symbol.  
Note that affect is computed for both topics and 
participants. An affect score on the topic panel in-
dicates overall affect contained in the utterances 
present in a given time frame, whereas the affect 
score in the participant panel indicates overall af-
fect in a given participant?s utterances for that time 
frame. 
3.1.3 Participants 
The participant panel (Figure 6) consists of three 
parts:  speaker labels, speaker contribution bar, and 
affect score. The speaker label is displayed in al-
phabetical order and is grayed out if there are no 
utterances containing the topic in the selected time 
frame. The speaker contribution bar, displayed as a 
horizontal histogram, shows the speaker?s propor-
tion of utterances during the time frame. Non ques-
tion utterances are displayed in red while 
utterances containing questions are displayed in 
green as seen. For example, in Figure 6, we can see 
that speaker me011 did most of the talking (and 
was generally negative), but speaker me018 had a 
higher proportion of questions.  
 
 
Figure 6. Participant Panel. 
 
3.1.4 Named Entities 
In the current implementation, the named entity 
panel consists of only list of entity labels present in 
a given time frame. We do not list each named en-
tity because of space constraints, but plan to inte-
grate a scroll bar so that we can display individual 
entities as opposed to the category labels. 
55
3.2 Semantic Graph 
Data that is viewed in the main UI can be sent to a 
semantic graph for further analysis. The graph al-
lows a user to choose to highlight the relationships 
associated with a topic, participant, or individual 
named entity. The user selects objects of interest 
from a list (see Figure 7), then the graph function 
organizes a graph according to the chosen object, 
see Figure 8, that extracts the information from the 
time-linked view and represent it in a more abstract 
view that denotes relationships via links and nodes.  
 
 
Figure 7. Semantic Graph Node Selection. 
 
The semantic graph can help highlight relation-
ships that might be hard to view in the main UI. 
For example, Figure 8 represents a subset of the 
Blackout data in which three participants, indicated 
by blue, all talked about the same named entity, 
indicated by green, but never talked to each other, 
indicated by the red conversation nodes.  
 
 
Figure 8. Graph of the Relationship between Three Par-
ticipants.  
 
4 Discussion 
In this paper, we have presented ChAT, a system 
designed to aid in the analysis of any kind of 
threaded dialogue. Our system is designed to be 
flexible in that the UI and processing components 
work with multiple data types. The processing 
components can be used independently, or within 
the UI. The UI aids users in in-depth analysis of 
individual conversations. The components can be 
run independent of the UI and in batch, resulting in 
an xml document containing the original tran-
scripts and the metadata added by the processing 
components. This functionality allows the data to 
be manipulated by traditional text mining tech-
niques, or to be viewed in any other visualization.  
We have not performed user evaluation on the 
interface. Our topic segmentation performs better 
than the current state of the art, and named-entity 
extraction we have integrated is commercial grade. 
We are currently working on an evaluation of the 
affect scoring. While our topic segmentation is 
good, we are working to improve the labels we use 
for the topics. Most importantly, we plan on ad-
dressing the usefulness of the UI with user studies. 
 
References 
R. Barzilay and M. Elhadad. Using lexical chains for 
text summarization. In Proc.of the Intelligent Scal-
able Text Summarization Workshop (ISTS?97), ACL, 
1997. 
D. Beeferman, A. Berger, and J. Lafferty. 1999. Statisti-
calmodels for text segmentation. Machine Learning, 
34(1?3):177?210. 
Carletta, J.C. and Kilgour, J. (2005) The NITE XML 
Toolkit Meets the ICSI Meeting Corpus: Import, An-
notation, and Browsing. MLMI'04: Proceedings of 
the Workshop on Machine Learning for Multimodal 
Interaction. Samy Bengio and Herve Bourlard, eds. 
Springer-Verlag Lecture Notes in Computer Science 
Volume 3361.  
 F. Choi. 2000. Advances in domain independent linear 
text segmentation. In Proc. of NAACL?00. 
van Deemter, Emiel Krahmer & Mari?t Theune. 2005 
.Real versus template-based Natural Language Gen-
eration: a false opposition?  Computational Linguis-
tics 31(1), pages 15-24. 
 
 M. Galley, Kathleen McKeown, Eric Fosler-Lussier, 
Hongyan Jing. Discourse Segmentation of Multi-
56
party Conversation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational 
Linguistics (ACL-03). July 2003. Sapporo, Japan. 
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. 
Williams, and J. Bensley. 2003. Answer Mining by 
Combining Extraction Techniques with Abductive 
Reasoning, Proceedings of the Twelfth Text Retrieval 
Conference (TREC ):375. 
M. A. Hearst. TexTiling: Segmenting text info multi-
paragraph subtopic passages. Computational Linguis-
tics, 23(1):33?64, 1997. 
J. Hirschberg and C. Nakatani. A prosodic analysis of 
discourse segments in direction-giving monologues. 
In Proc. ACL, pp. 286?293, Santa Cruz, CA, 1996. 
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart,N. 
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, 
and C. Wooters. 2003. The ICSI meeting corpus. In 
Proc. of ICASSP-03, Hong Kong 
N. E. Miller, P. Wong, M. Brewster, and H. Foote. 
TOPIC ISLANDS - A wavelet-based text visualiza-
tion system. In David Ebert, Hans Hagen, and Holly 
Rushmeier, editors, IEEE Visualization '98, pages 
189-196, 1998. 
A. Stolcke, E. Shriberg, D. Hakkani-Tur, G. Tur, Z. 
Rivlin, K. Sonmez (1999), Combining Words and 
Speech Prosody for Automatic Topic Segmentation. 
Proc. DARPA Broadcast News Workshop, pp. 61-64, 
Herndon, VA.  
 
P. Stone, 1977. Thematic text analysis: new agendas for 
analyzing text content. in Text Analysis for the Social 
Sciences ed. Carl Roberts. Lawrence Erlbaum Asso-
ciates. 
 
 
 
 
 
 
57
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 264?267,
Prague, June 2007. c?2007 Association for Computational Linguistics
PNNL: A Supervised Maximum Entropy Approach to Word Sense 
Disambiguation
Stephen Tratz, Antonio Sanfilippo, Michelle Gregory, Alan Chappell, Christian 
Posse, Paul Whitney
Pacific Northwest National Laboratory
902 Battelle Blvd, PO Box 999
Richland, WA 99352, USA
{stephen.tratz, antonio.sanfilippo, michelle, alan.chap-
pell, christian.posse, paul.whitney}@pnl.gov
Abstract
In  this  paper,  we  described  the  PNNL 
Word Sense Disambiguation system as ap-
plied  to  the  English  all-word  task  in  Se-
mEval 2007. We use a supervised learning 
approach,  employing  a  large  number  of 
features and using Information Gain for di-
mension  reduction.  The  rich  feature  set 
combined with a Maximum Entropy classi-
fier  produces results  that  are significantly 
better than baseline and are the highest F-
score  for  the  fined-grained  English  all-
words subtask of SemEval.
1 Introduction
Accurate  word  sense  disambiguation  (WSD)  can 
support  many  natural  language  processing  and 
knowledge management  tasks.  The  main goal  of 
the  PNNL  WSD system  is  to  support  Semantic 
Web applications, such as semantic-driven search 
and  navigation,  through  a  reliable  mapping  of 
words  in  naturally  occurring  text  to  ontological 
classes.  As described  in  Sanfilippo et  al.  (2006), 
this goal is achieved by defining a WordNet-based 
(Fellbaum,  1998)  ontology that  offers  a manage-
able set of concept classes, provides an extensive 
characterization of concept class in terms of lexical 
instances, and integrates an automated class recog-
nition algorithm. We found that the same features 
that are useful for predicting word classes are also 
useful in distinguishing individual word senses. 
Our main objective in this paper is to predict in-
dividual word senses using a large combination of 
features  including contextual,  semantic,  and syn-
tactic information. In our earlier paper (Sanfilippo 
et al, 2006), we reported that the PNNL WSD sys-
tem exceeded the performance of the best perform-
ers  for  verbs  in  the  SENSEVAL-3  English  all-
words task dataset. SemEval 2007 is our first op-
portunity  to  enter  a  word  sense  disambiguation 
competition.
2 Approach
While many unsupervised word sense disambigua-
tion systems have been created, supervised systems 
have generally produced superior  results  (Snyder 
and Palmer, 2004; Mihalcea et al, 2004). Our sys-
tem is based on a supervised WSD approach that 
uses  a  Maximum  Entropy  classifier  to  predict 
WordNet senses.
We  use  SemCor1,  OMWE 1.0  (Chklovski  and 
Mihalcea, 2002), and example sentences in Word-
Net  as  the  training  corpus.  We  utilize  the 
OpenNLP MaxEnt  implementation2 of  the  maxi-
mum  entropy  classification  algorithm  (Berger  et 
al.,  1996)  to  train  classification  models  for  each 
lemma and part-of-speech combination in the train-
ing  corpus.  These  models  are  used  to  predict 
WordNet  senses  for  words found in natural  text. 
For  lemma  and  part-of-speech  combinations  that 
are not present in the training corpus, the PNNL 
WSD system defaults to the most frequent Word-
Net sense.
2.1 Features
We use a rich set of features to predict individual 
word senses.  A large number of  features are ex-
tracted for each word sense instance in the training 
data.  Following  Dang & Palmer  (2005)  and Ko-
homban & Lee (2005), we use contextual, syntac-
tic and semantic  information to inform our word 
1
 http://www.cs.unt.edu/~rada/downloads.html. 
2
 http://maxent.sourceforge.net/.
264
sense disambiguation system. However,  there are 
significant  differences  between the specific  types 
of contextual,  syntactic  and semantic information 
we use in our system and those proposed by Dang 
& Palmer  (2005)  and Kohomban  & Lee (2005). 
More specifically,  we employ novel  features  and 
feature combinations, as described below. 
? Contextual information. The contextual infor-
mation we use includes the word under analy-
sis plus the three tokens found on each side of 
the word, within sentence boundaries. Tokens 
include both words and punctuation.
? Syntactic information. We include grammatical 
dependencies  (e.g.  subject,  object)  and  mor-
pho-syntactic  features such as part of speech, 
case, number and tense. We use the Connexor 
parser3 (Tapanainen and J?rvinen, 1997) to ex-
tract lemma information, parts of speech, syn-
tactic  dependencies,  tense,  case,  and  number 
information.  A sample output  of  a  Connexor 
parse is given in Table 1. Features are extract-
ed  for  all  tokens  that  are  related  through no 
more than 3 levels of dependency to the word 
to be disambiguated. 
? Semantic  information.  The semantic  informa-
tion  we  incorporate  includes  named  entity 
types (e.g. PERSON, LOCATION, ORGANI-
ZATION) and hypernyms. We use OpenNLP4 
and  LingPipe5 to  identify  named  entities,  re-
placing the strings identified as named entities 
(e.g., Joe Smith) with the corresponding entity 
type  (PERSON).  We also  substitute  personal 
pronouns  that  unambiguously  denote  people 
with the entity type PERSON. Numbers in the 
text  are  replaced  with  type  label  NUMBER. 
Hypernyms  are  retrieved  from WordNet  and 
added to the feature set for all noun tokens se-
lected by the contextual and syntactic rules. In 
contrast to Dang & Palmer (2005), we only in-
clude  the  hypernyms  of  the  most  frequent 
sense,  and  we  include  the  entire  hypernym 
chain (e.g. motor, machine, device, instrumen-
tality, artifact, object, whole, entity).
To address feature extraction processes specific 
to  noun and verbs,  we add the  following  condi-
tions.
3
 http://www.connexor.com/.
4
 http://opennlp.sourceforge.nt/.
5
 http://www.alias-i.com/lingpipe/.
? Syntactic  information  for  verbs.  If  the  verb 
does not have a subject, the subject of the clos-
est ancestor verb in the syntax tree is used in-
stead.
? Syntactic information for nouns. The first verb 
ancestor in the syntax tree is also used to gen-
erate features. 
? Semantic information for nouns. A feature in-
dicating whether a token is capitalized for each 
of the tokens used to generate features.
A sample of the resulting feature vectors that are 
used by the PNNL word sense disambiguation sys-
tem is presented in Table 2.
ID Word Lemma Grammatical 
Dependen-
cies
Morphosyntactic 
Features
1
2
3
4
5
6
the
engine
throbbe
d
into
life
.
the
engine
throb
into
life
.
det:>2
subj:>3
main:>0
goa:>3
pcomp:>4
@DN> %>N DET
@SUBJ %NH N NOM SG
@+FMAINV %VA V PAST
@ADVL %EH PREP
@<P %NH N NOM SG
Table 1. Connexor sample output for the sentence 
?The engine throbbed into life?.
the pre:2:the, pre:2:pos:DET, det:the, det:pos:DET, 
hassubj:det:
engine pre:1:instrumentality, pre:1:object, pre:1:artifact,
 pre:1:device, pre:1:engine, pre:1:motor, pre:1:whole, 
pre:1:entity, pre:1:machine, pre:1:pos:N, 
pre:1:case:NOM, 
pre:1:num:SG,subj:instrumentality,subj:object, subj:arti-
fact, subj:device, subj:engine, subj:motor, subj:whole, 
subj:entity, subj:machine, subj:pos:N, hassubj:, 
subj:case:NOM, subj:num:SG,
throbbed haspre:1:,haspre:2:,haspost:1:, haspost:2:, haspost:3:,
self:throb, self:pos:V, main:,throbbed, self:tense:PAST
into post:1:into, post:1:pos:PREP, goa:into, goa:pos:PREP, 
life post:2:life, post:2:state, post:2:being, post:2:pos:N, 
post:2:case:NOM, post:2:num:SG, hasgoa:, pcomp:life, 
pcomp:state, pcomp:being, pcomp:pos:N, 
hasgoa:pcomp:, goa:pcomp:case:NOM, 
goa:pcomp:num:SG
. post:3:.
Table  2. Feature  vector  for  throbbed in the sen-
tence ?The engine throbbed into life?.
As the example in Table 2 indicates, the combi-
nation of contextual, syntactic, and semantic infor-
mation types results in a large number of features. 
Inspection  of  the  training data  reveals  that  some 
features may be more important than others in es-
tablishing word sense assignment for each choice 
of word lemma. We use a feature selection proce-
265
dure to reduce the full set of features to the feature 
subset that is most relevant to word sense assign-
ment for each lemma. This practice improves the 
efficiency of our word sense disambiguation algo-
rithm. The feature selection procedure we adopted 
consists of scoring each potential feature according 
to  a  particular  feature  selection  metric,  and  then 
taking the best k features.
We choose Information Gain as our feature se-
lection metric. Information Gain measures the de-
crease in entropy when the feature is given versus 
when it is absent. Yang and Pederson (1997) report 
that  Information Gain outperformed other feature 
selection  approaches  in  their  multi-class  bench-
marks,  and  Foreman  (2003)  showed  that  it  per-
formed amongst the best for his 2-class problems. 
3 Evaluation
To evaluate our approach and feature set, we ran 
our  model  on  the  SENSEVAL-3  English  all-words 
task test data. Using data provided by the SENSE-
VAL website6, we were able to compare our results 
for  verbs  to  the  top  performers  on  verbs  alone. 
Upali S. Kohomban and Wee Sun Lee provided us 
with  the  results  file  for  the  Simil-Prime  system 
(Kohomban and Lee, 2005). As reported in Sanfil-
ippo et al (2006) and shown in table 3, our results 
for verbs rival those of top performers. We had a 
significant  improvement  (p-value<0.05)  over  the 
baseline of  52.9%, a marginal  improvement  over 
the second best performer (SenseLearner) (Mihal-
cea and Faruque, 2004), and we were as good as 
the top performer (GAMBL) (Decadt et al, 2004).7
System Precision Fraction of 
Recall
Our system 61% 22%
GAMBL 59.0% 21.3%
SenseLearner 56.1% 20.2%
Baseline 52.9% 19.1%
Table 3. Results for verb sense disambiguation on 
SENSEVAL-3 data, adapted from Sanfilippo et al 
(2006).
Since then, we have expanded our evaluation to 
all parts of speech. Table 4 provides the evaluation 
6
 http://www.senseval.org/.
7
 The 2% improvement in precision which our system 
showed as  compared to GAMBL was not statistically 
significant (p=0.21).
of our system as compared  to  the three top per-
formers on the SENSEVAL-3 data and the baseline. 
The baseline of 0.631 F-score8 was computed us-
ing the most frequent WordNet sense. The PNNL 
WSD system performs significantly better than the 
baseline (p-value<0.05) and rivals the top perform-
ers.  The performance of the PNNL WSD system 
relative to the other three systems and the baseline 
remains unchanged when the unknown sense an-
swers  (denoted  by a  ?U?)  are  excluded  from the 
evaluation.
System Precision Recall
PNNL 0.670 0.670
Simil-Prime 0.661 0.663
GAMBL 0.652 0.652
SenseLearner 0.646 0.646
Baseline 0.631 0.631
Table 4. SENSEVAL-3 English all-words.
System Recall Precision 
PNNL 0.669 0.671
GAMBL 0.651 0.651
Simil-Prime 0.644 0.657
SenseLearner 0.642 0.651
Baseline 0.631 0.631
Table 5. SENSEVAL-3 English all-words, No ?U?.
4 Experimental  results  on  SemEval  all-
words subtask
This was our first opportunity to test our model in 
a WSD competition. For this competition, we fo-
cused our efforts  on the fine-grained English all-
words task because our system was set up to per-
form fine-grained WordNet  sense  prediction.  We 
are  pleased that  our  system achieved the  highest 
score for this subtask. Our results for the SemEval 
dataset as compared to baseline are reported in Ta-
ble 6. The PNNL WSD system did not assign the 
unknown sense, ?U?, to any word instances in the 
SemEval dataset.
8
 This baseline is slightly higher than that reported by 
others (Snyder and Palmer 2004).
266
System F-score
PNNL 0.591
Baseline 0.514
p-value <0.01
Table 6. SemEval Results.
5 Discussion
Although these results are promising, there is still 
much work to be done. For example, we need to 
investigate the contribution of each feature to the 
overall performance of the system in terms of pre-
cision and recall. Such a feature sensitivity analysis 
will provide us with a better understanding of how 
the algorithm can be further improved and/or made 
more efficient by leaving out features whose con-
tribution is negligible. 
Another important point to make is that, while 
our system shows the best precision/recall results 
overall,  we  can  only  claim  statistical  relevance 
with  reference  to  the  baseline  and  results  worse 
than  baseline.  The  size  of  the  SemEval  data  set 
(N=465) is too small to establish whether the dif-
ference in precision/recall results with the other top 
systems is statistically significant. 
Acknowledgements
We would like to thank Upali  S. Kohomban and 
Wee Sun Lee for  providing us with their  SENSE-
VAL-3 English all-words task results file for Simil-
Prime. Many thanks also to Patrick Paulson, Bob 
Baddeley, Ryan Hohimer, and Amanda White for 
their  help  in  developing  the  word  class  disam-
biguation system on which the work presented in 
this paper is based.
References
Berger, A., S. Della Pietra and V. Della Pietra (1996) A 
Maximum  Entropy  Approach  to  Natural  Language 
Processing.  Computational  Linguistics,  volume  22, 
number 1, pages 39-71.
Chklovski, T. and R. Mihalcea (2002) Building a sense 
tagged corpus with open mind word expert. In  Pro-
ceedings of the ACL-02 workshop on Word sense dis-
ambiguation: recent successes and future directions.
Dang, H. T. and M. Palmer (2005) The Role of Semant-
ic Roles in Disambiguating Verb Senses. In Proceed-
ings of the 43rd Annual Meeting of the Association  
for Computational Linguistics,  Ann Arbor MI, June 
26-28, 2005. 
Decadt,  B., V. Hoste,  W. Daelemans and A. Van den 
Bosch (2004) GAMBL, genetic  algorithm optimiza-
tion of memory-based WSD. SENSEVAL-3: Third In-
ternational Workshop on the Evaluation of Systems  
for the Semantic Analysis of Text. Barcelona, Spain. 
Fellbaum,  C.,  editor.  (1998)  WordNet:  An  Electronic 
Lexical Database. MIT Press, Cambridge, MA.
Foreman, G. (2003) An Extensive Empirical Study of 
Feature  Selection  Metrics  for  Text  Classification. 
Journal  of  Machine  Learning  Research,  3,  pages 
1289-1305. 
Kohomban, U. and  W. Lee (2005) Learning semantic 
classes  for  word sense disambiguation.  In Proceed-
ings of the 43rd Annual meeting of the Association for  
Computational Linguistics, Ann Arbor, MI.
Mihalcea,  R.,  T.  Chklovski,  and  A.  Kilgarriff  (2004) 
The  SENSEVAL-3  English   Lexical  Sample  Task, 
SENSEVAL-3: Third International Workshop on the  
Evaluation of  Systems for the Semantic Analysis of  
Text. Barcelonna, Span.
Mihalcea,  R.  and  E.  Faruque   (2004)  SenseLearner: 
Minimally supervised word sense disambiguation for 
all words in open text.  SENSEVAL-3: Third Interna-
tional Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text. Barcelona, Spain.
Sanfilippo, A.,  S.  Tratz,  M. Gregory, A.  Chappell,  P. 
Whitney, C. Posse, P. Paulson, B. Baddeley, R. Hohi-
mer,  A.  White  (2006)  Automating  Ontological  An-
notation with WordNet. Proceedings to the Third In-
ternational WordNet Conference, Jan 22-26, Jeju Is-
land, Korea.
Snyder,  B.   and  M.  Palmer.  2004.  The  English  All-
Words  Task.  SENSEVAL-3:  Third  International 
Workshop on the Evaluation of  Systems for the Se-
mantic Analysis of Text. Barcelona, Spain. 
Tapanainen, P. and Timo J?rvinen (1997) A nonproject-
ive  dependency  parser.  In  Proceedings  of  the  5th 
Conference on Applied Natural Language Processing, 
pages 64?71, Washington D.C. Association for Com-
putational Linguistics.
Yang,  Y.  and  J.  O.  Pedersen  (1997)  A  Comparative 
Study on Feature Selection in Text Categorization. In 
Proceedings of the 14th International Conference on  
Machine Learning (ICML), pages 412-420, 1997.
267
